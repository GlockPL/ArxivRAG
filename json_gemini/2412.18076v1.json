{"title": "COMO: Cross-Mamba Interaction and Offset-Guided Fusion for Multimodal Object Detection", "authors": ["Chang Liu", "Xin Ma", "Xiaochen Yang", "Yuxiang Zhang", "Yanni Dong"], "abstract": "Single-modal object detection tasks often experience performance degradation when encountering diverse scenarios. In contrast, multimodal object detection tasks can offer more comprehensive information about object features by integrating data from various modalities. Current multimodal object detection methods generally use various fusion techniques, including conventional neural networks and transformer-based models, to implement feature fusion strategies and achieve complementary information. However, since multimodal images are captured by different sensors, there are often misalignments between them, making direct matching challenging. This misalignment hinders the ability to establish strong correlations for the same object across different modalities. In this paper, we propose a novel approach called the Cross-Mamba interaction and Offset-guided fusion (COMO) framework for multimodal object detection tasks. The COMO framework employs the cross-mamba technique to formulate feature interaction equations, enabling multimodal serialized state computation. This results in interactive fusion outputs while reducing computational overhead and improving efficiency. Additionally, COMO leverages high-level features, which are less affected by misalignment, to facilitate interaction and transfer complementary information between modalities, addressing the positional offset challenges caused by variations in camera angles and capture times. Furthermore, COMO incorporates a global and local scanning mechanism in the cross-mamba module to capture features with local correlation, particularly in remote sensing images. To preserve low-level features, the offset-guided fusion mechanism ensures effective multiscale feature utilization, allowing the construction of a multiscale fusion data cube that enhances detection performance. The proposed COMO approach has been evaluated on three benchmark multimodal datasets consisting of RGB and infrared image pairs, demonstrating state-of-the-art performance in multimodal object detection tasks. It offers a solution tailored for remote sensing data, making it more applicable to real-world scenarios.", "sections": [{"title": "1. Introductions", "content": "The object detection task enables rapid interpretation of images and the identification of object locations. As a key task in computer vision, it has been widely applied in various fields, including autonomous driving, remote sensing, and medical imaging (Zou et al., 2023; Ma et al., 2024b). However, in complex environments, such as low-light conditions, variable weather, and partial occlusion, the performance of single-modal object detection declines due to its limited capacity to effectively capture the salient features of the objects(Li et al., 2019; Liu et al., 2024a).\nMultimodal visual data, comprising images acquired from different sensors (Cao et al., 2019; Li et al., 2022) (e.g., RGB cameras, infrared sensors, Lidar, and Radar), offers a richer set of feature attributes for object detection(Guan et al., 2019; Zhou et al., 2020). By integrating multimodal data, complementary information can be leveraged, allowing the objects to exhibit distinct and prominent features across diverse scenarios.\nRecent advancements in multimodal fusion techniques have led to significant improvements in detection performance. Approaches such as pixel-level fusion (Zhang et al., 2023a), feature-level fusion (Qingyun et al., 2021; Xiao et al., 2024; Zhang et al., 2019), and decision-level fusion (Zhu et al., 2023) enable the effective integration of data from multiple modalities. These methods exploit the complementary nature of multimodal data, maximizing the information available about the object, thereby improving detection accuracy and robustness in challenging environments.\nDespite these advances, there remain challenges in multimodal object detection tasks. One of those is the misalignment between data from different sensors (Song et al., 2024). This misalignment can be caused by variations in camera angles, capture times, or sensor characteristics, leading to discrepancies in object positions and features. As shown in Fig. 1, misalignment problems caused by differences in capture times and camera angles are common challenges in remote sensing multimodal data fusion tasks. Regarding the capture time issue, remote sensing data typically originate from airborne platforms, such as satellites and drones, which provide a high viewing angle and wide coverage area. When capturing a high-speed moving target, even a brief time interval between shots can cause significant positional changes due to the rapid movement, leading to noticeable offsets. This positional shift can compromise the accuracy of detection, particularly in applications that demand precise target recognition and tracking. As for the camera angle issue, in multimodal data acquisition, two or more cameras are commonly used for simultaneous capture, such as optical and infrared cameras. However, differences in camera positions and viewing angles often result in misaligned imaging positions of the same target across different modalities. These discrepancies complicate data alignment and fusion, potentially reducing application accuracy and hindering the establishment of strong correlations between objects across modalities, thereby making accurate object detection in multimodal data more challenging (Chen et al., 2024a). More rigorously, we have analyzed the misalignment problem in DroneVehicle dataset, a large-scale UAV multimodal dataset (Sun et al., 2022). The results reveal that up to 35% labels exhibit offset issues, with some labels showing significant displacement, as shown in Fig. 2. Objects with a pixel offset of 1 to 5 pixels accounted for over 90% of all misaligned objects. This substantially impacts the accuracy of multimodal detection. Therefore, effective fusion strategies that account for offset corrections are crucial for enhancing the performance of multimodal remote sensing object detection tasks.\nFurthermore, multimodal data inherently contain more information compared to single-modal data, which increases the time required for data processing (Qingyun et al., 2021). Recently, feature-level fusion methods have gained popularity, yielding increasingly accurate results (Shen et al., 2024; Xiao et al., 2024). However, the dual-branch feature extraction structures and multiscale fusion mechanisms employed in these methods significantly increase computational resource demands and processing time (Song et al., 2024; Zhu et al., 2023). To address this issue, it is essential to develop efficient multimodal fusion strategies that can maintain high detection accuracy while streamlining the model for real-time processing.\nIn this work, to mitigate misalignment effects, reduce computational resources and time consumption, and enhance multimodal object detection performance, we propose a new method: CrOss-Mamba interaction and Offset-guided fusion (COMO) framework. The COMO framework incorporates the novel mamba technique (Gu and Dao, 2023) to develop the cross-mamba method, which formulates feature interaction equations, enabling serialized state computation. This approach reduces computational load and time consumption compared to current transformer-based methods.(Qingyun et al., 2021; Shen et al., 2024). Additionally, COMO leverages high-level features, which are less prone to mismatches, to facilitate inter-modal interactions and information fusion, addressing positional offset issues arising from variations in camera angles and capture times. COMO also incorporates a global and local scanning mechanism within the cross-Mamba method to capture features that encompass both global sequence information and local relevance, particularly in remote sensing images. To preserve low-level features, the offset-guided fusion mechanism ensures efficient multiscale feature utilization, thereby maximizing available information. Evaluated on three benchmark"}, {"title": "2. Related works", "content": "In recent years, research in visual multimodal fusion and object detection has gained significant attention due to the limitations of single-modal approaches in complex environments. This section reviews key contributions to the fields of visual multimodal fusion, multimodal object detection, and a notable deep learning model, the mamba model."}, {"title": "2.1. Visual Multimodal Fusion", "content": "Single-modal data is highly susceptible to situational changes, often leading to poor detection results (Sharma et al., 2020; Liu et al., 2024a). For instance, RGB images may perform well in clear conditions, but their effectiveness is significantly diminished in complex scenarios such as nighttime or cloudy weather (Redmon et al., 2016; Carion et al., 2020). Introducing additional visual modalities can greatly enhance the robustness of vision tasks by compensating for these limitations (Zhang et al., 2023a; Xiao et al., 2024).\nVisual multimodal fusion leverages data from multiple sensors or modalities (e.g., RGB, infrared) to enrich feature representations of objects. Various fusion methods have been proposed to exploit complementary information across modalities. Early approaches can be categorized into two groups: transform domain-based methods and spatial domain-based methods. Transform domain fusion methods are the focus of early research, and typical methods (Yin et al., 2018) include wavelet transform, curved wavelet transform, Laplace pyramid. These methods can effectively retain the detailed information in multimodal images by decomposing and reconstructing the image signals at different scales and frequencies. Li et al. (2017) reviewed the development of pixel-level fusion techniques, pointing out the wide application of transform techniques such as wavelets and curvilinear waves in image fusion. With the development of research, multimodal image fusion methods combining wavelet transform and deep learning have also emerged. Deng and Dragotti (2020) proposed a fusion method combining wavelet transform which showed better performance. Spatial domain-based image fusion methods directly process the original image pixel values and utilize local features, spatial frequency, or gradient information for fusion (Meher et al., 2019). These methods decompose and reconstruct the image by means of Laplace pyramid, Non-Subsampled Contour Wave Transform (NSCT), etc., and are more suitable for dealing with edge and detail information in the image. For example, the Laplace pyramid proposed by Burt and Adelson (1987) became"}, {"title": "2.2. Multimodal Object Detection", "content": "Unlike multimodal image fusion which aims at obtaining better visualization, multimodal object detection task is result-oriented and is more task-specific. Visual multimodal object detection extends traditional object detection tasks by incorporating multimodal data to enhance detection performance. It aims to utilize the complementary information between different modalities to enhance the robustness and accuracy of object detection, especially in complex environments, bad weather, or occlusion situations. Depending on the fusion strategy, multimodal object detection can be categorized into pixel-level fusion, feature-level fusion, and decision-level fusion methods.\nPixel-level fusion directly splices or overlays raw data from different modalities (e.g., RGB images and infrared images) and inputs them into the same object detection network. This approach often does not distinguish between modalities, but rather unifies all data as input to the network. Since the effect of direct splicing and then detection is not ideal, YOLOrs (Sharma et al., 2020) proposes a two-stage fusion method, which involves using deep convolutional networks to extract features from each modality independently. The extracted features are then fused through concatenation and element-wise cross-product operations, maximizing the information in the fused data body. SuperYOLO (Zhang et al., 2023a) proposes a fusion method called multimodal fusion(MF) to extract complementary information from various data to improve the small target detection task in remote sensing, and it pioneers the super-resolution branch to enhance the accuracy of the backbone feature extraction network.\nThe feature-level fusion approach is characterized by multiscale feature fusion and richer information retention, which enhances model robustness, strengthens generalization ability, and reduces information loss, making it particularly popular in current research (Guo et al., 2020). Numerous methods (Qingyun et al., 2021; Shen et al., 2024; Xiao et al., 2024; Song et al., 2024) have been proposed to improve the multimodal object detection results continuously. CFT (Qingyun et al., 2021) pioneered the use of the transformer framework in the field of multimodal object detection, which is based on the principle of splicing multimodal data patches and feeding them simultaneously into a self-attention structure to obtain inter-modal global attention results. ICAFusion (Shen et al., 2024), on the other hand, utilizes the cross-attention mechanism for inter-modal feature interaction fusion. CMADet (Song et al., 2024) aims to solve the problem of data misalignment between two modalities and realize multiscale feature alignment and detection. GM-DETR (Xiao et al., 2024) utilizes the state-of-the-art RT-DETR framework and proposes a novel training strategy, namely the modal complementation strategy. Performing two-stage training can give the model a better modal adaptation effect. OAFA (Chen et al., 2024a) is another multimodal detection method that accounts for feature misalignment between modalities. Its approach focuses on mitigating the impact of modality gaps on multimodal spatial matching by obtaining modality-invariant features within a shared subspace, thereby estimating precise offset values.\nIn decision-level fusion methods, object detectors are first independently trained for each modality. The results from each detector are then combined using strategies such as voting, weighted averaging, and other techniques to derive the final detection outcome. The MFPT method (Zhu et al., 2023) employs intra-modal and inter-modal transforms to enhance individual modal features, ultimately realizing an enhanced, feature-based decision-level fusion approach.\nThe multimodal data are acquired by two or more sensors, which are side-by-side and acquire data with different field-of-view angles. Moreover, there are data acquisition time differences between different sensors, which have a huge impact on objects moving at high speeds. Therefore, it is necessary to take into account the offset problem between the data when data fusion is performed. Meanwhile, multimodal object detection has more multi-branch feature extraction networks and feature fusion modules compared to single-modal object detection, so the time consumption increases. In order to make the multimodal object detection model have real-time capability, it is necessary to improve the detection accuracy while streamlining the model."}, {"title": "2.3. Mamba model", "content": "The mamba model (Gu and Dao, 2023) is an efficient sequence feature extraction model that has emerged in recent times. Its core idea is to selectively use the state spaces model (SSM) in sequence modeling to balance modeling power and computational efficiency. Compared to traditional recurrent neural networks (RNNs) or autoregressive models (e.g., Transformer), mamba maintains linear time complexity when processing long sequences by utilizing an efficient state space model. As a result, it can be computed efficiently even as the sequence length increases. When applied to the field of computer vision, mamba has achieved excellent results across various tasks.\nVision mamba (Zhu et al., 2024) is the first approach to introduce mamba models from natural language into computer vision. It draws on the ideas of ViT (Dosovitskiy, 2020) and proposes a bi-directional scanning mechanism to serialize the image data, thus allowing the overall model to achieve global attention and feature association. It proves the effectiveness of the mamba model for a wide range of visual tasks, opening up new paths for the field of computer vision. ChangeMamba (Chen et al., 2024b) explores for the first time the potential of the mamba architecture for remote sensing change detection tasks. U-Mamba (Ma et al., 2024a) designs a hybrid CNN-SSM module that integrates the local feature extraction capability of convolutional layers with the ability of SSM to capture long-range dependencies for use in the field of medical image segmentation. FusionMamba (Xie et al., 2024) explores the potential of the SSM model in the field of image fusion, which utilizes the mamba model to design a U-Net structure to fuse data from"}, {"title": "3. Methods", "content": "This section presents a detailed overview of the COMO approach, as illustrated in Fig. 3. We first introduce the overall structure of the COMO approach, followed by detailed descriptions of the key components: Mamba Interaction Block, Global and Local Scan Method, and Offset-Guided Fusion."}, {"title": "3.1. Overall Structure", "content": "Given a pair of visible and infrared images {Xrgb, Xir}, the proposed COMO approach obtains detection results beyond a single modality by performing intermodal interaction and fusion. To be specific, Xrgb and Xir are"}, {"title": "3.2. Mamba Interaction Block", "content": "The Mamba Interaction Block is shown in Fig. 4. It comprises two modules, the single-mamba block and the cross-mamba block. To leverage mamba's efficient feature extraction, we first implement two single-mamba blocks to extract features from single-modal data. These blocks serialize the output from the CNN backbone network, capturing global historical state information through various scanning modes. The blocks adapt the extracted features into sequences by repeatedly applying these operations. This method is specifically used for high-level features Sir and Srgb, which contain substantial semantic information and are minimally affected by spatial offsets.\nFor the input features represented as $S_{in}$, we apply adaptive max pooling and mean pooling to construct feature matrices $F_{in} \\in R^{H\\times W\\times C}$, ensuring consistent dimensions across varying image sizes:\n$F_{in} = P_{avg}(S_{in}) + P_{max}(S_{in}).$\nWe then perform a deep feature mapping of $F_{in}$ and add the dropout operator (Srivastava et al., 2014) thus making the model adaptive.\n$F_{m} = Drop(F_{h\\to C}(Silu(F_{C\\to h}(F_{in}))))$.\nHere, h is the channels of hidden features in the mapping process, $F(\\cdot)$ is the linear mapping operation, $Drop(\\cdot)$ means the randomized discarding of neurons with some probability, and $Silu(.)$ means the activation function for nonlinearization. The resulting tensor $F_{m}$ is then flattened into a token sequence, $I_{in} \\in R^{HW\\times C}$, simulating sequential data for state-space model algorithms. To mitigate the loss of two-level spatial information, we incorporate a learnable positional embedding $P\\in R^{HW\\times C}$ which provides explicit positional encoding. Finally, we establish shortcut data streams for manipulation, maintaining the integrity of the original feature extraction. After that, $I_{in}$ will be scanned in four directions expanding the serialization approach with positional encoding, thus expanding the data distribution. Then, the scanning results of each direction will go through S6 block separately for sequence feature state space model feature extraction, resulting in four outputs denoted as $y_{i}$:\n$\\{x_{i} = cross scan_{i}(I_{in}), \\\\ y_{i} = S6_{i}(x_{i}),\\}_{i = 1, 2, 3, 4}.$\n$I_{out} = \\sum_{i=1}^{4} reverse scan (y_{i}).$"}, {"title": "3.3. Global and Local Scan Method", "content": "The core of the mamba model is the S6 block, which excels at processing one-dimensional causal sequential data. However, the typical method of serializing images in visual imagery often relies on a global sequential scanning approach, similar to the global sequential modeling in Vim (Zhu et al., 2024) and VMamba (Liu et al., 2024b). While this approach is effective for language modeling, where understanding dependencies between consecutive words is essential, it does not align with the non-causal nature of 2D spatial relationships in images. Simple global serialized scanning can weaken the model's ability to discern these spatial relationships effectively.\nUnlike transformers, which compute relationships between all spatial locations, the mamba model focuses on the state relationships between neighboring locations. In remote sensing imagery, the relationships between objects and the global context are often less critical than those within the visual images. Consequently, the use of global scanning can undermine the strengths of the mamba model by weakening the association of objects distributed in a local region.\nTo address this issue, we propose the local scan method (LS) that divides the image into different windows to capture local dependencies while maintaining a global perspective efficiently. This strategy allows the model to focus on local relationships within each window while still considering the global context. By incorporating local scanning into the mamba model, we aim to enhance the model's ability to capture spatial relationships in visual imagery, particularly in remote sensing applications. As shown in Fig. 5, the LS method divides the image into multiple windows and scans each window sequentially. The local window size is a hyperparameter that can be adjusted based on the specific task requirements. We set the window size at most one-third of the image size to ensure that the model captures local dependencies effectively. As Fig. 4(b), Cross Mamba Block part shows we add 2 directions of local scan to the cross-mamba block to build the Global and Local Scan method which enabling the mamba interaction block to capture both local and global spatial relationships, enhancing its performance in visual multimodal object detection tasks."}, {"title": "3.4. Offset-Guided Fusion", "content": "To address the limitation of high-level features, which are less affected by offset but lack low-level texture details, we design an Offset-Guided Fusion module. This module integrates high-level features after interaction with low-level features via a top-down feature pyramid networks (FPN) (Lin et al., 2017) and a bottom-up path aggregation network (PAN) (Li et al., 2018). This process allows the high-level features to guide the low-level features, mitigating the offset problem while preserving the low-level information. At the same time, it combines the fusion module with the object detection neck module to avoid structural duplication and thus reduce the number of parameters and computational time. The module operates through two branches: the top-down FPN and the bottom-up PAN.\nThe Offset-Guided Fusion method is a multi-level fusion module, as shown in the Fig. 6. It utilizes high-level features without offset to guide the fusion of low-level features across multiple scales. Specifically, as illustrated in the figure, the fusion structure receives three types of input data: high-level features and low-level features from two different modalities. By implementing channel reconstruction and channel residual preservation, this approach builds a dual-branch feature fusion model, maximizing the information flow and achieving offset-guided fusion. The detailed process is as follows:\n$F(x) = \\sum_{i=1}^{N} (ConvBlock_{i}(x) + RepBlock(ConvBlock_{i}(x))).$\nHere, x is the input feature after concatenation, ConvBlocki(\u00b7) is the convolutional channel residual preservation block, and RepBlock(\u00b7) is the channel reconstruction block. The fusion process occurs across multiple scales, where"}, {"title": "4. Experiments", "content": "We present the experimental settings and results to validate the effectiveness of the COMO approach in multimodal object detection tasks. The experimental results demonstrate the effectiveness of the COMO approach in achieving state-of-the-art performance in multimodal object detection tasks."}, {"title": "4.1. Experimental settings", "content": "To comprehensively compare the performance of the models, we selected three datasets, each offering a different perspective, as benchmarks: DroneVehicle (Sun et al., 2022), LLVIP (Jia et al., 2021), and VEDAI (Razakarivony and Jurie, 2016). Detailed statistics for each dataset are presented in Table 1. For the comparison algorithms, we selected several highly relevant methods and reproduced them exactly to obtain comparable results. These methods include YOLOrs (Sharma et al., 2020), CFT (Qingyun et al., 2021), SuperYOLO (Zhang et al., 2023a), GHOST (Zhang et al., 2023b), MFPT (Zhu et al., 2023), ICAFusion (Shen et al., 2024), GM-DETR (Xiao et al., 2024), DaFF (Althoupety et al., 2024), and CMADet (Song et al., 2024).\nWe implement the COMO approach using two baseline object detectors YOLOv5 and YOLOv8. We used an NVIDIA RTX3090 GPU for all of"}, {"title": "4.2. Evaluation metrics", "content": "We used the standard mean average precision (mAP) introduced by MS-COCO (Lin et al., 2014) as the primary evaluation metric for the multimodal object detection task. The mAP is calculated as the mean of the average precision (AP) across all classes. The AP is calculated as the area under the precision-recall (P-R) curve, which is obtained by varying the confidence threshold. We used the mAP at an intersection over union (IoU) threshold of 50% (mAP50) as the complementary evaluation metric, where mAP50is calculated by averaging the APs at an IoU threshold of 50% across all classes.\nBold in the table of experimental results represents the best results and"}, {"title": "4.3. Experiment1: DroneVehicle Dataset", "content": "The DroneVehicle dataset is a large-scale dataset containing images captured by drones in various scenarios, making it highly representative. It provides a total of 28,439 pairs of RGB and infrared images for both day and night scenes. It consists of five categories of targets: car, truck, bus, van, freight car. Due to the positional offset between the two modalities, special consideration is required to achieve optimal detection results. The dataset includes two annotation formats: horizontal and rotated box annotations, with separate labels for each modality. For training, we selected 17,990 image pairs, and for testing, we used 1,469 image pairs. The labeled files from the infrared modality were used as the ground truth for both training and testing.\nWe compare the results of the proposed method with 9 state-of-the-art methods on the DroneVehicle dataset, as shown in Table 2 and the P-R curve is shown in Fig. 7. Our method achieves the best results on both the mAP50 and mAP metrics, with 86.1% and 65.5% respectively on the YOLOv8s baseline. Additionally, our method also outperforms other methods on the YOLOv5s baseline, achieving 85.3% and 63.4% on the mAP50 and mAP metrics respectively. These results demonstrate the effectiveness of the"}, {"title": "4.4. Experiment2: LLVIP Dataset", "content": "Getting good results on multiple datasets is an important way to explore the strengths and weaknesses of a model. Therefore, we chose to use a pedestrian detection dataset similar to the DroneVehicle data perspective, but with only one category. The LLVIP dataset (Jia et al., 2021) is a challenging dataset containing images in both infrared and visible modalities captured by road surveillance cameras under low light conditions. The dataset includes a total of 16,836 RGB and infrared image pairs.\nThe LLVIP dataset, with its lower viewing angle and closer proximity to objects, as well as containing only one pedestrian category, presents a slightly lower detection difficulty compared to DroneVehicle. However, the primary challenge with the LLVIP dataset is that it consists entirely of night scenes,"}, {"title": "4.5. Experiment3: VEDAI Dataset", "content": "To further evaluate our proposed method and explore its effectiveness on remote sensing images, we selected the small-scale VEDAI (Razakarivony and Jurie, 2016) dataset, a widely used benchmark for multimodal remote sensing object detection.\nThe VEDAI dataset consists of RGB and infrared images captured by aircraft and includes 8 vehicle classes, with over 3,700 annotated targets across more than 1,200 images. The dataset offers images in two resolutions: 1024\u00d71024 and 512\u00d7512. Since this dataset is a well-aligned airborne remote sensing dataset, offset issues are not a concern. Therefore, we applied the feature interaction module to the three feature extraction scales to fuse the data more effectively and obtain richer fusion information. In other words, we utilize the mamba interaction block to perform interaction operations on all input data at each of the three scales ($S_{ir}^{3}$, $S_{ir}^{4}$, $S_{ir}^{5}$, $S_{rgb}^{3}$, $S_{rgb}^{4}$, $S_{rgb}^{5}$) to obtain the fusion results at the three scales{Fir, Fir, Fir, Frgb, Frgb, Frgb}. For this dataset, we choose only 512 \u00d7 512 resolution images for both training and testing. This choice ensures that the model remains applicable to various datasets, not just the VEDAI dataset in this specific instance. In consideration of other methods that utilized a resolution of 1024 \u00d7 1024, we maintained this setting to ensure that the highest achievable accuracy could be obtained.\nTo expedite the validation of the model's performance, we did not adopt the commonly used ten-fold cross-validation method for the VEDAI dataset. Instead, we fixed one set of data for validation and used the remaining sets for training, reducing the time required for experimentation while still obtaining reliable results. Since all comparison algorithms are based on the same"}, {"title": "4.6. Ablation Study", "content": "We conducted extensive ablation experiments on the proposed modules to explore the effectiveness of each module and the interrelationships between modules. Unless specified mentioned, we primarily utilize the YOLOv5s model as a baseline and the DroneVehicle dataset as experimental data.\nAs shown in Table 6, we performed numerous ablation experiments to verify the validity of the individual components in the overall model. These include mamba interaction block (MIB), global and local scan method (GLS), and offset-guided fusion (OGF).\nInterestingly, the baseline model features a dual-branch architecture comprising two CSPDarknet53 networks for feature extraction. It employs simple convolutional modules for feature fusion before passing the results to the original neck network of YOLOv5 for detection. This baseline design was chosen because the YOLOv5 model alone is insufficient for multimodal object detection tasks. Our network can be seen as an improvement of this baseline, offering improved performance in multimodal object detection tasks. Since the local scan method is an improved method for cross-mamba block, this module cannot be completely isolated for ablation experiments. However,"}, {"title": "4.7. Comparison and analysis of the mamba interaction block", "content": "In order to find the optimal MIB module and to make an in-depth comparison with other methods, we designed different MIB composition methods and constructed a feature interaction module with a similar structure but consisting entirely of transformer modules, with the aim of fully demonstrating the advantages of the MIB module. The specific operation involved replacing the single-mamba block in the MIB with a self-attention block and substituting the cross-mamba block with a cross-attention block illustrated in Fig. 11. We also analyzed the structure of both models to explore the optimal model for multimodal object detection. We analyzed the number of single mode blocks processed in the two models consisting of the mamba model and the transformer model and fixed the number of cross-modalities module processing multiple modal data to 1. The final results are shown in Fig. 12.\nThe results show that the MIB module outperforms the transformer module in terms of multimodal object detection tasks. The MIB module achieves the best performance when the number of single blocks number is set to 3, with mAP50 of 85.3%. In contrast, the transformer module achieves the best performance when the number of single blocks is set to 0, with mAP50 of"}, {"title": "4.8. Discussion of the global and local scan method", "content": "Adjusting the patch size significantly affects the model's performance and effectiveness. A smaller patch size allows the model to capture finer local features and details, which is essential for detecting small objects or subtle changes in the image. However, this comes at the cost of increased computational load, as the model needs to process more patches. Conversely, a larger patch size enables the model to focus on global information, making it better suited for detecting large objects or broader patterns. The trade-off, however, is a potential loss of detail, particularly in scenarios with small objects or complex backgrounds. Additionally, since our method incorporates a local scanning mechanism, the design of the local window size is closely related to the patch size. Therefore, we conducted an analysis of the patch size and local size in the local scan method to explore the optimal parameters for the"}, {"title": "4.9. Discussion of the application scenarios", "content": "The proposed method COMO is designed to address the challenges of multimodal object detection tasks in various application scenarios. It can utilize the rich color and texture information from visible images while also leveraging the radiation information from infrared images. By reasonably fusing the two modalities to create a complementary information set, it achieves higher detection accuracy of target objects, even in conditions such as foggy weather, nighttime, and partial occlusion. This advantage holds significant practical value in real-world applications.\nAdditionally, COMO addresses the issue of target position offset, a challenge that existing methods struggle to overcome. It mitigates the object offset caused by differences in shooting angles and times by selecting high-level features that encapsulate more abstract attributes of the objects and are less affected by the offset for fusion. Furthermore, it employs the advanced cross-mamba method for inter-modal information interaction, enabling more comprehensive information construction. By using high-level features to guide the fusion of low-level features affected by offset, COMO maximizes the amount of information and ensures the ability to detect small objects. With these methods combined, COMO achieves higher precision in multimodal detection compared to other approaches.\nWe comprehensively explore the applicability of the COMO method from three perspectives: aerial, drone, and road surveillance, covering most scenarios in the field of remote sensing. In numerous experiments, COMO consistently achieves excellent detection results. Moreover, the required computational resources and processing time are relatively low, making it well-suited for practical applications."}, {"title": "5. Conclusion", "content": "In this paper, we propose a novel CrOss-Mamba interaction and Offset-guided fusion (COMO) approach for multimodal object detection tasks. This approach leverages the complementary strengths of different modalities to enhance detection accuracy while maintaining real-time processing capabilities. Key components of the COMO approach include the Mamba Interaction Block, Global and Local Scan Method, and Offset-Guided Fusion, which work synergistically to improve multimodal data fusion and detection performance.\nWe validate the effectiveness of the COMO method through experiments on three benchmark datasets, demonstrating its state-of-the-art performance. Additionally, the COMO approach requires fewer computational resources and reduced processing time, making it highly suitable for practical applications in diverse scenarios such as aerial, drone, and road surveillance. In future work, we plan to explore the application of the COMO approach in other multimodal object detection tasks and further optimize the model to achieve even better performance."}]}