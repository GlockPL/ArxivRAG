{"title": "WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM Agents", "authors": ["XINHANG LIU", "CHI-KEUNG TANG", "YU-WING TAI"], "abstract": "Constructing photorealistic virtual worlds has applications across various\nfields, but it often requires the extensive labor of highly trained profes-\nsionals to operate conventional 3D modeling software. To democratize this\nprocess, we introduce WorldCraft, a system where large language model\n(LLM) agents leverage procedural generation to create indoor and outdoor\nscenes populated with objects, allowing users to control individual object\nattributes and the scene layout using intuitive natural language commands.\nIn our framework, a coordinator agent manages the overall process and\nworks with two specialized LLM agents to complete the scene creation:\nForgelt, which integrates an ever-growing manual through auto-verification\nto enable precise customization of individual objects, and Arrangelt, which\nformulates hierarchical optimization problems to achieve a layout that bal-\nances ergonomic and aesthetic considerations. Additionally, our pipeline\nincorporates a trajectory control agent, allowing users to animate the scene\nand operate the camera through natural language interactions. Our system\nis also compatible with off-the-shelf deep 3D generators to enrich scene\nassets. Through evaluations and comparisons with state-of-the-art methods,\nwe demonstrate the versatility of WorldCraft, ranging from single-object\ncustomization to intricate, large-scale interior and exterior scene designs.\nThis system empowers non-professionals to bring their creative visions to\nlife.", "sections": [{"title": "1 INTRODUCTION", "content": "The creation of 3D virtual worlds offers extensive applications across\nentertainment and immersive technologies, including film, gaming,\nand mixed reality. It also facilitates advancements in robotics by\nproviding simulated environments to train embodied agents."}, {"title": "2 RELATED WORK", "content": "3D scene generation. Compared to the 3D generation of a single\nobject, generating a complex 3D scene populated with multiple\nobjects requires intricate detail modeling at various levels and a\nlayout with both aesthetic and functional design considerations.\nIn contrast, some works focus on compositional scene genera-\ntion. However, these methods often fail to handle\nobject geometry and appearance adequately or rely on pre-existing\n3D objects to compose the scene. Moreover, the synthesized object\nlayouts usually do not accurately capture complex object relation-\nships or respond precisely to user instructions. Thus, achieving\nfunctional, realistic 3D world creation with user-friendly customiza-\ntion remains an unresolved challenge."}, {"title": "3 METHOD", "content": "To effectively turn a natural language user query into a detailed\nindoor or outdoor 3D scene that incorporates functional, ergonomic,\nand aesthetic considerations, WorldCraft employs a GPT-4 agent as the coordinator of the scene generation pipeline (Sec-\ntion 3.1).\nAs illustrated in Figure 2, the generation pipeline consists of three\nprimary stages: (a) Object Creation, where we identify objects to\npopulate in the scene and utilize our proposed procedural asset\ngeneration agent, Forgelt (Section 3.2), or optionally, off-the-shelf\ndeep 3D generators to gather the asset collection. (b) Layout Gen-\neration, in which the coordinator operates the Arrangelt module\n(Section 3.3) to design a layout that satisfies both functional and\naesthetic constraints based on the collected assets. (c) Scene Ani-\nmation, where our trajectory control module enables users to guide\nthe movements of objects or the camera through conversational\ninputs, allowing the scene to be animated and producing videos of\ncomparable quality to those created by professional photographers\n(Section 3.4).\n3.1 Scene Generation Coordinator\nThe coordinator agent oversees the entire system. Its primary re-\nsponsibilities involve decomposing complex tasks and solving them\nby invoking and collaborating with other agents. Additionally, it\ncollects user feedback to meet their needs better.\nTask Decomposition. Directly generating a complete 3D scene\ncan be computationally expensive. Therefore, the coordinator agent\nbreaks down the task into a series of manageable sub-tasks. For\ncomplex requests, such as generating an entire apartment with\nmultiple rooms and numerous objects, instead of creating an overly\ncomplicated scene, the agent decomposes the environment into sub-\nspaces as a preprocessing step and then applies the same stages to\neach sub-space.\nAgent-Agent Interaction. The coordinator agent works in col-\nlaboration with more specialized agents to complete the overall\ntask. Specifically, in our system, there are agents dedicated to asset\ngeneration (Forgelt), layout generation (ArrangeIt), and scene ani-\nmation (Trajectory Control Agent). The coordinator assigns tasks to\nthese agents and facilitates inter-agent communication to improve\ngeneration quality and overall coherence.\nUser-Agent Interaction. While the system primarily requires only\na simple text prompt to begin, it supports extensive user engagement\nthroughout all stages. Users can customize individual objects, adjust layouts, and control\nmovements through multi-turn, intuitive conversations, ensuring\ntheir design goals are met and refined throughout the iterative cre-\native process.\n3.2 Forgelt: Mastering 3D Object Generation through\nAuto-Verification\nUnlike prevalent deep learning-based text- or image-to-3D genera-\ntion frameworks, Forgelt utilizes an LLM agent to navigate proce-\ndural generators, specifically Infinigen , to\ncreate a diverse array of 3D objects. It enables precise control and\ncustomization of object geometry and appearance through natural\nlanguage interactions.\nWhile LLMs excel in managing tools for specialized tasks, mas-\ntering complex procedural generators with numerous adjustable\nparameters is a challenge for general-purpose LLMs. To address\nthis, Forgelt constructs an ever-growing manual through an auto-\nverification mechanism. This manual guides the agent to iteratively\nmaster the use of procedural generation without the need for tedious\nhuman intervention or ground-truth labeling.\nEver-growing manual. We aim to dynamically construct a manual\nthat the agent can reference when using a procedural generator\nfor specific purposes. To achieve this, we employ another LLM as a\ncritic model to facilitate auto-verification.\n3.3 Arrangelt: 3D Layout Control through Hierarchical\nNumerical Optimization\nGiven a collection of 3D assets, our goal is to arrange them while\nconsidering design objectives such as ergonomics, aesthetics, and\nfunctionality, and to allow users to control the arrangement through\nnatural language instructions. To achieve this, we propose Arrangelt,\nan approach where the agent models the scene arrangement as a set\nof hierarchical numerical optimization problems and solves them\nusing a novel optimization protocol.\nAfter the object creation stage, we typically have a large set of\n3D assets, resulting in a prohibitive search space if we attempt to\nformulate a plan for all objects at once. Instead, we instruct the agent\nto recognize and leverage the hierarchical dependencies between\nobjects-for example, a bookshelf and the books it holds. Specifically,\nthe agent constructs an object tree and establishes subproblems to\nefficiently manage the complexity of the arrangement, as shown in\nFigure 5.\nEach of these subproblems is then formulated into a numerical\noptimization problem:\nminimize  L({p_i, \\theta_i}_{i=1}^m) = \\sum_{j=1}^m \\lambda_j L_j({p_i, \\theta_i}_{i=1}^m)\nsubject to  C_1, C_2,..., C_k,\nwhere the optimization variables are {p_i, \\theta_i}_{i=1}^m. p_i = (x_i, y_i, z_i) \\in\nR\u00b3 represents the 3D location of object i, and \\theta_i = (\\theta_{ix}, \\theta_{iy}, \\theta_{iz}) \\in\n[0, 2\u03c0]3 denotes its orientation in Euler angles. The objective is a\nweighted sum of terms {L1, . . ., Lm}, with their associated weights\n{\\lambda_1, ..., \\lambda_m}. {C1,..., Ck} are the constraints that must be satisfied.\nThe agent translates object relationships described in natural lan-\nguage into these objective terms and constraints to complete the\nformulation.\nWe have developed an optimization protocol that simplifies the\ncoding of objectives and constraints for an LLM agent. Central to\nour approach is a series of API functions that reflect various spatial\nrelationships and constraints, including:\nThe protocol offers the flexibility to model spatial relationships,\nenabling their implementation as either hard constraints or soft\nconstraints (score terms). For instance, regarding distance, a term\ncan be incorporated into the objective to adjust the distance between\nobjects, or hard constraints can be established to specify that the\ndistance must be either greater than or less than a predefined value.\nThis flexibility ensures that the LLM agent can effectively translate\ndesign intents into actionable layout directives.\nAfter modeling the arrangement problem within our optimization\nprotocol, we follow the methodology described in  and\nemploy simulated annealing  with the Metropolis-\nHastings criterion  to find the\noptimal arrangement.\n3.4 Video Synthesis with Conversational Trajectory\nControl\nUpon completion of object creation and layout generation, the user\ncan import the created world into software like Blender  for rendering. While users can manually control\nthe object trajectory by setting keyframes for their coordinates and\norientations, our approach simplifies this process for users without\nexperience in professional software, enabling them to direct the\nmovements of objects and cameras using natural language.\nWe build our conversational trajectory control module upon Chat-\nCam , a conversational camera control approach for\nNeRF and 3DGS representations. Our trajectory control module can\nbe regarded as an extension of ChatCam in two key ways: (1) it sup-\nports mesh representation, and (2) it goes beyond just controlling\nthe camera to include all objects in the scene.\nSpecifically, we follow ChatCam's methodology to extract scene-\nindependent trajectory descriptions, and use an autoregressive text-\nto-trajectory model to translate them into trajectory commands. To\nplace this trajectory within the scene, instead of ChatCam's image-\nbased anchor determination procedure, we directly instruct the LLM\nto set anchors based on explicit object bounding boxes."}, {"title": "4 EXPERIMENTS", "content": "In this section, we evaluate the effectiveness of our proposed World-\nCraft for 3D world creation across a range of challenging settings,\ncomparing it qualitatively and quantitatively with state-of-the-art\nmethods. Through evaluations and ablation studies, we provide em-\npirical evidence of the effectiveness of its core modules. We then\nshowcase our approach's capability to synthesize highly complex\nscenes. We kindly refer the reader to our supplementary document\nand video for additional experimental details and results.\n4.1 Experimental Setup\nImplementation details. We leverage OpenAI's GPT-4-0314  as both our agent and the critic model in the Forgelt\nsystem. The Forgelt agent navigates Infinigen  to procedurally generate 3D assets. We use Meshy\u00b9 as our\nadditional deep 3D generator. CineGPT , originally\ndesigned for camera control, now serves as our text-to-trajectory\nmodel in the trajectory control module, is utilized without fine-\ntuning for general objects.\nEvaluation metrics. We evaluate the generated scene from three\naspects: consistency with the input text, aesthetics (whether it is\nrealistic and visually pleasing), and functionality (whether it respects\nergonomics). Each of these aspects is rated on a scale from 1 to 10 by\nboth users and the GPT-4 model. For consistency, we additionally\nreport a CLIP score measuring the similarity between the rendered\nimage of the generated scene and the input text. We also report the\napproach's runtime for synthesizing a single scene.\n4.2 Complex Scene Generation\nTo better showcase the exceptional capabilities of our method in\ngenerating highly complex scenes, we present compelling examples\nin Figure 6 and ??. In the first example (Figure 6), our approach syn-\nthesizes a large, fully-furnished house tailored to the user-specified\nstyle. It further demonstrates our capability to adhere to user in-\nstructions by invoking a deep 3D generator to seamlessly integrate\nadditional objects into the scene. The second example in Figure 6\ndepicts a cityscape where skyscrapers are neatly arranged beside a\npark filled with golden, autumnal trees. ?? showcases our method's\nproficiency in creating expansive outdoor scenes, featuring proce-\ndurally generated natural elements alongside artistically crafted\nobjects from the deep 3D generator. This example also allows the\nuser to further edit and enhance the scene using natural language,\nillustrating the adaptability of our interface. This example also high-\nlights our trajectory control module's capability of turning user\ninstructions into corresponding object movements (see our supple-\nmentary video)."}, {"title": "4.3 Comparison", "content": "As shown in Figure 8, WorldCraft produces high-quality indoor 3D\nscenes. Compared to baseline approaches, our method produces\nmore realistic appearance and geometry. Our approach generates\nlayouts that are reasonable in terms of functionality, while the base-\nlines may violate common sense, such as placing a basketball hoop\nover the bed. Moreover, our approach generates scenes with a style"}, {"title": "4.4 Evaluation", "content": "Forgelt. In Figure 9, we present qualitative results of the Forgelt\nmodule, demonstrating its ability to control object geometry and\nappearance through natural language. Users can engage in multi-\nturn conversations to progressively refine the generated results and\nprovide supplementary inputs, such as textures, to better align with\nspecific design intentions. We also present quantitative validation\nof the dynamic manual construction procedure in ForgeIt. Specifi-\ncally, we experiment with two variants: one without manual input\n(zero-shot generation) and one with a static user-coded manual. As\nshown in Table 2, the Forgelt module with dynamic manual recon-\nstruction achieves the best performance in terms of consistency and\naesthetics.\nArrangeIt. In Figure 10, we present qualitative results of the Ar-\nrangelt module, where the agent extracts hierarchical relationships"}, {"title": "5 CONCLUSION", "content": "This work introduces WorldCraft, an LLM agent that utilizes proce-\ndural generation to create customizable indoor and outdoor scenes\npopulated with various objects. With WorldCraft, users can interact\nusing natural language to control individual object attributes and\nthe overall scene layout. We propose Forgelt, which develops an\never-growing manual through auto-verification to facilitate precise\ncustomization of individual objects. We also introduce Arrangelt,\nwhich formulates hierarchical optimization problems to determine\nlayouts that consider both ergonomic and aesthetic aspects. To\ncomplete our pipeline, a trajectory control module is designed that\nenables users to animate the scene and operate the camera through\nnatural language interactions. Our agent's 3D visual programming\ncapabilities are compatible with off-the-shelf deep 3D generators\nfor enhancing scene assets. Our experiments demonstrate the versa-\ntility of WorldCraft in customizing complex 3D scenes and assisting\nnon-professionals in realizing their creative visions."}]}