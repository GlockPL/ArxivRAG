{"title": "MuAP: Multi-step Adaptive Prompt Learning for Vision-Language Model with Missing Modality", "authors": ["Ruiting Dai", "Yuqiao Tan", "Lisi Mo", "Tao He", "Ke Qin", "Shuang Liang"], "abstract": "Recently, prompt learning has garnered considerable attention for its success in various Vision-Language (VL) tasks. However, existing prompt-based models are primarily focused on studying prompt generation and prompt strategies with complete modality settings, which does not accurately reflect real-world scenarios where partial modality information may be missing. In this paper, we present the first comprehensive investigation into prompt learning behavior when modalities are incomplete, revealing the high sensitivity of prompt-based models to missing modalities. To this end, we propose a novel Multi-step Adaptive Prompt Learning (MuAP) framework, aiming to generate multimodal prompts and perform multi-step prompt tuning, which adaptively learns knowledge by iteratively aligning modalities. Specifically, we generate multimodal prompts for each modality and devise prompt strategies to integrate them into the Transformer model. Subsequently, we sequentially perform prompt tuning from single-stage and alignment-stage, allowing each modality-prompt to be autonomously and adaptively learned, thereby mitigating the imbalance issue caused by only textual prompts that are learnable in previous works. Extensive experiments demonstrate the effectiveness of our MuAP and this model achieves significant improvements compared to the state-of-the-art on all benchmark datasets.", "sections": [{"title": "1 Introduction", "content": "Vision-Language (VL) pre-training (Su et al., 2019; Lu et al., 2019; Yu et al., 2019; Kim et al., 2021) has demonstrated remarkable success in various Vision-Language tasks like image recognition (Zhang et al., 2021; Liu et al., 2019), object detection (Jin et al., 2021; Sun et al., 2021), and image segmentation (Cao et al., 2021; Hu et al., 2019) by learning the semantic correlations between different modalities through large-scale image-text training. However, most previous research has assumed that all modalities are accessible during both training and testing phases, a condition that is often challenging to meet in real-world scenarios. This challenge arises from various factors, such as privacy and security concerns leading to the inaccessibility of textual data (Lian et al., 2023), or limitations in device observations resulting in missing visual data (Zeng et al., 2022; Ma et al., 2022). Hence, the widespread occurrence of missing modalities distinctly hinders the performance of vision-language models.\nRecently, as shown in Figure 1(a), there has been a notable advancement in the field of visual language (VL) by adopting prompt learning from Natural Language Processing (NLP). However, researchers do not consider scenarios where modalities are missing. For instance, CLIP (Radford et al., 2021) aligns image and language modalities through joint training on large-scale datasets. It leverages handcrafted prompts and a parameterized text encoder to generate precise classification weights, thereby enabling zero-shot learning. Nonetheless, it faces two formidable challenges: the need for expertise and multiple iterations in designing handcrafted prompts, as well as the impracticality of fully fine-tuning the entire model due to its tremendous scale. Consequently, CoOp (Zhou et al., 2022b) and CoCoOp (Zhou et al., 2022a) propose automated prompt engineering that converts contextual words in prompts into learnable vectors and achieves substantial improvements by exclusively fine-tuning dense prompts using a small number of labeled images. Furthermore, MaPLe (Khattak et al., 2023) delves into the limitations of solely using language prompts in previous works and presents multimodal prompt learning, which introduces a coupling function to connect text prompts with image prompts, facilitating mutual gradient propagation between the two modalities for more precise alignment.\nRecent research, such as MPVR (Lee et al., 2023), has proposed using prompt learning for scenarios with missing modalities, aiming to mitigate the performance degradation caused by disparities in modality absence in training or testing data samples. However, designing distinct prompts for each missing modality scenario inevitably leads to an exponential increase in the number of prompts as the number of modalities increases (as shown in Figure 1(b), a scenario with C modalities necessitates 2^C \u2013 1 prompts), seriously compromising the scalability of the model. Moreover, unlike the dual-prompt strategy used by MaPLe (Khattak et al., 2023), MPVR (Lee et al., 2023) adopts a coarse prompt strategy at the input or attention level by directly inserting prompts into multimodal transformers, without distinguishing textual and visual features.\nDespite MaPLe's (Khattak et al., 2023) dual-prompt strategy effectively harnessing the capabilities of both modalities, its coupling mechanism exhibits a propensity for relying predominantly on the textual modality, which may result in unbalanced learning of multimodal information. Furthermore, an excessive degree of coupling has the potential to impede the independent learning capacity of each modality. To address this, in Figure 1(c), we propose a novel Multi-step Adaptative Prompt Learning (MuAP) framework for multimodal learning in the presence of missing modalities. MuAP introduces a multi-step prompting mechanism that adaptively learns multimodal prompts by iteratively aligning modalities. Specifically, we perform prompt tuning sequentially from two perspectives: single-stage and alignment-stage. This allows each modality prompt to learn autonomously without interference from the other, facilitating an in-depth exploration of each modality in scenarios where certain modalities are missing. Finally, we obtain the downstream classifier results through multimodal prompt learning, where adaptive prompts effectively mitigate imbalanced learning caused by one-way coupling and only textual prompts are learnable in (Khattak et al., 2023).\nTo summarize, this paper makes the following key contributions:\n\u2022 To the best of our knowledge, this paper is the first study to analyze the robustness of prompt learning on missing modality data. We propose a novel missing-modality in the VL Model model with multi-step adaptive prompt learning, addressing the limitations of previous works and enhancing prompts through autonomous and collaborative learning simultaneously.\n\u2022 We devise a multi-step tuning strategy that encompasses single-stage and alignment-stage tunings, where we generate visual and language prompts adaptively through multi-step modality alignments for multimodal reasoning. This facilitates comprehensive knowledge learning from both modalities in an unbiased manner.\n\u2022 We conduct extensive experiments and ablation studies on three benchmark datasets. Extensive experiments demonstrate the effectiveness of our MuAP and this model achieves significant improvements compared to the state-of-the-art on all benchmark datasets."}, {"title": "2 Related work", "content": "Recent researches on Vision-Language Pre-training (VLP) aim to learn semantic alignment between different modalities by leveraging large-scale image-text pairs. There are two architectures of the existing VLP methods: single-stream and dual-stream architectures. In single-stream architectures, image and text representations are concatenated at the feature level and serve as input to a single-stream Transformer. For example, VisualBERT (Li et al., 2019) concatenates text embedding sequences and image embedding sequences, which are then passed through a Transformer network. Building upon this work, VL-BERT (Su et al., 2019) utilizes OD-based Region Features on the image side and incorporates a Visual Feature Embedding module. Similarly, ImageBERT (Qi et al., 2020) follows a single-stream model with OD for image feature extraction while introducing more weakly supervised data to enhance learning performance. Alternatively, the dual-stream architectures align image-text representations in a high-level semantic space using two separate cross-modal Transformers. For instance, CLIP (Radford et al., 2021) and its variants (such as CoOp (Zhou et al., 2022b) and MaPLe (Khattak et al., 2023)) employ ResNet (He et al., 2016) and ViT models as image encoders, while employing Transformers (Vaswani et al., 2017) as text encoders. Subsequently, they utilize contrastive learning to predict matching scores between each template entity and the current image, with the highest score indicating the image's classification result."}, {"title": "2.1 Vision-Language Pre-trained Model", "content": "Recent researches on Vision-Language Pre-training (VLP) aim to learn semantic alignment between different modalities by leveraging large-scale image-text pairs. There are two architectures of the existing VLP methods: single-stream and dual-stream architectures. In single-stream architectures, image and text representations are concatenated at the feature level and serve as input to a single-stream Transformer. For example, VisualBERT (Li et al., 2019) concatenates text embedding sequences and image embedding sequences, which are then passed through a Transformer network. Building upon this work. VL-BERT (Su et al., 2019) utilizes OD-based Region Features on the image side and incorporates a Visual Feature Embedding module. Similarly, ImageBERT (Qi et al., 2020) follows a single-stream model with OD for image feature extraction while introducing more weakly supervised data to enhance learning performance. Alternatively, the dual-stream architectures align image-text representations in a high-level semantic space using two separate cross-modal Transformers. For instance, CLIP (Radford et al., 2021) and its variants (such as CoOp (Zhou et al., 2022b) and MaPLe (Khattak et al., 2023)) employ ResNet (He et al., 2016) and ViT models as image encoders, while employing Transformers (Vaswani et al., 2017) as text encoders. Subsequently, they utilize contrastive learning to predict matching scores between each template entity and the current image, with the highest score indicating the image's classification result."}, {"title": "2.2 Prompt Learning for Vision-Language Tasks", "content": "As the diversity of Vision-Language (VL) tasks poses a challenge for individually fine-tuning large pre-trained models for each task, Prompt Learning emerges as an effective approach to tackle this challenge. It involves freezing the backbone neural network and introducing prompts, which comprise a small number of trainable parameters, to fine-tune the entire model. This allows for the zero-shot or few-shot application of pre-trained models to new VL tasks in a more parameter-efficient manner than training large models from scratch for each task. For example, CoOp (Zhou et al., 2022b) incorporates learnable prompts into the language encoder to fine-tune CLIP, while CoCoOp employs conditional prompts to further enhance the model's generalization ability. MaPLe (Khattak et al., 2023) argues that learning prompts for the text encoder alone in CLIP are insufficient to model the necessary adaptations required for the image encoder. To address this, MaPLe leverages multimodal prompt learning to fully fine-tune the text and image encoder representations, ensuring optimal alignment in downstream tasks. It employs a coupling function to connect the prompts learned in the text and image encoders, with only the text prompts being trainable."}, {"title": "3 Method", "content": "In this section, we detail our methodology by presenting a clear problem definition and introducing our proposed MuAP."}, {"title": "3.1 Problem Definition", "content": "In this work, we study the missing-modality multimodal learning where the presence of missing modalities can occur in both the training and testing phases. For simplicity while retaining generality, following (Huang et al., 2019), we consider a multimodal dataset that contains two modalities: M = {mt, mv}, where mt and mv denote textual, visual modalities respectively. The complete modality data can be represented as $R_{all} = {x_{mt}, x_{mv}, y_i}$, where $x_{mt}$ and $x_{mv}$ denote the textual and visual features respectively, yi denotes the corresponding class label. While the missing modality data are $R_{mt} = {x_{mt}, y_j}$ or $R_{mv} = {x_{mv}, y_k}$ representing text-only data and image-only data respectively. To keep the format of multimodal inputs, we adopt a straightforward strategy of assigning placeholder inputs, represented as $F_{mt}$ and $F_{Mv}$, to the instances with missing modalities. These placeholder inputs are null strings or blank pixels and serve to fill the absence of textual or visual data, respectively. Consequently, we obtain $R_{mt} = {x^e_{mt}, F_{mv}, y_j}$, $R_{mv} = {F_{mt}, x^e_{mv}, y_k}$, and the multimodal data with missing modality can be represented as R = {$R_{all}, R_{mt}, R_{mv}$}. Our goal is to address classification issues and improve the robustness of VL model with Prompt Learning with missing modalities R."}, {"title": "3.2 Overall Framework", "content": "Considering the resource constraints, we focus on the VL model with Prompt Learning and adopt Vision-and-Language Transformer (ViLT) (Kim et al., 2021) as the backbone, which is pre-trained on large-scale VL datasets and remains untrainable in downstream tasks. To mitigate the significant performance degradation of Prompt Learning models due to missing modality data, we propose a novel Multi-step Adaptative Prompt Learning (MuAP) model to enhance the model's robustness in various missing scenarios. As illustrated in Figure 2, MuAP mainly comprises three modules: Multimodal Prompt Generator, Prompt Strategy Design, and multi-step Prompt Tuning. Specifically, we first generate learnable specific prompts for each modality to achieve completeness tuning in prompting, deviating from previous methods (Zhou et al., 2022b,a) where only textual prompts were learnable. Subsequently, we introduce two prompt fusion strategies: head-fusion and cross-fusion, attaching prompts to blocks of the multimodal transformer. Additionally, we propose a multi-step tuning strategy for dynamic language and vision prompt tuning through modality alignments, allowing MuAP to gain knowledge from both modalities."}, {"title": "3.3 Revisiting ViLT", "content": "ViLT is a widely used Transformer-based multimodal pretraining model. It partitions images into patches of varying sizes, which are projected and embedded to generate latent representations. This allows the unified processing of images and text with minimal parameters. Its overall workflow commences by concatenating the text representation (denoted as t = [tcls; t1;...; tm]) with the image patches (denoted as v = [Vcls; V1; . . . ; vn]). These concatenated representations are then fed into multiple Transformer layers for processing. Specifically:\n$h^0 = [t + t_{modal} ; v + v_{modal}] \u2208 R^{L_vd}$ (1)\n$h^i = MSA(LN(h^{i-1})) + h^{i-1}, i = 1 ... L$ (2)\n$h^L = MLP(LN(h^{i-1})) + h^i, i = 1 ... L$ (3)\nwhere, t and v represent the embeddings of text and images, respectively. They are combined with their respective modality type embeddings $t_{modal}$ and $v_{modal}$ to form the initial input $h^0$. $L_v$ represents the length of the input sequence, while d denotes the dimension of the hidden states. The context vectors h undergo continuous updates through L layers of Transformer encoders, and the final output context sequence $h^L$ is utilized for downstream tasks."}, {"title": "3.4 Multimodal Prompt Generator", "content": "One main challenge in addressing missing modality learning with prompt learning lies in the design of prompt, and all modality absence situations are exponential. Drawing on the effectiveness of complete prompts in multimodal learning, we generate specific prompts for each modality, with the key distinction being that all the textual and visual prompts are both learnable. Unlike (Lee et al., 2023), where missing-aware prompts are generated for each possible situation resulting in an exponential increase as the number of modalities grows, our method adopts a linear growth pattern for prompts that significantly reduces the number of parameters and model complexity. To improve understanding and compensation for missing modalities, we create a simple network to generate specific prompts for each modality, aiding exploration and use of implicit data.\nSpecifically, when the input comprises C modalities, there exist C complete-type prompts. In our VL tasks, given C = 2 modalities of images and texts, we initialize $P_{mt}$ and $P_{mv} \u2208 R^{L_pd}$ as textual and image prompts respectively, representing the complete modality, where $L_p$ is the prompt length. Subsequently, the initial prompts are fed into a lightweight network $f_{missing}$, in a crosswise manner. This means that opposing prompts are used to generate prompts (e.g., using a complete-type prompt from the visual modality to generate a missing-type prompt for the textual modality). The goal of this process is to enhance perception and compensate for missing modalities. The formula for the generating process is as follows:\n$f_{missing}(P^i) = GELU(W^iLN(P^i)) + P^i$ (4)\n$P^m_{mv} = f_{missing} (P_{mt})$ (5)\n$P^m_{mt} = f_{missing} (P_{mv})$ (6)\nwhere $W^i$ represents the weight matrix specific to the i-th $f_{missing}$ module in the i-th layer of MSA, LN refers to the layer normalization operation, GELU is the activation function, and adding the original prompts $P^i$ represents the residual operation. The residual connection is present to retain the opposing modality information while the MLP is utilized to collect additional missing-specific features to provide more valuable supplementary for the missing input and facilitate multimodal fusion. In a more generalized form, let $P_m (m \u2208 M)$ represent the complete-type prompt for modality m, and $P^m$ represent the missing-type prompt for the same modality. When modality m is missing, the missing-type prompt $P_m$ is utilized in the subsequent module. Otherwise, the complete-type prompt $P_m$ is used."}, {"title": "3.5 Prompt Strategy Design", "content": "Designing prompt template and strategy is crucial for prompt-based learning. We focus on prompt strategy involving prompt configuration and placement. Two prompt strategies introduced in Figure 2: head-fusion prompting and cross-fusion prompting. Consistency in subsequent symbols assumed with complete input data for textual and visual modalities.\nHead-fusion Prompting. One simple way to incorporate prompts is to add them at the start of input sequences for each layer. We use element-wise summation for combining multimodal prompts. $P_{head}$ is expressed as:\n$P_{head} = P_{mt}\u2295P_{mv}, P \u2208 R^{L_p\u00d7d}$ (7)\nwhere \u2295 denotes the summation over prompts from each modality. Next, we concatenate $P_{head}$ with the input sequence of texts and images at each layer. Similar to ViLT (Kim et al., 2021), the formula can be expressed as follows:\n$h^i = [P_{head} ; t^i; v^i], i = 0\u2026 N_p$ (8)\nwhere $P_{head}$ denotes the head-fusion prompt of i-th layer, $N_p$ represents the number of MSA layers in ViLT. With the concatenating $P_{head}$ to the input sequences of the previous layer, the final output length increases to $(N_pL_p + L_v)$ in total. This allows the prompts for the current layer to interact with the prompt tokens inherited from previous layers, enabling the model to learn more effective instructions for prediction.\nCross-fusion Prompting. Motivated by (Khattak et al., 2023), another prompting approach is to insert modality-specific prompts into their corresponding modality inputs in a single-stream model. By doing this, we facilitate the interaction between modality-specific prompts and features. The cross-fusion prompting can be formalized as follows:\n$h^i = [P^i_{mt}; t^i; P^i_{mv}; v^i], i = 0\u2026 N_p$ (9)\nwhere $P^i_{mt}$, $P^i_{mv}$ represent the modality-specific prompts for the textual and visual modalities, respectively, at the i-th layer. It is noteworthy that, unlike (Khattak et al., 2023) which only replaces few parameters from the input sequence from each layer, cross-fusion prompt strategy follows head-fusion to attach the prompts at each MSA layer. This results in an expanded final output length of"}, {"title": "3.6 Multi-step Prompt Tuning", "content": "In this section, we introduce our proposed multi-step prompt tuning technique designed to adaptively learn multimodal prompts through multi-step sequential modality alignments. Specifically, we employ prompt tuning (Lester et al., 2021) of the pre-trained Transformer encoder to perform efficient parameter learning from multiple stages, including single-stage of each modality and a alignment-stage. This not only facilitates the acquisition of modality-specific information from individual visual and textual modalities but also captures the correlations between different modalities.\nSingle-stage prompt tuning. To fully account for the inherent differences between distinct modalities, we sequentially and separately freeze the two modality prompts to explore learnable prompts trained with contrastive learning. As illustrated in Figure 2, we iteratively train the learnable prompts in a step-wise manner. Initially, we optimize the textual prompts while keeping the visual prompts frozen, called text-step. Subsequently, we switch to optimizing the visual prompts while fixing the textual prompts, called image-step. This exclusive updating process enables the prompt tuning to capture modality-specific attributes respectively.\nSpecifically, in the two steps, we utilize the Kullback-Leibler (KL) divergence as $L_{kl}$ to measure the distribution difference between text and visual prompts. Additionally, we incorporate $L_{cls}$ as a classification loss to facilitate the fusion.\nTo mitigate overfitting issues caused by prompt engineering, we employ diverse combinations of parameters $A_t$ and $\u03bb_v$ in the two steps of prompt updating, which effectively preserves modality-specific information. The formulas are as follows:\nText-step: $L_{total} = L_{cls} + A_tL_{kl}(P_{mt}, P_{mv})$ (10)\nImage-step: $L_{total} = L_{cls} + A_vL_{kl}(P_{mt}, P_{mv})$ (11)\nDuring this separate training of modality prompts, the hyper-parameter A is used to combine with the KL loss. Specifically, $A_t$ and $\u03bb_v$ are set to 0.4 for the text prompt training step and 0.3 for the image prompt training step, respectively. In the process of single-stage prompt tuning, the two prompts undergo simultaneous updates through several alignment steps, with the experimental setup setting the number of steps to 3.\nAlignment-stage prompt tuning. To further adapt multimodal prompts and enhance the generalization capability of downstream tasks, we train the model again from a alignment stage. In this step, the visual and textual prompts are all trainable during the training. The overall training objective solely emphasizes the classification loss $L_{cls}$, which is formulated as follows:\nAlignment-stage : $L_{total} = L_{cls}$ (12)"}, {"title": "4 Experiments", "content": "We follow the approach outlined in (Lee et al., 2023) to evaluate our methods across three multimodal downstream tasks:\n\u2022 MM-IMDb (Arevalo et al., 2017) focuses on classifying movie genres using both images and text, handling cases where a movie fits into more than one genre.\n\u2022 UPMC Food-101 (Wang et al., 2015) is a multimodal classification dataset and comprises 5% noisy image-text paired data gathered from Google Image Search.\n\u2022 Hateful Memes (Kiela et al., 2020) is a challenging dataset for identifying hate speech in memes through images and text. It has 10k tough samples to challenge unimodal models and favor multimodal models.\nGiven the distinct classification tasks addressed by these datasets, we employ appropriate metrics tailored to each dataset. Specifically, for MM-IMDb, we utilize F1-Macro as a measure of multi-label classification performance. For UPMC Food-101, the metric is classification accuracy. For Hateful Memes, we assess performance using the AUROC."}, {"title": "4.1 Datasets and Metrics", "content": "Datasets We follow the approach outlined in (Lee et al., 2023) to evaluate our methods across three multimodal downstream tasks:\n\u2022 MM-IMDb (Arevalo et al., 2017) focuses on classifying movie genres using both images and text, handling cases where a movie fits into more than one genre.\n\u2022 UPMC Food-101 (Wang et al., 2015) is a multimodal classification dataset and comprises 5% noisy image-text paired data gathered from Google Image Search.\n\u2022 Hateful Memes (Kiela et al., 2020) is a challenging dataset for identifying hate speech in memes through images and text. It has 10k tough samples to challenge unimodal models and favor multimodal models.\nMetrics Given the distinct classification tasks addressed by these datasets, we employ appropriate metrics tailored to each dataset. Specifically, for MM-IMDb, we utilize F1-Macro as a measure of multi-label classification performance. For UPMC Food-101, the metric is classification accuracy. For Hateful Memes, we assess performance using the AUROC."}, {"title": "4.2 Baselines", "content": "To assess the effectiveness and robustness of our proposed method, we primarily compare it with the state-of-the-art models. These models include\n\u2022 Finetuned VILT: the original one without any additional prompt parameters in ViLT (i.e. only training the pooler layer and task-specific classifier).\n\u2022 MPVR (Lee et al., 2023): derived from the pre-trained ViLT backbone, this model integrates missing-aware prompts into its multimodal transformer design.\n\u2022 Visual BERT (Li et al., 2019): a modified Visual BERT focusing on pooler and classifier training.\n\u2022 Ma Model (Ma et al., 2022): using pre-trained ViLT, multi-task optimization, and automated search algorithm to find most efficient fusion technique."}, {"title": "4.3 Main Results", "content": "Basic Performance. Table 1 shows our new prompt learning method outperforms baselines, demonstrating the effectiveness of our design and training strategy. The Hateful Memes dataset is tough, making unimodal models struggle, especially with missing modalities. Our head-fusion approach surpasses missing-aware prompts on this dataset, showing a 1.94% average improvement. This highlights our prompt learning design's proficiency in handling missing data. Additionally, different fusion strategies lead to distinct modalities integration, with the cross-fusion approach often boosting performance in specific situations, such as when dealing with missing-image cases in the Hateful Memes dataset which surpasses MPVR by about 3.53%. However, it exhibits greater sensitivity to various missing cases, particularly when text is absent. In scenarios with limited textual data, cross-fusion can inadvertently emphasize the fusion of prompts combined with modality inputs, potentially impacting multimodal representation."}, {"title": "4.4 Robustness Comparison.", "content": "The performance differences in baseline models vary significantly in robustness to different missing rates. Results for various missing rates on Hateful Memes are displayed in Figure 3. Assessing robustness involves calculating the average drop rate between successive data points.\nMPVR exhibits inferior performance compared to ViLT in certain cases, demonstrating the highest vulnerability with a maximum drop rate of 4.18% in the missing-text scenario and an average drop of 3.53%. Our proposed method, compared to head fusion, achieves a significant performance enhancement, with a low drop rate of only 3.05%, and average improvements of 9.76% for MPVR and 10.95% for ViLT. Our cross-fusion strategy demonstrates enhanced performance in most settings of the missing-image scenario, with the lowest drop rate of 2.4%. It surpasses MPVR and ViLT by an average of 8.66% and 9.85%, respectively, underscoring the effectiveness of our method in bolstering the model's resilience and performance across varying missing rate conditions.\nPrompt learning enhances multimodal fusion, improving model performance. MPVR's prompting method lacks robustness, leading to overfitting and sensitivity to missing modality cases. Missing-aware ability alone is insufficient, necessitating more robust methods. Our prompt exhibits modality-specificity and achieves missing-awareness through diverse fusion techniques. Multi-step prompt tuning aligns distinct modalities via adjustments, highlighting a trade-off between model performance and robustness."}, {"title": "4.5 Ablation Study", "content": "One of the most innovative aspects of our approach is the multi-step prompt tuning, consisting of single-stage and alignment-stage steps. We conducted experiments to assess the impact of it. As shown in Table 2, the variation with multi-step prompt tuning achieves the best performance, while the model without any tuning performs the worst. The experiment demonstrates that without iterative tuning steps, the model fails to capture crucial modality-specific information, which is essential for effective multimodal fusion. Other variations (e.g., removing text-step, KL divergence) also show different degrees of performance decrease, indicating that this module we set up to align modalities has a significant positive effect."}, {"title": "Effectiveness of Multi-step Prompt Tuning", "content": "One of the most innovative aspects of our approach is the multi-step prompt tuning, consisting of single-stage and alignment-stage steps. We conducted experiments to assess the impact of it. As shown in Table 2, the variation with multi-step prompt tuning achieves the best performance, while the model without any tuning performs the worst. The experiment demonstrates that without iterative tuning steps, the model fails to capture crucial modality-specific information, which is essential for effective multimodal fusion. Other variations (e.g., removing text-step, KL divergence) also show different degrees of performance decrease, indicating that this module we set up to align modalities has a significant positive effect."}, {"title": "Effectiveness of Prompt Length", "content": "In our proposed approach, the prompt length Lp is a critical factor. For example, in the head-fusion prompting strategy, the final output length scales linearly with (NpLp + Lv). Therefore, a judicious choice of Lp is necessary to ensure computational efficiency and prevent information disruption during the training process. We analyze the effect of prompt length in Figure 4. Consistent with intuition, model performance improves as prompt length Lp increases, peaking at values between 12 and 16. This improvement can be attributed to the additional modal information provided at shorter lengths, preventing overfitting. However, a decline in performance is observed when the length exceeds 16. This observation indicates that excessively long prompts lead to a concatenation situation where the combined length nears the original embedding length, hindering effective learning."}, {"title": "5 Conclusion", "content": "In this paper, we have undertaken the pioneering effort to comprehensively investigate the robustness of prompt learning models when modalities are incomplete. Our experimental findings have revealed the high sensitivity of existing prompt learning models to the absence of modalities, resulting in substantial performance degradation. Building upon these insights, we propose a Multi-step Adaptive Prompt Learning (MuAP) framework for missing-modality in the Vision-Language Model. We generate learnable modality-specific prompts and explore two prompt strategies to facilitate prompt learning in missing-modality Transformer models. To enable adaptive learning of multimodal prompts, we employ a multi-step tuning mechanism encompassing single-stage and alignment-stage tunings to perform multi-step modality alignments. This enables MuAP to acquire comprehensive knowledge from both modalities in a balanced manner. Extensive experiments conducted on benchmark datasets validate the effectiveness of MuAP."}, {"title": "6 Limitation", "content": "First, due to time and computational constraints, we haven't tested our techniques on LLMs and larger datasets. Second, in our choice of modalities, we've focused solely on text and visuals using ViLT. It's crucial to incorporate additional modalities such as sound. It's essential for our proposed approach to demonstrate generalizability across diverse modalities, a focus for our upcoming work. Third, we have not explored more alignment methods due to the computational limitations. Finally, despite using few parameters, the overall improvement is not substantial, but the robustness verification has significantly enhanced. Moving forward, more interpretable analysis will be carried out to comprehend the principles of the parameters' effects."}, {"title": "A Implementation Details", "content": "Regarding text modality, we use the bert-base-uncased tokenizer to tokenize our input sequence. Depending on the dataset, the maximum length of text sentences is set differently. It is set to 128 for Hateful Meme, 512 for Food-101, and 1024 for MM-IMDB. For the image modality, following (Kolesnikov et al.), we extract 32 \u00d7 32 patches from the input image. Therefore, the input images are resized to 384 \u00d7 384 during the preprocessing stage.\nFor the missing situation, we follow (Lee et al., 2023) to keep the overall missing rate at 70%. Considering various missing scenarios, we mainly set three cases, including only the text modality (missing-text) or image modality (missing-imgae) missing \u20ac% while the other modality remains intact, and another type is both modalities (missing-both) are missing 5% separately. The specific missing scenarios in training and inference experiments are shown in Table 1.\nMoreover, the backbone parameters are initialized by pre-trained weights of ViLT. The length Lp of learnable prompts is set to 16 by default in both head fusion and cross fusion. We set the maximum prompt layer number to 6 (i.e. the indices of layers to pre-pend prompts start from 0 and end at 5). The base learning rate is set at 1 \u00d7 10-2 using the AdamW optimizer (Loshchilov and Hutter, 2018) and weight decay at 2 \u00d7 10-2 to remain unchanged from (Lee et al., 2023)."}, {"title": "B Details of Various Datasets", "content": "As previously mentioned, we have three distinct datasets: MM-IMDb (Arevalo et al., 2017), UPMC Food-101 (Wang et al., 2015), and Hateful Memes (Kiela et al., 2020), each with its own objectives and evaluation metrics.\nTo provide a clear overview of these datasets, Figure 5 illustrates a comparison of their task objectives. MM-IMDb focuses on classifying movie genres, UPMC Food-101 is designed for food type classification, and Hateful Memes presents a formidable challenge in detecting hate speech across multiple modalities. As depicted in Figure 5, the Hateful Memes dataset poses the greatest challenge due to its extensive composition of over 10, 000 newly generated multimodal instances. The intentional selection of these instances aims to pose difficulties for single-modal classifiers in accurately labeling them. For instance, a classifier"}, {"title": "C More Ablation Results", "content": "In Table 3, our findings reveal that head-fusion and cross-fusion prompting exhibit robustness to this practical situation across numerous configurations. They consistently rank among the top performers, except for e values of 70%. Our head-fusion prompting strategy exhibits remarkable performance, substantially enhancing both performance and robustness in the majority of scenarios, with an average AUROC of 66.02%, which is a 1.73% improvement compared to the average performance of MPVR (64.29%). Meanwhile, the cross-fusion prompting strategy ranks second in most cases, showing a more pronounced sensitivity to specific settings compared to the head-fusion prompting strategy. According to the findings elucidated in the paper, the cross-fusion prompting strategy proves to be effective in handling incomplete multimodal data, while the head-fusion prompting strategy exhibits exceptional robustness when dealing with complete multimodal data."}, {"title": "Generalization Ability", "content": "Initially, we assume that real-world scenarios may involve missing modality instances due to device malfunctions or privacy concerns. However, the majority of existing datasets comprise modality-complete and meticulously annotated data. To address this inconsistency, we conducted experiments to investigate the impacts of a prompt learning model trained on complete modality datasets. In detail, all"}]}