{"title": "Dataset Distillation for Offline Reinforcement Learning", "authors": ["Jonathan Light", "Yuanzhe Liu", "Ziniu Hu"], "abstract": "Offline reinforcement learning often requires a quality dataset that we can train a policy on. However, in many situations, it is not possible to get such a dataset, nor is it easy to train a policy to perform well in the actual environment given the offline data. We propose using data distillation to train and distill a better dataset which can then be used for training a better policy model. We show that our method is able to synthesize a dataset where a model trained on it achieves similar performance to a model trained on the full dataset or a model trained using percentile behavioral cloning. Our project site is available here. We also provide our implementation at this GitHub repository.", "sections": [{"title": "1 Introduction", "content": "A significant challenge in reinforcement learning (RL) is that the data generation process is coupled with the training process, and data generation requires frequent online interaction with the environment, which is not possible in many settings. Offline RL aims to solve this problem by decoupling the two and training the agent on a given a static, fixed dataset (Levine et al. (2020), Prudencio et al. (2023)). However, offline RL relies on a dataset generated by a good expert policy. We often do not have access to data generated by good policies, only mediocre ones. Offline training also means we face the distributional shift problem, where the policy trained on the dataset is produces a different data distribution than the one in the dataset.\nInstead of taking the usual offline RL approach of finding a better way to train a model given the offline dataset, we take an alternate approach of asking, is there a way to"}, {"title": "2 Methodology", "content": "We describe our general problem setting, the baseline methods of tackling the problem, and our method here."}, {"title": "2.1 Offline reinforcement learning problem setting", "content": "In our reinforcement learning setting, the environment is modelled as a Markov decision process (MDP) $M = (S, A, T, R, s_0, s_{-1})$ with state space $S$, action space $A$, $T(s_{t+1}|s_t, a)$ is the probabilistic transition function, $R(s_t, a) \u2208 R$ is the immediate transition reward, $s_0$ is the start state, and $s_{-1}$ is the end state since we are in an episodic setting. A policy function $\u03c0: S \u2192 \u2206(A)$ is a mapping from a state to a probability distribution over the action space. We will assume that both the $S$ and $A$ are discrete in our setting. When a"}, {"title": "2.2 Behavioral cloning", "content": "A common way to approach offline RL is to simply attempt to train a policy $\u03c0_\u03b8$ to imitate $\u03c0_\u03b2$. This is done in a supervised learning fashion by training $\u03c0_\u03b8$ to predict a given $s_i$ and minimizing the loss\n$L_{BC}(\u03b8|D) = \\sum_{(s_i, a_i, ...) \u2208 D} w(s_i, a_i, ...) ||\u03c0_\u03b8(s_i) \u2212 a_i||$ (1)\nwhere $||.||$ is some norm implying some notion of distance and $w$ is some weighting on the data-points in $D$. Usually we take the uniform weighting $w(*) = 1$. Since we usually use stochastic gradient descent (SGD) to minimize the loss, in this case we can simply use\n$p(*) = \\frac{w(s_i,a_i,...)}{\\sum_{(s_i,a_i,...)\u2208D} w(s_i,a_i,...)}$ as the probability of selecting the sample.\nSince $\u03c0_\u03b2$ might not be optimal, we can attempt to train a better policy $\u03c0_\u03b8$ by filtering out observations in $D$ that lead to poor outcomes. In other words, we let $w(s_i, a_i, ..., G_0) = I_{G>b}$ where $b$ is some threshold on good vs bad outcomes. We call the method of choosing $b$ such that we only end up with $x\\%$ of the initial dataset, and training a policy $\u03c0_\u03b8$ using BC on it as BCx\\%, or percentile behavior cloning. In other words, the goal of percentile behavior cloning is to filter out a better training dataset."}, {"title": "2.3 Synthetic dataset", "content": "Instead of filtering out bad samples in order to create a good training dataset, our method directly learns a good training sample instead. This is done through dataset distillation, a technique used in supervised learning Wang et al. (2018). We described how we \u2018train' a synthesized dataset $D_\u03c6$ here, parameterized by $\u03c6$, given the offline dataset $D_{real}$. Our method aims to reduce the gradient matching loss of $\u03c6$ with respect to some random initialization of the model weights $\u03b8$ according to some distribution $\u03b8 \u223c p_\u03b8$. Given some model parameters $\u03b8$, we first get the gradient of $\u03b8$ with respect to the BC loss we defined in 1 on both the real dataset, $\u2207_\u03b8L_{BC}(\u03b8|D_{real})$, and our synthetic one, $\u2207_\u03b8L_{BC}(\u03b8|D_\u03c6)$. Then we define the gradient matching loss as\n$L_{grad match}(\u03c6|\u03b8_i) = E_{\u03b8\u223cp_\u03b8} [||\u2207_\u03b8L_{BC}(\u03b8|D_{real}) \u2212 \u2207_\u03b8L_{BC}(\u03b8|D_\u03c6)||]$ (2)\nWe then use SGD to minimize this loss. This method helps guarantee that the synthesized dataset $D_{syn}$ will produce a gradient similar to that of $D$ when a model is trained on it."}, {"title": "3 Experimental Setup", "content": "We provide details on how we implemented our experiments below, including what environments we tested our method on, the architecture for our models, and how we trained the models."}, {"title": "3.1 Environment", "content": "We used the Procgen environment developed by OpenAI, a suite of 16 procedurally generated environments that allow the creation of adaptive environments with the use of different seeds Cobbe et al. (2020). The inherent mechanics of these environments serve as an ideal platform for evaluating a student model's ability to learn and adapt to variations introduced by different seeds. Furthermore, the diverse environments help our study as the agent is trained for a variety of challenges that may not occur during standard training procedures. This way we are allowed to scrutinize the agents' performance across different scenarios that could arise in practical implementations. Some sample games from the Procgen environment are shown in Figure 2.\nIn Procgen environments, the state space, consists of 64x64 pixel images (RGB array with three channels and values 0 to 255). The action space is discrete in nature and usually includes movements (up, down, left, right) and interactions like collecting items or opening doors. For our experiments, we consider three procedurally generated games: Bigfish, Starpilot, and Jumper. In Starpilot for example, the player must navigate a space ship to avoid being hit by bullets and shoot down enemies in an arcade game fashion. Enemies and obstacles are procedurally generated, so each 'map' is different. The key characteristic of the Procgen benchmark is that, given a different seed, the player encounters a different 'map' though the game rules are unchanged. A key challenge for AI agents lies in how well they can adapt to seeds that they have not seen before."}, {"title": "3.2 Model architectures and training", "content": ""}, {"title": "3.2.1 MODEL TRAINING", "content": "There are two model architectures that we consider the expert policy model and the student policy model. The expert model is the neural network that we use as an expert policy, which is then used to produce the offline dataset D. The student model is the neural network which we train on the offline data to produce a policy $\u03c0_\u03b8$. Recall that the goal of offline RL is to optimize the parameters $\u03b8$ in order to produce a good policy, as described in section 2.1.\nThe expert model is an agent with convolutional architecture found in IMPALA Espeholt et al. (2018), following the convention in Procgen paper Cobbe et al. (2020). We trained"}, {"title": "3.2.2 DATA CONSTRUCTION", "content": "To construct offline RL dataset, we ran 100 episodes of our trained experts on all three environments, and store the data as mentioned in Section 2.1. To construct the synthetic data, given the synthetic data size, we sample data randomly from the offline RL data generated by expert, and then use gradient matching loss to udpate the synthetic data as illustrated in Figure 1. We use our experts to the RL Model to obtain gradients and compute the loss. Here, we use SGD optimizer with learning rate 0.1 and momentum 0.5, training with 1000 epochs."}, {"title": "4 Results", "content": "We compare our method to (1) the expert policy that we trained in an online fashion by interacting with the environment until we achieved good performance, which was then used to generate the offline dataset D and (2) a student trained on datasets with different levels of filtering using percentile behavioral cloning as described in section 2.2. In other words, we want to benchmark how well our method performs at generating a quality dataset that can be used for training an offline RL model.\nThe student model was trained with the same number of stochastic gradient descent steps and same batch size for all baseline methods. We show the in-distribution performance, i.e. the performance on seeds contained in the offline dataset D, of the various methods in table 2 and figure 3. We see here that SYNTHETIC outperforms all percentile behavior cloning methods in both Jumper and Bigfish environments. SYNTHETIC does not outperform percentile BC on Starpilot. We observed that the Starpilot expert mainly takes one action during game the 'shoot' action - compared to Bigfish and Jumper where the expert takes a more even distribution of actions. Hence the dataset is hard to distill because of the imbalanced samples of different actions.\nSince we can procedurally generate out of distribution (OOD) scenarios in Procgen, we also tested the OOD performance of the various dataset generation methods, as shown in table 3 and figure 4. We see here that similar, SYNTHETIC outperforms percentile behavior cloning methods in Jumper environment. SYNTHETIC also matches all percentile BC performances on Bigfish. SYNTHETIC does not outperform percentile BC on Starpilot since the dataset of Starpilot is imbalanced.\nOur student model trained by SYNTHETIC only uses 150 data samples, as shown in table 4. Given much smaller dataset size (less than half of BC10%) and much fewer training steps as mentioned in table 1, SYNTHETIC achieves competitive results compared to behavioral cloning in different setups. SYNTHETIC also generalizes well out of distribution as the OOD performances matches the ID results, and often times in both Starpilot and Jumper outperforms the ID results."}, {"title": "5 Related work", "content": ""}, {"title": "5.1 Deep Reinforcement Learning", "content": "Deep reinforcement learning has seen incredible success recently in tackling wide-ranging problems, from chess and Go to Atari games and robotics Silver et al. (2016). We have also seen great improvements in the architecture used to design such agents, from PPO"}, {"title": "5.2 Knowledge Distillation", "content": "As using large deep neural networks started bringing remarkable success in multiple real- world scenarios related to large-scale data, it became important to deploy deep models within mobile devices and embedded systems. Bucilua et al. (2006) first addressed this"}, {"title": "5.3 Policy distillation", "content": "There have also been attempts to distill the policy of a expert (teacher) network down to a student network directly Rusu et al. (2015), known as policy distillation. Policy distillation is a specialized application of knowledge distillation where it adapts the principles of KD in the context of Reinforcement Learning. It is used to transfer knowledge from one policy to another in deep RL. Czarnecki et al. (2019)identified three techniques for distillation in DRL, making comparisons of their motivations and strengths through theoretical and empirical analysis, including expected entropy regularized distillation, which ensures convergence while learning quickly. Policy distillation can also be used to extract an RL agent's policy to train a more efficient and smaller network that performs expertly Rusu et al. (2015).\nWhile our work also teaches a student a policy based on some teacher policy, we take more indirect, offline approach where the student is only allowed to see offline data generated by the expert (teacher)."}, {"title": "5.4 Dataset Distillation", "content": "Dataset distillation is a dataset reduction method that synthesizes a smaller. In the original work, this is by feeding a randomly initialized model with samples from the real data and samples from the synthetic dataset and taking the gradient of the model with respect to these two data samples Wang et al. (2018). The difference between the two gradients is taken as the loss, and the values of the data in the synthetic dataset are updated using SGD (while keeping the model weights fixed) Since then, a wide variety of different distillation methods have been proposed (Yu et al. (2023), Lei and Tao (2023)). In one such work, instead of matching the gradients for a single sample, the sum of the gradients (total change in model parameters) after training on a series of samples is matched instead (Cazenavette et al. (2022)). Despite recent interest in this technique, to the best of the author's knowledge, there have not been any applications of dataset distillation to reinforcement learning yet."}, {"title": "5.5 Task Generalization", "content": "The goal of task generalization is to transfer what is learned from doing one task to other tasks, broadening the capabilities of the model. In the ideal scenario, the learned model should be able to apply its knowledge to changing tasks by using the core knowledge learned. (Taylor and Stone (2007)) suggests a new transfer method called \"Rule Transfer\" which aims to learn the rules of a source task and apply them to other target tasks. (Taylor et al. (2008)) aims to learn mappings between the transitions from the source to the target task. In the problem suggested in (Oh et al. (2017)), agents are required to learn to execute sequences of instructions after mastering subtask-solving skills. The problem gives out a good basis for generalizing to unseen tasks. In (Lehnert et al. (2020)), authors suggest that using reward predictions gives the agents better generalization capabilities."}, {"title": "5.6 Other Works", "content": "In the work (Yuan et al. (2021)), the authors propose a Reinforcement Learning based method for Knowledge Distillation for scenarios where multiple teachers are available. Their work is focused on NLPs and uses large pre-trained models like BERT and RoBERTa, where the framework dynamically assigns weights to different teacher models on each training instance, in order to maximize the performance of the distilled student model. In (Xie et al. (2021)), the authors present a novel framework (DRL-Rec) for knowledge distillation between RL-based models in list-wise recommendation, where they introduce one module by which the teacher decides on which lesson to teach to the student and another confidence-guided distillation through which it is determined how much the student should learn from each lesson."}, {"title": "6 Conclusion and limitations", "content": "There are several limitations to our study. The first one is that, limited by the computation resources and time we had, we only tested on three environments in Procgen. However, we believe that the experiments on these environments demonstrate the potential of our method, and we look forward to future work on other environments. We also focus on imitation policy learning in our work since our emphasis is on the dataset distillation, not the policy learning method. However, it is also possible to use other RL methods such as q-learning or actor-critic to train policies on the synthetic dataset. We mainly benchmark against percentile behavior cloning, since that is that is the closest method in the existing literature that 'filters' for a better quality training dataset.\nIn conclusion, we proposed and tested a method that synthesizes a better quality training dataset for offline reinforcement learning. The performance of our method suggests that the quality of the dataset a key component to training a better model, and a smaller but higher quality dataset can lead to similar or better performance compared to a larger one. We believe that these methods can be highly effective in settings with low amounts of data or noisy data, and where data cannot be collected online. There is still much to explore in this research space."}]}