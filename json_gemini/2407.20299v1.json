{"title": "Dataset Distillation for Offline Reinforcement Learning", "authors": ["Jonathan Light", "Yuanzhe Liu", "Ziniu Hu"], "abstract": "Offline reinforcement learning often requires a quality dataset that we can train a policy on.\nHowever, in many situations, it is not possible to get such a dataset, nor is it easy to train\na policy to perform well in the actual environment given the offline data. We propose using\ndata distillation to train and distill a better dataset which can then be used for training\na better policy model. We show that our method is able to synthesize a dataset where a\nmodel trained on it achieves similar performance to a model trained on the full dataset or a\nmodel trained using percentile behavioral cloning. Our project site is available here. We\nalso provide our implementation at this GitHub repository.", "sections": [{"title": "1 Introduction", "content": "A significant challenge in reinforcement learning (RL) is that the data generation process is\ncoupled with the training process, and data generation requires frequent online interaction\nwith the environment, which is not possible in many settings. Offline RL aims to solve this\nproblem by decoupling the two and training the agent on a given a static, fixed dataset\n(Levine et al. (2020), Prudencio et al. (2023)). However, offline RL relies on a dataset\ngenerated by a good expert policy. We often do not have access to data generated by good\npolicies, only mediocre ones. Offline training also means we face the distributional shift\nproblem, where the policy trained on the dataset is produces a different data distribution\nthan the one in the dataset.\nInstead of taking the usual offline RL approach of finding a better way to train a\nmodel given the offline dataset, we take an alternate approach of asking, is there a way to"}, {"title": "2 Methodology", "content": "We describe our general problem setting, the baseline methods of tackling the problem, and\nour method here."}, {"title": "2.1 Offline reinforcement learning problem setting", "content": "In our reinforcement learning setting, the environment is modelled as a Markov decision\nprocess (MDP) M = (S, A, T, R, s0, s\u22121) with state space S, action space A, T(st+1|st, a)\nis the probabilistic transition function, R(st, a) \u2208 R is the immediate transition reward, s0\nis the start state, and s\u22121 is the end state since we are in an episodic setting. A policy\nfunction \u03c0: S \u2192 \u2206(A) is a mapping from a state to a probability distribution over the\naction space. We will assume that both the S and A are discrete in our setting. When a"}, {"title": "2.2 Behavioral cloning", "content": "A common way to approach offline RL is to simply attempt to train a policy \u03c0\u03b8 to imitate\n\u03c0\u03b2. This is done in a supervised learning fashion by training \u03c0\u03b8 to predict a given si and\nminimizing the loss\n\nLBC(\u03b8|D) = \\sum_{(st,at,...) \\in D} w(st, a\u012f, ...) ||\u03c0\u03b8(st) \u2212 a\u0390|| \\qquad(1)\n\nwhere || is some norm implying some notion of distance and w is some weighting on the\ndata-points in D. Usually we take the uniform weighting w(*) = 1. Since we usually use\nstochastic gradient descent (SGD) to minimize the loss, in this case we can simply use\n\np(*)=\\frac{w(st,a t,...)}{\\sum_{(s t, a t,...) \\in D} w(st, a t,...)}\n\nas the probability of selecting the sample.\nSince \u03c0\u03b2 might not be optimal, we can attempt to train a better policy \u03c0\u03b8 by filtering out\nobservations in D that lead to poor outcomes. In other words, we let w(s\u012f, a\u012f, ..., Go) = IG>b\nwhere b is some threshold on good vs bad outcomes. We call the method of choosing b such\nthat we only end up with x% of the initial dataset, and training a policy \u03c0\u03bf using BC on it\nas BCx%, or percentile behavior cloning. In other words, the goal of percentile behavior\ncloning is to filter out a better training dataset."}, {"title": "2.3 Synthetic dataset", "content": "Instead of filtering out bad samples in order to create a good training dataset, our method\ndirectly learns a good training sample instead. This is done through dataset distillation, a\ntechnique used in supervised learning Wang et al. (2018). We described how we \u2018train' a\nsynthesized dataset Do here, parameterized by , given the offline dataset Dreal. Our method\naims to reduce the gradient matching loss of $ with respect to some random initialization of\nthe model weights @ according to some distribution @ ~ pe. Given some model parameters 0,\nwe first get the gradient of @ with respect to the BC loss we defined in 1 on both the real\ndataset, \u2207oLBC(0|Dreal), and our synthetic one, VoLBC(0|D\u2084). Then we define the gradient\nmatching loss as\n\nLgrad match($10i) = E0~po [||\u2207oLBC(0|Dreal) - VoLBC(0|D\u00a2)||] \\qquad(2)\n\nWe then use SGD to minimize this loss. This method helps guarantee that the synthesized\ndataset Dsyn will produce a gradient similar to that of D when a model is trained on it."}, {"title": "3 Experimental Setup", "content": "We provide details on how we implemented our experiments below, including what environ-\nments we tested our method on, the architecture for our models, and how we trained the\nmodels."}, {"title": "3.1 Environment", "content": "We used the Procgen environment developed by OpenAI, a suite of 16 procedurally generated\nenvironments that allow the creation of adaptive environments with the use of different\nseeds Cobbe et al. (2020). The inherent mechanics of these environments serve as an ideal\nplatform for evaluating a student model's ability to learn and adapt to variations introduced\nby different seeds. Furthermore, the diverse environments help our study as the agent is\ntrained for a variety of challenges that may not occur during standard training procedures.\nThis way we are allowed to scrutinize the agents' performance across different scenarios that\ncould arise in practical implementations. Some sample games from the Procgen environment\nare shown in Figure 2."}, {"title": "3.2 Model architectures and training", "content": "There are two model architectures that we consider the expert policy model and the\nstudent policy model. The expert model is the neural network that we use as an expert\npolicy, which is then used to produce the offline dataset D. The student model is the\nneural network which we train on the offline data to produce a policy \u03c0\u03bf. Recall that the\ngoal of offline RL is to optimize the parameters @ in order to produce a good policy, as\ndescribed in section 2.1.\nThe expert model is an agent with convolutional architecture found in IMPALA Espeholt\net al. (2018), following the convention in Procgen paper Cobbe et al. (2020). We trained"}, {"title": "4 Results", "content": "We compare our method to (1) the expert policy that we trained in an online fashion by\ninteracting with the environment until we achieved good performance, which was then used\nto generate the offline dataset D and (2) a student trained on datasets with different levels\nof filtering using percentile behavioral cloning as described in section 2.2. In other words,\nwe want to benchmark how well our method performs at generating a quality dataset that\ncan be used for training an offline RL model.\nThe student model was trained with the same number of stochastic gradient descent\nsteps and same batch size for all baseline methods. We show the in-distribution performance,\ni.e. the performance on seeds contained in the offline dataset D, of the various methods\nin table 2 and figure 3. We see here that SYNTHETIC outperforms all percentile behavior\ncloning methods in both Jumper and Bigfish environments. SYNTHETIC does not outperform\npercentile BC on Starpilot. We observed that the Starpilot expert mainly takes one action\nduring game the 'shoot' action - compared to Bigfish and Jumper where the expert takes\na more even distribution of actions. Hence the dataset is hard to distill because of the\nimbalanced samples of different actions.\nSince we can procedurally generate out of distribution (OOD) scenarios in Procgen,\nwe also tested the OOD performance of the various dataset generation methods, as shown\nin table 3 and figure 4. We see here that similar, SYNTHETIC outperforms percentile\nbehavior cloning methods in Jumper environment. SYNTHETIC also matches all percentile\nBC performances on Bigfish. SYNTHETIC does not outperform percentile BC on Starpilot\nsince the dataset of Starpilot is imbalanced.\nOur student model trained by SYNTHETIC only uses 150 data samples, as shown in\ntable 4. Given much smaller dataset size (less than half of BC10%) and much fewer\ntraining steps as mentioned in table 1, SYNTHETIC achieves competitive results compared\nto behavioral cloning in different setups. SYNTHETIC also generalizes well out of distribution\nas the OOD performances matches the ID results, and often times in both Starpilot and\nJumper outperforms the ID results."}, {"title": "5 Related work", "content": "Deep reinforcement learning has seen incredible success recently in tackling wide-ranging\nproblems, from chess and Go to Atari games and robotics Silver et al. (2016). We have\nalso seen great improvements in the architecture used to design such agents, from PPO"}, {"title": "5.2 Knowledge Distillation", "content": "As using large deep neural networks started bringing remarkable success in multiple real-\nworld scenarios related to large-scale data, it became important to deploy deep models\nwithin mobile devices and embedded systems. Bucilua et al. (2006) first addressed this"}, {"title": "5.3 Policy distillation", "content": "There have also been attempts to distill the policy of a expert (teacher) network down to a\nstudent network directly Rusu et al. (2015), known as policy distillation. Policy distillation\nis a specialized application of knowledge distillation where it adapts the principles of KD in\nthe context of Reinforcement Learning. It is used to transfer knowledge from one policy to\nanother in deep RL. Czarnecki et al. (2019)identified three techniques for distillation in DRL,\nmaking comparisons of their motivations and strengths through theoretical and empirical\nanalysis, including expected entropy regularized distillation, which ensures convergence while\nlearning quickly. Policy distillation can also be used to extract an RL agent's policy to train\na more efficient and smaller network that performs expertly Rusu et al. (2015).\nWhile our work also teaches a student a policy based on some teacher policy, we take\nmore indirect, offline approach where the student is only allowed to see offline data generated\nby the expert (teacher)."}, {"title": "5.4 Dataset Distillation", "content": "Dataset distillation is a dataset reduction method that synthesizes a smaller. In the original\nwork, this is by feeding a randomly initialized model with samples from the real data and\nsamples from the synthetic dataset and taking the gradient of the model with respect to\nthese two data samples Wang et al. (2018). The difference between the two gradients is\ntaken as the loss, and the values of the data in the synthetic dataset are updated using SGD\n(while keeping the model weights fixed) Since then, a wide variety of different distillation\nmethods have been proposed (Yu et al. (2023), Lei and Tao (2023)). In one such work,\ninstead of matching the gradients for a single sample, the sum of the gradients (total change\nin model parameters) after training on a series of samples is matched instead (Cazenavette\net al. (2022)). Despite recent interest in this technique, to the best of the author's knowledge,\nthere have not been any applications of dataset distillation to reinforcement learning yet."}, {"title": "5.5 Task Generalization", "content": "The goal of task generalization is to transfer what is learned from doing one task to other\ntasks, broadening the capabilities of the model. In the ideal scenario, the learned model\nshould be able to apply its knowledge to changing tasks by using the core knowledge learned.\n(Taylor and Stone (2007)) suggests a new transfer method called \"Rule Transfer\" which\naims to learn the rules of a source task and apply them to other target tasks. (Taylor et al.\n(2008)) aims to learn mappings between the transitions from the source to the target task.\nIn the problem suggested in (Oh et al. (2017)), agents are required to learn to execute\nsequences of instructions after mastering subtask-solving skills. The problem gives out a\ngood basis for generalizing to unseen tasks. In (Lehnert et al. (2020)), authors suggest that\nusing reward predictions gives the agents better generalization capabilities."}, {"title": "5.6 Other Works", "content": "In the work (Yuan et al. (2021)), the authors propose a Reinforcement Learning based\nmethod for Knowledge Distillation for scenarios where multiple teachers are available. Their\nwork is focused on NLPs and uses large pre-trained models like BERT and RoBERTa, where\nthe framework dynamically assigns weights to different teacher models on each training\ninstance, in order to maximize the performance of the distilled student model. In (Xie et al.\n(2021)), the authors present a novel framework (DRL-Rec) for knowledge distillation between\nRL-based models in list-wise recommendation, where they introduce one module by which\nthe teacher decides on which lesson to teach to the student and another confidence-guided\ndistillation through which it is determined how much the student should learn from each\nlesson."}, {"title": "6 Conclusion and limitations", "content": "There are several limitations to our study. The first one is that, limited by the computation\nresources and time we had, we only tested on three environments in Procgen. However,\nwe believe that the experiments on these environments demonstrate the potential of our\nmethod, and we look forward to future work on other environments. We also focus on\nimitation policy learning in our work since our emphasis is on the dataset distillation, not\nthe policy learning method. However, it is also possible to use other RL methods such as\nq-learning or actor-critic to train policies on the synthetic dataset. We mainly benchmark\nagainst percentile behavior cloning, since that is that is the closest method in the existing\nliterature that 'filters' for a better quality training dataset.\nIn conclusion, we proposed and tested a method that synthesizes a better quality\ntraining dataset for offline reinforcement learning. The performance of our method suggests\nthat the quality of the dataset a key component to training a better model, and a smaller\nbut higher quality dataset can lead to similar or better performance compared to a larger\none. We believe that these methods can be highly effective in settings with low amounts of\ndata or noisy data, and where data cannot be collected online. There is still much to explore\nin this research space."}]}