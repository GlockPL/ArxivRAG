{"title": "Conjunction Subspaces Test for Conformal and Selective Classification", "authors": ["Zengyou Hel*", "Zerun Li\u00b9", "Junjie Dong\u00b9", "Xinying Liu\u00b9", "Mudi Jiang\u00b9", "Lianyu Hu\u00b9"], "abstract": "In this paper, we present a new classifier, which integrates significance testing results over different random subspaces to yield consensus p-values for quantifying the uncertainty of classification decision. The null hypothesis is that the test sample has no association with the target class on a randomly chosen subspace, and hence the classification problem can be formulated as a problem of testing for the conjunction of hypotheses. The proposed classifier can be easily deployed for the purpose of conformal prediction and selective classification with reject and refine options by simply thresholding the consensus p-values. The theoretical analysis on the generalization error bound of the proposed classifier is provided and empirical studies on real data sets are conducted as well to demonstrate its effectiveness.", "sections": [{"title": "1 Introduction", "content": "Classification is one of the most important tasks in statistics, machine learning and data mining. To date, thousands of classification algorithms (classifiers) have been developed, ranging from simple lazy classifiers such as k-nearest neighbor (kNN) [1] to more complex ensemble methods like random forest [2]. Despite of the existence of"}, {"title": "1.1 Research Motivation", "content": "In this paper, we bridge four domains and introduce a versatile classification algorithm that can alleviate those limitations in corresponding domains and provide several desirable functionalities which are critical to modern applications in an integrated manner. More precisely, our new classifier is closely related to research efforts in the following domains.\n(1) Classification via significance testing. The development of classifiers that are capable of solving the classification problem from a significance testing aspect is an overlooked direction in the field of classification. The linkage between classification and statistical hypothesis testing was first discussed by [4]. More recently, several testing-based classification methods have been presented by [5-7]. However, all these existing testing-based classifiers are instance-based classification methods. That is, these classifiers are lazy learning methods without summarizing the training data to construct a predictive model. Furthermore, the classification accuracy of these methods is still not comparable to those main-stream classifiers such as random forests. Thus, how to construct a non-lazy testing-based classification method with good performance is still an open issue.\n(2) Conformal prediction. Conformal prediction (CP) is a general framework that can be employed to control the error rate of any classifier [8]. The basic idea is to predict a set of labels that are guaranteed to include the true label with a high probability. To achieve this objective, a nonconformity measure is utilized to perform a randomness test for each test sample. Based on the given training set, a p-value for each possible label is obtained under the null hypothesis that the test sample with its predicted label belongs to the same distribution as the training samples [9]. To construct the set of predicted labels, the p-value for each label is compared to a user-specified threshold. Despite of the success of various CP algorithms and systems, it inherently obtains the p-values through a pos-processing procedure on the prediction results of existing classifiers. That is, the significance testing issue and the calculation of p-value are not an integral part of the underlying classification algorithm. In addition, the obtained p-value is an empirical one, whose precision and uncertainty are highly dependent on the number of training samples. Hence, it would be plausible to have a classification algorithm that can cast the classification problem as a hypothesis testing issue in a straightforward manner and yield an analytical p-value for each possible label.\n(3) Selective classification. The term \u201cselective classification\" has been defined by [10], which refers to \u201cclassification with a rejection option\u201d. Here we expand its"}, {"title": "1.2 Method Outline and Contributions", "content": "In this paper, we present a new classifier based on random subspace ensemble and hypothesis testing, which works as follows. First of all, we repeatedly generate a set of random feature subspaces and choose the best one from this set according to a selection criterion. Secondly, in each subspace obtained in the previous step, we con-struct a significance-based classifier in which the classification problem is formulated as a statistical association testing issue. The null hypothesis is that the test sample has no association with the target class and hence a smaller p-value would indicate that the test sample is likely to belong to the corresponding class. Thirdly, we merge p-values on all subspaces via meta-analysis [17] to obtain a consensus p-value for each class. Here we employ the rOP (r-th ordered p-value) method [18] to fulfill the p-value combination task and select a best \"r\" during the training phase. The final consensus p-value for each class can be utilized to facilitate the classification decision such that class labels with smaller p-values are preferred to those labels with larger p-values.\nThe new classifier presented in this paper is named as COST (Conjunction Subspaces Test), which has the following appealing features. Firstly, it is a testing-based classifier so that it can provide p-values for prediction uncertainty quantification in a natural manner. We don't need to post-process the prediction results of third-part classification algorithms. Secondly, thanks to the good prediction performance of the random subspace ensemble classification framework, it can achieve very good classification accuracy that is comparable to off-the-shelf classifiers such as random forests even though its base learner is only built on statistical association testing. Finally, it"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Classification via Hypothesis Testing", "content": "Binary classification and hypothesis testing are largely regarded as two separate research topics [19]. As a result, their connection and differences have been rarely discussed. [19] summarized and compared these topics for the broad scientific community.\n[4] proposed a classification algorithm based on hypothesis testing in the two-class setting. If the test sample is placed into the wrong class, then the difference between the two classes will be blurred. Based on this idea, two tests with respect to the equality of two means are conducted. In each significance test, the test sample with an unknown class label is assumed to belong to one of the two classes. Accordingly, we will obtain two p-values and the test sample is assigned to the class that has a smaller p-value. [5] further extended the method of [4] by introducing a minimum distance into the classifier for image pixels.\n[20-22] investigated the properties of squared Euclidean interpoint distances (IPDs) among different samples taken from multivariate Bernoulli, multivariate Pois-son and multinomial distributions. Afterwards, a new testing-based classifier is proposed based on the IPDs among different samples by [6].\n[7] formulated the binary classification problem as a two-sample testing problem. More precisely, the method first calculates the distance between the test sample and each training sample to derive two distance sets. Then, the two-sample testing method called Wilcoxon-Mann-Whitney test is performed under the null hypothesis that the"}, {"title": "2.2 Conformal Prediction", "content": "The concept of conformal prediction was first introduced by [23]. Thereafter, extensive research efforts have been pursued under this framework and significant advances have been made. Due to the vast literature during the past decades, please refer to a recent introduction [24] on this topic to get a global view on recent advances. Here we only focus on the key difference between the CP method and our method and discuss those ensemble CP methods.\nThere are at least two critical differences between the CP method and our algo-rithm. First, CP operates on top of virtually existing classification methods to report empirical p-values for subsequent analysis. In contrast, our method inherently utilizes the significance testing procedure as an integral component to generate analytical p-values as outputs. Second, the null hypotheses are different, i.e., the null hypothesis in CP is that the test sample belongs to the same distribution as the training samples of a candidate class while the null hypothesis in our method is that the test sample has no association with the candidate class.\nVery recently, the combination of multiple conformal predictors has received much attention [25-28]. These methods are closely related to our algorithm since the inte-gration of multiple p-values via meta-analysis is typically adopted in some methods as well. Anyway, none of these methods has employed the rOP approach in the ensem-ble stage. In addition, the selection of an optimal parameter \"r\" during the training phase to obtain better classification accuracy is another merit of our algorithm."}, {"title": "2.3 Selective Classification", "content": "Both classifiers with a reject option [10, 13, 29, 30] and set-valued classifiers (classifier with a refine option) [12, 31, 32] have been extensively studied during the past decades. See [13] and [12] for an overview on the advances of these two topics. Meanwhile, we have witnessed an increased interest on developing selective classifiers in which both reject option and refine option are equipped [11, 14, 33].\nFrom the selective classification perspective, our method also provides both reject option and refine option in the same classifier. Note that CP methods can achieve this objective in a similar manner. The main difference between our method and CP methods have been discussed in Section 2.2. Here we highlight the fact that the CP methods typically will not resort to the reject option since it has to guarantee the probability of including the true label in the prediction set. In the experimental results,"}, {"title": "2.4 Random Projection Ensemble Classification", "content": "The random projection ensemble classification method combines classification results of from base classifiers on random projections of original feature vectors [15]. From the dimension reduction viewpoint, the random subspace ensemble classifier [16, 34, 35] can be regarded as a special case of the random projection ensemble classifier.\nOur algorithm is one special type of random subspace ensemble classifier as well. However, there are at least two key differences between our method and existing classifiers [16, 35] with respect to the base learner and the ensemble strategy. In general, any existing classifier can be employed as the base learner on each subspace, our method utilizes a quite simple classifier based on association testing that has never been discussed before. In addition, due to the special characteristics of testing-based base learners in our method, the p-value combination method is used to integrate multiple classification results from different base learners. Furthermore, to achieve the objective of selective classification using existing random subspace ensemble classifiers, we may have to follow the CP framework to post-process the original prediction results."}, {"title": "3 Method", "content": null}, {"title": "3.1 Notations", "content": "In this section, we will introduce the details of our COST algorithm. Assume that we have a training set of n labeled samples, denoted as $(X_1,Y_1), ..., (X_n, Y_n)$, where each xi represents a training sample and yi represents its corresponding label. In addition to these training samples, we also consider a test sample \\(\\hat{x}\\) of d feature values, whose true class label is denoted by \\(\\hat{y}\\). Other symbols used in the COST algorithm and their meanings are presented in Table 1. Note that the COST algorithm is specifically developed for categorical data. To handle continuous data, a discretization process can be utilized for data transformation."}, {"title": "3.2 An Overview of COST", "content": "An overview of the COST algorithm is shown in Figure 1. As shown in Figure 1: COST is composed of three key steps: subspace selection, p-value calculation and p-value combination.\n\u2022 Subspace selection. Within the COST framework, each iteration yields b2 random subspaces. Then, among these b2 subspaces, one subspace with the highest relative risk is identified and selected. After repeating this procedure b\u2081 times, b\u2081 subspaces are chosen for training the classifier."}, {"title": "3.3 Subspaces Selection", "content": "In each iteration, COST randomly generates b\u2082 random feature subsets of F = {a1,...,ad}, which are denoted by CS = {CS1,...,CSb2}. Each subset in CS corresponds to a respective candidate subspace. Furthermore, for any cs \u2208 CS, we impose a constraint on its size, i.e., 1 \u2264 |cs| \u2264 lmax. Here, lmax represents the maximum size of subspaces, which is specified as lmax = min(d, [\u221an]), where d denotes the number of total features and n represents the number of training samples.\nNext, we need to select one \"best\" subspace from these b2 candidate subspaces according to some criteria. For a candidate subspace cs, we use xi[cs] to denote the"}, {"title": "3.4 P-value Calculation", "content": "In each individual subspace, we need to calculate the p-value corresponding to each class. Suppose we are computing the p-value pi,j on the subspace si for class cj. The corresponding null hypothesis in this case is that, the test sample \\(\\hat{x}\\) is hypothesized not to belong to cj according to projected training samples on the subspace si. More precisely, the null hypothesis can be further stated as: \u201cthe test sample has no asso-ciation with cj on si\". The COST algorithm tackles the above statistical association testing issue using the Fisher's exact test to calculate an analytical p-value.\nSimilar to the notations used in Section 3.3, let $z_\\hat{i} = \\hat{x}[s_i]$ be projected feature value set of \\(\\hat{x}\\) on the subspace si. Then, o(zi) and oj(zi) are used to denote the number of occurrences of zi in all training samples and the training samples of class Cj, respectively. Meanwhile, oj is used to represent the number of training samples whose class label is cj.\nThen, we can construct a contingency table for two binary variables defined below. One is the class variable denoted by C, where C = 1 if the class label of training samples is cj and C = 0 otherwise. Another variable is denoted by E, where E = 1 if the projected feature value set of training samples on si is zi and E = 0 otherwise."}, {"title": "3.5 P-value Combination", "content": "For the test sample \\(\\hat{x}\\), we have calculated the p-values pi,j(1 \u2264 i \u2264 b\u2081, 1 \u2264 j \u2264 k) on all subspaces for all classes in Section 3.4. In this section, we combine the p-values {P1,j,...,Pb1,j} to obtain a consensus p-value pc, via meta-analysis.\nIn this paper, the meta-analysis method used for combining the p-values is the rOP (r-th ordered p-value) method. In this method, we first sort b\u2081 p-values in a non-decreasing order: $P_{(1),j} \u2264 \u2026 \u2264P_{(b_1),j}$ and then choose the r-th p-value $p_{(r),j}$ as the test statistic. According to [18], the $P_{(r),j}$ follows a Beta distribution with parameters \u03b1 = r and \u03b2 = b\u2081 - r + 1. Hence, we can obtain the consensus p-value pcj according to the cumulative distribution function of the Beta distribution.\nWe can assign the test sample \\(\\hat{x}\\) to the class with the minimum p-value to conduct a regular classification:\n\\(\\hat{y} = arg \\underset{C_i, 1<i<k}{min} P_{ci}\\)\nWithin the COST framework, the selection of a proper \"r\" is crucial to the final classification performance. Hence, a validation set Dv = {(xi, Yi)|1 \u2264 i \u2264 m} is utilized to choose the optimal value of \"r\". For each potential value of \"r\", the classification accuracy on Dr is computed. Consequently, the \"r\" that yields the highest classi-fication accuracy is chosen as the final parameter setting to be applied to the test set."}, {"title": "3.6 Selective Classification", "content": "Following the aforementioned procedures, we are able to ascertain the p-values for the test sample \\(\\hat{x}\\) for all classes $P_{c1},\u00b7\u00b7\u00b7, P_{ck}$. We calculate the number of p-values that are less than a given significance level \u03b1:\n\\(Sum = \\sum_{i=1}^k I(P_{ci} < \\alpha),\\)\nwhere I is an indicator function and \u03b1 is a user-specified parameter.\nBased on the value of Sum, we have the following classification options:\n\u2022 Sum = 1, this indicates that exactly only one p-value can pass the threshold. The test sample \\(\\hat{x}\\) is to be assigned distinctly to the class whose associated p-value falls below the predetermined threshold \u03b1.\n\u2022 Sum = 0, this indicates that all k p-values are no less than the threshold. Therefore, it is concluded that the test sample \\(\\hat{x}\\) does not belong to any class, and the reject option is opted.\n\u2022 Sum > 1, this indicates that more than one p-values are less than the threshold. Hence, the test sample could be considered to belong to at least two classes with a refine option."}, {"title": "4 Theoretical Analysis", "content": "COST is an ensemble classification algorithm based on order statistics. In this section, we will first investigate the convergence of COST under the setting of regular clas-sification, i.e., each test sample is assigned to the class with the smallest consensus p-value.\nSuppose \u0109 is the class label with the minimum p-value when the optimal chosen r is \u2191. Accordingly, the p-value that ranks at the position \u2191 in the ordered p-value sequence for class \u0109 is p(r),\u0109. As shown in the following Lemma, the classification decision of COST is equivalent to that of a bagging predictor.\nLemma 1. On the i-th subspace (1 \u2264 i \u2264 b\u2081), we can construct a threshold classi-fier as: $f(x,s_i) = C_j if I(p_{i,j} \u2264P_{(r),\\hat{c}})$. Then, these b\u2081 threshold classifiers can be aggregated to form a bagging predictor:\n$g(x) = arg \\underset{c_j, 1<j<k}{max} \\sum_{i=1}^{b_1} (f(x, s_i) = C_j)$.\nThe class label reported by g(x) is equivalent to that of COST.\nProof. According to the rOP method and COST, we know that for the class \u00ea, there are at least \u2191 p-values that are no larger than P(r),\u0109. That is, there are at least \u2191 threshold classifiers that will vote \u0109. Meanwhile, since p(\u2191),\u0109 is the smallest p-value among all p(\u2191),j(1 \u2264 j \u2264 k), without loss of the generality, we can assume that all other p(\u2191),j are strictly less than p(\u2191),\u0109. Hence, there will be at most \u2191 - 1 threshold classifiers that support the remaining k-1 classes. As a result, the bagging predictor g(x) will classify x to \u0109 since it will receive the largest number of votes from base classifiers. So the lemma is proved."}, {"title": "4.1 The Convergence Analysis", "content": "After establishing the equivalence between COST and the bagging predictor g(x) defined in Lemma 1 with respect to the classification result, we can follow the notation in [2] to define a marginal function as follows.\nDefinition 1. Given an ensemble classifier of b\u2081 base predictors f(x, s1), f(x, s2), ..., f(x, sb\u2081), we define the margin function of COST for the input sample \\(\\hat{x}\\) as follows:\n$MF(x, c) = \\frac{1}{b_1} \\sum_{i=1}^{b_1} (f(x, s_i) = c) - \\underset{c_j<>c}{max} \\frac{1}{b_1}\\sum_{i=1}^{b_1} (f(x, s_i) = C_j)$,\nwhere I is an indicator function.\nIn Definition 1, if \u0109 = \u0177 is supposed to the right class of test sample \\(\\hat{x}\\), then the margin function measures the extent to which the average number of votes for \u00ea exceeds the average vote of the second best class.\nLemma 2. As the number of base classifiers increases (b\u2081 \u2192 \u221e) and these base classifiers are supposed to be independent, for almost surely all i.i.d random subspaces"}, {"title": "4.2 The Upper Bound of Generalization Error", "content": "Definition 2. The generalization error is defined as \u025b = Px,y(MF(x,\u0109) < 0), where the subscripts X, Y indicate that the probability is over the sample space X and the class label space Y."}, {"title": "4.3 Time Complexity", "content": "The running time of the COST algorithm is mainly consumed by three procedures: subspace generation and selection, p-value calculation and the selection of parameter r."}, {"title": "4.3.1 Subspace generation and selection", "content": "In this step, we need to iterate b\u2081 times, with each iteration generating b\u2082 candi-date subspaces and selecting the best one, ultimately resulting in b\u2081 subspaces being produced.\nIn each iteration, we randomly generate b2 feature subsets of F = (a1, ..., ad). The time complexity required to obtain the frequency distributions of all distinct feature value sets on every class for each feature subset is at most O((n+m)\u00b7d) with the help of a hash table. Therefore, the time complexity required for all b2 candidate subspaces is O(b2 \u00b7 d\u00b7 (n + m)). Subsequently, we aim to select the candidate subspace with the highest relative risk.\nFor a given candidate subspace csi, 1 \u2264 i \u2264 b\u2082, we assume that it has u unique feature value sets. We can collect the frequency of each unique feature value set in every class in O(n) time. Consequently, we can compute the relative risk of each feature value in constant time O(1) using Equation (1). Therefore, the time complexity required to calculate the sum of relative risks of all u feature value sets and to obtain the average relative risk of csi is O(u). In summary, the time complexity for calculating the relative risks of b2 candidate subspaces and selecting the best one is O(b2 \u00b7 (n + u)). In the worst-case scenario, when u equals n, the time complexity is O(b2\u00b7 (n+n)) = O(b2.n).\nAfter iterating b\u2081 times, we have a time complexity of O(b1.b2.d\u00b7(n+m)+b1.b2.n) = O(b1.b2.d. (n + m))."}, {"title": "4.3.2 P-value calculation", "content": "For each subspace si, suppose it has u distinct feature value sets. To obtain the fre-quency of each feature value set within each class, we can just scan the training set once to fulfill this task. If the feature value set and each class label are stored in a hash table, we can finish this task in a linear expected time O(n).\nBased on the frequency information collected above, we can swiftly construct contingency tables akin to Table 2 for each class. If we compute the exact p-value according to Equation (4), then the worst-case time complexity will be O(n). To improve the running efficiency, we can adopt the p-value upper bound presented by [36] instead of the exact p-value. This upper bound can be calculated in O(1) if facto-rials up to n! have been calculated in advance and stored in the main memory. Hence, the time complexity of calculating p-values for all unique feature value sets across all classes is O(uk), where k is the number of classes. Consequently, the overall time complexity of calculating all p-values on the subspace si is O(n + uk). Note that in the worst-case scenario, where u equals n, hence the worst-case time complexity is"}, {"title": "4.3.3 The selection of parameter \u201cp\u201d", "content": "In the previous step, we have computed p-values for all possible feature value sets with respect to each class. Consequently, for the validation set, we can construct a mx b\u2081 matrix for each class, where m is the number of validation samples and b\u2081 is the number of subspaces. The element in the i-th row and j-th column of this matrix represents the p-value for the i-th validation sample in the j-th subspace with respect to that class. The time complexity for constructing such k tables is O(k. b\u2081\u00b7m). Subsequently, for each class-specific table, sorting p-values from b\u2081 subspaces for each sample in each row requires a time complexity of O(b\u2081 logb\u2081). Therefore, the time complexity for sorting p-values in all k tables is O(k \u00b7 m \u00b7 b\u2081 \u00b7 logb\u2081).\nNext, for each sorted table corresponding to each class, we select the m p-values in the r-th column to obtain the final p-values for all m validation samples with respect to each class and the time complexity is O(mk). As a result, we obtain a table of size m \u00d7 k, where the element in the i-th row and j-th column represents the p-value for the i-th validation sample in the j-th class.\nFinally, we assign the class label with the smallest p-value to each sample, thus obtaining the predicted labels for the validation set under the given parameter r, and the time complexity is O(mk). Comparing these predictions with the ground-truth labels yields the classification accuracy for the validation set under the parameter r, with the time complexity of this step being O(m). By iterating over every possible r, we select the r with the highest accuracy as the optimal parameter.\nTherefore, the overall time complexity of this part is O(kb\u2081\u00b7m+k\u00b7m\u00b7b\u2081\u00b7logb\u2081)+\nO(b\u2081 (mk + m \u00b7 k + m)) = O(k \u00b7 m \u00b7 b\u2081 \u00b7 logb\u2081)."}, {"title": "4.3.4 Overall time complexity", "content": "In summary, the overall time complexity of COST is the sum of time complexities of above three parts: O(b\u2081 \u00b7 b2 \u00b7 d\u00b7 (n + m) + b\u2081 \u00b7 n \u00b7 k + k \u00b7 m \u00b7 b\u2081 \u00b7 logb\u2081)."}, {"title": "5 Experiments", "content": "We conduct comprehensive experiments on 28 real-world data sets to assess the per-formance of the COST method. In particular, our objective is to address the following Research Questions (RQs):\n\u2022 RQ1: Can COST achieve comparable performance to classic classifiers such as random forest and those state-of-the-art random subspace ensemble classification algorithms in regular classification?\n\u2022 RQ2: Can COST achieve better performance than existing classifiers in the context of selective classification?\n\u2022 RQ3: Is COST robust to its parameters and the p-value combination method?"}, {"title": "5.1 Experimental Setup", "content": null}, {"title": "5.1.1 Data sets", "content": "The data sets used in the experiments could be downloaded from [37] and [38]. The main characteristics of 28 datasets are provided in Table 3. We treat the missing value in each feature as a special feature value."}, {"title": "5.1.2 Evaluation measures", "content": "\u2022 In the context of regular classification, the standard classification accuracy is employed as the evaluation criterion. In the experiment, we repeat a five-fold cross-validation 10 times to obtain the average accuracy on each data set.\n\u2022 In selective classification, a universally accepted evaluation criterion is still absent. For each test sample \\(\\hat{x}\\), let y denotes the set of its true labels and \u0177 denotes the set of labels predicted by a classifier with refine and reject options. Note that if the test sample is an outlier or it is rejected, an additional special class label co is introduced in such scenarios. Then, we can use the Jaccard coefficient to evaluate"}, {"title": "5.1.3 Baseline methods", "content": "In the performance comparison, we include the following baseline methods:\n\u2022 The classic classifiers: Naive Bayes (NB), Support Vector Machine (SVM), Decision Tree (DT), and Random Forest (RF). We use implementations in the scikit-learn package [39] with their default parameter settings (the default number of trees of RF is 100). The categorical features are first transformed into continuous ones through one-hot encoding before being fed into NB and SVM.\n\u2022 The state-of-the-art random subspace ensemble classification algorithms: Random Subsapce Ensemble (RaSE, [16]) and Parametric Random Subspace (PRS, [35]). The source codes for RaSE and PRS are available online at https://cran.r-project.org/web/packages/RaSEn/ and https://github.com/vahuynh/PRS/tree/main/code, respectively. As the example code in PRS utilizes k-nearest neigh-bors (kNN) as the base classifier, we have also chosen kNN as the base classifier for both PRS and RaSE. Similar to RF, the number of base classifiers for RaSE and PRS is set to be 100. For each base classifier, the number of candidate subspaces is fixed to be 10, and all other parameters are specified to be their default values. Sim-ilar to NB and SVM, the categorical features are first transformed into continuous features through one-hot encoding before being fed into RaSE and PRS.\n\u2022 The conformal ensemble predictors: Fisher method [25] and Majority-vote method [9]. The three base classifiers for both methods are DT, SVM and kNN. The source code for conformal prediction is available in the Python package \"crepes\" (https://github.com/henrikbostrom/crepes). In the Fisher method, we select the class with the highest p-value as the predicted class; In the Majority-vote method, we choose the class with the most votes as the predicted class."}, {"title": "5.2 Investigation of RQ1", "content": "For the task of regular classification, the performance comparison results in terms of classification accuracy, are depicted in Table 4. From this table, some important observations are summarized as follows.\nFirstly, compared to those classic classifiers, COST is slightly better than NB, DT and SVM. This happens probably because COST is an ensemble classification method so that it can yield better performance than these non-ensemble classifiers. However, the classification accuracy of COST is worse than that of RF mainly due to the fact the base classifier in COST is much simpler than decision tree in RF. Overall, COST is comparable to these classic classifiers in terms of the classification accuracy.\nSecondly, COST has quite similar performance to RaSE and PRS, two recently proposed subspace ensemble classification methods. This can be attributed to the fact that COST follows the same methodology used in RaSE for candidate subspace selection. Meanwhile, our method can also achieve comparable performance to two conformal ensemble classifiers.\nWe also compared the running time of these classification algorithms, as presented in Figure 2. It indicates that the COST algorithm demonstrates a notable efficiency advantage over other subspace ensemble methods such as RaSE and PRS. However, it's important to note that while COST excels within its sub-category, it still lags behind traditional, non-ensemble algorithms like DT, SVM, and NB. This suggests that we need to further improve its running efficiency in order to handle large data sets in practice."}, {"title": "5.3 Investigation of RQ2", "content": "To test the performance of COST in the context of selective classification, we compare it with BCOPS and GPS by employing the JacAcc defined in Section 5.1.2 as the performance indicator. The significance level parameter \u03b1 for COST, BCOPS, and GPS is set to be 0.05. Moreover, in line with the recommendations from the authors of the GPS algorithm, we have used the test data as \"unlabeled data\", combining it with the training data to form a new training set for GPS. Correspondingly, the"}, {"title": "5.4 Investigation of RQ3", "content": "Several parameters are required as input in COST. The core parameters of COST include b\u2081 and 62. Therefore, we focus on how the parameters b\u2081 and b2 influence the classification performance and running time of the COST algorithm.\nWe vary b\u2081 from 10 to 100 and record the JacAcc scores and the running time of COST on different datasets in Figure 5 and Figure 6. From Figure 5, it can be observed that as b\u2081 increases, the corresponding JacAcc score exhibits stability, with minimal fluctuations observed. This indicates that, from the perspective of JacAcc scores, COST is insensitive to the value of b\u2081. From Figure 6, as expected, we can see that as b\u2081 increases, the corresponding runtime of the COST algorithm also increases.\nFigure 7 and Figure 8 present the JacAcc scores and the running time of COST when b2 is varied from 1 to 15. From Figure 7, it can be observed that as b\u2082 increases, the corresponding JacAcc score also exhibits stability. It indicates that COST is also insensitive to the value of b\u2082 in terms of the JacAcc score. From Figure 8, we can see that similar to b\u2081, the runtime of the COST algorithm also increases as the value of b2 increases.\nFinally, to evaluate the impact of different p-value combination methods, we selected four distinct methods: rOP, Fisher, minP, and maxP. Detailed performance comparison results are illustrated in Figure 9. From Figure 9, we can see that the performance of maxP is worst. This is because the maxP method selects the largest p-value as the test statistic. As a result, the COST algorithm will have a significant bias towards adopting the reject option. In contrast, the minP method opts for the small-est p-value as the test statistic, which results in a tendency to favor the refine option. While after the training and validation process, rOP method selects a \"r\" value that yields the best performance, thereby achieving better classification performance."}, {"title": "6 Conclusion", "content": "In this paper, we present a new selective classification method named COST, which is a versatile classifier by combining ideas from different domains. Essentially, it is a testing-based classifier by combining significance testing results from multiple ran-domly chosen subspaces. The consensus p-value for each class can be easily deployed for"}]}