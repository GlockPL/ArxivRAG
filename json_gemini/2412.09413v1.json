{"title": "Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems", "authors": ["Yingqian Min", "Zhipeng Chen", "Jinhao Jiang", "Jie Chen", "Jia Deng", "Yiwen Hu", "Yiru Tang", "Jiapeng Wang", "Xiaoxue Cheng", "Huatong Song", "Wayne Xin Zhao", "Zheng Liu", "Zhongyuan Wang", "Ji-Rong Wen"], "abstract": "Recently, slow-thinking reasoning systems, such as o1, have demonstrated re- markable capabilities in solving complex reasoning tasks. These systems typically engage in an extended thinking process before responding to a query, allowing them to generate more thorough, accurate, and well-reasoned solutions. These systems are primarily developed and maintained by industry, with their core techniques not publicly disclosed. In response, an increasing number of studies from the research community aim to explore the technical foundations underlying these powerful reasoning systems. Building on these prior efforts, this paper presents a reproduc- tion report on implementing o1-like reasoning systems. We introduce an \u201cimitate, explore, and self-improve\" framework as our primary technical approach to train the reasoning model. In the initial phase, we use distilled long-form thought data to fine-tune the reasoning model, enabling it to invoke a slow-thinking mode. The model is then encouraged to explore challenging problems by generating multiple rollouts, which can result in increasingly more high-quality trajectories that lead to correct answers. Furthermore, the model undergoes self-improvement by iteratively refining its training dataset. To verify the effectiveness of this approach, we conduct extensive experiments on three challenging benchmarks. The experimental results demonstrate that our approach achieves competitive performance compared to industry-level reasoning systems on these benchmarks.", "sections": [{"title": "1 Introduction", "content": "Recently, slow-thinking reasoning systems, exemplified by OpenAI's o1 have significantly enhanced the capabilities of large language models (LLMs) [1] in tackling challenging tasks [2, 3, 4, 5]. Unlike previous reasoning approaches [6, 7, 8], these systems employ test-time scaling, allowing more time for contemplation before responding to a query. This thinking process is also reflected as a text generation process that produces long internal chains of reasoning steps, referred to as thoughts, to discover suitable solutions. By examining the generated thought data, we can observe various complex reasoning behaviors exhibited by LLMs, such as planning, divide-and-conquer, self-refinement, summarization, and backtracking. Initially, it may seem surprising that LLMs can manage such complex reasoning processes, even though we know that specific training or inference strategies are employed to support this capability.\nTo uncover the underlying mechanisms, the research community has been actively exploring slow- thinking reasoning systems and conducting extensive studies to investigate various potential ap- proaches to reproducing o1-like systems [9, 10, 11, 12, 13, 14, 15]. However, these studies are often limited to specific domains (e.g., mathematical domains) or developed using relatively weak base models, which makes the implemented systems significantly inferior to industry systems like o1. Implementing an open o1-like reasoning system\u2014with all key details publicly disclosed\u2014that can readily generalize across domains and achieve performance comparable to industry-level systems remains a challenging task.\nBuilding on existing research efforts in the literature, our team has been dedicated to advancing the reproduction of ol-like systems. To approach this goal, we released a technical report [9] in November detailing the implementation of a reasoning framework for addressing mathematical problems. Our framework comprises a policy model, a reward model, and a search algorithm. During inference, the policy model is guided by the reward model to perform the tree search to find correct solutions to mathematical problems. We provide an extensive discussion of the explored training and inference methods to implement such a system.\nDespite the promising improvements, we quickly realized that the implemented framework in our previous report might not be the correct path toward developing o1-like systems. We identified three major challenges that limit its potential. First, the domain-specific reward model we trained does not generalize well across different domains. Second, performing tree search during the inference stage was very time-consuming, making it impractical for real-world applications. Third, although test-time scaling works, we still cannot achieve train-time scaling to improve model performance. These considerations have led us to reconsider our technical approach to creating 01-like reasoning systems.\nOur approach is inspired by two main lines of recent progress. Firstly, DeepSeek and Qwen have released the API or checkpoints for o1-like systems [16, 17], allowing us to closely examine the actual thought processes rather than the summarized versions in o1. This is particularly important for us in obtaining initial labeled data for preliminary attempts. Secondly, we have empirically found that fine-tuning LLMs with a small amount of long chain-of-thought data can significantly enhance their performance on complex reasoning tasks, as also reported in previous studies [12, 18]. Based on these considerations, we speculate that o1 might implement a one-pass decoding process that encompasses both the internal thought and final solution. In other words, complex reward models and explicit tree search algorithms might not be necessary to support the reasoning process. This speculation has guided our efforts in developing this work for reproducing ol-like systems.\nSpecifically, we first propose a conceptional framework comprising an \u201cimitate, explore, and self- improve\" process for developing our approach. In the imitation phase, the LLM should learn to tackle tasks by first producing internal thoughts and then generating the solution. Given that this output format significantly differs from a standard response, additional demonstration data is necessary to support this imitation process. This data serves the dual purposes of format adherence (i.e., following a slow-thinking response) and ability elicitation (i.e., activating a slow-thinking mode). In the explo- ration phase, the LLM should expand its capacity elicited through the demonstration data provided during the imitation stage. We believe it's crucial for the LLM to engage in extensive exploration (typically using techniques like rollout or beam search) on complex tasks to help identify correct solutions to challenging problems. The enhanced outputs generated through exploration are valuable for boosting the model's capabilities. Finally, the LLM should leverage the successful trajectories acquired through exploration to further enhance its abilities. It is challenging to continuously obtain training data of higher quality than what the model itself can readily generate, and employing explo- ration or search methods can help to address this. Once this three-phase training cycle is established, the capabilities of LLMs can be gradually improved, particularly in handling difficult tasks.\nFollowing this proposal, in this technical report, we implement an 01-like reasoning system, which can achieve promising results compared in challenging reasoning tasks. Specifically, we collect a small amount of slow-thinking responses from the open o1-like API or checkpoints, and employ these responses as demonstration data to fine-tune our base model. We find that this simple strategy effectively elicits the slow-thinking capacities of LLMs and aligns with the desired output format of both thought and solution. We carefully study how to construct the demonstration dataset by"}, {"title": "2 Method", "content": "In this section, we provide a detailed introduction of our technical approach to implement 01-like reasoning systems."}, {"title": "2.1 Overview", "content": "In this work, we propose a three-phase training approach-imitate, explore, and self-improve-to develop reasoning systems similar to 01. After training, the inference phase is also completed by a single-pass text generation process, akin to prior prompt-based methods, with the key distinction that the generated response includes both the reasoning process and the solution. Next, we detail each phase below.\n\u2022 Imitate: The core idea is that both the internal thought process and the final solution should be generated in a single response. To achieve this, specific formatting tokens can be used to guide the model in producing such outputs [21, 22]. We argue that a well-established model, even with a small amount of long-form thought data, can easily adhere to ol-like output formats. This process is fundamentally about following a prescribed format. The key rationale is that, although the entire thought process may be complex, LLMs can effectively handle individual steps (e.g., planning, self-refinement, and verification). By using format-following, we can guide LLMs to seamlessly manage and connect these steps. If this hypothesis proves true, two major benefits can be realized: (1) large amounts of data are not needed for format-following, and (2) the approach can be easily generalized to various domains.\n\u2022 Explore: While imitation enables LLMs to generate ol-like outputs, it may not fully encourage the model to master or improve its ability to use long-form thought to tackle complex tasks. To address this, we believe it is crucial to incorporate exploration, allowing the model to generate progressively better training data on its own. We term this process exploration, as the reasoning model cannot directly (or easily) generate a correct solution for challenging tasks. Therefore, search strategies are needed to generate multiple candidate solutions, increasing the likelihood of finding the correct trajectory [23, 24] (i.e., the entire response consisting of thought and solution). In practice, evaluating the correctness of these attempted trajectories is challenging, requiring a simulated environment with well-trained reward models. In this work, we adopt a simplified method that directly compares the model's output with the ground-truth answer. Our results show that, for most of the collected problems, increasing the number of rollouts allows our base model to generate correct trajectories within a reasonable number of attempts.\n\u2022 Self-Improve: The third phase aims to further enhance the reasoning model's capabili- ties by utilizing progressively improved trajectory data. We hypothesize that providing high-quality demonstrations\u2014particularly those the model cannot easily generate\u2014will effectively strengthen its reasoning abilities. There are several ways to implement this. Typically, we can use rejection sampling for learning with high-quality samples, and direct preference optimization for learning by comparing high-quality trajectories with lower- quality ones (e.g., those that do not lead to the correct answer). Additionally, the \u201cexplore\" and \"self-improve\u201d phases can be combined through reinforcement learning to achieve sys- tematic model improvement, though this approach generally requires more computational resources and additional training time.\nWe show the overview of our method in Figure 1. Note that this framework is somewhat conceptual, and while we have made some preliminary attempts at instantiating it, our implementation does not fully realize its potential. In the following, we will detail the specific implementation of each part in our approach."}, {"title": "2.2 Imitation Learning for Slow-Thinking Reasoning", "content": "As discussed in Section 1, we propose using imitation learning to enable the LLM to engage in slow- thinking reasoning-producing an extended process of thought (referred to as long-form thought) before responding to a query. In this section, we will first discuss how to construct the long-form thought dataset for imitation learning (Section 2.2.1), and then present the fine-tuning method based on the long-form thought dataset (Section 2.2.2)."}, {"title": "2.2.1 Long-form Thought Dataset Construction", "content": "To guide the LLM in producing the long-form thought in a slow-thinking mode followed by the solution, we first need to construct a collection of high-quality demonstration data that exhibits this behavior."}, {"title": "2.2.2 Long-form Thought Instruction Tuning", "content": "After collecting instruction data for long-form reasoning, we fine-tune the model to replicate the behavior of the slow-thinking mode. Specifically, we first determine the data ratio for each domain through empirical experiments, and then optimize the model using supervised fine-tuning (SFT). The optimization settings are as follows: learning rate=1e-5, and batch size=96. For the base model, we select Qwen2.5-32B-Instruct, as it has been shown to perform effectively in extensive evaluations. And we utilize the following prompt for instruction tuning.\nAlthough we can distill a large amount of instruction data, we retain only several thousand demon- stration instances during SFT. Our ultimate goal is to assess the effectiveness of self-improvement learning within this approach. In our experiments, we empirically find that capable LLMs can readily learn to perform long-form thinking, and this ability can be transferred across domains. Further discussions and experimental details are provided in Section 3.3."}, {"title": "2.3 Exploration and Self-Improvement", "content": "Although we can increasingly annotate or distill more demonstration data, the process remains largely limited by extra efforts in producing the long-form thought data. In this section, we propose enabling LLMs to explore on their own, gradually generating more data for self-improvement. First, we guide the LLM to explore challenging problems (Section 2.3.1), then identify the trajectories that are suitable for the LLM's learning process (Section 2.3.2), and finally use the selected trajectories to further enhance the LLM's reasoning abilities (Section 2.3.3)."}, {"title": "2.3.1 Exploration on Hard Problems", "content": "Our main idea is to collect correct trajectories (consisting of both thoughts and solutions) to train our reasoning model. Since we do not include a trainable reward model, we focus on collecting a variety of problems with ground-truth answers for exploration. Specifically, for each problem, we perform multiple rollouts to generate candidate trajectories. This process continues until a solution containing the correct answer is produced. In this way, as we scale the number of rollouts, we can collect an increasing number of problems, while the quality of the trajectories improves as the reasoning model is further trained. This iterative process is crucial for self-improvement training.\nWe empirically find that challenging problems, which require longer thought processes for reasoning, are particularly useful for improving the model's performance. In contrast, simpler problems often do not contribute to slow-thinking reasoning and can even degrade model performance. Among the hard problems in our collected datasets, the mathematical domain contains a higher proportion, exemplified by the challenging problems from the Mathematical Olympiad. Another interesting observation is that long-form thinking appears to be an inherent capability of LLMs, not limited to specific domains. Even when trained exclusively on mathematical problems, the model can effectively reason in a slow-thinking mode across other domains. It is important to note that the number of hard problems is highly limited, so our training set will be relatively small in scale."}, {"title": "2.3.2 Iteratively Refined Training Data", "content": "We propose using iterative training to enhance the slow-thinking capabilities of our model, with the key idea being to generate progressively refined training datasets. This refinement can be approached from two main aspects. First, the dataset can be refined by incorporating more correct trajectories from challenging problems. Second, it can be refined by adding more high-quality trajectories generated by an improved reasoning model.\nSpecifically, let Do denote the original dataset, consisting of distilled trajectories from external reasoning systems, which is used to train our initial reasoning model. Once the model is trained, we use it to perform exploration and generate additional trajectories. These new trajectories are then added to Do, resulting in a new dataset D1. This process can be repeated iteratively by alternating between training stronger models and generating refined training data. In this way, we can continuously improve the training dataset as the reasoning model evolves.\nAt each refinement step, we also perform strict pre-processing to filter out low-quality trajectories, such as those that are short or noisy. Additionally, we find that perplexity can serve as a useful metric for data selection [26], allowing us to identify and retain more challenging trajectories as recognized by the current reasoning model.\nHowever, as discussed above, a significant limitation is the scarcity of challenging problems, espe- cially those paired with ground-truth answers. As a result, the pool of such problems will be quickly exhausted after only a few iterations. We plan to address this limitation in future work."}, {"title": "2.3.3 Optimization for Self-improvement", "content": "After discussing how to generate iteratively refined training data, we now introduce the optimization methods for self-improvement. Our aim is to study how exploration can enhance the reasoning capabilities of the models. To achieve this, we apply two straightforward optimization strategies, integrating the refined training datasets: supervised fine-tuning and direct preference optimization.\nSupervised Fine-tuning. We first consider using SFT. Since we employ length and perplexity as selection metrics to filter out low-quality rollouts, this approach can also be viewed as rejection sampling [27, 28]. We adopt the capable model Qwen2.5-32B-Instruct [29] as the base model, denoted as Mo. At the t-th iteration, Mo is firstly trained on the previous dataset Dt\u22121, resulting in the improved model Mt. This alternating process of generation and training is repeated multiple times, until our problem pool is exhausted or the maximum number of iterations is reached. Note that another training method is to train Mt based on Mt\u22121. However, this would not benefit the optimization in our experiments, and we speculate that the training set Dt is relatively small in scale, which may even lead to performance degradation in incremental training.\nDirect Preference Optimization. Another approach to improving the reasoning model is through direct preference optimization (DPO) [30]. For DPO, we need to select paired positive and negative instances for contrastive learning. As mentioned earlier, we select the correct responses with a higher perplexity score as positive instances and the incorrect responses with a lower perplexity score as negative instances, which enhances the discrimination difficulty for the reasoning model, allowing it to improve to a greater extent. Similar to the SFT method, at the t-th iteration, we take the checkpoint M1 (with the first-round training) as the base model for DPO training. Additionally, we incorporate an SFT loss to help stabilize the DPO training, using the same problem set. When using DPO, a straightforward method is to align the entire response. An alternative approach is to align only the thought part. As mentioned earlier, we observed that once the thought part is generated, the randomness in generating the solution part becomes quite limited. In other words, a detailed thought process often leads to a relatively certain solution. We will examine the effects of aligning different parts in Section 3.4.\nIn addition to the two methods described above, another promising training approach is reinforcement learning [31, 32], where the policy model is directly trained during the exploration process. However, due to computational resource constraints, we leave this approach for future work."}, {"title": "3 Experiments", "content": "In this section, we conduct experiments to examine the effectiveness of the implemented framework."}, {"title": "3.1 Evaluation Setup", "content": "To demonstrate the effectiveness of our framework, we mainly conduct experiments on three chal- lenging benchmarks: MATH-OAI [19], AIME2024, and GPQA [20]. MATH-OAI contains 500 competition mathematics problems from the MATH [33] test set. AIME2024 features 30 problems specifically designed to challenge top high school students with complex problem solving tasks. GPQA consists of 198 multiple-choice problems in biology, physics, and chemistry. In our experi- ments, we focus on mathematics as the primary domain, with biology, physics, and chemistry serving as auxiliary domains. Among the math benchmarks, MATH-OAI is considered relatively easier, while AIME2024 is regarded as very challenging. Additionally, due to the small number of test samples in AIME2024, its performance tends to fluctuate in our experiments.\nWe select Qwen2.5-32B-Instruct [29] as the backbone model because it demonstrates sufficient foundational capabilities to effectively engage in extended reasoning process. As for baselines, we select several leading 01-like models for comparison (i.e., ol-preview [5], DeepSeek-R1-Lite- Preview [16], and QwQ-32B [17]). In addition, we include GPT-40 [34] and Claude 3.5 Sonnet [35], which are advanced general-purpose models. We use greedy search to evaluate the performance of our model with maximum tokens set to 32k."}, {"title": "3.2 Main Results", "content": "In this part, we present a detailed performance comparison of various methods on the selected evaluation benchmarks, as shown in Table 2. The results include performance metrics for ol-like models, general-purpose models, and several approaches based on the backbone model with additional training methods. We report both the accuracy and the gain relative to the backbone's performance.\nFrom the table (the first part of Table 2), we can observe that industry-level slow-thinking reasoning systems achieve excellent performance across the three benchmarks, showing significant improve-"}, {"title": "3.3 Further Analysis of Data Mixture", "content": "During SFT training, we prepare a mixture of training data from different domains and varying difficulty levels. In this section, we examine the impact of this data mixture on the model's per- formance. Specifically, our training dataset consists of three main sources: hard mathematical problems (corresponding to difficulty levels such as AIME or the Mathematical Olympiad), normal mathematical problems (corresponding to the MATH-OAI difficulty level), and data from other domains (corresponding to other disciplines in GPQA). Since the math domain typically contains many challenging reasoning problems, we prioritize it as the main domain.\nFor the three sources, we experiment with different proportions for data mixture: w/o hard problems (removing the hard mathematical problems), w/o other domains (removing all non-math data), and mixed domain data (including all three parts with a carefully tuned distribution).\nWe present the performance comparison in Table 3 and derive three major findings. First, excluding the hard problem data leads to a significant drop in performance. This highlights the importance of hard problems in enhancing the reasoning model's capabilities, particularly on the most challenging benchmark, AIME, in our experiments. We observe that hard problems typically require a longer thought process to reach the correct solution (as indicated by the average thought length statistics), which helps better guide and teach LLMs to generate long-form thought.\nSecond, using mathematical data alone results in a strong performance across all three benchmarks, not limited to the math domain. This suggests that reasoning with long-form thought is an inherent capability of LLMs, which can be generalized across domains once properly elicited or taught. This finding is particularly significant for the design of generalized reasoning algorithms."}, {"title": "3.4 Further Analysis of DPO Training", "content": "Another aspect to consider is the setting of the DPO algorithm in Section 2.3.3. We introduce two major modifications to the original DPO algorithm: (1) aligning only the thought process, and (2) incorporating SFT for more stable optimization. To examine the impact of these strategies, we compare the performance using variants that align both the thought and the solution, as well as those that exclude the SFT loss.\nThe comparison results are presented in Table 4. An interesting finding is that aligning only the thought process can yield promising results. This is likely because the thought process is the core element to learn, and once it is well established, LLMs can readily generate the corresponding solution. Another observation is that the SFT loss seems to have little positive influence when aligning both the thought and solution, as the solution is already incorporated into the DPO training."}, {"title": "4 Conclusion", "content": "In this paper, we present a detailed introduction to a reproduced o1-like reasoning system. We outline a three-phase development approach for implementing such a capable system, where the model is initially trained using distilled long-form thought data and then undergoes self-improvement by exploring difficult problems. Our system has demonstrated strong performance on three challenging evaluation benchmarks. We find that the slow-thinking mode can be easily transferred across domains and is particularly effective at solving hard, complex problems. Our main findings can be summarized as follows:\n\u2022 The ability to perform long-form thinking can be effectively elicited by training with a small amount of high-quality demonstration data. Once established, this ability appears to naturally generalize across domains.\n\u2022 Demonstration data from the math domain is particularly well-suited for developing the long- form thinking ability of LLMs, and data with longer thought processes appears especially effective in enhancing the model's capacity to tackle challenging problems.\n\u2022 Unlike the formal responses generated by LLMs in a fast-thinking mode, the thought process is typically expressed in a flexible, informal manner, serving to guide LLMs toward the correct path to the solution.\n\u2022 The slow-thinking capacity can be effectively enhanced through exploration and self- improvement, whereas the improvements from offline learning methods seem to occur primarily in the initial iterations, especially for challenging tasks.\nThis work follows our previous study [9] on building complex reasoning systems. Despite the promising results, our exploration remains preliminary, and there is still a substantial capacity gap compared to industry-level systems. As future work, we plan to investigate how to scale our training approach and extend its capacity to more complex tasks. As always, we are committed to keeping our technical approach open, and we welcome collaboration and support in computational resources."}]}