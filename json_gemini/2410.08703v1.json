{"title": "On the token distance modeling ability of higher RoPE attention dimension", "authors": ["Xiangyu Hong", "Che Jiang", "Biqing Qi", "Fandong Meng", "Mo Yu", "Bowen Zhou", "Jie Zhou"], "abstract": "Length extrapolation algorithms based on Rotary position embedding (RoPE) have shown promising results in extending the context length of language models. However, understanding how position embedding can capture longer-range contextual information remains elusive. Based on the intuition that different dimensions correspond to different frequencies of changes in ROPE encoding, we conducted a dimension-level analysis to investigate the correlation between a hidden dimension of an attention head and its contribution to capturing long-distance dependencies. Using our correlation metric, we identified a particular type of attention heads, which we named Positional Heads, from various length-extrapolated models. These heads exhibit a strong focus on long-range information interaction and play a pivotal role in long-input processing, as evidenced by our ablation. We further demonstrate the correlation between the efficiency of length extrapolation and the extension of the high-dimensional attention allocation of these heads. The identification of Positional Heads provides insights for future research in long-text comprehension.", "sections": [{"title": "Introduction", "content": "The Transformer model has revolutionized natural language processing tasks, but it demonstrates limitations in modeling long sequences. Meanwhile, models like Mamba (Gu and Dao, 2023) that excel in capturing long-range dependencies struggle to meet the practical requirements of natural language modeling (Lieber et al., 2024). Consequently, there has been a recent surge of work focused on extending the context length in language models based on the Transformer architecture (Zhang et al., 2024; Xiong et al., 2023; Fu et al., 2024). Particularly, some of these efforts that leverage and enhance the capabilities of ROPE (Rotary Positional Embedding) (Jin et al., 2024; Peng et al., 2023; Chen et al., 2023a), have shown promising results in extrapolating the model's capacity to handle longer contexts (Wang et al., 2024).\nOpen-source large language models commonly employ Rotary Positional Embedding (RoPE) to model sequence positional information (Touvron et al., 2023; Jiang et al., 2023; Yang et al., 2023; Bai et al., 2023a). ROPE exhibits two desirable properties. Firstly, its exponential positional encoding introduces long-range attention decay, allowing the model to focus more on neighboring semantic information. Secondly, by utilizing trigonometric functions to differentiate frequencies, RoPE effectively captures different distances between tokens, enabling higher attention scores for tokens with longer semantic dependencies, facilitating semantic aggregation. When compared to length extrapolation methods based on sparse attention (Ratner et al., 2022; Xiao et al., 2023) or prompt compression (Yen et al., 2024; Xiao et al., 2024b), modifications to RoPE for length extrapolation do not result in the loss of fine-grained contextual information at a global level. Therefore, it possesses distinct advantages in tasks such as long text comprehension (Bai et al., 2023b; Lv et al., 2024), where the preservation of comprehensive contextual information is essential for practical applications.\nA prevailing viewpoint suggests that language models based on RoPE encounter out-of-distribution (OOD) issues when faced with contexts longer than the pre-training text length, specifically affecting the sampling of the trigonometric function component for token distances (Peng et al., 2023; Xiong et al., 2023). As a result, related studies have adjusted the attention resolution in the context of long texts and fine-tuned the model to adapt to longer token distances. We hypothesize that the effectiveness of such methods stems from"}, {"title": "Background", "content": "2.1 Rotary Position Embeddings\nLarge Language Models (LLMs) are primarily based on the Transformer architecture (Vaswani et al., 2017), with the attention mechanism at its core. A prevalent method for incorporating positional information in these models is Rotary Position Embeddings (RoPE) (Su et al., 2021), which leverages rotation matrices to encode the positional information of sequences.\nIn ROPE, the positional encoding for a hidden layer, with the hidden dimension denoted by d, uses a rotation matrix for each position m. The rotation matrix $R_m$ is defined as follows:\n$R_m = \\begin{pmatrix}\n    \\cos m\\theta_0 & -\\sin m\\theta_0 &  &  & 0 & 0\\\\\n    \\sin m\\theta_0 & \\cos m\\theta_0 &  & & 0 & 0\\\\\n     &  & \\ddots & &  & \\\\\n     &  & & & & \\\\\n    0 & 0 &  &  & \\cos m\\theta_{d/2-1} & -\\sin m\\theta_{d/2-1}\\\\\n    0 & 0 &  &  & \\sin m\\theta_{d/2-1} & \\cos m\\theta_{d/2-1}\n    \\end{pmatrix}$\nwhere\n$\\theta_i = 10000^{-2i/d}$\nExplicitly, for the query vector q at position m and the key vector k at position n, we have:\n$q = \\begin{pmatrix}q_0\\\\ q_1\\\\ :\\\\ q_{d-1}\\end{pmatrix}$, $k = \\begin{pmatrix}k_0\\\\ k_1\\\\ :\\\\ k_{d-1}\\end{pmatrix}$\nAfter applying RoPE, the transformed vectors $q_m$ and $k_n$ are given by:\n$q_m = R_m q = \\begin{pmatrix}q_{m_0}\\\\ q_{m_1}\\\\ :\\\\ q_{m(d-1)}\\end{pmatrix}$\n$k_n = R_n k = \\begin{pmatrix}k_{n_0}\\\\ k_{n_1}\\\\ :\\\\ k_{n(d-1)}\\end{pmatrix}$\nThe attention weights are then calculated using the dot product of the transformed vectors:\n$\\text{softmax} \\left(\\frac{q_m^T k_n}{\\sqrt{d}}\\right)$\nThe dot product for $q_m$ and $k_n$ is given by:\n$q_m^T k_n = \\sum_{i=0}^{d-1} q_{m,i}k_{n,i}$\n2.2 Length Extrapolation Methods\nWe have investigated methods to extend the context length of language models, particularly using Rotary Position Embedding (RoPE). Our research focuses on three prominent techniques: Yarn (Peng et al., 2023), CLEX (Chen et al., 2023a), and Self-Extend (Jin et al., 2024). Each method leverages different aspects of positional encoding to enhance long-range token interactions, showing favorable performance in our tests."}, {"title": "Defining Dimension Contribution in RoPE", "content": "In Rotary Position Embedding (RoPE), each dimension of the vectors $q_m$ and $k_n$ contributes to the attention score via their dot product. To thoroughly investigate the role of different dimensions in ROPE for semantic modeling, we utilize an algorithm that analyzes the contribution of each dimension to the attention scores.\nTo capture the contribution of each dimension, we employ the Hadamard product, i.e., element-wise multiplication, denoted by the symbol $\\odot$:\n$h = q_m \\odot k_n \\in \\mathbb{R}^d, h_i = q_{m,i}k_{n,i}$,\nwhere\n$h_{2i} = q_{2i}k_{2i} \\cos(m\\theta_i) \\cos(n\\theta_i) - q_{2i+1}k_{2i} \\sin(m\\theta_i) \\cos(n\\theta_i) - q_{2i}k_{2i+1} \\cos(m\\theta_i) \\sin(n\\theta_i) + q_{2i+1}k_{2i+1} \\sin(m\\theta_i) \\sin(n\\theta_i)$\n$h_{2i+1} = q_{2i}k_{2i} \\sin(m\\theta_i) \\sin(n\\theta_i) + q_{2i+1}k_{2i} \\cos(m\\theta_i) \\sin(n\\theta_i) + q_{2i}k_{2i+1} \\sin(m\\theta_i) \\cos(n\\theta_i) + q_{2i+1}k_{2i+1} \\cos(m\\theta_i) \\cos(n\\theta_i)$\nIn ROPE, every two dimensions correspond to trigonometric functions with the same frequency $\\theta_i$. We sum the values of these corresponding dimensions to form new vectors:\n$g \\in \\mathbb{R}^{\\frac{d}{2}}, g_i = h_{2i} + h_{2i+1}$\nfor $i = 0, 1, 2, ..., \\frac{d}{2} - 1$.\nThe value of $g_i$ reflects the contribution of each dimension in RoPE to the attention score. A higher value indicates a greater contribution of that dimension to the attention score, where:\n$g_i = h_{2i} + h_{2i+1} = (q_{2i}k_{2i} + q_{2i+1}k_{2i+1}) \\cos((m - n)\\theta_i) + (q_{2i}k_{2i+1} - q_{2i+1}k_{2i}) \\sin((m - n)\\theta_i)$.\nHere, $\\theta_i$ represents the positional encoding frequency for the i-th dimension.\nThe dot product of $q_m$ and $k_n$ can be expressed as:\n$q_m^T k_n = \\sum_{i=0}^{d-1} q_{m,i}k_{n,i} = \\sum_{i=0}^{d-1} h_i = \\sum_{i=0}^{\\frac{d}{2}-1} g_i$\nTherefore, we use the value $g_i$ to measure the contribution of $\\theta_i$ to the attention score.\nThis methodological framework enables a comprehensive analysis of how each dimension in Rotary Position Embedding (RoPE) contributes to the attention scores. Through this approach, we can delve into the role of different dimensions in ROPE for semantic modeling."}, {"title": "Experiments", "content": "4.1 Study on dimension-level contributions to attention scores\nThis study aims to answer the following question: Are there distinct patterns of attention contributions across different dimensions?\nTo examine this, we initially observed the overall contribution of each dimension to the attention scores. We sampled 17 inputs, and for each input, at each layer and each head of the model, we randomly selected 100 \u00d7 number of tokens qk pairs."}, {"title": "Correlation Plot", "content": "Indeed, according to the principles of RoPE, higher dimensions are responsible for modeling longer token distances. However, it remains to be examined whether this correlation strictly holds in the actual inference process of pre-trained models such as Llama. To investigate this, we primarily focused on Llama and employed the methods shown below to observe the original Llama model as well as three different length extrapolation methods. To comprehensively assess the influence of all dimensions in Rotary Position Embedding (RoPE) on a given query-key pair ($q_m$ and $k_n$), we propose an algorithm to compute the Dominant Dimension. This value is determined by analyzing the contribution scores assigned to each dimension within RoPE. The dominant dimension signifies that the attention score predominantly originates from the vicinity of this particular dimension. For each vector $g_i$ in (11), we apply the softmax function:\n$\\text{softmax}(g_i) = \\frac{e^{g_i}}{\\sum_{i=0}^{\\frac{d}{2}-1} e^{g_i}}$\nWe then compute the dot product of the softmax output with its corresponding position vector to determine the dominant dimension:\nDominant Dimension = softmax(g) \u00b7 pos\nwhere\npos = [0 1 ...  - 1]\nTo investigate the relationship between relative distance and dominant dimension, we sampled 17 prompts. For each prompt, across every layer and head of the model, we selected the top 100 tokens with the highest interaction attention scores for each token. This resulted in 100 times the number of tokens qk pairs. For each qk pair, dominant dimension was computed, and its relative distance $\\frac{n}{m}$ was recorded.\nFor each head, we obtained a collection of 100\u00d7 number of tokens Relative Distance - Dominant Dimension pairs. If a relative distance corresponds to multiple dominant dimensions, we averaged them to obtain the dominant dimension corresponding to that distance.\nThe correlation between token relative distances and the dominant dimension of attention scores is depicted in Figure 3. To ensure the generalizability of our findings, we conducted correlation analyses across multiple datasets, including several Chinese datasets. The results from these analyses are presented in detail in the appendix."}, {"title": "Observation", "content": "Through a thorough analysis of the relationship between the dominant dimension and the relative distance of each head of each layer of the model, we have drawn the following inspiring observation:\n1. In some heads of the model, there is a significant correlation between the dominant dimension and the relative distance, whereas, in other heads, this correlation is not observed.\n2. For the original Llama model, a sudden change in the dominant dimension occurs when the sequence length exceeds the pre-training length (4K). We observed a similar phenomenon in other models, such as Baichuan, as illustrated in Appendix A.1.\n3. For the length extrapolation method, by observing the dominant dimension of the model, it can be seen that this method extends the trend of the dominant dimension within the pre-training length range of Llama to a new"}, {"title": "OOD Explanation", "content": "To further elucidate why the original model exhibits a sudden change in behavior when exceeding the pretraining length, we conducted an ablation study on the dimension matrix $R_m$ of the rotary position encoding. The results are depicted in the figure. As shown in Figure 4, it can be observed that when the length is less than the pretraining length (4K), the image after removing $R_m$ shows little difference compared to the original. However, beyond the pretraining length, no abrupt changes occur. Therefore, we propose a plausible explanation based on the finding: As depicted in Figure 2, as the relative distance increases, the lower dimensions of the rotary positional encoding tend to resemble characteristics similar to random sampling, while the higher dimensions remain comparatively stable. Consequently, when the relative position exceeds the pretraining length (4K), the values in the lower dimensions gradually become overshadowed by noise from the trigonometric functions, whereas the values in the higher dimensions remain intact. The model training adjusts to accommodate this sampling characteristic of trigonometric functions. However, when the relative distance surpasses the model's pretraining length, the model struggles to adapt to this extended sampling range, leading to a scenario where the lower dimensions lose coherence, while the influence of the higher dimensions becomes predominant."}, {"title": "Finding the Positional Heads", "content": "4.3.1 Positional Heads Detection\nPositional Heads refer to attention heads with significant correlations mentioned in Section 4.2.2. In order to identify them, we quantified the distance-dimension correlation for each head using the Spearman rank correlation coefficient. This statistical measure was computed based on the visualization provided earlier. A Spearman correlation coefficient closer to 1 (in absolute value) indicates a stronger correlation, with the sign showing the direction. More details are in Appendix A.3.\n4.3.2 Influence of Positional Heads on long distance modeling\nTo validate the importance of attention heads with high distance-dimension correlations for long text comprehension, we conducted a masking procedure on these heads. Using the metrics described in the previous section, we identified the top 5% and top 10% heads based on their rankings and set their output to zero. We then compared the performance of these heads with randomly sampled 5% and 10% heads. The results, as shown in the following, demonstrate that heads with high distance-dimension correlations exhibit greater importance across various tasks."}, {"title": "Conclusion", "content": "We investigated the properties of attention heads with rotary position embeddings (RoPE) in commonly used Transformer architectures. Using long text comprehension tasks as a starting point, we explored the modeling of token-to-token distance within the model by deconstructing the contributions of different dimensions within the attention heads to the attention scores.\nWe found that due to the computational nature of rotary position embeddings, higher dimensions of the attention heads, which correspond to lower rotational frequencies, are more effective at distinguishing distances between tokens. Furthermore, attention heads that, through training, allocate attention scores across different dimensions according to token distances and exhibit a certain degree of correlation, demonstrate superior capabilities in modeling text distances. These heads are crucial for integrating information from varying distances in long text comprehension tasks.\nWe provide an analytical perspective on the currently popular rotary position embeddings, illustrating the attention patterns of models trained with ROPE. Future research can leverage the properties of these attention heads to address challenging tasks such as long text comprehension."}, {"title": "Limitations", "content": "Although we demonstrated the capability of ROPE in modeling textual distances, several limitations are worth noting. First, our dimensional decomposition approach is based on the explicit meaning of dimensions in rotary position embeddings; this method is not applicable to all types of position encodings. Nonetheless, we maintain that decoupling token distance in attention computation is crucial for integrating and understanding information across different distances. Second, due to computational resource constraints, we could not implement many hypotheses we wished to validate on a larger scale. Our observations were not validated with longer input sequences, and the impact of fine-tuning on these attention heads was not analyzed. We leave a more detailed experimental analysis to future work."}]}