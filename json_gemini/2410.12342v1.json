{"title": "TAS: DISTILLING ARBITRARY TEACHER and STUDENT VIA A HYBRID ASSISTANT", "authors": ["Guopeng Li", "Qiang Wang", "Ke Yan", "Shouhong Ding", "Yuan Gao", "Gui-Song Xia"], "abstract": "Most knowledge distillation (KD) methodologies predominantly focus on teacher-student pairs with similar architectures, such as both being convolutional neural networks (CNNs). However, the potential and flexibility of KD can be greatly improved by expanding it to novel Cross-Architecture KD (CAKD), where the knowledge of homogeneous and heterogeneous teachers can be transferred flexibly to a given student. The primary challenge in CAKD lies in the substantial feature gaps between heterogeneous models, originating from the distinction of their inherent inductive biases and module functions. To this end, we introduce an assistant model as a bridge to facilitate smooth feature knowledge transfer between heterogeneous teachers and students. More importantly, within our proposed design principle, the assistant model combines the advantages of cross-architecture inductive biases and module functions by merging convolution and attention modules derived from both student and teacher module functions. Furthermore, we observe that heterogeneous features exhibit diverse spatial distributions in CAKD, hindering the effectiveness of conventional pixel-wise mean squared error (MSE) loss. Therefore, we leverage a spatial-agnostic InfoNCE loss to align features after spatial smoothing, thereby improving the feature alignments in CAKD. Our proposed method is evaluated across some homogeneous model pairs and arbitrary heterogeneous combinations of CNNs, ViTs, and MLPs, achieving state-of-the-art performance for distilled models with a maximum gain of 11.47% on CIFAR-100 and 3.67% on ImageNet-1K. Our code and models will be released.", "sections": [{"title": "INTRODUCTION", "content": "Knowledge Distillation (KD) (Hinton et al., 2015; Romero et al., 2015) has been demonstrated as a powerful method to transfer knowledge from a pre-trained and cumbersome teacher model to a compact and efficient student model. Compared to the model trained from scratch, the performance of the student model distilled by appropriate teachers usually improves significantly. Commonly, knowledge transferred is derived from either the output logits (logits-based KD (Sun et al., 2024)) or the intermediate features (feature-based KD (Romero et al., 2015)) of the teacher model. Therefore, it is intuitive to understand different teachers have different knowledge (logits or features) determined by their unique architectures (Liu et al., 2021a).\nMost existing KD approaches focus on similar-architecture distillation (Romero et al., 2015; Tian et al., 2020; Liu et al., 2023) (called SAKD), i.e., optional teachers are restricted to a limited scope with structures similar to the student model. However, this homogeneous distillation presents two principal limitations: (1) Limited Potential: Compared to the broader range of arbitrary teachers (including homogeneous and heterogeneous ones), the restricted scope of teachers in SAKD may fail to include the optimal knowledge necessary to enhance the performance of certain students. For instance, as OFA (Hao et al., 2023) demonstrated, distilling knowledge from a heterogeneous ViT-Base to ResNet50 yields superior student performance compared to using a ResNet152 as the homogeneous teacher. (2) Limited Flexibility: The emergence of new models (Liu et al., 2022;"}, {"title": "2 RELEATED WORK", "content": "As shown in Fig. 2, the majority of existing KD methodologies concentrate on homogeneous distillation by using a single projector (e.g., single linear layer) to align the output logits (Hinton et al., 2015; Huang et al., 2022; Sun et al., 2024), intermediate features (Romero et al., 2015; Chen et al., 2021; Liu et al., 2023), feature embeddings (Tian et al., 2020), and module functions (Liu et al., 2023) of T.-S. pairs, thanks to the highly-similar features between homogeneous T.-S. pairs. However, they fall short in addressing the complexities of heterogeneous distillation, where the distinct features between heterogeneous T.-S. pairs pose significant challenges. Although OFA Hao et al. (2023) achieves consistent improvements for arbitrary T.-S. pairs, it does so at the expense of sacrificing feature information to logits. In this paper, our method dives into the nature of heterogeneous feature gaps (i.e., caused by inductive bias and module functions) and introduces a hybrid assistant to facilitate smoother feature transfer between heterogeneous T.-S. pairs.\nAdditionally, several other works are pertinent to our method: (1) We note that some methods attempt to distill the knowledge between CNNs and MSAs (Zhao et al., 2023), but they are tailored to specific T.-S. pairs rendering them impractical for our arbitrary T.-S. CAKD. (2) Certain methods apply progressive distillation to transfer the knowledge via a middle model (Mirzadeh et al., 2020; Cao et al., 2023; Liu et al., 2021a), but they are progressive training strategies that are not training algorithms designed for transferring knowledge between heterogeneous T.-S. pairs. (3) Lastly, while some logits-based methods can be easily applied to CAKD (Hinton et al., 2015; Sun et al.,"}, {"title": "2.1 TAXONOMY OF OUR METHODS", "content": ""}, {"title": "2.2 HYBRID MODEL", "content": "As illustrated in Fig. 3, different models exhibit different inductive biases and module functions. (Raghu et al., 2021) investigates the internal representation structures of ViT and CNN models, revealing significant differences between their heterogeneous features. (Park & Kim, 2021) further provides some fundamental explanations for this phenomenon. Specifically, CNNs are data-agnostic and channel-specific high-pass filters, while MSAs are data-specific and channel-agnostic low-pass filters. Therefore, researchers (Park & Kim, 2021) think CNNs and MSAs are complementary, which inspires them to design a hybrid model following the rules of \"alternately replacing CNN blocks with MSA blocks from the end of a baseline CNN model\". The hybrid model outperforms CNNs in both large and small data regimes (Park & Kim, 2021). Furthermore, the architecture of MLP models (Tolstikhin et al., 2021) is notably similar to ViTs not CNNs, so ConvMLP (Li et al., 2023a) also achieves advanced performance in basic visual tasks by the co-design of CNNs and MLPs. In a nutshell, hybrid CNN-MSA/MLP models improve performance and efficiency through the combination of different inductive biases and module functions.\nInspired by the design of the hybrid model, we mitigate the heterogeneous feature gaps by introducing a hybrid assistant model between cross-architecture T.-S. pairs."}, {"title": "3 METHOD", "content": ""}, {"title": "3.1 PRELIMINARIES", "content": "Existing KD methods perform well in homogeneous distillation, but they may fail in heterogeneous teachers and students. The primary reasons stem from fundamentally distinct feature and logit spaces, caused by different inductive biases and module functions of heterogeneous models.\nInductive Bias and module functions. Inductive bias refers to the set of assumptions that a model uses to make predictions on unseen data (Ren et al., 2022). Module functions describe how a model reads, encodes, decodes, and processes the data (Liu et al., 2023). As shown in Fig. 3, heterogeneous models exhibit different inductive biases and module functions. (1) CNN models (He et al., 2016) slide a set of learnable local kernels across the pixel-level image, focusing on local receptive fields. The weight-sharing kernels are applied across the entire image, providing the network with translation-equivariance to recognize an object regardless of location. (2) MSA models (Dosovit-"}, {"title": "3.2 THREE-LEVEL DISTILLATION PARADIGM", "content": "As shown in (a-e) of Fig. 2, existing methods usually apply a two-level paradigm in SAKD (Hinton et al., 2015; Romero et al., 2015; Liu et al., 2023), i.e., T.-S. scheme, to transfer directly the knowledge of teachers to students. Besides, some works apply progressive training strategy (Mirzadeh et al., 2020; Liu et al., 2021a) to transfer multi-teacher knowledge to a single student model in SAKD, but they are also a two-level paradigm for each T.-S. distillation. Despite these two-level paradigms being better suited for SAKD, transferring knowledge directly without any transition mechanism is challenging for our CAKD due to the spatial diversity of features in Fig. 1.\nIn the human learning process, the role of teaching assistants greatly reduces the information gaps between teachers and students. Motivated by this, as illustrated in Fig. 4, this paper introduces an assistant model as a middle bridge, where the knowledge is transferred by training a teacher-assistant-student scheme (TAS) as follows:\n$\\mathcal{L} = \\mathcal{L}_{TAS} (K_t, K_s) + \\mathcal{L}_{TAS} (K_t, K_a) + \\mathcal{L}_{TAS} (K_a, K_s),$ (1)\nwhere $\\mathcal{L}_{TAS}$ is our loss (details in Eq. (3)). $K_t$, $K_a$, and $K_s$ denote the knowledge of the teacher, assistant, and student model respectively.\nAssistant Model. Our assistant model connects the CNN modules and MSA/MLP modules derived from the students and teachers with a local-to-global (L2G) feature projector as shown in Fig. 4. Formulary, our assistant model can be described as follows:\n$P_a(x) = f_{cm} S_o (MSA \\circ PE) \\circ S S S(x),$ (2)\nwhere $x$ is the input image, $S$ denotes CNN models, $S_m$ denotes MSAs/MLPs, and $f_{cm}$ denotes the fully-connected layers of MSAs/MLPs. To connect CNN and MSA/MLP modules, we propose an L2G module that includes a patch embedding (Dosovitskiy et al., 2021) to convert the features into the required dimensions of subsequent MSA/MLP modules. Besides, to capture the long-distance dependency, our L2G also includes an MSA block to project the local features from CNN models to global receptive fields. For simplicity, the MSA module is a Swin block (Liu et al., 2021b) in this paper (more discussions in supplementary materials). Note that the L2G is the only extra learnable module in our assistant model.\nThe following considerations drive the design of our assistant model: (1) Inductive biases of CNNs and MSAs/MLPs are complementary and hybrid CNN-MSA/MLP models demonstrate good per-"}, {"title": "LOSS FUNCTION", "content": "As shown in Fig. 4, we only transfer the final features after average pooling and the logits for the following reasons. (1) Due to the weight-sharing between our assistant model and T.-S., it combines actually different inductive biases only in the final features, not early and middle features. (2) As shown in Fig. 5 and Fig. 1, the final features of different models are also very different in spatial, so we smooth them by average pooling to mitigate the spatial gaps between different features. The knowledge in Eq. (1) is formulated by $K_i = {f_i, p_i}$, i = t, a, s, where $f_i$ and $p_i$ denote the final features embeddings after average pooling and the output logits.\nIn this paper, we use spatial-agnostic InfoNCE loss $\\mathcal{L}_{InfoNCE}$ (He et al., 2020; Tian et al., 2020) and OFA loss $\\mathcal{L}_{OFA}$ to supervise the transfer of features and logits respectively, motivated by the following observations. (1) Wildly used MSE loss computes the pixel-wise metrics that are suitable for features having similar spatial information, but it will fail when the features are very spatially different (e.g., FitNet with MSE loss gets only 24.06% top-1 accuracy when the teacher is ConvNeXt-T and the student is Swin-P in CIFAR100 Tab. 1). Consequently, we use a contrastive loss InfoNCE (He et al., 2020; Tian et al., 2020) to transfer the structural information of feature embeddings (Tian et al., 2020), which captures complex interdependencies of features without spatial information. (2) As demonstrated in (Hao et al., 2023), the different inductive bias leads models to variant logit spaces. For example, local CNN models are more suitable for small objects, but global MSA/MLP models are more suitable for large objects. Therefore, $\\mathcal{L}_{OFA}$ enhances the information of the target class by adding a modulating parameter $\\gamma$ to the original KD loss, which prevents the student learning from being disturbed by irrelevant information of the teacher. In a nutshell, our $\\mathcal{L}_{TAS}$ is suitable for any representation distillation (e.g., the superior consistently performance in Tab. 2, Tab. 1, and Tab. 3):\n$\\mathcal{L}_{TAS} (K_t, K_s) = \\mathcal{L}_{OFA} (p_t, p_s) + \\mathcal{L}_{InfoNCE} (f_t, f_s) = (1 + p_{\\hat{c}}) log(\\frac{p_s^{\\hat{c}}}{p_t^{\\hat{c}}}) + \\sum_{i=1, i\\neq \\hat{c}}^{C} p_t^i log(\\frac{p_s^i}{p_t^i}) -log \\frac{exp(f_s \\cdot f_t / \\tau_2)}{\\sum_{i\\neq 0} exp(f_s \\cdot f_i / \\tau_2)} $ (3)\nFor each T.-S. pair, we transfer knowledge by $\\mathcal{L}_{TAS} (K_t, K_s) = \\mathcal{L}_{InfoNCE} (f_t, f_s) + \\mathcal{L}_{OFA} (p_t, p_s)$. Firstly, for $\\mathcal{L}_{OFA}$, the $\\hat{c}$ and $c$ denote the target class and predicted class of the input image. C is the all classes in the dataset. $\\mathcal{L}_{OFA}$ add a modulating parameter $\\gamma$ to enhance the target information when the teacher is not confident about the prediction. When $\\gamma$ = 1, $\\mathcal{L}_{OFA}$ is equal to $\\mathcal{L}_{KD}$ with temperate $\\tau$ = 1. Secondly, for $\\mathcal{L}_{InfoNCE}$, $f_s$ denotes an encoded student features by average pooling, and $F_t$ is a set of encoded teacher features in a mini-batch. In $F_t$, only one positive sample $f_t$ matches to $f_s$, i.e., the student's and teacher's feature from the same image is a positive pair. The InfoNCE loss is low when the features of student $f_s$ and teacher $f_t$ are from the same image and high when they are from different images. This loss has been widely demonstrated for aligning different feature representations (He et al., 2020; Tian et al., 2020). The temperature parameter $\\tau_2$ is learnable (Radford et al., 2021). Lastly, the entire loss is $\\mathcal{L} = \\mathcal{L}_{TAS} (K_t, K_s) + \\mathcal{L}_{TAS} (K_t, K_a) + \\mathcal{L}_{TAS} (K_a, K_s)$, where the formulas of $\\mathcal{L}_{TAS} (K_t, K_a)$ and $\\mathcal{L}_{TAS} (K_a, K_s)$ is like $\\mathcal{L}_{TAS} (K_t, K_s)$."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 IMPLEMENTARY DETAILS", "content": "Models. For a fair comparison, we evaluate our TAS using the same teacher-student pairs employed in OFA(Hao et al., 2023), including homogeneous distillation and heterogeneous combinations of CNNs, MSAs, and MLPs. Specifically, CNN models include ResNet (He et al., 2016), MobileNetv2 (Sandler et al., 2018), and ConvNeXt (Liu et al., 2022). MSA models cover ViT, DeiT (Dosovitskiy et al., 2021; Touvron et al., 2021), and Swin (Liu et al., 2021b), while MLP models consist of MLP-Mixer (Tolstikhin et al., 2021) and ResMLP (Touvron et al., 2022). We averagely divide the models into 4 stages following OFA (Hao et al., 2023).\nDatasets. We use the CIFAR100 (Krizhevsky et al., 2009) and ImageNet-1K dataset (Deng et al., 2009) for evaluation. CIFAR100 consists of 50K training samples and 10K testing samples in a resolution of 32\u00d732, while the ImageNet-1K dataset contains 1.2 million training samples and 50K validation samples with a resolution of 224\u00d7224. Since MSAs and MLPs accept image patches as input, we upsample the images in CIFAR100 to the resolution of 224x224 (Hao et al., 2023).\nBaselines. In line with OFA (Hao et al., 2023), we choose several powerful KD methods as our baselines for comparison. Specifically, the feature-based methods include FitNet (Romero et al., 2015), CC (Peng et al., 2019), RKD (Park et al., 2019), and CRD (Tian et al., 2020), while the logits-based methods comprise KD (Hinton et al., 2015), DKD (Zhao et al., 2022), and DIST (Huang et al., 2022). Originally, these methods were designed for SAKD, and thus OFA made some modifications to effectively apply them to CAKD scenarios.\nTraining Protocols. Following the OFA Hao et al. (2023), we utilize SGD optimizer for CNN-based students and AdamW optimizer for MSA- and MLP-based students. All models are trained for 300 epochs in the CIFAR100 dataset. As for the ImageNet-1K dataset, CNNs and MSA/MLP models are trained for 100 epochs and 300 epochs respectively. More details about training schedules and hyperparameters are in Appendix A."}, {"title": "4.2 MAIN RESULTS", "content": "Given extensive cross-architecture teacher-student model pairs, our TAS consistently achieves the best or most competitive performance on the CIFAR100 (+8.38% on average Top-1 accuracy) dataset and the ImageNet-1K dataset (+2.31% on average Top-1 accuracy).\nResults on CIFAR100. To evaluate the performance in enough cross-architecture situations, as shown in Tab. 1, we conduct extensive experiments in 12 combinations of heterogeneous T.-S. models. We have the following important observations in this small-scale dataset."}, {"title": "4.3 ABLATION STUDY", "content": "The architecture of the assistant model. In Tab. 4(A-B), we remove the module Sm in (B) and the MSA module in (A), the performance of different teacher-student pairs drops significantly. This demonstrates the power of combining the inductive biases by adding MSA modules following the CNN modules and combining the module functions by adding Sm. Besides, different MSA blocks have different functions for different T.-S. pairs (details in Appendix F), so we use the Swin block as our MSA block in L2G for simplicity.\nThe components of the three-level paradigm. (1) In Tab. 4 (C), we remove the supervision from the teacher to our assistant model, i.e., $\\mathcal{L}_{TAS}(K_t, K_a)$, which makes the assistant model learn no correct knowledge from the teachers and then transfers the incorrect knowledge to the students. So the distilled students have poor performance. (2) We remove the supervision from the assistant model to students in Tab. 4 (D), i.e., Ltas (Ka, Ks), which is the same as the traditional T.-S. scheme. Due to the gaps between heterogeneous students and teachers are not mitigated by our assistant model, the final performance of students is poor too. (3) We remove the supervision from the teacher model to students in Tab. 4 (E), i.e., $\\mathcal{L}_{TAS}(K_t, K_s)$. In this case, although the performance of distilled students is good in some situations, it is not optimal compared to our TAS. This is because the assistant model inevitably damages some knowledge from the teachers, and some easy"}, {"title": "4.4 DISCUSSION", "content": "Different components of the assistant model. We compare the performance when we use different modules of students and teachers to compose our assistant model in Tab. 5. Specifically, we conduct nine different connections between the student ResNet18 and the teacher (A) ViT-S / (B) Swin-T on CIFAR100. As shown in Tab. 5, the best result is 82.3% when the teacher is ViT-S and the assistant model is $S1\\rightarrow2\\rightarrow S3fm$ and is 81.61% when the teacher is Swin-T and the assistant model is $S\\rightarrow3 \\rightarrow S\\rightarrowf$. For the hybrid assistant model, CNN modules at the beginning are feature extractors, and MSA modules at the end are feature aggregators, which are complementary and both play important roles (Park & Kim, 2021; Dai et al., 2021). Therefore, although different T.-S pairs have different optimal assistants with different connections, we add an MSA/MLP stage following three CNN stages for the assistant model for simplicity, i.e., $S\\rightarrow3 \\rightarrow S\\rightarrowf$, in most situations.\nThe performance and features of the assistant model. Firstly, as shown in Tab. 6, our assistant model delivers superior performance compared to student models while falling short of the teacher's performance, thereby demonstrating its role as a bridge. Secondly, as shown in Fig. 5, the final features of our assistant model are derived from local CNN models and converted into global receptive fields, i.e., our assistant model combines the knowledge from different inductive biases and module functions in final feature spaces. Lastly, the features of the student, assistant, and teacher models are spatially different in Fig. 5, so it is reasonable to smooth them before transfering in Eq. (3)."}, {"title": "5 CONCLUSION", "content": "Limitations and Future Works. (1) It is noteworthy that for certain specific models, such as the extensively studied ResNet18, the performance resulting from distillation by a heterogeneous teacher is inferior to that achieved by a homogeneous teacher. Although our current focus is on the generalizability of various heterogeneous models, yielding significant performance improvements in"}, {"title": "B COMPARISONS WITH OTHER METHODS", "content": "We compare the differences between some similar methods and our TAS in Fig. 2. Firstly, to the best of our knowledge, our TAS is the first to apply three-level teacher-assistant-student scheme, which provides more flexible designs for knowledge transfer than existing two-level scheme. Secondly, our assistant model bridges the representation gaps between cross-architecture students and teachers by combining different inductive biases and module functions, making our TAS more suitable for cross-architecture distillation. Thirdly, as demonstrated in (Hao et al., 2023), the LOFA enhances the target information and hinders the transfer of incorrect information from the teacher by a modulating parameter $\\gamma$ (Hao et al., 2023), which is more suitable than LKL in cross-architecture distillation. Lastly, the LMSE aligns the features in a pixel-by-pixel manner, which is not reasonable for spatially different heterogeneous features, e.g.(A) and (E) in Fig. 6. Thus, as demonstrated in Tab. 10, we get the better performance by smoothing the features in spatial and apply contrastive learning by $\\mathcal{L}_{InfoNCE}$ to align the feature embeddings of cross-architecture models.\nThere are some works to input the student features to teachers in similar-architecture distillations, e.g., ReviewKD (Chen et al., 2021), FCFD (Liu et al., 2023), and so on (Chen et al., 2022b; Li et al., 2020). However, they are all designed for teacher-student pairs with similar architectures, suggesting different motivations and designs compared to our TAS for cross-architecture distillation. For example, FCFD (Liu et al., 2023) has the best performance and is the most similar method to our TAS, but some important designs of our TAS are very different from FCFD.\nFirstly, FCFD (Liu et al., 2023) is designed for CNN students and teachers, which needs to be modified seriously if we apply it to heterogeneous distillations. Besides, as demonstrated in Tab. 8, FCFD is not suitable for any cross-architecture teacher-student models. But our TAS is generic for any teacher-student pair.\nSecondly, although FCFD (Liu et al., 2023) also combines different module functions, the connections between students and teachers are random and mutual, which makes it hard to converge to the optimal spaces and brings huge training costs, especially for cross-architecture distillations. Conversely, our TAS considers that the CNN models are feature extractors and the MSA/MLP models are feature aggregators (Park & Kim, 2021), so the assistant is the CNN-MSA/MLP model and obeys the rule of \"alternately replacing Conv blocks with MSA blocks from the end of a baseline CNN model\" in (Park & Kim, 2021). For example, FCFD (Liu et al., 2023) includes the multiply random connections of MSA/MLP-CNN models (the first parts are MSA/MLP modules, and the latter parts are CNN modules) when we modify it to cross-architecture distillation. However, MSA/MLP-CNN models are unreasonable for the hybrid models (Park & Kim, 2021), leading to bad distillation performance."}, {"title": "LOSS FUNCTIONS FOR CROSS-ARCHITECTURE DISTILLATIONS.", "content": "In Tab. 10, we compare the results of FitNet with different settings on the CIFAR100 dataset. Firstly, the accuracy improves from 24.06 to 65.17 when we apply $\\mathcal{L}_{MSE}$ only to features of the final stage, rather than intermediate stages. This demonstrates the intermediate features are not suitable for feature alignment in some cross-architecture teacher-student pairs. Secondly, the accuracy improves from 65.17 to 76.79 when we apply average pooling to features of the final stage. This demonstrates the diverse spatial distributions of features are not suitable for feature alignment in some cross-architecture teacher-student pairs. Thirdly, the accuracy improves from 76.79 to 78.01 when we replace $\\mathcal{L}_{MSE}$ with $\\mathcal{L}_{InfoNCE}$. This is because $\\mathcal{L}_{InfoNCE}$ considers the releationships between different channels, but $\\mathcal{L}_{MSE}$ is pixel-by-pixel."}, {"title": "F L2G IN OUR ASSISTANT MODEL.", "content": "As shown in Tab. 11, when we replace the Swin block (Liu et al., 2021b) with ViT block (Dosovitskiy et al., 2021) in our L2G, the performance on different teacher-student pairs has different rises and falls. Thus, the MSA block in L2G is also important and is worth exploring in future works.\nOur L2G includes a patch embedding for dimension alignments of features and an MSA block for global information exchange.\nWhy do we use a patch embedding? The feature shape of a CNN model with the size (N, C, H, W), while that of an MSA/MLP model is denoted as (N, L, D). N indicates the batch size, and C, H, and W refer to the channel, height, and width of the CNN model's feature map respectively. L and D denote the patch number and embedding dimension of the ViT/MLP model's feature map. In"}, {"title": "G TRAINING COST.", "content": "Beyond performance considerations, training cost is critical for the distillation. We compare the training cost of the recent OFA (Hao et al., 2023) and our TAS framework in Tab. 12. In Fig. 4(f), OFA uses four extra feature projectors, but our TAS only uses one L2G to project the features at the third stage of CNN models. Therefore, as shown in Tab. 12, we introduce much fewer additional parameters and FLOPs on par with OFA (Hao et al., 2023) under different combinations of teacher and student models. Specifically, the number of extra parameters is about one-tenth that of OFA when the student and teacher are different architectures. As a result, TAS is more efficient than OFA (Hao et al., 2023)."}]}