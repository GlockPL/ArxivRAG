{"title": "CPTuning: Contrastive Prompt Tuning for Generative Relation Extraction", "authors": ["Jiaxin Duan", "Fengyu Lu", "Junfei Liu"], "abstract": "Generative relation extraction (RE) commonly involves first reformulating RE as a linguistic modeling problem easily tackled with pre-trained language models (PLM) and then fine-tuning a PLM with supervised cross-entropy loss. Although having achieved promising performance, existing approaches assume only one deterministic relation between each pair of entities without considering real scenarios where multiple relations may be valid, i.e., entity pair overlap, causing their limited applications. To address this problem, we introduce a novel contrastive prompt tuning method for RE, CPTuning, which learns to associate a candidate relation between two in-context entities with a probability mass above or below a threshold, corresponding to whether the relation exists. Beyond learning schema, CPTuning also organizes RE as a verbalized relation generation task and uses Trie-constrained decoding to ensure a model generates valid relations. It adaptively picks out the generated candidate relations with a high estimated likelihood in inference, thereby achieving multi-relation extraction. We conduct extensive experiments on four widely used datasets to validate our method. Results show that T5-large fine-tuned with CPTuning significantly outperforms previous methods, regardless of single or multiple relations extraction.", "sections": [{"title": "Introduction", "content": "Relation extraction (RE) is one of the fundamental tasks in natural language processing (NLP), aiming to extract relational facts from unstructured text into structured triplets [58,8]. Figure 1 illustrates the standard RE paradigm: given an instance that includes a sentence containing two identified entities, a model is required to categorize the relationship between them from a predefined set of relations. Human knowledge supported by RE significantly boosts the development of downstream knowledge-sensitive applications, such as dialogue systems [59], question answering [54], and knowledge graph completion [2].\nRecently advanced methods for relation extraction can be roughly recognized into two styles: classificational style and generative style. Both styles were recently developed based on pre-trained language models (PLMs) [18,38] and involved using handicraft templates to transform RE into a fundamental linguistic problem to evoke pre-training potential, namely prompt tuning [22]. For"}, {"title": "Related Work", "content": "Having seen the great success of pre-training technology, researchers adapted widely PLMs for RE in recent years. The earlier works [29,49] treated RE as a traditional discriminate task and focused on fine-tuning PLMs to capture rich semantic features of entity pairs and their context that are then fed into an additional classifier for multi-class classification. Since prompt tuning (see in next subsection) has shown remarkable potential on various NLP tasks, the latest methods began to use handcraft templates to transform RE into easier-handle problems. We can roughly divide them into two groups, namely classificational-style and generative-style. During pre-processing, both styles verbalize numerical relation labels as brief textual descriptions. Classificational methods [53] commonly use cloze templates to organize the RE instance (including highlighted entity pair) to align with the MLM task format. Generative methods [11,15] may also adopt a cloze template to reformat the instance but require a PLM to generate verbalized relation labels in an autoregressive manner. Beyond reforming RE as foundational linguistic tasks used for pre-training, some works convert RE into summarization [28], reading comprehension [17], and machine translation [33] with elaborate templates to conveniently induce the relevant knowledge contained in the upstream models that benefit RE."}, {"title": "Prompt Tuning", "content": "Prompt-based fine-tuning is an increasingly popular transfer learning strategy, which unlocks the PLMs' potential by using learnable or handcraft templates"}, {"title": "Method", "content": "In this section, we present the details of CPTuning, a contrastive prompt tuning paradigm that adapts Seq2Seq PLMs for RE and overcomes the EPO challenge. We transform RE into Seq2Seq text-infilling in Section 3.1, describe the generation and scoring of candidate relations in Section 3.2, and illustrate the contrastive-based learning procedure in Section 3.3."}, {"title": "Task Definition", "content": "Let D = {x,y} present a RE dataset, including the instance set X and the relation set Y. In general paradigm, given an instance text xi \u2208 X with two"}, {"title": "Candidate Relations Generation and Scoring", "content": "It is worth noting that since we assume more than one relation is established between two entities in a given RE instance, any candidate relation estimated by a relatively large probability should be checked for existence. Therefore, how to access the estimated likelihood of a candidate relation in our paradigm is considered. On the one hand, although we mask three types of slots in the source text, only the recovered relation slot in the target text is necessary. Moreover, each token of the generated target text is conditionally sampled from the PLM vocabulary, causing a risk of producing invalid relation words. We draw ideas of [15,28,11] and perform prefix-given constrained decoding (PGC) at inference time to overcome these two challenges. To this end, we divide a target text into dual parts: prefix z, including words before the relation slot, and relation r, including words after the sentinel [Z], and the probability P(t|M(s)) can in turn be unfolded:\n$$P(t|M(s)) = P(r|M(s), z) \\cdot P(z|M(s))$$\nReviewing in Figure 2(b) that all candidate relations regarding a certain instance share an identical prefix, we thus omit the second term on the right side of Eq.1 and solely model the relation part with pre-trained parameters $\\theta$:\n$$P(r|M(s), z; \\theta) = \\prod_{i=1}^{r}P((i)|M(s), z, r_{\\textless i}); \\theta)$$"}, {"title": "Contrastive-based Learning", "content": "According to Eq. 2, without considering EPO, our goal of fine-tuning the PLM is to maximize the conditional probability of gold relation words (plus a sentinel token [E]). This is commonly known as maximum likelihood estimation (MLE), which trains a model to minimize the following cross-entropy loss:\n$$L_{ce}(\\theta) = -\\sum_{i=1}^{|r|} log P(r(i) M(s), z, r'_{(\\textless i)}; \\theta)$$\nWhen it comes to the EPO scenario, this approach causes twofold problems. In training, given a sample with more than one gold relation, it is undetermined which one to be maximized. A step further, the fine-tuned model ideally predicts one of the gold relations by the probability of 1 while others by 0, significantly contradicting our scoring strategy. To make the model aware of the relative correctness of a candidate relation, we apply label smoothing [31] on Eq. 4:\n$$L_{lbls}(\\theta) = -\\sum_{i=1}^{|r|} P* (r(i)|M(s), z, r_{(\\textless i)}) log P(r(i)* |M(s), z, r_{(\\textless i)}; \\theta)$$\nwhich aims to fit the soft-label distribution:\n$$P* (r(i)|M(s), z, r_{(\\textless i)})=\\begin{cases}\n1 \u2013 \\beta, & r(i) = r(i)* \\\n\\frac{\\beta}{\\gamma-1}, & r(i) \u2260 r(i)* \\text{ and } r(i) \u2208 T_{i}\\\\\n0, & \\text{otherwise}\n\\end{cases}$$\nwhere $r(i)$ denotes a token the model predicted at the i-th step, and Ti collects the candidate words on the i-th layer of the Trie mentioned in Section 3.2. We call this technique layer-based label smoothing (LBLS) and graphically illustrate it in Figure 4b, where the hyperparameter \u03b2 is set to 0.3. In this case, the Trie has four layers, and N\u2081 to N4 are individually 1, 2, 1, and 4. It can be seen that LBLS trains a model to generate the gold relation words \"person country of death\" with a higher probability while generating others with a lower rather than directly overlooking them.\nAdditionally, to ensure the effectiveness of our scoring mechanism, we further propose a contrastive loss:\n$$L_{ctl}(\\theta) = \\sum_{r_i\u2208R_s} max( - f(r_i),0) + \\sum_{r_j\u2208R_\\text{s}} max(f(r_j) \u2013 \\zeta,0)$$\nwhere Rs denotes a candidate relation set w.r.t the samples, containing gold relations R, as well as fake relations R\\text{\u207b} randomly sampled from v(Y). In this"}, {"title": "Experiments", "content": "We use four well-known RE datasets to conduct experiments, including:\nTACRED [56] is one of the most popular RE datasets categorizing entities into subject and object classes and providing the spans and types of entities. The special case \"no_relation\" is also considered.\nTACREV [1] is developed on the TACRED basis. It widely re-annotates the incorrect samples in the development and test sets while retaining the original training set.\nRe-TACRED [43] is another revised version of TACRED that relabels the full dataset and rectifies a few relation labels with unclear meanings.\nNYT [39] is a well-known relation extraction dataset often used to evaluate a model in handling SEO (Single Entity Overlap) and EPO issues. The number of EPO entity pairs in the NYT training and test sets are 9782 and 987, respectively."}, {"title": "Baseline Methods", "content": "We select the following representative methods across classificational and generative styles as baselines for comparison. SFT, i.e., ROBERTa [25] or T5 [38] supervise fine-tuned with cross-entropy loss. For T5, the target text is our verbalized relation. GDPNet [51] searches indicative words to enhance relation representations. SpanBERT [14] learns to model spans to capture better structure information. MTB [41] learns relation representations directly from the entity-linked text. KnowBERT [36], retrieving entity embeddings to update word representations. K-Adapter [46] injects multiple kinds of knowledge into a model with adapters. LUKE [52] treats words and entities as tokens to directly model relations. CR [60] encourages several identical models to predict a similar"}, {"title": "Implementation Details", "content": "We implement the standard version of CPTuning with T5large as the base model. During fine-tuning, we train the model for at most 10 epochs, and the batch size is set to 32. We adopt an AdamW [27] optimizer with a linear learning schedule. The learning rate is initially set to 2.5e-5. It warms up during the first 10% training steps and decays to 0 gradually in the subsequent steps. All our experiments are conducted on one NVIDIA Tesla A100 GPU, and the codes are developed with Pytorch\u00b3 and Transformer\u2074 libraries. As for hyperparameter settings, we set a = 0.6 in Eq. 3 and \u03b2 = 0.2 in Eq. 6. The borderline A is set to 1.0, the threshold is set to 1.2, and the balance factor u is set to 0.1. Whenever the PGC beam search decoding is performed, the beam size K is 16. We also sample 16 gold and fake relations during contrastive training represented by Eq. 7. Following previous studies, we use micro F1 scores (%) as the metric for model evaluation."}, {"title": "Results", "content": "Table 2 presents the comprehensive evaluation results of our proposed CPTuning and the baseline methods. We begin by analyzing the results on single-relation TACRED and TACREV datasets, followed by the results on multi-relation NYT.\nSingle-ralation RE. Among the mentioned two types of baselines, FPC demonstrated the best performance in the classification methods group, while GenPT exhibited the best results in the generative group. We note that GenPT approaches RE as a Seq2Seq text-infilling problem, similar to our approach. To some extent, this suggests the compatibility between the two tasks, which is worth exploring in future studies. Besides, generative methods commonly outperform the classificational, which echoes the findings in previous studies. On the other hand, we observe that CPTuning significantly enhances the previously best performance of single-relation RE. Specifically, compared with GenPT, T5large fine-tuned with CPTuning-s2 achieved a 1.8 and 0.9 improvement in micro"}, {"title": "Analysis", "content": "Ablation Study. We conducted an ablation study to examine the effectiveness of our designed layer-based label smoothing (LBLS) and contrastive-based learning (CTL). We remove one or both of them from the learning procedure and show the corresponding CPTuning-s1 performance in Table 3. Note that without both strategies, CPTuning degenerates into a supervised fine-tuning method, which performs our transformed RE and trains a model to minimize the CE loss in Eq. 4. We name this variant CPTuningce, and it shows notorious superiorities over the baseline SFT, especially across the three single-relation RE datasets, indicating the effectiveness of our task transformation strategy. On the other hand, either augmenting the CE loss with LBLS - CPTuninglls or attaching it with a contrastive loss CPTuningctl can significantly improve the CPTuningce performance on NYT, and the maximum gain is achieved by using them together, i.e., the standard CPTuning. We also find that using both strategies together can further enhance single-relation RE performance. All these results reveal that the two proposed non-deterministic likelihood assumptions mutually enhance each other during learning.\nSemantics Analysis of Model Outputs. It is worth noting that CPTuning reforms RE as a Seq2Seq text generation task, generating relation words in the target text rather than predicting numerical labels. Intuitively, it understands the semantics entailed in a verbalized relation. We introduce a H-index to quantify the strength of this ability. Given an instance-relation pair (s,r), we can select top-M candidate relations C\u2081 from the model's outputs according to estimated likelihood. Also, we can find M relations C\u2082 that are semantically closest to r from the relation set according to semantic similarity determined by a bidirectional language model, like BERT [9]. We define H@M as the Intersection of Union (IoU) of the sets C\u2081 and C\u2082, which measures the consistency of likelihood estimation and semantics estimation. Results in Table 4 reveal that CPTuning is advanced in capturing the semantics feature of relation words. It"}, {"title": "Conclusion", "content": "In this paper, we proposed a novel contrastive-based prompt tuning method for RE, named CPTuning. Specifically, CPTuning reforms RE into a Seq2Seq text-infilling task using handcrafted templates. It further learns a generative language model to associate a candidate relation between two in-context entities with a probability mass above or below a threshold, corresponding to whether the relation is established. We conducted extensive experiments on four well-known RE datasets to validate our method. The results demonstrated that CPTuning effectively overcame EPO challenges ignored in previous works, and the T5-large model fine-tuned with CPTuning exhibited state-of-the-art performance across all single and multiple relations extraction tasks."}]}