{"title": "NLP Sampling: Combining MCMC and NLP Methods for Diverse Constrained Sampling", "authors": ["Marc Toussaint", "Cornelius V. Braun", "Joaquim Ortiz-Haro"], "abstract": "Generating diverse samples under hard constraints is a core challenge in many areas. With this work we aim to provide an integrative view and framework to combine methods from the fields of MCMC, constrained optimization, as well as robotics, and gain insights in their strengths from empirical evaluations. We propose NLP Sampling as a general problem formulation, propose a family of restarting two-phase methods as a framework to integrated methods from across the fields, and evaluate them on analytical and robotic manipulation planning problems. Complementary to this, we provide several conceptual discussions, e.g. on the role of Lagrange parameters, global sampling, and the idea of a Diffused NLP and a corresponding model-based denoising sampler.", "sections": [{"title": "1 Introduction", "content": "Sampling from a constrained set is a core problem in many areas: In robotics we may want to sample solutions subject to kinematic or physics constraints to tackle sequential manipulation planning. In Monte Carlo methods samples are generally used to estimate expectations. In generative models we generate samples to match a data distribution - analytical hard constraints have received less attention in this context so far.\nIn this work we consider how methods from across these field can be combined to provide strong constrained samplers. When starting this work we were in particularly interested in whether and how diffusion denoising ap-\nproaches can be leveraged also for model-based hard-constrained sampling of diverse solutions to non-linear mathematical programs. However, our initial approach to this end failed, and Sec. 5 includes a conceptual discussion on this topic and our insights. Many other methodologies from across the fields can fruitfully be integrated in a joint framework, which is the focus of the technical contributions in this study."}, {"title": "2 Related Work", "content": "Robotic Path Planning. Sampling-based methods have a long tradition in robotics. Prominently, sampling-based motion planning methods aim to find collision-free paths to a given goal under environmental constraints [13]. To this end these motion planners sample the high dimensional configuration space, using different strategies, until a path has been found [11]. While basic sample-based path planning does not consider differentiable feasibility constraints, Manifold RRT methods [18, 12] and other extensions account for such constraints. We will consider Manifold RRT as one candidate for what we will define as interior sampling, and thereby compare it to alternative MCMC methods, including Langevin and Hit-and-Run. However, as RRT methods are targeted to find continuous paths in configuration space, the sampling process expands continuously and cannot jump across infeasible regions to discover"}, {"title": "3 Restarting Two-Phase NLP Samplers", "content": "In the context of constrained optimization, \"Phase I optimization\" refers to the problem of finding a feasible point which can then, in Phase II, be used to seed an interior optimization method. We propose a family of sampling methods that equally adopts this basic two-phase approach. Specifically, a Restarting Two-Phase NLP Sampler iterates the following steps until enough samples are collected:\n(i) Sample a new seed x, potentially conditional to previously found samples D.\n(ii) A slack downhill method tries to find an (approximately) feasible point x within Kdown steps.\n(iii) If x is feasible, an interior sampling method, potentially requiring Kburn burn-in steps, tries to collect Ksam samples ~ $e^{-f(x)}$ within the feasible space. If the method does not respect feasibility exactly, it is combined with a direct slack reduction (manifold projection) step.\nAlg. 1 provides more explicit pseudocode, where s(x) is a slack vector of constraint violation, defined in detail below. While straight-forward, we are not aware of sampling techniques to adopt this basic two-phase approach. We propose this family as a generic framework to integrate methods from across the fields of MCMC sampling, constrained optimization, and robotics. The following sections discuss alternatives for slack downhill and interior sampling within this framework."}, {"title": "3.1 Downhill, Noise, & Step Rejection on the Relaxed NLP", "content": "We first describe a class of methods that can be used for either slack downhill or interior sampling as combining a gradient-based downhill step with (optional) noise and an (optional) step rejection mechanism. This includes Langevin variants as well as classical constrained optimization techniques.\nTo introduce this class of methods we define the underlying energy as follows: Given constraint functions $g:R^n \\rightarrow R^m$,$h: R^n \\rightarrow R^{m'}$ , we define $s(x) = ([g(x)]_+,|h(x)|) \\in R^{m+m'}$as the slack vector, where the ReLU []+ and absolute value |\u00b7| are applied element-wise. This is the stacked vector of all inequality and equality violations. The total violation is given as $1^Ts(x)$, which is also the $L_1$-slack penalty. However, in this work we focus on the $L_2$ slack penalty $s(x)^Ts(x)$ to exploit its Gauss-Newton steps. Given an interior energy $f: R^n \\rightarrow R$, we define the relaxed NLP as\n$F_{\\gamma\\mu}(x) = \\gamma f(x) + \\mu s(x)^*s(x)$ .  (2)"}, {"title": "3.2 Interior Sampling with Explicit Constrained Handling", "content": "The previous section described methods that operate on a differentiable energy F(x) - akin to unconstrained optimization. In contrast, in this section we consider sampling methods that deal with constraints"}, {"title": "3.2.1 Non-linear Metropolis-Adjusted Hit-and-Run (NHR)", "content": "A set of linear inequalities $Gx+\\bar{g}\\leq 0$ defines a polytope. Several approaches to sampling uniformly from a polytope are well-established [4] and can easily be adjusted using MH to also account for an interior target density $x\\sim e^{-f(x)}$. A standard family of approaches are Ball and Dikin Walks, which use step proposals sampled uniformly from a ball or ellipsoidal and reject them when outside the polytope [4]. Dikin Walks are very similar to Riemannian Langevin on $F_{1\\mu}$ for $\\mu\\rightarrow\\infty$.\nHowever, in this section we are particularly interested in another approach as it is quite different to typical MCMC walks with local proposals: Hit-and-Run (HR) [22]. HR samples a random direction in $R^n$ (uniformly), computes where this line intersects the inequalities, and then samples uniformly from the interior line segment. Alg. 2 defines standard HR.\nWe extend HR to become applicable to non-linear inequalities, as well as MH-adjust it to non-uniform interior $p(x) \\propto e^{-f(x)}$. Alg. 3 presents the novel method. All the clip operations use line 5 of the original Hit-and-Run for some $\\bar{g}$ and a. The inner loop (line 6) samples a candidate y on the line, evaluates the inequalities g(y), and if they are violated clips the interval $[\\beta_{lo}, \\beta_{up}]$ with only the violated inequalities using the linearization at y. Note that the initial clipping (line 5) with the linearization at x is not necessary, which leads to more exploration."}, {"title": "3.2.2 Non-linear Manifold Rapidly-Exploring Random Tree (mRRT)", "content": "RRTS generate samples by gradually growing from an initial sample (start configuration) towards uniformly covering the sample's connected component [13]. The incremental nature (within a connected component) is analogous to MCMC walks, but the fact that it generates a tree instead of a chain, i.e., grows \"at all leafs\" makes it explore fast and an interesting candidate for sampling in general, regardless that it was developed to return connected paths.\nRRTs have been generalized to grow on non-linear manifolds [18]. All these approaches require an initial sample on the manifold to grow from, which our Phase I step can provide. For completeness, we include our specific implementation of manifold RRT in Alg. 4. In this algorithm, growth steps are projects to be tangential to the equality constraints. A following SlackReduce step has to correct for non-linearities in the constraints, but also corrects for inequality violation by projecting to the nearest feasible point."}, {"title": "3.3 Conditional Restart Seeding", "content": "We discussed various alternatives for the slack downhill and interior sampling phases. (For slack reduce we will always use a Gauss-Newton slack step with full step length \u03b1 = 1.) The seeding of restarts conditional to previously found samples D is yet open.\nWe consider three options, where the last is, to our knowledge, novel:\n(i) Sample $x \\sim U[l, u]$ independent of previous samples, which makes all episodes i.i.d.\n(ii) Sample a candidates $\\{x_1, \\dots, x_c \\}$ uniformly and select $argmax_i min_{y\\in D} |y - x_i|$, which maximizes the nearest distance to the data D.\n(iii) Sample a candidates $\\{x_1,..,x_c\\}$ uniformly, compute the Gauss-Newton slack step $d_i$ for each, and select $argmin_i max_{y\\in D} \\frac{y-x_i}{|y-x_i|}\\frac{d_i}{|d_i|}$, which minimizes the alignment of the slack direction with any data direction."}, {"title": "4 Empirical Evaluations", "content": "The full sources to reproduce all evaluations are available here.1\n4.1 Evaluation Metrics and Minimum Spanning Tree Score (MSTS)\nWe generally found it hard to find a useful metric that reflects a degree of diversity in the sampled dataset D. For a low-dimensional toy problem we can use the Earth Mover Distance between the samples and ground truth samples (e.g. generated using naive rejection sampling) to check the \"correctness\" of sampling. However, in the problems of interest to us we care less about correctness after asymptotic mixing, but rather the rate of diversity and mode coverage we get with each and early samples.\nWe first considered metrics that analyze the neighborhood of samples, e.g. the spectrum of distances to the ten nearest neighbors. However, such metrics are fully ignorant on whether different modes are populated. Nearest neighbor-based metrics can capture uniformity or how locally equidistant samples are drawn, but not coverage of modes.\nWe found the size of the minimum spanning tree of all points in D an interesting quantity that does capture also distances between covered modes, as at least one edge needs to connected distant covered modes. Specifically, we define the Minimum Spanning Tree Score MSTSp(D) as the total cost of the tree when using edge costs $|x - x'|^p$.\nWhen edge costs are plain Euclidean distances, p = 1, it is guaranteed that $MSTS_1(D)$ strictly increases with n = |D| when adding points. Interestingly, the rate of increase seems to give an indication of the dimensionality of the sampled space: E.g., when grid-sampling n points from a box of dimension d we expect an $MSTS_1(D)$ of roughly $n^{1-1/d}$ (the number of lines of length 1 along one dimension of the grid). Our experiments suggest that a similar behavior is true for uniformly sampled points.3\nWith edge costs $|x - x'|^p$ for p > 1, the MSTSp(D) does not monotonously increase with |D| when adding points. Instead, with increasing |D| modes become more and more densely populated, with neighbor distances going to zero and contributing less to MSTS. However, distances between separated modes stay greater than zero and add up. Therefore, $MSTS_p(D)$ will (for large |D|) sum over mode separations and become a metric of separateness of modes.\nUsing restarts and interior sampling, our methods generate the sample dataset $D_n = \\{x_i\\}_{i=1}^n$ sequentially. Given the sequence $x_i, i = 0, .., n$, we compute $MSTS_p(D_n)$ for each data subset Dn so that we can plot"}, {"title": "5 Conceptual Discussions", "content": "5.1 Is there a role for Lagrange Parameters in NLP Sampling?\nFor zero interior energy, f(x) = 0, any feasible point is a solution to the NLP with zero dual variable. As is obvious from the 1st KKT condition, the role of Lagrange parameters is to counteract cost gradients -Vf(x) pulling into the infeasible region. Therefore, without costs, no Lagrange parameters. In other terms, if we only care about sampling feasible points (without energy), we can discard the whole idea of Lagrange parameters in the first place.\nFor non-zero costs, Lagrange parameters establish stationarity for a single optimum point located exactly on active constraints. This bears the question whether in NLP Sampling Lagrange parameters can equally play a role when sampling points, esp. points with active constraints. However, in view of our concrete proposed family of NLP Samplers, it seems the answer is negative. For instance, consider NHR sampling: The only difference between flat energy f(x) = 0 and non-zeros costs is the use of Metropolis-Hasting to calibrate the energies before and after a proposed step. At no point this method aims to sample exactly on active constraints. This argument seems to hold for any methods that truly samples in the interior, which leads to the question whether manifold sampling strategies could benefit from estimating Lagrange parameters. Such methods typically aim to move tangentially on the manifold and Lagrange parameters (e.g. added to the score of a Langevin method) could be used to steer gradient-based MCMC methods towards tangential steps. However, we can enforce tangential steps also much more explicitly using the constraint Jacobian (as mRRT does). Therefore, also for manifold samplers the potential role of estimated Lagrange parameters seems unclear. More fundamentally, also for manifold sampling we never care about a single point of stationarity, but rather about continuously making steps tangentially to the manifold.\nTherefore, in view of the broad range of approaches considered in this paper, we currently do not see a role for Lagrange methods in NLP sampling.\n5.2 Relaxed vs. Diffused NLP\nWe defined the relaxed NLP as $F_{\\gamma\\mu}(x) = \\gamma f(x) + \\mu s(x)^*s(x)$, mixing square slack penalties with the energy. In contrast, our initial interested in this line of work was with the idea of a diffused NLP: Let $p(x_0) \\propto e^{-f(x_0)} I_{g(x_0)\\leq 0} \\square_{h(x_0))=0}$ be our target distribution, where we renamed x by xo. A diffusion model describes the joint distribution $p(x_{0:T})$ as a Markov chain, where p(xo) is our target distribution, and each transition $p(x_{t+1}|x_t) = N(x_t; \\sqrt{\\alpha_t}x_{t-1}, (1 - \\alpha_t)I)$ mixes Gaussian noise [10]. We have\n$p(x_t|x_0) = N(x_t; \\sqrt{\\bar{a_t}}x_0, (1-\\bar{a_t})I)$, $\\bar{a_t} = \\prod_{s=1}^t \\alpha_s$,   (5)\n$p(x_t) = \\int p(x_t x_0) p(x_0) dx_0$   (6)\nWe call $p(x_t)$ the diffused NLP. Based on this, there are two interesting questions: (1) Can we analytically describe or approximate $p(x_t)$? And (2), can we design an efficient NLP Sampler that follows the typical denoising process, starting to sample from $p(x_T)$ and denoising step-wise until we have a sample from $p(x_0)$?\nConcerning (2), we failed to design an efficient NLP sampler following these lines of thoughts. The Appendix B sketches what we considered, but these approaches were orders of magnitude less efficient that the rather straight-forward two-phase methods proposed above. Especially, the aggressiveness and efficiency of GN steps in highly non-linear cases seemed to be lacking in our attempts.\nNevertheless, concerning (1), we can approximate the Diffused NLP $p(x_t)$ using local observations of f, g, h and their gradients. Note that using linearizations of f, g, h essentially means considering the local"}, {"title": "5.3 Globally Estimating the Partition Function", "content": "Consider a target distribution with two distant modes, e.g. with energy $F(x) = min\\{(x-x_1)^2/2+\\alpha, (x-x_2)^2/2 + \\beta\\}$ in 1D, where the mode at $x_1$ has base-energy \u03b1, and the mode at $x_2$ has base-energy \u03b2. In physics, these two modes could be macro states with different energy levels \u03b1 and \u03b2. The energy levels determine how many samples (micro states) populate each mode. In our example (if $x_1$ and $x_2$ are distant) a fraction $w_1 = \\frac{e^{-\\alpha}\\sqrt{2\\pi}}{Z}$ of samples will be in the first mode, and $w_2 = \\frac{e^{-\\beta}\\sqrt{2\\pi}}{Z}$ will be in the second mode.5 The point here is that only observing a local energy value $F(x_1) = \\alpha$ (or its gradient) provides no information on how much the mode is populated. Only the partition function Z tells which part of the samples really populate a mode, namely $ \\frac{e^{-\\alpha}\\sqrt{2\\pi}}{Z}$ in our case. In this view, discussing the partition function in sampling is analogous to the discussion of local vs. global optimization.\nOur methods aim to efficiently discover modes, but do not correctly calibrate their weighting. That is, the \"base of attraction\" decides on the population of a mode, rather than its energy level relative to other modes. We want to note that within each mode, interior sampling methods ensure correct calibration of sample probabilities, but MCMC methods (with essentially zero mixing probability between modes) are no solution to calibrating across modes. If samples can be clearly clustered into modes, one approach to calibrating across modes could be to, in retrospect, re-weight samples based on their energies and density.\nHowever, estimating the global partition function is beyond only calibrating across discovered modes: It implies estimating the total probability mass of modes we have not yet discovered at all. How to estimate this on the basis of our seeded restarting methods is an interesting challenge. It seems that global assumptions about the underlying functions (as made, e.g., in Bayesian optimization or other global optimization approaches) are necessary for an estimation, which is beyond the scope of this discussion. However, the above discussion helps to understand the gravity of globally correct sampling, which relates to our next discussion."}, {"title": "5.4 Diffusion Denoising Models vs. NLP Sampling", "content": "An initial motivation for this whole study was the impressive global coverage and diversity of samples generated by diffusion models such as DDPMs [10]. The generative process, Langevin dynamics guided by a model trained to predict the noise vector e from the original sample xo, has amazing properties in"}, {"title": "5.5 Sequential vs. Coordinated Sampling (e.g. Stein-Variational)", "content": "Our restarting two-phase methods generate samples sequentially: Each sample xi is generated by a run (involving downhill and interior sampling) that may be influenced by previous samples D. This is in contrast to parallel runs, where multiple particles descend in parallel to become samples of p(x), as in Stein-Variational Gradient Descent [14].\nOn a conceptual level, it is interesting to compare the amount of information available for informed sample generation in a sequential vs. coordinated approach: In a sequential approach later runs are informed of actually feasible previous samples, whereas in parallel runs all particles inform each other but have no information yet about truly feasible regions. In both approaches, samples can \"keep kernel distance\", but in the parallel approach much more symmetrically.\nWhile SVGD is guaranteed to eventually sample from p(x), naively applying it on p(x) (with zero value and score in the infeasible region) is prone to fail. However, there is a rich space of promising ideas to combine SVGD approaches with our framework: SVGD could be applied only for interior sampling and initialized with results from pure downhill runs. Or SVGD could be extended to include a downhill phase itself, where it is first applied on F01 and then on F\u2081\u03bc. Finally, the combination of both ideas is interesting: batch-wise restarting of parallel runs, where particles are not only informed of each other but also the feasible samples found in the previous batch. This was also demonstrated in previous work on"}, {"title": "6 Summary", "content": "The aim of this study is to better understand approaches to efficiently generate diverse solutions to constrained problems. The fields of MCMC, constrained optimization, as well as robotics can all contribute ideas to this challenge. With this work we aimed to provide an integrative view and framework to combine methods from these fields, and gain insights in their strengths from the empirical evaluations.\nOn the technical side, our contributions are:\n\u2022 We first proposed the NLP Sampling problem formulation, which is nothing but constrained sampling but under the typical assumptions of NLPs: the cost/energy and constraint functions can be accessed point-wise in a differentiable manner.\n\u2022 We proposed an integrative framework, restarting two-phase NLP samplers, to combine method-ologies flexibly.\n\u2022 We proposed novel basic MCMC samplers, in particular Non-linear Metropolis-Adjusted Hit-and-Run (NHR), which extends HR to non-uniform interior energies using Metropolis-Hasting as well as to non-linear inequalities. Using a basic margin approach we can also this can also handle non-linear equalities when used in conjunction with slack reduction in each step.\n\u2022 We proposed the Minimum Spanning Tree Score MSTSp as a novel metric to capture the diversity of generated samples. This metric greatly helped us to navigate through the large space of methods and conversely gain insights in the structure of our evaluation problems.\nBeyond these technical contributions we hope that we also contributed on a conceptual level, starting with highlighting the strong relations, e.g., between Riemannian-Langevin and GN steps, or between Metropolis Hasting and the Armijo rule. Sec. 5 highlights that this study was particularly inspired by the impressive properties of data-trained denoising models, esp. its global transport of probability mass to correctly cover all modes. Our discussion mentions corresponding ideas, but also the negative results we found when trying to realize NLP Sampling by a Langevin process on a (locally approximated) Diffused NLP. Based on these insights we discussed how learning and diffusion models could help to tackle NLP sampling in the future."}, {"title": "A Downhill, Langevin, Noise, and Step Rejection", "content": "In this section we consider an energy function F(x). As explained in Sec. 3.1 this may be the pure slack downhill energy F01 or the interior sampling energy F\u2081\u03bc, \u03bc\u00bb 1.\nDownhill Steps & Noise: The first ingredient of downhill methods is a plain gradient step -\u03b1VF(x) or a Gauss-Newton (GN) step \u2212\u03b1($\\nabla^2F(x)$ + \u03bbI)-1VF(x) with GN-approximated Hessian $\\nabla^2F(x)$ and regularization \u03bbI. We generally post-process these steps by clipping with a potential maximal stepsize dmax and accounting for box constraints.\nNoise injection is uncommon in classical optimization, but an interesting candidate to increase robustness to small local optima, plateaus or only piece-wise continuous gradients (as common in robotics applications). Isotropic noise adds Gaussian noise,\n$x \\leftarrow x + \\sigma z, z\\sim N(0, I_n)$,   (9)\nwith standard deviation \u03c3\u2208 R as parameter. Covariant noise adds Gaussian noise with precision matrix equal to the regularized Hessian H = $\\nabla^2F_{\\gamma\\mu}(x)$ + \u03bbI,\n$x \\leftarrow x + \\sigma\\sqrt{H^{-1}}z, z \\sim N(0, I_n)$,  (10)\nwhere $\\sqrt{H^{-1}}$ refers to the Cholesky decomposition of $H^{-1}$.\nThe use of $H^{-1}$ as noise covariance is motivated by interpreting the Hessian as a local metric relative to which the Gauss-Newton step is the covariant gradient. Using covariant noise for slack minimization F\u00b5 has the following effects: (1) Directions without slack have small eigenvalue \u03ba + \u5165 and therefore large standard deviation $\\sqrt{1/(\u03ba + \u03bb)}$. That is, the noise explores more along directions without slack, and is smaller in directions with slack. (2) In the limit \u00b5 \u2192 \u221e, $\\sqrt{H^{-1}}$ has zero eigenvalues along active directions. In this case, the noise become strictly tangential to slack isolines. Therefore, covariant noise realizes many aspects we would intuitively see as beneficial for manifold sampling and exploration along slack isolines.\nRelation to Langevin variants: Given a distribution $p(x) = \\frac{e^{-F(x)}}{Z}$ with energy F(x) and partition function Z, the score function is neg-energy gradient\n$s(x) = \\nabla log p(x) = \\nabla log\\frac{1}{Z}e^{-F(x)} = -\\nabla F(x)$,  (11)\nwhich is independent of Z. Langevin dynamics in continuous time is described by\n$\\dot{x} = -\\nabla F(x) + \\sqrt{2}\\xi$  (12)\nwith Brownian motion \u03be. For a discrete time (Euler integration) step, this translates to\n$x_{t+1} = x_t - \\tau \\nabla F(x_t) + \\sqrt{\\sqrt{2\\tau}}z, z \\sim N(0, 1)$  (13)\nwhere the variance of the Brownian noise step only increases with $\\sqrt{\\tau}$. In this view, what is special about Langevin is the relative calibration of stepsize and noise. We summarize this as:\nObservation 1. Discrete time Langevin dynamics (13) combines plain gradient steps \u2013ad with isotropic noise where the noise scaling $\\sigma = \\sqrt{2a}$ is tied to the step size a = \u03c4.\nRiemann Manifold Langevin (Girolami et al, 2011) is described by the continuous time dynamics\n$\\dot{x} = -G(x)^{-1}\\nabla F(x) + \\sqrt{2}G(x)^{-1}\\xi$,  (14)"}, {"title": "B Relation between Relaxed NLP and Diffused NLP", "content": "Sec. 5.2 discusses using the Diffused NLP instead of the Relaxed NLP as a basis for NLP Sampling. The Diffused NLP is the distribution $p(x_t)$ that arises from taking our $p(x) \\propto exp(\u2212 f(x)) I_{g(x)\\leq 0} \\square_{h(x)=0}$ and diffusing it following DDPMs [10]. The following derives an approximation of this $p(x_t)$.\n1D Interval case: Let's first investigate a 1D interval case, where $p(x_0), x_0 \\in R$ is defined via the NLP\nf(x) = 0 s.t. $l\\leq x \\leq u$.  (23)\nAs t is fixed, we introduce simplifying notation $a = \\sqrt{\\bar{a_t}}, \\sigma^2 = (1 -\\bar{a_t})I$, so that (5) becomes $p(x_t|x_0) = N(x_t; a x_0, \\sigma^2)$, and the convolution is\n$p(x_t) = \\int p(x_t|x_0) p(x_0) dx_0 = \\int p(x_t|x_0) dx_0$  (24)\n$=\\frac{1}{u-l}  \\int_{l}^{u} N(x_t; ax_0, \\sigma^2) dx_0 = \\frac{1}{u-l} \\int_{al}^{au} \\frac{1}{a} N(x_t; y, \\sigma^2) dy$  (25)\n$= \\frac{1}{a(u-l)} [\\Phi(\\frac{au-x_t}{\\sigma}) - \\Phi(\\frac{al-x_t}{\\sigma})]$  (26)\nwhere \u0424(x) = $\\int_{-\\infty}^{x} N(x, 0; 1)dx$ is the cumulative Gaussian. Figure 9 plots this for various \u0101, converging to the standard Gaussian for a \u2192 0.\nNote that this is a difference of two cumulative Gaussians. But generalization to multiple higher-dimensional inequalities is much simpler if this was a product of sigmoid-shaped functions, where each constraint contributes a factor to $p(x_t)$. We note that this difference of cumulative Gaussians can well be approximated as a product,\n$\\Phi(y) - \\Phi(z) \\approx \\Phi(y)\\Phi(-z)$ .  (27)\nFig. 9 (right) displays the diffusion when the exact difference is replaced by this product.\nGeneral inequalities: We can generalize this approximation to arbitrary inequalities in $R^n$. The l, u-bounds above can be rewritten as inequalities $g_1(x) = l - x$ and $g_2(x) = x - u$, respectively. Therefore, they contribute factors of the form \u0424(z) with $z = \\frac{[x - a g_i(x) + (1 - a)x]}{\\sigma}$. Consider\nf(x) = 0 s.t. g(x) \u2264 0,  (28)\nwhere $p(x_0)$ is uniform in the feasible polytope. Using a linear approximation $g_i(x) \\approx \\nabla g_i(x')(x - x') + g_i(x')$ each inequality contributes a factor\n$p_t(x) \\propto \\prod_i \\Phi[-\\frac{ag_i(x) - (1 - a)g_i(x) + x}{\\nabla g_i(x')\\sigma}] \\approx \\prod_i \\Phi[-\\frac{-\\nabla g_i(x')(x - ax') - ag_i(x')}{\\nabla g_i(x')\\sigma}]$  (29)\nNote that for a = 1 the argument is the (gradient-normalized) original inequality, while for a = 0 the offset of the hyperplane is shifted to go through zero - the parameter a therefore interpolates between the sigmoidal function going through the original inequality offset and going through zero.\nWithin application of hit-and-run, we will consider $p(x_t)$ along a random direction d, where $x = x + \\beta d$. In this case, we have\n$p_t(x) \\approx \\prod_i \\Phi[-\\frac{-\\nabla g_i(x')(\\beta d) - ag_i(x')}{\\nabla g_i(x')\\sigma}]$  (30)\n$= \\prod_i \\Phi[\\beta b_i]$ with $s_i = \\frac{-\\nabla g_i(x')d}{\\nabla g_i(x')\\sigma}$, $b_i = s_i \\frac{-\\nabla g_i(x')(x - ax') + ag_i(x')}{\\nabla g_i(x')\\sigma}$  (31)\nGeneral Equality: Consider\nf(x) = 0 s.t. h(x) \u2264 0,  (32)\nwhere $p(x_0)$ is uniform along the feasible manifold h(x) = 0. Eq. (5) described the exact diffusion of a point xo, namely to $x_t = ax_0 + z$ with $z \\sim N(0, \\sigma^2)$, where the mean is interpolated from xo to zero with decreasing a. For the equality case, we consider a local linearization $h(x) = h(x') + J_h(x')(x - x')$ with Jacobian $J_h$, and describe the diffusion of the feasible hyperplane by decomposing along tangential directions (along which the distribution would be uniform), and normal directions.\nLet $P_h = J_h(J_hJ_h)^{-1}J_h$ be the projection into the subspace normal to the hyperplane. The diffusion in this normal direction is generated by $x_t = ax_0 + P_h z$ with $z \\sim N(0, \\sigma^2)$. Given the local linearization around x', we choose to as the nearest projection $x_0 = x' - J_h(J_hJ_h)^{-1}h(x')$"}]}