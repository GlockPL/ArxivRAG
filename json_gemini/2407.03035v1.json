{"title": "NLP Sampling: Combining MCMC and NLP Methods for Diverse Constrained Sampling", "authors": ["Marc Toussaint*", "Cornelius V. Braun*", "Joaquim Ortiz-Haro"], "abstract": "Generating diverse samples under hard constraints is a core challenge in many areas. With this work we aim to provide an integrative view and framework to combine methods from the fields of MCMC, constrained optimization, as well as robotics, and gain insights in their strengths from empirical evaluations. We propose NLP Sampling as a general problem formulation, propose a family of restarting two-phase methods as a framework to integrated methods from across the fields, and evaluate them on analytical and robotic manipulation planning problems. Complementary to this, we provide several conceptual discussions, e.g. on the role of Lagrange parameters, global sampling, and the idea of a Diffused NLP and a corresponding model-based denoising sampler.", "sections": [{"title": "Introduction", "content": "Sampling from a constrained set is a core problem in many areas: In robotics we may want to sample solutions subject to kinematic or physics constraints to tackle sequential manipulation planning. In Monte Carlo methods samples are generally used to estimate expectations. In generative models we generate samples to match a data distribution - analytical hard constraints have received less attention in this context so far.\nIn this work we consider how methods from across these field can be combined to provide strong constrained samplers. When starting this work we were in particularly interested in whether and how diffusion denoising approaches can be leveraged also for model-based hard-constrained sampling of diverse solutions to non-linear mathematical programs. However, our initial approach to this end failed, and Sec. 5 includes a conceptual discussion on this topic and our insights. Many other methodologies from across the fields can fruitfully be integrated in a joint framework, which is the focus of the technical contributions in this study."}, {"title": "Related Work", "content": "Robotic Path Planning. Sampling-based methods have a long tradition in robotics. Prominently, sampling-based motion planning methods aim to find collision-free paths to a given goal under environmental constraints [13]. To this end these motion planners sample the high dimensional configuration space, using different strategies, until a path has been found [11]. While basic sample-based path planning does not consider differentiable feasibility constraints, Manifold RRT methods [18, 12] and other extensions account for such constraints. We will consider Manifold RRT as one candidate for what we will define as interior sampling, and thereby compare it to alternative MCMC methods, including Langevin and Hit-and-Run. However, as RRT methods are targeted to find continuous paths in configuration space, the sampling process expands continuously and cannot jump across infeasible regions to discover\ndisconnected feasible regions. A core focus of our work is to also consider a phase-one sampling method to seed interior sampling within separated modes or disconnected regions of the feasible input space. Further, path planning methods are not directly comparable to MCMC methods, as they do not correctly sample from a given energy. By combining Manifold RRTs with Metropolis-Hasting we will bridge these two approaches.\nRobotic Task-and-Motion Planning. Task-and-Motion Planning (TAMP; see [6] for a comprehensive survey) is a formulation of robotic manipulation planning, where discrete search over action sequences needs to be combined with continuous decisions about action parameters. The discrete action decisions impose feasibility constraints on the continuous decisions; and typical solvers need to generate continuous decisions subject to these constraints, e.g. using predefined samplers [7] or constrained optimization methods [19]. The present work was motivated by the core problem of generating diverse samples of feasible continuous decisions within TAMP, to ensure probabilistic completeness of the overall TAMP solver.\nThe idea of combining constrained optimization and samplers was previously discussed in [5, 15]. In the present work, we fully focus on the sub-problem of constrained sampling, combining the perspectives of MCMC and constrained optimization. The NLP samplers we propose here could be used as streams within PDDL-Stream [7], but also as a keyframe sampler within Logic-Geometric Programming [9, 20].\nMCMC, (Manifold-) Langevin, Polytope Sampling. MCMC methods [1] are designed to sample from a given density p(x); Langevin or Hamiltonian methods in particular can exploit a given score function (i.e. gradients are available). Some extensions of MCMC sampling methods to constrained manifolds exist [3]. However, some issues are insufficiently treated by such samplers: Manifold-MCMC methods typically assume the process is initialized feasible; the problem of finding a feasible initialization is neglected. Related to that, in cases involving disconnected manifolds (where MCMC steps across regions are highly unlikely), coverage and calibrated sampling across these modes is not neglected. Finally, these methods do not consider an inequality bounded feasible region, where we have interior space, in the terminology of constrained optimization. Our methods extend MCMC methods to address these issues.\nPolytope sampling [4] addresses sampling from within a linearly bounded feasible region. This includes approaches that are interestingly different from typical Manifold-MCMC or Manifold-RRT methods, in particular Hit-and-Run (HR) [22]. We will propose a generalization of HR to account for non-linear inequalities (as well as equalities) and non-uniform interior density, and include it in our family of methods as an alternative to Manifold-RRT or -MCMC for interior sampling.\nDiffusion Denoising Models. Probabilistic Diffusion Denoising Models (DDPMs) [10] use Langevin dynamics to generate samples. An important feature of the trained dynamics is that the generative process can cover multiple modes of the underlying density. This property makes it particularly interesting to what we want to achieve: a generative process to diversely sample from potentially disconnected constrained spaces. However, the power of Langevin dynamics to generate multi-modal samples in DDPMs has its origin in training on data that covers all the modes. So, therefore, the data contains the global information about the modes of the underlying distribution, and the trained denoising dynamics can explain this knowledge to learn bifurcating dynamics that enables to sample from all modes. This view is in contrast to our setting, where we only have access to the underlying NLP (constraint and energy functions), being able to evaluate them point-wise, but without global knowledge or data about where modes or local optima might be. In this view, we aim to achieve the same efficiency and global coverage as data-trained DPPM sampling methods, but without availability of global data and based only on point-wise evaluations of the underlying problem. We will integrate Langevin dynamics (as well as Riemannian Langevin) as an option for NLP Sampling and empirically compare it to alternatives. Sec. 5 includes a more extensive discussion of the relations between NLP Sampling and denoising models."}, {"title": "Restarting Two-Phase NLP Samplers", "content": "In the context of constrained optimization, \"Phase I optimization\" refers to the problem of finding a feasible point which can then, in Phase II, be used to seed an interior optimization method. We propose a family of sampling methods that equally adopts this basic two-phase approach. Specifically, a Restarting Two-Phase NLP Sampler iterates the following steps until enough samples are collected:\n(i) Sample a new seed x, potentially conditional to previously found samples D.\n(ii) A slack downhill method tries to find an (approximately) feasible point x within $K_{down}$ steps.\n(iii) If x is feasible, an interior sampling method, potentially requiring $K_{burn}$ burn-in steps, tries to collect $K_{sam}$ samples $\\sim e^{-f(x)}$ within the feasible space. If the method does not respect feasibility exactly, it is combined with a direct slack reduction (manifold projection) step.\nAlg. 1 provides more explicit pseudocode, where s(x) is a slack vector of constraint violation, defined in detail below. While straight-forward, we are not aware of sampling techniques to adopt this basic two-phase approach. We propose this family as a generic framework to integrate methods from across the fields of MCMC sampling, constrained optimization, and robotics. The following sections discuss alternatives for slack downhill and interior sampling within this framework."}, {"title": "Downhill, Noise, & Step Rejection on the Relaxed NLP", "content": "We first describe a class of methods that can be used for either slack downhill or interior sampling as combining a gradient-based downhill step with (optional) noise and an (optional) step rejection mechanism. This includes Langevin variants as well as classical constrained optimization techniques.\nTo introduce this class of methods we define the underlying energy as follows: Given constraint functions $g:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m,h:\\mathbb{R}^n \\rightarrow \\mathbb{R}^{m'}$, we define $s(x) = ([g(x)]_+,|h(x)|) \\in \\mathbb{R}^{m+m'}$ as the slack vector, where the ReLU []+ and absolute value $|\\cdot|$ are applied element-wise. This is the stacked vector of all inequality and equality violations. The total violation is given as $1^\\top s(x)$, which is also the $L_1$-slack penalty. However, in this work we focus on the $L_2$ slack penalty $s(x)^\\top s(x)$ to exploit its Gauss-Newton steps. Given an interior energy $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$, we define the relaxed NLP as\n$\\displaystyle F_{\\gamma\\mu}(x) = \\gamma f(x) + \\mu s(x)^\\top s(x) .$\nWhile introducing two scalings $\\gamma$ and $\\mu$ is an unusual convention, we want to explicitly include $\\gamma = 0$ to describe pure slack downhill (Phase I) as $F_{0\\mu}$, and interior sampling as $F_{\\gamma\\mu}$ for large $\\mu \\gg 1$. This relaxed NLP also implies the relaxed sampling problem $p_{\\gamma\\mu}(x) \\propto \\exp\\{-F_{\\gamma\\mu}(x)\\}$, so that we can consider various MCMC methods for both phases, slack downhill ($p_{0\\mu}$) and interior MCMC ($P_{\\gamma\\mu}$).\nFor a given an energy $F(x)$, Appendix A recaps the definitions of Langevin steps, Riemannian Langevin, Metropolis-Hasting (MH), Metropolis-Adjusted Langevin (MALA), Riemannian MALA, Gauss-Newton steps, and the Armijo rule of backtracking line search. We decided to move this textbook knowledge to the appendix, but want to highlight the strong relations between these methods here, some of which are obvious and well-known, some less. For instance, in the appendix we observe: (1) Discrete time Langevin dynamics (13) is the same as combining plain gradient steps $-\\alpha d$ with isotropic noise $\\sigma z, z \\sim \\mathcal{N}(0, I_n)$, where the noise scaling $\\sigma = \\sqrt{2\\alpha}$ is tied to the step size $\\alpha = \\tau$. (2) Discrete time Riemannian Langevin dynamics (14) [8] is the same as combining Newton downhill steps $-\\alpha H^{-1}g$ with covariant noise $\\sigma \\sqrt{H^{-1}}z, z \\sim \\mathcal{N}(0, I_n)$, where the noise scaling $\\sigma = \\sqrt{2\\alpha}$ is still tied to the step size $\\alpha = \\tau$.\nWhile the first observation is often mentioned, the direct relation between Riemannian Langevin and Newton methods is, in the original paper, not mentioned. Both observations render discrete time Langevin dynamics to be a special case of classical downhill with adding noise, where the noise scaling is tied in a very particular way to the choice of step size. Tying the noise scale to the step size is at the core of (continuous time) Langevin's property to actually generate samples of the underlying distribution which is relevant for interior sampling, but less for downhill.\nDiscrete time Langevin needs to be combined with Metropolis-Hasting (MH, see Eq. 17) to correctly sample from an underlying distribution p(x) - this is termed Metropolis-Adjusted Langevin (MALA). MH essentially rejects non-decreasing steps with some probability. It turns out that there is a strong relation to the Armijo rule used in classical backtracking line search. In the appendix we observe: (3) The acceptance rate (21) of a gradient downhill step $x' = x - \\alpha \\delta$ under Metropolis-Hasting differs from the Armijo rule with line search parameter $\\rho = \\frac{\\sigma^2}{2}$ only in that it uses the more symmetrical gradient estimate $\\delta$ instead of $d$, and accepts with some probability $< 1$ also steps with $F(x') > F(x) - \\frac{\\sigma^2}{2} \\delta^2$.\nIn view of these analogies we think of all mentioned methods as an instance of the family of methods that combines three steps:\n(i) We can use plain gradient or the Newton direction (2 options) to make a downhill step scaled by a;\n(ii) we can add isotropic or covariant or no noise (3 options) scaled by $\\sigma$; where $\\sigma$ might be tied to a for Langevin variants;\n(iii) and we can reject the overall step based on the Armijo rule or Metropolis-Hasting or always accept (3 options).\nAny of these methods can be applied on $F_{0\\mu}$ for slack downhill, or $F_{\\gamma\\mu}$ for interior sampling. Our evaluations will systematically compare them in both roles. To precise we add that, in our applications, we always approximate Newton steps for $s(x)^\\top s(x)$ as Gauss-Newton, i.e.:\n$\\displaystyle (\\nabla^2F_{\\gamma\\mu}(x) + \\lambda I)^{-1}\\nabla F_{\\gamma\\mu}(x) = (\\gamma\\nabla^2 f(x) + 2\\mu JJ^\\top_s + \\lambda I)^{-1}(\\gamma\\nabla f(x) + 2\\mu J_s s),$\nwhere all RHS terms (s and $J_s$) are evaluated at x and $\\lambda$ is a Levenberg-Marquardt damping (or Tikhonov regularization) parameter. Further, all stepping methods are subject to box bounds l, u \u2208 Rn and a potential maximum step length $\\alpha_{max}$.\nIt might surprise that Lagrange methods play no role in what we propose for NLP Sampling. Sec. 5.1 discusses this in more depth."}, {"title": "Interior Sampling with Explicit Constrained Handling", "content": "The previous section described methods that operate on a differentiable energy F(x) - akin to unconstrained optimization. In contrast, in this section we consider sampling methods that deal with constraints\nmore explicitly. We first propose an extension of Hit-and-Run - an interesting approach to deal with inequalities - to become applicable to our general NLP Sampling problem, and then discuss non-linear manifold RRT methods - an interesting approach to deal with equalities."}, {"title": "Non-linear Metropolis-Adjusted Hit-and-Run (NHR)", "content": "A set of linear inequalities $Gx+\\hat{g} \\leq 0$ defines a polytope. Several approaches to sampling uniformly from a polytope are well-established [4] and can easily be adjusted using MH to also account for an interior target density $x \\propto e^{-f(x)}$. A standard family of approaches are Ball and Dikin Walks, which use step proposals sampled uniformly from a ball or ellipsoidal and reject them when outside the polytope [4]. Dikin Walks are very similar to Riemannian Langevin on $F_{\\gamma\\mu}$ for $\\mu\\rightarrow\\infty$.\nHowever, in this section we are particularly interested in another approach as it is quite different to typical MCMC walks with local proposals: Hit-and-Run (HR) [22]. HR samples a random direction in $\\mathbb{R}^n$ (uniformly), computes where this line intersects the inequalities, and then samples uniformly from the interior line segment. Alg. 2 defines standard HR.\nWe extend HR to become applicable to non-linear inequalities, as well as MH-adjust it to non-uniform interior $p(x) \\propto e^{-f(x)}$. Alg. 3 presents the novel method. All the clip operations use line 5 of the original Hit-and-Run for some $\\hat{g}$ and a. The inner loop (line 6) samples a candidate y on the line, evaluates the inequalities g(y), and if they are violated clips the interval $[\\beta_{lo}, \\beta_{up}]$ with only the violated inequalities using the linearization at y. Note that the initial clipping (line 5) with the linearization at x is not necessary, which leads to more exploration."}, {"title": "Non-linear Manifold Rapidly-Exploring Random Tree (mRRT)", "content": "RRTS generate samples by gradually growing from an initial sample (start configuration) towards uniformly covering the sample's connected component [13]. The incremental nature (within a connected component) is analogous to MCMC walks, but the fact that it generates a tree instead of a chain, i.e., grows \"at all leafs\" makes it explore fast and an interesting candidate for sampling in general, regardless that it was developed to return connected paths.\nRRTs have been generalized to grow on non-linear manifolds [18]. All these approaches require an initial sample on the manifold to grow from, which our Phase I step can provide. For completeness, we include our specific implementation of manifold RRT in Alg. 4. In this algorithm, growth steps are projects to be tangential to the equality constraints. A following SlackReduce step has to correct for non-linearities in the constraints, but also corrects for inequality violation by projecting to the nearest feasible point."}, {"title": "Conditional Restart Seeding", "content": "We discussed various alternatives for the slack downhill and interior sampling phases. (For slack reduce we will always use a Gauss-Newton slack step with full step length $\\alpha = 1$.) The seeding of restarts conditional to previously found samples D is yet open.\nWe consider three options, where the last is, to our knowledge, novel:\n(i) Sample $x \\sim \\mathcal{U}[l, u]$ independent of previous samples, which makes all episodes i.i.d.\n(ii) Sample a candidates {$x_1, \\dots, x_c$} uniformly and select $\\text{argmax}_i \\min_{y\\in D} |y - x_i|$, which maximizes the nearest distance to the data D.\n(iii) Sample a candidates {$x_1,..,x_c$} uniformly, compute the Gauss-Newton slack step $d_i$ for each, and select $\\text{argmin}_i \\max_{y \\in D} \\frac{(y-x_i)^{\\top}d_i}{|y-x_i| |d_i|}$, which minimizes the alignment of the slack direction with any data direction."}, {"title": "Empirical Evaluations", "content": "The full sources to reproduce all evaluations are available here.\nWe generally found it hard to find a useful metric that reflects a degree of diversity in the sampled dataset D. For a low-dimensional toy problem we can use the Earth Mover Distance between the samples and ground truth samples (e.g. generated using naive rejection sampling) to check the \"correctness\" of sampling. However, in the problems of interest to us we care less about correctness after asymptotic mixing, but rather the rate of diversity and mode coverage we get with each and early samples.\nWe first considered metrics that analyze the neighborhood of samples, e.g. the spectrum of distances to the ten nearest neighbors. However, such metrics are fully ignorant on whether different modes are populated. Nearest neighbor-based metrics can capture uniformity or how locally equidistant samples are drawn, but not coverage of modes.\nWe found the size of the minimum spanning tree of all points in D an interesting quantity that does capture also distances between covered modes, as at least one edge needs to connected distant covered modes. Specifically, we define the Minimum Spanning Tree Score $MSTSp(D)$ as the total cost of the tree when using edge costs $|x - x'|^p$.\nWhen edge costs are plain Euclidean distances, p = 1, it is guaranteed that $MSTS_1(D)$ strictly increases with n = |D| when adding points. Interestingly, the rate of increase seems to give an indication of the dimensionality of the sampled space: E.g., when grid-sampling n points from a box of dimension d we expect an $MSTS_1(D)$ of roughly $n^{1-\\frac{1}{d}}$ (the number of lines of length 1 along one dimension of the grid). Our experiments suggest that a similar behavior is true for uniformly sampled points.\nWith edge costs $|x - x'|^p$ for p > 1, the $MSTSp(D)$ does not monotonously increase with |D| when adding points. Instead, with increasing |D| modes become more and more densely populated, with neighbor distances going to zero and contributing less to MSTS. However, distances between separated modes stay greater than zero and add up. Therefore, $MSTS_p(D)$ will (for large |D|) sum over mode separations and become a metric of separateness of modes.\nUsing restarts and interior sampling, our methods generate the sample dataset $D_n = \\{x_i\\}_{i=1}^n$ sequentially. Given the sequence $x_i, i = 0, .., n$, we compute $MSTSp(D_n)$ for each data subset $D_n$ so that we can plot the score as a function of n and the number of evaluations needed to generate n samples."}, {"title": "Illustrations", "content": "We start with illustrations of the methods' behavior, which is best seen in scatter plots of samples. We first consider the uniform box problem in 2D, with 4 inequalities $-(x + 1) \\leq 0, x - 1 < 0$ (elem-wise), flat energy f(x) = 0, and bounds l = (-2, \u22122), u = (2, 2) (which are known and used for seed sampling). Fig. 1 shows how pure downhill generates too many samples at the boundary; and that overstepping Gauss-Newton (GN) downhill steps can help to jump into the interior rather than staying at the border. The third and fourth plots illustrate how efficiently NHR mixes within the box with just $K_{burn} = 1$ or $K_{burn} = 2$ burn-in steps.\nTo check correctness in handling non-flat energies we added $f = 4(x - 1)^2$ to the 2D box problem, which is a Gaussian around 1, but clipped by the inequalities. Fig. 2 shows the results when using NHR, plain MCMC, or Langevin as interior sampling method, and $K_{burn} \\in \\{2,10,50\\}$ burn-in steps. The Earth Mover Distance confirms convergence to the true distribution for all three methods.\nTo illustrate mode exploration and the $MSTSp$ score, we define the modes problem in n dimensions as having $1 + 2n$ centers $c_{0,..,2n} \\in \\mathbb{R}^n$ and radii $r_{0,..,2n} \\in \\mathbb{R}$. Together they define a highly non-linear single inequality function\n$\\displaystyle g(x) = \\min_{i\\in \\{0,..,2^n\\}} \\frac{(x-c_i)^2}{r_i^2} - 1,$\nwhich describes a feasible space \\{x : g(x) < 0\\} which is composed of balls around each $c_i$ with radius $r_i$. We fixed $c_0 = 0$ to be at the origin, and each $c_i, i = 1, .., 2n$ to be at the corner $(\\pm 1, \\pm 1, .., \\pm 1)$ with signs the binary code of i. We further fixed the center mode to have radius $r_0 = 0.5$, and all other modes $r_i = 0.1$. The range bounds are l = \u22121.2, u = 1.2.\nThe two leftmost columns in Fig. 3 show results on the 2D modes problem, for pure downhill and downhill as well as $K_{burn} = 5$ NHR steps. The $MSTSp$ plots show that $MSTS_1$ continuously increases (more so when using NHR), while $MSTS_2$ converges to a constant, indicating total mode separateness. The two columns on the right show results for the 6D modes problem, where we have $1 + 2^6 = 65$ modes. The same method (using NHR with $K_{burn} = 5$) here struggles to discover modes; the occasional steps in the $MSTS_2$ score clearly indicate newly found modes. However, when using dist100 restart seeding modes are"}, {"title": "Benchmarks Problems", "content": "We evaluate the methods on 5 analytic problems, and 3 robotic problems. The 5 analytic problems are the uniform box in 2D and 6D, the \"modes problem\" in 2D and 6D, and random linear programs (LPs) in 2D. For the random LPs, we have a linear inequality $g(x) = Gx - 0.2$ with $G \\in \\mathbb{R}^{5n\\times n}$, and we sample G normal Gaussian.\nConcerning the robotics problems, we considered a standard Inverse Kinematics (IK) problem with a 7-dof Franka arm, there the goal is to reach reference point (yellow in Fig. 5a) with the endeffector; then a similar reaching problem but with a massive cylindrical obstacle (defining distance inequalities to all robot collision shapes) which can be circumvented to the left or right. For both of these problems, the feasible space is \"large\" in the sense that it contains many (also unintuitive) feasible configurations, but is dominated by the highly non-linear equality constraint and the many inequalities for the cylinder problem. The third robotics problem is a sequential manipulation planning problem where we jointly solve for 4 consecutive configurations, which are the start-of-box-push, the end-of-box-push, the reach-stick, and the touch-target-with-stick configurations. The story is that the target can only be reached with a stick, which is initially behind a box that first needs to be pushed away. Fig. 5c) illustrates a solution by overlaying all 4 configurations - this looks cluttered but is actually a very clean and nice solution, where the box is properly pushed aside to make space for the stick reach. The problem is 49-dimensional (larger than 4.7, as many additional parameters such as box placement, relative push pose, and stick grasp are co-optimized decision variables), and highly non-linear in all equalities and inequalities. The box pushing is modeled as stable push4. The feasible space is again \"large\", e.g., where the box is placed is fully open, but constrained in a complex manner. Pure downhill gets often stuck in infeasible local slack minima"}, {"title": "Results", "content": "Fig. 6 displays results from running over 172800 experiments, which cover 8 problems, 1584 tested method combinations, and 10 runs for each of these combinations. The methods combine a downhill method (GN, grad), downhill noise method (none, iso, cov), downhill reject method (none, Wolfe, MH), interior sampling method (NHR, MRRT, MCMC, Langevin), interior sampling burn in steps (0, 5, 20), interior samples taken (1, 5, 20), and the restart seed method (uni, nov, dist). Some nonsensical combinations are excluded, such as Metropolis-Hasting without noise, or various interior sampling methods while only taking a single sample. We put a limit $K_{down} = 50$ on downhill steps throughout, except for the push problem where we choose $K_{down} = 200$.\nFig. 6 uses two performance metrics (MSTS1-per-evals and samples-per-evals) to position all methods in a 2D performance space, where the performance metrics were computed after a full run (after either 1000 samples are collected or 100000 evaluations were exceeded). Here 'samples' means the number of returned (feasible) samples, and samples-per-evals is a number in [0,1]. These performance plots show better methods towards the top right. The plots show that groups of methods (with same interior sampling method) seem to appear on lines in this 2D performance plot. Note that the slope of these lines relates to MSTS1-per-sample, which relates to the diversity of returned samples. We therefore added isolines for this third performance indicator; where better method appear at higher \"polar angle\". The tiny black flags indicate the method details for each performance cross.\nWe see interesting trends in these full results. However, as plots are rather cluttered, we provide Fig. 7 to aggregate these results into a series of head-to-head method comparisons. This will allows us to sub-select promising methods to inspect and discuss their performance plots in more detail below. We summarize our findings in Fig. 6 and Fig. 7 as follows:\n\u2022 A clear finding is that burn-in steps for interior sampling do not pay off in these metrics - which makes sense as our metrics are relative to # evaluations needed to generate the samples. (But certain application scenarios might justify burn-in steps to warrant sample quality.)\n\u2022 We also observe that Gauss-Newton downhill steps generally outperform plain gradient steps in Phase I. This performance difference is exacerbated on the robotics problems.\n\u2022 Using downhill reject methods (Wolfe, MH) are regularly among the best methods. However, the difference is small. We explain this due to the important role of restarting: Single-run non-linear optimization and MCMC methods require Wolfe and MH to guarantee monotone convergence or probabilistic correctness. However, our samplers, if they happen to be stuck, will be restarted and the results indicate that aggressive downhill eventually pays off in terms of diversity and samples per evaluations.\n\u2022 Injecting noise during downhill does sometimes lead to good methods, but no noise is generally very competitive. In particular, we observe that injecting covariant noise tends to perform worse than isometric noise.\nBased on the above observations from Fig. 7, we reduce clutter in the plots by excluding method combinations accordingly. Fig. 8 provides the performance plots of the remaining methods for six problems. Our findings are:\n\u2022 Focusing on the color coding first, we find that NHR (green) interior sampling performs particularly well on linear programs (with high variance due to the randomized problems), including the box problem, but also the modes problem. In turn, on the robotics problems (dominated by hard equality constraints instead), mRRT (red) shows its strength. Esp. on these problems plain MCMC (blue) and Langevin (violet) are not efficient, as they are not manifold constrained.\n\u2022 Looking into the restart seeding (annotated with uniform, distance-based, and novelty-based), we find that esp. for the modes problem, diverse seeding can increase MSTS1-per-eval. However, especially for the robotics problems, diverse seeding seems to be clearly harmful! This came to a surprise, and drastically changed our conception of the structure of these problems: Apparently, seeding away from previously found feasible samples is harmful. This suggests that the feasible space is actually quite local in regard to the high-dimensional embedding space (49-dimensional, in the push example), but at the same time highly multi-modal (esp. for the cylinder and push problem) and non-linear.\nConcerning the alignment-minimizing seeding, the method leads to good diversity-per-sample (the polar coordinate) in the box, modes, and LP problems. But when evaluating the performance per evaluations the method is far behind plain uniform and distance-based reseeding: Clearly the evaluations needed to evaluate alignment of seed candidates are too wasteful in these scenarios.\n\u2022 Concerning the number $K_{sam}$ of interior sampling steps the results clearly indicate that using more interior sampling steps is more evaluation-efficient - but only for the right choice of interior sampler (NHR for the linear and modes problems; mRRT for the robotics problems). However, one should note that especially in the early phase one can see that this comes with less diversity-per-sample. E.g., the 6D modes problem profits from more frequent restarts (esp. with dist100 seeding).\n\u2022 Finally, Fig. 8 confirms that injection of isometric noise during downhill is among the good methods, but the same method setting without injecting noise is better. Therefore, concerning downhill, in our evaluations aggressive GN slack steps without further noise or rejection (Wolfe or MH) mechanism performs well throughout the problems."}, {"title": "Conceptual Discussions", "content": "For zero interior energy, f(x) = 0, any feasible point is a solution to the NLP with zero dual variable. As is obvious from the 1st KKT condition, the role of Lagrange parameters is to counteract cost gradients -Vf(x) pulling into the infeasible region. Therefore, without costs, no Lagrange parameters. In other terms, if we only care about sampling feasible points (without energy), we can discard the whole idea of Lagrange parameters in the first place.\nFor non-zero costs, Lagrange parameters establish stationarity for a single optimum point located exactly on active constraints. This bears the question whether in NLP Sampling Lagrange parameters can equally play a role when sampling points, esp. points with active constraints. However, in view of our concrete proposed family of NLP Samplers, it seems the answer is negative. For instance, consider NHR sampling: The only difference between flat energy f(x) = 0 and non-zeros costs is the use of Metropolis-Hasting to calibrate the energies before and after a proposed step. At no point this method aims to sample exactly on active constraints. This argument seems to hold for any methods that truly samples in the interior, which leads to the question whether manifold sampling strategies could benefit from estimating Lagrange parameters. Such methods typically aim to move tangentially on the manifold and Lagrange parameters (e.g. added to the score of a Langevin method) could be used to steer gradient-based MCMC methods towards tangential steps. However, we can enforce tangential steps also much more explicitly using the constraint Jacobian (as mRRT does). Therefore, also for manifold samplers the potential role of estimated Lagrange parameters seems unclear. More fundamentally, also for manifold sampling we never care about a single point of stationarity, but rather about continuously making steps tangentially to the manifold.\nTherefore, in view of the broad range of approaches considered in this paper, we currently do not see a role for Lagrange methods in NLP sampling."}, {"title": "Relaxed vs. Diffused NLP", "content": "We defined the relaxed NLP as $F_{\\gamma\\mu}(x) = \\gamma f(x) + \\mu s(x)^\\top s(x)$, mixing square slack penalties with the energy. In contrast, our initial interested in this line of work was with the idea of a diffused NLP: Let $p(x_o) \\propto e^{-f(x_0)} I_{g(x_0)\\leq 0} I_{h(x_0)=0}$ be our target distribution, where we renamed x by $x_o$. A diffusion model describes the joint distribution $p(x_{0:T})$ as a Markov chain, where $p(x_o)$ is our target distribution, and each transition $p(x_{t+1}|x_t) = \\mathcal{N}(x_t; \\sqrt{a_t}x_{t-1}, (1 - a_t)I)$ mixes Gaussian noise [10]. We have\n$\\displaystyle p(x_t|x_o) = \\mathcal{N}(x_t; \\sqrt{\\bar{a}_t}x_o, (1-\\bar{a}_t)I), \\qquad \\bar{a}_t = \\prod_{s=1}^t a_s,$\n$\\displaystyle p(x_t) = \\int p(x_t|x_o) p(x_o) dx_o$\nWe call $p(x_t)$ the diffused NLP. Based on this, there are two interesting questions: (1) Can we analytically describe or approximate $p(x_t)$? And (2), can we design an efficient NLP Sampler that follows the typical denoising process, starting to sample from $p(x_T)$ and denoising step-wise until we have a sample from $p(x_o)$?\nConcerning (2), we failed to design an efficient NLP sampler following these lines of thoughts. The Appendix B sketches what we considered, but these approaches were orders of magnitude less efficient that the rather straight-forward two-phase methods proposed above. Especially, the aggressiveness and efficiency of GN steps in highly non-linear cases seemed to be lacking in our attempts.\nNevertheless, concerning (1), we can approximate the Diffused NLP $p(x_t)$ using local observations of f, g, h and their gradients. Note that using linearizations of f, g, h essentially means considering the local"}, {"title": "Globally Estimating the Partition Function", "content": "Consider a target distribution with two distant modes, e.g. with energy $F(x) = \\min\\{(x-x_1)^2/2+\\alpha, (x-x_2)^2/2 + \\beta\\}$ in 1D, where the mode at $x_1$ has base-energy $\\alpha$, and the mode at $x_2$ has base-energy $\\beta$. In physics, these two modes could be macro states with different energy levels $\\alpha$ and $\\beta$. The energy levels determine how many samples (micro states) populate each mode. In our example (if $x_1$ and $x_2$ are distant) a fraction $\\omega_1 = e^{-\\alpha} \\sqrt{2\\pi}/Z$ of samples will be in the first mode, and $\\omega_2 = e^{-\\beta} \\sqrt{2\\pi}/Z$ will be in the second mode.5 The point here is that only observing a local energy value F($x_1$) = $\\alpha$ (or its gradient) provides no information on how much the mode is populated. Only the partition function Z tells which part of the samples really populate a mode, namely $e^{-\\alpha} \\sqrt{2\\pi}/Z$ in our case. In this view, discussing the partition function in sampling is analogous to the discussion of local vs. global optimization.\nOur methods aim to efficiently discover modes, but do not correctly calibrate their weighting. That is, the \"base of attraction\" decides on the population of a mode, rather than its energy level relative to other modes. We want to note that within each mode, interior sampling methods ensure correct calibration of sample probabilities, but MCMC methods (with essentially zero mixing probability between modes) are no solution to calibrating across modes. If samples can be clearly clustered into modes, one approach to calibrating across modes could be to, in retrospect, re-weight samples based on their energies and density.\nHowever, estimating the global partition function is beyond only calibrating across discovered modes: It implies estimating the total probability mass of modes we have not yet discovered at all. How to estimate this on the basis of our seeded restarting methods is an interesting challenge. It seems that global assumptions about the underlying functions (as made, e.g., in Bayesian optimization or other global optimization approaches) are necessary for an estimation, which is beyond the scope of this discussion. However, the above discussion helps to understand the gravity of globally correct sampling, which relates to our next discussion."}, {"title": "Diffusion Denoising Models vs. NLP Sampling", "content": "An initial motivation for this whole study was the impressive global coverage and diversity of samples generated by diffusion models such as DDPMs [10]. The generative process, Langevin dynamics guided by a model trained to predict the noise vector $\\epsilon$ from the original sample $x_o$, has amazing properties in\nand $\\omega_1 = \\sigma(\\beta - \\alpha)$, $\\omega_2 = \\sigma(\\alpha - \\beta)$.\nview of how it transports the seed density $p(x_T) = \\mathcal{N}(x_T|0, I)$ to the target density p(xo). In particular, it correctly transports to all modes of the target density, with correct relative calibration. The intuitive picture behind the \"Earth Mover Distance\" works well here: Diffusion Denoising takes the seed density $p(x_T) = \\mathcal{N}(x_T |0,1)$, splits its mass apart and moves it across the landscape, potentially uphill or across local dips of the target score of p(xo), so that eventually all probability mass is correctly moved to the final modes of the target density. How is this globally perfect transport of probability mass possible with DDPMs, when in the view of the previous discussion - it seems excruciatingly hard within NLP Sampling?\nThe answer is data. DDPMs are trained on samples from the target density, which are (assumed) a correct global representation of all modes and their relative calibration. DDPMs do not have to discover modes; they are trained to reproduce, with exact same weighting, the modes in the data.\nIn contrast, in the NLP Sampling problem formulation we only have access to point-wise evaluations of f, g, h with gradients - no global information is available at all.\nIn this view, NLP Sampling and learning generative models from data are very different. However, both are generative processes, and we established the close relations between the underlying methods. A core scientific question is if there is a possibility to design NLP samplers that realize a global transport similar to trained denoising models a global transport from an initial seed density to the target density, covering all modes in a calibrated manner.\nOur initial attempts to use the Diffused NLP to this end obviously (in retrospect) failed, as the Diffused NLP - when approximated from local evaluations of f, g, h with gradients - contains no global information at all, and denoising on the Diffused NLP is as locally stuck as any other local MCMC or gradient based or SQP method. Based on such local information, transport through local dips, global calibration, or even partition function estimation is out of reach. The family of methods studied here shows that - for a while - we gave up on the idea of realizing similar global sampling processes.\nHowever, a promising avenue now seems to combine \"data-free\" sampling methods as proposed here with generative model learning. Training denoising models from samples and modes we have already discovered seems straight-forward, as demonstrated in existing work [21, 17]. An interesting question seems whether online trained denoising methods could also directly be leveraged and integrated within restarting two-phase samplers, rather than just alternating between them."}, {"title": "Sequential vs. Coordinated Sampling (e.g. Stein-Variational)", "content": "Our restarting two-phase methods generate samples sequentially: Each sample $x_i$ is generated by a run (involving downhill and interior sampling) that may be influenced by previous samples D. This is in contrast to parallel runs, where multiple particles descend in parallel to become samples of p(x), as in Stein-Variational Gradient Descent [14].\nOn a conceptual level, it is interesting to compare the amount of information available for informed sample generation in a sequential vs. coordinated approach: In a sequential approach later runs are informed of actually feasible previous samples, whereas in parallel runs all particles inform each other but have no information yet about truly feasible regions. In both approaches, samples can \"keep kernel distance\", but in the parallel approach much more symmetrically.\nWhile SVGD is guaranteed to eventually sample from p(x), naively applying it on p(x) (with zero value and score in the infeasible region) is prone to fail. However, there is a rich space of promising ideas to combine SVGD approaches with our framework: SVGD could be applied only for interior sampling and initialized with results from pure downhill runs. Or SVGD could be extended to include a downhill phase itself, where it is first applied on $F_{0\\mu}$ and then on $F_{\\gamma\\mu}$. Finally, the combination of both ideas is interesting: batch-wise restarting of parallel runs, where particles are not only informed of each other but also the feasible samples found in the previous batch. This was also demonstrated in previous work on"}, {"title": "Summary", "content": "The aim of this study is to better understand approaches to efficiently generate diverse solutions to constrained problems. The fields of MCMC, constrained optimization, as well as robotics can all contribute ideas to this challenge. With this work we aimed to provide an integrative view and framework to combine methods from these fields, and gain insights in their strengths from the empirical evaluations.\nOn the technical side, our contributions are:\n\u2022 We first proposed the NLP Sampling problem formulation, which is nothing but constrained sampling but under the typical assumptions of NLPs: the cost/energy and constraint functions can be accessed point-wise in a differentiable manner.\n\u2022 We proposed an integrative framework, restarting two-phase NLP samplers, to combine methodologies flexibly.\n\u2022 We proposed novel basic MCMC samplers, in particular Non-linear Metropolis-Adjusted Hit-and-Run (NHR), which extends HR to non-uniform interior energies using Metropolis-Hasting as well as to non-linear inequalities. Using a basic margin approach we can also this can also handle non-linear equalities when used in conjunction with slack reduction in each step.\n\u2022 We proposed the Minimum Spanning Tree Score MSTSp as a novel metric to capture the diversity of generated samples. This metric greatly helped us to navigate through the large space of methods and conversely gain insights in the structure of our evaluation problems.\nBeyond these technical contributions we hope that we also contributed on a conceptual level, starting with highlighting the strong relations, e.g., between Riemannian-Langevin and GN steps, or between Metropolis Hasting and the Armijo rule. Sec. 5 highlights that this study was particularly inspired by the impressive properties of data-trained denoising models, esp. its global transport of probability mass to correctly cover all modes. Our discussion mentions corresponding ideas, but also the negative results we found when trying to realize NLP Sampling by a Langevin process on a (locally approximated) Diffused NLP. Based on these insights we discussed how learning and diffusion models could help to tackle NLP sampling in the future."}, {"title": "Downhill, Langevin, Noise, and Step Rejection", "content": "In this section we consider an energy function F(x). As explained in Sec. 3.1 this may be the pure slack downhill energy F01 or the interior sampling energy F1\u03bc, \u03bc\u00bb 1.\nDownhill Steps & Noise: The first ingredient of downhill methods is a plain gradient step \u2212\u03b1\u2207F(x) or a Gauss-Newton (GN) step \u2212\u03b1(\u22072F(x) + \u03bbI)\u22121\u2207F(x) with GN-approximated Hessian \u22072F(x) and regularization \u03bbI. We generally post-process these steps by clipping with a potential maximal stepsize \u03b1max and accounting for box constraints.\nNoise injection is uncommon in classical optimization, but an interesting candidate to increase robustness to small local optima, plateaus or only piece-wise continuous gradients (as common in robotics applications). Isotropic noise adds Gaussian noise,\nx \u2190 x + \u03c3z, z\u223cN(0, In),\nwith standard deviation \u03c3\u2208 R as parameter. Covariant noise adds Gaussian noise with precision matrix equal to the regularized Hessian H = \u22072F\u03b3\u03bc(x) + \u03bbI,\nx \u2190 x + \u03c3\u221aH\u22121z, z \u223c N(0, In),\nwhere \u221aH\u22121 refers to the Cholesky decomposition of H\u22121.\nThe use of H\u22121 as noise covariance is motivated by interpreting the Hessian as a local metric relative to which the Gauss-Newton step is the covariant gradient. Using covariant noise for slack minimization F\u03bc has the following effects: (1) Directions without slack have small eigenvalue \u03ba + \u03bb and therefore large standard deviation \u221a1/(\u03ba + \u03bb). That is, the noise explores more along directions without slack, and is smaller in directions with slack. (2) In the limit \u00b5 \u2192 \u221e, \u221aH\u22121 has zero eigenvalues along active directions. In this case, the noise become strictly tangential to slack isolines. Therefore, covariant noise realizes many aspects we would intuitively see as beneficial for manifold sampling and exploration along slack isolines.\nRelation to Langevin variants: Given a distribution p(x) = Ze\u2212F(x) with energy F(x) and partition function Z, the score function is neg-energy gradient\ns(x) = \u2207 log p(x) = \u2207 logZe\u2212F(x) = \u2212\u2207F(x),\nwhich is independent of Z. Langevin dynamics in continuous time is described by\nx\u02d9 = \u2212\u2207F(x) + \u221a2\u03be\nwith Brownian motion \u03be. For a discrete time (Euler integration) step, this translates to\nxt+1 = xt \u2212 \u03c4\u2207F(xt) + \u221a\u221a2\u03c4z, z \u223c N(0, 1)\nwhere the variance of the Brownian noise step only increases with \u221a\u03c4. In this view, what is special about Langevin is the relative calibration of stepsize and noise. We summarize this as:\nObservation 1. Discrete time Langevin dynamics (13) combines plain gradient steps \u2013\u03b1d with isotropic noise where the noise scaling \u03c3 = \u221a2\u03b1 is tied to the step size \u03b1 = \u03c4.\nRiemann Manifold Langevin (Girolami et al, 2011) is described by the continuous time dynamics\nx\u02d9 = \u2212G(x)\u22121\u2207F(x) + \u221a2G(x)\u22121\u03be,"}, {"title": "Relation between Relaxed NLP and Diffused NLP", "content": "Sec. 5.2 discusses using the Diffused NLP instead of the Relaxed NLP as a basis for NLP Sampling. The Diffused NLP is the distribution p(xt) that arises from taking our p(x) \u221d exp(\u2212 f(x)) Ig(x)\u22640 Ih(x)=0 and diffusing it following DDPMs [10]. The following derives an approximation of this p(xt).\n1D Interval case: Let's first investigate a 1D interval case, where p(xo), xo \u2208 R is defined via the NLP\nf(x) = 0 s.t. l\u2264 x \u2264u,\nAs t is fixed, we introduce simplifying notation a = \u221aat, \u03c32 = (1 \u2212at)I, so that (5) becomes p(xt|xo) = N(xt; \u03b1xo, \u03c32), and the convolution is\np(xt) = \u222b p(xt|xo) p(xo) dxo = \u222balu p(xt|xo) dxo =\n=\u222balu 1u\u2212l N(xt; \u03b1xo, \u03c32) dxo = 1u\u2212l \u222baluat N(xt;y, \u03c32) dy=\n=\u03b1(u\u2212l) [\u03a6(xut\u2212alu\u03c3) \u2212 \u03a6(xt\u2212all\u03c3)],\nwhere \u03a6(x) = \u222b \u2212\u221e x N(x, 0; 1)dx is the cumulative Gaussian. Figure 9 plots this for various a, converging to the standard Gaussian for a \u2192 0.\nNote that this is a difference of two cumulative Gaussians. But generalization to multiple higher-dimensional inequalities is much simpler if this was a product of sigmoid-shaped functions, where each constraint contributes a factor to p(xt). We note that this difference of cumulative Gaussians can well be approximated as a product,\n\u03a6(y) \u2212 \u03a6(z) \u2248 \u03a6(y)\u03a6(\u2212z).\nGeneral inequalities: We can generalize this approximation to arbitrary inequalities in R. The l, u-bounds above can be rewritten as inequalities g1(x) = l \u2212 x and g2(x) = x \u2212 u, respectively. Therefore, they contribute factors of the form \u03a6(z) with z = \u2212\u03b1gi(x)+(1\u2212\u03b1)xi. Consider\n\u03a6(x) = 0 s.t. g(x) \u2264 0,\nwhere p(xo) is uniform in the feasible polytope. Using a linear approximation gi(x) \u2248 \u2207gi(x\u2032)(x \u2212 x\u2032) + gi(x\u2032) each inequality contributes a factor\npt(x) \u221d \u220fi \u03a6[\u2212\u03b1gi(x\u2032)\u2212(1\u2212\u03b1)gi(x)\u221a\u2016\u2207gi(x)\u2016\u03c3] \u2248 \u220fi \u03a6[\u2212\u2207gi(x\u2032)(x\u2212\u03b1x\u2032)\u2212\u03b1gi(x\u2032)\u221a\u2016\u2207gi(x)\u2016\u03c3]\nNote that for \u03b1 = 1 the argument is the (gradient-normalized) original inequality, while for \u03b1 = 0 the offset of the hyperplane is shifted to go through zero the parameter \u03b1 therefore interpolates between the sigmoidal function going through the original inequality offset and going through zero.\nWithin application of hit-and-run, we will consider p(xt) along a random direction d, where x = x + \u03b2d. In this case, we have\npt(x) \u2248 \u220fi \u03a6[\u2212\u2207gi(x\u2032)(x+\u03b2d\u2212\u03b1x\u2032)\u2212\u03b1gi(x\u2032)\u221a\u2016\u2207gi(x)\u2016\u03c3]\nGeneral Equality: Consider\nf(x) = 0 s.t. h(x) \u2264 0,\nwhere p(xo) is uniform along the feasible manifold h(x) = 0. Eq. (5) described the exact diffusion of a point xo, namely to xt = \u03b1xo + z with z \u223c N(0, \u03c32), where the mean is interpolated from xo to zero with decreasing \u03b1. For the equality case, we consider a local linearization h(x) = h(x\u2032) + Jh(x\u2032)(x \u2212 x\u2032) with Jacobian Jh, and describe the diffusion of the feasible hyperplane by decomposing along tangential directions (along which the distribution would be uniform), and normal directions.\nLet Ph = J(JhJ)\u22121Jh be the projection into the subspace normal to the hyperplane. The diffusion in this normal direction is generated by xt = \u03b1xo + Phz with z \u223c N(0, \u03c32). Given the local linearization around x\u2032, we choose to as the nearest projection xo = x\u2032 \u2212 J(JhJ)\u22121h(x\u2032)"}]}