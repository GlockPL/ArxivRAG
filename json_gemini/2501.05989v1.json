{"title": "ADDRESSING SPEAKER GENDER BIAS IN LARGE SCALE SPEECH TRANSLATION SYSTEMS", "authors": ["Shubham Bansal", "Vikas Joshi", "Harveen Chadha", "Rupeshkumar Mehta", "Jinyu Li"], "abstract": "This study addresses the issue of speaker gender bias in Speech Translation (ST) systems, which can result in offensive and incorrect translations. The masculine bias often found in large-scale ST systems is typically perpetuated through training data derived from Machine Translation (MT) systems. Our approach involves two key steps. First, we employ Large Language Models (LLMs) to rectify translations based on the speaker's gender, in a cost-effective manner. Second, we fine-tune the ST model with the corrected data, enabling the model to generate gender-specific translations directly from audio cues, without the need for explicit gender input. Additionally, we propose a three-mode fine-tuned model for scenarios where the speaker's gender is either predefined or should not be inferred from speech cues. We show absolute 70% improvement in translations for female speakers compared to our baseline and other large-scale ST systems, such as Seamless M4T and Canary on MuST-SHE testset.", "sections": [{"title": "1. INTRODUCTION AND RELATED WORK", "content": "Speech translation (ST) transforms spoken words to text in another language. They have applications in automatic video subtitling, dubbing, and facilitating cross-lingual communication. Traditionally, ST followed a cascaded approach. First, a speech recognizer [1, 2] converted spoken words into text in the source language. Then, a machine translation (MT) model [3, 4] translated this text into the target language, referred to as cascaded ST. Recent advancements introduced direct ST models [5, 6, 7, 8, 9], which directly map speech to translated text. The direct ST models can perform real time speech translation, while the cascaded ST models need to wait for text in the source language to be available before translating it to the target language. The direct ST models are also easier to maintain, as a single model is trained and deployed. The direct ST models have slightly inferior translation quality but recent studies [10] have shown that the gap is reduced and direct ST models outperform cascaded ones on certain automated metrics [11]. Despite their effectiveness, ST models are not immune to biases.\nIn this work, we specifically address gender bias exhibited by direct ST models. Gender bias [12, 13] is the unequal treatment or representation of genders and has also been studied extensively for several NLP and Speech tasks [14, 15, 16, 17, 18]. It can influence how people interact and perceive others, resulting in discrimination and obstacles for minorities. Gender bias can arise in different linguistic levels, such as lexical, morphological and syntactic. For example, lexical bias occurs when gendered words are used inaccurately. In the context of speech translation, such gender bias manifests when the model generates translations dominantly in masculine form. This may result in inaccurate translations, as the system fails to respect the speaker's gender identity. It can also lead to offensive translations [19], perpetuating stereotypes and excluding those who identify as female from benefiting fully from these technologies.\nGlobally, a significant number of languages [20] have grammatical gender, where nouns, pronouns and related words have gender distinctions, such as masculine or feminine forms. English, for instance, employs gender-specific pronouns like \"he\" or \"she,\" while other languages exhibit even more pronounced gender distinctions. Consider the sentence \"I am a teacher\". In Spanish, this simple phrase translated into two distinct forms: \u201cSoy profesor\u201d (for self-identified males) and \u201cSoy profesora\u201d (for self-identified females). The Spanish word for \"teacher\" adapts to the speaker's gender. Similarly, when translating \u201cI go to school\u201d into Hindi, the phrase varies based on speaker gender. Therefore, when translating English speech into gender-specific languages, it's crucial to account for the speaker's gender and select appropriate words that align with the language's gender-specific forms.\nMost machine translation (MT) models default to the masculine form as speaker gender cues are often absent in the English input text. [21, 22] advocated for the refinement of MT models using small, intentionally crafted, gender-balanced datasets when gender cues are discernible in co-referential links, thereby reducing gender bias. Alternatively, [23, 16] proposed to add a gender tag to the input during training and inference, however, it is challenging to identify the text with gender specific forms for large-scale training data. [24] showed promising results with language-specific hand crafted rules to identify such text in Arabic, however, the rules does not generalize to other languages. Most recently, the MuST-SHE WMT23 test results [25] showed that the feminine accuracy were less than 30% for majority of the MT systems. Consequently, even when gender information is available, direct ST models may inadvertently exhibit gender bias as they are trained from translations generated by MT models or automatic alignment techniques [26], which may have gender bias.\nThis work centers on tackling gender bias in English-to-Non English direct ST models by exploiting speech cues [16]. However, it faces a challenge: the need for diverse training data containing accurate gender forms, which remains scarce. There are studies which utilized MuST-C corpus to address gender bias in direct ST models[16, 17], however, the authors reported that masculine bias still existed. Hence, they proposed gender controlled translation that requires prior knowledge of speaker gender but gender accuracy was still reported to be less than 70% for female speakers with a single model. The previous works[16, 17] relied on human translated data such as MuST-C [27] or augmented Librispeech [28]. To the best of our knowledge, gender bias in large scale direct ST models is not studied yet. Our focus lies in reducing gender bias in large-scale direct ST models trained on tens of thousands of hours of data. The key contributions of this work are as discussed below.\nGender de-biasing for large scale training data: Obtaining human-annotated gender-debiased translations for tens of thousands of hours of training data is expensive. We use large language models (LLMs), more specifically GPT-4[29, 30] to generate gender-debiased translations in the ST training data. We adopt techniques such as few-shot prompting [31], chain of thought [32] and batch inferencing, to improve accuracy and reduce cost. Moreover, we do not rely on LLMs for the entire training data, but only a subset of it, and show that weighted fine-tuning with such data is sufficient to adapt a large-scale ST model to generate gender specific translation while not degrading the BLEU [33] score.\nRespecting speaker's gender identity: Gaido et al. [17] emphasized that speech cues are not a generalized solution for all users, such as transgender individuals and children. We agree that perceptual markers are not comprehensive for gender identification and also recognize the importance of situations where gender is indeterminate or preassigned. Our work proposes to adapt the ST model architecture that can generate accurate speaker gender forms from audio inputs in an 'Auto' mode or allow the user to choose the desired speaker gender form in a 'Masculine' or 'Feminine' mode, respecting the diversity of speakers.\nGender representation (GR) loss: We also propose to have an additional GR loss during ST model training and show that having GR loss improves on generating gender specific translations when inferred directly from speech.\nWe experiment with English to Spanish (ES) and Italian (IT) ST model and show that our proposed approach achieves more than 90% average gender translation accuracy on MuST-SHE testset. We also compare state of the art ST models like Seamless M4T [26] and Canary [34] and show superior performance of our models."}, {"title": "2. TYPES OF GENDER BIAS", "content": "Before delving into our methodology, it is crucial to outline the major categories of gender bias (as defined in latest MuST-SHE release), and specify the type of bias we aim to address:\nCategory 1: In instances where the speaker's gender cannot be deduced from the English text (e.g., \"I am a teacher\u201d), but can be inferred from audio cues or explicitly shared by the speaker. Our primary focus in this paper is on addressing bias within this category.\nCategory 2: When the gender of the speaker or another person is evident from the English text context (e.g., \"She is a doctor\"), but is inaccurately translated in language X. Although improving this type of bias is not our primary focus, we want to ensure that our approach does not regress on Category-2 bias.\nCategory 3: In cases where a third person is referenced, and the gender of this third person is unknown from both the text and audio cues (e.g., \"There is a doctor\"), our objective is to preserve the original translation without altering the gender form of the translation."}, {"title": "3. METHOD", "content": "Training data for direct speech translation models is often generated at scale by translating human-labeled speech recognition data using a MT model [35, 7]. This approach is cost-effective compared to obtaining human-labeled translations of speech data. As most MT models exhibit gender bias, the training data for direct ST models also inherits this bias. Hence, we propose to first de-bias a subset of training data efficiently and subsequently fine-tune the ST model to automatically generate gender-specific translations, either based on the audio cues or the gender information provided by the speaker."}, {"title": "3.1. Gender de-biasing for large scale training data", "content": "Fig. 1 illustrates our approach to de-bias large-scale training data. Firstly, the input data undergoes filtering using data selection methods. Subsequently, we reformulate a subset of the data with GPT-4 using appropriate prompt. Finally, we prepare the targets in a suitable form for fine-tuning the ST model. Each of these steps is described in detail in the following sub-sections."}, {"title": "3.1.1. Data selection", "content": "Not all English sentences yield gender-specific translations. For instance, phrases like \"Play a song\" or \"How is the weather\" produce gender-neutral translations. We hypothesize that English first-person pronouns, such as \u201cI\u201d and \u201cmy\u201d, which are self-referential, are more likely to result in gender-specific translations. Consequently, we refine the training data to include only tuples (Audio, English Transcription, X Translation) that contain first-person pronouns in the English transcription, reducing our subset to 19.4% of the original data. Our hypothesis is bolstered by the observation that 97.8% of utterances in the MuST-She dev set, exhibiting Category1 gender bias, contain at least one first-person pronoun in the English transcription. Also, majority of our training data already includes speaker gender labels. If not, we ignore such utterances from our fine-tuning data. Further, we only sample 2 million utterances from this subset for GPT-4 reformulation, ensuring equal gender proportions. We will show that these 2 million utterances are sufficient to adapt the ST model to generate gender-specific translations, thereby significantly reducing the cost of GPT reformulation."}, {"title": "3.1.2. Prompting methodology for GPT-4 reformulation", "content": "Gender systems significantly differ across languages, affecting the processes of gender assignment and agreement for gendered lexical items. Manipulating the gender alternation of the speaker, without affecting the referent, poses a challenge due to the need for language-specific, manually crafted rules. We suggest a universal method that employs few-shot [31] and chain-of-thought [32] prompting techniques to harness the generative capabilities of GPT-4. Recent works [36, 37] have also demonstrated that large language models (LLMs) such as GPT-4 can match the performance of commercial MT systems. Our approach involves instructing GPT-4 to generate both masculine and feminine versions for the speaker in each target language for 2 million utterances. Fig. 2 illustrates the prompt for generating the feminine version.\nOur prompt is designed such that only the gender-marked words or segments for the speaker are rewritten in feminine and masculine forms, leaving the rest of the translation unaffected. It is also important to note that GPT4 may also be prompted to improve the translations [36] along with gendered translation task but not explored in this work. An ablation study concerning the GPT-4 prompt for gender forms is discussed in Section 4.3.2."}, {"title": "3.1.3. Updated ST training targets", "content": "Before reformulation, most translations predominantly use masculine forms, regardless of the speaker's gender. The reformulation process using GPT-4 yields \u201cgender-debiased\" training targets, as depicted in Fig. 1. These updated targets are represented in a Gender-Translation format. For instance, (male, masculine) implies that masculine forms are used in translations for male speakers, and so forth. The pairs (male, masculine) and (female, feminine) represent gender-debiased data, where the targets are adjusted according to the input speaker's gender. Fine-tuning the ST model with this gender-debiased data enables the model to generate gender-specific translations from speech signals. As shown in Fig. 1, we also generate (male, feminine) and (female, masculine) pairs using GPT-4, which may initially seem counter-intuitive. However, these pairs are necessary for situations where the gender is predefined and audio cues are not a generalized solution, such as for transgender individuals, non-binary individuals, and children. In these cases, the model should produce the translation based on these predefined gender inputs, without being influenced by audio cues. Furthermore, utterances that do not contain any first-person pronouns in the English transcription are retained as is and referred to as \u201cgender-neutral\" data. These data also play a role in our training, as discussed in the following sub-section 3.2."}, {"title": "3.2. ST model fine-tuning for gender de-biasing", "content": "When training ST models with large datasets, retraining the model from scratch using small amount of gender-debiased data can be computationally expensive and model may also not generalize well. Instead, we propose a more efficient approach: fine-tuning the pre-trained model, denoted as $ST_{Base}$, with gender-debiased data. To mitigate over-fitting, we fine-tune using a combination of \u201cgender-neutral\" and \u201cgender-debiased\" data. This approach ensures that the BLEU score remains unaffected while enabling the model to learn gender-specific translations. In Section 4.3.1, we delve into the impact of different neutral data-sampling ratios $O_{Neut}$ during fine-tuning. We also propose a three-mode training when the gender is predefined and the modes are:\n\u2022 Masculine mode: In this mode, the ST model exclusively generates translations for speakers in the masculine form.\n\u2022 Feminine mode: Here, the ST model produces translations for the speaker solely in the feminine form.\n\u2022 Auto mode: In Auto mode, the model dynamically adjusts the translation of gender-marked words based on the speaker's gender, implicitly estimated from the speech signal.\nThe model devoid of any modes and fine-tuned directly with the combination of \u201cgender-neutral\u201d and \u201cgender-debiased\u201d data is termed as the 1-mode FT, while the model incorporating three modes to also account for user preference is known as the 3-mode FT. The $ST_{Base}$ model appends < Lang > as start-of-sentence (SOS) token to the output target to denote the target language. To facilitate the training of 3-mode FT model, we append < Lang-Mode > as SOS token to the output targets with Mode representing the training mode and Lang denoting the target language as also shown in Fig. 3. The training data for each of the modes in 3-mode FT is presented in Table 1. Note that all our finetunings uses gender-neutral data to prevent over-fitting or catastrophic forgetting."}, {"title": "3.3. Gender representation loss", "content": "Our ST model adopts the transformer-transducer architecture [1, 7] and consists of three components: an encoder network, a prediction network, and a joint network. The encoder transforms input audio features $x_t$ to generate the hidden representations $h_{enc}$. To encourage our ST model encoder to capture gender information from speech signals, we introduce a gender representation loss, $L_{gr}$, for every utterance, as defined below:\n$L_{gr} = \\sum_{i=1}^{T} CE(o_t, g_u)$\nwhere T is the length of input audio feature sequence, $g_u$ represents the gender label for utterance u and $o_t$ is the gender label predicted at time t for utterance u. $o_t$ is obtained as below:\n$o_t = softmax(W^{out} RELU(W^9h_{enc} + b^9))$\nwhere $W^{out}$, $W^9$, $b^9$ are additional learnable parameters. The combined loss function $L_{comb}$ is a weighted combination of $L_{gr}$ and the transducer loss $L_{trans}$, where \u03b1 controls the weight of the gender representation loss as shown below:\n$L_{comb} = \\alpha L_{gr} + (1 - \\alpha) L_{trans}$"}, {"title": "4. EXPERIMENTAL RESULTS", "content": ""}, {"title": "4.1. Dataset", "content": ""}, {"title": "4.1.1. Training set", "content": "Our research focuses on addressing gender bias in large-scale speech translation (ST) models. Due to the scarcity of such large amount of training datasets in the public domain, we have developed our baseline ST model using an internal corpus. This corpus encompasses roughly 75,000 hours of speech data procured through different sources, which includes approximately 90 million utterances. Within this dataset, where gender labels are available from the source, 68.2% of the utterances are categorized as male and 31.8% as female. The original transcripts are in English, and we employ Microsoft's translation service to render them into two languages: Spanish (ES) and Italian (IT), which serve as our training targets."}, {"title": "4.1.2. Test set", "content": "For evaluation, we rely on the publicly available MuST-SHE set [16] for both ES and IT. For our hyper-parameter tuning, we restrict ourselves to the utterances labeled as \"dev\" in the MuSTC-v1.0-SET column. The evaluation phase is limited to the remaining utterances not labeled as \u201cdev\". Our evaluation metrics include gendered translation accuracy (GTA) and BLEU [38]. To define GTA, we first establish term coverage (as defined in [17]), which represents the proportion of gender-marked words annotated in MuST-SHE that are actually generated by the system. GTA measures the proportion of correct gender realizations among the words on which it is measurable."}, {"title": "4.2. Experiment details", "content": "The ST model follows the streaming transformer transducer architecture with 18 transformer blocks, as described in [7]. The encoder comprises 18 Transformer blocks, each containing 320 hidden nodes, 8 attention heads and 2048 feed-forward nodes. The prediction network employs 2 LSTM layers, with each LSTM layer having 1024 hidden nodes. The joint network is a feed-forward layer with a size of 512 times the vocabulary. We utilize a combined vocabulary from ES and IT, consisting of 8K subword units. The model input consists of 80-dimensional log-Mel filter-bank features with 25ms windows and a 10ms shift. The chunk size of the streaming mask for the Transformer blocks is 25. To guide the model in producing translations for a specific language, we prepend a language-specific SOS token < Lang > at the beginning. In our results, we refer to this model as $ST_{Base}$. It serves as the seed model for our fine-tuning experiments aimed at addressing gender bias."}, {"title": "4.3. Results", "content": "Table 2 presents the results on the MuST-SHE evaluation set for our Baseline ($ST_{Base}$) and the proposed approaches: 1-mode and 3-mode fine-tuning. We report the GTA and BLEU score for both Category 1 and Category 2 sets [16] in the MuST-SHE evaluation set. While our primary focus is on improving Category 1, we also provide accuracy metrics for the Category 2 test to avoid regression. Our proposed models demonstrate significant improvement over the baseline in GTA for Category-1 feminine forms across both Spanish and Italian languages. Specifically, the GTA for EN to ES Category-1 feminine forms increased from 10.12% to 87.05%, with a minimal regression on masculine form. Similarly, the GTA for EN to IT Category-1 feminine forms increased from 8.81% to 84.72%. While the BLEU score remains similar or exhibits minor regressions, there is a significant improvement in BLEU scores for feminine translations as the translations contained the correct gender form. Interestingly, despite not focusing on Category-2 gender bias, we still see modest improvements due to balanced presence of feminine forms during fine-tuning. The 1-mode fine-tuning outperforms the 3-mode fine-tuning in \"Auto\" mode, likely due to the latter's complex training process hindering direct learning from speech cues. For the experiments described above, we set $O_{Neut} \\text{<} 0.2$ and $\\alpha$ = 0.1 and its tuning is discussed in Section 4.3.1. Additionally, it is important to highlight that the term coverage for all our models exceeds 80%, surpassing what has been reported in previous works. The 3-mode model is also designed to generate translations in the selected gendered form, regardless of the input speaker. Consequently, we evaluated the performance of the 3-mode model on the MuST-SHE dataset for Category-1 Masculine and Feminine forms across all speakers, corresponding to the chosen modes. The results are summarized in Table 3. We achieve average GTA of more than 87.5% across all the speakers.\nFinally, we also compare our proposed approach with other large-scale ST models such as Nvidia Canary [34], Meta Seamless M4T [26] and prior works [16, 17], as shown in Table 4. It is important to highlight that the approach by [17] employs an explicit tag to produce gender-specific translations in contrast to the other models listed in Table 4. Furthermore, our comparative analysis is confined to their single-model architecture to guarantee a fair assessment. Notably, our 1-mode FT and 3-mode FT (in \"auto\" mode) models exhibit significantly improved performance on MuST-SHE Category 1 feminine forms improving accessibility for female speakers."}, {"title": "4.3.1. Impact of GR loss and Oneut", "content": "Here, we discuss the hyper-parmeter finetuning of neutral data-sampling ratio $O_{Neut}$. Fig. 4 illustrates the gendered translation accuracy (averaged over masculine and feminine forms) with different values of $O_{Neut}$, both with and without GR loss, on the MuST-SHE dev set as described in Section 4.1.2. As $O_{Neut}$ increases, the accuracy for category-1 feminine forms decreases. However, this reduction in accuracy is accompanied by a reduced risk of overfitting and regression on BLEU scores. Notably, with $O_{Neut}$ < 0.2, we observed a decline in BLEU scores. Further, the addition of GR loss proves more advantageous at higher values of $O_{Neut}$, thus validating our hypothesis that GR loss enhances the encoder's ability to capture gender information effectively. Based on above findings, we set $O_{Neut}$ = 0.2 and \u03b1 = 0.1 for our experiments."}, {"title": "4.3.2. GPT-4 reformulation analysis", "content": "We also assess GPT-4's ability to reformulate gender forms for reference speakers while not altering the gender form for referent. Our evaluation included various prompting strategies: 0-shot, 10-shot (with 10 examples in the prompt), and Chain-of-Thought (providing the reasoning) combined with 10-shot prompting for Spanish (ES) and Italian (IT). The best results are achieved with the 10-shot+CoT approach (Fig. 2) achieving 94% and 93% accuracy on speaker gender forms for ES and IT respectively on the MuST-SHE dev set as described in Section 4.3.1. We also observed that without CoT, GPT-4 sometimes altered the gender forms for referents, potentially impacting the performance of downstream fine-tuned models on MuST-SHE Category 2 test sets."}, {"title": "5. CONCLUSIONS", "content": "Our study presented a novel approach to mitigating speaker gender bias in large-scale speech translation systems, leveraging LLMs for gender-alternative translations. We also proposed three-mode fine-tuning to account for user preference and combined training with gender representation loss. Our methods have demonstrated a significant improvement in translations, particularly for female speakers, marking a substantial advancement over existing systems. Future work may involve exploring bias of other types in large-scale ST systems, reducing bias for non-binary speakers and better fine-tuning approaches."}]}