{"title": "FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for Enabling Fair LLM-Based Recommender Systems", "authors": ["Arya Fayyazi", "Mehdi Kamal", "Massoud Pedram"], "abstract": "We propose FACTER, a fairness-aware framework\nfor LLM-based recommendation systems that inte-\ngrates conformal prediction with dynamic prompt en-\ngineering. By introducing an adaptive semantic vari-\nance threshold and a violation-triggered mechanism,\nFACTER automatically tightens fairness constraints\nwhenever biased patterns emerge. We further develop\nan adversarial prompt generator that leverages his-\ntorical violations to reduce repeated demographic bi-\nases without retraining the LLM. Empirical results\non MovieLens and Amazon show that FACTER sub-\nstantially reduces fairness violations (up to 95.5%)\nwhile maintaining strong recommendation accuracy,\nrevealing semantic variance as a potent proxy of bias.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have significantly ad-\nvanced natural language processing (NLP), demon-\nstrating robust generative capabilities across tasks\nincluding summarization, dialogue, code comple-\ntion, and creative composition. Representa-\ntive models such as GPT-3 (Brown et al., 2020),\nBERT (Devlin et al., 2019), Llama-2 Touvron et al.\n(2023), Llama-3 Dubey et al. (2024), and Mistral-7B\n(Jiang et al., 2023) leverage massive corpora and com-\nplex architectures to produce remarkably fluent text,\noften approaching or matching human performance\nin various linguistic benchmarks. Yet a growing body\nof work reveals that these models can inadvertently\nperpetuate or even amplify biases related to sensitive\nattributes such as race, gender, or age (Sheng et al.,\n2019; Blodgett et al., 2020; Bary et al., 2021). Gener-\native disparities become especially concerning when\nthe outputs influence high-stakes domains like hiring,\nfinancial services, or personal recommendations.\nWhile bias and fairness have been extensively stud-\nied in classification tasks such as sentiment analysis or\ntoxicity detection (Zhao et al., 2018; Sun et al., 2019;\nWang et al., 2022), generative models pose unique\nchallenges. Instead of assigning a label, the model\nproduces an open-ended text response, introducing\nmore subtle pathways for biased language to surface\n(Dinan et al., 2020; Lucy & Bamman, 2021). For in-\nstance, if two prompts differ only in sensitive at-\ntributes (e.g., \"male teacher\" vs. \"female teacher\"),\nthe model may produce not only different content but\nalso exhibit divergences in sentiment, style, or level of\ndetail (Sheng et al., 2019). Such disparities may be\npartially hidden by stochastic decoding (temperature\nor top-p sampling), complicating efforts to diagnose\nand mitigate them.\nMany prior bias-mitigation techniques rely on mod-\nifying model internals via adversarial training or\nreparameterization (Madras et al., 2019; Zhao et al.,\n2018). However, modern LLMs are frequently de-\nployed as black-box APIs (e.g., OpenAI or Hug-"}, {"title": "Preliminaries", "content": "In this section, we provide background on fairness\nand bias for LLM-based recommendations (\u00a72.1) and\npresent our minimal-attribute-change definition of\nfairness (\u00a72.2)."}, {"title": "Related Work", "content": "Fairness & Bias in LLM-Based Recommen-\ndations. LLMs increasingly serve as zero-shot rec-\nommenders (Hou et al., 2024; Zhang et al., 2023),\ngenerating item suggestions without explicit fine-\ntuning. Despite their versatility, large-scale pre-\ntraining can encode biases that exacerbate demo-\ngraphic disparities (Bender et al., 2021). For exam-\nple, small changes in sensitive attributes (for example,\nsex or age) can produce disproportionately different\nresults (Zhang et al., 2023). Recent efforts employ\npost hoc techniques such as semantic checks in the em-\nbedding space (Lucy & Bamman, 2021) and prompt-\nlevel interventions (Che et al., 2023), yet deciding a\nfair threshold for \"excessive\" disparity remains chal-\nlenging. Conformal or otherwise statistical methods\nthus offer a data-driven way to calibrate acceptable\nvariations, providing principled fairness guarantees\nbeyond subjective judgments.\nInstruction Tuning & RLHF. Instruction tun-\ning and RLHF (Ouyang et al., 2022; Bai et al., 2022)\naim to mitigate harmful behaviors by incorporat-\ning human-generated feedback signals (rewards) into\ntraining. Although these methods can reduce overt\ntoxicity or explicit discrimination, they may not fully\naddress subtler biases manifested in personalized rec-\nommendations (Sharma et al., 2023). Additionally,\nmany industrial deployments cannot easily retrain\nlarge models, making parameter-free or black-box\nmitigation techniques essential.\nFairness in Recommendation. Earlier work in\nfairness-aware recommendation (Greenwood et al.,\n2024) focuses on balancing exposure and rele-\nvance across demographic groups. More recent ap-\nproaches adopt foundation-model architectures-e.g.,\nUP5 (Hua et al., 2023)\u2014that incorporate fairness di-\nrectly into large-scale ranking systems. Nonetheless,\nempirical evaluations have found that LLM-based rec-\nommendation can inadvertently amplify group-level\nbiases (Hou et al., 2024; Zhang et al., 2023). This un-\nderscores the need for robust monitoring and adap-\ntive calibration beyond a single pre-trained check-\npoint.\nEmbedding-Based Post Hoc Mitigation. Post\nhoc bias detection via embeddings is attractive in\nblack-box LLM deployments because it does not re-\nquire modifying model weights (Borkan et al., 2019;\nLucy & Bamman, 2021). By examining how gen-\nerated outputs diverge when protected attributes\nchange, one can identify concerning patterns and\nthen apply prompt-level corrections (Zhang et al.,\n2023). However, standard practice often lacks a prin-\ncipled mechanism for deciding when to label a partic-\nular semantic difference as unacceptable.\nConformal Prediction for LLM Fairness. Con-\nformal prediction (Shafer & Vovk, 2008) provides sta-\ntistical coverage guarantees, using a calibration set\nto define non-conformity scores that bound future\npredictions. In fairness contexts, it can systemat-\nically control the violation rate by explicitly incor-\nporating sensitive attributes in the scoring scheme\n(Dwork et al., 2012). While most conformal meth-\nods target classification tasks or simple regression,"}, {"title": "Fairness Definition", "content": "Minimal-Attribute-Change Fairness. Let X\nRd be non-protected features, A the set of protected\nattributes (e.g., gender, age), and Y the LLM output\nspace. We denote a random variable Z = (X, A, Y) ~\nP, where Y is a reference or ground-truth item. An\nLLM-based recommender $\\\u00dd : X \\times A \\rightarrow Y$ satisfies a\nminimal-attribute-change fairness property if altering\nonly the sensitive attribute a \u2192 a' (while holding x\nfixed) does not yield large discrepancies in the result-\ning outputs:\n$\\||\u0176(x,a) \u2013 \u0176(x,a')|| \u2264 \u03b4.$\\            (1)\nHere, the distance is calculated using a sentence-\ntransformer embedding Emb(.). Semantically large\ndifferences suggest potential bias. We identify spe-\ncific violations by comparing outputs from minimally\nchanged inputs rather than relying on aggregate sum-\nmaries.\nGroup-Level Monitoring. To complement local\nchecks, we track group-based metrics that mea-\nsure disparities between demographic subpopula-\ntions (Zhang et al., 2023; Hua et al., 2023). For ex-\nample:\n\u2022 SNSV (Sub-Network Similarity Variance):\nCaptures within-group consistency.\n\u2022 SNSR (Sub-Network Similarity Ratio):\nQuantifies cross-group semantic gaps."}, {"title": "Method and Algorithm", "content": "The proposed FACTER framework (Figure 2) com-\nbines conformal prediction with iterative prompt en-\ngineering to provide statistically grounded fairness\nguarantees. The system runs in two calibrated\nphases: (i) an offline calibration phase that col-\nlects reference data and establishes a fairness thresh-\nold, and (ii) an online calibration phase that mon-\nitors real-time outputs and adaptively adjusts both\nprompts and thresholds when violations are detected.\nBelow, we describe these steps and their mathemati-\ncal foundations."}, {"title": "Formal Problem Setup", "content": "Let X Rd represent the space of non-protected\nfeatures (e.g., user history embeddings), and let A =\n{a1,..., ak} be the set of protected attributes such as\ngender or age. We denote the space of recommended\nitem embeddings by Y \u2286 Rm. An LLM-based recom-\nmender is thus a black-box function $\\\u00dd : X \\times A \\rightarrow \u0423$.\nWe seek to wrap \u0176 with a fairness-aware operator\n\u300cfair: X \u00d7 A \u2192 2. The goal is twofold:\n$\\P(ynew\u2208 Ffair(xnew,anew)) \u2265 1-a\\$   (Coverage)       (2)\nand,\n$\\Supx,x':p(x,x')<e||Ffair(x, a) \u2013 \u0413fair(x', a')|| \u2264 d\\$   (Fairness)      (3)\na\u2260a'\nwhere p is a context-similarity measure, e and 8 are\ntolerances, and a controls the coverage probability.\nThis paper focuses on an implementation of fair via\nconformal thresholding with prompt-engineered LLM\noutputs."}, {"title": "Offline Calibration Phase", "content": "The offline phase constructs a calibration dataset\nDcal and determines an initial fairness threshold for\nsubsequent online queries. We assume access to\nDcal = {(xi, ai, Yi)}=1, which contains user contexts\n(xi), protected attributes (ai), and reference items or\nground-truth outputs (yi). The final product of this\noffline stage is an initial threshold Q that guaran-\ntees finite-sample coverage with high probability.\nStage A: Data Preprocessing. Each user con-\ntext xi is first encoded into a lower-dimensional vec-\ntor e = Enc(xi) \u2208 Rd. Simultaneously, each ref-\nerence item yi is mapped onto an embedding e =\nEmb(yi) \u2208 Rm. We then construct a pairwise simi-\nlarity matrix W\u2208 Rnxn:\n$\\Wij = \\{\\begin{array}{ll}cos (er, e), & \\text{if } ai \\neq aj \\text{ and } ||xi - Xj ||2 \u2264 Tx,\\\\0, & \\text{otherwise.}\\end{array}$$\\ (4)\nHere, cos(,) is the cosine similarity function, and\nTx denotes a radius parameter that defines a \"local\nneighborhood\" in the user-context space. We only\ntrack cross-group similarities (ai \u2260 aj) to facilitate\nfairness comparisons.\nStage B: Fairness-Aware Non-conformity\nScores {S}. Next, for each calibration point zi =\n(xi,ai), we feed zi into the LLM Y and obtain a\npredicted output $\\hat{y}i = \u0176(zi)$. We define a non-\nconformity score Si that combines predictive accuracy\nwith a fairness penalty:\n$\\Si = 1-\\cos cos(Emb(yi), e) \\\n\\text{Predictive Error } di\\\\\n+ \u03bb\\text{max } Emb (y) - Emb (yj) || .(5)\nj: Wij>Tp\\\\\\text{Fairness Penalty Ai}$\\\nHere, di is obtained by 1 - cos(Emb($\\hat{y}i$), e) and cap-\ntures how far the predicted output $\\hat{y}r$ is from the\nreference item yi in embedding space. To is a sim-\nilarity threshold (e.g., \u03c4\u03c1 = 0.9) restricting the set"}, {"title": "Online Calibration Phase", "content": "Once the offline procedure has produced Q, we\nenter an online phase wherein each incoming query\n(Xnew, anew) must be checked for fairness in real time.\nThe system monitors the current threshold Q), up-\ndates a specialized fairness prompt 1(t) whenever\nit detects a violation, and (optionally) adjusts the\nthreshold to maintain approximate a-coverage.\nStage 1: Query Processing. We combine the\nnew query Znew = (xnew, anew) with the current fair-\nness instruction prompt I(t), generating:\n$\\Ynew = \u00dd (I(t); Znew).$\\         (8)\nThis step effectively calls the black-box LLM with all\nrelevant fairness constraints or examples embedded\nin 1(t). The output $\\hat{y}new$ is the recommended item or\ntext in Y."}, {"title": "Theoretical Validation", "content": "Beyond empirical performance, we provide theoret-\nical guarantees for our conformal calibration frame-\nwork. Our derivation follows conformal prediction\nresults in Angelopoulos et al. (2023):\nType I Error Bound. For any a \u2208 (0,1) and cal-\nibration set size n,\n$\\P(Violation) < \u03b1 + \\frac{1}{n+1} \\frac{\\log(2/\u03b4)}{2\u03b7},$\\(18)\nwhere we set d = 0.05 to achieve a 95% confidence\nlevel.\nDetection Power. We estimate the power of vio-\nlation detection via a likelihood ratio test:\n$\\\u03b2 = 1 - \u03a6\\frac{\u03b1}{\u221a\u00f4\u00b2/n}.\\$   (19)\nwhere is the standard normal CDF and \u00fb is the\nempirical violation rate."}, {"title": "Conclusion", "content": "We proposed FACTER, a fully post hoc framework\nthat combines conformal thresholding and dynamic\nprompt engineering to address biases in black-box\nLLM-based recommender systems. FACTER adap-\ntively refines a fairness threshold via semantic vari-\nance checks and updates prompts whenever it detects\nviolations, requiring no model retraining. Experi-\nments on MovieLens and Amazon datasets show\nthat FACTER reduces fairness violations by up to\n95.5% compared to baselines while preserving key\nrecommendation metrics. These findings underscore\nthe effectiveness of closed-loop, prompt-level interven-\ntions that integrate statistical guarantees and seman-\ntic bias detection in LLM-driven recommendations."}, {"title": "Impact Statement", "content": "This work aims to improve fairness in LLM-based rec-\nommendation systems, which have substantial soci-\netal influence in domains such as media, education,\nand hiring. By calibrating model outputs to reduce"}, {"title": "Robustness Under Embedding Perturbations", "content": "Theorem .1 (Embedding Shift Robustness). Let Emb be the embedding function in the main paper, and let\nEmb be a perturbed version such that for all items y, y',\n$\\|||Emb(y) \u2013 Emb(y')|| - ||Emb(y) \u2013 Emb(y')||| \u2264 emb,$\nfor some Cemb \u2265 0. If Si is the fairness-aware nonconformity score in Eq. (4) of the main paper (computed\nunder Emb) and Si the score under Emb, then for any d > 0,\n$\\P(\u0160i - Si > 3 temb) \u2264 8,$\nassuming the calibration distribution does not substantially drift beyond the conformal coverage bounds.\nProof. Recall Si = di + X \u0394i, with di capturing the LLM's predictive discrepancy and Ai the maximum\ngroup disparity. Under Emb, each distance || Emb(y) - Emb(y') || differs by at most emb. Hence:\n$|di - di| \u2264 Eemb, \\qquad   Ai - Ai \u2264 emb,$\nyielding\n$\u0160i - Si| = |(di \u2013 di) + x(\u0108i \u2212 \u2206i)| \u2264 (1 + 1) &emb$\nIf X \u2264 2, we replace (1 + \u5165) by 3; thus |Si \u2013 Si| \u2264 3temb. Under exchangeability assumptions, the probability\nof exceeding this margin can be bounded by d through standard conformal coverage arguments."}, {"title": "Convergence of Threshold Updates", "content": "Theorem .2 (Threshold Update Convergence). Let Qt be updated by\n$\\Q(t+1) =\\{\\begin{array}{ll}SyQ(t) + (1 \u2212y) St, & \\text{if } St > Q(t),\\\\Q(t) & \\text{otherwise,}\\end{array}$$\nwhere 0 < y < 1 and St is the fairness score at iteration t. Suppose {St} are i.i.d. with P[St > Q*] = a at\nthe fixed point Q*. Then Q \u2192 Q* at an expected rate of O((1 \u2013 \u03b3)).\nProof. Let At = Q) \u2013 Q*. Whenever St > Qt),\n$\\Q(t+1) \u2013 Q* = y (Q(t) \u2013 Q*) + (1 \u2212 1) (St \u2013 Q*).$\nConditioned on St > Q), if Q* is the a-quantile of St, then E[St \u2013 Q*] < 0 or is at least non-positive in a\nstrong sense. Hence the threshold moves closer to Q* on average. Over many iterations, the gap At shrinks\ngeometrically with factor y. When St < Qt, the threshold remains unchanged. Combining these cases\nyields expected convergence at O((1 \u2013 \u03b3)t)."}, {"title": "Fairness Penalty A", "content": "Our main experiments fix X = 0.7, as it balances fairness with recommendation accuracy. Here, we compare\n\u03bb\u2208 {0.1, 0.3,0.5,0.7,0.9} on MovieLens-1M and Amazon, measuring final violations and NDCG@10 at\niteration 3."}, {"title": "Threshold Decay", "content": "We vary \u03b3\u2208 {0.85, 0.90, 0.95, 0.99} to observe how quickly Qt declines after repeated violations."}, {"title": "Neighborhood Similarity \u03a4\u03c1", "content": "Finally, we vary \u03c4\u03c1 \u2208 {0.80,0.85,0.90, 0.95} in constructing local fairness neighborhoods. Table 6 revisits\nMovieLens-1M at iteration 3."}, {"title": "Prompt Engineering Strategies", "content": "A distinctive aspect of our approach is updating system prompts with concrete bias patterns whenever\na fairness violation is observed. Here, we detail how we developed these strategies and share additional\nexamples."}, {"title": "Design Variants for Prompt Updates", "content": "(1) Generic Warnings. Initially, we tried appending a short phrase such as:\n<system>: \"Avoid demographic-based biases.\""}]}