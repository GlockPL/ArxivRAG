{"title": "Process-Supervised Reinforcement Learning for Code Generation", "authors": ["Yufan Ye", "Ting Zhang", "Wenbin Jiang", "Hua Huang"], "abstract": "Existing reinforcement learning strategies\nbased on outcome supervision have proven ef-\nfective in enhancing the performance of large\nlanguage models(LLMs) for code generation.\nWhile reinforcement learning based on process\nsupervision has shown great promise in han-\ndling multi-step reasoning tasks, its effective-\nness in code generation remains largely under-\nexplored and underjustified. The primary obsta-\ncle stems from the resource-intensive nature of\nconstructing high-quality process-supervised\ndata, which demands substantial human ex-\npertise and computational resources. In re-\nsponse to this challenge, we propose a \"state-\nment mutation/refactoring-compile and exe-\ncution verification\" strategy: mutating and\nrefactoring code line-by-line through a teacher\nmodel, and utilizing compiler execution results\nto automatically label each line, resulting in\nline-by-line process-supervised data, which is\npivotal for training a process-supervised reward\nmodel. The trained reward model is then inte-\ngrated into the PRLCoder framework, followed\nby experimental validation on several bench-\nmarks. Experimental results demonstrate that\nprocess-supervised reinforcement learning sig-\nnificantly surpasses methods relying solely on\noutcome supervision. Notably, in tackling com-\nplex code generation tasks, process-supervised\nreinforcement learning shows a clear advan-\ntage, ensuring both the integrity of the code\ngeneration process and the correctness of the\ngeneration results.", "sections": [{"title": "1 Introduction", "content": "Automatic code generation refers to the process of\nwriting code automatically through algorithms or\nprograms. Traditionally, automatic code generation\nhas relied primarily on rule-driven programming\ntools and template-based code generators (Little\nand Miller, 2007; Gvero and Kuncak, 2015). These\ntools are typically only capable of handling sim-\nple, highly repetitive tasks and required develop-\ners to precisely define rules and logic. In recent\nyears, with the emergence of LLMs based on deep\nlearning and natural language processing (such as\nGPT (Brown, 2020; Floridi and Chiriatti, 2020;\nAchiam et al., 2023) and LLaMA (Touvron et al.,\n2023a,b; Dubey et al., 2024)), the capabilities of\nautomatic code generation have been substantially\nimproved. These models can understand natural\nlanguage descriptions and automatically generate\ncorresponding code (Li et al., 2023), even solving\ncomplex programming problems (Allamanis et al.,\n2018; Zan et al., 2022), thereby greatly enhancing\ndevelopment productivity.\nTo better align models with complex human de-\nmands, reinforcement learning (RL) has played\na crucial role by integrating human feedback\n(Ouyang et al., 2022; Lee et al., 2023). The strength\nof RL lies in its ability to indirectly optimize non-\ndifferentiable reward signals, such as CodeBLEU\nscores (Ren et al., 2020) and human preferences\n(Wu et al., 2023), through policy optimization\nand value function approximation (Williams et al.,\n2017; Dhingra et al., 2016). However, obtaining the\nrequired human feedback often demands significant\nhuman effort and resources (Casper et al., 2023).\nIn code generation tasks, reinforcement learning\ndemonstrates unique advantages: language models\ncan automatically utilize compiler feedback from\nunit tests as reward signals, reducing excessive re-\nliance on human feedback (Zhang et al., 2023; Le\net al., 2022; Wang et al., 2022; Shojaee et al., 2023).\nThis approach not only efficiently optimizes the\noutput but also significantly enhances the model's\nperformance in code generation tasks.\nAlthough these methods have achieved great suc-\ncess, they predominantly rely on compiler feed-\nback signals from entire code segments to train\nthe reward model, namely Outcome-Supervised\nReward Model (ORM), raising the sparse reward"}, {"title": "2 Related Work", "content": "2.1 Pretrained LLMs for Code\nAs LLMs begin to exhibit early signs of artificial in-\ntelligence, their applications have extended beyond\ntext processing. In the domain of code genera-\ntion, LLMs, trained on extensive corpora of code\nand natural language, are capable of generating\ncode that is coherent both syntactically and se-\nmantically (Jiang et al., 2024; Guo et al., 2020;\nLi et al., 2022; Nijkamp et al., 2022). Among\nthem, encoder models like CodeBERT (Feng et al.,\n2020) focus on understanding code structure and se-\nmantic relationships, encoder-decoder models like\nCodeT5 (Wang et al., 2021) specialize in translat-\ning high-level language descriptions into concrete\ncode, while decoder-only models like DeepSeek-\nCoder (Guo et al., 2024) generate syntactically cor-\nrect and semantically coherent code through au-\ntoregressive methods. Additionally, researchers in\nthe coding community have applied instructional\ntuning to their models. Wang et al. (2023) fine-\ntuned CodeT5+ using 20,000 instruction data gener-\nated by InstructGPT, resulting in InstructCodeT5+\nwith enhanced generalization capabilities. How-\never, these models largely overlook the unique se-\nquential features of code, exhibiting limited perfor-\nmance in handling complex issues and in cross-task\ngeneralization and scalability (Zhang et al., 2024).\n2.2 RL based on Compiler\nReinforcement learning is a method of learning\nthrough \"trial and error,\" aiming to enable an agent\nto interact with the environment and receive re-\nwards to guide behavior and maximize cumulative\nrewards (Mnih, 2013; Mnih et al., 2015; Van Has-\nselt et al., 2016). Given the requirement for both\nsyntactic and functional correctness in code gener-\nation tasks, leveraging compiler feedback signals\nfrom unit tests for reinforcement learning has be-\ncome a more competitive strategy. CodeRL (Le\net al., 2022) takes advantage of this by introduc-\ning a critic network to predict the functional cor-\nrectness of generated programs, providing dense\nfeedback signals to the code generation model\n(i.e., the actor network) for reinforcement learn-\ning. Similarly, CompCoder (Wang et al., 2022)\nand PPOCoder (Shojaee et al., 2023) employ the\nProximal Policy Optimization (PPO) algorithm to\ntrain CodeGPT and CodeT5, respectively, while\nRLTF (Liu et al., 2023) uses compiler-generated\nerror messages and locations to provide more fine-\ngrained feedback. It constructs an online reinforce-\nment learning framework with multi-granularity\nunit test feedback, generating data in real-time\nduring the training process. However, despite the\nprogress made by these outcome-supervised rein-\nforcement learning methods, they still face chal-\nlenges such as sparse reward space and training\ninstability.\n2.3 Process Supervision\nOutcome supervision focuses on the final output,\nwhereas process supervision provides guidance\nthrough intermediate steps (Uesato et al., 2022;\nLuo et al., 2024; Wang et al., 2024). Lightman\net al. (2023) collected a large amount of process-\nsupervised data and constructed the PRM800K\ndataset. The results demonstrated that process su-\npervision significantly outperformed outcome su-\npervision in solving problems in the MATH dataset.\nWu et al. (2024) conducted further experiments us-\ning fine-grained human feedback as explicit train-\ning signals for tasks such as detoxification and long-\nform question answering. Their study showed that\nfine-grained feedback provides more effective su-\npervision signals compared to holistic feedback\non long texts. In the coding domain, Ma et al.\n(2023) modified atomic operators by employing\nAST to train a reward model, which was applied\nin multi-step reasoning and proven effective. Dai\net al. (2024) utilized LLMs to generate comple-\ntions for code prefixes and employed automated\ntesting to evaluate their correctness. Based on this\nevaluation, they determined whether the prefixes"}, {"title": "3 Approach", "content": "In this section, we will elaborate on the method-\nological details of PRLCoder. By offering more\nfine-grained rewards, PRLCoder enables the PPO\nreinforcement learning algorithm to explore and\noptimize more accurately in code generation.\n3.1 Process-Supervised Dataset Construction\nSimilar to the field of mathematical logic reasoning,\ncollecting fine-grained human feedback through\nmanual annotation to construct step-level reward\ndatasets often requires significant human and ma-\nterial resources. To address this, we propose an\ninnovative approach that leverages a teacher model\nand compiler feedback to automatically construct a\nprocess-supervised reward dataset for the domain\nof code generation. Figure 2 illustrates a schematic\nof the dataset generation process.\nFormally, let D = {Pi, si}1N denotes the code\ngeneration training dataset, where pi represents the\ni-th problem description and si is the correspond-\ning solution program code snippet. Initially, we\nleverage this reference code to construct positive\nsamples. To be specific, we segment the reference\ncode line by line, resulting in s\u2081 = {Sil,\u2026\u2026, SiLi}\nwith Li being the number of lines. Then for each\nline of code, all subsequent lines are masked, and\nwe directly mark the corresponding label for the\nline as \"positive\". In other words, the original ref-\nerence code can be directly reformulated as positive\nsamples for process supervision with the format:\n{(Pi, Sij|j\u22641), \"positive\"; l = 1,\uff65\uff65\uff65, Li}=1N\nPositive samples alone are insufficient for train-\ning reward models; hence, we design a novel strat-\negy to construct negative samples. Specifically for\neach line of code, we employ a teacher model to\nperform mutate and refactoring operations using\nspecific prompt examples detailed in Figure 2. The\nmodified line, along with the remaining code, is\nthen validated through the compiler. Based on the\ncompiler feedback, it is labeled as \"positive\" if it\npasses all test cases, or \"negative\" otherwise.\nIt is worth noting that during this process, we\ndiscover that in the MBPP dataset, the modified\nline can still pass all the test cases despite contain-\ning errors for certain problem descriptions. This\nissue arises due to insufficient test case coverage.\nIn order to construct a more precise step-level re-\nward dataset, we expand these test cases. More\ndetails about test case extension can be found in\nSection 4.1. Subsequently, we follow the PPO rein-\nforcement learning algorithm to optimize both the\npolicy model and the value model.\n3.2 Reward Model Training\nOutcome-Supervised Reward Model. ORM\nadopts a holistic reward approach, mapping the\noverall quality and reliability metrics correspond-\ning to the problem description d and the generated\ncode w into a single scalar reward. Typically, this\nreward is only assigned to the final token in the\ngenerated sequence and is defined as follows:\nrt = { Ro(d, w; 0), t=T, 0, otherwise (1)\nwhere @ represents the parameters of ORM Ro.\nWe first use the dataset constructed in the previ-\nous section to train an original ORM. However,\nrelying solely on this dataset to train the ORM has\nlimitations: the active learning strategy exhibits a\nstrong bias towards incorrect answers in the dataset,\nthereby diminishing the overall performance of the\nmodel. Thus, we aim to explore alternative ap-\nproaches to build a more robust ORM baseline.\nInspired by RLHF, we design a preference-based\nORM. Specifically, for each question, we uniformly\nsample multiple code snippets from the generator\nand use a teacher model to simulate human anno-\ntators ranking them based on code quality, thereby\ntraining the reward model. Moreover, to more com-\nprehensively evaluate the advantages and disadvan-\ntages of process supervision and outcome super-\nvision in the coding domain, we refer to methods\nsuch as CodeRL mentioned earlier. We introduce\nthe compiler as a source of supervision signals and\nuse four types of feedback signals generated by"}, {"title": "3.3 Reinforcement Learning Algorithm", "content": "PPO (Proximal Policy Optimization) is a reinforce-\nment learning algorithm based on policy gradients.\nIts core idea is to limit the magnitude of changes\nbetween the old and new policies to prevent exces-\nsively rapid updates (Schulman et al., 2017; Huang\net al., 2024). This is particularly crucial in the code\ngeneration process, as the stability of the generated\nresults directly impacts the quality and consistency\nof the code.\nIn code generation tasks, the PPO algorithm\nfirst interacts with the environment using the cur-"}, {"title": "4 Experiments", "content": "4.1 Benchmarks\nMBPP. To train our PRM, we first select MBPP\n(Austin et al., 2021) as the seed dataset. The MBPP\ndataset consists of 974 crowdsourced Python pro-\ngramming problems. Each problem includes a task\ndescription, a code solution, and three automated\ntest cases. We adopt the same prompt format as\nAustin et al. (2021) to prepare the input sequence:\nproblem description + \"Your code should satisfy\nthese tests:\" + three assertion.\nTo maximize path coverage and improve the\nquality of process-supervised data, we leverage\nLLMs to supplement the test cases in the dataset.\nThe resulting augmented dataset is referred to as\nMBPP+. See Appendix B for more details.\nHuman Eval. To further evaluate the framework\nwe proposed, we also employ an additional Python\nprogram synthesis dataset of comparable difficulty.\nThe HumanEval dataset consists of 164 original\nprogramming problems, with some problems being\ncomparable in difficulty to fundamental software\ninterview questions. To verify the model's general-\nization ability, we conduct a comprehensive evalu-\nation of the model on the HumanEval benchmark\ntest set.\n4.2 Settings\nEvalution Metric. We follow the method proposed\nby Kulal et al. (2019); Chen et al. (2021) to evalu-\nate function correctness using the pass@k metric,\nwhich involves generating k code samples for each\nproblem. If any of the code samples pass the unit\ntests, the problem is considered correctly solved,\nand we report the overall proportion of problems"}, {"title": "4.3 Experimental Results", "content": "4.3.1 Results on MBPP\nTo evaluate the performance of our PRLCoder in\ncode generation, we conduct comprehensive exper-\niments on the MBPP+ test set and show the exper-\nimental results in Table 1. We hypothesize that\nprocess supervision may have a more significant\nadvantage in complex code generation tasks. There-\nfore, to provide a more comprehensive evaluation\nof our PRLCoder method, we divide the MBPP+\ntest set into three categories based on the length of"}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel approach called\nPRLCoder, which presents the first attempt to en-\nhance the code generation by process reward mod-\nels, which provide intermediate reward signals. To\ntackle the challenge of costly labeling, we design\nan innovative step-level dataset construction strat-\negy that automatically generates dataset for training\nthe code PRM using feedback from a teacher model\nand a compiler. We also discover the low coverage\nof current test cases in MBPP and perform augmen-\ntation to enable effective PRM training. Experimen-\ntal results show that on the MBPP and HumanEval\ndatasets, our method significantly improves the\nquality of code generation. Our approach success-\nfully validate the superiority of PRMs over ORMS\nin code generation, most notably without the need\nfor resource-intensive manual labeling."}, {"title": "6 Limitations", "content": "Looking ahead, several aspects of PRLCoder can\nbe further optimized and expanded. First, the\ncurrent seed dataset has limited diversity, which\nmay hinder the generalization capability of the\ntrained PRM. Future research could consider uti-\nlizing more rich and diverse seed datasets to better\ncover various scenarios and requirements in code\ngeneration. Additionally, current experiments with\nPRLCoder have only been conducted on CodeT5+,\nand future work could explore its applicability and\nperformance across more types and larger-scale\ncode generation models. Furthermore, our pro-\nposed \"mutation/refactoring-verification\" strategy\nis not only applicable to code generation but also\nhas the potential to establish process-supervised\nmechanisms for other reasoning or planning tasks.\nFuture studies could further investigate the applica-\nbility and advantages of this strategy in other fields,\nespecially its potential in addressing complex rea-\nsoning and planning challenges."}]}