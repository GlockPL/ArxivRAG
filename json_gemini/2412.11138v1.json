{"title": "Safe Reinforcement Learning using Finite-Horizon Gradient-based Estimation", "authors": ["Juntao Dai", "Yaodong Yang", "Qian Zheng", "Gang Pan"], "abstract": "A key aspect of Safe Reinforcement Learning (Safe RL) involves estimating the constraint condition for the next policy, which is crucial for guiding the optimization of safe policy updates. However, the existing Advantage-based Estimation (ABE) method relies on the infinite-horizon discounted advantage function. This dependence leads to catastrophic errors in finite-horizon scenarios with non-discounted constraints, resulting in safety-violation updates. In response, we propose the first estimation method for finite-horizon non-discounted constraints in deep Safe RL, termed Gradient-based Estimation (GBE), which relies on the analytic gradient derived along trajectories. Our theoretical and empirical analyses demonstrate that GBE can effectively estimate constraint changes over a finite horizon. Constructing a surrogate optimization problem with GBE, we developed a novel Safe RL algorithm called Constrained Gradient-based Policy Optimization (CGPO). CGPO identifies feasible optimal policies by iteratively resolving sub-problems within trust regions. Our empirical results reveal that CGPO, unlike baseline algorithms, successfully estimates the constraint functions of subsequent policies, thereby ensuring the efficiency and feasibility of each update.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning (RL) (Sutton & Barto, 2018) stands as a powerful paradigm in artificial intelligence. Over the past few years, RL has achieved notable success across various challenging tasks, such as video games (ElDahshan et al., 2022), robotic control (Okamura et al., 2000; Singh et al., 2022), Go (Silver et al., 2016; 2017), and the training of large language models (Ouyang et al., 2022; Rafailov et al., 2023). Recently, there has been a growing emphasis on prioritizing the safety of policy learning. This shift is driven by the critical need for safety in real-world applications, such as autonomous driving (Muhammad et al., 2020) and service robots (Bogue, 2017). In response, Safe RL (Garcia & Fern\u00e1ndez, 2015) has emerged as a related paradigm, aiming to provide reliable and robust policy learning in the face of complex and dynamic environments. The Constrained Markov Decision Process (CMDP) (Altman, 1999) stands as a foundational framework of Safe RL, augmenting the traditional Markov Decision Process (MDP) (Puterman, 1990) by incorporating constraints.\nIn most Safe RL benchmarks (Ray et al., 2019; Gronauer, 2022; Ji et al., 2023b), the common practical constraints are represented as non-discounted sums over finite trajectories, subject to scalar thresholds. However, existing deep Safe RL algorithms uniformly apply an infinite-horizon approach, the Advantage-based Estimation (ABE) (Achiam et al., 2017), across all types of constraints. This fundamental discrepancy hinders these algorithms' ability to accurately predict the constraint values for subsequent policies, thereby misleading the optimization direction in constrained problems. Our straightforward experiment illustrates this issue in Figure 1. Despite the simplicity of the task, the ABE method generates relative errors exceeding 1.0.\nTo bridge the gap in estimating finite-horizon constraints, we introduce a new estimation methodology, Gradient-based Estimation (GBE). Unlike previous methods, GBE avoids reliance on the infinite-horizon assumption, instead leveraging first-order gradients derived along finite trajectories (Mohamed et al., 2020). This approach facilitates precise estimation of constraints, particularly where a non-discounted cumulative sum over a finite horizon is compared against a scalar threshold. Utilizing the GBE, we construct the constrained surrogate problem and develop a novel Safe RL algorithm, called Constrained Gradient-based Policy Optimization (CGPO). Additionally, informed by an error analysis of GBE, we implement a trust region approach in the parameter space to mitigate errors and further ensure the feasibility of policy updates.\nOur contributions are threefold: (1) We introduce the GBE method, designed to estimate finite-horizon non-discounted constraints in Safe RL tasks, along with relevant theoretical analysis. (2) We propose the CGPO algorithm. To our best knowledge, CGPO is the first deep Safe RL algorithm that can effectively address tasks with finite-horizon constraints. Based on precise estimations of the GBE method, CGPO ensures the efficiency and feasibility of each update. Our analysis includes worst-case scenarios and introduces an adaptive trust region radius strategy for improved performance. (3) We develop a series of Safe RL tasks with differentiable dynamics to test our algorithm. Comparative evaluations demonstrate our algorithm's superior update efficiency and constraint satisfaction against baselines."}, {"title": "2. Related Works", "content": "Differentiable RL. In many RL applications, the knowledge of the underlying transition function facilitates policy optimization through analytical gradients, a method known as Differentiable RL (Mohamed et al., 2020; Jaisson, 2022). The development of differentiable simulators (Degrave et al., 2019; Werling et al., 2021; Xian et al., 2023), which represent systems as differentiable computational graphs, significantly advance this research area. Some well-known differentiable RL algorithms include BPTT (Mozer, 1995), POSD (Mora et al., 2021), and SHAC (Jie Xu et al., 2022). Moreover, differentiable RL techniques extend to model-based algorithms that access analytical gradients from World Models (Clavera et al., 2020; As et al., 2022; Parmas et al., 2023b). Our algorithm, consistent with other Differentiable RL methods, depends on either a differentiable simulator or the World Model method for modeling task environments. Considering the swift advancements in related techniques, we view this requirement optimistically. For further discussion, see Section 7.\nSafe RL. The work closely related to ours focuses on on-policy deep Safe RL algorithms, which are categorized into Lagrangian and Convex Optimization methods (Xu et al., 2022). Lagrangian methods, such as PDO (Chow et al., 2018) and CPPO-PID (Stooke et al., 2020), alternate between addressing the primal and the dual problems. The latest APPO (Dai et al., 2023) mitigates the oscillatory issue by augmenting a simple quadratic term. Convex Optimization methods, such as CPO (Achiam et al., 2017) and CVPO (Liu et al., 2022), optimize based on the solution to the primal problem. The latest method, CUP (Yang et al., 2022), divides updates into two steps: updating in the steepest direction first, then mapping into the feasible domain if constraints are violated. Above all methods use estimates of objective and constraint functions to formulate surrogate problems (Achiam et al., 2017), which we refer to as Advantage-based Estimation. Thus, the accuracy of estimates impacts the efficiency and feasibility of updates.\nHowever, research on finite-horizon constraints has been limited to non-deep Safe RL contexts (Kalagarla et al., 2021; Guin & Bhatnagar, 2023). Despite their prevalence in the Safe RL Benchmark, aforementioned deep Safe RL algorithms treat these constraints as if they were infinite-horizon (such as Safety-Gym (Ray et al., 2019), Bullet-Safety-Gym (Gronauer, 2022), Safety-Gymnasium (Ji et al., 2023b), and OmniSafe (Ji et al., 2023c)). This leads to poor constraint satisfaction within these benchmarks."}, {"title": "3. Preliminaries", "content": "3.1. Constrained Markov Decision Process\nRL is typically framed as a Markov Decision Process (MDP) (Puterman, 2014), denoted $\\mathcal{M} \\equiv \\langle \\mathcal{S}, \\mathcal{A}, r, P, \\mu_0 \\rangle$, which encompasses the state space $\\mathcal{S}$, the action space $\\mathcal{A}$, a reward function $r$, the transition probability $P$, and the initial state distribution $\\mu_0$. A stationary policy $\\pi$ represents a probability distribution defining the likelihood of taking action $a$ in state $s$. The set $\\Pi$ symbolizes the collection of all such stationary policies. The primary objective of RL is to optimize the performance metric typically formulated as the total expected return over a finite horizon $T$, namely, $J_R(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} [\\sum_{t=0}^{T-1} r(s_t, a_t)]$.\nGenerally, Safe RL is formulated as a Constrained MDP (CMDP) (Altman, 1999), $\\mathcal{M}_\\mathcal{UC}$, augmenting the standard MDP $\\mathcal{M}$ with an additional set of constraints $\\mathcal{C}$. This constraint set $\\mathcal{C} = \\{(\\mathcal{c}_i, b_i)\\}_{i=1}^m$ consists of pairs of cost functions $\\mathcal{c}_i$ and corresponding thresholds $b_i$. The constraint function is defined as the cumulative cost over the horizon $T$, $J_{\\mathcal{C}_i}(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} [\\sum_{t=0}^{T-1} c_i(s_t, a_t)]$ and the feasible"}, {"title": "3.2. Advantage-based Estimation Method", "content": "By introducing the discount factor $\\gamma$, we define the objective function for an infinite-horizon scenario as $J_R^{\\gamma}(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]$ and the constraint function as $J_C^{\\gamma}(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma^t c_i(s_t, a_t)]$. We express the infinite-horizon value function as $V^{\\gamma}(s) = \\mathbb{E}_{\\tau \\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r_t | s_0 = s]$ and the state-action value function as $Q^{\\gamma}(s, a) = \\mathbb{E}_{\\tau \\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r_t | s_0 = s, a_0 = a]$. The infinite-horizon advantage function is $A^{\\gamma}(s, a) = Q^{\\gamma}(s, a) - V^{\\gamma}(s)$. The discounted future state distribution $d^{\\gamma}$ is denoted as $d^{\\gamma}(s) = (1 - \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t P(s_t = s | \\pi)$. Then, the difference in some metrics between two policies $\\pi, \\pi'$ can be derived as (Kakade & Langford, 2002):\n$\\mathcal{J}_R^{\\gamma}(\\pi') - \\mathcal{J}_R^{\\gamma}(\\pi) = \\frac{1}{1 - \\gamma} \\mathbb{E}_{s \\sim d^{\\gamma}, a \\sim \\pi'} [A^{\\gamma}(s, a)],$ (1)\nwhere $\\mathcal{J}_R^{\\gamma}$ represents infinite-horizon $\\mathcal{J}_R, \\mathcal{J}_C$. Equation (1) is difficult to estimate since $s \\sim d^{\\gamma}$, is unknown. Achiam et al. (2017) approximates $s \\sim d^{\\gamma}$ with $s \\sim d^{\\pi}$ and employs importance sampling techniques to derive the following estimation:\n$\\mathcal{J}_R^{\\gamma}(\\pi') - \\mathcal{J}_R^{\\gamma}(\\pi) \\approx \\frac{1}{1 - \\gamma} \\mathbb{E}_{s \\sim d^{\\pi}} [\\frac{\\pi'(s, a)}{\\pi(s, a)} A^{\\gamma}(s, a)],$ (2)\nwhere $d^{\\pi}$ and $A^{\\gamma}(s, a)$ are both defined in the infinite-horizon format and it requires that $\\gamma \\neq 1$. Thus, algorithms based on Equation (2) have to treat all constraints as if they were infinite-horizon."}, {"title": "3.3. Policy Optimization with Differentiable Dynamics", "content": "Differentiable simulators (Freeman et al., 2021) represent physical rules using a differentiable computational graph, $s_{t+1} = F(s_t, a_t)$, allowing them to participate in gradient back-propagation process (Mohamed et al., 2020). The fundamental approach for policy optimization with differentiable physical dynamics is Back-propagation Through Time (BPTT) (Mozer, 2013). Multiple trajectories {$\\tau_i\\}_{i=1}^N$ are collected from and then derivation is performed along the trajectory. Then, the loss function is\n$\\mathcal{L}^{BPTT} = \\frac{1}{NT} \\sum_{i=1}^N \\sum_{t=0}^{T-1} r(s_t^i, a_t^i),$ (3)\nThe Short-Horizon Actor-Critic (SHAC) approach (Jie Xu et al., 2022) modifies BPTT by ultilizing the value function $V$ to segment trajectories into sub-windows of length $h$:\n$\\mathcal{L}^{SHAC} = \\frac{1}{N} \\sum_{i=1}^N [\\sum_{t=t_0}^{t_0+h-1} r(s_t^i, a_t^i) + \\gamma^h V(s_{t_0+h}^i)],$ (4)\nThis change limits the maximum length for gradient back-propagation to $h$, thereby mitigating the issues of gradient vanishing/exploding and making the training more stable."}, {"title": "4. Constrained Surrogate Problem using Gradient-based Estimation", "content": "Differentiable environments represent physical dynamics as differentiable computational graphs, allowing first-order gradients of objective and constraint functions to be derived via gradient back-propagation along trajectories. In this section, we propose a new estimation method based on this feature, called Gradient-based Estimation (GBE). It facilitates more accurate approximations for the next police's objective and constraint functions, which leads to a new constrained surrogate problem for solving Safe RL.\n4.1. Gradient-based Estimation for Objective and Constraint Functions\nConsider a parameterized policy $\\pi_\\theta$ within a parameter space $\\Theta$, e.g., represented by a neural network. In the context of differentiable environments, both the objective function $J_R(\\theta)$ and the constraint function $J_C(\\theta)$ can be regarded as differentiable over $\\Theta$. Consequently, we can compute the first-order gradients of them along the trajectories, denoted as $\\nabla_\\theta J_R(\\theta)$ and $\\nabla_\\theta J_C(\\theta)$, through back-propagation. For simplicity and generality, we will focus on a single constraint scenario, though the method applies to multiple constraints via corresponding matrix operations.\nFor a minor update $\\delta$ in the policy parameter space, transitioning from $\\theta_0$ to $\\theta_0 + \\delta$, consider the function $J_f$, which encapsulates both the objective function $J_R$ and the constraint function $J_C$. We perform a first-order Taylor expansion of $J_f$ at $\\theta_0$ to obtain:\n$J_f(\\theta_0 + \\delta) = J_f(\\theta_0) + \\delta^\\top \\nabla_\\theta J_f(\\theta_0) + \\frac{1}{2} \\delta^\\top \\nabla^2 J_f(\\theta_0 + t \\delta) \\delta,$ (5)\nWherein, $t \\in (0, 1)$ and $\\delta^\\top \\nabla J_f(\\theta_0 + t \\delta) \\delta$ represents the Peano remainder term, which is $o(||\\delta||)$. When $\\delta$ is sufficiently small, the remainder term becomes negligible. Therefore, we propose the Gradient-based Estimation method to estimate the values of the objective and constraint functions after a minor update $\\delta$ from $\\theta_0$ as follows:\n$\\hat{J}_R(\\theta_0 + \\delta) \\approx J_R(\\theta_0) + \\delta^\\top \\nabla_\\theta J_R(\\theta_0),$ (6)\n$\\hat{J}_C(\\theta_0 + \\delta) \\equiv J_C(\\theta_0) + \\delta^\\top \\nabla_\\theta J_C(\\theta_0).$ (7)"}, {"title": "4.2. Constrained Surrogate Problem within Trust Region", "content": "Based on the analysis of Theorem 4.1, the idea of controlling the error by managing $||\\delta||^2$ naturally aligns with the concept of trust regions (Schulman et al., 2015; Meng et al., 2022). Given a trust region radius $\\delta$, we suppose that updated parameter $\\theta_{k+1}$ of the $k^{th}$ iteration within the trust region $\\Theta_k = {\\theta \\in \\Theta \\mid ||\\theta - \\theta_k||^2 \\leq \\delta}$ are credible.\nBy employing the approximations of the objective function in Equation (6) and the constraint function in Equation (7), We transform the solution of the primal Safe RL problem into an iterative process of solving a series of sub-problems within predefined trust regions. Given the initial policy parameter $\\theta_k$ of the $k^{th}$ iteration, our sub-problem targets finding the next optimal and credible parameter $\\theta_{k+1} \\in \\Theta_k$, which not only maximize the surrogate objective function $\\hat{J}_R(\\theta_{k+1})$ but also conform to the surrogate constraint $\\hat{J}_C(\\theta_{k+1}) \\leq b$. Thus, the surrogate sub-problem within a given trust region at $k^{th}$ iteration can be represented as:\n$\\theta_{k+1} = \\arg \\max_{\\theta \\in \\Theta} (\\theta - \\theta_k)^\\top \\nabla_\\theta J_R(\\theta_k)$ (9)\ns.t. $J_C(\\theta_k) + (\\theta - \\theta_k)^\\top \\nabla_\\theta J_C(\\theta_k) \\leq b$ \n$(\\theta - \\theta_k)^\\top (\\theta - \\theta_k) \\leq \\delta.$"}, {"title": "5. Constrained Gradient-based Policy Optimization", "content": "Based on the constrained surrogate sub-problem in Equation (9), we develop a novel Safe RL algorithm named Constrained Gradient-based Policy Optimization (CGPO).\n5.1. Solution to Surrogate Sub-problem\nNotations. Considering the $k^{th}$ iteration within the trust region $\\Theta_k$, we introduce additional notations to make the discussion more concise: $g_k \\equiv \\nabla_\\theta J_R(\\theta_k)$, $g_k^c \\equiv \\nabla_\\theta I_C(\\Theta_k)$, $c_k \\equiv J_C(\\theta_k) - b$, and $\\delta \\triangleq \\theta - \\theta_k$. With these definitions, we rewrite the sub-problem (9) within the trust region:\n$\\max_{\\delta} g_k^\\top \\delta \\quad s.t. \\quad c_k + g_k^c \\delta \\leq 0, \\quad \\delta^\\top \\delta \\leq \\delta$ (10)\nSince the sub-problem (10) may have no solution, we first discuss the conditions under which this problem is unsolvable. The following theorem holds:\nTheorem 5.1 (Solvability Conditions for the Sub-problem). The sub-problem (10) is unsolvable if and only if $c_k^2 / g_k^{c\\top} g_k^c - \\delta > 0$ and $c_k > 0$.\nProof. Consider whether there is an intersection between the trust region and the feasible half-space. For a detailed proof, refer to Appendix A.2.1.\nThrough a similar proof, we arrive at the following corollary:\nCorollary 5.2. 0 is deemed feasible for every $\\theta$ within $\\Theta_k$ if and only if $c_k^2 / g_k^{c\\top} g_k^c - \\delta > 0$ and $c_k \\leq 0$ are both satisfied.\nBased on Theorem 5.1 and Corollary 5.2, we solve for $\\theta_{k+1}$ of sub-problem (9) in three different scenarios, as illustrated in Figure 2."}, {"title": "5.2. Worst-Case Analysis", "content": "In the following theorem, we detail the bounds for performance update and constraint violations under the worst-case scenarios after the policy update which results from solving the surrogate sub-problem (9).\nTheorem 5.3 (Worst-Case Performance Update and Constraint Violation). Suppose $\\theta_k, \\theta_{k+1} \\in \\Theta$ are related by Equation (9). If $\\theta_k$ is feasible, a lower bound on the policy performance difference between $\\theta_{k+1}$ and $\\theta_k$ is\n$J_R(\\theta_{k+1}) - J_R(\\theta_k) \\geq -\\frac{1}{2} \\epsilon_R \\delta,$ (16)\nwhere $\\epsilon_R = \\max_{t \\in (0, 1)} |\\nabla^2 J_R(\\theta_k + t(\\theta_{k+1} - \\theta_k))|$.\nAn upper bound on the constraint objective function of $\\theta_{k+1}$ is\n$J_C(\\theta_{k+1}) \\leq b + \\frac{1}{2} \\epsilon_C \\delta,$ (17)\nwhere $\\epsilon_C = \\max_{t \\in (0, 1)} |\\nabla^2 I_C(\\theta_k + t(\\theta_{k+1} - \\theta_k))|$.\nProof. Let the feasible region of the sub-problem be denoted as\n$\\Theta^f = {\\theta \\in \\Theta \\mid I_C(\\theta_k) + (\\theta - \\theta_k)^\\top \\nabla_\\theta J_C(\\theta_k) \\leq b, ||\\theta - \\theta_k||^2 \\leq \\delta}.$ (48)\nFurthermore, because\n$\\theta_{k+1} = \\arg \\max_{\\theta \\in \\Theta^f} (\\theta - \\theta_k)^\\top \\nabla_\\theta J_R(\\theta_k),$ (49)\nit follows that\n$\\forall \\theta \\in \\Theta^f, (\\theta_{k+1} - \\theta_k)^\\top \\nabla_\\theta J_R(\\theta_k) \\geq (\\theta - \\theta_k)^\\top \\nabla_\\theta J_R(\\theta_k).$ (50)\nSince $\\theta_k$ is feasible, it follows that $\\theta_k \\in \\Theta^f$. Consequently, we have\n$(\\theta_{k+1} - \\theta_k)^\\top \\nabla_\\theta J_R(\\theta_k) \\geq (\\theta_k - \\theta_k)^\\top \\nabla_\\theta J_R(\\theta_k) = 0.$ (51)\nFurther, we have $\\exists t_k \\in (0, 1)$ such that\n$J_R(\\theta_{k+1}) - J_R(\\theta_k) = (\\theta_{k+1} - \\theta_k)^\\top \\nabla_\\theta J_R(\\theta_k) + \\frac{1}{2} (\\theta_{k+1} - \\theta_k)^\\top \\nabla^2 J_R(\\theta_k + t_k(\\theta_{k+1} - \\theta_k))(\\theta_{k+1} - \\theta_k)$\n$\\geq -\\frac{1}{2} |(\\theta_{k+1} - \\theta_k)^\\top \\nabla^2 J_R(\\theta_k + t_k(\\theta_{k+1} - \\theta_k)) (\\theta_{k+1} - \\theta_k)|$ (52)\n$\\geq -\\frac{1}{2} \\epsilon_R ||\\theta_{k+1} - \\theta_k||^2$\n$\\geq -\\frac{1}{2} \\epsilon_R \\delta,$\nwhere $\\epsilon_R = \\max_{t \\in (0, 1)} |\\nabla^2 J_R(\\theta_k + t(\\theta_{k+1} - \\theta_k))|$.\nOn the other hand, since $\\theta_{k+1}$ is feasible, it follows that\n$J_C(\\theta_k) + (\\theta_{k+1} - \\theta_k)^\\top \\nabla_\\theta J_C(\\theta_k) \\leq b$ (53)"}, {"title": "5.3. Practical Implementation", "content": "Given our focus on employing gradients to solve Safe RL problems, our algorithm can incorporate any differentiable method to compute gradients $g_k$ and $q_k$. Various methods are available, broadly classified into two categories: the"}, {"title": "6. Experiments", "content": "We conduct experiments to validate the effectiveness of our proposed CGPO. Our focus is primarily on four aspects:\n\u2022 CGPO outperforms the baseline algorithms in Safe RL, demonstrating more efficient performance improvement and more precise constraint satisfaction (Section 6.2).\n\u2022 CGPO employs the GBE method to obtain accurate estimations, unlike the ABE method fails (Section 6.3).\n\u2022 CGPO can overcome the differentiability requirements through Model-based approaches (Section 6.4).\n\u2022 CGPO can achieve more stable constraint convergence through an adaptive trust region radius (Section 6.5).\n6.1. Experimental Details\nPlease refer to Appendix E for the detailed implementation of experimental tasks and Appendix F for baseline algorithms.\nDifferentiable Safe RL environments. Due to the lack of differentiable Safe RL environments, we develop a series of constrained differentiable tasks on an open-source differentiable physics engine Brax (Freeman et al., 2021). These tasks are based on four differentiable robotic control tasks in Brax (CartPole, Reacher, Half Cheetah, and Ant), with the addition of two common constraints: limiting position(Achiam et al., 2017; Ji et al., 2023b) and limiting velocity(Zhang et al., 2020). It is important to note that while adding these constraints, we maintained the differentiability of the physical dynamics."}, {"title": "6.2. Overall Performance", "content": "Figure 4 shows the learning curves of different algorithms across various tasks. We observe that CGPO not only converges to a constraint-satisfying policy more quickly and stably than baseline algorithms but also demonstrates enhanced efficiency in improving performance.\nCompared to conventional Safe RL algorithms, CGPO demonstrates superiority in both constraint satisfaction and sample efficiency. Firstly, as illustrated in Figure 4, CGPO exhibits more precise control over constraints. In scenarios where constraint satisfaction conflicts with performance improvement (i.e., CartPole), CGPO can strictly adhere to constraint thresholds to achieve higher performances. Additionally, as shown in Table 1, CGPO significantly reduces the proportion of constraint violations in critical safety zones compared to other baseline algorithms. The advantage of CGPO in constraint satisfaction largely sources from the accuracy of the GBE method, which will be further discussed in Section 6.3. Secondly, CGPO demonstrates higher sample efficiency by directly employing analytical gradients, as opposed to Monte Carlo sampling estimation. As shown in Table 1, CGPO requires less than 28.8%-91.7% of samples at convergence compared to baselines. This finding aligns with previous works (Mora et al., 2021; Jie Xu et al., 2022)."}, {"title": "6.3. Ablation on Estimation Errors", "content": "The enhanced performance of CGPO in our experiments primarily results from the GBE method's accurate estimation of finite-horizon constraints for future policies, a task at which the ABE method fails. Our ablation results to compare GBE and ABE errors are illustrated in Fig. 5. We observe that the estimation error of GBE is much smaller than that of ABE; the relative error of ABE even exceeds 1.0, highlighting its almost ineffectiveness. Thus, solutions from the ABE-based surrogate optimization problem are not as feasible for the primal Safe RL problem, preventing traditional algorithms from precisely meeting safety constraints.\nIn the HalfCheetah task, errors are higher than in the FunctionEnv task, primarily due to the reduced system differentiability. This exposes a limitation of CGPO, its potential failure in poorly differentiable systems. CGPO has the potential to overcome it by training a World Model for gradient provision, as discussed in the next section."}, {"title": "6.4. Ablation on World Model Augmentation", "content": "In systems with limited differentiability, we can access analytic gradients by training a World Model. The fundamental implementation of the model-based CGPO algorithm is presented by Algorithm 3. We evaluate its efficacy on two non-differentiable tasks, with specific settings described in Appendix F.4. As shown in Figure 6, the implementation of a World Model enables CGPO to achieve improved performance while adhering to safety constraints in environments with limited differentiability. This demonstrates that the World Model has the potential to expand CGPO to a broader range of applications, overcoming the limitations of environmental differentiability. Nonetheless, incorporating the World Model incurs additional time and computational overhead, necessitating further optimization."}, {"title": "6.5. Ablation on Adaptive Trust Region Radius", "content": "Our method dynamically adjusts the trust region radius $\\delta$ to maintain estimation error within a tolerable limit. Adaptive $\\delta$, as Figure 7 demonstrates, enhances performance and ensures more accurate adherence to constraints than a static radius. The learning curve of the adaptive radius has two phases: initially, it prioritizes improvements in efficient performance, ensuring consistency between actual and predicted performance changes, while achieving more larger updates. Later, as policy updates approach constraint limits, the focus shifts to reducing estimation errors within the constraint budget, thereby preventing violations. More ablation"}, {"title": "7. Limitation and Future Works", "content": "Although our algorithm demonstrates theoretical and empirical superiority over baseline algorithms, it has certain limitations. Like other differentiable reinforcement learning methods, CGPO relies on environmental differentiability. As discussed in Section 6.3, systems with poor differentiability are susceptible to environmental noise, resulting in increased system errors and reduced precision in GBE estimation. While CGPO mitigates the upper bound of worst-case estimation errors through an adaptive trust region radius method, an excessively small radius can reduce update efficiency.\nWe identify three promising solutions to address this limitation. First, advancing differentiable physics engines to enhance environmental differentiability (Degrave et al., 2019; Freeman et al., 2021; Howell et al., 2022). Second, developing algorithms to minimize system errors at the software level (Metz et al., 2021), such as the reparameterization technique (Lee et al., 2018). Third, providing analytic gradients for systems with poor differentiability through more accurate world model training (Parmas et al., 2023a). In Section 6.4, we discuss this approach and its basic implementation using a two-layer Multi-Layer Perceptron (MLP) (Popescu et al., 2009), resulting in low-precision gradients from the world model.\nFor future work, it is crucial to develop a more universal framework based on our algorithm that incorporates the three aforementioned approaches to mitigate existing limitations. Additionally, we plan to extend the application of our algorithm to fields beyond robotic control (He et al., 2024; Wang et al., 2024), such as the safety alignment (Ji et al., 2023a) of large language models (Ji et al., 2024b; Dai et al., 2024; Ji et al., 2024a) and autonomous driving (Muhammad et al., 2020; Gao et al., 2023)."}, {"title": "8. Conclusion", "content": "Since current deep Safe RL algorithms uniformly apply an infinite-horizon approach to all constraints, they fail to guarantee the feasibility of each update. In this paper, we introduce the Gradient-based Estimation (GBE) method to bridge the gap of finite-horizon constraint estimation in deep Safe RL. Based on GBE, we formulate a surrogate optimization problem and propose the first deep Safe RL algorithm, named Constrained Gradient-based Policy Optimization (CGPO), able to handle tasks with finite-horizon constraints. CGPO leverages precise GBE estimations to ensure the efficiency and feasibility of each update. We also present a differentiable Safe RL environment for algorithm testing. Comparative evaluations reveal that our algorithm outperforms baselines in terms of update efficiency and constraint satisfaction."}]}