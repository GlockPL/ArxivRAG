[{"title": "Safe Reinforcement Learning using Finite-Horizon Gradient-based Estimation", "authors": ["Juntao Dai", "Yaodong Yang", "Qian Zheng", "Gang Pan"], "abstract": "A key aspect of Safe Reinforcement Learning (Safe RL) involves estimating the constraint condition for the next policy, which is crucial for guiding the optimization of safe policy updates. However, the existing Advantage-based Estimation (ABE) method relies on the infinite-horizon discounted advantage function. This dependence leads to catastrophic errors in finite-horizon scenarios with non-discounted constraints, resulting in safety-violation updates. In response, we propose the first estimation method for finite-horizon non-discounted constraints in deep Safe RL, termed Gradient-based Estimation (GBE), which relies on the analytic gradient derived along trajectories. Our theoretical and empirical analyses demonstrate that GBE can effectively estimate constraint changes over a finite horizon. Constructing a surrogate optimization problem with GBE, we developed a novel Safe RL algorithm called Constrained Gradient-based Policy Optimization (CGPO). CGPO identifies feasible optimal policies by iteratively resolving sub-problems within trust regions. Our empirical results reveal that CGPO, unlike baseline algorithms, successfully estimates the constraint functions of subsequent policies, thereby ensuring the efficiency and feasibility of each update.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning (RL) (Sutton & Barto, 2018) stands as a powerful paradigm in artificial intelligence. Over the past few years, RL has achieved notable success across various challenging tasks, such as video games (ElDahshan et al., 2022), robotic control (Okamura et al., 2000; Singh et al., 2022), Go (Silver et al., 2016; 2017), and the training of large language models (Ouyang et al., 2022; Rafailov et al., 2023). Recently, there has been a growing emphasis on prioritizing the safety of policy learning. This shift is driven by the critical need for safety in real-world applications, such as autonomous driving (Muhammad et al., 2020) and service robots (Bogue, 2017). In response, Safe RL (Garcia & Fern\u00e1ndez, 2015) has emerged as a related paradigm, aiming to provide reliable and robust policy learning in the face of complex and dynamic environments. The Constrained Markov Decision Process (CMDP) (Altman, 1999) stands as a foundational framework of Safe RL, augmenting the traditional Markov Decision Process (MDP) (Puterman, 1990) by incorporating constraints.\nIn most Safe RL benchmarks (Ray et al., 2019; Gronauer, 2022; Ji et al., 2023b), the common practical constraints are represented as non-discounted sums over finite trajectories, subject to scalar thresholds. However, existing deep Safe RL algorithms uniformly apply an infinite-horizon approach, the Advantage-based Estimation (ABE) (Achiam et al., 2017), across all types of constraints. This fundamental discrepancy hinders these algorithms' ability to accurately predict the constraint values for subsequent policies, thereby misleading the optimization direction in constrained problems. Our straightforward experiment illustrates this issue in Figure 1. Despite the simplicity of the task, the ABE method generates relative errors exceeding 1.0.\nTo bridge the gap in estimating finite-horizon constraints, we introduce a new estimation methodology, Gradient-based Estimation (GBE). Unlike previous methods, GBE avoids reliance on the infinite-horizon assumption, instead leveraging first-order gradients derived along finite trajectories (Mohamed et al., 2020). This approach facilitates precise estimation of constraints, particularly where a non-discounted cumulative sum over a finite horizon is compared against a scalar threshold. Utilizing the GBE, we construct the constrained surrogate problem and develop a novel Safe RL algorithm, called Constrained Gradient-based Policy Optimization (CGPO). Additionally, informed by an error analysis of GBE, we implement a trust region approach in the parameter space to mitigate errors and further ensure the feasibility of policy updates.\nOur contributions are threefold: (1) We introduce the GBE method, designed to estimate finite-horizon non-discounted constraints in Safe RL tasks, along with relevant theoretical analysis. (2) We propose the CGPO algorithm. To our best knowledge, CGPO is the first deep Safe RL algorithm that can effectively address tasks with finite-horizon constraints. Based on precise estimations of the GBE method, CGPO ensures the efficiency and feasibility of each update. Our analysis includes worst-case scenarios and introduces an adaptive trust region radius strategy for improved performance. (3) We develop a series of Safe RL tasks with differentiable dynamics to test our algorithm. Comparative evaluations demonstrate our algorithm's superior update efficiency and constraint satisfaction against baselines."}, {"title": "2. Related Works", "content": "Differentiable RL. In many RL applications, the knowledge of the underlying transition function facilitates policy optimization through analytical gradients, a method known as Differentiable RL (Mohamed et al., 2020; Jaisson, 2022). The development of differentiable simulators (Degrave et al., 2019; Werling et al., 2021; Xian et al., 2023), which represent systems as differentiable computational graphs, significantly advance this research area. Some well-known differentiable RL algorithms include BPTT (Mozer, 1995), POSD (Mora et al., 2021), and SHAC (Jie Xu et al., 2022). Moreover, differentiable RL techniques extend to model-based algorithms that access analytical gradients from World Models (Clavera et al., 2020; As et al., 2022; Parmas et al., 2023b). Our algorithm, consistent with other Differentiable RL methods, depends on either a differentiable simulator or the World Model method for modeling task environments. Considering the swift advancements in related techniques, we view this requirement optimistically. For further discussion, see Section 7.\nSafe RL. The work closely related to ours focuses on on-policy deep Safe RL algorithms, which are categorized into Lagrangian and Convex Optimization methods (Xu et al., 2022). Lagrangian methods, such as PDO (Chow et al., 2018) and CPPO-PID (Stooke et al., 2020), alternate between addressing the primal and the dual problems. The latest APPO (Dai et al., 2023) mitigates the oscillatory issue by augmenting a simple quadratic term. Convex Optimization methods, such as CPO (Achiam et al., 2017) and CVPO (Liu et al., 2022), optimize based on the solution to the primal problem. The latest method, CUP (Yang et al., 2022), divides updates into two steps: updating in the steepest direction first, then mapping into the feasible domain if constraints are violated. Above all methods use estimates of objective and constraint functions to formulate surrogate problems (Achiam et al., 2017), which we refer to as Advantage-based Estimation. Thus, the accuracy of estimates impacts the efficiency and feasibility of updates.\nHowever, research on finite-horizon constraints has been limited to non-deep Safe RL contexts (Kalagarla et al., 2021; Guin & Bhatnagar, 2023). Despite their prevalence in the Safe RL Benchmark, aforementioned deep Safe RL algorithms treat these constraints as if they were infinite-horizon (such as Safety-Gym (Ray et al., 2019), Bullet-Safety-Gym (Gronauer, 2022), Safety-Gymnasium (Ji et al., 2023b), and OmniSafe (Ji et al., 2023c)). This leads to poor constraint satisfaction within these benchmarks."}, {"title": "3. Preliminaries", "content": "3.1. Constrained Markov Decision Process\nRL is typically framed as a Markov Decision Process (MDP) (Puterman, 2014), denoted M \u2261 \u3008S, A, r, P, \u03bc\u2080), which encompasses the state space S, the action space A, a reward function r, the transition probability P, and the initial state distribution \u03bc\u2080. A stationary policy \u03c0 represents a probability distribution defining the likelihood of taking action a in state s. The set \u03a0 symbolizes the collection of all such stationary policies. The primary objective of RL is to optimize the performance metric typically formulated as the total expected return over a finite horizon T, namely,\n$J_R(\\pi) = E_{\\tau \\sim \\pi} [\\sum_{t=0}^{T-1} r(s_t, a_t)].$\nGenerally, Safe RL is formulated as a Constrained MDP (CMDP) (Altman, 1999), MUC, augmenting the standard MDP M with an additional set of constraints C. This constraint set C = {(c\u1d62,b\u1d62)} consists of pairs of cost functions c\u1d62 and corresponding thresholds b\u1d62. The constraint function is defined as the cumulative cost over the horizon T, $J_{C_i}(\\pi) = E_{\\tau \\sim \\pi} [\\sum_{t=0}^{T-1} c_i(s_t, a_t)]$, and the feasible policy set is \u03a0c = \u2229\u1d62{\u03c0\u2208\u03a0 | $J_{C_i}(\u03c0) \u2264 b_i$}. The objective of Safe RL is to find the optimal feasible policy,\n\u03c0* = arg max\u03c0\u2208\u03a0c JR(\u03c0).\n3.2. Advantage-based Estimation Method\nBy introducing the discount factor \u03b3, we define the objective function for an infinite-horizon scenario as $J_R^\u03b3(\u03c0) = E_{\u03c4\\sim \u03c0} [\\sum_{t=0}^\u221e \u03b3^t r(s_t, a_t)]$ and the constraint function as $J_C^\u03b3(\u03c0) = E_{\u03c4\\sim \u03c0} [\\sum_{t=0}^\u221e \u03b3^t c_i(s_t, a_t)]$. We express the infinite-horizon value function as $V^\u03b3(s) = E_{\u03c4\\sim \u03c0} [\\sum_{t=0}^\u221e \u03b3^t r_t | s_0 = s]$ and the state-action value function as $Q^\u03b3(s,a) = E_{\u03c4\\sim \u03c0} [\\sum_{t=0}^\u221e \u03b3^t r_t | s_0 = s, a_0 = a]$. The infinite-horizon advantage function is $A^\u03b3(s,a) = Q^\u03b3(s, a) - V^\u03b3(s)$. The discounted future state distribution d\u03b3 is denoted as $d^\u03b3(s) = (1 \u2212 \u03b3) \\sum_{t=0}^\u221e \u03b3^t P(s_t = s | \u03c0)$. Then, the difference in some metrics between two policies \u03c0, \u03c0' can be derived as (Kakade & Langford, 2002):\n$J_R^\u03b3(\u03c0') \u2212 J_R^\u03b3(\u03c0) = \\frac{1}{1-\u03b3} E_{s \\sim d^\u03b3} [\\frac{\u03c0'(s,a)}{\u03c0(s,a)} A^\u03b3(s,a)],$ (1)\nwhere $J_R^\u03b3$ represents infinite-horizon $J_R$, $J_C$. Equation (1) is difficult to estimate since s \u223c d\u03b3 is unknown. Achiam et al. (2017) approximates s \u223c d\u03b3 with s \u223c d\u03c0 and employs importance sampling techniques to derive the following estimation:\n$J_R^\u03b3(\u03c0'); J_R^\u03b3(\u03c0) \u2248 \\frac{1}{1-\u03b3} E_{s \\sim d^\u03c0} [\\frac{\u03c0'(s,a)}{\u03c0(s,a)} \\frac{d^\u03c0(s)}{d^\u03b3(s)} A^\u03b3(s,a)],$ (2)\nwhere $d^\u03c0$ and $A^\u03c0(s, a)$ are both defined in the infinite-horizon format and it requires that \u03b3 \u2260 1. Thus, algorithms based on Equation (2) have to treat all constraints as if they were infinite-horizon.\n3.3. Policy Optimization with Differentiable Dynamics\nDifferentiable simulators (Freeman et al., 2021) represent physical rules using a differentiable computational graph, st+1 = F(st,at), allowing them to participate in gradient back-propagation process (Mohamed et al., 2020). The fundamental approach for policy optimization with differentiable physical dynamics is Back-propagation Through Time (BPTT) (Mozer, 2013). Multiple trajectories {$\u03c4\u1d62$} are collected from \u03c0 and then derivation is performed along the trajectory. Then, the loss function is\n$L_{BPTT} = \\frac{1}{NT}\\sum_{i=1}^{N} \\sum_{t=0}^{T-1} r(s_t^i, a_t^i).$ (3)\nThe Short-Horizon Actor-Critic (SHAC) approach (Jie Xu et al., 2022) modifies BPTT by ultilizing the value function V to segment trajectories into sub-windows of length h:\n$L_{SHAC} = \\frac{1}{N} \\sum_{i=1}^N [\\sum_{t=t_0}^{t_0+h-1} r(s_t^i, a_t^i) + V_R^\\phi(s_{t_0+h}^i)].$ (4)\nThis change limits the maximum length for gradient back-propagation to h, thereby mitigating the issues of gradient vanishing/exploding and making the training more stable."}, {"title": "4. Constrained Surrogate Problem using Gradient-based Estimation", "content": "Differentiable environments represent physical dynamics as differentiable computational graphs, allowing first-order gradients of objective and constraint functions to be derived via gradient back-propagation along trajectories. In this section, we propose a new estimation method based on this feature, called Gradient-based Estimation (GBE). It facilitates more accurate approximations for the next police's objective and constraint functions, which leads to a new constrained surrogate problem for solving Safe RL.\n4.1. Gradient-based Estimation for Objective and Constraint Functions\nConsider a parameterized policy \u03c0\u03b8 within a parameter space \u0398, e.g., represented by a neural network. In the context of differentiable environments, both the objective function JR(\u03b8) and the constraint function JC(\u03b8) can be regarded as differentiable over \u0398. Consequently, we can compute the first-order gradients of them along the trajectories, denoted as \u2207\u03b8JR(\u03b8) and \u2207\u03b8JC(\u03b8), through back-propagation. For simplicity and generality, we will focus on a single constraint scenario, though the method applies to multiple constraints via corresponding matrix operations.\nFor a minor update \u03b4 in the policy parameter space, transitioning from \u03b8\u2080 to \u03b8\u2080 + \u03b4, consider the function JF, which encapsulates both the objective function JR and the constraint function JC. We perform a first-order Taylor expansion of JF at \u03b8\u2080 to obtain:\n$J_F(\\theta_0 + \\delta) \u2248 J_F(\\theta_0) + \\delta^T \\nabla_{\\theta}J_F(\\theta_0) + o(||\\delta||^2).$ (5)\nWherein, $t \u2208 (0, 1)$ and $\u03b4^T\u2207_{J_F}(\u03b8_0 + t\u03b4)\u03b4$ represents the Peano remainder term, which is $o(||\u03b4||)$. When \u03b4 is sufficiently small, the remainder term becomes negligible. Therefore, we propose the Gradient-based Estimation method to estimate the values of the objective and constraint functions after a minor update \u03b4 from \u03b8\u2080 as follows:\n$\\hat{J}_R(\\theta_0 + \\delta) \u2248 J_R(\\theta_0) + \\delta^T \\nabla_{\\theta}J_R(\\theta_0),$ (6)\n$\\hat{J}_C(\\theta_0 + \\delta) \u2261 J_C(\\theta_0) + \\delta^T \\nabla_{\\theta}J_C(\\theta_0).$ (7)\nRegarding the error analysis for these estimates, the following lemma states:\nLemma 4.1. Assume \u03b8\u2080 \u2208 \u0398 and JF(\u03b8) is twice differentiable in a neighborhood surrounding \u03b8\u2080. Let \u03b4 be a small update from \u03b8\u2080. If we estimate JF(\u03b8\u2080 + \u03b4) as $\\hat{J}_F(\\theta_0 + \\delta) = J_F(\\theta_0) + \\delta^T \\nabla_{\\theta}J_F(\\theta_0)$ and given that $\\epsilon = \\text{max}_{t \\in (0,1)} |J''_R(\\theta_0 + t\u03b4)|$, the estimation error can be bounded as:\n$| \\hat{J}_F(\\theta_0 + \\delta) - J_F(\\theta_0 + \\delta) | \\leq \\frac{1}{2} \\epsilon ||\\delta||^2.$ (8)\nProof. The primary source of estimation error is the neglect of higher-order infinitesimal remainders. Thus, the error is $| \\hat{J}_F(\\theta_0 + \\delta) - J_F(\\theta_0 + \\delta) | = |\\delta^T \\nabla J_F(\\theta_0 + t\u03b4)\u03b4| \\leq \\frac{1}{2} \\epsilon ||\\delta||^2$. See Appendix A.1 for more details.\nThis finding indicates that as policy parameters are updated from \u03b8\u2080 to \u03b8\u2080 + \u03b4, the upper bounds of the estimation errors for both the objective and constraint functions are positively correlated with the square of the L2 norm of the update vector \u03b4. Consequently, when employing JR and JC as surrogate objective and constraint functions, meticulous control over the magnitude of these updates becomes essential. By carefully managing $||\u03b4||^2$, we could ensure that the estimation error remains within an acceptable range, thus facilitating both effective performance improvement and precise adherence to constraints.\n4.2. Constrained Surrogate Problem within Trust Region\nBased on the analysis of Theorem 4.1, the idea of controlling the error by managing $||\u03b4||^2$ naturally aligns with the concept of trust regions (Schulman et al., 2015; Meng et al., 2022). Given a trust region radius \u2642, we suppose that updated parameter \u03b8\u2096\u208a\u2081 of the kth iteration within the trust region \u0398\u2096 = {$\u03b8\u2208\u0398 | ||\u03b8 \u2013 \u03b8\u2096||\u00b2 \u2264 \u03b4$} are credible.\nBy employing the approximations of the objective function in Equation (6) and the constraint function in Equation (7), We transform the solution of the primal Safe RL problem into an iterative process of solving a series of sub-problems within predefined trust regions. Given the initial policy parameter \u03b8\u2096 of the kth iteration, our sub-problem targets finding the next optimal and credible parameter \u03b8\u2096\u208a\u2081 \u2208 \u0398\u2096, which not only maximize the surrogate objective function $J_R(\u03b8_{k+1})$ but also conform to the surrogate constraint $J_C(\u03b8_{k+1}) \u2264 b$. Thus, the surrogate sub-problem within a given trust region at kth iteration can be represented as:\n$\u03b8_{k+1} = \\text{arg max}_{\u03b8\u2208\u0398} \\nabla J_R(\u03b8_k)$\n$\\text{s.t.} \\quad J_C(\u03b8_k) + (\u03b8 - \u03b8_k)^T \\nabla J_C(\u03b8_k) \u2264 b $\n$\\quad \\quad (\u03b8 - \u03b8_k)^T (\u03b8 - \u03b8_k) \u2264 \u03b4,$ (9)"}, {"title": "5. Constrained Gradient-based Policy Optimization", "content": "Based on the constrained surrogate sub-problem in Equation (9), we develop a novel Safe RL algorithm named Constrained Gradient-based Policy Optimization (CGPO).\n5.1. Solution to Surrogate Sub-problem\nNotations. Considering the kth iteration within the trust region \u0398\u2096, we introduce additional notations to make the discussion more concise: $g_k \u2261 \u2207_\u03b8J_R(\u03b8_k)$, $q_k \u2261 \u2207_\u03b8I_C(\u03b8_k)$, $C_k \u2261 J_C(\u03b8_k) \u2013 b$, and \u03b4 \u2269 \u03b8 \u2013 \u03b8\u2096. With these definitions, we rewrite the sub-problem (9) within the trust region:\n$\\text{\u0442\u0430\u0445}_{\u03b4} \\quad g^T \u03b4 \\\\ \\text{s.t.} \\quad c_k + q^T \u03b4 \u2264 0, \\quad \u03b4^T\u03b4 \u2264 \u03b4$ (10)\nSince the sub-problem (10) may have no solution, we first discuss the conditions under which this problem is unsolvable. The following theorem holds:\nTheorem 5.1 (Solvability Conditions for the Sub-problem). The sub-problem (10) is unsolvable if and only if $c_k^2/q^T q_k - \u03b4 > 0$ and $c_k > 0$.\nProof. Consider whether there is an intersection between the trust region and the feasible half-space. For a detailed proof, refer to Appendix A.2.1.\nThrough a similar proof, we arrive at the following corollary:\nCorollary 5.2. 0 is deemed feasible for every \u0472 within Ok if and only if $c_k^2/q^T q_k - \u03b4 > 0$ and $c_k \u2264 0$ are both satisfied.\nBased on Theorem 5.1 and Corollary 5.2, we solve for $\u03b8_{k+1}$ of sub-problem (9) in three different scenarios, as illustrated in Figure 2.\n(a) If $c_k^2/q^T q_k - \u03b4 > 0$ and $c_k > 0$, the entire trust region is infeasible. We update the policy along the direction of the steepest descent of the constraint function, namely,\n$\u03b8_{k+1} = \u03b8_k - \\frac{\\sqrt{\u03b4}}{||q_k||} q_k$ (b) If $c_k^2/q^T q_k - \u03b4 > 0$ and $c_k \u2264 0$, the trust region lies entirely within the constraint-satisfying half-space. Similarly, we update along the direction of the steepest ascent of the objective function, namely,\n(33)\n(34)\n$\u03b8_{k+1} = \u03b8_k + \\frac{\\sqrt{\u03b4}}{||g_k||} g_k$.\n(c) In other cases, the trust region partially intersects with the feasible region, so we have to solve the constrained quadratic sub-problem (10).\nThe sub-problem (10) is characterized as convex. When the trust region is partially feasible, the existence of at least one strictly feasible point is guaranteed. Consequently, strong duality holds as the conditions of Slater's theorem are fulfilled. By introducing two dual variables \u03bb and \u03bd, we construct the dual function of the primal problem as follows:\n$L(\u03b4, \u03bb, \u03bd) = \u2212g_k^T \u03b4 + \u03bd(c_k + q_k^T \u03b4) + \u03bb (\u03b4^T \u03b4 \u2212 \u03b4)$ (35)\nSince the sub-problem (10) satisfies the strong duality condition, any pair of primal and dual optimal points $(\u03b4^*, \u03bb^*, \u03bd^*)$ must satisfy the KKT conditions, namely,\n$\u2207_\u03b4L(\u03b4, \u03bb, \u03bd) = \u2212g_k + \u03bdq_k + \u03bb\u03b4 = 0,$ (36)\n$\u03bd (c_k + q_k^T \u03b4) = 0,$\n$\u03bb (\u03b4^T \u03b4 \u2212 \u03b4) = 0,$\n$c_k+ q_k^T \u03b4 \u2264 0, \u03b4^T \u03b4 \u2212 \u03b4 \u2264 0, \u03bd \u2265 0, \u03bb \u2265 0$ (39)\n(37)\n(38)\nNote that the optimal dual variables $(\\lambda^*, \\nu^*)$ for Equation (12)-(15) can be directly expressed as a function of $g_k$ and $q_k$. We provide their pseudo-code in Algorithm 2. For a detailed solution process of Equation (12)-(15), refer to Appendix A.2.2. Then, following the derivation from Equation (12), we update the policy by $\u03b8_{k+1} = \u03b8_k + \\frac{g_k-\u03bd q_k}{\\lambda}$ \nSo far, the three scenarios for updating policies by solving the surrogate sub-problem (9) have been fully presented. This forms the core component of our CGPO algorithm.\n5.2. Worst-Case Analysis\nIn the following theorem, we detail the bounds for performance update and constraint violations under the worst-case scenarios after the policy update which results from solving the surrogate sub-problem (9).\nTheorem 5.3 (Worst-Case Performance Update and Constraint Violation). Suppose Ok, 0k+1 \u2208 \u04e8 are related by\n(16)\nEquation (9). If Ok is feasible, a lower bound on the policy performance difference between Ok+1 and Ok is\n$J_R(\\theta_{k+1}) \u2013 J_R(\\theta_k) \\geq - \\frac{1}{2} \\epsilon_R \u03b4,$\nwhere $\u03f5_R = \\text{max}_{t \\in (0,1)} |\u2207 J_R(\\theta_k + t(\\theta_{k+1} \u2013 \u03b8_k))|$.\nAn upper bound on the constraint objective function of \u03b8k+1 is\n(17)\n$J_C(\\theta_{k+1}) \u2264 b + \\frac{1}{2} \\epsilon_C \u03b4,$\nwhere $\u03f5_C = \\text{max}_{t \\in (0,1)} |\u2207 I_C(\\theta_k + t(\\theta_{k+1} \u2013 \u03b8_k))|$.\nProof. Let the feasible region of the sub-problem be denoted as\n(48)\n$\u0398_F = {\u03b8 \u2208 \u0398 | I_C (\u03b8_k) + (\u03b8 \u2212 \u03b8_k)^T \u2207_\u03b8 I_C(\u03b8_k) \u2264 b, ||\u03b8 \u2212 \u03b8_k||\u00b2 \u2264 \u03b4}$ .\nFurthermore, because\n(49)\n$\\theta_{k+1} = \\text{arg max}_{\u03b8\u2208\u0398_F} (\u03b8 \u2013 \u03b8_k)^T \u2207_\u03b8 J_R(\u03b8_k),$\nit follows that\n(50)\n$\u2200\u03b8 \u2208 \u0398_F, (\u03b8_{k+1} \u2212 \u03b8_k)^T \u2207_\u03b8 J_R(\u03b8_k) \u2265 (\u03b8 \u2013 \u03b8_k)^T \u2207_\u03b8 J_R(\u03b8_k).$\nSince Ok is feasible, it follows that \u03b8k \u2208 \u0398F. Consequently, we have\n(51)\n$(\u03b8_{k+1} \u2013 \u03b8_k)^T \u2207_\u03b8 J_R(\u03b8_k) \u2265 (\u03b8_k \u2013 \u03b8_k)^T \u2207_\u03b8 J_R(\u03b8_k) = 0$.\nFurther, we have $\u2203t_k \u2208 (0,1)$ such that\n(52)\n$J_R(\u03b8_{k+1}) - J_R(\u03b8_k) = (\u03b8_{k+1} - \u03b8_k) \u2207_\u03b8 J_R(\u03b8_k)+\\frac{1}{2}(\u03b8_{k+1} - \u03b8_k)^T \u2207^2 J_R(\u03b8_k + t_k (\u03b8_{k+1} - \u03b8_k))(\u03b8_{k+1} - \u03b8_k)$\n$\u2265 - \\frac{1}{2}(\u03b8_{k+1} - \u03b8_k) \u2207^2 J_R(\u03b8_k + t_k (\u03b8_{k+1} - \u03b8_k))(\u03b8_{k+1} - \u03b8_k)$\n$\u2265 - \\frac{1}{2} \\epsilon_R ||\u03b8_{k+1} - \u03b8_k||^2$\n$\u2265 - \\frac{1}{2} \\epsilon_R \u03b4$ \nwhere $\u03f5_R = \\text{max}_{t \\in (0,1)} |\u2207 J_R(\\theta_k + t(\\theta_{k+1} \u2013 \u03b8_k))|$.\nOn the other hand, since \u03b8k+1 is feasible, it follows that\n(53)\n$J_C(\u03b8_k) + (\u03b8_{k+1} \u2013 \u03b8_k)^T \u2207_\u03b8 J_C(\u03b8_k) \u2264 b$\n$(54)$"}, {"title": "Furthermore, we have", "content": "$I_C(\u03b8_{k+1"}, "I_C(\u03b8_k) + (\u03b8_{k+1} - \u03b8_k) \u2207_\u03b8 I_C(\u03b8_k) + \\frac{1}{2}(\u03b8_{k+1} - \u03b8_k)^T \u2207^2 I_C (\u03b8_k + t_k (\u03b8_{k+1} - \u03b8_k)) (\u03b8_{k+1} - \u03b8_k)$\n$\u2264 b + \\frac{1}{2}(\u03b8_{k+1} - \u03b8_k) \u2207^2 I_C(\u03b8_k + t_k (\u03b8_{k+1} - \u03b8_k)) (\u03b8_{k+1} - \u03b8_k)$\n$\u2264 b + \\frac{1}{2} |(\u03b8_{k+1} - \u03b8_k) \u2207^2 I_C(\u03b8_k + t_k (\u03b8_{k+1} - \u03b8_k)) (\u03b8_{k+1} - \u03b8_k)|$\n$\u2264 b + \\frac{1}{2} \\epsilon_C ||\u03b8_{k+1} - \u03b8_k||^2$\n$\u2264 b + \\frac{1}{2} \\epsilon_C \u03b4$ \nwhere $\u03f5_C = \\text{max}_{t \\in (0,1)} |\u2207 I_C(\\theta_k + t(\\theta_{k+1} \u2013 \u03b8_k))|$.\nBy combining Equation (52) and Equation (54), Corollary 5.3 is thus proven.\nA.4. Gradient Calculation\nGiven our focus on employing gradients to solve Safe RL problems, our algorithm can incorporate any differentiable method for computing gradients $g_\u03b8$ and $q_\u03b8$, such as BPTT (Mozer, 2013) and SHAC (Jie Xu et al., 2022). Here, we give an example of implementation using SHAC approach.\nWe compute the gradients of the objective function $J_R$ and constraint functions $J_C$ concerning policy \u03b8 using a methodology similar to SHAC (Jie Xu et al., 2022). While training the policy network, we concurrently train a reward critic network $V_R^\u03c6$ and a cost one $V_C^\u03c8$. We divide the entire trajectory into short-horizon sub-windows, each with a length of h. Subsequently, the gradients of the objective function (constraint function) are calculated by multi-step rewards (costs) within sub-windows plus a final value estimation from the corresponding learned critics. Specifically, considering the N-trajectory collection {$\u03c4^i$}$_{i=1}^N$ obtained from a given policy $a_t = \u03c0_\u03b8(s_t)$ and differentiable simulator $s_{t+1} = F(s_t, a_t)$, we employ the following two loss functions to compute gradients on a short-horizon sub-window:\n$L_R(\u03b8) = \\frac{1}{N} \\sum_{i=1}^N [\\sum_{t=t_0}^{t_0+h-1} r(s_t^i, a_t^i) + V_R^\u03c6 (s_{t_0+h}^i)"], "\u03c4_i$": "n$\\frac{\u2202 L_R(\u03b8)"}, {"order": "n$\\frac{\u2202 L_R(\u03b8)}{\u2202 s_t^i} = \\frac{1}{N} \\frac{\u2202 R(s_t^i, a_t^i)}{\u2202 s_t^i} + ((\\frac{\u2202 F(s_t^i, a_t^i)}{\u2202 s_t^i})^T ((\\frac{\u2202 L_R(\u03b8)}{\u2202 s_{t+1}^i})), (59)\n$\\frac{\u2202 L_C(\u03b8)}{\u2202 s_t^i} = \\frac{1}{N} \\frac{\u2202 C(s_t^i, a_t^i)}{\u2202 s_t^i} + ((\\frac{\u2202 F(s_t^i, a_t^i)}{\u2202 s_t^i})^T ((\\frac{\u2202 L_C(\u03b8)}{\u2202 s_{t+1}^i})), (60)\n$\\frac{\u2202 L_R(\u03b8)}{\u2202 a_t^i} = \\frac{1}{N} \\frac{\u2202 R(s_t^i, a_t^i)}{\u2202 a_t^i} + ((\\frac{\u2202 F(s_t^i, a_t^i)}{\u2202 a_t^i})^T ((\\frac{\u2202 L_R(\u03b8)}{\u2202 s_{t+1}^i})), (61)\n$\\frac{\u2202 L_C(\u03b8)}{\u2202 a_t^i} = \\frac{1}{N} \\frac{\u2202 C(s_t^i, a_t^i)}{\u2202 a_t^i} + ((\\frac{\u2202 F(s_t^i, a_t^i)}{\u2202 a_t^i})^T ((\\frac{\u2202 L_C(\u03b8)}{\u2202 s_{t+1}^i})).\nFrom all the computed adjoints, we can compute the objective loss and the constraint loss by\n$\\frac{\u2202 L_R(\u03b8)}{\u2202 \u03b8} = \\sum_{i=1}^N \\sum_{t=t_0}^{t_0+h-1} (\\frac{\u2202 L_R(\u03b8)}{\u2202 a_t^i} (\\frac{\u2202 \u03c0_\u03b8(s_t^i)}{\u2202 \u03b8})),$ (63)\n$\\frac{\u2202 L_C(\u03b8)}{\u2202 \u03b8} = \\sum_{i=1}^N \\sum_{t=t_0}^{t_0+h-1} (\\frac{\u2202 L_C(\u03b8)}{\u2202 a_t^i} (\\frac{\u2202 \u03c0_\u03b8(s_t^i)}{\u2202 \u03b8})).$\nA.5. The Update Method of Critic Network\nAs"}]