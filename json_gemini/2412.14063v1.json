{"title": "Rango: Adaptive Retrieval-Augmented Proving for Automated Software Verification", "authors": ["Kyle Thompson", "Nuno Saavedra", "Pedro Carrott", "Kevin Fisher", "Alex Sanchez-Stern", "Yuriy Brun", "Jo\u00e3o F. Ferreira", "Sorin Lerner", "Emily First"], "abstract": "Formal verification using proof assistants, such as Coq, enables the creation of high-quality software. However, the verification process requires significant expertise and manual effort to write proofs. Recent work has explored automating proof synthesis using machine learning and large language models (LLMs). This work has shown that identifying relevant premises, such as lemmas and definitions, can aid synthesis. We present Rango, a fully automated proof synthesis tool for Coq that automatically identifies relevant premises and also similar proofs from the current project and uses them during synthesis. Rango uses retrieval augmentation at every step of the proof to automatically determine which proofs and premises to include in the context of its fine-tuned LLM. In this way, Rango adapts to the project and to the evolving state of the proof. We create a new dataset, CoqStoq, of 2,226 open-source Coq projects and 196,929 theorems from GitHub, which includes both training data and a curated evaluation benchmark of well-maintained projects. On this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is 29% more theorems than the prior state-of-the-art tool Tactician. Our evaluation also shows that Rango adding relevant proofs to its context leads to a 47% increase in the number of theorems proven.", "sections": [{"title": "I. INTRODUCTION", "content": "The cost of poor software quality is alarmingly high, with estimates suggesting that it incurs trillions of dollars in losses annually in the United States alone [43]. Formal verification, which enables developers to mathematically prove that software adheres to its intended behaviors and specifications, has been shown to help improve software quality. Notably, a study investigating C compilers [92], among them CompCert [47], LLVM, and GCC, observed that the only compiler for which no bugs were found was CompCert [92], which is formally verified using the Coq proof assistant.\nWhile formal verification can lead to high-quality software, it is expensive. For example, the code required to verify CompCert is 8 times as large as the code implementing its functionality [47]. A promising line of work towards automating formal verification is to automatically generate the proofs using machine learning techniques [10], [24], [25], [45], [73], [74]. Within this research space, there has been a recent and exciting line of work on exploring large language models (LLMs) for proof generation [32], [36], [37], [70], [91]. Prior LLM-based approaches for proof automation have shown that premises, such as lemmas and definitions, are important to add to the context [54], [91]. This builds off of retrieval-augmented generation (RAG) [49] approaches, which use a separate search step to add context to an LLM. For the task of proof synthesis, we call this technique retrieval-augmented proving (RAP), where a separate search step retrieves relevant information for proving a given theorem. One limitation of prior approaches using RAP is that they do not fully exploit the local information that is available when synthesizing proofs.\nIn this paper, we show that an essential component of RAP is to also include similar proofs in the context not only at the beginning of the proof, but to continue to give the LLM sources of inspiration and knowledge, adapting to the evolving proof state. The intuition is that having similar proofs in the context of the LLM, as determined at each step in the proof, can help guide the LLM in the right direction. Our work also distinguishes itself within the broader scope of all machine-learning-based proof generation approaches by using both premises and proofs in an online setting.\nWe reify this idea of adding relevant proofs, not just premises, to the context of an LLM in an approach and tool called Rango. At each proof step, Rango first determines which proofs and premises from the current project are most relevant for generating the next step. By retrieving in-project information, Rango is able to learn local proof strategies, adapting itself to the current project. Then, the next step is generated using a language model where the most relevant proofs and premises are given as context in addition to the current proof script (the steps taken so far) and the proof state (a set of logical formulas describing the goals that remain to be proven). Furthermore, the assessment of which proofs are relevant is done at each step in the proof, thus being able to adapt to the evolving nature of the proof.\nTo train Rango effectively, we collected a new large corpus of Coq data, which we call CoqStoq. We mine this corpus from GitHub using the CoqPyt Python client for CoqLSP [13]. We split CoqStoq into training data and a benchmark, on which we evaluate Rango. The benchmark contains all of the projects from a previous benchmark, CoqGym [90], that compile in Coq 8.18. Additionally, it contains CompCert and four projects from the Coq Community repository that are committed to long-term maintenance [17].\nIn summary, this paper's main contributions are:\n\u2022 An approach, Rango, to synthesizing proofs that adds to the context of the LLM not just premises but also relevant proofs. In this way, Rango adapts to the project and to the proof state at each step.\n\u2022 A new dataset, CoqStoq, for proof synthesis in Coq, comprising 196,929 theorems, and 2,225,515 proof steps from 2,226 different GitHub repositories. The dataset is split into training data and an evaluation benchmark.\n\u2022 An evaluation on CoqStoq comparing Rango to three state-of-the-art systems, Proverbot9001 [73], Tactician [45], and Graph2Tac [10]. Our evaluation shows that Rango does better than these tools, by proving 29% more theorems than Tactician, 66% more than Proverbot9001 and 4% more than Graph2Tac. Our evaluation also shows that adding relevant proofs to the context in Rango is important, leading to a 47% increase in the number of theorems proven.\nWe release Rango, CoqStoq, all trained models appearing in this paper, and all of the code required to reproduce the experiments in this paper at this link: https://github.com/rkthomps/coq-modeling."}, {"title": "II. MOTIVATING EXAMPLE IN COQ", "content": "To motivate our approach, we explain how formal verification works in Coq, and then demonstrate through a real example how Rango helps automate the proof-writing process.\nIn Coq, a proof engineer can state a theorem about their code and then write a proof that the theorem holds true. Theorems in Coq are types, and so proving them true amounts to constructing a proof term of the same type in Coq's Gallina language. However, since writing that term directly is challenging, proof engineers typically write proof scripts in Coq's Ltac language, which consist of tactics, such as induction. When executed, these tactics guide Coq in the construction of a complete proof term. Coq provides feedback after each tactic application and displays the current proof state, which includes the goals left to prove and the local context of assumptions. The proof engineer knows that they have proven the theorem when there are no more goals.\nA proof development in Coq consists of theorems and their associated proof scripts, potentially across multiple files. A proof engineer may even prove a series of lemmas with the sole intention of using them to prove a main theorem. Across a proof development, proof engineers often reuse bits and pieces from their proofs. However, different proofs have meaningful differences in, say, which lemmas are used. When building tools to help automate a proof engineer's proving process, it is important to fully utilize the expertise provided by the proof engineer. We accomplish this by directly leveraging information from existing proofs.\nLet's take a closer look at the CompCert proof development, which is in CoqStoq's benchmark. The file Memdata.v has the following theorem.\nLemma memval_inject_compose:\nforall f g v1 v2 v3,\nmemval_inject f v1 v2 -> memval_inject g v2 v3 ->\nmemval_inject (compose_meminj f g) v1 v3.\nTo prove such a theorem, one can attempt to apply tactics step-by-step, considering the current proof state and context of assumptions. Alternatively, one can instead, at each step, also take inspiration from existing proofs in the project. This is the Rango approach. At each step, the Rango tactic prediction model draws inspiration from the following proof from the file Values.v in the CompCert proof development, just one of the many that it retrieves.\nLemma val_inject_compose: forall f g v1 v2 v3, Val.\ninject f v1 v2 -> Val.inject g v2 v3 -> Val.inject\n(compose_meminj f g) v1 v3.\nProof.\nintros. inv H; auto; inv HO; auto. econstructor.\nunfold compose_meminj; rewrite H1; rewrite H3; eauto.\nrewrite Ptrofs.add_assoc. decEq. unfold Ptrofs.add.\napply Ptrofs.eqm_samerepr. auto with ints.\nQed.\nRango updates its sources of inspiration and knowledge at each step, as Rango also adapts to the current proof state. The proof script for val_inject_compose would not work outright for the theorem in question, but a part of it is useful, and so Rango begins to write the proof of memval_inject_compose as follows.\nProof.\nintros. inv H; inv H0; econstructor.\nAs Rango generates the next tactic, it relies both on learned knowledge from fine-tuning and on retrieved knowledge in the form of lemmas and proofs in the project. Later in the proof, Rango retrieves the following proof from earlier in Memdata.v."}, {"title": "III. THE RANGO APPROACH", "content": "Rango synthesizes Coq proofs using relevant proofs and lemmas from the current project at every step of the proof, adapting to the project and to the state of the proof. To do this, Rango uses two subsystems. The first subsystem, the tactic generator, generates individual proof steps, or tactics. The second subsystem, the searcher (Section III-D), uses the tactic generator to search for a complete proof by composing generated tactics. Figure 1 illustrates the interaction between the tactic generator and the searcher.\nRango's tactic generator can be further broken down into three components. The first two components, the proof retriever (Section III-A) and the lemma retriever (Section III-B), deter- mine which proofs and lemmas, respectively, are relevant for generating the next tactic. The third component, the language model (Section III-C), takes as input relevant proofs, relevant lemmas, a proof script, and a proof state, and then generates the next tactic in the proof.\nIn the remainder of this section, we describe each of the three components in the tactic generator, and we describe the searcher."}, {"title": "A. Proof Retriever", "content": "The proof retriever selects relevant previously completed proofs in the current project to provide as context to the language model as it generates the next tactic. At a given proof step, the proof retriever selects from a set of proofs called the proof bank. The proof bank consists of proofs from earlier in the file, or from one of the file's dependencies. Only proofs from the current project are in the proof bank. At every proof step, the proof retriever selects the k most relevant proofs from the proof bank for generating the next tactic.\nTo find the k most relevant proofs at a given point in the proof, the proof retriever compares the current proof state, s, with proof states from proofs in the proof bank. The proof retriever defines the relevance of a proof P from the proof bank to be the maximum similarity between s and a proof state $s_i$ from P. Specifically, given a function similarity that determines the similarity between two proof states, the proof retriever defines the relevance of a proof in the proof bank as\nrelevance(P) = $\\underset{s_i \\in states(P)}{max}$ similarity(s, $s_i$)\nwhere states(P) is the set of proof states in P.\nTo determine the similarity between two proof states, the proof retriever uses the BM-25 information retrieval tech- nique [72]. Given a set of documents and a query, BM-25 assigns each document a score based on its relevance to the given query. BM-25 determines the relevance of a document by comparing the word frequencies from the document to the word frequencies from the query. If the document and the query have similar word frequencies, then BM-25 considers the document to be relevant. In its calculation, BM-25 down- weighs words that appear across many documents as these words are often not helpful for determining relevance. Note that BM-25 is a sparse retrieval technique because it retrieves based on word frequencies.\nWhen the proof retriever uses BM-25, \u201cdocuments\" corre- spond to proof states from proofs in the proof bank, and the \"query\" corresponds to the current proof state. The \u201cwords\" in a proof state are the identifiers used in the proof state. We then define similarity in terms of BM-25 as follows: the similarity between the current proof state s and a proof state from a proof in the proof bank $s_i$ is defined to be the relevance assigned to $s_i$ by BM-25.\nNote that information retrieval techniques other than BM-25 can be substituted into Rango's proof retriever. In Section V-E, we explore the use of retrieval techniques that rely on neural networks, known as dense retrieval techniques.\""}, {"title": "B. Lemma Retriever", "content": "The lemma retriever retrieves the statements of the lemmas previously defined in the current project that could be directly used in the current proof. Note that the lemma retriever does not retrieve the proofs of these lemmas, just the statements.\nThe structure of the lemma retriever is similar to the structure of the proof retriever. The lemma retriever has access to the lemma bank, which is defined as the set of lemmas appearing earlier in the current file, or in one of the file's dependencies. The lemma bank only considers lemmas from the current project.\nLike the proof retriever, the goal of the lemma retriever is to select the j most relevant lemmas. The lemma retriever uses the sparse retrieval algorithm TF-IDF [77] to assign a relevance score to each lemma in the lemma bank with respect to the current proof state. Note that in this case, the \"query\" given to TF-IDF is the current proof state, the set of \"documents\" are the lemmas in the lemma bank, and the \u201cwords\" in a lemma correspond to the identifiers in the lemma. Again, like in"}, {"title": "C. Language Model", "content": "At a given proof step, the language model generates the next tactic using the following inputs:\n\u2022 relevant proofs retrieved from the proof retriever\n\u2022 relevant lemmas retrieved from the lemma retriever\n\u2022 the theorem statement and the proof script thus far\n\u2022 the current proof state\nTo obtain a language model that can effectively use this information, we fine-tune a pretrained decoder-only LLM. We construct fine-tuning examples from a set of training projects (see Section IV). Each fine-tuning example consists of a prompt containing the four inputs mentioned above, and a target containing the next tactic. Since the language model is a decoder-only LLM, the inputs and targets are concatenated during fine-tuning. Following prior work [26], the loss is only computed over the target so that the model learns to conditionally generate the target given the input and not the input itself.\nWe create each training example exactly as we would during inference. That is, we run the proof retriever and lemma retriever at every proof step in our dataset, and then construct the prompt using the retrieved proofs and lemmas.\nNote that language models can only process a limited number of tokens. To account for this constraint, we allocate a maximum number of tokens to each of the four inputs in each training example. Furthermore, we allocate a maximum number of tokens that can be generated by the language model. We truncate each input as follows. We keep the largest whole number of relevant proofs that does not exceed the token limit. We do the same for relevant lemmas. We keep the longest suffix of the theorem statement and proof script that does not exceed the token limit. We keep the longest suffix of the proof state that does not exceed the token limit. We truncate the output at training time by keeping the longest prefix that does not exceed the token limit. At inference time, we limit the number of tokens that the model can generate."}, {"title": "D. Searcher", "content": "Given a tactic generator, the searcher attempts to find a sequence of tactics that completes the proof. To find a sequence of tactics, the searcher uses a procedure that we call rollout search. Rollout search consists of a sequence of rollouts, where in each rollout, the searcher samples a tactic from the tactic generator using temperature sampling. The searcher then appends the tactic to the current proof, and uses Coq to check the resulting proof attempt. The proof attempt will be in one of three states: complete, invalid, or incomplete. If the proof attempt is complete, meaning that Coq shows no more goals to be proven, then the search is successful. If the proof attempt is invalid, then the tactic sampled from the language model resulted in an error, and the searcher begins a new rollout. If the proof attempt is incomplete, the searcher continues the current rollout by sampling yet another tactic from the tactic generator. The searcher executes rollouts until it finds a complete proof, or until a timeout."}, {"title": "IV. THE COQSTOQ DATASET", "content": "As part of our work we created CoqStoq, a new dataset of Coq proofs mined from open-source GitHub projects. We collect theorems and their respective proofs from all open- source repositories that listed Coq as its primary language as of November 5th, 2023. We applied no other filters to our data collection (e.g., stars or number of contributors), since Coq itself ensures that successfully compiled files provide sound proof data.\nWe first attempted to automatically compile each repository by executing any existing Makefile or by compiling each individual file in the order specified by a _CoqProject file present in the repository. Then, we used CoqPyt [13] to validate each Coq file. We considered a file to be valid if it reports no errors during compilation. We only included repositories in our training dataset with at least 1 valid file. We note that both the compilation and validation steps are"}, {"title": "V. EVALUATION", "content": "To understand Rango's performance, we propose the follow- ing research questions:\nRQ1: How does Rango compare against other proof synthesis tools in Coq?\nRQ2: How do the proof retriever and the lemma retriever contribute to Rango's ability to synthesize proofs?\nRQ3: How does adapting proof retrieval to the current proof step contribute to Rango's ability to synthesize proofs?\nRQ4: How do alternative retrieval techniques perform in Rango's proof retriever?\nRQ5: How does Rango perform when it is instantiated with a na\u00efve retrieval algorithm?\nRQ6: How does Rango's rollout search compare to best-first search?\nRQ7: What kinds of theorems can Rango prove and how do the proofs generated by Rango compare to the proofs generated by other tools?"}, {"title": "A. Experimental Setup", "content": "Rango's language model is a fine-tuned version of the 1.3 billion parameter model DeepSeek-Coder [31]. We fine-tune the LLM on a set of examples gathered from the training projects in our dataset CoqStoq. We fine-tune for 60,000 training steps with a batch size of 16 on 4 NVIDIA A100 GPUs. We use 2 gradient accumulation steps so that our effective batch size is 32. We choose the checkpoint with the best loss on our validation set. We fine-tune using LoRA [34] and FSDP [65]. We use the Adam Optimizer [42] with a learning rate of 10-3. We allocate 1,024 tokens to retrieved proofs, 512 tokens to retrieved lemmas, 512 tokens to the theorem and proof script, 1,024 tokens to the proof state, and 128 tokens to the output.\nDuring proof synthesis, Rango is allocated a single NVIDIA RTX 2080 for inference, and a single CPU with 16GB of RAM for proof checking. We use a 10 minute timeout for all of our proof attempts. Our timeout does not include the initialization costs of loading and compiling the file. We use a temperature of 1.0 for sampling."}, {"title": "B. RQ1: Rango versus Other Tools", "content": "We evaluate Rango against three state-of-the-art proof synthesis tools for Coq: Proverbot9001 [73], Tactician [45], and Graph2Tac [10]. Proverbot9001 (which we refer to as Proverbot) uses a custom architecture involving several Gated Recurrent Units and feed-forward neural networks [73]. Graph2Tac also uses a custom architecture based on Graph Neural Networks. It uses this architecture both to predict which tactic to use next, and which definitions in the environment (including helper lemmas) should be given as an argument to the tactic. Therefore, Graph2Tac can retrieve helper lemmas and definitions from the environment just like Rango. Lastly, Tactician maintains a database of proof states which is defined similarly to Rango's proof bank. At every proof step, Tactician finds the most similar proof states in its database by comparing sets of identifiers using k-NN [45]. Then, Tactician performs a search by directly applying tactics that were used at similar proof states.\nWe evaluate Rango, Tactician, and Proverbot on all 12 projects from CoqStoq's benchmark using Coq 8.18. Note that neither Tactician nor Proverbot are built to utilize a GPU during their respective proof search procedures. Therefore, we run each tool using a single CPU. We run Tactician using a 10 minute timeout. Proverbot is configured to run with depth limits instead of timeouts, so for most of the reported proofs, Proverbot fails before 10 minutes. In Table II, we report the results for Rango, Tactician, and Proverbot on the 10,396 theorems in the CoqStoq benchmark. Rango finds 29% more proofs than Tactician, and 66% more proofs than Proverbot.\nIn Table II, we also report results for Graph2Tac. Note that since Graph2Tac is only compatible with Coq 8.11, we evaluate Graph2Tac on different project versions than Rango. Furthermore, there are only 3 of the projects in CoqStoq's benchmark that Graph2Tac was not directly trained on. Therefore, we can only fairly evaluate on these three projects. We only compare theorems whose statements match exactly between project versions. Note that this does not guarantee that a proof in one project version will translate to a proof in the other project version since other definitions in the project may have changed between versions. For each theorem, we ran Graph2Tac on a single NVIDIA RTX 2080 and a single CPU with a timeout of 10 mins. Keeping differences between project versions in mind, Rango proves 4% more theorems than Graph2Tac.\nIn the last row of Table II, we show the combined number of theorems proven between Rango and each other tool. We can see that each tool finds proofs for a significant subset of theorems where Rango could not find a proof. Running Rango alongside Tactician, Proverbot, and Graph2Tac leads to 16%, 12%, and 16% respective increases in the number of theorems proven over running Rango alone.\nOne limitation of the CoqStoq benchmark is that its projects were created before the pretraining cutoff of Rango's underlying"}, {"title": "C. RQ2: Contribution of Proof and Lemma Retrievers", "content": "To determine the contribution of the proof retriever and the lemma retriever to Rango's performance, we train one version of Rango without the proof retriever, one version without the lemma retriever, and one version with neither the proof retriever nor the lemma retriever. We evaluate each of these variants on a randomly selected subset of 500 theorems from CoqStoq's benchmark. We call this subset of theorems our ablation set. We report the percentage of theorems that each of these variants prove in Table IV.\nWe also investigate the contribution of Rango's proof and lemma retrievers to its ability to adapt to held-out projects. We show that Rango's proof and lemma retrievers capture information about a project that would otherwise need to be stored in the weights of the underlying LLM. We show this by first training a separate version of each variant on an inter-file split as opposed to CoqStoq's inter-project split. That is, we randomly split the files in the CoqStoq dataset into a training, validation, and testing set. Then, we train each variant on the training set of the inter-file split. This way, the inter-file versions of the variants have information about all CoqStoq projects in their weights. Then, we compare inter-file variants to inter-project variants on a subset of 500 theorems that are in the testing sets of both the inter-file split and the inter-project split. We show the results in Table V. From these results, we see that all variants benefited from having project-specific information in the weights of their underlying LLMs. However, the variants that did not have proof retrieval benefited more than variants that did have proof retrieval. Specifically, Rango without proofs and Rango without proofs and lemmas obtained increases of 26% and 43% in the number of theorems proven when they were trained on an inter-file split. In contrast, Rango and Rango without lemmas obtained more modest increases of 15% and 11%. This indicates that Rango's proof retriever captures a significant amount of the information that would be gained by training directly on files from the current project."}, {"title": "D. RQ3: Retrieving at each Proof Step", "content": "Recall that Rango's proof retrieval mechanism adapts to the current proof state by retrieving relevant proofs at every step in the proof search. To measure the effect of retrieving at each proof step, we compare Rango to a variant that only performs proof retrieval at the first step in the proof search. In other words, the variant retrieves proofs based on the theorem statement, and uses the same retrieved proofs throughout the search. We compare Rango with this variant on our ablation set and show the results in Table VI. We observe that Rango proves 35% more theorems by retrieving proofs at every proof step than it does by retrieving proofs solely at the beginning of the proof search. Thus, retrieving at every proof step is critical to Rango's success."}, {"title": "E. RQ4: Comparing Retrieval Algorithms for Proof Retrieval", "content": "We compare sparse retrieval techniques to dense retrieval techniques for retrieving proofs. Sparse retrieval techniques, such as BM-25, are those that retrieve information based on word counts. In contrast, dense retrieval techniques retrieve information based on vector embeddings derived from a neural network.\nNote that a comparison has been made between sparse retrieval techniques and dense retrieval techniques for the selection of premises [91]. However, the techniques for training models to select premises do not transfer to training models to retrieve proofs due to a difference in objectives. In premise selection, the objective is to predict whether or not a premise will be used in the next proof step. For proof retrieval, the objective is to retrieve the proofs that are most helpful for generating the next proof step. However, at training time, there is no way to know which proofs satisfy this objective. Therefore, standard supervised learning techniques are not applicable."}, {"title": "F. RQ5: Rango with Na\u00efve Retrieval Variant", "content": "Rango uses a proof retriever and a lemma retriever to gather relevant context for a fine-tuned LLM to use when generating the next tactic in a proof. A na\u00efve form of retrieving proofs and lemmas could use the lines directly preceding the theorem being proven as context to the LLM. We call this retrieval technique prefix retrieval. In this section, we explore Rango- PRE: a variant of Rango whose only retrieval mechanism is prefix retrieval. Table VIII shows Rango-PRE's performance on the CoqStoq benchmark, and on our two post-cutoff projects. Note that while Rango proves more theorems than Rango-PRE,"}, {"title": "G. RQ6: Searcher Variants", "content": "We also explore proof search alternatives in Rango's searcher. Recall from Section III-D that Rango uses rollout search to search for a complete proof using its tactic generator. One weakness of the rollout search is that it does not use previous proof attempts to inform subsequent proof attempts. In this section, we compare rollout search to a best-first search, which is standard in LLM proof search implementations [32], [36], [37], [69], [91]. In our best-first search, we maintain a set of candidate proofs. In each search step, we select the candidate with the highest score given by Rango's language model. Then, we use the tactic generator to generate b distinct possible next tactics. Each tactic corresponds to a new candidate proof. We continue the search in this way until we either find a complete proof, or the search times out. Note that this search algorithm guarantees that each explored proof is distinct. Finally, we do not include invalid proofs or redundant proofs [73] as candidates.\nWe compare the searcher used in Rango, based on rollout search to two best-first search configurations. In one configu- ration, the searcher samples tactics at each search step using beam decoding. In the other configuration, the searcher samples tactics using temperature sampling. In each configuration, the searcher samples b = 4 tactics at each search step. Note that temperature sampling does not guarantee that the sampled"}, {"title": "H. RQ7: Understanding Rango's Proofs", "content": "In this section, we investigate which kinds of theorems Rango can prove, and we compare the proofs generated by Rango to the proofs generated by other proof synthesis tools.\n1) Kinds of Theorems Rango can Prove: The strongest indicator that we have found for whether or not Rango can prove a theorem is the length of its corresponding human- written proof. We show the success rate of Rango, Tactician, and Proverbot by human-written proof length in Figure 5. For all proof synthesis tools, Figure 5 shows a sharp decrease in the percentage of theorems proven as the length of human-written proofs increases.\nA weaker indicator for whether or not Rango can prove a theorem is the number of dependencies of the file containing the theorem. We show the success rates for Rango, Tactician and Proverbot in Fig 6 for files with varying numbers of dependencies where a dependency is a Coq file that is imported either directly or transitively. We notice that the success rates for all three proof synthesis tools drop for files that have a hundred or more dependencies. Despite Rango's decreased success rate on files with many dependencies, it is still able to find more proofs than other tools. We speculate that Rango's retrieval mechanisms allow it to remain competitive in files with many dependencies. For example, consider the following theorem from the file Values.v in the CompCert proof development which has 300 dependencies.\nTheorem swap_cmpu_bool:\nforall valid_ptr c x y,\ncmpu_bool valid_ptr (swap_comparison c) x y =\ncmpu_bool valid_ptr c\u0443\u0445.\nRango finds the following proof for this theorem using the lemma Int.swap_cmpu which is defined in a different file, and is not used anywhere in Values.v prior to this theorem.\nProof.\ndestruct x; destruct y; simpl; auto.\nrewrite Int.swap_cmpu. auto.\nQed.\nIn this case, Rango's lemma retriever identified Int.swap_cmpu as a relevant lemma for this proof, and its proof retriever found proofs that inspired this proof's structure."}, {"title": "I. Threats to Validity", "content": "A threat to internal validity is that while the pretraining data for LLMs often consist of data from public repositories, such as GitHub, it is not publicly known what is in the pretraining data for the LLM we fine-tune for Rango, DeepSeek-Coder 1.3B. Since CoqStoq's benchmark was taken from GitHub, it is possible that it intersects with the LLM's pretraining data. Most evaluations involving LLMs suffer from this test set contamination problem. We mitigate this issue by evaluating on two projects, Coq-BB5 and PnVRocqLib, that were created after the pretraining cutoff for DeepSeek-Coder 1.3B.\nAnother threat to internal validity is that, in our comparison of Rango to other tools, there are some differences in the Coq versions and machines used (CPU vs GPU). The evaluations of Rango, Tactician, and Proverbot all use Coq 8.18, and are all run on CoqStoq's benchmark. However, Graph2Tac only runs in Coq 8.11. We evaluated Rango and Graph2Tac on GPUs, while Tactician and Proverbot were run on CPUs since they are not intended to use GPUs."}, {"title": "VI. RELATED WORK", "content": "Formal verification aims to improve software quality, a prob- lem that takes up 50\u201375% of the total software development budgets [63]. Other methods of improving software quality include debugging [11], [40], [93], and testing [5], but only formal verification can guarantee code correctness. Automated techniques can similarly improve program quality [3], [39], [46], [51], [57], [86], [96], and can also help developers debug manually [21], but still do not guarantee correctness, and, in fact, often introduce new bugs [58], [76].\nRecent work in automating theorem proving in proof assistants, such as Coq [80], Lean [20], Isabelle [62], and Metamath [52], has focused on machine-learning based ap- proaches. Typically, with a machine learning approach, neural theorem prover uses a model to predict the next tactic to apply, which guides a search through the space of possible proofs. Early work explored the use of LSTM [24], [25], [90], RNN [35], [73], and GNN-based models [66]. Recent work has focused on the use of LLMs in neural theorem proving, either fine-tuning models [32], [36], [37], [70], [91] or prompting pretrained ones [8], [38], [79], [95].\nLarge foundation models have demonstrated incredible capabilities on a wide range of tasks [16], [64], [78], [82]. To adapt to knowledge-intensive tasks and new domains, researchers have explored the use of retrieval-augmented generation (RAG) [49] to boost performance [15], including sparse [72], [77] and dense [41] retrieval techniques.\nRecent work has explored retrieval augmentation for theorem proving, where they train models to retrieve premises, such as lemmas and definitions, that are relevant to the current proof goal, and then condition the next tactic generation on those premises [54], [91]. LeanDojo [91] trains a model to select which premises should be included as input to its LLM tactic generation model. Magnushammer [54], for Isabelle, takes this approach one step further and additionally trains a \u201creranker", "9": "with the aim of training a reinforcement learning approach to theorem proving without access to human-written proofs, which our approach uses.\nMost similar to our work are Graph2Tac"}]}