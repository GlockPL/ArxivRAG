{"title": "GENERATING HIGH-QUALITY SYMBOLIC MUSIC USING FINE-GRAINED DISCRIMINATORS", "authors": ["Zhedong Zhang", "Liang Li", "Jiehua Zhang", "Zhenghui Hu", "Hongkui Wang", "Chenggang Yan", "Jian Yang", "Yuankai Qi"], "abstract": "Existing symbolic music generation methods usually utilize discriminator to improve the quality of generated music via global perception of music. However, considering the complexity of information in music, such as rhythm and melody, a single discriminator cannot fully reflect the differences in these two primary dimensions of music. In this work, we propose to decouple the melody and rhythm from music, and design corresponding fine-grained discriminators to tackle the aforementioned issues. Specifically, equipped with a pitch augmentation strategy, the melody discriminator discerns the melody variations presented by the generated samples. By contrast, the rhythm discriminator, enhanced with bar-level relative positional encoding, focuses on the velocity of generated notes. Such a design allows the generator to be more explicitly aware of which aspects should be adjusted in the generated music, making it easier to mimic human-composed music. Experimental results on the POP909 benchmark demonstrate the favorable performance of the proposed method compared to several state-of-the-art methods in terms of both objective and subjective metrics. More demos are available at https://zzdoog.github.io/fg-discriminators/.", "sections": [{"title": "Introduction", "content": "Due to the high-level representation of music based on Musical Instrument Digital Interface (MIDI) and its variants, symbolic music generation models do not need to learn how to create the sounds of various instruments so that they can focus more on the music itself [12,17,2]. Since the high-level discrete tokens of music are similar to words of text, transformer-based models [9,8,10,18] have been widely applied in symbolic music generation, and towards the goal of generating high-quality music in recent years. Most symbolic music generation models are trained to maximize the likelihood of observed sequences. These methods can learn the patterns of discrete token sequences and ensure statistical consistency, but they may suffer from noticeable quality degradation when generating complex music sequences due to exposure bias [16]."}, {"title": "METHODOLOGY", "content": "The proposed model consists of an auto-regressive symbolic music generator and two fine-grained discriminators as shown in Fig. 2. First, the generator takes a representative condition music sequence c as the input, and attempts to generate whole music sequence align with the condition. Then, during the generator optimization, the fine-grained melody and rhythm discriminators provide more precise feedback to the generator by decoupling and analyzing the output of the generator. Simultaneously, the fine-grained discriminators continually enhance their discriminatory abilities relying on samples generated by the evolving generator to provide further feedback to the generator. The value function of the generator and fine-grained melody and rhythm discriminators are defined as\n$\\min \\max V = {E[\\log D_m(s_r)] + E[\\log D_r(S_r)]+\nE[\\log(1 - D_m(G_\\Theta(c)))] + E[\\log(1 \u2013 D_r(G_\\Theta(c)))]}$,\nwhere the G, $D_m$, and $D_r$ denote the generator, melody discriminator, and rhythm discriminator respectively. $\\Theta$ and $s_r$, denote the parameter of the generator and real sample from the dataset respectively."}, {"title": "Generator", "content": "We adopt the seq2seq symbolic music generation transformer model [19] as our generator. It takes condition music sequence as input and generates a complete and harmonious music composition that aligns with the input. The condition music sequence is the thematic material of each music composition, implies the main idea of the whole composition, retrieved from the complete music by clustering algorithm [19]. The overall loss function of the generator as follows:\n$L_G = L_{NLL} + \\alpha\\cdot L_{adv\\_Melody} + \\beta\\cdot L_{adv\\_Rhythm}$,\n$L_{NLL} = \\sum_{n=1}^N -\\log P(x_n|\\Theta, \\Sigma X_{1:n-1}, C)$,\nwhere the a and $\\beta$ are pre-defined hyper-parameters. Details of the two adversarial losses are in the following sections. Note that our fine-grained discriminators architecture applies equally to other state-of-the-art music generation methods."}, {"title": "Melody Discriminator with Pitch Augmentation Strategy", "content": "Melody is one of the primary properties of music. It provides a tuneful and recognizable musical line that serves as a focal point for listeners. The arrangement of pitches in a particular order and duration forms the melody [15]. Traditional NLL-trained models perform poorly in generating long and harmonious melodies due to the lack of specific guidance. To deal with this issue, we propose a melody discriminator with a pitch augmentation strategy to facilitate the discrimination of the melody in generated music.\nFirst, we decouple the melody information from symbolic music by replacing all the [Note-Velocity] tokens with the [mask] token. Then, to enhance our melody discriminator, we augment the original data via uniformly raising or decreasing the absolute pitch of all original notes to simulate the melody in different voice parts, as shown in Fig. 2 top right. All these decoupled melody data are fed into the melody discriminator which uses an encoder-only transformer with a multi-head self-attention mechanism as backbone [21]. During the adversarial training process, the adversarial loss from the melody discriminator and back-propagation gradients to the melody discriminator are formulated as :\n$L_{adv\\_Melody} = \\frac{1}{N} \\sum_{i=1}^N [\\log(1 \u2013 D_m(G_\\Theta(c^{(i)})))]$,\n$\\nabla_{\\Theta m} \\frac{1}{N} \\sum_{i=1}^N [\\log(D_\\Theta(s^{(i)})) + \\log(1 \u2013 D_m(G_\\Theta(c^{(i)})))]$,\nwhere $\\Theta m$ denotes the parameter of melody discriminator, $s^{(i)}$ and $c^{(i)}$ denote as i-th ground truth and conditional input."}, {"title": "Rhythm Discriminator with Bar-level Relative Positional Encoding", "content": "In addition to melody, rhythm is another crucial property of music, as it reflects the progression of notes and variations in velocity, governing the dynamics of music [7]. To improve the quality in terms of rhythm, we design a fine-grained rhythm discriminator.\nTo facilitate the discriminator to focus on rhythms instead of other music elements, we decouple rhythm information from the music by replacing the [Note-On-Pitch] token with the [Mask] token. Apart from that, we observe that the symbol \"bar\" plays a fundamental role in organizing and structuring music, which therefore can help establish the rhythmic framework of the music [14]. Based on this observation, we introduce a bar-level relative positional encoding as shown in Fig. 3. It accumulates position starting from the beginning of each bar and resets at the beginning of the next bar, i.e., from [Bar] token to next [Bar] token, embeds the bar-level relative position information into the decoupled music rhythm sequence. Like other relative position embedding implementations, our bar-level position embedding is also learnable. The general position encoding of a symbolic music token, e.g., t-th in the whole sequence and x-th within the current bar, is defined as follows:\n$A_{t,x} = cos/sin(t/1000^{2i/d}) + W_{BRPE} [\\delta(x, 1), \\delta(x, 2), . . . ]$,\nwhere the first part is traditional sine and cosine position encoding in the Transformer, the $W_{BRPE}$ is a learnable matrix, and $\\delta()$ is dirichlet function. The rhythm discriminator shares similar back-propagation and adversarial loss to the generator as the melody discriminator in Equation (4)."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "Experimental Setting", "content": "Dataset and preprocess. We employ the POP909 dataset [22] for performance evaluation. There are three separate tracks in each arrangement in the dataset: MELODY, BRIDGE and PIANO. To encode a MIDI file into a sequence of discrete tokens, we adopt the REMI-like [10] encoding method. In detail, we use metric-related tokens [Bar], [Tempo], [Position] and note-related tokens [Note-On-Pitch], [Note-Duration] and [Note-Velocity] to represent music, as shown in the generator part of Fig. 2. For fair comparisons, we retrain all the baseline models using the same data as ours, and reserve 4% of them only for evaluation where all models take the same music piece as the condition or the prefix sequence.\nImplementation Details. The proposed melody and rhythm discriminators use a 6-layer encoder-only Transformer as the backbone. Both of them have 8 heads for multi-head attention, 256 hidden dimensions, 1,024-dim feed-forward layers, and ReLU as the activation function. In the first stage, we pre-train the generator along with all baseline models using Adam optimizer (\u03b2\u2081=0.9 and \u03b22=0.99) [13] until the training NLL loss model below 0.55. Afterward, we pre-train the melody and rhythm discriminator using the dataset and the output of the trained generator for 120 epochs. During adversarial training, both a and \u03b2 are set to 0.15, and using the same optimizer in the first stage to train the generator for 100 epochs.\nBaselines. 1) GT [22]: the above-mentioned 4% of the dataset which is not included in the training set or validation set. 2) Music Transformer (MT) [9]: pioneer algorithm that successfully applied the transformer model to the domain of symbolic music generation. 3) Theme Transformer (TT) [19]: a theme-conditioned music generation model optimized by NLL loss only. 4) Anticipatory Music Transformer (AMT) [20]: the current state-of-art model for piano music generation based on transformer. 5) WGAN [25]: music generation model that utilizes a conventional global discriminator which will primarily serve to validate the effectiveness of our proposed fine-grained discriminator approach. 6,7) Ours (wRo) & Ours (wMo): our model uses only rhythm discriminator or melody discriminator in the adversarial training. 8) Ours: the complete fine-grained discriminator model."}, {"title": "Objective Evaluation", "content": "Evaluation Metrics. We employ various metrics to demonstrate the comprehensive performance of the models. First, following [23,3], we adopt 1) pitch class entropy, 2) scale consistency, and 3) groove consistency to evaluate entropy of the normalized note pitch class histogram, largest pitch-in-scale rate over all major and minor scales, and mean hamming distance of the neighboring measures. Then, we calculate the 4) pitch and 5) velocity divergence between the generated and real music to measure the distribution similarity in melody and rhythm respectively. Furthermore, we utilize a pre-trained music understanding model MIDI-BERT [1] and transform the music into feature vectors. The cosine 6) MIDI-BERT similarity between generated and real music can measure the proximity of generated music to real music in a higher-level feature."}, {"title": "Subjective Evaluation", "content": "To assess the quality of music samples generated by our model, we conduct a listening test with 17 survey participants. Ten of them can play at least one musical instrument and understand basic music theory. We provide 6 sets of 30 music samples for participants, consisting of ground truth and samples generated by each model. All generated MIDI files are rendered to audio using MuseScore General SoundFont [3]. In the questionnaire, each participant is asked to listen to all 30 samples and then rate them on a scale of 1 to 5 according to three criteria coherence, richness, and overall. Results are reported in Table 2.\nThe results show that our model achieves higher scores across all criteria than other models. It's worth noticing that while Music Transformer [9] surpasses our model in terms of the groove consistency metric in objective evaluation, it is less favorable than our model in terms of coherence and richness in subjective listening tests, especially richness. Based on the feedback from survey participants, we find that the music generated by Music Transformer contains a larger amount of repetition, leading to a monotonous listening experience. Benefiting from the fine-grained adversarial optimization, our model outperforms the SOTA single discriminator method WGAN. The performance improvements demonstrate the effectiveness of our method on both coherence and richness aspects and overall quality."}, {"title": "Ablation Studies and Qualitative Evaluation", "content": "As shown in Table 1, when using only the fine-grained melody discriminator, our method has shown a significant improvement compared to other baseline models in metrics strongly related to melody such as pitch class entropy, scale consistency, and pitch divergence, reaching a closer level to real music. Moreover, since melody and pitch are the core components of music expression [24], the melody discriminator enables the model to generate more realistic music, as indicated by the outstanding MIDI-BERT similarity. When solely using the fine-grained rhythm discriminator, our method also achieves better performance than baseline models in velocity divergence and MIDI-BERT similarity, proving the effectiveness of both fine-grained discriminators."}, {"title": "Conclusion", "content": "This work proposes a fine-grained discriminators architecture for the symbolic music generation task. We decouple the music into melody and rhythm for independent discrimination, which provides the generator with more specific feedback. We also devise a pitch augment strategy and a bar-level relative positional encoding scheme to enhance the learning of melody discriminator and rhythm discriminator, respectively. Extensive objective and subjective results demonstrate the effectiveness of the proposed method."}]}