{"title": "GENERATING HIGH-QUALITY SYMBOLIC\nMUSIC USING FINE-GRAINED\nDISCRIMINATORS", "authors": ["Zhedong Zhang", "Liang Li", "Jiehua Zhang", "Zhenghui Hu", "Hongkui Wang", "Chenggang Yan", "Jian Yang", "Yuankai Qi"], "abstract": "Existing symbolic music generation methods usually utilize\ndiscriminator to improve the quality of generated music via global per-\nception of music. However, considering the complexity of information in\nmusic, such as rhythm and melody, a single discriminator cannot fully\nreflect the differences in these two primary dimensions of music. In this\nwork, we propose to decouple the melody and rhythm from music, and\ndesign corresponding fine-grained discriminators to tackle the aforemen-\ntioned issues. Specifically, equipped with a pitch augmentation strategy,\nthe melody discriminator discerns the melody variations presented by\nthe generated samples. By contrast, the rhythm discriminator, enhanced\nwith bar-level relative positional encoding, focuses on the velocity of gen-\nerated notes. Such a design allows the generator to be more explicitly\naware of which aspects should be adjusted in the generated music, mak-\ning it easier to mimic human-composed music. Experimental results on\nthe POP909 benchmark demonstrate the favorable performance of the\nproposed method compared to several state-of-the-art methods in terms\nof both objective and subjective metrics. More demos are available at\nhttps://zzdoog.github.io/fg-discriminators/.", "sections": [{"title": "Introduction", "content": "Due to the high-level representation of music based on Musical Instrument Dig-\nital Interface (MIDI) and its variants, symbolic music generation models do not\nneed to learn how to create the sounds of various instruments so that they can\nfocus more on the music itself [12,17,2]. Since the high-level discrete tokens of mu-\nsic are similar to words of text, transformer-based models [9,8,10,18] have been\nwidely applied in symbolic music generation, and towards the goal of generat-\ning high-quality music in recent years. Most symbolic music generation models\nare trained to maximize the likelihood of observed sequences. These methods\ncan learn the patterns of discrete token sequences and ensure statistical consis-\ntency, but they may suffer from noticeable quality degradation when generating\ncomplex music sequences due to exposure bias [16]."}, {"title": "2 METHODOLOGY", "content": "The proposed model consists of an auto-regressive symbolic music generator and\ntwo fine-grained discriminators as shown in Fig. 2. First, the generator takes a\nrepresentative condition music sequence c as the input, and attempts to gener-\nate whole music sequence align with the condition. Then, during the generator\noptimization, the fine-grained melody and rhythm discriminators provide more\nprecise feedback to the generator by decoupling and analyzing the output of the\ngenerator. Simultaneously, the fine-grained discriminators continually enhance\ntheir discriminatory abilities relying on samples generated by the evolving gen-\nerator to provide further feedback to the generator. The value function of the\ngenerator and fine-grained melody and rhythm discriminators are defined as"}, {"title": "2.1 Generator", "content": "We adopt the seq2seq symbolic music generation transformer model [19] as our\ngenerator. It takes condition music sequence as input and generates a complete\nand harmonious music composition that aligns with the input. The condition\nmusic sequence is the thematic material of each music composition, implies the\nmain idea of the whole composition, retrieved from the complete music by clus-\ntering algorithm [19]. The overall loss function of the generator as follows:\n$L_G = L_{NLL} + \\alpha \\cdot L_{adv\\_Melody} + \\beta \\cdot L_{adv\\_Rhythm}$,\n$L_{NLL} = \\sum_{n=1}^{N} -logP(x_n|\\theta, \\sum X_{1:n-1}, c)$,\nwhere the a and \u03b2 are pre-defined hyper-parameters. Details of the two adversar-\nial losses are in the following sections. Note that our fine-grained discriminators\narchitecture applies equally to other state-of-the-art music generation methods."}, {"title": "2.2 Melody Discriminator with Pitch Augmentation Strategy", "content": "Melody is one of the primary properties of music. It provides a tuneful and rec-\nognizable musical line that serves as a focal point for listeners. The arrangement\nof pitches in a particular order and duration forms the melody [15]. Traditional\nNLL-trained models perform poorly in generating long and harmonious melodies\ndue to the lack of specific guidance. To deal with this issue, we propose a melody\ndiscriminator with a pitch augmentation strategy to facilitate the discrimination\nof the melody in generated music.\nFirst, we decouple the melody information from symbolic music by replac-\ning all the [Note-Velocity] tokens with the [mask] token. Then, to enhance\nour melody discriminator, we augment the original data via uniformly raising\nor decreasing the absolute pitch of all original notes to simulate the melody in\ndifferent voice parts, as shown in Fig. 2 top right. All these decoupled melody\ndata are fed into the melody discriminator which uses an encoder-only trans-\nformer with a multi-head self-attention mechanism as backbone [21]. During the\nadversarial training process, the adversarial loss from the melody discriminator"}, {"title": "2.3 Rhythm Discriminator with Bar-level Relative Positional\nEncoding", "content": "In addition to melody, rhythm is another crucial property of music, as it re-\nflects the progression of notes and variations in velocity, governing the dynamics\nof music [7]. To improve the quality in terms of rhythm, we design a fine-grained\nrhythm discriminator.\nTo facilitate the discriminator to focus on rhythms instead of other mu-\nsic elements, we decouple rhythm information from the music by replacing the\n[Note-On-Pitch] token with the [Mask] token. Apart from that, we observe\nthat the symbol \"bar\" plays a fundamental role in organizing and structuring"}, {"title": "3 EXPERIMENTS", "content": null}, {"title": "3.1 Experimental Setting", "content": "Dataset and preprocess. We employ the POP909 dataset [22] for performance\nevaluation. There are three separate tracks in each arrangement in the dataset:\nMELODY, BRIDGE and PIANO. To encode a MIDI file into a sequence of\ndiscrete tokens, we adopt the REMI-like [10] encoding method. In detail, we\nuse metric-related tokens [Bar], [Tempo], [Position] and note-related tokens\n[Note-On-Pitch], [Note-Duration] and [Note-Velocity] to represent music,\nas shown in the generator part of Fig. 2. For fair comparisons, we retrain all the\nbaseline models using the same data as ours, and reserve 4% of them only for\nevaluation where all models take the same music piece as the condition or the\nprefix sequence.\nImplementation Details. The proposed melody and rhythm discriminators\nuse a 6-layer encoder-only Transformer as the backbone. Both of them have 8\nheads for multi-head attention, 256 hidden dimensions, 1,024-dim feed-forward\nlayers, and ReLU as the activation function. In the first stage, we pre-train the\ngenerator along with all baseline models using Adam optimizer (\u03b2\u2081=0.9 and\n\u03b22=0.99) [13] until the training NLL loss model below 0.55. Afterward, we pre-\ntrain the melody and rhythm discriminator using the dataset and the output of\nthe trained generator for 120 epochs. During adversarial training, both a and\n\u03b2 are set to 0.15, and using the same optimizer in the first stage to train the\ngenerator for 100 epochs.\nBaselines. 1) GT [22]: the above-mentioned 4% of the dataset which is not in-\ncluded in the training set or validation set. 2) Music Transformer (MT) [9]:\npioneer algorithm that successfully applied the transformer model to the domain\nof symbolic music generation. 3) Theme Transformer (TT) [19]: a theme-\nconditioned music generation model optimized by NLL loss only. 4) Antici-\npatory Music Transformer (AMT) [20]: the current state-of-art model for"}, {"title": "3.2 Objective Evaluation", "content": "Evaluation Metrics. We employ various metrics to demonstrate the compre-\nhensive performance of the models. First, following [23,3], we adopt 1) pitch\nclass entropy, 2) scale consistency, and 3) groove consistency to evaluate\nentropy of the normalized note pitch class histogram, largest pitch-in-scale rate\nover all major and minor scales, and mean hamming distance of the neighbor-\ning measures. Then, we calculate the 4) pitch and 5) velocity divergence\nbetween the generated and real music to measure the distribution similarity in\nmelody and rhythm respectively. Furthermore, we utilize a pre-trained music un-\nderstanding model MIDI-BERT [1] and transform the music into feature vectors.\nThe cosine 6) MIDI-BERT similarity between generated and real music can\nmeasure the proximity of generated music to real music in a higher-level feature."}, {"title": "3.3\nSubjective Evaluation", "content": "To assess the quality of music samples generated by our model, we conduct a\nlistening test with 17 survey participants. Ten of them can play at least one\nmusical instrument and understand basic music theory. We provide 6 sets of 30\nmusic samples for participants, consisting of ground truth and samples generated\nby each model. All generated MIDI files are rendered to audio using MuseScore\nGeneral SoundFont [3]. In the questionnaire, each participant is asked to listen\nto all 30 samples and then rate them on a scale of 1 to 5 according to three\ncriteria coherence, richness, and overall. Results are reported in Table 2.\nThe results show that our model achieves higher scores across all criteria than\nother models. It's worth noticing that while Music Transformer [9] surpasses our\nmodel in terms of the groove consistency metric in objective evaluation, it is less\nfavorable than our model in terms of coherence and richness in subjective listen-\ning tests, especially richness. Based on the feedback from survey participants, we\nfind that the music generated by Music Transformer contains a larger amount\nof repetition, leading to a monotonous listening experience. Benefiting from the\nfine-grained adversarial optimization, our model outperforms the SOTA single\ndiscriminator method WGAN. The performance improvements demonstrate the\neffectiveness of our method on both coherence and richness aspects and overall\nquality."}, {"title": "3.4\nAblation Studies and Qualitative Evaluation", "content": "As shown in Table 1, when using only the fine-grained melody discriminator,\nour method has shown a significant improvement compared to other baseline"}, {"title": "4 Conclusion", "content": "This work proposes a fine-grained discriminators architecture for the symbolic\nmusic generation task. We decouple the music into melody and rhythm for in-\ndependent discrimination, which provides the generator with more specific feed-\nback. We also devise a pitch augment strategy and a bar-level relative positional\nencoding scheme to enhance the learning of melody discriminator and rhythm\ndiscriminator, respectively. Extensive objective and subjective results demon-\nstrate the effectiveness of the proposed method."}]}