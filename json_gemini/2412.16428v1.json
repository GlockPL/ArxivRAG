{"title": "Data-Driven Fairness Generalization for Deepfake Detection", "authors": ["Uzoamaka Ezeakunne", "Chrisantus Eze", "Xiuwen Liu"], "abstract": "Despite the progress made in deepfake detection research, recent studies have shown that biases in the train-\ning data for these detectors can result in varying levels of performance across different demographic groups,\nsuch as race and gender. These disparities can lead to certain groups being unfairly targeted or excluded.\nTraditional methods often rely on fair loss functions to address these issues, but they under-perform when\napplied to unseen datasets, hence, fairness generalization remains a challenge. In this work, we propose a\ndata-driven framework for tackling the fairness generalization problem in deepfake detection by leveraging\nsynthetic datasets and model optimization. Our approach focuses on generating and utilizing synthetic data\nto enhance fairness across diverse demographic groups. By creating a diverse set of synthetic samples that\nrepresent various demographic groups, we ensure that our model is trained on a balanced and representative\ndataset. This approach allows us to generalize fairness more effectively across different domains. We employ\na comprehensive strategy that leverages synthetic data, a loss sharpness-aware optimization pipeline, and a\nmulti-task learning framework to create a more equitable training environment, which helps maintain fairness\nacross both intra-dataset and cross-dataset evaluations. Extensive experiments on benchmark deepfake detec-\ntion datasets demonstrate the efficacy of our approach, surpassing state-of-the-art approaches in preserving\nfairness during cross-dataset evaluation. Our results highlight the potential of synthetic datasets in achieving\nfairness generalization, providing a robust solution for the challenges faced in deepfake detection.", "sections": [{"title": "INTRODUCTION", "content": "Deepfake technology, which combines \"deep learn-\ning\" and \"fake,\" represents a significant advancement\nin media manipulation capabilities. Leveraging deep\nlearning techniques, it enables highly convincing fa-\ncial manipulations and replacements in digital me-\ndia. While technologically impressive, this capabil-\nity poses serious societal risks, particularly in spread-\ning misinformation and eroding public trust. In re-\nsponse, researchers have developed various deepfake\ndetection techniques that have shown promising ac-\ncuracy rates in identifying manipulated content (Xu\net al., 2023; Zhao et al., 2021a; Ezeakunne and Liu,\n2023; Guo et al., 2022; Pu et al., 2022b; Wang et al.,\n2022).\nHowever, a critical challenge has emerged in\nthe form of fairness disparities across demographic\ngroups as described in (Trinh and Liu, 2021; Xu\net al., 2022; Nadimpalli and Rattani, 2022; Masood\net al., 2023). Studies have revealed that current detec-\ntion methods perform inconsistently across different\ndemographics, particularly showing higher accuracy\nrates for individuals with lighter skin tones compared\nto those with darker skin tones (Trinh and Liu, 2021;\nHazirbas et al., 2021). This bias creates a concern-\ning vulnerability where malicious actors could poten-\ntially target specific demographic groups with deep-\nfakes that are more likely to evade detection.\nWhile recent algorithmic approaches have shown\npromise in improving detection fairness when train-\ning and testing data use similar forgery techniques\n(Ju et al., 2024), the real-world application presents\na more complex challenge. Deepfake detectors, typ-\nically developed and trained on standard research\ndatasets, must ultimately operate in diverse real-world\nenvironments where they encounter deepfakes cre-\nated using various, potentially unknown forgery tech-\nniques. This generalization capability is crucial for\npractical deployment, particularly on social media\nplatforms where manipulated content proliferates.\nWhile current methods of deepfake detection fo-\ncus almost entirely on detection accuracy, it is very\ncritical to also consider how well these approaches"}, {"title": "Related Work", "content": "To train deepfake detection models that perform well\nduring training and in practice; on unseen datasets,\nseveral studies (Li et al., 2020b; Zhao et al., 2021b;\nGuan et al., 2022; Li, 2018) have introduced novel\nmethods for manually synthesizing a variety of face\nforgeries similar to deepfakes. These methods help\ndeepfake detection models learn more generalized\nrepresentations of artifacts, improving their ability to\nidentify deepfakes across different scenarios.\nThe authors in (Chen et al., 2022) proposed en-\nhancing the diversity of forgeries through adversar-\nial training to improve the robustness of deepfake de-\ntectors in recognizing various forgeries. FaceCutout\n(Das et al., 2021) uses facial landmarks and ran-\ndomly cuts out different parts of the face (such as\nthe mouth, eye, etc.) to improve the robustness of\ndeepfake detection. Face-Xray (Li et al., 2020b) on\nthe other hand, involves generating blended images\n(BI) by combining two different faces using a global\ntransformation and then training a model to distin-\nguish between real and blended faces. The authors"}, {"title": "Fairness in Deepfake Detection", "content": "Despite efforts to enhance the generalization of deep-\nfake detection to unseen data, limited progress has\nbeen made in mitigating biased performance when\ntesting on both the same (intra-domain) and different\n(cross-domain) dataset distributions.\nRecent studies have highlighted significant fair-\nness issues in deepfake detection, revealing biases in\nboth datasets and detection models (Masood et al.,\n2023). The work done by (Trinh and Liu, 2021)\nand (Hazirbas et al., 2021) discovered substantial er-\nror rate differences across demographic groups, while\n(Pu et al., 2022a) found that the MesoInception-4\nmodel by (Afchar et al., 2018) was biased against\nboth genders. The authors in (Xu et al., 2022) advo-\ncated for diverse dataset annotations to address these\nbiases, and (Nadimpalli and Rattani, 2022) introduced\na gender-balanced dataset to reduce gender-based per-\nformance bias. However, these techniques led to only\nmodest improvements and required extensive data an-\nnotation. Furthermore, (Ju et al., 2024) worked to im-\nprove fairness within the same dataset (intra-domain),\nbut did not address fairness generalization between\ndifferent datasets (cross-domain)."}, {"title": "Fairness Generalization in Deepfake Detection", "content": "Very little work has been done to achieve fairness\ngeneration in deepfake detection. The recent work\ndone in (Lin et al., 2024) proposed a novel feature-\nbased technique to achieve fairness generalization and\npreservation. They combine disentanglement learn-\ning, fairness learning, and loss optimization to pre-\nserve fairness in deepfake detection. In the disen-\ntanglement learning module, they utilized a disen-\ntanglement loss to expose demographic and domain-\nagnostic forgery features. While the fairness learn-\ning module fused the two features from the disentan-\nglement learning module to obtain predictions for the\nsamples. Their framework, however, was trained on\na dataset with imbalanced demographic groups which\nintroduced some bias to their model.\nOur framework approaches and solves fairness\ngeneralization using a data-driven approach. Our\nresearch prioritizes fairness and eliminates bias due\nto dataset imbalance by using synthetic datasets to\nbalance the dataset distribution across the different\ngroups. We also used a multi-task learning approach,\ncombined with fairness and loss sharpness-aware op-\ntimization and robust feature extraction to achieve\nsignificant performance improvements on both intra-\ndataset evaluation and cross-dataset evaluation."}, {"title": "Proposed Method", "content": "Our method aims to enhance both the generalization\nand fairness of deepfake detection models through a\ndata-centric strategy. This section provides a detailed\noverview of the problem formulation, followed by the\nprocess of generating our synthetic dataset. We then\noutline our approach for addressing dataset imbal-\nance and conclude with a discussion of our multi-task\nlearning framework, designed to simultaneously op-\ntimize for accuracy and fairness across demographic\ngroups."}, {"title": "Problem Setup", "content": "Given an input face image I, the goal is to train a deep-\nfake detection model g that classifies whether I is a\ndeepfake (fake) or a real face. The model g is defined\nas:\n$g: I \\rightarrow \\{0,1\\}$\nwhere g(I) indicates the predicted label, with\ng(I) = 1 denoting a deepfake and g(I) = 0 denoting a\nreal face.\nDetection Accuracy: The primary goal is to max-\nimize the detection accuracy. Given a set of face im-\nages $\\{I_i\\}_{i=1}^{N}$ with true labels $\\{y_i\\}_{i=1}^{N}$, the overall accu-\nracy of the model g is:\n$Accuracy = \\frac{1}{N} \\sum_{i=1}^{N} 1(g(I_i) = y_i)$\nwhere 1(\u00b7) is the indicator function that equals 1 if\nthe prediction matches the true label.\nFairness Constraint: The detector should per-\nform fairly across different demographic groups. Let\nD represent the demographic group associated with\nan image, and $D_k$ be the set of images from the demo-\ngraphic group k. Define Accuracy as the accuracy of\nthe detector on images from demographic group k:"}, {"title": "Synthetic Data Balancing", "content": "To enhance fairness in deepfake detection, we utilize\na technique based on self-blended images (SBI), as\nproposed in (Shiohara and Yamasaki, 2022). This\ntechnique involves generating synthetic images to\nbalance the demographic distribution of the dataset,\nthereby improving fairness across various demo-\ngraphic groups. The dataset samples are shown in\nFigure 2. Each real sample (upper row) has a cor-\nresponding fake sample (bottom row), so the dataset\nis also balanced across real and fake classes. The pro-\ncess of data balancing using a synthetic dataset is de-\ntailed in the sections below."}, {"title": "Generation of Synthetic Images", "content": "Let $I = \\{I_1,I_2,..., I_n\\}$ represent the set of real face\nimages, where each $I_i$ belongs to a specific demo-\ngraphic group. The goal is to generate a set of self-\nblended images $S = \\{S_1, S_2, ..., S_m\\}$ using the follow-\ning steps:\nBase Image Selection: Select a diverse subset of\nreal face images, $I_{base} \\subset I$, ensuring coverage across\ndemographic categories.\nAugmentation Process: Apply a series of transfor-\nmations to generate self-blended images. For a base\nimage $I_i$, the synthetic image $S_i$ is generated using a\nblend function B as follows:\n$S_j = B(I_i, T_k(I_i))$\nwhere $T_k$ represents a transformation operation such\nas scaling, rotation, or color adjustment, and B is the\nblending function that combines $I_i$ with $T_k(I_i)$."}, {"title": "Dataset Balancing", "content": "To balance the dataset, we increase the representation\nof underrepresented demographic groups:\nDemographic Representation: Define the demo-\ngraphic groups $D = \\{D_1,D_2,...,D_r\\}$. Let $I_{D_k} \\subset I$\nbe the set of images belonging to demographic group\n$D_k$. To balance the dataset, we create synthetic im-\nages $S_{D_k}$ for each group $D_k$:\n$B = I \\cup S$\nwhere B is the balanced dataset and $S = \\cup_{k=1}^{K} S_{D_k}$.\nThis dataset B, is balanced across all the demo-\ngraphics, and also equal in terms of the number of\nreal and fake samples."}, {"title": "Multi-task Learning", "content": "We propose a dual-task learning framework to en-\nhance the fairness of deepfake detection models by\nleveraging demographic information. The architec-\nture shown in Figure 3, is designed to improve the\ngeneralization of deepfake detection across various\ndemographic groups, reducing bias in prediction ac-\ncuracy between these groups.\nThe input images are passed through the Effi-\ncientNet encoder (Tan, 2019), which extracts high-\ndimensional feature representations. These features\ncapture both low-level information (like textures and\nedges) as well as high-level semantic features (such as\nfacial structure and potential tampering artifacts). The\nextracted features serve as the input for the real/fake\nclassification head and the demographic head.\nReal/Fake Classification Head: This head con-\nsists of a Multi-Layer Perceptron (MLP) that pro-\ncesses the extracted features and predicts whether the\ninput image is real or fake. The output of this MLP is\na probability score indicating the likelihood that the\nimage is a deepfake or a genuine one. This branch\nfocuses on learning the subtle cues and artifacts asso-\nciated with the deepfake generation, such as inconsis-\ntencies in lighting, facial details, or blurring artifacts\naround the face.\nDemographic Head: In parallel to the real/fake\nclassification, the features are passed through a sec-\nond MLP for demographic group classification. This\nMLP is designed to categorize the input into one of\neight demographic groups, such as Black-Male (B-\nM), White-Female (W-F), Asian-Male (A-M), etc.\nThe purpose of this head is to ensure that the model\nlearns demographic-specific features that are indepen-\ndent of deepfake artifacts. These features are crucial\nfor enhancing the fairness of the model across diverse\ndemographic groups."}, {"title": "Loss Function", "content": "We define the loss function to incorporate both the\nstandard classification loss and a fairness penalty.\nSpecifically, the total loss function L is composed of"}, {"title": "Optimization", "content": "We employ the Sharpness-Aware Minimization\n(SAM) optimizer (Foret et al., 2020) as our optimizer.\nSAM is designed to improve the generalization per-\nformance of deep neural networks by encouraging the\nmodel to find parameter spaces that are not only good\nat minimizing the training loss but also robust to small\nperturbations in the weight space. This results in a\nflatter and smoother loss landscape, which has been\nshown to correlate with better generalization.\nThe SAM optimizer modifies the traditional\ngradient-based update by introducing a two-step pro-\ncess that seeks to find parameters that minimize both\nthe loss and its sensitivity to perturbations. Let L(w)\ndenote the total loss function of the model, where w\nrepresents the network's parameters. SAM performs\nthe following steps:\nPerturbation Step: SAM first finds a perturbation\n\u03b5 that maximizes the loss function around the current\nparameters. The perturbation \u03b5 is chosen to make the\nloss worse within a neighborhood of the current pa-\nrameter values:\n$\\epsilon(w) = arg \\max_{\\|\\epsilon\\| \\leq \\rho} L(w+\\epsilon)$\nwhere \u03c1 controls the size of the neighborhood in\nwhich the optimizer searches for the sharpest direc-\ntions.\nParameter Update Step: Once the worst-case\nperturbation \u03b5 is found, the SAM optimizer updates\nthe parameters by minimizing the loss function at\nw + \u03b5:\n$w \\leftarrow w-\\eta \\nabla L(w+\\epsilon)$\nwhere \u03b7 is the learning rate.\nThis two-step process leads to solutions in flat-\nter regions of the loss surface, improving the model's\ngeneralization and robustness."}, {"title": "Experiments", "content": "To evaluate the fairness generalization capability of\nour proposed approach, we train our model on the\nwidely used FaceForensics++ (FF++) dataset (Rossler\net al., 2019). For testing, we use FF++, Deepfake De-\ntection Challenge (DFDC) (Dolhansky et al., 2020),\nand Celeb-DF (Li et al., 2020c). We used the real im-\nages from these datasets and created synthetic images\nas the fake images using the method in Section 3.2.\nSince the original datasets do not include demo-\ngraphic information, we follow the approach in (Ju\net al., 2024) for data processing and annotation. The\ndatasets are categorized by race and gender, form-\ning the following intersection groups: Black-Male (B-\nM), Black-Female (B-F), White-Male (W-M), White-\nFemale (W-F), Asian-Male (A-M), Asian-Female (A-\nF), Other-Male (O-M), and Other-Female (O-F). The\nCeleb-DF (Li et al., 2020c) does not contain the Asian\nsub-group."}, {"title": "Evaluation Metrics", "content": "For evaluating our approach, we use several perfor-\nmance metrics, including the Area Under the Curve\n(AUC), Accuracy, and True Positive Rate (TPR).\nThese metrics allow us to benchmark our method\nagainst previous works. To assess the degree of fair-\nness of the model, we analyze the accuracy gaps\nacross different races and genders."}, {"title": "Baseline Methods", "content": "For intra-dataset evaluation, we compare our ap-\nproach against recent works in deepfake detection\nfairness, specifically DAW-FDD (2023) (Ju et al.,\n2024) and Lin et al. (2024) (Lin et al., 2024). For\ncross-dataset evaluation, our method is compared to\nthe fairness generalization method proposed by Lin et\nal. (2024) (Lin et al., 2024). To the best of our knowl-\nedge, Lin et al. (2024) (Lin et al., 2024) is the prior\nwork addressing the fairness generalization problem.\nWe also used the Xception (Chollet, 2017) as a base-\nline for comparison during both intra-dataset evalua-\ntion and cross-dataset evaluation. The Xception net-"}, {"title": "Implementation Details", "content": "All experiments are conducted using PyTorch. For\nour method, we set the batch size to 16 and train the\nmodel for 100 epochs. To incorporate fairness, we use\na fairness penalty weight \\lambda of 20 (See Eq. 10). For\noptimization, we employ the SAM optimizer (Foret\net al., 2020). The learning rate is set to 5e-4, momen-\ntum is configured at 0.9, and weight decay is set to\n5 \u00d7 10-3."}, {"title": "Experimental Results", "content": "In this section, we discuss the results of our evalua-\ntions and present our findings. We start by discussing\nthe results of our intra-dataset evaluations and then,\nthe results of the cross-dataset evaluations."}, {"title": "Intra-dataset Evaluation Results", "content": "We perform intra-dataset evaluations to measure the\nmodel's performance on the same dataset distribution,\nassessing its ability to fit the training data. As shown\nin Table 1, our method achieves comparable perfor-\nmance to baseline approaches in deepfake detection.\nHowever, it demonstrates a significant improvement\nover baseline methods in fairness preservation across\ndemographic groups. Figure 4 presents the evaluation\nresults for the gender demographic group, while Fig-\nure 5 illustrates the results for the racial demographic\ngroup. As evidenced in these figures, the disparity\nbetween the Male and Female subgroups is substan-\ntially reduced in our approach with a 0.12% differ-\nence (in accuracy) between the subgroups, compared\nto the 1.58%, 2.90% and 3.87% differences between\nthe subgroups for the baseline approaches. Addition-\nally, the disparities among racial groups are signifi-\ncantly lower with our approach with a 0.71% differ-"}, {"title": "Cross-dataset Evaluation Results", "content": "We conducted cross-dataset evaluations to assess\nour model's performance on out-of-distribution data,\nevaluating its ability to generalize to unseen datasets.\nAs recorded in Table 2, and visualized in Figures\n6,7,8 and 9, our approach achieves comparable per-\nformance to baseline methods in deepfake detection\nwhile significantly outperforming them in fairness\ngeneralization across demographic groups.\nFigures 6 and 7 visualize the results of evaluations\nfrom Table 2 where the model was trained on FF++\nand tested on DFDC, while Figures 8 and 9 visual-\nizes the results from Table 2 for training on FF++ and\ntesting on Celeb-DF. These figures demonstrate that\nour approach exhibits minimal performance dispar-\nities across different gender and racial groups com-\npared to the baselines.\nAs shown in Table 2, on the DFDC dataset, the\nXception baseline has the lowest disparity in accuracy\nacross genders. The disparity in accuracy between\nmales and females is 1.85% for Xception, and then\n4.5% (Lin et al., 2024) and 8.03% (Ours). Across race\non the DFDC dataset, our method has the lowest accu-\nracy disparity across the races at 11.15% (which hap-\npens to be between Asian and white) with the baseline\nrecording accuracy disparities of 24.53% and 14.39%.\nAs shown in Table 2, on the Celeb-DF dataset,\nour method has a minimal disparity in accuracy across\nboth gender and race. Across gender, we got 15.47%\ndisparity in accuracy, while the baselines recorded\nwider disparities of 40.66% and 25.47% in accuracy.\nAcross race, our method recorded 11.54% as the max-"}, {"title": "Limitations", "content": "The limitations of our method are:\n\u2022 Dependence on Demographically Annotated\nDatasets: The effectiveness of our method relies\non the availability of detailed demographic anno-\ntations. Such datasets are often difficult to obtain\nand may not always be comprehensive enough,\npotentially impacting the fairness assessment.\n\u2022 Trade-Off Between Fairness and Detection Per-\nformance: Improving fairness across demo-\ngraphic groups can result in a trade-off with over-\nall detection performance. Enhancements aimed\nat reducing demographic disparities may lead to\na decrease in the model's overall accuracy in de-\ntecting deepfakes.\nThese limitations highlight the need for further\nresearch to address the challenges of dataset depen-\ndency and the balance between fairness and detection\nperformance."}, {"title": "Conclusion", "content": "We have presented a novel approach to address the\ncritical challenge of fairness generalization in deep-\nfake detection systems. Our method combines three\nkey innovations: a data-centric strategy using syn-\nthetic image generation to balance demographic rep-\nresentation, a multi-task learning architecture that si-\nmultaneously optimizes detection accuracy and de-\nmographic fairness, and sharpness-aware optimiza-\ntion to enhance generalization capabilities. Our com-\nprehensive evaluations demonstrate the effectiveness\nof this approach. In intra-dataset testing, our method\nachieved comparable detection performance to exist-\ning approaches while significantly reducing demo-\ngraphic disparities - showing only 0.12% accuracy\ndifference between gender groups compared to up\nto 3.87% in baseline methods, and 0.71% difference\nacross racial groups compared to up to 5.28% in\nbaselines. More importantly, in challenging cross-\ndataset scenarios, our approach demonstrated supe-\nrior fairness generalization. When tested on the\nCeleb-DF dataset, our method reduced gender-based\naccuracy disparities to 15.47% compared to up to\n40.66% in baselines, while maintaining strong over-\nall detection performance. These results suggest that\nour integrated approach of balanced synthetic data,\ndemographic-aware learning, and robust optimization\nprovides a promising direction for developing deep-\nfake detection systems that are both accurate and de-\nmographically fair. Future work could explore ex-\ntending these techniques to other domains where al-"}]}