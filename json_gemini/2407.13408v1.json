{"title": "DISCOVER: A Data-driven Interactive System for Comprehensive Observation, Visualization, and ExploRation of Human Behaviour", "authors": ["Dominik Schiller", "Tobias Hallmen", "Daksitha Withanage Don", "Elisabeth Andr\u00e9", "Tobias Baur"], "abstract": "Understanding human behavior is a fundamental goal of social sciences, yet its analysis presents significant challenges. Conventional methodologies employed for the study of behavior, characterized by labor-intensive data collection processes and intricate analyses, frequently hinder comprehensive exploration due to their time and resource demands. In response to these challenges, computational models have proven to be promising tools that help researchers analyze large amounts of data by automatically identifying important behavioral indicators, such as social signals. However, the widespread adoption of such state-of-the-art computational models is impeded by their inherent complexity and the substantial computational resources necessary to run them, thereby constraining accessibility for researchers without technical expertise and adequate equipment. To address these barriers, we introduce DISCOVER - a modular and flexible, yet user-friendly software framework specifically developed to streamline computational-driven data exploration for human behavior analysis. Our primary objective is to democratize access to advanced computational methodologies, thereby enabling researchers across disciplines to engage in detailed behavioral analysis without the need for extensive technical proficiency. In this paper, we demonstrate the capabilities of DISCOVER using four exemplary data exploration workflows that build on each other: Interactive Semantic Content Exploration, Visual Inspection, Aided Annotation, and Multimodal Scene Search. By illustrating these workflows, we aim to emphasize the versatility and accessibility of DISCOVER as a comprehensive framework and propose a set of blueprints that can serve as a general starting point for exploratory data analysis.", "sections": [{"title": "1 INTRODUCTION", "content": "Understanding human behavior is a core pursuit in social sciences, crucial for unraveling the complexities of human interactions and societal dynamics. However, analyzing human behavior comes with its fair share of challenges, particularly with traditional methods that involve laborious data collection and intricate analyses, often limiting thorough exploration due to resource demands.\n\nTo address these challenges, computational models have emerged as promising alternatives, offering automated identification of key behavioral cues like social signals. Yet, their widespread adoption faces hurdles, mainly due to their complexity and the hefty computational resources they require, making them less accessible to researchers without technical expertise or adequate equipment.\n\nIn light of these issues, we introduce DISCOVER ${ }^{1}$, a flexible opensource software framework tailored to streamline computational-driven data exploration for human behavior analysis. Our primary goal is to democratize access to advanced computational tools, empowering researchers from diverse backgrounds to conduct indepth behavioral analysis without extensive technical know-how.\n\nThroughout this paper, we showcase DISCOVER's versatility through four illustrative data exploration workflows. First, we describe the interactive semantic content exploration of transcriptions, achieved through direct integration of large language models. Second, we illustrate how the integrated social signal processing capabilities of DISCOVER can aid researchers in the visual exploration of data. Third, we show how DISCOVER can be used to aid the annotation of meaningful behavioral cues. Lastly, we showcase our framework's capabilities for multimodal scene search, enabling users to automate the identification of key scenes within datasets.\n\nAlthough all suggested workflows can be applied individually and adapted to user-specific needs, we suggest to consider them as blueprints for starting data exploration with DISCOVER."}, {"title": "2 RELATED WORK", "content": "Previous research in the field of human-computer interaction has suggested methods for displaying multimodal characteristics within particular conversational contexts to analyze human behavior.\n\nOver time, a range of annotation tools concentrating on affective computing and social cues has emerged, offering assistance to users in their efforts. Prominent examples include ELAN [44], ANVIL [28], and EXMARALDA [38]. These tools offer layer-based tiers to insert time-anchored labeled segments, that we call discrete annotations. Continuous annotations, on the other hand, allow an observer to track the content of an observed stimulus over time based on a continuous scale. One of the first tools that allowed labelers to trace emotional content in real-time on two dimensions (activation and evaluation) was FEELTRACE [13]. Its descendant GTRACE (general trace) [14] allows the user to define their own dimensions and scales. More recent tools to accomplish continuous descriptions are CARMA (continuous affect rating and media annotation) [21] and DARMA (dual axis rating and media annotation) [22].\n\nRecently, these tools have evolved to include automatic calculation of behavioral cues and social signals, eliminating the need for the manual annotation of data. For example ENODASH [19] has been designed to enhance tutors' retrospective understanding of learners' emotions, based on facial expressions, within a video-conferencing learning setting. REsCUE [2] helps to aid coaching practitioners to detected unconscious behavior of their clients. To this end, REsCUE uses an unsupervised anomaly detection algorithm to cluster. MultiSense [45] can assess the affective state of a person by inferring various indicators from audio-visual input signals. The tool focuses on the application within the mental health domain to assess indicators of psychological distress such as depression or post-traumatic stress disorder. MeetingCoach [36] is an AI-driven feedback dashboard designed to enhance the effectiveness and inclusively of video-conferencing meetings by providing personalized insights into meeting dynamics, such as engagement summaries of participants or speaking time distribution. MACH [25] social skills training, particularly focusing on job interview preparation, that analyses non-verbal behavior of an interviewee. The AffectToolbox [31] provides a software system aimed at aiding researchers in developing affect-sensitive studies and prototypes in affective computing. It provides accessible functions for analyzing users' affective states via a graphical user interface, including a variety of emotion recognition models for different modalities as well as a unified multimodal assessment. The CONAN Tool [33] has been developed with a focus on group conversation analysis. To this end, it automatically analyzes the gaze behavior, body movement, speaker activity, and facial expressions of participants using a single $360^{\\circ}$ camera.\n\nAll these visualization-based methods were developed with specific goals and target groups in mind. As a result, the choice of features to be displayed is usually tailored to this specific use case. Therefore these solutions suffer from a lack of customizability that prevents users from adapting the features to their individual needs.\n\nThe Social Signal Interpretation Framework(SSI) by Wagner et al. [42] presents an alternative approach, by implementing a modular, multimodal signal processing pipeline, facilitating both online and offline recognition tasks. The plug-in system within SSI allows users to develop custom modules and integrate them into the processing pipeline. Similar to SSI the Opensense [1] platform has been designed to facilitate real-time acquisition and recognition of social signals through multiple modalities. It also follows a modular pipeline design and builds on Microsoft's Platform for Situated Intelligence [8], which enables the processing of human behavioral signals and supports various sensor devices and machine learning tools. Barz et al. [6] developed the MultiSensorPipeline, a lightweight and adaptable framework for creating multimodal-multisensor interfaces using real-time sensor data. While the framework is conceptually similar to SSI and Opensense it focuses on a concise set of concepts and functionalities that ease the creation and execution of complex processing pipelines. The process of implementing specific modules is left to the user, making the tool better suited for developing prototypes of custom multimodal-multisensor processing pipelines than for using standard modules to analyze social signals. In contrast to the pure visual exploration of data, Providence implements a scene search approach to investigate social behavior in multimodal data. Hereby, a human analyst can formulate queries containing non-verbal (e.g. nodding, facial expressions), linguistic (e.g. sentiment), or para-linguistic (e.g. speech speed or volume) to search for specific scenes in a human conversation. Providence automatically extracts the respective features required by a query and searches for scenes fulfilling the specified conditions in the data.\n\nAlthough the approaches presented have their respective advantages and disadvantages, there are two common shortcomings: Efficient use of computing resources and a compromise between flexibility and complexity. Except for Providence, all the tools introduced are solely intended for operation on local machines. This leads to inefficient use in multi-user scenarios, in which users either have to take turns on a local machine or each user requires his own workstation that is capable to run such tools. Considering hardware demands and energy consumption of state-of-the-art machine learning models, this issue extends beyond financial concerns to ecological ones. Further, it becomes evident that off-the-shelf solutions have constraints in their applicability, whereas more adaptable approaches necessitate greater technical proficiency, thereby posing a barrier to entry."}, {"title": "3 FRAMEWORK", "content": "The architecture of DISCOVER follows a modular design, which facilitates flexibility, scalability, and ease of maintenance. At its core, the framework comprises several key components, each serving a distinct purpose in enabling laymen to perform data exploration tasks efficiently. An overview of the system architecture is shown in Figure 2. Below, we provide a textual skeleton outlining the main components and their functionalities.\n\n3.1 User Interface\nEach element in the DISCOVER framework utilizes APIs to communicate over the network, granting users with programming skills significant flexibility by allowing access via scripts, irrespective of the programming language employed. In order to make DISCOVER more user-friendly for individuals without technical expertise, we've incorporated its API into the open-source tool NOVA [24], to serve as the graphical user interface. NOVA aims to enhance the standard annotation process with the latest developments from contemporary research fields such as Cooperative Machine Learning and eXplainable Artificial Intelligence by giving annotators easy access to automated model training and prediction functionalities, as well as sophisticated explanation algorithms via its user interface.\n\nThe NOVA user interface has been designed with a special focus on the annotation of long and continuous recordings involving multiple modalities and subjects. A screenshot of a loaded recording session is shown in Figure 3. On the top, several media tracks are visualized and ready for playback. Note that the number of tracks that can be displayed at the same time is not limited and various types of signals (video, audio, facial features, skeleton, depth images, etc.) are supported. In the lower part, we see multiple annotation tracks of different types (discrete, continuous, and transcriptions) describing the visualized content.\n\nNOVA provides several functions to process the annotations created by multiple human or machine annotators. For instance, statistical measures such as Cronbach's $\\alpha$, Pearson's correlation coefficient, Spearman's correlation coefficient or Cohen's $\\kappa$ can be applied to identify inter-rater agreement.\n\n3.2 Annotation Database\nTo support a collaborative annotation process, DISCOVER maintains a database back-end, which allows users to load and save annotations from and to a MongoDB ${ }^{2}$ database running on a central server. This gives annotators the possibility to immediately commit changes and follow the annotation progress of others. Besides human annotators, a database may also be visited by one or more 'machine users'. Just like a human operator, they can create and access annotations. Hence, the database also functions as a mediator between human and machine. DISCOVER provides instruments to create and populate a database from scratch. At any time new annotators, schemes, and additional sessions can be added.\n\n3.3 Media File Storage\nDISCOVER uses a data storage component that follows the structure of the open-source cloud hosting framework Nextcloud ${ }^{3}$. Data can be hosted on a local drive and be shared on demand with people who have been granted access to the respective Database. Both the NOVA user interface, as well as the processing backend can access these files for visualization and processing respectively.\n\n3.4 Processing Server\nThe processing server acts as the computational engine powering data analysis tasks within our framework. It implements a lightweight web server that interacts with the annotation and data storage components to extract meaningful information from the data, that can subsequently be visualized in NOVA. The server-based architecture of DISCOVER provides key advantages concerning flexibility, accessibility, and resource efficiency. First of all, exposing a REST API makes the server not only accessible from the NOVA user interface but also via scripts. This results in a low entry barrier for not coding affine research, while maintaining the ability to include the processing functionality in custom scripts. Second, the server-based approach enables multiple users to process data in a central place to ensure optimal usage of computational resources. The standardized input and output formats of the processing modules also enable the sharing and further usage of results.\n\n3.5 Assistant\nRecently large language models (LLM), like Chat-GPT ${ }^{4}$ or LLama [41] have demonstrated remarkable capabilities for textual analysis tasks like text summarization [29, 30, 39], sentiment analysis [20, 46] or argument mining [26, 34]. To incorporate such capabilities into DISCOVER, we supplement the processor component with an assistant. Fundamentally the assistant is a lightweight web server that aims to integrate numerous LLMs with the rest of the DISCOVER infrastructure. To this end, the assistant is exposing a unified API that reroutes requests to either an external service provider (e.g. OpenAI), a self-hosted large language model (e.g. via Ollama ${ }^{5}$. Through seamless integration with the UI, users can interact with AI assistants via a chat interface to analyze textual data, such as dialogue transcripts. Depending on the performance of a model for specific tasks and privacy requirements a user can switch between available services dynamically during the exploration process."}, {"title": "4 MODULES", "content": "DISCOVER relies on exchangeable modules to infer behavioral indicators from recorded data. Each of these modules can be understood as a configurable building block, consisting of predefined inputs and outputs with module-specific options. DISCOVER is fully extendable with custom modules. However, to keep the entrance barrier low and provide value for a non-coding affine target group we also provide a number of ready-to-use processing modules. The following section provides an overview of the currently integrated modules.\n\n4.1 Face\nBounding Box Detector. The automatic derivation of behavioral indicators from a human face usually requires the localization of the facial area in an image or video. To this end, we rely on the BlazeFace model proposed by [7] et al. The BlazeFace model is a lightweight face detection model that has been developed to run on mobile devices and thus requires only a minimum of computational resources to achieve super-realtime performance.\n\nLandmarks and Meshes. For the further processing of the localized image, it's it is a common procedure to align facial images based on localized landmarks [11]. Facial image alignment involves geometric transformations like translation, rotation, and scaling to convert the input face image into a standardized form. To this end, we employ the face mesh model by Kartynnik et al. [27], which infers an approximate 3D mesh representation of a human face from a single camera.\n\nAction Units. Facial action units originate from an anatomical examination of the face and can be categorized according to the Facial Action Coding System (FACS) outlined by Ekman and Friesen [16]. For automatic action unit detection and intensity estimation, we integrated the LibreFace [11] framework which achieves state-of-the-art performance in both tasks while improving inference times over other methods.\n\nFacial Expression. Facial expression analysis involves automatically detecting subtle movements in facial muscles and identifying typical facial displays. The recognition of these expressions yields valuable insights into users' social and emotional states. To facilitate robust facial expression prediction we integrated multiple models into DISCOVER Emonet [40], Relevance-based data masking [37], LibreFace [11].\n\n4.2 Voice\nWhen analyzing human speech to gain behavioral insights, one needs to distinguish between the verbal and vocal components of speech. Verbal refers to communication that is expressed in words or language. It involves the use of language to convey ideas, thoughts, or information. Vocal on the other hand refers to the sounds produced by the voice or the act of speaking. In the context of communication, 'vocal' can refer to the tone, pitch, volume, and other qualities of speech.\n\n4.2.1 Verbal. A prerequisite to the analysis of the verbal content of spoken language is the conversion from speech to text (STT). STTSystems have been an active area of research for decades. For the implementation of our STT module we rely on WhisperX [3], an adaptation of the Whisper Model Radford et al. [35], that provides improved timestamp accuracy, support for longer audio sequences, and faster transcription performance.\n\nSpeaker Diarization. The available datasets are typically recorded with the focus of manual human analysis rather than automatic processing. A common example of this is the recording of a single audio signal for several speakers. While it is a mostly trivial task for a human listener to distinguish between the voices of speakers and map the content of the spoken language to the respective person, this information gets lost during the STT process. To account for this loss of information DISCOVER implements a speaker diarisation module, which maps segments of a common dialogue transcript to individual speakers. To this end we rely on PYANnOTe $[9,10]$ to cluster voiced segments in the audio signal. We then use an oracle approach to assign those clusters to individual speakers, by providing reference speaking terms within the audio signal.\n\nSentiment Analysis. Sentiment analysis is the process of computationally determining the emotional tone behind a piece of text, whether it's positive, negative, or neutral. It can be a valuable tool for analyzing human behavior, as it can provide a deeper understanding of individuals' emotions and opinions. To enable the automatic prediction of sentiment, DISCOVER currently integrates two approaches. A multi-lingual [4] ) and a German language specific [23] model.\n\n4.2.2 Vocal. Speech emotion recognition (SER) refers to the task of automatically detecting and interpreting emotions conveyed through speech. It involves analyzing various acoustic features, such as pitch, intensity, and rhythm, to infer the underlying emotional state of the speaker. DISCOVER integrates a pre trained model, proposed by Wagner et al. [43] to automatically detect valence, arousal, and dominance values from a human voice.\n\n4.3 Multimodal Feature Extraction\nBesides the above-mentioned modules, which directly provide insights about important indicators for human behavior to an analyst, DISCOVER also implements modality-specific feature extraction modules, that can be used to train custom detection models.\n\nVideo. For the video modality we use the DinoV2 pretrained vision transformer models [15,32] to extract features. Those models are pretrained in a self-supervised manner on a large dataset of 142 million images. As a result, DINOv2 models have demonstrated robust performance beyond training data, delivering usable general-purpose features without the need for fine-tuning.\n\nAudio. For the audio modality we rely on a pretrained w2vBERT 2.0 encoder [5]. Similar to the DinoV2 model, this model was trained unsupervised on a large data set of 4.5 million hours of audio, and demonstrates excellent performance for a variety of downstream tasks like speech to text, or expressive speech to speech translation. However, it is recommended to fine-tune the w2v-BERT 2.0 model before using it for a downstream task. Since this might not be feasible for technical unsavvy users we also integrated the openSMILE library [18], which extracts various handcrafted feature sets for the audio domain. Specifically, the GeMaps feature set [17] has been developed for general voice research and affective computing tasks and provides a good starting point for any speech-related classification task.\n\nText. When it comes to extracting features from text representations, the language of the text is a necessary consideration. Since it is a key aspect of our framework to be employable in versatile scenarios across multiple languages, DISCOVER integrates a multilingual textual feature extraction using the XLMroBERTa (XLM-R) model by Conneau et al. [12] This model consists of a transformerbased architecture, trained on vast amounts of multilingual data crawled from the internet. In their experiments, the authors analyzed the capabilities of XLM-R for several tasks, including name entity recognition, cross-lingual question answering, paraphrasing, and sentiment analysis. The reported results indicate that the model performs close to or even better than comparable monolingual models for languages where vast training resources are available. Furthermore, the model showed substantial improvements over other state-of-the-art models on low-resource languages across all tasks."}, {"title": "5 INTERACTIVE DATA EXPLORATION", "content": "5.1 Semantic Content Exploration\nIn the following section, we demonstrate the capabilities of DISCOVER using four exemplary workflows to examine unseen data and gather new insights. While each of our showcases is building upon the results of the previous one, every workflow can also be carried out independently of the others. The complete iterative data exploration pipeline is depicted in Figure 4.\n\nWe demonstrate how our workflow operates by utilizing recordings of interactions between teachers and parents. These videos are captured to evaluate the communication abilities of aspiring teachers in consultative situations and offer them constructive feedback. Within this context, the teacher is a trainee, while the role of the parent is portrayed by an actor. The subject of the discussion revolves around the child of the parent, who is facing challenges in school. Since the original discussion is in German a translated version can be found in Appendix A1. The following is an example workflow of the use case for data analysis from an analyst's point of view.\n\nAs a first step towards finding indicators of communicative quality in the recorded interactions, a user wants to get familiar with the data and the task. To facilitate those tasks, DISCOVER assistant enables the interactive exploration of the semantic content of the dialogue. To begin using the assistant a user first utilizes the WhisperX processing module to create a temporal-aligned transcript for each speaker from the recorded audio signal (see Appendix A1).\n\nAfter loading the transcription data into NOVA the user clicks on the Assistant tab and a chat window opens that establishes a connection with the assistant (see Figure 5). By checking the 'Context-Aware' checkbox in the bottom NOVA the transcript that has been loaded into NOVA before will be automatically sent to the assistant accompanying each message. That way a user can directly ask the assistant any questions regarding the semantic content of the dialogue. Firstly the user requests a summary of the interaction. The assistant replies with a concise summary of the transcript, providing the user with information about the general setting and topic of the dialogue, the roles of the interlocutors, and the course of the conversation. As the user has gained those insights about the data, the next step is to gather more information on relevant behavioral aspects to look out for. To this end, the user asks the assistant directly about important criteria to assess the quality of communication in parent-teacher conferences. The assistant answers by providing ten indicators for the assessment of dialogue quality, like Positive outcome or Collaborative approach. Each point is accompanied by a short description to clarify the meaning. From the previous summary, it is already clear that the teacher and parent are working together and the outcome of the dialog is positive as both parties agree on how they want to proceed in the future to support the child's learning. The user then continues to ask the assistant to evaluate the transcript concerning the now-identified indicators to get deeper insights beyond the summary. In return, the assistant provides further information about indicators like Empathy and Active Listening, based on the full transcript.\n\nFinally, the user also wants to know about non-verbal indicators for communication quality, to which the assistant again provides a list of key points to look out for like Facial Expressions / Smiles, Gaze or Vocal Inflections and Tone. Appendix A2 shows the complete dialogue between the user and the assistant.\n\n5.2 Aided Annotation\nThe processing modules described in Section 4 are a core aspect of DISCOVER. However, they largely depend on the availability of pre-trained models for processing. Depending on the use case there might be no fitting model available for the task that the user is looking for. To alleviate this problem, DISCOVER provides support for feature extraction, that can be used to train custom models directly from within the NOVA user interface. To this end, NOVA implements a cooperative machine learning workflow that consists of the following 5 steps: Initially, the model undergoes training (Step 1) and is then utilized to predict unseen data (Step 2). Following this, an active learning module determines which portions of the prediction necessitate manual review by human annotators (Step 3). Subsequently, those labels are then reviewed by a human and corrected if necessary. Finally, the initial model is retrained using the updated labeled data (Step 5). This iterative process continues until all data is annotated. By actively integrating human expertise into the learning process, we enable interactive guidance and enhancement of automatic predictions. Following the suggestions of the DISCOVER Assistant, the user trains a new model that is able to detect smiles, based on the extracted face mesh data. Before continuing the next step the model is used to detect all instances of smiles for the teacher as well as the parent.\n\n5.3 Visual Inspection\nDuring this stage, the user visually examines the session by navigating through sessions and their timeline. The objective at this stage is to identify patterns and formulate hypotheses regarding their manifestation, aiming to extract further insights. To confirm or question these hypotheses, users can select a processing module (see Section 4) directly from the NOVA User interface and start an extraction job on the processing server. Once the processing is done, the results can be directly visualized in the UI. This iterative process can be repeated as often as necessary for different modules. Since the results of the previous modules are stored either in the annotation database or on the media storage, the user can reuse them at any time, without recomputing them. To provide a clearer illustration of this process, we pick up on the previous example. Building upon the results of the semantic content exploration described in 5.1 the next steps are analyzing the non-verbal cues from the audio and the video signal and finding additional semantic indicators to assess the quality of communication based on the transcript.\n\nTo continue the exploration, the user predicts the facial expressions of both the teacher and the parent using the EmoNet module. Upon loading the model's predictions into NOVA. As depicted in Figure 6 it becomes apparent that the teacher's facial expressions predominantly oscillate between 'happy' and 'surprised,' whereas the parent's expressions tend to skew towards anger or sadness, although it's noted that anger may not always be accurate as the parent might simply be displaying a serious expression. In addition, the user now loads the smile annotation generated in the previous step (see Section 5.2).\n\nSmiling occurs notably more often at the beginning and end of the conversation for both roles, with the teacher exhibiting smiles more frequently throughout the conversation. As the increased number of smiles at the beginning and end can likely be attributed to formal politeness the user first focuses on the middle sections of the conversation. Especially visually identifying, mirroring behavior where the smile of one interlocutor is mirrored by the other one, provides interesting insights. For example, there is a notable scene in which the parents' smiles mirror the teacher's smile, a moment in which the parents receive new and helpful information, indicating active participation in the dialog.\n\nThis observation underscores the significance of integrating visual and verbal cues to capture the dynamics of communication and emotional expression within the interaction.\n\n5.4 Scene Search\nAs evidenced, the exploration of conversational scenes through the analysis of participants' multimodal behavior presents considerable promise for investigating social dynamics. The process of visual inspection provides an effective method of identifying the constellation of social cues inherent in a specific scene. However, it is of limited use when it comes to retrieving all instances of such scenes in a recording session. First of all, it's easy to overlook certain scenes when scrolling through the timeline of longer recording sessions. Second, the analysis of social cue constellations that require precise assessment of either timings (e.g. the immediate facial expression to an event) or values (e.g. the degree to which an action unit is activated), can not be efficiently performed this way. Lastly, it is easy to see how the reliable identification of patterns that consist of the interplay of multiple clues can quickly become overwhelming for a human analyst. To take all these aspects into account, DISCOVER provides an API that allows the user to define a set of rules that are used to automatically identify scenes based on previously defined conditions. Building upon the results of the visual inspection workflow (see Section 5.3), a user can now follow a blueprint script to download the smile detection annotations for both roles from the database, move a sliding window over the annotations and create a new annotation based on a predefined mirroring condition (see Figure 7)."}, {"title": "6 DISCUSSION", "content": "Feedback. To obtain initial feedback, we presented DISCOVER and the proposed workflow to an AI communication researcher who is involved in the interview and feedback process for the prospective teachers in the analyzed video recordings. In his comments he stated that the potential speed-up of data exploration through the proposed workflow is beneficial for the social science community: 'Basically you are making videos searchable. You can search for specific events as you search for a word in a transcript. This functionality greatly improves the analysis of large volumes of data.'\n\nFurther, he mentions that he considered DISCOVER to be especially useful for inductive analytic strategies, where a researcher reads through the data and allows new concepts to emerge: 'I can see how this would be useful for grounded theory approaches. In this process, researchers explore data iteratively without any previous assumptions about the findings to come up with new hypotheses and theories.'\n\nOn the other hand, he also mentioned how skeptical he is about using the tool to aid deductive strategies where a researcher applies an existing theory on new data. He stated that wrong or missing predictions of the automatically inferred cues could have a crucial impact on the results: 'When I analyzed the transcripts generated with Whisper, I was looking for filler and backchannels. It took me a while to realize, that most of those short sequences are filtered out during the transcription process. I needed to add them again manually to be able to use the transcription.'\n\nLastly, he also suggested how DISCOVER could be deployed as an interactive feedback tool in the future: 'This visualization could also be interesting for the students when providing feedback about their communication skills. It might be interesting to even let them explore their own conversations interactively.'\n\nLimitations and Prospects. While DISCOVER boasts a modular architecture, it's worth noting that certain modules may not yield perfect results, potentially introducing inaccuracies or limitations, especially in scenarios requiring precise results. Additionally, the absence of certain modules within DISCOVER could restrict the scope and comprehensiveness of analysis, given the varied nature of human behavior examination. On the other hand, the server-based architecture and modular structure of DISCOVER present notable advantages, facilitating the seamless exchange of processing components and enabling users to easily incorporate new modules or tailor modules to their specific needs. This adaptability empowers researchers to customize the tool according to their preferences and requirements. Furthermore, DISCOVER excels in fostering collaboration among researchers by facilitating the sharing of processed results and computing resources, thereby promoting efficient collaboration and nurturing a sense of community within the research domain. This collaborative aspect enhances the potential for collective insights and discoveries.\n\nMoreover, DISCOVER introduces innovative workflow components such as the assistant and cooperative machine learning functionalities, which enhance the efficiency and effectiveness of the analysis process, ultimately yielding more robust and insightful results. Despite its weaknesses, including potential module imperfections and missing functionalities, DISCOVER's strengths in its modular architecture, collaborative features, and unique workflow components position it as a promising tool for democratizing access to advanced computational methodologies in human behavior analysis."}, {"title": "7 CONCLUSION", "content": "We introduced DISCOVER, an open-source tool designed for human behaviour analysis utilizing state-of-the art machine learning models for feature extraction. Our framework offers a user-friendly interface through NOVA, streamlining the process and diminishing the necessity for laborious video and audio annotation.\n\nDISCOVER can be used to extract various helpful indicators to analyze human behavior such as transcription, facial expression, or emotions. Furthermore, we presented a prototypical workflow for the exploratory analysis of human behavior in new data. In the future, we plan to further enhance the predictive capabilities of DISCOVER by integrating new processing modules and improving the workflow."}]}