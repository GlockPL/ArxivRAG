{"title": "VOVTRACK: EXPLORING THE POTENTIALITY IN\nVIDEOS FOR OPEN-VOCABULARY OBJECT TRACKING", "authors": ["Zekun Qian", "Wei Feng", "Ruize Han", "Linqi Song", "Junhui Hou"], "abstract": "Open-vocabulary multi-object tracking (OVMOT) represents a critical new chal-\nlenge involving the detection and tracking of diverse object categories in videos,\nencompassing both seen categories (base classes) and unseen categories (novel\nclasses). This issue amalgamates the complexities of open-vocabulary object de-\ntection (OVD) and multi-object tracking (MOT). Existing approaches to OVMOT\noften merge OVD and MOT methodologies as separate modules, predominantly\nfocusing on the problem through an image-centric lens. In this paper, we pro-\npose VOVTrack, a novel method that integrates object states relevant to MOT and\nvideo-centric training to address this challenge from a video object tracking stand-\npoint. First, we consider the tracking-related state of the objects during tracking\nand propose a new prompt-guided attention mechanism for more accurate local-\nization and classification (detection) of the time-varying objects. Subsequently,\nwe leverage raw video data without annotations for training by formulating a\nself-supervised object similarity learning technique to facilitate temporal object\nassociation (tracking). Experimental results underscore that VOVTrack outper-\nforms existing methods, establishing itself as a state-of-the-art solution for open-\nvocabulary tracking task.", "sections": [{"title": "1 INTRODUCTION", "content": "Multi-Object Tracking (MOT) is a fundamental task in computer vision and artificial intelligence,\nwhich is widely used for video surveillance, media understanding, etc. In the past years, plenty of\ndatasets, e.g., MOT-20, DanceTrack, KITTI, as well as the algorithms, e.g., SORT, Tracktor,\nFairMOT, have been proposed for MOT problem. However, most previous\nworks focus on the tracking of simple object categories, i.e., humans and vehicles. Actually, it is\nimportant for the perception of various categories of objects in many real-world applications. Some\nrecent works have begun to study the tracking of generic objects. TAO is the first\nlarge dataset for the generic MOT, which includes 2,907 videos and 833 object categories. The later\nGMOT-40 includes 10 categories and 40 videos with dense objects in each video.\nWith the development of Artificial General Intelligence (AGI) and multi-modal foundation models,\nopen-world object perception has become a popular topic. Open-vocabulary object detection (OVD)\nis a new and promising task because of its generic settings. It aims to identify the various categories\nof objects from an image, including both the categories that have been seen during training (namely\nbase classes) and not seen (namely novel classes). Although OVD has been studied in a series of\nworks, the literature on open-vocabulary\n(multi-) object tracking is rare. The nearly sole work builds a benchmark for"}, {"title": "2 RELATED WORK", "content": "Multiple object tracking. The prevailing paradigm in MOT is the tracking-by-detection frame-\nwork, which involves detecting objects in each frame first, and then associat-\ning objects across different frames using various cues such as object appearance features, 2D motion features, or 3D motion features. Some methods leverage graph neural networks or transformers to learn the association relationships between objects, thereby enhancing tracking performance. To\nbroaden the object categories of the MOT task, the TAO benchmark has been\nproposed for studying MOT under the long-tail distribution of object categories. On this bench-\nmark, relevant methods include AOA, GTR, TET, QDTrack, etc. While these methods perform well, they are still lim-\nited to pre-defined object categories, which makes them unsuitable for diverse open-world scenarios.\nDifferently, this work handles OVMOT problem, which contains categories not seen during training.\nOpen-world/vocabulary object detection. Unlike traditional object detection with closed-set cate-\ngories that appear at the training time. The task of open-world object detection aims to detect salient\nobjects in an image without considering their specific categories. This allows the method to detect objects of categories beyond those present\nin the training set. However, such methods do not classify objects into specific categories and only\nregard the object classification task as a clustering task, achieving classification\nby calculating the similarity between objects and different class clusters. Consistent with the ob-\njective of open-world detection, open-vocabulary object detection requires the identification of cat-\negories not seen in the training set. However, unlike open-world detection, open-vocabulary object\ndetection needs to predict the specific object categories. To achieve this, some\nworks train the detector with text embeddings. Recently,\npre-trained vision-language models,e.g., CLIP connect visual concepts with\ntextual descriptions, which has a strong open-vocabulary classification ability to classify an image\nwith arbitrary categories described by language. Based on this, many works utilize pre-trained vision-language models to achieve open-vocabulary\nobject detection and few-shot object detection. In addition, some studies further enhance the effectiveness of prompt embeddings of class descriptions using prompt\nlearning methods, thereby improving the results of open-vocabulary detection. Different from the\nabove detection methods developed for single image, in this work, we propose a prompt-guided\ntraining method designed for open-vocabulary detection in continues videos, which can effectively\nenhance the detection performance for the video tracking task.\nOpen-world/vocabulary object tracking. There are relatively few works addressing the task of\nopen-world tracking. Related approaches aim to segment or track all the moved objects in the\nvideo or handle the generic object tracking using a class-agnostic detector. Recently, the TAO-OW benchmark is proposed to study open-world tracking problems, but its limitation lies in evaluating only\nclass-agnostic tracking metrics without assessing class-related metrics. To make the setting more\npractical, OVTrack first brings the setting of open vocabulary into the tracking task,\nwhich also develops a baseline method and benchmark based on the TAO dataset. However, the\nmethod in  directly uses an existing OVD algorithm for detection, and its training\nprocess only utilizes static image pairs and ignores the information from video sequences. Differ-\nently, we consider the tracking-related object states for detection, and also propose a self-supervised\nvideo-based training method designed for open-vocabulary tracking, making full use of video-level\ninformation to enhance the performance of open-vocabulary tracking."}, {"title": "3 PROPOSED METHOD", "content": "OVMOT requires localizing, tracking, and recognizing the objects in a video, whose problem formu-\nlation is provided in Appendix 1. We first describe the framework of our VOVTrack, which mainly\nincludes the object localization, object classification, and temporal association modules, as shown\nin Figure 2. For improving the localization and classification, in Section 3.2, we design a tracking-\nstate-aware prompt-guided attention mechanism, which can help the network learn more effective\nobject detection features. For learning the temporal association similarity, in Section 3.3, we pro-\npose a video-based self-supervised method to train the association network, which considers the\nappearance intra-/inter-consistency and category consistency, to enhance the tracking performance.\nLocalization: We employ the class-agnostic object proposal generation approach in Faster R-\nCNN to localize objects for both base and novel categories $C$ in the video. As\nsupported by prior researches, the localization strategy has shown robust generalization capabilities towards the novel object class\n$C^{novel}$. During the training phase, we leverage the above-mentioned generated RPN proposals as the\ninitial object candidates $P$. The localization result of each candidate $r \\in P$ is the bounding box\n$b \\in \\mathbb{R}^4$. For each $b$, we also obtain a confidence $p_c \\in \\mathbb{R}$ derived from the classification head. To re-\nfine the localization candidates, besides the classical non-maximum suppression (NMS), we also use\nthis confidence $p_c$. For a more effective $p_c \\in \\mathbb{R}$ in the classification head, we use a prompt-guided\nattention strategy, which will be discussed in later Section 3.2.\nClassification: Existing closed-set MOT trackers only track several specific categories of objects,\nwhich do not need to provide the class of each tracked object. This way, classification is a new sub-\ntask of the OVMOT. To enable the framework to classify open-vocabulary object classes, following\nthe OV detection algorithms, we leverage\nthe knowledge from the pre-trained model, i.e., CLIP, to help the network\nrecognize objects belonging to the novel classes $C^{novel}$. We distill the classification head using the\nCLIP model to empower the network's classification head to recognize new objects. Specifically,\nas shown in Figure 2, after obtaining the RoI feature embeddings $f_r$ from the localization head, we\ndesign the classification head with a text branch and an image branch to generate embeddings $f_{text}$\nand $f_{img}$ for each $f_r$. The supervisions of these heads are generated by the CLIP text and image\nembeddings. We use the method in Du et al. (2022) for CLIP encoder pre-training.\nFirst, we align the text branch with the CLIP text encoder. For $c \\in C_{base}$, the class text embedding\n$t_c$ of class $c$ can be generated by the CLIP text encoder $T(\\cdot)$ as $t_c = T(c)$. We compute the affinity"}, {"title": "3.2 TRACKING-STATE-AWARE PROMPT GUIDED ATTENTION", "content": "Tracking-state-aware prompt. In the classification head of existing open-vocabulary detection\nmethods, when selecting region candidates for calculating classification loss, they only consider\nwhether the maximal IOU between the candidates and ground-truth box exceeds a threshold. For\nthis tracking problem, we further consider whether the states of the candidates are appropriate for\ntraining the detection network.\nAs is well known, the objects present many specific states during tracking, such as occlusion/out-\nof-view/motion blur, etc, which are more frequent than the object detection in static images. So, it\nis important to identify such object states to achieve more accurate detection and tracking results.\nHowever, these states have often been overlooked in past methods because the labels of these states\nare difficult to obtain, not to mention incorporating them into the network training.\nPrompt, as a burgeoning concept, can be used to bridge the gap between the vision and language\ndata based on the cross-modal pre-trained models. We consider using specially designed prompts\nto perceive the tracking states of the objects and integrate such states into the model training. We\nrefer to such prompts as 'tracking-state-aware prompts'. We specifically employ pairs of adjectives\nwith opposite meanings to describe the object states during tracking. For example, 'unoccluded and\noccluded'. As shown in classification head of Figure 2, we add M pairs of tracking-state-aware"}, {"title": "3.3 SELF-SUPERVISED OBJECT ASSOCIATION WITH RAW VIDEO DATA", "content": "Considering the lack of annotated videos for OVMOT, we develop a self-supervised approach to\ntrain the association network by leveraging the consistency among the same objects during a video.\nObjective function. To learn the object appearance feature, we consider two aspects. The first one\nis intra factor, i.e., the self-consistency of appearance for the same object at different times. The\nsecond one is inter factor, i.e., the mutual-consistency between the appearance and motion cues\nduring tracking. This way, we formulate the optimized objective function as\n$\\max S = S_{intra}(t_i, t_j) + \\alpha \\cdot S_{inter}(t_i, t_j), s.t. \\forall t_i, t_j \\in T_c, \\forall c\\in C,$\nwhere $S$ represents the overall consistency objective to be maximized, $S_{intra}$ and $S_{inter}$ denote the\nintra and inter consistency measures respectively, while $\\alpha$ is a weight to balance them. Besides,\nconsidering the diversity of object categories in the OVMOT problem, we add the object category\nconsistency constraint for the consistency learning. Specifically, we construct the intervals, e.g., $T_c$,\nwhich contain several frames only with the same object category $c$, in which we select $t_i, t_j$. This is\nbecause we aim to learn the feature in a self-supervised manner without ID annotation, the objects\nwith various categories may bring about interference.\nLong-short-interval sampling. First, we consider the interval splitting of $T_c$ in Eq. (4). We split\nthe original videos into several segments of length $L$ and randomly sample the shorter sub-segments\nwith various lengths from each segment. These short-term sub-segments are then concatenated to\nform the training sequence. Such training sequences include long-short-term intervals. Specifically,\nwe select the adjacent frames from the same sub-segment, which allow the association head to learn\nthe consistency objectives under minor object differences. We also select the long-interval video\nframes from different sub-segments, which allow the association head to learn the similarity and\nvariation of objects under large differences.\nCategory-consistency constraint. Then we consider the category consistency constraint in $T_c$.\nAfter obtaining the sampled training sequences as discussed above, we utilize the localization head\nto extract object bounding boxes from each frame in the sequence. Since we only use unlabeled"}, {"title": "3.4 IMPLEMENTATION DETAILS", "content": "In Section 3.1, we use ResNet50 with FPN for localizing candidate regions. We set $\\lambda$ in Eq. (1) as\n0.007. In Section 3.2, we select four pairs of typical tracking state aware prompts, i.e., \u2018complete\nand incomplete', 'unoccluded and occluded', 'unobscured and obscured', and 'recognizable and\nunrecognizable'. We set $d_{low}$ as 0.3, $d_{high}$ as 0.6. In Section 3.3, the segment length L is set as 24.\nWe use the clustering algorithm of K-means. We set the margin $m$ as 0.5 and the $IoU_{thres}$ as 0.9. For\n$N$ frames in each sampled video sequence, we select $C_2^N$ and $C_3^N$ groups of frames to calculate the\nmatrices in Eqs. (5) and (7). We also select $C_2^N$ groups of frames to calculate the inter-consistency\nloss. The weighting coefficients $\\alpha$ is 0.9.\nIn the training stage, we first train the two-stage detector on the base classes of LVIS dataset referenced from for 20 epochs, and use prompt-guided attention\nproposed in Section 3.2 to fine-tune the model's classification head for 6 epochs. Then we train\nthe association head using static image pairs generated from for 6 epochs and self-\nsupervise the association head with TAO training dataset without annotation for\n14 epochs. In the inference stage, we select object candidates $P$ by NMS with IoU threshold 0.5.\nAdditionally, we set the similarity score threshold as 0.35 and maintain a track memory of 10 frames."}, {"title": "4 EXPERIMENTS", "content": "We follow  to select the dataset and metrics for evaluation. We leverage the com-\nprehensive and extensive vocabulary MOT dataset TAO as our benchmark for\nOVMOT. TAO is structured similarly to the LVIS taxonomy, categorizing object\nclasses based on their frequency of appearance into frequent, common, and rare groups. We use\nthe rare classes defined in LVIS as $C_{novel}$ and others as $C_{base}$. We evaluate the performance with the\ncomprehensive metric tracking-every-thing accuracy (TETA), which consists of the accuracies of\nlocalization (LocA), classification (ClsA), and association (AssocA)."}, {"title": "3.1 OVERVIEW AND VOVTRACK FRAMEWORK", "content": null}, {"title": "4.2 COMPARATIVE RESULTS", "content": "We compare our method with the latest trackers, TETer and QDTrack , which are trained on both $C_{base}$ and $C_{novel}$. We include the classical trackers like\nDeepSORT and Tracktor++  trained only on $C_{base}$ and\nenhanced by OVD method ViLD  to achieve open-vocabulary tracking, as baselines.\nWe also compare our method with the state-of-the-art OVMOT method, OVTrack .\nBesides, following we compare with the existing trackers (DeepSORT, Tracktor++,\nOVTrack) equipped with the powerful OVD method, i.e., RegionCLIP trained\non the extensive CC3M dataset.\nAs shown in Table 1, we present the OVMOT evaluation results on the TAO validation and test sets,\ndivided into base classes $C_{base}$ and novel classes $C_{novel}$. We can see that our method outperforms\nall closed-set and open-vocabulary methods on both the validation and test sets. Even though QD-\nTrack and TETer  have seen novel classes during training on\nTAO, our method significantly outperforms them in all metrics on both base and novel classes."}, {"title": "4.3 ABLATION STUDY", "content": "In this section, we conduct the ablation studies on all components proposed in our method, including\nthe ablation of prompt-guided attention, and the self-supervised learning related modules as:\n\u2022 w/o prompt-guided attention ($w_r$): Removing the prompt-guided attention in Section 3.2.\n\u2022 w/o piecewise weight strategy ($d_{low}$ and $d_{high}$): Removing the piecewise weighting strategy pro-\nposed in Section 3.2, by directly using the $w_r$ calculated by Eq. (3).\n\u2022 w/o self-supervised learning: Removing the whole self-supervised learning strategy in Section 3.3\n\u2022 w/o short-long-sampling: Removing the short-long-interval sampling strategy in Section 3.3\n\u2022 w/o category consistency: Removing the category-aware object clustering in Section 3.3."}, {"title": "4.4 QUALITATIVE ANALYSIS", "content": "We conduct some qualitative analysis to more intuitively show the effect of our prompt-guided\nattention and the visualized comparison results of our method with the state-of-the-art algorithm.\nIllustrations of the proposed prompt-guided attention. Figure 3 (a) shows cases of high prompt-\nguided attention, where we can see that the regions often have very distinctive category features, with\nno occlusion, and the image quality of the region is very high. In contrast, Figure 3 (b) presents cases\nof low prompt-guided attention, where we can observe that these regions often have issues such as\nheavy blurriness, occlusion, unclear visibility, and difficulty in identification. Such samples are very\nunsuitable for training the object localization and classification features, which are appropriately\nweakened through state-aware prompt-guided attention."}, {"title": "5 CONCLUSION", "content": "In this work, we have developed a new method namely VOVTrack to handle the OVMOT problem\nfrom the perspective of video object tracking. For this purpose, we first consider the object state\nduring tracking and propose tracking-state-aware prompt-guided attention, which improves the ac-\ncuracy of object localization and classification (detection). Second, we develop an object similarity\nlearning strategy for the temporal association (tracking) using only the raw video data without an-\notation, which unveils the power of self-supervised learning for open-vocabulary tracking tasks.\nExperimental results demonstrate the effectiveness of the proposed method and each component for\nopen-vocabulary tracking."}, {"title": "A APPENDIX", "content": null}, {"title": "APPENDIX 1. OVMOT PROBLEM", "content": "OVMOT requires the tracker to be capable of tracking objects from the open-vocabulary categories\nof objects. We first present the problem formulation of this task from the training and testing stages.\nAt training stage, the training data is {$X^{train}, A^{train} $} that contains video sequences $X^{train}$ and their\nrespective annotations $A^{train}$ of the objects. Given one frame in the video, each annotation $a \\in A^{train}$\nconsists of a 2D bonding box $b = [x, y, w, h]$, a unified ID $d$ over the whole video, and a category\nlabel $c$, where $(x, y)$ is the center pixel coordinates and $(w, h)$ is the width and height of the box,\nthe category belongs to the base class set, i.e., $c \\in C_{base} $.\nAt the testing stage, the inputs consist of video sequences $X^{test}$ and the set of all object classes\n$C = C_{base} \\cup C_{novel}$, where $C_{novel}$ denotes the novel categories not appearing in the training set, i.e.,\n$C_{novel} \\cap C_{base} = \\varnothing$. OVMOT aims to obtain the trajectories of all objects in $X^{test}$ belonging to classes\n$C$. Each trajectory $\\tau$ consists of a series of tracked objects $T_t$ at frame $t$, and each $T_t$ is composed\nof a 2D bounding box $b$, and its object category $c$. Note that, during the testing stage, we need to\nevaluate not only the results on the base class $C_{base}$, but also on the novel class $C_{novel}$. The results on\n$C_{novel}$ can validate the tracker's capability when facing objects from the open-vocabulary categories."}, {"title": "APPENDIX 2. TRAINING DATA ANALYSIS", "content": "As discussed above, we use the training dataset in TAO for association module training. Next, we\nwill analyze our experimental results from the perspective of the data quantity used for training.\nTAO dataset. As shown in the first row of Table 3, we can see that the original TAO dataset has very\nfew annotated frames, with only 18.1k frames, and limited box annotations of 54.7k. This is because\nthe annotations in TAO were made at 1 FPS, resulting in a very limited number of supervised frames\nand available annotations for training a robust tracker.\nAs shown in the next row, in our self-supervised method, we use all the raw video frames without\nrequiring any annotations. We can see that the usable frame quantity has increased to 30 times com-\npared to the original training set (with annotations). Also, the quantity of available object bounding\nboxes for self-supervised training has reached 399.9k, which is 7.5 times the original number of an-\nnotated ones. Moreover, by integrating the long-short-term sampling strategies, we can fully utilize\nall the long-short-term frames within in the TAO raw videos through our self-supervised method,\nthereby achieving better results.\nWe further discuss the results using the training datasets of LVIS and CC3M.\nLVIS dataset. As shown in Table 1 in the main paper, the comparison methods QDTrack and TETer trained on the LVIS dataset with both base and novel classes,\nstill yield poor results in TAO validation and test sets. This may be due to the imbalance in the data\nquantity of base and novel categories. Specifically, as seen in Table 3, although the LVIS dataset has\na large number of frames and annotations for its base classes, the data for its novel classes is very\nlimited, with the number of frames being $\\frac{3}{5}$ and the number of annotations even less, at $\\frac{2}{39}$."}, {"title": "APPENDIX 3. MODULE COMPLEMENTARITY ANALYSIS", "content": "When designing the entire framework, we also consider the complementarity of the localization,\nassociation, and association modules, enabling them to assist each other.\nImproving classification via association. Following the baseline we use the most\nfrequently occurring category within a trajectory as the category for all objects in that trajectory.\nThis approach indirectly improves classification results through better associations. Such assistance\nexplains the reason that category clustering operations in our self-supervised object association train\ning effectively increase classification accuracy, as shown in Table 2 in the main paper.\nImproving localization via association. Additionally, during the association process, some candi-\ndates from the localization module with low detection confidence scores are retained because their\nassociation similarity surpasses the threshold. This association similarity priority strategy ensures\nthat valid targets are retained, which improves the accuracy of localization.\nSimilarly, better localization and classification results also help achieve improved association results,\nmaking our entire framework a cohesive entirety with multiple modules working collaboratively."}, {"title": "APPENDIX 5. MORE DETAILS IN THE PROPOSED METHOD", "content": "Details during training. As mentioned in the main paper regarding the experimental procedure,\ncompared to using the existing Open-Vocabulary Detection (OVD) method directly\nfor localization and classification in OVTrack, we train the OVD process using the\nbase classes of the LVIS dataset and incorporate tracking-related states into the training process (Sec-\ntion 3.2). This significantly enhanced the localization and classification results in open-vocabulary\nobject tracking.\nAdditionally, in the training of the association module, different from our baseline method using the generated image pairs constructed by LVIS, we further introduce a self-supervised\nmethod for object similarity learning (Section 3.3). Specifically, we utilize all the video frames in\nthe TAO training dataset for self-supervised training, which makes full use of the\nconsistency among the objects in a video sequence and greatly improves the association task results."}, {"title": "Long-Short-Interval Sampling Strategy.", "content": "We consider the interval splitting of $T_c$ in Eq. (4). As\nshown in Figure 6, we split the original videos into several segments of length $L$ and randomly\nsample the shorter sub-segments with various lengths from each segment. These short-term sub-\nsegments are then concatenated to form the training sequence. Such training sequences include long-\nshort-term intervals. Specifically, we select the adjacent frames from the same sub-segment, which\nallow the association head to learn the consistency objectives under minor object differences. We\nalso select the long-interval video frames from different sub-segments, which allow the association\nhead to learn the similarity and variation of objects under large differences.\nMetrics. First, the localization accuracy (LocA) is determined through the alignment of all\nlabeled boxes a with the predicted boxes of T without considering classification: LocA =\n$\\frac{|TPL|}{|TPL|+|FPL|+|FNL|}$. Next, classification accuracy (ClsA) is calculated using all accurately localized\nTPL instances, by comparing the predicted semantic classes with the corresponding ground truth\nclasses ClsA = $\\frac{|TPC|}{|TPC|+|FPC|+|FNC|}$. Finally, association accuracy (AssocA) is determined using\na comparable approach, by matching the identities of associated ground truth instances with ac-\ncurately localized predictions AssocA = $\\frac{1}{|TPL|} \\sum_{b \\in TPL} \\frac{|TPA(b)|}{|TPA(b)|+|FPA(b)|+|FNA(b)|}$. The TETA\nscore is computed as the mean value of the above three scores TETA = $\\frac{LocA+ClsA+AssocA}{3}$"}]}