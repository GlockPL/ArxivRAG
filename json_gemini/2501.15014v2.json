{"title": "ON ACCELERATING EDGE AI: OPTIMIZING RESOURCE-CONSTRAINED ENVIRONMENTS", "authors": ["Jacob Sander", "Achraf Cohen", "Venkat R. Dasari", "Brent Venable", "Brian Jalaian"], "abstract": "Resource-constrained edge deployments demand AI solutions that balance high performance with stringent compute, memory, and energy limitations. In this survey, we present a comprehensive overview of the primary strategies for accelerating deep learning models under such constraints. First, we examine model compression techniques-pruning, quantization, tensor decomposition, and knowledge distillation-that streamline large models into smaller, faster, and more efficient variants. Next, we explore Neural Architecture Search (NAS), a class of automated methods that discover architectures inherently optimized for particular tasks and hardware budgets. We then discuss compiler and deployment frameworks, such as TVM, TensorRT, and OpenVINO, which provide hardware- tailored optimizations at inference time. By integrating these three pillars into unified pipelines, practitioners can achieve multi-objective goals, including latency reduction, memory savings, and energy efficiency-all while maintaining competitive accuracy. We also highlight emerging frontiers in hierarchical NAS, neurosymbolic approaches, and advanced distillation tailored to large language models, underscoring open challenges like pre-training pruning for massive networks. Our survey offers practical insights, identifies current research gaps, and outlines promising directions for building scalable, platform-independent frameworks to accelerate deep learning models at the edge.", "sections": [{"title": "1 Introduction", "content": "Integrating Artificial Intelligence (AI) models into tactical edge computing systems is fundamentally transforming military operations and evolving battlefield technology over time. In practical applications of tactical edge computing, AI models are deployed on constrained platforms that have significant limitations in processing power, memory, and communication capabilities. These constraints can hinder the use of complex AI models for real-time tasks such as surveillance, enemy identification, predictive simulations, and navigation.\nThe goal is to develop intelligent and adaptive optimization strategies that can overcome these resource limitations while enhancing the performance of AI models without significantly compromising their accuracy. This is particularly critical in military contexts, where timely and precise decision-making can be pivotal to the success of a mission. Therefore, the ability to effectively utilize AI models in tactical edge computing environments is crucial for military applications. This capability allows for the creation of intelligent systems and machines that can operate in harsh conditions with limited network access, ultimately providing invaluable support to troops in the field.\nOur survey aims to identify emerging approaches for optimizing Artificial Intelligence (AI) models to accelerate model inference. We focus on strategies that meet both model-specific and platform-specific constraints while maintaining accuracy. The survey will examine techniques such as model pruning, quantization, knowledge distillation, neural architecture search, and hardware-aware design. By exploring the latest advancements in AI model optimization, we hope to uncover promising methods that address the challenges of deploying AI models in resource-constrained environments. Ultimately, our goal is to contribute to developing more efficient, effective, and reliable AI-powered systems for critical applications.\nWe categorize optimization strategies into two main families, although in practice, scientists and engineers often combine approaches to enhance performance. First, we will examine Model Compression techniques, which focus on transforming a trained model into a smaller version that still maintains high performance. However, Model Com- pression techniques lack performance guarantees, leading researchers to seek higher-performing and more innovative architectures. Consequently, we will also address Neural Architecture Search (NAS) and Hyper-Parameter Optimization (HPO) methods that explore the configuration space to identify more efficient architectures based on defined objectives. Although these methods are formally separate from the aforementioned optimizations, we will discuss compile-time techniques that reduce model inference time during deployment. Finally, we will provide a detailed overview of experiments involving composite methods, which integrate Model Compression techniques with customized objectives and constraints to explore the NAS and HPO space.\nThis paper is organized as follows: Section 2 introduces model compression techniques and their characteristics, such as quantization, pruning, and knowledge distillation; Section 3 presents neural architecture search; Section 4 discusses the deployment aspects related to compilers and hardware. Section 5 discusses case studies and integrated approaches. Finally, in Section 6, conclusions and some possible research directions are presented."}, {"title": "2 Model Compression Techniques", "content": "Al models, especially deep learning ones, tend to be over-parameterized. This over-parameterization is crucial, making model compression feasible and highly effective. Model compression refers to reducing the size and computational complexity while preserving performance (e.g., accuracy).\nIn tactical edge computing environments, the size and computational demands of AI models can hinder their deployment on resource-constrained platforms. Model compression techniques aim to reduce the size of larger AI models and lower computational requirements during inference, all without significantly compromising performance. Compressing a model enables the use of advanced AI capabilities in tactical edge computing environments with limited computational power, memory, and storage."}, {"title": "2.1 Quantization", "content": "Quantization is the process of reducing the precision of weights and activations, often stored as 32-bit floating-point values, to lower bit representations such as 8-bit or binary, to optimize memory and computational efficiency Jacob et al. [2018]. Recent research on quantization in large language models (LLMs) highlights its importance for reducing model size and improving efficiency. Quantization techniques include post-training quantization (PTQ) and quantization-aware training (QAT) Chen et al. [2024a], with algorithms like LLM-QAT and SmoothQuant addressing challenges such as outliers and activation quantization Wang et al. [2024]. SmoothQuant addresses outliers in activation values by redistributing them to reduce their impact during quantization, enabling stable and efficient low-bit representations. Post- training quantization (PTQ) modifies the model's parameters after training without additional data or computational overhead. In contrast, quantization-aware training (QAT) incorporates quantization during training, allowing the model to adapt to the reduced precision, often yielding better performance. LLM-QAT is specifically tailored for large language models, adapting quantization-aware training techniques to handle the unique challenges posed by their massive parameter scales and diverse activation ranges. CVXQ leverages convex optimization techniques to enable highly flexible and efficient post-training quantization, particularly for extremely large models with billions of parameters Young [2024]. Activation-Aware Weight Quantization (AWQ) Lin et al. [2024a], EfficientQAT Chen et al. [2024a], and QLoRA Dettmers et al. [2023] all present alternatives to training a full-precision model from scratch to implement quantization-aware models. For example, AWQLin et al. [2024a] introduces a novel mechanism to optimize weight quantization based on activation patterns, ensuring high accuracy even under aggressive quantization regimes. Comparative studies reveal that the optimal quantization format (integer or floating-point) varies across layers, leading to the proposal of Mixture of Formats Quantization (MoFQ) for improved performance in both weight-only and weight-activation scenarios Zhang et al. [2024]. The RPTQ employs a channel-wise rearrangement and clustering strategy to manage activation range variations, enabling robust 3-bit activation quantization for large language models Yuan et al. [2023]. Outlier Suppression+ innovates by introducing channel-wise shifting and scaling, effectively addressing asymmetric outliers and achieving near-floating-point performance for both small and large models Wei et al. [2023]. These techniques can significantly reduce memory consumption, with OPT-175B (Meta's Open Pre-trained Transformer) quantization potentially leading to an 80% reduction Yuan et al. [2023].\nMixed precision training has emerged as an effective technique for improving the efficiency of large-scale AI models. Researchers introduced a method using half-precision floating-point numbers for weights, activations, and gradients, while maintaining a single-precision copy of weights to accumulate gradients Micikevicius et al. [2017]. Li et al. Li et al. [2023a] proposed a layered mixed-precision approach, adjusting training precisions for each layer based on its contribution to the training effect. The Channel-Wise Mixed-Precision Quantization (CMPQ) was developed Chen et al. [2024b], allocating quantization precision in a channel-wise pattern for large language models (OPT-2.7B and OPT-6.7B). Other research explored mixed precision low-bit quantization for neural network language models in speech recognition Xu et al. [2021], using techniques such as KL-divergence, Hessian trace weighted quantization perturbation, and mixed precision neural architecture search. These methods have shown notable enhancements in training speed, memory efficiency, and model compression, all while preserving performance in various deep learning applications.\nApplying weight and activation quantization together improves inference speed and model memory metrics, especially for quantization-aware training Menghani [2023], AI [2021]. See Table 1.\nIn certain situations, hardware memory constraints can affect inference performance. Using reduced-size weight representations can help lower latency. However, this process introduces a quantization error, which can be significant. Various algorithms for quantization and de-quantization have been developed to minimize the impact of this error Menghani [2023].\nWeight and activation quantization can be applied before or after training; for example, authors have taken large, pre-trained models and used a quantization policy to their weights and activations AI [2021]. This is an example of Post-"}, {"title": "2.2 Pruning", "content": "Pruning is a model compression technique that reduces the size of a neural network by removing unnecessary connec- tions, aiming to improve computational efficiency while maintaining performance. It is well-understood that neural nets are over-parameterized to guarantee a path to a reasonable local loss minimum during training Choromanska et al. [2014]. By dropping low-importance neurons or channels from the model, we can preserve high performance while minimizing the model's size and, thus, the resulting compute and energy costs.\nPruning techniques can be categorized by their timing (pre-training, during training, post-training), granularity (struc- tured, unstructured), and decision criteria (rule-based, learning-based) as shown in Fig. 2. Pruning methods can be classified based on their structural impact on deep neural networks (DNNs). Unstructured, or weight-wise pruning, is a fine-grained approach where individual weights in the network are pruned Lee et al. [2019]. This method is efficient for small networks, as it reduces the overall number of weights by applying binary masks to the least essential parameters Wang et al. [2020a]. In larger networks like LLMs, authors may merely set pruned weights to 0 instead of maintaining a separate binary mask for all parameters Frantar and Alistarh [2023]. Regardless, to realize the inference speed gains of a pruned network, specialized hardware or deployment-time compilers must be used to realize the gains from this fine-grained sparsification Cheng et al. [2023]. Structured pruning operates at a coarser level by removing groups of objects together: channels He et al. [2017], filters You et al. [2019], transformer attention heads Shim et al. [2021], neurons Ashkboos et al. [2024], layers Men et al. [2024], and blocks Ma et al. [2023]. These structured methods offer inference acceleration regardless of hardware because the sparsification is coarse. If an entire channel is pruned, for example, then at inference, it is ignored; if instead the neurons in that channel are pruned in an unstructured way, the channel is still called at inference time, and the compiler and hardware must make sure to skip 0 weight operations efficiently inside the retained channel. While structured pruning offers consistent inference acceleration across hardware, unstructured pruning often achieves higher compression ratios but requires specialized hardware or compiler optimizations to realize speedups.\nStructured pruning maximizes speed improvements by carefully selecting architectural elements to prune in a way that minimizes performance degradation. Semi-structured pruning is a catch-all category for many methods that blend elements of structured and unstructured pruning Xu et al. [2024a], Ma et al. [2020], Meng et al. [2020]. By sequentially performing coarse pruning and then fine-grained pruning of the remaining structures, authors achieve greater compression levels while retaining performance.\nAs we mentioned, pruning seeks to decrease neural networks' size and computational expenses by eliminating weights or structural elements that minimally affect overall performance. Following this, we will introduce various pruning strategies-which differ mainly by the timing of their application\u2014and emphasize significant research gaps.\nPre-Training Pruning: One forward-looking method is to prune networks based on randomly initialized weights Wang et al. [2020a]. Though appealing for saving training time (because pruned weights require no subsequent computation), this approach risks issues like layer collapse Tanaka et al. [2020]. It has been applied to convolutional neural nets (CNNs) Lee et al. [2019, 2020], but scaling it up to large models remains challenging, given the expense of training even pruned networks.\nPruning During Training: Another strategy embeds pruning into the training loop, iteratively updating which weights remain active. For instance, the RigL algorithm Evci et al. [2021] periodically removes and regrows weights to maintain model capacity. Structured sparsity learning (SSL) Wen et al. [2016], network slimming Liu et al. [2017], and differentiable methods like Differential Sparsity Allocation (DSA) Ning et al. [2020] similarly integrate pruning decisions with gradient-based updates. In neural architecture search, a related concept called Progressive Shrinking alternates between pruning and training to explore potential architectures Wang et al. [2020b]. However, due to high computational costs, these techniques see limited exploration for large-scale models.\nPruning After Training: Pruning after training is widely used thanks to the abundance of large pre-trained models, which can be pruned and then fine-tuned on downstream tasks. Several algorithms focus on mitigating accuracy loss without retraining Frantar and Alistarh [2023], Ashkboos et al. [2024], Kwon et al. [2022], while others adopt a pipeline of pruning plus fine-tuning on a smaller dataset Liu et al. [2021], Ma et al. [2023]. The Lottery Ticket Hypothesis Frankle and Carbin [2019] is particularly influential in this space: it suggests dense, trained networks harbor sub-networks (\"winning tickets\") that can match original performance at a fraction of the size. Research has confirmed these ideas in CNNs You et al. [2022] and transformer-based language models Chen et al. [2021], including variations that prune large networks post-training (e.g., APQ Wang et al. [2020b]) without full retraining. When retraining is needed but cost-prohibitive, lightweight fine-tuning schemes (e.g., LoRA) have shown promise for recovering performance Ma et al. [2023].\nInference Time Pruning: Inference time pruning (or dynamic pruning) adjusts the network per input Rao et al. [2019], bypassing full computation for simpler samples. By pruning based on input complexity Tang et al. [2021], resource usage can be reduced considerably without heavily compromising performance.\nCriteria for Pruning: Different pruning criteria guide which weights or modules to remove. Magnitude-based methods prune weights below a certain threshold Han et al. [2016], while norm-based methods discard entire channels (e.g., using L1 norm Li et al. [2017], He et al. [2017]). Alternative criteria include channel saliency Zhao et al. [2019], neuron sensitivity Santacroce et al. [2023], and module relevance Dery et al. [2024]. Likewise, some work leverages structural graphs to locate highly connected components for more efficient pruning Ma et al. [2023], Zhang et al. [2021]. Researchers have also introduced reinforcement learning to learn pruning policies He et al. [2018], Bencsik and Szemenyei [2022] automatically.\nSelecting a pruning strategy depends on model size, the target deployment scenario, and the available computational budget. Notably, while pruning yields smaller networks, it does not always guarantee proportional speedups on certain hardware backends. To address these gaps, a more quantitative outlook-comparing pruning to alternatives like quantization or tensor decomposition across metrics such as inference speed, memory footprint, and accuracy trade-offs-is needed. Such evaluations would be especially insightful for large-scale models that remain prohibitively expensive to retrain or fine-tune."}, {"title": "2.3 Tensor Decomposition", "content": "Tensor decomposition is a recognized model compression method that approximates a high-rank weight tensor using products of lower-rank factors. This technique decreases the parameter count and computation costs associated with a neural network. As shown in Fig. 3, substantial weight matrices and convolutional kernels can be broken down into smaller parts that effectively perform the same function, significantly reducing memory usage.\nHistorically, tensor decomposition was first applied to fully connected layers, factorizing large weight matrices into the product of two much smaller matrices. Subsequent research extended this idea to convolutional neural networks by decomposing large convolutional kernels into equivalent operations that increase the number of channels while reducing overall computation Li et al. [2023b]. Various decomposition strategies, such as CP, Tucker, and Tensor-Train Decomposition (TTD), provide different ways to balance representation capacity and resource efficiency.\nIn the context of foundation models, tensor decomposition shows particular promise. Because these models typically contain billions of parameters, decomposition can dramatically reduce memory usage and computational overhead, especially when paired with a short period of domain-specific fine-tuning Hajimolahoseini et al. [2021], Saha et al. [2024]. Recent research has focused on compressing large language models (LLMs) using tensor decomposition techniques. TensorGPT applies Tensor-Train Decomposition (TTD) to GPT-2 to compress token embeddings, achieving up to 65.64x compression ratios while maintaining comparable performance to the original model Xu et al. [2023a]. MoDeGPT introduces a modular decomposition framework that partitions Transformer blocks into matrix pairs. It applies various matrix decomposition algorithms, achieving 25-30% compression rates while maintaining 90-95% zero-shot performance on LLAMA-2/3 and OPT models Lin et al. [2024b]. These methods significantly reduce LLM size and computational requirements without substantial performance loss.\nHowever, implementing decomposition-based compression on real-world hardware still poses challenges, including needing specialized kernels to maintain throughput gains. Future directions in tensor decomposition include automated rank selection, hybrid approaches that integrate decomposition with other compression methods (e.g., quantization and pruning), and rigorous exploration of decomposition within advanced architectures like Transformers."}, {"title": "2.4 Knowledge Distillation", "content": "Distillation is a model compression technique that transfers knowledge from a large, complex model (teacher) to a smaller, efficient model (student) with minimal loss in performance, reducing resource costs and enabling deployment in resource-constrained environments Hinton et al. [2015]. Fig. 4 referees to distillation components and approaches.\nThe two essential steps in knowledge distillation (KD) are knowledge elicitation and student distillation Xu et al. [2024b]. The initial step involves extracting knowledge from the teacher model. For instance, the teacher can be given input data to produce the corresponding output logits Hinton et al. [2015]. White-box methods, like feature extraction, provide direct access to the intermediate activations of the teacher model. In contrast, black-box techniques only utilize the output logits from both the teacher and student models Xu et al. [2024b]. Furthermore, in the context of foundation models, other knowledge elicitation strategies can be employed; the teacher might label a dataset for training the student, generate synthetic data through input expansion (creating new inputs based on initial seed data), or use data curation, where teacher feedback is applied to refine the dataset. After eliciting the knowledge, the student model is trained to mimic or approximate the teacher's knowledge. This training typically involves a loss function that minimizes the divergence between student and teacher logits but can also incorporate supervised fine-tuning (which maximizes the likelihood of sequences generated by the teacher) and reinforcement learning, where teacher feedback enhances the student's performance. These strategies aim to narrow the gap between the outputs of the student and teacher, facilitating effective knowledge transfer Xu et al. [2024b].\nNumerous knowledge distillation methods have been established, each with design considerations, techniques, and uses. Adversarial Distillation highlights that the compact model produced through distillation may be susceptible to adversarial threats Goldblum et al. [2020] By employing the Generative Adversarial Network (GAN) framework, Goodfellow et al. [2014] researchers have created several training pathways that include Adversary, Teacher, and Student networks to enhance the final student's robustness Jung et al. [2024]. Additionally, GANs can be utilized for Data-Free Distillation, where the adversarial network produces synthetic examples, allowing the student to be trained without requiring extra data Chen et al. [2019a]. Although adversarial distillation bolsters robustness, it frequently raises computational demands and training complexity, which may hinder scalability.\nIn Multi-Teacher Distillation, multiple teacher models distill their knowledge into a single student model. The knowledge is usually represented as logits or features, providing a comprehensive and balanced distillation from diverse sources Hinton et al. [2015], Zhang et al. [2017]. Authors have developed alternative approaches by combining multi-teacher methods with Bayesian Neural Nets. For example, Vadera et al. show that a Bayesian Neural Net can generate an ensemble of teacher networks; this ensemble can then be used to train a student network as a multi-teacher distillation, with measurable benefits to the student's uncertainty calibration Vadera et al. [2020]. Cross-Modal Distillation involves transferring knowledge from a teacher model trained on one modality to a student model designed to work with a different modality. Authors propose that students deployed on small or expensive-to-label datasets may benefit from this kind of transfer learning, as a large teacher model in one modality may incorporate the knowledge that improves the performance of the student in another modality Gupta et al. [2015]. Recent advancements include combining iterations of self-distillation with cross-modal data for regularization Andonian et al. [2022], and filtering out cross-modal samples during distillation that do not add information to the student modality Huo et al. [2024]. In Graph-Based Knowledge Distillation, the data modality is restricted to graphs, and models to Graph Neural Networks (GNNs) Liu et al. [2023a]. In systems that utilize attention mechanisms, like transformers, Vaswani et al. [2023] a distillation loss can be defined using the difference in attention mechanism between student and teacher; this is defined as Attention-Based Distillation. It has found use across modalities, including text Wang et al. [2020c] and images Wang et al. [2022a]. The attention-based distillation loss is defined in Eq. (1).\n$L_{AT} = \\frac{1}{T} \\sum_{a=1}^{A_{h}} \\sum_{t=1}^{T} D_{K L} (A_{S, a, t} || A_{T, a, t})$ (1)\nwhere $A_{h}$ and $A_{t}$ represent the sequence length and the number of attention heads; more details can be found in this paper Wang et al. [2020c]. The number of teacher and student network layers are denoted with L and M; their last transformer layers are written $A_{f}$ and $A_{M}$\nThis is not to be confused with other methods that define an activation-based distillation loss, also sometimes called attention Crowley et al. [2019], See Eq. (2).\n$L_{AT} = L_{C E}(Y, 0(s)) + \\beta \\sum_{i=1}^{N_{L}} \\frac{||f(\\mathcal{A}_{t}^{i})||_{2}}{||f(\\mathcal{A}_{t}^{i})||_{2}} $ (2)\nwhere s is the output logits of the student network. The cross-entropy loss, $L_{C E}$, is added to the attention transfer component, which ensures that the difference between the spatial distributions of the student and teacher activations at selected layers in the network, $f(\\mathcal{A}_{t})$, and $f(\\mathcal{A}_{t})$, is minimized. This can be viewed as ensuring that the student network pays attention to the same things as the teacher network at those layers.\nIn addition to base model type and data modality, the distillation training task can also be defined in different ways. Offline Distillation is the conventional approach where the teacher model is pre-trained, and its knowledge is then used to guide the training of a smaller student model. Hinton et al. [2015] By contrast, Online Distillation occurs simultaneously, where both teacher and student models are trained together. This approach is appropriate when a high-capacity teacher model is unavailable, and knowledge needs to be distilled on the fly Gou et al. [2021]. This idea can even be extended to teacher-free frameworks, where ensembles of student models distill knowledge into each other simultaneously Zhang et al. [2017], Chen et al. [2019b]. Self-Distillation involves the same model acting as both teacher and student, iteratively refining its own outputs. Some authors argue that self-distillation serves as a form of regularization, preventing overfitting by softening the output logits Mobahi et al. [2020]. Other researchers utilize self-distillation to reduce the model size by transferring knowledge from deeper layers to shallower layers through an iterative process Zhang et al. [2019].\nGiven the size and scope of tasks for Foundation Models (FMs), there are numerous additional requirements defined by authors and various formulations of distillation to meet these requirements. In our survey, we adopt the framework proposed by Xu et al. Gou et al. [2021], which encompasses a broad definition of distillation. For instance, if a teacher model labels a dataset used to train a student model, we consider this a form of distillation since the underlying knowledge is still transferred, even if specific distillation losses are not explicitly defined.\nContext Following refers to the student model's ability to effectively interpret and respond to complex user inputs or contexts. Authors may view self-instruction as a form of self-distillation, where models generate their own input-output examples to further fine-tune themselves for specific tasks Wang et al. [2022b]. This approach can enhance the reasoning capabilities of smaller FMs by leveraging the strengths of larger models. Similarly, student models may select or filter data used by the teacher for fine-tuning, representing another form of self-distillation Li et al. [2024]. Additionally, schemes like ORCA Mukherjee et al. [2023] utilize step-by-step explanations from large foundation teacher models to train smaller models in reasoning through tasks. For conversational models, self-distillation with feedback involves the large teacher model ranking the utility of multiple proposed outputs from the small model, using this feedback to refine the small model's responses Xu et al. [2023b]. Recently, Knowledge-Augmented Reasoning Distillation (KARD) Kang et al. [2023] has been introduced to address the challenge of smaller FMs memorizing vast amounts of data required for expert-level question answering. KARD involves retrieving data from external sources and using it to fine-tune the small model's responses, enhancing both distillation for small FMs and the performance of general-purpose models Asai et al. [2023].\nAlignment pertains to the qualitative ability of a machine learning algorithm to understand and respond in accordance with human-intuitive values. Recent benchmarks for this quality in FMs include MT-Bench Zheng et al. [2023] and AlpacaEval Dubois et al. [2024]. In this context, distillation into a smaller model has been observed to decrease alignment with user intent Tunstall et al. [2023]. Ongoing efforts aim to develop methods for distilling teacher- model alignment into student models and enhancing the alignment of the resulting students. For example, Anthropic's Constitutional AI et. al. [2022] evaluates responses based on a concise set of human-generated rules to ensure alignment.\nAgent-Like Behavior involves enhancing the autonomous capabilities of the student model, enabling it to plan and execute actions in response to inputs by utilizing external\"tools\" (e.g., other models or APIs) to assist in task completion. One approach involves distilling two student models: the first, a \u2018sub-goal generator', takes the last ten actions of the agent and an overarching task as input to output the current sub-goal. The second student, an 'action generator', takes the current sub-goal, overarching task, and last ten actions to predict the next action Hashemzadeh et al. [2024]. Similar methodologies are found in the Lumos framework Yin et al. [2024a] and the FireAct framework Chen et al. [2023], which utilize a limited number of agent-action trajectories for fine-tuning. Other researchers have focused on enabling FMs to accurately call APIs, minimize hallucinations, and remain robust to distribution shifts over time in API calls by incorporating distillation techniques specific to API-call tasks Patil et al. [2023].\nSkill Distillation enables the student model to specialize in a particular domain, such as Natural Language Processing (NLP) or computer vision. Training a general-purpose student model using a large teacher across extensive datasets is often infeasible due to time and cost constraints. However, by carefully selecting a smaller, domain-specific data distribution, relevant skills can be effectively transferred to the student. Efforts to streamline the distillation process"}, {"title": "3 Neural Architecture Search", "content": "Neural Architecture Search (NAS) automates the design of neural network architectures by systematically exploring a search space of possible configurations. As highlighted by He et al. [2021], NAS is a sub-field of Automated Machine Learning (AutoML) specializing in neural architectures. Fig. 5 outlines concepts and tools related to NAS. It involves:\n1. Defining the search space (e.g., convolutional, recurrent, or fully connected layers),\n2. Selecting a search strategy (or Architecture Optimization method),\n3. Choosing an evaluation method to gauge candidate architectures' performance.\nAdditionally, researchers often distinguish architecture optimization (AO) from hyperparameter optimization (HPO): AO concerns layer configurations and connectivity, while HPO adjusts non-architectural factors such as batch size or learning rate."}, {"title": "3.1 Defining the Search Space", "content": "Neural Architecture Search begins with specifying which architectures are valid candidates. For instance, the architecture might be a sequence of convolutional and pooling layers, or a more complex graph of connected modules. This definition greatly impacts both the quality of results and the computational cost of NAS.\nPrimitive vs. Composite Components Researchers can allow only a set of primitive operations (e.g., \"3\u00d73 convo- lution, ReLU, max pooling\"), creating a large, expressive search space Zoph and Le [2017], Cai et al. [2017]. Such expressiveness can discover novel architectures but may demand intensive computation. Conversely, a higher-level \"cell-based\" or \"motif-based\" approach restricts the search space to composite building blocks (cells), greatly reducing complexity but potentially missing innovative designs.\nLeveraging Encodings An effective way to limit or navigate the search space is by encoding architectures into concise representations:\n\u2022 String-Based Encoding: The network is represented as a sequence of tokens denoting operations and hyperparameters (e.g., [Conv 32 3x3, ReLU, MaxPool, Dense 128]) Zoph and Le [2017], Cai et al. [2017].\n\u2022 Graph-Based Encoding: A directed acyclic graph (DAG) describes data flow, with edges as operations and nodes as data tensors (or vice versa). This DAG can be flattened as an adjacency matrix or represented path-by-path"}, {"title": "3.2 Search Strategies", "content": "Supernetworks Once-For-All (OFA) training Cai et al. [2020] is a widely-known supernetwork approach. One trains a large model that conceptually contains multiple sub-networks (created by pruning layers or channels). Each sub-network is then evaluated with minimal additional fine-tuning, drastically cutting the repeated training cost. This idea is related to Progressive Shrinking, where the model is iteratively pruned and fine-tuned to preserve performance Wang et al. [2020b]. However, supernetworks can be impractical for extremely large architectures (e.g., LLMs) because the combined supernetwork might exceed feasible memory or training time Xie et al. [2020]. Moreover, if the supernetwork's weight-sharing scheme is biased, the sub-network performance can be noisy or misleading.\nHypernetworks A hypernetwork predicts the weights for candidate architectures, bypassing the need to train each from scratch Li et al. [2020], Liu et al. [2019]. Graph Hypernetworks (GHNs) Zhang et al. [2018] extend this to a graph neural network (GNN) that reads an input DAG (the candidate architecture) and outputs all free parameters for that network. Evaluating many architectures thus involves:\n1. Feeding each architecture's graph into the GHN;\n2. Generating the corresponding weights;\n3. Quickly measuring its performance (e.g., on a small validation set).\nSince the GHN itself is trained only once, large-scale searches become more computationally tractable.\nPath-Transformation In path-transformation Cai et al. [2018], one starts with a large model and gradually transforms it by adding or pruning layers, guided by reinforcement learning (RL) or heuristic rules. Weight sharing between unchanged components avoids retraining from scratch each time. After each transformation, the model is quickly fine-tuned; the search strategy monitors validation metrics to decide the next step."}, {"title": "3.3 Objectives in NAS", "content": "NAS typically seeks to maximize performance (e.g., accuracy) and/or minimize resource use (e.g., latency, memory). However, fully training each candidate to measure accuracy is often impractical with a large search space. Hence, authors have proposed multiple surrogates or predictors to approximate accuracy.\nAccuracy Predictors and Early Stopping Instead of full training, an accuracy predictor Wang et al. [2020b], Cai et al. [2020] can take an architecture encoding as input and output an estimated accuracy. The predictor itself is learned from a small subset of architectures that have been fully trained. Alternatively, early stopping techniquesZhang et al. [2022] use partial training to guess final accuracy, which cuts search time. However, these predictions can be highly variable if the underlying loss surface is rugged or if the training set for predictor modeling is too small Choromanska et al. [2014], Xie et al. [2020], White et al. [2021].\nLatency and Parameters Latency (inference time) is crucial for real-time tasks. Yet, parameter count (or FLOPs) and latency do not necessarily correlate on modern hardware Li et al. [2021], Zhang et al. [2022]. In some cases, a model with more parameters can run faster if its architecture aligns better with a device's parallelization. Consequently, many researchers measure latency on target hardware, use a lookup table, or train a latency predictor model for quick estimates Tan et al. [2018], Wang et al. [2020b].\nEnergy and Robustness In battery-limited edge environments, energy consumption can be a direct objective Zhou et al. [2024]. Other specialized objectives include adversarial robustness Wu et al. [2024] and uncertainty calibration, depending on domain requirements."}, {"title": "3.4 Multi-Objective Optimization and the Pareto Frontier", "content": "Real-world applications frequently require multi-objective optimization (MOO). For instance, accuracy, latency, and memory can all matter simultaneously. A configuration is Pareto-optimal if no objective can be improved without worsening another Freitas [2024", "2024": ".", "2018": "for instance, uses a function that blends accuracy with latency constraints. One drawback is that scalarization can bias solutions toward certain regions of the Pareto frontier Zhang et al. [2022"}, {"2024": "."}]}