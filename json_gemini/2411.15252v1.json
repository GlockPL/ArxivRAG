{"title": "LocRef-Diffusion: Tuning-Free Layout and Appearance-Guided Generation", "authors": ["Fan Deng", "Yaguang Wu", "Xinyang Yu", "Xiangjun Huang", "Jian Yang", "Guangyu Yan", "Qiang Xu"], "abstract": "Recently, text-to-image models based on diffusion have achieved remarkable success in generating high-quality images. However, the challenge of personalized, controllable generation of instances within these images remains an area in need of further development. In this paper, we present LocRef-Diffusion, a novel, tuning-free model capable of personalized customization of multiple instances' appearance and position within an image. To enhance the precision of instance placement, we introduce a Layout-net, which controls instance generation locations by leveraging both explicit instance layout information and an instance region cross-attention module. To improve the appearance fidelity to reference images, we employ an appearance-net that extracts instance appearance features and integrates them into the diffusion model through cross-attention mechanisms. We conducted extensive experiments on the COCO and OpenImages datasets, and the results demonstrate that our proposed method achieves state-of-the-art performance in layout and appearance guided generation.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancements in image generation models, particularly those trained on large-scale datasets [1], have significantly improved the quality and diversity of generated images. Although text-based guidance is valuable, it may lack the precision and intuitiveness needed for controlling the output. To address this, conditioning methods such as edges, normal maps [2], and semantic layouts [3]-[6] have been introduced to offer finer control. Some methods [7]-[12] use reference images to guide appearance features. These methods can be categorized into fine-tuning and tuning-free approaches. Among tuning-free methods, adapter-based approaches [9] have shown strong performance in both image quality and computational efficiency. However, when generating images with multiple instances, existing methods often exhibit deficiencies in localization precision and reference image fidelity. Specifically, generated objects often fail to align accurately with designated locations. With regard to fidelity, there is a low degree of similarity between the generated objects and reference images. Additionally, feature leakage becomes more pronounced as the number of target instances increases.\nTo address the aforementioned issues, we propose a layout and reference image-guided tuning-free image personalization framework. Firstly, we design an image projection module named Appearance-Net to extract the detailed features of instances. Appearance-Net can effectively extract foreground features of instances while suppressing most background noise, thereby achieving high-fidelity feature transfer. Secondly, Layout-net is designed to precisely control the placement of objects within the scene. Region-aware cross-attention is applied to avoid cross-attention leakage among multiple instances. Our proposed method ultimately achieves fine-grained control over the position and appearance of multiple subjects in the image. Experimental results demonstrate that our method outperforms state-of-the-art techniques in layout and appearance guided generation.\nOur contributions are summarized as follows:\n\u2022 To advance the development of controllable vision generation, we introduce the LocRef-Diffusion model to precisely guide the positioning of multiple objects while ensuring high-fidelity feature transfer.\n\u2022 By preserving the pre-trained weights of the base model and incorporating new projection modules and localization layers, our model enables tuning-free open-world customized image generation.\n\u2022 Extensive experiments on COCO and OpenImages demonstrate that our model's zero-shot performance on customized image generation tasks significantly outperforms previous state-of-the-art methods."}, {"title": "II. RELATED WORK", "content": "Recent advancements in generative models have highlighted the success of diffusion model [13]. DALL-E2 [14] employs a diffusion prior module in conjunction with a series of diffusion decoders to create high-resolution images. Imagen eliminates the need for latent representations by directly applying diffusion to pixel-level data. Latent diffusion models (LDMs) [16] execute this diffusion process within the latent space of a variational autoencoder [17]. Among these advancements, the development of Stable Diffusion has been particularly rapid.\nB. Layout-to-Image Generation\nBecause text lacks the ability to precisely determine the positioning of generated instances. Some Layout-to-Image approaches build upon pre-trained T2I models by incorporating layout information into the generation process GLI-GEN [7] incorporates additional gated self-attention layers into pre-trained diffusion models to manage layout control, while MIGC [19] employs innovative layout attention modules specifically designed for handling bounding boxes.\nC. Customized Image generation\nRecently, there has been a growing interest in developing tuning-free methods for stylized image generation, as seen in [9], [11]. These methods utilize lightweight adapters to extract image features and inject them into the diffusion process via self-attention or cross-attention. IP-Adapter [9] and InstantStyle [11] share a similar concept where a decoupled cross-attention mechanism is introduced to separate the cross-attention layers for text features and image features. StyleAlign [20] swap the key and value features of the self-attention block in the original denoising process with those from a reference denoising process. Moreover, although these methods can generate high-fidelity images, users cannot specify the scenario and location of the target object."}, {"title": "III. METHOD", "content": "The pipeline of LocRef-Diffusion is illustrated in Fig.1. Given target objects, their specified locations, and a caption, LocRef-Diffusion generates a cohesive object-scene composition with high fidelity and diversity. To accomplish these outcomes, we introduce two key components: Layout-net and Appearance-Net. Layout-net is designed to precisely control the placement of objects within the scene, ensuring that they are positioned according to the specified locations. This is achieved by leveraging explicit layout information and an instance region cross-attention module, which guides the diffusion process to generate objects in the correct locations. Appearance-Net, on the other hand, is responsible for extracting appearance features from reference images. By integrating these features into the diffusion process through cross-attention mechanisms, Appearance-Net ensures that the generated objects maintain a high degree of similarity to the reference images, thereby preserving the fidelity and realism of the generated content. Collectively, these components empower LocRef-Diffusion to generate highly realistic and customizable object-scene compositions that accurately reflect the input specifications, while maintaining diversity and visual quality\nat the forefront of current text-to-image generation models.\nA. Observations\nImprecise localization. Previous approaches, such as GLIGEN, MIGC, LayoutDiffusion [21], typically convert the coordinates into embeddings through Fourier transforms. GeoDiffusion [22] discretizes the continuous coordinates by dividing the image into a grid of location bins, and each location bin is converted into location embedding. These embeddings are then integrated with the corresponding text features and embedded into the diffusion process. We have observed that Fourier transforms and grid discretization methods can introduce inaccuracies in positional control, often resulting in objects that are either excessively large or insufficiently sized relative to the designated bounding volume. These limitations highlight the need for more precise methods to accurately represent spatial coordinates within designated boundaries.\nCross-Attention Leakage. As discussed in Previous works [19], [21], When addressing multi-instance prompts, Stable Diffusion faces challenges with attribute leakage, which can manifest in two primary forms: Textual Leakage, owing to the causal attention masks utilized in the CLIP encoder, the tokens representing subsequent instances may exhibit semantic confusion. This phenomenon arises when the encoder fails to adequately distinguish between the attributes of different instances. Spatial Leakage, the cross-attention mechanism in Stable Diffusion lacks precise position control [19], resulting in instances affecting the generation of each other's regions. Addressing these forms of leakage is crucial for achieving high-fidelity and semantically coherent multi-instance generation.\nB. Appearance-Net\nWe employ a pre-trained CLIP image encoder to extract image features from visual prompts. During the training phase, the CLIP image encoder remains frozen. Given that reference images typically consist of both foreground and background elements, our objective is to effectively extract foreground features. While an instance segmentation model could be utilized to isolate a cleaner foreground, this approach introduces additional computational overhead. Moreover, our experiments will demonstrate that even without incorporating an instance segmentation model, our appearance network can effectively extract relatively clean foreground features. To achieve this, we employ a trainable projection network designed to differentiate between foreground and background features. Once trained to convergence, this projection network predominantly outputs foreground features while suppressing most background noise. The projection network used in this study comprises two linear layers and a normalization layer.\nC. Layout-net\nIn contrast to previous approaches [7], [19], [22], our approach involves the following steps to precisely control the generation boundaries of the foreground and simultaneously avoid cross-attention leakage as shown in Fig. 1.\n\u2022 First, on a 512x512 black background image, we designate multiple bounding boxes as the regions where the foreground will be generated. Each corresponding foreground reference image is resized and placed into its respective region within these bboxes, creating a composite image (CI). Subsequently, the CI undergoes feature extraction via the Appearance-Net, resulting in the CI-embedding.\n\u2022 Second, for each reference image (RI), feature extraction is performed independently through the Appearance-Net, producing RI-embeddings.\n\u2022 We add a new trainable cross-attention layer to each cross-attention layer in the original UNet model to insert the CI-embedding, thereby obtaining the hidden states corresponding to the CI-embedding (CIHS).\n\u2022 Fourth, we add another new trainable region cross-attention layer to insert the RI-embeddings, thereby obtaining the hidden states corresponding to the RI-embeddings (RIHS). Here, for each instance, to avoid cross-attention leakage, we apply a instance-mask corresponding to the bbox to restrict the attention region, ensuring that the RI embedding influences only the hidden states within the bbox region.\n\u2022 Fifth, We generate both the foreground mask (FM) and the background mask (BM) from the bounding boxes.\nThen, the final hidden states (HS) of the cross-attention layer can be carried out using the following method:\n$HS = HS * BM + (CIHS + RIHS) * FM$"}, {"title": "IV. EXPERIMENTS", "content": "A. Implementation Details\nData. To train the LocRef-Diffusion, we build a dataset from the COCO2017 and OpenImage. BLIP is used to generate consistent image descriptions.\nDetails. LocRef-Diffusion is built upon the pre-trained sd1.5 [16] model. During training, the original parameters of base model remain frozen, while only the Appearance-Net and Layout-net are trained. The total trainable parameters of Appearance-Net and Layout-net is about 56M, making the LocRef-Diffusion quite lightweight. We use AdamW with a learning rate set to 0.0001 and a weight decay of 0.01, and train the model for 100 epochs with batch size 32. During training, we resize the shortest side of the image to 512 and then center crop the image with 512x512 resolution. We employed several data augmentation to enhance the diversity of the training data, such as random horizontal flips and rotations within 10 degrees. Furthermore, the loss function includes enhanced weighting for foreground regions, with W f set to 2.3 and Wo set to 0.3, guiding the model to focus on fidelity to the reference images. For inference, we use the DDIM [23] scheduler with 50 iteration steps.\n$loss = loss * (BM * W_{b} + FM * W_{f})$\nWf is foreground region loss weight, W\u266d is background region loss weight."}, {"title": "B. Evaluation Metrics", "content": "Localization Assessment. The accuracy of the generated object positions is evaluated using YOLO detection scores. Specifically, we utilize a pre-trained YOLOv8 model to detect the bounding boxes of the generated objects and compare them with the input reference boxes. The precision of the model in generate target localization is quantified by the mean Average Precision (mAP) metric.\nVisual Assessment. We measure the similarity between the generated object and the reference object using the CLIP-I similarity score. This involves calculating the CLIP visual similarity between the generated target and the reference target. The similarity score ranges from 0 to 1, with values closer to 1 indicating a higher proficiency in shape transfer."}, {"title": "C. Results", "content": "We compared our method with other SOTA approaches, including GLIGEN, MS-Diffusion [24], and MIGC. All three methods support multi-instance localization, with MIGC focusing solely on localization accuracy, while GLIGEN and MS-Diffusion additionally offer personalized customization capabilities based on reference images. The comparative results on COCO2017 and OpenImages are presented in Table I. Our method exhibits superior performance in appearance fidelity, as evidenced by the CLIP-I metric, and it significantly outperforms other models. Furthermore, even when compared to state-of-the-art models that only include target localization functionality, our method also achieves strong performance in target localization. The qualitative results in Fig. 2 still show that our method outperforms other SOTA methods. Specifically, GLIGEN exhibits significant feature leakage, MS results in poorer localization."}, {"title": "D. Ablation Study", "content": "To validate the effectiveness of our proposed method, we conducted a series of ablation experiments focusing on three key modules of the Layout-net: Generated Region Guidance, Instance-Aware Fusion, and Instance Segmentation. As a contrast, the base model represents using only the composite image (CI) mentioned in Layout-net to guide image generation. The results, summarized in Table II, demonstrate that all modules contribute positively to the final outcome.\nGeneration Region Guidance. As shown in the table, incorporating Generation Region Guidance significantly improves localization performance, with the YOLOv8 mAP increasing from 0.232 to 0.548 (comparison between configurations 1 and 2). This indicates that our proposed Generation Area Guidance module substantially enhances target localization accuracy.\nInstance Aware Fusion. The addition of the Instance Aware Fusion module further boosts the injection of instance-specific features, such as class, coordinates, and image information, into the generated targets. This leads to improvements in both localization and visual scores, with a particularly notable enhancement in visual performance (comparison between configurations 2 and 3).\nInstance Segmentation. Although the Appearance-Net can already effectively extract foreground object features and filter out most background features, a comparison of configurations 3 and 4 clearly demonstrates that providing an explicit foreground segmentation enhances localization accuracy. This improvement is primarily attributed to the fact that, within the test set, some primary target objects possess appendages. Without the application of a foreground mask filter, the model tends to include these appendages in the generated region, thereby disrupting the layout and leading to an uncontrolled generation range for the main target. However, this result also indicates that our Appearance-Net can effectively extract features of the foreground targets, regardless of whether additional instance segmentation information is provided."}, {"title": "V. CONCLUSIONS", "content": "In this work, we introduced LocRef-Diffusion, a novel, tuning-free model capable of personalized customization of multiple instances' appearance and position within an image. The Appearance-Net is used to extract detailed features of instances, thereby achieving high-fidelity foreground instances feature transfer. The Layout-net is designed to precisely control the placement of instances within the image, ensuring that they are positioned according to the specified locations. Comprehensive experiments are conducted on COCO2017 and OpenImages. Experiment results verify the localization accuracy and appearance consistency of our LocRef-Diffusion."}]}