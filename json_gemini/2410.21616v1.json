{"title": "Identifying Selections\nfor Unsupervised Subtask Discovery", "authors": ["Yiwen Qiu", "Yujia Zheng", "Kun Zhang"], "abstract": "When solving long-horizon tasks, it is intriguing to decompose the high-level\ntask into subtasks. Decomposing experiences into reusable subtasks can improve\ndata efficiency, accelerate policy generalization, and in general provide promising\nsolutions to multi-task reinforcement learning and imitation learning problems.\nHowever, the concept of subtasks is not sufficiently understood and modeled yet,\nand existing works often overlook the true structure of the data generation process:\nsubtasks are the results of a selection mechanism on actions, rather than possible\nunderlying confounders or intermediates. Specifically, we provide a theory to\nidentify, and experiments to verify the existence of selection variables in such\ndata. These selections serve as subgoals that indicate subtasks and guide policy. In\nlight of this idea, we develop a sequential non-negative matrix factorization (seq-\nNMF) method to learn these subgoals and extract meaningful behavior patterns as\nsubtasks. Our empirical results on a challenging Kitchen environment demonstrate\nthat the learned subtasks effectively enhance the generalization to new tasks in\nmulti-task imitation learning scenarios. The codes are provided at this link.", "sections": [{"title": "1 Introduction", "content": "Being able to reuse learned skills from past experiences and conduct hierarchical planning (LeCun\n[2022], Hafner et al. [2022], Gehring et al. [2021]) is crucial for tackling real-world challenging tasks\nsuch as driving: it is meaningful to let higher levels of abstractions perform longer-term prediction\n(of subgoals), while lower levels of policy perform shorter-term actions. The concept of breaking\ndown a task into subtasks\u00b9 can be illustrated through the example of commuting to New York.\nYou are at home, and the overall task, commuting to New York, can be decomposed into\nsmaller, manageable subtasks. This includes subtasks like walking out of the house,\ngetting into the car, driving and catching an airplane. Even more\ngranular, each of these subtasks can be further broken down into smaller actions:\nwalking out of the house involves standing up, grabbing the luggage and\nwalking to the door. This method of decomposing a task into subtasks fits our intuition of\nhow humans perform actions and helps to simplify complex tasks, making them more manageable."}, {"title": "2 Preliminary", "content": "Imitation Learning. In standard Imitation Learning, we collected a set of trajectories D = {Tn}^N_{n=1}\nfrom an expert in a Markov Decision Process (MDP). An MDP is defined by (S, A, P, R, \u03c1\u03bf, \u03b3),\nwith S as the state space, A as the action space, P : S \u00d7 A \u00d7 S \u2192 [0, 1] as the transition probability,\nR:S\u00d7A\u2192R as the reward function, po as the initial state distribution, and y as the discount factor."}, {"title": "3 Subgoal as Selection", "content": "Overview The first step is to understand what a subtask is. We propose to understand subtask by\nbuilding the causal graph and distinguishing different potential structures. We assert that a subtask is\nindicated by a selection variable denoted as gt (subgoal), and will distinguish it from confounder\nand intermediate node (Sec. 3.1). Then, we give a formal definition of subtasks as sub-sequences\nthat can be representative of common behavior patterns and avoid uncertainties in policy (Sec. 3.2).\nWith the understanding of selection (Sec. 3.1) and formal definition (Sec. 3.2), we propose a novel\nsequential non-negative matrix factorization (seq-NMF) method that aligns with these two ideas to\nlearn subtasks from multi-task expert demonstrations (Sec. 3.3)."}, {"title": "3.1 Identifying Selections in Data", "content": "To better understand subtasks, we build a DAG G = {V, E} to represent the generation process of a\ntrajectory by setting each st, at as vertices (V), and add edges (E) by connecting St \u2192 St+1, at \u2192\nSt+1 to represent the transition function P. There are three potential patterns as follows:\nDefinition 4. (Confounder, selection and intermediate node) ct is a confounder if St \u2190 Ct \u2192 at, gt\nis a selection if St\u2192 gt \u2190 at, mt is an intermediate node if st\u2192 mt \u2192 at.\nIn other words, the causal Dynamic Bayesian Network (DBN) (Murphy [2002]) of a series of\n{St, at,...} is one of the following three scenarios:"}, {"title": "3.2 Definition of the Subtask", "content": "In Prop. 1, we provide the sufficient condition to identify selection (subgoal), that is stat | gt. In\nother words, the current action at is affected by both the current state st and the current subgoal gt.\nOnly st alone is not sufficient to determine action, but also the current subgoal gt guides the agent's\naction. e.g. When arriving at a crossroad, and you are deciding whether to turn left or right, it is\nonly when a subgoal (the left road or the right road) is selected, then the subgoal-conditioned policy\n\u03c0g(at, St, gt) is uniquely determined. Therefore, we can define subtasks as sub-sequences that can:\n(1) be representative of common behavior patterns (because gt guided the selection of at) (2) avoid\nuncertainties in policy \u03c0(at | St), that is, different distributions of action predictions given a state.\nThe formal definition goes as in Def. 5.\nDefinition 5. Subtask is defined as a set of J options O = {Oj}^J_{j=j}, s.t. for some partition of\ntrajectories, p = {St, at, ...} as a sub-sequence of (st, at) from any trajectory in and L is the\nmaximum length of the sub-sequence, and also the maximum lag of each subtask:\nmin | J, s.t.\n\\(\u2200p) (\u2203O_j) \u00a3p ~ O_j, and if (\u2203S\u00a1 \u2208 \u00a7p, \u00a7p\u2032(p \u2260 p\u2032))\nthat \u03c0_j (a_t | S_t = Si) \u2260 \u03c0_{j'} (a_t | S_t = Si), then si ~ O_j, O_{j'}, j \u2260 j' (1)\nWe require every sub-sequence to be generated from some option ~ Oj, which means that its first\nstate &p(0) \u2208 Zj and the last state is a termination state Bj (p(-1)) = 1, and the actions in \u00c9p are\ngenerated by \u03c0; (at | St). Importantly, when there are multiple policies available at one state si, i.e.\n\u03c0j(at | St = Si) \u2260 \u03c0j' (at | St = si), then these policies should correspond to different options, Oj\nand Oj, (j \u2260 j'), in order to avoid unmodeled uncertainty for the imitator.\nBy definition, subtasks are our way of recovering a minimal number of options from trajectories,\nand we view sub-sequences in the data as instantiations of options, because they are generated from\ncorresponding SMDPs. Then, each Tn should be some combination of those sub-sequences. On\nthe one hand, we aim to avoid excessively granular partitions that result in a large number of one-step\noptions, because it is essential to capture long-term patterns. On the other hand, we seek a sufficient\nnumber of subtasks to avoid policy ambiguity; when multiple options are available for a single state,\nwe should be able to differentiate them by setting distinct subgoals. Consequently, in situations with\nvarying distributions of action predictions, we employ different subtasks to capture these differences.\nThis involves selecting subgoals to predict actions that a single policy cannot suffice. For example,\ndetermining to turn left at a crossroad as one subgoal and turning right as another.\nJustification of necessity By this definition of subtasks, we reinforce the necessity of learning\nsubtasks from the perspective of avoiding policy uncertainties in multiple trajectories. Since the\ntrajectories are collected by a variety of human or robot experts under different tasks, they are likely\nto exhibit different optimal policies. If there is an ambiguity about which policy to imitate, i.e.,\nthere are multiple optimal action predictions at hand, we can comprehend it as there is a latent\nvariable gt affecting the prediction of at, namely a subgoal that helps to select action. Learning one\npolicy distribution p(at | st) from a mixture of different policies ignores the variety of behaviors,\nand focuses merely on the marginal policy \u3160(at | st) rather than a subgoal conditioned policy\n\u03c0g(at | St, gt), leading to unsatisfactory results. To our knowledge, we are the first ones to give a\ndefinition that eliminates the ambiguity in the definition of subtasks. While previous works provide\nonly general and vague definitions by human intuition (McGovern and Barto [2001], \u015eim\u015fek and"}, {"title": "3.3 Learning Subtasks with Seq-NMF", "content": "In Sec. 3.1, we give the conditions to identify subgoals as selections (verified in experiments in\nSec. 5.1). In Sec. 3.2, we provide a formal definition of subtasks. Combining these concepts, we\nconnect subgoals to subtasks: subgoals are multi-dimensional binary variables Gt \u2208 {0,1}\u221a, while\nsubtasks are sets of options O that generate sub-sequences fp = {St, at, ...}. In particular, we\nuse G)(fp) = 1(\u00a3p ~ Oj) to indicate whether the subgoal of a sub-sequence is generated from\nan SMDP captured by Oj, where 1(\u00b7) is the identify function. Similarly, we use H)([St, at]) =\n1(st \u2208 Ij) to indicate whether st is an initial state of Oj.\nThis formulation can be intuitively understood as follows: subtasks represent the behavior patterns\none might select to perform and are thus exhibited in the expert trajectories. In contrast, the subgoal\nis the selection variable itself, indicating whether or not this behavior pattern has been executed.\nMethod: Sequential Non-Negative Matrix Factorization Learning such a binary coefficient\nmatrix and feature pattern is closely related to the non-negative matrix factorization (NMF) (Lee and\nSeung [1999]), which focuses on decomposing a complex dataset into a set of simpler, interpretable\ncomponents while maintaining non-negativity constraints. A thorough review of NMF is in Appx. G.\nIn our setting, instead of using a vector to represent a pattern, we want the pattern to be tem-\nporally extended, sharing the merit of those works of extensions of NMF (Smaragdis [2004,\n2006], Mackevicius et al. [2019]). We set xt = (st; at) and concatenate xt across time to\nform a data matrix X. We then transform the problem of learning subgoals into a matrix fac-\ntorization problem: identifying the repeated patterns within sub-sequences. The entire data\nmatrix (trajectories) can be reconstructed with subtasks representing temporally extended pat-\nterns, and binary indicator representing which option is selected at each time step. We define\n0 = [01 02\nOj ]\u2208 RD\u00d7J\u00d7L as a three-dimensional tensor, with J subtask patterns\nO; \u2208 RD\u00d7L, and H = [ h1_h2\nh\u0442 ] \u2208 {0,1}J\u00d7T as corresponding indicator binary\nmatrix, where D = ds + da is the dimension of xt, and L is the maximum length of each subtask\npattern. We construct the decomposition as:\nX \u2248 O * H, where(O * H)_{dt} = \\sum_{j=1}^{J}\\sum_{l=0}^{L-1}OdjlH_{j(t-l)}, (2)\nand * is a convolution operator that aggregates the patterns across time lag L. Then the optimization\nproblem is transformed into:"}, {"title": "4 Transfering to New Tasks", "content": "After learning subtasks from demonstrations, it is intuitive to utilize them by augmenting the action\nspace with the subgoal selection. We learn a new policy that takes in both the state and subgoal as\ninput, same as in other literature (Sharma et al. [2019b], Kipf et al. [2019], Jing et al. [2021], Jiang\net al., Chen et al. [2023]).\nAmong all the different ways to perform IL, such as Behavioral Cloning (BC) (Bain and Sammut\n[1995]), Inverse Reinforcement Learning (IRL) (Abbeel and Ng [2004], Ng et al. [2000], Ziebart et al.\n[2008], Finn et al. [2016]), and Generative Adversarial Imitation Learning (GAIL) (Ho and Ermon\n[2016], Fu et al. [2018]), we adopt a GAIL-based approach that matches the occupancy measure\nbetween the learned policy and the demonstrations through adversarial learning to seek the optimal\npolicy. The overall objective is:\n\\min_{\\pi_{\\theta}} \\max_{D_{\\epsilon}} E_{(s_t,a_t,g_t) \\sim \\tau}log(1 \u2013 D_e(s_t, a_t, g_t)) + E_{(\\tilde{s}_t,\\tilde{a}_t,\\tilde{g}_t) \\sim \\pi_{\\theta}} log(D_e(\\tilde{s}_t,\\tilde{a}_t, \\tilde{g}_t)), (5)\nwhere \u03c0g is the augmented policy, and De : S \u00d7 A \u00d7 J \u2192 [0, 1] is a parametric discriminator that\naims at distinguishing between the samples generated by the learned policy and the demonstrations.\nThe policy is trained via PPO (Schulman et al. [2017]). Algorithms for the overall training procedure\nand the execution of policy can be found in Appx. C.2. Comparison with other IL algorithms is\nprovided in our experiment in Sec. 5.3."}, {"title": "5 Experiments", "content": "The goals of our experiments are closely tied to our formulation in Sec. 3. In Sec. 5.1, we verify the\nexistence of selection in data. In Sec. 5.2, we evaluate the effectiveness of seq-NMF in recovering\nsubgoals as selections. In Sec. 5.3, we demonstrate the learned subgoals and subtasks are transferable\nto new tasks by re-composition."}, {"title": "5.1 Verifying Subgoals as Selections", "content": "We first verify the theory in Sec. 3 that selections can be identified in data by the following CI tests:\n(1) whether stat | gt holds, (2) whether gt | at+1 | gt holds, and (3) whether st+1 \u22a5 gt | St, at\nholds. Our empirical results provide an affirmative answer to all these questions, suggesting that\nselections do exist, and they can serve as subgoals.\nSynthetic Color Dataset We follow the didactic\nexperiment in Jiang et al. and construct color se-\nquences similarly. The dataset consists of repeating\npatterns of repetitive color, either with 3 or 10 steps\nof time lag in each pattern, of which Fig. 4 is an illus-\ntration. Details about the construction and the CI test\nresults are elaborated in Appx. D.1.\nDriving Dataset In the driving environment, there are two tasks to finish. As shown in Fig. 5, both\ncars start at the left end, either facing up or down. The first task is to drive to the right end following\nthe yellow path, while the second task is to follow the blue path. Each state is represented by a tuple\n(x, y, 0) \u2208 R\u00b3 (coordinates and orientation), and each action is the angular velocity \u2206\u03b8\u2208 R. We\ncollected 100 trajectories in total, 50 for each task."}, {"title": "5.2 Evaluating the Effectiveness of Seq-NMF", "content": "Next, we evaluate the seq-NMF in recovering\nselections and discovering subtasks. Results in the\nDriving dataset are in Fig. 6 while those for Color are\nin Appx. D.4. The y-axis represents the dominance\nof each subtask in explaining the whole sequence.\nWe plot two sequences and each lasts for around 110\nsteps. Our algorithm finds the crossing point and\nautomatically partitions every trajectory into three sub-\ntasks (before reaching the first crossing point, in the\nmiddle, and after reaching the second crossing point)."}, {"title": "5.3 Transfering to New Tasks", "content": "Kitchen Dataset The evaluation of the imitation learning is\nperformed on a challenging Kitchen environment from D4RL(Fu\net al. [2020]) benchmark. Each agent is required to perform a\nsequence of subtasks including manipulating the microwave,\nkettle, cabinet, etc. Each task is composed of 4 subtasks and the\ncomposition is unknown to the agent. We use the demonstrations\nprovided by (Gupta et al. [2019]) for reproducibility, which\nonly contain state and action pairs but not reward. We use the\ndemonstrations of two tasks for training, and require the agent\nto accomplish a target task that has a different composition of\nsubtasks than any one of the demonstrations, but the units of\nsubtasks have been performed. Detailed description is included in Appx. D.2.\nBaseline methods We compare our method with the following state-of-the-art (SOTA) hierarchical\nimitation learning methods to prove its efficacy: (1) H-AIRL (Chen et al. [2023]) is a variant of the\nmeta-hierarchical imitation learning method proposed in Chen et al. [2023] that doesn't incorporate\nthe task context, and is learning an option-based hierarchical policy just as in our setting. (2)\nDirected-info GAIL(Sharma et al. [2019b]) and (3) Option-GAIL (Jing et al. [2021]) are two other\ncompetitive baselines that are proven to be effective in solving multi-task imitation learning.\nResults on new tasks with 4-subtasks We show the results on new tasks with 4-subtasks in Fig. 8.\nWe use the episodic accumulated return as the metric. Training is repeated 5 times randomly for\neach algorithm, with the mean shown as a solid line and the standard deviation as a shaded area.\nWe observe that our method outperforms all the baselines in both tasks. The agent trained with\nthe selection-based subtasks can quickly adapt to the new task with different subtask compositions,\nachieving a higher return. The results show that our method can effectively transfer the subtask\nknowledge learned from the demonstrations to new tasks.\nResults on new tasks with 5-subtasks\n(generalized) We conduct additional\nexperiments by considering a distribu-\ntion shift problem that involves longer-\nhorizon tasks, and plot the results in\nFig. 9. Specifically, under the Kitchen\nenvironment, we keep the same train-\ning set of tasks (each task is composed\nof 4 sequential manipulation subtasks),\nand tests the method's generalizability\nto a new task with different permutation\nof one more subtask, i.e. 5 sequential"}, {"title": "6 Conclusion", "content": "In short, we target at a subtask decomposing problem. While previous research has not sufficiently\nanalyze the concept of subtasks, which might lead to an inappropriate inference of subgoals, we\npropose to view subtasks as outcomes of selections. We first verify the existence of selection variables\nin the data based on our theory. From this perspective, we recognize subgoals as selections and\ndevelop a sequential non-negative matrix factorization (seq-NMF) method for subtask discovery. We\nrigorously evaluate the algorithm on various tasks and demonstrate the existence of selection and\nthe effectiveness of the method. Finally, our empirical results in a challenging multi-task imitation\nlearning setting further show that the learned subtasks significantly enhance generalization to new\ntasks, suggesting exciting directions on uncovering the causal process in the data, also showing a new\nperspective on improving the transferability of policy.\nThe main limitation in this work lies in that we are not yet able to deal with the scenarios where there\nmight be multiple factors at work, such as the case where there are both underlying confounders and\nselections. Confounders might be related to other types of distribution shift, e.g. change in the system\ndynamics, robot embodiment, etc. In future work, we will investigate the causal process in other\ncontexts, and aiming at providing a more general framework for subtask discovery."}, {"title": "A Related Works", "content": "Skill or option discovery While there are multiple terms across several lines of research, e.g. skills or\noptions, they all refer to the same problem and focus on learning a set of subtasks that can be used to solve a\ncomplex task via hierarchical planning. The problem is also our main focus here. The subgoals are defined\nheuristic as bottle-neck regions in tasks, for example, by finding states that are visited frequently by the expert\n(McGovern and Barto [2001], \u015eim\u015fek and Barto [2004]), finding a minimal cut of the state transition graph\n(\u015eim\u015fek et al. [2005]), or by clustering and other similarity measures (Wang et al. [2014], Paul et al. [2019]).\nRecently, the skills are learned through modeling skills as latent variables and maximizing likelihood. For\nexample, there are deep option learning methods (Krishnan et al. [2017], Fox et al. [2017], Bera et al. [2020]),\nhierarchical clustering (Zhu et al. [2022]), information theory-induced methods (Sharma et al. [2019a], Lee\n[2020]), segmentation and abstractions modeling (Kipf et al. [2019], Tanneberg et al. [2021]), and methods that\nleverage other criteria such as minimal description length (Zhang et al. [2021], Jiang et al.). However, these\nmethods do not provide a clear definition of skills or options, and the learned partitions face challenges in their\ninterpretability. We can address the problem by identifying sub-goals as selections that enhance interpretability.\nMulti-task IL (MIL) Current MIL methods aim at training a policy that can be conveniently adapted to\nmultiple tasks, often done so by incorporating a task context variable to condition on, such as (Seyed Ghasemipour\net al. [2019], Yu et al. [2019]). However, the task variable is not always available and these methods neglect\nthe hierarchical structure in accomplishing the task. Then Chen et al. [2023] takes into account skill/option\nframework to build a hierarchical policy along with the context variable. However, this approach inherently\nsuffers from optimization problems and require demanding hyperparameter tuning process, due to the overall\ncomplexity of the framework, while we provide a solution with disentangled optimization procedures that is\nefficient to deploy.\nWe also build a rich literature in the following topics, for which our insights provide important implications:\nCausal RL and IL This line of research introduces causal ideas to improve IL problems, by conditioning\nthe imitation policy on the causal parents (De Haan et al. [2019]) or by learning a compact causal structure\nbetween the states, actions and rewards (Lee et al. [2021]). However, the selection structure under this context is\nstill not sufficiently investigated, and we provide this new perspective.\nSelection bias In causal literature, previous works mainly focus on understanding selection as a distortion\nof data, and aim at alleviate the selection effect (Spirtes et al. [1995], Hern\u00e1n et al. [2004], Zhang [2008],\nBareinboim et al. [2014], Zhang et al. [2016], Correa et al. [2019], Forr\u00e9 and Mooij [2020], Versteeg et al. [2022],\nChen et al. [2024]). However, though controlling the selection bias is important, they fail to view the selection\nas a source of information that can facilitate learning and inferencing. Zheng et al. [2024] propose methods to\ndiscover the selection structure in the sequential data. We explore the underlying selection structure in imitation\nlearning settings and fill the missing gaps in comprehensively understanding the selection process in the data."}, {"title": "B Proofs", "content": "B.1 Proof of Proposition 1\nProposition 1. (Sufficient condition) Assuming that the graphical representation is Markov and faithful to the\nmeasured data, if stat dt, then dt is a selection variable, i.e., dt := gt, under the assumption that:\n1. (confounder, selection, and intermediate nodes can not co-exist) At each time step, dt can only be one\nof {ct, gt, mt}. (For a relaxation of this assumption, see Appx. B.4).\n2. (consistency in a time series) At every time step, dt plays the same role as one of {ct, gt, mt}.\nWe can use d-separation Pearl [2009] to distinguish the dt := gt case from the other two kinds of dependencies.\nStd atdt if dt := gt because there is a path St \u2192 gt \u2190 at, SO St \u2717 at | dt. But if dt := ct or\ndt := mt, because St Id at | Ct in the confounder case and st Id at | mt in the intermediate node case, then\nStat dt.\nB.2 Proof of Proposition 2\nProposition 2. (Necessary and sufficient condition) dt is a selection variable (dt := gt) if and only if\nStat dt and dt at+1 | dt+1.\nIn Prop. 1, we have already proved that st\u2717 at | dt is sufficient to show that dt := gt. For the necessary part,\nwe need to show that if dt is selection, then it entails that dt \u2717 at+1 | dt+1. Because gt d at+1 | gt+1, SO\nby d-separation, gt at+1 gt+1.\nB.3 Proof of Proposition 3\nProposition 3. (Necessary condition) If dt is a selection variable (dt := gt), then st+1 gt St, at. (Such\nindependency does not hold true for confounders case which is discussed in Appx. B.3)\nWhen conditioning on st and at, then gt and st+1 are d-separated. But for a confounder case, ct and st+1\nare not. The difference is essentially because that we have gt gt+1 \u2190 St+1 where there is an unshielded\ncolider gt+1 between gt and st+1 that blocks the path, while ct \u2192 Ct+1 \u2192 St+1 is not blocked. Therefore,\nSt+1 gt St, at, but St+1 Ct | St, at.\nB.4 Relaxation of Assumptions\nWe extend our theory in Sec. 3.1 by two types of relaxations of our assumptions. We provide a more general\ngraphical model that represents the data generation process in Fig. 11. There are two parts that will be discussed\nin this section: (1) the inclusion of co-existence of gt, ct and mt and (2) the higher-order structure behind those\nvariables (e.g. the set of higher-order confounders and selections are denoted as Uc and Us, respectively, in\nFig. 11).\nB.4.1 Co-existance of ct, gt and mt\nThe first point we assert in this section is that it is possible dt does not take only one of {ct, gt, mt}. In the\nmain paper, we only discuss the pure case for simplicity, however, it is also possible that dt can take multiple\ncharacters of {ct, gt, mt}. We argue that even if the data is generated under multiple hidden variables, for\nexample, both confounders and selections are at play, our sufficiency criteria for recognizing the existence of\nselection still holds, and propose the following Prop. 4.\nProposition 4. (Sufficient condition under multiple types of hidden variables) Assuming that the graphical\nrepresentation is Markov and faithful to the measured data, if st\u2717 at dt, then gt must exist in the hidden\nstructure, i.e., gt \u2208 dt (we use dt to denote the set of hidden variables), under the modified assumptions that :\n1. (one or more structures of confounder, selection, and intermediate node) At each time step, dt can be\na subset of {ct, gt, mt}. i.e. dt {ct, gt, mt}.\n2. (consistency in a time series) Similar as in Prop. 1.\nProof In the graphical model of Fig. 11, both confounders ({ct}) and selections ({gt}) are exhibited. Then\nconditioning on dt does not change the key CI criteria we proposed in Prop. 1: Stat | dt, because as long as\nthe path st\u2192 gt at is d-connected (when two variables are not d-separated, they are d-connected) when\nconditioning on gt, the dependency between st and at is preserved, regardless of whether there are confounders\nor intermediate nodes. If gt is not present, i.e. dt {ct, mt}, then there is no dependency between st and at\nwhen conditioning on dt.\nImplications Such a relaxation that includes multiple types of hidden variables is reasonable in real-world\nscenarios. While the observed data is generated by a subgoal-conditioned policy, it is still possible that there is\nsome hidden confounder that influences the joint data distribution. For example, when you are selecting which\nway to go to school, you may be influenced by the weather. Given the same state and action, say, you decide\nto walk out of home, changing the weather will change the distribution of the next state: it causes you to act\notherwise, e.g. stay at home. On the other hand, changing your actions would not influence the weather. In\nthis case, the weather is a confounder that is distinct from the subgoal, but still influences the data generation\nprocess. The main proposition Prop. 1 is one that provides the sufficient condition to identify selections, and\nProp. 4 implies that we could not eliminate the possibility of confounders and intermediates at play together with\nselections. Learning the hidden confounder or intermediate structures along with selections is another exciting\ntopic that we leave for future work.\nB.4.2 Higher-order Structure Behind Data\nIn the main paper, we assume that the true causal graph of\nthe data takes one of the three scenarios in Fig. 2, with no\nhidden high-level structures. A second relaxation we make\nis the inclusion of higher order structures. We argue that\nif there are higher-order structures in the data, and loosen\nthe assumption of a direct adjacency between dt and dt+1,\nthen Prop. 1 still holds true.\nIllustration Take the case of UG (the set of\nhigher-order goals) for example. The idea that there\npotentially could be hierarchical subgoals is natural: we\ntake the example of commuting to New York\nin Sec. 1 as an illustration. The first subtask\nwalking out of the house is further decomposed\nas standing up, grabbing the luggage and\nwalking to the door. Each one of the four behav-\niors can also be understood as smaller subtasks that involve\nmultiple more primitive actions: for walking, you need\nto move your left leg, right hand, then right leg, left hand,\netc. On a grander scale, commuting to New York\ncould possibly be a part of a bigger task of going\non a business trip: commuting to New York,\ngoing to a meeting,\ngiving a presentation. In this example,\nwalking to the door would be the first level of\nsubgoals (gt). Then it contributes to a higher level\nof subgoal: commuting to New York, which we\nrepresent as gt \u2208 UG. Then this second-order subgoal\nfurther contributes to a third-order subgoal, the final goal of\ngoing on a business trip, which we represent as gt \u2208 UG.\nAll these subgoals has a pattern of gt gt gt \u2192...\nwhere the higher levels of subgoals are always selective\nof lower levels of subgoals (achieving the lower-order\ngoals causes us to achieve the higher-order goals). The\nconsecutive subgoals are dependent of each other because\nof conditioning on higher-order subgoals: gt gt+1 | gt,\nbecause conditioning on gt will make gt and gt+1\nd-connected. Noting that this relaxation of high-order\nstructures is practical in the real-world setting, we provide\na corresponding proposition in Prop. 5:\nProposition 5. (Higher-order structure) Assuming that there are higher-order structures behind ct, gt and mt,\nand direct edges between dt and dt+1 are not necessarily at present. Then Prop. 1 is still valid.\nProof The higher-order structure does not change the key CI criteria we proposed in Prop. 1 and Prop. 2.\nWhen conditioning on dt, if dt is a confounder or intermediate, then it d-separates st and at and we will obtain\nStat. Only when dt is a selection/subgoal, it would be a collider on the path between st and at. Thus, only\nin the case of a selection, we will obtain st\u2717 at. Now we have proved that the sufficient condition in Prop.(1)\nstill holds.\nThis relaxation for the high-order structure implies that the direct adjacency between dt and dt+1 is not\nnecessary for the sufficiency criteria to hold."}, {"title": "C Algorithms", "content": "C.1 Algorithm for Seq-NMF\nAlgorithm 1 SeqNMF for learning subtasks\nInput: Data matrix X", "Asim\nOutput": "O", "1": "nInitialize O and H randomly\nSet i = 1\nwhile (i < maxIter) and (\u2206 cost > tolerance) do\nUpdate H using multiplicative update from Eqn. (4)\nRenormalize H so maximum value of H is 1", "H\nReturn": "O", "subgoals\nInput": "Subtask patterns O, expert demonstrations {st, at,\u2026\u2026}.\nInitialize the policy \u03c0g, discriminator De.\nfor each training episode do\nGenerate M trajectories {\u0161t, \u0101t, \u0161t} by exploring in the target environment with \u03c0\u338f and with\nAlgo. 3.\nUpdate De by minimizing LIL based on {st, at, gt} and {\u0161t, \u00e3t, gt}\nTrain ng by PPO (Schulman et al. [2017"}]}