{"title": "PSNE: Efficient Spectral Sparsification Algorithms for Scaling Network Embedding", "authors": ["Longlong Lin", "Yunfeng Yu", "Zihao Wang", "Zeli Wang", "Yuying Zhao", "Jin Zhao", "Tao Jia"], "abstract": "Network embedding has numerous practical applications and has received extensive attention in graph learning, which aims at mapping vertices into a low-dimensional and continuous dense vector space by preserving the underlying structural properties of the graph. Many network embedding methods have been proposed, among which factorization of the Personalized PageRank (PPR for short) matrix has been empirically and theoretically well supported recently. However, several fundamental issues cannot be addressed. (1) Existing methods invoke a seminal Local Push subroutine to approximate a single row or column of the PPR matrix. Thus, they have to execute n (n is the number of nodes) Local Push subroutines to obtain a provable PPR matrix, resulting in prohibitively high computational costs for large n. (2) The PPR matrix has limited power in capturing the structural similarity between vertices, leading to performance degradation. To overcome these dilemmas, we propose PSNE, an efficient spectral sParsification method for Scaling Network Embedding, which can fast obtain the embedding vectors that retain strong structural similarities. Specifically, PSNE first designs a matrix polynomial sparser to accelerate the calculation of the PPR matrix, which has a theoretical guarantee in terms of the Frobenius norm. Subsequently, PSNE proposes a simple but effective multiple-perspective strategy to enhance further the representation power of the obtained approximate PPR matrix. Finally, PSNE applies a randomized singular value decomposition algorithm on the sparse and multiple-perspective PPR matrix to get the target embedding vectors. Experimental evaluation of real-world and synthetic datasets shows that our solutions are indeed more efficient, effective, and scalable compared with ten competitors.", "sections": [{"title": "1 Introduction", "content": "Graphs are ubiquitous for modeling real-world complex systems, including financial networks, biological networks, social networks, etc. Analyzing and understanding the semantic information behind these graphs is a fundamental problem in graph analysis [2, 17, 24, 25, 49]. Thus, numerous graph analysis tasks are arising, such as node classification, link prediction, and graph clustering. Due to their exceptional performance, network embedding is recognized as an effective tool for solving these tasks. Specifically, given an input graph G with n nodes, network embedding methods aim at mapping any node v \u2208 G to a low-dimensional and continuous dense vector space x \u2208 Rk (k is the dimension size and k << n) such that the embedding vectors can unfold the underlying structural properties of graphs. Thus, the obtained embedding vectors can be effectively applied to these downstream tasks mentioned above [16, 48].\nMany network embedding methods have been proposed in the literature, among which matrix factorization has been empirically and theoretically shown to be superior to Skip-Gram based random walk methods and deep learning based methods [33, 44, 46], as stated in Section 2. Specifically, matrix factorization based solutions first construct a proximity matrix S according to their correspond- ing applications, in which S(i, j) represents the relative importance of node j with respect to (w.r.t.) node i. Then, the traditional sin- gular value decomposition algorithm is executed on S or some variants of S to obtain the target embedding vectors. Thus, different proximity matrices were designed and the Personalized PageRank (PPR) matrix emerges as a superior choice due to its good perfor- mance [46, 47, 51]. In particular, given two nodes i, j\u2208 G, the PPR value \u03a0(i, j) is the probability that a random walk starts from i and stops at j using k steps state transition, in which k follows the geometric distribution. Therefore, the PPR values can be regarded as the concise summary of an infinite number of random walks, possessing nice structural properties and strong interpretability for network embedding.\nDespite their success, most existing methods suffer from the following several fundamental issues: (1) They either have high costs to achieve provable performance, or hard to obtain em- pirical satisfactory embedding vectors for downstream tasks. Specifically, the state-of-the-art (SOTA) methods, such as STRAP [47] and Lemane [51], applied the seminal Local Push subroutine [3] to approximate a single row or column of the PPR matrix, re- sulting in that have high overheads, especially for massive graphs. For example, Ref. [23] reported that the current fastest Local Push algorithm takes about 2 seconds on a million-node YouTube graph. Thus, the Local Push algorithm takes over 23 days to compute the PPR matrix, which is infeasible even for a powerful computing cluster. For reducing computational costs, NRP [46] integrated the calculation and factorization of the PPR matrix in an iterative frame- work. However, NRP has unsatisfactory theoretical approximation errors and poor empirical performance because it loses the nonlin- ear capability, as stated in our experiments. (2) Existing methods are highly dependent on the assumption that the PPR matrix can reflect the structural similarity between two nodes [30]. Namely, larger PPR values, denoted as \u03a0(i, j), correspond to the higher structural similarity between nodes i and j. However, the original PPR matrix has limited power in capturing the structural simi- larity between vertices, leading to performance degradation.\nTake Figure 1 as an example, we can see that \u03a0(v\u2081, v\u2083) (= 0.054) is almost three times as many as \u03a0(v\u2081, v\u2087) (= 0.140). Thus, v\u2081 is more similar to v\u2087 than v\u2083 according to the PPR metric. However, v\u2081 and v\u2087 have no common neighbors, and their walking trajec- tories are not similar. On the contrary, v\u2081 and v\u2083 share their only neighbor v\u2082 and have almost the same walking trajectories, which is a key property used to characterize whether nodes are in the same cluster [6, 45]. Specifically, let P(u) be all walking trajectories starting with u, we have P(v\u2081) = {{v\u2081, p(v\u2082)}|p(v\u2082) \u2208 P(v\u2082)} and P(v\u2083) = {{v\u2083, p(v\u2082)}\\p(v\u2082) \u2208 P(v\u2082)}. Thus, P(v\u2081) and P(v\u2083) are identical except for the starting vertex. As a result, by the PPR met- ric, the classifier tends to mistakenly assign v\u2081 and v\u2087 to the same class even though v\u2081 and v\u2083 belong to the same community and have stronger structural similarities. So, exploring alternative methods for overcoming these limitations remains a huge challenge.\nTo this end, we propose a novel and efficient spectral sParsification method for Scaling Network Embedding (PSNE). Specifically, PSNE first non-trivially utilizes the theories of spectral sparsification and random-walk matrix polynomials [9, 35] to directly construct a sparse PPR matrix with a theoretical guarantee in terms of the Frobenius norm (Theorem 4.6), which avoids repeatedly comput- ing each row or column of the PPR matrix, reducing greatly the computational overheads. Then, a simple yet effective multiple- perspective strategy (Section 4.2) is proposed to further enhance the representation power of the approximate PPR matrix, which can alleviate the inherent defects of the original PPR metric. Finally, PSNE employs a randomized singular value decomposition algo- rithm to efficiently factorize the sparse and multiple-perspective PPR matrix and obtain high-quality target embedding vectors. In a nutshell, we highlight our contributions as follows.\n\u2022 We are the first to adopt the spectral sparsification theory to directly approximate the whole PPR matrix, circumventing the expensive costs for a single row or column of the PPR matrix in existing push-based methods.\n\u2022 We devise a simple but effective multiple-perspective strategy to further enhance the representation power of the approximate PPR matrix. A striking feature of the strategy is that it also can be generalized to improve the qualities of SOTA baselines.\n\u2022 Empirical results on real-world and synthetic datasets show that our proposed PSNE outperforms the quality by at least 2% than ten competitors in most cases. Besides, PSNE is also more efficient than existing PPR-based methods without sacrificing accuracy, showing a better trade-off between efficiency and accuracy."}, {"title": "2 Related Work", "content": "2.1 Random Walk Based Network Embedding\nRandom walk based methods are inspired by the Skip-Gram model [27]. The high-level idea is to obtain embedding vectors by keeping the co-occurrence probability of the vertices on the random walks. The main difference between these methods is how to generate positive samples by different random walk strategies. For exam- ple, DeepWalk [31] utilized truncated random walks. Line [36] and Node2vec [14] extended DeepWalk with more complicated higher-order random walks or DFS/BFS search schemes. APP [53] and VERSE [38] adopted the a-discounted random walks to obtain posi- tive samples. However, a shared challenge of these methods is high computational costs due to the training of the Skip-Gram model.\n2.2 Deep Learning Based Network Embedding\nDeep learning provides an alternative solution to generate embed- ding vectors. For example, SDNE [41] utilized multi-layer auto- encoders to generate embedding vectors. DNGR [8] combined ran- dom walk and deep auto-encoder for network embedding. PRUNE [21] applied the Siamese Neural Network to retain both the point- wise mutual information and the PageRank distribution. GraphGAN [42] and DWNS [11] employed generative adversarial networks [10] to capture the probability of node connectivity in a precise manner. AW [1] proposed an attention model that operates on the power series of the transition matrix. Note that although the Graph Neural Network (GNN) with feature information [20] has achieved great success in many tasks, network embedding, which only uses the graph topology like our paper, is still irreplaceable. Specifically, (1) obtaining the rich node feature information is very expensive and even is not always available for downstream tasks, resulting in limited applications [12]. (2) Existing GNNs are typically end-to-end and need different training processes for different downstream tasks, leading to inflexibility. On the contrary, by focusing on the graph topology, network embedding provides a structure feature for each node, which is independent of downstream tasks [37]. Thus, network embedding provides a trade-off between the accuracy of downstream tasks and the training cost. In short, the main bottle- necks of these deep learning methods are high computational costs and labeling costs, which fail to deal with massive graphs.\n2.3 Matrix Based Network Embedding\nOther popular methods are to factorize a pre-defined proximity matrix that reflects the structural properties of the graph. For exam- ple, GraRep[7] performs SVD on the k-th order transition matrix. NetMF [33] demonstrated the equivalence between random walk based methods and matrix factorization based methods. NetSMF [32] combined NetMF with sparsification techniques to further im- prove efficiency of NetMF. However, NetSMF cannot effectively capture the non-uniform higher-order topological information be- cause NetSMF inherits the defects of DeepWalk, resulting in poor practical performance in most cases, as stated in our empirical results. ProNE [50] utilized matrix factorization and spectral propa- gation to obtain embedding vectors. STRAP [47] adopted the PPR matrix as the proximity matrix for improving the performances of NetMF and NetSMF. However, STRAP applied the seminal Local Push subroutine [3] to approximate a single row or column of the PPR matrix, resulting in prohibitively high time&space overheads. HOPE [29], AROPE [52], NRP [46], FREDE[39], and SketchNE[44] derived embedding vectors by implicitly computing the proximity matrix. Thus, they abandoned nonlinear operations on proximity matrices, which limits their representation powers. Lemane [51] considered the decay factor \u03b1 in PPR should not be fixed but learn- able, resulting in more flexibility. However, this learning process brings high overheads for Lemane."}, {"title": "3 Preliminaries", "content": "We use G(V, E) to denote an undirected graph, in which V and E are the vertex set and the edge set of G, respectively. Let n = |V| (resp., m = |E|) be the number of vertices (resp., edges). A is the adjacency matrix with Aij as the element of i-th row and j-th column of A, D = diag(d\u2081, ..., dn) is the degree matrix with d\u1d62 = \u2211\u2c7cA\u1d62\u2c7c, L = D-A be the Laplacian matrix, P = D\u207b\u00b9A be the state transition matrix. Personalized PageRank (PPR) is the state-of-the-art proximity metric, which can measure the relative importance of nodes [3, 26]. The PPR value \u03a0(u, v) is the probability that an \u03b1-decay random walk from u stops at node v, in which an \u03b1-decay random walk has a probability \u03b1 to stop at the current node, or (1 \u2013 \u03b1) probability to randomly jump to one of its neighbors. Thus, the length of \u03b1-decay random walk follows the geometric distribution with success probability \u03b1. The PPR matrix \u03a0 are formulated as follows:\n$$\\Pi = \\sum_{r=0}^{\\infty} \\alpha (1 - \\alpha)^r P^r$$\nProblem Statement. Given an undirected graph G(V, E), the net- work embedding problem aims to obtain a mapping function f : V\u2192 Rk, in which k is a positive integer representing the embed- ding dimension size and k << n. An effective network embedding function f unfolds the underlying structural properties of graphs."}, {"title": "4 PSNE: Our Proposed Solution", "content": "Here, we introduce a novel and efficient spectral sParsification al- gorithm PSNE for Scaling Network Embedding. PSNE first applies non-trivially the spectral graph theories to sparse the PPR matrix with theoretical guarantees. Subsequently, PSNE devises multiple-perspective strategies to further enhance the representation power of the sparse PPR matrix. Finally, a random singular value decom- position algorithm is executed on the refined sparse PPR matrix to obtain target embedding. Figure 2 is the framework of PSNE.\n4.1 Spectral Sparsification for PPR Matrix\nDefinition 4.1 (Random-Walk Matrix Polynomials). For an undi- rected graph G and a non-negative vector \u03b2 = (\u03b2\u2081, ..., \u03b2\u1d7c) with \u2211\u1d62=\u2081\u1d7c \u03b2\u1d62 = 1, the matrix\n$$L_p(G) = D - \\sum_{r=1}^T \\beta_r D (D^{-1}A)^r$$\nis a T-degree random-walk matrix polynomial of G.\nTHEOREM 4.2. [Sparsifiers of Random-Walk Matrix Polyno- mials] For any undirected graph G and 0 < \u03f5 \u2264 0.5, there exists a matrix L with O(nlogn/\u03f5\u00b2) non-zeros entries such that for any x \u2208 R\u207f, we have\n$$(1 - \\epsilon)x^T L x \\le x^T L_p(G) x \\le (1 + \\epsilon)x^T Lx$$\nThe matrix L satisfying Equation 3 is called spectrally simi- lar with approximation parameter \u03f5 to Lp(G), which can be con- structed by the two-stage computing framework [9]. In the first stage, an initial sparsifier with O(Tm log n/\u03f5\u00b2) non-zero entries is found. In the second stage, a standard spectral sparsification algo- rithm [35] is applied in the initial sparsifier to further reduce the number of non-zero entries to O(n log n/\u03f5\u00b2). Note that the second stage requires complex graph theory to understand and consumes most of the time of the two-stage computing framework. Thus, in this paper, we first non-trivially utilize the first stage to obtain the coarse-grained sparsifier quickly. Then, we propose an effective multiple-perspective strategy to enhance the representation power of the coarse-grained sparsifier in the next subsection. Specifically, the T-truncated PPR matrix is given as follows:\n$$\\Pi' = \\Pi - \\sum_{r=T+1}^{\\infty} \\alpha (1-\\alpha)^r P^r = \\sum_{r=0}^{T} \\alpha (1 - \\alpha)^r P^r$$\nwhere T is the truncation order (T is also T in Definition 4.1). By combining Equation 2 and Equation 4, we have\n$$\\Pi' = \\alpha I + \\sum_{r=1}^T \\alpha (1 - \\alpha)^r P^r$$\n$$\\Pi' = \\alpha I + D^{-1} \\cdot \\sum_{r=1}^T \\alpha (1 - \\alpha)^r D P^r$$\n$$\\Pi' = \\alpha I + a_{sum}D^{-1} \\cdot \\sum_{r=1}^T \\alpha (1 - \\alpha)^r / a_{sum} DP^r$$\n$$\\Pi' = \\alpha I + a_{sum}D^{-1} \\cdot (D - L_p(G))$$\n$$\\Pi' = \\alpha I + a_{sum} \\cdot (I - D^{-1}L_p(G))$$\nWhere asum = \u2211\u1d7c\u1d62\u208c\u2081 \u03b1(1 \u2212 \u03b1)\u2071. Therefore, we establish a theoret- ical connection between the (truncated) PPR matrix and random- walk matrix polynomials. Based on this connection, we devise a novel sparsifier to obtain the approximate PPR matrix (Algorithm 1), reducing greatly the prohibitively high computational cost of existing local push-based embedding methods. Specifically, Algo- rithm 1 first initializes an undirected graph \u011e = (V, 0), in which V is the vertex set of the input graph G (Line 1). Subsequently, Lines 2-7 of Algorithm 1 adds O(N) edges to \u011e by executing iteratively the Path_Sampling Function (Lines 11-18). Finally, Algorithm 1 applies Equations 4-9 to get an approximate PPR matrix \u00f1 with O(N) non-zeros entries (Lines 8-10).\nRemark. The path length r in Line 4 is selected with the probability \u03b1(1 \u2212 \u03b1)/\u2211\u1d62=\u2081\u1d7c \u03b1(1 \u2212 \u03b1)\u2071 for satisfying the condition of Definition 4.1, that is \u03b1\u1d63 = \u03b1(1 \u2212 \u03b1)\u02b3 /\u2211\u1d62=\u2081\u1d7c \u03b1(1 \u2212 \u03b1)\u2071 and \u2211\u1d7c\u1d62\u208c\u2081 \u03b1\u1d63 = 1. As a re- sult, Algorithm 1 can obtain a sparse PPR matrix with a theoretical guarantee in terms of the Frobenius norm, which will be analyzed theoretically later. Besides, Algorithm 1 also leverages the intuition that closer nodes exhibit a higher propensity for information ex- change. Therefore, the shorter the random walk, the greater the probability of being selected to promote local interactions, enabling more effective capture non-uniform high-order structural proximi- ties among vertices for obtaining high-quality embedding vectors, which is verified in our experiments.\n4.2 From PPR to Multiple-Perspective PPR\nAs depicted in Figure 1, the original PPR metric cannot effectively capture the structural similarity between vertices. Besides, if we directly take the approximate PPR matrix obtained by Section 4.1 as the input to matrix factorization, there will also be performance degradation. To overcome these issues, we propose a simple yet effective multiple-perspective strategy to achieve the following goals: (1) Alleviating the inherent defects of the original PPR metric; (2) Enhancing the representation power of the coarse-grained and sparse PPR matrix obtained in Section 4.1.\nThrough deep observation as shown in Figure 1, we found that the original PPR measures node pair proximity from a single per- spective and ignores pattern similarity. For example, in a social network with users A, B, and C. C is the close friend of A, and A's assessment of B is not only influenced by A's subjective impression of B but also indirectly influenced by C's impression of B. However, the original PPR metric only considers A's impression of B. On top of that, the original PPR fails to illustrate that node v\u2081 exhibits a structural pattern more similar to nodes v\u2083 rather than v\u2087.\nBased on these intuitions, we propose a novel metric of Multiple- Perspective PPR (MP-PPR) to effectively compute the structural proximity between destination nodes and the source node with pattern similarity. Since considering the perspective of all nodes is computationally expensive with little performance improvement, we integrate only the perspectives of the one-hop neighbors of the source node. Consequently, the multiple-perspective proximity of destination node j w.r.t. the source node i is stated as follows.\n$$M(i, j)_s = \\sum_{h \\in N(i)} d_{hi}S_{hj} + d_{ii}S_{ij}$$\nwhere S is any proximity matrix (e.g., the PPR matrix) and N(i) represents the neighbor set of i. dhi (resp., d\u1d62\u1d62) is the multiple- perspective coefficient of node h (resp., i) to node i. In this paper, we take d_{hi} = w_{ph}i / (\\sqrt{(d_h+1)}\\sqrt{(d_i + 1)}) and d_{ii} = 1 / (d_i + 1) where wphj is pattern similarity between (v\u2095, v\u2c7c). To characterize the pattern similarity between nodes, we introduce the well-known anonymous walk [19] as follows.\nDefinition 4.3 (Anonymous Walk). If A = (v\u2081, v\u2082, ..., v\u2099) is a ran- dom walk trajectory, then its corresponding anonymous walk is the sequence of integers Anotrax = (f(v\u2081), f(v\u2082), .., f (v\u2099)), where f(.) is a mapping that maps nodes to positive integers.\nDifferent nodes on the same trajectories are mapped to different positive integers, which may coincide on different paths. For exam- ple, trajectories PA = (v\u2081, v\u2082, v\u2083, v\u2082, v\u2083) and PB = (v\u2083, v\u2084, v\u2082, v\u2084, v\u2082) share the common anonymous trajectory (1, 2, 3, 2, 3). We utilize the anonymous walk for pattern similarity calculation, which de- mands extensive trajectory sampling, limiting scalability. However, we have identified specific features in Equation 10 and Algorithm 1 as follows. (1) Equation 10 focuses on pattern similarity of neigh- boring nodes within one hop, reducing initial sampling needs. (2) Algorithm 1 already includes extensive path sampling, allowing for reduced sampling in anonymous random walks through strategic design (Lines 14 and 16 of Algorithm1).\nAlgorithm 2 PSNE\nInput: An undirected graph G(V, E); the truncation order T; the number of non-zeros N used in the PPR matrix sparsifier; the decay factor \u03b1 of PPR; the filter parameter \u03bc; the embedding dimension size k\nOutput: The network embedding matrix\n1: Obtaining a sparse PPR matrix \u00f1 by executing Algorithm 1\n2: Obtaining the multiple-perspective PPR M by Equation 10\n3: M\u00f1 \u2190 \u03c3\u03bc(M\u00f1) // \u03c3\u03bc is a non-linear activation function\n4: U, \u03a3, V \u2190 Randomized SVD (M\u00f1, k)\n5: return UV as the network embedding matrix\nAfter obtaining the anonymous trajectories, we utilize the Longest Common Subsequence [40] to ascertain the similarity between the two trajectories.\nTHEOREM 4.4. [ Longest Common Subsequence(LCSS)] For two trajectories PA = (a\u2081, a\u2082, ..., a\u2099) and PB = (b\u2081, b\u2082, ..., b\u2098) with lengths n and m respectively, where the length of the longest common subsequence is:\nLCSS(PA, PB) = \\\n \\begin{cases}\n0 & \\text{if PA or PB = \u00d8} \\\\\n1 + LCSS (a\\_{t-1}, b\\_{i-1}) & \\text{if } a\\_t = b\\_i \\\\\nmax (LCSS (a\\_{t-1}, b\\_i), LCSS (a\\_t, b\\_{i-1})) & \\text{otherwise}\n \\end{cases}\n\nwhere t = 1, 2, ..., n and i = 1, 2, ..., m and \u00d8 is empty trajectory.\nTherefore, the pattern similarity wpattern in Equation 10 is de- fined via anonymous walk paths and LCSS as follows:\nw_{pij} = \\frac{1}{s} \\sum \\frac{LCSS(AnoTra_i, AnoTra_j)}{lenth(AnoTra_i)}\nwhere AnoTrai and AnoTraj are anonymous random walk trajecto- ries starting from nodes i and j, respectively. lenth(.) is a function of trajectory length and s is the sampling numbers node pair (i, j). As shown in Table 1, v\u2083 exhibits a higher MP-PPR value than v\u2087. Thus, MP-PPR captures more reasonable proximity for network embedding from multiple perspectives without compromising the proximity between different nodes as reflected in the original PPR.\n4.3 Our PSNE and Theoretical Analysis\nBased on the above theoretical backgrounds, we devise an effi- cient spectral sparsification algorithm for scaling network embed- ding (Algorithm 2). Firstly, Algorithm 2 obtains a sparse PPR matrix with a theoretical guarantee in terms of the Frobenius norm (Line 1). Subsequently, it obtains the multiple-perspective PPR matrix M\u00f1 (Line 2). Finally, Lines 3-5 obtain the network embedding matrix by executing the randomized singular value decomposition (RSVD) algorithm [15]. Here, \u03c3\u03bc is a non-linear activation function (e.g., \u03c3\u03bc(x) = max(0, log(x\u03b7\u03bc))) with the filter parameter \u03bc. Next, we analyze the time&space complexities of the proposed PSNE and the corresponding approximation errors.\nTHEOREM 4.5. The time complexity and space complexity of Algo- rithm 2 are O(m log n + mk + nk\u00b2) and O(m log n + nk), respectively.\nPROOF. PSNE (i.e., Algorithm 2) has three main steps as follows:\n\u2022 Step 1 (i.e., Algorithm 1): Random-Walk Molynomial Sparsifier for PPR matrix. Lines 1-7 of Algorithm 1 sample O(N) paths to construct O(N) edges for the sparse graph \u011e. The expected value of r (r is the length of sample path), denoted as E(r), is given by \u2211\u1d62\u208c\u2081\u1d7c i(\u03b1(1 \u2212 \u03b1)\u2071/\u2211\u1d62=\u2081\u1d7c \u03b1(1 \u2212 \u03b1)\u2071) \u2264 1/\u03b1. As a result, Lines 1-7 of Algorithm 1 consume O(NE(r)) = O(N/\u03b1) time. In Lines 8-9, Algorithm 1 consumes O(N) time to compute the spare PPR matrix \u00f1. Thus, the time complexity of Algorithm 1 is O(N/\u03b1). For space complexity, Algorithm 1 takes O(N) extra space to store graph \u011e and matrix \u00f1. So, the space complexity of Algorithm 1 is O(N + n + m).\n\u2022 Step 2 (i.e., Lines 2-3 of Algorithm 2): Multiple-Perspective strat- egy. In particular, according to Equation 10, we can know that this step consumes O(mxavg) time to obtain MP-PPR matrix M, in which xavg = n/m is the average number of non-zero elements per row of \u00f1. Thus, the time complexity of step 2 is O(m + m). In most real-life graphs, m = O(n log n), thus, the time complexity of step 2 can be further reduced to O(m + N log n). The space complexity of step 2 is O(m + N log n + n).\n\u2022 Step 3 (i.e., Lines 4-5 of Algorithm 2): Randomized Singular Value Decomposition. By [15], we know that this step needs O(Nk + nk\u00b2 + k\u00b3) time and O(N+nk) space to get the network embedding matrix.\nIn a nutshell, the time (resp., space) complexity of PSNE is O(N/\u03b1+ m + N log n + Nk + nk\u00b2 + k\u00b3) (resp., O(m + N log n + nk)). Follow- ing the previous methods [35, 46], \u03b1 is a constant and N = O(m), thus, the time (resp., space) complexity of PSNE can be reduced to O(m log n + mk + nk\u00b2) (resp., O(m log n + nk)). Thus, we have completed the proof of Theorem 4.5.\nMissing proofs are deferred to our Appendix Section.\nTHEOREM 4.6. Let \u03a0 be the exact PPR matrix (i.e., Equation 1) and \u00f1 be the approximate sparse matrix obtaind by Algorithm 1, we have ||\u03a0 \u2013 \u00f1||F \u2264 \u221an((1 \u2212 \u03b1)\u1d40\u207a\u00b9 + 4\u03f5 \u00b7 asum).\nTHEOREM 4.7. Let M\u03a0 (resp., M\u00f1) be the MP-PPR matrix by execut- ing Equation 10 on \u03a0 (resp., \u00f1), we have ||\u03c3(M\u03a0, \u03bc) \u2013 \u03c3(M\u00f1, \u03bc) || F \u2264 n((1 \u2212 \u03b1)\u1d40\u207a\u00b9 + 4\u03f5 \u00b7 asum)."}, {"title": "5 Empirical Results", "content": "In this section, we answer the following Research Questions:\n\u2022 RQ1: How much improvement in effectiveness and efficiency is our PSNE compared to other baselines?\n\u2022 RQ2: Whether the multiple-perspective strategies can be inte- grated into other baselines to improve qualities?\n5.1 Experimental Setup\nDatasets. We evaluate our proposed solutions on several publicly- available datasets (Table 2), which are widely used benchmarks for network embedding [39, 44, 46, 51]. BlogCatalog, Flickr, and YouTube are undirected social networks where nodes represent users and edges represent relationships. The Protein-Protein Inter- action (PPI) dataset is a subgraph of the Homo sapiens PPI network, with vertex labels from hallmark gene sets indicating biological states. The Wikipedia dataset is a word co-occurrence network from the first million bytes of a Wikipedia dump, with nodes labeled by Part-of-Speech tags.\nBaselines and parameters. The following ten competitors are implemented for comparison: DeepWalk [31], Grarep [7], HOPE [28], NetSMF [32], ProNE [50], STRAP [47], NRP [46], Lemane [51], FREDE [39], and SketchNE [44]. Note that STRAP, NRP, and Le- mane are PPR-based embedding methods. For the ten competitors, we take their corresponding default parameters. The detailed pa- rameter settings of the proposed PSNE are summarized in Table 3. All experiments are conducted on a Ubuntu server with Intel (R) Xeon (R) Silver 4210 CPU (2.20GHz) and 1T RAM.\n5.2 Effectiveness Testing\nWe apply node classification to evaluate the effectiveness of our solutions. Node classification aims to accurately predict the labels of nodes. Specifically, a node embedding matrix is first constructed from the input graph. Subsequently, a one-vs-all logistic regression classifier is trained using the embedding matrix and the labels of randomly selected vertices. Finally, the classifier is tested with the labels of the remaining vertices. The training ratio is adjusted from 10% to 90%. To be more reliable, we execute each method five times and report their Micro-F1 and Macro-F1 in Figure 41. As can be seen, we can obtain the following observations: (1) Under different training ratios, our PSNE consistently achieves the highest Micro- F1 scores on four of the five datasets, and the highest Macro-F1 scores on PPI, BlogCatalog, and YouTube. For example, on YouTube, PSNE is 1.5% and 0.9% better than the runner-up in Micro-F1 and Macro-F1 scores, respectively. (2) The Micro-F1 and Macro-F1 of all baselines vary significantly depending on the dataset and training ratio. For example, DeepWalk outperforms other methods on Flickr (PSNE is the runner-up and slightly worse than DeepWalk) but has inferior Micro-F1 scores on other datasets. HOPE achieves the highest Macro-F1 score on Wikipedia but performs poorly on the Micro-F1 metric. (3) PSNE outperforms other PPR-based methods, including STRAP, NRP, and Lemane, with a margin of at least 2% in most cases. Specifically, for the Micro-F1 metric, our PSNE achieves improvements of 6%, 3%, 8%, 7%, and 2% over NPR on PPI, Wikipedia, BlogCatalog, Flickr, and Youtube, respectively. For example, for Flickr with more than a few million edges, our PSNE achieves 41% while NPR is 34% in the micro-F1 metric. For the Macro-F1 metric, PSNE surpasses all other PPR-based algorithms, achieving a notable lead of 0.5%, 0.6%, 2%, 4%, and 2% over the"}, {"title": "5.3 Efficiency Testing", "content": "For efficiency testing, we do not include non-PPR-based algorithms because all of them are outperformed by Lemane and NRP, as re- ported in their respective studies. On top of that, our proposed PSNE is a PPR-based method, so we test the runtime of other PPR- based methods (i.e., STRAP, NRP, and Lemane) for comparison. Table 4 presents the wall-clock time of each PPR-based method with 20 threads. As can be seen, NRP outperforms other methods (but it has poor node classification quality and improved by 2%- 25% by PSNE, as stated in Figure 4), and PSNE is runner-up and slightly worse than NRP. The reasons can be explained as follows: NRP integrates the calculation and factorization of the PPR matrix in an iterative framework to improve efficiency but lacks the non- linear representation powers for node embedding. However, our PSNE devises the sparsifiers of random-walk matrix polynomials for the truncated PPR matrix, avoiding repeatedly computing each row or column of the PPR matrix in the push-based methods (e.g., STRAP, Lemane). These results show that PSNE achieves significant speedup with high embedding quality compared with the baselines, which is consistent with our theoretical analysis (Section 4.3).\n5.4 Scalability Testing on Synthetic Graphs\nWe use the well-known NetworkX Python package [4] to generate two types of synthetic graphs ER [13] and BA [5] to test the scala- bility of our PSNE. Figure 3 only presents the results of Deepwalk, STRAP, HOPE, and our PSNE, with comparable trends across other methods. By Figure 3, we can know that when the number of nodes is small, the DeepWalk and HOPE have a runtime comparable to PSNE and STRAP. However, as the number of nodes increases, the runtime of DeepWalk and HOPE significantly rises, surpassing that of PSNE by an order of magnitude. Additionally, the increase in STRAP's runtime is greater than that of PSNE. These results indi- cate that our PSNE has excellent scalability over massive graphs while the baselines do not.\n5.5 Ablation Studies\nTo illustrate the impact of the multiple-perspective strategy on PSNE and other baselines, we report the results of the ablation study in Table 5. As can be seen, on PPI, both GraRep and HOPE have a 2%-3% improvement, while NetSMF, STRAP, ProNE, and PSNE exhibit more modest gains of 0.3%-0.8%. On the Wikipedia and BlogCatalog datasets, all methods benefit significantly from the multiple-perspective strategy, with HOPE achieving the high- est improvement on BlogCatalog (approximately 4.38%) and other methods averaging around 1.2% improvements. All experimental results were presented with a 50% training rate.\n5.6 Sampling Quality Analysis\nWe also observed that both PSNE and NetSMF use similar but completely different\u00b2 path sampling strategies to derive the node proximity matrix. Therefore, we will closely examine their differ- ences. In particular, Figure 4 has revealed that NetSMF exhibits lower accuracy than PSNE. In addition, NetSMF requires signif- icantly more path sampling to achieve acceptable accuracy. To illustrate this point, we compare the impact of sampling size on the"}, {"title": "5.7 Parameter Analysis", "content": "Following the previous methods [9", "results": 1}]}