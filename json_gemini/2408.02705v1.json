{"title": "PSNE: Efficient Spectral Sparsification Algorithms for Scaling Network Embedding", "authors": ["Longlong Lin", "Yunfeng Yu", "Zihao Wang", "Zeli Wang", "Yuying Zhao", "Jin Zhao", "Tao Jia"], "abstract": "Network embedding has numerous practical applications and has received extensive attention in graph learning, which aims at mapping vertices into a low-dimensional and continuous dense vector space by preserving the underlying structural properties of the graph. Many network embedding methods have been proposed, among which factorization of the Personalized PageRank (PPR for short) matrix has been empirically and theoretically well supported recently. However, several fundamental issues cannot be addressed. (1) Existing methods invoke a seminal Local Push subroutine to approximate a single row or column of the PPR matrix. Thus, they have to execute n (n is the number of nodes) Local Push subroutines to obtain a provable PPR matrix, resulting in prohibitively high computational costs for large n. (2) The PPR matrix has limited power in capturing the structural similarity between vertices, leading to performance degradation. To overcome these dilemmas, we propose PSNE, an efficient spectral sParsification method for Scaling Network Embedding, which can fast obtain the embedding vectors that retain strong structural similarities. Specifically, PSNE first designs a matrix polynomial sparser to accelerate the calculation of the PPR matrix, which has a theoretical guarantee in terms of the Frobenius norm. Subsequently, PSNE proposes a simple but effective multiple-perspective strategy to enhance further the representation power of the obtained approximate PPR matrix. Finally, PSNE applies a randomized singular value decomposition algorithm on the sparse and multiple-perspective PPR matrix to get the target embedding vectors. Experimental evaluation of real-world and synthetic datasets shows that our solutions are indeed more efficient, effective, and scalable compared with ten competitors.", "sections": [{"title": "1 Introduction", "content": "Graphs are ubiquitous for modeling real-world complex systems, including financial networks, biological networks, social networks, etc. Analyzing and understanding the semantic information behind these graphs is a fundamental problem in graph analysis [2, 17, 24, 25, 49]. Thus, numerous graph analysis tasks are arising, such as node classification, link prediction, and graph clustering. Due to their exceptional performance, network embedding is recognized as an effective tool for solving these tasks. Specifically, given an input graph G with n nodes, network embedding methods aim at mapping any node v \u2208 G to a low-dimensional and continuous dense vector space x \u2208 Rk (k is the dimension size and k << n) such that the embedding vectors can unfold the underlying structural properties of graphs. Thus, the obtained embedding vectors can be effectively applied to these downstream tasks mentioned above [16, 48].\nMany network embedding methods have been proposed in the literature, among which matrix factorization has been empirically and theoretically shown to be superior to Skip-Gram based random walk methods and deep learning based methods [33, 44, 46], as stated in Section 2. Specifically, matrix factorization based solutions"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Random Walk Based Network Embedding", "content": "Random walk based methods are inspired by the Skip-Gram model [27]. The high-level idea is to obtain embedding vectors by keeping the co-occurrence probability of the vertices on the random walks. The main difference between these methods is how to generate positive samples by different random walk strategies. For example, DeepWalk [31] utilized truncated random walks. Line [36] and Node2vec [14] extended DeepWalk with more complicated higher-order random walks or DFS/BFS search schemes. APP [53] and VERSE [38] adopted the a-discounted random walks to obtain positive samples. However, a shared challenge of these methods is high computational costs due to the training of the Skip-Gram model."}, {"title": "2.2 Deep Learning Based Network Embedding", "content": "Deep learning provides an alternative solution to generate embedding vectors. For example, SDNE [41] utilized multi-layer auto-encoders to generate embedding vectors. DNGR [8] combined random walk and deep auto-encoder for network embedding. PRUNE [21] applied the Siamese Neural Network to retain both the pointwise mutual information and the PageRank distribution. GraphGAN [42] and DWNS [11] employed generative adversarial networks [10] to capture the probability of node connectivity in a precise manner. AW [1] proposed an attention model that operates on the power series of the transition matrix. Note that although the Graph Neural Network (GNN) with feature information [20] has achieved great success in many tasks, network embedding, which only uses the graph topology like our paper, is still irreplaceable. Specifically, (1) obtaining the rich node feature information is very expensive and even is not always available for downstream tasks, resulting in limited applications [12]. (2) Existing GNNs are typically end-to-end and need different training processes for different downstream tasks, leading to inflexibility. On the contrary, by focusing on the graph topology, network embedding provides a structure feature for each node, which is independent of downstream tasks [37]. Thus, network embedding provides a trade-off between the accuracy of downstream tasks and the training cost. In short, the main bottlenecks of these deep learning methods are high computational costs and labeling costs, which fail to deal with massive graphs."}, {"title": "2.3 Matrix Based Network Embedding", "content": "Other popular methods are to factorize a pre-defined proximity matrix that reflects the structural properties of the graph. For example, GraRep[7] performs SVD on the k-th order transition matrix. NetMF [33] demonstrated the equivalence between random walk based methods and matrix factorization based methods. NetSMF [32] combined NetMF with sparsification techniques to further improve efficiency of NetMF. However, NetSMF cannot effectively capture the non-uniform higher-order topological information because NetSMF inherits the defects of DeepWalk, resulting in poor practical performance in most cases, as stated in our empirical results. ProNE [50] utilized matrix factorization and spectral propagation to obtain embedding vectors. STRAP [47] adopted the PPR matrix as the proximity matrix for improving the performances of NetMF and NetSMF. However, STRAP applied the seminal Local Push subroutine [3] to approximate a single row or column of the PPR matrix, resulting in prohibitively high time&space overheads. HOPE [29], AROPE [52], NRP [46], FREDE[39], and SketchNE[44] derived embedding vectors by implicitly computing the proximity matrix. Thus, they abandoned nonlinear operations on proximity matrices, which limits their representation powers. Lemane [51] considered the decay factor a in PPR should not be fixed but learnable, resulting in more flexibility. However, this learning process brings high overheads for Lemane."}, {"title": "3 Preliminaries", "content": "We use G(V, E) to denote an undirected graph, in which V and E are the vertex set and the edge set of G, respectively. Let n = |V| (resp., m = |E|) be the number of vertices (resp., edges). A is the adjacency matrix with Aij as the element of i-th row and j-th column of A, D = diag(d1, ..., dn) is the degree matrix with di = \u2211jAij, L = D-A be the Laplacian matrix, P = D\u00af\u00b9A be the state transition matrix. Personalized PageRank (PPR) is the state-of-the-art proximity metric, which can measure the relative importance of nodes [3, 26]. The PPR value \u03a0(u, v) is the probability that an a-decay random walk from u stops at node v, in which an a-decay random walk has a probability to stop at the current node, or (1 \u2013 a) probability to randomly jump to one of its neighbors. Thus, the length of a-decay random walk follows the geometric distribution with success probability a. The PPR matrix II are formulated as follows:\n$\\Pi = \\sum_{r=0}^{\\infty} \\alpha(1-\\alpha)^r P^r$ (1)\nProblem Statement. Given an undirected graph G(V, E), the network embedding problem aims to obtain a mapping function f : V\u2192 Rk, in which k is a positive integer representing the embedding dimension size and k << n. An effective network embedding function f unfolds the underlying structural properties of graphs."}, {"title": "4 PSNE: Our Proposed Solution", "content": "Here, we introduce a novel and efficient spectral sParsification algorithm PSNE for Scaling Network Embedding. PSNE first applies non-trivially the spectral graph theories to sparse the PPR matrix with theoretical guarantees. Subsequently, PSNE devises multiple-perspective strategies to further enhance the representation power of the sparse PPR matrix. Finally, a random singular value decomposition algorithm is executed on the refined sparse PPR matrix to obtain target embedding. Figure 2 is the framework of PSNE."}, {"title": "4.1 Spectral Sparsification for PPR Matrix", "content": "Definition 4.1 (Random-Walk Matrix Polynomials). For an undirected graph G and a non-negative vector \u03b2 = (\u03b21, ..., \u03b2\u03b3) with \u03a3\u2081=1 \u03b2i = 1, the matrix\n$L_p(G) = D - \\sum_{r=1}^{T} \\beta_r D (D^{-1}A)^r$ (2)\nis a T-degree random-walk matrix polynomial of G.\nTHEOREM 4.2. [Sparsifiers of Random-Walk Matrix Polynomials] For any undirected graph G and 0 < \u0454 \u2264 0.5, there exists a matrix L with O(nlogn/e\u00b2) non-zeros entries such that for any x \u2208 Rn, we have\n$(1 - \\epsilon)x^T L x \\leq x^T L_p(G) x \\leq (1 + \\epsilon)x^T L x$ (3)\nThe matrix L satisfying Equation 3 is called spectrally similar with approximation parameter e to Lp(G), which can be constructed by the two-stage computing framework [9]. In the first stage, an initial sparsifier with O(Tmlogn/e\u00b2) non-zero entries is found. In the second stage, a standard spectral sparsification algorithm [35] is applied in the initial sparsifier to further reduce the number of non-zero entries to O(nlogn/e\u00b2). Note that the second stage requires complex graph theory to understand and consumes most of the time of the two-stage computing framework. Thus, in this paper, we first non-trivially utilize the first stage to obtain the coarse-grained sparsifier quickly. Then, we propose an effective multiple-perspective strategy to enhance the representation power"}, {"title": "4.2 From PPR to Multiple-Perspective PPR", "content": "As depicted in Figure 1, the original PPR metric cannot effectively capture the structural similarity between vertices. Besides, if we"}, {"title": "4.3 Our PSNE and Theoretical Analysis", "content": "Based on the above theoretical backgrounds, we devise an efficient spectral sparsification algorithm for scaling network embedding (Algorithm 2). Firstly, Algorithm 2 obtains a sparse PPR matrix with a theoretical guarantee in terms of the Frobenius norm (Line 1). Subsequently, it obtains the multiple-perspective PPR matrix M\u012b (Line 2). Finally, Lines 3-5 obtain the network embedding matrix by executing the randomized singular value decomposition (RSVD) algorithm [15]. Here, \u03c3\u03bc is a non-linear activation function (e.g., \u03c3\u03bc(x) = max(0, log(x\u03b7\u03bc)) [44, 47, 51]) with the filter parameter \u03bc. Next, we analyze the time&space complexities of the proposed PSNE and the corresponding approximation errors."}, {"title": "5 Empirical Results", "content": "In this section, we answer the following Research Questions:\n\u2022 RQ1: How much improvement in effectiveness and efficiency is our PSNE compared to other baselines?\n\u2022 RQ2: Whether the multiple-perspective strategies can be integrated into other baselines to improve qualities?"}, {"title": "5.1 Experimental Setup", "content": "Datasets. We evaluate our proposed solutions on several publicly-available datasets (Table 2), which are widely used benchmarks for network embedding [39, 44, 46, 51]. BlogCatalog, Flickr, and YouTube are undirected social networks where nodes represent users and edges represent relationships. The Protein-Protein Interaction (PPI) dataset is a subgraph of the Homo sapiens PPI network,"}, {"title": "5.2 Effectiveness Testing", "content": "We apply node classification to evaluate the effectiveness of our solutions. Node classification aims to accurately predict the labels of nodes. Specifically, a node embedding matrix is first constructed from the input graph. Subsequently, a one-vs-all logistic regression classifier is trained using the embedding matrix and the labels of randomly selected vertices. Finally, the classifier is tested with the labels of the remaining vertices. The training ratio is adjusted from 10% to 90%. To be more reliable, we execute each method five times and report their Micro-F1 and Macro-F1 in Figure 41. As can be seen, we can obtain the following observations: (1) Under different training ratios, our PSNE consistently achieves the highest Micro-F1 scores on four of the five datasets, and the highest Macro-F1 scores on PPI, BlogCatalog, and YouTube. For example, on YouTube, PSNE is 1.5% and 0.9% better than the runner-up in Micro-F1 and Macro-F1 scores, respectively. (2) The Micro-F1 and Macro-F1 of all baselines vary significantly depending on the dataset and training ratio. For example, DeepWalk outperforms other methods on Flickr (PSNE is the runner-up and slightly worse than DeepWalk) but has inferior Micro-F1 scores on other datasets. HOPE achieves the highest Macro-F1 score on Wikipedia but performs poorly on the Micro-F1 metric. (3) PSNE outperforms other PPR-based methods, including STRAP, NRP, and Lemane, with a margin of at least 2% in most cases. Specifically, for the Micro-F1 metric, our PSNE achieves improvements of 6%, 3%, 8%, 7%, and 2% over NPR on PPI, Wikipedia, BlogCatalog, Flickr, and Youtube, respectively. For example, for Flickr with more than a few million edges, our PSNE achieves 41% while NPR is 34% in the micro-F1 metric. For the Macro-F1 metric, PSNE surpasses all other PPR-based algorithms, achieving a notable lead of 0.5%, 0.6%, 2%, 4%, and 2% over the"}, {"title": "5.3 Efficiency Testing", "content": "For efficiency testing, we do not include non-PPR-based algorithms because all of them are outperformed by Lemane and NRP, as reported in their respective studies. On top of that, our proposed PSNE is a PPR-based method, so we test the runtime of other PPR-based methods (i.e., STRAP, NRP, and Lemane) for comparison. Table 4 presents the wall-clock time of each PPR-based method with 20 threads. As can be seen, NRP outperforms other methods (but it has poor node classification quality and improved by 2%25% by PSNE, as stated in Figure 4), and PSNE is runner-up and slightly worse than NRP. The reasons can be explained as follows: NRP integrates the calculation and factorization of the PPR matrix in an iterative framework to improve efficiency but lacks the non-linear representation powers for node embedding. However, our PSNE devises the sparsifiers of random-walk matrix polynomials for the truncated PPR matrix, avoiding repeatedly computing each row or column of the PPR matrix in the push-based methods (e.g., STRAP, Lemane). These results show that PSNE achieves significant speedup with high embedding quality compared with the baselines, which is consistent with our theoretical analysis (Section 4.3)."}, {"title": "5.4 Scalability Testing on Synthetic Graphs", "content": "We use the well-known NetworkX Python package [4] to generate two types of synthetic graphs ER [13] and BA [5] to test the scalability of our PSNE. Figure 3 only presents the results of Deepwalk, STRAP, HOPE, and our PSNE, with comparable trends across other methods. By Figure 3, we can know that when the number of nodes is small, the DeepWalk and HOPE have a runtime comparable to PSNE and STRAP. However, as the number of nodes increases, the runtime of DeepWalk and HOPE significantly rises, surpassing that of PSNE by an order of magnitude. Additionally, the increase in STRAP's runtime is greater than that of PSNE. These results indicate that our PSNE has excellent scalability over massive graphs while the baselines do not."}, {"title": "5.5 Ablation Studies", "content": "To illustrate the impact of the multiple-perspective strategy on PSNE and other baselines, we report the results of the ablation study in Table 5. As can be seen, on PPI, both GraRep and HOPE have a 2%-3% improvement, while NetSMF, STRAP, ProNE, and"}, {"title": "5.6 Sampling Quality Analysis", "content": "We also observed that both PSNE and NetSMF use similar but completely different\u00b2 path sampling strategies to derive the node proximity matrix. Therefore, we will closely examine their differences. In particular, Figure 4 has revealed that NetSMF exhibits lower accuracy than PSNE. In addition, NetSMF requires significantly more path sampling to achieve acceptable accuracy. To illustrate this point, we compare the impact of sampling size on the"}, {"title": "5.7 Parameter Analysis", "content": "Following the previous methods [9], we also set N = cTm. Figure 6 only shows the effect of different parameter settings on Micro-F1 of PSNE in BlogCatalog and PPI, with comparable trends across Macro-F1 and other datasets. We have the following results: (1) By Figure 6(a), Mirco-F1 increases first and then decreases with increasing a. This is because the parameter a controls how much of the graph is explored by our PSNE. Thus, when a is close to 1, the proximity matrix focuses on one-hop neighbors and thus preserves the adjacency information. As a approaches 0, the proximity matrix incorporates more information coming from multi-hop higher-order neighbors, resulting in good performance. (2) By Figure 6(b) and 6(c), we observe that Mirco-F1 increases first and then decreases"}, {"title": "6 Conclusion", "content": "This paper presents an efficient spectral sparsification algorithm PSNE that first devises a matrix polynomial sparser to directly approximate the whole PPR matrix with theoretical guarantee, which avoids repeatedly computing each row or column of the PPR matrix."}, {"title": "8 Appendix", "content": null}, {"title": "8.1 Approximate Error Analysis", "content": "LEMMA 8.1. Let L = D-1/2Lp(G)D-1/2 and the corresponding sparsifier L = D-1/2\u012aD-1/2, in which L is the Laplacian matrix of G (Line 8 of Algorithm 1). Then, all the singular values of L \u2013 L are less than 4\u0454.\nPROOF. According to Theorem 1, we have\n$(1 - \\epsilon)x^T L_p(G) x \\leq x^T L x \\leq (1 + \\epsilon) x^T L_p(G) x$\nLet x=D-1/2y, we have\n$(1 - \\epsilon)y^T L y \\leq y^T (L-\\widetilde{L})y \\leq (1 + \\epsilon) y^T L y$\nSince e \u2264 0.5, we have\n$|y^T (L-\\widetilde{L})y| \\leq (1 + \\epsilon) y^T L y \\leq 2\\epsilon y^T L y$\nBy Courant-Fisher Theorem [34], we have\n$|\\lambda_{i} (L-\\widetilde{L}) | \\leq 2\\epsilon \\lambda_{i} (L) < 4\\epsilon$\nwhere i \u2208 [\u03b7], \u03bb\u2081 (A) is i-th largest eigenvalue of matrix A. This is because L is a normalized graph Laplacian matrix with eigenvalues in the interval [0, 2).\n$\\blacksquare$\nLEMMA 8.2. [18] If B, C be two n \u00d7 n symmetric matrices, for the decreasingly-ordered singular values A of B, C and BC,\n$\\lambda_{i+j-1} (BC) \\leq \\lambda_{i} (B) \\times \\lambda_{j} (BC)$"}]}