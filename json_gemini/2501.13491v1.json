{"title": "RECALL: Library-Like Behavior In Language Models is Enhanced by Self-Referencing Causal Cycles", "authors": ["Munachiso Nwadike", "Zangir Iklassov", "Toluwani Aremu", "Tatsuya Hiraoka", "Benjamin Heinzerling", "Velibor Bojkovic", "Hilal Alqaubeh", "Martin Tak\u00e1\u010d", "Kentaro Inui"], "abstract": "We introduce the concept of the self-referencing causal cycle (abbreviated RECALL)\u2014a mechanism that enables large language models (LLMs) to bypass the limitations of unidirectional causality, which underlies a phenomenon known as the reversal curse. When an LLM is prompted with sequential data, it often fails to recall preceding context. For example, when we ask an LLM to recall the line preceding \"O say does that star-spangled banner yet wave\" in the U.S. National Anthem, it often fails to correctly return \"Gave proof through the night that our flag was still there\"-this is due to the reversal curse. It occurs because language models such as ChatGPT and Llama generate text based on preceding tokens, requiring facts to be learned and reproduced in a consistent token order. While the reversal curse is often viewed as a limitation, we offer evidence of an alternative view: it is not always an obstacle in practice. We find that RECALL is driven by what we designate as cycle tokens sequences that connect different parts of the training data, enabling recall of preceding tokens from succeeding ones. Through rigorous probabilistic formalization and controlled experiments, we demonstrate how the cycles they induce influence a model's ability to reproduce information. To facilitate reproducibility, we provide our code and experimental details at https://github.com/samunaai/remember.", "sections": [{"title": "1 Introduction", "content": "Consider, by way of metaphor, a large language model (LLM) as the parametric equivalent of a physical library of knowledge (Lederman and Mahowald, 2024). A library evokes structured collections of books or documents, each cataloged for efficient retrieval. Similarly, pretraining LLMs on billions of tokens transforms them into repositories of encoded knowledge (Petroni et al., 2019; Heinzerling and Inui, 2020; Wang et al., 2024). Prompts, therefore, act as cross-references, directing retrieval of specific information, much like library indexes facilitate access to books on shelves.\nIn a library, we expect to retrieve information reliably. However, language models are not always suited for this. For example, when asked to recall the line preceding \"Between their lov'd home and the war's desolation\" in the U.S. National Anthem, a language model often fails to return \u201cO! thus be it ever, when freemen shall stand\" (see Figure 1). This is a symptom of the \"reversal curse,\" where models struggle to generalize reversed relationships from their training data.\nIn this work, we explore a mechanism within LLMs that naturally mitigates the reversal curse leveraging inherently occurring patterns in pretraining data. Specifically, we show how self-"}, {"title": "2 Related Work", "content": "The reversal curse, or \u201cinverse search problem,\" has been extensively studied in large language models. Early works by Berglund et al. (2023) and Allen-Zhu and Li (2023) identify the issue, noting that models struggle to generalize relationships such as \"A after B\" to \"B before A.\" This limitation, stemming from the autoregressive nature of models like GPT (Achiam et al., 2023) and LLaMA (Dubey et al., 2024), persists even in those with chain-of-thought capabilities (Guo et al., 2024).\nRecent studies highlight how reverse thinking enhances reasoning abilities (Chen et al., 2024), leveraging in-context learning to address tasks such as arithmetic and logical deduction. However, it is already known that the reversal curse does not manifest in in-context learning settings (Berglund et al., 2023), as models benefit from explicit contextual information during inference.\nBidirectional models, like BERT (Devlin et al., 2019), avoid the reversal curse by using masked token objectives (Wu et al., 2024), allowing them to reason about context in both directions. However, these models are not designed for autoregressive tasks such as next-token prediction, which underpins state-of-the-art chatbots.\nMethods to mitigate the reversal curse often involve data augmentation. For example, Guo et al. (2024) and Golovneva et al. (2024) explore token permutation techniques, while Springer et al. (2024) propose token repetition to enhance causal links in training data. Unlike these manual interventions, our work investigates naturally occurring patterns in pretraining data, and how they organically mitigate the reversal curse."}, {"title": "3 Formalizing RECALL", "content": "This section formalizes RECALL by introducing the concept of \"cycle tokens\" and their causal effects. To establish this foundation, we begin by revisiting the reversal curse in probabilistic terms."}, {"title": "3.1 Revisiting the Reversal Curse", "content": "Consider the set of all possible token sequences, denoted by S, for a given textual dataset that adheres to a true data distribution P. For simplicity, we assume the language is written in a left-to-right (LTR) script, such as English. However, the arguments generalize naturally to other text directions, such as right-to-left (RTL, e.g., Arabic) or top-to-bottom (TTB, e.g., Japanese), requiring only minor notational modifications.\nLet $S_{seq} \\in S$ be a sequence of n tokens, represented as $S_{seq} := [e_1, e_2,...,e_n]$. We partition $S_{seq}$ into two segments at some index i, where $S_l := [e_1, e_2, ..., e_i]$ represents the left-hand segment, and $S_r := [e_{i+1},..., e_n]$ represents the right-hand segment. Probabilistically, $S_r$ is the most likely continuation of $S_l$ under the distribution P. For example, S could correspond to the continuation \"O say does that star-spangled banner yet wave,\" while $S_l$ represents the preceding context \"Gave proof through the night that our flag was still there.\" Identifying that the latter precedes the former poses a challenge for autoregressive models.\nThe reason for this can be illustrated as follows:\nFor an autoregressive model M trained on the true data distribution, we have $P \\approx P_M$, where $P_M$ denotes the model's learned approximation of the true data distribution. Accordingly, we expect:\n$S_r = \\arg \\max_{s \\in S} P_M(s|S_l)$.\n(1)\nThis implies that M can readily produce the highest-probability right-hand sequence $S_r$, given a left-hand sequence $S_l$. However, the model struggles to select the correct $S_l$ given $S_r$, because:\n$S_l \\neq \\arg \\max_{s \\in S} P_M(s|S_r)$.\n(2)\nInstead, the model can only indirectly compute the left-hand sequence using Bayes' rule:\n$S_l = \\arg \\max_{s \\in S} P_M(S_r|s)P_M(s)$.\n(3)\nEquation (3) is explained in greater detail in Appendix B.1.\nHowever, computing the argmax in equation 3 would require iterating through all possible sequences s \u2208 S, pairing each s with the fixed $S_r$, and then evaluating the combined sequence $[s, S_r]$ using the model M to obtain $P_M(S_r|s)$ and $P_M(s)$."}, {"title": "3.2 Introducing Cycle Tokens", "content": "Our core hypothesis is that, instead of predicting $S_l$ directly from $S_r$, we construct a modified sequence $S:= [e_{i+1},..., e_n, e_1]$ by appending $e_1$ to the end of $S_r$. This modified sequence serves as a pointer back to the start of the original sequence, providing access to $S_l$. The token $e_1$ acts as a cycle token-so named because it induces a cycle in the causal flow of next-token prediction, enabling the model to effectively \u201csee\u201d left-hand tokens from the right-hand side.\nFrom this modified sequence, we can extract $S':= [e_2, e_3, ..., e_i]$ using continued next-token predictions. Importantly, the entire sequence $S_r$ does not need to be repeated; even a single cycle token can serve as a pointer, creating what we term a self-referencing causal cycle (see Figure 2).\nAs the length i of $S'_l$ grows, $S'_l$ increasingly resembles $S_l$. For example, consider two sentences of 100 words each and two sentences of 5 words each. With one differing letter in each pair, the longer sentences exhibit greater similarity.\nFormally:\n$S:= S_re_1 \\text{ and } S'_l := e_1 S'$,\nwhere $\\cdot$ denotes concatenation. Therefore:\n$S_l \\approx S'_l$.\n(5)\nNoting that:\n$S'_l \\rightarrow S_l \\text{ as } i \\rightarrow \\infty$.\nThus, in the presence of a self-referencing causal cycle, a left-to-right autoregressive model can approximate left-hand sequence information from the right-hand side. Interpreted intuitively, the self-referencing causal cycle is a mechanism that allows the model to 'loop back' and access earlier parts of sequences of any length, thereby introducing a bidirectional influence to a unidirectional model."}, {"title": "4 Experiments and Analysis", "content": "To demonstrate self-referencing causal cycles, we use simple few-token datasets and a small decoder-based transformer model (Vaswani, 2017) with two layers and eight attention heads. The datasets are designed for ease of interpretation. Implementation details are provided in Appendix D.\nSuppose we train our model on four-token sequences of the form $[e_1, e_2, e_3, e_1]$. Let el be an integer randomly selected from [1, 100], e2 from [101, 200], and e3 from [201,300]. If each integer is randomly selected, such that each el can only be paired with a unique e2, and each e2 with a unique e3, then the dataset will consist of 100 samples. For example, one possible training sample is $S_{train} = [79,155,264,79]$, where el appears twice at the beginning and the end of the sequence. During training, the transformer memorizes this sequence, and we test whether it can predict the sequence $S_{test} = [264, 79, 155]$. If the model can predict this sequence $S_{test}$ with 100% accuracy, it demonstrates that it can recover 155 from 264 by using the token 79 as a \"cycle\" to link the end of the sequence back to its beginning. In this case, token 79 serves as the cycle token.\nIn all experiments, we consider transitions from a right-hand token (denoted lowercase e) to a left-hand token, and from one sequence (denoted uppercase E) to another. For example, instead of memorizing $S_{train} = [e_1, e_2, e_3, e_1]$, we could memorize $S_{train} = [e_1, E_2, E_3, e_1]$ and test recovery of"}, {"title": "4.2 Stochastic Few-Token RECALL", "content": "The experiments in Section 4.1 assumed that each cycle token-sequence was followed by a unique token-sequence. For instance, in the single-token Hyperlink Composability experiment (Table 1), each el was succeeded by a unique e4 token.\nIn practice, cycle token-sequences often appear multiple times with varying subsequent tokens. For instance, a poem's title may recur across a Wikipedia page in different contexts (Section 4.3), introducing a candidate set of possible left-hand completions, as described in Section 3. To investigate how cycle tokens probabilistically select from a candidate set, we designed a series of experiments, building on the experimental settings of Section 4.1.\nIn Table 2, the Direct Stochasticity experiment expands upon the Baseline few-token experiment by introducing a candidate set ${e_{2i}}_{i=1}^{n}$. For each (el, e2i, e3) combination, we generate n possible e2\u00bf tokens. For example, if n = 3, then a fixed e1 sampled from [1,100] and e3 from [401,500] would recur in 3 samples with 3 distinct e2 values from [101,400]. The same reasoning applies to sequences, where a single E2 maps to n distinct sequences ${E_{2}}_{i=1}^{n}$. Here, n represents the number of candidates, distinct from the sequence length N.\nSimilarly, the Hyperlink Stochasticity experiment expands on the Hyperlink"}, {"title": "4.3 RECALL in Pretraining Corpuses", "content": "Viewing LLMs as dynamic knowledge repositories, we observe self-referencing causal cycles in widely recognized texts, which we refer to as key writings. These recurring phrases act as conceptual hyperlinks, guiding the retrieval of stored information. This hyperlinking behavior, while often overlooked, represents a core mechanism by which LLMs bridge long-range dependencies in text. Our key writings include timeless poems, iconic speeches, and universally familiar nursery rhymes, forming the backbone of cultural memory.\nTo curate our dataset, we queried ChatGPT for examples of popular texts and refined the selection to 50 key writings easily recognizable to those familiar with English literature. A detailed list, along with relevant weblinks, is provided in Appendix A. We cross-referenced this list with a Llama 3 405B model to verify the texts were part of its pretraining data. This verification was conducted offline to ensure no reliance on live queries. Additionally, we analyzed associated webpages to understand how key writings are embedded in real-world corpora (Brown et al., 2020; Touvron et al., 2023; Wang et al., 2021; Gao et al., 2021). Examples are shown in Figures 10 and 11 (in the Appendix).\nWe analyzed the frequency and distribution of cycle token sequences across relevant webpages, treating each key writing's title\u2014or a subsequence of it as a cycle token. These sequences act like cross-references in a library catalog, linking disparate sections of text and enabling efficient retrieval of distant information. Figure 8 illustrates how often these tokens recur, with phrases like \"Star-Spangled Banner\" appearing 73 times in its"}, {"title": "4.3.1 Observing Cycle Token-Sequences", "content": "corresponding Wikipedia article for the U.S. anthem. Figure 7 highlights their distribution, showcasing the extensive causal pathways that cycle tokens create, allowing backward retrieval without breaking left-to-right prediction flow."}, {"title": "4.3.2 Utilizing RECALL", "content": "In a library, readers navigate between sections by following references or repeated titles. Similarly, cycle tokens enable language models to \"jump\" across text sections, retrieving relevant information even when it is stored far from the original query. Without these natural hyperlinks, models often struggle to recall preceding information accurately. One example is the \"preceding line problem,\" adapted from (Golovneva et al., 2024). For"}, {"title": "5 Conclusions", "content": "The reversal curse is a well-documented challenge in generative language models, where the model struggles to predict preceding tokens based on succeeding tokens. While prior work has primarily focused on data augmentation or architectural changes to address this limitation, we propose an alternative perspective: language models, much like libraries, may already possess latent mechanisms to cross-reference token sequences within their pretraining data.\nIn this work, we introduce the concept of self-referencing causal cycles (abbreviated RECALL), which act as natural hyperlinks in a model's memory, allowing it to retrieve left-hand tokens from right-hand tokens. We demonstrate this concept at an axiomatic level through controlled few-token experiments on a small transformer model. The flexibility of these cycles under stochastic conditions suggests that RECALL mechanisms can scale to large language models. Notably, we find experimentally that self-referencing cycles emerge naturally from repeated token patterns in pretraining corpora and can enable models to overcome the reversal curse without requiring additional modifications. By leveraging these cycles, language models are able to overcome the limitations of autoregressive text generation and produce more reliable responses to prompts affected by the reversal curse."}, {"title": "Limitations", "content": "While this study demonstrates the potential of self-referencing causal cycles to mitigate the reversal curse, there are several limitations to consider. Our experiments are conducted in controlled settings with simplified token sequences. However, the performance of autoregressive models deployed in real-world applications may be influenced by additional factors, such as retrieval-augmented generation or web search. Further interpretability techniques may be required to precisely attribute parametric information retrieval to specific cycle tokens in the pretraining data. This poses a non-trivial challenge, as larger models often utilize fully or partially closed-source training data, and extracting pretraining data from the models themselves is intentionally designed to be difficult to maintain privacy and security."}, {"title": "B.1 The Argmax Over S", "content": "We shall illustrate how we arrive at Equation 3 from Equation 1 in Section 3.\nFirstly, as per Equation 1, we have:\n$S_r = \\arg \\max_{s \\in S} P_M(s|S_l)$.\nThe true probability distribution of a left-hand sequence seen in the training data, given a right-hand sequence, is given by:\n$P(s|S_r) = \\frac{P(S_r|s)P(s)}{P(S_r)}$\n(6)\nas per Bayes rule. Equivalently, we have that:\n$\\arg \\max_{s \\in S} P(s|S_r) = \\arg \\max_{s \\in S} \\frac{P(S_r|s)P(s)}{P(S_r)}$\n(7)\n$= \\arg \\max_{s \\in S} \\frac{P(S_r|s)P(s)}{C}$\n(8)\n$= \\arg \\max_{s \\in S} P(S_r|s)P(s)$, (9)\nwhere C is a constant, indicating that $P(S_r)$ does not affect the final arg max.\nIf the LLM is a perfect model of the true data distribution (i.e., a perfect library),\n$\\forall s \\in S : P(s) = P_M(s)$.\n(10)\nThus we have:\n$\\arg \\max_{s \\in S} P(s|S_r) = \\arg \\max_{s \\in S} P_M(S_r|s)P_M(s)$,\n(11)"}, {"title": "B.2 The Candidate Set Sie CS", "content": "One significant challenge in utilizing self-referencing causal cycles lies in efficiently computing the arg max over possible left-hand sequences s\u2208 S. A brute-force approach, iterating through all candidates s \u2208 S, is computationally infeasible. For a fixed vocabulary size v, the total number of k-length sequences is $v^k$, which grows exponentially as k increases.\nTo address this, we aim to construct a smaller candidate set Sie CS, with $|S_{i_c}|< |S|$, that can serve as a proxy for the full set S. By narrowing down the candidate set intelligently, we can perform the necessary arg max computation efficiently. Specifically, at each iteration, we pair a candidate s \u2208 Sie with Sr and evaluate the likelihood of observing [s, Sr] in context. Formally, this is expressed as:\n$S_l = \\arg \\max_{s \\in S_{i_c}} P_M(S_r|s)P_M(s)$\nIf St is constructed effectively, the arg max computed over St will match the arg max over the entire set S."}, {"title": "C Examples are Insufficient for Reversal", "content": "We conducted a token experiment using an 8-layer transformer designed to memorize simple token-sequences of the form (e, r, f). Each sample was constructed by randomly selecting e from [1,10000] and \u0192 from [10001, 20000], pairing with one of two relation tokens: r = 20001 or its inverse r' = 20002. The notation (e, r, f) stands for entity, relation, feature, and is analogous to the (s, r, o) structure commonly used in works such as (Takahashi et al., 2024), where e and f correspond to subject and object entities, respectively. Importantly, each e is paired uniquely with an f, resulting in 20000 unique entity-feature pairings, appearing in one of four possible configurations:\n(F, e, r, f) \u2014 (1)\n(F, f, r', e) \u2014 (2)\n(R, f, r, e) \u2014 (3)\n(R, e, r', f) \u2014 (4)"}, {"title": "D Experimental Settings", "content": "For the deterministic few-token experiments of Section 4.1, we use a small (~90,000 parameters) decoder-based transformer model with 2 layers and 8 attention heads. The default model embedding dimension is 36, with the exception of the Length-of-Path and candidate set size experiments, for which it is increased to 256. Training is conducted using cross-entropy loss, which is well-suited for the next-token prediction problem in transformer models. We run multiple random seeds but observe no change in results due to the well-defined nature of the problem. The experiments, implemented in PyTorch and NumPy, were performed on an NVIDIA A100 GPU and trained with the Adam optimizer (learning rate: 0.001, batch size: 1024). Reproduction requires a compute budget of slightly over 1 hour (1 hour and 9 minutes in our rerun). For the stochastic case experiments in Section 4.2, we maintain the same experimental settings, but with time required for reproduction increasing slightly to over 1.5 hours."}]}