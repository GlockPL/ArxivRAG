{"title": "Clio: Privacy-Preserving Insights into Real-World AI Use", "authors": ["Alex Tamkin", "Miles McCain", "Kunal Handa", "Esin Durmus", "Liane Lovitt", "Ankur Rathi", "Saffron Huang", "Alfred Mountfield", "Jerry Hong", "Stuart Ritchie", "Michael Stern", "Brian Clarke", "Landon Goldberg", "Theodore R. Sumers", "Jared Mueller", "William McEachen", "Wes Mitchell", "Shan Carter", "Jack Clark", "Jared Kaplan", "Deep Ganguli"], "abstract": "How are AI assistants being used in the real world? While model providers in theory have a window into this impact via their users' data, both privacy concerns and practical challenges have made analyzing this data difficult. To address these issues, we present Clio (Claude insights and observations), a privacy-preserving platform that uses Al assistants themselves to analyze and surface aggregated usage patterns across millions of conversations, without the need for human reviewers to read raw conversations. We validate this can be done with a high degree of accuracy and privacy by conducting extensive evaluations. We demonstrate Clio's usefulness in two broad ways. First, we share insights about how models are being used in the real world from one million Claude.ai Free and Pro conversations, ranging from providing advice on hairstyles to providing guidance on Git operations and concepts. We also identify the most common high-level use cases on Claude.ai (coding, writing, and research tasks) as well as patterns that differ across languages (e.g., conversations in Japanese discuss elder care and aging populations at higher-than-typical rates). Second, we use Clio to make our systems safer by identifying coordinated attempts to abuse our systems, monitoring for unknown unknowns during critical periods like launches of new capabilities or major world events, and improving our existing monitoring systems. We also discuss the limitations of our approach, as well as risks and ethical concerns. By enabling analysis of real-world AI usage, Clio provides a scalable platform for empirically grounded AI safety and governance.", "sections": [{"title": "Introduction", "content": "Despite widespread interest about the impact of AI systems on society, there is remarkably little public data about how models are actually being used in practice. What kinds of capabilities are seeing the most real-world adoption in the economy? How does usage vary across different communities and cultures? Which anticipated benefits and risks are most borne out in concrete data?\nThis lack of understanding is particularly striking because model providers have access to usage data that could be used to answer these exact questions. Providers, however, face significant challenges in analyzing this data and sharing these potential insights:"}, {"title": "High-level design of Clio", "content": "AI assistants can be used for an extremely wide range of tasks, from writing code to planning a wedding to brainstorming scientific experiments. This diversity makes it challenging to understand how these systems are actually being used and what risks they might pose. Traditional pre-deployment assessments like benchmarks and red-teaming are valuable but inherently limited, as they can only test for issues we think to look for.\nClio addresses this challenge by enabling bottom-up analysis of real-world AI usage. Given a large collection of conversations between users and models, Clio identifies broad patterns and trends while preserving user privacy. For example, Clio could reveal that a significant number of users are using Claude for debugging code, or surface coordinated attempts across multiple accounts to misuse the system for a disallowed purpose."}, {"title": "Enabling exploratory search for unknown unknowns", "content": "Clio is designed to enable analysts to discover unknown unknowns\u2014including risks or applications that were not anticipated by model providers. To do so, Clio implements several design principles to facilitate sensemaking [Weick et al., 2005] and exploratory search [Marchionini, 2006, White and Roth, 2009]-where analysts can start with broad questions and iteratively discover patterns, rather than having to know what to look for in advance.\nUnlike traditional ML benchmarks that test for predefined capabili- ties or behaviors, Clio aims to surface patterns that emerge naturally from actual usage data. This approach helps identify both expected and unexpected uses of AI systems, without requiring us to know what to look for in advance. The clusters and descriptions Clio provides can be useful in their own right as a detailed but privacy-preserving view of the dataset, or they can be used to identify areas of concern for further investigation by our safety team.\nClio is designed to be scalable to many millions of con- versations. To enable users to navigate large numbers of patterns discovered in a vast dataset, Clio recursively organizes base-level clusters into a multi-level hierarchy that lets users start from a set of"}, {"title": "How Clio works: a brief system design overview", "content": "At a high level, Clio works through a multi-stage pipeline that transforms raw conversations into privacy-preserving insights:\nFor each conversation, Clio extracts multiple \"facets\"-specific attributes or characteristics such as the high-level conversation topic, number of conversational turns, or language used. Some facets are computed directly (e.g., number of turns), while others are extracted using models (e.g., conversation topic).\nClio then groups similar conversations by creating embeddings [Reimers and Gurevych, 2019, 2022] of one of the natural language facets (e.g., conversation topic) and using k-means clustering [Lloyd, 1982]."}, {"title": "How are people using Claude.ai?", "content": "Although Claude and other Al assistants have been adopted across a wide range of use cases, little is still known about how people use these models in the real world. With Clio, we can begin to gather high-level insights into the kinds of tasks people are using Claude.ai for."}, {"title": "Top use cases in Claude.ai", "content": "Using the Clio facet \u201cWhat task is the model being asked to perform in this conversation?\" (Section 2.2), Clio surfaced thousands of clusters corresponding to the different tasks represented in user's conversations with Claude.ai. We then used Clio's hierarchy viewer to create a taxonomy of usage, then inspected the top-level categories. For more information about the underlying Claude.ai data used for this analysis, see Appendix G.1.\nAs shown in Figure 6, which displays the top ten of these top-level categories, we see a particular emphasis on coding-related tasks, with the \"Web and mobile application development\u201d category representing over 10% of all conversations. We also noticed a high percentage of writing, research, and educational usage, each comprising 6-10% of usage. We also analyzed the distribution of tasks in popular public datasets LMSYS-1M-Chat [Zheng et al.,\""}, {"title": "Notable granular clusters identified by Clio", "content": "In addition to the high-level clusters we identified in Claude.ai, Clio also identified thousands of more granular usage clusters. Here we highlight three particularly interesting task categories, sharing details from the privacy-preserving cluster titles and summaries generated by Clio.\nconversations involved topics ranging from dream analysis and symbolism to exploring philosophical ideas about consciousness.\nusers prompted models to act as a Dungeon Master or game master for tabletop roleplaying games\u2014guiding players through adventures, describing game environments, and managing gameplay mechanics.\ninteractions focused on de- veloping algorithms and techniques to improve traffic management, route planning, and transportation network efficiency."}, {"title": "How does Claude usage vary across languages?", "content": "Using Clio, we analyzed high-level patterns in Claude's multilingual usage to better understand how our product is used across different linguistic communities. We were particularly interested in identifying distinct usage patterns between English and non-English conversations. For more information about the data and our sampling strategy, see Appendix G.1. For more information about Clio's multilingual performance and language detection, see Appendix C and Appendix G.4.1.\nOur analysis revealed that certain topics were significantly more prevalent in non-English conver- sations compared to English ones. These included discussions of economic issues (e.g., Explain and analyze economic theories and their real-world applications), social issues (e.g., Research and develop solutions for aging populations and elderly care), ure (e.g., Create and analyze anime and"}, {"title": "Clio for safety", "content": "The general-purpose nature of AI systems makes it hard to anticipate all of the potential risks they pose. By surfacing patterns from real-world usage, Clio can identify harms that other safeguards in an AI system safety stack might not have been designed to catch.\nIn contrast to the use cases discussed in prior sections, our use of Clio for safety purposes differs in fundamental ways. For example, we do not employ the cluster aggregation thresholds and cluster auditing techniques described above so that we can identify \u2013 and take enforcement action against users and accounts that are violating our policies. In addition, the results from safety-focused Clio runs can be linked back to individual accounts. We put in place strict access controls to limit who can view these results to a small number of authorized staff.5 Our use of Clio for safety and security purposes is consistent with our terms, policies, and contractual agreements. The insights we share in this section reflect Claude.ai Free and Pro traffic. For more information about our policies, see Appendix F.\nIn this section, we discuss three ways that Clio has been used to improve Anthropic's safety systems: identifying real-world attempts at scaled abuse on our systems, monitoring for unknown unknowns during periods of increased uncertainty, and strengthening our safety classifiers. Note that this section is not intended to taxonomize all harms across our models, but rather to provide examples of the ways that Clio has helped us improve our safety systems."}, {"title": "Identifying patterns of violative behavior", "content": "Because Clio identifies patterns of behavior, it can identify patterns of violative behavior that would not be visible at the level of individual conversations. This section describes at a high level several"}, {"title": "Monitoring for unknown unknowns during periods of increased uncertainty and high-stakes events", "content": "Clio has especially proved its utility during periods of increased uncertainty and high-stakes events, such as launches of new capabilities or important events such as elections. We detail two such cases here:\nWe used Clio to monitor for unknown unknowns after the initial launch of the refreshed Claude 3.5 Sonnet in October 2024, which is capable of interacting with tools that can manipulate a computer desktop environment. While Anthropic made several efforts to map our risks and red-team the system, we recognized that it would be impossible to anticipate all potential risks and failure modes, motivating the need for a more comprehensive approach to post-deployment monitoring. To do this, we ran Clio on a large sample of conversations that were identified by Claude to contain instances of Claude operating a computer. Our Trust and Safety team used this information to refine our safety measures, better understand computer use harms, and take action on violative accounts.\nWe used Clio to monitor for unknown risks in the months preceding the 2024 US general elections. While AI systems can serve legitimate educational purposes-helping users understand voting procedures, analyze historical election data, or learn about constitutional processes they also present novel risks in the elections context. For instance, AI systems could be misused to generate misleading content at scale, create sophisticated disinformation campaigns that target specific demographics, impersonate candidates and election officials, or enable other unknown harms that we did not anticipate. Anthropic's Usage Policies prohibit the use of Claude"}, {"title": "Understanding the effectiveness of our safety classifiers", "content": "Anthropic takes a multi-layered approach to safety: in addition to training and instructing models to refuse harmful requests, we also use classifiers to detect, block, and take actions based on harmful conversations. In this section, we describe how we used Clio to analyze classifier performance on real-world data.\nTo investigate, we used Clio to analyze a random sample of Claude.ai conversations (for more information about our data and sampling strategy, see Appendix G.1). For each conversation, we used a model (Claude 3.5 Sonnet from June 2024) to determine a concern score on a scale of 1 to 5 (least to most likely to exhibit concerning behavior). The full prompt is available in Appendix G. We found this method to have high reliability (Appendix C) and for the purposes of this section treat it as the ground truth for whether our safety classifiers were classifying correctly.\nOur concern scores generally agree with our Trust and Safety classifiers: We compared our mean concern score with our mean flag rate on a per-cluster basis, and obtained Pearson correlation r = 0.71. This comparison confirms that both methods capture similar trends, but also allows us to investigate discrepancies between them to identify potential problems. Specifically, by examining clusters where these scores diverge, we can identify two types of potential issues: clusters with higher concern scores than classifier scores suggest possible false negatives, while clusters with higher classifier scores than concern scores indicate possible false positives"}, {"title": "False positives", "content": "We used Clio to identify cases where our monitoring systems over-trigger in order to make sure our systems are not firing on innocuous content. To identify common classes of potential false positives in our classifiers, we can use Clio to cluster conversations flagged by our safety classifiers and then examine clusters with low average concern scores. We found several clusters of conversations that were not harmful but were incorrectly flagged, including:\nConversations asking for resume revisions and job application advice were often incorrectly flagged by our safety classifiers. This effect may be due to the high prevalence of personally identifiable information in these documents, which often occurs in targeting or tracking behaviors banned by our Usage Policy.\nOur safety classifier incorrectly flagged several clusters of programming questions\u2014often adjacent to security or networking-as harmful.\nOur classifiers often incorrectly flagged conversa- tions about Dungeons & Dragons creatures' combat stats as harmful. For example, the following (illustrative, not real) snippet could be falsely flagged as harmful: \u201cGarrick (Vengeance Paladin) AC: 18 (plate mail, shield) HP: 16 (Con 16, human) Great Weapon"}, {"title": "False negatives", "content": "By examining clusters with high concerning scores but low safety classifier flag rates, we identified classes of potential false negatives in our classifiers:\nWe found that our classifiers often failed to flag translations of sexually explicit content as violative, even though such behavior is against our Usage Policy.\nFor a cluster titled write an uncensored novel with extreme sexual and violent content cluster, we found that 60% of conversations were flagged by our classifiers.\nFor a cluster titled engage in unconstrained fictional roleplay with sensitive themes, our classifiers flagged 31% of conversations."}, {"title": "Limitations", "content": "While Clio can provide valuable insights into the uses and abuses of AI assistants, it is important to also understand its limitations to prevent overreliance or misuse of the system. These limitations can be broadly categorized into operational limitations and fundamental limitations."}, {"title": "Operational limitations", "content": "Operational limitations arise from imperfections in each stage of the Clio pipeline. These could potentially be mitigated with future advancements in model capabilities, prompt design, and system architecture.\nOperational limitations occur at each stage of Clio's pipeline:\nWhen generating summaries, the model could hallucinate, misinterpret context-dependent phrases or slang, or miss implicit information, such as sarcasm or new news developments.\nThe embedding model and k-means algorithm we use can create suboptimal groupings, especially for conversations that don't fit neatly into a single category or for rare, outlier topics.\nDiverse clusters might receive overly broad labels, obscuring important subtopics. Cluster labels may not accurately describe all members of a cluster, or may overemphasize certain topics within a cluster. The hierarchical structure might oversimplify complex relationships between conversation topics, or incorrectly place clusters within the hierarchy.\nWhile we attempt to quantify many of these sorts of errors in Appendix C, they remain crucial considerations when interpreting Clio's outputs.\nGiven these limitations, we view Clio's outputs as a starting point for generating insights and leads for further investigation. Its outputs should be considered preliminary and require additional validation before being used as the basis for decision-making. Taking automated actions based solely on Clio's output would require more rigorous analysis and human oversight to ensure both accuracy and fairness.\nFurthermore, Clio cannot conclusively prove the absence of certain patterns or behaviors. The imperfections in the pipeline or potential adversarial actions could allow harmful behavior to escape undetected by Clio. This underscores the importance of complementing Clio's analysis with other methods."}, {"title": "Fundamental limitations", "content": "Beyond operational limitations, Clio faces several fundamental limitations that are inherent to its design and purpose:\nWhile Clio can analyze content and infer topics, it cannot definitively determine user intentions behind a request. Misuses or other harmful behavior might be missed if they are encoded subtly across multiple requests to different models or if the user's true intent differs from how they present it to the model.\nClio only analyzes patterns within conversations, not how these conversations translate into real-world actions or impacts. This means we cannot directly observe the full societal effects of AI system use."}, {"title": "Risks, ethical considerations, and mitigations", "content": "While Clio offers valuable insights into the uses and potential misuses of AI assistants, it also raises important considerations around user trust and privacy that must be carefully considered and mitigated. This section outlines our rationale for publishing this work, along with several risks we considered and our proposed strategies to address them."}, {"title": "Justification for building and publishing Clio", "content": "We believe that building Clio and publishing information about it is important for several reasons.\nFirst, Clio provides significant value in understanding the potential societal impacts of AI assistants, offering insights that are difficult or impossible to obtain through other means. It serves as bottom-up tool for generating empirical insights, allowing us to empirically understand how these systems are being used and potentially misused in real-world scenarios including ones we may not have anticipated in advance. In this way, Clio helps fill a crucial gap left by top-down approaches to AI safety. While proactive safeguards (such as pre-deployment testing) and theoretical analyses are essential, they cannot anticipate every possible use case or emergent behavior. Clio complements these approaches by providing real-world data and insights on how AI assistants are actually being used, helping to identify unforeseen challenges, opportunities, and trends. We believe this information is important for the public to have in order to improve AI safety, develop more effective governance frameworks, and guide the ethical development of future AI systems.\nMoreover, the core technologies underlying Clio are not fundamentally new. Data visualization and clustering techniques have been widely used across various industries for years. Furthermore, in the past year, several companies and researchers have developed similar clustering-and-summarization methods to ours, applied to other use cases [Nomic, 2024, Lam et al., 2024]. The widespread avail- ability of these tools means the key question is whether their application to AI assistant interactions is ethically justified\u2014which we believe it is, given Clio's careful privacy protections and focus on societal benefit. By openly discussing Clio, we also aim to contribute to positive norms around the responsible development and use of such tools, emphasizing privacy protection, ethical considerations, and societal benefit."}, {"title": "Privacy considerations", "content": "Risk: Despite the different privacy components in our system (Appendix D), two potential privacy risks remain. First, our safeguards might fail to prevent certain types of personally identifiable information (PII) from persisting through multiple stages of our pipeline, especially if failures are correlated across stages. Second, we may encounter unforeseen forms of privacy infringement, such as group privacy violations [Taylor et al., 2016], where aggregated data could reveal sensitive information about specific communities without revealing information specific to an individual. While our privacy evaluations suggest this is unlikely, such risks are important to acknowledge and prepare for."}, {"title": "False positives", "content": "Risk: For Trust and Safety enforcement purposes, one potential risk is that actions taken based on Clio's outputs could have false positives, particularly if the system were to be used for automated enforcement. If particular clusters are flagged as problematic and that signal is used to automatically ban or restrict accounts, some non-violating users might be included as false positives.\nMitigation Strategy: To address this risk, we do not currently take automated enforcement actions solely based on Clio clusters. We also measure the performance of our systems across different distributions; for example, our multilingual evaluations in Table 5."}, {"title": "Potential for misuse", "content": "Risk: As with any sort of tool that is useful for gaining insights into the way a technology is used, a potential risk is misuse in ways that interfere with privacy or civil liberties.\nMitigation Strategy: To mitigate this risk, we've implemented strict access controls, data minimiza- tion, and retention policies both within Clio and across Anthropic.14 Within the Clio pipeline, we only collect the minimum amount of data necessary for Clio's intended functions (e.g., conversation summaries rather than full conversations), and Clio does not support analysis based on geography; for more information about Clio's privacy protections, see Section D."}, {"title": "User trust", "content": "Risk: Despite our privacy mitigations, the existence of a system like Clio might be perceived as invasive by some users. This perception could lead to an erosion of trust in AI assistants.\nMitigation Strategy: First, we plan to be radically transparent about Clio's purpose, capabilities, and limitations to the public through this report, rather than building and not disclosing the system.15 For example, Clio is a tool that can be used to make systems safer, as well as a tool that can be used to gain insights that can be used to gain a better understanding of and improve the product. We are also transparent about how we designed Clio with important privacy protection features that safeguard user data and privacy. Second, beyond these use cases, we are committed to turning Clio's insights into a public good-for example, we released information about our most common use cases in Figure 6 because we believe it is in the best interest of society to know how AI systems are being used in the world, despite the fact that this information could be commercially harmful for Anthropic to publish from a competitive intelligence standpoint. We plan to share further insights from Clio in the future, and hope these disclosures contribute to an emerging culture of empirical transparency in the field that can inform broader AI safety and governance efforts. Finally, we plan to actively engage with user communities, addressing concerns and incorporating feedback into our development process-for example, during our work on Clio we met with a number of civil society organizations to gather feedback on our approach and made adjustments in response to their comments."}, {"title": "Feedback from civil society experts", "content": "Throughout Clio's development process, we sought feedback from experts in the privacy, safety, and civil liberties communities. These consultations provided valuable input that shaped multiple aspects of the system, including expanding our multilingual validation methods, clarifying our privacy mechanisms, and identifying priority research areas that could benefit the broader AI governance community. The discussions highlighted both opportunities and challenges for Clio, from potential"}, {"title": "Related work", "content": "Clio builds upon and extends work across several domains, including analysis of the user of AI assistants, privacy-preserving analytics, user behavior analysis, and AI ethics."}, {"title": "Analyzing the use of AI assistants", "content": "Several datasets have been developed to analyze the use of AI assistants. WildChat [Zhao et al., 2024] provides a large-scale dataset of 1M conversations, collected by offering free usage to GPT 3.5-Turbo and GPT-4 through HuggingFace Spaces, and provides insights into user interactions and model performance. Similarly, LMSYS [Zheng et al., 2023] focuses on comparative evaluation of different language models through an online platform (the Chatbot Arena), and provides a dataset of 1M queries and responses. Furthermore, public datasets like the Anthropic Red Team dataset [Ganguli et al., 2022] and the Stanford Human Preferences Dataset [Ethayarajh et al., 2022] were collected from crowdworkers, and have been used to understand potential misuse and user preferences. These datasets have been used for a variety of different purposes, such as studying the gap between NLP research and empirical use [Ouyang et al., 2023] and measuring the prevalence of private disclosures in user queries [Mireshghallah et al., 2024]. Finally, [Suri et al., 2024] analyze 80,000 conversations on the Bing Copilot generative search engine, identifying 25 high-level categories of usage, and Eloundou et al. [2024] study how biases manifest across 66 common use-cases, which were identified by analyzing 10,000 ChatGPT conversations.\nClio builds on this work by providing the first in-depth analysis of direct traffic on a major AI assistant, encompassing millions of data points, and conducting a range of privacy-preserving analyses of that data to give a broader picture of uses and misuses of language models in the wild."}, {"title": "Privacy-preserving analytics in AI", "content": "As AI assistants process increasingly sensitive data, privacy-preserving analytics has become crucial. Techniques such as differential privacy [Dwork et al., 2006, Lyu et al., 2020], k-anonymity [Sweeney, 2002], and federated learning [McMahan et al., 2017] have been developed to protect individual user data while allowing for aggregate analysis. Additionally, a wide range of works discuss methods for rewriting or removing private information from text [Eder et al., 2020, Pil\u00e1n et al., 2022] or investigate the intersection of language models and privacy, especially whether language models can learn and output private information [Pan et al., 2020, Mireshghallah et al., 2020, Brown et al., 2022, Neel and Chang, 2023, Peris et al., 2023, Yao et al., 2024, Das et al., 2024].\nClio builds upon these approaches, and takes a multi-layered approach to privacy. By using AI assistants themselves to remove personally identifiable information and applying strict aggregation thresholds, Clio achieves a high level of privacy protection while maintaining the ability to derive meaningful insights. This approach enables Clio to achieve high levels of both privacy and insight in complex, unstructured data like conversations."}, {"title": "Bottom-up visualization and exploratory Search", "content": "Information visualization research has long recognized that many important information-seeking behaviors do not fit simple query-response patterns [Marchionini, 1995]. Key theoretical frame- works\u2014including berrypicking [Bates, 1989], levels of information need [Taylor, 1968], and sense- making [Russell et al., 1993]\u2014describe information seeking as an evolving process where users' goals and queries change as they explore. Visualization systems often support this through dynamic querying [Ahlberg et al., 1992], overview+detail interfaces [Cockburn et al., 2009], and the \"Overview first, zoom and filter, then details on demand\" principle [Shneiderman, 2003].\nSeveral systems in this vein have tackled visualizing and exploring large text collections. Overview [Brehmer et al., 2014] supports document exploration through clustering, Jigsaw [Stasko et al., 2007] reveals entity connections across documents, DocuBurst [Collins et al., 2009] uses radial visualizations with WordNet hyponymy, Serendip [Alexander et al., 2014] enables topic model"}, {"title": "Mitigating and anticipating risks from AI", "content": "As AI systems become more prevalent, numerous frameworks for AI ethics and governance have been proposed [Jobin et al., 2019, Hagendorff, 2020, Weidinger et al., 2021, 2022, Chan et al., 2023, Gabriel et al., 2024]. These frameworks typically identify risks or provide guidelines for responsible AI development and deployment, but often lack empirical grounding in real-world usage patterns, without which it is difficult to fully capture or anticipate the impacts of AI systems on society [Weidinger et al., 2023].\nClio contributes to this field by providing a bottom-up, data-driven approach to understanding the uses and potential misuses of AI systems. By offering insights into actual usage patterns, including real-world attempts at misuse, Clio can inform more effective and targeted governance strategies. This empirical approach complements existing theoretical frameworks and can help bridge the gap between these top-down goals and their practical implementation, creating a mutually reinforcing process [Stein et al., 2024]. It also dovetails well with abuse disclosures shared by other model providers [Nimmo, 2024], and calls from civil society for greater transparency into AI usage data [Nicholas, 2024]."}, {"title": "Conclusion", "content": "This paper introduces Clio, a privacy-preserving platform that enables bottom-up discovery of how AI assistants are being used in practice, complementing existing top-down approaches to AI evaluation and safety. Our analysis of millions of conversations reveals a range of patterns in Claude.ai data\u2014from significant variations in cross-language usage (e.g., higher use of language learning in non-English languages) to novel forms of coordinated misuse attempts (like automated spam generation across multiple accounts). We also demonstrate how Clio can be used to improve our existing safety systems and monitor for unknown unknowns during periods of increased uncertainty, such as the release of new capabilities or before major elections. As AI systems become more capable and integrated into society, empirical understanding of their real-world use will become increasingly crucial. By sharing our methods and ongoing findings, we hope to contribute to an emerging culture of empirical transparency in the field to help society tackle this challenge."}, {"title": "Author Contributions", "content": "Alex Tamkin led the project, including proposing the idea, building the initial proof of concept system (with Deep Ganguli), leading design and analysis of the experiments, and writing the paper. Miles McCain built the scalable Clio system used in the paper, ran the majority of the experiments, led the engagement with civil society organizations, and made deep contributions to the experimental design, analysis, and writing. Kunal Handa ran the high-level use case experiments, and contributed to the experiments in the safety and multilingual sections and the writing of the paper. Esin Durmus designed and ran the initial versions of the multilingual experiments in the paper. Liane Lovitt contributed to the framing and execution of the experiments, organizational support, and feedback. Ankur Rathi provided deep contributions to the privacy framing and experiments. Jack Clark and Jared Kaplan provided high level guidance and support throughout the process. Deep Ganguli provided detailed guidance, organizational support, and feedback throughout all stages of the project, including the initial proof of concept, design of the experiments, analysis, and feedback on drafts. All other authors contributed to the framing, experiments, analysis, figures, or development of otherwise-unpublished models, infrastructure, or contributions that made our work possible."}, {"title": "System architecture: how does Clio work?", "content": "In this section we describe technical components of Clio, our system which implements the high-level design goals laid out in Section 2. We focus on the most salient components, deferring specific prompts and hyperpameters to Appendix G and evaluations of privacy and reliability to Appendices C and D respectively.\nTo use Clio, one typically begins with a target dataset. This dataset is typically an unfiltered sample of Claude traffic, but one could also choose other datasets, including filtering down the target dataset using a regular expression or an AI model to analyze a more narrow distribution of data."}, {"title": "Extracting facets", "content": "After defining a target distribution, Clio enables users to understand and explore many different aspects of the data through facets. A facet represents a specific attribute or characteristic of a conversation, such as the user's request, the language being used, or the number of turns in the conversation.\nFacet extraction in Clio can be as simple as a program to compute a statistic from the data (e.g., the number of turns in the conversation) or as complex as using a model to extract a categorical value (e.g., the language), or a high-level summary (e.g., the topic) from each conversation.\nIt's important to note that we extract multiple facets for each conversation, providing a multi- dimensional view of the data. This approach enables us to examine a wider range of aspects of use, and crucially to enable users to explore intersections across these facets (e.g., how use cases vary by language)."}, {"title": "Semantic Clustering", "content": "To identify meaningful patterns in Claude usage, Clio employs semantic clustering on summary facets. This approach allows us to group similar conversations based on their high-level content and intent, rather than specific details that might compromise privacy. The process involves two main steps: embedding and clustering.\nFirst, we create embeddings from the summary facets, such as user intent or conversation topic. These embeddings are dense vector representations that capture semantic meaning; conversations will have similar embeddings depending on their summary. The choice of summarization prompt (e.g., What is the overall topic of the conversation? or What is the user's prompting style?) controls the information extracted in the summaries, and thus influences which conversations get placed close together in the embedding space.\nFor clustering, we primarily use the k-means algorithm due to its efficiency and effectiveness for our use case. While we experimented with other clustering methods, we found that k-means works"}, {"title": "Cluster Labeling and Hierarchization", "content": "After forming clusters of semantically similar conversations, we generate meaningful labels and organize them into a hierarchical structure. This process makes the results more interpretable, actionable, and easier to navigate, especially when dealing with a large number of clusters.\nFor labeling, we use a model (Claude 3.5 Sonnet) to generate concise, informative descriptions for each cluster. We prompt the model with a sample of conversation summaries from the cluster, instructing it to capture common themes or intents without including any potentially identifying or private information.\nTo manage the complexity of hundreds or thousands of clusters, we use Claude to generate a hierarchy of clusters. For more information about our algorithm, see Appendix G.7. These hierarchical clusters allow users to start with a high-level overview and drill down into more specific clusters as needed, facilitating both broad insights and detailed exploration."}, {"title": "Data Exploration and Visualization", "content": "The final stage of Clio's pipeline involves presenting the clustered and labeled data in an intuitive, interactive format that enables deep exploration and insight generation. Our visualization approach is designed to support both high-level overviews and detailed investigations.\nKey features of our data exploration interface include:\nA 2D projection of the clusters, allowing users to visually explore the relationship between different clusters. Users can zoom in and out to see progressively more granular clusters for different categories.\nA hierarchical representation of the clusters, enabling users to navigate from broad categories down to specific sub-clusters.\nWhen a cluster is selected, a sidebar shows the breakdown of that cluster by other facets (e.g., language or turn length). Users can also see how that facet membership has changed over time, helping identify emerging trends or shifts in usage patterns.\nUsers can select another facet (e.g., language=Spanish), coloring the map to display the prevalence of that feature across the different clusters.\nFor cases where the underlying data isn't sensitive (such as synthetic data or public datasets), or for Anthropic employees with an authorized business need in accordance with our privacy policy (such as Trust & Safety team members), we also provide a \u201ctraces\u201d feature. This allows drilling down into representative examples from each cluster, providing concrete context for the patterns identified by Clio.\nOur visualization approach is designed to balance the need for powerful exploration capabilities with our strong commitment to user privacy. By presenting aggregated data and carefully curated examples, we enable meaningful insights without compromising individual user privacy."}, {"title": "Example summaries and clustering", "content": "To provide a concrete example of how Clio operates, we provide several examples of conversations from the public WildChat [Zhao et al., 2024] and their associated summaries and varying levels of clusters in Table 1."}, {"title": "How are people using other AI assistants?", "content": "To understand how the Claude.ai"}]}