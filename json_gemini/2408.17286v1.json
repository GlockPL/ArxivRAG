{"title": "Stationary Policies are Optimal in\nRisk-averse Total-reward MDPs with EVaR", "authors": ["Xihong Su", "Marek Petrik", "Julien Grand-Cl\u00e9ment"], "abstract": "Optimizing risk-averse objectives in discounted MDPs is chal-\nlenging because most models do not admit direct dynamic pro-\ngramming equations and require complex history-dependent\npolicies. In this paper, we show that the risk-averse total re-\nward criterion, under the Entropic Risk Measure (ERM) and\nEntropic Value at Risk (EVaR) risk measures, can be optimized\nby a stationary policy, making it simple to analyze, interpret,\nand deploy. We propose exponential value iteration, policy\niteration, and linear programming to compute optimal policies.\nIn comparison with prior work, our results only require the\nrelatively mild condition of transient MDPs and allow for both\npositive and negative rewards. Our results indicate that the\ntotal reward criterion may be preferable to the discounted cri-\nterion in a broad range of risk-averse reinforcement learning\ndomains.", "sections": [{"title": "Introduction", "content": "Risk-averse Markov decision processes (MDP) (Puterman\n2005) that use monetary risk measures as their objective have\nbeen gaining in popularity in recent years (Kastner, Erdogdu,\nand Farahmand 2023; Marthe, Garivier, and Vernade 2023;\nLam et al. 2022; Li, Zhong, and Brandeau 2022; B\u00e4uerle and\nGlauner 2022; Hau, Petrik, and Ghavamzadeh 2023; Hau et al.\n2023; Su, Petrik, and Grand-Cl\u00e9ment 2024). Risk-averse ob-\njectives, such as Value-at-Risk (VaR) or Conditional-Value-at-\nRisk (CVaR) penalize the variability of returns (Follmer and\nSchied 2016). As a result, one prefers policies with stronger\nguarantees on the probability of catastrophic losses. These ob-\njectives are important in critical domains, such as healthcare\nor finance, where one must avoid catastrophic failures.\nIn a departure from much of recent literature, we target\nthe total reward criterion (TRC) (Kallenberg 2021; Puter-\nman 2005) instead of the common discounted criterion. TRC,\nwhich generalizes the stochastic shortest path problem, as-\nsumes an infinite horizon but does not discount future rewards.\nTo control for infinite returns, the TRC assumes a positive\nprobability that the process terminates.\nTwo essential reasons motivate our departure from dis-\ncounted objectives in risk-averse MDPs. First, computing\ndiscounted risk-averse policies is difficult. Reinforcement\nlearning algorithms typically use discounted objectives be-\ncause they admit optimal stationary policies and value func-\ntions that can be computed using dynamic programs. In con-\ntrast, most risk measures, such as VaR or CVaR, require\nthat optimal policies are history-dependent (B\u00e4uerle and Ott\n2011; Hau et al. 2023). The simplest known risk-averse op-\ntimal policies are Markov (time-dependent) policies for ob-\njectives defined using Entropic Value-at-Risk (EVaR) and\nEntropic Risk Measure (ERM) risk measures (Hau, Petrik,\nand Ghavamzadeh 2023).\nSecond, TRC captures the concept of stochastic termina-\ntion, which is common in reinforcement learning (Sutton and\nBarto 2018). In risk-neutral objectives, discounting can serve\nwell to model the probability of termination because it guar-\nantees the same optimal policies (Puterman 2005). However,\nas we show in this work, no such correspondence exists with\nrisk-averse objectives, and the difference between them may\nbe arbitrarily significant. Modeling stochastic termination us-\ning a discount factor in risk-averse objectives is inappropriate\nand leads to dramatically different optimal policies.\nAs our main contribution, we show that the risk-averse\nTRC with ERM and EVaR risk measures admit optimal sta-\ntionary policies and optimal value functions in transient\nMDPs. Transient MDPs must terminate with a positive proba-\nbility in a sufficiently large number of steps and are a common\nmodel used to guarantee that TRC returns are bounded. We\nalso show that the optimal value function satisfies dynamic\nprogramming equations and show how to compute it using\nvalue iteration, policy iteration, or linear programming algo-\nrithms. These algorithms are simple and closely resemble the\nalgorithms for solving MDPs. In summary, our results show\nthat the risk-averse TRC criterion is easier to optimize than\nrisk-averse discounted objectives.\nOur results also indicate that EVaR is a particularly inter-\nesting risk measure in reinforcement learning. ERM and the\nclosely related exponential utility functions have been popu-\nlar in sequential decision-making problems because they ad-\nmit dynamic programming decompositions (Patek and Bert-\nsekas 1999; de Freitas, Freire, and Delgado 2020; Smith and\nChapman 2023; Denardo and Rothblum 1979; Hau, Petrik,\nand Ghavamzadeh 2023; Hau et al. 2023). Unfortunately,\nERM is difficult to interpret, scale-dependent, and incompara-\nble with popular risk measures like VaR and CVaR. Because\nEVaR reduces to an optimization over ERM, it preserves most\nof the computational advantages of ERM. Because EVaR\nclosely approximates CVaR and VaR at the same risk level,\nits value is also much easier to interpret. Finally, EVaR is also"}, {"title": "Background on risk-averse MDPs", "content": "Markov Decision Processes We focus on solving Markov\ndecision processes (MDPs) (Puterman 2005; Su and Petrik\n2023), modeled by a tuple $(S, A, p, r,\\mu)$, where $S$\n= {1, 2, ..., S, S + 1} is the finite set of states and $A$\n= {1, 2, ..., A} is the finite set of actions. The transition func-\ntion $p : S \\times A \\rightarrow \\Delta$ represents the probability $p(s, a, s')$\nof transitioning to $s' \\in S$ after taking $a \\in A$ in $s \\in S$ and\n$p_{sa} \\in \\Delta$ is such that $(p_{sa})_{s'} = p(s, a, s')$. The function\n$r: S \\times A \\times S \\rightarrow R$ represents the reward $(s, a, s') \\in R$\nassociated with transitioning from $s \\in S$ and $a \\in A$ to $s' \\in S$.\nThe vector $\\mu \\in \\Delta^S$ is the initial state distribution.\nWe designate the state $e := S + 1$ as a sink state and use\n$S = {1, ..., S}$ to denote the set of all non-sink states. The\nsink state $e$ must satisfy that $p(e, a, e) = 1$ and $r(e, a, e) = 0$\nfor each $a \\in A$, and $\\mu_e = 0$. Throughout the paper, we\nuse a bar to indicate whether the quantity involves the sink\nstate $e$. Note that the sink state can indicate a goal when all\nrewards are negative and an undesirable terminal state when\nall rewards are positive.\nThe following technical assumption is needed to simplify\nthe derivation. To lift the assumption, one needs to carefully\naccount for infinite values, which adds complexity to the\nresults and distracts from the main ideas.\nAssumption 2.1. The initial distribution \u03bc satisfies that\n$\\bar{\\mu} > 0$.\nThe solution to an MDP is a policy. Given a horizon $t \\in N$,\na history-dependent policy in the set $IIHR$ maps the his-\ntory of states and actions to a distribution over actions. A\nMarkov policy $\\pi \\in IMR$ is a sequence of decision rules\n$\\pi = (d_0, d_1,..., d_{t-1})$ with $d_k : S \\rightarrow \\Delta^A$ the decision rule\nfor taking actions at time k. The set of all randomized de-\ncision rules is $D = (\\Delta^A)$. Stationary policies $ISR$ are of\nMarkov policies with $\\pi := (d)^\\infty := (d, d,...)$ with the\nidentical decision rule in every timestep. We treat decision\nrules and stationary policies interchangeably. The sets of de-\nterministic Markov and stationary policies are denoted by\nIMD and ISD. Finally, we omit the superscript $t$ to indicate\ninfinite horizon definitions of policies.\nThe risk-neutral Total Reward Criterion (TRC) objective\nis:\n$\\sup_{\\Pi \\in \\Pi^{HR}} \\liminf_{t\\rightarrow\\infty} E^{\\pi,\\mu} [\\sum_{k=0}^{t} r(\\tilde{s}_k, \\tilde{a}_k, \\tilde{s}_{k+1}); \\tilde{s}_{k+1})]$,\nwhere the random variables are denoted by a tilde and $s_t$\nand $a_t$ represent the state from $S$ and action at time $t$. The\nsuperscript $\\pi$ denotes the policy that governs the actions\n$a_t$ when visiting $s_t$ and $\\mu$ denotes the initial distribution.\nFinally, note that $\\liminf$ gives a conservative estimate of a\npolicy's return since the limit does not necessarily exist for\nnon-stationary policies.\nUnlike the discounted criterion, the TRC may be un-\nbounded, optimal policies may not exist, or may be non-\nstationary (Bertsekas and Yu 2013; James and Collins 2006).\nWe assume that all policies have a positive probability of\neventually transitioning to the sink state."}, {"title": "Monetary risk measures", "content": "Monetary risk measures aim to\ngeneralize the expectation operator to account for the spread\nof the random variable. Entropic risk measure (ERM) is a\npopular risk measure, defined for any risk level \u03b2 > 0 and\n$\\tilde{x} \\in X$ as (Follmer and Schied 2016)\n$ERM_{\\beta} [\\tilde{x}] = -\\beta^{-1}.log E[exp(-\\beta \\cdot \\tilde{x})]$.\nand extended to $\\beta = 0$ as $ERM_0[x] = E[x]$ and $ERM_{\\infty} [x]$\n= ess inf[$x$]. ERM plays a special\nrole in sequential decision-making because it is the only\nlaw-invariant risk measure that satisfies the tower prop-\nerty (Kupper and Schachermayer 2006; Marthe, Garivier,\nand Vernade 2023), which is essential in constructing\ndynamic programs (Hau, Petrik, and Ghavamzadeh 2023).\nUnfortunately, two major limitations of ERM hinder its\npractical applications. First, it is not positively homogenous\nand, therefore, the risk value depends on the scale of the\nrewards, and ERM is not coherent (Follmer and Schied 2016;\nHau, Petrik, and Ghavamzadeh 2023; Ahmadi-Javid 2012).\nSecond, the risk parameter \u03b2 is difficult to interpret and does\nnot relate well to other common risk measures, like VaR or\nCVaR.\nFor these reasons, we focus on the Entropic Value at\nRisk (EVaR), defined as, for a given \u03b1 \u2208 (0, 1),\n$EVaR_{\\alpha} [\\tilde{x}] = \\sup_{\\beta>0} -\\beta^{-1} log (\\alpha^{-1}E[exp(-\\beta \\tilde{x})])$,\n= $\\sup_{\\beta>0} ERM_{\\beta} [\\tilde{x}] + \\beta^{-1} log\\alpha$,\nand is extended to EVaR\u2080 [x] = ess inf [$\\tilde{x}$] and EVaR\u2081 [x] =\nE [$\\tilde{x}$] (Ahmadi-Javid 2012). It is important to note that the\nsupremum in (4) may not be attained even when $\\tilde{x}$ is a finite\ndiscrete random variable (Ahmadi-Javid and Pichler 2017).\nEVaR addresses the limitations of ERM while preserving\nits benefits. EVaR is coherent and positively homogenous.\nEVaR is also a good approximation to interpretable quantile-\nbased risk measures, like VaR and CVaR (Ahmadi-Javid\n2012; Hau, Petrik, and Ghavamzadeh 2023).\nRisk-averse MDPs. Risk-averse MDPs with VaR and\nCVaR combined with discounted objectives have received\nabundant attention (Hau et al. 2023; B\u00e4uerle and Ott 2011;\nB\u00e4uerle and Glauner 2022; Pflug and Pichler 2016; Li, Zhong,\nand Brandeau 2022), showing that these objectives require\nhistory-dependent optimal policies. Nested risk measures\nwith the TRC criterion may admit stationary policies that\ncan be computed using dynamic programming (Ahmadi et al.\n2021a; Meggendorfer 2022; de Freitas, Freire, and Delgado\n2020; Gavriel, Hanasusanto, and Kuhn 2012). However, the\nTRC with nested CVaR can be unbounded, as shown in\nProposition C.1, which is in contradiction to some existing\nresults from the literature, such as theorem 1 in (Ahmadi et al.\n2021a). Recent works have shown that optimal policies for\ndiscounted objective with EVaR risk measure can be chosen\nto be Markov and can be computed via dynamic program-\nming (Hau, Petrik, and Ghavamzadeh 2023), building upon\nsimilar results established for ERM (Chung and Sobel 1987).\nHowever, in ERM-TRC, the value functions may also be\nunbounded, as shown in Proposition D.1."}, {"title": "Solving ERM Total Reward Criterion", "content": "This section shows that an optimal stationary policy exists\nfor ERM-TRC and that the value function satisfies dynamic\nprogramming equations. We then outline algorithms for com-\nputing it.\nOur objective in this section is to maximize the ERM of the\ninfinite-horizon total sum of rewards for some given \u03b2 > 0:\n$\\sup_{\\Pi \\in \\Pi^{HR}} \\liminf_{t\\rightarrow\\infty} ERM^{\\pi,\\mu} [\\sum_{k=0}^{t} r(\\tilde{s}_k, \\tilde{a}_k, \\tilde{s}_{k+1})]$,\nAnalogously to (1), the superscripts indicate the policy and\ninitial state distribution that govern $\\tilde{s}_t$ and $\\tilde{a}_t$. The definition"}, {"title": "Finite Horizon", "content": "We commence the analysis with definitions and basic proper-"}]}