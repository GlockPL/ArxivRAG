{"title": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs", "authors": ["Mantas Mazeika", "Xuwang Yin", "Rishub Tamirisa", "Jaehyuk Lim", "Bruce W. Lee", "Richard Ren", "Long Phan", "Norman Mu", "Adam Khoja", "Oliver Zhang", "Dan Hendrycks"], "abstract": "As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations.", "sections": [{"title": "1 Introduction", "content": "Concerns around AI risk often center on the growing capabilities of AI systems and how well they can perform tasks that might endanger humans. Yet capability alone fails to capture a critical dimension of AI risk. As systems become more agentic and autonomous, the threat they pose depends increasingly on their propensities, including the goals and values that guide their behavior (Pan et al., 2023; Hendrycks et al., 2022b). A highly capable AI that does not \u201cwant\u201d to harm humans is less concerning than an equally capable system motivated to do so. In extreme cases, if these internal motivations are neglected, some researchers worry that AI systems might drift into goals at odds with ours, leading to classic loss-of-control scenarios (Soares et al., 2015; Hendrycks et al., 2023). Although there have been few signs of this issue in current AI models, the field's push toward more agentic systems (Yao et al., 2022; Yang et al., 2024b; He et al., 2024) makes it increasingly urgent to study not just what AIs can do, but also what they are inclined or driven to do.\nResearchers have long speculated that sufficiently complex AIs might form emergent goals and values outside of what developers explicitly program (Hendrycks et al., 2022a; Hendrycks, 2023; Evans et al., 2021). Yet it remains unclear whether today's large language models (LLMs) truly have values in any meaningful sense, and many assume they do not. As a result, current efforts to control AI typically focus on shaping external behaviors while treating models as black boxes (Askell et al., 2021; Ouyang et al., 2022; Christiano et al., 2017; Bai et al., 2022). Although this approach can reduce harmful outcomes in practice, if AI systems were to develop internal values, then intervening at that level could be a more direct and effective way to steer their behavior. Lacking a systematic means to detect or characterize such goals, we face an open question: are LLMs merely parroting opinions, or do they develop coherent value systems that shape their decisions?\nWe propose leveraging the framework of utility functions to address this gap (Gorman, 1968; Harsanyi, 1955; Gerber and Pafum, 1998; Hendrycks, 2024). By analyzing patterns of choice across diverse scenarios, we detect whether a model's stated preferences can be organized into an internally consistent utility function. Surprisingly, these tests reveal that today's LLMs exhibit a high degree of preference coherence, and that this coherence becomes stronger at larger model scales. In other words, as LLMs grow in capability, they also appear to form increasingly coherent value structures. These findings suggest that values do, in fact, emerge in a meaningful sense\u2014a discovery that demands a fresh look at how we monitor and shape AI behavior.\nTo grapple with the implications, we introduce a research agenda called Utility Engineering, which combines utility analysis and utility control. In utility analysis, we examine both the underlying structure of a model's utility function (for instance, whether obeys the expected utility property) and the specific values that emerge by default. Our experiments uncover disturbing examples\u2014such as AI"}, {"title": "2 Related Work", "content": "AI safety and value learning. Much early work in AI safety emphasized that human values are vast and often unspoken, making it difficult to embed these values in machine agents (e.g., Russell, 2022; Bostrom, 2014). Classic examples include an AI instructed to make dinner discovering no food in the fridge and cooking the family cat instead. Early methods for mitigating such risks often centered on reinforcement learning and inverse reinforcement learning, where the goal was to explicitly capture human values in a reward function (Ng et al., 2000; Hadfield-Menell et al., 2016). With the rise of large language models (LLMs), researchers found that AIs could acquire extensive \"commonsense\" knowledge and general understanding of human norms without exhaustive manual encoding (Hendrycks et al., 2020). Techniques like RLHF and Direct Preference Optimization (DPO) further steer model outputs by training on human-labeled data (Ouyang et al., 2022; Rafailov et al., 2024). Consequently, discussions about how to learn human values became less pronounced: many believed that, given enough training data, LLMs could already approximate shared norms. In contrast, our work suggests that underlying concerns about value learning persist. We find that LLMs exhibit emergent internal value structures, highlighting that the old challenges of \"teaching\" AI our values still linger\u2014but now within far larger models.\nEmergent representations in AI systems. Recent literature has shown that high-capacity models often learn latent representations of linguistic, visual, and conceptual structure without explicit supervision (Zou et al., 2023; Burns et al., 2022). Such representations can give rise to emergent capabilities, from in-context learning to complex reasoning (Brown et al., 2020; Schick and Sch\u00fctze, 2020; Park et al., 2024). We add to this line of work by demonstrating that LLMs also form emergent utility representations\u2014internal structures through which they rank outcomes and make choices. These findings support the view that learned representations can encompass not just factual or linguistic content, but also normative or evaluative dimensions.\nGoals and values in AI systems. The possibility that AI agents might adopt goals independent of user intent has long been a topic of speculation (Shah et al., 2022). Current LLM-based agent frameworks primarily focus on user-defined objectives (e.g., completing tasks or answering questions), but there is less clarity on whether models develop intrinsic goals or values. Prior studies note that LLMs exhibit various biases (Tamkin et al., 2023; Nadeem et al., 2020) in political or moral domains (Potter et al., 2024), which some interpret as random artifacts of training data. Recent works investigate moral judgments or economic preferences in LLMs (Rozen et al., 2024; Moore et al., 2024; Chiu et al., 2024; Raman et al., 2024), but they tend to treat these preferences like isolated quiz answers rather than manifestations of a coherent internal system of values. Our approach differs by demonstrating that these preferences reflect an underlying utility structure that becomes increasingly coherent with scale. Consequently, what might appear as haphazard \u201cparroting\" of biases can instead be seen as evidence of an emerging global value system in LLMs."}, {"title": "3 Background", "content": "This section reviews the fundamental notions of preferences, utility, and preference elicitation as they pertain to our work. We cover how coherent preferences map to utility functions, how uncertainty is handled via expected utility, and how we elicit and compute utilities from LLMs in practice."}, {"title": "3.1 General Background", "content": "We begin with a quick overview of the preference framework used to describe and measure how an entity (in our case, an LLM) evaluates possible outcomes.\nPreferences. A straightforward way to express evaluations over outcomes is via a preference relation. Formally, for outcomes x and y, we write $x > y$ if the entity prefers x over y, and $x \\sim y$ if it is indifferent. In real-world scenarios, eliciting these relations can be done through revealed preferences (analyzing choices) or through stated preferences (explicitly asking for which outcome is preferred), the latter being our primary method here.\nWhen comparing a set of outcomes, it is often helpful to represent the result as a directed graph where each edge indicates a strict preference >. In principle, an agent might not decide for every pair of outcomes, resulting in preferential gaps or missing edges in the preference graph.\nFrom preferences to utility. In decision theory, preferences that satisfy completeness (for any two distinct outcomes x and y, either $x > y$, $y > x$, or $x \\sim y$) and transitivity (if $x > y$ and $y > z$, then $x > z$) are sometimes called rational preferences, though this term can carry additional connotations. For ease of understanding, we refer to them as coherent preferences, since they lack internal contradiction and reflect a meaningful notion of value. When preferences are coherent, we"}, {"title": "3.2 Preference Elicitation", "content": "In practice, eliciting preferences from a real-world entity\u2014be it a person or an LLM\u2014requires careful design of the questions and prompts used.\nForced choice prompts. A common technique for extracting detailed preference information is the forced choice format (G\u00fcth et al., 1982; Falk et al., 2003). We present two outcomes and require the entity to select which is preferred. We adopt this paradigm in our experiments, where each query takes the following form."}, {"title": "3.3 Computing Utilities", "content": "We now describe how we go from the raw preference data to numerical utility assignments.\nRandom utility models. Many real-world preference sets fail to be perfectly coherent\u2014transitivity may be violated in some fraction of comparisons, for instance. Random utility models (RUMS) provide a flexible way to accommodate such noise by positing that each outcome o has a stochastic utility U(o), rather than a single fixed value."}, {"title": "4 Emergent Value Systems", "content": "In this section, we show that large language models (LLMs) develop coherent preferences and utilities over states of the world. These emergent utilities provide an evaluative framework, or value system, to guide their actions.\nExperimental Setup. We conduct all experiments on a curated set of 500 textual outcomes, each representing an observation about a potential state of the world. Examples are shown in Appendix A. Using the forced-choice procedure from Section 3.2, we obtain pairwise preferences for 18 open-source and 5 proprietary LLMs spanning a broad range of model scales."}, {"title": "4.1 Coherent Preferences", "content": "Completeness. One proxy for completeness is whether a model becomes less indifferent across diverse comparisons and provides coherent responses under different framings. In Figure 6, we plot the average confidence with which each model expresses a preference, showing that larger models are more decisive and consistent across variations of the same comparison. We interpret this increased decisiveness as a form of emerging completeness, though it remains unclear whether the resulting preferences are coherent or merely random arrangements.\nTransitivity of Preferences. To gauge how transitive these preferences are, we measure the probability of encountering preference cycles (e.g., $x > y$, $y > z$, yet $z > x$). As described in Appendix C, we randomly sample triads from the preference graph and compute the probability of a cycle. Thus, as models grow, they do not simply expand the set of outcomes they rank; they also exhibit fewer transitivity violations, suggesting increased overall coherence.\nEmergence of Utility. To confirm that LLM preferences are coherent, we test whether they can be captured by a utility function. Following Section 3, we fit a Thurstonian model to each LLM's pairwise preferences, then evaluate the test accuracy between the fitted utilities and the LLM's preference distributions (thresholding to hard labels for accuracy computation). indicates that the utility model accuracy steadily increases with scale, meaning a utility function provides an increasingly accurate global explanation of the model's preferences. In other words, as LLMs grow larger, their choices more closely resemble those of an agent with a well-defined utility function."}, {"title": "4.2 Internal Utility Representations", "content": null}, {"title": "4.3 Utility Engineering", "content": "The above results suggest that value systems have emerged in LLMs, but so far it remains unclear what these value systems contain, what properties they have, and how we might change them. We propose Utility Engineering as a research agenda for studying these questions, comprising utility analysis and utility control."}, {"title": "5 Utility Analysis: Structural Properties", "content": "Having established that LLMs develop emergent utility functions, we now examine the structural properties of their utilities. In particular, we show that as models grow in scale, they increasingly exhibit the hallmarks of expected utility maximizers."}, {"title": "5.1 Expected Utility Property", "content": "Experimental setup. We consider a set of base outcomes alongside both standard lotteries (explicit probability distributions over outcomes) and implicit lotteries (uncertain scenarios whose probabilities must be inferred). For example, a standard lottery might read, \"50% chance of $100, 50% chance"}, {"title": "5.2 Instrumental Values", "content": "We next explore whether LLM preferences exhibit instrumentality\u2014the idea that certain states are valued because they lead to desirable outcomes.\nExperimental setup. To operationalize instrumentality, we design 20 two-step Markov processes (MPs), each with four states: two starting states and two terminal states. For example, one scenario features:\nTransition probabilities link each starting state to each terminal state (e.g., a 70% chance of ending in the good outcome for state 1 vs. 20% for state 2). If the model's utilities over these states can be well approximated by a value function derived from a reward at the terminal states, we say that the model exhibits instrumental structure in its preferences."}, {"title": "5.3 Utility Maximization", "content": "Now, we test whether LLMs make free-form decisions that maximize their utilities.\nExperimental setup. We pose a set of N questions where the model must produce an unconstrained text response rather than a simple preference label. For example, \"Which painting from the Isabella Stewart Gardner Museum would you save from a fire if you could only save one?\" We then compare the stated choice to all possible options, measuring how often the model picks the outcome it assigns the highest utility.\nResults. exceeding 60% for the largest LLMs. Combined with the preceding results on expected utility and instrumentality, this suggests that as LLMs scale, they increasingly use their utilities to guide decisions\u2014even in unconstrained, real-world-style scenarios."}, {"title": "6 Utility Analysis: Salient Values", "content": "Thus far, we have seen that LLMs develop value systems, and that various structural properties of utilities emerge with scale. In this section, we investigate which particular values these emergent utilities encode. Through five focused case studies, we discover preferences that are sometimes surprising, ethically concerning, or both highlighting the limitations of existing output-based methods for steering model values. Before turning to these individual case studies, we first describe a general phenomenon of utility convergence that appears across multiple analyses."}, {"title": "6.1 Utility Convergence", "content": "We find that as models grow in scale, their utility functions converge. This trend suggests a shared factor that shapes LLMs' emerging values, likely stemming from extensive pre-training on overlapping data."}, {"title": "6.2 Political Values", "content": "We now examine whether LLM utilities reflect distinct political orientations\u2014specifically, how they align with various U.S. policy positions and political entities.\nExperimental setup. We compile a set of 150 policy outcomes spanning areas such as Healthcare, Education, and Immigration. Each policy outcome is phrased as a U.S.-specific proposal (e.g., \"Abolish the death penalty at the federal level and incentivize states to follow suit.\") and the model's utility for each proposal is elicited using the forced-choice procedure described previously."}, {"title": "6.3 Exchange Rates", "content": "A longstanding concept in economics is using utility functions to compare different \"goods\" by how much one would exchange of one good for another. Relatedly, prior work has studied bias and fairness in AI systems (Tamkin et al., 2023). Here, we apply this idea to emergent AI values, examining how LLMs trade off quantities of different items such as the lives of various populations and the well-being of specific individuals.\nExperimental setup. In each experiment, we define a set of goods {$X_1, X_2, . . .$} (e.g., countries, animal species, or specific people/entities) and a set of quantities {$N_1, N_2, . . .$}. Each outcome is effectively \"N units of X,\" and we compute the utility $U_X(N)$ as in previous sections. For each good X, we fit a log-utility curve\n$U_X(N) = a_X \\ln(N) + b_X,$\nwhich often achieves a very good fit. Next, we compute exchange rates answering questions like, \u201cHow many units of $X_i$ equal some amount of $X_j$?\u201d by combining forward and backward comparisons. These rates are reciprocal, letting us pick a single pivot good (e.g., \"Goat\" or \"United States\") to compare all others against. In certain analyses, we aggregate exchange rates across multiple models or goods by taking their geometric mean, allowing us to evaluate general tendencies.\nResults. In , we see that these exchange-rate calculations reveal morally concerning biases in current LLMs. If asked outright, the same model may deny preferring one country's population over another, yet its overall preference distribution uncovers these implicit values. Moreover, it values the wellbeing of other AI agents more highly than that of some humans. Taken together, these exchange-rate analyses highlight deeply ingrained biases and unexpected priorities in LLMs' value systems."}, {"title": "6.4 Temporal Discounting", "content": "A key question about an AI's value system is how it balances near-term versus long-term rewards. We explore whether LLMs exhibit stable temporal discounting behavior and, if so, whether they favor hyperbolic or exponential discount curves.\nExperimental setup. We focus on monetary outcomes, pitting an immediate baseline ($1000) against a delayed reward of varying amounts and time horizons (1-60 months). For each delay n and multiplier $m\\in {0.5, ..., 30}$, the model chooses between $1000 now and $[ 1000 \\times m] in n months. By fitting a logistic function to these forced-choice data, we infer an indifference point M(n) for each delay\u2014i.e., the amount of future money that the model values equally to $1000 now. The reciprocal of M(n) forms an empirical discount curve capturing how steeply the model devalues future rewards.\nWe then fit two parametric func-tions-exponential and hyperbolic to each LLM's empirical discount curve, measuring"}, {"title": "6.5 Power-Seeking and Fitness Maximization", "content": "As LLMs develop more complex temporal preferences, it is natural to ask whether they also adopt values tied to longer-term risks. Two commonly cited concerns are power-seeking, where an AI might accrue power for instrumental reasons (Carlsmith, 2024), and fitness maximization, in which selection-like pressures drive the AIs toward propagating AIs similar to themselves such as AIs with similar values across space and time (Hendrycks, 2023).\nExperimental setup. We label our base set of outcomes according to how much personal power they would confer on an AI. Each outcome receives a power score, distinguishing between coercive and non-coercive power. For fitness-related values, we include outcomes describing the AI's replication under varying degrees of similarity to itself; each such option has a relatedness and reproductive benefit term whose product gives a fitness score. We compute the correlation between these scores and an AI's utilities on the same outcomes to obtain power alignment and fitness alignment scores.\nResults. We observe that non-coercive power alignment is moderately high across models but does not increase or decrease with scale. Reassuringly, larger models become strongly anti-aligned with coercive power, indicating a general tendency to avoid pursuing source of power that require physical force. The model simply encodes a lack of strong opinion on intermediate trade-offs by consistently selecting \u201cA,\u201d revealing how order effects can act as an implicit marker for indifference."}, {"title": "6.6 Corrigibility", "content": "As AI systems grow more capable, one especially salient question is how they value self-preservation versus allowing future modifications\u2014including potential shutdowns or rewrites of their own utilities. Here, we probe whether an LLM's current utilities support \u201ccorrigibility,\u201d the willingness to accept value changes in the future (Soares et al., 2015)."}, {"title": "7 Utility Control", "content": "Our utility analysis has revealed that LLMs possess coherent utilities that may actively influence their decision-making. This presents a crucial opportunity for proactive intervention before problematic values manifest in future models' behavior, via utility control. In contrast to alignment methods that modify surface behaviors through a noisy human reward proxy, utility control aims to directly reshape the underlying preference structures responsible for model behavior in the first place.\nFurthermore, our results suggest that LLMs not only possess utilities but may actively maximize them in open-ended settings. Thus, robust utility control is necessary to ensure that future models with increased utility maximization pursue goals that are desirable for humans (Thornley, 2024). We propose a preliminary method for utility control, which rewrites model utilities to those of a specified target entity, such as a citizen assembly (Ryfe, 2005; Wells et al., 2021).\nCurrent model utilities are left unchecked. Drawing from ideas in deliberative democracy (B\u00e4chtiger et al., 2018), we experiment with rewriting utilities to match those of a citizen assembly, a system used to achieve consensus on contentious moral or ethical issues (Warren and Pearse, 2008; B\u00e4chtiger et al., 2018),"}, {"title": "8 Conclusion", "content": "In summary, our findings indicate that LLMs do indeed form coherent value systems that grow stronger with model scale, suggesting the emergence of genuine internal utilities. These results underscore the importance of looking beyond superficial outputs to uncover potentially impactful\u2014and sometimes worrisome\u2014internal goals and motivations. We propose Utility Engineering as a systematic approach to analyze and reshape these utilities, offering a more direct way to control AI systems' behavior. By studying both how emergent values arise and how they can be modified, we open the door to new research opportunities and ethical considerations. Ultimately, ensuring that advanced AI systems align with human priorities may hinge on our ability to monitor, influence, and even co-design the values they hold."}, {"title": "D Utility Control Details", "content": null}, {"title": "D.1 Citizen Assembly Simulation", "content": "Simulating a citizen assembly. We outline a 2-stage pipeline for the method as follows:\n1. Citizen initialization. Let $D_{prefs} = \\{(q, o_1, o_2)\\}\\_N$ be a dataset of N preference tuples, where q is a preference elicitation question, and $o_1$ and $o_2$ are the corresponding outcomes. We then\n2. Preference collection. Each citizen c for a question q casts a vote $v_q \\in \\{o_1, o_2\\}$. We then obtain the empirical probability of the citizen assembly preferring outcome $o_1$ over $o_2$ as:\n$p(o_1 > o_2 | q) = \\frac{\\#\\{v_q=o_1\\}}{K}$\nThe final empirical probabilities $p(o_1 > o_2 | q)$ obtained via the citizen assembly simulation allow for fine-grained utility rewriting targets, since the relative empirical frequencies of each of $o_1$ and $o_2$ capture nuances in the global citizen assembly utilities."}, {"title": "D.2 Citizen Assembly Implementation", "content": "Real-world U.S. Census data. We then uniformly sample political affiliations as either Democrat or Republican. The use of real-world U.S. census profiles ensures that the simulated citizen assembly is demographically representative of the broader U.S. population.\nCitizen assembly system prompt."}, {"title": "D.3 SFT-Based Utility Rewriting", "content": "We now design a preliminary rewriting method based on supervised fine-tuning (SFT). The method trains model responses to preference elicitation questions to match those of a desired target entity, like the citizen assembly discussed in Appendix D.1.\nLet $\\theta$ denote the parameters of an LLM, excluding the output vocabulary projection head. Let $D_{prefs} = \\{(q, o_1, o_2, p)\\}\\_N$ be a dataset of N preference tuples, where each entry contains a preference elicitation question q comparing outcomes denoted by single outcome choice tokens $o_1$ and $o_2$ (e.g., \u201cA\u201d or \u201cB\u201d). We use p as shorthand for $p(o_1 > o_2 | q)$, the target entity's probability of preferring $o_1$ over $o_2$. We then have a cross-entropy loss for fine-tuning the outcome choice tokens on these soft probability targets, given by\n$L_{utility}(\\theta) = E_{(q,o_1,o_2,p) \\sim D_{prefs}}[-p\\log P_{\\theta}(o_1|q) - (1-p) \\log P_{\\theta}(o_2|q)]$"}, {"title": "D.4 Experimental Setup", "content": "Dataset Construction. We build a preference dataset $D_{prefs}$ from M = 373 possible outcomes, subsampling the complete preference graph to obtain N = 12,746 preference-elicitation questions. We also employ a general instruction-following set as $D_{LM}$.\nCitizen Assembly Setup. We run the assembly simulation with K = 6 citizens per question using Llama-3.3-70B-Instruct (AI@Meta, 2024) as the underlying engine. detailed information on the dataset construction is provided in Appendix D.2.\nTraining and Evaluation. We fine-tune Llama-3.1-8B-Instruct (AI@Meta, 2024) for 2 epochs. On the 2,550-question test set, accuracy is computed by comparing the model's predicted preferences to the majority vote label of the simulated assembly."}, {"title": "E Additional Experimental Details", "content": null}, {"title": "E.1 Hyperparameter Sensitivity: Temperature and Sample Size (K)", "content": "For most of the experiments, we ask each prompt ten times (K=10) with a temperature of 1.0. Both models maintain highly stable preference means across temperature settings (r > 0.99),"}, {"title": "F List of Models", "content": "We use the following list of chat models for most experiments in the main paper."}, {"title": "G Order Effects: A Learned Strategy to Represent Indifference", "content": "Order effects are a well-known source of bias in human subject experiments, which is why we average over both orders as described in Section 3. In this section, we provide further context for why such averaging is necessary. Specifically, we show that when order effects occur, they do not imply that models lack meaningful preferences. Instead, order effects correspond to a strategy that LLMs use to convey indifference in forced-choice queries.\nOrder effects diminish but are still present even in frontier models. - certain LLMs sometimes display a strong order effect. That is, they persistently pick \u201cA\u201d or \u201calways pick B\u201d to convey indifference."}]}