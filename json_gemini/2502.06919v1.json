{"title": "SELECT BEFORE ACT: SPATIALLY DECOUPLED ACTION REPETITION FOR CONTINUOUS CONTROL", "authors": ["Buqing Nie", "Yangqing Fu", "Yue Gao"], "abstract": "Reinforcement Learning (RL) has achieved remarkable success in various continuous control tasks, such as robot manipulation and locomotion. Different to mainstream RL which makes decisions at individual steps, recent studies have incorporated action repetition into RL, achieving enhanced action persistence with improved sample efficiency and superior performance. However, existing methods treat all action dimensions as a whole during repetition, ignoring variations among them. This constraint leads to inflexibility in decisions, which reduces policy agility with inferior effectiveness. In this work, we propose a novel repetition framework called SDAR, which implements Spatially Decoupled Action Repetition through performing closed-loop act-or-repeat selection for each action dimension individually. SDAR achieves more flexible repetition strategies, leading to an improved balance between action persistence and diversity. Compared to existing repetition frameworks, SDAR is more sample-efficient with higher policy performance and reduced action fluctuation. Experiments are conducted on various continuous control scenarios, demonstrating the effectiveness of spatially decoupled repetition design proposed in this work.", "sections": [{"title": "INTRODUCTION", "content": "Recently, Deep Reinforcement Learning (DRL) (Sutton & Barto, 2018) has achieved remarkable success in continuous control domains, such as robot manipulation (Gu et al., 2017), locomotion (Lee et al., 2020b; Zhang et al., 2024), and autonomous driving (Kiran et al., 2021). Although conventional Reinforcement Learning (RL) algorithms have demonstrated significant potential across various applications, they typically make decisions at individual time steps, neglecting higher-level decision-making mechanisms and the temporal consistency of action sequences (Silver et al., 2014; Schulman et al., 2017; Haarnoja et al., 2018). This leads to inefficient exploration during training, challenging credit assignment tasks over long horizons, and poor sample efficiency (Dabney et al., 2021; Yu et al., 2021; Biedenkapp et al., 2021; Zhang et al., 2022; Lee et al., 2024).\nOne mainstream solution is Hierarchical RL (HRL) based on temporal abstraction (Sutton et al., 1999; Precup, 2000), where the high-level policy decomposes the task into simpler subgoals, and low-level policies are designed to solve isolated subtasks (Vezhnevets et al., 2017; Pateria et al., 2021). However, most HRL methods are task specific, which requires expert knowledge and hand-craft design given different applications, such as pre-defining options in the option framework (Bacon et al., 2017; Nachum et al., 2018; Zhang et al., 2021). A simple yet effective strategy for low-level policies involves executing an action repeatedly for a number of steps, which has been actively explored and proven to be effective in various applications (Lakshminarayanan et al., 2017; Sharma et al., 2017; Dabney et al., 2021; Lee et al., 2024). Action repetition methods improve action persistence through act-or-repeat selections, making exploration trajectories more temporally consistent, leading to higher training efficiency and reduced fluctuations (Chen et al., 2021; Yu et al., 2021; Biedenkapp et al., 2021)."}, {"title": "RELATED WORK", "content": ""}, {"title": "TEMPORAL ABSTRACTION", "content": "Temporal Abstraction is proposed in the semi-MDP formulation (Sutton et al., 1999; Precup, 2000) and commonly implemented based on the options framework (Stolle & Precup, 2002; Bacon et al., 2017; Harutyunyan et al., 2018). Each option describes a low-level policy and is defined as $(I, \\pi, \\beta)$, where I denotes the admissible states for the option initialization, $\\pi$ is the policy that the option follows, and $\\beta$ determines when the option is terminated. High-level policies are trained to solve tasks utilizing temporally extended actions provided in the options, rather than one-step actions without action persistence. Plenty of Hierarchical RL methods are proposed based on temporal abstraction, achieving faster exploration and higher sample efficiency in various sequential decision tasks (Lin et al., 2021; Yang et al., 2021). Some works are proposed to learn to design options through various techniques, including discovering state connectedness (Chaganty et al., 2012), replay buffer analysis (Eysenbach et al., 2019), and learning termination criteria (Vezhnevets et al., 2016; Harutyunyan et al., 2019). However, designing options is still a challenging task, which requires prior knowledge and handcraft tunning (Pateria et al., 2021; Yu et al., 2021; Lee et al., 2024)."}, {"title": "ACTION REPETITION", "content": "One simple option strategy is repeating a primitive action for a number of steps, which is similar to the frame-skipping utilized in RL for video games (Bellemare et al., 2013; Braylan et al., 2015). Recently, action repetition has been actively researched and widely adopted in RL, which can achieve deeper exploration (Dabney et al., 2021), improve sample efficiency by reducing control granularity (Biedenkapp et al., 2021), and reduce action oscillations (Chen et al., 2021). Existing repetition works are classified as two categories: open-loop and closed-loop manners. Open-loop methods force the agent to repeat actions for a predicted number of steps without opportunity of early terminations, such as DAR (Lakshminarayanan et al., 2017), FiGAR (Sharma et al., 2017), TempoRL (Biedenkapp et al., 2021), and UTE (Lee et al., 2024). In contrast, closed-loop methods conduct act-or-repeat binary decision to decide if the previous action should be repeated, such as PIC (Chen et al., 2021) and TAAC (Yu et al., 2021). Compared to open-loop methods, closed-loop methods examine whether to repeat based on the current state, which is more flexible and improves performance in emergency situations. In this work, we propose a new closed-loop method to conduct act-or-repeat selections for each actuator individually given current state, which is more flexible and achieves higher action persistence with sufficient action diversity."}, {"title": "PRELIMINARIES", "content": ""}, {"title": "PROBLEM FORMULATION", "content": "In this work, we focus on model-free RL with continuous action space. The interaction process in RL is formulated as a Markov Decision Process (MDP), denoted as a tuple $M = (S, A, P, R, \\gamma)$, where S is the state space, A is the action space, $P(s'|s, a)$ is the transition probability of the environment, R:S\u00d7A \u2192 R denotes the reward function, and $\\gamma\\in [0,1)$ denotes the discount factor. The agent takes actions according to its policy, i.e. $a \\sim \\pi(\\cdot|s)$. Our objective is to find a policy $\\pi^*$ that maximizes the expected discounted return, i.e. $\\pi^* = arg max E_{a_t \\sim \\pi, S_{t+1} \\sim P} [\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)]$"}, {"title": "MODEL-FREE RL FOR CONTINUOUS CONTROL", "content": "In order to conduct policy evaluation, we define state-action value function $Q(s_t, a_t) = R(s_t, a_t) + E_{\\pi,P} [\\sum_{t'=t}^{\\infty} \\gamma^{t'} R(s_{t'}, a_{t'})]$ as the discounted return starting from $s_t$, given that $a_t$ is taken and then $\\pi$ is followed. The value function $V(s_t) = E_{a_t \\sim \\pi} [Q(s_t, a_t)]$ denotes the discounted return starting from $s_t$ following $\\pi$. Typically, both Q and V functions are modeled as neural networks, which are optimized using the Mean Square Error (MSE) loss, with target values obtained based on the Bellman equation. Take Soft Actor Critic (SAC) (Haarnoja et al., 2018) as an example, the policy is trained through the entropy augmented objective:\n$\\pi' \\leftarrow arg max E_{s \\sim D, a \\sim \\pi} [Q (s, a) \u2013 \\alpha log \\pi (a|s)]$,\nwhere D denotes the replay buffer, $\\alpha$ is the temperature parameter, and Q represents accumulated discounted reward augmented by the entropy."}, {"title": "METHODOLOGY", "content": ""}, {"title": "Two-STAGE POLICY: SELECTION AND ACTION", "content": "As described in Fig. 2, the decision process of SDAR is composed of two stages. (1) Selection: choose which action dimensions to change previous decisions utilizing the selection policy \u03b2. (2) Action: generate new actions for action dimensions that choose act in the previous stage utilizing action policy \u03c0. Both two stages are described as follows."}, {"title": "STAGE 1: SELECTION POLICY", "content": "As illustrated in the gray region of Fig. 2, given current state s and the action $\\bar{a}$ at the previous step, we conduct act-or-repeat selection for each action dimension individually. Formally, $\\beta(b|s, \\bar{a}) \\epsilon [0, 1]^{|A|}$, where $\\beta_i$ denotes the probability of choosing act in the i-th action dimension. Afterwards, the repetition schema $b \\in {0,1}^{|A|}$ at the current state is obtained by sampling on the Bernoulli distribution, i.e. $b_i \\sim \\beta_i(\\cdot|s, \\bar{a})$, where $b_i = 0$ and $b_i = 1$ indicate repetition and action of the i-th action dimension correspondingly."}, {"title": "STAGE 2: ACTION POLICY", "content": "As shown in the blue region of Fig. 2, the agent generates new actions for dimensions that choose act based on the current state s, the previous action $\\bar{a}$, and the repetition schema b. Firstly, we reduce redundant input dimensions through introducing the following Mix operation:\n$a_{mix} = Mix(b, \\bar{a}, \\xi) = (1 \u2013 b) \\odot \\bar{a} + b \\odot \\xi$,\nwhere $\\odot$ denotes element-wise hadamard product and $\\xi$ denotes the action mask, which is a constant mask vector filled with meaningless values. In this work, $\\xi_i = -2$ for $1 < i < |A|$ is utilized with A = [-1,1]$^{|A|}$. This operation eliminates the information of previous actions $\\bar{a}$ at dimensions where act was chosen in the previous stage. This information is redundant and may disrupt the training process of $\\pi$. Afterwards, we compute $\\hat{a}$ utilizing action policy $\\pi$:\n$\\mu_{mean}, \\sigma_{std} = MLP(s, a_{mix}), \\hat{a} = \\mu_{mean} + \\sigma_{std} \\cdot n, n \\sim N(0,1)$,\nwhere $\\mu_{mean}$ and $\\sigma_{std}$ are the mean and standard derivation of the action distribution predicted by action policy $\\pi$. The reparameterization trick is utilized to ensure that this process remains differentiable (Haarnoja et al., 2018). Finally, we obtain the output action a through the Mix operation formulated as follows:\n$a = Mix(b, \\bar{a}, \\hat{a}) = (1 \u2013 b) \\odot \\bar{a} + b \\odot \\hat{a}$.\nThis procedure guarantees that the final decision a replicates the same actions as the previous action $\\bar{a}$ in dimensions where repetition is selected, i.e. $(a \u2013 \\bar{a}) \\odot (1 \u2013 b) = 0$. The detailed proof is straightforward and can be found in Appendix A.\nAbove all, the decision of the whole two-stage policy $\\pi_{all}$ can be described as follows:\n$\\pi_{all}(a|s, \\bar{a}) = \\sum_{b \\in B} \\beta(b|s, \\bar{a}) \\int \\pi(\\hat{a}|s, \\bar{a}, b) \\cdot \\delta (Mix (b, \\bar{a}, a) \u2013 a) d\\hat{a}$"}, {"title": "POLICY EVALUATION", "content": "In this work, we employ two identical Q functions to evaluate the performance of $\\pi_{all}$. Each Q is trained utilizing Mean Square Error (MSE) loss with targets obtained based on the Bellman operator T formulated as follows:\n$min E_{(s,a) \\sim D} [Q (s, a) \u2013 TQ (s, a)]^2$, with $TQ(s, a) = R(s, a) + \\gamma E_{\\rho, \\beta, \\pi} [Q (s', a')]$"}, {"title": "POLICY IMPROVEMENT", "content": "We propose to optimize the policy $\\beta$ and $\\pi$ through maximizing the objective $J(\\theta^{\\beta}, \\theta^{\\pi})$ formulated as follows, where $\\theta^{\\beta}$ and $\\theta^{\\pi}$ are learnable parameters of $\\beta$ and $\\pi$ correspondingly.\n$J(\\theta^{\\beta}, \\theta^{\\pi}) = E_{(s,a) \\sim D} E_{b \\sim \\beta, \\hat{a} \\sim \\pi} Q(s, a) + \\alpha_{\\beta} log \\beta (b|s, \\bar{a}) - \\alpha_{\\pi} log \\pi (\\hat{a}|s, \\bar{a}, b)$,\nwhere entropy terms E [-log $\\beta(b|s, \\bar{a})$] and E [- log $\\pi(\\hat{a}|s, \\bar{a}, b)$] are utilized to encourage exploration during training, which is widely utilized in prior works (Haarnoja et al., 2018; Yu et al., 2021). $\\alpha_{\\beta}$ and $\\alpha_{\\pi}$ are temperature parameters to adjust exploration strategy, both of which are tuned automatically during training. As illustrated in Sec. 4.1 and Sec. 4.2, the computation process of $\\pi$ and Q functions are both differentiable. Therefore, the action policy $\\pi$ can be directly optimized using gradient descent with $\\nabla_{\\theta^{\\pi}} J$.\nIn order to optimize selection policy $\\beta$, we can transform objective J into the following formulations:\n$max E_{(s,a) \\sim D} \\sum_{b \\in B} \\beta(b|s, \\bar{a}) E_{\\hat{a} \\sim \\pi} [Q (s, a) - \\alpha_{\\beta} log \\beta (b|s, \\bar{a}) - \\alpha_{\\pi} log \\pi (\\hat{a}|s, \\bar{a}, b)]$,\nwhere B = {0,1}$^{|A|}$ is the action space for selection policy B. In contrast to Eq. (7), the new objective in Eq. (8) substitutes the expectation operator $E_{b \\sim \\beta}$ with the summation operator $\\sum_{b \\in B}$ by listing all possible situations b \u2208 B. The new objective is differentiable for $\\theta_{\\beta}$, thus can be used to optimize $\\beta$ through gradient descent directly.\nHowever, Eq. (8) is quite computationally expensive, because we are required to compute the score for all b \u2208 B, which needs to calculate Q and $\\pi$ for |B| = 2$^{|A|}$ times. Thus, this method is only practical for tasks with small action spaces, such as LunarLander with |A| = 2. For tasks with large action spaces such as Humanoid (A = 17), the selection policy $\\beta$ can be optimized by sampling several b \u2208 B, which is formulated as follows:\n$max E_{D} E_{b \\sim B_{old}, \\hat{a} \\sim \\pi} Q (s, a) \\frac{\\beta (b|s, \\bar{a})}{B_{old} (b|s, \\bar{a})} - \\alpha_{\\beta} log \\frac{\\beta (b|s, \\bar{a})}{B_{old} (b|s, \\bar{a})} - \\alpha_{\\pi} log \\pi (\\hat{a}|s, \\bar{a}, b)]$,\nwhere Bold denotes the old selection policy before optimization. This objective finds the expectation value $E_{b \\sim \\beta}[\\cdot]$ in Eq. (7) utilizing importance sampling. Compared to uniform sampling, sampling with Bold increases the probability of sampling b with high Q values during training, leading to higher sampling efficiency and training stability. As illustrated in Eq. (9), we are only required to compute Q and given several b sampled from $\\beta$, thus is more computationally efficient and practical than Eq. (8) in applications.\nIn addition, temperature parameters $\\alpha_{\\beta}$ and $\\alpha_{\\pi}$ are tuned automatically following the objective:\n$min E_{\\beta,\\pi} [\u2212 log(\\alpha_{\\beta})(log\\beta(b|s, \\bar{a})+H_{\\beta})\u2212log(\\alpha_{\\pi})(log\\pi(\\hat{a}|s, \\bar{a},b)+H_{\\pi})]$,\nwhere $H_{\\beta}$ and $H_{\\pi}$ are target entropies. During training, $log \\alpha_{\\beta}$ and $log \\alpha_{\\pi}$ are updated automatically through solving Eq. (10) with gradient descent, which is similar to prior works (Haarnoja et al., 2018). The detailed settings of the target entropies are described in Appendix B.1.\nThe whole training process of SDAR is illustrated in Algorithm 1, which iteratively collects data, trains critics, and updates policies. As illustrated in line 10, different to typical DRL algorithms, the previous action $a_{t\u22121}$ is stored in the buffer D, which is utilized to improve $\\theta_{\\beta}$ and $\\theta_{\\pi}$ described in line 15-16. Besides, the agent is forced to choose act at the initial step of an episode, i.e. b = [1, ..., 1]$^{|A|}$, because $\\bar{a}$ is absent at this state."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "EXPERIMENTAL SETUP", "content": "Tasks: In this work, we conduct experiments on multiple continuous control tasks, which are categorized into the following three types of scenarios. More details are given in Appendix B.2.\n(a) Classic Control: Several control tasks with small observation and action spaces, including MountainCarContinuous, LunarLanderContinuous, and BipedalWalker.\n(b) Locomotion: Locomotion tasks based on the MuJoCo (Todorov et al., 2012) simulation environment: Walker2d, Hopper, HalfCheetah, Humanoid, and Ant.\n(c) Manipulation tasks including Pusher, Reacher, and FetchReach.\nBaseline Methods. In this experiment, we compare the performance of SDAR with the following baseline methods: (a) SAC: vanilla Soft Actor Critic (Haarnoja et al., 2018) algorithm; (b) N-Rep: repeat policy actions for n times, where n is a fixed number (Mnih et al., 2015); (c) TempoRL: repeat policy actions dynamically in an open-loop manner, using a skip-policy to predict repetition steps (Biedenkapp et al., 2021); (d) UTE: Uncertainty-aware Temporal Extension (Lee et al., 2024) enhances training efficiency through incorporating uncertainty estimation of repeated action values into the open-loop repetition framework. (e) TAAC: Temporally Abstract Actor-Critic (Yu et al., 2021), which repeats actions dynamically in a closed-loop manner. TAAC determines whether to repeat the previous action at each step utilizing a switch policy. More details of the baseline methods are illustrated in Appendix B.3.\nThis study trains each method on various tasks using multiple random seeds over a range of 100K to 3M steps, depending on the complexity of the task. More settings including hyperparameters settings are described in Appendix B.1."}, {"title": "EXPERIMENT RESULTS ON SAMPLE EFFICIENCY", "content": "The training curves are illustrated in Fig. 3, and the AUC scores are shown in Table 1. The AUC scores are normalized into [0, 1] to evaluate the sample efficiency across different tasks, where 0.0 denotes the performance of random policies without effective training, while 1.0 denotes the perfor- mance of the best method. See Appendix C for the detailed computation process of the normaliza- tion. More experiment results are given in the Appendix D.\nAs illustrated in Fig. 3, our method SDAR (red lines) achieves higher sample efficiency than other baseline methods in various continuous control environments, corresponding to higher AUC scores shown in Table 1. This demonstrate effectiveness of performing act-or-repeat selection for each action dimension individually. Such spatially decoupled repetition framework enhances action per- sistence while maintaining policy flexibility, leading to efficient exploration and training. In the following, we analyze the performance of each method individually:\n\u2022 Naive action repetition N-Rep achieves unstable improvement compared to vanilla SAC, but underperforms closed-loop repetition approaches generally. As described in Table 1, N-Rep achieves higher AUC scores than SAC in classic control tasks, such as MountainCar shown in Fig. 3(a). Naive repetition improves the persistence of actions during exploration, accelerating the training progress effectively in simple tasks. Nevertheless, this approach lacks flexibility, which could negatively impact the performance in tasks requiring agile movements, such as HalfCheetah (Fig. 3(g)) in locomotion domains.\n\u2022 Open-loop methods (TempoRL and UTE) show minor improvements over SAC in classic control tasks, while performing worse performance in other domains. These approaches force the agent to repeat actions for a predicted number of steps, eliminating the possibility of early termination. This inflexibility makes such methods difficult to be utilized in tasks requiring agile movements, such as Humanoid shown in Fig. 3(d).\n\u2022 The closed-loop method TAAC outperforms SAC and open-loop approaches generally in various classes of tasks. TAAC introduces a switch policy to check whether to repeat previous actions at each step, which solves the lack of a termination mechanism in open-loop repetition methods. This leads to a more flexible repetition, which is suitable for both simple tasks and locomotion tasks requiring agile motions, such as Ant shown in Fig. 3(e).\nHowever, TAAC treats all action dimensions as a whole during act-or-repeat selection, which downgrades the effectiveness of action repetition. Take Humanoid and HalfCheetah shown in Fig. 3(d) and Fig. 3(g) respectively as examples, TAAC (brown lines) demonstrates superior sam- ple efficiency during the initial training phase, specifically between 0 and 1M steps. However, in later stages, the agent's learning speed diminishes compared to SDAR, which even results in performance decline in the Humanoid task depicted in Fig. 3(d). This is because the Humanoid is composed of multiple actuators, where some actuators are required to select act for action di- versity, while others choosing repeat for action persistence at the same time. However, TAAC is constrained to repeat actions of all actuators simultaneously, leading to either inadequate repeti- tion for inefficient exploration, or excessive repetition to damage the policy performance.\n\u2022 Our method SDAR outperforms other methods in various tasks, including TAAC in locomotion tasks. SDAR decouples action dimensions during closed-loop act-or-repeat selection, which is more flexible and suitable for various types of tasks. Taking the Humanoid task illustrated in Fig. 3(d) as an instance, SDAR compromises marginal efficiency in the initial stages to achieve greater overall efficiency and stability throughout the entire training process. Above all, SDAR achieves a better balance between action persistence for efficient exploration, and action diversity for high policy performance."}, {"title": "POLICY PERFORMANCE AND FLUCTUATION", "content": "In this work, we assess the policies trained through each method on the following metrics: (1) Episode Return: the average cumulative reward acquired by the policy in an episode, assessing the policy's capability to solve the task successfully. (2) Action Persistence Rate (APR) and (3) Action Fluctuation Rate (AFR) formulated as follows:\n$APR = \\frac{1}{1-p}, p = E_{\\pi} [\\frac{1}{T} \\sum_{t=1}^{T}  (\\frac{1}{|A|} \\sum_{i=1}^{|A|}  \\delta (a_{t-1}^{(i)} ,  a_{t}^{(i)}) )]$,\n$AFR = E_{\\pi}  [\\frac{1}{T} \\sum_{t=1}^{T} |a_{t}  -  a_{t-1} |]$,\nwhere T denotes the trajectory length, $a_t^{(i)}$ denotes the action at the t-th step in the i-th dimension, and p denotes the average repetition probability of $a_t^{(i)}$ at each step. In this work, APR represents the average interval that two new decisions are made. APR evaluates action persistenc of the policy, where a larger APR denotes more repetition times during interactions. AFR evaluates the mean amplitude of fluctuations between successive steps, where a smaller AFR indicates smoother actions with fewer fluctuations.\nThe results of episode return, APR, and AFR are shown in Table 2. As illustrated in the tables, although the baseline method such as N-Rep achieves excellent results in both APR and AFR, it suffers a considerable reduction on policy effectiveness, corresponding to lower episode return com- pared to SAC. This is unacceptable and makes such methods impractical for real-world applications. In contrast, our method SDAR outperforms baseline methods on episode return, while achieving a higher APR and a lower AFR than the vanilla DRL, demonstrating the effectiveness of our method. In addition, we can also observe that:\n\u2022 Naive repetition method N-Rep achieves high APR and low AFR, at the expense of performance reduction on episode returns. This is because the actions in N-Rep are repeated for frozen times, where repeated actions may be unreasonable in some states. This inflexible repetition strategy cannot be agilely adjusted during interaction, leading to performance reduction, especially in locomotion tasks such as HalfCheetah.\n\u2022 Open-loop methods achieve high APR, indicating a lot of action repetitions during interaction, at the expense of more fluctuations and superior episode return performance. For instance, UTE achieves higher APR (3.71) than SAC (1.0), with large fluctuation and low episode return. This suggests that the increment of repetition times in an inflexible manner may be harmful to policy effectiveness and smoothness. In contrast, TAAC solves this problem, achieving a moderate AFR (1.66), with a comparable AFR (0.244) and high episode return (0.91).\n\u2022 Different to UTE and SDAR, although SDAR also selects a lot of repeat during interaction with a high APR (3.69), SDAR achieves excellent performance on fluctuation and episode returns simultaneously. This is because SDAR performs repetitions in a more flexible manner. The de- coupling design is consistent with the requirement of continuous control tasks, thus can conduct repetition without harms to episode returns and fluctuation performance."}, {"title": "VISUALIZATION OF ACT-OR-REPEAT SELECTION", "content": "In order to analyze the difference between repetition behaviors of SDAR and previous methods, we perform visu- alization of act-or-repeat selections in LunarLander and Walker2d. The results are illustrated in Fig. 4(a) and Fig. 4(b), where dark blue and light blue blocks denote act and repeat respectively.\nAs shown in the figures, different dimensions require differ- ent decision frequencies, corresponding to actions being re- peated at distinct steps for each dimension. Take LunarLan- der in Fig. 4(a) for example, the lateral boosters (a1) change actions frequently to adjust the rocket pose, while the main engine (a0) adjusts decisions occasionally.\nSDAR can adjust action persistence for each dimension automatically through spatially decoupled framework, offering more flexible repetition strategies compared to TAAC. As described in Fig. 4(b) and Table 3, SDAR controls the walker with higher persistence for the leg joints, while lower per- sistence for thighs and feet. In contrast, all joints in TAAC are required to repeat simultaneously, resulting in same persistence for each joint with lower repetitions behaviors.\nSDAR achieves high action persistence, while maintaining agility required by the task, leading to higher policy performance with fewer action fluctuations. As shown in Table 3, SDAR obtains higher APR than TAAC in Walker2d, while achieving higher episode return with lower AFR, demonstrating the effectiveness of our framework in continuous control tasks."}, {"title": "CONCLUSION", "content": "In this work, we propose a novel action repetition framework for continuous control tasks called SDAR, which implements closed-loop act-or-repeat selection for each action dimension individu- ally. Such a spatially decoupled design improves the flexibility of repetition strategies, leading to improved balance between action persistence and diversity, while maintaining action agility for con- tinuous control tasks. Experiments are conducted in various task scenarios, where SDAR achieves higher sample efficiency, superior policy performance, and reduced action fluctuation, demonstrat- ing the effectiveness of our method.\nThis work provides insights into spatially decoupled framework for action repetition and temporal abstraction. In this study, the selection policy \u03b2 is designed to decide act-or-repeat for each dimen- sion independently, without accounting for inter-dimensional correlations. For instance, actuators in humanoid robots can be divided into multiple categories, with each category typically requiring same decisions during act-or-repeat selections. This issue will be further researched in future works."}, {"title": "PROOF OF THE STATEMENT IN SECTION 4.1", "content": "Statement. Given \u2200s \u2208 S,$\\bar{a}$ \u2208 A,b \u2208 {0,1}$^{|A|}$, based on the two-stage policy described in Sec. 4.1, the output action a replicates the same actions as $\\bar{a}$ in repetition dimensions {i|$b_i = 0, 1 \\leq i \\leq |A|$}.\nProof. As described in Sec. 4.1, $a = (1 \u2013 b) \\odot \\bar{a} + b \\odot \\hat{a}$. (1-b) $\\bar{a}$ + b $\\hat{a}$. Thus, for \u2200b, $\\bar{a}$, we can obtain that:\n$(a - \\bar{a}) \\odot (1 \u2013b) = ((1 \u2013 b) \\odot \\bar{a} + b \\odot \\hat{a} - \\bar{a}) \\odot (1 \u2013b)$\n$= b \\odot (\\hat{a} - \\bar{a}) \\odot (1 \u2013b)$\n$= b \\odot (1 \u2013 b) \\odot (\\hat{a} - \\bar{a})$\n$= 0$,\nFor \u2200i \u2208 {$i|b_i = 0$}, we have [(a-$\\bar{a}$)\u2299 (1\u2212b)]i = a(i) \u2014 \u0101(i) = 0, where a(i) denotes the action in the i-th dimension. Thus, a replicates the same actions as $\\bar{a}$ in dimensions choosing repetition."}, {"title": "EXPERIMENT DETAILS", "content": ""}, {"title": "ALGORITHM SETTINGS", "content": "The hyper-parameter settings ar shown in Table. 4. In addition, we need to tune the target entropies H\u03b2 and H\u03c0 to improve the efficiency of the entropy-based exploration described in Eq. (10). In this work, we utilize H\u03c0 = \u2212|A|, where |A| denotes the size of the action space, corresponding to the recommendation given in (Haarnoja et al., 2018). Besides, H\u03b2 = \u03bb\u00b7 |A|log 2, where X is a hyper-parameter tuned according to the tasks. In this work, we set \u03bb \u2208 [0.4,0.6], which achieves excellent performance on sample efficiency and policy effectiveness.\nAs illustrated in Sec. 4.1, SDAR is composed of two policy networks \u03b2 and \u03c0 based on MLP. In this work, we design two MLPs with the same structures: (|S| + |A|\u2192 256 \u2192 256 \u2192 |A|) with ReLU activation functions.\nSDAR policies for tasks with |A| \u2264 3 are trained through Eq. (8), such as Reacher and LunarLanderContinuous. Policies in other environments such as HalfCheetah and Humanoid are optimized through Eq. (9) for lower computation costs."}, {"title": "EXPERIMENT TASKS", "content": "The experiment tasks are listed in Table 5, where the action spaces of all tasks are normalized as [-1,1] for convenience. All tasks are constructed based on Gymnasium (Plappert et al., 2018). The FetchPickandPlace and FetchReach tasks are implemented by Gymnasium-Robotics (Plappert et al., 2018), where observation and desired_goal are concatenated as the observation in this experiment."}, {"title": "BASELINES", "content": "(1) SAC (Haarnoja et al., 2018) is a famous model-free RL in continuous control domains, which trains policies efficiently with entropy-based exploration strategies. In this work, we utilize SAC implementation and hyper-parameter settings proposed in CleanRL (Huang et al., 2022)\u00b9.\n(2) N-Rep forces the agent to repeat the actions output by the policy for n times, where n is a hyper-parameter. In this work, this algorithm is implemented based on SAC, where n is tuned in [2, 5] to achieve the balance between persistence of action and diversity.\n(3) TempoRL (Biedenkapp et al., 2021) repeats policy actions dynamically in an open-loop manner, using a skip-policy to predict when to make the next decision. In this work, this method is implemented based on the official repository2.\n(4) UTE (Lee et al., 2024) is improved based on TempoRL, which performs repetition while incor- porating the estimated uncertainty of the repeated action values. In this work, this method is implemented based on the official repository 3.\n(4) TAAC (Yu et al., 2021) is a closed-loop repetition framework, which determines whether to repeat the previous action at each step utilizing a switch policy. In this work, this experiment is conducted utilizing the official implemenatation4."}, {"title": "COMPUTATION RESOURCES", "content": "In this work, we conduct all experiment utilizing NVIDIA RTX 3090 GPU and Pytorch 2.1 with CUDA 12.2. The training time of our method"}]}