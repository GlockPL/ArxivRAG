{"title": "SSDM: Scalable Speech Dysfluency Modeling", "authors": ["Jiachen Lian", "Zoe Ezzes", "Jet Vonk", "Xuanru Zhou", "Brittany Morin", "David Baquirin", "Zachary Miller", "Marilu Gorno Tempini", "Gopala Anumanchipalli"], "abstract": "Speech dysfluency modeling is the core module for spoken language learning, and speech therapy. However, there are three challenges. First, current state-of-the-art solutions [1, 2] suffer from poor scalability. Second, there is a lack of a large-scale dysfluency corpus. Third, there is not an effective learning framework. In this paper, we propose SSDM: Scalable Speech Dysfluency Modeling, which (1) adopts articulatory gestures as scalable forced alignment; (2) introduces connectionist subsequence aligner (CSA) to achieve dysfluency alignment; (3) introduces a large-scale simulated dysfluency corpus called Libri-Dys; and (4) develops an end-to-end system by leveraging the power of large language models (LLMs). We expect SSDM to serve as a standard in the area of dysfluency modeling. Demo is available at https://eureka235.github.io.", "sections": [{"title": "1 Introduction", "content": "Speech dysfluency modeling is key for diagnosing speech disorders, supporting language learning, and enhancing therapy [1]. In the U.S., over 2 million individuals live with aphasia [3], while globally, dyslexia affects approximately one in ten people [4]. The U.S. speech therapy market is projected to reach USD 6.93 billion by 2030 [5]. This growth parallels developments in Automatic Speech Recognition (ASR), valued at USD 12.62 billion in 2023 [6], and Text-to-Speech (TTS), valued at USD 3.45 billion [7]. Moreover, the global language learning market is anticipated to be USD 337.2 billion by 2032 [8]. Conversely, substantial investments have been made in training speech-language pathologists (SLPs) [9, 10], and the high cost of treatment often remains out of reach for many low-income families [11-15]. Therefore, there is a crucial need for an AI solution that makes advanced speech therapy and language learning available and affordable for everyone.\nSpeech dysfluency modeling detects various dysfluencies (stuttering, replacements, insertions, deletions, etc) at both word and phoneme levels, with accurate timing and typically using a reference text [1]. (see Figs.1 for examples). Fundamentally, it is a spoken language understanding problem. Recent advancements have been driven by large-scale developments [16\u201331]. However, these efforts often focus on scaling coarse-grained performance metrics rather than deeply listening to and understanding the nuances of human speech.\nTraditional approaches to dysfluency modeling have relied on hand-crafted features [32\u201336]. Recent advancements have introduced end-to-end classification tasks at both utterance [37-48] and frame levels [49, 50]. Another concurrent work treats dysfluency modeling as a time-domain object detection problem. [51]. However, these methods often overlook internal dysfluency features like alignment [1] and struggle to detect and localize multiple dysfluencies within a single utterance. [1, 2] propose 2D-Alignment, a non-monotonic approach that effectively encodes dysfluency type and timing. Nonetheless, initial experiments show that this method struggles with scalability, limiting its further development. To address these concerns, we revisit this problem and summarize our contributions as follows:\n\u2022 We revisit speech representation learning from a physical perspective and propose neural articulatory gestural scores, discovered to be scalable representations for dysfluency modeling.\n\u2022 We introduce the Connectionist Subsequence Aligner (CSA), a differentiable and stochastic forced aligner that links acoustic representations and text with dysfluency-aware alignment.\n\u2022 We enable end-to-end learning by leveraging the power of large language models.\n\u2022 We open-source the large-scale simulated dataset Libri-Dys to facilitate further research."}, {"title": "2 Articulatory Gesture is Scalable Forced Aligner", "content": "2.1 Background\nRevisit Speech Representation Learning Self-supervised speech representations [52], large-scale ASR [16-18, 20], codec models [53\u201368], and speech language models (SLMs) [21\u201331] have emerged as universal paradigms across tasks and languages. However, high computing costs of scaling efforts is not affordable for academia researchers. In this work, we propose learning speech representations grounded in fundamental physical laws [69, 70]. This approach characterizes speech representations by the kinematic patterns of articulatory movements, a method we refer to as gestural modeling.\nGestural Modeling The concept of gesture, as defined by [71, 72], refers to articulatory movements in acoustic space, similar to body gestures in humans. [71, 72] introduced gestures as a dictionary of basic articulatory movements and gestural scores, representing the duration and intensity of these movements. This principle resembles the gait library and optimization used in robotics [73]. The computational modeling of gestures was first developed by [74], using sparse matrix factorization [75, 76] to decompose EMA data [77] into interpretable components. Further research by [78] and [79] streamlined this into an end-to-end neural approach. Gestural scores serve as speech representations. We discovered that they serve as scalable dysfluent phonetic forced aligner.\nScalable Dysfluent Phonetic Forced Aligner Dysfluency modeling requires detecting both the type and timing of dysfluencies, necessitating the use of forced alignment [1]. This alignment is often non-monotonic (e.g., stuttering). Thus, previous monotonic alignment methods [80, 81, 20, 82] perform poorly in the dysfluency domain. The primary challenge is the inherent uncertainty in what the speaker actually said, compounded by invariably inaccurate reference texts, as explained in [1]. Effective research in this area focuses on non-monotonic alignment modeling. [83] introduces the WFST [84] to capture dysfluencies such as sound repetition. However, it assumes the actual speech does not deviate significantly from the reference text. [1] proposed 2D-alignment as final dysfluent representation. Nevertheless, this method, and its extension [2], suffers from scalability issues: increasing training data does not lead to further improvements. In this work, we revisit the monotonic alignment to tackle the scalability problem. To achieve this, we need a scalable representation, and a scalable monotonic aligner (Sec. 3). This section focuses on the first part and proposes Neural Variational Gestural Modeling to deliver gestural scores H as scalable dysfluent speech representations. We also provide a visualization of gestures and gestural scores in Appendix. A.1.\n2.2 Neural Variational Gestural modeling\nDespite theoretical support [71, 72, 69, 70], gestural scores have not yet become a universal speech representation [52] due to several limitations. First, gestural modeling requires extensive, often unavailable, articulatory kinematic data. Second, there is not an effective learning framework. Third, the commonly used EMA data, sampled sparsely from human articulators [85\u201388], suffer from information loss. To overcome these challenges, we proposed Neural Variational Gestural Modeling. This model uses an offline inversion module (Sec. 2.2.1) to capture articulatory data, and a gestural VAE to extract gestural scores (Sec. 2.2.2), which are then refined through joint self-distillation with acoustic posteriors and textual priors (Sec. 2.2.3). This method ensures that the resulting gestural scores are effective and scalable dysfluent speech representation. (Evidenced in Sec. 6)\n2.2.1 Universal Acoustic to Articulatory Inversion (UAAI)\nSince the real articultory data are typically unavailable, we employ a state-of-the-art acoustic-to-articulatory inversion (AAI) model [89] pretrained on MNGU0 [85]. The model takes 16kHz raw waveform input and predicts 50Hz EMA features. Details are listed in Appendix. A.2.1.\n2.2.2 Gestural Variational Autoencoders\nAny motion data $X = [X_1, X_2, ..., X_t]$ can be decomposed into motion kernels $G \u2208 \\mathbb{R}^{T \\times d \\times K}$ and an activation function $H \u2208 \\mathbb{R}^{K \\times t}$ using convolutional matrix factorization (CMF) [76], where $X ~ \\Sigma G^{(i)} \\cdot H$. Here, t represents time, T the kernel window size, d the channel size, and K the number of kernels. When X is articulatory data, G corresponds to K gestures and H to the gestural scores (Visualization in Appendix A.1 and A.1.2). This work focuses on three aspects: (1) joint modeling of articulatory-specific duration and intensity, (2) self-distillation from both acoustic and textual data, and (3) multi-scale decoding of gestures and gestural scores.\nVariational Inference We employ point-level variational inference for $q_e(H|X)$, meaning for each point (k, i) in $H \u2208 \\mathbb{R}^{K \\times t}$, we model its posterior $q_e(H_{k,i}|X)$. This approach results in $K \\times t$ posteriors for each gestural score H, where $k = 1, . . ., K$ and $i = 1, ..., t$. We use pointwise inference for gestural scores due to its properties, such as overlapping durations across articulators and stochastic variations across accents. We will refer to this as patchwise rather than pointwise, as we are modeling a patch embedding for each point (k, i). In practice, we introduce an additional latent vector $Z_{k,i} \u2208 \\mathbb{R}^{P}$ as variational augmentation [90], where P is patch size. This setup formulates the duration posterior $q_\\phi(D_{k,i}|Z_{k,i}, X)$, intensity posterior $q_\\$(I_{k,i}|Z_{k,i}, X_{k,i})$, and latent posterior $q_\\$(Z_{k,i}|X)$. Patchwise operation is detailed in Appendix A.2.2. Consequently, our gestural encoder encodes the joint posterior $q_\\$(Z_{k,i}, D_{k,i}, I_{k,i}|X) = q_\\phi(D_{k,i}|Z_{k,i}, X)q_\\$(I_{k,i}|Z_{k,i}, X_{k,i})q_\\phi(Z_{k,i}|X)$.\nVAE Objective After variational inference, our decoder $p_\theta(X|H,G) = P_o(X|D, I, G)$ reconstructs X using duration D, intensity I, and gesture G. The evidence lower bound (ELBO) and its derivation are provided in Eq. 1 and Appendix A.4, respectively. The posterior $q_\\$(Z_{k,i}|X)$, modeled via vanilla variational inference [91], assumes standard normal priors for $p(Z_{k,i})$. The mechanisms of the duration and intensity encoders, $q_\theta(D_{k,i}|Z_{k,i}, X_{k,i})$ and $q_\\phi(I_{k,i}|Z_{k,i}, X_{k,i})$, are detailed in Sec. 2.2.2 and Sec. 2.2.2. Details on the decoder $P_o(X|D, I, G)$ are discussed in Sec. 2.2.2.\n$L_{ELBO} = Eq\u2084(Z,D,I\\X) [logpe(X|D, I, G)]$\n$-E(k,i)~S [KL (q$(Z_{k,i}, D_{k,i}, I_{k,i}|X)||p(Z_{k,i}, D_{k,i}, \u012b_{k,i}))]$  (1)\nDuration Posterior $q_\\phi(D_{k,i}|Z_{k,i}, X^{k,t})$ We employ the Gumbel softmax [92] to reformulate the duration posterior $q_\\phi(D_{k,i}|Z_{k,i}, X)$. Let $\u03c0_{k,i} \u2208 \\mathbb{R}^{C}$ denote the logits across all C discrete duration classes (values) for patch (k, i). For each class j, we obtain Gumbel noise $e_j = -log(-log(U_j))$, where $U_j$ ~ Uniform(0, 1). We then define $\\hat{\u03c0_j} = (log(\u03c0_j) + e_j)/\u03c4$, where \u03c4 is temperature parameter. Finally, we obtain the Gumbel softmax transformation as an approximation of the duration posterior in Eq.2. We set $p(D_{k,i}) = 1/C$, where C is the number of discrete duration classes. Background and detailed methodology can be viewed in Appendix. A.2.2.\n$q_\\phi(D_{k,i}=j|Z_{k,i},X)\u2248 \\frac{exp(\u5143_{k,i}^j)}{\\sum_{l=1}^C exp(\u5143_{k,i}^l)}$ (2)\nIntensity Posterior $q_\\phi(I_{k,i}|Z_{k,i}, X_{k,i})$ After sampling $I_{k,i} ~ q_\\phi(I_{k,i}|Z_{k,i}, X_{k,i})$, the model applies a per-gesture, region-wise impact. This can be formulated in Eq. 3. where $H_{i-D_{k,i}/2:i+D_{k,i}/2,k}$ represents the local window of impact, $I_{k,i}$ is the sampled impact value, and $D_{k,i}$ is the duration of the gesture. We actually applied Sigmoid function to deliver positive intensity values. The Hann function is used to apply the impact smoothly within the local window. The motivation behind this formulation is that most patches (k, i) are not activated, reflecting the sparse nature of human speech production and co-articulation [71, 78]. Visualizations can be checked in Appendix.A.2.2.\n$H_{i\u2212 D_{k,i}:i+D_{k,i},k} = Hann (Sigmoid(I_{k,i} ~ q_\\phi(I_{k,i}|Z_{k,i}, X_{k,i})), D_{k,i} ~ qq(D_{k,i}| Z_{k,i}, X))$ (3)\nOnline Sparse Sampling Given the limited number of patches contributing to gestural scores [72], we localize the impact within a specific window. We define a Combined Score $S_{k,i} = aI_{k,i} + bD_{k,i}$, where $I_{ki}$ and $D_{k,i}$ represent impact and duration, respectively, and a and b are hyperparameters. This score ranks the importance of each patch, with indices for each gesture computed as $r_{row} (k, i) = rank(-S_{k,i} within row k)$. Setting $m_{row}$ as the number of patches selected, we apply a sparse mask $M_{row}$ (Eq. 4) to derive the final sparse gestural scores, detailed in Eq. 4. This entire online sparse sampling process is differentiable. The parameters a, b, and $m_{row}$ are elaborated in the Appendix. For simplicity, we denote this process as $(i, k) ~ S$, with visualizations in Appendix A.2.3.\n$H_{k,i} \u2190 M_{k,i} \\cdot H_{k,i}$ where $M_{k,i} = 1 \\frac{1 if r_{row} (k, i) < m_{row}}{0 otherwise}$ (4)\nMulti-scale Gestural Decoder The decoder reconstructs $X = [X_1, X_2, ..., X_t] \u2208 \\mathbb{R}^{d \\times t}$ from gestures $G \u2208 \\mathbb{R}^{T \\times d \\times K}$ and gestural scores $\\hat{H} \u2208 \\mathbb{R}^{K \\times t}$. In this work, we retain the CMF operation [78] and extend it to multiple deep layers. We also introduce multi-scale mechanism, which has proven to be a robust tokenizer for various speech tasks [93, 94, 63, 95]. Denote: $f_{1/2}, f_{down}, down, f_{1/4} f_{up,e}, f_{up,0}$ as downsample/upsample modules with scales of 1/2 or 1/4. The convolutive matrix factorization operator A * B means $\\sum_{i=0}^{T-1}A^{(i)}. B_i$ where $A \u2208 \\mathbb{R}^{T \\times d \\times K}$ and activation function $B \u2208 \\mathbb{R}^{K \\times t}$."}, {"title": "3 Connectionist Subsequence Aligner (CSA) for Dysfluency Modeling", "content": "Then our multi-scale decoder is defined in Eq. 5, where r = 1 means no resolution change, and $f_{trans}$ represents any neural network, details of which can be found in the Appendix. Up to this point, $P_o(X|D, I, G)$ (Eq. 1) is defined. We provide more details in Appendix A.2.4.\n$X = \\sum_{r\u2208{1,2,4}} f_{up,r} (f_{trans,e} (G* f_{down,r}(\\hat{H})))$ (5)\n2.2.3 Gestural Scores as Phonetic Representations\nAfter obtaining gestural scores, we predict phoneme alignment for dysfluency modeling. For clean speech, alignment is acquired using the Montreal Forced Aligner (MFA) [81], while for dysfluent speech, it is simulated (see Section 5). The direct prediction of phoneme alignment from handcrafted features or self-supervised learning (SSL) units [52] is limited due to scalability issues with dysfluent speech, discussed further in Sec. 6. We utilize 4X downsampled gestural scores (from decoding), denoted as $\\hat{H}$, matching the resolution of acoustic features [96]. Let $\\tau = [T_1, T_2, . . ., T_{t'}]$ represent the phoneme alignment, where t' = t/4. Employing the Glow algorithm [97], we transform $\\hat{H}$ into \u03c4, expressed as $\u03c4 = f_f(\\hat{H})$, optimized via a softmax crossentropy objective $L_{phn}$.\nSelf-Distillation We distill gestural scores from pretrained acoustic features [96], which are then adapted to match gestural scores' dimensions. Instead of directly measuring the distance between acoustic embeddings and gestural scores, we use the alignment-conditioned gestural prior as an acoustic-conditioned gestural posterior. The reference text $C = [C_1, C_2, . . ., C_L]$ is processed by a text encoder to yield the latent Gaussian posterior $(\\mu^1_0, \u03c3^1_0), (\\mu^2_0, \u03c3^2_0), ..., (\\mu^L_0, \u03c3^L_0)$, with the gestural posterior modeled via the change of variable property $f_f$ as described in Eq. 6. Intuition, detailed methodology and visualization can be view in Appendix. A.3.\n$p_e(H|C) = p_e(T/C) |det(\\frac{\\partial T}{\\partial H})| = \\Pi_{k=1}^{K_1} \\Pi_{l=1}^{C_L}N (\u03c4_l^c; \u03bc_0^l, (\u03c3_0^l)^2) \\frac{1}{A_t} |det (f_f)|$ (6)\nConversely, given the acoustic embedding $A = [A_1, A_2, ..., A_L]$, a text encoder is employed to output the latent Gaussian posterior $(\u03bc^1_a, \u03c3^1_a), (\u03bc^2_a, \u03c3^2_a), ..., (\u03bc^{\u03c4'}_a, \u03c3^{\u03c4'}_a)$. The posterior $q_o (H|A)$ can be derived in a similar manner. The overall distillation loss is then presented in Eq. 7.\n$L_{dist} = KL (q_0(9_0(H|A)||p_0(\\hat{H}|C))$, where $q_0(H|A) = \\Pi_{l=1}^{K_2} \\Pi_{t'=1}^{A_t}N (H^c; \u03bc_a, (\u03c3_a)^2)$ (7)\nBoth $K_1$ and $K_2$ are normalization terms. The overall loss objective for neural variational gestural modeling is shown in Eq. 8, where $\u03bb_1, \u03bb_2, \u03bb_3$ are balancing factors.\n$L_{VAE} = -\u03bb1 L_{ELBO} + \u03bb2L_{phn} + \u03bb3L_{dist}$ (8)\n3.1 Monotonic Alignment is effective Dysfluency Aligner\nGiven the reference text $C = [C_1, C_2, ..., C_L]$ and dysfluent phonetic alignment $T = [T_1, T_2, ..., T_{t'}]$, the alignment between C and T is typically non-monotonic. For example, when people say \"pl-please,\" it is non-monotonically aligned with \"p-l-e-a-s-e.\" Prior work [1, 83] on non-monotonic dysfluent modeling has its limitations, as discussed in Sec. 2.1. In this work, we focus on monotonic alignment and argue that it is effective dysfluency aligner. The intuition is straightforward: we seek an aligner $\u03b3 : {1, 2, ..., L} \u2192 P({1, 2, . . ., t' })$ such that for each $i \u2208 {1, 2, . . ., L}$, Eq. 9 holds. The aligner y maps elements in C to consecutive subsequences in 7 without overlap. This property is beneficial for dysfluency detection, as for each element in C, we can determine the presence of dysfluencies such as insertion, deletion, repetition, block, replacement, etc., based on $\u03b3(C_i)$.\n$\u03b3(Ci) = [T_{si}, T_{si+1},..., T_{ei}]$ where $\\begin{cases}1 \u2264 S_i \u2264 e_i < t'  \\\\S_i < S_{i+1}, C_i < l_{i+1} & \u2200i \u2208 {1,2,..., L - 1} \\\\C_i < S_{i+1} & \u2200i \u2208 {1, 2, ..., L - 1}\\end{cases}$ (9)\n3.2 Local Subsequence Alignment (LSA) Achieves Semantic Dysfluency Alignment\nAll monotonic aligners satisfy Eq.9, which serves as a necessary condition. However, we also desire $\u03b3(Ci)$ to be semantically aligned with $C_i$. Consider the aforementioned example: one preferred"}, {"title": "4 Language Models and Overall Training Objective", "content": "alignment is (p)=[p,l,p], indicating the presence of a stutter. In contrast, if $\u03b3(p)=[p,1,p,l,e,a,s]$, it becomes challenging to identify any reasonable dysfluency, despite still satisfying Eq.9. In this work, we propose that Local Subsequence Alignment (LSA) is an effective approach for achieving semantically aligned y. Before delving into the main topic, we propose and introduce two terms: (i) Global Sequence Aligner (GSA), where the cost function involves the alignment of all elements in the sequence; this includes most sequence aligners such as DTW [98\u2013100], CTC [80], and MFA [81]; and (ii) Local Sequence Aligner (LSA), where the cost function involves only a subset of elements. One representative is longest common subsequence (LCS) alignment [101, 102].\n3.3 Connectionist Subsequence Aligner (CSA) Formulation\nObjective From gestural score $\\hat{H}$, we obtain phonetic alignment $\u03c4 = f_f(\\hat{H}) = [T_1, T_2, ..., T_{t'}]$. In practice, both \u03c4 and C are embeddings instead of explicit labels, where $C = [C_1, ..., C_L]$ are sampled from the text encoder $N(\u03bc_c, (\u03c3_c)^2), i = 1,..., L$, as proposed in Sec.2.2.3. Let t' denote the sequence length after removing duration from the original length t'. Duration will be reincorporated post-alignment. The alignment between C and T is already defined in Eq.9. We introduce another notation \u0393, where $\u0393(T_i)$ is the aligned token in C. $\u0393(\u03c4) = [\u0393(T_1), ..., \u0393(T_{t'})]$ represents the final alignment with respect to C, in comparison to alignment $\u03b3(C)$, which is with respect to T. There are possibly multiple (N) alignments $\u03b3^{LSA}(C)$, where j = 1, ..., N. Our goal is to optimize model @ to obtain the largest joint distribution of alignments $\\sum_{j=1}^{N} \u03b3^{LSA} (C)$. However, unlike CTC [80], we can't search alignments explicitly as the monotonic constraints are different. We propose approximating LSA. Let $\u0393' (\u03c4)$ be one approximate LSA alignment, and assume there are N possible LSA alignments: $\u0393'_j (\u03c4)$ where j = 1, ..., N. Our final objective is formulated in Eq. 10.\n$max_{P_o} E_{C,T} \\sum_{j=1}^{N} p_e (\u03b3^{LSA}(C)|C, T) = max_{P_o} E_{C,T} \\sum_{j=1}^{N} p_e (\u0393'_{LSA}(\u03c4)|C, T) \u2248 max_{P_o} E_{C,T} E[\\sum_{j=1}^{N} p_e (\u0393'\u03c4)]$ (10)\nApproximate LSA Alignments $\u0393' (\u03c4)$ We define $\u03b3^{1i}$ as the emission probability $p(C_j|\u03c4_i)$, and transition probability $p_o(\u03c4_j|\u03c4_i)$. Let $C_l^j$ denote the embedding sampled from the distribution $N(\u03bc_0^j, (\u03c3_0^j)^2)$ (Sec.2.2.3). The emission probability is given in Eq. 11. We approximate the transition probability using a separate neural network $p_o (\u03c4_j|\u03c4_i)$.\n$\u03b3^{1i} = p_o(C_j|\u03c4_i) ~ \\frac{expi C_j  ^ \u03c4j}{\\sum_{i=1}^L exp-C_l \u03c4j}$ (11)\nIt is possible to list all LCS alignments $\u0393'_i(r)$, where i = 1, ..., N, via soft alignments [100], which are also differentiable. However, we propose that by simply introducing the LCS constraint on the vanilla CTC [80] objective, the LCS can be implicitly applied, which we call the Connectionist Subsequence Aligner (CSA). Let us consider Figure 4 (left) for intuition. For a single alignment $\u0393'_i (\u03c4)$, the emission probability and transition probability will only be applied if C\u2081 is already aligned (C\u2081 in the figure). We refer to these as Transition Skip and Emission Copy. Now, let us move to the LCS-constrained forward-backward algorithm [80]. Taking the forward algorithm (Figure 4 (mid)) for illustration, Emission Copy is reflected in $\u03b1_{i,j}$ via an identity multiplier on $A_{i-1,j}$. Transition Skip is reflected on both $\u03b1_{i-1,j}$ and $A_{i-1,j-1}$, where we apply a transition on $\u03b1_{i-1,j-1}$. This also implicitly leverages language modeling. We also consider all previous tokens $C_{j-2}, ..., C_k, ..., C_0$; however, no transition is applied, but a discounted factor \u03b4k is utilized instead. This indicates a significant jump (deletion), which we denote as a dysfluency module, although all other modules model dysfluencies equally. Forward and backward algorithms are displayed in Eq. 12 and Eq. 13.\n$\u03b1_0^{i,j} = \\sum_{j \\approx \\approx}^{j} \u03a3_k \u03b4_k\u03b1_{i-1,j-k} \\cdot (p_o(C_j-1|C) \\cdot 1{k=1} + 1{k\u22601})$ (12)\n$\u03b2t'_L = \u03b2_{i+1,j+1} + \u03a3_k \u03b4_k\u03b2t'+1,j+k \\cdot \u03b3^{i+1,j+k} \\cdot (p_o(C|C_{j+1}) \\cdot 1{k=1}+1{k\u22601})$ (13)\nWe initialize $\u03b1_{1,1} = \u03b2_{t'',L} = 1, \u03b1_{(i, 1)} = 0 \u2200i > 0, \u03b2_{(i, 1)} = 0 \u2200i < t'$, and $\u03b2_{(1,j)} = 0 \u2200j < L$. Our CSA objective is displayed in Eq. 14, where we take the summation over all reference tokens and time stamps.\n$L_{CSA} = -E  \\frac{1}{N}\\sum\\sum_{j=1} p(\u03b3' (T)) = -\\sum\\sum_ \u03b3^{i,j}$ (14)\nSampling As the alignment $\u0393' (T)$ is required for the next module, it is necessary to sample it during training. Traditional beam search methods are impeded by reduced inference speeds. To mitigate this, we employ the Longest Common Subsequence (LCS) algorithm offline on $C_e$ and $T_e$ to derive the alignments. The final alignment is denoted as $y(C) = [1,..., TS]$, as presented in Eq. 9. This methodology yields a sequence of inputs in the form of CSA-O = [(C\u00a3, y(C\u00a3)), ..., (C\u33a2, y(C\u00a3))]."}, {"title": "4 Language Models and Overall Training Objective", "content": "Following LTU [23], we utilize speech representations (alignment) [(C\u00a3, y(C\u00a3)), \u2026\u2026\u2026, (C\u00a3,y(CE))] (Sec. 3.3), along with word-level timestamps, reference text C, and instruction C\u00b9, as input to LLaMA-7B [104]. During the training process, we incorporate annotations that include per-word disfluency with timestamps. Our approach strictly adheres to the procedures outlined in [23] and employs Vicuna instruction tuning [105] with LoRA [106]. As this is not our core contribution, we provide details in Appendix A.8. We use the same autoregressive training objective as [23], denoted as LLAN. The overall loss objective for SSDM is shown in Eq. 15.\n$L_{SSDM} = L_{VAE} + L_{CSA} + L_{LAN}$ (15)"}, {"title": "5 Libri-Dys: Open Sourced Dysfluency Corpus", "content": "Traditional rule-based simulation methods [1, 37, 50] operate in acoustic space, and the generated samples are not naturalistic. We developed a new pipeline that simulates in text space. To achieve this, we first convert a sentence into an IPA phoneme sequence. Then, we develop TTS rules for phoneme editing to simulate dysfluency, providing five types of dysfluency: Repetition(phoneme & word), Missing(phoneme & word), Block, Replacement and Prolongation. These rules are applied to the entire LibriTTS dataset [107], allowing the voice of generated speech to vary from the 2456 speakers included in the LibriTTS. The TTS-rules, entire pipeline, dataset statistics, MOS evaluation and phoneme recognition results are available in Appendix A.9. Overall Libri-Dys is 7X larger than LibriTTS, with a total size of 3983 hours. Data is opensourced at https://bit.ly/4aoLdWU."}, {"title": "6 Experiments", "content": "6.1 Data Setup\nFor training, we use VCTK++[1] and Libri-Dys datasets. For testing, we randomly sample 10% of the training data. Additionally, we incorporate nfvPPA[108] data from our clinical collaborations, which includes 38 participants\u2014significantly more than the 3 speakers in prior studies [1, 2]. It is approximately 1 hour of speech. Further details are provided in Appendix A.10.1.\n6.2 Experiments Setup\nThe neural gestural VAE (Eq.8), CSA (Eq.14), and language modeling components are trained sequentially, with each stage completed before the next begins. Subsequently, we perform end-to-end learning to implement curriculum learning. Our objective is to evaluate the dysfluent intelligibility and scalability of our proposed gestural scores, as well as the dysfluency detection performance of each proposed module. Detailed training configurations can be found in Appendix A.12.\n6.3 Scalable Intelligibility Evaluation\nWe evaluate phonetic transcription (forced alignment) performance using simulated data from VCTK++[1] and our proposed Libri-Dys dataset. The framewise F1 score and dPER[1] are used as evaluation metrics. Five types of training data are used: VCTK++, LibriTTS (100%, [107]), Libri-Dys (30%), Libri-Dys (60%), and Libri-Dys (100%). HuBERT [109] SSL units and H-UDM alignment (WavLM [96]) fine-tuned with MFA [81] targets are adopted. Additionally, we examine Gestural Scores (GS). GS-only refers to gestural VAE training (Eq.1), GS w/o dist excludes Ldist, and GS w/ dist includes it, following Eq.8. Results are presented in Table 1. H-UDM consistently outperforms HuBERT due to the WavLM backbone. Gestural scores from Eq. 1 show inferior results due to sparse sampling. However, GS demonstrates better scalability compared to SSL units. Using phoneme alignment loss Lphn significantly increases intelligibility, matching SSL unit results. GS outperforms SSL units with more training data. The inclusion of the self-distillation objective yields the best performance and scalability. Scaling factors SF1 for F1 score and SF2 for dPER are computed as (c-b) x 0.3 + (b \u2212 a) \u00d7 0.4 for results [a, b, c] from Libri-Dys [30%, 60%, 100%]. In terms of intelligibility, Gestural Score delivers the best scalability.\n6.4 Scalable Dysfluency Evaluation\nWe follow [2] by using F1 (type match) and MS (matching score). The matching score is defined as follows: if the IoU (Intersection over Union) between the predicted time boundary and the annotations is greater than or equal to 0.5, and the type also matches, it is considered detected. We use H-UDM [2], the current state-of-the-art time-aware dysfluency detection model, as the baseline. Under our SSDM framework, we include several ablations: (1) We remove LLaMA and use a template matching"}, {"title": "7 Limitations and Conclusions", "content": "In this work, we proposed SSDM (Scalable Speech Dysfluency Modeling), which outperforms the current best speech understanding systems by a significant margin. However, there are still several"}]}