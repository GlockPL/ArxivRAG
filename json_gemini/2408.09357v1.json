{"title": "Meta-Learning Empowered Meta-Face: Personalized Speaking Style Adaptation for Audio-Driven 3D Talking Face Animation", "authors": ["Xukun Zhou", "Fengxin Li", "Ziqiao Peng", "Kejian Wu", "Jun He", "Biao Qin", "Zhaoxin Fan", "Hongyan Liu"], "abstract": "Audio-driven 3D face animation is increasingly vital in live streaming and augmented reality applications. While remarkable progress has been observed, most existing approaches are designed for specific individuals with predefined speaking styles, thus neglecting the adaptability to varied speaking styles. To address this limitation, this paper introduces MetaFace, a novel methodology meticulously crafted for speaking style adaptation. Grounded in the novel concept of meta-learning, MetaFace is composed of several key components: the Robust Meta Initialization Stage (RMIS) for fundamental speaking style adaptation, the Dynamic Relation Mining Neural Process (DRMN) for forging connections between observed and unobserved speaking styles, and the Low-rank Matrix Memory Reduction Approach to enhance the efficiency of model optimization as well as learning style details. Leveraging these novel designs, MetaFace not only significantly outperforms robust existing baselines but also establishes a new state-of-the-art, as substantiated by our experimental results.", "sections": [{"title": "Introduction", "content": "Audio-driven 3D talking face animation has become increasingly prevalent in various sectors, including gaming (Lin, Yuan, and Zou 2021), live streaming (Hu et al. 2021b), and animation production (Richard et al. 2021). Leveraging advanced technologies such as 3D parametric models (Peng et al. 2023b), Neural Radiance Fields (Peng et al. 2024), and Gaussian splatting (Cho et al. 2024), these methods have achieved significant success in achieving accurate lip synchronization and facial emotions. Nonetheless, the intricate relationship between facial expressions and accompanying audio still needs to be explored. In other words, the problem of speaking style adaptation still needs to be addressed.\nSeveral methods have been proposed to address this issue in recent years (Cudeiro et al. 2019; Fan et al. 2022; Peng et al. 2023b,a). For instance, SelfTalk (Peng et al. 2023a) introduces a self-training pipeline that leverages text and 3D lip meshes for speaking style adaptation. Meanwhile, FaceFormer (Fan et al. 2022) focuses on training different individuals with predefined speaking styles. Although these approaches mark some progress, they still face significant challenges: 1) Firstly, most methods require a substantial amount of data (Fan et al. 2022; Song et al. 2024) for effective speaking style distillation. 2) Secondly, the prevalent use of cross-training technologies (Peng et al. 2023b; Chai et al. 2024) for speaking style adaptation necessitates paired sentences, reducing flexibility in application. Therefore, a pertinent question emerges: Is it possible to devise a flexible and straightforward method for speaking style adaptation in audio-driven 3D face animation that utilizes minimal data?\nTo answer this question, we introduce MetaFace, a novel framework designed explicitly for Personalized Speaking Style Adaptation in Audio-Driven 3D Talking Face Animation. The core idea of MetaFace, illustrated in Fig. 1(b), is grounded in the principle that Audio-Driven 3D talking face animation can significantly benefit from the foundational concepts of meta-learning. Specifically, to facilitate rapid adaptation with minimal data, MetaFace consists of a Robust Meta Initialization Stage (RMIS) and a Low-rank Matrix Memory Reduction Approach for speaking style adaptation. In MetaFace, the RMIS is utilized to initialize the network weights from a pre-trained 3D talking face animation model, infusing the network with basic style cues of the target individual. Subsequently, the latter employs the renowned LORA (Hu et al. 2021a) fine-tuning strategy to equip the model with personalized details quickly. Furthermore, acknowledging the challenges that few-shot training often faces in handling training and test data from different domains separately, MetaFace introduces a Dynamic Relation Mining Neural Process (DRMP) to enable the model to establish additional connections between training and testing samples, thereby further enhancing the performance of speaking style adaptation.\nExtensive experiments are conducted using the widely recognized VOCASet (Cudeiro et al. 2019) and BIWI (Fanelli et al. 2010) datasets. The experimental outcomes demonstrate that MetaFace outperforms existing state-of-the-art methods, showcasing its superior performance in the field.\nOur contributions can be summarized as follows:\n\u2022 We introduce MetaFace, a novel framework designed explicitly for Personalized Speaking Style Adaptation in Audio-Driven 3D Talking Face Animation, leveraging the foundational principles of meta-learning.\n\u2022 We develop key components within MetaFace, including the Robust Meta Initialization Stage, the Dynamic Relation Mining Neural Process, and the Low-rank Matrix Memory Reduction Approach, collectively enhancing the framework's performance to achieve state-of-the-art results.\n\u2022 We rigorously evaluate our method against existing techniques, demonstrating its superior effectiveness through extensive experiments conducted on several widely recognized datasets."}, {"title": "Related Work", "content": "This paper primarily focuses on audio-driven 3D talking face animation, explicitly emphasizing the problem of personalized speaking style adaptation. To this end, this section first reviews related works concerning Lip Synchronization in 3D Face Animation. Subsequently, it discusses contributions in Style Speaking Adaptation in 3D Face Animation. Finally, we explore the role of Meta-learning and Parameter Adaptation, which serve as foundational technologies in the MetaFace framework."}, {"title": "Lip Synchronization in 3D Face Animation", "content": "For most 3D face animation techniques, the primary objective remains the achievement of precise lip synchronization. Early methods (Ezzat and Poggio 1998, 2000) primarily rely on audio produced by Text-to-Speech technologies (Black et al. 1998) and real-life facial motion videos animated using visemes (Fisher 1968) to extract features synchronized with audio. Although these methods are relatively straightforward, they depend heavily on meticulously organized datasets, posing significant challenges in creating animations tailored to individual users. To overcome these limitations, deep learning-based approaches (Karras et al. 2017; Fu et al. 2024; Peng et al. 2023b; Dan\u011b\u010dek et al. 2023; Peng et al. 2023a) have been introduced. Karras et al. (2017) pioneers using neural networks to generate mesh movements directly from audio features. Subsequently, Richard et al. (2021) introduces a method that separates audio-correlated and uncorrelated information, yielding more realistic animation performance. Rather than investigating audio features, Fan et al. (2022) focuses on the prediction mode, enhancing the efficiency of transformer models in facial motion prediction by employing an auto-regressive model. Building on this foundation, Xing et al. (2023) further develops a code query-based approach to speech-driven 3D facial animation, improving realism by minimizing mapping uncertainties and outperforming existing methodologies. Recently, inspired by lip reading techniques, Peng et al. (2023a) further enhances the accuracy of lip movements by integrating audio, textual content, and lip shapes, achieving state-of-the-art results.\nAlthough the methods mentioned above have achieved remarkable progress, they primarily focus on improving lip synchronization, while the equally important question of personalized speaking style adaptation has been largely overlooked. In our work, we introduce MetaFace to address this issue in both an effective and efficient manner."}, {"title": "Speaking Style Adaptation in 3D Face Animation", "content": "Recent advancements in 3D Face Animation have increasingly focused on adapting speaking styles. Studies primarily explore two approaches: utilizing external personality labels (Cudeiro et al. 2019; Tian, Yuan, and Liu 2019; Fan et al. 2022; Peng et al. 2023b), and decoupling facial features from audio (Chai et al. 2024; Peng et al. 2023b). The first approach employs personality labels to foster the learning of personalized embeddings, exemplified by Cudeiro et al. (2019), who created a personalized dataset and a model to integrate individual styles. This foundation has spurred methods that incorporate emotional aspects (Dan\u011b\u010dek et al. 2023) or specific speaking styles (Thambiraja et al. 2023; Fan et al. 2022; Song et al. 2024). The second approach focuses on disentangling speaking style from audio features, with innovations like cross-sample training to separate emotional and content features (Peng et al. 2023b; Chai et al. 2024; Fu et al. 2024). Zhang et al. (2021) and Song et al. (2024) further enhance this by utilizing identity and facial motions as additional training conditions, improving model performance even with minimal data (Thambiraja et al. 2023).\nAlthough these approaches mark significant progress, they still face substantial challenges. Firstly, most methods necessitate a considerable amount of data for effective speaking style distillation. Additionally, the prevalent use of cross-training technologies for speaking style adaptation requires paired sentences, reducing application flexibility. MetaFace addresses these issues by leveraging meta-learning and low-rank fine-tuning principles."}, {"title": "Meta-learning and Parameter Adaptation", "content": "Meta-learning is a branch of machine learning methods that enables deep models to adapt to novel categories with minimal training data (Hospedales et al. 2021). The concept of meta-learning was first introduced by Wirth and Perkins (2008), which explored adapting the learning rate for novel category adaptation. Subsequently, MAML (Finn, Abbeel, and Levine 2017) proposed a model-agnostic meta-learning approach for fast adaptation by utilizing the concept of learning to learn. Following this, numerous works have been proposed to enhance this line of research from diverse perspectives (Huisman, Van Rijn, and Plaat 2021). In parallel, advancements in model pretraining have significantly influenced meta-learning. Initially introduced by Devlin et al. (2018), pretraining models on large datasets and fine-tuning them on smaller tasks has been influential across various domains (Wang et al. 2022; Du et al. 2022). However, with the advent of large language models like GPT-3 and GPT-4 (Brown et al. 2020; Achiam et al. 2023), fine-tuning entire models for small tasks has become computationally prohibitive. To address this limitation, Hu et al. (2021a) proposes a solution by decomposing model parameters into two low-rank matrices, significantly reducing computational costs while maintaining performance. This approach, known as LoRA, has been widely adopted across various applications (Wang et al. 2024; Dettmers et al. 2024; Zhang, Rao, and Agrawala 2023).\nAlthough numerous meta-learning methods and parameter adaptation techniques have been proposed and widely applied across various domains, their potential in efficient 3D face animation still needs to be explored. In this paper, we adopt the concept of meta-learning and low-rank fine-tuning for speaking style adaptation in 3D talking face animation."}, {"title": "Method", "content": "This work explores the critical issue of personalized speaking style adaptation. Contrary to existing supervised learning frameworks such as those proposed by (Fan et al. 2022), (Peng et al. 2023b), and (Song et al. 2024), which delineate the mapping relationship between audio and facial animation for all speakers within a dataset, our study advocates for a \"meta-face\" methodology, tailored explicitly for superior adaptation to novel individuals.\nSpecifically, we first examine an audio-driven 3D facial animation dataset assembled from a diverse group of individuals, denoted as $P = \\{1,2,\\cdots, p,\\cdots,|P|\\}$, where $p$ identifies the $p$-th participant. For each individual $p$, the dataset encapsulates observed audio-face pairings, represented as $D_p = \\{(a_{p,m}, v_{p,m}), m \\in \\{1,2,\\ldots,|D_p|\\}\\}$. Here, $a_{p,m} \\in \\mathbb{R}^{T_a}$ specifies the audio sequence of the individual, spanning a duration of $T_a$. Concurrently, $v_{p,m} \\in \\mathbb{R}^{T_v \\times L \\times 3}$ describes the corresponding facial motion sequence, aligned to a reference template face $T \\in \\mathbb{R}^{L \\times 3}$. The parameters $T_v, L$, and $3$ denote the length of the facial sequence, the number of facial landmarks, and the spatial coordinates (x, y, z), respectively. The collective dataset is represented as $D = \\{D_p, p \\in P\\}$.\nThe final objective is to construct a model, denoted by $f_\\theta$, by leveraging the samples from the dataset $D$. This model is subsequently refined through fine-tuning with the personalized dataset $P_l$. This adaptation process enhances the model's efficacy for on the dataset specific to individual $l$. More formally, for a given task $i$, the query and support set are defined as $w_i = \\{(a_k, v_k), k \\in \\{1,2,\\ldots, K_i\\}\\}$, with $K_i$ representing the count of samples within $w_i$.\n$\\theta^* = \\arg \\min_{\\theta} \\sum_{(a,v) \\in D} \\mathcal{L}(f_\\theta(a), v) \\quad (1)$\nUpon obtaining the optimal parameters $\\theta^*$, the model is further personalized as follows:\n$\\theta_l = \\arg \\min_{\\theta} (\\sum_{(a,v) \\in P_l} \\mathcal{L}(f_\\theta(a), v) + \\lambda (1 - \\lambda)\\mathcal{L}(f_{\\theta^*}(a), v))\\quad (2)$\n, where $\\lambda$ is a tuning parameter that balances the influence of the personalized data $P_l$ against the pre-trained model parameters. Fig. 2 demonstrates the operational framework of MetaFace. Starting with a pre-trained 3D talking face animation model alongside a minimal training dataset, MetaFace initially engages the Robust Meta Initialization Stage to expedite the model weight updates. This stage allows the model to rapidly assimilate basic cues from a new individual efficiently. Additionally, this phase incorporates a Dynamic Relation Mining Neural Process into the 3D face animation model. This enhancement facilitates the model's ability to discern the domain disparities between previously modelled individuals and the new subject and to unearth intrinsic relationships, thereby promoting model adaptability to the speaking style domain of the new individual. Subsequently, the primed deep model undergoes refinement through the Low-rank Matrix Memory Reduction Approach. This method enhances the LoRA (Hu et al. 2021a) technique, further aiding the model in acquiring intricate details of novel speaking styles.\nThe following subsections delve into the intricacies of the Robust Meta Initialization Stage, the Dynamic Relation Mining Neural Process, and the Low-rank Matrix Memory Reduction Approach."}, {"title": "Robust Meta Initialization Stage", "content": "To equip a pretrained 3D talking face animation model with the capability to learn an individual's unique speaking style, we begin with the Robust Meta Initialization Stage. This stage updates the model's weights to produce an initial weight configuration. We define the audio input as $a$ and the face animation bias as $v$, while the speaking feature distribution for person $l$ is represented as $p(l)$.\nGiven that traditional training approaches, which compute the loss using the model $f_\\theta$ over the entire batch, may not adequately address unseen speaking styles, we employ a strategy inspired by Finn, Abbeel, and Levine (2017). This involves training a meta face model as a robust initialization for adapting to new samples.\nFor a new individual $l$, we construct a person-specific dataset $D_l$ containing $K_l$ samples drawn from the distribution of speaking features of person $l$. Each sample consists of audio $a$ and face motion bias $v$. The model is initially updated using the gradients calculated from the performance of model $\\theta$ on these $K_l$ samples, as indicated in Equation 3:\n$\\theta_p = \\theta_l - \\alpha v \\sum_i \\mathcal{L}(f_{\\hat{\\theta}_l}(a_i, l), v_i) \\quad (3)$\nThe parameters $\\theta_p$, updated locally, are deemed optimal for the dataset $D_p$. To verify the precision of these parameters, we resample another $K$ observations $\\phi'$ from $p(l)$, which are used as a query set to compute the gradient through the loss $\\mathcal{L}(f_{\\hat{\\theta}}(a', l), V')$. Subsequently, the original model parameters $\\theta$ are refined using the following update rule:\n$\\theta = \\theta \u2013 \\beta \\nabla_\\theta \\sum_{l=1}^K \\mathcal{L}(f_{\\theta_p}(a_l, l), v_l) \\quad (4)$\n,where $\\beta$ is the learning rate. Detailed procedures of metafce algorithm are delineated in Algorithm 1."}, {"title": "Low-rank Matrix Memory Reduction Approach", "content": "Following the Robust Meta Initialization Stage, the subsequent phase involves the meticulous model fine-tuning. This crucial step enables the model to assimilate additional, intricate details specific to each individual. The fine-tuning process is pivotal as it refines the model's ability to capture and replicate the nuanced characteristics inherent in the speaking style of the subject, thereby enhancing the overall adaptability and effectiveness of the model. Given the impracticality of maintaining a fully adaptive model for each individual, we adopt a strategy that leverages a low-rank decomposition approach, as suggested by Hu et al. (2021a), to reduce the memory footprint significantly. Specifically, for the linear layers' weight matrix $X \\in \\mathbb{R}^{w\\times o}$ within the model parameters $\\theta$, we apply a low-rank decomposition. Traditionally, updates to the linear parameters are performed as $X = X + \\Delta X$. However, in our approach, these updates are replaced by $X = X + \\Delta BA$, where $B \\in \\mathbb{R}^{w\\times u}$ and $A \\in \\mathbb{R}^{u\\times o}$ are the low-rank matrices and $u$ denotes the predefined dimensions, significantly less than $w$ or $o$.\n$X_{new} = X + \\Delta BA \\quad (5)$\nFor the entire model's parameters $\\theta$, we construct a low-rank product $\\theta_m$ to update the personality network. To enhance the adaptive capacity of the model concerning individual speaking styles, we propose that the LoRA parameters $\\theta_m$ should also be concurrently trained with $\\theta$ during the meta-learning phase. This dual training process involves $\\theta$ and $\\theta_m$ when training the meta face.\n$\\theta_{final} = Train(\\theta, \\theta_m) \\quad (6)$\nNotably, while traditional approaches typically initialize the LoRA parameters randomly during fine-tuning, we innovate by initializing $\\theta_m$ using meta-learning techniques. This initialization contributes to memory reduction and enhances the model's ability to capture detailed nuances of the speaking style. During the fine-tuning phase on a specific individual, only the $\\theta_m$ parameters are updated, thereby maintaining a low memory footprint while fine-tuning to adapt to the unique characteristics of the individual's speaking style."}, {"title": "Dynamic Relation Mining Neural Process", "content": "In the Robust Meta Initialization Stage, as detailed in Section, the updating rule considers each individual a distinct task. This approach inadvertently neglects the potential facial correlations among individuals, where similarly characterized persons might exhibit akin facial movements. Harnessing these correlations could significantly bolster the training of the meta-parameters. We have introduced the Dynamic Relation Mining Neural Process to tap into and exploit these correlations.\nSpecifically, for each task $w_i = \\{D_{i,l} | l \\in S_i \\cup Q_i\\}$, we postulate that $w_i$ emanates from a stochastic process $h_i$, with each data point $(a_k, v_k)$ epitomizing a sample from $h_i$. The conditional probability distribution $p(v_{1:K_i}|a_{1:K_i})$ can be delineated as follows:\n$p(v_{1:K_i}|a_{1:K_i}) = \\int p(h_i) p(v_{1:K_i}|a_{1:K_i}, h_i) d h_i \\quad (7)$\nwhere $a_{1:K_i}$ and $v_{1:K_i}$ represent all samples $(a_k, v_k)$ from task $w_i$, respectively, and $K_i = |w_i|$ denotes the count of samples in $w_i$. The Neural Process approximates the stochastic process $h_i$ using a random vector $p(z_i)$, reformulating the equation as:\n$p(v_{1:K_i}|a_{1:K_i}) = \\int p(z_i) \\prod_{k=1}^{K_i} p(v_k | a_k, z_i) d z_i \\quad (8)$\nGiven the intractability of the posterior distribution, the Neural Process employs variational inference, introducing a variational posterior $q(z_i | w_i)$. The Evidence Lower-Bound (ELBO) is thus derived as:\n$\\log p(v_{1:K_i}|a_{1:K_i}) \\geq \\mathbb{E}_{q(z_i|w_i)}[\\sum_{k=1}^{K_i} \\log p(v_k | a_k, z_i)] + \\mathbb{E}_{q(z_i|w_i)}[\\log \\frac{p(z_i)}{q(z_i|w_i)}] \\quad (9)$\nFor simplicity, let us assume $p(z_i) \\sim \\mathcal{N}(0, I)$. Considering each task $w_i$ is comprised of a support set $S_i$ and a query set $Q_i$, the ELBO is reformulated to enhance training efficiency:\n$\\log p(v_{Q_i}|a_{Q_i}, v_{S_i}, a_{S_i}) \\geq \\mathbb{E}_{q(z_i|Q_i)}[\\sum_{k \\in Q_i} \\log p(v_k | a_k, z_i)] + K L(q(z_i|Q_i)||q(z_i|S_i))\\quad (10)$\nHere, the first term is interpreted as the reconstruction loss for 3D facial animation, and the second term serves as a regularization term, fostering consistency between the support set and the query set."}, {"title": "Loss Function", "content": "To train MetaFace, we employ three distinct loss functions: a reconstruction loss, a velocity loss, and a Lnp Loss.\nThe reconstruction loss measures the distance between the predicted facial motion bias $\\mathcal{V}_{pred}$ and the ground truth $\\mathcal{V}_{gt}$, as shown in Equation 11.\n$\\mathcal{T}_{recon} = \\frac{1}{N} \\sum_{n=1}^{N} (\\mathcal{V}_{pred,n} - \\mathcal{V}_{gt,n})^2 \\quad (11)$\nTo minimize jittery outputs, following Peng et al. (2023a), we use a velocity term to ensure the model learns the facial motion velocity:\n$\\mathcal{T}_{vel} = \\frac{1}{T-1} \\sum_{t=2}^{T} \\frac{1}{N} \\sum_{n=1}^{N} ((\\mathcal{V}_{t}^{pred,n} - \\mathcal{V}_{t-1}^{pred,n}) - (\\mathcal{V}_{t}^{gt,n} - \\mathcal{V}_{t-1}^{gt,n}))^2 \\quad (12)$\nFor the Lnp loss, as previously discussed, the loss function $\\mathcal{T}_{lnp}$ can be formulated as:\n$\\mathcal{T}_{lnp} = \\sum_i p(z_i|a_{q}) \\log \\frac{p(z_i|a_{q}, a_{q})}{p(z_i|a_{S})} \\quad (13)$\nThe total loss function can be formulated as:\n$\\mathcal{T} = w_1\\mathcal{T}_{recon} + w_2\\mathcal{T}_{vel} + w_3\\mathcal{T}_{lnp} \\quad (14)$\nwhere $w_1, w_2$, and $w_3$ are hyperparameters that indicate the weight of each corresponding loss component."}, {"title": "Experiments", "content": "To assess the efficacy of MetaFace, we conduct experiments using two publicly recognized datasets: VOCASet (Cudeiro et al. 2019) and BIWI (Fanelli et al. 2010). VOCASet includes 480 sentences, each lasting between three to four seconds, captured from 12 subjects using 4D scans at a rate of 60 fps. The BIWI dataset contains 15,000 frames featuring 20 subjects engaged in speech, captured at 25 fps. We sample the audio at 16 kHz and extract audio features using the wav2vec model (Baevski et al. 2020). In our meta-learning framework, we select an 11-way 1-shot pretraining strategy. The global learning rate is set at $\\alpha = 1 \\times 10^{-4}$, and the fine-tuning learning rate is $\\beta = 5 \\times 10^{-5}$. Loss weights are assigned as follows: $w_{recon} = 1000$, $w_{vel} = 1000$, and $w_{Lnp} = 10$. During the task-specific fine-tuning stage, we use four samples from the training sentences of the test subjects, following the methodology described in the imitator model. For evaluation, we measure the L2 vertex error across the entire face ($l2_{face}$) and within the lip regions ($l2_{lip}$) to assess the accuracy of the facial animations. Additionally, we employ Dynamic Time Warping (DTW) to evaluate lip synchronization ($lipsync$) as per the method outlined by Thambiraja et al. (2023). We also gauge performance using the mean of the maximum error per frame ($lip_{max}$), following the approach described by Richard et al. (2021)."}, {"title": "Quantitative Evaluation", "content": "We compare MetaFace with leading-edge methodologies such as VOCA (Cudeiro et al. 2019), FaceFormer (Fan et al. 2022), SelfTalk (Peng et al. 2023a), StyleTalk (Song et al. 2024), and Imitator (Thambiraja et al. 2023). For VOCA, it is trained on the BIWI dataset employing the official implementation. FaceFormer, SelfTalk, and StyleTalk undergo external experiments, specifically fine-tuned on four sentences arbitrarily selected from the training set of the test subjects, in alignment with the practices delineated by Thambiraja et al. (2023). Similarly, MetaFace is trained on four samples from the training set of the test subjects. As delineated in Table 1, MetaFace outperforms all compared methods in terms of reduced facial animation errors and enhanced lip synchronization. These exemplary results signify substantial enhancements in both overall facial movements and lip motions. More precisely, MetaFace registers a 31% reduction in facial motion distance and a 9.2% reduction in lip synchronization distances on the VOCASet, surpassing the personalized talking face generation benchmarks set by Thambiraja et al. (2023). On the BIWI dataset, MetaFace achieves a 30% reduction in whole face motion error and a 22% improvement in lip synchronization error."}, {"title": "Qualitative Evaluation", "content": "In addition to the quantitative comparisons, we also present a series of visualization results in Fig.3. We conduct comparisons of our model, MetaFace, against established methods such as VOCA, FaceFormer, SelfTalk, and Imitator on the VOCASet, and with FaceFormer, SelfTalk, and TalkingStyle on the BIWI dataset. Each method is fine-tuned using four samples from the target individual. The images used for this comparative analysis are extracted from videos included in our supplementary materials. As illustrated in Fig.3, MetaFace evidently surpasses the competing methods in terms of visual outcomes. Notably, when articulating the sound /\u0259u/, MetaFace exhibits the most lifelike facial movements, with the mouth forming a semicircle and slightly protruding forward. For closed syllables such as /pe/ and /pe/, the movements of the mouth shapes with MetaFace are markedly more restrained compared to other methods. Similarly, for open syllables like /ai/ and /ae/, our method aligns more closely with the ground truth results"}, {"title": "User study", "content": "To explore the practical efficacy of 3D face animation, we conduct a user study using existing datasets, adhering to the methodologies established by FaceFormer (Fan et al. 2022), Imitator (Thambiraja et al. 2023), and SelfTalk (Peng et al. 2023a). In this study, participants view videos from each method side-by-side and select the most realistic animation. We compute the support ratio for each method to quantify preference. MetaFace is compared against SelfTalk and Imitator, which are recognized as leading models in personalized talking face animation and lip synchronization. As depicted in Table 2, our method surpasses other models in terms of lip synchronization and overall realism. Notably, MetaFace achieves a support ratio of 66.4% against Imitator on the VOCASet, emphasizing its superior performance in realistic facial animation."}, {"title": "Ablation Study", "content": "In this section, we conduct an ablation study to demonstrate the effectiveness of our key design components. The experiments are conducted on the VOCASet, with results presented in Table 3.\nImpact of the Robust Meta Initialization Stage: The primary objective of MetaFace is to learn a meta face that serves as an initialization for all faces. As illustrated in the third row of Table 3, the robust meta initialization contributes significantly to the overall performance. Specifically, this meta-learning approach results in a 24.4% decrease in 12face and a 25.7% decrease in lipsync.\nImpact of the Dynamic Relation Mining Neural Process: This component enables MetaFace to discern the relationship between observed speaking styles and unobserved ones. As shown in Table 3, the contribution of the Neural Process primarily leads to a 14.4% reduction in the maximum 12 distance within the lip region and an 11% reduction in lip synchronization distance.\nImpact of the Low-rank Matrix Memory Reduction Approach As indicated in the first row of Table 3, the model excluding LoRA (Low-Rank Adaptation) achieves the most favorable outcomes. While utilizing the full model parameters delivers optimal performance, it concurrently necessitates a hundredfold increase in model parameter training requirements. The integration of the LoRA model substantially reduces trainable parameters by approximately 99.5%, with a modest performance decrease not exceeding 7%. From the second row of Table 3, it is evident that meta-learning plays a crucial role in enhancing outcomes. Meta training of the low-rank decomposition module improves lip synchronization by 17% and reduces face motion error by 16%."}, {"title": "Generalization Test towards Adaptation Samples", "content": "We also investigate the generalization ability of our model concerning the number of adaptation samples. In our experiments, we vary the number of seen sample sequences and train the model with randomly selected sentences. Results, shown in Table 4, indicate that increasing the sample count significantly reduces the 12face error. Specifically, using four sentences decreases the error by up to 36%, demonstrating that a greater number of samples notably improves the model's generalization capability in facial feature reconstruction."}, {"title": "Conclusion", "content": "In this paper, we have introduced MetaFace, a novel model for audio-driven 3D face animation that addresses the challenge of personalized speaking style adaptation. MetaFace comprises three key components: the Robust Meta Initialization Stage (RMIS), which facilitates fundamental speaking style adaptation; the Dynamic Relation Mining Neural Process (DRMN), which establishes connections between observed and unobserved speaking styles; and the Low-rank Matrix Memory Reduction Approach, which enhances the efficiency of model optimization and the learning of style details. By integrating these novel designs, MetaFace not only significantly surpasses robust existing baselines but also sets a new benchmark in the field."}]}