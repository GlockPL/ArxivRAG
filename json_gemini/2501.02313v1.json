{"title": "DiffGraph: Heterogeneous Graph Diffusion Model", "authors": ["Zongwei Li", "Lianghao Xia", "Hua Hua", "Shijie Zhang", "Shuangyang Wang", "Chao Huang"], "abstract": "Recent advances in Graph Neural Networks (GNNs) have revolutionized graph-structured data modeling, yet traditional GNNS struggle with complex heterogeneous structures prevalent in real-world scenarios. Despite progress in handling heterogeneous interactions, two fundamental challenges persist: noisy data significantly compromising embedding quality and learning performance, and existing methods' inability to capture intricate semantic transitions among heterogeneous relations, which impacts downstream predictions. To address these fundamental issues, we present the Heterogeneous Graph Diffusion Model (DiffGraph), a pioneering framework that introduces an innovative cross-view denoising strategy. This advanced approach transforms auxiliary heterogeneous data into target semantic spaces, enabling precise distillation of task-relevant information. At its core, DiffGraph features a sophisticated latent heterogeneous graph diffusion mechanism, implementing a novel forward and backward diffusion process for superior noise management. This methodology achieves simultaneous heterogeneous graph denoising and cross-type transition, while significantly simplifying graph generation through its latent-space diffusion capabilities. Through rigorous experimental validation on both public and industrial datasets, we demonstrate that DiffGraph consistently surpasses existing methods in link prediction and node classification tasks, establishing new benchmarks for robustness and efficiency in heterogeneous graph processing. The model implementation is publicly available at: https://github.com/HKUDS/DiffGraph.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning with heterogeneous graphs has emerged as a pivotal paradigm in modern data science, reflecting the complexity of real-world systems. Unlike homogeneous graphs with uniform node and edge types, heterogeneous graphs encapsulate rich, multi-faceted interactions among diverse entities, enabling more expressive data representations. These advanced graph structures have demonstrated remarkable effectiveness across numerous domains, from bibliographic academic data [16, 39], medical data [5], to recommender systems [44]. The fundamental objective lies in leveraging this inherent diversity to enhance various analytical tasks. By harnessing these heterogeneous relationships, researchers have achieved significant breakthroughs in node classification [33], link prediction [8], and graph classification [31], consistently demonstrating superior performance compared to traditional homogeneous approaches.\nRecent years have witnessed remarkable advancements in heterogeneous graph neural networks (HGNNs). Through relation-aware message passing frameworks, earlier study [49] has improved graph learning tasks by effectively capturing both complex structural patterns and diverse semantic information within heterogeneous graphs. Notable research works include HGT [33], which leverages graph attention mechanisms to dynamically assess the importance of various heterogeneous paths, thereby enhancing both semantic-level and node-level representation learning. Similarly, HeteGNN [49] introduces a framework that simultaneously models heterogeneous structures and their associated content features. Building upon the success of self-supervised learning (SSL) [26, 46] in addressing data scarcity and noise challenges, recent research efforts [23, 33, 40] have effectively incorporated SSL techniques to further advance heterogeneous graph learning capabilities.\nDespite significant advances in heterogeneous graph learning, two critical challenges remain insufficiently addressed. First, current HGNN approaches demonstrate limited capability in handling"}, {"title": "2 METHODOLOGY", "content": "This section outlines the details of the proposed DiffGraph framework. The architecture of DiffGraph is illustrated in Figure 1."}, {"title": "2.1 Heterogeneous Graph Learning", "content": "Heterogeneous Graphs. In a heterogeneous graph, the node set is denoted as V = {vi}, with the set of node types denoted as V. There exists a node type mapping function \u03c6 : V \u2192 V, where \u03c6(vi) represents the type of node vi. Similarly, the set of edges is denoted as & = {(vi, vj)}, with an edge type mapping function \u03c8 : & \u2192 E. Here, E denotes the set of edge types, and \u03c8(vi,vj) represents the type of edge (vi, vj). With these notations, we formally define the heterogeneous graph as G = (V, &, \u03c6, \u03c8). For convenience, a heterogeneous graph is recorded by the binary adjacency matrix A of size |V|\u00d7 |V|\u00d7 |E|. An entry $a^r_{ij}$ \u2208 A equals 1 if there is a link of type r \u2208 E between nodes vi and vj, otherwise $a^r_{ij}$ = 0.\nHeterogeneous Graph Prediction. A predictive model f for heterogeneous graphs can be divided into an encoding phase and a prediction phase, formally as: f(G) = Predo Enc(G). The encoding phase Enc(\u00b7) learns d-dimensional hidden representations E \u2208 R|V|\u00d7d for nodes vi \u2208 V, while the prediction phase Pred(\u00b7) generates task-specific predictions based on the learned embeddings. Typical graph forecasting tasks include node classification and link prediction. For node classification, the prediction network Pred(\u00b7) of DiffGraph employs a multilayer perceptron (MLP) to infer nodes' classes from their learned embeddings. For link prediction, the dot-product operator is utilized to infer the existence of edges based on the embeddings of the connected nodes in Pred(\u00b7).\nMotivated by the simplicity and effectiveness of Graph Convolutional Networks (GCNs) [12], our DiffGraph adopts a GCN-based encoding process for Enc(\u00b7), which is formally defined as follows:\n$E = Pooling({E_r|r \u2208 E}), E_r = \u03a3_{l=1}^L E_{r,l}$ \n$E_{r,1} = norm(\u03b4(D^{-1/2}A'D^{-1/2}E_{r,l-1}))$  (1)\nwhere $A'$ \u2208 R|V|\u00d7|V| denotes the specific adjacency matrix for each heterogeneous relationr \u2208 E, and D denotes its corresponding diagonal degree matrix. Er.1 \u2208 R|V|\u00d7d represents the embedding"}, {"title": "2.2 Cross-view Heterogeneous Graph Denoising", "content": "2.2.1 Denoising Auxiliary Graphs using Target Graph. To enhance the performance of heterogeneous graph prediction, we propose a cross-view denoising approach to maximally extract task-relevant information from heterogeneous graphs. Initially, we identify the task-relevant subgraph within the compound heterogeneous graph as the target graph Gt. An example of such a target graph is the purchase behavior graph between users and items in an e-commerce recommender system. Once this target graph Gt is removed from the entire graph G, the remaining graph is referred to as the auxiliary graph Gs (also known as the source graph).\nOur cross-view denoising framework for heterogeneous graphs aims to maximize the mutual information between the auxiliary view and the target view by learning a parametric function g that maps the auxiliary graph space to the task-relevant data space. This learnable denoising function g takes the source graph data Gs as the input and utilizes the target graph data Gt as labels. Specifically, this denoising framework can be formulated as follows:\n$argmin_{ef,eg} L_{main}(Y, f (G)) + \u03bb \u00b7 L_{deno}(G_t, g(G_s))$ (2)\nwhere $L_{main}$ represents the loss for the main prediction task, such as BPR loss [29] for link prediction and cross-entropy loss for node classification. Here, y denotes the task-specific labels. Meanwhile, $L_{deno}$ denotes the learning objective for our denoising function g, which measures the differences between the task-relevant target graph Gt and the denoising output of function g. The learning process tunes the parameter sets of and \u03b8g, which are parameters for the forecasting model f and the denoising network g, respectively. A weighting parameter \u03bb is used to balance the denoising task."}, {"title": "2.2.2 Latent Heterogeneous Graph Diffusion", "content": "Inspired by the impressive performance of diffusion models in capturing complex data generation processes across diverse data types [4], we propose to instantiate our denoising function as a heterogeneous graph diffusion model. This model aims to distill task-relevant information from the auxiliary graph by iteratively refining the graph data. Given the complexity of graph-structured data, we propose conducting this diffusion process in the latent representation space of the heterogeneous graph data. Specifically, our diffusion model aims to achieve the following step-by-step transformation:\n$G \u2194^\u03c0 ELE \u2190^{\u03c6'} E G \u2194^{\u03c0'} G^*$\nwhere \u03c0 and \u03c0' denote the bidirectional mapping between the graph-structured data and the latent representations. By utilizing the heterogeneous GCN-based encoder in the prediction function f, E captures heterogeneous semantic information from the source view. Function o represents the forward diffusion process, which incrementally adds noise to E. Correspondingly, \u03c6' refers to the reverse diffusion process, which removes the noise step by step.\nRationale for Latent Diffusion over Graph Diffusion. In the context of graph-based task learning, the role of the graph encoder is to aggregate the embeddings of neighboring nodes, transforming the structural semantics of the graph into low-dimensional information within the latent space. This characteristic suggests that by learning the denoising process \u03c6' within the latent space, our hidden-space diffusion model can effectively filter out the imprecise semantic differences within heterogeneous graphs, thereby better supporting the learning of the target graph.\nMoreover, graph data exhibits several challenging characteristics that hinder effective diffusion, including its sparse and discrete nature, as well as its large data space of exponential complexity. In contrast, the latent space is dense and continuous, and as a compressed representation of the graph, it usually has a smaller size. These key advantages motivate us to conduct graph diffusion in the latent representation space, instead of in the graph space."}, {"title": "2.2.3 Forward and Reverse Diffusion", "content": "Forward Process. The forward process of our diffusion model consists of T steps of gradual noise addition. This process begins with the encoded hidden representations, denoted as Ho = E. It then iteratively increases"}, {"title": "2.3 Model Training for DiffGraph", "content": "2.3.1 Diffusion Loss Function. To optimize the hidden-space diffusion module, we adopt the embedding Et of the target graph as the evidence lower bound (ELBO). Based on the data specific to each side, ho = Eu, Ev \u2208 Et, the paradigm for maximizing the"}, {"title": "3 EVALUATION", "content": "We evaluate the performance of our DiffGraph framework by studying the following Research Questions (RQs):\n\u2022 RQ1: How does the proposed DiffGraph framework perform on link prediction, node classification, and industry datasets?\n\u2022 RQ2: How effective are the designed modules in DiffGraph?\n\u2022 RQ3: How do different settings of key hyperparameters impact the graph prediction accuracy of our DiffGraph method?\n\u2022 RQ4: How is the efficiency of DiffGraph compared to baselines?\n\u2022 RQ5: How effectively can our DiffGraph approach perform to alleviate the issue of data sparsity for graph data?\n\u2022 RQ6: How well can DiffGraph handle noisy heterogeneous graphs?\n\u2022 RQ7: Can DiffGraph provide explanations in specific cases?"}, {"title": "3.1 Experimental Settings", "content": "3.1.1 Datasets. We evaluate DiffGraph on link prediction and node classification tasks. For link prediction, we utilize Tmall, Retailrocket, and IJCAI datasets. For node classification, we employ DBLP and AMiner (focusing on academic networks), along with an Industry dataset from a gaming platform. Detailed statistics are presented in Table 1, with dataset descriptions below.\nLink Prediction Dataset: Tmall: An E-commerce dataset with user views, favorites, cart additions, and purchases, filtered to users with \u22653 purchases [43]. Retailrocket: A dataset with user page views, cart additions, and transactions, where purchases are primary and other interactions auxiliary [44]. IJCAI: A dataset from IJCAI15 with heterogeneous user behaviors similar to Tmall.\nNode Classification Dataset: Industry: A game platform dataset with retention as target behavior and three auxiliary behaviors (purchases, friendships, tasks), forming user-item, user-user, and user-task graphs. DBLP: A dataset subset of authors categorized into"}, {"title": "3.2 Overall Performance Comparison (RQ1)", "content": "This section examines whether DiffGraph outperforms existing baselines in different graph tasks. The results are presented in Table 2. From the outcomes, we draw the following conclusions:\n\u2022 Superior performance of DiffGraph. Across both link prediction and node classification, DiffGraph consistently outperforms all baselines, demonstrating superior performance. These results underscore the advanced graph forecasting capabilities of the DiffGraph framework. Additionally, the performance advantages of DiffGraph on the industry dataset further validate its effectiveness in real-world applications. We attribute this success to the model's ability to eliminate inaccurate and spurious semantics from heterogeneous graph structures through the latent heterogeneous graph diffusion paradigm. This semantic refactoring process enables the model to learn more precise and meaningful heterogeneous relations, allowing DiffGraph to effectively mine valuable heterogeneous information and enhance specific tasks.\n\u2022 Effectiveness of heterogeneous relationships. The results indicate that methods based on heterogeneous graph learning [17,"}, {"title": "3.3 Ablation Study (RQ2)", "content": "To study the impact of DiffGraph's sub-modules, we remove or replace essential modules and evaluate the resulting performance. These ablated models include: i) -D: Omits the holistic diffusion module. ii) -U: Disregards user-side information of recommendation data. iii) -I: Similar to -U, eliminates item-side information. iv) -H: Excludes auxiliary heterogeneous graphs when training. v) DAE: Replace the diffusion module with a denoising autoencoder. The evaluation results for link prediction is listed in Figure 2. We also conduct ablation study for node classification on the Industry data. The results in terms of AUC, is as follows: -H, 0.7840, -D, 0.7901, DAE, 0.7911, DiffGraph, 0.8025. Through meticulous examination, we make the following noteworthy observations:\n\u2022 Removing the diffusion module \"-D\" leads to performance degradation, highlighting both the adverse effects of noise in auxiliary heterogeneous data and demonstrating the effectiveness of our latent feature-level diffusion model in its denoising function.\n\u2022 Comparing \"-U\" to DiffGraph demonstrates the value of heterogeneous user aggregation. Similarly, \"-I\" and \"-H\" variants confirm the benefits of heterogeneous information. Notably, \"-D\" outperforming \"-I\" suggests that removing item-side diffusion both loses valuable signals and introduces noise.\n\u2022 The \"DAE\" variant shows only limited performance improvement over completely removing the denoising module (i.e., \"-D\"). This demonstrates the difficulty of denoising heterogeneous graph data using existing DAE approaches. Consequently, it validates the superiority of the stepwise noise addition and removal process in our latent heterogeneous graph diffusion module."}, {"title": "3.4 Impact of Hyperparameters (RQ3)", "content": "This section examines the impact of hyperparameter settings. The results are presented in Figure 3. We make the following analysis.\n\u2022 Graph propagation iterations L. Tested within 1, 2, 3, 4. Performance improves up to L = 3; beyond this, additional layers may introduce noise and result in over-smoothing issues [3, 43].\n\u2022 Embedding dimensionality d. anges tested were 16, 32, 64, 128, 256. As d increases, there's a general improvement in performance, peaking before d = 256, where performance slightly declines due to potential overfitting.\n\u2022 Maximum diffusion steps T. Varied from 10 to 250. Increasing the diffusion steps T typically enhances the performance for DiffGraph, but very high values degrade it, likely due to the excessive noise disrupting social information integrity.\n\u2022 Noise scale S. Evaluated at scales 1e-3, 1e-4, 1e-5, 1e-6 with respective AUC scores of 0.7966, 0.8025, 0.7988, 0.7974 on the Industry dataset. Optimal noise scales improve model performance by enhancing denoising effectiveness; however, excessively high scales diminish performance by obscuring key graph semantics."}, {"title": "3.5 Efficiency Study (RQ4)", "content": "This section assesses the computational efficiency of DiffGraph. On industry data, we conducted a convergence analysis shown in Figure 4. We also compared the per-epoch running time of DiffGraph and baselines on two link datasets with the following results: Tmall: HGT, 16.824s, MBGCN, 12.644s, DiffGraph, 6.558s. Retail_rocket: HGT, 2.451s, MBGCN, 2.312s, DiffGraph, 1.811s. The results show DiffGraph consistently outperforms baselines in training efficiency, benefiting from its effective hidden diffusion."}, {"title": "3.6 Performance w.r.t. Data Sparsity (RQ5)", "content": "We further assess how the diffusion model in DiffGraph address the data sparsity issue in heterogeneous graph learning. For the Tmall dataset, we split nodes into five groups based on interaction counts (e.g., \"<8\", \"<65\"), and evaluate the performance on each group. The results are shown in Figure 5. Our observations include:\n\u2022 Trend in Performance. As edges per node increase, so does performance across all methods, indicating better embeddings"}, {"title": "3.7 Exploration of Anti-Noise Capacity (RQ6)", "content": "To study the model robustness against data noise, we evaluate the percentage of performance degradation for each heterogeneous relation under different noise ratios (Table 3). In our experiment, 0, 10%, 30% and 50% of the heterogeneous relations are randomly replaced by noise signals. We tested the performance of DiffGraph and HGT on the Tmall dataset. The results demonstrate that our model experiences a markedly less pronounced decline in performance in comparison to the baseline model. Such findings substantiate the enhanced denoising capability of our DiffGraph approach."}, {"title": "3.8 Case Study (RQ7)", "content": "We utilized the t-SNE technique to visualize the embeddings of different relation types from the Tmall dataset, color-coding each type for distinction. By comparing the behavior embedding of our DiffGraph to its variant \"-D\" (which lacks the denoising module), it's evident that our model achieves clearer separation, drawing embeddings with similar semantics closer together. This demonstrates our model's proficiency in capturing and refining heterogeneous data through hidden-space diffusion and denoising, effectively reducing noise and enhancing semantic accuracy. By optimizing the semantic diffusion from source to target, our method efficiently processes and highlights diverse information across graph structures, significantly improving learning outcomes."}, {"title": "4 RELATED WORK", "content": "Graph Neural Networks. Graph learning has evolved substantially, with GNNs demonstrating significant impact across domains including spammer detection [22, 42, 50], recommender systems [9,"}, {"title": "5 CONCLUSION", "content": "This paper presents DiffGraph, a heterogeneous graph diffusion model that advances graph learning through two key innovations: 1) A latent diffusion mechanism that progressively filters noise while preserving task-relevant signals from source data, and 2) An enhanced semantic transition framework that better captures relationships across different interaction types, enabling more nuanced heterogeneous graph modeling. Extensive experiments conducted on both public benchmarks and industrial datasets demonstrate our approach's superiority in terms of prediction accuracy and model robustness. The results consistently validate the effectiveness of our proposed techniques across various scenarios. Future work could explore extending our model to dynamic heterogeneous graphs, where both node attributes and graph structure evolve over time."}, {"title": "ETHICAL CONSIDERATIONS", "content": "In this section, we examine the ethical implications of the DiffGraph framework, which is designed to distill essential information from auxiliary relational data for enhancing prediction tasks in heterogeneous graphs. Our discussion addresses potential ethical challenges and considerations that may arise during both the implementation and deployment phases of the DiffGraph framework.\nData Privacy and Consent. The DiffGraph framework processes auxiliary relational data to enhance predictions in heterogeneous graphs, which inherently raises important privacy and consent considerations. When handling relational data, particularly those containing personal information, strict compliance with data protection regulations (e.g., GDPR, CCPA) is paramount. The potential for unintended inference or exposure of sensitive information through relational analysis presents a significant privacy concern, especially if explicit user consent is not properly obtained. To address these challenges, implementations must incorporate robust safeguards, including strict access control mechanisms, advanced"}]}