{"title": "R-Trans - A Recurrent Transformer Model for Clinical Feedback in Surgical Skill Assessment", "authors": ["Julien Quarez", "Matthew Elliot", "Oscar Maccormac", "Nawal Khan", "Marc Modat", "Sebastien Ourselin", "Jonathan Shapey", "Alejandro Granados"], "abstract": "In surgical skill assessment, Objective Structured Assessments of Technical Skills (OSATS scores) and the Global Rating Scale (GRS) are established tools for evaluating the performance of surgeons during training. These metrics, coupled with feedback on their performance, enable surgeons to improve and achieve standards of practice. Recent studies on the open-source dataset JIGSAW, which contains both GRS and OSATS labels, have focused on regressing GRS scores from kinematic signals, video data, or a combination of both. In this paper, we argue that regressing the GRS score, a unitless value, by itself is too restrictive, and variations throughout the surgical trial do not hold significant clinical meaning. To address this gap, we developed a recurrent transformer model that outputs the surgeon's performance throughout their training session by relating the model's hidden states to five OSATS scores derived from kinematic signals. These scores are averaged and aggregated to produce a GRS prediction, enabling assessment of the model's performance against the state-of-the-art (SOTA). We report Spearman's Correlation Coefficient (SCC), demonstrating that our model outperforms SOTA models for all tasks, except for Suturing under the leave-one-subject-out (LOSO) scheme (SCC 0.68-0.89), while achieving comparable performance for suturing and across tasks under the leave-one-user-out (LOUO) scheme (SCC 0.45-0.68) and beating SOTA for Needle Passing (0.69). We argue that relating final OSATS scores to short instances throughout a surgeon's procedure is more clinically meaningful than a single GRS score. This approach also allows us to translate quantitative predictions into qualitative feedback, which is crucial for any automated surgical skill assessment pipeline. A senior surgeon validated our model's behaviour and agreed with the semi-supervised predictions 77% (p = 0.006) of the time.", "sections": [{"title": "2 Introduction", "content": "Surgical skill assessment is the cornerstone of every training regime. It allows trainers and trainees to quantify progress and performance. Traditionally, this"}, {"title": "3 Methods", "content": "We propose a recurrent model called R-Trans where segments of kinematic data are processed into intermediate OSATS scores (Fig. 1). Those scores are then averaged into trial-level OSATS predictions. Our model is trained in an end-to-end fashion on all available OSATS scores except quality of final product, a score that relies entirely on vision. We assess the model's performance on the GRS label by aggregating the individual OSATS predicted scores."}, {"title": "3.1 Problem Formulation", "content": "An input signal $X_i \\in \\mathbb{R}^{D\\times T_i}$, of feature size D and length $T_i$, is divided into equal segments $x_s$ of size L ($x \\in \\mathbb{R}^{D \\times L}$): ${x_1, x_2, ..., x_S} \\in X_i$ where S is the total number of segments, i.e. $S = \\frac{T_i}{L}$. For simplicity, in the rest of this paper, we omit i and use s as a subscript to refer to different segments within a trial i, i.e. $x_s \\rightarrow x_{is}$. We fit a function $F_n$ to map $X_i$ to the label space Y : ($F_n : X_i \\rightarrow Y_n$):\n$Y_n = F_n(x_1, x_2, ..., x_S, ... X_S)$\nwhere $y_n \\in Y$ is the $n^{th}$ category OSATS score. The GRS is the aggregate of OSATS scores: $Y = \\sum y_n$. Considering clinical practice where the given score is representative of their average performance through the trial i.e.: $y_n = \\sum Y_{sn}$, we rewrite Eq. 1 into Eq. 2, where $f_n$ maps a segment to the nth OSATS intermediate label ($x_s \\rightarrow y_{ns}$). Note that there is no ground truth for $y_{ns}$ and we learn $\\hat{y}_{ns}$ in a semi-supervised manner.\n$Y_n = \\frac{1}{S} \\sum_{s=1}^{S} f_n(x_s)$"}, {"title": "3.2 Model Overview", "content": "Our model processes segments of a kinematic signal recurrently by taking two inputs: the previous hidden state of the recurrent network, $z_{s-1} \\in \\mathbb{R}^{L \\times D}$, and the current segment-level kinematic signal, $x_s$ (Fig 1a). The two inputs are fused into the current hidden state, $z_s = h(x_s, z_{s-1})$, through the model backbone h (Fig. 1b). We initialise $z_0$ as a zero-filled tensor. Each hidden state is then passed to five classification heads, $C_n$, giving $f_n(x_s) = C_n(h(z_{s-1}, x_s))$. The output of our model is a final OSATS score, the average of all segment-level OSATS scores:\n$Y_n = \\frac{1}{S} \\sum_{s=0}^{S} C_n[h(z_{s-1}, x_s)]$\nThe backbone h is composed of three fusion modules (Fig. 1c) similar to Yang et al [28], where previous temporal information is fused with the current input through a series of multi-head self- and cross-attention blocks. A residual connection is used between the input $x_s$ and the second fusion module output to address vanishing gradients and favour feature reuse of the temporal sequence.\nThe classification heads $C_n$ are five MLPs classifying the hidden state $z_s$ into segment-level OSATS score predictions $\\hat{y}_{ns} = C_n(z_s)$. Each MLP layer consists of batch normalisation, ReLU activation function, and fully connected layers.\nLoss: R-Trans is trained end-to-end using cross-entropy losses. Each cross-entropy loss is applied to the average of the classification head segment predictions $\\hat{y}_n = \\sum \\hat{y}_{ns}$ for a given OSATS category label $y_n$. An L2 penalty term is added to regularize the network and help with generalisation. Our final loss is expressed as:\n$L = \\sum_{n=0}^{N} CE(\\hat{y}_n, y_n) + \\lambda * L2$"}, {"title": "3.3 Experimental Design", "content": "Dataset: We evaluated our model on the JIGSAWS dataset [3]. This dataset consists of eight clinicians evaluated on three distinct tasks, namely needle pass-ing (NP), suturing (SU), and knot-tying (KT), where video and kinematic data were recorded. Altogether, there are 39, 28, and 36 labelled data for SU, NP, and KT, respectively. The labels are comprised of six OSATS scores ([1-5]) and one GRS ([6-30]) score the aggregate of all OSATS scores. The OSATS scores include: 1) respect for tissue, 2) suture/needle handling, 3) time and motion, 4) flow of operation, 5) overall performance, and 6) quality of final product. In this study, we only use kinematic data, the first five OSATS scores, and an updated GRS score accordingly. Quality-of-final product was shown by Kasa et al. [23] to be an image/video-specific score and was not considered in this paper.\nCross-validation: Following the JIGSAW cross-validation framework, we eval-uate our method using Leave-One-Supertrial-Out (LOSO) and Leave-One-User-Out (LOUO), whereby either the i-th trial across surgeons or all trials performed by the same surgeon are left out as the test set, respectively.\nPerformance Metric: Similar to relevant work [5, 11, 17, 8], we evaluate our method using Spearman's Correlation Coefficient (SCC) $\\rho$ to compare the pre-dicted ranked GRS score with the ground truth (Eq. 5). We report SCC averaged across folds for each cross-validation scheme. The intermediate OSATS scores are averaged into final OSATS scores after processing the whole kinematic signal. The final five OSATS scores are then summed to give a video-level GRS score."}, {"title": "4 Results", "content": "We report the performance of our model against previous work that uses kine-matic data or in combination with video data. With the exception of SU, R-trans outperforms previous models under the LOSO validation framework achieving SCC of 0.89, 0.78, and 0.68 for KT, NP, and across all tasks, respectively. Although we outperformed SOTA models for NP and matched results for SU and across tasks under the LOUO scheme, the very imbalanced label distribution in KT for LOUO is likely to have made our model underperform. When looking at the performance of our model in predicting OSATS scores under the LOSO validation scheme, we outperform other models in the KT and across task. We also report specific OSATS SCC in under the LOSO for KT and outperform Kasa et al. [23] for all OSATS scores except in overall performance. To the best of our knowledge, this is the only work reporting SCC on OSATS-specific categories using kinematic data. To validate the semi-supervised outputs"}, {"title": "5 Conclusion and Future Work", "content": "In this work, we adopt a different take on the problem formulation for skill as-sessment which can be expanded to more complex recurrent architectures and other fields of skill assessment. Through our competitive results on the JIG-SAW dataset, we demonstrate the feasibility of this approach, while providing more granular feedback for skill assessment. In future work, our focus will be on enhancing the robustness of validation, incorporating video data, and extend-ing our approach to datasets featuring longer and more complex tasks. Surgical tasks/interventions lasting multiple hours are very demanding to annotate. Semi-supervised methods that extract intermediate labels, at the gesture, step, and phase levels could allow for more automated and more granular assessment of surgeons' performance."}]}