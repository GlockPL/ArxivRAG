{"title": "Decompose-ToM: Enhancing Theory of Mind Reasoning in Large Language Models through Simulation and Task Decomposition", "authors": ["Sneheel Sarangi", "Maha Elgarf", "Hanan Salam"], "abstract": "Theory of Mind (ToM) is the ability to understand and reflect on the mental states of others. Although this capability is crucial for human interaction, testing on Large Language Models (LLMs) reveals that they possess only a rudimentary understanding of it. Although the most capable closed-source LLMs have come close to human performance on some ToM tasks, they still perform poorly on complex variations of the task that involve more structured reasoning. In this work, we utilize the concept of \"pretend-play\", or \"Simulation Theory\" from cognitive psychology to propose \u201cDecompose-ToM\u201d: an LLM-based inference algorithm that improves model performance on complex ToM tasks. We recursively simulate user perspectives and decompose the ToM task into a simpler set of tasks: subject identification, question-reframing, world model updating, and knowledge availability. We test the algorithm on higher-order ToM tasks and a task testing for ToM capabilities in a conversational setting, demonstrating that our approach shows significant improvement across models compared to baseline methods while requiring minimal prompt tuning across tasks and no additional model training. Our code is publicly available.", "sections": [{"title": "1 Introduction", "content": "Social reasoning, the ability to understand and navigate complex interpersonal dynamics and social contexts, is crucial for large language models (LLMs) as it both enables better interactions with users (Salam et al., 2022, 2023), and paves the way for more powerful LLM-based systems that may act in socially appropriate and helpful ways within the same environment as humans (Li et al., 2023; Jiang et al., 2024). A key factor in the human ability to socially reason is the presence of the theory of mind (ToM) (Baron-Cohen, 1995; Christensen and Michael, 2012). ToM is the ability to attribute and infer the mental states of others, a capability children start developing at a young age (Premack and Woodruff, 1978). However, the performance of LLMs on this task is contentious. In the ToMi (Le et al., 2019) dataset, some recent methods have improved the capabilities of frontier models to near-human levels (Sclar et al., 2023; Wilf et al., 2024). At the same time, the research community has designed newer datasets with variations to the ToM task that continue to be a challenge for LLMs.\nFor example, higher-order ToM capabilities involve the ability to model the beliefs of others about the beliefs of others and so on (Kinderman et al., 1998). Studies have highlighted the pivotal role of higher-order Theory of Mind (ToM) not only in managing intricate communication scenarios such as discussions involving multiple parties (Liddle and Nettle, 2006), but also in fostering empathetic interactions and providing emotional support (Mitchell and Phillips, 2015). However, LLM models continue to perform poorly in tasks that aim to measure this capability (Wu et al., 2023). Likewise, LLM models struggle to generalize even lower-order Theory of Mind (ToM) reasoning demonstrated in datasets like ToMi to more realistic dialogue-based scenarios, as observed by Kim et al. (2023). This raises the question: Can LLMs or LLM-based approaches effectively address these challenges while maintaining task generalization?\nInsights from developmental psychology offer potential solutions. The development of ToM in children has been closely linked to the act of \"pretend play\" (Kavanaugh, 2006; Lillard, 1993; Qu et al., 2015). Emerging around 18 months of age-significantly earlier than the age at which children reliably solve Theory of Mind (ToM) tasks like those in ToMi-this process involves role-taking and the attribution of mental states, both of which are essential for developing ToM capabilities (Leslie, 1987). Similarly, another precursor to ToM capabilities in children is the ability to solve"}, {"title": "2 Related Work", "content": "Towards a Machine Theory of Mind\nThe pursuit of a machine Theory of Mind (ToM)\nhas been a significant focus within the research\ncommunity for many years. Early investigations\nutilizing neural language models revealed that these\nmodels often faced challenges in accurately infer-\nring mental states (Nematzadeh et al., 2018). As\nlarge language models (LLMs) have advanced, re-\nsearchers have increasingly turned their attention\nto assessing these models' ToM capabilities. For\nexample, Sap et al. (2022) employed versions of\nthe Sally-Anne false-belief test (Baron-Cohen et al.,\n1985) using the ToMi (Le et al., 2019) dataset to\nillustrate the limited performance of LLMs like\nGPT-3 on tasks that require belief inference.\nIn contrast, more recent and sophisticated mod-\nels such as GPT-4 (OpenAI, 2023) have demon-\nstrated performance that approaches or even sur-\npasses human-level accuracy on benchmarks like\nToMi (Kosinski, 2023) and BigToM (Gandhi et al.,\n2023). Despite these promising developments, con-\ncerns persist regarding the robustness of LLMs'\nToM capabilities. Notably, the effectiveness of\nthese models tends to diminish significantly when\nsubjected to adversarial perturbations (Ullman,\n2023; Shapira et al., 2023), raising questions about\nthe consistency and reliability of their mental state\ninferences.\nStrategies for ToM Performance in LLMs.\nTraditional methods that have worked to improve\nLLM performance on reasoning tasks such as\nChain of Thought prompting (Wei et al., 2022),\nhave seen limited success in improving LLMs'\nToM performance (Moghaddam and Honey, 2023).\nThere have been recent works such as Symbolic-\nTOM (Sclar et al., 2023), and SimToM (Wilf et al.,\n2024) attempting to improve the performance of\nLLMs on false-belief ToM tasks. Symbolic-ToM\nutilizes LLMs to form a belief-state graph repre-\nsentation, before attempting to answer a question.\nThis method provides an elegant solution for even\nhigher-order ToM tasks, but it increases the algo-\nrithm's complexity exponentially. Additionally, the\nmethod of adapting it to more naturalistic settings\nremains unclear. SimToM proposes a simple 2-\nstep method involving perspective-taking, taking\ninspiration from Simulation Theory (Shanton and\nGoldman, 2010). While drawing from similar in-\nspiration in its design philosophy, SimToM doesn't\nconsider more complex ToM scenarios such as\nhigher-order reasoning."}, {"title": "3 Methodology", "content": "Drawing inspiration from the concepts of pretend-play (Kavanaugh, 2006; Lillard, 1993; Qu et al., 2015) and knowledge-access (Wellman and Liu, 2004), in a similar vein to SimToM (Wilf et al., 2024), our approach \u201cDecompose-ToM\u201d decomposes the ToM task into a recursive perspective simulation task, and a granular statement awareness task. Concretely, this means that for each statement describing a theory of mind testing scenario, we prompt the LLM to output whether an interested agent is aware of it. Additionally, to assist the LLM in this task, we keep track of a symbolic world state representation that is both generated and updated by the LLM. We recursively chain agent simulations, simplifying the question to be answered at each stage until we get a question that is answered by a fact, and not an agent's belief. This process is demonstrated in Appendix B as Algorithm 1, Algorithm 2, and Figure 1. Our approach can thus be divided into three components: (1) Initialization, (2) Simulation Processing & World-state Updating, and (3) Question-Answering\u00b9.\nInitialization. This step extracts the character whose perspective will be simulated and then simplifies the question to frame it as if it were asked to the character. We accomplish both these tasks by few-shot prompting the LLM with hand-crafted demonstrations. For example, if the question states: \"Where does A think B thinks the object O is?\u201d We detect the person to be simulated as A, and the simplified question would thus be: \u201cWhere does B think the object O is?\u201d. At this step we also set up the text-based world state by prompting the LLM to detect the locations mentioned in a story, and output these arranged into a simple world state representation such as: \"Living Room:[], Bedroom:[]\".\nSimulation Processing & Updating World State.\nThis step creates a new story consisting of only statements known by the identified character from the overall story. For each statement, we prompt the LLM to answer whether the given character knows the statement by inferring from prior statements in the story and the given world-state representation. After the LLM makes a decision, we update the world state with another LLM call if it needs to be updated after the given statement. If the LLM decides that the given statement is known by the agent, we add the statement to the sub-story. After iterating through all the statements in the story, we obtain a newly created story that represents the story from the perspective of the identified character."}, {"title": "4 Experiments and Results", "content": "4.1 Datasets\nHi-ToM (Wu et al., 2023). Hi-ToM is designed to evaluate the higher-order theory of mind abilities of language models, going up to the fourth-order theory of mind reasoning tasks. It is based on the Sally-Anne test (Baron-Cohen et al., 1985) with characters entering, leaving, and moving items in rooms. The provided task is a multiple-choice question-answering task with 15 choices per question. The dataset consists of 600 questions spanning two categories: \u201cTell\u201d and \u201cno-Tell\u201d. The \"Tell\" category adds an element of deception, by asking LLMs to reason whether an agent is lying by inferring when they left the room. We present our results averaged over both categories.\nFANTOM (Kim et al., 2023). FANTOM is a dataset consisting of dialogue-based interactions between characters. Characters may leave or enter the conversation at any time, and questions are based on correctly navigating this information incompleteness. This provides a naturalistic setting to test our model on. While the dataset provides a large suite of tasks, we use the binary-choice belief task for our evaluations due to its structural similarity to the Hi-ToM dataset. The dataset consists of 1,540 questions, containing first and second-order ToM questions. We show averaged results over these categories due to generally similar accuracy gains for models across evaluation methods. Additionally, the dataset consists of stories in short and long-length variants, which we show results for separately to evaluate our method's performance over longer context lengths.\nWe provide example stories from both datasets, alongside details of any pre-processing conducted, in Appendix C.\n4.2 Evaluation\nWe experiment with four base LLMs, and evaluate each of them via:\nZero-shot prompting (Baseline): Model directly returns the multiple choice option of the answer\nZero-shot chain of thought prompting: Model \"thinks step-by-step\" before outputting the answer\nSimToM prompting (Wilf et al., 2024): Two-step prompting, first involving perspective-taking, then finding the answer\nOur approach (Decompose-ToM): Decomposes tasks into recursive simulations and knowledge access subtasks.\nEach method is applied consistently to compare performance across models and datasets. We evaluate the open-source models Llama-3-8B and 70B (Dubey et al., 2024), alongside the closed source models Gemini-1.5-Flash (Reid et al., 2024) and GPT4-03. We share the model version details in Appendix D. We use similar prompts for both datasets with minor changes in the instructions to better fit and describe each task and to resolve additional ambiguities specific to each dataset.\n4.3 Results and Discussion\nResults for Hi-ToM. We observe improvements in performance for all models except for the Gemini Flash model. The largest increase is seen for the GPT-40 model with an average accuracy gain of 28.13% over the SimToM method, which slightly outperformed the chain-of-thought method. Similarly, the Llama-3-70B model saw the next largest increase of 22.5% over the SimToM method, which slightly outperformed the baseline method. The Llama-3-8B model and the Gemini-Flash model both show slight increases over the baseline and SimToM method respectively.\nUsing Decompose-ToM, we observe that the performance of all models is preserved better than other methods for higher-order tasks. GPT-40 and Llama-3-70B models even see improvements of 6.6% and 13.3% respectively from orders 1 to 4. The Llama-3-8B and the Gemini-Flash-1.5 models show a dip in accuracy over the range. We hypothesize that larger models such as GPT-40 and Llama-3-70B can consistently perform well in the statement awareness task, whereas less capable models may fail. Conversely, the observed increase in accuracy for larger models may be attributed to"}, {"title": "5 Conclusion", "content": "In this work, we introduce a novel LLM-powered inference algorithm inspired by the concepts of pretend-play and knowledge-access from cognitive psychology to enhance Theory of Mind (ToM) capabilities in LLMs. Our method decomposes complex ToM tasks into simpler sub-tasks: recursive perspective simulation and granular statement awareness, enabling models to better understand and attribute mental states in higher-order and naturalistic settings.\nWe evaluated our approach on the Hi-ToM and FANTOM datasets, demonstrating improvements over baseline methods across both open-source and closed-source LLMs. Notably, our method showed strong performance gains in higher-order ToM tasks and effectively maintained accuracy across longer context lengths, addressing key challenges in current ToM evaluations for LLMs.\nThese results suggest that leveraging cognitive developmental strategies can effectively enhance the social reasoning abilities of LLMs without requiring additional model training or extensive prompt engineering. Our approach offers a flexible and generalizable framework for improving ToM performance in LLMs, which could have broad implications for the development of socially-aware AI systems. Additionally, future work can explore employing these algorithms through LLM agentic frameworks, allowing LLMs to autonomously conduct the process with a single instruction."}, {"title": "6 Limitations", "content": "A key limitation of our work is that it assumes that the agent's memory is never updated backward. For example, if a story doesn't strictly follow a chronological order by claiming midway through that an agent who hadn't been mentioned yet was present earlier (implying that they knew about what happened earlier), our method does not update this in the agent simulation. We believe that this problem of incomplete information can be resolved by disambiguating the relevant story in the pre-processing stage on the lines of our procedure for the FANTOM dataset (refer to Appendix C). However, we haven't tested this extensively and leave this for future work.\nAdditionally, our approach is highly computationally expensive compared to other approaches due to the need for per-statement processing, we hope to come up with more efficient methods in future work."}, {"title": "A Generalizability", "content": "We've designed Decompose-ToM in a manner that makes it highly generalizable, and easy to use in a plug-and-play manner. The core Decompose-ToM function simply requires a question, options, and a story. Users can optionally pass any instructions the model should follow when interpreting the story, and the unit of an information chunk. By information chunk, we refer to the atomic unit of information that an agent either knows or doesn't. For example, the information chunk in Hi-ToM was a sentence, and the chunk in FANTOM was a dialogue. By default, the unit is a sentence.\nHowever, although the system is generalizable the user will likely have to tune the simulation, and question-answering prompts to get optimal results. In future work, we hope to have the system be able to dynamically define information chunks, and understand the required processing steps automatedly."}, {"title": "B Algorithms", "content": "Algorithms 1 and 2 describe the algorithms for the simulation process and story update proposed in this work."}, {"title": "C Disambiguating Datasets and Examples", "content": "Hi-ToM The Hi-ToM dataset consists of some unintentional ambiguities, which are also present in precursor ToM datasets such as ToMi (Le et al., 2019). These ambiguities do not allow us to ascertain the actual ToM capabilities of language models.\nFANTOM Unlike Hi-TOM, FANTOM doesn't have markers of entry into conversation at the beginning of the story and this may lead to problems for our algorithm since it does a simple forward pass over the statements. Thus, we preprocess FANTOM stories to initialize the world state with the agents initially in a conversation to be in an \"in-conversation\" location state. We do this by simply detecting the agents in conversation at the beginning using a single LLM prompt, and setting up the initial world-state accordingly."}, {"title": "D Model Details", "content": "We run Meta's Llama-3-70B, and Llama-3-8B models using vLLM (Kwon et al., 2023) on 4 A100 GPUs. We use OpenAI's gpt-4o-2024-08-06 model and Gemini-1.5-Flash-001 through the publicly available APIs."}, {"title": "E Discussion on Degradation across order for Smaller Language Models", "content": "We observe that LLMs such as Llama-3-8b, and Gemini-1.5-Flash suffer degradation across orders when we use our method. On evaluating this phenomenon we identify that this is caused due to the snowballing effect of errors in the knowledge-awareness task. That is, if the LLM incorrectly labels a statement x, which the agent should have known, as unknown, at simulation step n. Then, at every subsequent simulation step n+i, the LLM will have incomplete information to answer the statement to answer the statements after x. If statement X is crucial, such as information about an agent entering or leaving a room, this error will erase all statements following x at step n+1.\nAdditionally, even if statement x is not crucial to the currently simulated agent, it may have important context for an agent that will be simulated subsequently. Thus, the simulation is highly sensitive to errors in the knowledge-awareness step. As the number of such steps required increases with higher orders of ToM being tested, the probability of snowballing errors increases, thus reducing the performance of the model."}, {"title": "F Implementing SimToM", "content": "We implement SimToM by handcrafting prompts for both tasks, by taking guidance from the original SimToM prompts for the ToMi dataset, and combining it with our handcrafted prompts for the Decompose-ToM method. We conduct limited prompt-tuning and answer validation to ensure performance isn't degraded due to issues such as parsing errors."}, {"title": "G Prompts", "content": "In the following, we present all of the prompts that were used in this work."}]}