{"title": "2SSP: A Two-Stage Framework for Structured Pruning of LLMs", "authors": ["Fabrizio Sandri", "Elia Cunegatti", "Giovanni Iacca"], "abstract": "We propose a novel Two-Stage framework for Structured Pruning (2SSP) for pruning Large Language Models (LLMs), which combines two different strategies of pruning, namely Width and Depth Pruning. The first stage (Width Pruning) removes entire neurons, hence their corresponding rows and columns, aiming to preserve the connectivity among the pruned structures in the intermediate state of the Feed-Forward Networks in each Transformer block. This is done based on an importance score measuring the impact of each neuron over the output magnitude. The second stage (Depth Pruning), instead, removes entire Attention submodules. This is done by applying an iterative process that removes the Attention submodules with the minimum impact on a given metric of interest (in our case, perplexity). We also propose a novel mechanism to balance the sparsity rate of the two stages w.r.t. to the desired global sparsity. We test 2SSP on four LLM families and three sparsity rates (25%, 37.5%, and 50%), measuring the resulting perplexity over three language modeling datasets as well as the performance over six downstream tasks. Our method consistently outperforms five state-of-the-art competitors over three language modeling and six downstream tasks, with an up to two-order-of-magnitude gain in terms of pruning time. The code is available at available at https://github.com/FabrizioSandri/2SSP.", "sections": [{"title": "1. Introduction", "content": "The sheer size of the recent, billion-scale Large Language Models (LLMs) is one of the main reasons for their successful performance. However, it comes at the cost of computational budget in terms of required GPUs as well as time for pre-training and inference, which in turn has serious economic and environmental impacts. Therefore, studying approaches to reduce the computational burden of such models while minimizing their performance degradation has become a pressing matter.\nOne of the main approaches to address this issue is through Network Pruning (Frantar & Alistarh, 2023; Ma et al., 2023), which mainly focuses on reducing the size of pre-trained LLMs as well as their inference time. Among the several pruning methods available in the literature, reliable inference speed-ups (Kurtic et al., 2023; Ashkboos et al., 2024) have been achieved mainly through structured pruning, i.e., approaches that remove entire portions of the model. Different strategies to select which portions of the network to remove have been proposed, see Figure 1, identifying Width pruning, which removes rows/columns and/or single layers, Depth Pruning (Blocks), which removes entire Transformer Blocks, and Depth Pruning (Submodules), which removes entire submodules (i.e., the Attention submodule-in the following, referred to as \"Attention\", for brevity-and/or Feed-Forward Network (FFN)) from the Transformer Blocks. width pruning has the main advantage of having a lower granularity level in the removal search space, which leads to a more refined identification of unimportant components of the model. On the other hand, the advantage of depth pruning lies in the lower computation time required to obtain the sparse structure as well as the larger inference speed-up that comes from the removal of entire blocks/submodules.\nContributions So far, the aforementioned categories of structured pruning have been investigated independently of one another and their combination is currently a relatively unexplored research direction. In this paper, we propose a Two-Stage Framework for Structured Pruning (2SSP), a new structured pruning approach that, to our knowledge, is the first to combine width and depth pruning, hence exploiting the advantages of both approaches. The proposed 2SSP works in two stages. The first stage works at a lower granularity level, i.e. it uses width pruning to remove neurons from the intermediate state of FFNs based on the output's magnitude. This is done while preserving the network connectivity, which is a critical measure for reducing performance degradation in sparse structures (Vysogorets &\nKempe, 2023; Cunegatti et al., 2024b; Iurada et al., 2024).\nMoreover, the first stage is applied only to the FFN parts of"}, {"title": "2. Related Work", "content": "In this section, we discuss the different techniques proposed in the literature to remove parameters from pre-trained LLMs (i.e., for pruning after training). Methods for removing parameters from a model, while minimizing its performance degradations, can be roughly categorized into unstructured and structured pruning. The main difference between these two approaches is that for a given sparsity and task, unstructured pruning allows for achieving better performance for the task, but the real speed-up is mainly theoretical (since the parameters are only masked, rather than practically removed); whereas, using structured pruning (the focus of this paper), the speed-up at inference/training time is substantial (because of actual structure removals), but the performance degradation on the task may be higher."}, {"title": "2.1. Unstructured Pruning", "content": "Unstructured pruning aims to reduce the model's parameters by identifying critical weights (inside a weight matrix W), in any position, e.g., based on weight magnitude (Jaiswal et al., 2024), Hessian matrix (Frantar & Alistarh, 2023), or activations (Sun et al., 2024; Zhang et al., 2024a; Farina et al., 2024). On top of these methods, several techniques have been proposed either to select the best block-wise sparsity allocation, e.g., based on outliers (Yin et al., 2024), activation alignment (Cunegatti et al., 2024a), and optimal allocation search (Li et al., 2024), or to further minimize the reconstruction error (Zhang et al., 2024b; Xu et al., 2024)."}, {"title": "2.2. Structured Pruning", "content": "Structured pruning removes entire parameter groups or structures from the LLM. Different categories of structured pruning algorithms have been proposed, which can be roughly categorized w.r.t. the granularity of the structures they remove. The earliest approach is named Width Pruning, which encompasses methods that remove specific portions of weight matrices (such as rows and/or columns), or layers inside each Transformer block (such as a single Linear layer). The pruning decision can be based on gradient information (Ma et al., 2023; Fang et al., 2024), computational invariance (Ashkboos et al., 2024), perturbative pruning (Dery et al., 2024), hardware-aware inference vs. performance trade-off (Kurtic et al., 2023), Fisher information (van der Ouderaa et al., 2024), or knowledge-distillation (Muralidharan et al., 2024).\nRecently, a new parading of pruning, namely Depth Pruning, has emerged shifting the pruning focus from weight matrices and layers to either entire Transformer blocks or entire Transformer submodules (Attention and/or FFN). The main idea behind this category of pruning algorithms is to assign an importance score to each block/submodule and then prune it w.r.t. to its given score. These scores can be given either based on the change in hidden representation between blocks (similarity-based) or by computing each block's importance w.r.t. the model performance on a given task (performance-based)."}, {"title": "3. Methods", "content": "We now introduce 2SSP, a novel pruning framework that combines two stages of structured pruning: a first stage (s1) which prunes at the level of entire neurons by removing rows and columns from the FFN within the Transformer blocks; and a second stage (s2), which performs submodule-based depth pruning by removing entire Attentions.\nNotations Given a Transformer-based LLM M, let B denote the number of identical blocks it is made of, and b represents a generic block. We define a submodule of block b as either the Attention or the FFN of that block. The Attention mechanism involves three linear projections, namely queries $W_{query}$, keys $W_{key}$, and values $W_{value}$, and an output projection $W_{out}$, which for a given input X outputs a final representation computed as $softmax((XW_{query}(XW_{key})^T)/\\sqrt{d_k})(XW_{value})W_{out}$.\nFor the most recent LLMs, such as the one tested in this paper, the FFN consists of three linear projections: gate $W_{gate}$, up $W_{up}$, and down $W_{down}$, where the input is processed sequentially through the first two before the down projection. For a given input X, the FFN forward output is given by $(\\sigma(XW_{gate}) \\odot (XW_{up}))W_{down}$, where $\\sigma$ is the activation function.\nThe hidden dimension of the model is represented by $d_{model}$, while $d_{int}$ denotes the intermediate dimension of the FFN, corresponding to the output dimension of the gate and up projections, and to the input dimension of the down projection. For any Linear layer $l$ within these submodules, let $W_l \\in \\mathbb{R}^{d_{out} \\times d_{in}}$ denote the weight matrix of the layer. We use $\\mathcal{D}_{cal}$ to denote the calibration dataset used for pruning.\nNeuron Pruning (s1) The method aims to prune the neurons of the intermediate representation within the Linear layers of the FFN following the Attention in each Transformer block. The rationale is that the FFN's hidden state generates an intermediate representation that can be compressed by removing entire neurons from its hidden state, obtaining a lower-dimensional representation that preserves the most important features of the input sequence. We show that a high fraction of neurons in this representation created within the FFN is irrelevant, and thus can be pruned without significantly impacting performance.\nThe algorithm prunes an equal number of neurons from"}, {"title": "Neuron Pruning (s1)", "content": "each FFN, removing the neurons in the hidden state that have the lowest impact, measured as the magnitude of their activated output. The magnitude is calculated as the average $L_2$ norm across the tokens in a set of calibration samples from a given calibration dataset $\\mathcal{D}_{cal}$. The top-K neurons are then retained in the FFN of each block.\nLet the intermediate representation of the FFN in block b be denoted as $Z_b \\in \\mathbb{R}^{T \\times d_{int}}$, where T is the sequence length and $d_{int}$ is the dimension of the intermediate representation of the FFN in block b. For each neuron j, we compute an importance score $s_j$ across the calibration dataset $\\mathcal{D}_{cal}$:\n$s_j = \\frac{1}{|\\mathcal{D}_{cal}|} \\sum_{c=1}^{\\mathcal{D}_{cal}} ||z_j^{(c)}||_2$ \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(1)\nwhere $z_j^{(c)}$ is the activation of the j-th neuron for the c-th sequence in the calibration dataset, and $||\\cdot||_2$ denotes the $L_2$ norm over the tokens in the calibration sequence.\nFormally, let $W_{in} \\in \\mathbb{R}^{d_{int} \\times d_{model}}$ and $W_{out} \\in \\mathbb{R}^{d_{model} \\times d_{int}}$ be the input and output projection matrices of the FFN, respectively, where $d_{model}$ is the model's hidden dimension. We define a binary mask $m \\in \\{0, 1\\}^{d_{int}}$ that selects the top-K neurons to preserve based on their importance scores $s_j$. The pruned weight matrices are then computed as:\n$W_{in} = W_{in}[m=1, :] \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad (2)$\n$W_{out} = W_{out}[:, m=1] \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad(3)$\nTo illustrate the mechanism, let us consider a simple FFN with two Linear layers, denoted as $l_{in}$ and $l_{out}$. Removing"}, {"title": "Attention Pruning (s2)", "content": "a neuron from the hidden state of the FFN necessitates the removal of all the associated weights in both $l_{in}$ and $l_{out}$ connected to that particular neuron. More precisely, given the weight matrices $W_{in} \\in \\mathbb{R}^{d_{int} \\times d_{model}}$ and $W_{out} \\in \\mathbb{R}^{d_{model} \\times d_{int}}$, respectively of $l_{in}$ and $l_{out}$, the pruning of a hidden state neuron involves eliminating (in this order, to preserve network connectivity) the corresponding row in $W_{in}$ and the associated column in $W_{out}$.\nAttention Pruning (s2) Pruning only the FFN submodules limits the effectiveness of the algorithm, as only a restricted fraction of the total number of parameters can be pruned leaving all the Attention parameters intact. To address this limitation, inspired from the observation derived in (Siddiqui et al., 2024) where it has been shown how the Attentions can be removed to a certain degree (~ 33%) with almost no performance degradation, we propose a second pruning stage that, after removing a certain fraction of FFN neurons, also prunes the remaining parameters from the Attentions. Unlike FFNs, Attentions do not create a single hidden state due to the sequential application of multiple mathematical operators, such as scaling, softmax, and matrix multiplication. This makes pruning entire neurons impractical. We address this challenge by adopting the submodules pruning mechanism proposed in (Zhong et al., 2024), by removing only Attentions (and not also FFNs as in the original mechanism) from the sparse network $M_1$ obtained after the first stage. Specifically, we iteratively remove the Attentions leading to the lowest perplexity on the calibration dataset until the target sparsity is reached.\nFormally, let A = {$a_1$,...,$a_B$} be the set of Attentions across all the B blocks of the Transformer. At each step t, we select the Attention module $a^*$ whose removal minimizes the perplexity on the calibration dataset:\n$a^* = \\underset{a \\in A_t}{argmin} \\ PPL(M_t \\setminus a, D_{cal})$ \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(4)\nwhere $A_t$ is the set of the remaining Attention modules at step t, $M_t \\setminus a$ denotes the model at step t with the Attention module a removed, and PPL represents the perplexity metric, calculated as the exponential of the negated average log-likelihood over the calibration samples of $\\mathcal{D}_{cal}$."}, {"title": "Balancing the Sparsity Rate", "content": "Balancing the Sparsity Rate As explained above, the proposed 2SSP algorithm works in two stages. Hence, given a target sparsity rate s, selecting the pruning rate allocated for each of the two stages is also required.\nThrough empirical analysis, we found out that the following equation provides a reliable metric for determining the optimal number of Attention modules to prune at any given sparsity rate (see Figure 6):\n$N_{Attn} = round\\left(B \\frac{s|W_{total}|}{\\alpha |W_{Attn}|}\\right)$\t\t\t\t\t\t\t\t(5)\nwhere $\\alpha = 1.5$ (see Section 4.3), $W_{FFN}$ represents the number of FFN parameters per block, and $|W_{Attn}|$ represents the number of Attention parameters per block. This equation captures two critical aspects of this pruning process. First, it ensures that the number of pruned Attention parameters scales with increasing sparsity rates. Second, it adjusts the Attention pruning rate based on the relative sizes of the FFN and Attention modules. Specifically, when $|W_{FFN}|$ is large, indicating that FFN parameters dominate the block structure, the equation reduces the proportion of Attention parameters to be pruned. This adaptive behavior helps maintain the balance between the number of Attentions and the number of FFN parameters pruned."}, {"title": "4. Experiments", "content": "Setup We implement 2SSP using PyTorch (Paszke et al., 2019) with HuggingFace's Transformer library (Wolf et al., 2020) for model and dataset management. All experiments are conducted on a cluster of four NVIDIA A30 GPUs (24GB memory each).\nModels To test the effectiveness of our approach across different LLM families, we tested the 7B-parameter variants of Mistral-v0.3 (Jiang et al., 2023), Llama-2 (Touvron et al., 2023), and Qwen-2.5 (Yang et al., 2024), while using the 14B-parameter version Phi-3 (Abdin et al., 2024).\nTasks and Datasets We employ perplexity over language modeling tasks as the primary evaluation metric, given its established usage in assessing pruning algorithms (Frantar &\nAlistarh, 2023; Ashkboos et al., 2024; Sieberling et al.,\n2024) and the fact that it has been empirically proved to be a reliable metric for evaluating compressed models (Jin et al.,\n2024). We measure perplexity across three datasets: the\nfull WikiText2 dataset (Merity et al., 2017), along with subsets of samples from C4 (Raffel et al., 2020) and Fine Web (Penedo et al., 2024) datasets, which are all popular benchmarks in the pruning literature (Sieberling et al., 2024). We also conduct downstream evaluations using LM Eval Harness (Gao et al., 2024), including MMLU (Hendrycks et al.,\n2021), WinoGrande (WQ) (Sakaguchi et al., 2021), PiQA\n(Tata & Patel, 2003), HellaSwag (HS) (Zellers et al., 2019),\nand ARC (easy and challenge, in the following referred to as \"ARC-e\" and \u2018ARC-c\u201d, respectively) (Clark et al., 2018).\nBaselines We compare 2SSP against a broad set of state-of-the-art structured pruning methods for LLMs. Since our proposed approach works by combining width pruning with depth submodule pruning, we designed the experimental setup to include baselines from the three different categories of pruning granularity mentioned above:"}, {"title": "4.1. Experimental Results", "content": "In this section, we evaluate 2SSP w.r.t. the selected baselines in terms of performance over both language modeling and downstream tasks. We also include experiments about the runtime required by our approach to obtain the final sparse models. We show how our approach outperforms the competitors in terms of task performance and when confronting the pruning runtime vs. performance trade-off. We also evaluate the quality of the sparse models w.r.t. their inference speed-up w.r.t. the corresponding dense models.\nNumerical Results The first task we evaluate is language modeling over WikiText2, C4, and FineWeb at three different sparsity rates, namely 25%, as in (Men et al., 2024; Zhong et al., 2024; Ashkboos et al., 2024), 37.5%2, taken as intermediate value, and 50%, as in (Sieberling et al., 2024), avoiding higher values as it is established that structured pruning algorithms struggle to obtain reasonable performance above such sparsity rates."}, {"title": "4.2. Ablation Studies", "content": "In this section, we conduct different ablations studies to test the robustness of our proposed approach. In particular, we focus our analysis on the first stage (s1) and on the combination of depth pruning and width pruning, which are the distinctive aspects of our method."}, {"title": "Pruning Rows-Columns vs. Columns-Rows in s1", "content": "Pruning Rows-Columns vs. Columns-Rows in s1 We conduct an ablation study to analyze the first stage of 2SSP, hence the neuron-based pruning approach. As discussed earlier, given the intermediate representation of an FFN, our method removes entire neurons by pruning their corresponding rows and columns in the input and output projection matrices, respectively. We explore an \u201cinverted\" approach that, instead, prunes columns in the input projection matrix and rows in the output projection matrix, thereby preserving all neurons in the intermediate representation while reducing the dimensionality dmodel of the hidden state. The perplexity results in Table 4 show how the row-columns strategy employed by 2SSP is the most reliable choice since inverting the order or pruning leads to worse results even by orders of magnitude in terms of perplexity. This can be explained by the fact that pruning rows first and then columns (but not the other way around) preserves the network connectivity."}, {"title": "Neuron Selection based on L1 vs. L2 in s1", "content": "Neuron Selection based on L1 vs. L2 in s1 To evaluate the robustness of our neuron selection criterion, we study the impact of using $L_1$ norm as an alternative magnitude metric for neuron selection, comparing it against the $L_2$ norm used in Eq. (1). In this variation, we modify the importance score $s_j$, by replacing the $L_2$ norm with the $L_1$ norm. The perplexity results in Table 4 show that using $L_1$ leads to worse performance across all models and sparsity rates compared to the main 2SSP version based on $L_2$."}, {"title": "Running s1 only vs. s1 + s2", "content": "Running s1 only vs. s1 + s2 We also separate the two stages included in 2SSP, to show the effectiveness of their combination. However, while it is possible to ablate on the first stage only, we exclude the case of having the second stage only since this stage's applicability is constrained by the model's architecture and hence it is not applicable to all models. In fact, the number of Attention parameters in an LLM ranges from approximately 19% in Llama-2 to 33% in Mistral-v0.3, which limits the possibility of conducting this ablation across the three different sparsity rates chosen throughout the whole paper."}, {"title": "4.3. Hyperparameter Tuning", "content": "4.3. Hyperparameter Tuning\n2SSP relies on two main hyperparameters, namely \u03b1 used in Eq. (5) and the calibration set size $\\mathcal{D}_{cal}$. We detail their selection choices in this section. Note that for the calibration set size of the second stage, we set it to one sample, as done in (Sieberling et al., 2024), to balance pruning runtime vs. performance and provide a fair comparison, as also explained in Section 4.1. Therefore, we limit our analysis to the calibration set size applied to the first stage."}, {"title": "Choice of \u03b1 parameter", "content": "Choice of \u03b1 parameter A crucial aspect that makes our method effective is an accurate balancing between the number of parameters pruned in the first and the second stages. To achieve such balance, we introduced Eq. (5), which, given a desired sparsity rate s for the whole model, controls the number of Attentions to prune ($N_{Attn}$). This equation uses a parameter \u03b1 to regulate the rate of Attention pruning w.r.t. the overall sparsity: lower values of \u03b1 result in a more gradual increase in the number of Attentions w.r.t. s, while higher values of \u03b1 produce higher values of $N_{Attn}$ already at lower sparsity rates. Through empirical evaluation across multiple sparsity rates and models, we determine $\\alpha=1.5$ to be near-optimal, as shown in Figure 5."}, {"title": "5. Conclusions", "content": "In this paper, we introduced 2SSP, a new structured pruning algorithm that aims to combine Width Pruning for FFN submodules with Depth Pruning for Attentions. Our approach works in two stages by firstly pruning neurons in the intermediate state of FFN submodules, and then iteratively removing Attentions based on the model performance computed as perplexity. We tested 2SSP over three different families of LLMs ranging from 7B to 14B at three different sparsity rates. The results demonstrate how our proposed algorithm consistently outperforms the state-of-the-art baselines on both language modeling and downstream tasks. 2SSP achieves these results while requiring limited pruning runtime, which positions our method as state-of-the-art over the performance vs. pruning runtime trade-off. Finally, we conducted in-depth ablation and tuning studies to demonstrate the robustness of our proposed method, focusing in particular on the neuron pruning mechanism of the first stage and the sparsity rate balancing between the two stages."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "A. Additional Results", "content": "Here we present additional results of performance (perplexity on WikiText2) vs. pruning runtime of 2SSP and the baselines on Mistral-v0.3 7B (Figure 8), Qwen-2.5 7B (Figure 9), and Phi-3 14B (Figure 10). Across all models and sparsity rates, our 2SSP consistently demonstrates superior performance compared to existing pruning methods."}, {"title": "A.2. Few-shot results", "content": "Our experiments on downstream tasks reported in the main paper focused on zero-shot evaluation of pruned models across six different benchmarks. This section investigates the performance of our approach in a few-shot setting. In detail, we conduct 3-shot evaluations on the same benchmark datasets from Table 2, namely MMLU (Hendrycks et al., 2021), WinoGrande (WQ) (Sakaguchi et al., 2021), PiQA (Tata & Patel, 2003), HellaSwag (HS) (Zellers et al., 2019), and ARC (easy and challenge, in the following referred to as \u201cARC-e\u201d and \u2018ARC-c\u201d, respectively) (Clark et al., 2018). We present the results at a sparsity of 37.5% in Table 5, demonstrating once more that 2SSP consistently outperforms the baseline models across most tasks and maintains superior average performance in the few-shot regime across all models."}, {"title": "B. Experimental Details", "content": "We implement our experiments using the HuggingFace Transformers library (Wolf et al., 2020). The models we use are sourced from the HuggingFace model hub, with their corresponding repositories listed in Table 6.\nFor the calibration dataset, we use the Colossal Clean Crawled Corpus (C4) (Raffel et al., 2020), specifically the HuggingFace repository allenai/c4. Due to the substantial size of the training split, we fetch only a subset of samples extracted from the first shard of the dataset. Since sequence lengths in C4 vary, we first concatenated all sequences into a single corpus, tokenized it, and then divided it into segments of exactly 2048 tokens each.\nFor evaluation, we use the wikitext-2-raw-v1 split available from the wikitext repository on HuggingFace. Additionally, for perplexity evaluation, we include two other datasets: a subset of the validation split of C4 and samples from the sample-10BT subset of the HuggingFaceFW/fineweb-edu repository. Similar to the calibration dataset, the evaluation datasets are processed by concatenating their sequences, tokenizing the resulting corpus, and splitting it into sequences of 2048 tokens each. Specifically, Wikitext consists of 163 sequences, C4 contains 288 sequences, and FineWeb comprises 259 sequences."}, {"title": "C. Qualitative Results", "content": "In this section, we present some qualitative examples obtained from pruned models by evaluating their performance on concrete generation tasks. Specifically, we examine the Mistral-v0.3 model pruned using 2SSP at three distinct sparsity rates: 25%, 37.5%, and 50%. The pruned models are tested on the content generation task starting from two distinct prompts: \"Who is Albert Einstein?\u201d and \u201cExplain the theory of relativity\". We compare the quality of their generated responses at each sparsity rate, providing examples for each case. The results for the first prompt are shown in Table 7, while those for the second prompt are shown in Table 8.\nThe qualitative results confirm that, as expected, higher sparsity leads to a decrease in the quality of generated responses. However, it is noteworthy that despite the reduced accuracy, 2SSP is capable of maintaining the structural coherence of the sentences, often preserving the flow of the narrative. This coherence is achieved at the cost of factual correctness, with pruned models occasionally introducing incorrect or irrelevant information."}]}