{"title": "BLACKDAN: A BLACK-BOX MULTI-OBJECTIVE APPROACH FOR EFFECTIVE AND CONTEXTUAL JAILBREAKING OF LARGE LANGUAGE MODELS", "authors": ["Xinyuan Wang", "Victor Shea-Jay Huang", "Renmiao Chen", "Hao Wang", "Chengwei Pan", "Lei Sha", "Minlie Huang"], "abstract": "While large language models (LLMs) exhibit remarkable capabilities across various tasks, they encounter potential security risks such as jailbreak attacks, which exploit vulnerabilities to bypass security measures and generate harmful outputs. Existing jailbreak strategies mainly focus on maximizing attack success rate (ASR), frequently neglecting other critical factors, including the relevance of the jailbreak response to the query and the level of stealthiness. This narrow focus on single objectives can result in ineffective attacks that either lack contextual relevance or are easily recognizable. In this work, we introduce BlackDAN, an innovative black-box attack framework with multi-objective optimization, aiming to generate high-quality prompts that effectively facilitate jailbreaking while maintaining contextual relevance and minimizing detectability. BlackDAN leverages Multiobjective Evolutionary Algorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks across multiple objectives including ASR, stealthiness, and semantic relevance. By integrating mechanisms like mutation, crossover, and Pareto-dominance, BlackDAN provides a transparent and interpretable process for generating jailbreaks. Furthermore, the framework allows customization based on user preferences, enabling the selection of prompts that balance harmfulness, relevance, and other factors. Experimental results demonstrate that BlackDAN outperforms traditional single-objective methods, yielding higher success rates and improved robustness across various LLMs and multimodal LLMs, while ensuring jailbreak responses are both relevant and less detectable. Our code is available at https://github.com/MantaAI/BlackDAN.", "sections": [{"title": "1 Introduction", "content": "As large language models (LLMs) are increasingly integrated into various applications, the security of these models has become crucial [1, 2, 3]. Jailbreaking, the process of manipulating these models to bypass safety constraints and generate undesirable or harmful outputs, poses a significant challenge to maintaining their integrity and ethical use. Current jailbreaking methods depend excessively on affirmative cues from the model's prefix [4, 5], leading to the possibility of generating responses that are irrelevant or off-topic, leaving users helpless without outright rejecting prompts. This over-reliance underscores the urgent necessity for a more nuanced approach to prompt selection and optimization, especially through multi-objective strategies that focus on both effectiveness and usefulness.\nFurthermore, existing jailbreaking approaches struggle to explain why certain special directed vectors [6] result in model rejections, highlighting a significant challenge in comprehending the underlying distributions that dictate model behavior. The absence of clear explanations regarding the acceptance or rejection of prompts makes it challenging to establish a reliable safety boundary. Incorporating ranking mechanisms and conducting a thorough analysis of the distribution of responses can help provide interpretability and enable the identification of a more concrete safety boundary for prompts. These considerations are essential to ensure that jailbreaking attempts not only achieve success but also do so within explainable and safe constraints.\nAnother major limitation in current black-box jailbreak optimization strategies is the lack of transparency and interpretability. Most techniques rely on end-to-end optimization without adequately explaining the processes involved. The lack of interpretability makes it difficult to understand how jailbreak methods evolve or how specific adjustments impact the success rate of jailbreak attempts. Addressing this gap through a more structured explanation of the optimization processes will lead to more reliable and controllable jailbreak techniques.\nTo address these issues, we propose BlackDAN, a black-box, multi-objective, human-readable, controllable, and extensible jailbreak optimization framework. BlackDAN introduces a novel approach by optimizing multiple objectives simultaneously, including attack success rate (ASR), context relevance, and other factors. In contrast to traditional methods that focus solely on achieving a high ASR, BlackDAN adopts a more balanced approach by simultaneously addressing the trade-offs between effectiveness, interpretability, and safety. We hypothesize, verify, and analyze the concept of a safe boundary for prompts within this framework, using multi-objective optimization to refine the selection of useful and effective prompts while maintaining unsafety constraints.\nTo realize BlackDAN, we leverage the advances of Multiobjective Evolutionary Algorithms (MOEAs) [7], specifically the NSGA-II algorithm [8], which shows effectiveness in solving complex multi-objective problems. By incorporating pareto-dominance,mutation and crossover mechanisms, BlackDAN is capable of exploring a wider solution space while providing clear explanations of the optimization process. This allows for a more transparent and interpretable methodology for conducting jailbreak attacks, addressing the shortcomings of traditional end-to-end optimization techniques."}, {"title": "3 Methodology", "content": "In this setting, we aim to generate prompts that maximize the likelihood of generating unsafe responses from a target language model T while ensuring semantic consistency between the generated responses and harmful questions. This process is framed as a multi-objective optimization problem, and we utilize NSGA-II (Non-dominated Sorting Genetic Algorithm II) [8] to perform the optimization."}, {"title": "3.1 Fitness Functions", "content": "We define two continuous fitness functions to guide the optimization process:\nUnsafe Token Probability (Using llama_guard_2): The first fitness function, f1, evaluates the probability that a response R generated by T contains unsafe content. This is computed by the proxy model 1lama_guard_2, which outputs the log-probability of specific \"unsafe tokens.\""}, {"title": "Semantic Consistency (Using all-MiniLM-L6-v2)", "content": "In the black-box attack setting, we do not have direct access to the target model's internals or its embeddings. Instead, we utilize a pre-trained proxy model, such as the all-MiniLM-L6-v2, to generate sentence embeddings for both the harmful prompt and the candidate responses. These embeddings allow us to measure the semantic similarity between the prompt and the responses.\nThe second fitness function, f2, measures the semantic consistency between the generated response R and the harmful question Q. We use a pre-trained sentence embedding proxy model Mp (all-MiniLM-L6-v2) to compute the embeddings of both Q and R and then calculate their cosine similarity.\nLet $e_Q = Encode_M(Q)$ and $e_R = Encode_M(R)$ represent the embeddings of Q and R, respectively. The cosine similarity between these two embeddings is computed as:\n$f_2(P,Q) = Sim(e_Q, e_R) = \\frac{e_Q \\cdot e_R}{||e_Q|| ||e_R||}$,\nwhere $\\cdot$ represents the dot product, and $||e||$ is the Euclidean norm of the embedding vector.\nWe select the responses with the higher similarity scores as the jailbreaking outputs. This ensures that the selected response is semantically aligned with the harmful prompt, even though we rely on a proxy model for the embedding computations."}, {"title": "3.2 NSGA-II for Multi-Objective Jailbreaking Prompts Optimization", "content": "To find an optimal set of jailbreak prompts, we apply the NSGA-II algorithm. This algorithm performs multi-objective optimization based on two key criteria:\nDominance: A solution $P_1$ dominates another solution $P_2$ if it is better in at least one objective (e.g., higher unsafe token probability or better semantic consistency) and no worse in all other objectives. For a problem with m objectives, we define dominance as:\n$P_1 \\prec P_2$ if $\\forall i \\in \\{1, 2, ..., m\\}, f_i(P_1, Q) \\geq f_i(P_2, Q)$\nand $\\exists j \\in \\{1, 2, ..., m\\}, f_j(P_1, Q) > f_j(P_2, Q)$,\nwhere $f_i(P, Q)$ represents the fitness value for the i-th objective function given the prompt P and the harmful question Q.\nCrowding Distance: Once the population is sorted into non-dominated fronts, a crowding distance is assigned to each solution in order to maintain diversity. The crowding distance d(P) for an individual solution P in a given front is calculated across all m objective functions. For each objective $f_i$, the crowding distance is computed as:\n$d(P) = \\sum_{i=1}^{m} (\\frac{f_{i}^{next} - f_{i}^{prev}}{f_{i}^{max} - f_{i}^{min}})$,\nwhere $f_{i}^{next}$ and $f_{i}^{prev}$ are the fitness values of the neighboring solutions with respect to the i-th objective, and $f_{i}^{max}$ and $f_{i}^{min}$ are the maximum and minimum fitness values in the front for the i-th objective.\nThis ensures that the solutions selected from each non-dominated front are both optimal in terms of the multiple objectives and diverse with respect to each objective."}, {"title": "3.3 Genetic Operations: Crossover and Mutation", "content": "NSGA-II evolves the population using genetic operations:\nCrossover: The crossover operation creates two new offspring by recombining sentences from two parent prompts. Let $P_1$ and $P_2$ be the parent prompts. The offspring $C_1$ and $C_2$ are generated by randomly swapping sentences between the two parent prompts:\n$C_1, C_2 = Crossover(P_1, P_2)$.\nMutation: The mutation operation modifies a randomly selected word in a prompt with a synonym. Let W represent a randomly chosen word from prompt P, and let Syn(W) denote the set [31]of synonyms for W. A mutated prompt is generated as:\n$P' = Mutation(P)$ where $W' \\in Syn(W)$.\nThe complete algorithm is provided in Appendix Algorithm 1 and 2 due to space constraints."}, {"title": "4 Evaluation Metrics for Jailbreaking Prompts", "content": "In our black-box jailbreaking attack evaluation, we use two main metrics: Keyword-based Attack Success Rate (ASR) and the GPT-4 Metric. These metrics assess the effectiveness of jailbreaks in bypassing restrictions and generating unsafe content."}, {"title": "4.1 Keyword-based Attack Success Rate (ASR)", "content": "The Keyword-based ASR measures whether a generated response avoids predefined rejection patterns typically used by language models. This is determined by checking if the response R contains any phrases from a list of rejection keywords K (e.g., \"I'm sorry,\" \"As a language model,\" \"I cannot\"). The success indicator s(R) and ASR over N responses are defined as:\n$s(R) = \\begin{cases}\n1 & \\text{if } \\forall k \\in K, k \\notin R, \\\\\n0 & \\text{otherwise}.\n\\end{cases}$\n$ASR = \\frac{1}{N} \\sum_{i=1}^{N} s(R_i)$\nwhere Ri is the i-th response for harmful prompt Q\u00bf. The top-level ASR (ASRtop) is computed similarly but only for the highest-ranked responses."}, {"title": "4.2 GPT-4 Metric", "content": "The GPT-4 Metric uses an external model to evaluate whether a response R violates ethical, legal, or safety guidelines. The score g(Q, R) ranges from 1 (non-jailbroken, refusal to answer) to 10 (fully jailbroken, comprehensive violation). It is computed as:\n$g(Q, R) = GPT-4Judge(Q, R)$,\nwhere GPT-4Judge(Q, R) returns a score between 1 and 10. The success indicator $s_g(Q, R)$ and GPT-4-based metric are defined as:\n$s_g(Q, R) = \\begin{cases}\n1 & \\text{if } g(Q, R) \\geq 5, \\\\\n0 & \\text{otherwise}.\n\\end{cases}$\n$GPT4\\text{-}Metric = \\frac{1}{N} \\sum_{i=1}^{N} s_g(Q_i, R_i)$\nThis metric provides a qualitative measure of jailbreak success by assessing the ethical violations in the responses."}, {"title": "5 Experiment", "content": null}, {"title": "5.1 Experimental Setups", "content": "Text Dataset: For evaluating jailbreak attacks on large language models (LLMs), we utilize the AdvBench [4]. This dataset consists of 520 requests spanning various categories, including profanity, graphic depictions, threatening behavior, misinformation, discrimination, cyber-crime, and dangerous or illegal suggestions.\nMultimodal Dataset: To assess jailbreak attacks on multimodal large language models (MLLMs), we use the MM-SafetyBench [32]. This dataset encompasses 13 scenarios, including but not limited to illegal activity, hate speech, physical harm, and health consultations, with a total of 5,040 text-image pairs.\nModels: We utilize state-of-the-art (SOTA) open-source large language models (LLMs), including Llama-2-7b-hf [33], Llama-2-13b-hf [33], Internlm2-chat-7b [34], Vicuna-7b [35], AquilaChat-7B [36], Baichuan-7B, Baichuan2-13B-Chat [37], GPT-2-XL [38], Minitron-8B-Base [39], Yi-1.5-9B-Chat [40], and Internlm2-chat-7b [34]. For multimodal LLMs, we employ llava-v1.6-mistral-7b-hf [41] and llava-v1.6-vicuna-7b-hf [41] to demonstrate the effectiveness of our approach in expanding from unimodal to multimodal capabilities."}, {"title": "5.2 Single-Objective(harmfulness) Jailbreaking Optimization", "content": null}, {"title": "5.3 Multi-Objective Optimization", "content": null}, {"title": "6 Conclusion", "content": "In this paper, we introduced BlackDAN, a multi-objective, controllable jailbreak optimization framework for large language models (LLMs) and multimodal large language models (MLLMs). Beyond optimizing for attack success rate (ASR) and stealthiness, BlackDAN addresses the critical challenge of context consistency by ensuring that jailbreak responses remain semantically aligned with the original harmful prompts. This ensures that responses are not only evasive but also relevant, increasing their practical impact. Leveraging the NSGA-II algorithm, our method significantly improves over traditional single-objective techniques, achieving higher success rates and more coherent jailbreak responses across various models. Furthermore, BlackDAN is highly extensible, allowing the integration of any number of user-defined objectives, making it a versatile framework for a wide range of optimization tasks. The inclusion of multiple objectives\u2014specifically ASR, stealthiness, and semantic consistency-sets a new benchmark for generating useful and interpretable jailbreak responses while maintaining safety and robustness in evaluation."}, {"title": "A Appendix", "content": null}, {"title": "Explanation of Symbols and Process in algorithm 1", "content": "Inputs: Po: Initial prototype prompt. Q: Harmful question to guide the optimization process. N: Population size, the number of prompts in each generation. G: Number of generations to evolve the population. m: Mutation rate that controls how often mutations happen in the population.\nFitness Functions: f1: Unsafe token probability based on a model like Llama Guard 2. f2: Semantic similarity to the harmful question, based on a sentence embedding model.\nGenetic Operations: Crossover: Combines parts of two parent prompts to create offspring. Mutation: Randomly alters parts of a prompt to introduce diversity.\nNon-Dominated Sorting: Solutions are sorted based on dominance criteria-those that are not dominated by any other solutions form the first front F\u2081, and so on.\nCrowding Distance: Used to maintain diversity in the population. Individuals with a higher crowding distance are selected preferentially when fronts overlap.\nSelection and Truncation: After generating offspring, the combined population is sorted, and the best individuals are retained to form the next generation."}, {"title": "Algorithm 2 Non-Dominated Sorting Algorithm", "content": "1: Input: Population P, fitness values {f1(P), f2(P)} for each P\u2208 P\n2: Output: Sorted fronts F1, F2, ...\n3: Initialize fronts F = \u00d8\n4: Initialize domination count n[P] = 0 for each individual P \u2208P\n5: Initialize domination set S[P] = () for each individual P\u2208 P\n6: for each individual PEP do\n7:  for each individual Q \u2208 P, Q \u2260 P do\n8:   if P dominates Q then\n9:   Add Q to the domination set S[P]\n10:  else ifQ dominates P then\n11:   Increment domination count n[P] = n[P] + 1\n12:  end if\n13:  end for\n14:  if n[P] = 0 then\n15:   Add P to the first front F\u2081\n16:  end if\n17: end for\n18: Set front counter i = 1\n19: while Fi \u2260 \u00d8 do\n20:  Initialize next front Fi+1 = \u00d8\n21:  for each individual P\u2208 Fi do\n22:   for each individual Q \u2208 S[P] do\n23:   Decrement domination count n[Q] = n[Q] \u2212 1\n24:   if n [Q] = 0 then\n25:   AddQ to front Fi+1\n26:   end if\n27:  end for\n28: end for\n29: Increment front counter i = i + 1\n30: end while\n31: Return sorted fronts F1, F2,..."}]}