{"title": "How Propense Are Large Language Models at Producing Code Smells? A Benchmarking Study", "authors": ["Alejandro Velasco", "Daniel Rodriguez-Cardenas", "David N. Palacio", "Luftar Rahman Alif", "Denys Poshyvanyk"], "abstract": "Large Language Models (LLMs) have shown significant potential in automating software engineering tasks, particularly in code generation. However, current evaluation benchmarks, which primarily focus on accuracy, fall short in assessing the quality of the code generated by these models, specifically their tendency to produce code smells. To address this limitation, we introduce CodeSmellEval, a benchmark designed to evaluate the propensity of LLMs for generating code smells. Our benchmark includes a novel metric: Propensity Smelly Score (PSC), and a curated dataset of method-level code smells: CodeSmellData. To demonstrate the use of CodeSmellEval, we conducted a case study with two state-of-the-art LLMS, CodeLlama and Mistral. The results reveal that both models tend to generate code smells, such as simplifiable-condition and consider-merging-isinstance. These findings highlight the effectiveness of our benchmark in evaluating LLMs, providing valuable insights into their reliability and their propensity to introduce code smells in code generation tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have been increasingly adopted to automate multiple tasks in software engineering (SE), including code completion [1]-[3], code summarization [4], program repair [5], clone detection [6], and test case generation [7]. The applications of LLMs in SE have evolved from classification tasks (e.g., defect detection, requirements triage, vulnerability severity) to generative tasks, such as sequence synthesis, where a prompt/context is given to instruct the LLMs to generate text (e.g., code summarization) and/or code (e.g., code generation) [8]. Ensuring the quality of the predictions made by those models is essential, regardless of the type of task. Generative tasks, in particular, require rigorous evaluation due to the complexity of structured information within the source code. Among the evaluation criteria, we have focused specifically on code smells.\nCode smells are defined as symptoms of poor design and implementation choices in SE. Previous studies have shown the negative impact of code smells on the comprehensibility and maintainability of code [9], [10]. Research has increas-ingly focused on studying the use of LLMs for detecting and refactoring code smells [9], [11]\u2013[18], and these models have demonstrated some success when their output is validated against the ground-truth. However, traditional metrics such as BLEU [19], CodeBLEU [20], ROUGE [21], and METEOR [22] fall short at assessing the LLMs' tendency for introducing code smells while generating code. This lim-itation is particularly relevant for developers using commer-cial LLMs (e.g., ChatGPT, Copilot, and Claude \u00b9), which might generate code with smells like invalid-names, too-many-arguments, or unnecessary-lambda. Although LLMs are becoming more integrated into SE, and code smell detection has been extensively studied, there is still little known as to how likely LLMs are at generating smells.\nDespite an apparent success of using LLMs for automating software tasks, developers still face two key challenges: 1) they need more information to assess which LLM is more reliable, and 2) they cannot evaluate model performance beyond accuracy, as traditional metrics often overestimate code quality. This leads to important questions: What is the propensity of an LLM to generate code smells? and Which types of code smells are most propense to be generated?\nTo address these concerns, we propose a new benchmark, CodeSmellEval, designed to estimate the propensity of an LLM at generating code smells. CodeSmellEval draws inspiration from Syntax Decomposition [23], [24] and introduces a model-agnostic evaluation metric called the Propensity Smelly Score (PSC). PSC analyzes the logits from the final layer of an LLM, providing insights into the model's propensity of generating code smells. Additionally, our benchmark includes a new dataset, CodeSmellData, comprising 142k unique method-level code smells of 13 different types, mined from GitHub.\nTo demonstrate the utility of our benchmark, we devised an exploratory case study using CodeSmellEval in two LLMs (i.e., CodeLlama and Mistral). We assessed the distribution of PSC values for each type of code smells across both models. Our analysis revealed that both models are propense to generate the same types of code smells, with a few exceptions. For example, consider-merging-isinstance (R1701), chained-comparison (R1716), and broad-exception-caught (W0718) were among the top, whereas disallowed-name (C0104), too-many-arguments (R0913), and non-ascii-name (C2401) were bellow the PSC propensity threshold. We hope that our findings will shed light on the propensity of current LLMs to introduce code smells, enabling a more systematic and rigorous evaluation of code quality beyond canonical accuracy.\nTo summarize, our key contributions are as the following: 1) a new metric for evaluating the propensity of LLMs to produce"}, {"title": "II. BENCHMARK", "content": "In this section, we present our benchmark, CodeSmellEval, which consists of three key components: (1) PSC, a metric designed to estimate the propensity of an LLM to introduce code smells, (2) our evaluation dataset, CodeSmellData, com-prising 142k curated instances of method-level code smells mined from GitHub, and (3) a protocol, which outlines the methodology for using our benchmark."}, {"title": "A. Propensity Smelly Score (PSC)", "content": "PSC is an evaluation metric that works by extracting the non-normalized log-probabilities (logits) $Z$ for each token prediction from the last hidden layer of a decoder-based trans-former (e.g., GPT). To estimate the probabilities of expected tokens in a sequence w, we apply the softmax function ($\\sigma$) to each logit $z_i$. For a token $w_i = t$, where t belongs to the vocabulary $V$, we compute the probability $P(w_i = t|w_{<i}) \\approx \\sigma(z_i)_t = \\frac{e^{z_{i,t}}}{\\sum_{t'\\in V}e^{z_{i,t'}}}$. In this equation, $z_{i,t}$ represents the logit for the expected token t at position i, and the denominator normalizes the probabilities by summing the exponentiated logits for all vocabulary tokens. Since decoder-based mod-els are auto-regressive, the preceding context influences the computation of $\\sigma(z_i)_t$.\nMeaningful Structures M. Using an alignment function ($\\delta$), tokens $w_i \\in w$ are grouped into a meaningful structure $\\mu \\in M$ (Eq. 1). Then, an aggregation function ($\\theta$) (Eq. 2) com-putes a central tendency statistic (e.g., median, mean, mode) of their probabilities, resulting in an overall probability estimate for predicting $\\mu$ (i.e., propensity score). Once $P(w_i|W_{<i})$ is computed, the probability of each token's predicted value is treated independently to calculate the overall estimate of $\\mu$. Eq. 2 outlines how to compute the PSC for a meaningful concept $\\mu$. Indexes that points out to smelly code are defined as $0 \\le i \\le k \\le j \\le |w|$.\n$\\delta_{\\mu}(w) : w \\rightarrow (i, j), \\mu \\in M$ (1)\n$\\theta_{\\mu}(w,i,j) = E_{k=i}^j[P(W_k|W_{0...k-1})]$ (2)\nCode Smells. The definition of the set of meaningful structures $M$ used in function $\\delta$ (Eq. 1) depends on the problem context. For example, token probabilities can be aggregated using syntax-based decomposition, based on elements defined by the grammar of a programming language (e.g., identifiers, conditionals, statements). In this paper, we propose defining the set of concepts $M$ using types of method-level code smells."}, {"title": "III. CASE STUDY", "content": "To demonstrate the practical application of our benchmark, we conducted a case study evaluating the propensity of two LLMs to generate the code smells in CodeSmellData. We formulated the following main research question:"}, {"title": "RQ1 [Propensity of Code Smells]", "content": "What types of code smells are more propense to be generated by M1 and M2?\nSelected LLMs. Although PSC is model-agnostic, we se-lected two popular decoder-based transformers as they are well-suited for generative tasks. The first model, CodeLlama-7b-Instruct-hf (M1) [29], has a vocabulary size of 32, 016 tokens. The second model, Mistral-7B-v0.3 (M2) [30], has a vocabulary size of 37, 768 tokens. Both models have 7 billion parameters, 32 hidden layers, and 32 attention heads. The models were loaded on an Ubuntu 20.04 system with an AMD EPYC 7532 32-Core CPU, an NVIDIA A100 GPU with 40GB VRAM, and 1TB of RAM.\nEvaluation Methodology. To address RQ1, we computed PSC global estimates (refer to Sec. II-A) for both models (i.\u0435., M1 and M2) using the collected snippets for all identified code smells. Due to memory constraints, we limited the evaluation to datapoints in CodeSmellData with a maximum size of 400 tokens. We further sampled 100 datapoints from each of the 13 code smells with at least 100 instances in the dataset to ensure fair statistical treatment. Also, note that all the selected datapoints came from distinct Python methods. We then followed the protocol steps (refer to Sec. II-C) to compute the global estimates of PSC for each code smell."}, {"title": "A. Results & Discussion", "content": "Fig. 1 presents the probability scores computed for each code smell using M1 and M2. Remarkably, 10 out of 13 code smells have a probability higher than the propensity threshold of 0.5, indicating that both models are propense to generating these smells. As shown in Table II, the top five smells with the highest PSC in both models are consider-merging-isinstance (R1701), chained-comparison (R1716), broad-exception-caught (W0718), too-many-nested-"}, {"title": "RQ1 [Propensity of Code Smells]", "content": "Both CodeLlama-7b-Instruct-hf [29] and Mistral-7B-v0.3 [30] are propense to generate code-smells Convention, Refactor, and Warnings. With few exceptions: disallowed-name (C0104), too-many-arguments (R0913), and non-ascii-name (C2401).\nWe believe that both LLMs are propense to generate the detected code smells, as they are trained on publicly available code, which often contains quality issues [29]. This assump-tion is supported by our dataset creation process, since we extracted code smells from mined code snippets in public repositories. However, a separate study is necessary to confirm this hypothesis, as this paper primarily focuses on demonstrat-ing the use of CodeSmellEval. An in-depth evaluation of the underlying reasons why code smells appear in generated code is beyond the scope of this paper."}, {"title": "IV. RELATED WORK", "content": "Considerable research has been devoted to collecting code smell data, detecting, and repairing. Code smell often indicates deeper issues in the codebase, affecting code quality and performance [9]\u2013[13], [31]\u2013[33]. Our related work is focused on research on code smell datasets and papers that have reported benchmarks on generating code smells.\nNasrabadi et al. [34] introduced an SLR with the most updated code smells datasets. Most datasets are training or testing code smell detection on LLMs [12], [33], [35]. Datasets can be created manually [36]\u2013[38] or automatically [39], [40] via refactorings, however, the validations are time-consuming so most automatically generated datasets are not validated [34]. Our dataset CodeSmellData is automatically mined and manually curated to confirm the code smell type and location.\nExisting benchmarks focus on detecting code smells using different techniques (e.g., SVM [35], few-shot learning [12], chain-of-thoughts [11]). CodeLMSec [12] evaluates code gen-eration models' vulnerability to generating insecure prompts, while LCG [2], iSmell [18], and PromptSmell [41] examine LLM-based approaches. Unlike these, our benchmark uses logits to assess an LLM's propensity for generating smelly code, rather than merely classifying or detecting it. By aligning meaningful tokens to highlight smelly segments, our approach offers a novel, interpretable metric [28], [42], suitable for black-box code generation models."}, {"title": "V. FUTURE PLANS", "content": "Based on the results of our case study, we demonstrated the computation of PSC for 13 method-level code smells. Our next step is to conduct a systematic empirical evaluation that includes additional types of smells and a broader range of LLMs. This effort will involve mining more instances of underrepresented code smells in CodeSmellData. Specifically, we will analyze sampling error when computing PSC for each code smell to mitigate the risk of sampling bias and ensure the representativeness of all smell types in CodeSmellData. Furthermore, since our analysis has so far focused solely on method-level granularity, we plan to expand CodeSmellData to include higher-level smells, such as god-class and feature-envy. In future versions of CodeSmellData, we will use other static analysis tools alongside Pylint to reduce dependency on a single tool for identifying code smell locations, which is crucial for implementing the alignment function (Eq. 1) in CodeSmellEval.\nOur proposed benchmark identifies the types of code smells that LLMs are propense to generate. Future research should focus on explaining this phenomenon by uncovering the rea-sons behind LLMs' propensity to introduce specific types of code smells. For instance, by identifying the most relevant features in the input that influence the prediction of specific types of code smells. This research direction is essential for enhancing the trustworthiness [23] of LLMs and also developing defense techniques to mitigate such behavior. We believe interpretability techniques such as LIME [43] or SHAP [44] will help us to reach this goal."}]}