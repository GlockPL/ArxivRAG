{"title": "Towards Domain Adaptive Neural Contextual Bandits", "authors": ["Ziyan Wang", "Hao Wang"], "abstract": "Contextual bandit algorithms are essential for solving real-world decision making\nproblems. In practice, collecting a contextual bandit's feedback from different\ndomains may involve different costs. For example, measuring drug reaction from\nmice (as a source domain) and humans (as a target domain). Unfortunately, adapting\na contextual bandit algorithm from a source domain to a target domain with\ndistribution shift still remains a major challenge and largely unexplored. In this\npaper, we introduce the first general domain adaptation method for contextual\nbandits. Our approach learns a bandit model for the target domain by collecting\nfeedback from the source domain. Our theoretical analysis shows that our algorithm\nmaintains a sub-linear regret bound even adapting across domains. Empirical\nresults show that our approach outperforms the state-of-the-art contextual bandit\nalgorithms on real-world datasets.", "sections": [{"title": "1 Introduction", "content": "Contextual bandit (CB) algorithms have shown great promise for naturally handling explo-\nration/exploitation trade-off problems with optimal statistical properties. Notably, LinUCB [1]\nand its various adaptations [2-9], have been shown to be able learn the optimal strategy when all data\ncome from the same domain. However, these methods fall short when applied to data from a new\ndomain. For example, a drug reaction prediction model trained by collecting feedback from mice\n(the source domain) may not work for humans (the target domain).\nSo, how does one effectively explore a high-cost target domain by only collecting feedback from a\nlow-cost source domain, e.g., exploring drug reaction in humans by collecting feedback from mice\nor exploring real-world environments by collecting feedback from simulated environments? The\nchallenge of effective cross-domain exploration is multifaceted: (1) The need for effective exploration\nin both the source and target domains depends on the quality of representations learned in the source\ndomain, which in turn requires effective exploration; this leads to a chicken-and-egg dilemma. (2)\nAligning source-domain representations with target-domain representations is nontrivial in bandit\nsettings, where ground-truth may still be unknown if the action is incorrect. (3) Balancing these\naspects - effective exploration and accurate alignment in bandit settings \u2013 is also nontrivial.\nTo address these challenges, as the first step, we allow our method to simultaneously perform effective\nexploration and representation alignment, leveraging unlabeled data from both the source and target\ndomains. Interestingly, our theoretical analysis reveals that naively doing so leads to sub-optimal\naccuracy/regret (verified by our empirical results) and naturally leads to additional terms in the\ntarget-domain regret bound. We then follow the regret bound derived from our analysis to develop an\nalgorithm that adaptively collect feedback from the source domain while aligning representations\nfrom the source and target domains. Our contributions are outlined as follows:\n\u2022 We identify the problem of contextual bandits across domains and propose domain-adaptive\ncontextual bandits (DABand) as a the first general method to explore a high-cost target domain\nwhile only collecting feedback from a low-cost source domain."}, {"title": "2 Related Work", "content": "Contextual Bandits. In the realm of adaptive decision-making, contextual bandit algorithms,\nepitomized by LinUCB [1], have carved a niche in efficiently balancing the exploitation-exploration\nparadigm across a spectrum of use cases, notably in complex adaptive systems such as recommender\nsystems [1, 10]. These algorithmic frameworks, along with their myriad adaptations [2-4, 7, 5, 6, 9,\n8, 11], have decisively outperformed conventional bandit models that operate devoid of contextual\nawareness [12]. This superiority is underpinned by theoretical models, paralleling the insights\nin [12], where LinUCB variants are validated to conform to optimal regret boundaries in targeted\nscenarios [13]. However, a discernible gap in these methodologies is their reduced efficacy in a new\ndomain, particularly when getting feedback involves high cost. Our proposed DABand bridges this\ngap by adeptly aligning both source-domain and target-domain representations in the latent space,\nthereby reducing performance drop when transfer across different domains.\nDomain Adaptation. The landscape of domain adaptation has been extensively explored, as ev-\nidenced by a breadth of research [14-25]. The primary objective of these studies has been the\nalignment of source and target domain distributions to facilitate the effective generalization of models\ntrained on labeled source data to unlabeled target data. This alignment is conventionally attained\neither through the direct matching of distributional statistics [15, 26, 27, 20, 24] or via the integration\nof an adversarial loss [28-30, 19, 31, 21-23]. The latter, known as adversarial domain adaptation,\nhas surged in prominence, bolstered by its theoretical foundation [32, 33, 19, 34], the adaptability\nof end-to-end training in neural network architectures, and its empirical efficacy. However, these\nmethods only work in offline settings, and assumes complete observability of labels in the source\ndomain. Therefore they are not applicable to our online bandit settings.\nDomain Adaptation Related to Bandits. There is also work related to both domain adaptation and\nbandits. Specifically, Guo et al. [35] propose a domain adaptation method using a bandit algorithm to\nselect which domain to use during training.\nWe note that their goal and setting is different from our DABand's. (1) [35] focuses on improving\naccuracy in a typical, offline domain adaptation setting. In contrast, our DABand focuses on\nminimizing regret in an online bandit setting. (2) [35] assumes complete access to ground-truth labels\nin the source domain, while in our bandit setting, ground-truth may still be unknown if the action is\nincorrect. Therefore their work is not applicable to our setting."}, {"title": "3 Theory", "content": "In this section, we formalize the problem of contextual bandits across domains and derive the\ncorresponding regret bound. We then develop our DABand inspired by this bound in Sec. 4. All\nproofs of lemmas, theorems can be found in the Appendix.\nNotation. We use [k] to denote the set {1,2,\u2026\u2026, k}, for $k \\in \\mathbb{N}^+$. The Euclidean norm of a vector\n$x \\in \\mathbb{R}^d$ is denoted as $||x||_2 = \\sqrt{x^Tx}$. We denote the operator norm and Frobenius norm of a matrix\n$W\\in \\mathbb{R}^{m\\times n}$ as $||W||$ and $||W||_F$, respectively. Given a semi-definite matrix $A \\in \\mathbb{R}^{d\\times d}$ and a vector\n$x \\in \\mathbb{R}^d$, we denote the Mahalanobis norm as $||x||_A = \\sqrt{x^T Ax}$. For a function $f(T)$ of a parameter $T,\nwe denote as $O(f(T))$ the terms growing in the order of $f (T)$, ignoring constant factors. We assume\nall the action spaces are identical and with cardinality of K, i.e., $|A_1| = |A_2| = \\dots = |A_N| = K$.\nWe use $\\langle\\cdot,\\cdot\\rangle$ to denote the inner product of two vectors. We denote as $a_i^D$ the action $i$ in domain $D$,\nand omit the subscript $D$ when the context is clear, e.g., $x_{ia}^D = x_{ia}$."}, {"title": "3.1 Preliminaries", "content": "Typical Contextual Bandit Setting. Suppose we have N samples from an unknown domain D,\nwhich is represented as $\\{x_{ia}, a \\}_\\{i\\in[N], a\\in[K]\\}$. Here, $x_{ia} \\in D$ denotes the context for action $a \\in A_i$,"}, {"title": "3.2 Our Cross-Domain Contextual Bandit Setting", "content": "Typical contextual bandits operate in a single domain D. In contrast, our cross-domain bandit\nsetting involves a source domain S and a target domain T. In this setting, one can only collect\nfeedback (reward) from the source domain, but not from the target domain. Specifically: (1) We\nassume a low-cost source domain (experiments on mice), where for each round i, one has access\nto the contexts $\\{x_{ia}^S\\}\\_\\{a\\in[K]\\}$ for each candidate actions, chooses one action $\\hat{a}\\_i$, and receives reward\n$r_i^S \\thicksim r(x_{i,\\hat{a}\\_i})$. (2) Additionally, we assume a high-cost target domain where collecting feedback (reward) is\nexpensive (experiments on humans); therefore, one only has access to the contexts $\\{x_{ia}^T\\}\\_\\{a\\in[K]\\}$ for\neach candidate actions for each round, but cannot collect feedback (reward) $r_i^T$ for any action in\nthe target domain. The goal in our cross-domain contextual bandits is to learn a policy of choosing\nactions $a_i^T$ in the target domain to minimize the target-domain zero-shot regret for N (hypothetical)\nfuture rounds:\n$R_T = \\sum\\_{i=1}^{N} r\\_{i, a_i^{*T}} - \\sum\\_{i=1}^{N} r\\_{i, \\hat{a}\\_i^S}.$\nDifference between Eqn. 1 and Eqn. 2. In the typical setting, Eqn. 1 uses different updated policies\nfor each round; this is also true for the source domain. In contrast, the target regret in Eqn. 2 use\nthe same fixed policy obtained from the source domain for all N (hypothetical) rounds because\none cannot collect feedback from the target domain. See the difference between Definition 3.4 and\nDefinition 3.5 below for a more formal comparison."}, {"title": "3.3 Formal Definitions of Error", "content": "Definition 3.1 (Labeling Function and Hypothesis). We define the labeling function $f_D$ and the\nhypothesis $h$ for domain D as follows:\n$f_D(x^D_a) = r(x^D_a) = \\langle\\theta^*, \\phi(x^D_a)\\rangle + \\epsilon_i$\n$h(x^D_a) = \\hat{f}(x^D_a) = \\langle\\hat{\\theta}, \\hat{\\phi}(x^D_a)\\rangle + \\epsilon$\nwhere $\\theta^*$ is the optimal predictor for domain D, $\\hat{\\theta}$ denotes the estimated predictor, and $\\epsilon_i$ is the\nrandom noise.\nNext, we analyze how well our hypothesis $h$ estimates the labeling function $f$ (i.e., ground-truth\nhypothesis). Definition 3.2 below quantifies the closeness between two hypotheses by calculating the\nabsolute difference in their estimated rewards.\nDefinition 3.2 (Estimated Error between Two Hypotheses). Assuming all contexts $x_i^D$ are from\ndomain D, the error between two hypotheses $h_1, h_2 \\in H$ on domain D given selected actions\n$\\{a_i\\}\\_{i\\in[N]} \\in [K]$ is\n$\\mathcal{E}\\_D(h\\_1, h\\_2) = \\sum\\_{i=1}^{N} | h\\_1(x\\_i^{Da\\_i}) - h\\_2(x\\_i^{Da\\_i}) | = \\sum\\_{i=1}^{N} | \\langle\\hat{\\theta}\\_1, \\hat{\\phi}\\_1(x\\_i^{Da\\_i})\\rangle - \\langle\\hat{\\theta}\\_2, \\hat{\\phi}\\_2(x\\_i^{Da\\_i})\\rangle|.$\nNote that the domain D above can be the source domain S or the target domain T. We then define\nthe error of our estimated hypothesis in these domains.\nDefinition 3.3 (Source- and Target-Domain Error). With N samples from the source domain S,\nand $f_S$ denoting the labeling function for S, the source-domain error is then\n$\\epsilon\\_S(f\\_S, h) = \\sum\\_{i=1}^{N} | f\\_S(x\\_i^{Sa\\_i}) - h(x\\_i^{Sa\\_i})| = \\sum\\_{i=1}^{N} | \\langle\\theta\\_S, \\phi\\_S(x\\_i^{Sa\\_i})\\rangle - \\langle\\hat{\\theta}, \\hat{\\phi}(x\\_i^{Sa\\_i})\\rangle | ,$\nwhere $x_i^{Sa_i}$ comes from S. Furthermore, $\\hat{a}\\_i = \\arg \\max\\_a h(x\\_{ia}^S)$ and $a\\_i^* = \\arg \\max\\_a f\\_S(x\\_{ia}^S)$.\nFor simplicity, we shorten the notation $\\epsilon\\_S(f\\_S, h)$ to $\\epsilon\\_S(h)$ and similarly use $\\epsilon\\_T(h)$ for domain T.\nAssuming $x_i^{Ta_i}$ comes from T, the estimated error for the target domain is then:\n$\\epsilon\\_T(f\\_T, h) = \\sum\\_{i=1}^{N} | f\\_T(x\\_i^{Ta\\_i}) - h(x\\_i^{Ta\\_i}) | = \\sum\\_{i=1}^{N} | \\langle\\theta\\_T^*, \\phi\\_T^*(x\\_i^{Ta\\_i})\\rangle - \\langle\\hat{\\theta}, \\hat{\\phi}(x\\_i^{Ta\\_i})\\rangle |$"}, {"title": "3.4 Source Regret and Target Regret", "content": "Below define the regret for the source and target domains.\nDefinition 3.4 (Source Regret). Assuming $x_i^{Sa_i}$ comes from domain S, the source regret (i.e., the\nregret in the source domain) is\n$R\\_S = \\sum\\_{i=1}^{N} (f\\_S(x\\_i^{Sa\\*\\_i}) - f\\_S(x\\_i^{Sa\\_i})) = \\sum\\_{i=1}^{N} (\\langle\\theta\\_S^*, \\phi\\_S(x\\_i^{Sa\\*\\_i})\\rangle - \\langle\\theta\\_S^*, \\phi\\_S(x\\_i^{Sa\\_i})\\rangle).$\nThe goal in our cross-domain contextual bandits is to learn a policy of choosing actions $a_i^T$ in the\nhigh-cost target domain (e.g., human experiments) by collecting feedback only in the source domain\n(e.g., mouse experiments). Formally, we would like to minimize the target regret as defined below.\nDefinition 3.5 (Target Regret and Problem Formulation). Denoting the estimated hypothesis as\n$h = \\{\\phi, \\theta\\}$, the target regret we aim to minimize is defined as\n$R\\_T = \\sum\\_{i=1}^{N} (f\\_T(x\\_i^{Ta\\*\\_i}) - f\\_T(x\\_i^{T\\hat{a}\\_i}))$ s.t. $\\hat{a}\\_i = \\arg \\max\\_a \\langle\\hat{\\theta}, \\hat{\\phi}(x\\_{i,a})\\rangle + \\alpha||\\hat{\\phi}(x\\_{i,a})||[A\\_S]^{-1}, \\hat{h} = \\arg \\min\\\\_h \\epsilon\\_S(h),$\nwhere $A\\_S = \\gamma I + \\sum\\_{i=1}^{N} \\hat{\\phi}(x\\_{ia})[\\hat{\\phi}(x\\_{ia})]^T$ denotes the context matrix accumulated by the selected\ncontext features in the source domain."}, {"title": "3.5 Cross-Domain Error Bound for Regression", "content": "Prior to introducing our final regret bound, it is necessary to define an additional component. A\nfundamental challenge in general domain adaptation problems is to manage the divergence between\nsource and target domains. Unfortunately, previous domain adaptation theory only covers classifica-\ntion tasks [36, 19]. In contrast, the problem of contextual bandits is essentially a reward regression\nproblem. To address this challenge, we introduce the following new definition to bound the error\nbetween the source and target domains.\nDefinition 3.6 ($\\mathcal{H}\\Delta\\mathcal{H}$ Hypothesis Space for Regression). For a hypothesis space $\\mathcal{H}$, the symmetric\ndifference hypothesis space $\\mathcal{H}\\Delta\\mathcal{H}$ is the set of hypotheses s.t.\ng$\\in \\mathcal{H}\\Delta\\mathcal{H} \\quad \\leftarrow \\quad g(x) = |h(x) - h'(x)|$.\nWe then can further define the Divergence for $\\mathcal{H}\\Delta\\mathcal{H}$ Hypothesis Space:\n$\\delta\\_{\\mathcal{H}\\Delta\\mathcal{H}}(S, T) = 2 \\sup\\_{h,h'\\in \\mathcal{H}} |\\mathbb{E}\\_{x\\thicksim S}[|h(x) \u2013 h'(x)|] \u2013 \\mathbb{E}\\_{x\\thicksim T}[|h(x) - h'(x)|]|$.\nDefinition 3.7 (Optimal Hypothesis). The optimal hypothesis, denoted as $h^*$, can balance the\nestimated error for both source and target domain. Formally,\n$h^* = arg min \\_\\{h\\in H\\} \\epsilon\\_S(h) + \\epsilon\\_T(h),$\nand we define the minimum estimated error for the optimal hypothesis $h^*$ as\n$\\psi = \\epsilon\\_S(h^*) + \\epsilon\\_T(h^*)$."}, {"title": "3.6 Final Regret Bound", "content": "With all the definitions of source/target regret in Definition 3.4 and Definition 3.5, we can then bound\nthe target regret using Theorem 3.1 below."}, {"title": "4 Method", "content": "With Theorem 3.1, we can then design our DABand algorithm to obtain optimal target regret. We\ndiscuss how to translate each term in Eqn. 5 to a differential loss term in Sec. 4.1 and then put them\ntogether in Sec. 4.2."}, {"title": "4.1 From Theory to Practice: Translating the Bound in Eqn. 5 to Differentiable Loss Terms", "content": "Source Regret. Inspired by Neural-LinUCB [11], we use LinUCB [1] to update $\\theta$ and the following\nempirical loss function when updating the encoder parameters $\\phi$ by back-propagation in round i:\n$L\\_{RS} = \\sum\\_{a=1}^{K} | \\langle\\theta, \\hat{\\phi}(x\\_{ia})\\rangle - r(x\\_{i,\\hat{a}}) |^2,$\nwhere $\\theta$ is the contextual bandit parameter, and $\\hat{\\phi}$ is the encoder shared by the source and target\ndomains, i.e., we set $\\phi\\_S = \\phi\\_T$ during training."}, {"title": "5 Experiments", "content": "In this section, we compare DABand with existing methods on real-world datasets.\nDatasets. To demonstrate the effectiveness of our DABand, We evaluate our methods in terms of\nprediction accuracy and zero-shot target regret on three datasets, i.e., DIGIT [37], VisDA17 [38],\nand S2RDA49 [39]. See details for each dataset in Appendix B.\nBaselines. We compare our DABand with both classic and state-of-the-art contextual bandit algo-\nrithms, including LinUCB [1], LinUCB with principle component analysis (PCA) pre-processing\n(i.e., LinUCB-P), Neural-LinUCB [11] (NLinUCB), and Neural-LinUCB variant that incorporates\nPCA (i.e., NLinUCB-P). Details for each baselines and discussion are in Appendix C. Note that\ndomain adaptation baselines are not applicable to our setting because it only works in offline settings\nand assumes complete observability of labels in the source domain (see Appendix C for details).\nZero-Shot Target Regret (Accuracy). We evaluate different methods on the zero-shot target regret\nsetting in Definition 3.5, where all methods can only collect feedback from the source domain, but\nnot the target domain. In this setting, accuracy is equal to 1 minus the average target regret, i.e.,\nACC = 1 -RT, where RT is defined in Eqn. 2 and Definition 3.5. Therefore higher accuracy\nindicates lower target regret and better performance."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a novel domain adaptive neural contextual bandit algorithm, DABand,\nwhich adeptly combines effective exploration with representation alignment, utilizing unlabeled data\nfrom both source and target domains. Our theoretical analysis demonstrates that DABand can attain\na sub-linear regret bound within the target domain. This marks the first regret analysis for domain\nadaptation in contextual bandit problems incorporating deep representation, shallow exploration,\nand adversarial alignment. We show that all these elements are instrumental in the domain adaptive\nbandit setting on real-world datasets. Moving forward, interesting future research could be to uncover\nmore innovative techniques for aligning the source and target domains (mentioned in Appendix E),\nparticularly within the constraints of: (1) bandit settings, as opposed to classification settings, and (2)\nthe high-dimensional and dense nature of the target domain, contrasted with the sparse and simplistic\nnature of the source domain."}, {"title": "D Implementation Details", "content": "In this section, we provide detailed insights into the implementation of our approach, applied to two\ndistinct datasets: DIGIT and VisDA17. We use Pytorch to implement our method, and all experiments\nare run on servers with NVIDA A5000 GPUs.\nDIGIT. Within the DIGIT dataset framework, we use the MNIST dataset as the source domain\nand MNIST-M as the target domain. To ensure compatibility between the datasets, we standardize\nthe channel size of images in the source domain (cs = 1) to align with that in the target domain\n(cT = 3). Each image undergoes normalization and is resized to 28 \u00d7 28 pixels with 3 channels to\naccommodate the format requirements of both domains. Then, an encoder is utilized to diminish\nthe data's dimensionality to a more manageable latent space. Following this reduction, the data\nis processed through two fully connected neural network layers, ending in the final latent space\nnecessary for loss computation as delineated in main paper. For the optimal hyperparameters, we set\nthe learning rate to 1 \u00d7 e\u22125, with \\lambda is chosen from {1.0, 5.0, 10.0, 15.0, 20.0} and kept the same for\nall experiments. Additionally, we set the exploration rate \\alpha to 0.05.\nVisDA17. We use the VisDA17 dataset's training set as the source domain, with the validation set\nfunctioning as the target domain. We adhere to preprocessing steps established by [45] to ensure\nuniformity across domains: Each image is normalized and resized to 224 \u00d7 224 pixels with 3 channels,\nmatching the requisite specifications for both source and target domains. For hyperparameters, the\nlearning rate of 1 \u00d7 e-5 is applied, with \\lambda is chosen from {1.0, 5.0, 10.0, 15.0, 20.0} and then kept\nthe same for all experiments. We set the exploration rate \\alpha to 0.05.\nS2RDA49. We selects 10 classes from the original 49 classes since it matches the target domain\nsamples in VisDA17 [38]. We adhere to preprocessing steps established by [45] to ensure uniformity\nacross domains: Each image is normalized and resized to 224 x 224 pixels with 3 channels. For hy-\nperparameters, the learning rate of 1 \u00d7 e\u00af\u00b3 is applied, with \\lambda chosen from {1.0, 5.0, 10.0, 15.0, 20.0}\nand then kept the same for all experiments. We set the exploration rate \\alpha to 0.01."}, {"title": "E Limitation", "content": "This work contains several limitations. Specifically, we highlight below:\nIntuitive Perspective. The bandit algorithm indeed enjoys interpretability and the accuracy of its\nclosed-form updates, but this is true only because it typically employs a linear model.\nUnfortunately, linear models do not work very well for real-world data, which is often high-\ndimensional. For example, the empirical results for LinUCB and LinUCB-P in Table 1 and Table 2\nshow that such linear contextual bandit algorithms significantly underperform state-of-the-art neural\nbandit algorithms, which use back-propagation (similar results are shown in [9, 11]). In summary\ndeep learning models with back-propagation is necessary due to the following reasons:\n\u2022 High-Dimensional Data. To enhance performance in real-world high-dimensional data (e.g.,\nimages), the integration of deep learning (and back-propagation) is necessary, though this may\nsacrifice a portion of the algorithm's explanatory power.\n\u2022 Covariate Shift and Aligning Source and Target Domains in the Latent Space. In our\nsettings where there is covariance shift between the source and target domains, a deep (nonlinear)\nencoder is required to transform the original context into a latent space where source-domain\nencodings and target-domain encoders can align. This also necessitates deep learning models\nwith back-propagation.\nIndeed, there is a trade-off between interpretability and performance. This would certainly be an\ninteresting future direction, but it is out of the scope of this paper.\nTheoretical Perspective. Our Theorem 3.1 is sharp, as all the inequalities are based on lemmas in\nthe paper and the Cauchy inequality. Identifying the criteria under which the target regret bound\nreaches equality as well as how one can achieve it in practice would be interesting future work.\nEmpirical Perspective. The performance of DABand largely relies on the alignment quality between\nthe source and target domains. Therefore, for two domains that cannot be aligned (for example, most\ndomain adaptation tasks are predefined, and the data for both domains is pre-processed and cleaned,\nnot original real-world data), it is challenging to evaluate how DABand can still transfer knowledge"}, {"title": "F Discussions", "content": "F.1 Difference between Bandit and Classification Settings\nIn the classification setting, given a sample x, a model predicts its label \u0177. Since its ground-truth label\nis known, one can directly apply the cross-entropy loss to perform back-propagation and update our\nmodel. In contrast, in bandit settings, for each sample x, our model predicts an action @ by optimizing\nthe estimated rewards. One then submits this predicted action to the environment and receives\nfeedback that only indicates whether the predicted action a is the optimal action a* or not. If not, we\nare informed that our prediction is incorrect, yet we do not receive information on what the correct\n(optimal) action should be. Moreover, during training, we only train on 300 episodes for DIGIT\nand VisDA17 and 100 episodes for S2RDA49. Each episode contains 64 samples. This implies that\ncompared with those DA methods which train on multiple epochs, in our settings, all of the models\nonly see a sample once. Such complexity makes the bandit setting much more challenging.\nF.2 Importance of Bandits and DABand\nBandit algorithms are designed to navigate environments where feedback is sparse, costly, and\nindirect. By efficiently learning from limited feedback \u2013 identifying not just when a prediction is\nwrong but adapting without explicit guidance on the right choice \u2013 bandit algorithms offer a strategic\nadvantage in dynamically evolving settings. Our DABand exemplifies this by leveraging a low-cost\nsource domain to improve performance in a high-cost target domain, thereby reducing cumulative\nregret (improving accuracy) while minimizing operational costs. DABand not only reduces the\nexpense associated with acquiring and labeling vast datasets but also capitalizes on the intrinsic\nadaptability of bandit algorithms to learn and optimize in complex, uncertain environments.\nF.3 Discussion on Zero-shot Target Regret Bound\nF.3.1 Significance of Theorem 3.1\nNote that Theorem 3.1 is nontrivial. While it does resemble the generalization bound in domain\nadaptation, there are key differences. As mentioned in Observation (3) in Sec. 3.6, our target regret\nbound includes two additional crucial terms not found in domain adaptation. Specifically:\n\u2022 Regression Error in the Source Domain. $\\sum\\_{i=1}^{N} (\\langle\\theta_S^*, \\phi\\_S(x\\_i^{Sa\\_i})\\rangle - \\langle\\hat{\\theta}, \\hat{\\phi}(x\\_i^{Sa\\_i})\\rangle)$, which\ndefines the difference between the true reward from selecting action a\u2081 and the estimated reward\nfor this action.\n\u2022 Predicted Reward. $\\sum\\_{i=1}^{N} | \\langle\\hat{\\theta}, \\hat{\\phi}(x\\_i^{Sa\\_i})\\rangle |$, which serves as a regularization term to regularize\nthe model to avoid overestimating rewards.\nThe results of the ablation study in Table 4 in Sec. 5 highlight the significance of these two terms.\nMoreover, we would also like to highlight the key novelty/challenges in deriving Theorem 3.1:\n\u2022 Generalization of $\\mathcal{H}\\Delta\\mathcal{H}$ Distance to Regression. In the original multi-domain generalization\nbound [1], the $\\mathcal{H}\\Delta\\mathcal{H}$ divergence between source and target domains is derived for classification\nmodels. However, contextual bandit is essentially a reward regression problem, and therefore\nnecessitates generalizing the $\\mathcal{H}\\Delta\\mathcal{H}$ distance from the classification case to the regression case.\nThis presents a significant technical challenge, and leads to a series of modification in the proof.\n\u2022 Decomposing the Target Regret into the Source Regret with Other Terms. The subsequent\nchallenge involves decomposing the regret bound for the target domain (i.e., the target regret)\ninto a source regret term and other terms to upper-bound the target regret. This is necessary\nbecause we do not have access to feedback/reward from the target domain (we only have access"}, {"title": "G Potential Impact of DABand", "content": "Our DABand has many potential real-world applications, especially when obtaining responses is\ncostly. For instance, in testing new drugs, we can construct responses from mice for new drug A,\nand then use DABand to obtain the zero-shot hypothetical regret bound for responses in humans. In\nanother scenario, we can collect data (responses) on humans for another published, similar drug B,\nand then transfer knowledge by aligning the divergence between drug A and drug B. If both regrets\nreveal acceptable performance, we might not need excessive costs for back-and-forth testing, which\nsignificantly speeds up the process and reduces costs."}]}