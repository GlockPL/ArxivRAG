{"title": "Quality Assessment in the Era of Large Models: A Survey", "authors": ["ZICHENG ZHANG", "YINGJIE ZHOU", "CHUNYI LI", "BAIXUAN ZHAO", "XIAOHONG LIU", "GUANGTAO ZHAI"], "abstract": "Quality assessment, which evaluates the visual quality level of multimedia experiences, has garnered significant attention from researchers and has evolved substantially through dedicated efforts. Before the advent of large models, quality assessment typically relied on small expert models tailored for specific tasks. While these smaller models are effective at handling their designated tasks and predicting quality levels, they often lack explainability and robustness. With the advancement of large models, which align more closely with human cognitive and perceptual processes, many researchers are now leveraging the prior knowledge embedded in these large models for quality assessment tasks. This emergence of quality assessment within the context of large models motivates us to provide a comprehensive review focusing on two key aspects: 1) the assessment of large models, and 2) the role of large models in assessment tasks. We begin by reflecting on the historical development of quality assessment. Subsequently, we move to detailed discussions of related works concerning quality assessment in the era of large models. Finally, we offer insights into the future progression and potential pathways for quality assessment in this new era. We hope this survey will enable a rapid understanding of the development of quality assessment in the era of large models and inspire further advancements in the field.", "sections": [{"title": "1 Introduction", "content": "Thanks to the rising demand for high-quality multimedia consumption and an enhanced Quality of Experience (QoE), specialized quality assessment tools have been developed to predict the quality levels of various media types. These tools are instrumental in improving a wide range of applications, including low-level enhancements (such as dehazing, brightening, and denoising), as well as compression/transmission systems, and 3D scanning and reconstruction processes. Quality assessment serves as a guiding light, directing the optimization of numerous algorithms, which is crucial and fundamental for both academic and industry. Specifically, quality assessment can be divided into subjective quality assessment and objective quality. Subjective assessment is often regarded as the most reliable and accurate method because the human visual system (HVS) is the ultimate recipient of visual signals in most visual communication systems. However, subjective testing is time-consuming and costly, and it cannot be directly integrated as an optimization metric in practical systems. Objective quality assessment methods, which are typically designed or trained using data from subjective assessments, can automatically predict visual quality. These methods are ideal for timely evaluation and optimization of system performance.\nIn the era before large models, quality assessment typically focuses on specific domains such as image quality assessment (IQA), aesthetic quality assessment (AQA), video quality assessment (VQA), and 3D quality assessment (3DQA). IQA deals with traditional technical distortions like noise, blur, and compression artifacts. AQA evaluates the artistic and compositional elements of images, assessing factors such as color harmony, balance, and emotional impact. VQA addresses temporal issues in video streams, including frame rate, resolution, and motion artifacts, to ensure smooth and clear playback. Meanwhile, 3DQA focuses on the visual quality of 3D models, including point clouds, voxels, and meshes, evaluating aspects like texture, resolution, and structural accuracy. These specialized assessments help optimize multimedia content across various platforms and devices. Many researchers employ small expert models based on handcrafted features with support vector regression (SVR), convolution neural networks, or transformers to deal with quality assessment tasks. These expert models are typically trained and validated using databases specific to particular domains. However, they often lack the ability to adapt directly to other domains and provide scores without accompanying explanations.\nIn the era of large models, we can leverage large language models (LLMs) and large multi-modal models (LMMs) to provide detailed descriptions of quality information and answer questions related to the quality of multimedia content. This approach enhances the explainability of quality assessments. Additionally, the rise of AI-generated content (AIGC) necessitates robust quality assessment tools to monitor and enhance generation quality. Consequently, numerous new quality assessment benchmarks and methods have been developed to evaluate the multimedia outputs of these large models. For instance, various multi-modal benchmarks have been introduced to assess the text response quality of LMMs, and many quality assessment methods are now focusing on predicting quality in a unified manner, rather than being confined to specific domains. Considering the significant changes in the quality assessment field, there is a pressing need for a comprehensive and updated survey. This survey would provide a better top-down understanding of the history, current state-of-the-art, and future trends in the field. Our survey is constructed on the core question:\nHow to evaluate large models and how to utilize large models as evaluators?\nAs limited by pages, we confine our survey pool to encompass only papers that are considered important in the corresponding field. Specifically as shown in Fig. 1, our survey is organized as follows:\n(1) We begin by reflecting on the development of quality assessment before the advent of large models, offering a comprehensive comparison with the developments during the large-model era.\n(2) We summarize the advancements in large multi-modal models (LMMs) and the corresponding multi-modal benchmarks. Additionally, we explore the emerging focus areas and the developmental psychology behind quality assessment for AI-generated content (AIGC).\n(3) We review the methods of using large models as evaluators, discussing the differences and relationships among various techniques."}, {"title": "2 Quality Assessment in the Pre-Large-Model Era", "content": "In this section, we explore the landscape of quality assessment before the integration of large models. We will examine the methodologies (Image/Aesthetic/Video/3D Quality) as well as classic datasets traditionally employed, the limitations of these earlier approaches, and how they laid the groundwork for the development and necessity of large model frameworks."}, {"title": "2.1 Image Quality Assessment", "content": "Image quality assessment (IQA) aims at predicting the technical visual quality of images, which consistently attracts considerable attention [21]. Wang et al. [195] discuss the limitations of error sensitivity-based frameworks in IQA. Despite these challenges, researchers continue to develop objective metrics to predict perceived quality. Sheikh et al. [163] conduct a comprehensive subjective quality study with 779 distorted images to benchmark future research. Wang et al. [194] focus on computational models for predicting perceptual quality, detail leading algorithms, and propose new research directions. Moorthy et al. [132, 133] introduce visual importance pooling strategies and construct blind IQA indices using natural scene statistics, evaluated on the LIVE database. Researchers explore various approaches to enhance IQA, including measuring image sharpness in the frequency domain [37], utilizing relative gradient statistics and Adaboosting neural networks for blind IQA [48, 109], and employing deep learning for distortion-generic blind IQA [14]. Mohammadi et al. [131] provide an overview of quality assessment methods for conventional and emerging technologies like HDR and 3D images. Zhai et al. [233] emphasize the importance of perceptual quality assessment in visual communication systems.\nRecent studies develop IQA methods focusing on more diverse image content, such as deep learning-based approaches for VR images [82] and blind 360-degree IQA using multi-channel convolutional neural networks [172]. Research on smartphone photography [47] and face image quality [158], including perceptual full-reference tasks [31] and unsupervised techniques [31], remains significant. Advancements in no-reference IQA include multiscale feature representation [31, 245], multi-dimension attention networks [31, 119, 120, 246], and CNN [19] via object detection."}, {"title": "2.2 Aesthetic Quality Assessment", "content": "Research on aesthetic quality assessment traditionally focuses on general photo sets without specific content considerations. Li et al. [91] shift this focus to consumer photos with faces, while Jin et al. [76] develop methods for learning artistic portrait lighting templates, emphasizing Haar-like local lighting contrast features. Su et al. [170] propose an aesthetic modeling method for scenic photographs, covering both implicit and explicit features through a learning process. In the realm of CNNs, Kao et al. [79] and Dong et al. [42] explore CNNs for aesthetic quality assessment. Kao et al. treat it as a regression problem [79], and Dong et al. focus on understanding photo quality [42]. Lienhard et al. [102] predict aesthetic quality scores of facial images by computing features related to technical aspects. Building on these foundations, Kao et al. propose a hierarchical CNN framework, demonstrating superior performance [78], and later incorporate semantic information into aesthetic assessments [77].\nPersonalized algorithms for aesthetic assessment are introduced by Park et al. [144], aiming to increase user satisfaction. Jin et al. predict aesthetic score distributions using CNNs for a nuanced approach [74] and later introduce Aesthetic Attributes Assessment with the DPC-Captions dataset [75]. Wu et al. develop a method using deep convolutional neural networks (DCNNs) for predicting product design awards [212]. Kim et al. explore subjectivity in aesthetic quality assessment using a large-scale database [83]. Kuang et al. present a deep multimodality learning approach for UAV video aesthetic assessment [87]. Wang et al. propose a multidimensional aesthetic quality assessment method for mobile game images, focusing on fineness, color harmony, and overall quality [190]. Jin et al. introduce a new aesthetic mixed dataset and train a meta-reweighting network to address image aesthetic quality evaluation challenges [73]. Wu et al. introduce the PEAR framework, combining aesthetic rating and image reconstruction [205]. Chambe et al. highlight the use of deep learning in assessing professional photograph aesthetics using the AVA dataset [22]. These studies underscore the diversity of approaches in aesthetic quality assessment across various domains."}, {"title": "2.3 Video Quality Assessment", "content": "Video quality assessment (VQA) is essential in diverse image and video processing applications, such as compression, communication, printing, analysis, registration, restoration, and enhancement [35, 101, 240]. Wang et al. [197] propose a new approach for designing VQA metrics, emphasizing structural distortion as a measure of perceived visual distortion. Seshadrinathan et al. [159] introduce a quality metric for video sequences that incorporates motion information, highlighting the significance of motion in VQA. Advancements in VQA methodologies increasingly integrate models of human visual perception. For instance, Wang et al. [196] suggest using a statistical model of human visual speed perception in VQA frameworks. Zhai et al. [232] expand VQA across multiple dimensions, assessing video quality across different spatial and temporal resolutions. Ninassi et al. [138] develop a perceptual full-reference VQA metric evaluating temporal distortions at eye fixation levels, emphasizing the importance of temporal variations in spatial visual distortions. Vu et al. [185] introduce a spatiotemporal most-apparent-distortion model that considers motion artifacts to estimate motion-based distortion in videos. Wu et al. [201, 202, 206] employ fragment sampling to improve the efficiency of VQA models. Zhou et al. [41, 265] carry out a VQA model for exposure correction evaluation, further enhancing the adaptability of VQA methods.\nSubjective evaluations remain crucial in VQA algorithm assessment. Seshadrinathan et al. [160] conduct a study with human observers on distorted video sequences, leading to the creation of the LIVE Video Quality Database. Chikkerur et al. [32] propose a classification scheme for objective VQA methods based on whether they consider natural visual or human visual system characteristics. Recent advancements include display device-adapted video quality-of-experience assessments. Rehman et al. [154] introduce SSIMplus, a full-reference measure predicting real-time perceptual video quality based on human visual system behaviors, video content characteristics, display device properties, and viewing conditions. Bampis et al. [10] develop reduced-reference models like SpEED-QA, which efficiently compute perceptually relevant quality features using local spatial operations on image frames and frame differences."}, {"title": "2.4 3D Quality Assessment", "content": "3D quality assessment (3DQA) has become increasingly popular with the advent of virtual reality (VR), augmented reality (AR), and the metaverse [267]. Alexiou et al. [5] observe a rising interest in point clouds, leading to the development of objective quality metrics. Su et al. [169] advance this field by creating a comprehensive 3D point cloud database for subjective user studies, thereby facilitating future research. Diniz et al. [39] introduce a quality metric based on multiple distances between reference and test point clouds, adapting the LBP descriptor for non-uniform point distributions. Recent advancements in machine learning and deep learning have significantly impacted 3DQA. Lu et al. [117] propose an assessment method based on vision tasks to evaluate machine perception of point cloud quality. Liu et al. [237] develop PQA-Net, a no-reference point cloud quality assessment model utilizing multi-view projection. Zhang et al. [237] further develop MS-GraphSIM, a multiscale model that considers geometric and color features to accurately predict human perception. Additionally, Zhang et al. [242, 243] create a no-reference metric for colored 3D models, including point clouds and meshes. Javaheri et al. [237] propose a point-to-distribution metric that outperforms existing metrics.\nTo address domain adaptation challenges, Yang et al. [219] present a no-reference assessment approach using natural images as the source domain and point clouds as the target domain. Zhang et al. explore different modalities to predict point cloud quality from rendered images or videos [46, 238, 251, 261]. Furthermore, Zhang et al. [244] propose MM-PCQA, a multi-modal learning approach that combines 2D texture and semantic information with 3D geometry distortion sensitivity. More recently, attention is directed toward the efficiency of 3DQA with advancements like those proposed by Zhang et al. [248, 249]. Additional works focus on 3D digital human quality assessment [25, 257, 260]. Overall, the literature on 3DQA showcases a broad spectrum of approaches, from traditional metrics to advanced deep learning models, all aiming to accurately evaluate the quality of 3D models."}, {"title": "3 Assessment of Large Models", "content": "Assessment is crucial in the realm of large models, particularly for Large Multi-modal Models (LMMs) designed to tackle multi-modal challenges and engage with users. Evaluating LMMs is complex due to the diverse aspects of assessment and the lack of a standardized evaluation framework. Consequently, numerous multi-modal benchmarks have been introduced to assess both general and specific capabilities through various validation strategies. Despite these efforts, identifying an optimal solution remains an ongoing challenge. Additionally, the emergence of text-to-image/video and image-to-image generative large models has led to the creation of diverse visual AI-generated content (AIGC). Unlike traditional multimedia, visual AIGC content presents unique quality challenges, such as text alignment, generative-specific distortions, and unnaturalness. Addressing these issues necessitates the development of innovative quality assessment methods. In this section, we first provide an overview of the development of large multi-modal models. Next, we examine multi-modal benchmarks, with a particular focus on their ability-related aspects. Finally, we discuss the evaluation methods for visual AIGC."}, {"title": "3.1 Large Muli-modal Models", "content": "Large language models (LLMs) such as GPT-4 [140], T5 [34], and LLaMA [180] demonstrate remarkable linguistic proficiency across general human knowledge domains. These models extend their capabilities to multimodal tasks by incorporating visual inputs through CLIP [150] and additional adaptation modules, as seen in large multimodal models (LMMs) [36, 51, 90, 108, 235]. Specifically, OpenFlamingo [8] integrates gated cross-attention dense blocks into the pretrained language encoder layers. InstructBLIP [36] builds on BLIP-2 [96] by adding vision-language instruction tuning. To advance open-source LMMs, numerous projects utilize GPT-4 [140] for generating data to fine-tune vision-language models, exemplified by the LLaVA series [106\u2013108]. To effectively benchmark the diverse and rapidly developed LMMs, it is essential to employ robust assessment approaches. These methods should not only highlight the strengths and weaknesses of the models but also guide the direction for future improvements. Such evaluations are crucial for understanding and enhancing the capabilities of LMMs in various applications."}, {"title": "3.2 Multi-modal Benchmarks", "content": "A comprehensive comparison of mainstream benchmarks is presented in Table 2. These multimodal benchmarks facilitate the quality assessment of LMMs through a structured approach. The process begins with defining evaluation criteria, followed by the collection of relevant instances. Subsequently, either human annotators or other LLMs assist in annotating the data. This is followed by crafting specific prompts (multiple-choice questions, visual questions, multi-round dialogue) designed to elicit responses from LMMs. Finally, a detailed analysis is conducted on the gathered responses to assess the effectiveness and accuracy of the LMMs.\n(1) Assessment for image understanding capabilities. Image understanding traditionally includes tasks such as classification, detection, and identification. Early multi-modal benchmarks primarily address fundamental aspects of image comprehension, such as captioning significant elements and responding to simple queries [2, 26, 54]. However, these datasets mainly focus on extracting knowledge contained within the images themselves. To evaluate large models effectively, it becomes essential to test external knowledge. This includes recognizing well-known figures and comprehending specialized terms that necessitate a global understanding. The OK-VQA benchmark [124] is introduced to fill this gap by offering a variety of visual questions that rely on external knowledge and require intricate reasoning. With the rapid advancements in large models, particularly in interactive capabilities, new multi-modal benchmarks have been introduced. These benchmarks [161, 222, 229] challenge the models with open-ended questions that demand comprehensive, thoughtful responses to foster deeper discussions and analysis. Despite these developments, evaluating the responses of large models to open-ended questions remains a formidable challenge. OwlEval [222] employs manual evaluation metrics to measure the quality of responses across various models. Additionally, some benchmarks utilize established NLP metrics like BLEU-4 [143] and CIDEr [183] to evaluate answers against set correct responses. The effectiveness of generative models, particularly GPT [140], has been highlighted in recent research [114, 208], which shows improved evaluation outcomes that align more closely with human judgment. More recent benchmarks [68, 208, 253, 254] have adopted multiple-choice formats. This shift offers advantages such as easier annotation and the straightforward transformation of visual questions and image captions into multiple-choice questions. Closed-answer formats enhance the accuracy of output assessments from large models and simplify the derivation of performance statistics based on question correctness. In addition, the focus of benchmarks has evolved from simple to complex, from general to specific, and from coarse-grained to fine-grained. This progression signifies a move towards more detailed and specialized evaluations of large models.\n(2) Assessment for video understanding capabilities. The multi-modal benchmarks for video understanding are still evolving. SEED-Bench [88] focuses on temporal understanding, encompassing action recognition, action prediction, and procedure understanding. MVBench [99] extends these capabilities by evaluating the temporal comprehension of LMMs across 20 challenging video tasks, introducing a novel static-to-dynamic task generation method and utilizing automatic conversion of public video annotations into multiple-choice questions for fair and efficient assessment. Video-MME [50] further explores LMM capabilities in video processing, offering a variety of video types across six visual domains and durations ranging from seconds to an hour. MLVU [264] tackles the challenges in evaluating long video understanding by extending video lengths, incorporating diverse video genres such as movies and surveillance footage, and developing varied evaluation tasks to comprehensively assess LMMs' performance in long-video understanding.\n(3) Assessment for science-related capabilities. Multi-modal benchmarks designed to evaluate science-related capabilities require LMMs to accurately comprehend science content depicted in images and understand relevant scientific terminology. MathVista [118] and Math-Vision [188] assess the math reasoning abilities of LMMs through a collection of high-quality mathematical problems set in visual contexts from real math competitions, which cover a range of mathematical disciplines and difficulty levels. More recently, MMMU [230] has been developed to include 11.5K meticulously curated multimodal questions derived from college exams, quizzes, and textbooks. These questions span six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Technology & Engineering, providing a comprehensive scope for evaluating the academic understanding capabilities of LMMs.\n(4) Assessment for hallucination-aware capabilities. When investigating LMMs, researchers have noted a significant language bias, where the models' reliance on language priors often overshadows the visual context. To address this, RAVIE [105] and HallusionBench [57] have been introduced to perform quantitative analyses of the models' response tendencies, logical consistency, and various failure modes related to hallucinations. These benchmarks aim to assess and mitigate the imbalance between language and visual processing in LMMs.\n(5) Assessment for 3D understanding capabilities. The area of 3D understanding is gaining traction to enhance the decision-making capabilities of autonomous agents. Yet, current 3D datasets and methodologies are often constrained to narrow applications. To broaden this scope, LAMM-Bench [224] and M3DBench [100] have been introduced to evaluate large models' proficiency in interpreting multimodal 3D content, thereby paving the way for LMMs to act as generalists in a wider array of 3D tasks."}, {"title": "3.3 Visual AIGC Evaluation", "content": "One look is worth a thousand words. Drawing inspiration from this timeless proverb, numerous researchers have concentrated their efforts on developing text-to-image/video (T2I/V) models that vividly translate textual descriptions into visual representations.\n3.3.1 T2I/V Mdoels Development. Notable innovations such as AlignDRAW [123] and text-conditional GAN [153] have pioneered unique architectural approaches to image synthesis. The field has seen further advancements with the introduction of stable diffusion models [155, 156], significantly pushing the boundaries of T2I technology. On the commercial side, major companies are utilizing extensive datasets to develop and deploy highly effective T2I large models, including DALL-E [151], Midjourney [62], and Parti [228], among others. More recently, many efforts have been put into developing text-to-video large models. Extending pre-trained T2I models with temporal components is a standard method. CogVideo [63], built on CogView2 [38], introduces a multi-frame-rate hierarchical approach to better synchronize text-video sequences. Make-a-video [166] incorporates efficient spatial-temporal modules into a diffusion-based T2I architecture, specifically DALLE-2 [152]. VideoFusion [122] similarly utilizes DALLE-2 and implements a segmented diffusion method. A series of models including LVDM [61], Text2Video-Zero [81], Tune-A-Video [213], AnimateDiff [58], Video LDM [15], MagicVideo[263], ModelScope [187], and VidRD [55], draw from the advancements of stable diffusion [155] in video creation. Show-1 [234] merges pixel-based and latent-based approaches within video diffusion models. LaVie [192] modifies the core transformer block to accommodate spatial-temporal dynamics. Recently, OpenAI introduced Sora [17], an impressive T2V model capable of producing 60-second high-fidelity videos, setting a new direction in T2V technology.\n3.3.2 Quality assessment datasets for visual AIGC. To tackle the challenge of quality assessment for visual AIGC, many quality assessment datasets have been proposed during the last two years. A brief illustration of these datasets is shown in Table 3.\n(1) Quality assessment datasets for AI-generated images (AIGI): In recent years, multiple AIGI datasets have been introduced. The DiffusionDB [198] dataset is launched as the inaugural large-scale text-to-image prompt dataset, comprising 14 million AIGIs created by stable diffusion based on real user prompts and hyperparameters. The HPS [214] dataset gathers 98,807 AIGIs generated in the Stable Foundation Discord channel, accompanied by 25,205 selections made by humans. ImageReward [215] offers a dataset with 137k prompt-image pairings derived from DiffusionDB, where each pair is evaluated on three criteria: overall rating, image-text alignment, and fidelity. The Pick-A-Pic [84] dataset features over 500,000 instances and 35,000 unique prompts, with each instance comprising a prompt, two generated AIGIs, and a preference label. The AGIQA-1K [239], AGIQA-3K [95], and AIGCIQA2023 [186] datasets include 1,080, 2,982, and 2,400 AIGIs respectively. Specifically, the AGIQA-1K dataset first proposes the three most important evaluation dimensions for AIGIs: technical quality, aesthetic quality, and text alignment. The AGIN [30] dataset assembles 6,049 AIGIs and performs an extensive subjective study to ascertain human perspectives on overall naturalness. More recently, the AIGIQA-20K [92] dataset has been released, featuring 20,000 AIGIs generated by 15 prominent T2I models, with MOSs obtained from 21 evaluators. To tackle the challenge of omnidirectional image quality assessment in VR/AR environment, the AIGCOIQA2024 [217] dataset conducts comprehensive benchmarks by generating 300 images from 5 AIGC models using 25 text prompts, assessing human visual preferences in terms of quality, comfortability, and correspondence, and evaluating state-of-the-art IQA models' performance on this database. CMC-Bench [94] proposes to evaluate the cooperative performance of Image-to-Text and Text-to-Image models for ultra-low bitrate image compression, demonstrating that some model combinations outperform advanced visual codecs and highlighting areas for further optimization in LMMs.\n(2) Quality assessment datasets for AI-generated videos (AIGV): In contrast to AIGI datasets, there are fewer AIGV datasets. Chivileva et al. [33] introduce a dataset comprising 1,005 videos generated by five T2V models, with 24 users participating in a subjective study. EvalCrafter [113] develops a dataset for user study from 500 prompts using five T2V models, resulting in a total of 2,500 AIGVs. Similarly, the FETV [115] dataset employs 619 prompts and four T2V models, with three users for annotation. VBench [69] is more extensive, encompassing approximately 1,700 prompts and four T2V models. To expand the scale, the T2VQA-DB [86] dataset has been proposed, featuring 10,000 AIGVs produced by nine different T2V models, with 27 subjects engaged in collecting the MOSs. More recently, the GAIA [29] dataset has been carried out to focus on the action quality of the generated videos. It's worth mentioning the first competition track on AIGI/AIGV quality assessment has been held by NTIRE 2024 Quality Assessment for AI-Generated Content - Track 1/2: Image/Video [112].\n3.3.3 Quality assessment methods for visual AIGC. To be candid, the development of quality assessment methods for visual AIGC has lagged significantly behind the needs of AIGC technology. Researchers continue to struggle with accurately evaluating the quality of visual AIGC outputs. Initially, popular metrics for T2I/V generation such as Inception Score (IS) [157], Frechet Video Distance (FVD) [182], and CLIP Similarity (CLIPSim) [200], are found inadequate in reflecting actual user preferences. IS, which employs the Inception Network [176], generates a distribution that intends to capture image/video quality and diversity but has been criticized for its imprecision. FVD measures the similarity between the feature distributions of generated videos and natural videos using I3D features [20], with a lower FVD indicating a more natural-looking video. However, obtaining a suitable natural video for comparison is often impractical. CLIPSim leverages the CLIP model [148] to assess the alignment between the original text and the generated video content but falls short by neglecting temporal information and overall perceptual video quality.\nTo address these shortcomings, many researchers have reverted to traditional IQA methods. These traditional approaches, which evaluate technical distortions such as noise, blur, and semantic content, have provided partial solutions for AIGC evaluation in terms of technical quality. Recognizing the unique challenges of visual AIGC content, several IQA models have been specifically developed for AIGIs. Notably, HPS [214] and PickScore [84] use CLIP-based [149] models to mimic human preferences for generated images. ImageReward [215] employs a BLIP-based [97] architecture to predict image quality. Furthermore, the capability of LMMs is being explored to enhance AIGC evaluation tasks. Q-bench [208] is the first to investigate LMMs' performance in assessing visual quality. Subsequent studies such as those by Wu et al. [209\u2013211, 252] have introduced training procedures to utilize LMMs for IQA tasks. Additionally, Q-Refine [93] is a quality-enhanced refiner designed to guide the refining processes in T2I models. Wang et al. [189] have further employed the coherence and semantic content discernment capabilities of LMMs to aid in the evaluation process. The details of these models are discussed in Section 4.\nSimilarly, for the AIGV evaluation aspects, the traditional VQA methods are used to predict the spatial and temporal quality of the generated AIGVs. Although these methods are capable of solving the specific dimension, they still can not satisfy the AIGV evaluation since the evaluation is conducted comprehensively from many aspects. Then there are several works targeting the VQA tasks of AIGVs. VBench [69] and EvalCrafter [113] build benchmarks for AIGVs by designing multi-dimensional metrics. MaxVQA [207] and FETV [115] propose separate metrics for the assessment of video-text alignment and video fidelity, while T2VQA [86] handles the features from the two dimensions as a whole. We believe the development of the VQA model for AIGV will certainly benefit the generation of high-quality videos."}, {"title": "4 Large Models in Assessment Roles", "content": "Considering the impressive capabilities of large models in visual understanding, it is logical to leverage them as evaluators for both traditional IQA/VQA tasks and emerging AIGC evaluation tasks. The benefits are clear: large models can interact with users and respond to quality-related inquiries, enhancing the scope and flexibility of assessments. However, a significant challenge arises because these models primarily produce textual outputs, whereas many specific tasks require quantifiable scores. The textual responses generated by these models can be ambiguous and lack the precision needed for definitive evaluations. Researchers have developed numerous innovative methods to employ large models as evaluators, which can broadly be categorized into two main strategies: 1) Prompt-driven Evaluation: This involves designing specific prompts that encourage large models to directly generate desired outcomes. By carefully crafting the input prompts, researchers can guide the models to produce outputs that align closely with evaluation objectives. 2) Feature-based Assessment: This strategy entails using large models to extract features that are indicative of quality. Once these quality-aware features are obtained, they can be used to perform regression analyses to quantitatively assess the quality of the content. These strategies showcase the flexibility and potential of large models in automating and enhancing content evaluation processes."}, {"title": "4.1 Prompt-driven Evaluation", "content": "Considering the various prompt techniques developed in the natural language processing (NLP) field, such as in-context learning [40], standard prompting [165], and chain-of-thought reasoning [191, 199], it is common to adapt these methods to engage large models in quality assessment tasks.\n4.1.1 Single-stimulus Prompt-driven Evaluation. When evaluating single images, the prompt used for large models can be direct and straightforward. Q-Bench [208] offers a simple example of how to craft prompts for articulating the quality attributes of single images, as demonstrated in the sample provided below:\n-User: Assume that you are an expert in quality assessment. Please describe the quality, aesthetics, and other low-level appearance of the image |<IMAGE>| in detail. Then give the final quality rating based on your previous description.\n-Response: [The content of quality description] + [Quality Rating].\nThis prompt endows large models with a specific role and divides the quality assessment into two parts: quality description and final rating. The quality description enables large models to thoroughly examine the images, forming the foundation for the subsequent rating. X-iqe [27] further utilizes the chain-of-thought approach by breaking down the quality prompt into multiple rounds of quality-aware dialogue. This sequence encompasses fidelity evaluation\u2192alignment evaluation aesthetic evaluation and follows the prompt order from image description\u2192task-specific analysis\u2192scoring. Here we present the sample chain-of-thought prompt for multi-dialogue quality assessment:\n-User: Assume that you are an expert in quality assessment. Please describe the quality, aesthetics, and other low-level appearance of the image |<IMAGE>| in detail.\n-Response: [The content of quality description].\n-User: Based on your description, how is the (specific-quality-dimension) of this image?\n-Response: [Fine-grained analysis of (specific-quality-dimension)].\n-User: Give the rating of (specific-quality-dimension) and overall perceptual quality of the image.\n-Response: [Quality Rating for (specific-quality-dimension) and overall perceptual quality].\nThe single-stimulus evaluation is simple and scalable, directly showcasing the quality understanding of large models, and can be easily adapted to many application scenarios. Promisingly, the single-stimulus prompt-driven evaluation is considered a leading candidate for future explainable no-reference quality assessment tasks.\n4.1.2 Multiple-stimulus Prompt-driven Evaluation. In some scenarios, pairwise or listwise comparisons are essential for determining superior image quality [221, 239, 241]. To address this, several researchers have developed specialized prompt systems for multiple-stimulus, prompt-driven evaluations. For example, 2AFC-LMMs [270] assess the IQA capabilities of LMMs using a two-alternative forced choice method. This approach involves devising coarse-to-fine pairing rules and employing maximum a posteriori (MAP) estimation [181] to convert pairwise preferences of different LMMs into global ranking scores. The pairwise comparison sample is illustrated as follows:\n-User: Assume that you are an expert in quality assessment.\nThis is the first image: |<IMAGE1>|.\nThis is the second image: |<IMAGE2>|.\nWhich image has better visual quality?\n-Response: The first/second image.\nIt's worth noting that in the prompt sample provided, it is crucial to specify which image is the first and which is the second. Without this clarification, LMMs may not be able to distinguish between the first and second images. Additionally, Wu et al. utilize MAP estimation under Thurstone's Case V assumption [179] to efficiently aggregate pairwise rankings and propose a novel method to gather partial rankings by presenting a set of images to LMMs for simultaneous ranking."}, {"title": "4.2 Feature-based Assessment", "content": "4.2.1 Assessment by CLIP. CLIP [149"}]}