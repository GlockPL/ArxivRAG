{"title": "Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings", "authors": ["Nidula Elgiriyewithana*", "N D Kodikara"], "abstract": "In this research, we present an innovative, parameter-efficient model that utilizes the attention U-Net architecture for the automatic detection and eradication of non-speech vocal sounds, specifically breath sounds, in vocal recordings. This task is of paramount importance in the field of sound engineering, despite being relatively under-explored. The conventional manual process for detecting and eliminating these sounds requires significant expertise and is extremely time-intensive. Existing automated detection and removal methods often fall short in terms of efficiency and precision. Our proposed model addresses these limitations by offering a stream-lined process and superior accuracy, achieved through the application of advanced deep learning techniques. A unique dataset, derived from Device and Produced Speech (DAPS), was employed for this purpose. The training phase of the model emphasizes a log spectrogram and integrates an early stopping mechanism to prevent overfitting. Our model not only conserves precious time for sound engineers but also enhances the quality and consistency of audio production. This constitutes a significant breakthrough, as evidenced by its comparative efficiency, necessitating only 1.9M parameters and a training duration of 3.2 hours markedly less than the top-performing models in this domain. The model is capable of generating identical outputs as previous models with drastically improved precision, making it an optimal choice.", "sections": [{"title": "1 Introduction", "content": "Sound engineering encompasses a wide array of activities, including the manipulation and production of audio signals for diverse applications such as music, speech, film, and broadcasting. A prevalent challenge in this field is the occurrence of non-speech vocal sounds in audio recordings, such as breaths and lip smacks. These sounds can be distracting and may degrade the quality and intelligibility of the audio[1], particularly when the recording is intended for professional or academic use. Historically, this issue has been addressed through manual editing of audio waveforms or the use of noise gates, which dynamically reduce signals below a certain threshold by adjusting gain and ratio settings[2]. However, these methods pose their own challenges, including substantial time consumption and the need for domain expert knowledge[3]."}, {"title": "2 Related work", "content": "In recent years, there has been growing interest in developing algorithms for automatic detection and removal of breath sounds from speech signals, and several studies have explored automated methods for detecting and removing breath sounds"}, {"title": "3 Methodology", "content": "Our study proposes a comprehensive methodology that includes a meticulously designed model, a robust model training regimen, and a specialized U-Net model. Each of these components is crucial in addressing the complex task of eliminating breath sounds from vocal recordings."}, {"title": "3.1 Model Architecture", "content": "The backbone of our study is the specialized U-Net model emphasized with attention mechanism[11]. Originally envisioned for biomedical image segmentation, the U-Net model has been noted for its capacity to capture both local features and expansive context within an image. In our case, this image is the spectrogram, derived from the Short Time Fourier Transform (STFT) of the audio files. The Short Time Fourier Transform (STFT) can be represented as:\n$\\text{STFT}(x) = \\sum_{n=-\\infty}^{\\infty} x[n].w[n - m] \\cdot e^{-jw m}$                                                                                                                              (1)\nwhere x[n] is the input signal, w[n \u2013 m] is the window function, and m is the time index. The STFT renders the audio file as a 2D array, segregating and capturing nuanced patterns along temporal and frequency domains[12].Although the STFT yields a complex signal, our methodology focuses solely on its amplitude. The amplitude undergoes processing through a softmask followed by traversal through our tailored U-Net algorithm[13]."}, {"title": "3.2 Loss Function", "content": "The design of the loss function is pivotal in the training of the network model. The loss function essentially quantifies the discrepancy between the predicted output and the actual output. The objective of the training process is to minimize this discrepancy. In this research, we have employed a custom loss function for the signal enhancement task. This loss function is unique in that it not only incorporates a Mean Absolute Error (MAE) term but also an additional term that specifically focuses on preserving speech over reducing noise. The Mean Absolute Error (MAE) is a popular metric in regression problems and is defined as the average of the absolute differences between the predicted and actual values[14]. In our context, the MAE term is given by:\n$MAE = \\frac{1}{N} \\sum_{i=1}^{N} |Y_{true,i} - Y_{pred, i}|$                                                                                                                   (2)\nWhere $Y_{true,i}$ and $Y_{pred,i}$ are the true and predicted amplitude values of the speech signal, respectively, and N is the total number of samples.\nThe second term in our loss function is designed to prioritize the preservation of speech over noise reduction. This is achieved by penalizing the model more heavily when it fails to correctly predict the amplitude of the speech signal. Mathematically, this is expressed as:\n$SpeechLoss = 2 * \\frac{1}{N} \\sum_{i=1}^{N} (\\frac{(Y_{true,i})^2}{Y_{pred,i} * |Y_{true,i}|})$                                                                                                            (3)\nThe final loss function is a sum of the MAE and the speech loss terms:\n$Loss = MAE + SpeechLoss$                                                                                                                                                                (4)\nThe use of this custom loss function ensures that the model not only minimizes the overall error in amplitude prediction but also prioritizes the preservation of speech components over noise reduction."}, {"title": "3.3 Dataset", "content": "The underlying data for our methodology comes from the Device and Produced Speech (DAPS) Dataset [15]. The DAPS dataset houses 100 authentic and clean"}, {"title": "3.4 Model Training", "content": "The model training phase revolves around a log spectrogram which operates on a 22,050Hz sampling rate. Our approach necessitates a frame length of 4096 and a frame step of 512. Additional parameters include a 'hann' window function and the setting of 'center' to True for more precise frequency representation. To facilitate a quick and precise learning process, an V100 GPU with its high computation power is exploited. The training procures the use of Adam optimization and incorporates an early stopping mechanism that continuously scrutinizes validation loss to ensure the model's prevention from overfitting and assures achievement of an optimal state."}, {"title": "4 Results", "content": "In this results section, we detail the performance of our model that's designed to remove non-speech sounds, such as breaths in vocal recordings. We compare it with two of the best models currently available, Inception and MobileNet V2, referenced in the work of Ramirez et al. [4]. Our model's accuracy is measured by its ability to identify and remove breath sounds from the recordings, which is presented as a percentage. Along with accuracy, we also use the Mel-Frequency Cepstral Coefficients (MFCC) Distance to measure spectral precision, and the Perceptual Evaluation of Speech Quality (PESQ) to assess the clarity of speech. The detailed results are shown in Table 1, highlighting our model's effectiveness in improving the quality of vocal recordings by removing unwanted breath sounds."}, {"title": "5 Discussion and Future Work", "content": "The study showcases a parameter-efficient model that utilizes deep learning to accurately detect and remove non-speech vocal sounds like breath noises from vocal recordings. The model excels in post-processing, reducing manual work significantly thus, benefitting sound engineering. However, the real potential lies in extending the model to real-time systems such as live broadcasts or interactive voice response systems. In spite of using fewer parameters, the model yielded better results than existing models, as confirmed by audio quality metrics. Future work could focus on devising objective methods to evaluate sound removal, exploring other deep learning approaches, and expanding the dataset to include varied vocal recordings for broader non-speech sound management."}, {"title": "6 Conclusion", "content": "This research has introduced a parameter-efficient model, which utilizes the attention U-Net architecture for the automatic detection and removal of breath sounds in vocal recordings. Demonstrating a significant need for research in this area in the sound engineering field, the paper emphasizes the inefficiencies and inaccuracies of previous automated methods. Our approach, characterized by its innovation and superior performance, leverages advanced deep learning techniques and a carefully"}]}