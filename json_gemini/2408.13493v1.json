{"title": "Thresholded Lexicographic Ordered Multiobjective Reinforcement Learning", "authors": ["Alperen Tercan", "Vinayak S. Prabhu"], "abstract": "Lexicographic multi-objective problems, which impose a lexicographic importance order over the objectives, arise in many real-life scenarios. Existing Reinforcement Learning work directly addressing lexicographic tasks has been scarce. The few proposed approaches were all noted to be heuristics without theoretical guarantees as the Bellman equation is not applicable to them. Additionally, the practical applicability of these prior approaches also suffers from various issues such as not being able to reach the goal state. While some of these issues have been known before, in this work we investigate further shortcomings, and propose fixes for improving practical performance in many cases. We also present a policy optimization approach using our Lexicographic Projection Optimization (LPO) algorithm that has the potential to address these theoretical and practical concerns. Finally, we demonstrate our proposed algorithms on benchmark problems.", "sections": [{"title": "1 Introduction", "content": "The need for multi-objective reinforcement learning (MORL) arises in many real-life scenarios and the setting cannot be reduced to single-objective reinforcement learning tasks in general [22]. However, solving multiple objectives requires overcoming certain inherent difficulties. In order to compare candidate solutions, we need to incorporate given user preferences with respect to the different objectives. This can lead to Pareto optimal or non-inferior solutions forming a set of solutions where no solution is better than another in terms of all objectives. Various methods of specifying user preferences have been proposed and evaluated along three main fronts: (a) expressive power, (b) ease of writing, and (c) the availability of methods for solving problems with such preferences. Typically there are tradeoffs when a particular front is emphasized. For example, writing preference specifications that result in a partial order of solutions instead of a total order makes the specification easier for the user but may not be enough to describe a unique preference. Three main motivating scenarios differing on when the user preference becomes available or used have been studied in the literature. (1) User preference is known beforehand and is incorporated into the problem a priori. (2) User preference is used a posteriori, i.e., firstly a set of representative Pareto optimal solutions is generated, and the user preference is specified over it. (3) An interactive setting where the user preference is specified gradually during the search and the search is guided accordingly.\nOur present work is in the a priori setting. The most common specification method in this setting is linear scalarization which which requires the designer to assign weights to the objectives and take a weighted sum, thus making solutions comparable [2]. The main benefit of this technique is that it allows the use of many standard off the shelf algorithms as it preserves the additivity of the reward functions. However, expressing user preference with this technique requires significant domain knowledge and preliminary work in most scenarios [8]. While it can be the preferred method when the objectives can be expressed in comparable quantities, e.g. when all objectives have a monetary value, this is not the case most of the time. Usually, the objectives are expressed in incomparable quantities like money, time, and carbon emissions. Additionally, a composite utility combining the various objectives, and an approximation of that with linear scalarization limits us to a subset of the Pareto optimal set.\nTo address these drawbacks of linear scalarization, several other approaches have been proposed and studied. Nonlinear scalarization methods like Chebyshev [13] are more expressive and can capture all of the solutions in the Pareto optimal set, however, they do not address the user-friendliness requirement. In this paper, we will focus on an alternative specification method that overcomes both limitations of linear scalarization, named Thresholded Lexicographic Ordering (TLO) [3] [8]. In lexicographic ordering, the user determines an importance order for the objectives. The more important objectives dominate the lower order objectives. Given two candidate solutions, a lower ranked objective is used to rank the two solutions only if all higher order objectives have the same values. The thresholding part of the technique allows a more generalized definition for being the same w.r.t. an objective. The user provides a threshold for each objective except the last, and the objective values are clipped at the corresponding thresholds. This allows the user to specify values beyond which they are indifferent to the optimization of an objective. There is no threshold for the last objective as it is considered an unconstrained open-ended objective.\nDespite the strengths of the TLO specification method, the need for a specialized algorithm to use it in reinforcement learning (RL) has prevented it from being a common technique. The Thresholded Lexicographic Q-Learning (TLQ) algorithm was proposed as such an algorithm and has been studied and used in several papers [8] [5]. While it has been noted that this algorithm does not enjoy the convergence guarantees of its origin algorithm (Q-Learning), we found that its practical use is limited to an extent that has not been discussed in the literature before. In this work, we investigate such issues of TLQ further. We also present a Policy Gradient algorithm as a general so-"}, {"title": "Our Contributions", "content": "Our main contributions in this work are as follows: (1) We demonstrate the shortcomings of existing TLQ variants on a new important problem class that was not known before. To the best of our knowledge, only Vamplew et al. [21] had previously shown a case where TLQ did not work. We formulate a taxonomy of the problem space in order to give insights into TLQ's performance in different settings. Using our taxonomy, we demonstrate a new significant class of problems on which TLQ fails. We exhibit this new problematic class on a common control scenario where the primary objective is reaching a goal state and the other secondary objectives evaluate trajectories taken to the goal. (2) We propose a lexicographic projection algorithm which computes a lexicographically optimal direction that optimizes the current unsatisfied highest importance objective while preserving the values of more important objectives using projections onto hypercones of their gradients. Our algorithm allows adjusting how conservative the new direction is w.r.t. preserved objectives and can be combined with first-order optimization algorithms like Gradient Descent or Adam. We also validate this algorithm on a simple optimization problem from the literature. (3) We explain how this algorithm can be applied to policy-gradient algorithms to solve Lexicographic Markov Decision Processes (LMDPs) and experimentally demonstrate the performance of a REINFORCE adaptation on the cases that were problematic for TLQ."}, {"title": "2 Related Work", "content": "[3] was one of the first papers that investigate RL in multi-objective tasks with preference ordering. It introduces TLQ as an RL algorithm to solve such problems. Vamplew et al. [21] showed that TLQ significantly outperforms Linear Scalarization (LS) when the Pareto front is globally concave or when most of the solutions lie on the concave parts. However, LS performs better when the rewards are not restricted to terminal states, because TLQ cannot account for the already received rewards. Later, Roijers et al. [15] generalized this analysis by comparing more approaches using a unifying framework. To our knowledge, [21] is the only previous work that explicitly discussed shortcomings of TLQ. However, we found that TLQ has other significant issues that occur even outside of the problematic cases they analyze.\nWray et al. [25] introduced Lexicographic MDP (LMDP) and the Lexicographic Value Iteration (LVI) algorithm. LMDPs define the thresholds as slack variables which determine how worse than the optimal value is still sufficient. While Wray et al. [25] proved the convergence to desired policy if slacks are chosen appropriately, such slacks are generally too tight to allow user preferences. This is also observed in Pineda et al. [14] which claimed that while ignoring these slack bounds negates the theoretical guarantees, the resulting algorithm still can be useful in practice.\nLi and Czarnecki [8] investigated the use of Deep TLQ for urban driving. It showed that the TLQ version proposed in [3] introduces additional bias which is especially problematic in function approximation settings like deep learning. Also, it depends on learning the true Q function, which can not be guaranteed. To overcome these drawbacks, it used slacks instead of the static thresholds and proposed a different update function. While Zhang et al. [28] also similarly utilize slacks, it defines them in terms of action probabilities instead of Q-values. Hayes et al. [5] used TLQ in a multi-objective multi-agent setting and proposed a dynamic thresholding heuristic to deal with the explosion of the number of thresholds to be set."}, {"title": "3 Background", "content": "Multiobjective Markov Decision Process (MOMDP). A MOMDP is a tuple $(S, A, P, R, \\gamma)$ where\n*   $S$ is the finite set of states with initial state $s_{init} \\in S$ and a set of terminal states $S_F$,\n*   $A$ is a finite set of actions,\n*   $P: S \\times A \\times S \\rightarrow [0, 1]$ is the state transition function given by $P(s, a, s') = P(s'|s, a)$, the probability of transitioning to state $s'$ given current state $s$ and action $a$.\n*   $R = [R_1, ..., R_K]^T$ is a vector that specifies the reward of transitioning from state $s$ to $s'$ upon taking action $a$ under $K$ different reward functions $R_i: S \\times A \\times S \\rightarrow \\mathbb{R}$ for $i \\in \\{1, ..., K\\}$.\n*   $\\gamma \\in \\mathbb{R}$ is a discount factor.\nIn such a MOMDP, a finite trajectory $\\tau \\in (S \\times A)^* \\times S$ is a sequence $\\tau = s_0 a_0 s_1 a_1 ... a_{T-1} s_T$ where $s_i \\in S$, $a_i \\in A$ and indices denote the time steps. The evolution of an MDP is governed by repeated agent-environment interactions, where in each step, an agent first picks actions in a state $s$ according to some probabilistic"}, {"title": "Value-function Algorithms for Optimal Policies", "content": "and for each of these actions $a$ the environment generates next states according to $P(s'|s, a)$. Each reward function $R_i$ corresponds to an objective or, the discounted rewards sum that the agent tries to maximize. Control over a MOMDP requires finding an optimal policy function $\\pi^* : S \\times A \\rightarrow [0,1]$ which assigns probability $P^* (a|s)$ to actions $a \\in A$. In this paper, we use the episodic case of MDP where the agent-environment interaction consists of sequences that start in $s_{init}$ and terminates when a state in $S_F$ is visited. The length of the episode, $T$, is finite but not a fixed number. In MDP literature, this is known as \"indefinite-horizon\" MDP. The episodic case can be ensured by restricting ourselves to suitable policies which have a non-zero probability for all actions in all states.\nWe define the quality of a policy with respect to an objective $O_i \\in \\{1,..., K\\}$ by the value function $V^{\\pi} : S \\rightarrow \\mathbb{R}$ given by $V_i(s) = \\mathbb{E}_{\\pi}[\\sum_{t=0}^T \\gamma^t R_i(s_t, a_t, s_{t+1})|s_0 = s]$. Intuitively, $V_i(s)$ is the expected return from following policy $\\pi$ starting from state $s$ w.r.t. objective or. The overall quality of a policy is given by the vector valued function $V^{\\pi} : S \\rightarrow \\mathbb{R}^K$ which is defined as $V^{\\pi} (s) = [V_1^{\\pi} (s), ..., V_K^{\\pi}(s)]^T$. As $V$ is vector-valued, without a preference for comparing $V_i$ values across different $i$, we only have a partial order over the range of $V$, leading to Pareto front of equally good quality vectors. Further preference specification is needed to order the points on the Pareto front. A Lexicographic MDP (LMDP) is a class of MOMDP which provides such an ordering. It adds another component to MOMDP definition:\n*   $\\mathcal{T} = (T_1, ..., T_{K-1}) \\in \\mathbb{R}^{K-1}$ is a tuple of threshold values where $T_i$ indicates the threshold value for objective $o_i$ beyond which there is no benefit for $o_i$. The last objective does not require a threshold; hence, there are only $K - 1$ values. Then, $\\mathcal{T}$ can be used to compare value vectors $u, v \\in \\mathbb{R}^K$ by defining the thresholded lexicographic comparison $>^{\\mathcal{T}}$ as $u >^{\\mathcal{T}} v$ iff there exists $i \\le K$ such that:\n    *   $\\forall j < i$ we have $u_j \\ge min(v_j, T_j)$; and\n    *   if $i < K$ then $min(u_i, T_i) > min(v_i, T_i)$,\n    *   otherwise if $i = K$ then $u_i > v_i$.\nIntuitively, we compare $u$ and $v$ starting from the most important objective ($j = 1$); the less important objectives are considered only if the order of higher priority objectives is respected.\nObjectives $o_i$ for $i < K$ are said to be constrained objectives. A constrained objective $o_i$ is said to be satisfied when it is greater than or equal to its corresponding threshold $T_i$. The relation $\\ge$ is defined as $> \\vee =$; where $u = v$ if $min(u_i, T_i) = min(v_i, T_i)$ for each constrained objective and $u_K = v_K$.\nValue-function Algorithms for Optimal Policies. An optimal policy $\\pi^*$ is one that is better than or equal to any other policy, i.e., if $V^{\\pi^*} (s) \\ge V^{\\pi} (s) \\forall s \\in S$ for all other policies $\\pi \\in \\Pi$ [3]. There are two approaches to finding optimal policies in RL: Value-function algorithms and Policy Gradient algorithms. Value function based methods estimate the optimal action-value function $Q^*$ and construct $\\pi^*$ using it. The action-value function under $\\pi$, $Q^{\\pi} : S \\times A \\rightarrow \\mathbb{R}^K$, is defined as: $Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi}[\\sum_{t=0}^T \\gamma^t R(s_t, a_t, s_{t+1})|s_0 = s, a_0 = a]$. The optimal action-value function, $Q^*$, is defined as: $Q^* (s, a) = \\underset{\\pi \\in \\Pi}{Max} Q^{\\pi} (s, a)$. Then, $\\pi^*$ is obtained as: $\\pi^*(s,a) = 1$ if $a = arg \\underset{a' \\in A}{max} Q^*(s, a')$, and 0 otherwise. In single objective MDPs, the Bellman Optimality Equation seen in Eq. 1 is used to learn $Q^*$ as it gives an update rule that converges to $Q^*$ when applied iteratively.\n$Q^*(s, a) = \\mathbb{E}_{s'~P} [(R(s, a, s') + \\gamma \\underset{a'}{max} Q^*(s', a'))]$(1)\nQ-learning [24] is a very popular algorithm that takes this approach."}, {"title": "4 TLQ: Value Function Based Approaches for TLO", "content": "Previous efforts to find solutions to the LMDPs have been focused on value-function methods. Apart from [25], which takes a dynamic programming approach, these have been variants of Thresholded Lexicographic Q-Learning (TLQ), an LMDP adaptation of Q-learning [3] [8]. While these methods have been used and investigated in numerous papers, the extent of their weaknesses has not been discussed explicitly.\nIn order to adapt Q-learning to work with LMDPs, one cannot simply use the update rule in Q-learning for each objective and learn the optimal value function of each objective completely independent of the others. Such an approach would result in the actions being suboptimal for some of the objectives. Based on this observation, two variants of TLQ [3][8] have been proposed, which differ in how they take the other objectives into account. We analyze these variants by dividing their approaches into two components: (1) value functions and update rules; and (2) acceptable policies for action selection. However, it should be noted that these components are inherently intertwined due to the nature of the problem the value functions and acceptable policies are defined recursively where each of them uses the other's last level of recursion. Due to these inherent circular referencing, the components will not be completely independent and some combinations of introduced techniques may not work.\nValue Functions and Update Rules. The starting point of learning the action-value function for both variants is $Q^* = (Q_1, ..., Q_k)$ where each $Q_i : S \\times A \\rightarrow \\mathbb{R}$ is defined as in Section 3 only with the change that the maximization is not done over the set of all policies $\\Pi$ but over a subset of it $\\Pi_{i-1} \\subset \\Pi$ which will be described below. G\u00e1bor et al. [3] propose learning $Q^* : S \\times A \\rightarrow \\mathbb{R}^K$ where each component of $Q^*$ denoted by $Q_i$ is defined as: $Q_i(s,a) \\triangleq min(T_i, \\hat{Q_i}(s, a))$ In other words, it is the rectified version of $Q$. It is proposed to be learned by updating $Q(s, a)$ with the following value iteration which is adapted from Eq. 1\n$\\hat{Q_i}(s, a) \\leftarrow min\\bigg( T_i, \\mathbb{E}_{s'} \\big[ \\sum_{s' \\sim P(.,a,s)} (R_i(s, a, s') + \\gamma \\underset{\\pi \\in \\Pi_{i-1}}{max} \\hat{Q_i}(s', \\pi (s'))\\big] \\bigg)$(2)\nNotice that similar to the definition of $Q$, the main change during the adaptation is limiting the domain of max operator to $\\Pi_{i-1}$ from $\\Pi$. On the other hand, Li and Czarnecki [8] propose that we estimate $\\hat{Q_i}^*$ instead and use it when the actions are being picked. This $\\hat{Q_i}^*$ uses the same update rule as Eq. 1 with only change being maximization over $\\Pi_{i-1}$."}, {"title": "5 Policy Gradient Approach for TLO", "content": "In this section, we introduce our policy gradient approach that utilizes consecutive gradient projections to solve LMDPs. Policy gradient methods treat the performance of a policy, $J(\\theta)$, as a function of policy parameters $\\theta$ that needs to be maximized and employ standard gradient ascent optimization algorithms. Following this modularity, we start with proposing a general optimization algorithm, the Lexicographic Projection Algorithm (LPA), for multiobjective optimization (MOO) problems with thresholded lexicographic objectives. Then, we will show how single objective Policy Gradient algorithms can be adapted to this optimization algorithm.\nAs gradients w.r.t. different objectives can be conflicting for MOO, various ways to combine them have been proposed. Uchibe and Doya [20] proposes projecting gradients of the less important objectives onto positive halfspaces of the more important gradients. Such a projection vector has the highest directional derivative w.r.t. the less important objective among directions with non-negative derivatives w.r.t. the important objectives. This is assumed to protect the current level for the important objective while trying to improve the less important. However, a non-negative derivative actually does not guarantee protection of the current level as infinitely small step sizes are not used in practice. For example, for a strictly concave function, the change in a zero-derivative direction will be always negative for any step size greater than 0. Therefore, we propose projecting onto hypercones which allows more control over how safe the projection is. A hypercone is the set of vectors that make at most a $\\Delta$ angle with the axis vector; a positive halfspace is a special hypercone where $\\Delta = \\frac{\\pi}{2}$. Increasing $\\Delta$ brings the projection closer to the axis vector. A hypercone $C_a$ with axis $a \\in \\mathbb{R}^n$ and angle $\\Delta$ is defined as\n$C_a = \\bigg\\{x \\bigg| x \\in \\mathbb{R}^n \\land (||x|| = 0 \\lor \\frac{a^T x}{||a||||x||} > cos \\Delta \\bigg)\\bigg\\}$(3)\nWe can derive the equation for projection of $g$ on $C_a$ by firstly showing that $g$, $a$, and the projection $g^p$ are planar using Karush-Kuhn-Tucker (KKT) conditions. Then, we can derive the formula by using two-dimensional geometry giving us $g^p =$\n$\\frac{cos \\Delta}{sin(\\Delta + \\phi)} \\bigg( g - a \\frac{||g||}{||a||} \\frac{sin \\phi}{sin(\\frac{\\pi}{2} - \\Delta)} \\bigg)$(4)\nwhere $\\phi$ is the angle between $a$ and $g$. Moving forward, we assume a function projectCone(g, a, $\\Delta$) which returns $g^p$ according to this equation. Note that when $g \\in C_a$, projectCone(g, a, $\\Delta$) := g.\nLexicographic Projection Algorithm (LPA). A Thresholded Lexicographic MOO (TLMOO) problem with K objectives and n parameters can be formulated as maximizing a function F : A $\\rightarrow \\mathbb{R}^K$ where $A \\subset \\mathbb{R}^n$, and the ordering between two function values"}, {"title": "5.1 Remark", "content": "Remark 5.1 Under the $\\mu$-strong convexity assumptions, a $\\Delta$-Pareto-stationary point can be understood as a point where the norm of largest ascent step cannot be greater than $\\frac{2 sin(\\Delta)}{\\mu} max_j ||\\nabla F_j(x)||$"}, {"title": "6 Experiments", "content": "We evaluate the performance of the Lexicographic REINFORCE algorithm on two Maze problems and a many-objective benchmark from the literature. In both experiments, we use a two layer neural network for policy function."}, {"title": "7 Conclusion", "content": "In this work, we considered the problem of solving LMDPs using model-free RL. While previous efforts on this problem have focused on value-function based approaches, applicability of these approaches over different problem settings and investigation of their shortcomings have been limited. Our first contribution was providing further insights into the inherent difficulties of developing value function based solutions to LMDPs. Towards this end, we demonstrated a new significant class of failure scenarios for the existing methods.\nOur second focus in this work was developing and presenting a policy-gradient based approach for LMDPs. Policy gradient algorithms have not been studied in MDPs with thresholded lexicographic objectives before even though they are more suitable for this setting as they bypass many inherent issues with value functions, such as non-convergence due to non-greedy policies w.r.t. value functions, and the need for different threshold values across the state space. For this, we developed a general thresholded lexicographic multi-objective optimization procedure based on gradient hypercone projections. Then, we showed how policy gradient algorithms can be adapted to work with LMDPs using our procedure, and demonstrated the performance of our REINFORCE adaptation on three case studies. While our results are promising for the REINFORCE adaptation, future research could be further empirical studies with more stable policy-gradient algorithm adaptations, and over more complex tasks."}]}