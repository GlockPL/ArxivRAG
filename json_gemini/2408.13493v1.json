{"title": "Thresholded Lexicographic Ordered Multiobjective Reinforcement Learning", "authors": ["Alperen Tercan", "Vinayak S. Prabhu"], "abstract": "Lexicographic multi-objective problems, which impose a lexicographic importance order over the objectives, arise in many real-life scenarios. Existing Reinforcement Learning work directly addressing lexicographic tasks has been scarce. The few proposed approaches were all noted to be heuristics without theoretical guarantees as the Bellman equation is not applicable to them. Additionally, the practical applicability of these prior approaches also suffers from various issues such as not being able to reach the goal state. While some of these issues have been known before, in this work we investigate further shortcomings, and propose fixes for improving practical performance in many cases. We also present a policy optimization approach using our Lexicographic Projection Optimization (LPO) algorithm that has the potential to address these theoretical and practical concerns. Finally, we demonstrate our proposed algorithms on benchmark problems.", "sections": [{"title": "1 Introduction", "content": "The need for multi-objective reinforcement learning (MORL) arises in many real-life scenarios and the setting cannot be reduced to single-objective reinforcement learning tasks in general [22]. However, solving multiple objectives requires overcoming certain inherent difficulties. In order to compare candidate solutions, we need to incorporate given user preferences with respect to the different objectives. This can lead to Pareto optimal or non-inferior solutions forming a set of solutions where no solution is better than another in terms of all objectives. Various methods of specifying user preferences have been proposed and evaluated along three main fronts: (a) expressive power, (b) ease of writing, and (c) the availability of methods for solving problems with such preferences. Typically there are tradeoffs when a particular front is emphasized. For example, writing preference specifications that result in a partial order of solutions instead of a total order makes the specification easier for the user but may not be enough to describe a unique preference. Three main motivating scenarios differing on when the user preference becomes available or used have been studied in the literature. (1) User preference is known beforehand and is incorporated into the problem a priori. (2) User preference is used a posteriori, i.e., firstly a set of representative Pareto optimal solutions is generated, and the user preference is specified over it. (3) An interactive setting where the user preference is specified gradually during the search and the search is guided accordingly.\nOur present work is in the a priori setting. The most common specification method in this setting is linear scalarization which which requires the designer to assign weights to the objectives and take a weighted sum, thus making solutions comparable [2]. The main benefit of this technique is that it allows the use of many standard off the shelf algorithms as it preserves the additivity of the reward functions. However, expressing user preference with this technique requires significant domain knowledge and preliminary work in most scenarios [8]. While it can be the preferred method when the objectives can be expressed in comparable quantities, e.g. when all objectives have a monetary value, this is not the case most of the time. Usually, the objectives are expressed in incomparable quantities like money, time, and carbon emissions. Additionally, a composite utility combining the various objectives, and an approximation of that with linear scalarization limits us to a subset of the Pareto optimal set.\nTo address these drawbacks of linear scalarization, several other approaches have been proposed and studied. Nonlinear scalarization methods like Chebyshev [13] are more expressive and can capture all of the solutions in the Pareto optimal set, however, they do not address the user-friendliness requirement. In this paper, we will focus on an alternative specification method that overcomes both limitations of linear scalarization, named Thresholded Lexicographic Ordering (TLO) [3] [8]. In lexicographic ordering, the user determines an importance order for the objectives. The more important objectives dominate the lower order objectives. Given two candidate solutions, a lower ranked objective is used to rank the two solutions only if all higher order objectives have the same values. The thresholding part of the technique allows a more generalized definition for being the same w.r.t. an objective. The user provides a threshold for each objective except the last, and the objective values are clipped at the corresponding thresholds. This allows the user to specify values beyond which they are indifferent to the optimization of an objective. There is no threshold for the last objective as it is considered an unconstrained open-ended objective.\nDespite the strengths of the TLO specification method, the need for a specialized algorithm to use it in reinforcement learning (RL) has prevented it from being a common technique. The Thresholded Lexicographic Q-Learning (TLQ) algorithm was proposed as such an algorithm and has been studied and used in several papers [8] [5]. While it has been noted that this algorithm does not enjoy the convergence guarantees of its origin algorithm (Q-Learning), we found that its practical use is limited to an extent that has not been discussed in the literature before. In this work, we investigate such issues of TLQ further. We also present a Policy Gradient algorithm as a general so-"}, {"title": "Our Contributions.", "content": "Our main contributions in this work are as follows: (1) We demonstrate the shortcomings of existing TLQ variants on a new important problem class that was not known before. To the best of our knowledge, only Vamplew et al. [21] had previously shown a case where TLQ did not work. We formulate a taxonomy of the problem space in order to give insights into TLQ's performance in different settings. Using our taxonomy, we demonstrate a new significant class of problems on which TLQ fails. We exhibit this new problematic class on a common control scenario where the primary objective is reaching a goal state and the other secondary objectives evaluate trajectories taken to the goal. (2) We propose a lexicographic projection algorithm which computes a lexicographically optimal direction that optimizes the current unsatisfied highest importance objective while preserving the values of more important objectives using projections onto hypercones of their gradients. Our algorithm allows adjusting how conservative the new direction is w.r.t. preserved objectives and can be combined with first-order optimization algorithms like Gradient Descent or Adam. We also validate this algorithm on a simple optimization problem from the literature. (3) We explain how this algorithm can be applied to policy-gradient algorithms to solve Lexicographic Markov Decision Processes (LMDPs) and experimentally demonstrate the performance of a REINFORCE adaptation on the cases that were problematic for TLQ."}, {"title": "2 Related Work", "content": "[3] was one of the first papers that investigate RL in multi-objective tasks with preference ordering. It introduces TLQ as an RL algo-rithm to solve such problems. Vamplew et al. [21] showed that TLQ significantly outperforms Linear Scalarization (LS) when the Pareto front is globally concave or when most of the solutions lie on the concave parts. However, LS performs better when the rewards are not restricted to terminal states, because TLQ cannot account for the already received rewards. Later, Roijers et al. [15] generalized this analysis by comparing more approaches using a unifying framework. To our knowledge, [21] is the only previous work that explicitly discuss shortcomings of TLQ. However, we found that TLQ has other significant issues that occur even outside of the problematic cases they analyze.\nWray et al. [25] introduced Lexicographic MDP (LMDP) and the Lexicographic Value Iteration (LVI) algorithm. LMDPs define the thresholds as slack variables which determine how worse than the optimal value is still sufficient. While Wray et al. [25] proved the convergence to desired policy if slacks are chosen appropriately, such slacks are generally too tight to allow user preferences. This is also observed in Pineda et al. [14] which claimed that while ignoring these slack bounds negates the theoretical guarantees, the resulting algo-rithm still can be useful in practice.\nLi and Czarnecki [8] investigated the use of Deep TLQ for urban driving. It showed that the TLQ version proposed in [3] introduces additional bias which is especially problematic in function approximation settings like deep learning. Also, it depends on learning the true Q function, which can not be guaranteed. To overcome these drawbacks, it used slacks instead of the static thresholds and proposed a different update function. While Zhang et al. [28] also similarly utilize slacks, it defines them in terms of action probabilities instead of Q-values. Hayes et al. [5] used TLQ in a multi-objective multi-agent setting and proposed a dynamic thresholding heuristic to deal with the explosion of the number of thresholds to be set.\nHowever, we discovered that these works on using a Q-learning variant with thresholded ordering perform very poorly in most cases due to non-Markovianity of the value function they try to learn. It is possible to bypass this issue by using policy gradient approaches as they do not require learning an optimal value function. In order to handle conflicting gradients, some modifications to the gradient descent algorithm are needed. Recent work on modified gradient descent algorithms came mostly from Multi Task Learning literature, which could be considered a multiobjective optimization problem [1] [17] [10] [12] [11]. While these papers use similar ideas with our work, their setting is different than ours as they do not have any explicit importance order; hence, not applicable to our setting. Uchibe and Doya [20] have the most similar setting to ours in gradient-based algorithms. It considers a set of constraints with an unconstrained objective. Then, the gradient of the unconstrained objective is projected onto the positive half-space of the active (violated) constraints and adds a correction step to improve the active constraints. When no valid projection is found, the most violated constraints are ignored until a valid projection exists. This is one of the main differences with our setting: As we have an explicit importance-order of the objectives, it is not acceptable to ignore a constraint without considering the importance order. Also, we project the gradients onto hypercones instead of hyperplanes, which is a hypercone with $\\frac{\\pi}{2}$ vertex angle. Thus, our algorithm allows varying degrees of conservative projections to prevent a decline in the constraints.\nWhile there are many other recent works on Constrained Markov Decision Process (CMDPs) like [23, 6], their approaches are not applicable as an importance order over constraints is not allowed. Recently, using RL with lexicographic ordering began to attract attention from other communities as well. For example, Hahn et al. [4] use formal methods to construct single objective MDPs when all of the objectives are \u03c9-regular.\nSkalse et al. [18] propose both value-based and policy-based ap-proaches for LMDPs. Their value-based approach is based on slacks like [8] and they require using very small slack values. This protects their approach from the issues with having relaxations by limiting their setting to strict lexicographic order. For policy-based methods, they use Lagrangian relaxation and their setting is again a strict lexicographic ordering, i.e. it does not allow treating values above a threshold equal. As we are primarily interested in solving tasks with thresholded objectives, this paper does not address our question."}, {"title": "3 Background", "content": "Multiobjective Markov Decision Process (MOMDP). A MOMDP is a tuple $(S, A, P, R, \\gamma)$ where\n*   $S$ is the finite set of states with initial state $s_{init} \\in S$ and a set of terminal states $S_F$,\n*   $A$ is a finite set of actions,\n*   $P:S\\times A \\times S \\rightarrow [0, 1]$ is the state transition function given by $P(s, a, s') = P(s' | s, a)$, the probability of transitioning to state $s'$ given current state $s$ and action $a$.\n*   $R = [R_1, ..., R_K]^T$ is a vector that specifies the reward of transitioning from state $s$ to $s'$ upon taking action $a$ under $K$ different reward functions $R_i: S\\times A\\times S \\rightarrow \\mathbb{R}$ for $i \\in \\{1, ..., K\\}$.\n*   $\\gamma \\in \\mathbb{R}$ is a discount factor.\nIn such a MOMDP, a finite trajectory $\\tau \\in (S \\times A)^* \\times S$ is a sequence $\\tau = s_0 a_0 s_1 a_1... s_{T-1} a_{T-1} s_T$ where $s_i \\in S, a_i \\in A$ and indices denote the time steps. The evolution of an MDP is governed by repeated agent-environment interactions, where in each step, an agent first picks actions in a state $s$ according to some probabilistic"}, {"title": "Value-function Algorithms for Optimal Policies.", "content": "An optimal policy $\\pi^*$ is one that is better than or equal to any other policy, i.e., if $V^{\\pi^*}(s) \\geq V^{\\pi}(s) \\forall s \\in S$ for all other policies $\\pi \\in \\Pi$ [3]. There are two approaches to finding optimal policies in RL: Value-function algorithms and Policy Gradient algorithms. Value function based methods estimate the optimal action-value function $Q^*$ and construct $\\pi^*$ using it. The action-value function under $\\pi, Q^{\\pi}: S\\times A \\rightarrow \\mathbb{R}^K$, is defined as: $Q^{\\pi}(s, a) = \\mathbb{E}[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t, s_{t+1}) | s_0=s, a_0 = a]$. The optimal action-value function, $Q^*$, is defined as: $Q^*(s, a) = \\underset{\\pi \\in \\Pi}{Max} Q^{\\pi}(s, a)$. Then, $\\pi^*$ is obtained as: $\\pi^*(s,a) = 1$ if $a = arg \\underset{a' \\in A}{max} Q^*(s, a')$, and 0 otherwise. In single objective MDPs, the Bellman Optimality Equation seen in Eq. 1 is used to learn $Q^*$ as it gives an update rule that converges to $Q^*$ when applied iteratively.\n$Q^*(s, a) = \\mathbb{E}_{s' \\sim P} [(R(s, a, s') + \\gamma \\underset{a'}{max} Q^*(s', a'))]$ (1)\nQ-learning [24] is a very popular algorithm that takes this approach."}, {"title": "4 TLQ: Value Function Based Approaches for TLO", "content": "Previous efforts to find solutions to the LMDPs have been focused on value-function methods. Apart from [25], which takes a dynamic programming approach, these have been variants of Thresholded Lexicographic Q-Learning (TLQ), an LMDP adaptation of Q-learning [3] [8]. While these methods have been used and investigated in numerous papers, the extent of their weaknesses has not been discussed explicitly.\nIn order to adapt Q-learning to work with LMDPs, one cannot simply use the update rule in Q-learning for each objective and learn the optimal value function of each objective completely independent of the others. Such an approach would result in the actions being suboptimal for some of the objectives. Based on this observation, two variants of TLQ [3][8] have been proposed, which differ in how they take the other objectives into account. We analyze these variants by dividing their approaches into two components: (1) value functions and update rules; and (2) acceptable policies for action selection. However, it should be noted that these components are inherently intertwined due to the nature of the problem: the value functions and acceptable policies are defined recursively where each of them uses the other's last level of recursion. Due to these inherent circular referencing, the components will not be completely independent and some combinations of introduced techniques may not work.\nValue Functions and Update Rules. The starting point of learning the action-value function for both variants is $Q^* = (Q_1, ..., Q_K)$ where each $Q_i: S\\times A \\rightarrow \\mathbb{R}$ is defined as in Section 3 only with the change that the maximization is not done over the set of all policies $\\Pi$ but over a subset of it $\\Pi_{i-1} \\subset \\Pi$ which will be described below. G\u00e1bor et al. [3] propose learning $Q^* : S \\times A \\rightarrow \\mathbb{R}^K$ where each component of $Q^*$ denoted by $Q_i$ is defined as: $Q_i(s,a) \\gets min(T_i, \\tilde{Q_i}(s, a))$. In other words, it is the rectified version of $\\tilde{Q_i}$. It is proposed to be learned by updating $\\tilde{Q}(s, a)$ with the following value iteration which is adapted from Eq. 1\n$\\tilde{Q}(s, a) \\leftarrow \\mathbb{E}_{s' \\sim P} [min(T_i, (R_i(s, a, s') + \\gamma \\underset{\\pi \\in \\Pi_{i-1}}{max} \\tilde{Q}(s', \\pi(s')))]$ (2)\nNotice that similar to the definition of $Q$, the main change during the adaptation is limiting the domain of $max$ operator to $\\Pi_{i-1}$ from $\\Pi$. On the other hand, Li and Czarnecki [8] propose that we estimate $\\tilde{Q}^*$ instead and use it when the actions are being picked. This $\\tilde{Q}^*$ uses the same update rule as Eq. 1 with only change being maximization over $\\Pi_{i-1}$."}, {"title": "5 Policy Gradient Approach for TLO", "content": "In this section, we introduce our policy gradient approach that utilizes consecutive gradient projections to solve LMDPs. Policy gradient methods treat the performance of a policy, $J(\\theta)$, as a function of policy parameters $\\theta$ that needs to be maximized and employ standard gradient ascent optimization algorithms. Following this modularity, we start with proposing a general optimization algorithm, the Lexicographic Projection Algorithm (LPA), for multiobjective optimization (MOO) problems with thresholded lexicographic objectives. Then, we will show how single objective Policy Gradient algorithms can be adapted to this optimization algorithm.\nAs gradients w.r.t. different objectives can be conflicting for MOO, various ways to combine them have been proposed. Uchibe and Doya [20] proposes projecting gradients of the less important objectives onto positive halfspaces of the more important gradients. Such a projection vector has the highest directional derivative w.r.t. the less important objective among directions with non-negative derivatives w.r.t. the important objectives. This is assumed to protect the current level for the important objective while trying to improve the less important. However, a non-negative derivative actually does not guarantee protection of the current level as infinitely small step sizes are not used in practice. For example, for a strictly concave function, the change in a zero-derivative direction will be always negative for any step size greater than 0. Therefore, we propose projecting onto hypercones which allows more control over how safe the projection is. A hypercone is the set of vectors that make at most a $\\Delta$ angle with the axis vector; a positive halfspace is a special hypercone where $\\Delta = \\frac{\\pi}{2}$. Increasing $\\Delta$ brings the projection closer to the axis vector. A hypercone $C_a^{\\Delta}$ with axis $a \\in \\mathbb{R}^n$ and angle $\\Delta$ is defined as\n$C_a^{\\Delta} = \\{x | x \\in \\mathbb{R}^n \\land (||x||=0 \\lor \\frac{a^T x}{||a|| ||x||} > cos \\Delta)\\}$ (3)\nWe can derive the equation for projection of $g$ on $C_a^{\\Delta}$ by firstly showing that $g$, $a$, and the projection $g^p$ are planar using Karush-Kuhn-Tucker (KKT) conditions. Then, we can derive the formula by using two-dimensional geometry giving us $g^p =$\n$\\frac{cos \\Delta}{sin(\\Delta + \\phi)} (g - a \\frac{||g||}{||a||} (sin(\\Delta + \\phi) - sin \\Delta cos \\phi)) \\\\ if \\frac{a^T g}{||a|| ||g||} < cos \\Delta$ (4)\nwhere $\\phi$ is the angle between $a$ and $g$. Moving forward, we assume a function $projectCone(g, a, \\Delta)$ which returns $g^p$ according to this equation. Note that when $g \\in C_a^{\\Delta}, projectCone(g, a, \\Delta) := g$.\nLexicographic Projection Algorithm (LPA). A Thresholded Lexicographic MOO (TLMOO) problem with $K$ objectives and $n$ parameters can be formulated as maximizing a function $F : \\mathcal{A} \\rightarrow \\mathbb{R}^K$ where $\\mathcal{A} \\subset \\mathbb{R}^n$, and the ordering between two function values $F(\\theta_1), F(\\theta_2)$ is according to $>_\\tau$ as defined as in Section 3. Notice that when we have multiple objectives, the gradients will form a $K$-tuple, $G = (\\nabla F_1,\\nabla F_2, ...,\\nabla F_K)$, where $\\nabla F_i$ is the gradient of ith component of $F$.\nSince TLMOO problems impose a strict importance order on the objectives and it is not known how many objectives can be satisfied simultaneously beforehand, a natural approach is to optimize the objectives one-by-one until they reach the threshold values. However, once an objective is satisfied, optimizing the next objective could have a detrimental effect on the satisfied objective. We can use hypercone projection to avoid this. More formally, when optimizing $F_i$, we can project $\\nabla F_i$ onto the intersection of $\\{C_{\\nabla F_j}^{\\Delta}\\}_{j<i}$ where $\\Delta$ is a hyperparameter representing how risk-averse our projection is, and use the projection as the new direction. If such an intersection does not exist, it means that it is not possible to improve $F_i$ without taking a greater risk and we can terminate the algorithm.\nWe show in Proposition 5.1 that the LPA algorithm provides a strict increase in the value of the objective it currently optimizes while not decreasing the value of previously optimized objectives until it reaches a point where it cannot improve conservatively.\nProposition 5.1 Assume that when applied with a small enough step size on a TLMOO of $F_i$ concave and L-smooth for every $i \\in \\{1... K\\}$, LPA yields the sequence in parameter space $\\{x^{(l)}\\}_{l \\in \\mathbb{N}}$ with $x^{(l)} \\in \\mathbb{R}^n \\forall l \\in \\mathbb{N}$. Define the objective under optimization at step $l$ as $i = min_i \\{j < K: F_j(x^{(l)}) < T_j\\}$ where $min\\{\\} = K$. Then, $F_i(x^{(l+1)}) \\ge F_i(x^{(l)})$ for all $j < i_l$ and the relation is strict for $j = i_l$ when $x^{(l)}$ is not the optimal point of $F_{i_l}$ or is a $\\Delta$-Pareto-stationary point for the objective functions $F_1, F_2, ..., F_{i_l}$.\nThe proof and the exact lower bounds on $F_j(x^{(l+1)}) - F_j(x^{(l)})$ in the proposition are in the supplementary material.\nProposition 5.1 uses the concept of A-Pareto-stationary points, a generalization of Pareto-stationary points that allows for parameterized conservativeness. A point x is said to be A-Pareto-stationary for the objective functions $\\{F_1, F_2, ..., F_k\\}$ if there is not an ascent direction u such that $\\underset{j \\in 1...k}{max} \\angle(u, \\nabla F_j) \\le -\\Delta$.\nRemark 5.1 Under the $\\mu$-strong convexity assumptions, a $\\Delta$-Pareto-stationary point can be understood as a point where the norm of largest ascent step cannot be greater than $\\frac{2 sin(\\Delta)}{\\mu} \\underset{j}{max} ||\\nabla F_j(x)||$.\nImproving this approach with a heuristic that prevents overly conservative solutions leads to better performance in certain cases. Conservative updates usually lead to further increases on the already satisfied objectives instead of keeping them at the same level. This means most of the time, we have enough buffer between the current value of the satisfied objectives and their thresholds to sacrifice some of it for further gains in the currently optimized objective. Then, we can define a set of \"active constraints\" a subset of all satisfied objectives - for which we will not accept any sacrifice and only consider these when projecting the gradient. The \"active constraints\" can be defined loosely, potentially allowing a hyperparameter that determines the minimum buffer needed to sacrifice from an objective.\nThe FindDirection function in Algorithm 2 (our LPA algorithm) incorporates these ideas. This function takes the tuple of all gradients M, the tuple of current function values $F(\\theta)$, threshold values $\\tau$, the conservativeness hyperparameter $\\Delta$, a boolean $AC$ that determines whether \"active constraints\" heuristic will be used or not, and a buffer value $b$ to be used alongside active constraints heuristic as inputs. Then, it outputs the direction that should be followed at"}, {"title": "6 Experiments", "content": "We evaluate the performance of the Lexicographic REINFORCE algorithm on two Maze problems and a many-objective benchmark from the literature. In both experiments, we use a two layer neural network for policy function.\nExperiment 1: Non-reachability. The primary objective is non-reachability, i.e. it takes non-zero values in some non-terminal states. For this, we flip our objectives from the previous maze setting and define our primary objective as minimizing the cost incurred from the bad tiles. HHs give -5 reward and hhs give -4 reward. +1 reward is given in the terminal state to extend the primary objective to have rewards in both terminal and non-terminal states. The secondary objective is to minimize the time taken to the terminal state. We formalize this by defining our secondary reward function as 0 in terminal state and -1 everywhere else. We use the Maze in Figure 3 for this experiment. Note that this is the setting that Vamplew et al. [21] has found that TLQ fails. However, our experiments show that Lexicographic REINFORCE can solve this setting too.\nWe found that out of 10 seeds, 7 find policies that have 90% success over 100 episodes. The change in satisfaction rates of individual objectives for a successful seed can be seen in Figure 4. Notice that"}, {"title": "7 Conclusion", "content": "In this work, we considered the problem of solving LMDPs using model-free RL. While previous efforts on this problem have focused on value-function based approaches, applicability of these approaches over different problem settings and investigation of their shortcomings have been limited. Our first contribution was providing further insights into the inherent difficulties of developing value function based solutions to LMDPs. Towards this end, we demonstrated a new significant class of failure scenarios for the existing methods.\nOur second focus in this work was developing and presenting a policy-gradient based approach for LMDPs. Policy gradient algorithms have not been studied in MDPs with thresholded lexicographic objectives before even though they are more suitable for this setting as they bypass many inherent issues with value functions, such as non-convergence due to non-greedy policies w.r.t. value functions, and the need for different threshold values across the state space. For this, we developed a general thresholded lexicographic multi-objective optimization procedure based on gradient hypercone projections. Then, we showed how policy gradient algorithms can be adapted to work with LMDPs using our procedure, and demonstrated the performance of our REINFORCE adaptation on three case studies. While our results are promising for the REINFORCE adaptation, future research could be further empirical studies with more stable policy-gradient algorithm adaptations, and over more complex tasks."}]}