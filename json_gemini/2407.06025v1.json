{"title": "ILLM-TSC: INTEGRATION REINFORCEMENT LEARNING AND LARGE LANGUAGE MODEL FOR TRAFFIC SIGNAL CONTROL POLICY IMPROVEMENT", "authors": ["Aoyu Pang", "Maonan Wang", "Man-On Pun", "Chung Shue Chen", "Xi Xiong"], "abstract": "Urban congestion remains a critical challenge, with traffic signal control (TSC) emerging as a potent solution. TSC is often modeled as a Markov Decision Process problem and then solved using reinforcement learning (RL), which has proven effective. However, the existing RL-based TSC system often overlooks imperfect observations caused by degraded communication, such as packet loss, delays, and noise, as well as rare real-life events not included in the reward function, such as unconsidered emergency vehicles. To address these limitations, we introduce a novel integration framework that combines a large language model (LLM) with RL. This framework is designed to manage overlooked elements in the reward function and gaps in state information, thereby enhancing the policies of RL agents. In our approach, RL initially makes decisions based on observed data. Subsequently, LLMs evaluate these decisions to verify their reasonableness. If a decision is found to be unreasonable, it is adjusted accordingly. Additionally, this integration approach can be seamlessly integrated with existing RL-based TSC systems without necessitating modifications. Extensive testing confirms that our approach reduces the average waiting time by 17.5% in degraded communication conditions as compared to traditional RL methods, underscoring its potential to advance practical RL applications in intelligent transportation systems. The related code can be found at https://github.com/Traffic-Alpha/iLLM-TSC.", "sections": [{"title": "1 Introduction", "content": "Traffic congestion has become a critical issue in urban governance. The increasing number of vehicles in urban areas significantly impacts travel efficiency, increases traffic accidents, and exacerbates environmental pollution. Advances in electronics and computer technology have paved the way for intelligent transportation systems, which aim to enhance transportation efficiency through optimized control and scheduling decisions. Intelligent traffic signal control (TSC) systems, in particular, offer a viable solution to address these efficiency and safety-related challenges [1]. Traditional TSC schemes, such as the Webster method [2] and SCATS [3], provide foundational approaches to TSC system design. However, these methods are heavily reliant on predetermined parameter settings and struggle to adapt dynamically to fluctuations in traffic flow [4].\nTo address the limitations of traditional TSC methods, recent research has employed reinforcement learning (RL) to utilize real-time sensor data at intersections for decision-making [5]. RL is capable of learning from data and dynamically adjusting control strategies to accommodate real-time variations in traffic, showing impressive performance in both single-intersection and multi-intersection scenarios. The effectiveness of RL has been confirmed in large-scale implementations. However, most existing studies that apply RL in TSC operate under the assumption of perfect observation conditions and absence of communication issues, which are not reflective of real-world scenarios. For instance, as shown in Fig. 1, the communication process among vehicles, roadside units (RSUs), and base stations can face problems such as packet delays, losses, and noise. These issues can significantly degrade the performance of RL models [6]. When dealing with degraded communication scenarios, RL-based methods may make erroneous decisions, potentially jeopardizing traffic safety [7, 8]. Moreover, very few works consider long-tail scenarios, such as those involving emergency vehicles [9]. These challenges undermine the reliability of RL-based methods in practical applications, presenting substantial obstacles to the real-world deployment of RL-based TSC.\nRecognizing the limitations of both traditional methods and RL alone, researchers have begun to explore alternative solutions that leverage new technological paradigms. Large Language Models (LLMs) are considered prototypes of General Artificial Intelligence (GAI) and are seen as potential solutions for addressing the adaptability issues of TSC algorithms to dynamic environments [10]. Some efforts have begun exploring the application of LLMs in TSC. For example, [11] proposed a framework that utilizes LLMs to optimize urban TSC, overcoming the limitations of traditional methods in coping with rapidly changing traffic environments. Similarly, [12] developed a model called LA-Light, providing agents with various tools to address complex TSC challenges in the real world. However, despite LLMs' excellent performance in solving decision problems by enhancing the model's generalization ability, their performance remains suboptimal for specific problems, as they do not learn environment-specific policies [13]. In contrast, RL models can effectively learn policies directly from environment-specific traffic data. Thus motivated, this work aims to leverage the synergy of RL and LLM to achieve better performance."}, {"title": "2 Related Works", "content": "2.1 Traffic Signal Control Methods\nEfficient TSC strategies are crucial for reducing urban congestion. Most of the traditional TSC methods optimize traffic signals based on simplistic assumptions or fixed rules. For instance, the Webster method [2] calculates the ideal cycle length and allocation of traffic signal phases at intersections based on traffic volume and the assumption of traffic flow stability over specific periods. In addition, Self-Organizing Traffic Light Control (SOTL) [14] uses a set of predetermined rules to decide whether to continue with the current traffic signal phase or initiate a phase change.\nWhile traditional TSC systems have achieved some success in mitigating traffic congestion, their effectiveness is constrained by their dependence on real-time traffic data and their limited adaptability to quickly changing traffic conditions in complex environments. RL-based TSC methods are receiving increasing attention for their dynamic management of traffic signals [15]. These RL systems typically use factors such as queue length [8, 16, 17], vehicle waiting time [18, 19], or intersection pressure [20] as key indicators in their reward functions to reduce traffic congestion. Additionally, the frequency of signal switching has been considered [21, 22] to prevent the negative impacts of rapid signal changes, such as increased stop-and-go driving and the risk of accidents. While RL-based TSC systems provide flexibility in optimizing traffic flow through adjustments to reward functions, finding the right balance among these factors is still a challenging task. Furthermore, a reward function that fails to account for infrequent yet crucial events may not provide agents with the necessary guidance to manage unforeseen circumstances adeptly [23].\n2.2 Large Language Model Applications\nAs LLMs gain recognition across multiple domains [24], there have been several successful applications in the field of intelligent transportation recently [11,12,25\u201327]. [12] proposed a TSC method based on the LLM-Assisted framework, integrating LLMs into TSC by placing them at the center of the decision-making process and combining them with perception and decision-making tools to control traffic signals. Furthermore, [25] introduced LLMLight, a novel framework that uses LLMs as decision-making agents for TSC. By leveraging the advanced generalization capabilities of LLMs, this framework facilitates a reasoning and decision-making process similar to human intuition, thus improving traffic control effectiveness. However, LLMs have not been trained specifically for these corresponding tasks,"}, {"title": "2.3 Prompt Engineering", "content": "Currently, LLMs often rely on superficial statistical patterns rather than systematic reasoning, which can degrade their performance when faced with simple prompts [35]. To address this limitation, prompt engineering has emerged as a crucial technique for enhancing the functionality of LLMs. It helps these models organize their responses more logically, resulting in improved answer quality [36,37]. This approach involves using task-specific prompts to boost model effectiveness without modifying the core model parameters [38].\nPrompt engineering utilizes carefully crafted instructions, enabling LLMs to perform effectively across a variety of tasks and domains [39]. This adaptability is a significant departure from traditional methods, which typically require model retraining or substantial fine-tuning to tailor performance to specific tasks. The importance of prompt engineering is underscored by its ability to direct model responses, thereby enhancing the adaptability and practical applicability of LLMs.\nRecent studies have been continuously exploring innovative approaches and applications of prompt engineering within LLMs [37,40\u201342]. [42] introduce the Tree of Thoughts (ToT), which uses chain prompts to encourage the exploration of ideas as an intermediate step in using LLMs to solve general problems. Furthermore, [40] proposes a prompt refinement grading framework that enhances LLM capabilities through more detailed instructions and objectives. In addition, [43] presents a Retrieval Augmented Generation (RAG) architecture, combining information retrieval components with seq2seq generators, which can better handle knowledge-intensive tasks. Faced with scenarios like TSC with strong rule constraints, we combine these works to design a set of prompts to exploit LLM performance better, prevent LLM hallucinations, and improve overall framework performance."}, {"title": "3 Preliminaries and Problem Formulation", "content": "3.1 Traffic Movements and Phases\nMovement: A standard four-legged road intersection comprises four entrances: East (E), West (W), North (N), and South (S). Traffic movement refers to vehicles transitioning from an incoming approach to an outgoing one. As depicted in Fig. 2, this intersection has four entrances, labeled E0, E1, E2, and E3. Each entrance facilitates two movements: turning left (l) and proceeding straight (s). For instance, E0_l denotes a vehicle originating from E0 and intending to turn left. This study omits right turns, premised on the assumption that right turns are perpetually permissible in jurisdictions adhering to left-hand traffic regulations. As a result, there are eight possible movements at the intersection, referred to as M1, M2, M3, M4, M5, M6, M7, and m8 in the subsequent sections, and mi \u2208 M.\nPhase: A phase represents a set of movements that can be executed concurrently without causing conflicts. Given the consideration of only green and red signals, a phase can be conceptualized as a collection of permitted movements, with all others being prohibited. Fig. 2 illustrates the four phases of a standard four-way intersection, denoted as P = {0, 1, 2, 3}. For example, Phase-1 allows movements E0_s and El_s to occur simultaneously.\nTime and Time Slot: An RL agent takes an action to choose the next phase every time slot of 7 seconds. Unless specified otherwise, we set r = 5 in the sequel.\n3.2 TSC as an MDP Problem\nAn RL agent learns to make decisions through interactions with an environment modeled as a Markov Decision Process (MDP) [44], defined by the tuple (S, A, P, R, y). In the context of TSC, S represents the set of states, which"}, {"title": null, "content": "encapsulates both static and dynamic aspects of the intersection. A denotes the set of possible actions that adjust the traffic signals. P defines the transition probabilities between states, R is the reward function, typically the negative average waiting time of vehicles, and \u03b3\u2208 (0,1) is the discount factor. At time t, the agent's strategy, given state St \u2208 S and action a \u2208 A, is defined by the policy \u03c0 : S \u2192 A such that \u03c0(st) = at. The objective is to develop an agent for TSC with an optimal policy \u03c0* that determines the best action a based on dynamic traffic conditions to maximize the cumulative reward:\n\u03c0*(8) = arg max E [\u03a3t=0 \u03b3trt | so = s, ao = a]\n                                                   a                                         \u221e\n(1)\nwhere rt represents the reward given the current state st and the action taken at. Details of the RL model are discussed in Section 4. This study focuses on the MDP model under two practical scenarios: (1) Imperfect state acquisition, referred to as the degraded communication scenario, and (2) Situations unaccounted for by R, such as emergency vehicles or accidents, referred to as the long-tail events.\nIn real-world applications, an agent interacts with the environment through some communication channel. Since the communication channel is often imperfect, observations can be affected by noise and may even be lost during transmission [7]. The mathematical model for the data transmitted under degraded communication conditions is expressed as:\nfdegraded(x) = {x+\u03b2\u22c5\u03b7 with probability (1\u2212p), \u03b7\u223cN(0,1) (2)\n                                                            0                                                        with probability p\nwhere x represents the original data being transmitted. Here, p denotes the packet loss rate, which reflects the probability that a data packet is completely lost during transmission due to factors such as limited bandwidth or interference among vehicles [45]. The variable \u03b7 ~ N(0, 1) represents the noise, introducing random errors that can lead to incorrect interpretations of traffic conditions [46]. The parameter \u1e9e is a scaling factor for the noise. Observations during the training process of the agent are generally perfect, thus \u03c0(fdegraded(st)) may have difficulty providing an appropriate at, and it may even harm normal TSC."}, {"title": null, "content": "Another issue is that real-world scenarios are complex, and many situations are often neglected in designing R for TSC systems. An example is the arrival of an emergency vehicle such as an ambulance at an intersection, as illustrated in Fig. 1. In this situation, the primary objective shifts from maximizing intersection efficiency to enabling the emergency vehicle to pass through as swiftly as possible. These rare but critical scenarios are difficult to account for comprehensively in the design of R.\nTherefore, this study aims to leverage the generalization and logical reasoning capabilities of LLMs to develop more resilient TSC systems. We propose a policy improvement method designed to handle both unconsidered elements in the reward function and missing state information. This approach enhances our method's suitability for scenarios characterized by degraded communication and long-tail events, ensuring that the system can maintain both efficiency and safety under a broader range of real-world conditions."}, {"title": "4 Methodology", "content": "As illustrated in Fig. 3, we introduce a framework called iLLM-TSC that combines LLM and an RL agent for TSC. This framework initially employs an RL agent to make decisions based on environmental observations and policies learned from the environment, thereby providing preliminary actions. Subsequently, an LLM agent refines these actions by considering real-world situations and leveraging its understanding of complex environments. This approach enhances the TSC system's adaptability to real-world conditions and improves the overall stability of the framework. Details regarding the RL agent and LLM agent components are provided in the following sections.\n4.1 RL Agent Design\nIn the proposed method, the RL agent plays a pivotal role in making informed TSC decisions. This section outlines the three essential components that constitute the RL agent: state, action, and reward. Each component is critical for the agent's operation within the TSC environment, influencing its decision-making process in real-time traffic management.\nState: The following five state variables (SVs) are defined to characterize each movement mi for i = 1,2,.,8\n1. Flow Speed (SV\u2081): The average speed of vehicles in the movement, or -1 if there are no vehicles in the movement;\n2. Mean Occupancy (SV2): This is the ratio between the total length of roads occupied by vehicles and the total length of lanes, averaged over one-time slot;\n3. Jam length meters (SV3): This variable captures the total length of traffic jams in meters for each movement;\n4. Jam length vehicles (SV4): This variable captures the total length of traffic jams in terms of the number of vehicles;\n5. Current Phase (SV5): A binary variable, indicating whether the movement is passable at the time of current observation or not."}, {"title": null, "content": "In our study, the state of each movement mi at time t is then represented by the vector S = [SV\u2081(t), SV\u2082(t), . . ., SV(t)] for i = 1, 2, . . ., 8. Each element of this vector captures specific traffic data relevant to the movement mi at the intersection.\nAfter transmission, the state information for each movement is subjected to communication degradation, modeled as S = fdegraded(S). This transformation simulates the effects of packet loss and noise on the transmitted data, as discussed earlier. Consequently, the collective state for all movements at time t, considering degraded communication scenarios, is given by:\nst = [S\u0142,S?,\u00a8\u00a8\u00a8,S]T, (3)\nwhere st \u2208 R8\u00d75 and (\u00b7)\u00b9 is the transpose operator.\nAction: Upon the conclusion of a given time slot, the RL agent executes an action at \u2208 P, selecting a phase from all the array of available phases for the forthcoming time slot.\nReward: The objective of the RL agent is to minimize the waiting time of vehicles. In this work, the average waiting time of vehicles is employed as the reward.\n4.2 Training RL Agent for Initial TSC Decisions\nThe training of the RL-based agent is a critical aspect of the proposed framework. In this study, the Proximal Policy Optimization (PPO) algorithm [47] is employed to train the RL agent. PPO is chosen due to its suitability for policy-based reinforcement learning in environments with both discrete and continuous action spaces. This study utilizes two primary neural networks, namely a policy network denoted as \u03c0\u03b8 and a value network denoted as v\u03d5, where \u03b8 and \u03d5 are the respective network parameters. The policy network generates a probability distribution over the actions, given the current state, while the value network estimates the expected future return for that state. The traffic intersection observations st are used as input for both networks, generating \u03c0\u03b8(st) and v\u03d5(st). Next, the following PPO objective is proposed by taking into account both a policy loss Lp(\u03b8) and a value function loss Lv(\u03d5):\nF(\u03b8,\u03d5) = \u2212Lp(\u03b8) + \u03bbLv(\u03d5), (4)\nwhere X is a hyperparameter that balances the two loss terms. Furthermore, the policy loss function Lp(\u03b8) is defined as:\nLp(\u03b8) = \u00ca [min (J(\u03b8) \u00c2t, clip (J(\u03b8), 1 \u2013 \u03f5,1 + \u03f5) \u00c2\u2081)], (5)\nwhere \u00ca\u201e denotes the empirical expectation under policy \u03c0\u03b8 while e represents the clipping range. Furthermore, J(\u03b8) and At take the following form:\nJ(\u03b8) = \u03c0\u03c1(\u03b1t St)\n\u03c0\u03c1(at St)'\n= rt+1 + \u03b3\u03c5\u03c6(St+1) \u2013 U$(St),\n(6)\nAt\n(7)\nwhere y is the discount factor that determines the present value of future rewards. \u03c0\u03bf(\u00b7|\u00b7) and \u03c0\u00f5(\u00b7|\u00b7) stand for the current and previous policy, respectively. The previous policy \u03c0\u00f5(\u00b7|\u00b7) is used as a reference to ensure that the policy updates are consistent with the previous policy.\nFinally, the value function Lv($) is computed as the mean-squared error between the predicted state values v\u3085(s) and the actual discounted returns:\nLu(6) = B [(A)\u00b2]\nAt\n(8)\nBy minimizing this loss, the agent learns to predict the expected return from each state more accurately, which in turn helps the agent to choose better actions.\n4.3 Encoding Traffic Scenario into Text\nAs previously discussed, implementing RL-based TSC systems in real-world environments presents challenges, primarily due to the systems' limited understanding of the environment, which is crucial for effective decision-making. Conversely, LLMs can enhance decision-making by interpreting the current traffic conditions at intersections through natural language."}, {"title": "4.4 Enhancing RL Decisions with LLM Feedback", "content": "After the traffic scene is translated into a natural language description, the LLMs can be employed to refine the decisions made by the RL agent. This process begins by inputting prompt \u03a9 into the LLM model PLLM. The LLM aims to maximize the probability of generating the optimal sequence of actions as described by Eq. (9), producing each character sequentially.\np(Ot | \u03a9) = \u03a0 PLLM(Oi | N, 0<i), (9)\ni=1\nOt\nwhere or represents the i-th token, and Ot indicates the total number of tokens in the final response. The term o<i denotes all tokens preceding the i-th token. To ensure the LLM outputs are in the required format, we utilize regular expressions to extract the decision from Ot. However, as the LLM output may not always conform to the expected format, a maximum of K attempts is allowed. If the system exceeds K attempts without obtaining a correctly format-ted response, the original RL decision is executed. Algorithm 1 details the combined RL and LLM decision-making process in iLLM-TSC.\nAccording to Algorithm 1, the iLLM-TSC framework synergizes the strengths of RL and LLM to enhance the decision-making process in TSC systems. The LLM contributes its advanced capabilities in logical reasoning to analyze the actions proposed by the RL agent, thereby increasing the reliability of the system. Should there be a discrepancy between the RL suggestion and the LLM's logical framework, the LLM intervenes to adjust the action, ensuring the stability and robustness of the traffic management system. This collaborative approach not only leverages the policy power of RL but also utilizes the contextual understanding of LLM to optimize traffic control decisions effectively."}, {"title": "5 Experiment and Analysis", "content": "5.1 Experimental Setting\nExtensive experiments were conducted on the Simulation of Urban Mobility (SUMO) platform. We simulated an intersection with the phase structure shown in Fig. 2, where a green light was followed by a yellow light for 3 seconds before changing to a red light. Extension to more intersection types and more complex light phasing can be performed straightforwardly. The duration of each time slot 7 is set to 5 seconds. The LLM used in the following experiment was GPT-4 recently released by OpenAI. The RL parameters were configured as follows: the discount factor y was set to 0.99, the trace-decay parameter A was set to 0.9, and the policy clipping range e was established at 0.2. Additionally, we simulated a packet loss rate p of 0.2 and introduced a scaling factor for noise \u1e9e of 0.1 to mimic realistic communication conditions. Furthermore, to evaluate the robustness and adaptability of the RL model when integrated with LLM feedback, we set the maximum number of attempts to refine the RL decision based on LLM feedback at K = 3.\n5.2 Compared Methods\nOur evaluation encompasses five benchmark methods: two traditional TSC methods and three RL-based algorithms, detailed as follows:\n1. Fixed duration control: Fixed duration signal control algorithm is the most traditional method of TSC by pre-setting the traffic signal's phase sequence, phase duration, and cycle time in advance. In our experiment the fixed duration is set 25s;\n2. SOTL model [14]: SOTL algorithms utilize straightforward rules and indirect communication to enable traffic lights to organize and adjust to dynamic traffic conditions autonomously, leading to a reduction in waiting times.\n3. ADLight [18]: ADLight is a method for controlling traffic signal lights based on the PPO strategy in RL. Integrating features of movement and action with the set current phase duration ensures uniformity in model structure across different intersections.\n4. EMVLight [9]: EMVLight is an RL framework that considers emergency vehicles (EMV) in TSC. This framework addresses the coupling issue between EMV navigation and TSC. EMVLight not only reduces the travel time of background vehicles but also shortens the waiting time of EMVs.\n5. SARL-TSC [16]: SARL-TSC proposes a two-stage framework and leverages temporal information to enhance the robustness of the RL model against observation-impaired. The feature extraction model utilized in this framework is the EAttention.\n5.3 Evaluation Metrics\nWe leverage mean travel time, mean waiting time, and mean speed of vehicles to evaluate the performance of different actions made by TSC agents.\n1. Mean travel time: The mean travel time quantifies the average duration of all the vehicles traveling from their origins to their destinations."}, {"title": "5.4 Comparative Performance Analysis", "content": "Table 1 provides a performance comparison between iLLM-TSC and baseline algorithms under both normal and degraded communication scenarios. Under normal circumstances, iLLM-TSC ensures rapid passage for emergency vehicles while sustaining efficiency for conventional vehicles. Specifically, iLLM-TSC enhances the average travel time, average waiting time, and mean speed for all vehicles by 2%, 11.9%, and 3%, respectively, compared to AD-Light. The significant improvement in mean waiting time relative to travel time and speed can be attributed to the RL optimization target being primarily focused on reducing waiting times at intersections, whereas travel time and speed are influenced by overall traffic density. This prioritization results in a more substantial reduction in waiting times. Furthermore, when compared to SARL-TSC, which utilizes temporal sequence data, iLLM-TSC shows a minor deficit in overall vehicle throughput strategy. This difference arises because SARL-TSC focuses solely on maximizing junction efficiency, neglecting the prioritization of emergency vehicles. In contrast, iLLM-TSC reduces the waiting time for emergency vehicles by 62.9% compared to SARL-TSC, benefiting from the integration of LLM capabilities that enhance scene understanding and enable human-like reasoning during incidents.\nCompared to EMVLight, a method specifically addressing emergency vehicle prioritization, iLLM-TSC has a slightly longer average waiting time for emergency vehicles by 13.0%. However, EMVLight faces challenges in optimally balancing reward weights for normal vehicles and emergency vehicles during training, which compromises its vehicle control strategy for normal traffic conditions, leading to a 19.4% increase in average waiting time for all vehicles compared to iLLM-TSC. Overall, iLLM-TSC not only facilitates quicker passage for emergency vehicles but also maintains efficient average waiting times for general traffic. The addition of an LLM provides the system with enhanced adaptability and the capability to manage unexpected situations effectively.\nUnder degraded communication scenarios, all RL models that rely on observations experience a decline in performance, with the exception of the traditional Fixed Time control method. For instance, when compared with the ADLight model, our iLLM-TSC framework achieves a 17.5% improvement in mean waiting time. Furthermore, SARL-TSC, which performs optimally under normal conditions, exhibits a 10% increase in mean waiting time compared to iLLM-TSC in scenarios with compromised communication. While the Fixed Time control method remains unaffected by communication quality degradation, its mean waiting time is 25% higher than that observed with iLLM-TSC. This disparity is due to iLLM-TSC's ability to compensate for information loss. Specifically, when data is missing or incomplete, the LLM component of iLLM-TSC detects issues in the current scenario and disregards the RL suggestions, instead applying common sense and any relevant information to reach a decision. Further details and examples of this process will be provided in Section 5.6.\nAdditionally, iLLM-TSC significantly enhances emergency vehicles response efficiency in degraded communication scenarios. Compared to EMVLight, iLLM-TSC improves the mean waiting time for emergency vehicles by 35%, and also shows better performance in terms of mean speed and travel time. These results demonstrate that iLLM-TSC's strategy of utilizing RL for decision-making under normal conditions and leveraging LLM capabilities under special circumstances effectively enhances system performance across various scenarios."}, {"title": "5.5 Prompt Ablation Experiments", "content": "To investigate the influence of prompt components on the performance of LLMs, we conducted a series of ablation experiments based on the prompt structure depicted in Fig. 4. The prompt is composed of five elements: (1) the role of the LLM (role), (2) essential hints for decision-making (hints), (3) description of the traffic scenario (traffic scenario), (4) logical reasoning chain (logic), and (5) the format of the answer (format). Given the necessity for the LLM to comprehend the environment and respond in a specific format, the role, traffic scenario description, and answer format"}, {"title": null, "content": "are considered fundamental components. Consequently, our experiments focused on evaluating the impact of the hints and logic elements. The specific experimental conditions are detailed in Table 2. To assess the effects of varying prompts on different LLMs, we employed both GPT-3.5 and GPT-4 models. We introduced the mean waiting time ratio as the performance metric, defined as the ratio of the mean waiting time under each experimental condition to that obtained using the base level (Level 1) prompt with GPT-3.5 in the same traffic scenario. The experimental results are illustrated in Fig. 5.\nAs depicted in Fig. 5(a), both GPT-3.5 and GPT-4 models exhibit optimal performance with the Level 3 prompt configuration. The transition from Level 1 to Level 2 prompts results in approximately a 20% improvement for GPT-3.5 and a 15% improvement for GPT-4 in performance, highlighting the beneficial role of adding logical reasoning components to the prompts. Further enhancement of about 5% is observed when progressing from Level 2 to Level 3, highlighting the significance of providing hints for LLM decision-making. Additionally, comparisons between the two models show that GPT-4 outperforms GPT-3.5 when using the same prompt, suggesting that advancements in LLM technology also play a crucial role in decision-making effectiveness. Moreover, for emergency vehicle scenarios, as shown in Fig. 5(b), employing a Level 3 prompt with GPT-3.5 results in a significant improvement of about 64% compared to the Level 1 prompt. This indicates that a well-structured prompt, incorporating logical reasoning and hints, substantially aids the LLM in making more effective decisions in complex traffic management scenarios."}, {"title": "5.6 Typical Case Studies", "content": "In this section, we analyze how RL and LLMs collaborate in decision-making through three cases."}, {"title": "5.6.1 Case 1: Normal Scenario", "content": "As depicted in Fig. 8, the first scenario represents a regular traffic condition at an intersection without any unusual or long-tail scenarios or communication issues. In this scenario, the traffic phase selected by the RL agent allows some vehicles to proceed, even though this phase might not correspond to the direction with the highest number of waiting vehicles. The LLM, using its capability to understand and describe the scene, confirms the absence of any special events currently affecting the traffic. At this point, the optimization goal of the LLM aligns with that of the RL agent. Consequently, the LLM endorses and supports the RL agent's decision, which contributes to alleviating congestion at the intersection. This case demonstrates that iLLM-TSC does not merely prioritize the green light for the longest queue but instead supports the RL decision and applies reasoning akin to human common sense, a capability often lacking in traditional rule-based algorithms."}, {"title": "5.6.2 Case 2: Normal Scenario with Emergency Vehicles", "content": "RL agents can sometimes fail to make optimal decisions in rare or unexpected situations, particularly when such scenarios are not included in the design of the reward function. Our approach, iLLM-TSC, which integrates a LLM with RL, addresses this limitation effectively. Fig. 8 illustrates a scenario where an emergency vehicle approaches an intersection. The RL component of iLLM-TSC initially selects Phase 0, prioritizing lanes with higher vehicle accumulation, as the training did not specifically account for emergency vehicles. Recognizing the need to prioritize emergency responses, the LLM intervenes to override this decision by activating Phase 2. This action holds back other vehicles, thereby allowing the emergency vehicle to pass through the intersection promptly. This case study demonstrates that iLLM-TSC not only aims to minimize average waiting times but also effectively manages exceptional traffic situations. Such adaptability is crucial in real-world traffic management, where diverse and unpredictable scenarios often arise."}, {"title": "5.6.3 Case 3: Degraded Communication Scenarios", "content": "Communication disruptions can severely impact the performance of RL agents, particularly when the real-time data deviates from the training conditions. Fig. 9 depicts a situation characterized by communication problems, specifically packet loss, which prevents the accurate assessment of queue lengths at edge E0. Consequently, the RL agent defaults to activating Phase 0, which governs the movements E0_l and E1_l. These lanes exhibit relatively low occupancy"}, {"title": null, "content": "rates of 6.9% and 17.5%, respectively. However, the LLM detects an anomaly in the reported congestion level, which is erroneously noted as -1, signaling a loss of critical information. Upon analyzing the current traffic conditions, the LLM identifies significant congestion in the E3_s movement, with 21 waiting vehicles. To alleviate this, the LLM recommends switching to Phase 2, which prioritizes the E3_s movement, thereby effectively addressing the congestion in this lane. Additionally, the unreliable data from the E0_s movement makes Phase 1 a less dependable option due to the information loss. This case illustrates how iLLM-TSC can adapt to communication faults by leveraging the LLM's capability to interpret incomplete data and make informed decisions to optimize traffic flow."}, {"title": "6 Conclusion", "content": "In this study, we introduce the iLLM-TSC framework aimed at enhancing the reliability of RL-based TSC systems in real-world scenarios. Specifically, this paper addresses the challenge of imperfect observations due to degraded communication scenarios and rare traffic conditions not considered by the reward function with our dual-step framework. By leveraging the generalization capabilities of LLMs and making preliminary judgments about RL agent actions, our method enhances the adaptability of TSC systems to handle the complexities of real-world situations. This dual-step decision-making process enables our method to effectively manage complex environments while maintaining the robust performance characteristics of RL methods. Additionally, iLLM-TSC can be seamlessly integrated with existing RL-based TSC systems. Extensive experiments and tests validate the effectiveness of the proposed iLLM-TSC framework. In scenarios with degraded communication, iLLM-TSC reduces the average waiting time by 17.5% compared to traditional RL methods. This significant improvement underscores the enhanced scene comprehension capabilities of LLMs tailored specifically for TSC applications. It is important to note that this initial work only scratches the surface of the potential integration of LLM-assisted frameworks with RL and is expected to inspire further exploration of the synergy between LLM and RL."}]}