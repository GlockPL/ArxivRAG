{"title": "Best-Arm Identification in Unimodal Bandits", "authors": ["Riccardo Poiani", "Marc Jourdan", "Emilie Kaufmann", "R\u00e9my Degenne"], "abstract": "We study the fixed-confidence best-arm identification problem in unimodal bandits, in which the means of the arms increase with the index of the arm up to their maximum, then decrease. We derive two lower bounds on the stopping time of any algorithm. The instance-dependent lower bound suggests that due to the unimodal structure, only three arms contribute to the leading confidence-dependent cost. However, a worst-case lower bound shows that a linear dependence on the number of arms is unavoidable in the confidence-independent cost. We propose modifications of Track-and-Stop and a Top Two algorithm that leverage the unimodal structure. Both versions of Track-and-Stop are asymptotically optimal for one-parameter exponential families. The Top Two algorithm is asymptotically near-optimal for Gaussian distributions and we prove a non-asymptotic guarantee matching the worse-case lower bound. The algorithms can be implemented efficiently and we demonstrate their competitive empirical performance.", "sections": [{"title": "INTRODUCTION", "content": "In a bandit pure exploration problem, an agent interacts with a set of K probability distributions denoted by $\\nu = (V_i)_{i\\in[K]} \\in \\mathcal{D}^K$, called arms, whose means $\\mu = (\\mu_i)_{i\\in[K]}$ belong to some subset $S \\subseteq \\mathbb{R}^K$ materializing the possible structure between arms. In each time step $t\\geq 1$, one arm $I_t \\in [K]$ is selected, and the agent observes a realization $X_t \\sim V_{I_t}$. Based on as few observations as possible, her goal is to confidently answer some questions about the unknown mean vector $\\mu$, while leveraging the underlying structure.\nThe most studied pure exploration task is Best-Arm Identification (BAI), in which the agent seeks to identify the arm having the highest mean when $S = \\Theta^K$ is the (unstructured) set of all possible means vectors. In this paper, we consider best-arm identification when the set $S$ of means is constrained to have a unimodal structure, namely the means are known to increase up to their unique maximum and then decrease with the index. Let $i^*(\\mu) = \\operatorname{argmax}_{i\\in[K]} \\mu_i$ be the unique maximum, which we denote by $i^*$ or $*$ when $\\mu$ is clear from the context. The set of unimodal instances is\n$S := \\{x \\in \\mathbb{R}^K | |i^*(x)| = 1, \\forall j < i^*(x), x_j \\leq x_{j+1}, \\\\ x_{j \\geq i^*(x)}, x_j \\geq x_{j+1}\\}$.\nWe study the fixed-confidence formulation of the problem (Even-Dar et al., 2006; Jamieson and Nowak, 2014; Garivier and Kaufmann, 2016), in which the agent aims at minimizing the number of samples used to identify $i^* (\\mu)$ with confidence $1 - \\delta \\in (0,1)$. The agent's strategy is given by three rules: the sampling rule $(I_t)$, where each $I_t$ is $\\mathcal{F}_{t-1}$-measurable, a stopping rule $\\tau_\\delta$, which is a stopping time w.r.t. $\\mathcal{F}_t$ and a recommendation rule $\\hat{i}$, which is $\\mathcal{F}_{\\tau_\\delta}$-measurable. The algorithm's sample complexity corresponds to its stopping time $\\tau_\\delta$, which counts the number of rounds before termination. An algorithm is said to be $\\delta$-correct on the problem class $\\mathcal{D}^K$ having means $S$ if its probability of stopping and not recommending a correct answer is upper bounded by $\\delta$, i.e. $\\mathbb{P}_{\\nu} (\\tau_\\delta < +\\infty, \\hat{i}_{\\tau_\\delta} \\neq i^*(\\mu)) \\leq \\delta$ for all instances $\\nu \\in \\mathcal{D}^K$ having mean $\\mu\\in S$. The agent should design a $\\delta$-correct algorithm minimizing the expected sample complexity $\\mathbb{E}[\\tau_\\delta]$.\nUnimodal bandits have been studied extensively in the regret minimization literature (Yu and Mannor, 2011; Combes and Prouti\u00e8re, 2014; Paladino et al., 2017; Trinh et al., 2020; Saber et al., 2021), in which observations are rewards that the agent seeks to maximize."}, {"title": "", "content": "We denote by $\\mathcal{F}_t = \\sigma (I_1, X_1,..., I_t, X_t)$ the $\\sigma$-algebra generated by the observations collected up to time $t$."}, {"title": "INTRODUCTION", "content": "It has also been tackled in the fixed-budget best-arm identification setting (Cheshire et al., 2020), where the agent should minimize the probability of misidentifying $i^*(\\mu)$ with a fixed number of samples (Karnin et al., 2013; Audibert and Bubeck, 2010). These studies were motivated by potential applications to sequential pricing (Yu and Mannor, 2011) or bidding in sponsored search auctions (Edelman and Ostrovsky, 2005). Their extension to graphical unimodal bandits (Combes and Prouti\u00e8re, 2014) can be relevant for recommender systems (Paladino et al., 2017; Trinh et al., 2020). In this paper, we propose the first analysis of unimodal BAI in the fixed-confidence setting. We study unimodal bandits as a theoretical curiosity, in which the structure induces a sparsity pattern in the optimal allocation. This sparsity induced by the structure is a central component of our algorithms.\nRelated Work In fixed-confidence BAI, the seminal work of Garivier and Kaufmann (2016) introduced the Track-and-Stop (TaS) algorithm for unstructured bandits. It achieves asymptotically optimal expected sample complexity rates by computing the optimal allocation of the empirical estimator of the means at each round. Since then, several studies have proposed adaptation of TaS in different structured pure exploration problems, e.g., linear bandits (Jedra and Proutiere, 2020), multiple answers problems (Degenne and Koolen, 2019), and many others (Moulos, 2019; Agrawal et al., 2020; Russac et al., 2021). We will show how to account for the unimodal structure in TaS while providing a computationally efficient implementation. Within the class of structured bandits, it is worth mentioning that sparsity patterns in the instance-dependent lower bound have arisen in the minimum threshold problem (Kaufmann et al., 2018), good arm identification (Jourdan and R\u00e9da, 2023) and in multi-fidelity bandits (Poiani et al., 2024).\nOther approaches lead to asymptotically optimal algorithms in general structured pure exploration problems: the online optimization-based approach (M\u00e9nard, 2019; Degenne et al., 2019; Wang et al., 2021) and the Top Two approach (Russo, 2016; Jourdan et al., 2022; You et al., 2023). The online optimization-based approach could be applied to solve unimodal bandits. However, it does not explicitly exploit the sparsity pattern, hence incur sub-optimal performance in the moderate regime of $\\delta$. In addition to being computationally less demanding for large values of $K$, Top Two algorithms can be modified to include the sparsity pattern of unimodal BAI.\nContributions (1) We show that the unimodal structure induces sparsity in the optimal allocation (achieving the characteristic time $T^*(\\mu)$) in which only the best arm and its neighbors are contributing asymptotically (Theorem 2.2). However, in the moderate-confidence regime, we prove a worst-case lower bound whose linear dependence on $K$ suggests that sparsity is only possible asymptotically (Theorem 2.3). (2) We carefully exploit the unimodal structure in the recommendation and stopping rules, which ensure $\\delta$-correctness (Lemma 3.1). As sampling rules, we propose adaptations of the Track-and-Stop sampling rules using forced exploration or optimism and a Top Two sampling rule using optimism. We detail an efficient implementation of those three algorithms and show that they achieve competitive empirical performance. (3) The two Track-and-Stop algorithms are asymptotically optimal for one-parameter exponential families (Theorems 3.2 and 3.3). The optimistic Top Two algorithm enjoys a non-asymptotic guarantee matching the worse-case lower bound and recovering near asymptotic optimality (Theorem 3.4)."}, {"title": "2 LOWER BOUNDS", "content": "We present lower bounds on the expected sample complexity of any $\\delta$-correct algorithm for Unimodal BAI: an instance-dependent one scaling as $T^* (\\mu) \\log(1/\\delta)$ (Theorem 2.2) and a worst-case one scaling as $K/\\Delta^2$ where $\\Delta$ is the minimum gap (Theorem 2.3)."}, {"title": "2.1 Dependence in the Bandit Model \u03bc andthe Risk Parameter d", "content": "We recall a standard instance-dependent lower bound on the expected stopping time $\\mathbb{E}_{\\nu} [\\tau_\\delta]$ of any $\\delta$-correct BAI algorithm (Garivier and Kaufmann, 2016).\nTheorem 2.1. For any $\\delta$-correct algorithm and any unimodal instance $\\nu \\in \\mathcal{D}^K$ with mean $\\mu \\in S$, we have $\\mathbb{E}_{\\nu} [\\tau_\\delta] \\geq T^* (\\mu) \\log \\frac{2.45}{\\delta}$ with\n$T^{*}(\\mu)^{-1} := \\sup_{\\omega \\in \\Delta_K} \\min_{i^{*} \\atop \\Theta \\in \\operatorname{Alt}(\\mu)} f_i(\\omega, \\mu) \\text{ and }f_i(\\omega, \\mu) := \\inf_{\\theta\\in\\operatorname{Alt}_{i^{*}(\\mu)}} \\sum_{j\\in[K]} \\omega_j d(\\mu_j, \\theta_j),$ (2)"}, {"title": "2.2 Dependence in the Number of Arms", "content": "The above lower bound depends on the risk parameter $\\delta$ and the instance $\\mu$ through $T^* (\\mu)^{-1}$. Therefore, it is independent of $K$ due to the reduction to three-arm unstructured BAI (Theorem 2.2). For Gaussian distributions with unit variance, it yields that $T^* (\\mu) \\sim \\sum_{i\\in \\mathcal{N} (*)} (\\mu_* - \\mu_i) ^{-2}$. To capture the dependency in $K$ for Unimodal BAI, we derive a second lower bound showing that a linear dependence in $K$ is actually unavoidable for moderate regime of $\\delta$. The proof technique is borrowed from Simchowitz et al. (2017); Al Marjani et al. (2022), see Appendix B.\nTheorem 2.3. Let $\\Delta > 0$ and, for $i \\in [K]$, let $\\nu^{(i)} := N(\\mu^{(i)}, \\mathbb{I}_K)$ where $\\mu^{(i)} \\in S$, $\\mu^{(i)}_* = \\Delta$ and $\\mu^{(i)}_j = 0$ if $j\\neq i$."}, {"title": "3 ALGORITHMS", "content": "After specifying our recommendation and stopping rules (Section 3.1), we propose three sampling rules to solve unimodal BAI. We introduce U-TaS, which is an asymptotically optimal variant of TaS (Section 3.2). To alleviate the (wasteful) forced exploration of U-TaS, we provide an efficient implementation of the asymptotically optimal Optimistic TaS (O-TaS) algorithm, and show that O-TaS exploits the sparsity pattern (Section 3.3). Since the non-asymptotic guaranty of O-TaS do not match Theorem 2.3, we introduce an optimistic Top Two algorithm (Section 3.4) having near matching guarantees compared to Theorems 2.1 and 2.3, as well as the lowest computational cost."}, {"title": "3.1 Recommendation and Stopping Rules", "content": "As done extensively in the literature, we consider the generalized likelihood ratio (GLR) stopping rule (Garivier and Kaufmann, 2016). The GLR between two sets of bandit instances A and B is\n$\\operatorname{GLR}_t(A, B) = \\log \\frac{\\sup_{\\alpha\\in A} L_\\alpha(X_1, ..., X_t)}{\\sup_{\\theta \\in B} L_\\theta(X_1,..., X_t)},$"}, {"title": "3.2 Track-and-stop", "content": "Track-and-stop (Garivier and Kaufmann, 2016) (TaS) is one of the most famous meta-algorithm to design asymptotically optimal algorithms. To leverage the unimodal structure, we propose a computationally efficient variant of TaS, and refer to it as U-TaS.\nAfter sampling each arm once, U-TaS solves the optimization problem defining the characteristic time of an empirical unimodal instance $\\hat{\\mu}(t)$ obtained by transforming $\\hat{\\mu}(t)$. Let $t > K$ and $i^*(\\hat{\\mu}(t)) := i^*(\\hat{\\mu}(t))$. U-TaS computes $\\omega^*(\\hat{\\mu}(t))$ where\n$\\hat{\\mu}_i(t) := \\begin{cases} \\hat{\\mu}_{i-1}(t) & \\text{if } i > i^*(t) \\text{ and } \\hat{\\mu}_i(t) > \\hat{\\mu}_{i-1}(t), \\\\ \\hat{\\mu}_{i+1}(t) & \\text{if } i < i^*(t) \\text{ and } \\hat{\\mu}_i(t) > \\hat{\\mu}_{i+1}(t), \\\\ \\hat{\\mu}_i(t) & \\text{otherwise.} \\end{cases}$ (10)\nWhile Tas solves $\\omega^*(\\hat{\\mu}(t))$, $\\omega^*(\\lambda)$ can only be computed efficiently when $\\lambda \\in S$ (see below). Then, U-TaS computes the lo projection $\\omega(t)$ of $\\omega^*(\\hat{\\mu}(t))$ onto $(\\Delta(t))^{\\frac{1}{2t+K2}}$ where $\\Delta = \\Delta_K \\cap [\\epsilon, 1]^K$. Given this allocation $\\omega(t)$, U-TaS selects the next arm to be pulled by tracking the cumulative sum of weights, i.e.\n$I_t \\in \\operatorname{argmax}_{i\\in[K]} \\sum_{s \\in [t]} \\omega_i(s) - N_i(t).$ (11)\nThis guarantees that $N(t) = \\sum_{s\\in [t-1]} \\omega(s)$.\nAsymptotic Optimality Theorem 3.2 shows that U-TaS is asymptotically optimal (see Appendix D).\nTheorem 3.2. Using the GLR stopping rule as in (8), TaS is $\\delta$-correct and $\\limsup_{\\delta \\rightarrow 0} \\mathbb{E}_{\\nu}[T_\\delta]/\\log(1/\\delta) \\leq$"}, {"title": "3.3 Optimistic Track-and-Stop", "content": "Optimistic Track-and-Stop (Degenne et al., 2019) (\u041e-TaS) is a variant of TaS using optimism to foster exploration. We provide an efficient implementation of O-TaS for Unimodal BAI, and show it fosters sparsity.\nStructured Confidence Region Let $\\Theta_t$ be a confidence region around the empirical model $\\hat{\\mu}(t)$, namely\n$\\Theta_t := \\{\\theta | \\forall i \\in [K], N_i(t) d(\\hat{\\mu}_i(t), \\theta_i) \\leq f(t)\\},$ (12)\nwhere $f$ is distribution-dependent (e.g. a logarithm). We have $\\Theta_t = \\times_{i\\in[K]} [\\alpha_i(t), \\beta_i(t)]$ where $[\\alpha_i(t), \\beta_i(t)]$ by independence of the arms. Let $\\gamma_1, \\gamma_2 > 1$ and $f_U(t) := \\frac{\\gamma_1(\\gamma_1 + \\gamma_2)}{\\gamma_2} \\log t$. For $\\nu = N(\\mu, \\mathbb{I}_K)$, let\n$E_t := \\{ \\mu_i \\in [\\hat{\\mu}_i(s) \\pm \\sqrt{f_U(s)/N_i(s)}]\\} .$\nIt satisfies that $\\mathbb{P}(E^c_t) < \\frac{K}{t^2}$ for all $t \\in \\mathbb{N}$. The structured confidence region is defined as $\\tilde{\\Theta}_t := \\Theta_t \\cap S$.\nOptimistic Unimodal Instance After sampling each arm once, O-TaS compute an optimistic unimodal bandit $\\mu^+(t)$, and its corresponding oracle weights $\\omega(t)$, as the model within $\\tilde{\\Theta}_t$ that maximizes the function $\\lambda \\rightarrow T^*(\\lambda)^{-1}$, i.e.\n$\\mu^+(t) \\in \\operatorname{argmax}_{\\lambda \\in \\tilde{\\Theta}_t} \\sup_{\\omega \\in \\Delta_K(\\lambda)} \\min_{i \\in \\mathcal{N}(i^*(\\lambda))} g_i(\\omega, \\lambda), \\\\ \\omega(t) \\in \\operatorname{argmax}_{\\omega \\in \\Delta_K (\\mu^+(t))} \\min_{i \\in \\mathcal{N}(i^+(t))} g_i(\\omega, \\mu^+(t)),$\nwhere $i^+(t) := i^*(\\mu^+(t))$. Given the proportions $\\omega(t)$, O-TaS selects the arm $I_t$ with C-tracking as in (11).\nTheoretical Guarantees Theorem 3.2 is a corollary of a non-asymptotic upper bound (Theorem E.17) showing that O-TaS is asymptotically optimal. Unfortunately, the non-asymptotic bound for O-TaS does not match the K-dependency from Theorem 2.3.\nTheorem 3.3. Suppose that our distributions satisfy Assumptions E.1, E.2 and E.3 (bounded parameters, sub-Gausiannity and KL concentration). Using the GLR stopping rule as in (8), O-TaS is $\\delta$-correct and $\\limsup_{\\delta \\rightarrow 0} \\mathbb{E}_{\\nu}[T_\\delta]/\\log(1/\\delta) \\leq T^*(\\mu)$ for all $\\mu \\in S$.\nAs local information is sufficient for global optimality, O-TaS exploits the sparsity pattern by limiting the samples from $[K] \\setminus (\\mathcal{N}(i^*) \\cup \\{i^*\\})$. By using the structured $\\Theta_t$, we exhibit a random time after which O-TaS only pulls arms in $\\mathcal{N}(i^*) \\cup \\{i^*\\}$ (Theorem E.15). In Figure 1, we see that O-TaS improves on U-TaS which lacks this property due to forced exploration.\nEfficient Implementation In most pure exploration problems, Optimistic TaS is computationally intractable. For Unimodal BAI, O-Tas has an efficient implementation since computing $\\mu^+(t)$ requires O(K) operations. By iterating over the candidate optimistic answer $i \\in [K]$, we notice that the associated optimal optimistic instance $\\mu^{(i)}(t)$ is obtained as\n$\\hat{\\mu}_j^{(i)} (t) = \\begin{cases} \\beta_i(t) & \\text{if } j = i, \\\\ \\min \\{ \\beta_j(t), \\max \\{ \\hat{\\mu}_{j-1}^{(i)}(t), \\alpha_j(t) \\} \\} & \\text{if } j < i, \\\\ \\max \\{ \\alpha_j(t), \\min \\{ \\hat{\\mu}_{j+1}^{(i)}(t), \\beta_j(t) \\} \\} & \\text{if } j > i. \\end{cases}$ (13)\nBy using dynamic programming, we can compute $\\mu^{(i)}(t)$ from $\\mu^{(i-1)}(t)$ and check $\\mu^{(i)}(t) \\in S$ in constant time. Therefore, the above procedure computes $\\mu^+(t) = \\operatorname{argmax}_{S \\cap \\{ \\hat{\\mu}^{(i)}(t)\\} i \\in [K]} T^* (\\hat{\\mu}^{(i)} (t))^{-1}$ with a computational cost of O(K). It uses at most K calls to $T^*(\\cdot)^{-1}$. We refer the reader to Appendix E.6 for a proof and more details on the implementation."}, {"title": "3.4 Top Two Algorithm", "content": "Top Two algorithms have been studied for BAI (Russo, 2016; Qin et al., 2017; Shang et al., 2020; Jourdan et al., 2022). However, few Top Two algorithms have been analyzed in structured settings. By adapting TTUCB (Jourdan and Degenne, 2023), we propose the"}, {"title": "3.4.1 Non-asymptotic Guarantees", "content": "While UniTT is defined for general classes of distributions, we analyze it for Gaussian distributions"}, {"title": "3.4.2 Proof Sketch of Theorem 3.4", "content": "We sketch the proof Theorem 3.4 below and highlight its novelties, see Appendix F for details. Under $E_t$, we prove that $B_t = i^*$ except for a sublinear number of times (Lemma F.1). By definition of the stopping rule (8) with $\\hat{i}_t$ as in (7), not stopping implies that\n$c(t - 1, \\delta) > \\max_{i\\in[K]} \\min_{j\\in\\mathcal{N}(i)} W_t(i, j) \\geq W_t(B_t, C_t),$\nwhere we use the challenger in (15). Under suitable concentration and for $B_t = i^*$, using $W_t$ in (5) yields\n$c(t - 1, \\delta) \\geq \\frac{\\frac{2+\\gamma_1/2}{2 N_{C_t}(t)} + \\frac{2+\\gamma_1/2}{2 N_{i^*}(t)}}{\\mathcal{T}_{1/2}(\\mu)^{-1}} - O(\\log t).$ (18)\nUsing tracking (Lemma G.3), we have $N_{i^*}(t) \\approx t/2$. When $|\\mathcal{N}(i^*)| = 1$, we have $N_{C_t}(t) \\approx t/2$ as well. Re-ordering terms in (18) yields a $(t, \\delta, \\mu)$-dependent condition. As $\\mathcal{T}_\\mu(\\delta)$ is defined as the last time this condition can hold, it yields our first upper bound. When $|\\mathcal{N}(i^*)| = 2$, we prove that the ratio in the r.h.s. of (18) is larger than 1, e.g. by showing that $N_{C_t}(t) \\geq t\\omega_{1/2, C_t}$. We consider two different approaches."}, {"title": "4 EXPERIMENTS", "content": "We present experiments to (i) compare our three proposed algorithms, (ii) highlight the pitfalls of other asymptotic optimal algorithms such as DKM (Degenne et al., 2019), LMA (M\u00e9nard, 2019) or FW (Wang et al., 2021)), and (iii) showcase the benefits of exploiting the unimodal structure by comparing ourselves to unstructured TaS and TTUCB. We report the boxplots of the stopping times for 1000 independent runs using $\\delta = 0.01$. We use the heuristic $c_K (t, \\delta) = \\log(\\frac{K \\log(et)}{\\delta})$"}, {"title": "PERSPECTIVES", "content": "We studied the unimodal structure in fixed-confidence BAI. The derived lower bounds suggest that sparse optimal allocations are sufficient asymptotically, yet dense exploration is unavoidable in the moderate confidence regime. Algorithmically, we showcased how to exploit the sparsity induced by the unimodal structure in three different algorithms having efficiently implementation. On top of their good empirical performance, the proposed algorithms enjoy strong theoretical guarantees on their expected sample complexity: asymptotic optimality or non-asymptotic guarantees.\nDespite our promising results leveraging a known structural assumption, the unimodal structure is rather an exception than the norm. In all generality, it is challenging to fully exploit the structure algorithmically. Even for linear bandits, extending the analysis of Top Two algorithms is still an open problem."}]}