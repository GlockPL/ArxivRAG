{"title": "Learning Generalizable Features for Tibial Plateau Fracture Segmentation Using Masked Autoencoder and Limited Annotations", "authors": ["Peiyan Yue", "Die Cai", "Chu Guo", "Mengxing Liu", "Jun Xia", "Yi Wang"], "abstract": "Accurate automated segmentation of tibial plateau fractures (TPF) from computed tomography (CT) requires large amounts of annotated data to train deep learning models, but obtaining such annotations presents unique challenges. The process demands expert knowledge to identify diverse fracture patterns, assess severity, and account for individual anatomical variations, making the annotation process highly time-consuming and expensive. Although semi-supervised learning methods can utilize unlabeled data, existing approaches often struggle with the complexity and variability of fracture morphologies, as well as limited generalizability across datasets. To tackle these issues, we propose an effective training strategy based on masked autoencoder (MAE) for the accurate TPF segmentation in CT. Our method leverages MAE pretraining to capture global skeletal structures and fine-grained fracture details from unlabeled data, followed by fine-tuning with a small set of labeled data. This strategy reduces the dependence on extensive annotations while enhancing the model's ability to learn generalizable and transferable features. The proposed method is evaluated on an in-house dataset containing 180 CT scans with TPF. Experimental results demonstrate that our method consistently outperforms semi-supervised methods, achieving an average Dice similarity coefficient (DSC) of 95.81%, average symmetric surface distance (ASSD) of 1.91mm, and Hausdorff distance (95HD) of 9.42mm with only 20 annotated cases. Moreover, our method exhibits strong transferability when applying to another public pelvic CT dataset with hip fractures, highlighting its potential for broader applications in fracture segmentation tasks. The code will be publicly available.", "sections": [{"title": "I. INTRODUCTION", "content": "Tibial plateau fractures (TPF) present a significant challenge in medical imaging due to their complexity and variability [1]. The detection and analysis of these fractures are further complicated by their specific characteristics and diverse presentations, making accurate diagnosis labor-intensive for clinicians. Automated segmentation methods based on deep learning hold promise for addressing these issues, yet their effectiveness relies heavily on large and annotated datasets that are challenging to obtain. The acquisition of such data is limited by patient privacy concerns, medical ethics, and their requirement for expert medical annotations. Annotating TPF from computed tomography (CT) is particularly demanding, as it requires precise identification of fracture patterns, evaluation of severity, and consideration of individual anatomical variations, all within images often affected by noise, artifacts, and inconsistent imaging parameters. These challenges make the creation of high-quality annotated datasets both costly and time-consuming, underscoring the need for segmentation methods that perform effectively with limited annotated data.\nSemi-supervised learning, which utilizes a small set of labeled images in combination with a large pool of unlabeled images, has been successfully applied to various bone segmentation tasks [2]\u2013[9]. However, these semi-supervised learning methods still face limitations in TPF segmentation tasks. Firstly, existing semi-supervised learning methods for bone segmentation often rely on consistency regularization or self-training approaches [10]\u2013[15]. These methods typically use predictions generated by a teacher model on unlabeled data to supervise the training of a student model. However, as shown in Fig. 1, the diverse types and complex fragment morphologies of TPF pose huge challenges. Models trained on"}, {"title": "II. RELATED WORK", "content": "In recent years, several studies have focused on the automated segmentation of bone structures. Besler et al. [16] proposed an enhance-and-segment pipeline, which combined Hessian-based filtering and graph cut segmentation for proximal femur segmentation in CT. Gao et al. [17] introduced a multi-angle projection network for rib fracture segmentation in CT, utilizing rib extraction, fracture segmentation, and multi-angle projection fusion modules to capture rib and fracture features. Liu et al. [18] developed a two-stage approach for pelvic fracture segmentation, using a pelvic bone segmentation network followed by a fracture segmentation network. Cai et al. [19] proposed a 3D U-Net-based method for knee CT segmentation, generating 3D fracture maps and aiding Schatzker classification to improve clinical diagnostic efficiency. Zhou et al. [20] designed a framework for hip fracture segmentation, incorporating a cross-scale attention mechanism and a surface supervision strategy to enhance fracture representation and boundary accuracy.\nSome studies have focused on semi-supervised segmentation of bone structures. Zhao et al. [2] proposed a self-training method for finger bone segmentation in hand X-rays, utilizing a U-Net model with a conditional random field module to generate pseudo-labels for unlabeled images. Burton et al. [3] introduced a teacher-student model for knee MRI segmentation, employing a Monte Carlo patch sampling strategy to improve accuracy. Liu et al. [4] explored generative adversarial network (GAN)-based methods for lumbosacral structure segmentation on thin-layer CT, using semi-cGAN"}, {"title": "III. METHODS", "content": "As illustrated in Fig. 2, the proposed segmentation network comprises two stages: a MAE [21] pretraining stage and an UNETR [22] fine-tuning stage. The MAE employs an asymmetric encoder-decoder design. Its Vision Transformer (ViT) [23] encoder processes only visible tokens, while a lightweight decoder reconstructs masked patches using the patch-wise output from the encoder and trainable mask tokens. The UNETR follows the U-Net [24] principle of skip connections, linking features from multiple encoder resolutions to the decoder. The UNETR decoder takes a sequence of representations from the encoder, reshapes them to restore spatial dimensions, and progressively upsamples and concatenates them with shallower features to achieve high-resolution segmentation outputs. The detailed structures of the MAE and UNETR are described in the following subsections."}, {"title": "B. Pretrain stage with MAE", "content": "Masked image modeling (MIM) [21], [25]\u2013[27] is widely used in self-supervised learning because of its simple design and strong performance. The main idea of MIM is to mask some image patches at the input and reconstruct them at the output. This helps the network learn to predict the missing parts by using information from the surrounding context. We believe this ability to use contextual information is important for bone CT scans, as it helps capture the overall bone structures and the details of fractures.\nAmong the different MIM methods, MAE is both simple and effective. The encoder of MAE consists of 12 identical ViT blocks, each containing a multi-head self-attention layer and a fully connected feedforward network (see Fig. 3). The input image is divided into patches of size 16\u00d716\u00d716, which are linearly projected and flattened into one-dimensional vectors. Positional encoding is added to these patches to retain spatial information. Among these patches, 75% are masked, leaving only the visible patches to be processed by the ViT blocks. Similar to the encoder, the decoder in MAE comprises 8 layers of ViT blocks. The decoder's input includes all patches, which consist of visible patches output from the encoder and masked patches replaced with mask tokens. Each mask token is a shared, learnable vector representing the patch to be reconstructed. Positional encoding is also added to the decoder's input. The decoder's final layer is a linear projection whose output channels correspond to the number of pixel values in a patch. The output of the decoder is reshaped to form a reconstructed image. The training loss function computes the mean squared error (MSE) between the reconstructed and original images in the pixel space. As with most MIM strategies, the reconstruction loss is computed only on the masked patches.\nThrough this self-supervised reconstruction task, the encoder learns both the global structures and detailed features of the bone from the contextual information, which effectively benefits subsequent segmentation tasks."}, {"title": "C. Finetune stage with UNETR", "content": "U-Net has been widely used in medical image segmentation tasks, achieving favorable results across various applications due to its \"U-shaped\" architecture for the encoder and decoder. Similarly, UNETR adopts this successful \"U-shaped\" design, utilizing ViT blocks as the encoder, which makes it naturally compatible with ViT encoders pretrained using the MAE strategy. In UNETR, the decoder receives a sequence of feature representations from the encoder. Specifically, the outputs from the 3rd, 6th, 9th, and 12th layers of the encoder during the pretraining phase, along with the original image, are utilized as inputs to the decoder side of UNETR. These feature representations are reshaped to restore their spatial dimensions and then progressively upsampled. They are further concatenated with features from shallower layers to generate higher-resolution segmentation outputs.\nThe detailed architecture of the UNETR employed in our method is illustrated in Fig. 4. The segmentation task is optimized using a combined loss function comprising Dice loss and cross-entropy loss for supervision."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "Experiments were mainly carried on CT scans obtained from 180 patients with TPF. All these data were collected from the Shenzhen Second People's Hospital. This study was conducted retrospectively and therefore the research received a waiver of approval from our institutional review board. In addition, we further employed a public pelvic CT dataset [28] containing 103 scans with hip fractures (HF) to test the transferability of our method.\nFor preprocessing, voxels with CT values outside the range of -500 to 500 HU were excluded using a coarse filter, followed by windowing and centering to enhance and normalize the CT volumes. The TPF dataset was resampled to a voxel spacing of 0.328\u00d70.328\u00d71.000mm\u00b3, and the HF dataset was resampled to 0.839\u00d70.839\u00d70.800mm\u00b3. The TPF dataset was split into 20 labeled and 50 unlabeled training cases, 10 validation cases, and 100 testing cases. Similarly, the HF dataset was divided into 20 labeled training cases, 10 validation cases, and 73 testing cases."}, {"title": "B. Comparison and Evaluation", "content": "We compared our method with mainstream semi-supervised approaches under varying labeled data proportions. The baseline followed the student-teacher co-training paradigm, where the teacher generated pseudo-labels for weakly augmented unlabeled data, and the student was trained on weakly augmented labeled data (with ground truth) and strongly augmented unlabeled data (with pseudo-labels). For the TPF dataset, strong augmentations comprised random masking, rotations, scaling, and translations.\nTo further investigate the transferability of our method across different datasets, we pretrained the encoder using MAE on the TPF dataset and finetuned it for HF segmentation task. We compared the performance of our approach with models trained exclusively on HF data, including UNETR and U-Net."}, {"title": "V. CONCLUSION", "content": "This study presents an accurate and automated method for the segmentation of tibial plateau fractures in CT scans. The primary motivation is to leverage the distinct features of bone structures, particularly the anatomical and fracture characteristics, to enhance segmentation performance. We propose a pretrain-finetune strategy based on MAE, where the pretraining stage captures global anatomical structures and fine-grained fracture details from unlabeled data, and the fine-tuning stage uses limited labeled data to guide segmentation. This approach effectively reduces dependence on extensive annotations while learning generalizable and transferable features. Experiments on both in-house and public datasets prove the efficacy of the proposed method. Compared with existing semi-supervised approaches, our method demonstrates superior performance and strong transferability across different datasets, highlighting its potential for broader applications in fracture segmentation and other medical imaging tasks."}]}