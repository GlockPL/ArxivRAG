{"title": "Provable Privacy Advantages of Decentralized Federated Learning via Distributed Optimization", "authors": ["Wenrui Yu", "Qiongxiu Li", "Milan Lopuha\u00e4-Zwakenberg", "Mads Gr\u00e6sb\u00f8ll Christensen", "Richard Heusdens"], "abstract": "Federated learning (FL) emerged as a paradigm designed to improve data privacy by enabling data to reside at its source, thus embedding privacy as a core consideration in FL architectures, whether centralized or decentralized. Contrasting with recent findings by Pasquini et al., which suggest that decentralized FL does not empirically offer any additional privacy or security benefits over centralized models, our study provides compelling evidence to the contrary. We demonstrate that decentralized FL, when deploying distributed optimization, provides enhanced privacy protection - both theoretically and empirically - compared to centralized approaches. The challenge of quantifying privacy loss through iterative processes has traditionally constrained the theoretical exploration of FL protocols. We overcome this by conducting a pioneering in-depth information-theoretical privacy analysis for both frameworks. Our analysis, considering both eavesdropping and passive adversary models, successfully establishes bounds on privacy leakage. In particular, we show information theoretically that the privacy loss in decentralized FL is upper bounded by the loss in centralized FL. Compared to the centralized case where local gradients of individual participants are directly revealed, a key distinction of optimization-based decentralized FL is that the relevant information includes differences of local gradients over successive iterations and the aggregated sum of different nodes' gradients over the network. This information complicates the adversary's attempt to infer private data. To bridge our theoretical insights with practical applications, we present detailed case studies involving logistic regression and deep neural networks. These examples demonstrate that while privacy leakage remains comparable in simpler models, complex models like deep neural networks exhibit lower privacy risks under decentralized FL. Extensive numerical tests further validate that decentralized FL is more resistant to privacy attacks, aligning with our theoretical findings.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated Learning (FL) enables collaborative model training across multiple participants/nodes/clients without directly sharing each node's raw data [1]. FL can operate on either a centralized/star topology or a decentralized topology, as shown in Figure 1 [2]. The prevalent centralized topology requires a central server that interacts with each and every node individually. The main procedure of a centralized FL protocol typically unfolds in three steps: 1) Nodes train local models based on their own private dataset and transmit model updates, such as gradients, to the server; 2) The server aggregates the local models to a global model and redistributes to the nodes; 3) Nodes update the local models based on the global model and send the model updates back to the server. The process is iteratively repeated until convergence. However, a centralized server is not always feasible due to its high communication demands and the need for universal trust from all nodes. In addition, it poses a risk of a single point of failure, making the network vulnerable to targeted attacks. As an alternative, decentralized FL circumvents these issues by facilitating direct data exchanges between (locally) connected nodes, thereby eliminating the need for a central server for model aggregation.\nDecentralized FL protocols, also known as peer-to-peer learning protocols, fall into two main categories. The first involves average-consensus-based protocols. With these protocols, instead of sending model parameters to a central server, nodes collaborate together to perform model aggregation nodes in a distributed manner. The aggregation is typically done by partially averaging the local updates within a node's neighborhood. Examples of these protocols are the empirical methods where the aggregation is done using average consensus techniques such as gossiping SGD [3], D-PSGD [4], and variations thereof [5], [6]. The second category comprises protocols that are based on distributed optimization, referred to as optimization-based decentralized FL. These (iterative) methods directly formulate the underlying problem as a constrained optimization problem and employ distributed solvers like ADMM [7], [8], [9] or PDMM [10], [11], [12] to solve them. The constraints are formulated in such a way that, upon convergence, the learned models at all nodes are identical. Hence, there is no explicit separation between updating local models and the update of the global model, i.e., the three steps in centralized FL mentioned before are executed simultaneously.\nDespite not directly sharing private data with servers or nodes, FL is shown vulnerable to privacy attacks as the exchanged information, such as gradients or weights, still poses a risk for privacy leakage. Existing work on privacy leakage predominantly focuses on the centralized case. A notable example is the gradient inversion attack [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], an iterative method for finding input data that produce a gradient similar to the gradient generated by the private data. Such attacks are based on the assumption that similar gradients are produced by similar data samples."}, {"title": "A. Paper contribution", "content": "In exploring the privacy aspects of centralized versus decentralized FL, many works claim that the decentralized FL is more privacy-preserving than centralized FL without any privacy argument [23], [24], [25]. The main idea is that sensitive information, such as private data, model weights, and user states, can no longer be observed or controlled through a single server. However, recent empirical findings challenge such a claim, particularly for average-consensus-based decentralized FL protocols. It is shown in [26] that these protocols may not inherently offer better privacy protections over centralized FL, and might even increase susceptibility to privacy breaches. For instance, it has been shown that an arbitrary colluding client could potentially obtain the same amount of information as a central server in centralized FL when inverting input private data via gradients. In contrast, the privacy implications of optimization-based decentralized FL protocols have, to the best of our knowledge, been rarely investigated. This research gap is partly due to the fact that analytically tracking the privacy leakage in distributed algorithms, particularly over multiple iterations, is very challenging. The main difficulty lies in distinguishing and comprehending how information is correlated between these iterations.\nIn this paper, we take the first step to perform a theoretical privacy analysis of both centralized and decentralized FL frameworks by analyzing the information flow within the network. Our key contributions are summarized below:\n\u2022 Analytical privacy bounds of decentralized FL: We conduct an information-theoretical privacy analysis of both centralized and decentralized FL protocols, using mutual information as a key metric. To the best of our knowledge, this is the first information-theoretical privacy analysis in this context. Notably, we derive two privacy bounds for the optimization-based decentralized FL and show that its privacy loss is upper bounded by the loss of centralized FL (Theorem 1 in Section V). We further exemplify the derived privacy gap through two applications, including logistic regression and Deep Neural Networks (DNNs).\n\u2022 Empirical validation through privacy attacks: For DNN applications, we show that in the case of optimization-based decentralized FL, gradient inversion attacks can be applied to reconstruct the original input data, but the reconstruction performance is degraded compared to centralized FL due to the limited amount of information available to the adversary. A similar trend is also observed when evaluated using membership inference attacks. Overall, decentralized FL employing distributed optimization is shown to be less vulnerable to privacy attacks compared to centralized FL, consistent with our theoretical findings. This finding challenges the previous belief that decentralized FL offers no privacy advantages compared to centralized FL [26] (see Section VIII-E for a detailed explanation)."}, {"title": "B. Outline and Notation", "content": "The paper is organized as follows. Section II reviews necessary fundamentals, and Section III introduces the involved metrics for quantifying privacy. Section IV introduces the optimization-based decentralized FL protocol. Section V analyzes the privacy of both centralized and decentralized FL protocols and states the main result. Section VI analyzes logistic regression example. Section VII, VIII analyze the application of DNNs. Conclusions are given in Section IX.\nWe use bold lowercase letters to denote vectors x and bold uppercase letters for matrices X. Calligraphic letters X denotes sets. The $i$th entry of a vector x is denoted $x_i$. The superscript $(\\cdot)^T$ denotes matrix transposition. I is used to denote the identity matrix of appropriate dimensions. 0 and 1 are the all-zero and all-one vectors. $\\nabla$ denotes the gradient. The value of the variable $x$ at iteration $t$ is denoted as $x^{(t)}$. We use $\\| \\cdot \\|$ to indicate the $l_2$-norm and $\\text{ran}(\\cdot)$ and $\\text{ker}(\\cdot)$ to denote the range and kernel of their argument, respectively. For the sake of notational simplicity, we represent random variables using capital letters, regardless of whether its outcome is a scalar, a vector, or a matrix. $\\mathcal{M}(x, i) := \\mathbb{1}\\{(x, i) \\in \\mathcal{D}\\}$ is the indicator variable showing whether $x$ is in the dataset $\\mathcal{D}$."}, {"title": "II. PRELIMINARIES", "content": "This section reviews the necessary fundamentals for the remainder of the paper.\nA. Centralized FL\nWithout loss of generality, we focus on classification problems involving $n$ nodes, each with its local dataset $\\{(x_{ik}, l_{ik}): k = 1,...,n_i\\}$, where $x_{ik} \\in \\mathbb{R}^v$ represents an input sample, $l_{ik} \\in \\mathbb{R}$ is the associated label and $n_i$ is the number of input samples. The dimension $v$ of the data samples is application-dependent. Collecting the $x_{ik}$s and $l_{ik}$s, we define $x_i = (x_{i1},...,x_{in_i})^T$, and $l_i = (l_{i1},..., l_{in_i})^T$. Let $f_i(w_i, (x_i, l_i))$ denote the cost function of node $i$ where $w_i \\in \\mathbb{R}^d$ is the model weight to be learned from the input dataset $(x_i, l_i)$, whose dimension, again, depends on the application. In the remainder of the paper we will omit the $(x_i, l_i)$ dependency for notational convenience when it is clear from the context and simply write $f_i(w_i)$. A typical centralized FL protocol works as follows:\n1) Initialization: at iteration $t = 0$, the central server randomly initializes the weights $w_i^{(0)}$ for each node.\n2) Local model training: at each iteration $t$, each user $i$ first receives the model updates from the server and then computes its local gradient, denoted as $\\nabla f_i(w_i^{(t)})$, using its local data $x_i$.\n3) Model aggregation: the server collects these local gradients and performs aggregation to update the global model. The"}, {"title": "B. Decentralized FL", "content": "aggregation is often done by weighted averaging and typically uniform weights are applied, i.e., $\\frac{1}{n}\\sum_{i=1}^n \\nabla f_i(w_i^{(t)})$. Subsequently, each node $i$ then updates its own model weight by\n$$w_i^{(t+1)} = w_i^{(t)} - \\mu \\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(w_i^{(t)}),$$\nwhere $\\mu$ is a constant controlling the convergence rate. The last two steps are repeated until the global model converges or until a predetermined stopping criterion is reached.\nThis algorithm is often referred to as the FedAvg [1].\nB. Decentralized FL\nDecentralized FL works for cases where a trusted centralized server is not available. In such cases, it works on a so-called distributed network which is often modeled as an undirected graph: $\\mathcal{G} = (\\mathcal{V},\\mathcal{E})$, with $\\mathcal{V} = \\{1,2,..., n\\}$ representing the node set and $\\mathcal{E} \\subseteq \\mathcal{V} \\times \\mathcal{V}$ representing the edge set. $\\mathcal{N}_i = \\{j \\mid (i, j) \\in \\mathcal{E}\\}$ denotes the set of neighboring nodes of node $i$. In this decentralized setup, each node $i$ can only communicate with its neighboring nodes $j\\in \\mathcal{N}_i$, facilitating peer-to-peer communication without any centralized coordination.\n1) Average consensus-based approaches: The model aggregation step requires all nodes' local gradients. Many decentralized FL protocols work by deploying distributed average consensus algorithms to compute the average of local gradients, i.e., computing $\\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(w_i^{(t)})$ in Eq. (1) without any centralized coordination. Example average consensus algorithms are gossip [27] and linear iterations [28], which allow peer-to-peer communication over distributed networks.\nThe common decentralized FL often works similarly to the FedAvg algorithm, except for the step of model aggregation. For instance, D-PSGD [4], [29] uses gossip averaging with neighbors to implement the aggregation, i.e.,\n$$w_i^{(t+1)} = w_i^{(t)} - \\frac{\\mu}{d_i} \\sum_{j\\in \\mathcal{N}_i} \\nabla f_j(w_i^{(t)}),$$\nwhere $d_i = |\\mathcal{N}_i|$ is the degree of node $i$.\n2) Distributed optimization-based approaches: The goal of optimization-based decentralized FL is to collaboratively learn a global model, given the local datasets $\\{(x_i,l_i) : i \\in \\mathcal{V}\\}$, without any centralized coordination. The underlying problem can be posed as a constrained optimization problem given by\n$$\\min_{\\{w_i: i\\in \\mathcal{V}\\}} \\sum_{i\\in \\mathcal{V}} f_i(w_i),$$\nsubject to $\\{(i, j) \\in \\mathcal{E} : B_{i|j} w_i + B_{j|i} w_j = 0\\},$ where $B_{i|j}$ and $B_{j|i}$ define linear edge constraints. To ensure all nodes share the same model at convergence (consensus constraints) we have $B_{i|j} = -B_{j|i} = \\pm I$. In the following, we will use the convention that $B_{i|j} = I$ if $i < j$ and $B_{i|j} = -I$ otherwise.\nIn what follows we will refer to centralized FL as CFL. While decentralized FL encompasses both average consensus-based and distributed optimization-based approaches (recall Section II-B), for simplicity, we will use the abbreviation DFL to specifically refer to the optimization-based decentralized FL as it is our main focus. We will differentiate between the"}, {"title": "C. Threat models", "content": "two methods in contexts where such distinction is necessary to avoid confusion.\nC. Threat models\nWe consider two types of adversary models: the eavesdropping and the passive (also known as honest-but-curious) adversary model. While eavesdropping can typically be addressed through channel encryption [30], it remains a pertinent concern in our context. This relevance stems from the nature of iterative algorithms, where communication channels are utilized repeatedly, continuously encrypting each and every message incurs high communication overhead. Therefore, in our framework, we assume that network communication generally occurs over non-secure channels, except for the initial network setup phase, details of which will be discussed later (see Section IV). The passive adversary consists of a number of colluding nodes, referred to as corrupt nodes, which comply with the algorithm instructions but utilize the received information to infer the private input data of the other so-called honest nodes. Consequently, the adversary has access to the following information: (a) all information gathered by the corrupt nodes, and (b) all messages transmitted over unsecured (i.e., non-encrypted) channels."}, {"title": "III. PRIVACY EVALUATION", "content": "When quantifying privacy, there are mainly two types of metrics: 1) empirical evaluation which assesses the susceptibility of the protocol against established privacy attacks, and 2) information-theoretical metrics which offer a robust theoretical framework independent of empirical attacks. In this paper, we first evaluate privacy via an information-theoretical metric and then deploy empirical attacks to validate our theoretical results.\nA. Information-theoretical privacy metric\nAmong the information-theoretical metrics, popular ones include for example 1) $\\epsilon$-differential privacy [31], [32] which guarantees that the posterior guess of the adversary relating to the private data is only slightly better (quantified by $\\epsilon$) than the prior guess; 2) mutual information [33] which quantifies statistically how much information about the private data is revealed given the adversary's knowledge, In this paper, we choose mutual information as the information-theoretical privacy metric. The main reasons are the following. Mutual information has been proven effective in measuring privacy losses in distributed settings [34], and has been applied in various applications [35], [36], [37], [38], [39]. Secondly, mutual information is intrinsically linked to $\\epsilon$-differential privacy (see [40] for more details) and is more feasible to realize in practice [41], [42].\n1) Fundamentals of mutual information: Given two (discrete) random variables $X$ and $Y$, the mutual information $I(X; Y)$ between $X$ and $Y$ is defined as\n$$I(X; Y) = H(X) - H(X|Y),$$\nwhere $H(X)$ represents the Shannon entropy of $X$ and $H(X|Y)$ is the conditional Shannon entropy, assuming they"}, {"title": "IV. DFL USING DISTRIBUTED OPTIMIZERS", "content": "exist.\u00b9 It follows that $I(X; Y) = 0$ when $X$ and $Y$ are independent, indicating that $Y$ carries no information about $X$. Conversely, $I(X; Y)$ is maximal when $Y$ and $X$ share a one-to-one correspondence.\nDenote $\\mathcal{V}_h$ and $\\mathcal{V}_e$ as the set of honest and corrupt nodes, respectively. Let $\\mathcal{O}$ denote the set of information obtained by the adversary. Hence, the privacy loss, measured by the mutual information between the private data $x_i$ of honest node $i \\in \\mathcal{V}_h$ and the knowledge available to the adversary, is given by\n$$I(X_i; \\mathcal{O}).$$\nB. Empirical evaluation via privacy attacks\nTo complement our theoretical analysis, we incorporate empirical privacy attacks to validate our findings. In machine learning, based on the nature of the disclosed private data, privacy breaches typically fall into three types: membership inference [43], [44], [45], the property inference [44], [46] and the input reconstruction attack [47], [48], [49], [50], where the revealed information is membership (whether a particular data sample belongs to the training dataset or not), properties of the input such as age and gender, and the input training data itself, respectively. Given that Eq. (5) measures how much information about the input training data is revealed, we align our empirical evaluation by mainly focusing on input reconstruction attacks. In FL, the gradient inversion attack has been extensively studied for its effectiveness in reconstructing input samples (see Section VIII-C for extended results using membership inference attacks).\n1) Gradient inversion attack: The gradient inversion attack typically works by iteratively refining an estimate of the private input data to align with the observed gradients generated by such data. For each node's local dataset $(x_i, l_i)$, the goal of the adversary is to recover the input data $(x_i^*, l_i^*)$ based on the observed gradient. A typical setup is given by [13]:\n$$(x^*, l^*) = \\arg \\min_{x',l'}||\\nabla f_i(w_i, (x'_i, l'_i)) - \\nabla f_i(w_i, (x_i, l_i)) ||^2,$$\nand many variants thereof are proposed [14], [15], [16], [17], [18], [19], [20], [21], [22]. To evaluate the quality of reconstructed inputs, we use the widely adopted structural similarity index measure (SSIM) [51] to measure the similarity between the reconstructed images and true inputs. The SSIM index ranges from -1 to 1, where $\\pm1$ signifies perfect resemblance and 0 indicates no correlation.\nAnalytical label recovery via local gradient: While it appears that both the input data $x_i$ and its label $l_i$ in Eq. (6) require reconstruction through optimization, existing work normally assumes that the label is already known. This is because the label can often be analytically inferred from the shared gradients [14], [45]. The main reason is as follows. Consider a classification task where the neural network has L layers and is trained with cross-entropy loss. Assume $n_i = 1$ for simplicity (one data sample at each node). Let $y = (y_1,\\dots, y_C)$ denote the outputs (logits), where $y_i$ is the score (confidence) predicted for the $i$th class. With this, the cross-entropy loss over one-hot labels is given by\n$$f_i(w_i) = - \\log \\left(\\frac{e^{y_{l_i}}}{\\sum_{j} e^{y_j}}\\right) = - y_{l_i} + \\log \\left(\\sum_j e^{y_j}\\right),$$\nwhere $\\log(\\cdot)$ denotes the natural logarithm. Let $w_{i,L,c}$ denote the weights in the output layer $L$ corresponding to output $y_c$. The gradient of $f_i(w_i)$ with respect to $w_{i,L,c}$ can then be expressed as [14]:\n$$\\nabla_{w_{i,L,c}} f_i(w_i) = \\frac{\\partial f_i(w_i)}{\\partial w_{i, L,c}} = \\frac{\\partial f_i(w_i)}{\\partial y_c} \\frac{\\partial y_c}{\\partial w_{i, L,c}} = g_c a_{L-1},$$\nwhere $a_{L-1}$ is the activation at layer $L - 1$ and $g_c$ is the gradient of the cross entropy Eq. (7) with respect to logit $c$:\n$$g_c = \\frac{e^{y_c}}{\\sum_{j} e^{y_j}} - \\delta_{c,l_i},$$\nwhere $\\delta_{c,l_i}$ is the Kronecker-delta, defined as $\\delta_{c,l_i} = 1$ when $c = l_i$ and $\\delta_{c,l_i} = 0$ otherwise. Consequently, $g_c < 0$ for $c = l_i$ and $g_c > 0$ otherwise. Since the activation $a_{L-1}$ is independent of the class index $c$, the ground-truth label $l_i$ can be inferred from the shared gradients since $\\nabla' f_i(w_{i,L,l_i})\\nabla' f_i(w_{i,L,c}) = g_{l_i}g_c||a_{L-1}||^2 < 0$ for $c \\neq l_i$ and positive only for $c = l_i$. When dealing with $n_i > 1$, recovering labels becomes more challenging, yet feasible approaches are available. One example approach shown in [45] leverages the fact that the gradient magnitude is proportional to the label frequency in untrained models. Hence, in the centralized FL case, label information can often be deduced from the shared gradients thus improving both the efficiency and accuracy of the reconstructed input $x^*$ when compared to the real private input $x_i$ [14]. While for the decentralized FL protocol, we will show that the label information cannot be analytically computed for certain cases, thereby inevitably decreasing both the efficiency and reconstruction quality (see details in Remark 4)."}, {"title": "IV. DFL USING DISTRIBUTED OPTIMIZERS", "content": "This section introduces distributed solvers considered in this work, explains pivotal convergence properties relevant to subsequent privacy analyses, and gives details of the decentralized protocol using distributed optimization techniques.\nA. Distributed optimizers\nGiven the optimization problem Eq. (3), many distributed op- timizers, notably ADMM [52] and PDMM [10], [11] have been proposed. From a monotone operator theory perspective [53], [11], ADMM can be seen as a $\\frac{1}{\\rho}$-averaged version of PDMM, allowing both to be analyzed within the same theoretical framework. Due to the averaging, ADMM is generally slower than PDMM, assuming it converges. Both ADMM and PDMM solve the optimization problem Eq. (3) iteratively, with the update equations for node $i$ given by:\n$$w_i^{(t+1)} = \\underset{w_i}{\\arg \\min} \\left(f_i(w_i) + \\sum_{j\\in \\mathcal{N}_i} z_{i|j}^{(t)T} B_{i|j} w_i + \\frac{\\rho d_i}{2} ||w_i||^2\\right),$$\n$$z_{j|i}^{(t+1)} = (1 - \\theta)z_{j|i}^{(t)} + \\theta \\left(p B_{j|i} w_i^{(t+1)} + \\frac{1}{2} (z_{j|i}^{(t)} + 2p B_{i|j} w_i^{(t)})\\right),$$"}, {"title": "V. PRIVACY ANALYSIS", "content": "where $\\rho$ is a constant controlling the rate of convergence. The parameter $\\theta \\in (0,1]$ controls the operator averaging with $\\theta = \\frac{1}{\\rho}$ (Peaceman-Rachford splitting) yielding ADMM and $\\theta = 1$ (Douglas-Rachford splitting) leading to PDMM. $z$ is called auxiliary variable having entries indicated by $z_{i|j}$ and $z_{j|i}$, held by node $i$ and $j$, respectively, related to edge $(i, j) \\in \\mathcal{E}$. Eq. (10) updates the local variables (weights) $w_i$, whereas Eq. (11) represents the exchange of data in the network through the auxiliary variables $z_{j|i}$.\nB. Differential A/PDMM\nThe optimality condition for Eq. (10) is given by\u00b2\n$$0 = \\nabla f_i(w_i^{(t)}) + \\sum_{j \\in \\mathcal{N}_i}B_{i|j}z_{j|i}^{(t)} + \\rho d_i w_i^{(t)}.$$\nGiven that the adversary can eavesdrop all communication channels, by inspection of Eq. (12), transmitting the auxiliary variables $z_{j|i}^{(t)}$ would expose $\\nabla f_i(w_i^{(t)})$, as $w_i^{(t)}$ can be determined from Eq. (11). Encrypting $z_{j|i}^{(t)}$ at every iteration would address this, albeit at prohibitive computational expenses. To circumvent this, only initial values $z_{j|i}^{(0)}$ are securely transmitted and $\\Delta z_{j|i}^{(t+1)} = z_{j|i}^{(t+1)} - z_{j|i}^{(t)}$ being unencrypted in subsequent iterations [54], [55]. Consequently, upon receiving $\\Delta z_{j|i}^{(t+1)}$, $z_{j|i}^{(t+1)}$ is reconstructed as\n$$z_{j|i}^{(t+1)} = \\Delta z_{j|i}^{(t+1)} + z_{j|i}^{(t)} = \\sum_{\\tau=1}^{t+1} \\Delta z_{j|i}^{(\\tau)} + z_{j|i}^{(0)}.$$\nLet $t_{\\text{max}}$ denote the maximum number of iteration and denote $\\mathcal{T} = \\{0,1,..., t_{\\text{max}}\\}$. Hence, eavesdropping only uncovers\n$$\\{\\Delta z_{j|i}^{(t+1)} : (i, j) \\in \\mathcal{E}, t \\in \\mathcal{T}\\},$$\nand $z_{j|i}^{(t+1)}$ remains undisclosed unless the initialized $z_{j|i}^{(0)}$ is known.\nC. DFL using differential A/PDMM\nADMM is guaranteed to converge to the optimal solution for arbitrary convex, closed and proper (CCP) objective functions $f_i$, whereas PDMM will converge in the case of differentiable and strongly convex functions [11]. Recently, it has been shown that these solvers are also effective when applied to non-convex problems like training DNNs [12]. Note that for complex non-linear applications such as training DNNs, although exact solutions of Eq. (10) are usually unavailable, convergence analysis of approximated solutions has been extensively investigated. For instance, it is shown in [12] that PDMM, using quadratic approximations, achieves good performance for non-convex tasks such as training DNNs. Moreover, convergence guarantees with quantized variable transmissions are investigated in [56].\nDetails of DFL using differential A/PDMM solvers are summarized in Algorithm 1. Note that at the initialization it requires that each node randomly initialize $z_{i|j}^{(0)}$ from independent distributions having variance $\\sigma_z^2$ and sends it\n\u00b2Note that ADMM can also be applied to non-differentiable problems where the optimality condition can be expressed in terms of subdifferentials: $0 \\in \\partial f_i(w_i^{(t)}) + \\sum_{j \\in \\mathcal{N}_i}B_{i|j}z_{j|i}^{(t)} + \\rho d_i w_i^{(t)}.$ to neighboring $j \\in \\mathcal{N}_i$ via secure channels, also referred to as the subspace perturbation technique [57], [55]. The core concept involves introducing noise into the auxiliary variable $z$ to obscure private data from potential exposure, while the convergence of $w$ is not affected. To explain this idea, consider Eq. (11) in a compact form:\n$$z^{(t+1)} = (1 - \\theta)z^{(t)} + \\theta (P z^{(t)} + 2c P_C w^{(t)}),$$\nwhere $C = [B_+, B_-]^T$ and $B_+$ and $B_-$ contains the positive and negative entries of B, respectively. Additionally, P is a permutation matrix that interchanges the upper half rows and lower half rows of the matrix it multiplies, leading to $P_C = [B_+, B_-]^T$. Denote $\\Psi = \\text{ran}(C) + \\text{ran}(PC)$, its orthogonal complement is denoted by $\\Psi^{\\perp} = \\text{ker}(C^T) \\cap \\text{ker}((PC)^T)$. Let $\\Pi$ represent the orthogonal projection. We can then decompose $z$ into components within $\\Psi$ and $\\Psi^{\\perp}$ as $z^{(t)} = z_\\Psi^{(t)} + z_{\\Psi^{\\perp}}^{(t)}$. Note that the component $z_{\\Psi^{\\perp}}^{(t)} \\neq 0$, requiring that the number of edges should be no smaller than the number of nodes. This condition is, however, not met in CFL with a star topology.\nIt has been proven in [54] that\n$$z_{\\Psi^{\\perp}}^{(t)} = (P z_{\\Psi^{\\perp}}^{(t)} + \\frac{1}{2p}) + (1 - 2\\theta) (z_{\\Psi^{\\perp}}^{(0)} - P z_{\\Psi^{\\perp}}^{(0)}).$$ Thus, for a given graph structure and $\\theta$, $z_{\\Psi^{\\perp}}^{(t)}$ depends solely on the initialization of the auxiliary variable $z_{i|j}^{(0)}$. Consequently, if $z_{i|j}^{(0)}$ is not known by the adversary, so does $z_{i|j}^{(t)}$ for subsequent iterations. This is key that privacy advantages can be provided when compared to centralized FL (in Remark 1 we will analyze the privacy loss for CFL when applying a similar trick).\nIn this section, we conduct the comparative analysis of privacy loss in both CFL and DFL protocols, specifically focusing on the FedAvg algorithm and the decentralized"}, {"title": "V. PRIVACY ANALYSIS", "content": "In this paper", "applications": "logistic regression and training a DNN. In the case of simple logistic regression", "subdifferentials": 0}, {"title": "VI. LOGISTIC REGRESSION", "content": "Logistic regression is widely adopted in various applications and serves as a fundamental building block for complex$$\\min_{\\mathcal{N"}, {"mathcal{T}": "nabla f_i(w_i^{(t+1)}) - \\nabla f_i(w_i^{(t)}),$$\nalone\n$$ \\in \\mathcal{T}, \\ \\substack{\\text{i.e.,} \\  w_j^{(t_{\\text{max}"}, "mathcal{V}.\\\n$$"]}