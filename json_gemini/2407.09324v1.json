{"title": "Provable Privacy Advantages of Decentralized Federated Learning via Distributed Optimization", "authors": ["Wenrui Yu", "Qiongxiu Li", "Milan Lopuha\u00e4-Zwakenberg", "Mads Gr\u00e6sb\u00f8ll Christensen", "Richard Heusdens"], "abstract": "Federated learning (FL) emerged as a paradigm designed to improve data privacy by enabling data to reside at its source, thus embedding privacy as a core consideration in FL architectures, whether centralized or decentralized. Contrasting with recent findings by Pasquini et al., which suggest that decentralized FL does not empirically offer any additional privacy or security benefits over centralized models, our study provides compelling evidence to the contrary. We demonstrate that decentralized FL, when deploying distributed optimization, provides enhanced privacy protection - both theoretically and empirically - compared to centralized approaches. The challenge of quantifying privacy loss through iterative processes has traditionally constrained the theoretical exploration of FL protocols. We overcome this by conducting a pioneering in-depth information-theoretical privacy analysis for both frameworks. Our analysis, considering both eavesdropping and passive adversary models, successfully establishes bounds on privacy leakage. In particular, we show information theoretically that the privacy loss in decentralized FL is upper bounded by the loss in centralized FL. Compared to the centralized case where local gradients of individual participants are directly revealed, a key distinction of optimization-based decentralized FL is that the relevant information includes differences of local gradients over successive iterations and the aggregated sum of different nodes' gradients over the network. This information complicates the adversary's attempt to infer private data. To bridge our theoretical insights with practical applications, we present detailed case studies involving logistic regression and deep neural networks. These examples demonstrate that while privacy leakage remains comparable in simpler models, complex models like deep neural networks exhibit lower privacy risks under decentralized FL. Extensive numerical tests further validate that decentralized FL is more resistant to privacy attacks, aligning with our theoretical findings.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated Learning (FL) enables collaborative model training across multiple participants/nodes/clients without directly sharing each node's raw data [1]. FL can operate on either a centralized/star topology or a decentralized topology, as shown in Figure 1 [2]. The prevalent centralized topology requires a central server that interacts with each and every node individually. The main procedure of a centralized FL protocol typically unfolds in three steps: 1) Nodes train local models based on their own private dataset and transmit model updates, such as gradients, to the server; 2) The server aggregates the local models to a global model and redistributes to the nodes; 3) Nodes update the local models based on the global model and send the model updates back to the server. The process is iteratively repeated until convergence. However, a centralized server is not always feasible due to its high communication demands and the need for universal trust from all nodes. In addition, it poses a risk of a single point of failure, making the network vulnerable to targeted attacks. As an alternative, decentralized FL circumvents these issues by facilitating direct data exchanges between (locally) connected nodes, thereby eliminating the need for a central server for model aggregation.\nDecentralized FL protocols, also known as peer-to-peer learning protocols, fall into two main categories. The first involves average-consensus-based protocols. With these pro- tocols, instead of sending model parameters to a central server, nodes collaborate together to perform model aggregation nodes in a distributed manner. The aggregation is typically done by partially averaging the local updates within a node's neighborhood. Examples of these protocols are the empirical methods where the aggregation is done using average consensus techniques such as gossiping SGD [3], D-PSGD [4], and variations thereof [5], [6]. The second category comprises protocols that are based on distributed optimization, referred to as optimization-based decentralized FL. These (iterative) methods directly formulate the underlying problem as a constrained optimization problem and employ distributed solvers like ADMM [7], [8], [9] or PDMM [10], [11], [12] to solve them. The constraints are formulated in such a way that, upon convergence, the learned models at all nodes are identical. Hence, there is no explicit separation between updating local models and the update of the global model, i.e., the three steps in centralized FL mentioned before are executed simultaneously.\nDespite not directly sharing private data with servers or nodes, FL is shown vulnerable to privacy attacks as the exchanged information, such as gradients or weights, still poses a risk for privacy leakage. Existing work on privacy leakage predominantly focuses on the centralized case. A notable example is the gradient inversion attack [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], an iterative method for finding input data that produce a gradient similar to the gradient generated by the private data. Such attacks are based on the assumption that similar gradients are produced by similar data samples."}, {"title": "A. Paper contribution", "content": "In this paper, we take the first step to perform a theoretical privacy analysis of both centralized and decentralized FL frameworks by analyzing the information flow within the network. Our key contributions are summarized below:\n\u2022 Analytical privacy bounds of decentralized FL: We conduct an information-theoretical privacy analysis of both centralized and decentralized FL protocols, using mutual information as a key metric. To the best of our knowledge, this is the first information-theoretical privacy analysis in this context. Notably, we derive two privacy bounds for the optimization-based decentralized FL and show that its privacy loss is upper bounded by the loss of centralized FL (Theorem 1 in Section V). We further exemplify the derived privacy gap through two applications, including logistic regression and Deep Neural Networks (DNNs).\n\u2022 Empirical validation through privacy attacks: For DNN ap- plications, we show that in the case of optimization-based decentralized FL, gradient inversion attacks can be applied to reconstruct the original input data, but the reconstruction performance is degraded compared to centralized FL due to the limited amount of information available to the adversary. A similar trend is also observed when evaluated using membership inference attacks. Overall, decentralized FL employing distributed optimization is shown to be less vulnerable to privacy attacks compared to centralized FL, consistent with our theoretical findings. This finding challenges the previous belief that decentralized FL offers no privacy advantages compared to centralized FL [26] (see Section VIII-E for a detailed explanation)."}, {"title": "B. Outline and Notation", "content": "The paper is organized as follows. Section II reviews necessary fundamentals, and Section III introduces the in- volved metrics for quantifying privacy. Section IV introduces the optimization-based decentralized FL protocol. Section V analyzes the privacy of both centralized and decentralized FL protocols and states the main result. Section VI analyzes logistic regression example. Section VII, VIII analyze the application of DNNs. Conclusions are given in Section IX.\nWe use bold lowercase letters to denote vectors x and bold uppercase letters for matrices X. Calligraphic letters X denotes sets. The ith entry of a vector x is denoted $x_i$. The superscript $(\\cdot)^T$ denotes matrix transposition. I is used to denote the identity matrix of appropriate dimensions. 0 and 1 are the all-zero and all-one vectors. $\\nabla$ denotes the gradient. The value of the variable $x$ at iteration $t$ is denoted as $x^{(t)}$. We use $\\| \\cdot \\|$ to indicate the $l_2$-norm and $ran(\\cdot)$ and $ker(\\cdot)$ to denote the range and kernel of their argument, respectively. For the sake of notational simplicity, we represent random variables using capital letters, regardless of whether its outcome is a scalar, a vector, or a matrix. $M (\\boldsymbol{x}, i) := \\mathbb{1}\\{(\\boldsymbol{x}, l) \\in \\mathcal{D}\\}$ is the indicator variable showing whether $x$ is in the dataset $\\mathcal{D}$."}, {"title": "II. PRELIMINARIES", "content": "This section reviews the necessary fundamentals for the remainder of the paper.\nWithout loss of generality, we focus on classification problems involving $n$ nodes, each with its local dataset $\\{(x_{ik}, l_{ik}): k = 1,...,n_i\\}$, where $x_{ik} \\in \\mathbb{R}^v$ represents an input sample, $l_{ik} \\in \\mathbb{R}$ is the associated label and $n_i$ is the number of input samples. The dimension $v$ of the data samples is application-dependent. Collecting the $x_{ik}$s and $l_{ik}$s, we define $x_i = (x_{i1},...,x_{in_i})^T$, and $l_i = (l_{i1},..., l_{in_i})^T$. Let $f_i(w_i, (x_i, l_i))$ denote the cost function of node $i$ where $w_i\\in \\mathbb{R}^d$ is the model weight to be learned from the input dataset $(x_i, l_i)$, whose dimension, again, depends on the application. In the remainder of the paper we will omit the $(x_i, l_i)$ dependency for notational convenience when it is clear from the context and simply write $f_i(w_i)$. A typical centralized FL protocol works as follows:\n=0, the central server\n1) Initialization: at iteration $t$\nrandomly initializes the weights $w_i^{(0)}$ for each node.\n2) Local model training: at each iteration $t$, each user $i$ first receives the model updates from the server and then computes its local gradient, denoted as $\\nabla f_i(w_i^{(t)})$, using its local data $x_i$.\n3) Model aggregation: the server collects these local gradients and performs aggregation to update the global model. The"}, {"title": "A. Centralized FL", "content": "aggregation is often done by weighted averaging and typi- cally uniform weights are applied, i.e., $\\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(w_i^{(t)})$. Subsequently, each node $i$ then updates its own model weight by\n$w_i^{(t+1)} = w_i^{(t)} - \\mu \\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(w_i^{(t)})$, (1)\nwhere $\u03bc$ is a constant controlling the convergence rate. The last two steps are repeated until the global model converges or until a predetermined stopping criterion is reached.\nThis algorithm is often referred to as the FedAvg [1]."}, {"title": "B. Decentralized FL", "content": "Decentralized FL works for cases where a trusted centralized server is not available. In such cases, it works on a so-called distributed network which is often modeled as an undirected graph: $\\mathcal{G} = (\\mathcal{V},\\mathcal{E})$, with $\\mathcal{V} = \\{1, 2,..., n\\}$ representing the node set and $\\mathcal{E} \\subseteq \\mathcal{V} \\times \\mathcal{V}$ representing the edge set. $\\mathcal{N}_i = \\{j | (i, j) \\in \\mathcal{E}\\}$ denotes the set of neighboring nodes of node $i$. In this decentralized setup, each node $i$ can only communicate with its neighboring nodes $j \\in \\mathcal{N}_i$, facilitating peer-to-peer communication without any centralized coordination.\n1) Average consensus-based approaches: The model ag- gregation step requires all nodes' local gradients. Many decentralized FL protocols work by deploying distributed average consensus algorithms to compute the average of local gradients, i.e., computing $\\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(w_i^{(t)})$ in Eq. (1) without any centralized coordination. Example average consensus algorithms are gossip [27] and linear iterations [28], which allow peer-to-peer communication over distributed networks.\nThe common decentralized FL often works similarly to the FedAvg algorithm, except for the step of model aggregation. For instance, D-PSGD [4], [29] uses gossip averaging with neighbors to implement the aggregation, i.e.,\n$\\boldsymbol{w}_i^{(t+1)} = \\boldsymbol{w}_i^{(t)} - \\frac{\\mu}{d_i} \\sum_{j \\in \\mathcal{N}_i} f_j(\\boldsymbol{w}_i^{(t)}),$ (2)\nwhere $d_i = |\\mathcal{N}_i|$ is the degree of node i.\n2) Distributed optimization-based approaches: The goal of optimization-based decentralized FL is to collaboratively learn a global model, given the local datasets $\\{(x_i, l_i) : i \\in \\mathcal{V}\\}$, without any centralized coordination. The underlying problem can be posed as a constrained optimization problem given by\n$\\underset{\\{\\boldsymbol{w}_i: i \\in \\mathcal{V}\\}}{\\text{min}} \\sum_{i \\in \\mathcal{V}} f_i(\\boldsymbol{w}_i),$ (3)\nsubject to $(i, j) \\in \\mathcal{E} : B_{i|j}\\boldsymbol{w}_i + B_{j|i}\\boldsymbol{w}_j = 0,$\nwhere $B_{i|j}$ and $B_{j|i}$ define linear edge constraints. To ensure all nodes share the same model at convergence (consensus constraints) we have $B_{i|j} = -B_{j|i} = \\pm I$. In the following, we will use the convention that $B_{i|j} = I$ if $i < j$ and $B_{i|j} = -I$ otherwise.\nIn what follows we will refer to centralized FL as CFL. While decentralized FL encompasses both average consensus- based and distributed optimization-based approaches (recall Section II-B), for simplicity, we will use the abbreviation DFL to specifically refer to the optimization-based decentralized FL as it is our main focus. We will differentiate between the"}, {"title": "C. Threat models", "content": "We consider two types of adversary models: the eaves- dropping and the passive (also known as honest-but-curious) adversary model. While eavesdropping can typically be ad- dressed through channel encryption [30], it remains a pertinent concern in our context. This relevance stems from the nature of iterative algorithms, where communication channels are utilized repeatedly, continuously encrypting each and every message incurs high communication overhead. Therefore, in our framework, we assume that network communication generally occurs over non-secure channels, except for the initial network setup phase, details of which will be discussed later (see Section IV). The passive adversary consists of a number of colluding nodes, referred to as corrupt nodes, which comply with the algorithm instructions but utilize the received information to infer the private input data of the other so-called honest nodes. Consequently, the adversary has access to the following information: (a) all information gathered by the corrupt nodes, and (b) all messages transmitted over unsecured (i.e., non- encrypted) channels."}, {"title": "III. PRIVACY EVALUATION", "content": "When quantifying privacy, there are mainly two types of metrics: 1) empirical evaluation which assesses the susceptibil- ity of the protocol against established privacy attacks, and 2) information-theoretical metrics which offer a robust theoretical framework independent of empirical attacks. In this paper, we first evaluate privacy via an information-theoretical metric and then deploy empirical attacks to validate our theoretical results.\nAmong the information-theoretical metrics, popular ones include for example 1) $\\epsilon$-differential privacy [31], [32] which guarantees that the posterior guess of the adversary relating to the private data is only slightly better (quantified by $\\epsilon$) than the prior guess; 2) mutual information [33] which quantifies statistically how much information about the private data is revealed given the adversary's knowledge, In this paper, we choose mutual information as the information-theoretical privacy metric. The main reasons are the following. Mutual information has been proven effective in measuring privacy losses in distributed settings [34], and has been applied in various applications [35], [36], [37], [38], [39]. Secondly, mutual information is intrinsically linked to $\\epsilon$-differential privacy (see [40] for more details) and is more feasible to realize in practice [41], [42]."}, {"title": "A. Information-theoretical privacy metric", "content": "1) Fundamentals of mutual information: Given two (dis- crete) random variables X and Y, the mutual information $I(X; Y)$ between X and Y is defined as\n$I(X; Y) = H(X) \u2013 H(X|Y),$ (4)\nwhere $H(X)$ represents the Shannon entropy of X and $H(X|Y)$ is the conditional Shannon entropy, assuming they exist. It follows that $I(X; Y) = 0$ when X and Y are independent, indicating that Y carries no information about X. Conversely, $I(X; Y)$ is maximal when Y and X share a one-to-one correspondence.\nDenote $V_h$ and $V_c$ as the set of honest and corrupt nodes, respectively. Let $\\mathcal{O}$ denote the set of information obtained by the adversary. Hence, the privacy loss, measured by the mutual information between the private data $x_i$ of honest node $i \\in V_h$ and the knowledge available to the adversary, is given by\n$I(X_i; \\mathcal{O}).$ (5)"}, {"title": "B. Empirical evaluation via privacy attacks", "content": "To complement our theoretical analysis, we incorporate empirical privacy attacks to validate our findings. In machine learning, based on the nature of the disclosed private data, privacy breaches typically fall into three types: membership inference [43], [44], [45], the property inference [44], [46] and the input reconstruction attack [47], [48], [49], [50], where the revealed information is membership (whether a particular data sample belongs to the training dataset or not), properties of the input such as age and gender, and the input training data itself, respectively. Given that Eq. (5) measures how much information about the input training data is revealed, we align our empirical evaluation by mainly focusing on input reconstruction attacks. In FL, the gradient inversion attack has been extensively studied for its effectiveness in reconstructing input samples (see Section VIII-C for extended results using membership inference attacks).\n1) Gradient inversion attack: The gradient inversion attack typically works by iteratively refining an estimate of the private input data to align with the observed gradients generated by such data. For each node's local dataset $(x_i, l_i)$, the goal of the adversary is to recover the input data $(x_i^*, l_i^*)$ based on the observed gradient. A typical setup is given by [13]:\n$(x_i^*, l_i^*) = \\underset{x_i', l_i'}{\\text{arg min}} ||\\nabla f_i(w_i, (x_i', l_i')) - \\nabla f_i(w_i, (x_i, l_i)) ||^2,$ (6)\nand many variants thereof are proposed [14], [15], [16], [17], [18], [19], [20], [21], [22]. To evaluate the quality of reconstructed inputs, we use the widely adopted structural similarity index measure (SSIM) [51] to measure the similarity between the reconstructed images and true inputs. The SSIM index ranges from -1 to 1, where $\\pm 1$ signifies perfect resemblance and 0 indicates no correlation.\nAnalytical label recovery via local gradient: While it appears that both the input data $x_i$ and its label $l_i$ in Eq. (6) require reconstruction through optimization, existing work normally assumes that the label is already known. This is because the label can often be analytically inferred from the shared gradients [14], [45]. The main reason is as follows. Consider a classification task where the neural network has L layers and is trained with cross-entropy loss. Assume $n_i = 1$ for simplicity (one data sample at each node). Let $\\boldsymbol{y} = (Y_1,\\ldots, Y_C)$ denote the outputs (logits), where $y_i$ is the score (confidence) predicted for the $i$th class. With this, the cross-entropy loss over one-hot labels is given by\n$f_i(w_i) = - \\log \\left(\\frac{e^{y_{l_i}}}{\\sum_{j \\in \\mathcal{Y}} e^{y_j}}\\right) = -y_{l_i} + \\log \\left(\\sum_{j \\in \\mathcal{Y}} e^{y_j}\\right)$ (7)\nwhere $\\log(\\cdot)$ denotes the natural logarithm. Let $w_{i,L,c}$ denote the weights in the output layer $L$ corresponding to output $y_c$. The gradient of $f_i(w_i)$ with respect to $w_{i,L,c}$ can then be expressed as [14]:\n$\\begin{aligned} \\frac{\\partial f_i(w_{i,L,c})}{\\partial w_{i, L, c}} &= \\frac{\\partial f_i(w_i)}{\\partial y_c} \\frac{\\partial y_c}{\\partial w_{i, L, c}} \\\\ &= (g_c a_{L-1}), \\end{aligned}$ (8)\nwhere $a_{L-1}$ is the activation at layer $L - 1$ and $g_c$ is the gradient of the cross entropy Eq. (7) with respect to logit $c$:\n$g_c = \\frac{e^{y_c}}{\\sum_{j \\in \\mathcal{Y}} e^{y_j}} - \\delta_{c,l_i}$ (9)\nwhere $\\delta_{c, l_i}$ is the Kronecker-delta, defined as $\\delta_{c,l_i} = 1$ when $c = l_i$ and $\\delta_{c,l_i} = 0$ otherwise. Consequently, $g_c < 0$ for $c = l_i$ and $g_c > 0$ otherwise. Since the activation $a_{L-1}$ is independent of the class index $c$, the ground-truth label $l_i$ can be inferred from the shared gradients since $\\nabla' f_i(w_{i,L,l_i}) \\nabla' f_i(w_{i,L,c}) = g_{l_i}g_c ||a_{L-1}||^2 < 0$ for $c \\neq l_i$ and positive only for $c = l_i$. When dealing with $n_i > 1$, recovering labels becomes more challenging, yet feasible approaches are available. One example approach shown in [45] leverages the fact that the gradient magnitude is proportional to the label frequency in untrained models. Hence, in the centralized FL case, label information can often be deduced from the shared gradients thus improving both the efficiency and accuracy of the reconstructed input $x^*$ when compared to the real private input $x_i$ [14]. While for the decentralized FL protocol, we will show that the label information cannot be analytically computed for certain cases, thereby inevitably decreasing both the efficiency and reconstruction quality (see details in Remark 4)."}, {"title": "IV. DFL USING DISTRIBUTED OPTIMIZERS", "content": "This section introduces distributed solvers considered in this work, explains pivotal convergence properties relevant to sub- sequent privacy analyses, and gives details of the decentralized protocol using distributed optimization techniques."}, {"title": "A. Distributed optimizers", "content": "Given the optimization problem Eq. (3), many distributed op- timizers, notably ADMM [52] and PDMM [10], [11] have been proposed. From a monotone operator theory perspective [53], [11], ADMM can be seen as a $\\frac{1}{\\rho}$-averaged version of PDMM, allowing both to be analyzed within the same theoretical framework. Due to the averaging, ADMM is generally slower than PDMM, assuming it converges. Both ADMM and PDMM solve the optimization problem Eq. (3) iteratively, with the update equations for node $i$ given by:\n$w_i^{(t)} = \\underset{w_i}{\\text{arg min}} \\left(f_i(w_i) + \\sum_{j \\in \\mathcal{N}_i} \\left<z_{i|j}, B_{i|j} w_i\\right> + \\frac{\\rho d_i}{2} \\left\\|w_i\\right\\|^2\\right),$ (10)\n$z_{j|i}^{(t+1)} = (1 - \\theta)z_{j|i}^{(t)} + \\theta(\\rho B_{j|i}w_i^{(t)} + 2\\rho B_{i|j}w_i^{(t)}),$ (11)\nwhere $\u03c1$ is a constant controlling the rate of convergence. The parameter $\u03b8 \u2208 (0, 1]$ controls the operator averaging with $\u03b8 = \\frac{1}{\\rho}$ (Peaceman-Rachford splitting) yielding ADMM and $\u03b8 = 1$ (Douglas-Rachford splitting) leading to PDMM. $z$ is called auxiliary variable having entries indicated by $z_{i|j}$ and $z_{j|i}$, held by node $i$ and $j$, respectively, related to edge $(i, j) \u2208 E$. Eq. (10) updates the local variables (weights) $w_i$, whereas Eq. (11) represents the exchange of data in the network through the auxiliary variables $z_{j|i}$."}, {"title": "B. Differential A/PDMM", "content": "The optimality condition for Eq. (10) is given by\u00b2\n$0 = \\nabla f_i(\\boldsymbol{w}_i^{(t)}) + \\sum_{j \\in \\mathcal{N}_i} B_{i|j}z_{j|i}^{(t)} + \\rho d_i \\boldsymbol{w}_i^{(t)}.$ (12)\nGiven that the adversary can eavesdrop all communication channels, by inspection of Eq. (12), transmitting the auxiliary variables $z_{j|i}^{(t)}$ would expose $\\nabla f_i(\\boldsymbol{w}_i^{(t)})$, as $\\boldsymbol{w}_i^{(t)}$ can be deter- mined from Eq. (11). Encrypting $z_{j|i}^{(t)}$ at every iteration would address this, albeit at prohibitive computational expenses. To circumvent this, only initial values $z_{j|i}^{(0)}$ are securely transmitted and $\\Delta z_{j|i}^{(t+1)} = z_{j|i}^{(t+1)} - z_{j|i}^{(t)}$ being unencrypted in subsequent iterations [54], [55]. Consequently, upon receiving $\\Delta z_{j|i}^{(t+1)}$, $z_{j|i}^{(t+1)}$ is reconstructed as\n$z_{j|i}^{(t+1)} = \\Delta z_{j|i}^{(t+1)} + z_{j|i}^{(t)} = \\sum_{\\tau=1}^{t+1}\\Delta z_{j|i}^{(\\tau)} + z_{j|i}^{(0)}.$ (13)\nLet $t_{\\text{max}}$ denote the maximum number of iteration and denote $\\mathcal{T} = \\{0,1,..., t_{\\text{max}}\\}$. Hence, eavesdropping only uncovers\n$\\Delta z_{j|i}^{(\\tau+1)} : (i, j) \\in \\mathcal{E}, t \\in \\mathcal{T}\\},$ (14)\nand $z_{j|i}^{(t+1)}$ remains undisclosed unless the initialized $z_{j|i}^{(0)}$ is known."}, {"title": "C. DFL using differential A/PDMM", "content": "ADMM is guaranteed to converge to the optimal solution for arbitrary convex, closed and proper (CCP) objective functions $f_i$, whereas PDMM will converge in the case of differentiable and strongly convex functions [11]. Recently, it has been shown that these solvers are also effective when applied to non-convex problems like training DNNs [12]. Note that for complex non-linear applications such as training DNNs, although exact solutions of Eq. (10) are usually unavailable, convergence analysis of approximated solutions has been extensively investigated. For instance, it is shown in [12] that PDMM, using quadratic approximations, achieves good performance for non-convex tasks such as training DNNs. Moreover, convergence guarantees with quantized variable transmissions are investigated in [56].\nDetails of DFL using differential A/PDMM solvers are summarized in Algorithm 1. Note that at the initialization it requires that each node randomly initialize $z_{i|j}$ from independent distributions having variance $\u03c3^2_z$ and sends it to neighbor $j \u2208 \\mathcal{N}_i$ via secure channels.\n\u00b2Note that ADMM can also be applied to non-differentiable problems where the optimality condition can be expressed in terms of subdifferentials: $0 \u2208 \u2202f_i(\\boldsymbol{w}_i^{(t)}) + \\sum_{\\mathcal{N}_i} B_{i|j}z_{j|i}^{(t)} + \u03c1d_i\\boldsymbol{w}_i^{(t)}.$"}, {"title": "V. PRIVACY ANALYSIS", "content": "In this section, we conduct the comparative analysis of privacy loss in both CFL and DFL protocols, specifically focusing on the FedAvg algorithm and the decentralized approach introduced in Algorithm 1. For simplicity, we will primarily consider the case \u03b8 = 1, i.e., PDMM, but the results can be readily extended to arbitrary $\u03b8 \u2208 (0, 1]$."}, {"title": "A. Privacy loss of CFL", "content": "In CFL, the transmitted messages include the initial model weights $w_i^{(0)}$, and the local gradients $\\nabla f_j(w_j^{(t)})$ at all iterations $t \u2208 T$ of all nodes $j \u2208 V$. Hence, by inspection of Eq. (1), we conclude that knowledge of local gradients and initial weights $w_i^{(0)}$ is sufficient to compute all updated model weights $w_i^{(t)}$ at every $t \u2208 T$. Hence, the eavesdropping adversary has the following knowledge\n$\\mathcal{O}_{CFL} = \\{\\nabla f_j(\\boldsymbol{w}_j^{(t)}), \\boldsymbol{w}_i^{(t)}\\}_{j\\in \\mathcal{V}, t\\in \\mathcal{T}}.$ (16)\nThe passive adversary, on the other hand, has the following knowledge\n$\\{x_j, \\boldsymbol{w}_i^{(0)}, \\nabla f_j(\\boldsymbol{w}_j^{(t)})\\}_{j\\in \\mathcal{V}_c, t\\in \\mathcal{T}}.$ (17)\nCombining both sets, the privacy loss, quantified by the mutual information between the private data $x_i$ and the knowledge available to the adversary (as in Eq. (5)), is given by\n$\\begin{aligned} I(X_i; \\mathcal{O}_{CFL}) = I(X_i; \\{x_j\\}_{j\\in \\mathcal{V}_c}, \\{\\nabla f_j(\\boldsymbol{w}_j^{(t)}), \\boldsymbol{w}_i^{(t)}\\}_{j\\in \\mathcal{V}, t\\in \\mathcal{T}}). \\end{aligned}$ (18)\nRemark 1. We could securely transmit the initialized model weights $w_i^{(0)}$ in CFL, analogous to the initial auxiliary variable $z_{j|i}^{(0)}$ in DFL. However, such secure transmission would not reduce the privacy loss in Eq. (18). The main reason is that at convergence all local models will be identical, i.e., $\\boldsymbol{w}_j^{(t_{\\text{max}})} = \\boldsymbol{w}_k^{(t_{\\text{max}})}$ for $(j, k) \u2208 E$. Thus, as long as there is one corrupt node, the passive adversary has knowledge of all $\\boldsymbol{w}_i^{(t_{\\text{max}})}$s. By inspecting Eq. (1) we can see that the difference $\\boldsymbol{w}_i^{(t+1)} - \\boldsymbol{w}_i^{(t)}$ at every iteration is known, and thus $\\boldsymbol{w}_j^{(t)}$ for all $j \u2208 V$."}, {"title": "B. Privacy loss of DFL", "content": "By inspection of Algorithm 1, the eavesdropping adversary can intercept all messages transmitted along non-secure chan- nels, thus having access to:\n$\\left\\{\\Delta z_{j\\|k}^{(\\tau+1)}\\right\\}_{(\\mathcal{j},k)\\in E,\\tau\\in T}.$ (19)\nFor any edge in the network, the transmitted information will be known by the passive adversary as long as one end node is corrupt. Accordingly, we define $E_h = \\{(j, k) \u2208 V_h \u00d7 V_h\\}$, $E_c = E\\E_h$ as the set of honest and corrupt edges, respectively. Given that the passive adversary can collect all information obtained by the corrupt nodes, by inspecting Algorithm 1 it thus has the knowledge of $\\{x_j\\}_{j\u2208V} \\cup \\{z_{j|k}^{(0)}, \\Delta z_{j\\|k}^{(\\tau+1)}\\}_{\\mathcal{j}\\|k}(\\mathcal{j},k)\u2208E_c,\\tau\u2208T$. Combining this with the eavesdropping knowledge in Eq. (19) we conclude that the adversary has the following knowledge:\n$\\{x_j\\}_{j\u2208V} \\cup \\{z_{j\\|k}^{(0)}\\}_{\\mathcal{j}\\|k}(\\mathcal{j},k)\u2208E_c \\cup \\{\\Delta z_{j\\|k}^{(\\tau+1)}\\}_{\\mathcal{j}\\|k}(\\mathcal{j},k)\u2208E,\\tau\u2208T$.\nThe information loss of an honest node $i \u2208 V_h$'s private data is thus given by\n$\\begin{aligned} I(X_i; \\mathcal{O}_{DFL}) = I(X_i; \\{x_j\\}_{j\\in \\mathcal{V}_c}, \\{z_{j\\|k}^{(0)}\\}_{\\mathcal{j}\\|k}(\\mathcal{j},k)\u2208E_c, \\{\\Delta z_{j\\|k}^{(\\tau+1)}\\}_{\\mathcal{j}\\|k}(\\mathcal{j},k)\u2208E,\\tau\u2208T), \\end{aligned}$ (20)"}, {"title": "Proposition 1", "content": "Let $\\mathcal{G"}, "h = (V_h,E_h)$ be the subgraph of $\\mathcal{G}$ after eliminating all corrupt nodes. Let $\\mathcal{G}_{h,1},..., \\mathcal{G}_{h,k_h}$ denote the components of $\\mathcal{G}_h$ and let $V_{h,k}$ be the vertex set of $\\mathcal{G}_{h,k}$. Without loss of generality, assume the honest nodes $i$ belong to the first honest component, i.e., $i \u2208 V_{h,1}$. The adversary has the following knowledge about node $i \u2208 V_{h,1}$:\ni) Noisy local gradients:\n\u2200t \u2208 T: \u2207f_i(w_i^{(t)}) + \\sum_{k \u2208 N_{j,h}} B_{j|k}z_{j|k}^{(0)}, (21)\nii) Difference of local gradients:\n\u2200t \u2208 T: \\nabla f_i(w_i^{(t+1)}) - \\nabla f_i(w_i^{(t)}), (22)\niii) The aggregated sum of local gradients in honest compo- nent $\\mathcal{G"]}