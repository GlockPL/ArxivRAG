{"title": "STACKFEED: STRUCTURED TEXTUAL ACTOR-CRITIC KNOWLEDGE BASE EDITING WITH FEEDBACK", "authors": ["Naman Gupta", "Shashank Kirtania", "Priyanshu Gupta", "Krishna Kariya", "Sumit Gulwani", "Arun Iyer", "Suresh Parthasarathy", "Arjun Radhakrishna", "Sriram K. Rajamani", "Gustavo Soares"], "abstract": "Large Language Models (LLMs) often generate incorrect or outdated information, especially in low-resource settings or when dealing with private data. To address this, Retrieval-Augmented Generation (RAG) uses external knowledge bases (KBs), but these can also suffer from inaccuracies. We introduce STACKFEED, a novel Structured Textual Actor-Critic Knowledge base editing with FEEDback approach that iteratively refines the KB based on expert feedback using a multi-actor, centralized critic reinforcement learning framework. Each document is assigned to an actor, modeled as a ReACT agent, which performs structured edits based on document-specific targeted instructions from a centralized critic. Experimental results show that STACKFEED significantly improves KB quality and RAG system performance, enhancing accuracy by up to 8% over baselines.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) often produce incorrect or outdated information, particularly in low-resource settings or when handling private data. Even if the information provided is accurate, LLMs can generate hallucinated or imaginary content alongside it (Maynez et al., 2020; Zhou et al., 2021). A promising solution to address these issues is the integration of retrieval components that extract relevant information from external knowledge sources, known as Retrieval-Augmented Generation (RAG) (Chen et al., 2017; Khandelwal et al., 2020; Guu et al., 2020; Izacard et al., 2022; Shi et al., 2023). For clarity, we will refer to these external knowledge sources as Knowledge Bases (KBs). However, KBs themselves can suffer from inaccuracies, incompleteness, or outdated content. To address these challenges, there is growing interest in Knowledge Editing (KE) techniques to enhance LLMs with up-to-date and accurate knowledge.\nAdvancements in KE have focused on updating the model's parameters (De Cao et al., 2021a; Meng et al., 2022; 2023), adding new parameters to model (Huang et al., 2023; Yu et al., 2024), and holding additional memory (Madaan et al., 2022; Wang et al., 2024a;b). Contrary to approaches that either update model parameters or add new parameters that require white-box access to LLMs, memory-based approaches can work with black-box access to LLMs. In similar line of thought, recently, KE approaches have also focused on refining the KBs themselves (Li et al., 2024). For example, the method proposed in Li et al. (2024) continuously updates KBs with new information, such as the current identity of the British Prime Minister. This approach demonstrates that directly editing the KB is more effective than simply adding new documents, which may coexist with outdated or inaccurate ones. Removing older documents is often not feasible, as only certain sections may be incorrect, while other parts could still provide valuable information for different queries. However, in applications like chatbots or code generation using API documentation, where updated information might not be readily available in document form, expert intervention can be crucial (Ramjee et al.,"}, {"title": "RELATED WORK", "content": "The STACKFEED framework addresses a key limitation of current RAG systems: the inability to dynamically update Knowledge Bases (KBs) without retraining or altering model parameters. Our work draws from research in Retrieval-Augmented Generation (RAG), Continual Learning, Model Editing, and feedback-driven prompt optimization, incorporating insights from Multi-Agent Reinforcement Learning (MARL) to propose an effective solution for KB editing.\nRetrieval Augmented Generation (RAG): RAG systems enhance LMs by retrieving relevant knowledge from a KB based on the input query and appending it to the context, thereby addressing the limitations of standalone LMs that lack sufficient context and produce inaccurate answers (Chen et al., 2017; Khandelwal et al., 2020; Guu et al., 2020; Izacard et al., 2022; Shi et al., 2023). These systems dynamically construct contexts from unstructured KBs without modifying the LM's internal parameters. STACKFEED further enhances RAG systems by refining the KB itself based on feedback, ensuring more accurate and up-to-date information.\nContinual Learning: Continual Learning (CL) methods address the challenge of updating LMs in non-stationary environments by ensuring that new information is learned without forgetting previously acquired knowledge (Jin et al., 2022; Xu et al., 2023; Padmanabhan et al., 2023; Aky\u00fcrek et al., 2024). These methods are often computationally intensive and require large-scale retraining, making them less suitable for scenarios requiring frequent updates or minimal computational resources. STACKFEED, by contrast, leverages expert feedback to perform direct edits to the KB, avoiding the need for extensive retraining."}, {"title": "EXAMPLE AND OVERVIEW", "content": "Figure 1 illustrates our technique applied to the ARKS Pony domain (Su et al., 2024a), where a knowledge base (KB) for the low-resource programming language Pony supports a natural language-to-code task. Due to Pony's rarity, language models often generate code that fails to compile. To address this, we use the Pony compiler as an expert to provide feedback in the form of compile errors.\nEvaluating the Knowledge Base State: We start with an initial KB, including documents like builtin-array.md. The system retrieves relevant documents based on the given task (e.g., counting non-inversions in an array) and generates a program, which is evaluated by the compiler, resulting in feedback (e.g., compile errors).\n\u2461Centralized Feedback Analysis: We analyze compile errors to generate reflections that explain why the errors occurred. For instance, if the apply method in the Array class is partial and may raise an error, the reflection suggests adding a ? to handle potential failures. These reflections are matched to the documents they pertain to, refining the understanding of errors.\n\u2462Distributing Gradients: Reflections are generalized into gradients, which summarize modifications needed for each document. For example, the theme might be the partial nature of functions like apply and update, which need better error handling in the documentation.\n\u2463 Generating Edit Actions: Gradients are converted into structured edit actions, such as adding or modifying content in specific sections of the documents.\n\u2464 Re-evaluation and MCTS Search: After edits are applied, the KB is re-evaluated, generating new feedback and a reward score. This score guides a Monte Carlo Tree Search (MCTS) to explore"}, {"title": "METHODOLOGY", "content": "We will start by describing a typical Retrieval-Augmented Generation (RAG) system over unstructured Knowledge Bases.\nErrors in such systems can arise from multiple components: 1) the LLM B might fail to reason correctly over the provided information, 2) the retriever R might not select the right set of relevant documents from K, or 3) the knowledge base K itself might contain incorrect or incomplete information. We assume an expert is monitoring the system, identifying when answers are incorrect, determining which component is at fault, and providing feedback on why the answer is incorrect and what the correct answer must be.\nThis work focuses on scenarios where incorrect answers result from issues in the Knowledge Base (K). Our goal is to improve K by addressing mistakes in K and filling in missing information based on expert feedback, thus enhancing the RAG system's performance on future queries."}, {"title": "PROBLEM FORMULATION", "content": "We are provided with a training set $T = \\{(q_i, o_i, c_i, f_i)\\}_{i=1}^n$, where $q_i$ is a user query, $o_i$ is the RAG system's answer, $c_i$ is the correct answer, and $f_i$ is an optional expert feedback on incorrect answers. We also assume access to a scoring function $g$, which compare $o_i$ and $c_i$ to output a score. The objective is to optimize the knowledge base $K$ to maximize the sum of the scores for all queries in the training set:\n$K^* = \\underset{K}{\\text{arg max }} \\frac{1}{|T|} \\sum_{(q_i, a_i, c_i, f_i) \\in T} g(B(q_i, \\Gamma(q_i, K)), c_i)$"}, {"title": "KNOWLEDGE BASE EDITING AS STATE SEARCH", "content": "In our problem setting, the Knowledge Base (K) is defined as a collection of documents $K = \\{D_i\\}_{i=1}^n$. We assume each document consists of a number of chunks of text and can be represented as $D_i = [C_{ij}]$. The state $s \\in S$ of the system is represented by the current configuration of the KB, i.e., the content of all documents in K.\nGiven a query $q_i$ and a set of retrieved documents $\\Gamma(q_i, K)$, the LLM B generates an answer $o_i$. When errors arise due to incomplete or incorrect information in the retrieved documents, our goal is to identify the optimal configuration of K that improves the accuracy of the system's responses. Thus, we define our state search problem as finding the best state $s^*$ of the KB.\nState Space: The state space S encompasses all possible configurations of the KB. Each state s corresponds to a particular set of document contents, represented as: $s = \\{D_i\\}_{i=1}^n$, where $D_i$ denotes the content of document i and n is the number of documents in K. The state s captures the overall structure and content of the KB at any given point. We set $s_0 = K$.\nState Transition Function: The state transition function $T(s, u)$ defines how the KB changes in response to the action u taken by the agent. Each action contains modifications to one or more documents within the KB, resulting in a new KB configuration. The state transition is formalized as: $s' = T(s, u)$, where s' is the new state of the KB after applying u.\nAction Space: The action space A consists of list of diffs $d_i$ corresponding to each document $D_i$. Essentially, $u = [d_i]_{i=1}^K$.\nEnvironment: We model the environment simply as a \u201cpatch\u201d function, that takes the diff generated by the agent and patches the KB to produce the new state.\nOptimization Objective: Following Equation 1, our objective then is to find the optimal state $s^*$ of the KB that maximizes the overall performance of the RAG system, as measured by a global reward function R. The optimization problem is formulated as:\n$S^* = \\underset{SES}{\\text{arg max }} R(s) = \\underset{SES}{\\text{arg max }} \\frac{1}{|T|} \\sum_{(q_i,a_i,c_i, f_i) \\in T} g(B(q_i, \\Gamma(q_i, S)), c_i)$", "latex": ["K^* = \\underset{K}{\\text{arg max }} \\frac{1}{|T|} \\sum_{(q_i, a_i, c_i, f_i) \\in T} g(B(q_i, \\Gamma(q_i, K)), c_i)", "S^* = \\underset{SES}{\\text{arg max }} R(s) = \\underset{SES}{\\text{arg max }} \\frac{1}{|T|} \\sum_{(q_i,a_i,c_i, f_i) \\in T} g(B(q_i, \\Gamma(q_i, S)), c_i)"]}, {"title": "STACKFEED", "content": "The proposed approach STACKFEED is designed to enhance a RAG system by refining the underlying Knowledge Base (K) using expert feedback. Our approach employs a multi-actor, centralized critic architecture, where each actor is responsible for making updates to a specific document within K, and a centralized critic uses global feedback to coordinate these updates. The objective is to iteratively improve K such that the overall accuracy of the RAG system is maximized."}, {"title": "REWARD SIGNAL", "content": "For a given query $q_i$ and the generated answer $o_i$, the expert provides feedback $(c_i, f_i)$ that includesa ground truth answer $c_i$ and qualitative expert feedback $f_i$ on any errors. The global reward signal is derived from $c_i$ as per the scoring function s (Refer Equation 2)."}, {"title": "KB EDITING AGENT", "content": "To effectively incorporate expert feedback, we employ a multi-actor, centralized critic architecture."}, {"title": "COMPLETENESS AND GENERALIZATION", "content": "We observe consistent improvements over the PROMPTAGENT-E baseline in completeness and generalizability scores, with STACKFEED achieving approximately 2x performance gains on Ring and Pony datasets. However, feedback incorporation remains limited, likely due to suboptimal retrieval or limited document-query associations hindering learnability of edit. STACKFEED also demonstrates higher generalizability and lower variance, attributed to its structured and focused document edits that enhance coherence."}, {"title": "STACKFEED MAKES HIGH QUALITY COHERENT EDITS", "content": "As seen in Table 3, STACKFEED produces edits with a coherence score of 4 or higher for most datasets. For KBs which need long term maintenance (like language and code documentation as seen in the ARKS datasets), STACKFEED makes more coherent edits compared to the baseline. This is especially true for long documents as seen in the ARKS Pony dataset. For news-article like dataset like CLARK-news with factual edits. Incoherency is naturally induced when the facts of the article are changed. For instance, an article on the coronation of a king will lose coherency when the article is updated to add information about the coronation of a new king."}, {"title": "CONCLUSION", "content": "We introduced STACKFEED, a novel framework for refining Knowledge Bases (KBs) in Retrieval-Augmented Generation (RAG) systems using a multi-actor, centralized critic architecture. STACKFEED enables efficient KB updates without retraining or altering model parameters by leveraging feedback-driven structured edits and textual gradients.\nOur approach achieved superior performance in preserving knowledge base (KB) coherence, consistency, and completeness, resulting in enhanced RAG system responses. Nonetheless, there remains considerable potential for further advancements. Future work will focus on refining these three metrics to elevate system performance even further."}, {"title": "APPENDIX", "content": ""}, {"title": "PROMPTS USED IN STACKFEED", "content": "\"\"\" There exists a Language Model based software named CodeRAG that automatically does the following task for a developer: task task_desc\nCodeRAG uses a knowledge base to perform this task: kb_desc\nA developer used CodeRAG to perform the task on multiple files, and CodeRAG made some errors on them.\nHere is one knowledge base file that was involved in these errors: \"\"\" for i, file in\nenumerate (kb_files): prompt += f\"\"\" File i+1: id: file ['id'] content:\nn<file>\nnfile['content']\nn</file>\nn\"\"\" if \\\"special_notes\\\" in file and file [\\\"special_notes\\\"] != \\\"\\\": prompt += f\"\"\"\nnspecial_notes: file ['special_notes']\"\"\"\n\"\"\" The following are the reflections on the errors made by CodeRAG: reflections_str\nThe reflections show the relationship of the file with the errors made by CodeRAG. If the file is named\n\\\"None,\\\" it means the information about the error on which the reflection is based does not fully fit any\nknowledge base file.\nYour task is to use the reflections on the errors made by CodeRAG and provide a generalization on the\nissues with the file and how it can be improved to prevent the errors.\nYou should mention common issues found in the reflections and provide a plan for improving the knowledge\nbase files to prevent future errors. Use the reflections to suggest additions or changes in the file,\nexplaining what new content should be added to prevent errors. Before suggesting your plan, give context\non the errors using code snippets and other relevant information from the reflections.\nYou have a scratchpad to reason and plan your generalization. Your scratchpad is for your use only and\nwill not be shared with anyone else. The scratchpad is represented by the <scratchpad></scratchpad>\ntags.\nYour generalization should follow this format: <scratchpad> The contents of the scratchpad </scratchpad>\n Your generalization for this file </generalization>\nYou must provide the filled-out scratchpad and generalization in the above format.\nGeneral guidelines: 1. Carefully analyze the reflections to understand the errors CodeRAG is making. 2.\n\\\"None\\\" is a special file, representing that to fix the error, the information should be in a new file.\""}]}