{"title": "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks", "authors": ["WENPENG XING", "MINGHAO LI", "MOHAN LI", "MENG HAN"], "abstract": "Embodied Al systems, including robots and autonomous vehicles, are increasingly integrated into real-world applications, where\nthey encounter a range of vulnerabilities stemming from both environmental and system-level factors. These vulnerabilities manifest\nthrough sensor spoofing, adversarial attacks, and failures in task and motion planning, posing significant challenges to robustness and\nsafety. Despite the growing body of research, existing reviews rarely focus specifically on the unique safety and security challenges of\nembodied AI systems. Most prior work either addresses general AI vulnerabilities or focuses on isolated aspects, lacking a dedicated\nand unified framework tailored to embodied AI. This survey fills this critical gap by: (1) categorizing vulnerabilities specific to\nembodied AI into exogenous (e.g., physical attacks, cybersecurity threats) and endogenous (e.g., sensor failures, software flaws)\norigins; (2) systematically analyzing adversarial attack paradigms unique to embodied AI, with a focus on their impact on perception,\ndecision-making, and embodied interaction; (3) investigating attack vectors targeting large vision-language models (LVLMs) and\nlarge language models (LLMs) within embodied systems, such as jailbreak attacks and instruction misinterpretation; (4) evaluating\nrobustness challenges in algorithms for embodied perception, decision-making, and task planning; and (5) proposing targeted strategies\nto enhance the safety and reliability of embodied AI systems. By integrating these dimensions, we provide a comprehensive framework\nfor understanding the interplay between vulnerabilities and safety in embodied AI.\nAdditional Key Words and Phrases: large vision language models, large language models, embodied AI, adversarial attack.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid advancement of AI has established Embodied Al as a key technology in domains such as autonomous\ndriving, industrial automation, and smart home systems. By combining perception, decision-making, and actuation,\nthese systems excel in handling complex real-world tasks. However, as shown in Fig. 1, their reliance on intricate\ninteractions between sensors, actuators, and algorithms also exposes them to a broad spectrum of vulnerabilities,\nincluding dynamic and complex environments, jamming and spoofing attacks on sensors, and system failures. These\nrisks raise concerns over unauthorized actions and reputation damage, underscoring the critical need for robust security\nmeasures to ensure their safe and reliable deployment.\nWe identify three key characteristics of robust embodied systems [27]: autonomy, embodiment, and cognition.\nAutonomy refers to the system's capacity to make informed, independent decisions, enabling it to adapt to dynamic and\nunpredictable scenarios. However, this independence also introduces vulnerabilities, such as decision-making errors in\ncomplex environments. Embodiment denotes the ability to interact with physical environments, integrating physical\npresence with decision-making processes to achieve seamless interaction. Cognition encompasses the system's capacity\nfor understanding, reasoning, and interpreting its actions, ensuring that its behavior aligns with both internal goals and\nexternal constraints. Yet, cognitive processes can also be exploited through sensor-to-model attacks or manipulation of\nlearned models.\nThe remainder of this article is structured to systematically address the vulnerabilities, attack vectors, and mitigation\nstrategies in embodied AI systems, as illustrated in Fig. 2. Section 2 introduces a detailed taxonomy of vulnerabilities in\nembodied AI, categorized into exogenous, endogenous, and inter-dimensional risks, with further exploration of specific\nissues such as physical attacks (2.1.2), sensor validation failures (2.2.1), and ethical challenges in interactive agents (2.3.2).\nSection 3.1 examines the relationship between vulnerabilities and attack strategies, including an analysis of threat"}, {"title": "2 VULNERABILITY CATEGORY", "content": "The risks inherent to embodied Al systems can be broadly classified into three key dimensions: Exogenous Vulnera-\nbility (Section 2.1), which originates from external factors such as dynamic environments or adversarial attacks, and\nEndogenous Vulnerability (Section 2.2), which arise from internal system failures, including hardware malfunctions and\nsoftware vulnerabilities. Additionally, there exists a category of Inter-Dimensional vulnerability (Section 2.3), where\ninteractions between external and internal factors exacerbate system fragility."}, {"title": "2.1 Exogenous Vulnerability", "content": "Exogenous risks arise from the system's interaction with its environment or external malicious actors. In this\nsection, we categorize these risks into Dynamic Environmental Factors (Section 2.1.1), Physical Attacks (Section 2.1.2),\nAdversarial Attacks (Section 2.1.3), Cybersecurity Threats (Section 2.1.4), and Human Interaction and Safety Protocol\nFailures (Section 2.1.5)."}, {"title": "2.1.1 Dynamic Environmental Factors", "content": "Embodied Al systems, especially in dynamic environments, rely on sensor\ndata for perception and interaction [89]. However, environmental changes or adversarial perturbations can mislead\ndeep neural networks, leading to misclassifications with safety risks [241]. Sensor failures in autonomous vehicles,"}, {"title": "2.1.2 Physical Attacks", "content": "This attack strategy involves direct hardware tampering, enabling adversaries to manipulate\ncomponents, disrupt performance, or cause physical damage [62]. As cyber-physical systems (CPSs) integrate diverse\ntechnologies, ensuring robust security becomes increasingly complex [171]. Furthermore, the rise of Industry 4.0\nexpands the attack surface, exposing CPSs to novel threats with potential economic and physical consequences [130]."}, {"title": "2.1.3 Adversarial Attacks", "content": "Adversarial attacks present a significant challenge to embodied Al systems, such as au-\ntonomous vehicles and robots, by introducing imperceptible perturbations to input data, thereby deceiving deep\nlearning models responsible for real-time perception and decision-making. These attacks, which can be executed in both\nwhite-box and black-box settings, are particularly concerning in safety-critical applications like autonomous driving\nand robotic surgery, where erroneous decisions can result in severe consequences, including physical harm and system\nfailures [77, 202, 228]. Moreover, the physical embodiment of these systems makes them susceptible to adversarial\nexamples crafted for real-world environments, further amplifying the risks [65]. Although defense mechanisms such as\nadversarial training have shown promise, ensuring the robustness of embodied systems against such attacks remains\nan open problem [138]. For further details of adversarial attacks, we refer readers to Section 3."}, {"title": "2.1.4 Cybersecurity Threats", "content": "As embodied AI systems increasingly integrate with the Internet of Things (IoT) and\ncloud-based infrastructures, they become vulnerable to a wide range of cybersecurity attacks. These attacks can exploit\nweak security configurations, such as default passwords or unencrypted communications, to gain unauthorized access\nor disrupt system operations. A notable example is the Mirai botnet attack, which exploited weak passwords in IoT\ndevices to launch a large-scale Distributed Denial of Service (DDoS) attack [15]. Similarly, drones can be hijacked\nthrough GPS spoofing, leading to unauthorized or dangerous actions [13]."}, {"title": "2.1.5 Human Interaction and Safety Protocol Failures", "content": "Collaborative robots (cobots) and drones, designed to operate\nalongside humans, pose significant safety risks if their safety protocols are compromised. These risks are often\nexacerbated by human error or malicious intent. For instance, a malfunctioning cobot at a Volkswagen plant led to the\ndeath of a worker due to a failure in the robot's safety protocols [14]."}, {"title": "2.2 Endogenous Vulnerability", "content": "Endogenous risks originate within the system itself, including hardware failures, software bugs, and design flaws.\nThese risks are often more predictable but can still lead to severe consequences if not adequately mitigated."}, {"title": "2.2.1 Sensor and Input Validation Failures", "content": "Embodied Al systems rely on a variety of sensors to gather data about\ntheir environment. However, sensor malfunctions or improper input validation can lead to incorrect environmental\nassessments, resulting in unsafe actions. For example, in autonomous vehicles, sensor failures or misinterpretations of\nsensor data have been linked to accidents, as the system may fail to detect obstacles or misjudge distances [228]."}, {"title": "2.2.2 Hardware and Mechanical Failures", "content": "Mechanical components in embodied AI systems, particularly in industrial\nand medical robots, are prone to wear and failure over time. These failures can disrupt system operations and, in critical\napplications, lead to severe consequences. For instance, mechanical malfunctions in surgical robots, such as the Da\nVinci Surgical System, have resulted in emergency interventions and, in some cases, the need to convert to open surgery\n[104]. In industrial settings, hardware failures can lead to production delays or safety hazards."}, {"title": "2.2.3 Software Vulnerabilities and Design Flaws", "content": "Embodied AI systems are also susceptible to software bugs and design\nflaws that can compromise their safety and reliability. These vulnerabilities may arise from improper handling of edge\ncases, insufficient testing in diverse environments, or flaws in decision-making algorithms. For example, a poorly\ndesigned control algorithm in an autonomous vehicle could lead to unsafe driving behaviors in complex traffic scenarios.\nSimilarly, software vulnerabilities in industrial robots could be exploited to cause operational disruptions or safety\nincidents [229]."}, {"title": "2.3 Inter-Dimensional Vulnerability", "content": "Some risks may span both exogenous and endogenous dimensions, where external factors exacerbate internal\nvulnerabilities. For example, a cyber attack (exogenous) that exploits a software vulnerability (endogenous) could\nlead to unauthorized control of a robot or vehicle, resulting in physical damage or operational disruption. Similarly,\nenvironmental factors such as extreme temperatures or humidity could accelerate hardware degradation, leading to\nmechanical failures. The remainder of this section will discuss Instruction Misinterpretation (Section 2.3.1), Ethical and\nSafety Implications of Interactive Agents (Section 2.3.2), and the Lack of Robustness in Unseen Environments (Section\n2.3.3)."}, {"title": "2.3.1 Instruction Misinterpretation", "content": "One of the primary safety concerns in interactive tasks is the potential for mis-\ninterpretation of instructions. For example, in Talk2car [56] tasks, where agents control self-driving cars based on\nnatural language instructions, a misinterpretation could lead to dangerous driving behaviors or accidents. Similarly, in\nindoor navigation tasks like ALFRED [193], incorrect object manipulation or navigation could result in damage to the\nenvironment or harm to humans if deployed in real-world robotic systems [193]."}, {"title": "2.3.2 Ethical and Safety Implications of Interactive Agents", "content": "In tasks like EQA (Embodied Question Answering), where\nagents engage in dialogue with users, there is a risk of the agent providing incorrect or misleading information. This\ncould have serious consequences in critical applications such as healthcare or autonomous driving [48]. Ensuring that\nagents are transparent about their limitations and capable of handling uncertainty is crucial for maintaining safety and\ntrust in these systems."}, {"title": "2.3.3 Lack of Robustness in Unseen Environments", "content": "Many VLN tasks, such as Behavioral Robot Navigation, test agents\nin both seen and unseen environments [8]. However, agents often struggle to generalize to new, unseen environments,\nwhich could lead to failures in navigation or task completion. In real-world applications, this could result in agents\nbecoming stuck, causing delays, or even creating hazardous situations if the agent is unable to adapt to unexpected\nobstacles or changes in the environment."}, {"title": "3 ATTACK TO VULNERABILITIES", "content": "The advent of multimodal large models, particularly LVLMs derived from LLMs, has revolutionized embodied AI\nby enabling advanced capabilities in perception, reasoning, and interaction across vision and language tasks. These\nmodels leverage extensive pretraining, high capacity, and sophisticated alignment techniques to achieve state-of-the-art\nperformance. However, their multimodal nature-integrating both visual and textual modalities-significantly expands\nthe attack surface, introducing unique vulnerabilities. Adversaries can exploit visual inputs, textual prompts, or their\ninteractions to craft sophisticated attack vectors, exposing critical gaps in existing defenses.\nThis section systematically examines the vulnerabilities of these systems, beginning with a Vulnerability Analysis\n(Section 3.1) that highlights the expanded attack surface, adversarial vulnerabilities, and challenges arising from\ntransitions between text and actions. Next, the Threat Model (Section 3.2) outlines attacker capabilities and potential\ntargets, followed by an Attack Taxonomy (Section 3.3) that categorizes threats into three overarching types: exogenous,\nendogenous, and inter-dimensional vulnerability-centric attacks.\nSubsequent sections delve into specific attack methodologies, including Cybersecurity Threats (Section 3.4), Sensor\nSpoofing Attacks (Section 3.5), and Adversarial Attacks (Section 3.6)."}, {"title": "3.1 Vulnerability Analysis", "content": "The vulnerabilities of LLMs and LVLMs arise from fundamental limitations in their training paradigms, data coverage,\nand architectural properties. This section examines three key areas of concern: Training and Data Limitations in Section\n3.1.1, Adversarial Vulnerabilities in neural networks in Section 3.1.2, and Expanded Attack Surface in multimodal\nsystems in Section 3.1.3."}, {"title": "3.1.1 Training and Data Limitations", "content": "LLMs' autoregressive training paradigm focuses on next-word prediction, which\ndiverges from the goal of generating helpful, truthful, and harmless responses. Safety considerations are not inherently\nembedded, and fine-tuning with safety datasets offers limited mitigation. Pre-training on uncurated internet data [30]\nintroduces biases [59] and toxic content [226], reinforcing stereotypes or generating harmful outputs. These limitations\nin training paradigms and data exacerbate the model's vulnerability to adversarial manipulation and misuse."}, {"title": "3.1.2 Adversarial Vulnerabilities", "content": "Deep neural networks (DNNs), despite their nonlinear architecture, often exhibit\nnear-linear behavior in high-dimensional input spaces, making them susceptible to adversarial examples [32]. Small\ninput perturbations can cause significant prediction shifts, as demonstrated by attacks like FGSM and Carlini & Wagner\n(CW) [32]. Additionally, incomplete input space coverage during training [161] leads to blind spots, increasing overfitting\nand reducing generalization [155]. Near decision boundaries, steep gradients further amplify this vulnerability [162].\nDNNs are also highly sensitive to high-frequency components [202], creating a mismatch between human perception\nand model behavior that adversaries can exploit."}, {"title": "3.1.3 Expanded Attack Surface", "content": "LVLMs process multimodal inputs (e.g., text and vision), enhancing capabilities but\nintroducing new vulnerabilities. Adversarial signals in one modality can propagate across others, amplifying their impact\n[122]; for example, manipulated visuals can disrupt textual reasoning. The growing adoption of LVLMs, especially in\nembodied AI and sensor-driven systems, expands the attack surface. Zhang et al. [255] demonstrated that adversarial\nattacks on LVLMs pose significant risks in autonomous driving."}, {"title": "3.1.4 Transitions from Text to Actions", "content": "Traditional jailbreak attacks, which are designed to bypass LLM safety mecha-\nnisms, may not be fully applicable to embodied systems. Embodied LLMs must not only generate text but also plan and\nexecute actions in the physical world. This requires a new attack paradigm that takes into account the unique challenges\nof action planning and execution in embodied systems. For example, embodied LLMs often generate structured outputs,\nsuch as JSON or YAML, which are then used by downstream control modules to execute actions [74, 169, 219]. Attackers\ncould exploit these structured outputs to manipulate the system's behavior, leading to unsafe or unintended actions."}, {"title": "3.2 Threat Model", "content": "The framework of an adversarial threat model is defined by two primary aspects: the attacker's abilities and their\nintended goals."}, {"title": "3.2.1 Attacker Capabilities", "content": "\u2022 In white-box attacks, adversaries have full access to the system's architecture, parameters, and APIs, as is\ncommon in open-source embodied Al systems or simulators [20, 192]. Examples of classic white-box attacks\ninclude FGSM [11], PGD [69], APGD [45], and CW [32]. This enables the creation of highly targeted and\nsophisticated attacks.\n\u2022 Gray-box Attacks: Gray-box attacks arise when adversaries have partial access, typically via high-level APIs or\nexternal interfaces, but lack control over lower-level components [118]. These attacks often exploit vulnerabilities\nin external inputs like sensor data or user commands.\n\u2022 Black-box Attacks: In black-box attacks, adversaries lack knowledge of the system's internals and interact\nonly through input queries. Despite proprietary protections in commercial systems, attackers can still exploit\nexternal inputs to cause harm."}, {"title": "3.2.2 Attack Targets", "content": "The specific targets of an attack can vary depending on the attacker's goals. Common targets in\nembodied Al systems include:\n\u2022 Perception Systems: Attacks on sensors, such as cameras, LiDAR, and GPS (discussed in Section 3.5), can disrupt\na system's ability to accurately interpret its environment, resulting in faulty decision-making. For example,\nsensor spoofing can cause an autonomous vehicle to misjudge distances or fail to detect obstacles. The Drone\nGPS Spoofing Attack [13] is a notable example, where a civilian drone was hijacked via GPS spoofing, leading\nto illegal use.\n\u2022 Control Systems: Attacks on the system's control algorithms' vulnerabilities (discussed in Section 4) can lead to\nunsafe actions, such as erratic movements or failure to follow safety protocols. For instance, an attacker could\nmanipulate a robot's control system to cause it to collide with objects or humans.\n\u2022 Communication Channels: Embodied AI systems often rely on wireless communication for coordination and\ncontrol. Attacks on these communication channels, such as jamming or Man-in-the-Middle Attacks (MitM)\n(discussed in Section 3.4), can disrupt the system's ability to receive critical updates or commands."}, {"title": "3.3 Attack Taxonomy", "content": "Attacks to embodied AI can be systematically categorized based on attacker capabilities, target components and\nvulnerabilities, and their overall impact on system performance. This taxonomy encompasses a range of strategies,\nincluding data poisoning, sensor spoofing, and adversarial perturbations, each with distinct mechanisms and implications.\nIn this section, we provide a comprehensive overview of these attack types:\n\u2022 Exogenous Vulnerability Centric Attacks\nData-Centric Attacks\n* Data Poisoning: Attackers inject malicious data into the training set, causing the model to learn\nincorrect associations, potentially leading to failures in safety-critical applications like autonomous\ndriving or robotic surgery.\nInput Manipulation Attacks\n* Adversarial Attacks: Adversarial attacks are a significant threat to machine learning models, par-\nticularly those used in perception systems. Szegedy et al. [172, 203] first demonstrated that many\nmachine learning algorithms are vulnerable to adversarial examples, which are small perturbations\nin input data that lead to incorrect model predictions. This vulnerability is largely due to the inherent\nnon-linearity of neural networks [78].\n* Sensor Spoofing Attacks: These attacks exploit vulnerabilities in the sensor data pipeline of embodied\nAl systems, targeting the processes of data acquisition, processing, and interpretation. Unlike adver-\nsarial attacks, which manipulate digital inputs to deceive machine learning models, sensor spoofing\nattacks occur in the physical domain, manipulating the environment or sensor signals directly. For\nexample, attackers can inject false data into sensors like LiDAR, GPS, or cameras, causing robots to\nmisinterpret their surroundings and exhibit unintended behaviors, such as collisions or navigation\nfailures [236, 263], misleading autonomous vehicles [233], or embedding malicious voice commands\nto exploit smart assistants for unauthorized actions.\n* Command Injection: In NLP-based systems like voice assistants or robots, attackers can inject\nmalicious commands via voice or text inputs, potentially triggering unintended actions such as\nunlocking doors, disabling security systems, or performing dangerous maneuvers.\n* Jailbreak Attacks: Jailbreak attacks on LLMs and LVLMs involve crafting prompts that bypass the\nmodel's safety mechanisms, posing significant risks to information security. In contrast to command\ninjection attacks, which primarily target physical systems, jailbreak attacks exploit vulnerabilities in\nthe model's response generation. Two representative techniques include:\nPrompt Injection: Manipulating the model's output by injecting malicious prompts that cause\nthe LLM to generate harmful or unintended responses."}, {"title": "3.4 Cybersecurity Threat", "content": "Cybersecurity threats have become increasingly sophisticated, targeting the core components of intelligent systems\nto exploit vulnerabilities and disrupt operations. From intercepting sensitive communications through Man-in-the-\nMiddle Attacks (MitM) (Section 3.4.1) to Sensor-to-Model Attacks (Section 3.4.2), compromising firmware (Section\n3.4.3), and exploiting Side-Channel Attacks (Section 3.4.4), these threats undermine system integrity and confidentiality.\nAdditionally, Ransomware Attacks (Section 3.4.5) and Supply Chain Attacks (Section 3.4.6) further amplify risks by\ntargeting critical infrastructure and trusted dependencies. Understanding these attack vectors is essential for developing\nproactive and resilient embodied Al systems."}, {"title": "3.4.1 Man-in-the-Middle Attacks (MitM)", "content": "MitM attacks exploit the reliance of embodied systems on real-time commu-\nnication, particularly over wireless protocols, to intercept and manipulate data exchanged between the system and\nexternal services such as cloud servers or remote control units [43]. These attacks are especially critical in autonomous\nsystems, where the integrity of real-time data is essential for safe operation. For instance, in autonomous vehicles,\nMitM attacks can tamper with navigation or sensor data, leading to unsafe rerouting or misinterpretation of the\nenvironment [177]. Similarly, in robotic systems, such attacks can compromise control commands, resulting in mission\nfailures, production defects, or even physical harm to humans. The growing connectivity of these systems-enabled\nby inter-agent communication, infrastructure-linked networks in autonomous vehicles, and cloud-based control in\nrobotics-further broadens the attack surface. Weaknesses in communication protocols, particularly those lacking\nrobust encryption, provide adversaries with opportunities to intercept and modify data in transit, enabling malicious\noutcomes such as data theft, system disruption, or complete system takeover [5].\nTo mitigate the risks posed by MitM attacks, several countermeasures are essential. Strong encryption protocols, such\nas Transport Layer Security (TLS) or IPsec, can help protect data integrity and confidentiality during transmission [194].\nMoreover, real-time intrusion detection systems (IDS) can monitor network traffic for anomalies that may indicate an\\ongoing MitM attack, providing an additional layer of defense [46]."}, {"title": "3.4.2 Sensor-to-Model Attacks", "content": "Sensor-to-model attacks target the integrity of sensor data, which is fundamental to\nthe perception and decision-making processes in embodied AI systems [149]. In these systems, accurate sensor data is\ncrucial for tasks such as navigation, object recognition, and interaction with the physical environment. By injecting\nfalsified or malicious data into the system, attackers can manipulate the system's understanding of its surroundings,\nleading to potentially dangerous outcomes. This makes sensor-to-model attacks both stealthy and highly effective,\nespecially in real-time systems where rapid decision-making is required [212]. For example, in autonomous driving, an\nattacker could inject spoofed LiDAR or camera data, causing the vehicle to perceive obstacles that do not exist, leading\nto unnecessary evasive maneuvers or even collisions [165]. Similarly, in industrial robotics, manipulated sensor data\ncould cause a robot to misinterpret its environment, resulting in production errors or safety hazards.\nTo mitigate the risks posed by sensor-to-model attacks, robust sensor fusion algorithms and anomaly detection\ntechniques are essential. Sensor fusion, which combines data from multiple sources (e.g., LiDAR, radar, and cameras),\ncan help identify inconsistencies between sensor readings, making it harder for attackers to inject false data into all\nsensors simultaneously. Additionally, anomaly detection systems can monitor sensor data streams for unusual patterns\nor behaviors, providing an additional layer of defense against data manipulation. These techniques, when combined"}, {"title": "3.4.3 Firmware Attacks", "content": "Firmware attacks represent a particularly insidious threat to embodied systems, as they target\nthe low-level software that controls hardware components. In embodied Al systems, such as autonomous vehicles,\ndrones, and robots, the firmware governs critical functionalities, including sensor data processing, actuator control, and\ncommunication protocols. By compromising the firmware of key components such as sensors or actuators, attackers\ncan gain persistent control over the system, potentially bypassing higher-level security mechanisms. For example, in\nthe case of autonomous drones, a malicious firmware update could alter flight control parameters, leading to erratic\nor unsafe behavior [44"}]}