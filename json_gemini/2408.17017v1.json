{"title": "Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling", "authors": ["Guangya Wan", "Yuqi Wu", "Jie Chen", "Sheng Li"], "abstract": "Self-Consistency (SC) is a widely used method to mitigate hallucinations in Large Language Models (LLMs) by sampling the LLM multiple times and outputting the most frequent solution. Despite its benefits, SC results in significant computational costs proportional to the number of samples generated. Previous early-stopping approaches, such as Early Stopping Self Consistency and Adaptive Consistency, have aimed to reduce these costs by considering output consistency, but they do not analyze the quality of the reasoning paths (RPs) themselves. To address this issue, we propose Reasoning-Aware Self-Consistency (RASC), an innovative early-stopping framework that dynamically adjusts the number of sample generations by considering both the output answer and the RPs from Chain of Thought (CoT) prompting. RASC assigns confidence scores sequentially to the generated samples, stops when certain criteria are met, and then employs weighted majority voting to optimize sample usage and enhance answer reliability. We comprehensively test RASC with multiple LLMs across varied QA datasets. RASC outperformed existing methods and significantly reduces sample usage by an average of 80% while maintaining or improving accuracy up to 5% compared to the original SC.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive performance across various tasks, such as question-answering (QA), common-sense reasoning, and math reasoning . However, the reliability and faithfulness of LLMs' Reasoning Paths (RPs) remain questionable due to the phenomenon known as hallucination. Consequently, many researchers have proposed different prompting methods to mitigate hallucination and improve overall performance. A widely used method is Self-Consistency (SC), a majority voting technique where multiple output samples (e.g., 64 samples) are generated for a given input, and the final decision is the mode among the samples. Despite its effectiveness, the SC strategy has significant computational overhead proportional to the number of sampled outputs, assuming these outputs are of equal length. For example, testing the entire MATH dataset with SC costs about $2,000 using the GPT-4 API with a sampling size of 64, posing a substantial burden for many researchers and organizations. Thus, it is essential to minimize the cost of SC while maintaining performance. Various early-stopping mechanisms have been proposed to address this issue, showing significant reductions in sampling cost while preserving the original SC accuracy improvement. In the original Self-Consistency (SC) and previously proposed early-stopping variations, only the final answers derived from the sampled RPs were considered for improving QA accuracy or determining stop conditions due to the intuition that different ways of thinking lead to a unique correct answer in complex reasoning tasks. However, these approaches neglect the quality and content of the RPs themselves, leading to the loss of critical information regarding potential hallucinations in the RPs. Additionally, studies by suggest that CoT reasoning is possible even with invalid demonstrations-prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics. This implies that even if the output answer of a RP is correct, the RP's contents may still be neither faithful nor logical. Moreover, previous SC approaches use a simple majority voting mechanism to determine the final answer, assigning equal weight to each sampled RP, which allows unfaithful outputs to influence the final decision. Therefore, a larger number of samples is required to achieve a more robust consensus.\nTo address these limitations, we propose Reasoning-Aware Self-Consistency (RASC) as shown in Fig. 1, which assigns an individual confidence score to each sampled RP based on its RP quality and output consistency, and then performs early-stopping based on the distribution of confidence scores. By utilizing weighted majority voting with the confidence score, we can optimally determine the minimum number of samples needed to generate a more reliable answer. RASC introduces a novel early-stopping mechanism for large language models that dynamically adjusts the number of sample evaluations, aiming to reduce redundant computational expense by considering both the quality of sample content and the consistency of RPs. We incorporate RASC to three different LLMs and evaluate the performance on five diverse datasets, including elementary math, commonsense, and challenging math problems. Our approach consistently outperforms fixed-budget and state-of-the-art early-stopping strategies, reducing sample usage by an average of 80% without compromising accuracy. Additionally, increasing the budget allows our confidence score-driven weighted majority voting to improve QA accuracy by an average of 6%. We validated RASC on 14 out-of-domain QA datasets, demonstrating consistent significant improvements. In summary, our contributions are:\n\u2022 Reasoning-Aware Self-Consistency (RASC) Framework: We propose an innovative early-stopping framework that dynamically modulates sample evaluations by considering both RP quality and output consistency.\n\u2022 Development of a Confidence Score Approximator: We design seven lightweight textual indicators to evaluate content quality and faithfulness, combining with a weighted majority voting system that enhances sampling efficiency and accuracy.\n\u2022 Robust Evaluation: RASC's effectiveness and robustness are demonstrated across different LLMs and datasets, achieving significant improvements in both efficiency and accuracy."}, {"title": "Related Work", "content": "Chain-of-Thoughts (CoT) Prompting: Introduced by Wei et al. (2022), CoT prompting enhances the performance of Large Language Models (LLMs) by guiding them through step-by-step reasoning, simplifying complex problems into manageable steps. This technique allows models to generate intermediate Reasoning Paths (RPs) and achieve more accurate conclusions. Several studies have built upon the original CoT concept. In this work, we specifically focus on zero-shot CoT prompting to construct RPs within our framework. This approach is notable for its straightforward prompt design, which requires no specific preparatory work or examples.\nSelf-Consistency: Wang et al. (2022b) introduced Self-Consistency (SC), which enhances the performance of Chain-of-Thought (CoT) prompting by sampling multiple diverse reasoning chains and aggregating their outcomes through a simple majority voting mechanism. While this approach achieves higher accuracy, it also incurs increased computational costs. Furthermore, the original SC settings employ a fixed number of samples for each question, and such a fixed sampling strategy becomes redundant and cost-inefficient, particularly for simpler questions. Addressing this inefficiency and redundancy is one of the main goals of our work.\nSelf-Consistency Early Stopping: To address the computational demands of traditional Self-Consistency (SC), innovative early stopping strategies have been developed. Li et al. (2024b) introduced the Early Stopping Self-Consistency (ESC), which halts RP sampling once the outputs within a fixed window are consistent. Similarly, Aggarwal et al. (2023) proposed the Adaptive Consistency (ASC), stopping sampling once a predefined probability threshold is reached, based on the distribution of sampled outputs. While both methods significantly reduce sampling costs by fourfold with minimal performance loss, they primarily focus on output consistency and neglect the quality of the RPs - a central aspect of our research."}, {"title": "Reasoning-Aware Self-Consistency", "content": "Reasoning-Aware Self-Consistency (RASC) is an early-stopping framework for Self-Consistency prompting that reduces redundant sampling costs while maintaining accuracy improvements. Unlike previous methods that consider only the final answer sequences, RASC determines stopping conditions by evaluating both the Reasoning Paths (RPs) and the output answers. In Fig. 1, we illustrate the differences between traditional SC and RASC. Traditional SC improves accuracy by sampling a fixed number of RPs and determining the final answer based on the mode of these output answers. Our method, however, evaluates both the quality of the CoT RPs and the output answers. Each sample is assigned an individual confidence score based on these evaluations. The cumulative confidence score determines the stopping point. Finally, a confidence score-driven majority voting is used to decide the final answer."}, {"title": "Reasoning-Aware Quality Features", "content": "To determine the content quality and answer consistency of sampled RPs, we establish a set of 7 quality features to calculate their confidence scores. The individual confidence score $CS_i$ can be viewed as a soft measurement of hallucination, where higher confidence suggests less uncertainty in LLM's generation. Since we aim for lightweight indicators requiring minimal feature extraction, we only consider textual features that do not require computationally intensive models or external tools. We classify these features into individual quality features and relative quality features.\nIndividual Quality Features: We include 4 individual quality features, including content length $L_i$, input coherence $C_i$, format error $F_i$, and error-admitting $E_i$, to extract intrinsic information regarding each sampled RP $x_i$. We begin with factors identified by Jin et al. (2024) and Li et al. (2024a), who discuss the importance of RP textual length in terms of QA accuracy and human preference. Generally, lengthening the reasoning steps considerably enhances LLM's reasoning abilities, and humans typically prefer longer reasoning chains. A common type of hallucination, instructional violation, occurs when LLMs do not follow user-defined instructions, implying a higher chance of hallucination and lower overall RP quality. The most obvious and easy-to-detect instructional violation is the RP format $F_r$. In QA tasks, users expect LLMs to respond in a clearly structured format to parse the necessary information. Additionally, consistency between the RP solution and the user question is crucial for instruction-following, implying coherence $C_i$ between the input question and the output solution. Hence, we include instructional violations, such as format errors (parsing errors) and input coherence (deviation from input question), as quality features. With advancements in LLM research, most LLMs can now detect errors in RP during generation. For instance, GPT-4 might acknowledge incorrect reasoning by stating, \"There seemed to be a mistake in the previous calculations.\" Such admissions $E_i$ typically imply that the RP quality is doubtful and the answer unreliable. Therefore, we identify common error-admitting terms to evaluate the quality and faithfulness of sampled RPs. We include detailed methods on individual feature extraction strategy in Appendix D.\nRelative Quality Features: To reduce SC sample cost, the relative quality between additional samples and previous samples is essential to determine the stop condition. We design 3 features to capture relative relationships. Originating from Bang's (2023) study, semantically similar RPs are more likely to generate similar answers. Inspired by Tu et al. (2020), we adopt semantic similarity $S_i$ as a relative quality indicator. For each sampled RP $RP_i$, we compare the semantic similarity between this additional RP and all previously sampled RPs $RP_1,..., RP_{i-1}$. For answers, similarity refers to consistency between additional answer $y_i$ and previous answers $Y_1, . . ., Y_{i-1}$. In this study, we adopt three similarity computation mechanisms: aggregation similarity $S_a$, pairwise similarity $S_p$, and bi-gram similarity $S_b$, as illustrated in Fig. 2. The selection of mechanisms depends on the nature of the target. For CoT RPs, which generally contain more vocabulary, aggregation similarity is more suitable due to its robustness. For answers with limited vocabulary (e.g., multiple-choice questions with four options), aggregation similarity becomes trivial. Pairwise comparison fits better for answers due to its high sensitivity. Detailed mathematical robustness and sensitivity proofs of aggregation and pairwise similarity are included in Appendix E.2 Theorem E.2, in which we prove that the aggregation similarity mechanism is more robust to noise while pairwise is more sensitive for large vocabulary documents. Bigram similarity $S_b$ is defined as the similarity between the additional RP/answer and the preceding RP/answer. This mechanism ensures we capture local relative information since both aggregation and pairwise mechanisms capture global relationships. The validity of these newly proposed relative quality features is confirmed both theoretically through mathematical proof and empirical results. We also compared common similarity algorithms such as Euclidean distance and Cosine Similarity combined with various tokenizers,"}, {"title": "Confidence Score and Stop Condition", "content": "Confidence Score Computation: Consider a dataset $D = \\{(x_i, Y_i)\\}_{i=1}^M$, where each $x_i \\in \\mathbb{R}^d$ is a feature vector representing the quality features of the i-th RP, and $y_i \\in \\{0, 1\\}$ is a binary label indicating the correctness of the answer. We seek to learn a confidence scoring function $f : \\mathbb{R}^d \\rightarrow [0, 1]$ from a family of parameterized functions $F$, where each function is denoted as $f_\\theta$ for parameters $\\theta$. The optimal parameters $\\theta^*$ are obtained by minimizing the following objective:\n$\\theta^* = \\arg \\min_\\theta {\\frac{1}{M}\\sum_{i=1}^M L(f_\\theta(x_i), y_i)}$\nHere, $L$ represents the loss function (e.g., cross-entropy) and $\\Lambda$ is a hyperparameter that controls the regularization strength. We explore several lightweight classic classification models such as logistic regression (LR), Naive Bayes (NB), random forest (RF), and pre-trained deep learning models to represent $F$. Specifically, the input to the confidence scoring function $f_\\theta$ is a feature vector $X_i = [L_i, C_i, F_i, E_i, S_a, S_p, S_b]$ representing the individual quality features (content length $L_i$, input coherence $C_i$, format error $F_i$, and error-admitting $E_i$) and relative quality features (aggregation similarity $S_a$, pairwise similarity $S_p$, and bi-gram similarity $S_b$) of the i-th sampled RP. The output of the confidence scoring function $f_\\theta$ is a confidence score $CS_i \\in [0, 1]$ assigned to the corresponding RP. For the Stop Condition, let $\\{RP\\}_1^K$ be a sequence of $K$ sampled RPs, with corresponding confidence scores $\\{CS_i\\}_{i=1}^K$ computed using the optimized confidence scoring function $f_{\\theta^*}$. We define a buffer $B$ to store RPs with confidence scores above a predefined threshold $T$:\n$B = \\{RP | CS_i > T\\}.$\nThe sampling process terminates when the buffer's size meets a predefined capacity $N (|B| > N)$. Upon reaching the stop condition, the final answer is determined through weighted majority voting. The weights applied are the confidence scores of the RPs in $B$. Mathematically speaking, this is:\n$Answer = \\arg \\max_{\\alpha \\in A} \\sum_{RP \\in B \\atop Answer(RP)=a} CS_i,$\nwhere $A$ represents the set of all possible answers. This summation calculates the total confidence score for each possible answer $a$, accumulating scores only from those RPs in the buffer $B$ whose answers match the answer candidate $a$.\nIn this framework, the confidence scoring function $f_{\\theta^*}$ assigns a score to each sampled RP based on its quality. Sampling proceeds until the buffer $B$, filled with RPs exceeding the threshold $T$, reaches the capacity $N$. The final answer is determined through weighted majority voting based on the confidence scores in $B$. For details, see Algorithm 1."}, {"title": "Experiments", "content": "Models: In our experiments, we focus on three state-of-the-art language models: LLAMA3-8B (Meta, 2024), GPT3.5-turbo (OpenAI, 2024), and Claude-3-Haiku (Anthropic, 2024). Details about the number of parameters, computational budget, and computing infrastructure used in our experiments can be found in Appendix A.\nBaseline Methods: We compare RASC against three established baseline methods: SC (Wang et al., 2022b), ASC (Aggarwal et al., 2023), and ESC (Li et al., 2024b). Please refer to Section 2 for more details on these methods\nDatasets: Our evaluation leverages five Question Answering datasets. For Math Reasoning, we collected data from GSM-8K and MathQA, focusing on specific subsets within these datasets, including GSM-8K-hard, GSM-8K-test, MathQA-challenge, and MathQA-dev, to test our models against both nuanced and direct queries. We also collected data from BiGBench to assess the versatility of our methods across different reasoning domains. We stratified the data based on categories and models to build confidence scoring models using the training segments, with subsequent evaluations performed on the test data. All presented results pertain to the test data unless otherwise specified. To test generalizability, we incorporated data from previous works involving ESC and ASC, evaluating our pipeline using unseen domains and diverse prompts (See Appendix G for more details).\nEvaluation Metrics: We focus on the trade-off between accuracy and the number of generated samples required to achieve a target reasoning accuracy that exceeds all baseline methods while minimizing the number of samples generated. To quantitatively assess this trade-off, we adopted a customized metric that balances the contributions of accuracy and cost in our evaluations. This metric considers both the normalized accuracy and the normalized cost of generating predictions compared to the SC baseline. The details of this metric are provided in the Appendix F. Note that this metric is primarily used for evaluating our pipeline and selecting the best hyperparameters. For the main results, we will focus on presenting the accuracy and the number of generated samples.\nImplementation Details: After extensive experiments (See Figure 3 and Figure 4), we determined that using three individual models (N = 3) and a confidence threshold of 0.1 optimizes our customized metric. LR was selected as the confidence scoring model due to its superior performance on the test set. For generating predictions from language models, we employed a zero-shot CoT prompting approach. Details about the specific prompts used can be found in Appendix C. We adhered to the standard practices established in the SC methodology, setting the temperature parameter to 0.7 for all models, and the baseline method of SC utilizes a default of 40 samples. Due to budget constraints, we collected 500 samples per category per model, resulting in a total sample size of 9,000 in the final data.We also adopted a parser from Langchain and obtained the correctness label; please refer to Appendix D for details."}, {"title": "Main Results", "content": "Quantitative Analysis: The step reduction of different methods across various benchmarks is summarized in Table 1. RASC consistently outperforms other methods, reducing the number of samples required by up to 87.5% while simultaneously preserving comparable reasoning accuracy across all datasets. This success can be attributed to RASC's unique approach of leveraging information from both the generated answers and the reasoning paths (RPs) from the CoT prompting to determine the extent of hallucination in the generated contents. Such dynamic stopping criteria allow RASC to achieve superior performance in terms of both sample efficiency and reasoning accuracy across a wide range of datasets and models. The robustness and consistency of RASC's performance across different random seeds are further demonstrated in Tables 4 and 5 in Appendix B.1. Note that the tradeoff between sample efficiency and accuracy may vary across different categories and datasets. This is because the hyperparameters, such as the number of individual models (N) and the confidence threshold (T), are tuned based on the overall performance instead for individual models or categories. In our experiments, we adopted a consistent setting of N=3 and T=0.1 across all categories and datasets to maintain comparability.\nCost Analysis: Table 2 compares the accuracy and API cost of different methods when applied to GPT-3.5-TURBO, using SC as the baseline. This comparison highlights RASC's significant reduction in API costs by 84.6%, alongside a 7.9% improvement in accuracy. These results demonstrate the main contribution of our pipeline: reducing cost as a result of reducing the number of samples generated while preserving the accuracy of the original SC method. Refer to the table 6 in the appendix B.2 for details on analysis on other models."}, {"title": "Analysis", "content": "Controlling Accuracy-Samples Generation Tradeoff: Our ablation study, illustrated in Figures 3 and 4, explores the impacts of hyperparameters (N) and the confidence threshold (T) on the performance of the RASC method. These results provide valuable insights for fine-tuning RASC to achieve optimal performance by balancing accuracy and computational cost."}, {"title": "Conclusion", "content": "In this paper, we introduce Reasoning-Aware Self-Consistency (RASC), a novel approach that enhances the reliability and efficiency of large language models (LLMs) by dynamically adjusting the number of samples based on the quality and consistency of Reasoning Paths (RPs). RASC assigns confidence scores to each sampled RP and employs weighted majority voting, significantly improving the process of generating reliable answers while reducing computational costs. Our evaluations demonstrate RASC's effectiveness in improving effectiveness and efficiency compared to traditional Self-Consistency methods, marking a significant step towards optimizing LLMs for practical applications."}, {"title": "Limitations", "content": "Despite the demonstrated effectiveness of Reasoning-Aware Self-Consistency (RASC) in improving the efficiency and reliability of large language models (LLMs), several limitations remain:\n\u2022 Hyperparameter Sensitivity and Computational Overhead: RASC's performance heavily relies on carefully tuned hyperparameters, which may vary across datasets, models, and applications. Additionally, the feature extraction and confidence scoring introduce computational overhead that could be significant in large-scale or real-time scenarios.\n\u2022 Limited Feature Set and Soft Approximation: The current preliminary feature set may not capture all nuances of generated content, potentially affecting the model's effectiveness. Moreover, the individual confidence scores provide a soft approximation of hallucination likelihood, which may not accurately capture all instances.\n\u2022 Potential Bias in Training Data: Biases present in the training data used to build confidence scoring models could influence RASC's outputs, emphasizing the importance of ensuring data diversity and fairness to mitigate biases and enhance the fairness of generated RPs.\nThese limitations highlight areas for future research and underscore the need for continued improvement and adaptation of RASC to ensure its applicability and effectiveness in diverse settings."}, {"title": "Computational Details", "content": "The language models used in our experiments have varying numbers of parameters:\n\u2022 LLAMA3-8B: This model has 8 billion parameters.\n\u2022 GPT3.5-turbo: The exact number of parameters for this model is not publicly disclosed by OpenAI.\n\u2022 Claude-3-Haiku: The number of parameters for this model is not publicly available from Anthropic.\nTo generate all the collected data for our experiments using the LLAMA3-8B model, we consumed a total of approximately 100 GPU hours. This computational budget was used for running the model, generating CoT samples, and processing the results.\nOur experiments were conducted on a computing infrastructure equipped with the following hardware:\n\u2022 GPU: NVIDIA GeForce RTX 3070 Ti\n\u2022 CPU: 16 cores 11th Generation Intel Core i7 Processors\nIn this study, several online available Python packages are used to conduct experiment and analysis, the packages are as follows:\n\u2022 NLTK: For calculating Jaccard Similarity, Ngram, tokenizer.\n\u2022 statistics: For computing logistic regression.\n\u2022 PyTorch: For using pre-trained LLM.\n\u2022 pandas: For data manipulation.\n\u2022 json: loading and saving json data.\n\u2022 sklearn: For supervised learning models training and evaluation.\n\u2022 adaptive_consistency: For implementing adaptive consistency algorithm.\n\u2022 Levenshtein: For computing Levenshtein distance.\n\u2022 transformer: For huggingface PTM usage.\n\u2022 LangChain: For LLM API usage and answer parser."}, {"title": "Experiments: Additional Results", "content": "To evaluate the robustness and consistency of our Reasoning-Aware Self-Consistency (RASC) method, we conducted experiments using multiple random seeds for data sampling and model initialization. Tables 4 and 5 present the mean, standard deviation, and p-values of accuracy and cost (number of generated samples) of the RASC method on various benchmark categories across different models. Table 4 shows the mean accuracy of RASC along with the standard deviation across different random seeds. The small standard deviations indicate that the performance of RASC remains consistent and stable regardless of the specific data split or model initialization. Additionally, the p-values are calculated to test if the accuracy of RASC is significantly better than the performance of the Self-Consistency (SC) method. The majority of the p-values are well below the significance level of 0.05, suggesting that RASC achieves statistically significant improvements over SC in most cases. Similarly, Table 5 presents the mean cost (number of generated samples) of RASC along with the standard deviation across different random seeds. The low standard deviations demonstrate that the sample efficiency of RASC is robust and consistent across various data splits and model initializations. The p-values in this table are calculated to test if the cost of RASC is significantly smaller than the fixed budget of 40 samples used in SC. The extremely low p-values (all below 10-10) provide strong evidence that RASC significantly reduces the number of required samples compared to SC, regardless of the random seed. The results in these tables highlight the robustness and reliability of our RASC method. The consistent performance and sample efficiency across different random seeds suggest that the improvements achieved by RASC are not sensitive to specific data splits or model initializations. This robustness is crucial for the practical application of RASC in real-world scenarios, where the method needs to perform well under various conditions and datasets.\nIn this subsection, we present additional experimental results on our own datasets to further analyze the performance and cost-effectiveness of the proposed RASC method compared to other baseline methods. Table 2 shows a cost analysis of different methods on the Claude-3-Haiku model, averaged over one thousand questions. RASC achieves the highest accuracy while significantly reducing the cost compared to the standard Self-Consistency (SC) method. Figure 6 compares the accuracy of our RASC method across different categories on our collected datasets. In most categories, we observe a performance gain with our method, with a slight drop in the GSM8K_test category due to a more significant reduction in sample generation. Figure 7 presents the accuracy comparison of RASC across different models on our collected datasets. We find that the performance either significantly improves (GPT3.5/llama3) or remains comparable to the previous method for all models. Figure 8 provides a complete view of the accuracy comparison of RASC across different categories and models of our collected datasets using a heatmap representation.\nTo further validate the effectiveness of RASC, we evaluate its performance on the datasets used in the Adaptive Self-Consistency (ASC) paper. Figure 9 compares the accuracy of RASC across different models on ASC's datasets. We observe a significant improvement on the vicuna-13b model but a slight degradation in performance on the code-davinci-002 model, which is compensated by a more substantial reduction in sample generation, as shown in Figure 11. Figure 10 presents a comprehensive view of the accuracy comparison of RASC across different categories and models of ASC's datasets using a heatmap representation. Figure 11 compares the reduction in the number of samples generated by RASC across different models on ASC's datasets. We find a smaller improvement on the vicuna-13b model but a more significant reduction on the code-davinci-002 model, despite a slight performance drop in accuracy, as shown in Figure 9. Figure 12 provides a complete view of the number of samples generated by RASC across different categories and models of ASC's datasets using a heatmap representation.\nWe also evaluate the performance of RASC on the datasets used in the Early-Stopping Self-Consistency (ESC) paper. Figure 13 compares the accuracy of RASC across different models on ESC's datasets, showing consistency with existing methods for all three models they used. Figure 14 presents a comprehensive view of the accuracy comparison of RASC across different categories and models of ESC's datasets using a heatmap representation. Figure 15 compares the number of samples generated by RASC across different models on ESC's datasets, demonstrating a significant improvement over existing methods for all three models they used. Figure 16 provides a complete view of the number of samples generated by RASC across different categories and models of ESC's datasets using a heatmap representation."}, {"title": "Sample Generation Prompt", "content": "In this study, we adopt CoT prompting which explicitly requires LLMs to RP to the question step by step. Since the OOD test dataset uses few-shot prompting and budget constrain, we instead utilize zero-shot prompting to generate RPs in our own datasets. The prompt is defined as follows:\nSystem Message: You are a professional specialized in {subject}. You need to help me answer the given question. Notice that you need to solve the question step by step and as detailed as possible. Do not jump to the answer directly. You must follow the RP format instructions.\nHuman Message:{question}"}, {"title": "Individual Quality Features Extraction", "content": "In this study, we design four individual quality features to measure the reliability of each individual reasoning path. The four individual quality features are RP length, input coherence, format error, and error-admitting terms.\nRP length varies from question category to question category. For instance, the required length of RP for solving a simple math problem (such as 'what is 32 + 42') is significantly less than solving partial differential equations. Therefore, we pre-define some thresholds for different categories. If the proposed RP length is more than this threshold, we consider it as long (1 as a binary indicator), otherwise is small. In our RP generation stage, we explicitly require LLM to state the step number (Step 1: ...; Step 2: ...). Therefore, in our study, the threshold is used to compare the number of proposed steps from RP. In the OOD dataset or in general cases where this instruction is not provided, we consider the number of sentences as the length of PR.\nSince we are aiming for a lightweight feature extraction method, advanced name entity recognition or similar keyword comparison methods are not used to compare the RP with the input question. Instead, semantic similarity with Jaccard is used to approximate the coherence between the RP and input questions. The higher the similarity, the RP is considered to have more input coherence.\nThe prompt-response nature of LLMs does not allow users to explicitly obtain well-structured fields (such as JSON or dictionary in Python) since the responses are often in string format. Therefore, the common practice is asking LLMs to output response strings with clear format and then using regular expressions to parse out these fields of interest. When the testing QA dataset is large and diverse, it is hard to propose a uniform parser to extract information. Moreover, instructional following hallucination is another common behaviour for LLMs. This prevents users from really extracting what they want from responses, especially in massive test cases.\nA common instruction that people use to extract answers in QA tasks is asking LLMs to explicitly end their response with fixed term The Answer is xxx. Once the LLM varies its expression (for instance xxx is the answer), the parser fails and outputs the wrong answer even if the RP is actually correct. However, this is an unavoidable problem as long as instruction following hallucination still exists. Therefore, it is considered a low-quality indicator since it definitely outputs a false answer (parser fails). In our study, we adopt parsers provided by LangChain since it contains well-structured parsing logic. Yet due to the unstable behaviour of LLMs, this parser fails often as well."}, {"title": "Frequent Error-admitting Terms for Various LLMS", "content": "By observing the generated samples, we observe several common error-admitting terms that LLMs often use to acknowledge the previous mistakes they made during the RP. We conduct systematical evaluation and list out some of the most frequent error-admitting terms used by various LLMs in Fig.17-19. These error-admitting terms, such as \"There seemed to be a mistake in the previous calculation\" have demonstrated a high risk of sample hallucination. Therefore, by parsing out these terms, we obtain an error-admitting quality feature."}, {"title": "Relative Quality Feature Extraction", "content": "In this study, we use semantic similarity to determine the relative quality features between reasoning paths. To further describe the method for obtaining semantic similarities, we include a thorough discussion of similairty algorithms and mechanisms."}, {"title": "Similarity Algorithms", "content": "The proposed relative quality feature requires computing the semantic similarity between different sampled RPs. Therefore, it is essential to find (1) a tokenization method that captures the semantics of the given text, and (2) an appropriate calculation to compute similarity between embeddings. In our experiment, we tested several widely used tokenization methods, including TF-IDF (Robertson, 2004), Count Tokenizer, GloVe (Pennington et al., 2014), and Sentence Transformer (Reimers and Gurevych, 2019). For similarity computation, we tested classic Euclidean distance and cosine similarity. On the other hand, Jaccard Similarity and Levenshtein distance are two NLP textual similarity algorithms which can be applied directly to the corpus, they are included in our test as well. The comparison of some of the tokenizer and computation results is shown in Fig.E.1. Our results indicate that these combinations can all perform good semantic similarity calculations. Therefore, we use the Jaccard Similarity Index as our primary similarity algorithm in our study due to its computational efficiency (required by our method) and comparable performance."}, {"title": "Similarity Mechanism Robust and Sensitivity Proof", "content": "In this study", "by": "n$J(A", "Lemma": "nLemma E.1. Let $N(X)$ be a system follows Gaussian Distribution with $X = \\{X_1", "Inequality": "P(|X - \\mu| \\geq k\\sigma) \\leq \\frac{1"}, {"by": "n$\\overline{X"}, "frac{1}{t-1}\\sum_{i=1}^{t-1} X_i$\nThe variance of the average is:\n$Var(\\overline{X}) = \\frac{1}{(t-1)^2} \\sum_{i=1}^{t-1} \\sigma_i^2$\nApplying Chebyshev's Inequality to $\\overline{X}$, we get:\n$P(|\\overline{X} - \\mu| \\geq k o_{\\overline{X}}) \\leq \\frac{1}{k^2}$\nwhere $o_{\\overline{X}} = \\sqrt{Var(\\overline{X})} = \\sqrt{\\frac{1}{(t-1)^2} \\sum_{i=1}^{t-1} \\sigma_i^2}$.\nDespite averaging, if the individual $X_i$ have high variances $\\sigma_i^2$, then $o_{\\overline{X}}$ will also be large. This implies that:\n$P(|\\overline{X} - \\mu| \\geq k \\cdot \\sqrt{\\frac{1}{(t-1)^2} \\sum_{i=1}^{t-1} \\sigma_i^2}) \\leq \\frac{1}{k^2}$\nThus, $\\overline{X}$ still has a considerable probability of deviating from its mean, indicating persistent sensitivity. Hence, the individual sensitivity of $X_i$ accumulates and affects the overall sensitivity of $N(X)$ regardless of the averaging operation.\nTo prove the robustness and sensitivity of the adopted similarity mechanisms, we introduce the following theorem:\nTheorem E.2. Aggregation similarity is more robust to noise compared to pairwise similarity when the document $C_t$ vocabulary size is large. Both aggregation and pairwise similarity are sensitive when the document vocabulary size is small.\nProof. Let $C_{agg,t-1} = C_{1:t-1} = \\bigcup_{i=1}^{t-1} C_i$:"]}