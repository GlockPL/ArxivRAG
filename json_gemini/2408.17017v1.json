{"title": "Dynamic Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling", "authors": ["Guangya Wan", "Yuqi Wu", "Jie Chen", "Sheng Li"], "abstract": "Self-Consistency (SC) is a widely used method to mitigate hallucinations in Large Language Models (LLMs) by sampling the LLM multiple times and outputting the most frequent solution. Despite its benefits, SC results in significant computational costs proportional to the number of samples generated. Previous early-stopping approaches, such as Early Stopping Self Consistency and Adaptive Consistency, have aimed to reduce these costs by considering output consistency, but they do not analyze the quality of the reasoning paths (RPs) themselves. To address this issue, we propose Reasoning-Aware Self-Consistency (RASC), an innovative early-stopping framework that dynamically adjusts the number of sample generations by considering both the output answer and the RPs from Chain of Thought (CoT) prompting. RASC assigns confidence scores sequentially to the generated samples, stops when certain criteria are met, and then employs weighted majority voting to optimize sample usage and enhance answer reliability. We comprehensively test RASC with multiple LLMs across varied QA datasets. RASC outperformed existing methods and significantly reduces sample usage by an average of 80% while maintaining or improving accuracy up to 5% compared to the original SC.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive performance across various tasks, such as question-answering (QA) (Tan et al., 2023; Arefeen et al., 2024; Li et al., 2024c), common-sense reasoning (Kojima et al., 2022a; Krause and Stolzenburg, 2023; Zhao et al., 2024), and math reasoning (Ahn et al., 2024; Imani et al., 2023; Zhang et al., 2024). However, the reliability and faithfulness of LLMs' Reasoning Paths (RPs) remain questionable due to the phenomenon known as hallucination (Huang et al., 2023; Zhang et al., 2023). Consequently, many researchers have proposed different prompting methods to mitigate hallucination and improve overall performance. A widely used method is Self-Consistency (SC), a majority voting technique where multiple output samples (e.g., 64 samples) are generated for a given input, and the final decision is the mode among the samples (Wang et al., 2022b).\nDespite its effectiveness, the SC strategy has significant computational overhead proportional to the number of sampled outputs, assuming these outputs are of equal length. For example, testing the entire MATH dataset with SC costs about $2,000 using the GPT-4 API with a sampling size of 64, posing a substantial burden for many researchers and organizations (Lewkowycz et al., 2022). Thus, it is essential to minimize the cost of SC while maintaining performance. Various early-stopping mechanisms have been proposed to address this issue, showing significant reductions in sampling cost while preserving the original SC accuracy improvement (Li et al., 2024b; Aggarwal et al., 2023).\nIn the original Self-Consistency (SC) and previously proposed early-stopping variations, only the final answers derived from the sampled RPs were considered for improving QA accuracy or determining stop conditions due to the intuition that different ways of thinking lead to a unique correct answer in complex reasoning tasks (Wang et al., 2022b). However, these approaches neglect the quality and content of the RPs themselves, leading to the loss of critical information regarding potential hallucinations in the RPs. Additionally, studies by Wang et al. (2022a) and Jin et al. (2024) suggest that CoT reasoning is possible even with invalid demonstrations-prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics. This implies that even if the output answer of a RP is correct, the RP's contents may still be neither faithful nor logical. Moreover, previous SC approaches use a simple majority voting mechanism to determine the final answer, assigning equal weight to each sampled RP, which allows unfaithful outputs to influence the final decision. Therefore, a larger number of samples is required to achieve a more robust consensus.\nTo address these limitations, we propose Reasoning-Aware Self-Consistency (RASC) as shown in Fig. 1, which assigns an individual confidence score to each sampled RP based on its RP quality and output consistency, and then performs early-stopping based on the distribution of confidence scores. By utilizing weighted majority voting with the confidence score, we can optimally determine the minimum number of samples needed to generate a more reliable answer. RASC introduces a novel early-stopping mechanism for large language models that dynamically adjusts the number of sample evaluations, aiming to reduce redundant computational expense by considering both the quality of sample content and the consistency of RPs. We incorporate RASC to three different LLMs and evaluate the performance on five diverse datasets, including elementary math, commonsense, and challenging math problems. Our approach consistently outperforms fixed-budget and state-of-the-art early-stopping strategies, reducing sample usage by an average of 80% without compromising accuracy. Additionally, increasing the budget allows our confidence score-driven weighted majority voting to improve QA accuracy by an average of 6%. We validated RASC on 14 out-of-domain QA datasets, demonstrating consistent significant improvements. In summary, our contributions are:\n\u2022 Reasoning-Aware Self-Consistency (RASC) Framework: We propose an innovative early-stopping framework that dynamically modulates sample evaluations by considering both RP quality and output consistency.\n\u2022 Development of a Confidence Score Approximator: We design seven lightweight textual indicators to evaluate content quality and faithfulness, combining with a weighted majority voting system that enhances sampling efficiency and accuracy.\n\u2022 Robust Evaluation: RASC's effectiveness and robustness are demonstrated across different LLMs and datasets, achieving significant improvements in both efficiency and accuracy."}, {"title": "Related Work", "content": "Chain-of-Thoughts (CoT) Prompting: Introduced by Wei et al. (2022), CoT prompting enhances the performance of Large Language Models (LLMs) by guiding them through step-by-step reasoning, simplifying complex problems into manageable steps. This technique allows models to generate intermediate Reasoning Paths (RPs) and achieve more accurate conclusions. Several studies have built upon the original CoT concept (Zhou et al., 2023; Yao et al., 2023; Besta et al., 2024; Brown et al., 2020). In this work, we specifically focus on zero-shot CoT prompting (Kojima et al., 2022b) to construct RPs within our framework. This approach is notable for its straightforward prompt design, which requires no specific preparatory work or examples.\nSelf-Consistency: Wang et al. (2022b) introduced Self-Consistency (SC), which enhances the performance of Chain-of-Thought (CoT) prompting by sampling multiple diverse reasoning chains and aggregating their outcomes through a simple majority voting mechanism. While this approach achieves higher accuracy, it also incurs increased computational costs. Furthermore, the original SC settings employ a fixed number of samples for each question, and such a fixed sampling strategy becomes redundant and cost-inefficient, particularly for simpler questions. Addressing this inefficiency and redundancy is one of the main goals of our work.\nSelf-Consistency Early Stopping: To address the computational demands of traditional Self-Consistency (SC), innovative early stopping strategies have been developed. Li et al. (2024b) introduced the Early Stopping Self-Consistency (ESC), which halts RP sampling once the outputs within a fixed window are consistent. Similarly, Aggarwal et al. (2023) proposed the Adaptive Consistency (ASC), stopping sampling once a predefined probability threshold is reached, based on the distribution of sampled outputs. While both methods significantly reduce sampling costs by fourfold with minimal performance loss, they primarily focus on output consistency and neglect the quality of the RPs a central aspect of our research."}, {"title": "Reasoning-Aware Self-Consistency", "content": "Reasoning-Aware Self-Consistency (RASC) is an early-stopping framework for Self-Consistency prompting that reduces redundant sampling costs while maintaining accuracy improvements. Unlike previous methods that consider only the final answer sequences, RASC determines stopping conditions by evaluating both the Reasoning Paths (RPs) and the output answers. In Fig. 1, we illustrate the differences between traditional SC and RASC. Traditional SC improves accuracy by sampling a fixed number of RPs and determining the final answer based on the mode of these output answers. Our method, however, evaluates both the quality of the CoT RPs and the output answers. Each sample is assigned an individual confidence score based on these evaluations. The cumulative confidence score determines the stopping point. Finally, a confidence score-driven majority voting is used to decide the final answer."}, {"title": "Reasoning-Aware Quality Features", "content": "To determine the content quality and answer consistency of sampled RPs, we establish a set of 7 quality features to calculate their confidence scores. The individual confidence score $CS_i$ can be viewed as a soft measurement of hallucination, where higher confidence suggests less uncertainty in LLM's generation. Since we aim for lightweight indicators requiring minimal feature extraction, we only consider textual features that do not require computationally intensive models or external tools. We classify these features into individual quality features and relative quality features (See Fig.2).\nIndividual Quality Features: We include 4 individual quality features, including content length $L_i$, input coherence $C_i$, format error $F_i$, and error-admitting $E_i$, to extract intrinsic information regarding each sampled RP $x_i$. We begin with factors identified by Jin et al. (2024) and Li et al. (2024a), who discuss the importance of RP textual length in terms of QA accuracy and human preference. Generally, lengthening the reasoning steps considerably enhances LLM's reasoning abilities, and humans typically prefer longer reasoning chains. A common type of hallucination, instructional violation, occurs when LLMs do not follow user-defined instructions, implying a higher chance of hallucination and lower overall RP quality (Zhang et al., 2023; Huang et al., 2023; Hosking et al., 2023). The most obvious and easy-to-detect instructional violation is the RP format $F_r$. In QA tasks, users expect LLMs to respond in a clearly structured format to parse the necessary information. Additionally, consistency between the RP solution and the user question is crucial for instruction-following, implying coherence $C_i$ between the input question and the output solution (Li et al., 2024a). Hence, we include instructional violations, such as format errors (parsing errors) and input coherence (deviation from input question), as quality features. With advancements in LLM research, most LLMs can now detect errors in RP during generation. For instance, GPT-4 might acknowledge incorrect reasoning by stating, \"There seemed to be a mistake in the previous calculations.\" Such admissions $E_i$ typically imply that the RP quality is doubtful and the answer unreliable (Li et al., 2024a). Therefore, we identify common error-admitting terms to evaluate the quality and faithfulness of sampled RPs. We include detailed methods on individual feature extraction strategy in Appendix D.\nRelative Quality Features: To reduce SC sample cost, the relative quality between additional samples and previous samples is essential to determine the stop condition. We design 3 features to capture relative relationships. Originating from Bang's (2023) study, semantically similar RPs are more likely to generate similar answers. Inspired by Tu et al. (2020), we adopt semantic similarity $S_i$ as a relative quality indicator. For each sampled RP $RP_i$, we compare the semantic similarity between this additional RP and all previously sampled RPs $RP_1, ..., RP_{i-1}$. For answers, similarity refers to consistency between additional answer $y_i$ and previous answers $Y_1, . . ., Y_{i-1}$. In this study, we adopt three similarity computation mechanisms: aggregation similarity $S_a$, pairwise similarity $S_p$, and bi-gram similarity $S_b$, as illustrated in Fig. 2. The selection of mechanisms depends on the nature of the target. For CoT RPs, which generally contain more vocabulary, aggregation similarity is more suitable due to its robustness. For answers with limited vocabulary (e.g., multiple-choice questions with four options), aggregation similarity becomes trivial. Pairwise comparison fits better for answers due to its high sensitivity. Detailed mathematical robustness and sensitivity proofs of aggregation and pairwise similarity are included in Appendix E.2 Theorem E.2, in which we prove that the aggregation similarity mechanism is more robust to noise while pairwise is more sensitive for large vocabulary documents. Bigram similarity $S_b$ is defined as the similarity between the additional RP/answer and the preceding RP/answer. This mechanism ensures we capture local relative information since both aggregation and pairwise mechanisms capture global relationships. The validity of these newly proposed relative quality features is confirmed both theoretically through mathematical proof and empirical results. We also compared common similarity algorithms such as Euclidean distance and Cosine Similarity combined with various tokenizers, Jaccard similarity coefficient $J$, and Levenshtein distance (see Appendix E.1). Our empirical results indicate they have similar performance in semantic similarity (see Fig. 20 in Appendix E.1). Hence, we adopt Jaccard to compute similarity due to its lightweight and low computational overhead."}, {"title": "Confidence Score and Stop Condition", "content": "Confidence Score Computation: Consider a dataset $D = \\{(x_i, y_i)\\}_{i=1}^M$, where each $x_i \\in \\mathbb{R}^d$ is a feature vector representing the quality features of the i-th RP, and $y_i \\in \\{0, 1\\}$ is a binary label indicating the correctness of the answer. We seek to learn a confidence scoring function $f : \\mathbb{R}^d \\rightarrow [0, 1]$ from a family of parameterized functions $\\mathcal{F}$, where each function is denoted as $f_\\theta$ for parameters $\\theta$. The optimal parameters $\\theta^*$ are obtained by minimizing the following objective:\n$$\\theta^* = \\arg \\min_\\theta \\frac{1}{M} \\sum_{i=1}^M \\mathcal{L}(f_\\theta(x_i), y_i)$$\nHere, $\\mathcal{L}$ represents the loss function (e.g., cross-entropy) and $\\lambda$ is a hyperparameter that controls the regularization strength. We explore several lightweight classic classification models such as logistic regression (LR), Naive Bayes (NB), random forest (RF), and pre-trained deep learning models to represent $\\mathcal{F}$. Specifically, the input to the confidence scoring function $f_\\theta$ is a feature vector $X_i = [L_i, C_i, F_i, E_i, S_a, S_p, S_b]$ representing the individual quality features (content length $L_i$, input coherence $C_i$, format error $F_i$, and error-admitting $E_i$) and relative quality features (aggregation similarity $S_a$, pairwise similarity $S_p$, and bi-gram similarity $S_b$) of the i-th sampled RP. The output of the confidence scoring function $f_{\\theta^*}$ is a confidence score $CS_i \\in [0, 1]$ assigned to the corresponding RP. For the Stop Condition, let $\\{RP\\}$ be a sequence of K sampled RPs, with corresponding confidence scores $\\{CS_i\\}_{i=1}^K$ computed using the optimized confidence scoring function $f_{\\theta^*}$. We define a buffer $B$ to store RPs with confidence scores above a predefined threshold T:\n$$B = \\{RP_i | CS_i > T\\}_{i=1}^K$$\nThe sampling process terminates when the buffer's size meets a predefined capacity N ($|B| > N$).\nUpon reaching the stop condition, the final answer is determined through weighted majority voting. The weights applied are the confidence scores of the RPs in B. Mathematically speaking, this is:\n$$Answer = \\arg \\max_{a\\in A} \\sum_{RP_i\\in B \\atop Answer(RP_i) = a} CS_i$$\nwhere A represents the set of all possible answers. This summation calculates the total confidence score for each possible answer $a$, accumulating scores only from those RPs in the buffer B whose answers match the answer candidate a.\nIn this framework, the confidence scoring function $f_{\\theta^*}$ assigns a score to each sampled RP based on its quality. Sampling proceeds until the buffer B, filled with RPs exceeding the threshold T, reaches the capacity N. The final answer is determined through weighted majority voting based on the confidence scores in B. For details, see Algorithm 1."}, {"title": "Experiments", "content": "Models: In our experiments, we focus on three state-of-the-art language models: LLAMA3-8B (Meta, 2024), GPT3.5-turbo (OpenAI, 2024), and Claude-3-Haiku (Anthropic, 2024). Details about the number of parameters, computational budget, and computing infrastructure used in our experiments can be found in Appendix A.\nBaseline Methods: We compare RASC against three established baseline methods: SC (Wang et al., 2022b), ASC (Aggarwal et al., 2023), and ESC (Li et al., 2024b). Please refer to Section 2 for more details on these methods\nDatasets: Our evaluation leverages five Question Answering datasets. For Math Reasoning, we collected data from GSM-8K (Cobbe et al., 2021) and MathQA (Amini et al., 2019), focusing on specific subsets within these datasets, including GSM-8K-hard, GSM-8K-test, MathQA-challenge, and MathQA-dev, to test our models against both nuanced and direct queries. We also collected data from BiGBench (bench authors, 2023) to assess the versatility of our methods across different reasoning domains. We stratified the data based on categories and models to build confidence scoring models using the training segments, with subsequent evaluations performed on the test data. All presented results pertain to the test data unless otherwise specified. To test generalizability, we incorporated data from previous works involving ESC and ASC, evaluating our pipeline using unseen domains and diverse prompts (See Appendix G for more details).\nEvaluation Metrics: We focus on the trade-off between accuracy and the number of generated samples required to achieve a target reasoning accuracy that exceeds all baseline methods while minimizing the number of samples generated. To quantitatively assess this trade-off, we adopted a customized metric that balances the contributions of accuracy and cost in our evaluations. This metric considers both the normalized accuracy and the normalized cost of generating predictions compared to the SC baseline. The details of this metric are provided in the Appendix F. Note that this metric is primarily used for evaluating our pipeline and selecting the best hyperparameters. For the main results, we will focus on presenting the accuracy and the number of generated samples.\nImplementation Details: After extensive experiments (See Figure 3 and Figure 4), we determined that using three individual models (N = 3) and a confidence threshold of 0.1 optimizes our customized metric. LR was selected as the confidence scoring model due to its superior performance on the test set. For generating predictions from language models, we employed a zero-shot CoT prompting approach. Details about the specific prompts used can be found in Appendix C. We adhered to the standard practices established in the SC methodology, setting the temperature parameter to 0.7 for all models, and the baseline method of SC utilizes a default of 40 samples. Due to budget constraints, we collected 500 samples per category per model, resulting in a total sample size of 9,000 in the final data.We also adopted a parser from Langchain and obtained the correctness label; please refer to Appendix D for details."}, {"title": "Main Results", "content": "Quantitative Analysis: The step reduction of different methods across various benchmarks is summarized in Table 1. RASC consistently outperforms other methods, reducing the number of samples required by up to 87.5% while simultaneously preserving comparable reasoning accuracy (refer to Figure 8 in Appendix B.2. for more information) across all datasets. This success can be attributed to RASC's unique approach of leveraging information from both the generated answers and the reasoning paths (RPs) from the CoT prompting to determine the extent of hallucination in the generated contents. Such dynamic stopping criteria allow RASC to achieve superior performance in terms of both sample efficiency and reasoning accuracy across a wide range of datasets and models. The robustness and consistency of RASC's performance across different random seeds are further demonstrated in Tables 4 and 5 in Appendix B.1. Note that the tradeoff between sample efficiency and accuracy may vary across different categories and datasets. This is because the hyperparameters, such as the number of individual models (N) and the confidence threshold (T), are tuned based on the overall performance instead for individual models or categories. In our experiments, we adopted a consistent setting of N=3 and T=0.1 across all categories and datasets to maintain comparability.\nCost Analysis: Table 2 compares the accuracy and API cost of different methods when applied to GPT-3.5-TURBO, using SC as the baseline. This comparison highlights RASC's significant reduction in API costs by 84.6%, alongside a 7.9% improvement in accuracy. These results demonstrate the main contribution of our pipeline: reducing cost as a result of reducing the number of samples generated while preserving the accuracy of the original SC method. Refer to the table 6 in the appendix B.2 for details on analysis on other models."}, {"title": "Analysis", "content": "Controlling Accuracy-Samples Generation Tradeoff: Our ablation study, illustrated in Figures 3 and 4, explores the impacts of hyperparameters (N) and the confidence threshold (T) on the performance of the RASC method. These results provide valuable insights for fine-tuning RASC to achieve optimal performance by balancing accuracy and computational cost.Figure 3 shows the effect of varying the number of individual models (N) on the accuracy and the average number of samples generated by RASC. As N increases, the accuracy improves, but at the cost of generating more samples. This is because a larger N allows for more diverse predictions and a higher chance of reaching consensus for improved accuracy. However, this also requires more computational resources. The optimal value of N should be chosen based on the desired balance between accuracy and computational cost. In our experiments, we found that N = 3 provides a good trade-off.\nFigure 4 shows how changing the confidence threshold (T) affects the accuracy and average number of samples generated by RASC. As T increases from 0.1 to 0.5, accuracy improves, but more samples are needed, thus increasing computational cost. However, when T exceeds 0.5, accuracy drops. This is because a high threshold makes RASC too selective, thus rejecting correct samples that don't meet the strict confidence requirement. This can lead to difficulty reaching consensus and may bias towards common answers. The optimal threshold aims to balance accuracy and inclusivity. In our experiments, a threshold of 0.1 struck a good balance between accuracy and efficiency.\nThese ablation studies demonstrate the flexibility of the RASC method in controlling the trade-off between accuracy and computational cost by adjusting the hyperparameters N and T. By carefully tuning these parameters, RASC can be optimized for different scenarios and requirements, depending on the priority given to accuracy or efficiency.\nImpact of Confidence Scoring Model: The performance of different confidence scoring models, as summarized in Table 3, identifies Logistic Regression as the most effective model in terms of accuracy and sample utilization. This finding shows the importance of selecting an appropriate confidence scoring model to maximize the benefits of the RASC approach. These results are obtained by training the confidence scoring models on the training set and then fine-tuning the best set of hyperparameters (N and T) using the customized metrics. The final evaluation is performed on the test set to ensure the robustness of the model. Among the confidence scoring models evaluated Logistic Regression achieves the highest accuracy of 46.0% while requiring the lowest average number of samples (5.87) compared to other models. This indicates that it effectively captures the relationship between the features extracted from the RP and the likelihood of hallucination, which allows RASC to make informed decisions during the weighted majority voting process.\nThe accuracy of Random model drops below the baseline of Self-Consistency. The poor performance of the Random model, which assigns confidence scores randomly, emphasizes the importance of using a well-designed confidence scoring model in the RASC approach. Without a meaningful assessment of the generated content's quality and consistency, the weighted majority voting process cannot effectively distinguish between reliable and unreliable samples, leading to suboptimal results.\nThe HHEM Model, which is a DeBERTa-based hallucination detector sourced from Hugging Face (Honovich et al., 2022), does not perform as well as the other models in this context. This suggests that relying solely on a pre-trained model without considering the specific characteristics and requirements of the RASC approach may not yield the best results. The superior performance of models like Logistic Regression (LR), Naive Bayes (NB), and Random Forest (RF), which utilize manual feature engineering tailored to the RASC framework, highlights the importance of crafting features that capture the nuances of the generated content as we discussed in the method section.\nOut-of-Distribution Performance: We evaluate RASC's performance on out-of-distribution (OOD) data to assess its generalizability in unseen queries. The left panel of Figure 5 presents the results for the dataset obtained from the work by ASC, showcasing RASC's performance on various unseen categories. Across all categories, RASC consistently achieves higher accuracy than the SC method while requiring fewer steps. The right panel of Figure 5 illustrates RASC's performance on the dataset obtained from the work by ES, which includes different LLMs and prompts. Once again, RASC demonstrates superior performance across all categories, maintaining high accuracy while significantly reducing the number of steps. These results underscore RASC's adaptability to different language models and its effectiveness in handling diverse prompts. For more details regarding the performance comparison, refer to Figure 10, 14, 12, 16, in the Appendix B.2. The results presented in Figure 5 provide strong evidence for RASC's ability to maintain high performance and efficiency even when faced with unseen data, LLMs, and prompts. Such robustness and generalizability are crucial for real-world applications, where the model must adapt to a wide range of scenarios and challenges without sacrificing accuracy or computational resources."}, {"title": "Conclusion", "content": "In this paper, we introduce Reasoning-Aware Self-Consistency (RASC), a novel approach that enhances the reliability and efficiency of large language models (LLMs) by dynamically adjusting the number of samples based on the quality and consistency of Reasoning Paths (RPs). RASC assigns confidence scores to each sampled RP and employs weighted majority voting, significantly improving the process of generating reliable answers while reducing computational costs. Our evaluations demonstrate RASC's effectiveness in improving effectiveness and efficiency compared to traditional Self-Consistency methods, marking a significant step towards optimizing LLMs for practical applications."}, {"title": "Limitations", "content": "Despite the demonstrated effectiveness of Reasoning-Aware Self-Consistency (RASC) in improving the efficiency and reliability of large language models (LLMs), several limitations remain:\n\u2022 Hyperparameter Sensitivity and Computational Overhead: RASC's performance heavily relies on carefully tuned hyperparameters, which may vary across datasets, models, and applications. Additionally, the feature extraction and confidence scoring introduce computational overhead that could be significant in large-scale or real-time scenarios.\n\u2022 Limited Feature Set and Soft Approximation: The current preliminary feature set may not capture all nuances of generated content, potentially affecting the model's effectiveness. Moreover, the individual confidence scores provide a soft approximation of hallucination likelihood, which may not accurately capture all instances.\n\u2022 Potential Bias in Training Data: Biases present in the training data used to build confidence scoring models could influence RASC's outputs, emphasizing the importance of ensuring data diversity and fairness to mitigate biases and enhance the fairness of generated RPs.\nThese limitations highlight areas for future research and underscore the need for continued improvement and adaptation of RASC to ensure its applicability and effectiveness in diverse settings."}, {"title": "Computational Details", "content": "The language models used in our experiments have varying numbers of parameters:\n\u2022 LLAMA3-8B: This model has 8 billion parameters (Meta, 2024).\n\u2022 GPT3.5-turbo: The exact number of parameters for this model is not publicly disclosed by OpenAI (OpenAI, 2024).\n\u2022 Claude-3-Haiku: The number of parameters for this model is not publicly available from Anthropic (Anthropic, 2024).\nTo generate all the collected data for our experiments using the LLAMA3-8B model, we consumed a total of approximately 100 GPU hours. This computational budget was used for running the model, generating CoT samples, and processing the results."}, {"title": "Computing Infrastructure", "content": "Our experiments were conducted on a computing infrastructure equipped with the following hardware:\n\u2022 GPU: NVIDIA GeForce RTX 3070 Ti\n\u2022 CPU: 16 cores 11th Generation Intel Core i7 Processors"}, {"title": "Used Packages", "content": "In this study, several online available Python packages are used to conduct experiment and analysis, the packages are as follows:\n\u2022 NLTK: For calculating Jaccard Similarity, Ngram, tokenizer.\n\u2022 statistics: For computing logistic regression.\n\u2022 PyTorch: For using pre-trained LLM.\n\u2022 pandas: For data manipulation.\n\u2022 json: loading and saving json data.\n\u2022 sklearn: For supervised learning models training and evaluation.\n\u2022 adaptive_consistency: For implementing adaptive consistency algorithm.\n\u2022 Levenshtein: For computing Levenshtein distance.\n\u2022 transformer: For huggingface PTM usage.\n\u2022 LangChain: For LLM API usage and answer parser."}, {"title": "Experiments: Additional Results", "content": "To evaluate the robustness and consistency of our Reasoning-Aware Self-Consistency (RASC) method, we conducted experiments using multiple random seeds for data sampling and model initialization. Tables 4 and 5 present the mean, standard deviation, and p-values of accuracy and cost (number of generated samples) of the RASC method on various benchmark categories across different models. Table 4 shows the mean accuracy of RASC along with the standard deviation across different random seeds. The small standard deviations indicate that the performance of RASC remains consistent and stable regardless of the specific data split or model initialization. Additionally, the p-values are calculated to test if the accuracy of RASC is significantly better than the performance of the Self-Consistency (SC) method. The majority of the p-values are well below the significance level of 0.05, suggesting that RASC achieves statistically significant improvements over SC in most cases.\nSimilarly, Table 5 presents the mean cost (number of generated samples) of RASC along with the standard deviation across different random seeds. The low standard deviations demonstrate that the sample efficiency of RASC is robust and consistent across various data splits and model initializations. The p-values in this table are calculated to test if the cost of RASC is significantly smaller than the fixed budget of 40 samples used in SC. The extremely low p-values (all below $10^{-10}$) provide strong evidence that RASC significantly reduces the number of required samples compared to SC, regardless of the random seed. The results in these tables highlight the robustness and reliability of our RASC method. The consistent performance and sample efficiency across different random seeds suggest that the improvements achieved by RASC are not sensitive to specific data splits or model initializations. This robustness is crucial for the practical application of RASC in real-world scenarios, where the method needs to perform well under various conditions and datasets."}, {"title": "Additional Results on Our Datasets", "content": "In this subsection, we present additional experimental results on our own datasets to further analyze the performance and cost-effectiveness of the proposed RASC method compared to other baseline methods. Table 2 shows a cost analysis of different methods on the Claude-3-Haiku model, averaged over one thousand questions. RASC achieves the highest accuracy while significantly reducing the cost compared to the standard Self-Consistency (SC) method. Figure 6 compares the accuracy of our RASC method across different categories on our collected datasets. In most categories, we observe a performance gain with our method, with a slight drop in the GSM8K_test category due to a more significant reduction in sample generation. Figure 7 presents the accuracy comparison of RASC across different models on our collected datasets. We find that the performance either significantly improves (GPT3.5/llama3) or remains comparable to the previous method for all models. Figure 8 provides a complete view of the accuracy comparison of RASC across different categories and models of our collected datasets using a heatmap representation."}, {"title": "Additional Results on ASC's Datasets", "content": "To further validate the effectiveness of RASC, we evaluate its performance on the datasets used in the Adaptive Self-Consistency (ASC) paper (Aggarwal et al., 2023). Figure 9 compares the accuracy of RASC across different models on ASC's datasets. We observe a significant improvement on the vicuna-13b model but a slight degradation in performance on the code-davinci-002 model, which is compensated by a more substantial reduction in sample generation, as shown in Figure 11. Figure 10 presents a comprehensive view of the accuracy comparison of RASC across different categories and models of ASC's datasets using a heatmap representation. Figure 11 compares the reduction in the number of samples generated by RASC across different models on ASC's datasets. We find a smaller improvement on the vicuna-13b model but a more significant reduction on the code-davinci-002 model, despite a slight performance drop in accuracy, as shown in Figure 9. Figure 12 provides a complete view of the number of samples generated by RASC across different categories and models of ASC's datasets using a heatmap representation."}, {"title": "Additional Results on ESC's Datasets", "content": "We also evaluate the performance of RASC on the datasets used in the Early-Stopping Self-Consistency (ESC) paper (Li et al., 2024b). Figure 13 compares the accuracy of RASC across different models on ESC's datasets, showing consistency with existing methods for all three models they used. Figure 14 presents a comprehensive view of the accuracy comparison of RASC across different categories and models of ESC's datasets using a heatmap representation. Figure 15 compares the number of samples generated by RASC across different models on ESC's datasets, demonstrating a significant improvement over existing methods for all three models they used. Figure 16 provides a complete view of the number of samples generated by RASC across different categories and models of ESC's datasets using a heatmap representation."}, {"title": "Sample Generation Prompt", "content": "In this study, we adopt CoT prompting which explicitly requires LLMs to RP to the question step by step. Since the OOD test dataset uses few-shot prompting and budget constrain, we instead utilize zero-shot prompting to generate RPs in our own datasets. The prompt is defined as follows:\nSystem Message: You are a professional specialized in {subject}. You need to help me answer the given question. Notice that you need to solve the question step by step and as detailed as possible. Do not jump to the answer directly. You must follow the RP format instructions.\nHuman Message:{question}"}, {"title": "Individual Quality Features Extraction", "content": "In this study, we design four individual quality features to measure the reliability of each individual reasoning path. The four individual quality features are RP length, input coherence, format error, and error-admitting terms."}, {"title": "Reasoning Path Length", "content": "RP length varies from question category to question category. For instance", "what is 32 + 42": "is significantly less than solving partial differential equations. Therefore", "1": "...", "2": "..."}]}