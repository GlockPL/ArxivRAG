{"title": "Towards Fault Tolerance in Multi-Agent Reinforcement Learning", "authors": ["Yuchen Shi", "Huaxin Pei", "Liang Feng", "Yi Zhang", "Danya Yao"], "abstract": "Agent faults pose a significant threat to the performance of multi-agent reinforcement learning (MARL) algorithms, introducing two key challenges. First, agents often struggle to extract critical information from the chaotic state space created by unexpected faults. Second, transitions recorded before and after faults in the replay buffer affect training unevenly, leading to a sample imbalance problem. To overcome these challenges, this paper enhances the fault tolerance of MARL by combining optimized model architecture with a tailored training data sampling strategy. Specifically, an attention mechanism is incorporated into the actor and critic networks to automatically detect faults and dynamically regulate the attention given to faulty agents. Additionally, a prioritization mechanism is introduced to selectively sample transitions critical to current training needs. To further support research in this area, we design and open-source a highly decoupled code platform for fault-tolerant MARL, aimed at improving the efficiency of studying related problems. Experimental results demonstrate the effectiveness of our method in handling various types of faults, faults occurring in any agent, and faults arising at random times.", "sections": [{"title": "I. INTRODUCTION", "content": "Cooperative multi-agent systems have been widely applied in various fields, such as autonomous driving [1]\u2013[3], drone formation [4], [5], and multi-robot control [6]. However, the occurrence of individual faults within the system is inevitable, especially in the complex operational environment, and it poses a significant threat to the system's ability to continue executing its intended tasks [7], [8]. The existing studies demonstrate that there are numerous approaches to successfully enhance the fault tolerance of traditional cooperative control methods [9], [10], such as the leader-following method [11] and artificial potential field method [12] in formation control, thereby reducing the impact of individual faults on the system's task execution.\nBenefiting from the strong exploration ability of reinforcement learning in complex environments, multi-agent reinforcement learning (MARL) algorithms, employing the actor-critic [13], [14] based centralized training with decentralized execution (CTDE) framework [15], have emerged as the mainstream approaches for addressing cooperative problems in multi-agent systems. Notable examples include multi-agent deep deterministic policy gradient (MADDPG) [15], multi-agent proximal policy optimization (MAPPO) [16], etc. However, MARL faces unique challenges in dealing with unexpected agent faults within a system, as we discuss in this paper.\nTaking the predator-prey system as an example, we present the primary challenges when an agent within the system fails. The impact of faults on MARL algorithms, both during training and execution, gives rise to two primary challenges. The first challenge is the chaotic inputs of networks, The second challenge is sample imbalance. To capture the unpredictability of fault timing and the affected agents, faults must be randomly assigned to different agents at random time steps during simulations. This randomness can lead to significant variability among the transitions stored in the replay buffer."}, {"title": "II. RELATED WORKS", "content": "In this section, we focus on addressing highly critical system-level faults in agents, where the affected agent loses its ability to observe or act, as opposed to faults occurring in specific components of the agent [18]\u2013[21]. In the existing studies, Pei et al. [22] construct a rule-based model to achieve fault tolerance in multi-vehicle cooperative driving at signal-free intersections. Kamel et al. [23] design task-reassignment algorithms taking use of Hungarian algorithm, to ensure the completion of robot teams task after a robot's fault.\nCTDE framework for MARL avoids the problem of non-stationarity when agents learn independently, Recently, many studies have endeavored to enhance MARL algorithms, aiming to increase their adaptability to real-world MARL environments [28]\u2013[33], such as R-MADDPG for partial observable environments [34], and MADDPG-M for environments with extremely noisy observations [35]. Similarly, we propose our model based on MADDPG, making it capable of fault tolerance problems in MARL.\nAttention mechanism has become a popular technique due to its superior performance, interpretability, and ease of integration with basic models [36]. Attention mechanism is widely employed for a variety of deep learning models across many different domains and tasks, including natural language processing [37]\u2013[39], computer vision [40], [41], reinforcement learning [42]\u2013[45]. Iqbal & Sha [42] using centrally computed critics that share an attention mechanism, dynamically selecting which agents to attend to, enable more effective learning in MARL. In our approach, the attention mechanism plays a crucial role in enhancing fault tolerance.\nTransitions reweighting is an effective method to improve utilization efficiency of transitions. There are typically two types: prioritizing easy transitions and prioritizing difficult transitions."}, {"title": "III. BACKGROUND", "content": "A multi-agent system with N potentially faulty agents can be modeled as decentralized partially observable Markov decision processes (Dec-POMDPs) [49], [50], defined as < S, A, T, R, Y,O,\u03b3, F >, where S is the set of states, A = {A1, ..., AN} is the set of joint actions, T is the transition function, O = {01, ..., ON} is the set of observations, and Y = {Y1, ..., YN} is the set of observation functions. At each time step t, agent i receives a partial observation o\u2081 = Y\u00bf(s) : S \u2192 O\u017c, takes action a\u0131 according to policy \u03c0\u03b8\u03b5: \u03b1\u1f31 = \u03c0\u03b8\u03b5 (\u039f\u017c) : Oi \u2192 Ai, and receives a reward ri = R(s,\u03b11,...,\u03b1\u03bd) : S \u00d7 A\u2081 \u00d7 ... \u00d7 AN \u2192 R. Assume that fault occurs under a certain probability p, which is defined by state s and the current time t: p = F(s,t). Therefore, the environment at the next time step can be described as s' = T(s, a1, ..., an, p) : S \u00d7 A\u2081 \u00d7 ... \u00d7 AN \u2192 S. The objective for each agent is to maximize its expected discounted reward E[Ri] = \u0395[\u03a3=t'-tri,t'], where y \u2208 [0,1] is the discount factor, ri,t' is the reward at time step t' and T is the end time step of an episode.\nMADDPG [15] is a variant of the deterministic policy gradient algorithm [51] for multi-agent systems, with centralized Q-value function and decentralized policies. Considering N agents with continuous deterministic policies \u03bc = {\u03bc\u03b8\u2081,\u2026\u2026, \u03bc\u03b8\u03bd}, parameterized by \u03b8i, the policy gradient for agent i is:\n$\\nabla_{\\theta_{i}} J(\\theta)=\\mathbb{E}_{\\mathbf{U}(D)}[\\nabla_{\\theta_{i}} \\mu_{\\theta_{i}}\\left(a_{i} \\mid o_{i}\\right) \\nabla_{a_{i}} Q_{i}(\\mathbf{x}, \\mathbf{a})\\left.\\mid_{a_{i}=\\mu_{\\theta_{i}}\\left(o_{i}\\right)}\\right]$\nQ_i(x, a) is agent i's Q-value function, parameterized by \u03c6i, that accepts all agents' actions a = [\u03b11,...,\u03b1\u03bd] and state information x."}, {"title": "IV. METHODS", "content": "Our method builds upon the widely used MADDPG algorithm and can be described in three integral parts.\nFirst, we detail the configuration of inputs for the critic and actor to represent fault states and facilitate the use of attention mechanisms. In brief, the critic takes into account local observations and actions of all agents as input, and the actor only receives the current agent's local observations as input. Detailed descriptions of input settings are provided in Section IV-A.\nSecond, we design the AACFT model by incorporating attention mechanisms into the critic and actor networks, enabling it to automatically tackle the special information resulting from an unexpected agent fault. It is worth noting that, in such a case, the information associated with the faulty agent affects the critic and actor differently. For the critic, the learning algorithm needs to prioritize shifting attention away from the observations of the faulty agent and focus on other relevant information within the input. For the actor, the learning algorithm should be designed to make flexible decisions regarding the importance of the faulty agent at different stages of task execution. The attention mechanisms in the critic and actor play different roles when addressing fault issues. Therefore, it is impertive to discuss fault tolerance in the critic and actor separately, which we address in Sections IV-B and IV-C.\nFinally, in Section IV-D, we extend Prioritized Experience Replay (PER) to AACFT to enhance the training efficiency of both the critic and the actor, and provide the complete algorithm."}, {"title": "A. Input Configuration", "content": "Similar to the basic MADDPG algorithm, in our method, the critic of agent i takes in all agents' observations o = [01, ..., ON] and actions a = [a1, ..., \u03b1\u03bd] and outputs Q-value Q (o, a). The actor of agent i takes in its observation or and outputs action"}, {"title": "B. Fault Tolerance in Critic", "content": "Unexpected agent faults can directly impact the input of the critic, as the faulty agent j becomes unable to observe or move, rendering the original oj and aj values invalid. To enable the critic to distinguish whether an agent fails or not, we set oj = z\u00b71, aj = 0 when F\u2081 = 0, i.e., agent j fails. Each bit of of is a special flag z, the absolute value of which should be much larger than the normal values to make the anomalies more prominent in the input. While this setting benefits the critic in detecting the occurrence of faults, the excessive value of special flag z significantly disrupts critic network training, especially when the faults happen randomly.\nTo tackle this issue, we introduce an attention module into the structure of the critic. This module helps in diverting attention from the observations of the faulty agent and instead focuses on other pertinent information within the input. As shown in Fig. 2, the encoder fe transforms the observation and action of agent i into its embedding ei = fi(0i, ai). The attention module calculates the attention weight aij to the embeddings ej according to Equ. (3), and the attention embedding bi used for evaluating the Q-value of agent i according to Equ. (4). Then, the decoder gf extracts agent i's Q-value Q(o, a) from bi:\n$Q(o,a) = g(b_i) = g_i(\\sum_{j=1}^N\\alpha_{ij}W_v e_j)$,\nwhere & denotes the parameter set of critics, including the individual parameters of all agents' encoders and decoders, and the public parameters of Wq, Wk, Wv in Equ. 3.\nAll critics are updated together to minimize a joint regression loss function L($) due to parameter sharing:\n$L(\\phi)=\\mathbb{E}_{\\mathbf{U}(D)}\\left[\\sum_{i=1}^{N} F_{i} \\cdot\\left(Q^{i}(\\mathbf{o}, \\mathbf{a})-y_{i}\\right)^{2}\\right]$\n$y_{i}=r_{i}+\\gamma Q^{\\prime}\\left(\\mathbf{o}^{\\prime}, \\mathbf{a}_{1}, \\ldots, \\gamma\\right) |_{\\alpha_{i}=\\mu_{\\theta_{i}}\\left(o_{i}\\right)}$,\nwhere $' and '$ denote the delayed parameters of the target critics and target policies, respectively. The actor of the faulty agent is disabled, so that its critic which guides the actor's"}, {"title": "C. Fault Tolerance in Actors", "content": "Unexpected agent faults can also have a direct impact on the input of the actor, as the faulty agent j's state could affect the observations of the remaining normal agents, i.e., Oij, where i\u2208 {1, ..., N}\\j. To enable the actor to distinguish the agent fault and selectively use the state information of the faulty agent, we set the value of oij according to the significance of the faulty agent j at different stages of task execution. Specifically, if the state of faulty agent j still makes sense to decisions of other agents after it fails, then oij = Yi(Sj,to), where to is the time step fault occurs. Otherwise, similar to the critic network, we set oij = z\u00b71 to make the anomalies more prominent and easier to identify.\nAccording to the above, the sudden occurrence of agent faults places high demands on the performance of the actor network, requiring it to possess the ability to dynamically determine whether to prioritize attention to the faulty agent at different stages of task execution. To reach this goal, in addition to the special design of the input state representation, we innovatively introduce an attention module in the actor to flexibly incorporate information about the faulty agent. As shown in Fig. 2, different from the structure of the critic network, an actor aims to output one action for the whole observation space. Therefore, referring to models with attention for classification [41], we add a token eio for agent i to help output the action. On such a basis, the encoder fa transforms oij into its embedding eij = fij(Oij). The attention module calculates the attention weight ai,oj of the token eio to eij and its attention embedding bio. Then, the decoder g\u00e5 outputs agent i's action a\u017c from bio:\n$\\mu_{\\theta_{i}}\\left(o_{i}\\right)=g_{i}\\left(b_{i o}\\right)=g_{i}\\left(\\sum_{j=1}^{N}\\alpha_{i o, j} W_{v} e_{i j}\\right)$,\nwhere i is the parameter set of agent i's actor, which includes the parameters of all encoders and the decoder, and the public parameters of W, W, W\nTo update agent i's actor, a\u017c is removed from the input of Q and replaced with \u03bc\u0473 (0\u2081), while other agents' actions a\\i are fixed. Each actor is updated to minimize its loss, i.e., the opposite of the value of the new action combination:\n$L(\\theta_{i})=-\\mathbb{E}_{\\mathbf{U}(D)}\\left[F_{i} \\cdot Q(\\mathbf{o},(\\mathbf{a}_{i},\\mathbf{a}_{\\backslash i}))\\left.\\mid_{a_{i}=\\mu_{\\theta_{i}}\\left(o_{i}\\right)}\\right]$\nSimilar to the situation with the critic, due to the disablement of the faulty agent's actor, the corresponding transitions are excluded."}, {"title": "D. Prioritized Experience Replay for AACFT", "content": "Imbalanced transitions stored in the replay buffer are reweighted to improve training efficiency. As mentioned in Section IV-B and Section IV-C, N+1 modules conduct updates. To apply PER to AACFT, PER is employed across all modules. A transition exhibits different importance for each module, so different priorities should be assigned. In AACFT, the critics are updated simultaneously due to parameter sharing, so a single priority queue is set up for them, named as Qc. On the other hand, the actors of different agents are updated separately, so each actor is assigned its own priority queue, named as {Qa,1,..., Qa, N}.\nWhen the transition (x, x', a, r, F) is stored, it is assigned a corresponding priority in each queue, with the value being the highest priority currently in the queue. It is important to"}, {"title": "Algorithm 1 AACFT with priority", "content": "Input: batchsize k, replay buffer U(D), agents number N, exponents a and \u03b2, maximum training steps S, replay period K\n1: for s = 1 to S do\n2: Step the environment and store (x, x', a, r, F) in U(D).\n3: Append priority for critic queue Qc \u2190 [Qc, max Qe]\n4: for i = 1 to N do\n5: Append priority for actor queues Qa,i \u2190 [Qa,i, max Qa,i] if Fi = 1\n6: end for\n7: if s = 0 mod K then\n8: // Update the critics\n9: Sample k transitions from U(D) to form a batch u(D). Transition j \u223c P(j) = pj\u03b1 /\u2211k Pk\u03b1\n10: Compute importance-sampling weights for k transitions in u(D). wj = (N \u00b7 P(j))-\u03b2 / maxi Wi\n11: Update the critics by loss L($) = E(o,a,r,o',w)\u223cu(D) [\u03c9\u00b7\u2211i=1 Fi. (Q(o, a) \u2013 Yi)2]\nN\n12: Update the priorty of k transitions in Qe\n13: // Update the actors\n14: for i = 1 to N do\n15: Sample k transitions from U(D) to form a batch u(D). Transition j \u223c P(j) = pj\u03b1 /\u2211kPk\u03b1\n16: Compute importance-sampling weights for k transitions in u(D). wj = (N \u00b7 P(j))-\u03b2 / maxi Wi\n17: Update the actor of agent i by loss L(0\u2081) = \u2212E(o,a,w)\u223cu(D) [\u03c9\u00b7 Q(0, (a\u017c, a\\i))]|a\u2081=\u03bc\u03bf; (0\u017c)\n18: Update the priorty of k transitions in Qazi\n19: end for\n20: end if\n21: end for"}, {"title": "V. A NEW PLATFORM FOR FAULT-TOLERANT MARL", "content": "Current reinforcement learning platforms integrate multiple algorithms and environments [52], [53], facilitating the study of different algorithms' performance across various environments. However, they present challenges when adapting to fault-tolerant MARL algorithms. Faults can affect multiple aspects of experiments, creating strong dependencies between different modules in the code. For instance, in the main program, when the environment advances to the next timestep, it must decide whether to inject faults. After faults are introduced, the scenario scripts need to simulate abnormal observations or actions caused by the faults, while the algorithm scripts may need to filter and process data from faulty agents. Existing platforms face low efficiency in training and testing fault-tolerant MARL models due to these complexities.\nWe design and open-source a new platform for fault-tolerant MARL. The platform consists of a core structure and supporting modules"}, {"title": "VI. EXPERIMENTS", "content": "This section evaluates and validates our proposed methods from various perspectives. In Subsection VI-B1, we assess the performance of a multi-agent system trained without considering potential faults when a fault occurs, demonstrating the necessity of enhancing fault-tolerant capabilities of RL algorithms. In Subsection VI-B2, we compare the AACFT proposed in this paper with three baseline methods, illustrating that AACFT can improve the fault tolerance of MARL. In Subsection VI-B3, we visualize an episode of a scenario to show how the trained agents execute actions to mitigate the adverse effects of faults. In Subsection VI-B4, we visualize the attention distribution to validate the role of attention in AACFT. We analyze the changes in attention before and after faults to understand its impact, and we conduct ablation studies by removing attention in the critic and actor modules separately. In Subsection VI-B5, to verify the effectiveness of PER, we conduct comparative experiments and analyze the changes in the number of different types of transitions sampled in the training batches. Finally, in Subsection VI-B6, we test the performance of the trained model at different fault timesteps to demonstrate AACFT's adaptability to temporal variations.\nTo comprehensively validate the experimental objectives presented in the previous subsection, we design four scenarios modified from Multi-Agent Particle Environment (MPE) [15]. Schematic diagrams are shown in Fig. 4.\nAbandonment Scenario The abandonment scenario is a fault-tolerant variation of the basic predator-prey setup. At the start of each episode, we randomly generate two obstacles, three cooperative agents (\"predators\") to be trained, and one target agent (\"prey\") to be pursued. In this scenario, faults may disrupt the communication structure among agents, potentially reducing the number of functioning agents in the system. The remaining agents must maintain communication and continue pursuing the target, while disregarding and abandoning any faulty agents.\nRecovery Scenario In addition to potential communication breakdowns, faults may also introduce temporary tasks for the agents. To validate our method, we design a scenario called the Recovery Scenario. This scenario mirrors the abandonment scenario when agent 2 fails, but with a key difference: agent 3 holds valuable resources that must be recovered by agent 2 in the event of a fault. In our experiments, a successful recovery is defined as agent 2 touching the faulty agent. We assume agent 2 can recover the resources from the faulty agent 3 and continue the pursuit. The distinct roles of agents 2 and 3 also test the system's ability to handle asymmetric faults.\nNavigation Scenario In the navigation scenario, agents are required to dynamically reschedule between multiple task objectives, testing their ability to allocate resources or targets effectively. When no faults occur, three agents are tasked with covering two landmarks. The challenge in this scenario arises when a fault unexpectedly occurs, requiring the remaining agents to reassess their target landmarks and adjust based on the current states of the agents and the landmarks.\nPatrol Scenario In the patrol scenario, faults introduce entirely new targets and tasks for the agents, challenging the balance of training for tasks before and after a fault occurs. When no faults are present, three agents are tasked with efficiently patrolling the bottom half of the map. However, an enemy agent randomly attacks one of the patrolling agents, causing it to fail. The remaining two agents then detect the location of the faulty agent and initiate a pursuit. The reward r of each agent is composed of several parts:\n$r = r_{com} + r_{out} + r_{col} + r_{dis} + r_{goal}$,\nwhere rcom, rout, col, dis are rewards encouraging the agent to maintain the communication structure, stay within the boundary, avoid collisions with teammates, and approach the target, respectively. These four rewards are individual rewards and are received by each agent independently, whereas rgoal is a team reward, granted to all agents when any one of them completes the task."}, {"title": "B. Results and Analysis", "content": "We use vanilla MADDPG to train the multi-agent system in the basic predator-prey scenario without considering potential faults. Compared to the test in a no-fault environment, the task completion rate drops from 0.872 to 0.382 in the test where agent 2 fails at time step 5. This demonstrates the necessity of a fault-tolerant model.\nAs depicted in Fig. 6, all methods achieve similar results in no-fault scenarios.\nAs can be seen, the distribution of attention is highly consistent with the process shown in the episode example in Fig. 8, indicating that attention effectively guides the actor and critic in handling faults and other information.\nWe select the more challenging patrol scenario to validate the role of PER in further enhancing fault tolerance performance.Vanilla AACFT struggles to learn any effective strategy"}, {"title": "VII. CONCLUSION", "content": "In this paper, we propose a method that combines optimized model architecture with training data sampling strategy to address potential faults in MARL. Our algorithm leverages an attention mechanism to automatically identify faults and dynamically adjust the level of attention given to fault-related information. Simultaneously, a prioritization mechanism is employed to adjust the tendency of sample selection. These features allow the algorithm to automatically and efficiently utilize valuable data, effectively addressing the critical issues of fault tolerance.\nMoreover, we have developed and open-sourced a new platform to support researchers in studying fault tolerance in MARL. Experimental results demonstrate that our method not only addresses the unique problems specific to MARL but also effectively deals with common issues such as communication maintenance and task allocation brought about by faults.\nLooking ahead, we plan to design more types of faults within our platform to examine whether our method can adapt to the effects of different fault information and enhance its generalizability. Concurrently, we will explore the performance of the proposed method in more complex environments to further validate and enhance the method's effectiveness and practicality."}]}