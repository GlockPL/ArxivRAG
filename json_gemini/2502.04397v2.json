{"title": "Multimodal Medical Code Tokenizer", "authors": ["Xiaorui Su", "Shvat Messica", "Yepeng Huang", "Ruth Johnson", "Lukas Fesser", "Shanghua Gao", "Faryad Sahneh", "Marinka Zitnik"], "abstract": "Foundation models trained on patient electronic health records (EHRs) require tokenizing medical data into sequences of discrete vocabulary items. Existing tokenizers treat medical codes from EHRs as isolated textual tokens. However, each medical code is defined by its textual description, its position in ontological hierarchies, and its relationships to other codes, such as disease co-occurrences and drug-treatment associations. Medical vocabularies contain more than 600,000 codes with critical information for clinical reasoning. We introduce MEDTOK, a multimodal medical code tokenizer that uses the text descriptions and relational context of codes. MEDTOK processes text using a language model encoder and encodes the relational structure with a graph encoder. It then quantizes both modalities into a unified token space, preserving modality-specific and cross-modality information. We integrate MEDTOK into five EHR models and evaluate it on operational and clinical tasks across in-patient and out-patient datasets, including outcome prediction, diagnosis classification, drug recommendation, and risk stratification. Swapping standard EHR tokenizers with MEDTOK improves AUPRC across all EHR models, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.30% on EHRShot, with the largest gains in drug recommendation. Beyond EHR modeling, we demonstrate using MEDTOK tokenizer with medical QA systems. Our results demonstrate the potential of MEDTOK as a unified tokenizer for medical codes, improving tokenization for medical foundation models.", "sections": [{"title": "1. Introduction", "content": "Electronic health records (EHRs) are the backbone of modern healthcare, capturing a person's health state with increasing precision across diverse modalities. Structured EHR data, encoded through standardized medical codes, support a wide range of applications, from personalized risk prediction (Goldstein et al., 2016; Yu et al., 2024b) and disease trajectory modeling (Jensen et al., 2017; Heumos et al., 2024) to emulation of clinical trials (Katsoulakis et al., 2024; Kraljevic et al., 2024). The cornerstone of structured EHRs is medical coding systems, which assign standardized alphanumeric codes to various aspects of patient health, including diseases, procedures, medications, and laboratory tests. These codes come from widely used terminologies such as ICD-9, ICD-10, SNOMED CT, CPT, and ATC, among others (Foley et al., 1992; Organization et al., 1988; Organization, 2004; Donnelly et al., 2006; Dotson, 2013; Miller & Britt, 1995). Although essential for interoperability, medical codes introduce challenges for models, particularly in the tokenization process, which transforms structured EHR data into token sequences that foundation models can process.\nTransformer-based models for structured EHRs (Poulain"}, {"title": "2. Related work", "content": "Domain-specific Tokenizers. Tokenizers tailored for specific domains have been employed to process various types of data, including language (Sennrich et al., 2016; Kudo & Richardson, 2018; Song et al., 2021; Wang et al., 2024b; Minixhofer et al., 2024), images (Zhou et al., 2022; Yu et al., 2022; 2024a; Zha et al., 2024), videos (Choudhury et al., 2024), graphs (Perozzi et al., 2024; Yang et al., 2024a), and molecular and material sciences (Fu et al., 2024; Tahmid et al., 2024; Qiao et al., 2024). While these tokenizers perform well within their respective domains, they are not directly applicable to medical codes, which contains spe-"}, {"title": "3. Approach", "content": "MEDTOK is a multimodal medical tokenizer that leverages both text descriptions and relational contexts of medical codes. MEDTOK operates as a tokenization function $f(\u00b7)$ that maps a medical code $m \u2208 M$ to a sequence of elements $T$ in the vocabulary $V$ with a size of $N$ by leveraging both its textual definition $D(m)$ and a subgraph $G(m)$ extracted from a biomedical knowledge graph $G$. Here, $M$ is a set of 617,490 medical codes from eight medical coding systems: ICD-9, ICD-10-CM, ICD-10-PCS, SNOMED CT, ATC, NDC, CPT, and RxNORM.\nProblem definition. We formulate our problem as follows. Our goal is to train a multimodal tokenizer $f(.)$ so that $T = f(D(m),G(m))$, where $T = [t_1, t_2, ..., t_r]$ and $t_i \u2208 V, 1 \u2264 i \u2264 T$. Then the generated $T$ for medical code $m$ could be integrated to any EHR-based models $h(\u00b7)$ and LMs models $p(\u00b7)$ to perform predictive or generative tasks.\nOverview. Fig. 2 illustrates the architecture of MEDTOK, which takes both the medical code description and contextual knowledge from biomedical KGs as input. MEDTOK takes two steps, multimodal tokenization and token packing."}, {"title": "3.1. Multimodal Tokenization", "content": "Given a medical code m, paired with its description t and its biological subgraph G, MEDTOK first adopts the text encoder, denoted as Et and the graph encoder, denoted as Eg, to generate two embeddings: the text semantic embedding $x_t \u2208 R^{d_t}$ for t and the graph-level embedding $x_g \u2208 R^{d_g}$ for G. These embeddings are computed as $x_t = E_t(t)$ and $x_g = E_g(G)$ for G, where Et and Eg represent the text and graph encoders, respectively.\nModality-specific Embeddings. MEDTOK then adopts two linear projectors: $f_t : R^{d_t} \u2192 R^d$ and $f_g : R^{d_g} \u2192 R^d$, to generate modality-specific embeddings $e_t^f \u2208 R^d$ and $e_g^f \u2208 R^d$, respectively, where $e_t^f = f_t(x_t), e_g^f = f_g(x_g)$, and d is the dimension of specific embeddings.\nCross-modality Embeddings. Moreover, MEDTOK incorporates a cross-attention module to derive cross-modality embeddings $e_t^c \u2208 R^d$ and $e_g^c \u2208 R^d$, Specifically, the embedding $e_t^c$ is computed as:\n$e_t^c = softmax(\\frac{W_q x_t (W_k x_g)^T}{\\sqrt{d}})(W_v x_g)$ (1)\nSimilarly, the embedding $e_g^c$ is given by:\n$e_g^c = softmax(\\frac{W_q x_g (W_k x_t)^T}{\\sqrt{d}})(W_v x_t)$ (2)\nwhere $W_q, W_k, W_v \u2208 R^{d\u00d7d}$ represents the query, key, and value weight matrix.\nTokenization. After generating modality-specific and cross-modality embeddings, for each embedding, MEDTOK quan-"}, {"title": "3.2. Token Packing", "content": "Unlike image-text paired data, where modalities have significant overlap, the two modalities (text and graph) of medical codes used in this work are more distinct yet complementary: the text focuses on clinical definitions, while the graph encodes domain-specific relationships not fully conveyed through text alone. Therefore, in addition to capturing shared information, it is crucial to extract modality-specific information during the tokenization process to ensure that the resulting tokens are highly informative.\nInspired by (Wang et al., 2024a), we optimize the obtained tokens $(I(e_t^f), I(e_g^f), I(e_t^c), I(e_g^c))$ by separately optimizing the shared and modality-specific information across these tokens and their corresponding quantized vectors.\nFor shared information, given the tokens $I(e_t^f)$ and $I(e_g^f)$, the objective first is to adopt Kullback-Leibler (KL) divergence optimize them by ensuring their distance matrices $dist(e_t^f, C)$ and $dist(e_g^f, C)$ follow a similar distribution, as following:\n$L_{KL} = D_{KL}(softmax(-dist(e_t^f, C)) || softmax(-dist(e_g^f, C)))$\nThen optimize the quantized vectors $\\hat{e_t^f}$, $\\hat{e_g^f}$ to be highly informative about the other modality, while minimizing redundancy, as follows:\n$\\hat{e_t^f}^* = arg \\underset{\\hat{e_t^f}}{max}(I(\\hat{e_t^f}; e_g^f) \u2013 \u03b2I(\\hat{e_t^f}; e_t^f|e_g^f))$ (6)\n$\\hat{e_g^f}^* = arg \\underset{\\hat{e_g^f}}{max}(I(\\hat{e_g^f}; e_t^f) \u2013 \u03b2I(\\hat{e_g^f}; e_g^f|e_t^f))$ (7)\nFor specific information, given the tokens $I(e_t^c)$ and $I(e_g^c)$, the objective is to optimize these tokens by ensuring that the quantized vectors $\\hat{e_t^c}$, $\\hat{e_g^c}$ retain as much modality-specific information as possible, with minimal shared information, as follows:\n$\\hat{e_t^c}^* = arg \\underset{\\hat{e_t^c}}{max} (I(\\hat{e_t^c}, \\hat{e_g^c}; e_t^c) \u2013 \u03bbI(\\hat{e_t^c}; e_g^c)^*))$ (8)\n$\\hat{e_g^c}^* = arg \\underset{\\hat{e_g^c}}{max} (I(\\hat{e_g^c}, \\hat{e_t^c}; e_g^c) \u2013 \u03bbI(e_t^c; \\hat{e_g^c}^*))$ (9)\nBased on the derivation of (Wang et al., 2024a), the common tokens could be optimized by the combination of InfoNCE loss between $\\hat{e_t^f}$ and $\\hat{e_g^f}$, and the alignment loss between $e_t^f$ and $e_g^f$. Then the loss for packing shared information across two modalities is formulated by: $L_{token}^c = L_{InfoNCE}(\\hat{e_t^f}, \\hat{e_g^f}) + L_{orthogonal}(e_t^f, e_g^f)$. Additionally, the modality-specific tokens could be optimized by the combination of InfoNCE loss between $\\hat{e_t^c}, \\hat{e_g^c}$ and the quantized vectors of their augmented data, and the orthogonal loss between $\\hat{e_t^c}, \\hat{e_g^c}$, and $e_t^c, e_g^c$, respectively. Then the loss for packing specific information across two modalities is formulated"}, {"title": "3.3. Training and Inference for MEDTOK", "content": "Training Stage. During the training stage, MEDTOK is trained by the sum of codebook loss $L_c$, KL divergency loss $L_{KL}$, loss for packing common and specific tokens across two modalities $L_{token}$, where $L = L_c + L_{KL} + L_{token}$.\nInference Stage. After pre-training, MEDTOK can be seamlessly integrated into any model or pipeline dealing with medical codes, providing unified medical tokens for downstream tasks."}, {"title": "4. Experiments", "content": "Medical coding systems. We collected a total of 617,490 medical codes from eight commonly used coding systems: ICD-9 (Organization et al., 1988), ICD-10-CM (Fung et al., 2020), ICD-10-PCS (Averill et al., 2001), SNOMED \u0421\u0422 (Donnelly et al., 2006), ATC (Miller & Britt, 1995), NDC (Palmer, 2006), CPT (Dotson, 2013), and RxNORM (Nelson et al., 2011), as shown in Table 1. These codes cover various events, including procedures, diagnoses, and medications. Each code is paired with a textual description from official documents and a subgraph from PrimeKG (Chandak et al., 2023). Details for data pre-processing are available in Appendix A.\nPatient EHR datasets. We used three publicly available EHR datasets: MIMIC-III (Johnson et al., 2016), MIMIC-IV (Johnson et al., 2024), and EHRShot (Wornow et al., 2023). MIMIC-III and MIMIC-IV are in-patient datasets with medical records for ICU patients, while EHRShot is a dataset containing longitudinal medical records that include both out-patients and ICU/ED patients. MIMIC datasets include NDC medications and ICD-9 / ICD-10 codes for diagnoses and procedures. In contrast, EHRShot mainly uses RxNorm codes for medications, SNOMED codes for"}, {"title": "4.1. MEDTOK tokenizer with in-patient EHR models", "content": "Table 3 presents the AUPRC values for each baseline and their integration with our MEDTOK for five tasks in two in-patient datasets. Compared to baselines that treat each medical code as an individual token, integrating our MEDTOK consistently improves performance across all five tasks, achieving an average improvement of 3.29% on MIMIC-III and 2.67% on MIMIC-IV. This improvement comes from more informative tokens generated by MEDTOK, which strengthen the EHR-based models. Among five tasks, MEDTOK demonstrates the most significant impact on drug recommendation tasks, highlighting the value of incorporating prior knowledge into our tokenizer.\nTo further assess the effectiveness of MEDTOK, we compare it against two tokenization methods: the text-based BERT tokenizer and the graph-based VQGraph tokenizer. Figure 3 presents the performance of each tokenizer when integrated with a Transformer-based EHR model (TransformEHR) across five tasks on two in-patient datasets. MEDTOK consistently outperforms BERT and VQGraph in all tasks and datasets, demonstrating the superiority of its tokenization strategy."}, {"title": "4.2. MEDTOK tokenizer with out-patient EHR models", "content": "Table 4 presents the AUPRC values for each baseline and its integration with MEDTOK across two task types on the outpatient HRShot dataset. The results reveal that our tokenizer has the most significant impact on mortality prediction in Operational Outcomes, achieving an average improvement of 11.30%. It also significantly improves the detection of new diagnoses of Acute MI, with an average improvement of 8.80%. As shown in Fig. 3, a comparison of three types of tokenizers further demonstrates the effectiveness of MEDTOK in integrating both graph and textual modalities. Additionally, when comparing performance across two in-patient datasets, we observe that MEDTOK is particularly beneficial for longitudinal data."}, {"title": "4.3. Ablation studies", "content": "In the ablation studies, to eliminate potential bias from different model architectures, we integrate MEDTOK with a vanilla Transformer-based model (e.g., TransformEHR) to examine the impact of the adopted modalities and the vocabulary size in MEDTOK on performance.\nEffects of modalities on MEDTOK. To evaluate the impact of the two modalities (text, graph) used in MEDTOK -medical code definitions and biological subgraphs derived from a biomedical knowledge graph - we assess its performance by removing the text and graph modalities separately. As shown in Fig. 4, MEDTOK, when leveraging both modalities, achieves the best performance across all tasks on"}, {"title": "4.4. Using MEDTOK tokenizer for medical QA", "content": "MEDTOK demonstrates strong performance in EHR-based tasks, as shown in Tables 3-4. To further assess its capabilities, we explore its effectiveness in a generation task, specifically multiple-choice medical question answering (MedicalQA), where the goal is to select the correct answer to a given clinical question (Singhal et al., 2023). We evaluate whether MEDTOK enhances few-shot learning in MedicalQA by integrating its tokenized representations with a LLM (LLaMA3.1-8B (Dubey et al., 2024)). MEDTOK-generated tokens are used as prefix tokens, which provide structured medical context before the main input, allowing the LLM to incorporate additional domain knowledge.\nFor this evaluation, we use the MedDDx dataset (Su et al., 2024), which contains questions at three difficulty levels: Basic, Intermediate, and Expert. The process consists of three steps: (1) Disease code mapping \u2013 Extract disease mentions from each question and retrieve their corresponding medical codes. (2) Tokenization via MEDTOK - Convert medical codes into structured tokens using MEDTOK. (3) Prefix Token Fine-Tuning \u2013 Fine-tune LLaMA3.1-8B using MEDTOK tokens as prefix inputs before the question text. We fine-tune the model on 617 intermediate-level questions and evaluate performance on 227 expert-level and 158 basic-level questions. The results in Figure 5 show an accuracy improvement of 0.3% on MedDDx-Basic and 2.3% on MedDDx-Expert, suggesting that MEDTOK can enhance"}, {"title": "5. Conclusion", "content": "Tokenizing medical codes is a critical yet challenging step in developing foundation models for electronic health records (EHRs). Existing tokenizers treat medical codes as isolated textual units, failing to capture their structured relationships within large-scale medical ontologies. With more than 600,000 codes that span multiple terminologies, standard tokenization methods struggle to scale while preserving the rich semantic and relational context necessary for downstream clinical and operational tasks. We introduced MEDTOK, a multimodal tokenizer of medical codes that integrates textual definitions and relational ontologies of medical codes to create a unified token representation. MEDTOK applies vector quantization to encode both modalities in a structured token space, preserving cross-modality relationships. We integrated MEDTOK with five EHR models, evaluating its impact across inpatient (MIMIC-III, MIMIC-IV) and outpatient (EHRShot) settings, as well as in fine-tuning a medical question-answering system. Our results establish MEDTOK as a generalizable tokenizer for medical codes, shedding light on how optimizing the tokenization process can benefit medical foundation models."}, {"title": "A. Data preprocessing details", "content": "The medical codes dataset consists of medical codes, their descriptions, and associated knowledge subgraphs, encompassing eight commonly used health coding systems: ICD-9-CM (procedures and diagnoses), ICD-10-CM, ICD-10-PCS, NDC (National Drug Codes), SNOMED CT, ATC (Anatomical Therapeutic Chemical Classification), CPT (Current Procedural Terminology), and RxNorm. All code lists were obtained from official sources. Specifically, ICD-9 and ICD-10 (CM and PCS) were sourced from the CMS website; NDC codes from the U.S. Food and Drug Administration (FDA) database; and CPT (Level I HCPCS) from the Physician Fee Schedule (PFS) Relative Value Files at CMS. SNOMED CT, RxNorm (active codes only), and ATC were downloaded via the National Library of Medicine (NLM), part of the National Institutes of Health (NIH)."}, {"title": "A.1.1. MEDICAL CODES KNOWLEDGE GRAPHS CREATION", "content": "In the final dataset, each medical code is linked to a knowledge graph capturing relevant medical insights and relationships. We constructed these subgraphs in two steps: mapping each code to one or more nodes in the PrimeKG knowledge graph; and extracting node-centered subgraphs to represent the code's associated knowledge and connections. To facilitate mapping, we leveraged several external resources, notably the UMLS database and MONDO Disease Ontology files. Medical codes were first mapped to Concept Unique Identifiers (CUIs) in the UMLS database, then linked to PrimeKG nodes via a custom UMLS-to-PrimeKG file. Because PrimeKG includes MONDO annotations, we also aligned medical codes to MONDO terms using the mondo.owl file, thus achieving direct integration with PrimeKG nodes. Additionally, a custom entity linker was employed to enhance coverage by translating medical codes into descriptive text (via PyHealth's MedCode InnerMap) and matching these descriptions to PrimeKG node names. When exact matches were unavailable, we resorted to an NLP-based linker (SciSpacy with UMLS) to measure semantic similarity. For drug codes, the rxnav.nlm.nih.gov API was used to map RxNorm codes to ATC identifiers, which were then associated with DrugBank entities through a predefined ATC-to-DrugBank mapping."}, {"title": "A.1.2. MEDICAL CODES TEXTUAL DEFINITION CREATION", "content": "Initially, each medical code's description was taken from its official source. For medication codes (e.g., NDC) where the original text was sparse, additional details were derived from attributes such as trade name, proprietary name, and pharmacological classification. These preliminary definitions were then refined and enriched using ChatGPT-4 (turbo), with prompts tailored to each coding system but sharing a common goal of elaborating on clinical uses (for drugs), procedural steps (for procedures), or mechanistic and clinical context (for diagnoses)."}, {"title": "B. Implementation details", "content": "Hardware. MEDTOK is training on a machine equipped with 4 NVIDIA H100. All experiments were conducted with 1 NVIDIA H100.\nSoftware. We implement MEDTOK using Python 3.9.19, PyTorch 2.3.1, Transformers 4.43.1, and Tokenizers 0.19.1. All LMs and LLMs adopted in this study are downloaded from Hugging Face, except for OpenAI models."}, {"title": "B.2. Details in MEDTOK training", "content": "MEDTOK is trained on 4 NVIDIA H100 GPUs by using the loss defined in the Section 3.2. During the training stage, we set the training step as 3000 with a global batch size of 1024, the dimension of quantized vectors is 64. In terms of the models' weights, we freeze the text encoder in MEDTOK and the graph encoder is trainable during the training stage."}, {"title": "B.3. Implementation details of baseline models", "content": "All results presented in this study were obtained using the same machine on which the MEDTOK was trained.\nETHOS experiments were conducted using the authors' original repository. For each experimental setting, three models"}, {"title": "C. Task definitions and data preparation under in-patient setting", "content": "Task definition. Mortality (MT) prediction estimates the mortality label of the subsequent visit for each sample, with the last sample dropped. Formally,\n$f: (V_1, V_2,..., V_{t-1}) \u2192 Y[v_t]$,\nwhere y[vt] \u2208 {0, 1} is a binary label indicating the patient's survival status recorded in visit vt."}, {"title": "C.2. Readmission prediction", "content": "Task definition. Readmission prediction checks if the patient will be readmitted to the hospital within o days. Formally,\n$f : (V_1, V_2, ..., V_{t-1}) \u2192 Y[\u03c4(v_t) \u2013 \u03c4(v_{t-1})]$, where y \u2208 {0,1} and \u03c4(vt) denotes the encounter time of visit vt. Specifically,\n$Y[\u03c4(v_t) - \u03c4(v_{t-1})] =\\begin{cases} 1 & \\text{if } \u03c4(v_t) - \u03c4(v_{t-1}) \u2264 \u03c3, \\\\ 0 & \\text{otherwise}. \\\\ \\end{cases}$\nIn our study, we set o = 15 days."}, {"title": "C.3. Length-of-Stay (LOS) prediction", "content": "Task definition. Length-of-Stay (LOS) prediction follows the formulation of Harutyunyan et al., estimating ICU stay length for each visit. Formally, f : (v1, v2, ..., vt) \u2192 Y[vt], where y[vt] \u2208 R1\u00d7C is a one-hot vector indicating its class among C possible categories. We define 10 classes, {0, 1, ..., 7, 8, 9}, representing the following durations: 0 for one day or less, 1-7 for within one week, 8 for one to two weeks, and 9 for at least two weeks."}, {"title": "C.4. Phenotype prediction", "content": "Task definition. Phenotype prediction aims to classify which acute care conditions are present in a given patient record:\nf : (V_1, V_2, ..., V_t) \u2192 y[v_t], where y[vt] \u2208 R1\u00d7C is a one-hot vector indicating its class among C possible categories. This task is a multilable classification problem with macro-averaged AUC-ROC being the main metric."}, {"title": "C.5. Drug recommendation", "content": "Task definition. Drug recommendation aims to recommend drugs for a patient according to the patient's visit history and diagnosis in current visit: f : (V_1, V_2, ..., V_t) \u2192 y[v_t], where y[vt] \u2208 R1\u00d7C is a one-hot vector indicating its class among C possible categories. This task is a multilable classification problem with macro-averaged AUC-ROC being the main metric."}, {"title": "C.6. Out-patient Setting", "content": "Under this setting, we adopt two types of tasks in EHRShot, including operational outcomes prediction and assignment of new diagnosis. In the field of operational outcomes, we follow the same task definitions in long length of stay prediction, which only consider if a patient stay in the hospital less than 7 days or more than 7 days. In terms of readmission task, we set the time window as 15 days, which is the same as that under in-patient setting. We also add another operational outcome tash, which is mortality prediction. The definition of mortality prediction is the same as that under the in-patient setting."}]}