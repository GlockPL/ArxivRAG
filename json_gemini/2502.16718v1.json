{"title": "NatSGLD: A Dataset with Speech, Gesture, Logic, and Demonstration for Robot Learning in Natural Human-Robot Interaction", "authors": ["Snehesh Shrestha", "Yantian Zha", "Saketh Banagiri", "Ge Gao", "Yiannis Aloimonos", "Cornelia Ferm\u00fcller"], "abstract": "Recent advances in multimodal Human-Robot Interaction (HRI) datasets emphasize the integration of speech and gestures, allowing robots to absorb explicit knowledge and tacit understanding. However, existing datasets primarily focus on elementary tasks like object pointing and pushing, limiting their applicability to complex domains. They prioritize simpler human command data but place less emphasis on training robots to correctly interpret tasks and respond appropriately. To address these gaps, we present the NatSGLD dataset, which was collected using a Wizard of Oz (WoZ) method, where participants interacted with a robot they believed to be autonomous. NatSGLD records humans' multimodal commands (speech and gestures), each paired with a demonstration trajectory and a Linear Temporal Logic (LTL) formula that provides a ground-truth interpretation of the commanded tasks. This dataset serves as a foundational resource for research at the intersection of HRI and machine learning. By providing multimodal inputs and detailed annotations, NatSGLD enables exploration in areas such as multimodal instruction following, plan recognition, and human-advisable reinforcement learning from demonstrations.", "sections": [{"title": "I. STUDY OVERVIEW", "content": "Human communication often involves the concurrent use of language and gestures, as noted by Tomasello ( [1], p. 230). Language-based communication conveys explicit knowledge, gestures excel at expressing tacit, context-dependent information, and together they allow people to express with efficient clarity. For tasks involving both explicit and tacit knowledge, such as food preparation, cooking, and cleaning, this kind of hybrid communication approach becomes essential, especially in natural human-robot interactions\u00b9. Enabling robots to collaborate with humans in a natural way can significantly reduce cognitive load for humans. For example, as shown in Fig. 1, a human could simply say, \"Cut that into pieces\", while pointing at a potato. The gesture conveys tacit knowledge, such as the location of the potato, in a way that is intuitive for both the human and the robot. Similarly, a command like \u201cPlease stir that\" accompanied by an enactment of stirring, communicates not only what to stir but also how - slowly, gently, rapidly, or in specific directions. In these natural interactions, humans often focus on the critical subtasks and assume the robot possesses the commonsense knowledge to handle the rest. For instance, a human might assume the robot knows it needs to pick up a knife before cutting the potato. This requires the robot to predict what the human expects it to do based on multimodal inputs (speech and gestures), the current observation of the environment, and translating these into symbolic task representations, such as temporal logical formulas. This alignment with human expectations allows the robot to generate motion trajectories that effectively carry out the task. However, most existing Human-Robot Interaction (HRI) datasets either focus exclusively on speech (e.g., Google Home, Amazon Alexa) [2]\u2013[6], or solely on gestures [7]-[15]. Some combine speech and gestures but are limited to perception tasks such as object recognition or manipulation [16]-[19]. These datasets do not fully capture the complexity of natural, multimodal communication in real-world tasks. To address this gap, we introduce the NatSGLD dataset alongside a simulator built on the Unity game engine (described in Fig. 1 and Fig. 2). This dataset enables our simulated robot to understand natural human speech and gesture commands for complex tasks such as food processing and cooking. We annotated these multimodal commands with expert-teleoperated robot demonstrations within the simulator and ground-truth understanding of participant-commanded tasks as Linear Temporal Logic (LTL) formulas\u00b2. We employed a Wizard of Oz (WoZ) method [20], where participants interacted with a robot they believed to be autonomous. NatSGLD includes 1,143 multimodal commands from 18 participants across 11 activities, 20 objects, and 16 object states. The dataset provides a rich collection of data, including robot states, participants' speech transcripts, gestures,"}, {"title": "II. METHODS", "content": "Current robots are unable to perform kitchen tasks at real-time speeds that humans naturally expect from one another. Interacting with slowly-moving robots can influence how human participants command them, affecting their choice of language and gestures. To address this, we developed a real-time photorealistic simulator for HRI research, built using Unity3D [57] and integrated with a customized ROS plugin [58]. Our simulator facilitates real-time execution of robotic tasks, thereby enhancing its applicability for future HRI studies.\nOur simulator is designed to handle intricate, multi-step tasks commonly encountered in real-world scenarios, such as cutting or pouring. For instance, during a task like cutting a tomato, the robot must locate and grasp the knife, stabilize itself, and perform a series of cuts. Although perfect simulation of such activities remains an open challenge, our simulator leverages state-of-the-art techniques to represent cutting by dynamically replacing object meshes with segmented parts once cut actions are initiated. Additionally, the simulator generates a variety of sensor data types essential for robot learning, including multi-camera RGB image views, depth images, semantic and instance segmentation maps (Fig. 2). These outputs, essential for training vision-based models, enhance the simulator's utility for learning complex tasks through multimodal inputs.\nThe simulator ensures seamless real-time interaction between participants and the robot. It operates on a high-performance computer (Intel i7, 16GB RAM) connected to a 55\" display, providing smooth processing and rendering. A live camera feed of participants is overlaid on the screen, giving the impression that the robot can \"see\" them, enhancing immersion. The robot maintains eye contact with humans when listening to them and focuses on objects while performing actions. The simulator captures both human and robot perspectives, providing comprehensive"}, {"title": "A. Simulation Development"}, {"title": "B. Data Collection", "content": "The study, approved by our university's Institutional Review Board, involved 18 participants (aged 18 to 31 years, Mean: 21, SD: 4) with equal gender distribution (9 male, 9 female) and a mix of technical (44%) and non-technical (56%) backgrounds. None had prior experience interacting with robots.\nTo ensure the naturalness of human behavior in our study, we implemented a Wizard-of-Oz (WoZ) experimental design [20], where participants were led to believe they were interacting with a fully autonomous and capable robot. To enhance this illusion, participants first watched a video demonstrating the robot performing complex tasks autonomously. During the practice session, they issued commands to the robot, which it seamlessly executed without errors, reinforcing the perception of its autonomy. Unbeknownst to them, a concealed operator controlled the robot's actions in real time (see Fig. 3), ensuring precision while maintaining the illusion of autonomy. This approach fostered spontaneous and authentic interactions between participants and the robot.\nChallenging tasks like food preparation and cooking, commanded by participants, often involve both sequential and concurrent actions. LTL formally captures these temporal relations [59], which is crucial for tasks like \u201cChop the vegetables while preheating the oven\". Additionally, LTL offers a compact, flexible task representation that can be converted into automata [60], [61], allowing the extraction of symbolic plans. Thus, we use LTL to annotate the ground-truth interpretation of participants' multimodal commands for training robots. In the example from Fig. 1, the derived LTL formula is:\nAlongside LTL, we collected expert-teleoperated demonstrations to train robots at the control level by capturing detailed sensor data and joint controls. This is crucial for training and evaluating robot decision-making models. The combination of LTL and demonstrations enables the dataset to assess task understanding and motion execution across both simple and complex scenarios. To collect demonstrations, an expert teleoperated the robot head and end-effector, and the detailed joint angles were generated using real-time inverse kinematics (IK [62], [63]). All the sensor data and robot joint control outputs at each step were recorded.\nParticipants' speech was transcribed manually by the researchers and encoded using the Glove model [64] and BART [65] to extract embeddings, while gestures were captured as a sequence of body poses using pose estimation algorithm OpenPose [3]. Each multimodal command was annotated with a human expert in Linear Temporal Logic (LTL), reflecting the task's temporal structure and the subtasks inferred by the human based on both speech, gestures, and context.\nTo ensure fairness and minimize biases, we accounted for participant demographics, including gender, age, and personality traits. Data on participants' experiences, such as workload assessments via NASA-TLX [66], were also collected. These factors provide insights into how different participants perceive and interact with robots, which is critical for developing unbiased, generalizable human-robot interaction models."}, {"title": "1) Wizard-of-Oz. Experiment Design:"}, {"title": "2) Linear Temporal Logic (LTL):", "content": "Challenging tasks like food preparation and cooking, commanded by participants, often involve both sequential and concurrent actions. LTL formally captures these temporal relations [59], which is crucial for tasks like \u201cChop the vegetables while preheating the oven\". Additionally, LTL offers a compact, flexible task representation that can be converted into automata [60], [61], allowing the extraction of symbolic plans. Thus, we use LTL to annotate the ground-truth interpretation of participants' multimodal commands for training robots. In the example from Fig. 1, the derived LTL formula is:\nX (G (C_Potato U Potato_FarFrom_CT) & G F\n(Potato_OnTopOf_CB & Potato_CloseTo_CB) & X (G\n(C_Knife U Knife) & G (C_Knife U Knife_FarFrom_CT)\n& G (C_Knife U Knife_OnTopOf_Potato) & G ((C_Potato &\nKnife_CloseTo_Potato) U Potato_Pieces)))"}, {"title": "3) Robot Demonstrations:", "content": "Alongside LTL, we collected expert-teleoperated demonstrations to train robots at the control level by capturing detailed sensor data and joint controls. This is crucial for training and evaluating robot decision-making models. The combination of LTL and demonstrations enables the dataset to assess task understanding and motion execution across both simple and complex scenarios. To collect demonstrations, an expert teleoperated the robot head and end-effector, and the detailed joint angles were generated using real-time inverse kinematics (IK [62], [63]). All the sensor data and robot joint control outputs at each step were recorded."}, {"title": "4) Post-processing and Annotation:", "content": "Participants' speech was transcribed manually by the researchers and encoded using the Glove model [64] and BART [65] to extract embeddings, while gestures were captured as a sequence of body poses using pose estimation algorithm OpenPose [3]. Each multimodal command was annotated with a human expert in Linear Temporal Logic (LTL), reflecting the task's temporal structure and the subtasks inferred by the human based on both speech, gestures, and context."}, {"title": "5) Ethical and Bias Considerations:", "content": "To ensure fairness and minimize biases, we accounted for participant demographics, including gender, age, and personality traits. Data on participants' experiences, such as workload assessments via NASA-TLX [66], were also collected. These factors provide insights into how different participants perceive and interact with robots, which is critical for developing unbiased, generalizable human-robot interaction models."}, {"title": "III. NATSGLD DATASET", "content": "The NatSGLD dataset is organized into three main sections: the simulator, scripts, and data.\nThe simulator is provided as a Unity 3D project, along with instructions for setting up third-party add-ons. It can be controlled using keyboard shortcuts, Python scripts, and ROS messages. Robot trajectories can also be replayed by specifying the trajectory records included in the dataset.\nThere are four types of scripts: a) experiment execution scripts, b) data collection scripts, c) post-processing scripts (synchronization, extraction, parsing, compression), and d) analysis and annotation tools for replaying datasets and modifying annotations (e.g., speech, gestures, tasks, robot social behaviors like robot's eye contact, nodding, and confusion).\n\u2022 Metadata (.csv): The metadata, stored as a CSV file, serves as the primary database. Each record corresponds to a command issued by participants, identified by a unique global record number (DBSN) and participant ID (PID). The sequence number of commands within a session is recorded as SSN. The onset (start time, ST) and offset (end time, ET) of each command are provided in milliseconds from the start of the experiment, and the presence of speech or gestures is indicated by a boolean value (1 for presence, 0 for absence).\nVideos (.mp4): Organized into subfolders by participant (e.g., P41, P45, P50), each folder contains various views:  - Participant View: Audio at 44kHz and video at 30fps - Experiment View: Video at 30fps  - Multicam Room View: Video at 30fps"}, {"title": "A. Simulator"}, {"title": "B. Scripts", "content": "There are four types of scripts: a) experiment execution scripts, b) data collection scripts, c) post-processing scripts (synchronization, extraction, parsing, compression), and d) analysis and annotation tools for replaying datasets and modifying annotations (e.g., speech, gestures, tasks, robot social behaviors like robot's eye contact, nodding, and confusion)."}, {"title": "C. Data", "content": "\u2022 Metadata (.csv): The metadata, stored as a CSV file, serves as the primary database. Each record corresponds to a command issued by participants, identified by a unique global record number (DBSN) and participant ID (PID). The sequence number of commands within a session is recorded as SSN. The onset (start time, ST) and offset (end time, ET) of each command are provided in milliseconds from the start of the experiment, and the presence of speech or gestures is indicated by a boolean value (1 for presence, 0 for absence).\nVideos (.mp4): Organized into subfolders by participant (e.g., P41, P45, P50), each folder contains various views:  - Participant View: Audio at 44kHz and video at 30fps - Experiment View: Video at 30fps  - Multicam Room View: Video at 30fps"}, {"title": "IV. USAGE NOTES", "content": "Access the NatSGLD simulator, dataset, and scripts at https://github.com/sneheshs/natsgld. The software was verified in Window 10, Mac OS Monterey, and Ubuntu 20.04. The repository contains detailed documentation including dependencies, installations instructions, scripts for execution, data collection, post-processing, annotating, and data loading."}, {"title": "V. ACKNOWLEDGMENTS", "content": "The work was supported by NSF grant OISE 2020624. We thank the peers and faculty members from the University of Maryland for their feedback, especially Dr. Vibha Sazawal, Dr. Michelle Mazurek, Vaishnavi Patil, and Lindsay Little. We thank the contributions of the students listed in Appendix G."}, {"title": "APPENDIX A", "content": "During recruitment, we aimed for balanced demographics in gender, age, and personality traits. We initially screened for diversity in gender and age and adjusted recruitment to address any underrepresented groups. Although we didn't screen for personality traits upfront, we collected self-reported personality data and reviewed it post-collection and validated that the participants had a mix of traits to avoid potential bias."}, {"title": "APPENDIX B", "content": "To balance the spontaneity of natural behavior with the control of a lab study, we carefully designed this Wizard-of-Oz (WoZ) experiment to make participants believe the robot was fully autonomous, ensuring they used natural speech and gestures spontaneously. Upon arrival, participants were led through steps to reinforce this belief, including:\na) clearly informing them that the robot was capable of autonomous operation,\nb) showing videos of Baxter performing kitchen tasks, and\nc) allowing them to experience Baxter responding to freestyle commands during a practice session.\nAt the end of the experiment, participants were debriefed on the WoZ design, its purpose, and informed that the robot was remotely tele-operated.\nWe conducted multiple pilot studies to validate factors that could affect participant behavior to validate independent and dependent control variables as well as the workflow. We experimented with the (a) background noise, (b) perceived robot personality and capability based on the robot's face and name, (c) staging to keep the participants engaged, (d) considerations of the priming effects from practice sessions, (e) WoZ clues that participants might be able to use to figure out the hidden agenda, and (f) the effect of experiment instructions. These findings informed our experiment design decisions."}, {"title": "APPENDIX C", "content": "It is important to clean up the data and make it easy for researchers to use the data. Careful iterative steps were taken to prepare the data to ensure integrity, quality, validity, and fairness. The raw data is processed, annotated, validated, visualized, and curated for downstream analysis and machine learning tasks in the following way.\ni) Multi-camera Calibration: A standard 12\u00d78 5\" checker board was recorded using ROS, and Kalibr package [74] to compute the cameras intrinsic and extrinsic matrix. If the average re-projection error was greater than 1 px., the calibration was repeated.\nii) Multi-camera Audio-Video Synchronization and Data Compression: All the data was recorded using ROS bag. These recorded video frames from each cameras tend to have dynamically varying frames per second rates anywhere from 25 fps to 32 fps which makes it difficult to synchronize with sound. For this reason, the audio recording is extracted from a well established audio-video camera such as Apple iPhone camera. A flashing color screen from another computer is placed in the middle of the lab within all the cameras' field of view. A ROS start message is also published to store and identify the starting flag of the session for all other data. The changing color from red to blue is used to denote the mark of the starting frame and the ROS bag start message time is used for offsetting other messages. The frames are then streamed to a canvas that is 6\u00d7 the size of 720p i.e., 2560\u00d72560 where"}, {"title": "APPENDIX D", "content": "There are eleven distinct subtasks are featured in NatSGLD dataset (in alphabetical order):\n\n\nTake Off Transfer"}, {"title": "APPENDIX E", "content": "Objects are categorized into twenty groups, which are:\n\u2022 Food Ingredients: Carrot, Celery, Pepper, Potato, Salt, Soup, Spices, Oil, Tomato, Veges\n\u2022 Cooking Utensils and Tools: Cutting Board, Knife, Bowl, Spatula, Ladle.\n\u2022 Cookware: Lid, Pan, Pot.\n\u2022 Kitchen Appliances and Fixtures: Stove, Sink.\nObject states and attributes include:\n\u2022 States: Whole, Cut (Pieces), On, In, Out, Covered, Uncovered, Turned On, Turned Off, and Contains.\n\u2022 Location and Rotation: Annotated with six degrees of freedom for location and rotation.\n\u2022 Participant Interaction: Gaze as rotation angles and pointing gestures presence are indicated."}, {"title": "APPENDIX F", "content": "We present the problem of speech-gesture-conditioned task understanding as a translation task that converts a pair of speech and gestures into an LTL formula. This can be seen as a variant of the multi-modal machine translation problem. In this formulation, the inputs consist of the following components:\n1) Source Language Vocabulary Vsource: This set encompasses words in the source language Lsource, which serves as the language in which human instructions are provided.\n2) Gesture Contexts Z: These sequences are derived from human skeleton data and encapsulate diverse gestures made during interactions.\n3) Target LTL Vocabulary Vtarget: This set encompasses LTL symbols in the target language Ltarget, which is used to formulate LTL task descriptions.\nL(x, z; \u03b8) = \u2212\u2211 CrossEntropy (\u0177t (x, z, Yt\u22121), yt; \u03b8) \nto determine if predicted and ground-truth formulas f1 and f2 are equivalent, we employ the spot.are_equivalent () function, illustrated in the following link\u00b3. We provide a sample Python code snippet for reference:\nSpot-Score = \u2211 spot. are_equivalent(fi, gi)\nThere exists no previous work that specifically addresses the task of learning a mapping from combined natural language sentences and gestures to an LTL formula. The closest work are [76], [77], which tackles the challenge of translating natural language sentences into LTL formulas. Given this unique context, we establish two baseline models for comparison, each focusing on using either speech or gestures independently.\nRelevant works in logic formula inference have used Jaccard Similarity [78], [79] as their evaluation metrics. Jaccard Similarity, defined as J(A, B) = , measures the similarity between two sets by comparing their intersection |A \u2229 B| to their union |AUB|. In evaluating logic formula prediction, Jaccard Similarity quantifies the overlap between the predicted and actual logic formulas, providing a robust assessment by considering the shared elements (true positives) and disregarding non-overlapping elements (false positives and false negatives)."}, {"title": "A. Problem Formulation"}, {"title": "B. Approach", "content": "Our approach, depicted in Fig. 5, entails a framework for acquiring the ability to forecast an LTL formula from a combination of speech and gesture. The architecture employs a two-stream network that is fed both a word sequence (speech) and a series of skeletal representations (gesture context). This dual input is then processed to yield a sequence of LTL symbols as output. Our training process involves optimizing the network through a cross-entropy loss, in accordance with Eq. 1. The Encoder A is implemented by using a pretrained large language model BART [65]. The Encoder B is pretrained by encoding and reconstructing our gesture data."}, {"title": "C. Baselines and Settings", "content": "There exists no previous work that specifically addresses the task of learning a mapping from combined natural language sentences and gestures to an LTL formula. The closest work are [76], [77], which tackles the challenge of translating natural language sentences into LTL formulas. Given this unique context, we establish two baseline models for comparison, each focusing on using either speech or gestures independently.\nRelevant works in logic formula inference have used Jaccard Similarity [78], [79] as their evaluation metrics. Jaccard Similarity, defined as J(A, B) = , measures the similarity between two sets by comparing their intersection |A \u2229 B| to their union |AUB|. In evaluating logic formula prediction, Jaccard Similarity quantifies the overlap between the predicted and actual logic formulas, providing a robust assessment by considering the shared elements (true positives) and disregarding non-overlapping elements (false positives and false negatives)."}, {"title": "D. Results and Analysis", "content": "We conduct a comprehensive assessment, combining quantitative and qualitative analyses, to underscore the significance of incorporating speech and gestures in Multi-Modal Human Task Understanding tasks. In this endeavor, we introduce a comparative analysis between two state-of-the-art large language models (LLMs): T5, developed by Google, and BART, created by Meta. This comparison aims to evaluate their efficacy in predicting Linear Temporal Logic (LTL) formulas from inputs comprising either speech alone or a combination of speech and gestures.\nTables I and II showcase the performance metrics for these models, illustrating the distinct benefits of leveraging both speech and gestures for a deeper understanding of tasks articulated by humans. Our findings indicate that integrating speech with gestures not only enhances model performance but also significantly outperforms the results obtained by relying on a single modality.\nFurthermore, the juxtaposition of T5 and BART models in our analysis yields additional insights into the nuanced ways different Language Model architectures process and interpret multi-modal data. The comparative results presented in Tables I and II affirm the premise that, although speech is the predominant mode of communication, the inclusion of gestures provides indispensable contextual support, enhancing the interaction model's comprehension capabilities."}, {"title": "APPENDIX G", "content": "We thank our peers, faculties, staff, and research assistants from the Perception and Robotics Group (PRG), the University of Maryland Computer Science department (UMD CS), and the University of Maryland Institute for Advanced Computer Studies (UMIACS) for their continued support, their valuable discussions and feedback.\nSpecial thanks to Dr. Nirat Saini, Dr. Virginia Choi, Dr. Chethan Parameshwara, Dr. Nitin Sanket, and Dr. Chahat Deep Singh. We want to thank and recognize the contributions of Aavash Thapa, Sushant Tamrakar, Jordan Woo, Noah Bathras, Zaryab Bhatti, Youming Zhang, Jiawei Shi, Zhuoni Jie, Tianpei Gu, Nathaneal Brain, Jiejun Zhang, Daniel Arthur, Shaurya Srivastava, and Steve Clausen. Without their contributions and support, this work would not have been possible. Finally, a special thanks to Dr. Jeremy Marvel of the National Institute of Standards and Technology (NIST) for his support and advice."}]}