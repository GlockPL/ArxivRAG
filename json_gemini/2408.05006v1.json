{"title": "Enhancing the Code Debugging Ability of LLMs via Communicative Agent Based Data Refinement", "authors": ["Weiqing Yang", "Hanbin Wang", "Zhenghao Liu", "Xinze Li", "Yukun Yan", "Shuo Wang", "Yu Gu", "Minghe Yu", "Zhiyuan Liu", "Ge Yu"], "abstract": "Abstract-Debugging is a vital aspect of software development, yet the debugging capabilities of Large Language Models (LLMs) remain largely unexplored. This paper first introduces DEBUGEVAL, a comprehensive benchmark designed to evaluate the debugging capabilities of LLMs. DEBUGEVAL collects data from existing high-quality datasets and designs four different tasks to evaluate the debugging effectiveness, including BUG Localization, BUG Identification, Code Review, and Code Repair. Additionally, to enhance the code debugging ability of LLMs, this paper proposes a CoMmunicative Agent BaSed DaTa REfinement FRamework (MASTER), which generates the refined code debugging data for supervised finetuning. Specifically, MASTER employs the Code Quizzer to generate refined data according to the defined tasks of DEBUGEVAL. Then the Code Learner acts as a critic and reserves the generated problems that it can not solve. Finally, the Code Teacher provides a detailed Chain-of-Thought based solution to deal with the generated problem. We collect the synthesized data and finetune the Code Learner to enhance the debugging ability and conduct the NeuDebugger model. Our experiments evaluate various LLMs and NeuDebugger in the zero-shot setting on DEBUGEVAL. Experimental results demonstrate that these 7B-scale LLMs have weaker debugging capabilities, even these code-oriented LLMs. On the contrary, these larger models (over 70B) show convincing debugging ability. Our further analyses illustrate that MASTER is an effective method to enhance the code debugging ability by synthesizing data for Supervised Fine-Tuning (SFT) LLMs. All data and codes are available at https://github.com/NEUIR/DebugEval.", "sections": [{"title": "I. INTRODUCTION", "content": "In the realm of software development, code debugging is an indispensable process for ensuring the functionality and reliability of applications [1], [2]. With the complexity of software systems grows, traditional debugging methods, which often rely on heuristics [3], [4] and predefined patterns [5], [6], are reaching their limitations. The emergent ability of Large Language Models (LLMs) [7], [8] has opened up new horizons in automated debugging, offering a more flexible and comprehensive approach to identifying and rectifying code errors [9].\nThe capabilities of LLMs have been extensively explored for code-related tasks such as code generation and translation [10]\u2013[14]. However, their debugging capabilities remain relatively underexplored. Recently, researchers have begun to focus on using LLMs for self-debugging to repair buggy code iteratively [9], [15], [16]. To better evaluate the code debugging ability, researchers are now building benchmarks to assess the code debugging capabilities of LLMs [17], [18]. Nevertheless, existing code debugging benchmarks face two main issues:\n1) They primarily design tasks around code repair, which is insufficient for a comprehensive evaluation of code debugging ability. 2) Constructing buggy code using GPT-4 [19] fails to capture the complexity and diversity of code errors encountered in real development environments.\nTo address these challenges, this paper introduces DE-BUGEVAL, a comprehensive benchmark designed to evaluate the debugging capabilities of LLMs. DEBUGEVAL introduces four tasks: BUG Localization, BUG Identification, Code Review, and Code Repair, which are designed to test the LLMs' ability not only to identify and classify errors but also to provide correct code solutions. These tasks are of different difficulty levels and represent common debugging scenarios in real software development environments, making the evaluation both representative and challenging. Besides, each task in DEBUGEVAL includes Python, C++, and Java. Additionally, DEBUGEVAL incorporates the real user-written buggy codes and GPT-4 generated buggy codes to better simulate real-world software development. While collecting user-written buggy codes, we also implement strict quality control to prevent data leakage in DEBUGEVAL.\nAdditionally, this paper proposes a coMmunicative Agent based daTa rEfinement framework (MASTER), which fo-cuses on improving the debugging ability of Large Language Models (LLMs). To ensure the quality of the Supervised Fine-Tuning (SFT) data, MASTER defines three agents: Code Quizzer, Code Learner, and Code Teacher, which collaborate to synthesize high-quality code debugging data for finetuning the Code Learner. Specifically, the Code Quizzer generates a diverse set of code debugging problems, guiding the LLM to acquire debugging knowledge during SFT. The Code Learner then acts as a critic, evaluating the educational value of the synthesized problems. Problems that the Code Learner answers incorrectly are collected as SFT data, and the Code Teacher provides detailed solutions and explanations for these problems. Finally, we finetune the Code Learner using the synthesized SFT data and develop our NeuDebugger model.\nWe benchmark 13 open-source and closed-source LLMs on DEBUGEVAL and evaluate the overall performance of NeuDebugger. The results are shown in Figure 1, indicating that: 1) Models with 7 billion parameters exhibit relatively weaker debugging capabilities, whereas 70 billion parameter models and closed-source models perform better. 2) The DeepSeek series models demonstrate superior performance, with the open-source model DeepSeek-Coder-V2-0724 outperforming the closed-source model GPT-40-mini. 3) NeuDebugger-DS-6.7B and NeuDebugger-Llama3-8B, based on DeepSeek-Coder-6.7B-Ins and Llama3-8B-Ins respectively, achieve improvements of 27.7% and 4.1% when trained on data synthesized by MASTER, indicating that MASTER can significantly refine SFT data to enhance model performance on debugging tasks.\nFurther analysis reveals that collecting data solely for SFT does not enhance the debugging ability of LLMs [20]. The Code Teacher effectively teaches the Code Student in three tasks: BUG Localization, BUG Identification, and Code Review, by generating the Chain-of-Thought (CoT) [21]. However, CoT outcomes decrease the performance of LLMs in the Code Repair task, as they introduce additional noise and disrupt the code structure. During SFT, different models exhibit distinct learning behaviors. We find that synthesized data can significantly improve the performance of code-oriented LLMs, such as DeepSeek-Coder-6.7B-Ins, than general LLMs, such as Llama3-8B-Int. These experimental findings provide important insights and directions for future research on enhancing the debugging capabilities of LLMs."}, {"title": "II. RELATED WORK", "content": "This section first introduces some backbone models for code understating and generation and then introduces the related work of code debugging and repair.\nCode-Oriented Language Models. To tailor language models for code understanding, related work mainly focuses on pretrained language models and guides them to learn the code semantics of syntax, and idiomatic [22]\u2013[24]. CodeBERT masks tokens of Natural Language (NL) and Program Language (PL) and then asks the pretrained language models to fill-in the masked spans. Then CodeBERT follows the ELECTRA method [25] and pretrains language models to detect whether the tokens are replaced, which helps models to better capture the code semantics [26]. DOBF [27] goes a step further by taking into account the unique attributes of code-related tasks, which focuses more on masking and generating the class, function, and variable names of code segments. CodeT5 [28] continuously pretrains T5 models [29] using the span masking strategy and also refines the masking strategy by focusing more on the identifiers within code. Such a pretraining method asks the T5 [29] model to generate these identifiers, thereby enhancing its ability to identify and understand identifier information in code-related tasks. Furthermore, some researchers also incorporate multi-modal data sources, such as code, comments, and abstract syntax trees (AST), to pretrain language models, which also helps to improve the code understanding ability by aligning the semantic between code semantics and natural language [30], [31].\nRecently, Large Language Models (LLMs), such as Chat-GPT [7] and Llama [8], have demonstrated their emergency ability in dealing with different tasks, especially for code understanding and generation tasks. To enhance the code generation ability, some widely used LLMs, such as Chat-GPT [7], also mix some code data in the pretraining corpus, which has proven its advantage in enhancing the reasoning ability of LLMs [32]\u2013[34]. Some typical code-based LLMs also collect some instruction data of different code-related tasks to supervised finetune LLMs, which significantly improves the code generation ability of LLMs [35]\u2013[37]. Even though LLMs have strong effectiveness in generating code segments, the code segments usually contain bugs [9], decreasing the pass rate of generated codes. To alleviate these problems, the existing efforts primarily concentrate on employing an iterative code repair approach to continuously refine generated code segments [9], [15], [16].\nCode Debugging and Repair. Early debugging models primarily rely on feature-based methods, such as using tem-plates [5], [6], heuristic rules [3], [4], or constraints [38], [39] to correct the buggy codes. However, the effectiveness of these feature-based debugging methods is hard to broaden to correcting different bug errors and dealing with more complex code bugs, because of the limited patterns or rules that need predefinition by researchers.\nWith the development of Pre-trained Language Models (PLMs), the work also follows the pretraining and then finetuning strategy to build the debugging model and deal with various code bugs occurred in real life. For example, Xia et al. [40] use the code-oriented pretrained model, CodeX [41], to explore the capabilities of PLMs in debugging. It shows that CodeX [41] achieves convinced code repair performance, especially on both Python and Java program languages. Kolak et al. [42] also use GPT-2 [43] and CodeX [41] to evaluate their effectiveness in generating the correct patch line when given corresponding code prefix. All the research has proven that the effectiveness of PLM based debugging models mainly thrives on the code understanding ability obtained during pretraining. Different from previous debugging work, recent research focuses more on correcting the bugs generated by LLMs. Self-Debug [9] prompts LLMs to generate the code reviews to aid themselves to refine generated codes, while Self-Repair [44] incorporates human-provided feedback for repairing the buggy code. Furthermore, Self-Edit [15] trains an additional fault-aware editor to repair codes by leveraging the error messages from the test case evaluation and generated code segments. Wang et al. [16] further explore the effectiveness of interactive Chain-of-Repair (CoR), which uses LLMs to generate the"}, {"title": "III. EVALUATING THE DEBUGGING ABILITY OF LLMS WITH DEBUGEVAL", "content": "In this section, we introduce the benchmark DEBUGEVAL, which is built to evaluate the debugging capabilities of Large Language Models (LLMs) from different aspects. We first describe the task definition of the designed task in DEBUGEVAL (Sec. III-A). Then we detail the process of constructing the DEBUGEVAL benchmark (Sec. III-B).\nDEBUGEVAL introduces four different tasks to evaluate the debugging ability of LLMs. The evaluation tasks include BUG Localization, BUG Identification, Code Review, and Code Repair.\nThe data statistics of DEBUGEVAL are shown in Table I. For each task, there are questions in Python, C++, and Java to evaluate LLMs' debugging performance on different programming languages. There are 578, 2320, 2400, and 414 test instances respectively for BUG Localization, BUG Identification, Code Review, and Code Repair tasks, nearly evenly distributed among the three programming languages. In the rest of this subsection, we present the illustrations of these four evaluation tasks in Figure 2 and delve deeper to describe each evaluation task.\nBUG Localization. The BUG Localization task focuses on identifying the specific line(s) of code that contains the error. It evaluates the ability of LLMs to point out the exact location where the error occurs within a code snippet, which is usually regarded as the first step in the debugging process. For each test instance of the BUG Localization task, we give a buggy code P, extract four code snippets {SA, SB, SC, SD} from P, and then ask LLMs to identify the golden code snippet SE, which contains error.\nBUG Identification. In this task, the LLMs should classify the type of error that occurred in the code. Specifically, given a program P with code error(s), we ask LLMs to classify the error type E from four choices, including SyntaxError, ReferenceError, LogicError, and MultiErrors. SyntaxError indicates that the code contains a syntax error. ReferenceError in programming typically occurs when code attempts to access a variable, function, or object that has not been declared or is out of scope. The LogicError represents that the code is usually syntactically correct but contains logical error, not getting the expected output. MultiErrors indicates that the code segment contains various errors of SyntaxError, ReferenceError, and LogicError.\nCode Review. For the Code Review task, we give the correct code Ci and the error code Ei to ask LLMs to distinguish the error one. Specifically, only a few code snippets are different between Ci and E. Moreover, we swap the choice identifiers (A and B) of Ci and Ei in the experiment to avoid any potential bias.\nCode Repair. The Code Repair [16], [17] task requires to generate the corrected version P' for the given buggy code P, which is the ultimate test of the model's debugging capabilities. The code repair task is more difficult among these four debugging tasks. It not only involves code error detection/identification but also needs to correct the error of the code. After generating the corrected code P', we evaluate the correctness of P' by using n test cases X = {(x1, y1), ..., (xn, yn)}. Specifically, we feed the input xi of the test case (xi, yi) to the corrected code P' and then get the execution result P'(xi). If there exists the test case (xi, yi) \u2208 X that satisfies P'(xj) \u2260 yj, it indicates that the code P' still contains errors that need to be addressed. The Code Repair task aims to transform the faulty program P to P' that pass all test cases (\u2200(xi, yi) \u2208 (Xi, Yi), P'(xi) = Yi).\nTo ensure the quality of DEBUGEVAL, we collect high-quality data from reliable data sources, such as Debug-Bench [17] and LiveCodeBench [52], and the AtCoder website."}, {"title": "IV. COMMUNICATIVE AGENT BASED DATA REFINEMENT FRAMEWORK", "content": "Supervised Fine-Tuning (SFT) has been widely adopted to enhance the performance of LLMs in specific domains [37], [57] using human-labeled datasets or LLM-generated data [15]. However, SFT's reliance on the availability and quality of la-beled data limits its overall effectiveness. In this case, this paper introduces the coMmunicative Agent based daTa rEfinement framework (MASTER), which automatically refines code debugging data for Supervised Fine-Tuning. As shown in Figure 3, we give the task examples to LLMs and then prompt LLMs to play different roles to synthesize the SFT data (Sec. IV-A). Finally, MASTER employs different agents to refine the synthesized data to guarantee the quality of the SFT data (Sec. IV-B).\nTo conduct the data refinement, MASTER constructs three agents that collaboratively generate and refine the debugging problem data to synthesize high-quality SFT datasets. As illustrated in Figure 4, we employ different prompts to guide LLMs to play the roles of Code Quizzer, Code Learner, and Code Teacher. The details of each agent are described below.\nCode Quizzer. The Code Quizzer is designed to generate high-quality problems for the SFT data. It uses a stronger LLM as the backbone model and provides the instruction: \"You are a code debugging expert, skilled in generating code debugging problems to challenge programmers\u201d. This setup enables the Code Quizzer to generate tailored problems by analyzing examples of debugging tasks. These problems are intended to evaluate the Code Learner's ability to solve the corresponding debugging tasks.\nCode Learner. The Code Learner shares the same backbone model as the SFT model and serves as the critic to evaluate the educational value of the problems generated by the Code Quizzer. Using the prompt: \u201cYou are a student, please provide an answer to the following code debugging question using your own knowledge\", the Code Learner is tasked with solving the problem based on its memorized knowledge."}, {"title": "V. EXPERIMENTAL METHODOLOGY", "content": "In this section, we describe the Supervised Fine-Tuning (SFT) strategies, evaluation metrics, details of evaluated foundation models, and implementation details of our experiments.\nSupervised Fine-Tuning Strategies. As shown in Table III, we describe the experimental details of different supervised fine-tuning strategies, including Vanilla SFT and MASTER."}, {"title": "VI. EVALUATION RESULTS", "content": "In this section, we benchmark LLMs on DEBUGEVAL and evaluate the overall performance of NeuDebugger. Then we conduct ablation studies and discuss the influence of different SFT data amounts on the model performance. The next experiment explores the effectiveness of NeuDebugger in handling the problems of different code error types. Finally, case studies are presented.\nOverall, larger-scale LLMs exhibit stronger code debugging ability. As indicated by the evaluation results, LLMs exceeding 70B parameters generally demonstrate consistent performance across various debugging tasks. Both the BUG Localization and BUG Identification tasks are multiple-choice questions with four options, where the accuracy of random guessing is approximately 25%. Unfortunately, most 7B-scale LLMs achieve less than 30% accuracy on both tasks. This phenomenon underscores the importance of model scale in maintaining emergent abilities and acquiring critical knowledge through supervised fine-tuning (SFT) on code data [20], [35].\nIn our experiments, we choose DSCoder-6.7B-Ins and Llama3-8B-Ins as the backbone models and then finetuning these two LLMs using the synthesized data generated by MAS-TER to conduct NeuDebugger-DS-6.7B and NeuDebugger-Llama3-8B models, respectively. In contrast to other 7B-scale LLMs, our NeuDebugger significantly enhances the code debugging effectiveness of foundation models and achieves competitive performance comparable to the 70B models. This demonstrates that building high-quality SFT data is essential for ensuring the code understanding and code debugging ability of these 7B models. Besides, both NeuDebugger-DS-6.7B and NeuDebugger-Llama3-8B perform better than the foundation model DSCoder-6.7B-Ins and Llama3-8B-Ins on the four tasks in DEBUGEVAL, bringing improvements of 27.7% and 4.1%, respectively. These improvements demonstrate that MASTER can refine code debugging data to significantly improve model performance on debugging tasks.\nAmong the four tasks defined in DEBUGEVAL, LLMs typically produce better results in both BUG Localization and Code Review tasks. For example, GPT-40-mini-0718 achieves accuracy scores of 82.4 and 89.1 on these tasks, respectively. This indicates that these LLMs have strong code understanding capabilities by finetuning with code generation tasks, allowing them to effectively identify buggy code snippets and exhibit better code execution abilities. On the contrary, all LLMs demonstrate less effectiveness in both BUG Identification and Code Repair tasks, which focus more on assessing the code debugging ability of LLMs. For the BUG Identification task, LLMs are required to identify the cause of bugs. The reduced effectiveness of LLMs in this task illustrates the difficulty current LLMs have in deriving bug causes. The Code Repair task is even more complex, requiring LLMs to locate buggy snippets, determine the error type, and then fix the code. The suboptimal performance of these 70B LLMs further indicates the challenges they face in self-debugging [9]. This phenomenon has also been observed in previous work [16].\nThe researchers repair codes by incorporating additional feedback from code compilers, which aims to enhance the"}, {"title": "VII. CONCLUSION", "content": "This paper presents DEBUGEVAL, an innovative benchmark designed to assess the debugging capabilities of Large Language Models (LLMs) from multiple perspectives. We introduce MASTER, a method that utilizes LLMs to generate high-quality supervised fine-tuning (SFT) datasets specifically for debugging tasks, thereby improving the performance of smaller models. Our experiments indicate that LLMs with 7B parameters are less effective in these debugging tasks and MASTER effectively enhances code debugging capabilities by refining data for SFT."}]}