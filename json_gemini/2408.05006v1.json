{"title": "Enhancing the Code Debugging Ability of LLMs via Communicative Agent Based Data Refinement", "authors": ["Weiqing Yang", "Hanbin Wang", "Zhenghao Liu", "Xinze Li", "Yukun Yan", "Shuo Wang", "Yu Gu", "Minghe Yu", "Zhiyuan Liu", "Ge Yu"], "abstract": "Debugging is a vital aspect of software development, yet the debugging capabilities of Large Language Models (LLMs) remain largely unexplored. This paper first introduces DEBUGEVAL, a comprehensive benchmark designed to evaluate the debugging capabilities of LLMs. DEBUGEVAL collects data from existing high-quality datasets and designs four different tasks to evaluate the debugging effectiveness, including BUG Localization, BUG Identification, Code Review, and Code Repair. Additionally, to enhance the code debugging ability of LLMs, this paper proposes a CoMmunicative Agent BaSed DaTa REfinement FRamework (MASTER), which generates the refined code debugging data for supervised finetuning. Specifically, MASTER employs the Code Quizzer to generate refined data according to the defined tasks of DEBUGEVAL. Then the Code Learner acts as a critic and reserves the generated problems that it can not solve. Finally, the Code Teacher provides a detailed Chain-of-Thought based solution to deal with the generated problem. We collect the synthesized data and finetune the Code Learner to enhance the debugging ability and conduct the NeuDebugger model. Our experiments evaluate various LLMs and NeuDebugger in the zero-shot setting on DEBUGEVAL. Experimental results demonstrate that these 7B-scale LLMs have weaker debugging capabilities, even these code-oriented LLMs. On the contrary, these larger models (over 70B) show convincing debugging ability. Our further analyses illustrate that MASTER is an effective method to enhance the code debugging ability by synthesizing data for Supervised Fine-Tuning (SFT) LLMs. All data and codes are available at https://github.com/NEUIR/DebugEval.", "sections": [{"title": "I. INTRODUCTION", "content": "In the realm of software development, code debugging is an indispensable process for ensuring the functionality and reliability of applications [1], [2]. With the complexity of software systems grows, traditional debugging methods, which often rely on heuristics [3], [4] and predefined patterns [5], [6], are reaching their limitations. The emergent ability of Large Language Models (LLMs) [7], [8] has opened up new horizons in automated debugging, offering a more flexible and comprehensive approach to identifying and rectifying code errors [9].\nThe capabilities of LLMs have been extensively explored for code-related tasks such as code generation and translation [10]\u2013[14]. However, their debugging capabilities remain relatively underexplored. Recently, researchers have begun to focus on using LLMs for self-debugging to repair buggy code iteratively [9], [15], [16]. To better evaluate the code debugging ability, researchers are now building benchmarks to assess the code debugging capabilities of LLMs [17], [18]. Nevertheless, existing code debugging benchmarks face two main issues:\n1) They primarily design tasks around code repair, which is insufficient for a comprehensive evaluation of code debugging ability. 2) Constructing buggy code using GPT-4 [19] fails to capture the complexity and diversity of code errors encountered in real development environments.\nTo address these challenges, this paper introduces DE- BUGEVAL, a comprehensive benchmark designed to evaluate the debugging capabilities of LLMs. DEBUGEVAL introduces four tasks: BUG Localization, BUG Identification, Code Review, and Code Repair, which are designed to test the LLMs' ability not only to identify and classify errors but also to provide correct code solutions. These tasks are of different difficulty levels and represent common debugging scenarios in real software development environments, making the evaluation both representative and challenging. Besides, each task in DEBUGEVAL includes Python, C++, and Java. Additionally, DEBUGEVAL incorporates the real user-written buggy codes and GPT-4 generated buggy codes to better simulate real-world software development. While collecting user-written buggy codes, we also implement strict quality control to prevent data leakage in DEBUGEVAL.\nAdditionally, this paper proposes a coMmunicative Agent based daTa rEfinement framework (MASTER), which focuses on improving the debugging ability of Large Language Models (LLMs). To ensure the quality of the Supervised Fine-Tuning (SFT) data, MASTER defines three agents: Code Quizzer, Code Learner, and Code Teacher, which collaborate to synthesize high-quality code debugging data for finetuning the Code Learner. Specifically, the Code Quizzer generates a diverse set of code debugging problems, guiding the LLM to acquire debugging knowledge during"}, {"title": "II. RELATED WORK", "content": "This section first introduces some backbone models for code understating and generation and then introduces the related work of code debugging and repair.\nCode-Oriented Language Models. To tailor language models for code understanding, related work mainly focuses on pretrained language models and guides them to learn the code semantics of syntax, and idiomatic [22]\u2013[24]. CodeBERT masks tokens of Natural Language (NL) and Program Language (PL) and then asks the pretrained language models to fill-in the masked spans. Then CodeBERT follows the ELECTRA method [25] and pretrains language models to detect whether the tokens are replaced, which helps models to better capture the code semantics [26]. DOBF [27] goes a step further by taking into account the unique attributes of code-related tasks, which focuses more on masking and generating the class, function, and variable names of code segments. CodeT5 [28] continuously pretrains T5 models [29] using the span masking strategy and also refines the masking strategy by focusing more on the identifiers within code. Such a pretraining method asks the T5 [29] model to generate these identifiers, thereby enhancing its ability to identify and understand identifier information in code-related tasks. Furthermore, some researchers also incorporate multi-modal data sources, such as code, comments, and abstract syntax trees (AST), to pretrain language models, which also helps to improve the code understanding ability by aligning the semantic between code semantics and natural language [30], [31].\nRecently, Large Language Models (LLMs), such as Chat- GPT [7] and Llama [8], have demonstrated their emergency ability in dealing with different tasks, especially for code understanding and generation tasks. To enhance the code generation ability, some widely used LLMs, such as Chat- GPT [7], also mix some code data in the pretraining corpus, which has proven its advantage in enhancing the reasoning ability of LLMs [32]\u2013[34]. Some typical code-based LLMs also collect some instruction data of different code-related tasks to supervised finetune LLMs, which significantly improves the code generation ability of LLMs [35]\u2013[37]. Even though LLMs have strong effectiveness in generating code segments, the code segments usually contain bugs [9], decreasing the pass rate of generated codes. To alleviate these problems, the existing efforts primarily concentrate on employing an iterative code repair approach to continuously refine generated code segments [9], [15], [16].\nCode Debugging and Repair. Early debugging models primarily rely on feature-based methods, such as using tem- plates [5], [6], heuristic rules [3], [4], or constraints [38], [39] to correct the buggy codes. However, the effectiveness of these feature-based debugging methods is hard to broaden to correcting different bug errors and dealing with more complex code bugs, because of the limited patterns or rules that need predefinition by researchers.\nWith the development of Pre-trained Language Models (PLMs), the work also follows the pretraining and then finetuning strategy to build the debugging model and deal with various code bugs occurred in real life. For example, Xia et al. [40] use the code-oriented pretrained model, CodeX [41], to explore the capabilities of PLMs in debugging. It shows that CodeX [41] achieves convinced code repair performance, especially on both Python and Java program languages. Kolak et al. [42] also use GPT-2 [43] and CodeX [41] to evaluate their effectiveness in generating the correct patch line when given corresponding code prefix. All the research has proven that the effectiveness of PLM based debugging models mainly thrives on the code understanding ability obtained during pretraining. Different from previous debugging work, recent research focuses more on correcting the bugs generated by LLMs. Self- Debug [9] prompts LLMs to generate the code reviews to aid themselves to refine generated codes, while Self-Repair [44] incorporates human-provided feedback for repairing the buggy code. Furthermore, Self-Edit [15] trains an additional fault- aware editor to repair codes by leveraging the error messages from the test case evaluation and generated code segments. Wang et al. [16] further explore the effectiveness of interactive Chain-of-Repair (CoR), which uses LLMs to generate the"}, {"title": "III. EVALUATING THE DEBUGGING ABILITY OF LLMS WITH DEBUGEVAL", "content": "In this section, we introduce the benchmark DEBUGEVAL, which is built to evaluate the debugging capabilities of Large Language Models (LLMs) from different aspects. We first describe the task definition of the designed task in DEBUGEVAL (Sec. III-A). Then we detail the process of constructing the DEBUGEVAL benchmark (Sec. III-B)."}, {"title": "A. Task Definition", "content": "DEBUGEVAL introduces four different tasks to evaluate the debugging ability of LLMs. The evaluation tasks include BUG Localization, BUG Identification, Code Review, and Code Repair.\nThe data statistics of DEBUGEVAL are shown in Table I. For each task, there are questions in Python, C++, and Java to evaluate LLMs' debugging performance on different programming languages. There are 578, 2320, 2400, and 414 test instances respectively for BUG Localization, BUG Identification, Code Review, and Code Repair tasks, nearly evenly distributed among the three programming languages. In the rest of this subsection, we present the illustrations of these four evaluation tasks in Figure 2 and delve deeper to describe each evaluation task.\nBUG Localization. The BUG Localization task focuses on identifying the specific line(s) of code that contains the error. It evaluates the ability of LLMs to point out the exact location"}, {"title": "Code Repair", "content": "The Code Repair [16], [17] task requires to generate the corrected version $P'$ for the given buggy code $P$, which is the ultimate test of the model's debugging capabilities. The code repair task is more difficult among these four debugging tasks. It not only involves code error detection/iden- tification but also needs to correct the error of the code. After generating the corrected code $P'$, we evaluate the correctness of $P'$ by using $n$ test cases $X = \\{(x_1, y_1), ..., (x_n, y_n)\\}$. Specifically, we feed the input $x_i$ of the test case $(x_i, y_i)$ to the corrected code $P'$ and then get the execution result $P'(x_i)$. If there exists the test case $(x_i, y_i) \\in X$ that satisfies $P'(x_j) \\neq y_j$, it indicates that the code $P'$ still contains errors that need to be addressed. The Code Repair task aims to transform the faulty program $P$ to $P'$ that pass all test cases $(\\forall (X_i, Y_i) \\in (X_i, Y_i), P'(x_i) = Y_i)$."}, {"title": "B. Details of Data Construction", "content": "In this subsection, we elaborate on the source data collection and construction method for the DEBUGEVAL dataset.\nTo ensure the quality of DEBUGEVAL, we collect high- quality data from reliable data sources, such as Debug- Bench [17] and LiveCodeBench [52], and the AtCoder website\u00b9.\nThe DebugBench focuses on the code repair task, which needs to call LeetCode API to evaluate the correctness of the"}, {"title": "IV. COMMUNICATIVE AGENT BASED DATA REFINEMENT FRAMEWORK", "content": "Supervised Fine-Tuning (SFT) has been widely adopted to enhance the performance of LLMs in specific domains [37], [57] using human-labeled datasets or LLM-generated data [15]. However, SFT's reliance on the availability and quality of la- beled data limits its overall effectiveness. In this case, this paper introduces the coMmunicative Agent based daTa rEfinement framework (MASTER), which automatically refines code debugging data for Supervised Fine-Tuning. As shown in Figure 3, we give the task examples to LLMs and then prompt LLMs to play different roles to synthesize the SFT data (Sec. IV-A). Finally, MASTER employs different agents to refine the synthesized data to guarantee the quality of the SFT data (Sec. IV-B)."}, {"title": "A. Agent Building", "content": "To conduct the data refinement, MASTER constructs three agents that collaboratively generate and refine the debugging problem data to synthesize high-quality SFT datasets. As illustrated in Figure 4, we employ different prompts to guide LLMs to play the roles of Code Quizzer,Code Learner, and Code Teacher. The details of each agent are described below.\nCode Quizzer. The Code Quizzer is designed to generate high-quality problems for the SFT data. It uses a stronger LLM as the backbone model and provides the instruction: \"You are a code debugging expert, skilled in generating code debugging problems to challenge programmers\u201d. This setup enables the Code Quizzer to generate tailored problems by analyzing examples of debugging tasks. These problems are intended to evaluate the Code Learner's ability to solve the corresponding debugging tasks.\nCode Learner. The Code Learner shares the same backbone model as the SFT model and serves as the critic to evaluate the educational value of the problems generated by the Code Quizzer. Using the prompt: \u201cYou are a student, please provide an answer to the following code debugging question using your own knowledge\", the Code Learner is tasked with solving the problem based on its memorized knowledge.\""}, {"title": "B. SFT Data Refinement with Communicative Multi-Agents", "content": "MASTER achieves automated data refinement through multi- agent collaboration, leveraging the expertise of stronger models to enhance the capabilities of weaker ones. The data refinement process consists of three main steps.\nIn the initial step (Step A), the Code Quizzer synthesizes various code debugging problems based on examples from the debugging task. To ensure diversity in the synthesized data, we instruct the Code Quizzer to generate different debugging problems aligned with the tasks defined in DEBUGEVAL, including Bug Localization, Bug Identification, Code Review, and Code Repair. The data synthesis process of each debugging task is guided by a single example, which serves as the demonstrations [59]. This approach ensures that the synthesized data encompasses a range of error types and difficulty levels, which is crucial for distilling debugging knowledge from the Code Quizzer/Teacher model during SFT Code Learner.\nAfter synthesizing the debugging problems, we proceed to Step B. At this step, the Code Learner attempts to solve the problems provided by the Code Quizzer. In this case, the Code Learner acts as a critic, assessing the educational value of each synthesized problem for the Code Learner. If the Code Learner solves the problem correctly, it indicates that the learner already possesses the necessary knowledge to solve the problem, and thereby the problem is discarded. On the other hand, if the Code Learner provides an incorrect solution, the problem is reserved as the SFT data, due to its educational value for guiding the Code Learner.\nFinally, in Step C, the Code Teacher reviews the reserved problems and generates detailed explanations and solutions. These Chain-of-Thought based explanations may include error type identification, error explanations, and the correct solutions to solve the problems. This feedback is essential for the Code Learner to comprehend the problems and refine their solutions. The responses generated by the Code Teacher are treated as the final outputs for the synthesized problems, forming the SFT data to fine-tune our NeuDebugger model."}, {"title": "V. EXPERIMENTAL METHODOLOGY", "content": "In this section, we describe the Supervised Fine-Tuning (SFT) strategies, evaluation metrics, details of evaluated foundation models, and implementation details of our experiments.\nSupervised Fine-Tuning Strategies. As shown in Table III, we describe the experimental details of different supervised fine-tuning strategies, including Vanilla SFT and MASTER."}, {"title": "VI. EVALUATION RESULTS", "content": "In this section, we benchmark LLMs on DEBUGEVAL and evaluate the overall performance of NeuDebugger. Then we conduct ablation studies and discuss the influence of different SFT data amounts on the model performance. The next experiment explores the effectiveness of NeuDebugger in handling the problems of different code error types. Finally, case studies are presented."}, {"title": "A. Overall Performance", "content": "The evaluation results of different LLMs and our NeuDebug- ger on DEBUGEVAL are presented in Table IV. We compared LLMs of varying scales to assess their code debugging effectiveness.\nOverall, larger-scale LLMs exhibit stronger code debugging ability. As indicated by the evaluation results, LLMs exceeding 70B parameters generally demonstrate consistent performance across various debugging tasks. Both the BUG Localization and BUG Identification tasks are multiple-choice questions with four options, where the accuracy of random guessing is approximately 25%. Unfortunately, most 7B-scale LLMs achieve less than 30% accuracy on both tasks. This phenomenon underscores the importance of model scale in maintaining emergent abilities and acquiring critical knowledge through supervised fine-tuning (SFT) on code data [20], [35].\nIn our experiments, we choose DSCoder-6.7B-Ins and Llama3-8B-Ins as the backbone models and then finetuning these two LLMs using the synthesized data generated by MAS- TER to conduct NeuDebugger-DS-6.7B and NeuDebugger- Llama3-8B models, respectively. In contrast to other 7B-scale LLMs, our NeuDebugger significantly enhances the code debugging effectiveness of foundation models and achieves competitive performance comparable to the 70B models. This demonstrates that building high-quality SFT data is essential for ensuring the code understanding and code debugging ability of these 7B models. Besides, both NeuDebugger-DS-6.7B and NeuDebugger-Llama3-8B perform better than the foundation model DSCoder-6.7B-Ins and Llama3-8B-Ins on the four tasks in DEBUGEVAL, bringing improvements of 27.7% and 4.1%, respectively. These improvements demonstrate that MASTER can refine code debugging data to significantly improve model performance on debugging tasks.\nAmong the four tasks defined in DEBUGEVAL, LLMs typically produce better results in both BUG Localization and Code Review tasks. For example, GPT-4o-mini-0718 achieves accuracy scores of 82.4 and 89.1 on these tasks, respectively. This indicates that these LLMs have strong code understanding capabilities by finetuning with code generation tasks, allowing them to effectively identify buggy code snippets and exhibit better code execution abilities. On the contrary, all LLMs demonstrate less effectiveness in both BUG Identification and Code Repair tasks, which focus more on assessing the code debugging ability of LLMs. For the BUG Identification task, LLMs are required to identify the cause of bugs. The reduced effectiveness of LLMs in this task illustrates the difficulty current LLMs have in deriving bug causes. The Code Repair task is even more complex, requiring LLMs to locate buggy snippets, determine the error type, and then fix the code. The suboptimal performance of these 70B LLMs further indicates the challenges they face in self-debugging [9]. This phenomenon has also been observed in previous work [16]. The researchers repair codes by incorporating additional feedback from code compilers, which aims to enhance the"}, {"title": "B. Ablation Studies", "content": "The ablation studies are conducted to explore the effective- ness of the MASTER model in finetuning LLMs.\nWe compare different SFT strategies, including Vanilla SFT, MASTER (Answer), MASTER (CoT), and NeuDebugger. The Vanilla SFT strategy gathers high-quality SFT data from UltraInteract [35], InstructCoder [60], and RepairLlama [61] for fine-tuning large language models (LLMs). Then, we use the MASTER framework to construct the SFT data and explore three different SFT strategies: MASTER (Answer), MASTER (CoT), and NeuDebugger. MASTER (Answer) indicates that we remove the Code Teacher in MASTER and ask LLMs to directly give the correct choice during SFT. MASTER (CoT) asks the Code Student to mimic the thought of problem solving of the Code Teacher. The NeuDebugger method combines the SFT strategies from both MASTER (Answer) and MASTER (CoT) by mixing the datasets from both SFT strategies for finetuning backbone models, except for the code repair data of MASTER (COT).\nAs shown in Table V, both DSCoder-6.7B-Ins and Llama3- 8B-Ins perform worse with the Vanilla SFT method compared to the baseline, indicating that the data quality for SFT remains a challenge in fine-tuning LLMs. When the MASTER synthesized data is used for SFT, the code debugging ability of these models is significantly enhanced. This illustrates that both DSCoder-6.7B-Ins and Llama3-8B-Ins are less effective at learning debugging knowledge from human/GPT-4 annotated data [20]. Furthermore, the MASTER (CoT) method generally achieves much better performance than MASTER (Answer), except for the code repair task. This may be because the Chain-of-Thought outcomes generated by Code Teacher can better explain the reasons behind answer choices but might incorporate additional noise in code repair tasks. By combining SFT data from both MASTER (CoT) and MASTER (Answer), NeuDebugger achieves the best performance among all SFT strategies. All these experimental results demonstrate the effectiveness of the MASTER model, which employs multi- agents to synthesize and refine SFT data."}, {"title": "VII. CONCLUSION", "content": "This paper presents DEBUGEVAL, an innovative benchmark designed to assess the debugging capabilities of Large Lan- guage Models (LLMs) from multiple perspectives. We introduce MASTER, a method that utilizes LLMs to generate high-quality supervised fine-tuning (SFT) datasets specifically for debugging tasks, thereby improving the performance of smaller models. Our experiments indicate that LLMs with 7B parameters are less effective in these debugging tasks and MASTER effectively enhances code debugging capabilities by refining data for SFT."}]}