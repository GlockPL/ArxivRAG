{"title": "High-Frequency Enhanced Hybrid Neural Representation for Video Compression", "authors": ["Li Yu", "Zhihui Li", "Jimin Xiao", "Moncef Gabbouj"], "abstract": "Neural Representations for Videos (NeRV) have simplified the video codec pro-\ncess and achieved swift decoding speeds by encoding video content into a neural\nnetwork, presenting a promising solution for video compression. However, exist-\ning work overlooks the crucial issue that videos reconstructed by these methods\nlack high-frequency details. To address this problem, this paper introduces a\nHigh-Frequency Enhanced Hybrid Neural Representation Network. Our method\nfocuses on leveraging high-frequency information to improve the synthesis of fine\ndetails by the network. Specifically, we design a wavelet high-frequency encoder\nthat incorporates Wavelet Frequency Decomposer (WFD) blocks to generate\nhigh-frequency feature embeddings. Next, we design the High-Frequency Fea-\nture Modulation (HFM) block, which leverages the extracted high-frequency\nembeddings to enhance the fitting process of the decoder. Finally, with the\nrefined Harmonic decoder block and a Dynamic Weighted Frequency Loss, we\nfurther reduce the potential loss of high-frequency information. Experiments on\nthe Bunny and UVG datasets demonstrate that our method outperforms other", "sections": [{"title": "1. Introduction", "content": "Nowadays, with the proliferation of high-speed Internet and streaming ser-\nvices, video content has become the dominant component of Internet traffic.\nAccording to statistics, in 2023, more than 65% of total Internet traffic is video\ncontent (Corporation, 2023), and this percentage is expected to continue increas-\ning. In the past, video compression was usually achieved by traditional codecs\nlike H.264/AVC (Wiegand et al., 2003), H.265/HEVC (Sullivan et al., 2012),\nH.266/VVC (Bross et al., 2021), and AVS (Zhang et al., 2019). However, the\nhandcrafted algorithms in these traditional codecs would limit the compression\nefficiency.\nWith the rise of deep learning, many neural video codec (NVC) technologies\nhave been proposed (Lu et al., 2019; Li et al., 2021; Agustsson et al., 2020; Wang\net al., 2024b). These approaches replace handcrafted components with deep\nlearning modules, achieving impressive rate-distortion performance. However,\nthese NVC approaches have not yet achieved widespread adoption in practical\napplications. One reason for this is that these approaches often require a large\nnetwork to achieve generalized compression over the entire data distribution,\nwhich is more computationally intensive and frequently leads to slower decoding\nspeeds compared to traditional codecs. Moreover, the generalization capability\nof the network depends on the dataset used for model training, leading to poor\nperformance on out-of-distribution (OOD) data from different domains (Zhang\net al., 2021a), and even when the resolution changes.\nTo overcome these challenges associated with NVCs, researchers have turned\nto implicit neural representations (INRs) as a promising alternative. Recently,\nINRs have gained increasing attention for their ability to efficiently represent"}, {"title": "2. Related work", "content": "Implicit neural representations (INRs) is an innovative approach, utilizing\na neural network to learn a mapping function fo(x) \u2192 y to map the input\ncoordinates x to corresponding values y (e.g., RGB colors). The network is\ntrained to approximate a target function g such that || fo(x) - g(x)|| \u2264 \u0454. Once\ntraining is complete, we can obtain a lossy reconstruction of the original sig-\nnal by performing a forward pass through the trained network. Subsequently,\nwe can apply standard neural network compression methods to further reduce\nthe model's size. Recently, INRs have been increasingly applied in video repre-\nsentation (Zhang et al., 2024b) because of their potential in applications such\nas video compression, inpainting, and denoising. Early methods (Zhang et al.,"}, {"title": "2.2. Spectral Bias", "content": "Several studies have demonstrated that neural networks exhibit spectral bias\n(Rahaman et al., 2019; Xu et al., 2019), meaning they typically prioritize fitting\nlow-frequency signals and tend to overlook high-frequency signals. These high-\nfrequency signals encode fine image details like vertical and horizontal edges.\nMissing these high-frequency components can lead the network to synthesize\nblurry images with undesirable artifacts. To address this bias, several strategies\nhave been proposed for pixel-based methods (Tancik et al., 2020; Damodaran\net al., 2023). For example, Tancik et al. (2020) introduced positional encoding\ninto the input coordinates of MLPs to enhance the ability of INRs to express"}, {"title": "2.3. Wavelet Transform", "content": "The wavelet transform, by effectively decomposing a given signal into differ-\nent frequency components, has demonstrated remarkable capabilities in image\nprocessing. Recently, benefiting from the ability of the wavelet transform to\ndecompose images into multiple sub-bands for multi-resolution analysis (Yang\net al., 2022; Yu et al., 2021; Wang et al., 2022; Zhang et al., 2024a; Wang et al.,\n2024a), researchers have successfully integrated it into convolutional neural net-\nworks (CNNs), achieving significant advancements in various computer vision\ntasks. For example, Yang et al. (2022) combines the wavelet transform with\ngenerative adversarial networks (GANs), enhancing the quality of synthesized\nimages from a frequency domain perspective and applying it to few-shot im-\nage generation. Similarly, Yu et al. (2021) decomposes images into multiple\nfrequency components and uses these signals to fill in damaged image areas, ef-\nfectively achieving image repair and reconstruction. Wang et al. (2022) enhances\nthe frequency awareness of the model by directly decomposing the intermediate\nfeatures of generators and discriminators into the wavelet domain for frequency\ndomain supervision. The wavelet transform offers an effective method for ex-\ntracting and utilizing high-frequency information in images. Based on this, we"}, {"title": "3. Method", "content": "The proposed approach follows the hybrid-based representation method (Chen\net al., 2023) that expresses a video as frame-specific embeddings and a video-\nspecific decoder. Our goal is to train the neural network to overfit the input\nvideo to the network parameters, thereby obtaining a neural representation that\nenables the reconstruction of the video through the embeddings and decoder af-\nter training. As shown in Fig. 2, our representation network includes three\nprimary components: a content encoder, a wavelet high-frequency encoder, and\na frequency-aware decoder. The light-weight ConvNeXt block (Liu et al., 2022)\nand strided convolution layer are used as the basic encoding unit in two en-\ncoder branches. The wavelet high-frequency encoder realizes the filtering and\nextraction of high-frequency information by integrating the Wavelet Frequency\nDecomposer (WFD) block. The frequency-aware decoder incorporates the pro-\nposed Harmonic block and High-Frequency Feature Modulation (HFM) block\nto fuse features and perform spatial upsampling.\nLet X\n= {X1,X2,X3,...,x} denote the input video sequence, where xt E\nRH\u00d7W\u00d73 is a video frame at time t with spatial resolution H \u00d7 W. The in-\nput frame xt is initially processed by the content encoder and converted into\ncompact content ef\u2208R\u00d7\u00d7de, which downsamples the spatial dimensions by\na factor of f. Meanwhile, the wavelet high-frequency encoder, equipped with\nWFD block, maps xt into high-frequency embedding er \u2208 Rxxdh with a\ndownsampling factor of g. Subsequently, the obtained en is then passed into the\nHFM block to learn the spatial modulation vector that modulates ef and then\nit passes through the proposed Harmonic block to perform spatial upsample.\nThe final reconstructed frame \u00eet is obtained by the last head layer, which maps\nthe features into the pixel domain. Once training is completed, the overfitted"}, {"title": "3.2. Wavelet High-Frequency Encoder", "content": "Considering the inherent spectral bias of neural networks in synthesizing\nhigh-frequency details, we propose directly extracts extra high-frequency em-\nbeddings and feed into the decoder. Various methods can be used to ex-\ntract high-frequency signals from images, such as the discrete cosine transform\n(DCT) and the discrete wavelet transform (DWT). To obtain disentangled high-\nfrequency information, we employ a simple yet effective Haar wavelet transform\n(Daubechies, 1990) to transform the encoded features from the spatial domain\nto multiple frequency domains for frequency selection. The Haar wavelet con-\ntains four kernels: LLT, LHT, HLT, and HHT. The transformation is defined\nas follows:\n$L^T = \\frac{1}{\\sqrt{2}} [1,1]$, $H^T = \\frac{1}{\\sqrt{2}} [-1,1]$\n(1)"}, {"title": "3.3. Frequency-Aware Decoder", "content": "The task of the decoder is to reconstruct the video frame using compact\nfeature embeddings as inputs, comprising the Harmonic block and the High-\nFrequency Feature Modulation block, which fuse features and perform spatial\nupsampling, respectively.\nHarmonic Block. The decoder typically consisting of stacked upsampling\nblocks. Each upsampling block generally includes a convolutional layer that\nexpands the channel size, followed by a pixel shuffle layer and an activation\nlayer to increase the spatial resolution of features, as shown in Fig. 2 (Right).\nPrevious methods often employ the GELU activation function in these blocks,\nwhich which can limit the ability of network to capture high-frequency signals\nin images. Sitzmann et al. (2020) has demonstrated that the activation function\nis crucial, significantly impacting the convergence and accuracy of the neural\nnetwork. Drawing inspiration from the real Fourier transform, we proposed the\nadaptive harmonic activation function to build the Harmonic decoder block.\nThe function is defined as follows:\n\u03c3(x) = w\u2081\u00b7 sin (x) + w2. cos (x)\n(3)"}, {"title": "High-Frequency Feature Modulation Block.", "content": "To fully integrate the ob-\ntained content features ef and high-frequency features en from different branches,\na straightforward fusion approach like addition or concatenation is inadequate\nas it may introduce some degree of high and low-frequency aliasing. In addition,\nemploying an popular attention-based fusion strategy can more precisely com-\nbine the features but results in a sharp increase in computational complexity and\nintroduces unnecessary parameter redundancy. To balance the performance and\ncomputational cost, we designed a light-weight High-Frequency Feature Mod-\nulation (HFM) block, as illustrated in Fig. 4. Specifically, the HFM receives\nhigh-frequency features as input and predicts the modulation vectors y and \u1e9e\nthrough two sub-networks fa and ft. Both fa and fo consist of three convolu-\ntional layers, each with a kernel size of 1 \u00d7 1. The content features are then\nmodulated by applying the computed y and \u1e9e, where y serves as a multiplier and\n\u1e9e as an additive bias, allowing the feature maps to be adjusted adaptively. Next,\na feed-forward network (FN) (Zamir et al., 2022) is employed to further enhance\nfeature representation by improving the modeling of spatial and channel-wise"}, {"title": "3.4. Loss Function", "content": "Given our goal of reconstructing video frames with fine detail, we opted for\na combination of L1 loss with SSIM as a training loss function. The spatial loss\nLspa is defined as:\n$L_{spa} = \u03b1 \u00b7 ||\u00eet - xt||\u2081 + (1 \u2212 \u03b1) \u00b7 (1 \u2013 ssim(xt, xt))$\n(6)\nwhere a is a weighting factor balancing the contributions of the L1 loss and the\nSSIM loss. However, relying solely on spatial domain losses may not capture the\nhigh-frequency details essential for preserving fine textures. Therefore, we in-\ncorporate a frequency loss during training. Specifically, we perform Fast Fourier\nTransforms (FFTs) on both the reconstructed image \u00eet and the ground truth\nimage xt to convert them into the frequency domain and compute the L1 loss\nbetween these frequency representations. To focus the network on synthesizing\nchallenging high-frequency details, we apply a dynamic weighting strategy in-\nspired by (Jiang et al., 2021). We calculate the spectral weight matrix using the\nspectral difference W = (|FFT(\u00eet) \u2013 FFT(xt)|). However, directly using this\nspectral difference can lead to large weight values due to extreme differences\nin certain frequency components, potentially destabilizing the training process.\nTo mitigate this issue, we apply a logarithmic transformation to mitigating the"}, {"title": "4. Experiments", "content": "In this section, we conduct comprehensive experiments on the Big Buck\nBunny (Bunny) (Roosendaal, 2008) and UVG (Mercat et al., 2020) datasets to\nevaluate the effectiveness of our method. The Bunny dataset consists of a 720\n\u00d7 1280 resolution video with 132 frames. The UVG dataset consists of seven\nvideos at 1080 \u00d7 1920 resolution with 600 or 300 frames. In line with previous\nworks, we perform a central crop of 640 \u00d7 1280 or 960 \u00d7 1920 on the Bunny\nand UVG datasets and reshape the UVG dataset into 480 \u00d7 960 for additional\ncomparison. For our experiments, the downsampling factor f and g lists for\nthe content encoder and wavelet high-frequency encoder are set as follows: [5,\n4, 4, 2, 2] and [2, 2, 2, 2, 2] for the Bunny, [5, 4, 4, 3, 2] and [3, 2, 2, 2, 2]\nfor the UVG, respectively. Similarly, the upsampling factors of the decoder are\nset to [5, 4, 4, 2, 2] for the Bunny and [5, 4, 4, 3, 2] for the UVG. We set the\nsize of the content embedding as 16 \u00d7 2 \u00d7 4 and the high-frequency embedding\nto 10 x 20 x 2. We employ peak-signal-to-noise ratio (PSNR) and multi-scale\nstructural similarity index (MS-SSIM) to evaluate the reconstruction quality"}, {"title": "4.2. Main Results", "content": "We conduct a comparative analysis of our method against NeRV (Chen\net al., 2021), E-NeRV (Li et al., 2022), and HNeRV (Chen et al., 2023). We\ntrained all methods for 300 epochs using the default hyperparameters and loss\nfunctions of each method. We adjust the number of channels in the decoder\nto obtain representation models of different sizes. The comparison results illustrate that our method outperforms other methods\nacross all sizes. Table 2 presents the performance across varying training epochs,\nfurther emphasizing the robustness of our approach."}, {"title": "4.2.2. Video Compression Performance", "content": "After fully training the model, we apply global unstructured pruning us-\ning the Layer Adaptive Magnitude Pruning (LAMP) score (Lee et al., 2020) to\nidentify and remove less significant parameters, effectively reducing the model's\ncomplexity without significantly impacting performance. We then implement\n8-bit quantization to reduce the precision of model weights and Huffman en-\ntropy coding Huffman (1952) to further compress the model size. We trained\nthree models of different sizes and compressed them to obtain rate-distortion\ncurves, which illustrate the trade-off between compression rate and reconstruc-\ntion quality. We compare our method's compression performance to NeRV,\nHNeRV, and the traditional HEVC (Sullivan et al., 2012), configured with the\nslow preset and no B-frames to ensure a fair comparison. Figure 6 shows the\nrate-distortion curves evaluated on the UVG dataset. The results demonstrate\nthat our method outperforms both the baseline HNeRV method and HEVC in\nterms of PSNR and MS-SSIM metrics, indicating superior video quality at com-\nparable or lower bitrates. These experimental results validate the effectiveness\nof our compression strategy and its potential for practical video compression\napplications."}, {"title": "4.2.3. Ablation Study", "content": "In this section, we conduct ablation studies on the Bunny dataset to verify\nthe contribution of various components in our method. We generated multiple\nvariants of the original model and trained them over 300 epochs to evaluate\ntheir performance.\nHigh-frequency encoder. To evaluate the effectiveness of High-frequency\nencoder, we conducted two experiments as shown in Table 4. In the first variant\n(V1), we removed the high-frequency embedding from our model while keeping\nthe number of parameters in V1 the same as in the original model, where the\naverage PSNR drops by 1.25 dB. In the second variant (V2), we replaced the\nwavelet high-frequency encoder with the content encoder, resulting in an av-\nerage PSNR decrease of 0.15 dB. As shown in Fig. 7, we further visualize the\nintermediate feature maps of the two encoders, showing that the content encoder\ncaptures low-frequency information such as image structure, while the wavelet\nhigh-frequency encoder captures more complex texture information. This ob-\nservation highlights the capability of our wavelet high-frequency encoder to in-\ndependently extract the desired high-frequency features effectively."}, {"title": "Fusion strategy.", "content": "We conducted several experiments to investigate the\neffectiveness of HFM. We replaced our proposed HFM with different fusion\nstrategies. In variant (V3), we concatenated the two features along the channel\ndimension and then processed them further through a convolution layer. In vari-\nant (V4), we added the high- and low-frequency features element-by-element. In\nvariant (V5), we performed inter-attention based on the channel dimension to\nachieve interactive fusion. As shown in Table 5, the result shows our proposed\nmethod achieves the best performance, demonstrating the effectiveness of our\nHFM."}, {"title": "Harmonic block.", "content": "To verify the effectiveness of the Harmonic block, we\nreplace its harmonic activation function with the GELU activation function\nand the sinusoidal activation function (Sitzmann et al., 2020) for variant (V6)\nand variant (V7). The result in Table 6 indicates that the adaptive harmonic\nactivation facilitates the reconstruction of the details of the image by introducing\nperiodic waveform features into the neural network."}, {"title": "Loss function.", "content": "In Variant (V8), we removed the frequency loss component,\nusing only the spatial loss. This modification led to a decrease in reconstruction\nquality, with the PSNR dropping by 0.39 dB compared to our full loss function.\nIn Variant (V9), we replaced our loss function with the L2 loss alone. This\nresulted in an even greater reduction in performance, with a PSNR decrease\nof 0.45 dB. In Variant (V10), we investigated the impact of the logarithmic\nfunction in the spectral weighting matrix by omitting it from our loss function."}, {"title": "5. Conclusion", "content": "In this paper, we introduce a High-Frequency Enhanced Hybrid Neural Rep-\nresentation Network to enhance the video fitting process. By extracting and\nleveraging high-frequency information and introducing refined Harmonic de-\ncoder block, we improve the capacity of the network to accurately regress intri-\ncate details in the reconstructed videos. With a Dynamic Weighted Frequency\nLoss, our approach demonstrates the superiority in reconstructing fine image\ndetails on Bunny and UVG datasets."}, {"title": "CRediT authorship contribution statement", "content": "Li Yu: Conceptualization, Methodology, Software, Writing - review & edit-\ning. Zhihui Li: Methodology, Writing - original draft, Validation, Writing\nreview & editing. Jimin Xiao: Methodology, Supervision, Project administra-\ntion. Moncef Gabbouj: Supervision, Project administration."}, {"title": "Declaration of Competing Interest", "content": "The authors declare that they have no known competing financial interests or\npersonal relationships that could have appeared to influence the work reported\nin this paper."}]}