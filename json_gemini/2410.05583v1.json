{"title": "NEGMERGE: CONSENSUAL WEIGHT NEGATION FOR\nSTRONG MACHINE UNLEARNING", "authors": ["Hyoseo Kim", "Dongyoon Han", "Junsuk Choe"], "abstract": "Machine unlearning aims to selectively remove specific knowledge from a model.\nCurrent methods, such as task arithmetic, rely on fine-tuning models on the forget\nset, generating a task vector, and subtracting it from the original model. However,\nwe argue the effectiveness of this approach is highly sensitive to hyperparameter\nselection, necessitating careful validation to identify the best model among many\nfine-tuned candidates. In this paper, we propose a novel method that leverages all\ngiven fine-tuned models rather than selecting a single one. By constructing task\nvectors from models trained with varied hyperparameters and merging only the\ncomponents of the task vectors with consistent signs, we perform unlearning by\nnegating the merged task vector from the original model. Given that existing meth-\nods also utilize multiple fine-tuned models, our approach delivers more effective\nunlearning without incurring additional computational costs. We demonstrate the\neffectiveness of our method on both vision-language models and standard image\nclassification models, showing improved unlearning performance with minimal\ndegradation on the retain set, outperforming state-of-the-art techniques. The code\nis available at https://github.com/naver-ai/negmerge.", "sections": [{"title": "INTRODUCTION", "content": "Recent advances in pre-training (Devlin, 2018; Dosovitskiy et al., 2021; Radford et al., 2021; Oquab\net al., 2023; Achiam et al., 2023; Liu et al., 2024) have achieved remarkable performance, primar-\nily driven by the use of large-scale datasets. However, the datasets often include underfiltered, un-\nwanted, or sensitive private information, which raises critical concerns about privacy protection. The\nRight to be Forgotten regulation (Hoofnagle et al., 2019) allows individuals to request the deletion\nof their personal data. However, applying this concept to machine learning models is challenging\nbecause the training process deeply embeds the data into the model's parameters, making it difficult\nto remove its influence. The most straightforward solution is to remove the data from the training set\nand retrain the model from scratch, which requires enormous computational resources. As a result,\nensuring that models forget learned patterns becomes a challenging task. Machine unlearning (War-\nnecke et al., 2021; Golatkar et al., 2020; Thudi et al., 2022; Koh & Liang, 2017; Jia et al., 2023;\nChen et al., 2023; Fan et al., 2023) offers a solution by enabling models to erase specific knowledge\nwithout the need for full retraining.\nDespite promising results, many existing methods struggle to remove only the target knowledge\nwhile preserving the rest. This challenge arises because fine-tuning often disrupts knowledge in the\nretain set (i.e., remaining data) during attempts to erase knowledge from the forget set (i.e., data to\nbe forgotten) (Chen et al., 2023; Fan et al., 2023). A known method robust to this issue is task arith-\nmetic (Ilharco et al., 2022), where direct fine-tuning of the model is avoided. Instead, this method\ncalculates a task vector the parameter-wise difference between the original model and a model\nfine-tuned on the forget set. The task vector is then subtracted from the original model through\na negation operation. This process, referred to as forgetting by negation, has demonstrated strong\nunlearning performance while preserving the model's knowledge, similar to continual learning re-\nsearches (Kirkpatrick et al., 2017; Aljundi et al., 2018) addressing catastrophic forgetting (Kirk-\npatrick et al., 2017). However, we argue that task arithmetic has limitations; not all fine-tuned"}, {"title": "2 RELATED WORK", "content": "Machine Unlearning. Recent machine unlearning methods can be categorized into two main\ngroups; unlearning specific knowledge in vision-language pre-trained models (Ilharco et al., 2022;\nOrtiz-Jimenez et al., 2024) and unlearning data in standard classification networks (Chen et al.,\n2023; Fan et al., 2023). Traditionally, these categories have been viewed as separate fields.\nIn the formal setup, the negation method in task arithmetic (Ilharco et al., 2022) is commonly used\nfor unlearning specific knowledge. A recent advancement is the neural tangent kernel-based linear\nnegation method (Ortiz-Jimenez et al., 2024), which addresses weight disentanglement issues in\ntask arithmetic by linearizing models and fine-tuning them in their tangent space. Both techniques\ndepend on a single fine-tuned model to compute the task vector.\nOn the other hand, unlearning with a standard image classifier usually involves fine-tuning the orig-\ninal model. Fine-tuning (Warnecke et al., 2021) and l\u2081-sparse (Jia et al., 2023) aim to overfit the\nmodel only on the retain set to erase the knowledge of the forget set. Meanwhile, Influence (Koh\n& Liang, 2017) and SalUn (Fan et al., 2023) utilize both the retain and forget sets to selectively\ndegrade performance on the forget set while maintaining it on the retain set.\nWhen the forget set is much smaller than the retain set, using retain set for unlearning can be ineffi-\ncient. This challenge has led to the development of methods that focus on unlearning using only the\nforget set. Several approaches (Golatkar et al., 2020; Chen et al., 2023) attempt this by relabeling\nthe forget set to different classes and fine-tuning the model. However, these methods often suffer\nfrom catastrophic forgetting of the retain set, as the retain set is not used during fine-tuning.\nThis paper proposes a unified approach to tackle both classification tasks using vision-language\nmodels and standard models. Furthermore, our method also focuses on using only the forget set\nfor unlearning. We recognize the inherent trade-off between unlearning performance and retaining\nperformance on the retain set. Relying on a single model to address this trade-off is inefficient. To\novercome this, we propose a new approach that utilizes multiple fine-tuned models. By building on\ntask arithmetic, our method computes a more effective task vector from these models, enhancing\nunlearning performance.\nModel Merging. The concept of Model soups (Wortsman et al., 2022) addresses inefficiencies\nin the validation process, where many models are discarded, and only the best one is retained.\nThis approach advocates for merging the weights of sub-optimal models to enhance generalization\nperformance without additional computational demands. Following this insight, more advanced\nmodel merging techniques have emerged. Task Arithmetic (Ilharco et al., 2022) introduces the"}, {"title": "3 METHOD", "content": "3.1 BACKGROUND\nTask Arithmetic. Task arithmetic (Ilharco et al., 2022) defines a task vector $\\tau_t = \\Theta_{ft} - \\Theta_{pre}$.\nSpecifically, the vectors are the result of subtracting (negating) the weights of a pre-trained model\n$\\Theta_{pre}$ from those of a model $\\Theta_{ft}$ fine-tuned on a target task t. We can adjust the model in the desired\ndirection by adding or subtracting the sum of these task vectors $\\tau = \\sum_t \\tau_t$ from the original model's\nweights, according to the formula $\\Theta_{new} = \\Theta_{pre} + \\lambda \\tau$. This approach is more computationally\nefficient than fine-tuning, as it leverages pre-trained models from public repositories and eliminates\nthe need for additional training.\nA key application of task arithmetic is to make a model forget certain capabilities (Ilharco et al.,\n2022). This can be achieved through the negation of task vectors from the original weight, which\ndecreases performance on a target task. For instance, task arithmetic can be applied to unlearning\nin models like CLIP (Radford et al., 2021), which is a strong vision-language model. In the original\npaper, the authors demonstrated that task vectors derived from a CLIP model fine-tuned on a specific\ndataset (e.g., Cars) could reduce the model's accuracy on the fine-tuning dataset while maintaining\noverall accuracy on a general dataset (e.g., ImageNet). However, while task arithmetic has shown\npromising results for machine unlearning, there has been little research on fine-tuning models and\ncomputing task vectors for more effective unlearning. Our research addresses this gap.\nMotivation. Our pilot study identifies two major challenges. First, unlearning performance is highly\nsensitive to the hyperparameters used for fine-tuning. Figures 1 (b) and (c) exhibit accuracy on both\nthe forget set and retain set, which can vary by up to 15 percentage points depending on the hyperpa-\nrameters. Second, finding a balance between reducing accuracy on the forget set while maintaining\naccuracy on the retain set is challenging. As shown in Figure 1 (a), improving performance on the\nretain set tends to result in a clear decrease in performance on the forget set, and vice versa."}, {"title": "3.2 NEGMERGE: IMPROVED TASK ARITHMETIC FOR MACHINE UNLEARNING", "content": "Given multiple models fine-tuned on the forget set, which applied various training configurations\nto ensure diversity among the fine-tuned models, we propose a method that neatly aggregates the"}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETUPS\nDatasets and Backbones. In the CLIP scenario (i.e., referred to as the scenario using a vision-language model), we follow the training and evaluation protocols of Ilharco et al. (2022). We assess\nunlearning performance on eight datasets: SUN397 (Xiao et al., 2016), Cars (Krause et al., 2013),\nRESISC45 (Cheng et al., 2017), EuroSAT (Helber et al., 2019), SVHN (Yuval, 2011), GTSRB (Stal-\nIkamp et al., 2011), MNIST (LeCun, 1998), and DTD (Cimpoi et al., 2014). We use the pre-trained\nCLIP ViT-{B/32, B/16, L/14} models (Radford et al., 2021) for these experiments. In the stan-\ndard classifier scenario, we evaluate unlearning performance on CIFAR-10 (Krizhevsky et al., 2009)\nusing a ResNet-18 (He et al., 2016) model.\nBaselines and Metrics. For the CLIP scenario, we compare our method with five existing meth-\nods: Task Arithmetic (Ilharco et al., 2022), Uniform Merge (Wortsman et al., 2022), Greedy\nMerge (Wortsman et al., 2022), TIES-Merging (Yadav et al., 2024), and MagMax (Marczak et al.,\n2024). For the Greedy Merge, we rank models by their loss on the retain set and merge them in a\ndirection that minimizes this loss. We evaluate performance by measuring accuracy on the forget set\nDf and the retain set Dr.\nIn the standard classifier scenario, we follow Fan et al. (2023) to compare our method against\neight unlearning techniques: Fine-tuning (Warnecke et al., 2021), Random Labeling (Golatkar\net al., 2020), Gradient Ascent (Thudi et al., 2022), Influence Unlearning (Koh & Liang, 2017),\nl1-sparse (Jia et al., 2023), Boundary Shrink and Expand (Chen et al., 2023), and SalUn (Fan et al.,\n2023). We also compare against Task Arithmetic (Ilharco et al., 2022) and Uniform Merge (Worts-\nman et al., 2022). The objective is to match the unlearned model's performance to that of a fully\nretrained model. Greedy Merge (Wortsman et al., 2022) is infeasible for comparison in this scenario,\nonly using the forget set. We use the accuracies of the retain set Dr, forget set Df, and test set Dtest\nto evaluate performance. To assess privacy protection, we employ the Membership Inference Attack\n(MIA) metric (Carlini et al., 2022), aiming to achieve similar results to the fully retrained model.\nImplementation Details. In the CLIP scenario, for fine-tuning, we set the batch size to 128 and use\na learning rate of le-5 with a cosine annealing schedule. We utilize the AdamW optimizer, applying\na weight decay of 0.1. During fine-tuning, the output of CLIP's text encoder, specifically the final\nclassification layer, remains frozen. We enhance the diversity of the fine-tuned models by adjusting\nthe configurations of RandAugment. Specifically, we vary the number of sequential augmentation\ntransformations (ranging from 1 to 3) and the magnitude of these transformations (ranging from 1 to\n10). A total of 30 models are fine-tuned. Unlike previous works, we incorporate data augmentation\ndirectly into the fine-tuning process, which requires adjusting the number of training epochs to better\naccommodate the augmented data. Consequently, the number of training epochs is set as follows:\n70 epochs for Cars, 100 epochs for DTD, 40 epochs for EuroSAT, GTSRB, RESISC45, SUN397,\nand 30 epochs for MNIST and SVHN.\nIn the standard image classifier unlearning scenario, for the CIFAR-10 dataset, we set the batch size\nto 256 and the learning rate to 0.05. Since CIFAR-10 has a relatively lower image quality, we do\nnot apply data augmentation. Instead, we vary the training hyperparameters. We set the number of\nepochs to 40, 50, and 60, the weight decay to 0.00005, 0.0001, and 0.00001, and the label smoothing\nto 0, 0.05, and 0.1 to enhance the diversity of the fine-tuned models. The total number of models\nused in the model merge is 27."}, {"title": "4.2 EXPERIMENTAL RESULTS", "content": "CLIP Unlearning Scenario. Table 1 presents the evaluation results across three variants of the\nCLIP model (ViT-B/32, ViT-B/16, and ViT-L/14) in the CLIP unlearning scenario. Our method\nachieves the best reduction in accuracy on the forget set Df across all backbone models, which\ndemonstrates its generalizability regardless of model size and architecture."}, {"title": "4.3 EMPRICAL ANALYSES", "content": "Regarding Our Key Assumptions. Our method relies on two key assumptions: (1) Effective un-\nlearning requires the fine-tuned model to maintain high performance on the forget set without de-\ngrading performance on the retain set, and (2) To accomplish this, only elements with consistent\nsigns across task vectors should be used during the merging process.\nTable 3 presents the evaluation results on both the forget set and retain set for the models derived by\nadding task vector to the original model. According to the results, most merging methods exhibit\nhigh performance on the forget set Df. However, we observe that, except for our method, the\nperformance on the retain set Dr significantly drops. Given that our unlearning method achieves\nthe highest performance, this supports our first assumption that high performance on the forget set\nis necessary while maintaining performance on the retain set. Additionally, unlike our method,\nUniform Merge, which merges all elements, leads to a substantial performance drop on the retain"}, {"title": "5 CONCLUSION", "content": "In this paper, we propose a novel machine unlearning technique, NegMerge, based on task arith-\nmetic and model merging. We hypothesize that multiple fine-tuned models are necessary for ef-\nfective unlearning based on the observation of a trade-off between accuracy on the forget set and\nthe remain set. Building on the fact that existing techniques generate numerous fine-tuned models\nthrough validation using various hyperparameters, we propose a method that utilizes all derived fine-\ntuned models. Assuming that elements with consistent signs across task vectors obtained from the\nfine-tuned models are related to the forget set, we merge only those elements. This approach enables\nus to compute task vectors that fit the forget set more effectively while preserving the knowledge\nin the retain set, thus overcoming the trade-off. We then perform forgetting by negation with the\nmerged task vector. Our NegMerge is tested on the CLIP ViT models and the standard ResNet18\nclassifier, achieving new state-of-the-art performance across nine datasets.\nLimitations. Limitation of this work is its reliance on empirical approaches without formal theoret-\nical justification. In future research, we aim to validate our assumptions theoretically and develop\nan analytical solution informed by these insights."}]}