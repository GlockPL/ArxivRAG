{"title": "Top-GAP: Integrating Size Priors in CNNs for more Interpretability, Robustness, and Bias Mitigation", "authors": ["Lars Nieradzik", "Henrike Stephani", "Janis Keuper"], "abstract": "This paper introduces Top-GAP, a novel regularization technique that enhances the explainability and robustness of convolutional neural networks. By constraining the spatial size of the learned feature representation, our method forces the network to focus on the most salient image regions, effectively reducing background influence. Using adversarial attacks and the Effective Receptive Field, we show that Top-GAP directs more attention towards object pixels rather than the background. This leads to enhanced interpretability and robustness. We achieve over 50% robust accuracy on CIFAR-10 with PGD $$\\epsilon$$ = 8/255 and 20 iterations while maintaining the original clean accuracy. Furthermore, we see increases of up to 5% accuracy against distribution shifts. Our approach also yields more precise object localization, as evidenced by up to 25% improvement in Intersection over Union (IOU) compared to methods like GradCAM and Recipro-CAM.", "sections": [{"title": "1 Introduction", "content": "Modern computer vision has made remarkable progress with the proliferation of Deep Learning, particularly convolutional neural networks (CNNs). These networks have demonstrated unprecedented capabilities in tasks ranging from image classification to semantic segmentation [54]. However, the explainability of these models remains a critical problem.\nMany previous attempts to improve explainability have focused on improving class activation maps of the already trained networks. We propose a different approach that focuses on a novel method to regularize the network during training. A constraint is added to the training procedure that limits the spatial size of the learned feature representation which a neural network can use for a prediction. Unlike [35], we do not need KKT conditions or the Lagrangian. The disadvantage of direct constrained optimization is that it can make gradient descent fail to converge if the algorithm is not modified. Instead, we force the network to only use the most important k locations in the feature map. The \"importance\" stems from an additional sparsity loss that forces the network to output an empty feature map. Part of the loss tries to increase k locations, while another part tries to set all of them to zero. This constraint simplifies the optimization problem and allows us to keep the same accuracy as the unconstrained problem.\nRestricting the output feature maps fundamentally changes the way the network works internally. In Fig. 1, we see an example on how the constraint affects the class activation map (CAM). We also found that the networks trained with our approach become more robust. The intuition behind our proposed method is based on the observation that if the sample size of a class is too small, the network may tend to focus on the background instead of the object itself [41, 42]. This can lead to undesirable biases in the classifier. In our approach, the constraint forces the network to not focus so much on the background.\nThe main contributions of this paper are:\nSize Priors: We propose Top-GAP, a regularization technique incorporating a size prior directly into the network architecture. This method constrains the number of pixels the network utilizes during training and inference. It is beneficial for object classification tasks in contexts without perspective projections, such as biomedical imaging and datasets with centered objects.\nEffective Receptive Field (ERF): We link Top-GAP to the ERF and measure the influence of the feature output on the background and object pixels. We show that Top-GAP directs the network's focus towards object pixels, reducing background influence.\nRobustness to Adversarial Attacks and Distribution Shifts: Further evidence that the background is less important is given by our robustness experiments. Top-GAP improves robustness against PGD and Square Attack, achieving up to 50% higher accuracy without adversarial training. It also enhances accuracy by up to 5% for datasets such as Waterbirds [42].\nIntersection over Union: By adjusting the pixel constraint, our method enables the network to focus more precisely on specific objects, leading to up to a 25% improvement in Intersection over Union (IOU) compared to GradCAM and Recipro-CAM [7]."}, {"title": "2 Related Work", "content": "Our work is related to different strands of research, each dealing with different aspects of improving the features and robustness of neural networks. This section outlines these research directions and introduces their relevance to our novel approach.\nAdversarial robustness. It has been shown that neural networks are susceptible to small adversarial perturbations of the image [15]. For this reason, many methods have been developed to defend against such attacks. Some methods use additional synthetic data to improve robustness [16, 51]. [51] makes use of diffusion models, while [16] uses an external dataset. Other methods have shown that architectural decisions can influence robustness [21, 36]. A disadvantage of all these approaches is that the clean accuracy and training speed are negatively affected [10, 38.\nBias mitigation and guided attention. A notable line of research concentrates on channeling the network's focus towards specific feature subsets. Of concern is the prevalence of biases within classifiers, arising due to training on imbalanced data that perpetuates stereotypes [5]. Biases may also stem from an insufficient number of samples [4, 6, 55], causing the network to emphasize incorrect features or leading to problematic associations. For instance, when the ground truth class is \"boat\", the network might focus on waves instead of the intended object.\n[18, 53] introduce training strategies to use CAMs as labels and refine the classifier's attention toward specific regions. In contrast, [39] proposes transforming the input images to mitigate biases tied to protected attributes like gender. Moreover, [26] suggests a method to uncover latent biases within image datasets.\nWeakly-supervised semantic segmentation (WSSS). [25] focuses on accurate object segmentation given class labels. The Puzzle-CAM paper [23] introduces a novel training approach, which divides the image into tiles, enabling the network to concentrate on various segments of the object, enhancing segmentation performance. There are many more publications that focus on improving WSSS [45]. Some making use of foundational models such as Segment Anything Model (SAM) [24] or using multi-modal models like CLIP [37].\nPriors. Prior knowledge is an important aspect for improving neural network predictions. For example, YOLOv2 [40] calculated the average width and height of bounding boxes on the dataset and forced the network to use these boxes as anchors. However, there are many other works that have tried to use some prior information to improve predictions [8, 20, 35, 48, 57]. In particular, [35] has proposed to add constraints during the training of the network. For example, they propose a background constraint to limit the number of non-object pixels. However, they only train the coarse output heat maps with convex-constrained optimization. The problem is that the use of constraints can make it harder to find the global optima. Therefore, it is harder to train the whole network.\nOur approach. Much like bias mitigation strategies and attention-guided techniques, we direct the network's focus to specific areas. However, our approach does not require segmentation labels and only minimally changes the CNN architectures. The objective is to maintain comparable clean accuracy and the number of parameters, while significantly improving the interpretability of objects. In contrast to WSSS, we do not intend to segment entire objects, but instead continue to concentrate on the most discriminative features. Given that we modify the classification network itself, we also diverge from methods that solely attempt to enhance CAMs of pretrained models."}, {"title": "3 Method", "content": "In most cases of image classification, the majority of pixels are not important for the prediction. Usually, only a small object in the image determines the class. Our approach is geared towards these cases. In contrast, many modern CNNs implicitly operate under the assumption that every pixel in an image can be relevant for identifying the class. This perspective becomes evident when considering the global average pooling (GAP) layer [27] used in modern CNNs. The aim of the GAP layer is to eliminate the width and height dimensions of the last feature matrix, thereby making it possible to apply a linear decision layer. The GAP layer averages all locations within the last feature matrix without making a distinction between the positions or values. This means that a corner position is treated in the same way as a center position. We also note that each of the locations in the last feature matrix corresponds to multiple pixels in the input image. This is known as the receptive field. Now, we want to define more formally the terminology.\nDefinition 1 (Effective receptive field). Let $$X_{i(p),j(p)}^{(p)}$$ be the feature matrix on the pth layer for 1 < p \u2264 n with coordinates (i(p), j(p)). The input to the neural network is at p = 1 and the output feature map at p = n. Then the effective receptive field (ERF) of the output location (i(n), j(n)) with respect to the input pixel (i(1), j(1)) is given by\n$$\\frac{\\partial X_{i(n), j(n)}^{(n)}}{\\partial X_{i(1), j(1)}^{(1)}}$$\nThis definition assumes that each layer has only a single channel. For multiple output channels, we compute $$\\sum_{k=1}^{c(n)} \\frac{\\partial X_{i(n), j(n), k}^{(n)}}{\\partial X_{i(1), j(1)}^{(1)}}$$ where c(n) are the channels of the last feature map. The ERF characterizes the impact of some input pixel on the output.\nDefinition 2 (Global Average Pooling). The feature output of the neural network $$X^{(n)}$$ is averaged to obtain a single value. This operation is known as Global Average Pooling (GAP) and is defined as:\n$$GAP(X^{(n)}) = \\frac{1}{h^{(n)}w^{(n)}} \\sum_{i=1}^{h^{(n)}} \\sum_{j=1}^{w^{(n)}} X_{i,j}$$\nwhere h(n) is the height and w(n) is the width of the output feature map. In practice, there is not only one channel but c(n) channels.\nAn example shall explain the two terms. In case of EfficientNet-B0 [46], $$X^{(n)}$$ has dimension 7 \u00d7 7 \u00d7 1280 for an input image of size 224 \u00d7 224 where c(n) = 1280 are the channels. The GAP(\u00b7) operation reduces X(n) to a vector of size 1280 \u00d7 1. All of the 7 \u00d7 7 locations have an effect on the classification. With the help of the ERF, we can measure how much the 2242 input pixels contribute to the 72 output locations.\nAnother method to analyze what the neural network focuses on are the so-called class activation maps. These methods modify X(n) so that we get a visualization of what is important for the neural network.\nDefinition 3 (Class Activation Map). The product of multiplying the output tensor X(n) by some weight coefficient W is known as a class activation map (CAM) [56]. The standard CAM, also known as \"CAM\", uses the weights of the linear decision layer L.\nIn the previous example, the linear decision layer L would map the 1280 channels to c(n+1) class channels. The output of the CAM would be in this case 7\u00d77\u00d7c(n+1). Each of the c(n+1) maps can be upsampled to obtain a visualization.\nDefinition 4 (GradCAM). GradCAM is a generalization of CAM to non-fully convolutional neural networks (non-FCN) such as VGG. It is equivalent to the standard CAM for FCN like ResNet. It is defined as follows\n$$GradCAM(X, c) = ReLU(\\sum_{k} W_{k, c} X_{k})$$\nwith $$W_{k,c} = GAP(\\frac{\\partial L}{\\partial X})_{k}$$, k being the channel index of X and c being the index of the linear layer. Usually the last feature map X(n) is chosen for X.\nIn addition to GradCAM, there are many other CAM methods. However, they are all based on reducing the channels of X(n) in order to obtain a visualization. Instead of improving GradCAM, as so many approaches have done before [9, 13, 22, 33, 49], we propose that the output of the CNN should be both a CAM and a prediction. Then we can regularize the CAM during training and can more fundamentally influence what is highlighted in the CAM.\nOur approach involves integrating an object size constraint directly into the network, designed to enforce the utilization of a limited set of pixels for classification. This constraint allows for noise reduction and the elimination of unnecessary pixels from the CAM. In cases where specific-sized features determine the class, we can incorporate this prior knowledge into the neural network, enhancing its classification accuracy.\nBefore introducing the object constraint, we first change the model structure to output a higher-resolution CAM."}, {"title": "3.1 Changing the model output structure", "content": "Figure 2 shows the general structure of our architecture. The backbone can be any standard CNN such as VGG [44], ResNet [17], ConvNeXt [30] or Efficient Net [46]. Depending on the backbone, we use the last 3 or 4 feature maps as input to a feature pyramid network (FPN) [28]. We note that the original FPN as used for object detection was simplified in order to reduce parameters. All the feature maps are upsampled to the size of the largest feature map and added together. We found no advantage in using concatenation. This output is given to a final convolutional layer that has the number of output classes as filters. Note that a convolutional layer with kernel size 1 is used for the implementation of the final linear layer. Optionally, dropout can be applied as regularization during training. Lastly, we employ Top-GAP to obtain a single probability for each class. Top-GAP is introduced in the following section.\nFor convenience, we explicitly define two modes for our model (refer to Fig. 2):\n1.  CAM: The output feature map is upsampled to the size of the input image and normalized to be in the range [0, 1].\n2.  training/prediction: Top-GAP is enabled to obtain the probabilities for each class.\nWithout our modified model, we would need to use a method such as Grad-CAM to obtain a visualization.\nLet us compare the two approaches: EfficientNet-B0 with GradCAM and Efficient Net-B0 with our output structure (see Fig. 2). GradCAM does not require any additional parameters because it generates the activation map from the model itself. If we change the model structure, we have more parameters, but also more influence on what is seen in the CAM. If we were to replace GradCAM with LayerCAM or some other method, it would never have the same impact as changing the model training itself (our approach). In addition, GradCAM does not combine multiple feature maps by default to achieve better localization.\nIn our approach, the standard output linear layer of some classification model like Efficient Net-B0 is substituted with f + 1 convolutional layers, where f corresponds to the number of feature maps. This leads to a small increase in the number of parameters."}, {"title": "3.2 Defining the pixel constraint (Top-GAP)", "content": "Instead of using the standard GAP layer, we replace the average pooling by a top-k pooling, where only the k highest values of the feature matrix are considered for averaging. This pooling layer limits the number of input pixels that the network can use for generating predictions.\nIn a standard CNN, the last feature map is at layer n. In our model (Fig. 2), the last feature map is at n + 1 because we replaced the linear decision layer L by a 1 x 1 convolution."}, {"title": "Definition 5 (Top-GAP)", "content": "We define the Top-GAP layer as follows:\n$$TopGAP(X, k)_{t} = \\frac{1}{k} \\sum_{i=1}^{k} X_{i,t}$$\nwhere X represents the ordered feature matrix $$X^{(n+1)}$$ with dimensions $$h^{(n+1)}w^{(n+1)} \\times c^{(n+1)}$$, where $$c^{(n+1)}$$ corresponds to the number of output classes. Each of the $$c^{(n+1)}$$ column vectors is arranged in descending order by value, and k values are selected. i indicates the ranking, with i = 1 being the largest value and i = k being the smallest. t is an index indicating the channel. We select for each channel different values.\nWhen k = 1, we obtain global max pooling (GMP). When $$k = h^{(n+1)}w^{(n+1)}$$, the layer returns to standard GAP. The parameter k enforces the pixel constraint, and its value depends on the image size. For instance, if the largest feature map has dimensions 56 \u00d7 56, then 562 values are selected. Hence, when adjusting this parameter, it is crucial to consider the relative object size in the highest feature map."}, {"title": "3.3 Classification loss function", "content": "The last component of our method involves changing the loss function. While the Top-GAP(\u00b7) layer considers only locations with the highest values, these locations might not necessarily be the most important ones. Thus, it becomes essential to incentivize the reduction of less important positions to zero.\nTo achieve this, we add an l\u2081 regularization term to the loss function, inducing sparsity in the output. The updated loss function is defined as follows:\n$$L = \\lambda ||X^{(n+1)}||_{1} + CE(\\hat{y}, y),$$(1)\nwhere CE(\u0177, y) represents the cross-entropy loss between the prediction \u0177 = softmax (Top-GAP(X,k)) and the ground truth y. X is the ordered X(n+1) feature output in our model, while k is a fixed non-trainable parameter. Here, \u03bb controls the strength of the regularization. We found that for most datasets \u03bb = 1 is sufficient."}, {"title": "4 Evaluation", "content": "In this section, we will systematically test the claims of our method on several datasets. Since there is no ground truth for explainability, we focus mainly on surrogate measures. Our main surrogate measure for interpretability is the background of the image. We show that our method causes the network to focus less on it. Furthermore, we also evaluate how our model behaves in the presence of distribution shifts. A description of the datasets used here can be found in the appendix A."}, {"title": "4.1 Hypothesis: the gradient of object pixels becomes more important", "content": "We want to show that with our method not all pixels in the input image have the same influence on the output feature map X(n+1). Recall that in our model, the last feature map is at n + 1 because we have replaced the linear layer with a convolutional layer.\nIn most datasets (e.g. ImageNet), the object to be classified is located in the center of the input image. While each pixel in the input image corresponds to multiple values in the output feature map X(n+1), the general position is the same. The center in the output is also the center in the input.\nThe input pixels should contribute much more to the center than to the background of X(n+1). We want to quantify how much influence the input pixels have on the center of X(n+1) and on the corner of X(n+1). For this, we use Definition 1 and define a metric.\nDefinition 6 (ERF distance). Let $$ERF(1,1) = \\frac{1}{hw} \\sum_{i,j} \\frac{\\partial X_{i,j}^{(n+1)}}{\\partial X_{1,1}}$$ to be the absolute change of the output corner position (1,1) with all input pixels (i, j). Similarly, we define $$ERF(\\frac{h}{2},\\frac{w}{2})$$ to be the change of the output center position with respect to the input, where h and w is the width of the output feature map. Then the ERF distance is ERF($$\\frac{h}{2}, \\frac{w}{2}$$) - ERF(1,1).\nIntuitively, we expect a low value for ERF (1, 1) because the corner position of the feature map contains less information. Similarly, ERF($$\\frac{h}{2}, \\frac{w}{2}$$) should be a high value because the object is in the center. If the difference between the two values is low, it means that each pixel contributes similarly to the output.\nTable 2 shows that for the standard CNN the center of the image has the same effect as the corner. ERF(1,1) has the same value range as ERF($$\\frac{h}{2}, \\frac{w}{2}$$). Compare this to our approach, where there is a large difference between the center and the corner ERF.\nDuring backpropagation, the neural network goes from the end to the beginning of the network and updates the weights. With our method, we set the gradient at the background positions of the last feature matrix to almost zero. This also affects all other layers as a consequence of the chain rule.\nThe visualization in Fig. 3 confirms the numerical results. Since there are 72 = 49 positions for the standard ResNet and 562, we only considered 9 pixel positions. We see that the gradient disappears at the locations where there is no object. More numerical details are provided in the appendix in Tab. A1 and Tab. A2."}, {"title": "4.2 Hypothesis: the background is less susceptible to adversarial attacks", "content": "The last experiment showed that by changing the values in the output feature matrix, we also change the effect of the input pixels. Another idea to prove that the network focuses less on the background is to use adversarial attacks. The goal of such an attack is to change the input pixels so that the classification prediction is different. Using our method, we expect the attack area to be smaller since the network uses the background less.\nIt is important to emphasize that we only need to achieve higher robustness against standard models. Our goal here is to force the network to focus on different regions in order to achieve better interpretability. We do not intend to compete with adversarially trained networks. Adversarial training (AT) is slower, leads to less clean accuracy and does not make the networks more interpretable.\nIn Tab. 4, we see that when using the standard networks, the values of SAR(O) and SAR(B) are close to each other. This means that the center has the same effect as the background. The network concentrates on all pixels equally. With our approach, we can manipulate the class more easily by changing the object pixels."}, {"title": "4.3 Hypothesis: classification makes use of object pixels", "content": "Another way to show that we are directing the attention of the network to the object is through distribution shifts. To address this, we use the Waterbirds dataset [42], where the backgrounds of images are replaced. Furthermore, we evaluate accuracy on ImageNet-Sketch [50] and ImageNet-C [19].\nAn improvement in accuracy can be observed for all datasets. While there are works that show higher accuracy for datasets such as ImageNet-Sketch [12], they are based on specialized training methods (self-supervised, semi-supervised) and/or more data. Our proposed method comes \"without cost\" in the sense that it works for any architecture and dataset, without requiring more GPU resources. It can be viewed as a regularization technique. This increase in robustness does not negatively affect the accuracy. We also see a comparable accuracy when using 5-fold stratified cross validation (refer to Tab. 6)."}, {"title": "4.4 Hypothesis: increased interpretability due to pixel constraints", "content": "The last experiments have shown that we can direct the attention of the network to the object. So far, we have used the pixel constraint k, which maximizes the metric. However, it is also possible to vary this value to incorporate human knowledge into the prediction. In Fig. 4, we measure the sparsity of our CAM."}, {"title": "5 Discussion and Outlook", "content": "In this paper, we presented a new approach to improve the explainability of CNNs. Our method focuses on controlling the number of pixels a network can use for predictions, resulting in CAMs with lower noise and better localization. The results show that our approach is effective on a variety of datasets and architectures. We have consistently observed both visually and numerically more concise feature representations in the CAMs. In addition, our approach provides a novel form of network regularization. By forcing the network to focus exclusively on objects of a predefined size, we reduce the risk of highlighting irrelevant regions, which can be critical for applications that require precise object localization or for reducing bias.\nLimitations. Determining the optimal value for the pixel constraint parameter k currently depends on hyperparameter tuning. It is possible to explore automated methods for determining this parameter to improve efficiency and adaptability. Second, given the variety of object sizes, it may not be ideal to rely on a single parameter for all objects. Only in specific areas such as biomedical imaging, where object size are not influenced by perspective projections (e.g. microscope) typically show low size variances. Investigating ways to dynamically adjust this parameter for different object sizes would be a valuable line of research. Finally, the proposed FPN module can be further refined to improve accuracy even more."}, {"title": "B Effective Receptive Field (ERF)", "content": "In Tab. 2, we only showed the difference between the center and the corner ERF. In the following tables, we provide the individual values. The gradients were z-normalized to have mean at 0 and standard deviation at 1."}, {"title": "D Sparsity", "content": "For almost all datasets and architectures, our approach achieved sparser CAMs. We see especially large decreases for ImageNet."}, {"title": "E Effect of $$l_1$$ normalization on robustness and interpretability", "content": "$$l_1$$ regularization has a strong influence on the results. Here we want to show that without the other components we would have lower interpretability, robustness and/or accuracy.\nWe consider the following variants of ResNet-18:\n*   $$l_1$$ regularization only on the last feature output (activations) with $$\\lambda$$ = 1.0\n*   $$l_1$$ regularization only on the last feature output (activations) with $$\\lambda$$ = 0.1\n*   $$l_1$$ regularization on all activations with $$\\lambda$$ = $$10^{-3}$$\n*   $$l_1$$ regularization on all activations with $$\\lambda$$ = $$10^{-5}$$\n*   ours: our approach\nWe use the CUB-200-2011 dataset. $$\\lambda$$ denotes the strength of the regularization."}, {"title": "E.1 Interpretability and accuracy", "content": "First, we analyze the effective receptive field and accuracy."}, {"title": "E.2 Robustness", "content": "Next, we analyze the level of robustness with respect to $$l_1$$ regularization."}]}