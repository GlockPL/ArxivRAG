{"title": "A Performance Study of LLM-Generated Code on Leetcode", "authors": ["Tristan Coignion", "Cl\u00e9ment Quinton", "Romain Rouvoy"], "abstract": "This study evaluates the efficiency of code generation by Large Language Models (LLMs) and measures their performance against human-crafted solutions using a dataset from Leetcode. We compare 18 LLMs, considering factors such as model temperature and success rate, and their impact on code performance. This research introduces a novel method for measuring and comparing the speed of LLM-generated code, revealing that LLMs produce code with comparable performance, irrespective of the adopted LLM. We also find that LLMs are capable of generating code that is, on average, more efficient than the code written by humans. The paper further discusses the use of Leetcode as a benchmarking dataset, the limitations imposed by potential data contamination, and the platform's measurement reliability. We believe that our findings contribute to a better understanding of LLM capabilities in code generation and set the stage for future optimizations in the field.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have recently increased in popularity, especially with the advent of ChatGPT [28]. While LLMs have been spreading over various application domains, such as text or image generation, certain types of LLMs are being developed solely for code-related purposes. These LLMs aim to assist the developers by saving time and effort through the generation of code, documentation, unit tests, etc. Many of these LLMs only come in a \"raw\" form, that is, they do not integrate themselves into the developer's coding process. These include models, such as CODEGEN [27], STARCODER [21], WIZARDCODER [24], CODET5 [38], and INCODER [14]. On the other hand, some LLMs are already seamlessly integrated into the developer's IDE as code assistants, like GITHUB COPILOT,1 AMAZON CODEWHISPERER, 2 and TABNINE.3\nThere has been a significant amount of work dedicated to comprehending how these LLMs perform in various situations and defining their limits. For instance, several works address the security of the code generated by such models [29, 30, 33] or the prevalence of bugs in the generations [20]. Many researchers are also investigating how developers interact with LLMs and how such models fit into the programming workflow [7, 30, 34] There is also a broad research effort to measure the actual efficiency of these LLMs by creating common benchmarks for comparison [5, 10, 17, 37, 43] or by actually measuring different qualities related to the generations, such as the success rate [42] or the robustness of the model regarding variations [13].\nTo the best of our knowledge, there is no research work evaluating the performance of the code generated by LLMs. Yet, having code that runs faster is an often sought-after characteristic of a program. Indeed, programming efficiency is paramount, especially when resources are scarce or programs are deployed on a large scale. In today's context where the energy consumption of software systems has become a major concern, improving software efficiency is particularly relevant since increasing the performance of a program can also lead to energy consumption reduction [2, 36].\nThe process of code optimization is lengthy and intricate, requiring careful attention and a certain level of expertise, especially when searching for the best-performing algorithm, selecting the most appropriate data structure, or struggling with memory hierarchy. Yet, this process is necessary to identify opportunities for improvement that may result in minor reductions in execution time. LLMs can be used as a way to make this process easier, e.g., by generating performance-improving code edits [11, 15, 25]. Garg et al. [16] also presented RAPGEN, a method for generating zero-shot prompts to enhance performance. However, while users seem to put a lot of trust in code generated by LLMs, they still have trouble reviewing it [33, 34], which can lead to slow code being shipped in production, especially if an LLM generates inefficient code.\nThe key contributions of this paper are as follow: (i) We study the performance of the code generated by 18 LLMs on 204 problems and investigate performance differences across models using a novel method for measuring and comparing the performance of LLM-generated code. (ii) We compare the performance of the code generated by LLMs to the code written by humans. (iii) Incidentally, we evaluate the usability of Leetcode, a public repository of algorithmic problems that we use as a dataset.\nFrom section 2 to section 4, we describe the tasks dataset and the models we selected, outline our experiment setup, and explain the methodology we followed to analyze the obtained results, respectively. We report in section 5 on the results of our evaluation and provide a critical discussion in section 6. Finally, section 7 presents the related works, and section 8 concludes the paper."}, {"title": "2 METHODOLOGY", "content": "This paper covers the performance of code generated by various LLMs. In particular, we aim to answer the following research questions:\n\nRQ1: Can Leetcode be used as a dataset and a benchmark platform for evaluating LLMs? Leetcode can serve as both a dataset of problems and a tool to evaluate and measure solutions to the problems. Particularly, we study if the dataset is subject to recitation and if the measures Leetcode provides are reliable.\nRQ2: Are there notable differences between the performance of the code generated by different LLMs? LLMs differ greatly in terms of generating correct code, so we want to know if they also differ in terms of generating efficient code.\nRQ3: Is there an effect of the success rate and the temperature of the LLM on the code's performance? Having a higher temperature decreases the capacity of the LLMs to generate valid code, so we aim to study if this also applies to the performance of the code. In the same way, we also study if an LLM that is very good at generating valid code on one problem, is going to generate efficient solutions to this problem.\nRQ4: How efficient are the solutions generated by the LLMs compared to human solutions? Comparing the LLMs to a set of human-authored solutions can provide insights into their position relative to humans in terms of code performance."}, {"title": "2.1 Research questions", "content": null}, {"title": "2.2 Tasks & Dataset", "content": "Task selection. The input tasks-i.e., problems specified by a prompt-we consider to generate code from various LLMs has to meet the following requirements:\n\n\u2022 A given problem, should offer multiple candidate solutions, whose generated code performs differently. This ensures one can observe differences across the various LLMs;\n\u2022 Generated solutions should exhibit variable execution times. Given more complex inputs, one should differentiate O(n) from O(2n) and O(n\u00b2) algorithms.\n\nAs a result, task datasets, such as HUMANEVAL or Mostly Basic Python Programming (MBPP), which are classically used when evaluating LLMs for code assessments [1, 4, 10, 27], cannot be considered for our purpose. Indeed, while they do provide unit tests to drive the generations, the size of the inputs in these unit tests remains small and fails to scale to appreciate performance issues. Moreover, the solutions that need to be generated are often very short, which would lead to fewer possible variations between implementations. Also, the fact that many problems are not algorithmic by nature makes them less prone to inefficient practices and performance variation. To face such issues, we used input prompts that were built from Leetcode problems. Leetcode is an online judge platform that suggests programming problems to registered users. It addresses the above limitations as it provides algorithmic problems with varying levels of difficulty and test cases with large input sizes. Leetcode also exposes a GRAPHQL API5 to fetch relevant metadata"}, {"title": "2.3 LLMs Under Study", "content": "Our empirical study covers a total of 18 LLMs, specifically designed for coding purposes. We selected the 18 popular code LLMs from Hugging Face, as well as GITHUB COPILOT, which is an online closed-source code assistant. The LLMs were selected based on the number of downloads and likes they exhibited. We chose GITHUB \u0421\u041e\u0420\u0406-LOT to offer a comparison between a commercial LLM and open-source LLMs, but did not choose any other GPT models from OpenAI because of their cost. Table 1 summarizes all the LLMs considered in this study. This includes variants of the same base LLM-i.e., models with varying sizes (in billions of parameters) or different training data, such as variants of CODEGEN, INCODER, and CODET5. LLMs belonging to the same family are models closely related in terms of training data and method. We first performed our experiment in March 2023 with a subset of the models presented here, with the \"old\" dataset. We then performed it a second time in September 2023 with all the models and the \"new\" dataset."}, {"title": "3 EXPERIMENT SETUP", "content": "This section describes our experiment setup to generate and validate the solutions produced by the LLMs. First, we describe how solutions were generated from each LLM. Then, we outline the three-step process used to filter invalid solutions. Finally, we explain how the run time of the generated solutions was measured."}, {"title": "3.1 Code Generation", "content": "We generated 10 solutions for each problem from our dataset by varying the temperature of the LLMs used to generate them. Specifically, we considered 6 different temperatures (0.1, 0.2, 0.4,0.6, 0.8, and 1.0). As COPILOT's temperature cannot be configured, we used its default temperature for generating all the problems.\nGenerating code with GITHUB COPILOT. Automatically generating with GITHUB COPILOT for a reproducible experiment proved to be a difficult task. Firstly, the code suggestion feature of COPILOT activates when typing in a text editor with the installed COPILOT plugin. Additionally, GITHUB COPILOT produces code based on a context that encompasses the current file and the files previously accessed by the user, impacting the generated solutions. Lastly, we noticed a caching mechanism on the GITHUB COPILOT server side, which resulted in very similar or identical solutions if we generated multiple solutions for a given problem in the same session. To address these issues, we used a generation method similar to the one used by D\u00f6derlein et al. [13] by instrumenting the GITHUB COPILOT Neovim plugin' and restarting the plugin between every generation to avoid the caching effect. This method allowed us to automatically generate solutions in a quick and isolated fashion. On top of that, GITHUB COPILOT provides 2 means of generation: inline generations (the suggested code is integrated with the editor) and panel generations (COPILOT generates at most 10 completions and displays them on a panel next to the editor). We chose to exclusively use inline generations, as panel generations yielded worse results in terms of functional correctness than inline generations.\nGenerating code with open-source models. Regarding the open-source models, we generated solutions by deploying the models on servers provided by the GRID5000 platform [6]. We used the Deepspeed library to make the generation process faster and fit larger models on our GPUs. Concerning the sampling, we used the same methods as Chen et al. [10] and used nucleus-sampling [18] with top p = 0.95. The maximum number of tokens to be generated was set to 600. This is because the LLMs we used have a limited context size of 1024 tokens (prompt included). Thus, to avoid exceeding the context size, we had to limit the number of tokens to generate. We also verified it did not significantly impact the functional validity of the LLMs.\nIn total, we generated 2, 040 solutions with COPILOT and 12, 240 with each of the eight other models, resulting in 210, 120 generated solutions overall."}, {"title": "3.2 Validation", "content": "Each generated solution was tested to ensure its functional correctness following a three-step process. At every step, if a solution was found to be invalid, it was excluded from subsequent stages of the experiment. The process was as follows:\ni) Local validation. We filtered out code generations that included easy-to-spot errors, such as syntax errors or runtime errors, by using the small inputs we fetched earlier (see Section 2.2 Test cases). While this step was not strictly necessary, it quickly reduced the number of solutions to be validated in the next step;\nii) Leetcode validation. Next, we submitted the solutions to the Leetcode judge system using the Leetcode GRAPHQL API, where they underwent a rigorous test suite managed by Leetcode. We kept the solutions that passed all the test cases or exceeded Leetcode's allocated time limit. The latter was kept to ensure that correct solutions that were too slow remained included in the benchmark;\niii) Exclusion of timeouts and other errors. Finally, we excluded the solutions that reported errors when executed with our benchmarking setup using the large inputs fetched beforehand. We invalidated the code generations that took more than 10 seconds to run or raised an error. Most of the errors raised in this step were recursion errors caused by differences between Leetcode's Python interpreter and ours. Indeed, Leetcode's seemed to have a higher recursion limit than ours, which we set to 10, 000 instead of the default 1,000 (we could not manage to set it any higher). Additional errors occurred because we developed our helper classes differently from Leetcode's implementation. Although these classes are provided by Leetcode during the submission process, they are not publicly available. Out of the 4, 930 invalidated solutions that were caught in this step, 4, 863 (98.6%) were due to timeouts, 20 (0.04%) to recursion errors, and 47 (0.1%) to other errors. Following this validation process, the initial set of 210, 120 generated solutions was pruned down to 7, 481 (3.6%) valid solutions remaining across the 18 LLMs."}, {"title": "3.3 Measuring run time", "content": "We measured the performance as the run time of the generated solutions using pytest-benchmark, 10 which runs pytest unit tests multiple times to obtain run times statistics. The measurements were performed using parameters that ensured each solution ran at least 10 times and for at least 1 second in total. We did not perform warm-up runs of the benchmarks, as we did not notice any significant difference in the measured time during preliminary testing. To facilitate the measurement protocol, the generated solutions were sorted into \"runs\", based on the specific LLM and temperature that were used during the code generation. Within each run, which was defined by a unique combination of model and temperature, the solutions were measured in sequence during a single program execution. Furthermore, in every run, we added the canonical solutions we previously collected, which would run alongside the generated solutions. This approach ensured that the same canonical solutions were executed in every run, allowing us to maintain measurement stability. Specifically, we calculated the standard deviation of the canonical solution run times across all runs, thus providing a reliable measure of the variability of the measurement protocol. We observed that over 96% (196 out of 204) of the canonical solutions had a standard deviation lower than 1/10th of their average run time, which we deemed to be an acceptable level of variation.\nThe cluster we used to run the benchmark was the chiclet cluster of the Grid5000 testbed.11 It hosts 2 AMD EPYC 7301, with 16 cores per CPU and 128GB of memory. When using the node, all the cores of both CPUs were reserved, but only one was used at a time to maximize the stability of the measurement protocol."}, {"title": "3.4 Replication package", "content": "All the artifacts of this study, including our results, code, and datasets, are available in the following public repository: https://zenodo.org/doi/10.5281/zenodo.7898304."}, {"title": "4 DATA ANALYSIS", "content": "In this section, we describe the methods we adopted to analyze our results. These methods fall into one of the two following categories: functional correctness and code performance."}, {"title": "4.1 Functional Correctness", "content": "The functional correctness of an LLM defines how much the LLM outputs code conforming to the program contract (as specified by the input prompt). To evaluate the functional correctness of our LLMs, we computed their pass@k metrics with k = 1 and k = 10, using the unbiased estimator proposed by Chen et al. [10]. The pass@k unbiased estimator which, from k samples produced, considers the test as successful if one of these samples passes all the tests, is computed as follows (with n the total number of samples, c the number of correct samples and E the expected value):\npass@k := E(1 \u2212 (1 \u2212 \\frac{c}{n})^{k})\n(1)\nAs Chen et al. [10] suggest, we calculated the pass@k for each temperature when evaluating an LLM's functional correctness and considered the best one as the pass@k for that LLM."}, {"title": "4.2 Code Performance", "content": "To measure the code performance, we considered three different metrics. First, we used the memory usage reported by Leetcode. Then, we computed the median of the run times measured by pytest-benchmark for every generated solution. Lastly, to compare the LLMs solutions to human-submitted solutions, we also used the rank reported by Leetcode when validating the solution. This rank is a number between 0 and 100 that indicates the share of submitted solutions that are slower than the current solution (e.g., if a solution has a rank of 90, it is faster than 90% of the submitted solutions on Leetcode).\nTo assess the LLMs' performances, we conducted pairwise comparisons as follows: For each pair of LLMs, we identified problems where both models generated more than 5 valid solutions. For each identified problem, we conducted a Student t-test on the mean run time of the generations to determine if there was a significant difference. Then, for each pair A-B of LLMs, we computed the ratio of problems where A's code was significantly faster than B's and where B's code was significantly faster than A's."}, {"title": "5 RESULTS", "content": "In this section, we summarize the key observations from our experiment and answer our research questions. Our results are also available in the form of a companion notebook in our replication package. The companion notebook offers more insight into the results and additional graphs."}, {"title": "5.1 RQ1: Can Leetcode be used as a dataset and a benchmark platform for evaluating LLMs?", "content": null}, {"title": "5.1.1 Can Leetcode problems be adopted as a dataset for LLM generation?", "content": "As on can observe in Figure 2, the generated codes exhibit on a significant drop in functional correctness between the two datasets. The difference here is pretty staggering for every tested LLM, reporting on a tenfold decrease in pass@k. We believe this issue may stem from data contamination in the old dataset. Data contamination occurs when an LLM is assessed on data that was included in the training dataset, introducing bias into the evaluation process. In our case, a significant number of questions in the old dataset are widely known and have been extensively shared on GitHub. For instance, a search for the prompt of the \"3sum\" Leetcode problem on GitHub yields approximately 4.000 matches in public repositories. These questions are also old enough to likely be included in the training datasets of the LLMs under study, as the majority of their training datasets have a cut-off date between 2021 and 2022. Due to this data contamination, LLMs tend to recite, reproducing verbatim source code when generating solutions. This phenomenon is more pronounced when the prompt is highly specific and lacks contextual information, as seen in Leetcode prompts"}, {"title": "5.1.2 Are Leetcode measurements reliable?", "content": "Run time. As reported in Figure 3, the coefficient of variation of the Leetcode measures (0.089) is slightly higher than the coefficient of variation of the local measures (0.035). This suggests that Leetcode's measuring setup is less suited to ensure accurate benchmarks.\nWe also study the correlation between our local measurements and Leetcode's, and we notice two issues: (1) the times measured locally and by Leetcode only slightly correlate on average (0.28). For some problems, the measures are highly correlated (> 0.8) while, for others, they are almost not (< 0.2). This is more apparent when we look at the scatter plot showing the measures of some problems in Figure 4. In this problem, there are four clusters of generations with a different locally measured time, but the clusters are indiscernible in terms of Leetcode time. This could be due to two main reasons. Firstly, as previously discussed, the variance of the measures from Leetcode is higher and as such, there is much more noise in the measures, decreasing the precision. Secondly, the tests we employed may be more focused on performance testing than Leetcode's. Notably, our test suite comprises only three tests featuring significantly large inputs, potentially accounting for certain disparities in the results.\nAlthough still usable, relying on the time reported by Leetcode introduces some limitations due to its higher variance. Consequently, Leetcode's measures cannot allow us to discern the differences in run time between different code implementations as precisely as locally measured time.\nMemory usage. While we did not measure the memory ourselves, we observed the variation of the memory usage measure provided by Leetcode. We notice that the memory usage for the same solutions decreases over time. There is indeed a slight correlation of -0.24 between the day of the year we tested our solution and the memory usage. The fact that the memory usage measure evolves renders comparisons between LLMs harder. While we could theoretically offset the memory usage when we detect changes over time, it would require testing the canonical solutions alongside the generated one on Leetcode.\nLeetcode rank. The time ranking that Leetcode returns when we test a solution represents the share of submitted and valid solutions that are slower than ours. While this could be a great tool to rank LLMs among human-submitted solutions, we find that the ranking is heavily affected by our submissions and time. As you can see in Figure 5, the overall rank of the LLMs we tested decreases over time. To verify this, we tested GITHUB COPILOT twice, once as the first LLM, and a second time after testing all the LLMs. The first test of COPILOT has an overall rank of 77, and the second test ranks down to 54, despite it being tested with the same solutions. This effect of the rank evolving becomes obvious when you consider that the rank is determined using all the previously accepted solutions, including ours. This means that by testing thousands of our solutions on Leetcode, we are actively changing the ranks of future tests."}, {"title": "RQ1:", "content": "The evaluation of LLMs using Leetcode questions as a dataset presents some challenges. Although Leetcode's questions could serve as a valuable dataset akin to HUMANEVAL, limitations arise due to the constraint that only the problems published after the LLM's training dataset formation are usable for evaluating the LLM in question. This creates potential difficulties in reproducibility, particularly as new LLMs emerge, especially if they do not exclude Leetcode problems from their training datasets [19]. Additionally, while Leetcode's provided metrics, such as run time, memory usage, and rank may offer practicality in various scenarios, their usability and reliability are questioned when compared to more traditional measurement methods. The presence of these challenges emphasizes the need for careful consideration and scrutiny when adopting Leetcode to evaluate LLMs."}, {"title": "5.2 RQ2: Are there notable differences in performances between LLMs?", "content": "The pairwise comparison depicted in Figure 6 reveals subtle distinctions in the performances of various LLMs. Notably, some models, such as StarCoder and the CodeLlama model with 13B parameters specialized in Python, consistently exhibit slightly superior results compared to others. Despite these observed variations, the mean Cohen's d effect size measures a mere 0.024, a statistically insignificant magnitude. This suggests that the practical impact of these differences on the mean speed of code generation is remarkably small. For instance, when comparing CodeLLama-13-instruct and CodeGen25-7B-mono, CodeLLama outperforms the latter in a statistically significant manner in 3 problems out of 8. However, it is crucial to note that the mean performance difference between these models is a mere 0.02 standard deviation. It thus seems that improving an LLM in terms of functional validity does not significantly impact the performance of the code it generates. This may be due to different factors, such as the fact that most LLMs share the same datasets or that they are trained to produce valid code and not fast code. Improving the performance of an LLM could be done by curating a training dataset of only efficient code and fine-tuning one of the foundational models we used, or by using reinforcement learning to \"teach\" the model to produce better code. Madaan et al. produced an LLM that could improve the performance of code [25]. This LLM could be leveraged in a generation pipeline to directly improve the generated code."}, {"title": "5.3 RQ3: Is there an effect of the functional validity of the LLM and its temperature on the generated code's performance?", "content": "Functional validity. When calculating for every problem the correlation between the success rate of the LLM that generated the solution and the run time of the solution, we find that there is only a very slight negative correlation (-0.08) between the success rate and the performance. There is close to no correlation (-0.11) observed between the success rate of the model and the variation in the performance of the generated code.\nTemperature. There is no correlation (0.05) observed between the temperature of the generations and the performance of the generated code. This means that the temperature does not affect how fast the solutions are. However, we observe that temperature is moderately correlated (0.41) with higher variations in performances. This means that higher temperatures tend to increase the variation in performance across generations. The complete distribution of the correlation for each of the 24 problems can be seen in Figure 7. So, while increasing the temperature leads to a lower success rate [13], it can help find a faster solution with an extended exploration of generations. The fact that the temperature increases the variation in performances comforts the idea that higher temperatures lead to more diverse outcomes."}, {"title": "RQ3:", "content": "Our analysis of LLMs indicates that the quality of generated code does not have a substantial impact on its performance. However, we observed that modifying temperature settings within an LLM significantly affects the diversity of code performances produced. This implies that, while code validity may not be a decisive factor in performance, adjusting temperature settings can be a valuable strategy to enhance the variety of outcomes in code generation processes."}, {"title": "5.4 RQ4: How fast are LLMs compared to humans ?", "content": "As previously stated, the Leetcode time ranking evolves, so we chose to compare the second model we tested on Leetcode with humans (COPILOT being the first, it did not have enough generations overall because of its lack of a temperature setting). The results of this comparison are depicted in Figure 8. The comparison is done using the Leetcode ranking, with the assumption that most of the previous submissions were made by humans.\nWe observe in Figure 8 that the solutions generated from LLMs are faster than most previous submissions with a mean rank of 73%, and that it even generated some solutions that were faster than 95% of the previous submissions."}, {"title": "RQ4:", "content": "It seems that the LLMs are faster than most of the human solutions on Leetcode, on average. If the LLM we tested were in an actual competition, his valid solutions would be on average faster than 73% of the other solutions on Leetcode."}, {"title": "6 DISCUSSION", "content": "In this section, we discuss the results reported in the previous section, their implications, and the limitations of our study."}, {"title": "6.1 Discussion of the results", "content": "On the Leetcode measures and usability. The data contamination issue we unveiled poses a significant challenge in the evaluation of LLMs, as it prevents an accurate evaluation of their real performances. Because Leetcode's problems are not filtered from the training datasets, as research on LLMs continues, even the newer problems might contaminate future training datasets, thus rendering reproduction of our study harder [19]. This conclusion also holds for any study using Leetcode as an evaluation dataset, such as [8, 13, 26]. However, we believe that the methodologies employed and the conclusions drawn would likely hold validity with alternative sets of questions from Leetcode or other performance-oriented datasets. This suggests that future studies seeking to replicate our findings would primarily need to change the dataset employed for assessing LLMs. Addressing the data contamination concern could involve leveraging pre-filtered evaluation datasets, such as HUMANEVAL, already separated from the training processes, and repurposing them into performance evaluation datasets.\nOn functional correctness. The ranking of the functional correctness of the LLMs is consistent with the previous evaluations of the LLMs on HUMANEVAL [1, 4, 10, 21, 24, 27, 32]. However, it seems that InCoder performs worse than presented in its introductory paper [14], which may be due to our experimental protocol-using it only for left-to-right generation instead of infilling like it was built for.\nWe observe that Leetcode's problems seem harder for the LLMs to solve than HUMANEVAL's problems. Indeed, StarCoder, the model that performed the best with a pass@1 of 0.09, had a pass@1 of 0.408 on HUMANEVAL [21]. We believe this is due to multiple factors. First, our Leetcode prompts and expected solutions are longer than in HUMANEVAL (the average length of solution in HUMANEVAL is 180 characters vs 425 characters in our dataset), thus increasing the chance of a generation to fail. Indeed, having to generate more code can lead to a higher chance of making a mistake, as shown by a correlation of -0.30 between the solution's length and the success rate of the problem. Second, Leetcode problems come from programming competitions and need a lot of thinking to be solved. The causal generation of the LLMs does not allow a \"thinking\" process to happen, which for harder problems causes a drop in functional validity. This could be solved by making the LLMs mimic human thinking with methods, such as Chain-of-Thought prompting [39].\nOn the performance of generated code. To the best of our knowledge, our methodology for evaluating LLMs based on the performance of generated code is novel and could serve as a benchmarking approach for future studies involving new LLMs and datasets. While we aimed for a singular performance score for LLMs, similar to pass@k, the varying rates at which LLMs generate valid solutions posed a challenge, leading us to employ pairwise comparisons. An improvement to our method could involve using an optimal solution as a reference point and comparing the speed of each generated solution to the optimal solution's speed. Speed would thus be expressed as a factor relative to the optimal time, addressing the issue of problems having different time scales and potentially laying the groundwork for a more comprehensive \"performance score.\"\nThe low success rate of LLMs on Leetcode posed a considerable challenge for their comparison. Among the 204 problems, only 24 had (i) valid solutions from at least 10 different models (out of the 18 assessed LLMs) and (ii) at least 10 valid solutions in total (see to the companion notebook). This low success rate complicated pairwise comparisons, as numerous models could not be compared due to an insufficient number of common problems with a substantial number of generated solutions.\nOur discoveries offer valuable insights for developers in their choice of LLMs. For example, when developers are considering an LLM for tasks like code generation, such as with GitHub Copilot, the performance of the generated code may be an important concern. With our findings, developers can be assured that there is no significant variance in the performance of code generated by different LLMs. This means that if they aim to have fast code, there's no necessity to consistently opt for the largest model available; instead, a smaller one suffices. We also hope that our findings will incentivize further research on building new LLMs that produce even more efficient code."}, {"title": "6.2 Limits and Threats to Validity", "content": "Regarding functional correctness, although we did our best to generate code in the most optimal conditions, some changes to the input prompt (i.e., adding examples and constraints), or the configuration of the models may have changed the performance of the LLMs. However, we believe that this should not significantly impact the validity of our experiment, as all the models were configured similarly.\nGITHUB COPILOT being a closed-source tool, it might be retrained without the community's knowledge, which could potentially lead to a modification in the tool's performance in upcoming experiments.\nThe majority of our validation and benchmarking process relied on Leetcode's test suite and online judge system. Thus, it is possible that some big test cases we extracted for the benchmarking favored some types of implementations over others. We mitigated this issue by systematically fetching three test cases and by having a large dataset of problems to generate from. During our experiments, we also tried to generate plausible inputs using random generators, but this method did not yield satisfying results because randomly generating data structures with specific shapes, or properties (e.g., generating valid regular expressions) is difficult to achieve.\nOne gap in our study is that we do not consider memory usage at all when studying the LLMs. This is because our benchmarking setup did not allow for memory monitoring and doing otherwise would have cost us a lot of time. Moreover, we believe our results obtained with only the run time to be self-sufficient.\nThe way we compared an LLM to a human using Leetcode rankings gives a good idea of where the LLM stands in terms of performance. However, because the ranking evolves and we have no information about the population the LLM is ranked against, the results here should be taken with a grain of salt.\nIt is also important to note that Leetcode as an evaluation dataset suffers from some issues. As we only evaluate the LLMs on algorithmic problems, the performances of the LLMs are hard to generalize across all programming fields. However, this is difficult to improve on for similar reasons to HUMANEVAL: we only have a limited context size and have to make the LLM generate in one go a completion to some code, which must be self-contained (meaning, all the information needed to generate the solution must be in the prompt). Also, in terms of performance, it is difficult to find another kind of self-contained code than algorithmic problems that have, such variations in performance. While studying the LLMs on SQL generation could be a good idea, it would not fit into our studies with generalist programming languages.\nRegarding Leetcode as a platform, we are heavily dependent on it for validating the generated solutions and are limited by its daily rate limits of 1,000 submissions per account. For future studies, it would be great to consider alternatives to Leetcode, such as the project CodeNet [31], which does not have as many restrictions as Leetcode."}, {"title": "8 CONCLUSION", "content": "In this study, we presented a comprehensive analysis of the performance of code generated by various LLMs using a novel methodology that measures and compares the runtime speed of solutions to algorithmic problems. Our findings suggest that the performance of the generated code is largely similar across different models, regardless of their size or training data. Furthermore, increasing the temperature parameter during code generation leads to a greater variance in performance, though not necessarily to better or worse solutions"}]}