{"title": "AuditWen: An Open-Source Large Language Model for Audit", "authors": ["Jiajia Huang", "Haoran Zhu", "Chao Xu", "Tianming Zhan", "Qianqian Xie", "Jimin Huang"], "abstract": "Intelligent auditing represents a crucial advancement in modern audit practices, enhancing both the quality and efficiency of audits within the realm of artificial intelligence. With the rise of large language model (LLM), there is enormous potential for intelligent models to contribute to audit domain. However, general LLMs applied in audit domain face the challenges of lacking specialized knowledge and the presence of data biases. To overcome these challenges, this study introduces AuditWen, an open-source audit LLM by fine-tuning Qwen with constructing instruction data from audit domain. We first outline the application scenarios for LLMs in the audit and extract requirements that shape the development of LLMs tailored for audit purposes. We then propose an audit LLM, called AuditWen, by fine-tuning Qwen with constructing 30k instruction dataset from 15 audit tasks and 3 layers. In evaluation stage, we proposed a benchmark with 5k instructions that covers a set of critical audit tasks derived from the application scenarios. With the benchmark, we compare AuditWen with other existing LLMs from information extraction, question answering and document generation. The experimental results demonstrate superior performance of AuditWen both in question understanding and answer generation, making it an immediately valuable tool for audit.", "sections": [{"title": "1 Introduction", "content": "Audit is an independent economic supervision activity conducted by governmental agencies or a special organ in accordance with the law to conduct pre-and-post-event reviews of major projects and financial revenues and expenditures of financial institutions or enterprises. In recent years, with the development of big data, the data foundation and audit methodology of national audit are also undergoing changes [14]. The audit methodology is transitioning from big data audit to intelligent audit [7], aiming at recommending or selecting the optimal strategy for audit decision-making through the extensive integration of machine learning, deep learning, and other information technologies.\nWith the emergence of ChatGPT 3, large language models (LLMs) [17] have attracted much attention from researchers. Its smooth natural dialogue and document generation capabilities have rendered it widely used in various fields, such as in financial [12], medical [9], legal [10] and so on. A large language model is a deep learning model with a very high number of parameters and computational power that can automatically learn the syntax, semantics, and context of input natural language and can generate text of corresponding to it. As a powerful artificial intelligence technology, large language model possess a strong capacity for understanding and generating natural language and can provide innovative solutions for the audit.\nHowever, the current general LLMs commonly encounter issues like a deficiency in domain-specific knowledge and the existence of data bias. Similar to their application in other domain-oriented tasks, LLMs face challenges when directly applied to auditing, including difficulties in understanding input issues clearly and providing accurate responses to fact-based tasks, a phenomenon known as hallucination [17]. Moreover, auditors argue that intelligent auditing with LLMs should prioritize collaboration between individuals and the model to jointly accomplish complex audit tasks [7]. This demand necessitates that LLMs not only comprehend concepts, entities, and knowledge within the audit domain, but also master the fundamental processes of audit work to assist auditors in achieving high-quality results. LLMs excel in context memory, knowledge retrieval, and text generation, thereby offering unique advantages in this regard.\nTherefore, it is essential to train a LLM specifically for the audit domain, aligning with the actual requirements and raw data of auditing practices. By refining and tailoring LLM tasks to align with auditing requirements, the audit-focused LLM should grasp the terminology, concepts, and regulations of auditing, ultimately delivering more precise and dependable results, especially for the complicated audit tasks. Guided by the practical applications of national audit, this study aims to identify potential uses of LLM in the audit domain, collect high-quality audit-relevant raw texts and further construct an instruction dataset to build a large language model tailored for audit by fine-tuning a state-of-the-art LLM. This model is referred to as AuditWen.\nThe contributions of this study are as follows:\n(1) Scenarios abstraction. We have categorized the application scenarios of LLM in audit as core requirements, regulatory requirements, and derived requirements. The abstracted scenarios can serve as a roadmap for future researchers to advance the development of LLMs for auditing purposes.\n(2) Multi-audit-tasks. We abstract the corresponding NLP (natural language processing) tasks of LLM from 3 layers, including (a) phrase layer with information extraction and phrase classification, (b) sentence layer with audit-issue summary, audit legal recommendation and QA tasks, (c) document layer with audit risk analysis and audit report generation.\n(3) First open-source audit LLM. It is the first open-source LLM for audit. We have openly released the AuditWen 4, including the instruction dataset, the evaluation benchmark and the model to encourage open research and transparency in the research field.\n(4) Outstanding performance. AuditWen shows significant performance on various of audit NLP tasks compared with the state-of-the-art LLMs, especially in audit"}, {"title": "2 Related Works", "content": "Open Sourced Large Language Models. The GPT (Generative Pre-Training) series of models released by OpenAI has ushered in a new era of large language model. GPTs and other LLMs demonstrate powerful language understanding and generation capabilities through pre-training on extensive text datasets followed by fine-tuning for diverse NLP tasks. Most of the open-source LLMs, such as LLaMA [4], Alpaca [15], Baichuan [1], ChatGLM, Qwen-VL Chat [8], have parameters ranging from 7B and 13B up to 65B. This rapid increase in the number of parameters results in notable enhancements in model power and performance, enabling LLMs to excel in NLP tasks. Generally, LLM building process consists of four main stages, i.e., pre-training, supervised fine-tuning (SFT), reward modeling and reinforcement learning from human feedback. Among the four stages, supervised fine-tuning of a base LLM with instruction dataset can produce superior answers to user queries compared to the base model, all at a lower cost. Along this line, some domain LLMs are proposed by constructing domain-oriented instruction dataset and fine-tuning base LLM (e.g,. LLaMA) with the dataset. For example, PIXIU [12] is an LLM specialized in financial domain, whereas HuaTuo [5] is tailored for the medical domain, both fine-tuned using LLaMA. However, there is currently a lack of open-source LLMs and instruction tuning data specifically tailored for auditing purposes.\nLLM tasks and domain-oriented benchmarks. To compare the performance of different LLMs, researchers have designed various types of LLM evaluation benchmarks and released evaluation reports [20][21]. Among them, Microsoft Research Asia [21] has comprehensively sorted out and summarized 219 relevant studies from the perspectives of evaluation objects, evaluation fields and evaluation methods. In general, the current evaluation tasks are mainly designed from the perspectives of information extraction, text classification and text generation. The evaluation tasks of information extraction mainly include named entity recognition (NER) and key element recognition. The task of text classification includes emotion classification, text classification and entity classification. Text generation tasks include answer generation based on input question, machine translation, document generation in a specified form. Based on the above classification of evaluation tasks, researchers have released the open-sources of the domain evaluation benchmark datasets and fine-tuned domain large language models, such as PIXIU[12], FinBen [13], LAiW [10], HuaTuo [5] and so on.\nCurrently, there is no established benchmark for evaluating LLMs in the field of audit. According to the audit service requirements, this study designs 15 different LLM tasks across 3 layers, constructs the corresponding instruction datasets, and release multi-dimensional evaluation results for both existing mainstream LLMs and our fine-tuned audit-specific LLM, AuditWen."}, {"title": "3 Application Scenarios of LLM in Audit Domain", "content": "3.1 Audit issue summary and laws recommendation\nThe primary task of audit is to identify any potential audit issues within a project and determine which laws and regulations can serve as the audit basis. From this perspective, auditors are seeking LLMs to assist in summarizing audit issues based on audit working papers and recommending suitable laws and regulations as both qualitative and punishment basis.\nThe primary challenge in the application is that an internal auditor may have a divergent qualitative basis for an audit issue compared to a social auditor based on the case description in the audit working paper. For example, an internal auditor may use items from enterprise internal control manual as qualitative basis without any penalty provision, while a social auditor may refer to items in Accounting Law and Criminal Law for punishment. To address this challenge, we propose an audit issue schema that summarizes audit issue from case description and aligns them with the clauses of laws and regulations simultaneously, as shown in Figure 1. We hope to bridge a gap between the clause of laws and regulations and the audit issue."}, {"title": "3.2 Audit Relevant Question and Answer", "content": "The secondary task of LLM used in audit is to answer question related to audit, such as questions list in Table 1. These questions pertain to defining an audit concept, understanding the specifics of a particular clause of a law, determining the methods for investigating and verifying audit issues, and identifying the necessary data to be collected. These diverse questions prompt us to gather relevant audit documents pertaining to audit cases, audit criteria, audit guidelines, and so on. When assessing the quality of answers generated by LLM, it is crucial to minimize the occurrence of hallucination responses and ensure the retrieval of original text based on existing system documents and other relevant content."}, {"title": "3.3 Audit assistant", "content": "Further derive requirement of LLM applied in audit domain is LLM can act as an intelligent assistant and help auditor to extract specified phrase from audit document, do accounting relevant numerical calculation, generate an outline for an audit report and further fill content based on the given audit working papers. The possible case questions are list in Table 2. Audit assistant usually need to execute fine-grained NLP task step by step, such as information extraction, multi-documents summarization and document generation.Additionally, audit assistants must achieve collaborative work between humans and machines with the guidance of human-provided knowledge."}, {"title": "4 AIT: Audit Instruction Dataset and Tuning", "content": "In this section, we initially outline the tasks of audit LLM based on the application scenarios of audit. Then we collect source data and design relevant instruction dataset and evaluation benchmark for audit LLM. At last, we build AuditWen by fine-tuning Qwen [8] with AIT."}, {"title": "4.1 Task abstraction for audit LLM", "content": "Based on the application scenarios of audit, we abstract the audit tasks from three levels, namely, sentence, paragraph and documents, as example shown in Table 4.\nSentence level This level focus on information extraction from sentence and phrase classification."}, {"title": "Audit NER", "content": "Accurately extract audit entity from text is the most elementary task for understanding audit content. We have developed an audit name entity recognition (NER) datasets from annotated sentences that include three types of entities, ORG, audit-issue and audit-basis, as shown in Table 3."}, {"title": "Relation Classification", "content": "Based on two audit entities extracted from a sentence, this task needs to predict the relation between the entity pair from given category set. The relations are defined in Table 7 in Appendix. This task can be used to expand audit knowledge graph by extracting information from unstructured text using LLM."}, {"title": "Phrase classification", "content": "Predict the category of an audit phrase from a set of options, where the phrase is (1) an audit-item entity that need to be classified into one of the given audit item type. (2) An audit issue relevant entity that need to be classified into one of the given audit type. (3) An law and regulation name that need to be classified into one of the given law and regulation category."}, {"title": "Paragraph level", "content": "Question answer (QA) is the task of answering an audit question based on provided information, as shown in Table 1. In this level, we defined several types of question and answer tasks to make LLM understand the common question in audit."}, {"title": "Definition of audit entity", "content": "namely answer the definition of an audit entity, such as what is internal audit? The task makes LLM understand the concept and explanation of common audit entity."}, {"title": "Audit-legal relevant question", "content": "namely answer the question related to audit law, standards, guidelines. These part of QA pairs are very important for tuning an audit LLM, since the core scenario of audit LLM is to recommend appropriated laws and regulations as the audit basis for given audit issue."}, {"title": "Audit-issue relevant question", "content": "namely answer the question related to audit issue, including (1) use a phrase to summarize the audit issue based on case description, (2) describe the specific performance of an audit issue, (3) recommend appropriate laws for a given audit issue."}, {"title": "Other-audit relevant question", "content": "These QA pairs refer to (1) what method can be used in an audit case and what material need to prepare further, (2) what is the objective of an audit project, (3) list out the audit items of an audit project."}, {"title": "Documents level", "content": "This level focus on comprehensive documents analysis and generation, including audit risk/problem analysis, audit case/report generation, as shown in Q3-Q5 of Table 2."}, {"title": "Risk/problem analysis", "content": "namely analyzes the latent risks or issues of an audit project based on provided background information."}, {"title": "Audit document generation", "content": "namely generate an outline, or a template or a complete document based on input query, including (1) generate the audit process for a certain audit case, (2) outline the structure of an audit report for a specific audit matter."}, {"title": "4.2 Instruction dataset construction", "content": "Building upon the audit-oriented LLM tasks, we have developed an Audit Instruction Tuning dataset (AIT) specific to each task. Based on raw texts collected from audit domain discussed in Section 5.1, we need to construct a proper instruction for each of the raw texts.\nFirst of all, for sentence level tasks and part of questions presented in paragraph level, we write five different instructions for each task and evaluate their performance on current LLM based on PIXIU project 6.Then the best instruction is saved for further constructing more instruction data. For audit-legal relevant question in paragraph level that concerns to items in audit laws, we used GPT-4 to generate a question and corresponding answer. For audit report generation task, we write one proper instruction for it because the query of this task is concise. AIT is the first large-scale instruction-tuning and evaluation benchmark dataset for audit LLMs that condensed from audit applications.\nGenerally, following the instructions proposed in PIXIU [12], we build instruction tuning samples with the following templates:"}, {"title": "4.3 Fine-tuning", "content": "We further build AuditWen by fine-tuning Qwen [8] with AIT because AIT is Chinese dataset and evaluation results on several LLMs show that Qwen achieves best performance on our evaluation benchmark dataset. To fine-tune the audit LLM, the audit instruction datasets outlined in Section 4.2 are divided into training, validation, and test sets. All the tasks in the training and validation sets are mixed together for fine-tuning, while each test set is utilized to evaluate the performance of AuditWen and other baseline LLMs."}, {"title": "5 Experimental Results", "content": "5.1 Statistics of instruction dataset\nTo obtain domain data source for fine-tuning an audit LLM, we collect raw documents that relevant to definition of audit entity, audit relevant laws and kinds of structured audit cases that describe the detail process of an audit project, including audit issue, audit method, audit punish law and audit items. The raw data collected from baidubaike, public audit textbook, open law and other public website.\nFrom the raw dataset, we construct an entity-relation classification dataset where two audit entities extracted from a given sentence and it's need to classify the relation between them from given category set. Here, the relations between an entity pair are defined in Table 7 in Appendix, and the truth category tag is labeled by human annotation. The rest of the classification tasks and entity extraction tasks are constructed with the similar way. Based on the raw classification task description and truth category tag, we converted each of them into instruction data with Template (1), as discussed in Section 4.2.\nTo construct audit-legal relevant instruction dataset, we gathered a substantial amount of audit-relevant laws, regulations, criterions and segmented each raw law or regulation into individual items. Then, GPT4 [11] is utilized to generate a question-answer pair (QA pair) based on the input items, as the examples shown in Table 8 in Appendix. The similarity between the original legal-item and the generated QA-pair are evaluated by BERT Score (F1) [16]. The similarity analysis reveals that over 80.1% of QA pairs exhibit a similarity score greater than 0.8, while 19% of QA pairs fall within the similarity range of 0.7 to 0.8,, which denotes that GTP4 can generate QA pair from given legal-item with high quality. Therefore, these QA-pairs can serve as instruction data that effectively capture the essence of the original legal content.\nFor the audit case/report generation task, we collected some representative audit cases or reports with various forms and convert each of them into an instruction data, where the query is a short instruction while the answer is a long document with given form. For the rest of the tasks in paragraph level, raw information are extracted from structured audit cases and converted into instruction data with Template (2) in accordance with specific conditions, as the examples shown in Table 4."}, {"title": "5.2 Evaluation of different LLMS", "content": "Baseline Models. Several strong and representative baseline models are selected to compare with our AuditWen model. For open-sources LLMs, Qwen-7B-Chat, ChatGLM3-6B are selected to perform zero-shot or 5-shot prompting on the audit evaluation benchmark dataset. For close-source LLM, GPT-4 [11] is selected.\nEvaluation Metrics. As the tasks in sentence level are information extraction and classification, missing is employed to evaluate the proportion of prediction results that can be successfully inferred from LLM, while accuracy and F1 are employed to evaluate the classification effectiveness. As the tasks in paragraph level and document(s) level are Q&A task, BERT Score (F1) [16], BART Score [18] are employed to evaluated the similarity between the predict answer and the truth answer. For these two metrics, pre-train models with Chinese language are utilized, i.e., bert-base-chinese and CPT [19].\nIn addition, we evaluate the definition of audit entity and legal recommendation with ROUGE [2], because the answer of these tasks need to be more precise compared with other QA tasks. As word segmentation is a part of ROUGE evaluation, a user dictionary specific to the audit domain is created and loaded into the jieba segmentation tool. For the rest of the tasks, BERT Score (F1) and BART Score are used to evaluate the answer quality.\nOverall Performance. From the 9 audit tasks evaluation results shown in Table 6, our fine-tuned model, AuditWen, significantly outperforms its base model QWen-7B-Chat and other state-of-the-art LLMs, especially in paragraph level and document level tasks. It is because fine-tuned the base LLM with domain-oriented instruction data enables the model to acquire domain-relevant knowledge, comprehend domain-specific queries, and generate outputs in the writing style typical of the audit domain.\nIn the NER task, AuditWen demonstrates significantly higher entity F1 scores compared to baseline models in the 5-shot evaluation, indicating that baselines struggle to accurately identify named entities when provided with five examples from each category for inference.\nIn phrase classification tasks, including audit entity/ audit issue and legal name classification, AuditWen achieves competitive results compared to GPT-4, and outperforms the other models in F1 and accuracy, while ChatGLM3-6B and GPT-4 achieve much lower missing rate. Furthermore, comparing the the zero-shot evaluation results of QWen-7B-Chat and AuditWen across a range of phrase classification tasks, it is observed that QWen-7B-Chat may struggle in zero-shot inference due to a high missing rate, whereas AuditWen excels in overcoming this challenge and achieves higher accuracy.\nComparing the zero-shot and 5-shot result of different models, it is evident that baseline LLMs achieve higher accuracy and lower missing rates under the 5-shot setting, whereas AuditWen demonstrates higher accuracy under the zero-shot setting for relation classification and legal name classification(LNC). It denotes the model can be used for inference without providing extra samples, which further demonstrates the superior domain-generalization capabilities of AuditWen .\nIn the paragraph level and document tasks, AuditWen achieves much higher BERT Score and BART Score in legal recommendation, other-audit relevant question and risk/problem analysis. We believe that the success of AuditWen in these tasks is not only attributed to the suitable instruction template but also to the scale of the fine-tuning dataset for the task.\nFurther Analysis. We further analyze the influences of instruction template on the performance of LLMs on different tasks. For the tasks of relation classification and entity classification, varying instruction templates yield different results for Qwen-7B-Chat and ChatGLM3-6B. For instance, when the question is placed at the beginning of the template, Qwen-7B-chat exhibits poor performance on classification tasks, with accuracy rates of 0.019 for audit-issue phrase classification and 0.009 for legal name classification. ChatGLM3-6B also shows the similar result. When constructing instruction data using Template (1), ChatGLM3-6B achieves the highest accuracy and F1 score in audit entity classification, whereas it only attains an accuracy of 0.312 on the task when the question is placed at the beginning of the template. In addition, for relation classification and legal name classification(LNC), AuditWen shows better performance on zero-shot rather than on 5-shot. This may be attributed to the model's better utilization of general patterns and prior knowledge, as well as its reduced reliance on specific tasks or domains."}, {"title": "5.3 Case study", "content": "Audit-relevant document, such as audit report, is totally different from the document in general domain. Table 8.3 in Appendix compares the audit reported generated by AuditWen and GPT-4. The result demonstrates that the AuditWen can both understand the task with generating a report outline of satisfying the template and can generate more detail content of matching the outline, although only dozens of train instructions in audit case/report generation task for fine-tuning. On the contrary, base models without fine-tuning with the task fail to generate the audit report of meeting specified format."}, {"title": "6 Conclusion and Discussion", "content": "In this study, we presented AuditWen, the first audit-oriented open-source large language model. Along with the model, we also release the fine-tune model AuditWen and the evaluation benchmark dataset. Drawing from the discussion on application scenarios of LLM in audit, we have identified various audit tasks. Subsequently, we gather and construct a large-scale audit instruction dataset to fine-tune a domain-specific large language model tailored for audit tasks. The extensive evaluation results on the proposed benchmark dataset demonstrated the effectiveness of the AuditWen.\nNevertheless, while acknowledging the positive contribution of this study, we also recognize the following limitations. Resource Constraints. Due to time constraints, the scale of dataset for fine-tuning AuditWen is limited, which may not support for fine-tuning model with larger scales. Model and Training Constrains. We only presented the AuditWen models with 7B parameters. Due to computational and resource constraints, AuditWen models with 14B or 30B have not been released so far.\nFor the further work, more relevant source texts about audit cases and statute will be collected and more elaborate tasks such as audit-issue phrase extraction from clause of statute will be constructed. Based on these dataset and tasks, we devote to train a larger-scale of audit-oriented LLM."}]}