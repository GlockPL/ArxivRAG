{"title": "VisTabNet: Adapting Vision Transformers for Tabular Data", "authors": ["Witold Wydma\u0144ski", "Ulvi Movsum-zada", "Jacek Tabor", "Marek \u015amieja"], "abstract": "Although deep learning models have had great success in natural language processing and computer vision, we do not observe comparable improvements in the case of tabular data, which is still the most common data type used in biological, industrial and financial applications. In particular, it is challenging to transfer large-scale pre-trained models to downstream tasks defined on small tabular datasets. To address this, we propose VisTabNet a cross-modal transfer learning method, which allows for adapting Vision Transformer (ViT) with pre-trained weights to process tabular data. By projecting tabular inputs to patch embeddings acceptable by ViT, we can directly apply a pre-trained Transformer Encoder to tabular inputs. This approach eliminates the conceptual cost of designing a suitable architecture for processing tabular data, while reducing the computational cost of training the model from scratch. Experimental results on multiple small tabular datasets (less than 1k samples) demonstrate VisTabNet's superiority, outperforming both traditional ensemble methods and recent deep learning models. The proposed method goes beyond conventional transfer learning practice and shows that pre-trained image models can be transferred to solve tabular problems, extending the boundaries of transfer learning.", "sections": [{"title": "1 Introduction", "content": "Deep learning achieved tremendous success in various domains, including natural language processing (NLP) [33], computer vision (CV) [16], or reinforcement learning (RL) [20]. By feeding modern neural network architectures with a large amount of data, we can generate a model that not only solves the underlying task but also can be transferred to downstream tasks at low conceptual and computational cost. Transformers have recently become one of the most prominent neural architectures used in NLP [31] and CV [7]. By applying a self-attention mechanism, transformers allow capturing global dependencies across the entire text or image, demonstrating remarkable improvements over convolutional and recurrent neural networks. In consequence, it is not surprising that several works focus on applying transformers beyond CV and NLP domains.\nIn real-world applications, one of the most common data types is tabular data, comprising samples (rows) with the same set of features (columns). Tabular data is used in many fields, including biology [29], medicine [25], finance [5], manufacturing [22], and many other applications that are based on relational databases. According to recent reports, data science and machine learning developers work with tabular data as often as with texts or images\u00b9. This proportion is partially reflected in the Kaggle service\u00b2, where 6 688 of available datasets are tagged as \"tabular\", 4 908 datasets contain the tag \"image\" and 178 datasets are tagged as \"text\".\nAlthough practical applications are dominated by tabular data, deep learning models do not confirm significant improvements over traditional ensemble models based on decision trees, such as XGBoost and Random Forests, in this domain\u00b3 [3, 15]. Due to its heterogeneous nature and intricate feature relationships, the complexity of tabular data poses challenges to existing deep learning models [11, 19]. Moreover, the small sample size of many tabular datasets is extremely problematic to handle by deep models. Inspired by the success of transformers in CV and NLP, analogical architectures have been crafted for tabular data [9], but it is not obvious how to pre-train and transfer these models to downstream tasks [34]. Moreover, creating and training transformer-based models for every single domain requires high conceptual and computational cost and may not work for small datasets, which are, however, ubiquitous for tabular data.\nIn this paper, we follow an emerging trend in deep learning and focus on transferring as much knowledge from other modalities as possible to construct a tabular transformer. Inspired by the idea of transfer learning, we reuse the Vision Transformer (ViT) to solve tasks defined for tabular data; see Figure 1. In contrast to the typical reasoning behind transferring a feature extractor inside the same domain, we explore cross-"}, {"title": "2 Related Work", "content": "Tabular data is one of the most prevalent mediums in the world, right next to natural language, with over 5400 datasets present in OpenML [30] alone. For comparison, the most common NLP task in the Huggingface Dataset [17] repository, text classification, is present as a tag in just 2300 datasets.\nIn contrast to computer vision or natural language processing, shallow models, such as Support Vector Machines [6], Random Forests [2] and Gradient Boosting [8], are usually the first choice for learning from tabular datasets. In particular, the family of Gradient Boosting algorithms [8], including XGBoost [3], LightGBM [14], and CatBoost [24], achieve impressive performance and frequently exceed the performance of deep learning models. Both Gradient Boosting as well as Random Forests generate an ensemble of weak learners composed of decision trees, but they differ in the way those trees are built and combined.\nTo take advantage of the flexibility of neural networks, various architectures have recently been proposed to improve their performance on tabular data. Inspired by CatBoost, NODE performs a gradient boosting of oblivious decision trees, which is trained end-to-end using gradient-based optimization [23]. The aim of Net-DNF is to introduce an inductive bias in neural networks corresponding to logical Boolean formulas in disjunctive normal forms [13]. It encourages localized decisions, which involve small subsets of features. Tab-Net uses a sequential attention mechanism to select a subset of features, which are used at each decision step [1]. Hopular is a deep learning architecture in which every layer is composed of continuous modern Hopfield networks [27]. The Hopfield modules allow one to detect various types of dependencies (feature, sample, and target) and have been claimed to outperform concurrent methods on small and medium-sized datasets. The authors of [12] show that the key to boosting the performance of deep learning models is the application of various regularization techniques. They demonstrate that fully connected networks can outperform competitive techniques by applying an extensive search of possible regularizers. The authors of [9] introduced modified versions of ResNet and Transformer and showed that the latter outperforms previous neural network models on large datasets. In follow-up papers, the authors worked to transfer the constructed transformer model to other tabular datasets [34]. Although multiple authors of recent deep learning models often claim to outperform shallow ensemble models, other experimental studies seem to deny these conclusions, showing that typical ensemble methods with careful hyperparameter tuning still presents superior performance [10, 28]. The authors of [11, 19] investigated the situations when deep networks outperform gradient-boosted trees.\nIn various biological applications, authors try to adapt the architectures created for NLP and CV to the biological domain. In the problem of predicting antimicrobial peptides (AMPs), a language model pre-trained on protein fragments was transferred to classify hemolytic activity [26]. The authors of [21] present an image-based deep neural network model to predict AMPs. For this purpose, sequence and structure information is converted into a 3-channel image. In our paper, we go a step further and show that it is possible to transfer ViT pre-trained on images to the case of"}, {"title": "3 VisTabNet model", "content": "In this section, we introduce VisTabNet an adapter network, which allows for a direct transfer of ViT to tabular data. First, we recall the basic idea behind ViT, which is one of the main ingredients of our approach. Next, we discuss possible ways of transferring deep learning models. Finally, we give a detailed description of VisTabNet."}, {"title": "3.1 Vision Transformer architecture", "content": "The Vision Transformer (ViT) stands as a monumental shift in how we approach image classification challenges [7] and takes inspiration from transformers originally designed for NLP tasks. The fundamental idea is simple, yet powerful. It treats images not as a grid of pixels but as a sequence of smaller, fixed-size patches akin to words in a sentence. Each of these patches is then flattened and projected into a higher-dimensional space, where the sequential processing familiar in NLP tasks is applied.\nThe architecture comprises several key components, starting with the patch embedding layer. Here, an input image $x \\in \\mathbb{R}^{H \\times W \\times C}$ is divided into a sequence of patches $x_1,...,x_n \\in \\mathbb{R}^{P \\times P \\times C}$, where (H, W) is the resolution of the image, C is the number of channels, and P is the resolution of the patches. These patches are flattened and transformed into the so-called patch embeddings $t_1,...,t_n \\in \\mathbb{R}^{D}$ using a trainable linear projection:\n(3.1)\n$f : \\mathbb{R}^{P^{2}.C} \\ni x_i \\rightarrow t_i \\in \\mathbb{R}^{D}$.\nTo retain the positional information, which is inherent in image data, position embeddings are added to the patch embeddings, mirroring the process in traditional transformers that deal with text. Additionally, ViT prepends a learnable embedding CLS to the sequence of embedded patches $T_0 = [CLS, t_1, ..., t_n]$, whose state at the output of the Transformer Encoder serves as the image representation.\nFollowing the embedding layer f, the Transformer Encoder is built of multi-head self-attention layers $g_i$, which sequentially transform image representations:\n(3.2)\n$T_i = g_i(T_{i-1})$.\nAttention layers $g_i$ allow the model to weigh the importance of different patches in relation to each other, learning global dependencies across the entire image. Unlike conventional convolutional approaches that emphasize local patterns first and more complicated patterns in the deeper layers, the ViT's attention mechanism inherently allows for the capture of both local and global contextual relationships right from the start, across all layers.\nFinally, the classification head h is attached to the transformed form of the CLS token to produce the final output. This structure enables ViTs to learn intricate patterns and relationships within the image, leading to their success in various image classification tasks."}, {"title": "3.2 Transferability", "content": "Basic idea behind transfer learning is that a part of a neural network pre-trained on an upstream task is used for solving a downstream task. In image processing, we typically transfer initial part of the network (a few first layers), which is responsible for extracting basic features of the image. It has been proven that these features are common for various image datasets [32], and, in consequence, there is no need to learn them for each dataset individually. The user supplies this initial part (feature extractor) with custom layers (e.g. classification head) designed to return the response for a downstream task. To use such a network on a downstream task, one can either train only the weights of the newly created output layers, or adjust the whole network (update the weights of the feature extractor and the output layers). The later approach usually works better if there is enough"}, {"title": "3.3 Cross-modal transfer of ViT", "content": "Building upon the foundational principles of transfer learning, we now explore the feasibility of transferring ViT from the image domain to tabular data a cross-modal transfer that poses unique challenges.\nIn the case of ViT, we have patch embedding layer f, ViT encoder g, and classification head h. If we perform transfer inside the image domain it is natural to transfer $g \\circ f$ and replace only the classification head h. To transfer ViT to tabular data, we cannot directly apply this strategy because the structure of tabular and image data differs. For this reason, we first replace the patch embedding layer f with an adaptation network $\\pi$, which is responsible for adjusting tabular input to the form acceptable by ViT encoder. If we now align the distribution of transformed tabular data with the distribution of patch embeddings using adaptation network $\\pi$, image and tabular inputs to ViT encoder will become more similar. Forcing similarity between these tabular and patch embeddings will lead to the transferability of the ViT encoder.\nAccording to the definition recalled in the previous subsection, ViT encoder $g_{\\theta}$ with pre-trained weights $\\theta$ is transferable from image to tabular data, if we can find the weights $\\phi$ and $\\psi$ such that $h_{\\psi} \\circ g_{\\theta} \\circ \\pi_{\\phi}$ performs at least as good as $h' \\circ g \\circ \\pi$ trained from scratch on a given tabular task. In this paper, we show that this property holds in most cases for ViT (Table 2).\nThe introduced adaptation network $\\pi$ is used to adjust tabular input $x \\in \\mathbb{R}^{M}$ to the form acceptable by the ViT Encoder. It consists of multiple projections $\\pi_i: \\mathbb{R}^{M} \\rightarrow \\mathbb{R}^{D}$, for i = 1,...,n. Each projection $\\pi_i$ implemented by a simple feed-forward network is responsible for creating a single view of the tabular input $v_i = \\pi_i(x)$. These views play a role analogous to the patch embeddings $t_i \\in \\mathbb{R}^{D}$ used in ViT. By replacing the patch embedding layer (3.1) with the adaptation network $\\pi = (\\pi_1,..., \\pi_n)$, we project tabular data into the patch embedding space, which is the input to the Transformer Encoder (multi-head self-attention layers).\nNext, by supplying tabular views with the CLS token, we process the sequence $T_0 = [CLS, v_1, ..., v_n]$ by the ViT encoder (3.2) pre-trained on image data. Finally, we replace the original ViT classification head h by the network h' responsible for classifying tabular inputs. While the introduction of the adaptation layer $\\pi$ is a unique feature of the cross-modal transfer, the modification of the classification head is a common step in transfer learning and, particularly, in fine-tuning ViT.\nIn a typical strategy of training VisTabNet, the parameters of the ViT encoder g are frozen and do not change during training. We only modify (train) the weights of the adaptation network and the classification head h'. Due to the small number of trainable parameters compared to the complexity of the whole VisTabNet model, we can use the benefits of a large model trained at relatively low cost. In particular, this allows us to use VisTabNet on small tabular datasets. Alternatively, we can fine-tune the whole model and adjust the parameters of the ViT encoder as well. In Table 2, we show that this approach can often increase the final score.\nOur findings shed a new light on the area of transfer learning. First, we demonstrate that transfer learning goes beyond using pre-trained feature extractor and can be applied to middle layers of the network. Second, we show that it is possible to effectively perform a cross-modal transfer from image to tabular data. In cross-modal transfer, we use a large-scale model with pre-trained dependencies, but at the same time, we avoid the computationally expensive process of training it from the ground up. This is especially profitable in training of deep models on small tabular data containing less than 1k samples, which are ubiquitous in the tabular domain."}, {"title": "4 Experiments", "content": "This section presents the experimental evaluation of VisTabNet. We start by comparing VisTabNet with state-of-the-art shallow and deep methods in tabular data classification. Next, we investigate the application of VisTabNet in the case of an extremely small number"}, {"title": "4.1 Tabular Data Classification", "content": "First, we benchmark VisTabNet against well-established shallow methods and recent deep learning models on publicly available examples of tabular data in the classification tasks. To take the advantage of our transfer learning approach, we intentionally focus on small datasets with less than 1k samples, in which VisTabNet performs best. At the end of this subsection, we evaluate VisTabNet in the few shot scenario.\nExperimental setup We consider small datasets retrieved from the UCI repository, which are summarized in Table 4. Small datasets are the most challenging case for deep learning methods, but thanks to applying transfer learning principle, VisTabNet is capable of reducing the overfitting issue.\nVisTabNet is compared to the following methods: (i) RF: Random Forests [2], (ii) GB: Gradient Boosting [8], (iii) XGBoost [3], (iv) LightGBM [14], (v) ResNet [9], (vi) FT: Feature Transformer [9], (vii) NODE: Neural Oblivious Decision Ensembles) [23]. These methods were selected due to their popularity and proven effectiveness in tabular data classification tasks, serving as a comprehensive baseline for measuring VisTabNet's performance [11, 19].\nWe apply double cross-validation procedure. The hyperparameters are selected using train-validation splits, while the models' performance is reported on train-test split. For each dataset, we perform careful hyperparameter optimization using PyHopper library, executing 50 optimization steps with four running in parallel and a seeding ratio of 0.5. The best hyperparameters are the ones that perform best on the validation set, so the test set is never used for tuning. Each method uses identical train-validation-test splits. To avoid random effects, the experiments are repeated three times on different splits. Addressing the potential issue of class imbalance, we employ the Random OverSampler to re-sample the training dataset.\nAs an evaluation metric, we employ Matthews Correlation Coefficient (MCC) [18], which is known to be robust to imbalance classification problems [4]. It calculates the correlation coefficient between the observed and predicted classifications, producing a value that ranges from -1 to 1. A coefficient of 1 signifies a perfect prediction, 0 is no better than random guessing, and -1 indicates total disagreement between prediction and observation.\nResults VisTabNet achieves the highest average MCC score and obtains the best rank, see Table 1. Its mean score is 2.5 percentage points higher than the second-best deep model (NODE) and 1.62 percentage points higher than the best shallow model (RF). It demonstrates that the cross-modal transfer applied by VisTabNet is more effective than training deep networks from scratch, especially in the context of small datasets. While the competitive transformer model (FT) obtains relatively good rank, it failed to succeed on multiple datasets, which resulted in worse mean MCC score. The results also confirm that shallow methods represent strong baselines, which are difficult to outperform by advanced deep models. Moreover, comparing the standard deviations show that the performance of VisTabNet is more stable than competitive deep models.\nFew-shot transfer learning Transfer learning is extremely efficient in the case of small sample problems. In this part, we consider an extreme case, where only a few examples of each class are available in a downstream task (from 1 to 10 examples per class), which is analogous to N-shot scenario. We restrict our attention to 5 datasets (Credit Approval, Cylinder Bands, Dermatology, Libras, Zoo) and shallow methods, which are not so prone to overfitting as deep models.\nThe results presented in Figure 2 show that VisTabNet outperforms the rest of the approaches when more than 2 examples per class were available. While RF and GB return better results for 1-shot case, they are not able to use as much information from more examples as VisTabNet. It confirms superior transfer learning capabilities of VisTabNet."}, {"title": "4.2 Analysis of VisTabNet components", "content": "In this part, we analyze the main building blocks of VisTabNet. We investigate the influence of finetuning techniques, the selection of transformer encoder, depth of the adaptation and classification networks as well as reduction of layer in the ViT encoder. This analysis was also conducted on 5 additional datasets: Credit Approval, Cylinder Bands, Dermatology, Libras, and Zoo.\nBackbone selection VisTabNet can be instantiated with various ViT architectures, e.g. ViT Base, or ViT Large. There appears a question of how the selection of ViT backbone influences the final performance of the model. Second question concerns the selection of the optimization procedure. We can either (i) train only the adaptation and output networks as it was done in our main benchmark, or (ii) fine-tune the ViT encoder after initial training of the adaptation and output networks, or (iii) train all components of VisTabNet at once (including ViT encoder). Finally, we can ask what is a benefit of applying pre-trained ViT encoder. For this"}, {"title": "5 Conclusion", "content": "In this paper, we introduced a cross-modal transfer, which allows for reusing a neural network pre-trained on images to process tabular data. This idea was realized on the ViT architecture, in which we replaced patch embedding network with an adaptation layer. By forcing the similarity between transformed tabular inputs and the embeddings of image patches, we obtained transferability of ViT encoder with a minimal conceptual and computational cost. Our approach demonstrates that transfer learning goes beyond reusing feature extractor in computer vision, and can be applied to middle layers of neural networks as well as is feasible in cross-modal setting. As a future work, we leave the question whether a cross-modal transfer can be applied to network architectures different from transformers. Finding positive answers to this problem can open up new avenues in transfer learning."}, {"title": "A Experimental setup", "content": "To aid in reproducing the results, we present technical details regarding our experiments.\nInitially, we divided the dataset into training and testing parts, allocating three-quarters of the data for training and the remaining quarter for testing. Subsequently, both the training and testing parts were preprocessed based on the characteristics observed in the training set, ensuring that the models were trained on data representative of the real-world scenarios they would encounter.\nTo further refine the training process, the training dataset was then split again, this time into training and validation datasets with proportions of four fifths and one fifth, respectively. This resulted in final proportions of the train, valid, and test sets being 12/20, 3/20, and 5/20 of the entire dataset. This split was instrumental in tuning the models and preventing overfitting.\nUpon completion of hyperparameter optimization, the training and validation datasets were merged into a single full_train dataset. The models then underwent final training on this full_train dataset, utilizing the hyperparameters identified as optimal in the previous step. This comprehensive training regime, culminating in testing on the separate test split, was designed to mitigate any risk of cross-contamination in the results, ensuring the integrity and reliability of our findings.\nHyperparameter optimization was performed using the PyHopper library, executing 50 optimization steps with four running in parallel and a seeding ratio of 0.5. This optimization was carried out on the train/validation splits, allowing us to fine-tune the models for optimal performance. We used the following ranges of hyperparameters for each method:\nLightGBM\nnum_leaves = choice(2, 4, 8, 16, 32, 64),\nmax_depth = choice(-1, 2, 4, 8, 16, 32, 64),\nlearning_rate =float(0.001, 0.1, log=True),\nn_estimators = choice(10, 50, 100, 200, 500, 1000)\nXGBoost\nn_estimators = int(50, 1000, multiple_of=50, init=50),\nmax_depth = choice(2, 3, 5, 10, 15),\nlearning_rate = float(1e-5,1e-1, log=True),\nmin_child_weight = choice(1, 2, 4, 8, 16, 32),\ngamma = choice(0, 0.001, 0.1, 1)\nRandom Forest\nn_estimators = int(50, 3000, multiple_of=50),\nmax_features = choice(None, 'sqrt', 0.2, 0.3, 0.5, 0.7),\ncriterion = choice('gini', 'entropy'),\nmax_depth = choice(None, 2, 4, 8, 16)\nGradient Boosting\nn_estimators = int(50, 3000, multiple_of=50, init=50),\nmax_depth = choice(2, 3, 5, 10, 15),\nlearning_rate = float(1e-5,1e-1, log=True)\nNODE\nlayer_dim = int(64, 1024, power_of=2),\nnum_layers = int(1,5),\ndepth = int(2, 7)"}, {"title": "B Detailed results", "content": "Figure 5 presents the MCC score of VisTabNet when only the part of the ViT encoder was transferred to VisTabNet architecture. Other layers were completely removed.\nFigure 6 presents MCC scores across training epochs for 5 datasets. Red color indicates the phase of training adaptation and classification networks while blue color shows the fine-tuning phase of the entire model (including Vit encoder). As can be seen the learning rate has to be carefully scheduled to avoid drops in performance."}]}