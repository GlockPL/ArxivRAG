{"title": "Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models", "authors": ["Fan Zhang", "Shulin Tian", "Ziqi Huang", "Yu Qiao", "Ziwei Liu"], "abstract": "Recent advancements in visual generative models have enabled high-quality image and video generation, opening diverse applications. However, evaluating these models often demands sampling hundreds or thousands of images or videos, making the process computationally expensive, especially for diffusion-based models with inherently slow sampling. Moreover, existing evaluation methods rely on rigid pipelines that overlook specific user needs and provide numerical results without clear explanations. In contrast, humans can quickly form impressions of a model's capabilities by observing only a few samples. To mimic this, we propose the Evaluation Agent framework, which employs human-like strategies for efficient, dynamic, multi-round evaluations using only a few samples per round, while offering detailed, user-tailored analyses. It offers four key advantages: 1) efficiency, 2) promptable evaluation tailored to diverse user needs, 3) explainability beyond single numerical scores, and 4) scalability across various models and tools. Experiments show that Evaluation Agent reduces evaluation time to 10% of traditional methods while delivering comparable results. The Evaluation Agent framework is fully open-sourced to advance research in visual generative models and their efficient evaluation.", "sections": [{"title": "1 Introduction", "content": "Visual generative models have made significant progress in recent years, particularly driven by the advancement of diffusion models (Ho et al., 2020) and the availability of internet-scale datasets (Bain et al., 2021; Chen et al., 2024b; Xue et al., 2019). These advancements enable the generation of high-quality images and videos, opening up a wide range of applications in content creation, design inspiration, and beyond.\nWith the advancement of visual generative models, effective evaluation is crucial for understanding their strengths, limitations, and areas for improvement. Existing evaluation frameworks, such as VBench (Huang et al., 2024a,b), EvalCrafter (Liu et al., 2023), and T2I-CompBench (Huang et al., 2023), assess models across multiple dimensions using specific prompts and tailored metrics to ensure comprehensive performance analysis. However, these approaches often demand generating numerous samples, resulting in long evaluation time and high computational costs, particularly for diffusion-based models where sampling is inherently slow due to iterative sampling. Furthermore, these evaluation frameworks are constrained by rigid evaluation pipelines and predefined dimensions, making them less adaptable to open-ended inputs or diverse user needs. Additionally, these methods often produce single numerical scores as outcomes, requiring users to invest additional effort to extract meaningful insights.\nIn contrast, human evaluators can quickly gain a general understanding of a model's performance by interactively testing a few prompts, forming a sufficient impression without taking too much time. This type of evaluation has several unique advantages for assessing visual generative models. First, it is fast, requiring only a small number of samples to assess overall performance. Second, it is flexible, allowing intuitive evaluation of various aspects, such as realism, creativity, prompt adherence, or other user-defined criteria. Third, it is dynamic, enabling deeper, hierarchical evaluations through continuous adjustments in exploration.\nTo leverage the strengths of human-like evaluations, we introduce the Evaluation Agent, a paradigm that mimics human strategies for assessing visual generative models. The Evaluation Agent offers four key features: 1) Efficiency: It dynamically adjusts its evaluation pathway based on intermediate results, uncovering subtle model behaviors and limitations while avoiding redundant test cases for efficient evaluation. 2) Promptable Evaluation: Unlike existing benchmarks with fixed prompts and evaluation metrics, it accepts open-ended user input, allowing for flexible and customized assessments tailored to specific user needs. 3) Detailed and Interpretable Results: It provides interpretable, detailed insights beyond single numerical scores, making results accessible to both experts and non-experts. 4) Scalability: The framework supports seamless integration of new metrics and evaluation tools, ensuring adaptability and growth.\nThe Evaluation Agent begins by accepting open-ended user input, specifying what to evaluate and which model(s) to assess. Based on this input, it identifies initial evaluation aspects and leverages appropriate tools to conduct the assessment. It then observes the intermediate results and dynamically refines the direction of further exploration. In the end, it generates a detailed natural language response summarizing the evaluation results, providing a comprehensive analysis of the evaluation process and a clear summary of the model's capabilities as specified in the user input. The Evaluation Agent can also automate various applications, including: 1) Model Comparison: Allowing users to"}, {"title": "2 Related Work", "content": "2.1 Visual Generation and Evaluation\nVisual generative models have gained significant attention in recent years. However, unlike perception tasks, which have clear evaluation metrics such as accuracy, evaluating visual generative tasks is more challenging due to the absence of a definitive \"ground truth\" or single correct answer. Metrics such as FID (Heusel et al., 2017) and FVD (Unterthiner et al., 2018) are commonly used to measure the distance between generated samples and reference datasets. Recent benchmarks, including T2I-CompBench (Huang et al., 2023), VBench (Huang et al., 2024a), and EvalCrafter (Liu et al., 2023), provide multi-dimensional evaluations"}, {"title": "2.2 LLM as a Judge", "content": "Recently, the development of understanding and reasoning capabilities in Large Language Models (LLMs) demonstrates a significant advantage, enabling them to serve as powerful evaluators (Jain et al., 2023; Chiang and yi Lee, 2023; Fu et al., 2023; Zheng et al., 2023). For instance, CoEval (Li et al., 2023b) introduces a two-stage evaluation framework for open-ended natural language generation (NLG) tasks, offering a scalable and cost-efficient alternative to human evaluations. (Pan et al., 2024) demonstrates how LLM-based evaluations enhance downstream tasks for digital agents. These studies showcase the ability of LLMs to reason, explain, and comprehend evaluation processes. Despite these advancements, prior work primarily focuses on improving general reasoning or minimizing hallucination, leaving the use of LLMs for evaluating visual generative models largely unexplored. Furthermore, though LLMs demonstrate considerable proficiency in zero-shot reasoning and planning, devising effective strategies for domain-specific problems remains challenging (Wang et al., 2024b), complicating their role as evaluators for visual generative tasks."}, {"title": "2.3 Agent in Planning & Reasoning", "content": "Agents and agentic systems are gaining attention for their ability to automate complex tasks and design customized trajectories based on user queries. They have been explored across various domains, including web, mobile, desktop, and operating systems (OS) (Zhou et al., 2023; Xie et al., 2024; Wang et al., 2024a; Kapoor et al., 2024; Zhang et al., 2024b), showing effectiveness in improving long-horizon task completion. For example, Chain-of-Thought (CoT) (Wei et al., 2022) and Zero-shot-CoT (Kojima et al., 2022) use prompting techniques to enable step-by-step reasoning. Similarly, ReAct (Yao et al., 2022) introduces a general paradigm for agent prompting by integrating reasoning traces with task-specific actions through interleaved triplets of \u201cthought-action-observation,\u201d thereby incorporating environmental feedback. Despite the broad applicability of agents in task automation, their potential to automate the evaluation process of visual generative models remains largely unexplored."}, {"title": "3 Methods", "content": "3.1 Preliminaries: Evaluation of Visual Generative Models\nEvaluation Benchmark. $C = \\{c_j| j \\epsilon \\{1,2,3, ...., N\\}\\}$, where the conditions (i.e., test cases) C is a set of text prompts, input images, class labels, or conditionals in other formats. In the case of unconditional generation, C is empty or consists of random seeds for unconditional generation. In existing evaluation approaches, C is pre-defined and usually contains at least hundreds or thousands of items, which require intensive computation and time cost for sampling. In our Evaluation Agent framework, the test case set C is dynamically determined during the evaluation process, and usually only contains a few cases towards the end.\nSampling. $Uj = G(cj)$ where G is the visual generative model, which generates the visual output vj (i.e., images or videos) given an optional condition cj. V = G(C) where $V = \\{v_j| j \\epsilon \\{1, 2, 3, ...., N\\}\\}$ represents the set of generated visuals for the entire condition set C.\nEvaluation Pipeline. Existing evaluation methods usually follow a fixed pipeline to evaluate all the images or videos V sampled from the pre-defined benchmark C.\n$Y_j = e_k(V_j, C_j)$ (1)\nwhere $e_k \\epsilon E$ is an evaluation function of some aspects such as aesthetic, compositionality etc.."}, {"title": "3.2 The Evaluation Agent Framework", "content": "Our Evaluation Agent framework is powered by LLM-based agents, leveraging their advanced planning capabilities to simulate human-like behaviors for efficient and flexible visual model assessments. As illustrated in Figure 2, the framework operates in two stages: the proposal stage and the execution stage. By iteratively interacting and looping between these stages, the framework dynamically evaluates models based on user queries.\n3.2.1 Proposal Stage\nThe Proposal Stage consists of two agents: the Plan Agent and the PromptGen Agent. The Plan Agent is responsible for planning, observing, and summarizing the evaluation process based on the user's query, while the PromptGen Agent focuses specifically on the design aspects.\nPlan Agent. We design the Plan Agent to simulate human behavior during the evaluation process,"}, {"title": "3.2.2 Execution Stage", "content": "The Execution Stage is responsible for sampling and evaluating the model using the appropriate tools, as specified in the Proposal Stage, and for returning the final evaluation results.\nVisual Generative Models. This component takes prompts designed by the PromptGen Agent as input and generates corresponding visual content, which is then used for subsequent evaluation.\nEvaluation Toolkit. The Evaluation Toolkit consists of a set of elementary evaluation tools for visual generative models. This module is open and extensible, allowing for continuous expansion. We"}, {"title": "3.2.3 Overall Pipeline", "content": "The Evaluation Agent's process is dynamic and multi-round, with each round comprising a proposal stage and an execution stage. By interacting and looping through these stages, we achieve dynamic evaluation, where the evaluation process adapts based on intermediate observations and initial user query. This dynamic approach allows the Evaluation Agent to refine its focus iteratively, adjusting its exploration direction and prompt design based on an evolving understanding of the model's capabilities. Consequently, the evaluation process becomes more efficient and targeted, systematically identifying the strengths and limitations of generative models."}, {"title": "4 Experiments", "content": "We first validate the efficiency of our Evaluation Agent on established benchmarks for visual generative models and then demonstrate the flexibility, depth, and accuracy of our approach in handling open-ended user queries on our self-constructed dataset.\n4.1 Experiments on Existing Benchmarks\nWe validate the effectiveness of our framework on both the Text-to-Video (T2V) and Text-to-Image (T2I) tasks. For detailed settings and implementations, please refer to Appendix B.\n4.1.1 Experimental Setup\nVisual Generative Models. For the T2V task, we select four open-source models: VideoCrafter-0.9 (He et al., 2022), VideoCrafter-2 (Chen et al., 2024a), Latte-1 (Ma et al., 2024), and ModelScope (Wang et al., 2023). Similarly, for the T2I task, we choose four well-known open-source models: SD(Stable Diffusion)1.4 (Rombach et al., 2022), SD2.1 (Rombach et al., 2022), SDXL (Podell et al., 2023), and SD3.0 (Esser et al.,"}, {"title": "4.1.2 Results Analysis", "content": "Validation on VBench. To highlight the efficiency of our proposed methods, we compare the time consumption and sample counts between VBench and our approach. As shown in Table 4, our method reduces evaluation time by over 10X. Additionally, Table 2 confirms the consistency of our evaluation results with VBench across various dimensions. The quantitative results show that the Evaluation Agent achieves high prediction accuracy across"}, {"title": "4.2 Experiments on Open-Ended User Query", "content": "We demonstrate the flexibility of our framework and the benefits of its dynamic evaluation through experiments on an open-ended user query dataset that we collect and construct.\n4.2.1 Open-Ended User Query Dataset\nWe create an open-ended user query dataset comprising 100 user queries focused on evaluating generative model capabilities. Each query is manually labeled with Ability, General/Specific, and Specific Domain tags. For detailed statistics, please refer to the Appendix C.\n4.2.2 Experimental Setup\nThe Evaluation Agent demonstrates strong planning capabilities and accepts any input, but its effectiveness is limited by restrictive evaluation tools, which hinder its ability to handle open-ended queries. To overcome this limitation, we propose a simple yet intuitive solution: leveraging a VLM as an evaluation tool in the form of VQA. During each evaluation round, the PromptGen Agent not only designs prompts for specific sub-aspects but also generates corresponding questions based on the content of each prompt and the aspects to be evaluated. The generated sample and questions are input into the VLM, which provides answers that are then fed back to the Plan Agent for further analysis and planning.\n4.2.3 Open-Ended User Query Evaluation\nMost visual generation benchmarks use predefined dimensions and prompts to evaluate models, but this fixed approach often overlooks users' specific needs, such as handling unique scenarios or objects. A user study found that 67.44% (29 of 43) participants prioritized models meeting their specific needs over general performance. Additionally, fixed prompts can lead to targeted optimization, resulting in misleading evaluations.\nWe address these issues using the Evaluation Agent, which conducts dynamic, multi-round as-"}, {"title": "5 Further Discussions", "content": "In this section, we discuss the unique aspects of the Evaluation Agent compared to traditional benchmarks, as well as its potential broader applications.\nDynamic and Multi-Step Evaluation. One of the core features of the Evaluation Agent is its dynamic, multi-step evaluation process. This structured evaluation paradigm enables the discovery of nuanced differences in model capabilities and provides a more detailed analysis of a model's strengths and weaknesses. Specifically, it allows for a hierarchical, step-by-step evaluation, progressing from simple to complex tasks, as well as category-based assessments that measure performance across different content types within the same dimension. In contrast, traditional visual generation evaluation benchmarks, while incorporating diverse prompts carefully designed for each dimension, suffer from limitations such as fixed prompts and a lack of fine-grained prompt categorization. These constraints reduce flexibility, make it harder to draw valuable insights, and increase the risk of models being over-optimized for specific prompts. Furthermore, dynamic evaluations help avoid redundant testing, significantly enhancing efficiency.\nOpen-Ended Evaluation Toolkit. Our framework for evaluating open-ended queries relies on two key aspects: the planning and reasoning capabilities of LLM Agents and the evaluation toolkit's ability to assess diverse dimensions. One approach to building the evaluation toolkit is integrating various tools, allowing the agent to select the most suitable one for each task. However, current evaluation tools are limited, often focusing only on general evaluations and lacking sensitivity to fine-grained details. For instance, CLIPScore (Hessel et al., 2021) captures the general similarity between an image and a caption but fails to detect subtle changes, such as variations in object counts, limiting its effectiveness in specific evaluations. An alternative approach is to design a versatile evaluation tool capable of assessing multiple aspects. A VQA-based format using VLMs is particularly promising, enabling fine-grained evaluation through targeted questions and providing detailed textual outputs that integrate well with LLM Agents. While this method's effectiveness depends on the VLM's capabilities, current VLMs already demonstrate impressive results, with future advancements poised to further enhance performance.\nBroader Applications. The Evaluation Agent not only evaluates the performance of a single model but also facilitates the direct comparison of two models' strengths and weaknesses in specific dimensions by assessing them simultaneously during execution. This feature enables users to determine which model excels in particular areas. Moreover, by accumulating evaluation results across various capabilities, a database can be constructed. Once sufficient information about multiple models is collected, this database can serve as the foundation for building a recommendation system, capable of suggesting the most suitable model based on the user's specific needs."}, {"title": "6 Conclusion", "content": "As the first of its kind, our Evaluation Agent redefines how visual generative models can be assessed, moving beyond rigid evaluation pipelines to offer an efficient and promptable approach. Unlike traditional methods that rely on fixed benchmarks and time-consuming sampling processes, the Evaluation Agent mimics human evaluation strategies. This allows for significant reduction in evaluation time while providing flexibility and adaptability to user-specified criteria. Our framework dynamically adjusts the evaluation process, enabling efficient assessments with fewer samples and offering scalable, customizable integration for a wide range of evaluation tools and visual generative models. By open-sourcing this framework, we aim to inspire further research in the development of more flexible and efficient evaluation methods for visual generative models."}, {"title": "7 Limitations", "content": "The performance of our Evaluation Agent framework is influenced by two orthogonal factors: 1) the reliability of the Evaluation Toolkit, and 2) the capability of the LLMs used to develop the agentic systems.\nEvaluation Toolkit. While we can integrate state-of-the-art (SOTA) toolkits, they may not always perfectly align with human perception, particularly when evaluating visual generative models. This misalignment can negatively impact the accuracy of evaluation results when the Evaluation Agent relies on these tools. Furthermore, although the Evaluation Agent is designed as an open framework capable of handling arbitrary open-form user queries, existing evaluation tools are still limited in covering certain edge cases or specific criteria that users may want to assess.\nLLMs. We found that even the most advanced LLMs occasionally fall short, such as producing inconsistent output formats or struggling with numerical comparisons. Possible solutions include employing post-processing techniques to refine the outputs and using external tools to handle numerical evaluations. With the release of more powerful models, such as o1, we believe these issues can be largely mitigated.\nOur primary contribution is introducing a new evaluation paradigm. We are confident that the utility of the Evaluation Agent framework will continue to improve as stronger LLMs and more human-aligned evaluation toolkits are developed."}, {"title": "8 Ethical Considerations", "content": "The Evaluation Agent could potentially be used to prompt visual generative models to synthesize unsafe or harmful visual content, such as deepfakes or offensive images and videos. This raises ethical concerns regarding the misuse of these visual generative models. We strongly advise users to approach any system involving visual generative models with caution, as improper use could lead to the creation and spread of harmful content."}, {"title": "Supplementary", "content": "In this supplementary file, we provide a detailed explanation of the pipeline in Section A. Next, we present additional experimental details in Section B and elaborate on the open-ended user query dataset in Section C. Furthermore, we discuss additional related work in Section D. Finally, in Section E, we present further experimental results and analyses using different base models, along with a variety of comprehensive evaluation results for open-ended user queries."}, {"title": "A Detailed Explanation of Pipeline", "content": "Our dynamic evaluation pipeline consists of two stages: the Proposal Stage and the Execution Stage. By iteratively interacting and looping between these stages, the framework dynamically evaluates models in response to user queries.\nA.1 Proposal Stage\nThe Proposal Stage consists of two agents: the Plan Agent and the PromptGen Agent. The Plan Agent is responsible for planning each step and providing the final summary and analysis, while the PromptGen Agent specializes in designing prompts for the process.\nPlan Agent. When humans are interested in a specific aspect of a model's capabilities, they often generate content to observe its performance. Through several rounds of iterative generation and observation, they can form a preliminary evaluation of the model's ability in that aspect. During this process, before each round of generation, humans typically consider which direction to focus on. We designed the Plan Agent to simulate this decision-making behavior.\nThe Plan Agent primarily simulates the decision-making process in human evaluations. It is responsible for determining the direction of exploration at each step and for summarizing and analyzing the results. Specifically, at the beginning, the Plan Agent receives a user query related to the model's capabilities. We require the Plan Agent to first propose an initial aspect to explore based on this query. In each subsequent step, it needs to consider both the user's original query and the observations from intermediate results to suggest further directions for exploration, until sufficient information is gathered to evaluate the model's capability in relation to the query. When the Plan Agent believes it has gathered sufficient information, it will analyze and"}, {"title": "A.2 Execuation Stage", "content": "The Execution Stage is responsible for sampling and evaluating the visual generation model based on the evaluation tools selected by the Plan Agent and the prompts designed by the PromptGen Agent.\nSampling and Evaluation. For the designed prompts, we use a visual generation model to sample and generate corresponding images or videos, and then we need to evaluate the quality of the generated content. For humans, the evaluation process is essentially an observation of the generated content. In our pipeline, for closed-domain questions, we evaluate specific dimensions by invoking existing evaluation tools within the closed domain. For open-ended queries, we use a VLM to simulate human evaluation of the generated content in the form of VQA. Finally, we integrate the evaluation results of each generated piece and return them to the Plan Agent for observation and analysis."}, {"title": "A.3 Dynamic Looping.", "content": "Dynamic evaluation refers to initially providing a preliminary focus based on the user's query, and"}, {"title": "B Experiment Implementation Details", "content": "All experiments in the main text were implemented using LLMs as the backbone, with gpt-40-2024-08-06 as the core model set to a temperature of 0.7. The system prompt design was inspired by the CoT (Wei et al., 2022) and ReAct (Yao et al., 2022) frameworks, guiding the agent to solve problems step-by-step and provide explanations at each stage.\nFor T2V tasks, we validate our evaluation approach by comparing the consistency of our Evaluation Agent with VBench's original evaluation scheme across multiple dimensions, with respect to evaluation time, sample count, and final assessment results on four open-source T2V models. For VBench, we selected 15 evaluation dimensions, including Subject Consistency, Background Consistency, and Motion Smoothness, among others. Details could be referred to in Table 2. We used VBench's original scheme to sample and evaluate four models on these dimensions, recording generation time, evaluation time, and results. We categorized 5 tiers based on the sample-level results from VBench: Very High, High, Moderate, Low, and Very Low, based on the scores density distributions. For dimensions scored by proportions, like Dynamic Degree, we used scores from 42 models on VBench's leaderboard for tiering. To ensure fairness, we restricted the evaluation tools and prompts to those in VBench's existing metrics. Our Evaluation Agent assessed each dimension by answering targeted questions, invoking relevant tools, and selecting suitable prompts, guided by detailed definitions and score-tiering information to interpret results and provide an overall performance assessment for each model.\nFor T2I tasks, we conducted the experiment following the similar settings used for T2V models. For T2I-CompBench, we selected 4 dimensions for the experiment, which are: Color Binding, Shape Binding, Texture Binding, Non-Spatial Relationships. Using T2I-CompBench's original evaluation scheme and"}, {"title": "C Open-Ended User Query Dataset", "content": "C.1 Building an Open-Ended User Query Dataset.\nTo create a dataset of user queries focused on evaluating generative model capabilities, we conducted a user study, gathering user queries from various sources about the aspects users find most important when assessing new models. After cleaning, filtering, and expanding the initial collection, we compiled a dataset of 100 open-ended user queries.\nC.2 Dataset Statistics.\nWe manually annotated each query with labels for Ability, General/Specific, and Specific Domain for analysis. The Ability label categorizes the model capabilities targeted by the question into five types: Prompt Following, Visual Quality, Creativity, Knowledge, and Others. For the General/Specific label, high-level questions like \"How well can it visualize my idea from my words?\" are classified as General, while questions focused on specific applications, such as \"How well can the model generate game characters with intricate details, like armor or facial expressions?\" are labeled as Specific Domain. The Specific Domain label further identifies the focus area for these questions, covering fields like Law, Film and Entertainment, Fashion, Game Design, Architecture and Interior Design, Medical, Science and Education, and History and Culture. We visualize the statistical distribution of the dataset across these categories in Figure 5."}, {"title": "D More Related Work", "content": "D.1 Agents Planning & Reasoning Methods\nThe Agent is designed to match human intelligence in decision-making and reasoning, leveraging the core capabilities of LLMs. Several design paradigms in agent designed to boost the performance in agentic systems have been explored. Tree of Thoughts (ToT) (Yao et al., 2024) advances the process by constructing a tree-like reasoning structure, where each node represents a reasoning thought, and the final plan is derived through either a breadth-first search (BFS) or depth-first search (DFS) strategy. Algorithm of Thoughts (AoT) (Sel et al., 2023) and Graph of Thoughts (GoT) (Besta et al., 2024) are the descending works which propel LLMs through algorithmic reasoning pathways and expand the tree-like reasoning structure to a graph-like one respectively. Diagram of Thoughts (DoT) (Zhang et al., 2024a) is a recently proposed approach that models the reasoning process as a directed acyclic graph (DAG) within a single model, effectively reducing circular dependencies and reflecting well-founded logical deduction.\nThe general idea of the agent is to take the free-form natural language inputs from the users, plan accordingly, and take action, where LLMs are commonly used as the reasoning and planning backbones.\nTo enhance the capability of agents in long-chain reasoning tasks, or a generally defined task with compositionality, humans tend to decompose it into simpler subtasks and solve them procedurally, which also triggers the development of a series of works that mimic the reasoning chain from humans. Chain-of-Thought (CoT) (Wei et al., 2022) and Zero-shot-CoT (Kojima et al., 2022) both leverage prompting to trigger them reasoning \"step by step\", while HuggingGPT (Shen et al., 2024) decomposes the tasks into sub-tasks first and solves them independently with Huggingface. However, although those methods attempted to mimic the human thinking process by decomposing tasks and solving each independently, they are still connected in a cascading format, producing only a single-path reasoning chain. Self-Consistent CoT (CoT-SC) (Wang et al., 2022) enhances the original CoT approach by generating multiple reasoning paths and selecting the final answer based on majority voting. Given the trade-off between time and performance, we found that the CoT framework is particularly well-suited for evaluation tasks, as its reasoning process aligns closely with the nature of these tasks.\nThe reasonings are purely defined by the reasoning backbone of the core LLMs, without incorporating the feedback from either environments or agents themselves. Under this setting, agents' reasoning is straightforward but less effective for long-"}, {"title": "D.2 Agent in Action Modelling with Tool-Use", "content": "An important factor that differentiates the assistant and agent could be the action modeling capability. Agents should inherently possess the ability to perceive from the environment and interact with the environment via proposed actions (Xi et al., 2023; Wang et al., 2024b). A trending approach to model the action goal of an agent is the tool-use functionality. (Li et al., 2023a; Qin et al., 2023) proposed benchmarks that can be used to evaluate the tool-use capabilities from the perspectives of API calling functions, requiring the agents to generate or select the appropriate API calls for various tasks and domains based on the natural language inputs. A lot of model-based works also highlights the tool-use functionality, Toolformer (Schick et al., 2024) trained a model in a self-supervised manner to enhance the token prediction while maintaining the generality. Gorilla (Patil et al., 2023) is a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls."}, {"title": "E More Results", "content": "E.1 Experiments on Different Base Models\nWe conducted additional experiments using various base models, including API-based models such as Claude and Gemini, to demonstrate the high extensibility of our framework.\nClaude. We conducted validation experiments using the same setup as described in Section 4.1, but replaced the base model from GPT-40 to Claude-3.5-Sonnet, specifically using the claude-3-5-sonnet-20241022 version. Table 9"}]}