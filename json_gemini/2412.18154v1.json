{"title": "GENESUM: Large Language Model-based Gene Summary Extraction", "authors": ["Zhijian Chen", "Chuan Hu", "Min Wu", "Qingqing Long", "Xuezhi Wang", "Yuanchun Zhou", "Meng Xiao"], "abstract": "Emerging topics in biomedical research are continuously expanding, providing a wealth of information about genes and their function. This rapid proliferation of knowledge presents unprecedented opportunities for scientific discovery and formidable challenges for researchers striving to keep abreast of the latest advancements. One significant challenge is navigating the vast corpus of literature to extract vital gene-related information, a time-consuming and cumbersome task. To enhance the efficiency of this process, it is crucial to address several key challenges: (1) the overwhelming volume of literature, (2) the complexity of gene functions, and (3) the automated integration and generation. In response, we propose GENESUM, a two-stage automated gene summary extractor utilizing a large language model (LLM). Our approach retrieves and eliminates redundancy of target gene literature and then fine-tunes the LLM to refine and streamline the summarization process. We conducted extensive experiments to validate the efficacy of our proposed framework. The results demonstrate that LLM significantly enhances the integration of gene-specific information, allowing more efficient decision-making in ongoing research.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, genomic research has documented extensive data on gene functions, characteristics, and expressions in various species, significantly advancing our understanding. However, the extraction and summary of specific gene knowledge from this burgeoning literature remains a daunting, labor-intensive task, and is mainly carried out by experts. For example, the Entrez Gene database at the National Center for Biotechnology Information (NCBI) [1], which stores comprehensive gene summary information, required numerous researchers to develop and maintain. In addition, the vast majority of genes lack succinct and descriptive summaries [2]. Implementing automation in gene summarization can simplify this procedure, complement the knowledge base, and enable biologists to quickly comprehend essential information about target genes. Existing literature has partially address the gene summary problem. (1) extractive-summarization, which has been the traditional approach due to its straightforward methodology of selecting key sentences directly from texts [3], [4]. However, it often results in summaries that are somewhat disjointed and may lack overall coherence if the extracted sentences do not flow naturally together. (2) generative-summarization, addresses these limitations by synthesizing new content that is not only faithful to the original information but also more cohesive and concise [5]. However, the performance of these generative models is often constrained by the capabilities of the underlying neural networks, which can impact the accuracy and depth of the generated summaries.\nTo enhance the efficiency of this process, addressing the following challenges is essential: (C1) Overwhelming Volume of Literature: The sheer quantity of publications makes it difficult to identify and assimilate key knowledge [6], [7], [8] about specific genes. (C2) Complexity of Gene Functions: Genes often have multiple functions and are involved in various pathways. (C3) Integration of Gene Functions with Literature Knowledge: Effectively combining detailed gene function descriptions with insights derived from the literature is essential for forming a complete picture of gene roles and interactions.\nSummary of Technical Contributions: To achieve this, we introduce Gene Summary Extractor (GENESUM), a two-stage automated gene summary extractor powered by a large language model (LLM). Initially, our system retrieves literature relevant to the target gene and analyzes the inherent relationships among knowledge entities to eliminate redundant content. This step ensures that only the most relevant and unique information is processed. We then fine-tune the LLM to enhance and streamline the summarization process, producing concise and informative summaries that effectively synthesize gene functions and literature insights. The contributions can be listed as follows:\n\u2022 Innovative Formulation and Application: We have defined the gene summary problem within the context of modern bioinformatics and are the first to apply LLMs to this challenge.\n\u2022 Advancement in Data Handling Techniques: We have developed sophisticated data preprocessing techniques that significantly enhance the efficiency and accuracy of information extraction from genetic databases.\n\u2022 Empirical Validation: Through comprehensive experiments and case studies on real-world datasets, we demonstrate the effectiveness of our framework."}, {"title": "II. LLM-BASED GENE SUMMARY GENERATION FRAMEWORK", "content": "A. Literature Retrieving and Filtering.\nThe Importance of Signature Filtering. Various kinds of texts often possess unique signature terms [9], [10]. For instance, the frequent occurrence of words like sweat, competition, and racing in a corpus may suggest that the topic is related to sports or competitions. Likewise, gene knowledge summaries have their own signature terms, particularly those that appear more often. These terms can be identified by comparing their expected versus observed frequencies.\nSignature Terms Filtering Method. We use the Pearson's chi-square test [11] to extract topic signature terms from reference summaries in the training set by comparing the occurrence of terms in reference summaries with that of the randomly selected biological literature. Let R denote the set of reference summaries in the training set and $\\bar{R}$ denote the set of randomly selected Biological literatures. The null hypothesis and alternative hypothesis are as follows:\n$H_o : P(t_i | R) = p = P(t_i | \\bar{R})$\n$H_1 : P(t_i | R) = p_1 \\neq p_2 = P(t_i | \\bar{R})$\nUnder the null hypothesis, the item $t_i$ appears with equal probability in both $R$ and $\\bar{R}$. and $t_i$ is independent of R. In contrast, the alternative hypothesis indicates that the term $t_i$ is correlated with R. In this context, $H_o$ states that term $t_i$ is not a signature term, while $H_1$ proposes that term $t_i$ is a high-frequency signature term.\nWe then construct the following 2-by-2 contingency table.\nThe Pearson's chi-square statistic is computed by\n$\\chi^2 = \\sum_{i,j=1}^{2} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}$\nwhere $O_{ij}$ is the observed frequency and $E_{ij}$ is the expected frequency. According to the chi-square calculation formula, a larger $\\chi^2$ value indicates that the observed frequency is far from the expected frequency, suggesting rejection of the null hypothesis that $t_i$ is a signature term. A smaller $\\chi^2$ value indicates that the observed frequency is close to the expected frequency, supporting the hypothesis that $t_i$ is not a signature term. We retain signature terms with larger $\\chi^2$ to form a set of signature terms, and sentences containing less than three signature terms will be filtered [10]. Finally, a set of candidate sentences is formed.\nB. Gene Ontology Rewrite.\nWhy We Need GO Terms Description. Each gene is unique due to its own functional and structural characteristics. GO annotations [12] provide gene-specific information and have proven useful for selecting Gene Reference into Function (GeneRIF) candidates. We aim to establish a multi-angle gene-specific description in three aspects: molecular function, biological process, and cellular component.\nPrompt-based Gene Ontology Descriptions Rewrite. In this study, we adopted a single-turn dialogue approach and constructed a Prompt [13] tailored to meet task requirements. As shown in Figure2, this process primarily involved the following steps: (1) Provide task instructions. Give LLM a clear and precise task description. (2) Provide representative examples to aid ChatGPT in fully understanding contextual semantic information and improving model performance. We offer four example sentences, encompassing three aspects of GO annotations. (3) Express generation requirements to ensure standardized output. This paper mandates that LLM produces only one sentence per response, not exceeding 10 words, to ensure accurate expansion without additional redundant information. Finally, we collect all generated descriptions to form a gene function description set.\nC. Clustering and Streamline.\nWe denote the set of filtered sentences resulting from the above steps as: $S = \\{s_1, s_2, \\ldots, s_n\\}$ and the set of descriptions as: $T = \\{t_1, t_2, \\ldots, t_m\\}$, where $n$ and $m$ is the size of filtered sentence set and GO term description set.\nVectorization. We first convert each textual data into numerical vectors. In this study, we employ BioBERT [14], [15], [16] for vectorization, which is a domain-specific language representation model pretrained on a large biomedical corpus. Biomedical texts contain rich semantic information, and polysemy is common across different domains. To comprehensively capture sentence information, the encoder part of BioBERT serves as the embedding layer of the model, responsible for generating sentence vector embeddings. BioBERT effectively captures semantic information from the text and converts it into vector representations as follows:\n$V_{s_i} = BioBERT(s_i)$\n$V_{t_i} = BioBERT(t_i)$\nAfter vectorizing the filtered sentence set and GO terms descriptions separately, we can combine them into a global vector matrix V:\n$V = \\{V_{s_1}, V_{s_2}, ..., V_{s_n}, V_{t_1}, V_{t_2}, ..., V_{t_m}\\}$\nClustering. We adopted K-means [17], [18] as the clustering method. Each element in V is described by z features, where z is the same as the hidden side of BioBERT. The observation matrix of n + m objects across z features is structured (each row represents an object, and each column represents a feature). The range of k is set from 3 to 10. This choice stems from the fact that GO annotations have three aspects, thus requiring at least 3 clusters. Additionally, many genes have around 10 GO annotations on average. This approach allows for dynamic adjustment of k based on the performance of clustering with different values, with the aim of achieving optimal results.\nStep-1: The fist step involves randomly selecting k points (each representing an object) from the data matrix V as the initial cluster centers.\nStep-2: Calculate the distance from each object and assign it to the cluster with closet center.\nStep-3: Then we update the centroids of each cluster.\nStep-4: Repeat the aforementioned Step-2 and Step-3 until the position of each cluster center no longer change.\nStep-5: Calculate the Calinski-Harabasz (CH) score for different values of k and select the value of k that yields the highest score to cluster.\nSimilarity comparison. At this stage, we adopt the GO term descriptions to identify the sentence with the lowest cosine distance as key sentences [19] [20]:\n$d_{ij} = Cosine(V_{t_i}, V_{s_j}) = \\frac{V_{t_i}^T V_{s_j}}{\\|V_{t_i}\\| \\times \\|V_{s_j}\\|}$\nwhere $V_{t_i}$ and $V_{s_j}$ are part of the same cluster. A higher cosine value reflects a smaller angle between the vectors, indicating better alignment and greater similarity between the n-dimensional vectors. Consequently, we evaluate the similarity of each sentence and description on the basis of the cosine distance and choose the sentence that shows the highest semantic similarity to the GO terms.\nD. Injection and Generation.\nMethod for Knowledge Injection. The advent of large language models has revolutionized natural language processing, offering significant advantages in text generation and adaptation. The introduction of LoRA (Low-Rank Adaptation) [21] has been pivotal, reducing the number of training parameters, which reduces training time, storage, and computational demands. Combined with prefix adjustment, LoRA improves the adaptability and efficiency of the model.\nWe employ Gemma-7B as the base model, utilizing LoRA to fine-tune the process to generate refined summaries of genetic knowledge [22]. The process begins with the extraction of key sentences for each gene from our training dataset using a key sentence extraction module. Subsequently, we craft a task-specific prompt, as shown in Figure 3, which acts as a prefix during fine-tuning. This setup ensures that the model, post-LoRA fine-tuning, recognizes and efficiently executes the task of generating gene summaries.\nLet the pre-training weight of Gemma-7B be $W_o \\in \\mathbb{R}^{d \\times k}$.\nThe updates can be represented by a low-rank decomposition:\n$W_o + \\Delta W = W_o + W_B W_A$\nwhere $W_B \\in \\mathbb{R}^{d \\times r}$, $W_A \\in \\mathbb{R}^{r \\times k}$, and the rank $r < min(d, k)$. For a linear layer $h = W_o x$, the forward pass is modified to be:\n$h = W_o x + \\Delta W x = W_o x + W_B W_A x$\nThe matrix $W_A$ is initialized with random Gaussian values and $W_B$ is initialized to zero, setting the initial value of $\\Delta W = W_B W_A$ to zero at the start of training. We adjusted only the attention weights for the downstream task and froze the MLP modules, applying LoRA to fine-tune all linear layers simultaneously.\nMethod for Generating Gene Summaries. Following the fine-tuning phase, the fine-tuned Gemma-7B model is used to generate the final gene summaries. Generation is driven by prompts that were used during the fine-tuning phase, ensuring consistency and relevance in the summaries produced. The generation process leverages the trained model's ability to synthesize information and produce output that is not only accurate, but also aligned with scientific discourse."}, {"title": "III. EXPERIMENTAL RESULTS", "content": "In this section, we present detailed experimental setups and conduct comprehensive experimental analyzes and case studies to validate the efficacy of the proposed model.\nA. Experimental Setup.\nDataset Description. We utilized gene function description information from the summary attribute of the database as a reference from NCBI sub-database Entrez Gene. We obtained Medline PubMed IDs for all documents related to the respective genes from the PubMed data provided by Entrez Gene. Among numerous human-related genes, we selected 8,887 genes that had existing gene function description information for experimentation.\nEvaluation Metrics We adopt the ROUGE-1, ROUGE-2, and ROUGE-L as metrics for evaluation as the same in [3], [23].\nBaseline Algorithms To assess the performance of our model, we selected three categories comprising six different models. The first group of baselines is extractive-summarization approaches: (1) Random. This baseline randomly selects five sentences from the candidate sentences about genes and generates the description by the same LLM as GENESUM. (2) LTR [3] use three features as a basis for sentence selection: gene ontology relevance, topic relevance, and TextRank. The second group of baselines is General LLM: We adopt (3) Llama2-70B [24] and (4) ChatGPT-3.5 [24] with prompt and each gene's related literature as context. The third group of baselines is Biology-related LLM: We selected (5) BioMistral-7B-DARE [25] and (6) Llama3-OpenBioLLM-8B [26], large-scale biomedical models trained on meticulously curated training datasets in the field of biology and medicine. Following those approaches with large language models, we created prompts to enable them to automatically identify key sentences from candidate sentences about genes and generate summaries of genetic knowledge.\nHyperparameter Settings and Reproducibility In our experiments, the significance level is set to 0.001, thus the corresponding chi-square value is 10.83. Terms with a $\\chi^2$ value above 10.83 would be selected as signature terms. We have obtained a total of 3710 terms. We consider all fully connected layers in gemma-7B as the target layers to be adapted and rank r=32. Gradient accumulation step is set to 4. Gradient accumulation allows accumulating gradients over multiple batches before updating model parameters, which helps in handling larger batch sizes without consuming excessive memory. The warming steps for the learning rate are 2. The learning rate is set to 3e-4.\nEnvironmental Settings All experiments were conducted on the Ubuntu 18.04.6 LTS operating system, AMD EPYC 7742 CPU, and 1 NVIDIA A100 GPU, with the framework of Python 3.8.10 and PyTorch 2.0.1.\nB. Experimental Results\nOverall Comparison This experiment aims to answer: Can our model effectively generate a summary of genetic knowledge? Table I report the overall comparison results in terms of ROUGE-1-score, ROUGE-2-score, and ROUGE-L-score. Our model significantly outperforms six baselines across three ROUGE metrics, due to leveraging the GO information of each gene for sentence selection and employing fine-tuning of a large model for sentence generation. Furthermore, when comparing performance ratios between different methods, ROUGE-2 shows a greater improvement compared to the other two metrics, by at least fourfold. Biomedical concepts typically appear in candidate sentences with multiple words (e.g.,Gene therapy,Stem cells, Blood pressure, Cell membrane), and higher ROUGE-2 scores indicate our method's ability to capture this characteristic of biomedical texts. From this perspective, it underscores the strengths of our approach.\nStudy of the Technical Component. This experiment aims to answer: What is the impact of each technical component? To answer the question, we design four different ablation variations, each adopting a different strategy to select the"}, {"title": "IV. RELATED WORK", "content": "The field of genetic knowledge has evolved through various methodologies, mainly classified as: Extractive summarization techniques focus on selecting key sentences [28] or fragments directly from the text without altering the original wording by semantic similarity [10] reinforcement learning [4], information-theoretic [29] and learning-to-rank [3]. Generative summarization [5] involves rewriting or generating new sentences that encapsulate the meaning of the original text, often providing more coherent and concise summaries. Those approaches underscore the integration of multiple scoring features to refine the selection process in extractive summarization, which is limited by the generative ability of the underlying language model. In the current era of large language models [30], [31], [32], [33], [34], there is a significant shift towards more sophisticated summarization techniques [24], [35], particularly in handling complex genetic data [36]. We propose leveraging a Language Model (LLM) to expand Gene Ontology (GO) annotations into complete sentences. This method utilizes similarity measures to pinpoint key sentences, ensuring that the summaries are not only relevant but also concise and coherent."}, {"title": "V. CONCLUSION AND REMARKS", "content": "In this paper, we address the challenge of efficiently summarizing the extensive and rapidly expanding literature on gene functions, characteristics, and expressions. To overcome these challenges, we introduce a two-stage automated gene summary extractor, GENESUM, utilizing a large language model (LLM). This system initially refines the literature retrieval process to reduce redundancy and subsequently employs fine-tuning to enhance the summarization output. These advancements signify a substantial step forward in automating and improving the accessibility of gene-related knowledge."}]}