{"title": "Highly Optimized Kernels and Fine-Grained Codebooks for LLM Inference on Arm CPUS", "authors": ["Dibakar Gope", "David Mansell", "Danny Loh", "Ian Bratt"], "abstract": "Large language models (LLMs) have transformed the way we think about lan-\nguage understanding and generation, enthralling both researchers and developers.\nHowever, deploying these models for inference has been a significant challenge\ndue to their unprecedented size and resource requirements. Facilitating the effi-\ncient execution of LLMs on commodity Arm CPUs will expand their reach to\nbillions of compact devices such as smartphones and other small devices. While\nquantizing model weights to sub-byte precision (for example, 4 bits per weight\nor less) has emerged as a promising solution to ease memory pressure, the group\nquantization formats commonly used for LLM quantization have significant com-\npute overheads and a resource-intensive dequantization process. As a result, a\nhigher proportion of compute instructions do not perform multiplies, i.e., real work,\nrendering them unsuitable for meeting the required latency requirements for LLM\nvariants deployed on commodity CPUs. In addition, CPU-based LLM inference\nhas received far less attention in previous efforts. In this work, we propose a set\nof highly optimized kernels to accelerate LLM inference, demonstrate the best\npossible performance, and unleash the full potential of CPUs, particularly Arm\nCPUs. These kernels amortize the cost of loading the operands and the cost of\nweight unpacking across multiple output rows. This, along with the introduction of\nan optimized interleaved group data layout format for weights and decompression\npath optimizations to reduce unnecessary operations and dequantization overhead\nwhile maximizing the use of vector and matrix multiply operations, significantly\nimproves the efficiency of MAC operations. Furthermore, we present a group-\nwise non-uniform codebook-based quantization method for ultra-low-precision\nquantization of LLMs to better match non-uniform patterns in their weight distri-\nbutions, allowing large-scale LLMs to fit on smaller devices and demonstrating\nbetter throughput during token generation while ensuring better quality than the\nstate-of-the-art. Experiments show that applying these improvements to LLMs\nwith 4-bit and 2-bit quantization results in at least 3-3.2\u00d7 improvement in prompt\nprocessing and 2\u00d7 improvement in autoregressive decoding on a single Arm CPU\ncore, compared to LLaMA.cpp-based solution. The optimized kernels are available\nat https://github.com/ggerganov/llama.cpp.", "sections": [{"title": "1 Introduction", "content": "Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range\nof tasks. A language model can predict the next word given a context or a question. LLMs are trained\nwith massive amounts of data to learn language patterns. They can perform tasks ranging from\nsummarizing and translating texts to responding in chatbot conversations. As a result, facilitating\ntheir efficient execution on Arm CPUs will expand their reach to billions of Arm devices. LLMs are\noften memory-bandwidth and memory capacity-bound, with memory accesses dominated by weights,\nallowing CPUs the opportunity to achieve competitive performance and outperform other processors\nand accelerators in terms of overall inference/cost. Furthermore, Arm CPUs are pervasive, providing\nportability and flexibility, so a new LLM compression scheme can work seamlessly on Arm CPUs\nwithout much effort. Given all the advantages, this work attempts to unlock the full potential of\nLLMs on Arm CPUs deployed in dataceneters, smartphones, and edge devices.\nDeploying these LLMs for inference has been a significant challenge due to their unprecedented\nsize and resource requirements. One of the primary performance bottlenecks in LLM inference for\ngenerative tasks is memory bandwidth. Quantization has been an effective approach to converting\nhigh-precision (16 or 32-bit) model weights to lower-precision values without significantly affecting\naccuracy. It lowers the model's memory and computational requirements, making it better suited\nfor deployment on devices with limited resources. While 8-bit quantization reduces LLM storage\nrequirements by half, the large scale size of LLMs necessitates quantizing them to even lower\nprecisions (for example, 4, or even lower bit-widths). When quantized to 2 bits, the Llama3 70B-like\nlarge-scale foundation model requires less than 20 GB of memory. As a result, there has been a\nsignificant research interest in achieving even greater compression through ultra-low-precision (e.g.,\n2 bits per weight) quantization, non-uniform quantization, and complex compression schemes. While\nquantization has emerged as a promising solution to the memory bandwidth problem, expensive\ndecoding schemes and large model footprint access of existing quantization methods continue to have\na significant impact on runtime performance. With this advancement in quantization formats and\ncompression algorithms targeting LLMs, a major obstacle still to be overcome is providing effective\nsystem support for these compressed numerical formats for extreme compression regimes (4 or fewer\nbits per weight) so that LLMs can be executed quickly and accurately on end devices such as GPUs\nor CPUs. This motivates the development of faster inference kernels as well as runtime-friendly, fast,\nand accurate quantization methods.\nWhile much of the prior research has focused on GPU-based inference as the primary target sce-\nnario [18, 21, 26, 17, 15], CPU-based inference has received significantly less attention. Although\nopen-source CPU-based inference frameworks, such as LLaMA.cpp [14], can offer decent time-\nto-first-token and generative performance on off-the-shelf CPUs, in this work, we investigate the\npossibility of achieving significantly better runtime performance by designing optimized matrix-\nvector multiplications (GEMV) and matrix-matrix multiplications (GEMM) kernels targeting LLM\ninference of varying low-bitwidths on Arm CPUs.\nFurthermore, existing quantization methods do not perform well at extreme compression ratios,\nsuch as 2-bit quantization, and result in either poor runtime performance despite advanced kernel\noptimizations or notable degradation in quality. We then propose a novel non-uniform codebook-\nbased post-training quantization method that enables ultra-low-precision quantization of LLMs while\noutperforming the state-of-the-art in terms of text generation quality and runtime performance. This\nis accomplished through the innovation of applying non-uniform codebook-based quantization over\ngroup-wise structured LLM weight matrices, fine-grained codebook assignment to weight groups, in\nconjunction with identifying and using as few codebooks for all LLM layers as possible, so that the\ncodebooks for all of them can be stored in an Arm CPU's register file (for example, a single 128-bit\nvector register). This, combined with Arm CPU-optimized codebook-based group-wise quantized\nmatrix multiply kernels, results in significantly improved runtime performance for foundation models\nin the domain of LLM. While we consider ARM CPU-based generative inference as the motivating\nsetup for our work, our techniques are general and should be extensible to other settings as well.\nThe key contributions of this work are as follows:\n\u2022 We develop a set of highly optimized GEMV and GEMM kernels for various low bit-\nwidth group-wise quantized LLMs. With the help of SIMD-aware weight packing and fast\ndecompression path optimizations, these kernels can fully take advantage of available vector\nand matrix multiply instructions to maximize MAC unit utilization, minimize overhead\nand memory accesses, and achieve the best possible performance (to date) on Arm CPUs.\nOur optimized 4-bit group-wise quantization kernels enable 3-3.2\u00d7 faster throughput in the\ncompute-bound time-to-first-token (prefill) and 2\u00d7 higher throughput in the memory-bound\ntoken generation (autoregressive decoding) stages of LLM inference on Arm CPUs when\ncompared to the state-of-the-art. We also present highly optimized kernels for various\nnon-uniform quantization methods, such as scalar and vector quantization types, as well as"}, {"title": "2 Related work", "content": "Quantization of LLMs. Due to the massive size of LLMs, post-training quantization (PTQ) methods\nhave emerged as an essential technique for accelerating LLMs during inference and running them\nefficiently. There has been a surge of interest and an increasing body of work in developing accurate\nPTQ methods targeting LLMs in recent times, as doing so can directly lower the cost of running\nthem. By reducing the precision of pre-trained LLMs, PTQ methods save memory and speed up LLM\ninference while preserving most of the model quality at scale when compared to the performance\nand compute requirements of other compression techniques such as pruning and quantization-aware\ntraining (QAT). Early PTQ works on LLMs such as ZeroQuant [33], LLM.int8() [7], and nuQmm [24]\ndemonstrate the potential to use fine-grained quantization (i.e., group-wise quantization) for model\nweights to achieve better accuracy while at the cost of slightly less compression in comparison to\nstandard coarser-grained quantization methods. Subsequent quantization works, such as GPTQ [13],\ncompress LLM weights more aggressively to 3 or 4 bits, unlocking the possibility of running massive\nLLMs on consumer hardware. GPTQ employs layer-wise quantization in conjunction with Optimal\nBrain Quantization (OBQ) [12], in which the easiest weights are quantized first, and all remaining\nnon-quantized weights are adjusted to compensate for the precision loss. This, combined with fine-\ngrained group-wise quantization, results in high compression rates while maintaining high quality.\nQuIP [2] and QuIP# [28] apply incoherence processing to further quantize LLMs to 2 bits per weight,\nrecognizing that quantization benefits from incoherent weights and corresponding proxy Hessian\nmatrices. SqueezeLLM [16], AWQ [20], and SpQR [9] lines of work observe that a small subset\nof LLM model weights produce noticeably large quantization errors, therefore storing them with\nhigher precision to counteract the accuracy degradation caused by their weight quantization and\ndemonstrating improved PTQ accuracy over previous works. While aggressive weight quantization is\ncritical for LLMs to reduce inference costs, activation quantization is less of an issue due to their\nsmaller memory footprint, so activations are typically quantized to 8 bits. The presence of activation\noutliers can sometimes pose a challenge to weight-activation co-quantization, and subsequent works\nsuch as SmoothQuant [32], Outlier Suppression [30, 31], and OmniQuant [25] addressed this by\nintroducing a per-channel scaling transformation that shifts the quantization difficulty from activations\nto weights, allowing activations to be quantized to 8 bits.\nIn general, most previous LLM PTQ studies have used group-wise quantization, along with some\nadvanced techniques to handle outliers [36, 34, 1, 23, 19, 22]. The open-source inference framework\nLLaMA.cpp [14] also employs group-wise quantization in conjunction with higher precisions for\ncritical layers, such as 4-bits for the vast majority of layers and higher precisions for a few.\nSystem support for low-bit quantized LLMs. The majority of the aforementioned quantization\ntechniques focused on GPU inference as their primary target scenario. For group-wise quantized\nLLMS, GPTQ-for-LLaMA offers 4-bit (INT4) Triton kernels, while GPTQ provides 3-bit (INT3)\nCUDA kernels. vLLM [18] implements optimized CUDA kernels for INT4, INT8, and FP8 data types\nfor both Nvidia and AMD GPUs, and it also enhances memory efficiency by using PagedAttention\nto manage attention key and value memory effectively. Nvidia's TensorRT-LLM inference library\nintegrates optimized GPU kernels from FasterTransformer and employs tensor parallelism to enable\nscalable inference across multiple GPUs. FlashAttention [6, 5] combines all of the attention operations\ninto a single kernel and tiles the weight matrices into smaller blocks to better fit the small SRAM,\nreducing the number of memory accesses between GPU high-bandwidth memory (HBM) and GPU\non-chip SRAM. LUT-GEMM [24] uses lookup tables to perform bitwise computations on GPU\nCUDA cores."}, {"title": "3 Background and motivation", "content": "LLM inference. LLMs are made of transformer layers. Given an input prompt, each round through\nthis LLM network generates a new token, and the new token is fed into the LLM for generating the\ntoken in the next round. Ideally, for the next round, the LLM should need the initial prompt and the\nanswer generated so far as the input context to generate the next token. However, since all the tokens\nexcept the last generated token remain the same as the previous round, in order to save on redundant\ncomputation, the LLM stores the embeddings for them in KV caches when they are generated for the\nfirst time. So in the next round, the LLM simply retrieves the history, state, or embeddings of the\nprevious tokens and processes the last generated token in conjunction with the previous embeddings\nto generate the next token. The LLM updates the history with the last token and repeats the process\nuntil a complete answer is generated. Except for the first round, because the text generation at each\nstep primarily depends on the last generated token, i.e., a single row of input or activation, the text\ngeneration phase for a single inference case mainly involves GEMV operations. On the other hand,\nprocessing the initial multi-token prompt or text generation for batched inference cases (i.e., many\nconcurrent users) involves many rows of input or activation, necessitating GEMM operations.\nGroup-wise quantization. For typical operators in LLMs, weight matrices are significantly larger\nthan activation matrices. As a result, compression of the weight matrix is critical to reducing memory\nand bandwidth consumption, so they are typically quantized to 4 or fewer bits. Typically, tensor-wise\nuniform quantization is used for 8-bit quantization with 256 distinct quantized values, where a single\nfloating-point scale for the entire tensor can convert the quantized values to actual weights with very\nlow quantization noise. However, quantizing a 16- or 32-bit float value to a 4-bit integer (INT4)\nor even fewer bits is complex, as an INT4 can only represent 16 distinct values, compared to the\nvast range of the FP32. One issue with this tensor-wide quantization approach is that LLM weight\ntensors can feature \u201coutliers\u201d having much larger magnitude than the other weights; a scale factor\nchosen to accommodate the outliers results in the remaining weights being represented much less\naccurately, lowering the quantized model's accuracy. As a result, when the weight matrix is quantized\nto 4 or fewer bits, it is typically quantized using group-wise quantization. Group-wise quantization\nhas a finer granularity than standard tensor-wise or channel-wise quantization [4], allowing it to\nreduce quantization noise natively while approaching the full-precision (floating point) quality of a\nfoundation model. Group-wise quantization quantizes in groups, whereby weights are divided into\ngroups of 32, 64, or 256, as shown in Figure 1. Each group is then quantized individually to mitigate\nthe effect of outliers and increase precision.\nFor example, the Q4_0 group quantization format from LLaMA.cpp considers a group size (V) of\n32 and uses an FP16 scale factor to quantize weight values to 4 bits before interleaving the top and\nbottom halves of the 32 4-bit weights into 16 bytes. In the case of activations, size and bandwidth are"}, {"title": "4 Arm CPU architecture optimized kernel design for LLMs", "content": "To this end, we present the design of GEMV and GEMM kernels for group-quantized LLMs optimized\nfor various families of Arm CPU architectures. We consider weights and activations to be quantized\nto 4 and 8 bits, respectively, before delving into optimizations for ultra-low-precision weights (e.g.,\n2 bits per weight) and non-uniform quantization. Matrix-multiply kernels (GEMV and GEMM) of\nQKV, output projections, and FFN layers of an LLM operate on 4-bit weights and 8-bit activation\ninputs, primarily carrying out efficient integer computation and generating FP32 outputs. Attention\nlayers perform computations in higher-precision, such as FP16 or FP32. Prior to performing GEMV\nand GEMM for a LLM layer, FP32 outputs from a previous layer undergo group-wise, dynamic\nquantization instead of per-tensor, static quantization (i.e., scaling factors computed offline) to\nproduce 8-bit activation inputs. Dynamic quantization ensures low quantization noise and high\naccuracy.\n4.1 Optimized GEMV for autoregressive decoding phase\nTo increase the reuse of the input activation vector as well as the use of MAC and vector operations,\nthe GEMV kernel considers a series of consecutive weight columns of an LLM weight matrix at\na time, as shown in Figure 3. The use of multiple weight columns in the GEMV kernel leads to\nincreased reuse of the quantized activation vector and associated scale factor, as well as fewer load\noperations. Furthermore, our optimized GEMV kernel uses vector instructions for weight scale factor\nconversions, enabling it to convert FP16 scale factors of multiple quantized weight groups from\ndifferent weight columns to FP32 values using a single vector operation. While multiple weight\ncolumns improve the reuse of the input vector and the use of vector operations in a GEMV kernel,\nthe overhead from reduction operations and dequantization operations pose a significant challenge\nto group-quantized GEMVs in achieving good MAC unit utilization and thus a high percentage of\nuseful work. We address the overhead of reduction operations through SIMD-aware weight packing,\nwhich interleaves weights from multiple weight columns prior to performing GEMV, and we reduce\ndequantization overhead by saving signed values directly into group-quantized weights.\n4.1.1 SIMD-aware weight packing\nBefore decompressing quantized operands and performing the necessary arithmetic operations of a\nmatrix multiply kernel, the operands must be loaded from memory into the register file. Naive loading\nof 4-bit consecutive weight elements from a quantized group of a single output channel requires the\nfused multiply-accumulate (dot product) instruction vdotq_laneq_s32 to perform additional reduc-\ntion operations. Reduction operations must be performed on partial dot products from different vector\nlanes to obtain the final dot product result for a quantized weight group, as discussed in Section 3\nand Figure 2. This can be avoided if different vector lanes of the vdotq_laneq_s32 instruction\noperate on weight elements from different output channels. This ensures that the accumulators for the\nvdotq_laneq_s32 instruction's various vector lanes can accumulate results from different output\nchannels rather than multiple parts of the same output channel. This in turn necessitates a strided\nweight layout for each vector lane in computation, as illustrated in Figure 3 (middle). A naive weight\nloading scheme would necessitate loading the corresponding quantized group from multiple output\nchannels, incurring additional overhead from pointer arithmetic operations and address calculation\nfor each output channel. Furthermore, non-contiguous access patterns of quantized groups across\nchannels in memory prevent achieving the best possible DRAM bandwidth."}, {"title": "4.2 Optimized GEMM for prompt phase (time-to-first-token)", "content": "While GEMV operations make up the majority of the computations in the decode stage for a single\ninference case (i.e., a request from a single user), GEMM operations dominate the prompt phase\n(prefill stage) for both single and batched inference cases. Furthermore, for batched inference cases,\nGEMM consumes the majority of the computation for the decode stage as well.\nWe apply the same optimizations to developing Arm CPU architecture optimized GEMM kernels as\nwe do for GEMV kernels. In particular, in order to increase compute throughput further, maximize\nthe use of vector operations, and avoid pseudo-scalar operations, we make use of the matrix-matrix\nmultiply-accumulate (MMLA) instruction. The MMLA instruction can perform twice as many MAC\noperations when compared to an equivalent SIMD dot product instruction (for example, 128-bit DOT)\nused in the GEMV kernel. An 128-bit MMLA instruction performs double operations by processing\nmultiple rows of activations at once. It multiplies a 2x8 matrix of 8-bit integer values in the first\nsource vector by an 8x2 matrix of 8-bit integer values in the second source vector to produce a 2x2\nmatrix of 32-bit integer product values.\nHowever, this requires reordering activations from multiple input rows to correspond to the weight\nreordering and value ordering required by the MMLA operation. Because each matrix multiply kernel\nis always preceded by a dynamic re-quantization kernel for FP32 activations, the reorder kernel for\nactivations can be fused into it with minimal latency overhead. Furthermore, a larger number of\ninput activation rows and output weight channels creates high pressure on vector register files for\nstoring partial dot products, causing large GEMM kernels to be register-bound on Arm CPUs due\nto the nature of the output stationary dataflow. As a result, the size of the vector register file for an\nArm processor architecture type influences the number of concurrently processed rows and columns\nand the design of a GEMM kernel. In particular, we design three types of group-quantized GEMV\nand GEMM kernels based on specifications such as the availability of SDOT or MMLA instructions,\nweight channel interleaving patterns, SIMD vector widths, and register file sizes of different available\nArm CPU types."}, {"title": "4.3 Turning intrinsics into assembly", "content": "While we can generally rely on the compiler and use regular intrinsics for the group-quantized\nGEMV and GEMM kernels, we always find that the compiler does not generate fully optimized code,\nespecially when there is high register pressure while running GEMMs during the prefill stage for\nsingle inferences and the prefill and decode stages for batched inferences. As a result, we convert\nintrinsics into assembly code to maximize the use of available vector registers and MAC units, avoid\nspilling of register values to memory and associated restore code, and improve instruction-level\nparallelism, leading to improved compute efficiency."}, {"title": "5 Group-wise non-uniform codebook-based quantization", "content": "Uniform quantization divides the range of weight values into equal intervals and assigns a quantization\nlevel to each interval. It distributes quantized values uniformly and equidistantly. As a result, despite\nbeing commonly used in conjunction with group-wise quantization for LLMs, it is not very flexible\nin matching the non-uniform patterns typically found in LLM weight distributions [16], resulting in\nsuboptimal accuracy, especially for low-precision LLM quantization. Non-uniform quantization with\na codebook allows for a more flexible allocation of high-precision weight values. Given a weight\ndistribution, non-uniform codebook-based quantization can identify k centroids that best represent\nthe weight values and map weights to them. For example, when quantizing a weight distribution\nto 4-bits, state-of-the-art codebook-based quantization techniques aim to determine the 16 centroid\nvalues that best represent the values. Each high-precision weight can then be represented by the\n4-bit index of a centroid in the codebook instead of its original bit-width. In addition, non-uniform\ncodebook-based quantization requires storing the codebook itself and incurring associative overhead.\nWhile there are a few recent non-uniform codebook-based quantization techniques for LLMs in the\nliterature [16, 29, 2, 28], they either do not exhibit good runtime for both phases (prompt processing\nand autoregressive decoding) of LLM inference under ultra-low-bit precision scenarios or do not\nextend well to achieving extreme degrees of compression, as discussed below.\n5.1 Challenges of prior non-uniform quantization techniques\nSqueezeLLM [16]. In order to be more sensitive to the importance of the weights, SqueezeLLM\nquantization first clusters the weights using a weighted k-means clustering algorithm where the\ncentroids of the cluster (codebook) are chosen to be close to the sensitive weights. In other words,\nrather than scaling high-precision weights group-wise into the range provided by a given number\nof bits, SqueezeLLM uses weighted k-means clustering on all weights in a tensor row, mapping\nweights to codebooks with the number of codebooks determined by the bit per weight a quantization\nscheme wishes to spend. However, the improvement in representation of the weight distribution by\nthe SqueezeLLM quantization comes at the cost of loading the codebook for each row of a weight\nmatrix along with the index assignments for the weights. This, combined with FP16 values for the\nper-row codebook entries, results in inefficient floating-point computation and significantly slows\ndown LLM inference, as observed in our evaluations. Furthermore, before using the SqueezeLLM\nquantization, the codebooks for each layer of an LLM must be determined; the same codebook\nentries will not work for an unseen LLM. In addition, SqueezeLLM does not support low-precision\nquantization below 4-bits, which is required to fit large-scale LLMs to resource-constrained devices\nand is thus the primary focus of our proposed method.\nGPTVQ [29]. Recent work GPTVQ extends the potential of non-uniform quantization for higher\nlevels of compression (for example, 2-bit and 3-bit quantization) and outperforms its uniform\ncounterpart. Notably, it makes use of vector quantization, which involves quantizing multiple weights\ntogether and mapping them to a single centroid in a codebook rather than representing each quantized\nvalue with a centroid in the codebook, resulting in a more versatile quantization grid across multiple\ndimensions. It also performs codebook quantization to 8-bits and shares the codebook across multiple\nrows/columns of an LLM layer. While it demonstrates the potential of codebook-based quantization\nfor ultra-low-precision quantization, the accuracy of the resulting LLMs suffers noticeably (for\nexample, for 2-bit quantization).\nQuIP# [28]. State-of-the-art 2-bit quantization technique QuIP# improves upon previous work\nby leveraging lattice codebooks for vector quantization and incoherence processing for superior\noutlier suppression. E8 lattice codebooks encode the magnitude of quantized values in a group of\neight. Besides, QuIP# forces an even number of positive (or negative) signs of quantized values\nin a group of eight. This enables the use of seven bits to record the sign of eight quantized values.\nWhile the combination of these optimizations allows QuIP# to achieve good LLM quality in extreme"}, {"title": "5.2 The group-wise codebook-based quantization method", "content": "Our innovation is motivated by the following observations and insights: For LLM weight matrices,\nwhich are commonly quantized group-wise, there may be some variations in the shape of the Gaussian\ndistribution of values between groups. However, after being scaled by the group-wise scale factors,\nthe Gaussian distributions of various groups with different shapes should be clustered into a small set\nof shapes, each of which can be represented with its own codebook.\nMotivated by these insights, we apply a group-wise structure to divide the high bit-width floating\npoint weights into groups and scale each group separately first, using its own scale factor. The\nscale factor is chosen so that the ranges of values after scaling can be represented by codebook's\nbit-width. For example, if the required bit-width of centroid values in a codebook is a signed 8-bit\ninteger, the scale factor for a group scales the group's weights to the -128 to 127 range. We then\nuse a two-phase clustering algorithm (constraint-satisfaction-guided clustering algorithm) to cluster\nthe scaled group of weight values into a small number of codebooks, each with a few centroid\nvalues. In phase 1, similar weight groups or groups of scaled weight values with similar Gaussian\ndistributions are divided into C clusters by converting each group of scaled weight values to a\nprobability distribution. This is accomplished by finding the histogram of the scaled weight values\nand normalizing it, converting the discrete distribution of intensities into a discrete distribution of\nprobabilities, and then applying k-means clustering to these probability distributions, which now\nrepresent the different groups, to cluster the similar groups. Phase 2 then applies k-means clustering\nanalysis to each clustered group of values (created in phase 1) to identify a few centroid values that\nbest represent the probability distribution of weight values within each and repeats it for C clustered\ngroups of values to create C codebooks to find the different non-uniform weight distributions present\nin the high-precision weights. The clustering of similar groups into the same codebook aids a weight\ngroup later during post-training quantization in locating the closest codebook of the C codebooks\nthat best represents its values, as shown in Figure 5, while the small number of centroid values for\neach codebook ensures that high-precision centroid values, such as four 8-bit signed centroids in a\ncodebook, can be encoded using a lower bit-width index (2-bit indices here) in the codebook.\nFor extreme compression cases, we observe that quantization bit-width and associated number of\ndistinct values or quantization bins (e.g., 16 distinct values for 4-bit quantization) have the greatest\nimpact on LLM accuracy. Changes in the bit-precision of scale factors, on the other hand, have less\nof an impact on LLM accuracy. The various codebooks in group-wise codebook-based quantization\nessentially help in adapting a group to choose a subset of relatively higher-precision data type while\nusing low bit-width indices (4 most important quantization bins of a distribution for a group here"}, {"title": "5.3 Example group-wise codebook-based quantization for 2-bit width", "content": "We apply our codebook-based quantization technique to compress LLMs to about 2, 3, or 4 bits per\nweight. Our 2-bit codebook-based quantization scheme uses a small number of codebooks, such as\nfour, eight, or sixteen, depending on the required compression size and accuracy. As demonstrated\nin Algorithm 1, they are discovered by first applying a group-wise structure and scale factors to\nLLM weight matrices, followed by dividing and clustering similar groups (groups with similar\ndistributions) into a small number. The small number of clustered groups then use a clustering\nalgorithm individually to cluster values in them into an equal number of codebooks, each with four\ncentroids. Later, during post-training quantization of a group-wise structured LLM, the various\ngroups choose one of the codebooks that best matches their distribution with the lowest reconstruction\nMSE (mean square error). As a result, each group typically requires two to four bits to encode the\nselected codebook's index, as well as two bits for each of its elements to encode one of the codebook's\nfour centroids.\nFor example, for some 2-bit linear layers in LLMs, our 2-bit codebook-based quantization technique\nhas four codebooks, each with four signed 8-bit integer centroids. It has a group size of 256, divided"}, {"title": "6 Experiments", "content": "6.1 Evaluation setup\nWe compare our optimized kernels and codebook-based quantization method to LLaMA.cpp, both\nin terms of inference throughput (tokens generated / second) as well as in terms of accuracy of the\nresulting models, measured in terms of perplexity (PPL). LLaMA.cpp lowers the entire computation\ngraph to C++ to minimize overhead on CPUs. We use a prompt sequence length of 128 and an\noutput token generation length of 128, and FlashAttention is enabled for all throughput measurement\nexperiments.\n6.2 Inference throughput for 4-bit uniform quantization\nTable 1 compares the runtime performance of our optimized 4-bit group-wise quantized kernel\n(Q4_0_8_8) to that of LLaMA.cpp's 4-bit kernel (Q4_0) on Graviton3 processors with 64 Arm\nNeoverse V1 CPU cores for different batch sizes (number of users). For the LLaMA-3 8B model,\nQ4_0_8_8 improves inference throughput (tokens per second) by 3-3.2\u00d7 during the prefill stage and\nby up to 2x during the autoregressive decoding or token generation stage. The token generation phase\nfor a bath size of one at high core (thread) counts is memory bound, so our optimized GEMV kernels\nfor it, while offering a significant speedup\u00b9 for a smaller number of cores, cannot provide a tangible\nimprovement at 64 cores over LLaMA.cpp's reference Q4_0 kernel. In the case of the LLaMA.cpp\nFP16 implementation, the prefill rate and token generation throughput increase from 123.5 tokens/s\nto 136.2 tokens/s and 16.9 tokens/s to 106.4 tokens/s, respectively, as batch size increases from 1 to\n32. While the LLaMA.cpp's reference Q4_0 kernels can offer some improvement in throughput over\ntheir FP16 implementation, our 4-bit optimized kernels improved end-to-end throughput significantly\nover FP16 and Q4_0, as shown in Table 1.\nWe also develop different variants of our 4-bit optimized kernels based on their weight interleaving\npatterns, the use of various vectorization techniques, and advanced matrix-matrix multiply (MMLA)\noperations when designing a CPU kernel. Table 2 compares the performance of different variants of\nour optimized 4-bit group-wise quantized kernels. The Q4_0_8_8 and Q4_0_4_8 kernels are designed\nwith MMLA operations, whereas the Q4_0_4_4 kernels do not include any MMLA instructions. The"}, {"title": "6.3 Inference throughput for 4-bit non-uniform quantization", "content": "We extend our proposed SIMD-aware weight packing and fast decompression path optimizations\nto 4-bit non-uniform codebook-based quantization methods, such as LLaMA.cpp's IQ4_NL [14", "8": "or Student Float-like [10"}]}