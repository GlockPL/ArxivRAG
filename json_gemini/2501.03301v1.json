{"title": "Rethinking Byzantine Robustness in Federated Recommendation from Sparse Aggregation Perspective", "authors": ["Zhongjian Zhang", "Mengmei Zhang", "Xiao Wang", "Lingjuan Lyu", "Bo Yan", "Junping Du", "Chuan Shi"], "abstract": "To preserve user privacy in recommender systems, federated recommendation (FR) based on federated learning (FL) emerges, keeping the personal data on the local client and updating a model collaboratively. Unlike FL, FR has a unique sparse aggregation mechanism, where the embedding of each item is updated by only partial clients, instead of full clients in a dense aggregation of general FL. Recently, as an essential principle of FL, model security has received increasing attention, especially for Byzantine attacks, where malicious clients can send arbitrary updates. The problem of exploring the Byzantine robustness of FR is particularly critical since in the domains applying FR, e.g., e-commerce, malicious clients can be injected easily by registering new accounts. However, existing Byzantine works neglect the unique sparse aggregation of FR, making them unsuitable for our problem. Thus, we make the first effort to investigate Byzantine attacks on FR from the perspective of sparse aggregation, which is non-trivial: it is not clear how to define Byzantine robustness under sparse aggregations and design Byzantine attacks under limited knowledge/capability. In this paper, we reformulate the Byzantine robustness under sparse aggregation by defining the aggregation for a single item as the smallest execution unit. Then we propose a family of effective attack strategies, named Spattack, which exploit the vulnerability in sparse aggregation and are categorized along the adversary's knowledge and capability. Extensive experimental results demonstrate that Spattack can effectively prevent convergence and even break down defenses under a few malicious clients, raising alarms for securing FR systems.", "sections": [{"title": "Introduction", "content": "As an essential way to alleviate information overload, recommender systems are widely used in e-commerce (Ying et al. 2018), media (Wang et al. 2018; Wu et al. 2019a), and social network (Fan et al. 2019), recommending items that users may be interested in. Despite the remarkable success, conventional recommender systems require centrally storing users' personal data for training, increasing privacy risks.\nRecently, federated learning (FL) (McMahan et al. 2016) has emerged as a privacy-preserving paradigm and successfully applied to the recommendation area. In federated recommendation (FR) (Sun et al. 2024; Luo, Xiao, and Song 2022), the global item embeddings are uploaded to a central server for aggregation. Meanwhile, each user's interaction data and privacy features are kept on the local client. In this way, the privacy of local data is well protected.\nUnlike general FL systems, FR has a unique sparse aggregation mechanism. As shown in Fig. 1, for general FL, each element (circle) of model parameters can be updated by all n clients, named dense aggregation. While for FR, the interactions of users and items are usually sparse (Ma et al. 2008), resulting in each item's embedding can only be updated by partial clients. For example, client n can only produce and send substantive gradients {$\\nabla v_1^n$, $\\nabla v_3^n$ } for its interacted items {$\\upsilon_1$, $\\upsilon_3$}. For the remaining items, the updates are zero vectors or empty, named sparse aggregation.\nBy far, FR has provided satisfactory performance without collecting users' private data, extending recommendation applications to privacy-sensitive scenarios. Despite success, the model security, as an essential principle, has received increasing attention. Here we consider the worst-case attack, i.e., Byzantine attack (Fang et al. 2024, 2019; Blanchard et al. 2017a), where attackers are omniscient and collusive, and can control several clients to upload arbitrary malicious gradients. Note that Byzantine robustness is especially critical for FR, since in the domains applying FR, e.g., e-commerce, malicious clients can be injected easily by registering new accounts. This raises one question naturally: With the unique sparse aggregations, how robust the federated recommendation model is against Byzantine attacks?\nFor this question, existing Byzantine works cannot be directly employed, since they mainly focus on dense aggregation in general FL (Rodriguez-Barroso et al. 2022; Xu et al. 2021; Blanchard et al. 2017b). Despite a few prior attacks against FR emerges (Yuan et al. 2023; Yu et al. 2023; Rong et al. 2022; Wu et al. 2022), they also neglect to analyze how the sparse aggregation affects the robustness of FR. We answer this problem by solving two challenges: (1) How to define Byzantine robustness under sparse aggregations? Existing Byzantine attacks and defenses are mainly defined based on the dense aggregation mechanism in the general FL. In FR, due to sparse user-item interaction, for an item, its embedding is updated only by its interacted users, and the remaining users upload zero-valued or empty updates. So the aggregated item embedding may be skewed towards zero-valued when directly applying existing dense aggregators, since the zero-value update is majority. Hence, it is vital to transfer them into FR and re-examine their theoretical guarantee and effectiveness. (2) How to design general Byzantine attacks against FR for attackers with different levels of knowledge and capability in reality. Specifically, it is hard to have full knowledge of all users, due to the large number of participating users in FR. Besides, since user-item interactions are usually sparse, Byzantine clients should not update too many items. Otherwise, a monitor based on the number of user interactions can be triggered easily under such aggressive modifications (Wu et al. 2022).\nIn this paper, we make the first effort to investigate the Byzantine robustness of federated recommendation from the perspective of sparse aggregations. For the first challenge, we transfer the existing aggregators to FR by treating the aggregation for a single item as the smallest execution unit. Namely, for each item embedding, the gradients are collected and aggregated separately and concurrently. Based on this, we further point out that such a sparse aggregation mechanism of FR will lead to a unique Byzantine vulnerability: items with different degrees receive different amounts of updates, leading to individual robustness. The degrees of all items usually meet long tail distribution in reality (Abdollahpouri, Burke, and Mobasher 2019), where most items (named tailed items) are only interacted with by a few users, making them extremely fragile. For the second challenge, we design a series of attack strategies, named Spattack, based on the vulnerability from sparse aggregation in FR. Then we categorize them along the attacker's knowledge and capability into four classes. To be specific, following (Xie, Koyejo, and Gupta 2020; Fang et al. 2019; Baruch, Baruch, and Goldberg 2019), we consider both omniscient attacker (Spattack-O) and limited non-omniscient attacker (Spattack-L). Then we further divide them depending on whether limiting the maximum number of each client's poisoned items or not. In summary, our contributions are three folds:\n(1) We first systematically study the Byzantine robustness of FR from the perspective of unique sparse aggregation, by treating the aggregation for a single item as the smallest execution unit. We theoretically analyze its convergence guarantee and point out a special vulnerability of FR.\n(2) We propose a family of effective attack strategies, named Spattack, utilizing the vulnerability from sparse aggregation. Then Spattack can be categorized into four different types along attacker's knowledge and capability.\n(3) We perform experiments on multiple benchmark datasets for different FR systems. The results show that our Spattack can prevent the convergence of vanilla even defense FR models by only controlling a few malicious clients."}, {"title": "Background and Preliminary", "content": "Here, a recommender system contains a set of users $U = \\{U_1,\\dots, U_n\\}$ and a set of items $V = \\{v_1, \\ldots,\\upsilon_m\\}$, where n and m are the numbers of users and items, respectively. Each user $u_i \\in U$ has a local training dataset $D_i$, consisting of implicit feedback tuples $(u_i, v_j, r_{ij})$. These tuples represent user-item interactions (e.g., purchased, clicked), where $r_{ij} = 1$ and $r_{ij} = 0$ indicate positive and negative instances, respectively, i.e., whether $u_i$ interacted with $v_j$. For each user $u_i$, we define $V_{u_i} = \\{v_j \\in V|(u_i, v_j, r_{ij}) \\in D_i\\}$ as the set of the items that interact with $u_i$. Let $U = [u_1,\\ldots, u_n]$ and $V = [v_1,\u00b7\u00b7\u00b7, v_m]$ denote the embeddings of users and items, respectively. The recommender system is trained to predict the rating score $r_{ij} = f_e(u_i, v_j)$ between $u_i$ and $v_j$, where $r_{ij}$ represents how much $u_i$ likes $v_j$, $f_e$ is the score function, {U, V, $\\Theta$\\} are learnable parameters. Then, the system recommends an item list for each user that they might be interested in by sorting the rating scores. In traditional centralized training, the personal dataset $D_i$ of each user $u_i$ is stored on a central server, yielding a total dataset D for model training, which will increase the privacy risks."}, {"title": "Federated Recommendation", "content": "Considering privacy issues, in FR, the privacy data $D_i$ of user $u_i$ is kept on the local device. The shared model parameters $V$ and $\\Theta$ are aggregated over clients by sending the local gradients to a central server. According to the base recommender, the parameters are different: in Matrix Factorization (MF) recommender models, the interaction function is fixed and $\\Theta$ is an empty set. In deep learning-based recommender models, $\\Theta$ is the set of weights of neural networks. Following (Rong et al. 2022), we adopt the classic and widely used MF as the base recommender for simplicity, where $f$ is fixed to be dot product, i.e., $r_{ij} = U_i\u00a9 V_j$. Following (Rong et al. 2022), we take Bayesian Personalized Ranking (BPR) (Rendle et al. 2009), a pairwise personalized ranking loss, as the local loss of each client:\n$L_i(u_i, V) = - \\sum\\limits_{\\upsilon_j, \\upsilon_k \\in V_{u_i} \\atop r_{ij}=1 / r_{ik}=0} \\ln\\sigma(r_{ij} - r_{ik}),$ (1)\nwhere $\\sigma$ is the logistic sigmoid function. It assumes that the user prefers the positive items over all negative items. In each training iteration, the central server sends the current item embeddings $V^t$ to all clients. For each user $u_i$, the client computes loss $L_i(u_i, V^t)$ then locally updates its private user embedding at epoch t as follows:\n$u_i^{t+1} \\leftarrow u_i^t - \\eta \\cdot \\nabla_{u_i},$ (2)"}, {"title": "Byzantine Attack and Defense", "content": "Byzantine Attack. In Byzantine attacks, the attacker aims to degrade model performance and even prevent convergence by controlling a few malicious clients. As shown in Fig. 2(a), malicious client $\\bar{u}_i$ is allowed to send arbitrary (red) gradient $\\nabla V^{\\bar{u}}_i$. Following existing Byzantine attack studies (Xu et al. 2021; Fang et al. 2019; Baruch, Baruch, and Goldberg 2019), considering the worst case, we assume attackers have full knowledge of all benign gradients {$\\nabla\\Theta^1,\u2026,$\\nabla$\\Theta^n$\\} and all the malicious clients are collusive by default, which help to understand the severity of model poisoning threats.\nByzantine Defense. Since servers have no access to the raw training data of clients, the defense is generally implemented on the server side as a robust aggregator, which can filter Byzantine updates and guarantee model convergence. As shown in Fig. 2(a), let {$\\nabla\\Theta^1,\u2026,$\\nabla$\\Theta^n$\\} be the gradient vectors of n benign clients in FL. The server collects and aggregates the training gradient of each client model using a federated aggregator. In non-robust FL settings, coordinate-wise Mean in form of $MEAN(\\nabla\\Theta^1,...,\\nabla\\Theta^n) = \\frac{1}{n}\\sum\\limits_{i=1}^n \\nabla\\Theta^i$ is an effective aggregation rule. However, MEAN can be manipulated by several malicious clients (Blanchard et al. 2017b). Therefore, multiple robust aggregators (Yin et al. 2018; Blanchard et al. 2017b; Xu et al. 2021) are proposed to filter the Byzantine updates. For example, coordinate-wise Median aggregator computes the median for each element $\\Theta_i$ in parameter $\\Theta$ across all clients, yielding 0.5 breakdown point (Yin et al. 2018). Namely, when the fraction of malicious clients is less than 0.5, the Median aggregator can guarantee the model convergence under Byzantine attacks, yielding the correct gradient (green star) in Fig. 2(a)."}, {"title": "Methodology", "content": "In this section, we re-define the Byzantine robustness under sparse aggregation of FR, and theoretically point out the inherent vulnerability. Based on such vulnerability and considering attackers' knowledge and capability, we design a family of attack strategies, named Spattack."}, {"title": "Problem Definition", "content": "For Byzantine attacks, attackers can inject some Byzantine users $\\bar{U} = {\\bar{u}_1,\u2026,\\bar{u}_{\\bar{n}}}$, limiting the proportion of malicious ones less than $\\rho$, i.e., $\\bar{n}/(n + \\bar{n}) < \\rho$. A malicious client $\\bar{u}_i$ can upload arbitrary gradient values $\\nabla V^{\\bar{u}_{i,t}}$ at any epoch t, to directly perturb the item embedding. The server will collect and aggregate all gradients including benign {$\\nabla V^{1,t},\u2026, \\nabla V^{n,t}$\\} and malicious {$\\nabla V^{\\bar{u}_{1,t}},\u2026,\\nabla V^{\\bar{u}_{\\bar{n},t}}$}. Let AGR(\u00b7) be the aggregation operator of federated learning, which can be the most common MEAN(\u00b7) or statistically robust MEDIAN(\u00b7). Our Byzantine attacker aims to prevent model convergence, namely, keeping the recommendation loss $L_i$ from decreasing. Formally, in FR, the objective of the Byzantine attack is defined as the following optimization problem:\n$\\max\\limits_{\\{\\nabla V^{\\bar{u}_{i,t}}\\}_{i=1}^{\\bar{n}}} \\sum\\limits_{i=1}^{n} (L_i(u_i^{t+1}, V^{t+1}) - L_i(u_i, V^t)),\\\\\ns.t. V^{t+1} = V^t - \\eta \\cdot AGR(\\{\\nabla V^{i,t} : i \\in [n]\\} \\cup \\{\\nabla V^{\\bar{u}_{i,t}} : i \\in [\\bar{n}]\\}),\\\\\nu_i^{t+1} = u_i^t - \\eta \\cdot \\nabla_{u_i}^t, \\text{ for } i \\in [n],\\\\\\frac{\\bar{n}}{n+\\bar{n}} < \\rho, $ (4)\nwhere attackers aim to find the optimal set of malicious gradients {$\\nabla V^{\\bar{u}_{i,t}} : i \\in [\\bar{n}]$\\}, to raise the loss after updating. Before solving this optimization problem, we find that FR has a unique sparse aggregation mechanism defined as follows:"}, {"title": "Definition 1. (Dense/Sparse Aggregation)", "content": "Let $\\theta \\in \\mathbb{R}^d$ be the shared model parameter vector. If there exists an element $\\theta_i$ ($i \\in [d]$) for which only a subset of clients can produce valuable updates, the parameter $\\theta$ is sparsely aggregated. If all clients can support it, it is a dense aggregation.\nAs shown in Fig. 2(a), in general FL, each element (green circle) of model parameters is assumed to be involved in all clients' loss functions, i.e., dense aggregation. Different from FL, for a client $u_i$ in FR, not all item embeddings V = {v1,\u2026, vm} are employed in local loss $L_i$ in Eq. 1, i.e., sparse aggregation. For example, client $u_1$ in Fig. 2(b) only computes valuable gradients {$\\nabla v_1^1$, $\\nabla v_3^1$ } for items $v_1$ and $v_3$, while the gradients for the remaining items are either zero or an empty set. When directly applying the aggregators on all $V_i$, the embedding will be skewed towards zero value. Therefore, we need to adapt them to FR."}, {"title": "Adapting Dense Aggregator to Sparse", "content": "We adapt existing aggregators from dense to sparse aggregation by treating the aggregation for a single item as the smallest execution unit. As shown in Fig. 2(b), the aggregator is conducted separately for each item. Taking the embedding of j-th item as an example, the embedding is updated by:\n$v_j^{t+1} = v_j^t - \\eta \\cdot AGR(\\{\\nabla v_j^{i,t} | \\text{ user } i \\in U_{v_j} \\}),$ (5)\nwhere $U_{v_j}$ is the set of users that the item $v_j$ interacts with, $\\nabla v_j^{i,t}$ is the gradient of item $v_j$ sent from client ui at epoch t. Only if the user ui has interaction with item $v_j$, the gradients $\\nabla v_j^{i,t}$ can be aggregated to $v_j^{t+1}$ separately and concurrently. Intuitively, the numbers of received gradients are varied for different items, leading to each item having personal robustness. Therefore, we need to theoretically re-examine the convergence guarantee of existing aggregators against Byzantine attacks under the sparse aggregation."}, {"title": "Byzantine Robustness Analysis", "content": "Robustness of FR without Defense. Like general FL, FR without defense often uses the Mean aggregator to compute the average of input gradients, which is highly susceptible to Byzantine attacks. Even one malicious client can also destroy the Mean aggregator as stated in Proposition 1.\nProposition 1. For each item vj, let {$\\nabla v_j^{i,t} | \\text{ user } i \\in U_{v_j} $} be the set of benign gradient vectors at epoch t. Consider a Mean aggregator averaging updates for each element. Let $\\nabla \\bar{v}_j$ be a malicious update with arbitrary values in Rd. The output of MEAN({$\\nabla v_j^{i,t} | \\text{ user } i \\in U_{v_j} $} $\\cup \\nabla \\bar{v}_j$) = $\\frac{1}{\\mathcal{M}} (\\sum\\limits_{i \\in U_{v_j}} \\nabla v_j^{i,t} + \\nabla \\bar{v}_j)$ can be controlled as zero vector by only single malicious $\\nabla \\bar{v}_j$. When all the items are attacked, one malicious client can prevent convergence.\nProof. If the attacker registers one malicious client, where the embedding gradient of each item $v_j$ is $\\nabla \\bar{v}_j = - \\sum\\limits_{i \\in U_{v_j}} \\nabla v_j^{i,t}$, the output of aggregator is zero vector, which can prevent convergence.\nRobustness of FR with Defense. The most common defense method is to use aggregators that are statistically more robust against outliers than Mean. In these defenses, FL models have a consistently high breakdown point, e.g., when $\\rho < 50\\%$, Median can theoretically guarantee the convergence of FL as proved in (Yin et al. 2018). However, we find that FR models have varied breakdown points for different items, which depends on the item's degree. Specifically, each item embedding can only be updated by specific clients with whom the item interacts. Obviously, the popular item with massive updates is more robust. Unfortunately, in FR, only a few items interact frequently (head items), while the remaining items interact less frequently (tailed items). We plot the popularity (item degree) of the Steam recommendation dataset (Cheuque, Guzm\u00e1n, and Parra 2019) in Fig. 2(c). We find that 97% tailed items (red long tail area) have interactions less than 200 times, and only 3% head items (green area) interact frequently over 200 times. Therefore, existing statistically-based FL defenses will fail to guarantee the convergence of most items. Formally, let x be the degree of an item and p(x) be its probability. We assume that the probability distribution can be defined as a typical power-law distribution $p(x) = C x^{-\\beta}$, where C is a normalization constant and \u03b2 is the scaling parameter. The failure of defenses can be formally characterized as follows:\nProposition 2. Let \u03b1 be the breakdown point of robust federated aggregator, the amount of benign and malicious clients are n and $\\bar{n}$ respectively, and \u03b2 is the scaling parameter of the power-law distribution of items' degree with constant C. Then at least $1 - C\\frac{\\alpha}{\\bar{n}(1-\\alpha)}(\\frac{1}{\\beta} - 1)$ percent of items' embeddings can be broken down."}, {"title": "Spattack: Byzantine Attack Strategies", "content": "Intuition. In Eq. 4, the attacker aims to keep the recommendation loss from decreasing to prevent recommender convergence. Considering the unique vulnerability from sparse aggregation, i.e., the majority of tailed items have a lower breakdown point, we can conclude that: (1) The gradients are farther away from true gradients, the more considerable corruption is. (2) More items are disrupted in the training process, leading to more powerful attacks. Therefore, the attack objective of the proposed Spattack can be simplified to maximally uploading gradients farther away from true gradients and greedily disrupting the embeddings of items.\nAttack Taxonomy. In real scenarios, depending on the attacker's knowledge about benign gradients and the maximum number of poisoned items in each malicious client, we outline different scenarios of Spattack that can be launched. As shown in Tab. 1, we have four possible scenarios:\nSpattack-O-D is considered a worst case, where the attacker is both omniscient and omnipotent, i.e., attackers can obtain benign gradients at each epoch and the maximum number of poisoned items is not limited. Following the first intuition that the malicious gradients farther away from true gradients can cause larger corruption, attackers upload the gradients in the opposite direction of the benign ones. Formally, for a item vj, we collect benign gradient $\\nabla v_j^{i,t}$ from ui, where ui interacts with vj, i.e., $U_i \\in U_{v_j}$. Then we compute the sum of the collected benign gradients to obtain the expected gradient $\\nabla = \\sum\\limits_{u_i \\in U_{v_j}} \\nabla v_j^{i,t}$. Lastly, each malicious client $\\bar{u}_i \\in \\bar{U}$ will upload malicious gradients $\\nabla \\bar{v}_j^t = \\frac{1}{\\mathcal{M}}\\nabla$, following the second intuition that greedily disrupts items, the attack effectiveness will be maximized by uploading poisoning gradients for all items. In this attack, the non-robust Mean aggregator will output zero gradients, while statistically robust aggregators will select malicious gradients for the majority of tailed items, preventing the convergence of item embeddings. According to Proposition 1 and Proposition 2, even only having a small portion of malicious clients, Spattack-O-D can still guarantee to disrupt the majority of item embeddings.\nSpattack-L-D uploads random noise as malicious gradients for all items, where attackers are non-omniscient but omnipotent, i.e., attackers do not have any knowledge about the benign gradients but can attack all items. Specifically, attackers construct the malicious gradient by randomly sampling from the Gaussian noise and keeping the same noise in all malicious clients. Under the Mean aggregator, the aggregated gradients can be skewed by such noise. Even worse, the statistically robust aggregators, e.g., Median, can pick the uploaded random noise as output for tailed items. So this attack can still prevent model convergence.\nSpattack-O-S and Spattack-L-S only upload malicious gradients for partial items, where attackers are non-omnipotent. Let mmax be the maximum number of poisoned items in each malicious client. The larger mmax, the stronger the attack, but the excessive mmax may lead to the attack being detected. To limit malicious users to behaving like benign users, we restrict mmax as the maximum number of interactions in benign clients. Specifically, to make the injections of malicious clients as imperceptible and effective as possible, based on the distribution of item popularity, we use a sampling operation to determine the poisoned items for each malicious client. Therefore, the attacker can automatically assign more malicious gradients to the items having more interactions. Then we generate malicious gradients based on the opposite benign gradients (Spattack-O-S) or random noise (Spattack-L-S), respectively."}, {"title": "Notations", "content": "We present all notations relevant to our paper in Tab. 4.\nTable 4: Notations\nD\tall interactions\nDi\tlocal interaction of user $u_i$\nU\tset of n benign users\n$\\bar{U}$\tset of \u00f1 malicious users\nV\tset of m items\n$V_{u_i}$\tset of items interacting with $u_i$\n$U_{v_i}$\tset of users interacting with $v_j$\nU\tthe user embeddings where $U = \\{U_1, ..., U_n\\}$\nV\tthe item embeddings where $V = \\{V_1, ..., V_m\\}$\n$\\nabla V^{i,t}$\tthe embedding gradient of V from benign user $u_i$ at epoch t\n$\\nabla V^{\\bar{u},t}$\tthe embedding gradient of V from malicious user $\\bar{u}_i$ at epoch t\n$\\nabla v_j^{i,t}$\tthe embedding gradient of $v_j$ from benign user $u_i$ at epoch t\n$\\nabla \\bar{v}_j^t$\tthe embedding gradient of $v_j$ from malicious user $\\bar{u}_i$ at epoch t\n$\\Theta$\tthe parameters of neural network model\n$[n]$\tSet of integers {1,\u2026,n}\n$\\eta$\tlearning rate\n$\\rho$\tthe proportion of malicious users\n$\\alpha$\tthe breaking point of statistically robust aggregator"}]}