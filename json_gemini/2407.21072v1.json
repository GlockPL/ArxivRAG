{"title": "BEYOND METRICS: A CRITICAL ANALYSIS OF THE VARIABILITY\nIN LARGE LANGUAGE MODEL EVALUATION FRAMEWORKS", "authors": ["Marco AF Pimentel", "Cl\u00e9ment Christophe", "Tathagata Raha", "Prateek Munjal", "Praveen K Kanithi", "Shadab Khan"], "abstract": "As large language models (LLMs) continue to evolve, the need for robust and standardized evaluation\nbenchmarks becomes paramount. Evaluating the performance of these models is a complex challenge\nthat requires careful consideration of various linguistic tasks, model architectures, and benchmarking\nmethodologies. In recent years, various frameworks have emerged as noteworthy contributions to the\nfield, offering comprehensive evaluation tests and benchmarks for assessing the capabilities of LLMs\nacross diverse domains. This paper provides an exploration and critical analysis of some of these\nevaluation methodologies, shedding light on their strengths, limitations, and impact on advancing the\nstate-of-the-art in natural language processing.", "sections": [{"title": "1 Introduction", "content": "The rapid advancements in the field of natural language processing (NLP) have been fueled by the development of\nincreasingly sophisticated Large Language Models (LLMs). From OpenAI's GPT series [1] and Google's BARD [2, 3]\nto a plethora of open-source models such as Llama [4, 5], the Falcon series [6], Jais [7] and ohers [8, 9, 10], these\nmodels showcase remarkable language understanding and generation capabilities. As the abilities of LLMs continue to"}, {"title": "2 Background", "content": "In this investigation, we focus on multiple-choice question tests, in which for each question only one of the provided\nchoices is the correct answer (see Box 1). This represents a rather simple task, in contrast to open-ended questions, for\nexample. However, our examination reveals that even within this seemingly simple problem formulation, significant\nvariability exists, stemming from subtle implementation details and disparities. Notably, the majority of these disparities\narise from differences in the methods used to select the final answer option from the model's output. To help us\nformulate the problem, we focus on evaluating multiple-choice tasks using autoregressive language models, such as\nGPT (Generative Pre-trained Transformer) like architectures, which underpins the architecture of the models mentioned\nabove."}, {"title": "2.1 OpenCompass", "content": "OpenCompass, an open-source evaluation framework for language models, uses the token probability comparison\napproach for extracting the prediction of the model [25]. Specifically, the probabilities predicted by the model for all\npossible answers are compared, such that the probability for the option ak is given by $P(q_{m+1}|q_{0:m})$; in the example\nabove ak \u2208 {\u201cA\u201d, \u201cB\u201d, \u201cC\u201d, \u201cD\u201d }, i.e., the first letter corresponding to each choice. We note that the framework uses\nperplexity as the key metric, rather than relying solely on (log) likelihood.\nIn order to reduce the likelihood of the model generating responses that fall outside the intended range of answers, a\n\"few shots\" approach is typically used, in which the prompt is augmented with one or more instances/examples with their\ncorrect answers as well (see Appendix B). In instances where the model might have otherwise generated an unrelated\nword (or token), the inclusion of a few shots attempts to ensure that the model is guided by known examples and by\nbetter understanding of the expected behaviour. The approach of incorporating a handful of examples in the prompt\ntypically improves the model's performance and is a standard evaluation method, as demonstrated in assessments such\nas MMLU, where five shots (i.e., five examples) are prepended to each prompt for evaluation across various benchmarks\n[26]."}, {"title": "2.2 LM Evaluation Harness", "content": "The evaluation harness framework from EleutherAI [24] also uses the token probability comparison methodology. In\nthis case, however, it computes the likelihood of the entire continuation sequence using the process described above (i.e.,\nit uses the full answer sequence which contains the letter followed by the text of the answer). In its simplest method, for\neach k choice in A, the likelihood is determined using\n$\\sum_{j=m+1}^{n_k}log P(q_j|q_{0:j-1}),$\nwhere the aggregation of the probabilities is achieved by summing the log of the individual probabilities for numerical\nstability\u00b2 Conceptually, this entails determining the probability of a generated sequence, sampled from the given"}, {"title": "2.3 HELM", "content": "Another popular framework is the Holistic Evaluation of Language Models (or HELM) project [19]. The method\nused for evaluating the model using MCQs in HELM is different from the implementations described in the previous\ntwo approaches. Instead of comparing the token probabilities from the given answer choices, HELM utilizes the\nmodel's next token output probabilities to generate text. This generated text is then compared to the expected answer.\nSpecifically for MCQ tasks, HELM implements metric functions such as exact match to assess the correspondence\nbetween the generated text and the correct answer. This approach allows for a more natural evaluation of the model's\nunderstanding and ability to generate relevant responses, rather than simply selecting the most probable option from a\npredefined set of choices.\nWhile the evaluation methodology takes a distinct approach, the few-shot prompt remains generally similar. However,\nfor a given instance, we note that if the model assigns the highest probability to a token that deviates from the intended\nrange of answers, even though it is not part of the set of choices, the model's response would be deemed incorrect,\nresulting in a lack of scoring for that particular instance. In other words, the evaluation process hinges on the model's\nability to prioritize the correct tokens within the specified answer choices. In the event that the model, despite a generally\nsimilar few-shot prompt, allocates the highest probability to a token outside the intended range, the response in deemed\nincorrect. This is substantially different from the methods described above, in which only the probabilities associated\nwith the given set of answers are included for computing the model's performance.\nWe also note that frameworks such as HELM [19] and Langtest [27] attempt to enhance the evaluation process of LLMs\nby offering a broader array of tasks and metrics (in addition to accuracy). These frameworks offer a comprehensive set\nof evaluation criteria that delves into various aspects of language understanding and generation, providing additional\nmetrics such as calibration, robustness, fairness, bias, toxicity, and efficiency. Such metrics are out of the scope of this\nstudy."}, {"title": "3 Methods", "content": "In this study, we focus on two prominent evaluation frameworks, OpenCompass and Eval harness. As described above,\nthese frameworks utilize a token-probability comparison method to assess LLMs in the context of multiple-choice\nquestion answering benchmarks. Our investigation centers on providing a detailed account of the accuracy metrics"}, {"title": "3.1 Evaluation tasks and datasets", "content": "In this section, we describe the test datasets used to evaluate the models on different tasks. When selecting these\ndatasets, preference was given to widely acknowledged sources used across various domains, such as general and\nmedical contexts. Additionally, these encompass a range of context lengths and types of reasoning."}, {"title": "3.2 Language model architectures", "content": "We employed two distinct family of language models, namely Llama2 [5] and Mistral [31], to evaluate their performance\nwithin the context of the OpenCompass and Eval harness benchmarks. These pretrained generative text models have\nbeen shown to perform well across various benchmarks.\nLlama2 models, part of the family of language models developed by Meta AI, are a set of LLMs with varying size,\nranging from 7 billion to 70 billion parameters. It is an auto-regressive language model based on the transformer decoder\narchitecture with some notable differences from models like GPT-3. For example, Llama2 employs the SwiGLU\nactivation function rather than ReLU and uses rotary positional embeddings in lieu of absolute learnable positional\nembeddings [5]. The latest release of Llama2 also introduces architectural refinements geared towards enhanced\nperformance, extending the context length to up to 4,096 tokens. Bigger models (70B) use Grouped-Query Attention\n(GQA) to better leverage long sequences and improved inference scalability.\nMistral-7B (v0.1), introduced by Mistral AI, is a 7.3-billion parameter model with a similar architecture to that of\nLlama2 [31]. It also uses grouped-query attention which enhances the inference process by caching key and value\nvectors for previously decoded tokens in the sequence, thereby reducing processing time. In addition, it uses a sliding\nwindow-based attention mechanism which replaces full attention, characterized by square compute cost. In this\nmechanism, each token can attend to at most 4,096 tokens from the preceding layer, resulting in a linear compute cost."}, {"title": "3.3 Evaluation metrics", "content": "The assessment of the three models on the aforementioned datasets involves the utilization of the accuracy metrics\nderived from OpenCompass and Eval harness. The final accuracy for a dataset is determined by the percentage of\nquestions answered correctly. To determine this accuracy, we can follow different methods to identify the correct\noption selected by a model (as described in the previous section). Our focus centers on the accuracy metrics computed\nusing the different methods employed by different evaluation frameworks. We attend to the following methods for this\npurpose:\n\u2022 OpenCompass' accuracy (denoted OC accuracy): this approach invoves extracting the model's prediction\nby assessing the next token probabilities and determining whether the selected choice (that with the highest\nlikelihood) is the correct answer (as described above and used in [25]);\n\u2022 Raw (unnormalized) accuracy (Raw accuracy): similarly to the previous method, it involves comparing token\nprobabilities, but in this case, the probabilities of the full answers' sequences are used to determine the correct\noption, using the sum of the log likelihoods of all tokens; this corresponds to the method used and reported in\nthe eval harness' framework (see section 2.2);\n\u2022 Token-normalized accuracy (T-norm accuracy): this method involves normalizing the likelihhood for each\nanswer (as obtained for the metric above) by dividing the sum by the number of tokens to avoid giving too\nmuch \"weight\" to longer answers (section 2.2);\n\u2022 Byte-normalized accuracy (B-norm accuracy): this method also attempts to avoid biasing the model toward\nfavoring longer choices by normalizing the likelihood using the average log probability per character (using\nthe number of characters in the full answer sequence); as mentioned in section 2.2, it is also used and reported\nin the eval harness' framework.\nThe evaluation is carried out in a zero-shot setting, meaning that no examples are added to the prompt. Also, to ensure\nconsistency and mitigate the impact of variations in prompts on model results, a standardized prompt design was\nemployed across the evaluation frameworks and methods. Refer to Appendix B for further details on the adopted\nprompt design for each dataset."}, {"title": "4 Results", "content": "The performance of the models across selected benchmark datasets is summarized in the Table 2, highlighting the\naccuracy metrics obtained within each one of the four evaluation methods.\nFor each evaluation scenario, we note that Llama2-70B consistently outperforms other Llama2 (smaller) variants and\nMistral-7B across all benchmark datasets. However, within each benchmark dataset, a substantial variability in the\nperformance of the different models across the four methods is observed (Figure 1). As examples, for MMLU, the\nperformance of Mistral-7B ranges from 61.4% and 65.8%; while for HellaSwag, the performance of Llama2-70B\nfluctuates between 64.8% and 83.8%.\nWe also note that the effect of the normalization methods (e.g., B-norm accuracy) is not consistent across the benchmark\ndatasets; while for HellaSwag, the normalization-based accuracy metrics are higher than the raw-based accuracy metric,\nwhich does not take into account the normalization of the log likelihoods of the responses (according to their length),\nthe opposite behaviour is observed for other datasets (Table 2). To delve deeper into the factors influencing correct\noption selection, we investigate the impact of normalizing response likelihoods. For each question-answer pair, we\nassessed the length of options and compared the length of the correct option with that of the wrong options (this is set to\nbe the median length of the wrong options).\nThe Bland-Altman plot (depicted in black in Figure 2) illustrates the length difference between right and wrong options\nfor the (whole) HellaSwag dataset. No significant inherent bias in the length of the correct options compared to the\nlength of the wrong options is observed (and the same is observed for the other datasets too; figure not included in the\nmanuscript). I.e., the lengths of correct and wrong answer options of the records included in the benchmark dataset are\nsimilar. Notably, the figure also shows the length difference (in red) for those instances in which Mistral-7B incorrectly"}, {"title": "5 Discussion", "content": "In this paper, we aimed to explore the details of metric calculation methodologies employed by prominent evaluation\nframeworks and their implications on the performance assessment of LLMs and the subsequent interpretability of\nevaluation results. Specifically, we scrutinized the accuracy metrics, describing the intricacies of their calculation\nmethodologies. Extending our examination beyond theoretical considerations, we directed our efforts towards the\nevaluation of widely recognized open-access language models. This evaluation encompassed different question-\nanswering datasets, including those structured as multiple-choice scenarios.\nThe results of our analyses shed light on some critical aspects of the evaluated LLMs and the associated methodologies.\nNotably, considering each method individually, Llama2-70B consistently demostrates superior performance across all\nbenchmark datasets when compared to both its smaller variants and Mistral-7B (Table 2). This has been observed in\nother studies, in which the performance of Llama2-70B has been deemed to be superior in diverse evaluation benchmark\ntasks, including question-answering tasks [5, 31].\nNevertheless, despite this overall superiority, the results from our analysis demonstrate a substantial variability within\neach benchmark dataset across the four evaluation methods (Figure 1). The observed fluctuations in performance,\nranging between 5% and 26%, highlight the sensitivity of the reported accuracy metrics to the method-specific\nimplementations.\nIn evaluation frameworks such as eval-harness by Eleuther AI [24], for calculating accuracy of a model, the likelihood\nof the (complete) response (for each option) is determined, and the option with the highest likelihood is deemed to be\nthe correct one. In order to avoid the introduction of a bias toward favoring shorter answers, eval-harness has introduced\na normalization step, in which the overall likelihood for each choice is divided by a measure of the length of the choice's\nsequence. These different approaches, as expected, produce different results.\nIn addition, Figure 2 highlights the impact of normalization in the selection bias of the correct option by a model (in red)\non HellaSwag. In the top panel (without normalization of the response's likelihood), a substantial bias is evident, with\nthe model preferring shorter answers; whereas, if normalization of the response's likelihood (bottom panel) is employed,\nthe bias is noticeably reduced, underscoring the effect of normalization on enhancing the reliability of the response\nselection, and ultimately the model's accuracy. However, it is important to note that the impact of normalization on the\nbias is not universally consistent across all datasets. In contrast to the observed mitigation of bias in the HellaSwag\ndataset, the same analysis for other datasets reveals that the normalization process seems to introduce a bias rather than\nreducing it in the selection of the correct option. This can be observed, for example, in the MedQA dataset (Figure 3)."}]}