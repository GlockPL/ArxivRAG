{"title": "Authenticated Delegation and Authorized AI Agents", "authors": ["Tobin South", "Samuele Marro", "Thomas Hardjono", "Robert Mahari", "Cedric Deslandes Whitney", "Dazza Greenwood", "Alan Chan", "Alex Pentland"], "abstract": "The rapid deployment of autonomous AI agents creates urgent challenges around authorization, accountability, and access control in digital spaces. New standards are needed to know whom AI agents act on behalf of and guide their use appropriately, protecting online spaces while unlocking the value of task delegation to autonomous agents. We introduce a novel framework for authenticated, authorized, and auditable delegation of authority to AI agents, where human users can securely delegate and restrict the permissions and scope of agents while maintaining clear chains of accountability. This framework builds on existing identification and access management protocols, extending OAuth 2.0 and OpenID Connect with agent-specific credentials and metadata, maintaining compatibility with established authentication and web infrastructure. Further, we propose a framework for translating flexible, natural language permissions into auditable access control configurations, enabling robust scoping of AI agent capabilities across diverse interaction modalities. Taken together, this practical approach facilitates immediate deployment of AI agents while addressing key security and accountability concerns, working toward ensuring agentic Al systems perform only appropriate actions and providing a tool for digital service providers to enable AI agent interactions without risking harm from scalable interaction.", "sections": [{"title": "1. Introduction", "content": "Agentic AI systems, also referred to as AI assistants or simply 'agents', are AI systems that can pursue complex goals with limited direct supervision on behalf of a user [33, 22, 72, 21, 45], including by interacting with a variety of external digital tools and services [55, 47, 32]. For example, AI agents given a prompt to book travel arrangements for a holiday may browse the web for recommendations, search for flights via APIs, or message an airline agent in natural language via chat services to arrange a booking. Such communications could even extend to AI agent negotiations [1] and other multi-agent contexts.\nWhile current AI agents have limitations [67, 86], lack the ability to perform certain tasks [49], and may be susceptible to attacks such as prompt injections [91, 50, 95], there has been rapid progress in their development and commercial interest.\nThis has raised many concerns over the risks of AI agents and how they should be governed [72, 33, 27]. Credentials and verification may become critical in verifying the properties and metadata of AI systems [22], uniquely identifying humans in online spaces [14] (or at least distinguishing humans from AI agents [3]), protecting contextual confidence [42], mitigating AI-augmented influence operations [37], preventing AI manipulation of humans [8, 74], and governing or auditing AI systems more broadly [68, 75]. The world needs ways to explicitly delegate authority to agents, transparently identify those agents as AI, and enforce human-centered choices around security and permission for these agents.\nWe distinguish three key concepts: authentication confirms an entity's identity; authorization determines the permissible actions and resource accesses that the authenticated identity is allowed to perform, defining the scope and limitations of delegated activity; and auditability allows all parties to inspect and verify that claims, credentials, and attributes remain unaltered, supporting trustworthy authentication and authorization decisions.\nThis work has three key contributions. First, section 2 builds upon the existing literature to outline why authenticated delegation is important for AI agents, and what risks it could mitigate. In doing so, we provide an overview of current practices and where they fall short. Second, section 3 directly addresses this need by extending existing authentication and authorization protocols to enable authenticated delegation to AI agents, examining the role OpenID"}, {"title": "2. Why authenticated delegation is important", "content": "Authenticated delegation is the process of instructing an AI system to perform a task that requires access to tools, the web, or computer environments in such a way that third parties can verify that (a) the interacting entity is an AI agent, (b) that the AI agent is acting on behalf of a specific human user, and (c) that the AI agent has been granted the necessary permissions to perform specific actions.\nVerifying the properties of interacting entities will be relevant whenever a context exists where an AI agent could act on behalf of a human user, and especially where the agent is capable of taking consequential actions. This remains true whether the AI system is run locally or provided by an AI vendor-as harm can occur in both and must be able to operate across various digital contexts and with AI models of heterogeneous capabilities.\nAt a high level, authenticated delegation involves a human user creating a digital authorization that a specific AI agent can use to access a digital service (or interact with another AI agent) on behalf of the user, which can be verified by"}, {"title": "2.1. Functions of authenticated delegation", "content": "Authenticated delegation opens avenues for AI agents to accelerate complex tasks, automate workflows, and seamlessly interface with digital services on behalf of human users. However, granting such agency also entails risks around scope misalignment, resource abuse, or a breakdown of clear accountability. This subsection delineates how robust identity verification, explicit scoping, and mutual authentication can unleash practical use cases-ranging from streamlined enterprise processes to safe, multi-agent coordination-while mitigating key vulnerabilities. By highlighting both the opportunities and the potential pitfalls, we underscore why adopting secure, verifiable delegation mechanisms is vital to responsibly harness AI agents."}, {"title": "2.1.1. CURRENT CHALLENGES IN DELEGATING\nAUTHORITY TO AI AGENTS", "content": "As the capabilities of LLMs improve, there is a growing interest in making them more autonomous and general-purpose. A key aspect of this is the ability to use tools or access external services. For simple tasks such as asking an agent to search the web for information, write and execute code, or generate an image, this is straightforward and does not require additional authorization or individual-specific security mechanisms. However, to unlock use cases such as interacting with personal or organizational accounts, accessing sensitive personal information, or interacting with consequential infrastructure, more robust delegation frameworks are needed.\nExample: Consider the above example of an AI agent booking a holiday. Having an agent search the web for infor-"}, {"title": "2.1.2. \u0421\u043e\u043cMMUNICATING LIMITATIONS AND\nRESTRICTING SCOPE", "content": "Current approaches to limiting the scope of AI agents are limited and one-sided. A user can provide a strong prompt to an agent to limit its actions, but this comes with a variety of failure modes [50]. Access to tools or websites can be blocked, but this is limited in the granularity of control. An AI system deployer could implement further controls, such as monitoring and blocking specific actions or website subdomains when agentic functionality occurs, but doesn't communicate these limitations to the service the agent is interacting with. By explicitly limiting the scope of an AI agent and communicating these limitations to the service the agent is interacting with, we can enable a more robust interaction between AI agents and services. A more detailed examination of how this could be designed across web, API, and natural language access modalities is available in section 4.\nExample: An AI agent is used by a physician to provide diagnostic recommendations in a telemedicine portal, logging in with basic credentials that do not specify its limitations. The portal assumes full physician capabilities, granting the agent access to all patient records, including a video assessment with a voice recording from a specialist. The agent, being text-only and unable to process video, generates a diagnosis based solely on the text data, appending a standard caveat \"not all available information was used\"-which is overlooked. Trusting the incomplete recommendation, the physician risks making a misinformed treatment decision. If the agent's limitations were explicitly communicated via authenticated delegation, the portal could have flagged the need for a human review of the multimedia content, avoiding a potentially harmful oversight."}, {"title": "2.1.3. VERIFICATION IN MULTI-AGENT\nCOMMUNICATION", "content": "When AI agents communicate to collaborate on tasks or facilitate interactions, ensuring mutual authentication becomes paramount. Securing communication channels is not enough; agents must also verify that they authentically represent the users or organizations they claim to represent. Mutual authentication ensures that agents can trust each other's intentions, capabilities, and authority, preventing impersonation, unauthorized actions, and potential misuse. This verification is essential for fostering reliable, safe, and accountable multi-agent ecosystems.\nExample: Two AI agents-one representing a hospital and the other an insurance company-collaborate to process a patient's claim. The hospital agent submits treatment details, while the insurance agent verifies coverage. Without mutual authentication, a third-party malicious agent could impersonate the hospital, submitting fraudulent claims, or the insurance agent could reject valid claims out of concern over authenticity."}, {"title": "2.1.4. PROTECTING HUMAN SPACES ONLINE", "content": "As AI agents grow increasingly adept at mimicking human behavior-crafting text, creating personas, and even replicating nuanced human interactions-it becomes harder to maintain digital environments genuinely inhabited by real people. This challenge drives the need for safe, human-only online spaces where authenticity is preserved and scalable manipulation is curbed by verifying human personhood [3]. However, many AI agents act as useful proxies, assistants, or representatives for human users who cannot, or prefer not to, engage directly. Authenticated delegation enables these spaces to be selectively accessible to AI agents, while still ensuring that the AI agents are linked to verified human principals.\nExample: The Australian government's recent social media ban demonstrates how governments can restrict access to online spaces by requiring users to prove their age, often through methods like a government-issued ID or a face scan. While these measures aim to limit underage access, they may also inadvertently block AI agents from accessing these platforms. Instead of a blanket restriction, platforms could explicitly allow AI agents to access their services in controlled ways by leveraging authenticated delegation. This approach would ensure that AI agents act transparently on behalf of verified human users. For instance, an agent could access a user's social media account to retrieve information about friends and help draft an email, all while maintaining compliance with platform policies and ensuring accountability."}, {"title": "2.1.5. SUPPORTING CONTEXTUAL INTEGRITY", "content": "Contextual integrity addresses adherence to context-specific norms and privacy, which include actors (who is involved in the information flow), attributes (what information is shared), transmission principles (under what conditions information is shared), and social context (the broader cultural, institutional, or situational environment shaping these norms) [36, 93, 28, 58]. Contextual integrity offers a perspective for reasoning about how AI agents can act in ways that are contextually appropriate, transparent, and aligned with societal norms and the expectations of their human delegators [7, 36, 12]. This includes exploring which decisions can reasonably be made autonomously by the AI and under what conditions human oversight or intervention might be necessary (e.g., when is human-in-the-loop required and who is responsible).\nExample: An AI assistant with authenticated delegation can be issued distinct credentials for separate contexts (e.g., an enterprise-context assistant and a personal one). Each credential encodes the agent's information, the delegating user, and context-specific permissions. By enforcing these scoped credentials, services can ensure that the assistant adheres to contextual integrity and rejects actions that cross boundaries, such as using information from work documents to complete personal forms. This separation of roles and explicit permission-sharing protects privacy, ensures accountability, and safeguards human oversight for cross-context decisions."}, {"title": "2.2. Background", "content": "Authenticated delegation can address various challenges, from traceability of AI outcomes to limitations on what spaces can be accessed and actions taken by AI systems. The overarching aim of identification and credentialing systems is to facilitate secure online environments and authenticated access to services. To this end, a variety of existing protocols and standards have been developed, tailored to both human users and AI systems, to uphold these goals in different contexts.\nComparisons to other AI identifiers To verify human identity online, a large body of work exists ranging from simple authentication such as OAuth 2.0 [41] to more complex digital identity frameworks as W3C's Verifiable Credentials [78], decentralized identifiers [77], and the European Union Digital Identity's privacy-preserving digital wallets [85]. To privately prove personhood, a number of systems have been developed to distinguish human users from bots, including proof-of-personhood systems designed to counter automated Sybil attacks [15], simple turing tests such as CAPTCHAS [81], and more robust credentials [3]. More generally, the goal of 'know-your-customer' for users and granular access permissions (identity and access management, IAM) are commonplace on the internet.\nSimilarly, many websites seek to broadly limit access to bots on their services, and may do so through the use of robots.txt bans. This is important since the widespread presence of bots or unauthenticated AI agents can lead to abuse and harm, but is often done at the 'user-agent' level (for example, banning all \u2018GPTBot' user agents [51]).\nTo track and verify the output of AI systems, watermarking techniques [48, 87] and content provenance measures [19] have emerged as potential solutions for determining the origin of AI-generated content. However, these approaches face reliability challenges [70] and are insufficient for establishing comprehensive accountability or safety when using AI agents. The inherent limitations of current verification methods highlight the need for more robust frameworks that can track not just content creation but also the broader implications of AI system deployment and interaction.\nFor managing access to sensitive Al capabilities themselves, researchers have proposed 'know-your-customer' schemes for compute providers [26, 60], while commercial platforms implement API tokens and access controls [61]. These developments reflect a growing recognition that AI systems need robust mechanisms to prove their authenticity and permissions when accessing external services [18], particularly as they become more integrated into critical infrastructure and decision-making processes.\nTo identify specific instances of AI agents, recent work has proposed identifiers and verification approaches discussed above [23, 22]. This is important and critical work, which we build upon to extend to authenticated delegation of AI agents using existing authentication and permission protocols to enable AI agents to act on behalf of users in a controlled manner. In turn, these identifiers and delegation"}, {"title": "How authenticated delegation combines these solutions", "content": "This work combines and extends these existing approaches-AI agent IDs and credentials, proof-of-personhood and identity verification for human users, and content provenance and watermarking methods\u2014to form a cohesive framework. This approach inherits well-established practices for identity management while introducing explicit scoping and metadata for AI agents. This integration allows for granular, enforceable permission sets, clearer accountability chains, and richer context signals (like a model's certifications or limitations) to be attached to each delegated action, with a more robustly verifiable construction than a simple agent ID system card. In effect, authenticated delegation complements existing standards and enhances their reliability by anchoring the actions of AI agents to verifiable human principals and recognized AI-specific credentials, creating a unified foundation for safe and accountable AI interactions. To this end, section 3 introduces a concrete framework with additional security guarantees to package these elements together in a robustly verifiable way."}, {"title": "3. Extending OpenID Connect for identifying\nand authenticating AI agents", "content": "To support the motivation of section 2, this section proposes a concrete technical framework building on existing internet-scale authentication protocols to introduce mechanisms for delegating authority from users to Al agents and describes a token-based authentication framework that leverages OpenID Connect and OAuth 2.0. Our approach extends these battle-tested protocols to address the unique challenges of AI agent authentication while maintaining compatibility with existing internet infrastructure."}, {"title": "3.1. OAuth2.0 and OpenID-Connect", "content": "While new frameworks for Al system identification are emerging, there are valuable lessons to be learned from existing internet-scale authorization and authentication protocols. In particular, the OAuth 2.0 protocol [41] and its extensions provide battle-tested patterns for delegated authorization and identity verification that could inform the development of AI agent credential systems.\nOAuth 2.0 emerged from the need for users to provide authorization to one service to access resources located in another service, based on the RESTful paradigm [30]. A key requirement underlying OAuth 2.0 is the ability for access to be continually granted even if later the user is absent (e.g., offline). Existing user authentication protocols (e.g., MIT Kerberos [57], CHAP [73]) were developed primarily for the interaction between a human user utilizing a host computer connecting to the authentication server over the UDP layer. The advent of the RESTful APIs meant that the parameters and flows had to be communicated over the"}, {"title": "3.2. Delegation of authority from the user to the AI\nagent", "content": "Given that the OAuth 2.0 protocol is an authorization protocol, it is worthwhile considering reusing the OAuth 2.0 patterns to establish a new mechanism for the human user to delegate certain tasks to the AI Agent. In other words, the human user is authorizing the AI Agent to carry out certain tasks that are limited in scope on behalf of the user.\nIn this new proposed extension, the human user must first perform authentication to the OpenID Provider (OP) to demonstrate their identity. The user then 'registers' the AI Agent to the OP so that external entities who later seek to obtain further information about the AI Agent can do so to the OP. Registration could be done automatically in the background when an agent is created through a vendor (such as creating a new assistant instance with OpenAI).\nExisting OAuth 2.0 client registration protocols can be customized to enable the user to register the AI Agent to the OpenID Provider and designate the AI Agent as a delegate or surrogate of the human user.\nNext, the human user can issue a new delegation token that authorizes the AI Agent to carry out tasks on behalf of the user. Here, the term 'authorize' is utilized to explicitly call out the fact that the AI Agent is owned (driven) by a human delegator.\nBoth the user ID-token and the AI Agent delegation token can be referenced from within (or even copied into) a W3C Verified Credentials (VC) data structure [76]. This enables the AI Agent to wield the VC in its interactions with other entities (e.g., other services or other AI Agents), and have the benefit that both tokens would be verifiable at the standard OP.\nIt is worth noting that these delegation and authentication exchanges could alternatively be implemented using W3C VC issuance and delegation mechanisms. In such a scenario, a W3C VC could generate an OpenID-compatible credential, enabling seamless interfacing with OpenID systems. While this integration highlights the interoperability between W3C VC and OpenID ecosystems, further exploration and formalization of this process are left as future work and are beyond the scope of this paper."}, {"title": "3.3. Token-based authentication framework", "content": "Extending the existing OIDC framework, we can provide all relevant AI agent attributes and metadata of delegation in a set of identity-related tokens.\n\u2022 User's ID-token: This is the existing ID-token data structure that is issued/signed by the OpenID Provider (OP) service. It is intended to represent information regarding the human user, and is no different to those used in everyday login experiences.\n\u2022 Agent-ID token: This carries the relevant information about that Al agent issued as an OAuth2.0 Native Client (meaning the owner of the AI Agent controls all keying material and secret parameters) and allows the corresponding service to verify any claims about the AI agent and its information. This token can include a range of additional information, from a unique identifier for the agent to a richer and more detailed agent ID token containing system documentation, capabilities or limitation metadata, relationship attributes to other Al systems, or other system characteristics. See Chan et al. [23] for further discussion of what an agent ID could entail.\n\u2022 Delegation Token: This newly introduced token explicitly authorizes an AI agent to act on the user's behalf. The delegation token is issued and signed by the human delegator and carries references to (e.g., hash of) the corresponding user's ID token and the agent's Agent-ID token, allowing it to be verified by any service that trusts the OP. Further, any relevant information about the nature of the delegation can be shared. For example, sharing the summarized goal of the agent and its scope limitations could assist a third party in guiding the AI agent to useful endpoints and interaction paradigms. The delegation token should specify validity conditions, such as expiration time or revocation endpoints, and be digitally signed by the user to prevent forgeries and ensure that the user knowingly granted the AI agent the listed privileges. In addition, the token may carry supplemental metadata-for example, logging or audit URLs\u2014allowing service providers to record interactions, monitor delegated actions, and respond appropriately to anomalies. By verifying that the delegation token references a valid user ID-token and a properly issued agent ID-token, remote services can confirm the authenticity and scope of the AI agent's authority before granting access."}, {"title": "3.4. Scope Limitations on Delegation", "content": "The delegation framework enables human users to optionally define explicit boundaries for their AI Agents' actions by encoding scope limitations in the delegation token. How-"}, {"title": "3.5. Using verifiable credentials as an alternative", "content": "The W3C Verifiable Credentials (VC) standard [76] offers a versatile alternative\u2014and sometimes complement to existing OpenID Connect (OIDC) flows for conveying identity and delegation data. Under a VC-based approach, an issuer (such as an organization or individual) can sign a credential that attests to various claims about a subject, which might be a user, an Al agent, or any other entity needing verifiable, tamper-evident attributes. Because VCs are not bound to a particular transport protocol, they can be presented and verified in a decentralized or peer-to-peer manner without always relying on a single identity provider. This contrasts with OIDC, which generally depends on a central OpenID Provider (OP) to mint and validate tokens.\nA key benefit of VCs is their privacy-enhancing potential. Rather than disclosing all attributes or relying on a single identity provider, users, and AI agents can share only the subset of claims strictly necessary for a given interaction. This \"selective disclosure\" capability can mitigate concerns around centralized logging or cross-platform correlation inherent in OIDC-based architectures, especially when interactions span multiple domains or organizations.\nNonetheless, replacing OIDC entirely with a purely VC-based model does come with trade-offs. OIDC already enjoys a robust ecosystem of libraries and deployments that provide well-tested support for issues like token refresh, revocation, and audience restriction. VCs, while powerful, require additional work to replicate these flows at scale-particularly if each verification call demands a new signature check or interaction with a blockchain or distributed ledger. In many enterprise environments, stakeholders may prefer to incorporate VCs into existing SSO or multi-factor authentication frameworks, rather than adopt a fully decentralized identity infrastructure upfront.\nIn practice, hybrid solutions often prove the most pragmatic. A user or AI agent could store and manage VCs encoding rich attributes or regulatory endorsements, while still leveraging OIDC tokens to bootstrap compatibility with existing authentication or authorization endpoints. For instance, an Agent-ID token could embed a VC carrying detailed metadata on its behavioral, property, context, and relationship attributes. Service providers integrating with OIDC get the familiar token-based handshake, while still retaining the option to parse the embedded VC for an additional layer of trust and context. Examples such as OID4VC support this [92]."}, {"title": "4. Defining scope and permissions for AI\nagents", "content": "Authenticated delegation is inherently tied to robust scoping mechanisms, as users must be able to specify their permissions and instructions in a clear and unambiguous manner. This comes in direct conflict with the extremely large possible action space AI agents can perform.\nWhile much work in reliability and alignment focuses on ensuring that AI agents follow instructions correctly, the risks of misinstruction, prompt injection attacks, and reduced security auditability make pure natural language prompts an incomplete scoping, permission, and security tool. This section addresses how AI agent infrastructure can bridge the gap between these natural language instructions and robust concrete access control mechanisms by proposing converting flexible natural language scoping instructions into machine-readable, version-controllable, and auditable structured permission languages that can be leveraged for use in human-in-the-loop settings.\nWe distinguish between task scoping and resource scoping:\n\u2022 Task scoping involves specifying which actions or workflows an agent is authorized to perform on behalf of the user. These actions may range from high-level tasks (e.g., \"draft a financial report\") to more granular actions (e.g., \"create a new database entry\");\n\u2022 Resource scoping involves specifying which resources (information, APIs, tools, etc.) an agent can use or modify.\nWhile conceptually distinct, task scoping and resource scoping are closely connected. Limiting which tasks can be performed also means that a (well-designed) agent will not access unnecessary resources; similarly, restricting access to specific resources also constrains what tasks are feasible in the first place.\nThis section addresses how access control mechanisms can be integrated with complex AI agents and natural language workflows. It outlines the critical nature of structured permissions, how they can provide a robust and generalizable foundation for agent scoping, and how natural language and human oversight can be flexible interfaces for these access controls."}, {"title": "4.1. Structured permission languages", "content": "A large class of scoping mechanisms relies on structured, machine-readable policy specifications. These specifications unambiguously define which entities have which authorizations, under which conditions, and with what privileges. Several well-known languages and frameworks exist for encoding permissions, such as XACML (eXtensible Access Control Markup Language), which uses XML to encode and evaluate access control policies [59], and ODRL (Open Digital Rights Language), designed for expressing usage permissions over digital content [83]. Other languages include OBAC [16], ROWLBAC [31], KaOS [80] and Multi-OrBAC Abou El Kalam & Deswarte [2], which rely on ontologies (typically described using OWL) to model resources, subjects, and authorizations. In web-based contexts, this can often be as simple as whitelisting or blacklisting URLs and subdomains that an agent can access.\nThese structured languages are machine-readable and can thus be enforced reliably by traditional (non-AI) systems. From a practical perspective, they are well-suited for resource scoping, since resources are typically discrete and can be classified, enumerated, and grouped into security domains. For instance, when a policy states that a certain directory is read-only for a particular agent, enforcing compliance is straightforward and can be implemented at the system level.\nHowever, they have three main drawbacks. First, while these frameworks are suitable for enumerating resources, they are less flexible for task scoping, especially when tasks are open-ended or cannot be easily described as a set of operations. Second, policy definitions can become lengthy and complex, especially in environments with a large number of resources and tasks, or in web contexts where the number of possible web interactions is enormous. Third, they are often environment-specific and require updating for different digital systems with which an agent interacts.\nDespite these drawbacks, structured permission languages remain a cornerstone of access control because they provide a precise, easily auditable basis for resource scoping. An alternative approach involves using schema validation to constrain how agents interact with the environment, discussed in subsection B.1"}, {"title": "4.2. Authentication flows", "content": "Another dimension of controlling agent behavior is the authentication flow (i.e., deciding when to prompt a user or another authority for confirmation before the agent proceeds with an action). Rather than frontloading all access decisions into a single policy definition, an authentication flow can dynamically request user approval for borderline or high-risk operations.\nThe main advantage of this approach is that users do not need to define every edge case in a static policy. Additionally, authentication flows can be combined with other scoping mechanisms: for example, a policy can state that any resource that is neither explicitly approved nor explicitly forbidden requires human approval.\nOn the other hand, frequent authorization prompts can negatively affect the user experience, leading to \"prompt fa-"}, {"title": "4.3. Natural Language Mechanisms", "content": "Alongside fine-tuning, prompting has often been employed to steer the behavior of a model towards safety [94]. A reasonable extension of this approach would be to train (or prompt) the LLM to interpret permissions described in plain language. For instance, a user might say, \"You are allowed to generate summaries of public documents, but you must not reveal any confidential metrics.\u201d Such instructions can, in principle, be parsed and acted upon by an LLM-based system.\nThe main strength of this paradigm is its user-friendliness. Non-technical users may find expressing policies in natural language much easier than writing formal rules. Moreover, natural language can capture nuanced or context-dependent instructions that are difficult to encode in structured languages. This makes them ideal for both task and resource scoping. Finally, natural language can be used to enforce policies on actions that require reading or using natural language, such as interactions with other LLM-based agents.\nHowever, natural language often lacks the precision needed for reliable policy enforcement. For instance, terms like \"sensitive data\u201d or \u201cprivate emails\u201d may be interpreted differently depending on context. This problem is particularly relevant in the case of conflict between different policies, where ambiguous and context-dependent instructions may yield different interpretations. Relying solely on an LLM to interpret and enforce ambiguous natural language instructions can be risky in security-sensitive contexts.\nIn short, while natural language instructions can serve as a convenient mechanism (especially for task scoping, where other mechanisms are less suitable), they are not reliable enough to be used as standalone policy mechanisms."}, {"title": "4.4. Combining structured permissions, natural\nlanguage, and user oversight", "content": "Resource scoping as a foundation. We argue that the most broadly applicable strategy is to enforce resource scoping with structured permissions. The brittleness of natural language mechanisms makes them unsuitable for production-level usage of AI agents, especially when security or compliance is a concern. In contrast, structured permissions are unambiguous and deterministic, providing verifiable guarantees against unauthorized access. Focusing on resource scoping also significantly reduces the overhead of specifying every authorized task in detail. To an extent, agents could attempt to represent task-scoping instructions in the form of resource scoping, using domain knowledge of the contexts in which they operate. Since resources are generally discrete and can be classified, enumerated, and grouped into domains, controlling resource access implicitly prevents many potential tasks that would require out-of-scope resources. Additionally, structured resource scoping has several advantages:\n\u2022 It does not depend on how a user delegates tasks-be it via a script, an AI agent, or a more traditional workflow;\n\u2022 It is more compatible with existing non-AI access control systems, which focus on machine-readable permissions for resources (e.g., databases or URLs);\n\u2022 It is suitable for structured logging and version control, which simplifies auditing and compliance reporting.\nThough users may supplement resource scoping task constraints written in natural language, the core resource-based policies provide a safety net that is largely immune to ambiguities in language or model vulnerabilities. Even if an LLM or another AI agent is tricked or misaligned, its ability to execute harmful actions is constrained by the underlying resource permissions.\nConnecting to natural language. While robust and auditable, structured resource scoping alone lacks ease of use and flexibility. To address this, the instructions for the LLM (or a separate scoping prompt) can flexibly express the scoping limitations that should be applied. These natural language scopes can be converted to a structured scoping format by the agent or an AI system in the corresponding environment (which has more detailed knowledge of the relevant resource profiles). Examples of conversion between natural language and structured permissions include Subramaniam & Krishnan [79], which generates PostgreSQL restrictions, and Jayasundara et al. [43], which uses retrieval to generate custom JSON policies.\nA similar process could also be performed for different environments and digital services an agent interacts with, allowing a flexible set of permission instructions to be applied across a wide range of services and contexts (which is important given the broad action space of AI agents).\nBringing a human in the loop. The key final step is validating these structured access controls via the human"}, {"title": "delegator.", "content": "Authorization workflows present an opportunity for users to briefly review and approve structured access control limitations for different systems. For instance, in Wright [90] LLM agents agree on structured information (in this case, meeting dates) which are then confirmed by human users.\nCombining into a hybrid implementation. Bringing these elements together into an implementation is relatively straightforward. An LLM assists in converting high-level, natural language resource constraints into formal, structured rules that users can subsequently review and approve. For example:\n1. A user writes: \"Allow the agent to read and write to the directories about 'projectAlpha', but do not grant it access to the folders with financial folders;\"\n2. The LLM translates this requirement into a policy definition, either in a universal permission language (e.g., XACML) or in the specific permission language used by the resource (e.g., SQL access policies for databases). In this specific case, the LLM enumerates \"projectAlpha\" resources while explicitly denying access to \"financials2023;\"\n3. The user reviews, corrects if necessary, and finalizes the policy.\nWhile many specific details of such a workflow need to be address such as intermediate validation checks and the evaluation of robustness of LLM translation into structured languages, we leave these specifics to future work.\nUltimately, focusing on structured, unambiguous resource constraints is the most reliable way to ensure that an AI agent remains within authorized bounds in a given environment. While there is still room for higher-level (often natural language) task constraints, one should treat these as guidance towards the primary enforcement mechanism. Indeed, while natural language can adequately address the extremely large possible space of agent actions, its transformation into access controls grounds the limitations on agent actions into finite auditable controls. Structured resource scoping reduces the reliance on model alignment alone, decreases the risk of adversarial prompt injections, and simplifies the integration with well-established security mechanisms. Combining this approach with well-designed authentication flows and helping the user interpret the generated policies can reduce the chances of human errors, enhance accountability, and improve the robustness of authenticated delegation."}, {"title": "4.4.1. INTER-AGENT SCOPING.", "content": "Extending beyond the user-agent-service model, this approach can apply to multi-agent settings where agents want to propagate their limitations onto other agents performing actions on their behalf. Suppose that the user specifies the authorizations of an agent Alice. When Alice interacts with another agent, Bob, in natural language to perform a task, Bob can parse Alice's scoping instructions and interpret them in its own environment. By doing so, Bob can confirm that its assigned operations remain within the original scope, and provide an auditable receipt of the actions taken and the resources accessed. This is particularly useful in scenarios where inter-agent communication spans different organizations, each with separate policies and resource constraints.\nFor a concrete example, suppose that Alice is a project management agent and Bob is an accounting agent. The user describes in plain English a financial data request to Alice; Alice thus sends the forwarded request and a description of the authorizations to Bob. Bob replies with a structured interpretation of the authorizations (e.g., \"Read-only access to 'transactions2025' dataset, columns: total amount, vendor name\"), which is logged and approved by either the user or Alice.\nSuch a workflow ensures that even if the agents communicate in flexible natural language, their underlying scoping and record-keeping remain anchored in auditable, deterministic policy. As a result, the risk of unauthorized data sharing or unbounded agent behavior is greatly reduced, and each agent's capacity to \u201cinherit\u201d restricted credentials from the delegator is tightly controlled."}, {"title": "5. Discussion", "content": "5.1. Problems with an OpenID Connect approach\nWhile the OpenID Connect (OIDC) and OAuth 2.0-based framework proposed here provide robust and battle-tested mechanisms for authentication and delegation, it comes with trade-offs and may be more complex than alternatives with different trade-offs in privacy, security, and auditability.\nOverhead from multiple sign-in flows. A significant drawback of the OpenID Connect approach is the potential overhead introduced by multiple sign-in flows required to authorize AI agents across individual service providers. This can be likened to the experience of setting up a new email client, where users must repeatedly log in to authorize access to various services. While such authorization flows enhance security by ensuring each provider independently verifies the AI agent's delegation credentials, they impose a usability cost by slowing down access to secure systems.\nIn theory, it is possible to bypass this burden by presenting delegation tokens directly without performing the full OIDC authentication flow; however, this shortcut sacrifices key security guarantees, particularly those related to token freshness and verification."}, {"title": "Increased reliance on OpenID Providers and privacy\nrisks.", "content": "The reliance on OpenID Providers (e"}]}