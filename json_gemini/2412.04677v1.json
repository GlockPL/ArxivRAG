{"title": "Zephyr quantum-assisted hierarchical Calo4pQVAE for particle-calorimeter interactions", "authors": ["Ian Lu", "Sebastian Gonzalez", "J. Quetzalcoatl Toledo-Marin", "Abhishek Abhishek", "Hao Jia", "Deniz Sogutlu", "Sehmimul Hoque", "Colin Gay", "Roger Melko", "Geoffrey Fox", "Eric Paquet", "Maximilian Swiatlowski", "Wojciech Fedorko"], "abstract": "With the approach of the High Luminosity Large Hadron Collider (HL-LHC) era set to begin particle collisions by the end of this decade, it is evident that the computational demands of traditional collision simulation methods are becoming increasingly unsustainable. Existing approaches, which rely heavily on first-principles Monte Carlo simulations for modeling event showers in calorimeters, are projected to require millions of CPU-years annually-far exceeding current computational capacities. This bottleneck presents an exciting opportunity for advancements in computational physics by integrating deep generative models with quantum simulations. We propose a quantum-assisted hierarchical deep generative surrogate founded on a variational autoencoder (VAE) in combination with an energy conditioned restricted Boltzmann machine (RBM) embedded in the model's latent space as a prior. By mapping the topology of D-Wave's Zephyr quantum annealer (QA) into the nodes and couplings of a 4-partite RBM, we leverage quantum simulation to accelerate our shower generation times significantly. To evaluate our framework, we use Dataset 2 of the CaloChallenge 2022. Through the integration of classical computation and quantum simulation, this hybrid framework paves way for utilizing large-scale quantum simulations as priors in deep generative models.", "sections": [{"title": "1 Introduction", "content": "The High Luminosity Large Hadron Collider (HL-LHC), expected to be operational by the end of this decade, will offer unprecedented opportunities to measure the Higgs boson properties, explore the Standard Model in greater depth, while also searching for physics beyond the Standard Model [1]\nA critical component of this endeavor is the vast amount of data obtained from numerical simulations, which play a crucial role in both the design of future experiments and in the analysis of current ones.\nThese simulations, done with Geant4 [2, 3], accurately describe the collisions at the Large Hadron Collider (LHC). But this comes at the price of being computationally intensive. These simulations describe the interactions between detectors and primary particles, but also account for the interaction with secondary particles produced as the primary particles interact with the detector material. Such is the case with calorimeters, which measure energy deposition from showers of secondary particles. Current projection for the HL-LHC run estimate millions of CPU-years per year [4]. Simulating one single event with Geant4 in an LHC experiment requires approximately 1000 CPU seconds, with the calorimeter simulation being the most resource-intensive module [5]. Through the generation of these showers, non-negligible computational resources are being employed in keeping track of these particles. Deep generative surrogates are being developed to model the particle-calorimeter interactions in the simulation pipeline, potentially reducing the overall time to simulate single events by several orders of magnitude. Examples of these are Generative Adversarial Networks [6\u20138], which are now an integral part of the simulation pipeline [9, 10]. Similarly VAEs [4, 11, 12], Normalizing Flows [13, 14], Transformers [15], Diffusion models [16-18] and combinations thereof [19-22], where the last reference combines a VAE with a two-partite quantum annealer (QA). The framework combining VAE with QAs has also been used in different contexts [23, 24]."}, {"title": "2 Methods", "content": "We illustrate our framework by using Dataset 2 of the CaloChallange-2022 [25]. This dataset consists of 100k Geant4-simulated electron showers ranging from 1 GeV to 1 TeV incident particle energy, sampled from a log-uniform distribution. The voxelized detector is in the form of a concentric cylinder with 45 layers in the axial direction of which each layer is made up of an alternating collider-absorber, active (silicon) and passive (tungsten), material. Each layer consists of 144 voxels (volumetric pixels), 9 radially and 16 in the angular direction to yield a total of 45 x 16 x 9 = 6480 voxels in one event as shown in Fig. 1. Each event has its corresponding incident particle energy as its label. We preprocess the data similar to [19], except we omit the last step where the new variable is standardized. Instead we apply a shift to the logits to preserve the sparsity of the shower in the new variables, i.e., the new variable is zero whenever the voxel energy is zero.\nOur model is a variational autoencoder [26] with a 4-partite conditioned restricted Boltzmann machine [27] as the prior, as illustrated in Fig. 2 (a). We used a hierarchical encoder composed of three sub-encoders. These hierarchy levels enforce couplings among latent units by introducing conditioning among latent nodes. In addition, these hierarchy levels introduce skip connections akin to residual networks [28]. We feed the encoded sample from each of the three sub-encoder outputs to three of the partitions in the RBM, while the fourth partition is used to condition the RBM. The RBM condition parameter is the binarized incident particle energy of the event. The prior is the 4-partite restricted Boltzmann machine, where the connections between nodes mimic the Zephyr topology of D-wave's QA [29]. The encoded sample is then fed to the hierarchical decoder, as shown in Fig. 2 (c) where n sub-decoders are allocated to generate 45/n layers per sub-decoder. The hierarchical decoder conditions subsequent layers of the shower based on previous layers through hierarchies of auto regressive sub-decoders to simulate the physical propagation of particle scattering in the calorimeter during the evolution of a shower. The hierarchical decoder in Calo4pQVAE consists of 9 subdecoders, each generating 5 layers, making up the entire 45-layer voxelized shower. The decoder outputs a mask vector and an activation vector, and their Hadamard product yields the generated shower. We use the Gumbel trick [30] in our framework to generate both the encoded shower as well as to generate the output mask. Our code is publicly available and can be found here [31]."}, {"title": "3 Results", "content": "We trained our model classically for 100 epochs via the evidence lower bound (ELBO) function similar to [21], set the number of Gibbs sampling steps for the RBM to 3000 and used contrastive divergence [32]. During the first 45 epochs we linearly annealed the activation function slope used in the Gumbel trick, from 5 to 500. Afterwards, we continued the training for another 45 epochs, afterwards we froze the encoder and decoder parameters and continued training the prior up to 150 epochs in total. The model was trained using NVIDIA RTX A6000. We validate our model using D-wave's Advantage2_prototype2.3 for inference. It has been well documented how QAs can reach a freeze-out state [33], akin to glass-forming melts under a fast quench [34]. Despite this, it has been shown that the distribution in this freeze-out state can be approximated with a Boltzmann distribution [23]. We estimate the effective inverse temperature of the QA by means of a mapping with an attractive fix point at the QA's effective inverse temperature. This mapping is robust and converges faster than the method used in [21].\nIn Fig. 4 we show the histograms for sparsity index (defined as the ratio between zero-energy voxels and total number of voxels), the energy per event and the shower standard deviation of the shower angular fluctuation, for ground truth, reconstruction, classical samples and QA samples. In Fig. 5 we compare the mean energy along the radial, angular and axial axis between the ground truth and our model."}, {"title": "4 Conclusion", "content": "In this paper we presented our 4-partite quantum-assisted deep generative model for calorimeter synthetic data generation. This framework provides competitive performance for simulating particle showers at the LHC experiments while running extremely quickly on D-wave quantum annealers. The quality of the synthetic data is average compared to other approaches [16, 19]. This may be due in part to the sparse connectivity of the RBM, which mimics the QA connectivity. In addition to RBM connectivity, our framework could benefit from using attention layers similar to [19], which we leave for future work. Furthermore, Ref. [22] presents an improved model that reaches KPD and FPD values of the order of 0.9 \u00b7 10\u20133 and 450 \u00b7 10-3, making our framework competitive compared to the frameworks analyzed in the CaloChallenge [38].\nThe generation time using GPU is dominated by the block Gibbs sampling steps to reach equilibrium. However, the number of steps to reach equilibrium is strongly dependent on training [39]. In our framework, we used 3000 steps, which is more than typically used. Under these conditions, the generation time per event using GPU is roughly 500 times faster than Geant4. Although the annealing time per sample using QPU is 20 \u00b5s, there is technical overhead. Under the previous conditions, the generation time per event using QPU is roughly one order of magnitude faster than using GPU. A more rigorous analysis is required in this comparison, due to the nuances involved in estimating the generation time using QPU and using GPU, and since our preliminary results indicate that they differ by one order of magnitude. We leave this for future work and reiterate that our framework is significantly faster than Geant4. In conclusion, our work on Calo4pQVAE demonstrates the utility of hybrid classical and quantum frameworks in generative AI. This hybrid framework opens new opportunities for leveraging large-scale quantum simulations as priors within deep generative models for high-energy physics and potentially beyond."}]}