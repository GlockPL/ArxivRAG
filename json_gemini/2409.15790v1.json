{"title": "SMALL LANGUAGE MODELS:\nSURVEY, MEASUREMENTS, AND INSIGHTS", "authors": ["Zhenyan Lu", "Xiang Li", "Dongqi Cai", "Rongjie Yi", "Fangming Liu", "Xiwen Zhang", "Nicholas D. Lane", "Mengwei Xu"], "abstract": "Small language models (SLMs), despite their widespread adoption in modern smart devices, have re-\nceived significantly less academic attention compared to their large language model (LLM) counter-\nparts, which are predominantly deployed in data centers and cloud environments. While researchers\ncontinue to improve the capabilities of LLMs in the pursuit of artificial general intelligence, SLM\nresearch aims to make machine intelligence more accessible, affordable, and efficient for everyday\ntasks. Focusing on transformer-based, decoder-only language models with 100M-5B parameters,\nwe survey 59 state-of-the-art open-source SLMs, analyzing their technical innovations across three\naxes: architectures, training datasets, and training algorithms. In addition, we evaluate their capa-\nbilities in various domains, including commonsense reasoning, in-context learning, mathematics,\nand coding. To gain further insight into their on-device runtime costs, we benchmark their infer-\nence latency and memory footprints. Through in-depth analysis of our benchmarking data, we offer\nvaluable insights to advance research in this field.", "sections": [{"title": "1 Introduction", "content": "The evolution of language models is diverging. On the one hand, in pursuit of artificial general intelligence following\nthe scaling law, larger and larger language models (LLM) have been born in datacenters that host hundreds of thousands\nof GPUs [34, 71]. The ultimate goal of this path is to demonstrate that machines can solve the most challenging\nlanguage tasks and greatly advance human civilization (e.g., science). On the other hand, small language models\n(SLM) are proposed for resource-efficient deployment, especially on devices such as desktops, smartphones, and even\nwearables. A vision for SLMs is to democratize machine intelligence, making it accessible and affordable to anyone,\nanywhere, at any time \u2013 just like the human brain that everyone possesses.\nBoth LLM and SLM are important in reshaping our daily lives, yet the latter receives significantly less attention in\nacademia. There has been very limited literature that understands SLM's capability [37, 59, 79] or their runtime cost\non devices [41, 36, 69], often with limited scale or depth. In the real world, however, SLMs have already been inte-\ngrated into commercial off-the-shelf (COTS) devices on a massive scale [74]. For instance, the latest Google/Samsung\nsmartphones have built-in LLM services (Gemini Nano), allowing third-party mobile apps to leverage LLM capabil-\nities through prompts and LoRA modules [1]. The most recent iOS system on iPhones and iPads also includes an\non-device foundation model, deeply integrated with the operating system for better performance and privacy [2]."}, {"title": "2 SLM Architecture, Datasets, and Training", "content": ""}, {"title": "2.1 Overview of SLMS", "content": ""}, {"title": "2.2 Model Architecture", "content": "While we focus on only decoder-only transformer SLMs, their specific configurations still diversify, as shown in\nFigure 2(a). The core of Transformer is the multi-head self-attention(MHA) mechanism and the Feed-Forward Neural\nNetwork(FFN).\nModel architecture analysis. We conduct statistical analysis on the following several components of the model ar-\nchitecture: 1) The type of self-attention; 2) The type of feed-forward neural network; 3) The intermediate ratio of the\nfeed-forward network; 4) The activation function of the feed-forward neural network; 5) The type of layer normaliza-\ntion; 6) The vocabulary size. Figure 2(a) shows the architecture of SLM and the pie chart shows the distribution of six\ncomponents. Figure 2(b) shows how these distributions change over time.\n1) The type of self-attention. The self-attention mechanism is the core of the Transformer model. In general, SLMs\nmainly use three types of attention mechanism: Multi-Head Attention (MHA), Multi-Query Attention (MQA), Group-\nQuery Attention (GQA) and Multi-Head Latent Attention(MLA). Multi-Head Attention is a mechanism that allows the\nmodel to focus on different parts of the input data simultaneously by employing multiple attention heads, which is the\nmost widely used self-attention mechanism in the Transformer models. Multi-Query Attention simplifies multi-head\nattention by using a single shared query across all heads but allowing different key and value projections. This reduces\nthe complexity in both space and time. Group-Query Attention is a variant of multi-head attention that reduces com-\nputational complexity by sharing query representations across multiple heads, while allowing separate key and value\nrepresentations. The idea is to use fewer query groups but still preserve a level of diversity in the attention mechanism.\nMulti-Head Latent Attention achieves better results than MHA through low-rank key-value joint compression, and\nrequires much less Key-Value(KV) Cache.\nFigure 2(b) shows the changing situation of choosing three self-attention mechanisms during these time periods\nfrom 2022 to 2024. We can see that MHA is gradually being phased out and replaced by GQA.\n2) The type of feed-forward neural network. Feed-forward network can be summarized into two types: the Standard\nFFN and the Gated FFN. The Standard FFN is a two-layer neural network with a activation function. The Gated FFN\nadds an additional gate layer.\nThe Figure 2(b)\u2461 shows the changing situation of type of FFN during these time periods from 2022 to 2024. It shows\nthat Standard FFN is gradually being phased out and replaced by Gated FFN.\n3) The intermediate ratio of the feed-forward neural network. The intermediate ratio of FFN is the ratio of the inter-\nmediate dimension to the hidden dimension. Figure 2(b)\u2462 shows that the intermediate ratio of the Standard FFN is\ncommonly set to be 4, while the intermediate ratio of the Gated FFN is rather diversified ranging from 2 to 8.\n4) The activation function of the feed-forward neural network. There are 4 main kinds of activation functions used\nin FFN: ReLU (Rectified Linear Unit), GELU (Gaussian Error Linear Unit), GELUtanh, SiLU (Sigmoid Linear Unit).\nObserved from Figure 2(b) \u2463, the activation function of FFN was mostly ReLU in 2022, and then changed to GELU\nand its variants in 2023. For those released in 2024, SiLU becomes the dominant type.\n5) The type of layer normalization. There are two main types of layer normalization: LayerNorm and RMSNorm. The\nFigure 2(b) shows the changing situation of type of the type of layer normalization during these time periods from\n2022 to 2024. layer normalization is gradually being phased out and replaced by RMS normalization.\n6) The vocabulary size. The vocabulary size is the total number of unique tokens that an SLM can recognize. The\nFigure 2(b) shows the changing situation of the vocabulary size during these time periods from 2022 to 2024. We\ncan see that the vocabulary size of the model is gradually increasing. The vocabulary of the latest models is often\nlarger than 50k\nModel architecture innovations. While the vanilla transformer architecture has been well recognized for its scaling\nability, there still exist a few architecture-level innovations in the tested SLMs, namely parameter sharing and layer-\nwise parameter scaling.\n1) Parameter Sharing. Parameter Sharing is a technique used in large language models to reuse the same set of weights\nacross different layers or components of the network. This approach allows the model to significantly reduce the\nnumber of parameters, leading to more efficient training and inference, while maintaining performance.\nEmbedding-lm_head sharing. Sharing the weights of the embedding with the final Im_head layer is the most common\nweight sharing technique. It is the sharing of the word embedding layer and has nothing to do with the rotary position\nencoding. Models such as Gemma, and Qwen all used this sharing technique.\nlayer-wise attention/FFN sharing. In this approach, the same set of weights is reused across multiple layers of the\nmodel. This is commonly seen in SLM/LLM, where all the transformer layers share the same parameters. For example,"}, {"title": "2.3 Training Datasets", "content": "We investigate how the open-sourced pre-training datasets are used in training the SLMs. Overall, we find 12 such\ndatasets being used:\n\u2022 The Pile [26] (825B tokens): a combination of smaller corpora in various domains.\n\u2022 FineWeb-Edu [55] (1.3T tokens): a collection of educational text filtered from Fine Web.\n\u2022 StarCoder [40] (35B tokens): Python tokens.\n\u2022 Cosmopedia [12] (25B tokens): a dataset of synthetic textbooks, blogposts, stories, posts and WikiHow\narticles generated by Mixtral-8x7B-Instruct-v0.1.\n\u2022 RefinedWeb [56] (5T tokens): despite extensive filtering, high-quality data extracted from the web remains,\nplentiful, obtained from CommonCrawl.\n\u2022 RedPajama [21] (1.2T tokens): includes over 100B text documents coming from 84 CommonCrawl snapshots\nand processed using the CCNet pipeline.\n\u2022 Dolma [60]: a English corpora, which is deduplicated inner corpus and across corpus using MinHash algo-\nrithms.\n\u2022 WuDaoCorpora [75] (4T tokens): a super large-scale Chinese corpora, containing about 3T training data and\n1.08T Chinese characters.\n\u2022 ROBERTa [43] CCNewsV2: containing an updated version of the English portion of the CommonCrawl News\ndataset.\n\u2022 PushShift ().io Reddit [11]: a social media data collection, analysis, and archiving platform that since 2015\nhas collected Reddit data and made it available to researchers.\n\u2022 DCLM-baseline [39] (1.35T tokens): a standardized corpus extracted from Common Crawl, effective pre-\ntraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations.\n\u2022 CulturaX [52] (6.3T tokens): a substantial multilingual dataset in 167 languages.\nThe usage preference of pre-training datasets. We then conducted statistics on the usage frequency of the datasets\nfor training SLM from 2022 to 2024. The results are illustrated in Figure 3. It shows that The Pile is the most widely\nused pre-training dataset especially in 2022 and 2023; yet more recently, more such datasets are proposed and the\nchoice becomes diversified. In fact, The Pile has been abandoned in pre-training SLMs recently, and datasets such"}, {"title": "2.4 Training Algorithms", "content": "There have been a few novel training methods to improve the model capability.\nMaximal Update Parameterization(\u00b5P) controls initialization, layer-wise learning rates, and activation magnitudes\nto ensure analytically stable training independent of a model's layer widths. In addition to improving training stability,\n\u00b5P also improves the transferability of training hyperparameters from smaller to larger scale models, which permits\ndirectly using the same settings for some optimizer hyperparameters, most notably the learning rate. For example,\nCerebras-GPT trains models with Maximal Update Parameterization.\nKnowledge Distillation is a crucial concept in the realm of Large Language Models (LLM). It involves extracting\nvaluable knowledge from a large and complex teacher model and transferring it to a smaller and more efficient student\nmodel. The essence of this technique is to have the student model learn to approximate the behavior and predictions\nof the teacher. This is achieved by minimizing the difference between their outputs. According to our statistics,\nLaMini-GPT and Gemma-2 adopt Knowledge Distillation.\nTwo Stage Pre-training Strategy is a training strategy that involves training a model in two distinct phases. During\nthe pretraining phase, MiniCPM only uses large-scale coarse-quality pre-training data, which is abundant and can\nsupport continuous training when provided with more computational resources. During the annealing phase, we use\ndiverse and high-quality knowledge and ability-oriented SFT data, mixed into the pre-training data. MninCPM adopts\nTwo Stage Pre-training Strategy."}, {"title": "3 SLM Capabilities", "content": ""}, {"title": "3.1 Evaluation Datasets and Metrics", "content": "We used 12 datasets across three domains to evaluate the SLM performance.\n\u2022 Commonsense Reasoning Datasets:\nHellaSwag [76]: Tests narrative understanding through plausible sentence completion.\nTruthfulQA [42]: Assesses the model's ability to avoid providing false information.\nWinogrande [58]: Evaluates pronoun ambiguity resolution using commonsense reasoning.\nCommonsenseQA [64]: Presents multiple-choice questions requiring everyday knowledge.\nPIQA [15]: Focuses on physical commonsense reasoning and object interactions.\nOpenBookQA [51]: Combines scientific knowledge with commonsense for open-book science ques-\ntions.\nBoolQ [18]: Tests commonsense and factual reasoning with yes/no questions.\n\u2022 Problem-Solving Datasets:\nARC Easy [19]: Contains simple science questions testing general knowledge and reasoning.\nARC Challenge [19]: Presents complex science exam questions requiring knowledge integration.\nMMLU [31]: Evaluates problem-solving across diverse academic disciplines.\n\u2022 Mathematics Datasets:\nGSM8K [20]: Assesses grade-school-level mathematical reasoning skills.\nMinerva Math [38]: Evaluates advanced mathematical reasoning across various topics.\nWe use accuracy as the primary evaluation metric. Accuracy measures the proportion of correct predictions to total\nexamples. The default shown accuracy is instructed by 5 shots, as it is the most common setting in the released model.\nFor commonsense reasoning, problem-solving, and mathematics tasks, accuracy evaluates the model's ability to select\ncorrect options or provide accurate solutions."}, {"title": "3.2 Overall Capabilities", "content": "As shown in Figure 5, we conducted experiments on selected SLMs across three tasks-commonsense reasoning,\nproblem-solving, and mathematics\u2014to analyze their progress. The results show substantial performance improve-\nments across all tasks between 2022 and 2024. Specifically, model performance improved by 10.4%, 13.5%, and\n13.5% for the three tasks, respectively. In comparison, the state-of-the-art open-source LLaMA model exhibited an\naverage improvement of only 7.5% over the same period. Notably, the Phi family, trained on closed-source datasets,"}, {"title": "3.3 In-context Learning Capabilities", "content": "We conduct in-context learning experiments using various models and their 2B-parameter variants (or the closest\navailable ones) across 8 tasks, including commonsense reasoning and problem-solving tasks. Generally, SLMs benefit\nsignificantly from in-context learning across all tasks. Exceptions include the HellaSwag and PIQA datasets, where\nall models perform similarly regardless of the number of in-context learning shots. These datasets are simpler and\ndo not benefit as much from in-context learning as more complex datasets, such as ARC Challenge. On average, in-\ncontext learning with 5 shots improves the performance of zero-shot SLMs by 2.1% across all tasks. The only notable\nexception is LaMini, which shows a decrease of over 2% in performance. We hypothesize that this model may be\noverfitting the training data, and additional context shots introduce noise. Among the models, Gemma 2 exhibits the\nmost significant improvement, with a 4.8% increase in accuracy. Interestingly, we observe that as model size increases,\nthe in-context learning capability of SLMs is enhanced."}, {"title": "4 SLM Runtime Cost", "content": "Setup In this section, we first provide an overall analysis of the latency and memory used by models with different\nparameter sizes. Next, we examine the impact of quantization methods and hardware on model latency. Finally, we\nbreak down the latency and memory usage to identify the key factors affecting them in different parts of the model.\nWe evaluated 20 models on two types of edge devices: the Jetson Orin Module, commonly used in edge AI devices\nsuch as drones and small robots, and a smartphone, which people rely on in their daily lives. The detailed specifications\nare shown in Table 3. All experiments on the Jetson used its GPU, while those on the smartphone were performed using\nits CPU. To eliminate the impact of inference engine implementations, we carry out all experiments using llama.cpp,\na widely recognized open-source inference engine.\nWe primarily recorded metrics as model parameter, latency during the prefill and decode phases, and runtime memory\nusage. Due to variations in how each model officially reports its parameter counts, we relied on the parameter values\nobtained from llama.cpp. Inference is divided into two phases: prefilling and decoding. During the prefill phase, the"}, {"title": "4.1 Overview", "content": ""}, {"title": "4.1.1 Inference Latency", "content": "In Figure 7, the inference latency including first token time and decode latency per token for the models ranging in\nsize from 0.1B to 3B were measured, revealing that they can be categorized into three intervals: 0.1-1B, 1-2B, and\n2-3B. The inference latency within each interval is relatively similar and aligns with the latency increase as the model"}, {"title": "4.1.2 Memory Footprint", "content": "The evaluation of memory footprint in Figure 7 used llama.cpp on Jetson. The size of models range from 0.1B\nto 3B parameters and the memory footprint range from 275MB to 2456MB. Due to llama.cpp defaulting to allocate\nKV cache and compute buffer according to the maximum context length of the model, models that support longer\ncontexts end up consuming significantly more memory than others. In our experiments, we set the maximum context\nlength for all models to 2048 to standardize memory usage. Under the same context length, memory usage is linearly\nrelated to model size. However, some models exhibit memory usage that does not align with their size, such as\nGemma-2B, Bloom-560M, and Bloom-1B1. These models have larger vocabularies compared to others: Gemma-2B\nhas a vocabulary size of 256,000, while the Bloom series has a vocabulary size of 250,880. The OpenELM series has\nlower memory usage compared to models of similar parameter size for two reasons. First, it uses a vocabulary size of\n32,000, smaller than the 50,000 used by most models. Second, it employs GQA, which reduces the KV cache, instead\nof MHA. We will explain in \u00a7 4.3.2 why vocabulary size has a significant impact on model memory usage."}, {"title": "4.2 Impact of Quantization and Hardware", "content": ""}, {"title": "4.2.1 Impact of Quantization", "content": "The benefits of quantization for reducing inference latency on server-side GPUs likely stem from three factors: higher\ncomputational throughput of Tensor Cores for int8 operations, reduced memory access overhead, and the decrease in\nheat generated by reduced memory access. On mobile devices, such as Jetson, support for int8 computation is lacking,\nbut memory access overhead can still be effectively reduced. This reduction comes from data compression due to the\nlower precision of activation values and parameters, which in turn improves cache utilization.\nWe utilized five quantization methods to test the latency of Phi-1.5, as shown in Figure 8. Qn_K (and Qn_K_M) refer\nto the quantization of a model to n bits using the k-quants method with a medium (M) number of parameters, while\nQn_0 specifically refers to symmetric quantization of a model to n bits. For the prefill phase, when the prompt length is\nrelatively short, quantization can reduce latency by at least 25%. However, this benefit diminishes as the prompt length\nincreases. When the prompt length approaches 50, the Q6_K and Q3_K quantization methods result in latency that is\nnearly identical to, or even exceeds, that of the unquantized FP16 model. On the other hand, the Q8_0, Q4_K_M, and\nQ5_K methods provide stable performance improvements. Among these, Q4_K_M performs the best, reducing latency\nby an average of 50%. Quantization during the decode stage delivers more consistent performance gains, reducing\ndecode latency by up to 75% and no less than 17%. As in the prefill stage, the Q4_K_M method proves to be the most\neffective, while Q6_K remains the least efficient."}, {"title": "4.2.2 Impact of Hardware", "content": "We conducted tests using Bloom-1B1 on two types of edge devices: the Jetson Orin NX 16GB, which utilizes its GPU,\nand the Meizu 18 Pro, which relies on its CPU. During the prefill phase, for a single token, the Jetson is approximately\n10 to 20 times faster than the Meizu 18 Pro. Both the Jetson and the Meizu 18 Pro show a linear increase in first token\ntime as the prompt length increases, with the Jetson's advantage becoming more obvious as the prompt length grows.\nDuring the decode phase, the latency per token increases as the number of generated tokens grows. On the Meizu 18\nPro, the latency rises sharply from 1 to 10 tokens and then levels off after 10 tokens. This initial steep rise in latency\nfrom 1 to 10 tokens is due to the temperature increase, which triggers the Dynamic voltage and frequency scaling\n(DVFS) or thermal throttling to adjust power consumption and frequency, thereby reducing computational efficiency."}, {"title": "4.3 Latency and Memory Breakdown", "content": ""}, {"title": "4.3.1 Latency Breakdown", "content": "In Figure 10, we conducted a breakdown analysis of Qwen2-0.5B and Qwen1.5-0.5B, models with the similar size but\ndifferent latency, and measured the time distribution across the Embedding, Attention, FFN(Feed-Forward Network),\nand LM_Head. For Qwen1.5 and Qwen2, the prefill phase is predominantly characterized by the high involvement of\nthe Attention and FFN layers. In Qwen1.5, the Attention layer has a slightly higher proportion than the FFN layer,\nwhereas in Qwen2, the FFN layer's contribution is noticeably greater than that of the Attention layer. This is due to\nQwen2 having a wider FFN layer compared to Qwen1.5. During the decode phase, the proportion of the Attention\nlayer in Qwen1.5 increases, which could be attributed to the increased length of the KV (Key-Value) Cache. As for\nQwen2, it still has longest time for FFN.\nWe also conducted an operator breakdown analysis on Qwen1.5-0.5B and Qwen2-0.5B. Regardless of the model or\nwhether it is during the prefill or decode phase, the operator mul_mat_vec_q, which represents matrix-vector multi-\nplication, accounts for over 80% of the time. The mul_mat_vec_q operator in Qwen2-0.5B has a higher proportion\ncompared to Qwen1.5-0.5B, which may also be due to its wider FFN layer."}, {"title": "4.3.2 Memory Breakdown", "content": "In \u00a7 4.1.2, we found that in addition to model size, vocabulary size also has a significant impact on memory usage.\nIn the figure, we provide a breakdown analysis of the model's memory usage. The runtime memory is primarily"}, {"title": "5 Conclusions and Future Directions", "content": "This paper makes a comprehensive survey and measurement to small language models (100M\u20135B parameters), in-\ncluding their capabilities, runtime cost on devices, and innovations. We then summarize the key insights to inspire\nfuture research on small language models. Specifically, we expect following directions worthy explorations.\nCo-design and co-optimizations of SLM architecture and device processors. With given parameter size, the con-\ncrete SLM architecture still has huge impacts on the runtime speed, as discussed in \u00a74. This includes both the basic\ntransformer configuration (e.g., depth-width ratio, attention type, activation) and how efficiently they can be quan-\ntized for execution on integer-optimized processors like NPUs [68]. To push the limit of SLMs towards optimal\naccuracy-speed tradeoff, we advocate for extreme co-design and optimizations of SLM architecture with specific de-\nvice hardware, possibly searching for a speed-optimal architectures before pre-training on them.\nConstructing high-quality synthetic dataset. Two recent pre-training datasets, DCLM and FineWeb-Edu, have\nexhibited superior performance and greatly closed the gap between SLMs trained on open/closed datasets. The key\ninnovation of them is to use carefully trained model to filter out high-quality data portion from a large corpora. We\nbelieve that we are still at the very beginning of such synthetic data research, and the space to be explored remains\nhuge. It is urgent to standardize a process of synthetic data curation (deduplication, filtering, mixing, evaluation, etc)."}]}