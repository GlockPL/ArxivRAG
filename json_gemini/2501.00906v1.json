{"title": "Large Language Model Based Multi-Agent System Augmented Complex Event Processing Pipeline for Internet of Multimedia Things", "authors": ["Talha Zeeshan", "Abhishek Kumar", "Susanna Pirttikangas", "Sasu Tarkoma"], "abstract": "This paper presents the development and evaluation of a Large Language Model (LLM), also known as foundation models, based multi-agent system framework for complex event processing (CEP) with a focus on video query processing use cases. The primary goal is to create a proof-of-concept (POC) that integrates state-of-the-art LLM orchestration frameworks with publish/subscribe (pub/sub) tools to address the integration of LLMs with current CEP systems. Utilizing the Autogen framework in conjunction with Kafka message brokers, the system demonstrates an autonomous CEP pipeline capable of handling complex workflows. Extensive experiments evaluate the system's performance across varying configurations, complexities, and video resolutions, revealing the trade-offs between functionality and latency. The results show that while higher agent count and video complexities increase latency, the system maintains high consistency in narrative coherence. This research builds upon and contributes to, existing novel approaches to distributed AI systems, offering detailed insights into integrating such systems into existing infrastructures.", "sections": [{"title": "1. Introduction", "content": "The rapid advancement of artificial intelligence (AI) technologies has revolutionized the way we process and analyze data, particularly in the field of complex event processing, such as video query analysis. Traditional CEP systems often struggle with the dynamic demands of modern applications such as real-time or near real-time video analytics that require the integration of diverse data sources, for example, thousands of surveillance cameras deployed in a city, leading to limitations in their performance and applicability.\nModern CEP pipelines are domain-specific and often struggle to adapt to dynamic changes in the environment in a timely manner. State-of-the-art applications (such as live video streaming on TikTok, YouTube etc.) generate an increasing volume of diverse, complex data that needs to be handled in the appropriate manner depending on the use case. Large Language Models (LLMs), also known as foundation models, inherently possess the ability to handle and analyze dynamic forms of data and therefore provide the necessary foundation upon which a dynamic CEP pipeline can be created which can support a diverse range of domains. Our work is derived from this growing need for automated efficient and scalable complex event processing (CEP) systems capable of handling dynamic changes and adapting to changing scenarios. Hence, the primary motivation behind our work is to introduce the usage of LLM agents to augment existing CEP pipelines and explore the potential of LLM-based multi-agent systems to enhance the capabilities of CEP frameworks. By leveraging the strengths of LLMs in such a system, we aim to create a more robust and flexible CEP framework that can fulfil these changing requirements.\nThis paper focuses on two key research questions:\nRQ1: What is the current landscape of LLM-based single or multi-agent systems? LLM-based agent framework is a rapidly emerging field of research. The objective is to provide a comprehensive literature review that highlights the various applications and domains of LLM agent systems.\nRQ2: How can Large Language Models (LLMs) be integrated with multi-agent systems to augment complex event processing (CEP) pipelines that meet stringent quality of service (QoS) metrics associated with the given use case? Integrating LLM-MAS with existing pub/sub infrastructures such as Kafka, necessitates a strategic approach to ensure seamless transition and operation. Given that LLM-backed agents introduce a novel approach to automate existing implementations, it is important to ensure that the proof-of-concept (POC) interfaces effectively with current systems, tools, and technologies. This ensures compliance with existing standards and minimizes disruption to established workflows.\nWith these research questions in mind, we study the feasibility of utilizing an"}, {"title": "LLM Agents", "content": "LLM-MAS framework such as Autogen in tandem with Pub/Sub tools such as Kafka to satisfy the requirement and present a proof-of-concept (POC) that unifies the state-of-the-art LLM Multi-Agent system (LLM-MAS) framework and Pub/Sub tools and technologies to create a CEP pipeline. Our contributions are as follows:\n1. A comprehensive survey, gap analysis of LLM-based autonomous agent applications.\n2. A proof-of-concept for an LLM-MAS augmented CEP pipeline with pub/sub broker tools that can be tested, evaluated and expanded.\n3. A discussion of lessons learned from deploying multi-agent-based systems for CEP use cases.\nThe rest of this paper is organized as follows: section 2 describes the background technologies relevant to the focus of this paper; section 3 provides a comprehensive survey, gap analysis and future trend of LLM-based autonomous agent applications; section 4 presents system design and implementation of our proof-of-concept; section 5 presents experimental protocols, use case, results along with the discussion and limitations; and finally section 6 provides concluding remarks along with future research directions."}, {"title": "2. Background", "content": "This section presents background of key technologies relevant to the focus of our work."}, {"title": "2.1. Distributed AI Systems and the Computing Continuum", "content": "The current state-of-the-art in the Distributed AI (DAI) and computing continuum is primarily characterized by the trend of how AI systems can be designed and deployed while utilizing horizontal resource scaling offered by the continuum. These trends focus on a variety of domains that include the integration of AI with edge computing (aka. Edge AI), advancements in federated learning, the rise of TinyML (AI/ML for IoT), and the adoption of blockchain technology for enhanced security and decentralization in AI systems [1].\nThe computing continuum can be described as mixture of cloud, edge, fog computing and IoT that allows for real-time data processing and decision making at the edge of the network [2]. The concept of the computing continuum represents an advanced model in the evolution of distributed computing frameworks. It extends beyond the traditional cloud-centric models by incorporating a wide array of"}, {"title": "2.2. Publish/Subscribe Paradigm", "content": "The Publish/Subscribe (Pub/Sub) paradigm is a messaging model predominantly utilized in distributed systems [6, 7]. It is characterized by a decoupled interaction model where the entities producing the messages (publishers) are separated from the entities receiving these messages (subscribers) [6]. This model is instrumental in systems where the dissemination of information is event-driven and requires broad yet targeted distribution [8].\nThe core concepts encompassing the Pub/Sub paradigm can be summarized as follows:\n\u2022 Publishers: Publishers are responsible for generating and sending out messages. Crucially, they operate independently of the subscribers, often unaware of the recipients of the information they publish.\n\u2022 Subscribers: Subscribers express their interest in certain types of messages by subscribing to specific topics or event channels. They receive messages that match their subscription criteria, enabling them to process relevant information as needed.\n\u2022 Event Channels/Topics: These are the pathways or categories to which messages are published. Publishers send messages to these channels, and subscribers receive messages from the channels they have subscribed to."}, {"title": "2.3. Large Language Models (LLMs) / Foundation Models", "content": "Large Language Models (LLMs) have brought upon a revolution in terms of how we think of and approach artificial intelligence. These large models can take\nin images, text and audios as inputs and produce textual outputs [11, 12], providing powerful tools for natural language processing, reasoning, and decision-making tasks [13].\nLLMs can be described as neural network models that are trained on enormous volume of text corpus and can have millions or billions of parameters. For context, models like BERT (Bi-directional Encoder Representation from Transformers) [14] is composed of 110 million parameters, ChatGPT which was based on the GPT-3.5 model at the time of its public release, boasted 175 billion parameters [15], LLAMA [12] is a collection of LLMs developed by meta that boast models whose parameters range from 7 billion to 70 billion and PaLM-2 [16] possesses up to 340 billion parameters.\nMost modern LLMs are based on the Transformer architecture that provides unmatched capacity and parallelization [17]. The transformer architecture was first introduced by Vaswani et.al [18] that relies exclusively on self-attention mechanisms and was the foundation of first generation LLMs. However, most current high-performing LLMs (listed above) are based on the casual-decoder architecture. This architecture can be described as a decoder-only design that utilizes a diagonal mask matrix so that the current token can only access information from tokens earlier in the stack [17]. This break through in LLM architecture and design led to the creation of high-performing LLMs such as GPT-4 which proved to be a disruptive technology. With the advent of such models, new research avenues were primed for exploration and use-cases across all-domains experienced disruptive effects due to the power of LLMs. Liu et.al [19] in their paper highlight the affect of ChatGPT on various research domains, ranging from audio-speech processing, economics, astrophysics and cryptography. Huang et al. [20] in their paper, showcased the potential of LLMs to function as zero-shot planners, extracting actionable knowledge for agents in an interactive environment. Their work focused on grounding high-level tasks, expressed in natural language, into a series of actionable planned steps. They found that large pre-trained language models can effectively decompose high-level tasks into actionable plans. For example, in their environment, a high level tasks such \"make fried eggs\" can be decomposed into an actionable plan such as; \"Open Fridge, Take out 2 eggs from the egg carton, Place pan on stove, add oil, ...\". Continuous advancement in LLMs lead to the creation of multi-modal LLMs that can process not just textual input, but other forms of media such as audio-video, thus opening the door to research based in complex-event processing."}, {"title": "2.4. Autogen: An LLM Multi-Agent Framework", "content": "AutoGen is an open-source framework that enables the development of applications through multi-agent conversations [21]. This section explores the AutoGen LLM orchestration framework, detailing its architecture, functionality, and usage in the presented proof-of-concept.\nAutogen allows for the creation of LLM applications by utilzing \"Agents\" that are capable of inter-agent communication. Agents can be described as entities that have LLM capabilities and are able to send and receive messages from other agents. Each agent can be configured to execute specific tasks within a workflow, such as code generation, code execution, or syntax validation [21]. This inter-agent communication capability enables the creation of complex workflows that surpass the capabilities of single-agent frameworks. AutoGen facilitates the creation of LLM applications that utilize multiple agents capable of conversing with each other to accomplish a diverse range of tasks. The framework supports the integration of LLMs, human inputs, and external tools and technologies to create agents that can operate in a flexible, modular, and customizable manner [21].\nBy leveraging recent advances in chat-optimized LLMs like GPT-4 [11], Llama [12], AutoGen allows these agents to cooperate, reason, and validate each other's outputs through structured conversations. This multi-agent orchestration framework thus provides a robust foundation for developing sophisticated LLM applications, where agents can dynamically interact and cooperate to achieve complex objectives."}, {"title": "2.4.1. Autogen Agents", "content": "AutoGen's flexible and modular architecture is built around the concept of \"conversable agents,\u201d which are designed to handle specific roles within a multi-agent conversation. These agents can send and receive messages, maintain context, and execute tasks based on their configured capabilities. Augogen has three foundation agents each with its own special role and purpose listed below:\n\u2022 Conversable Agent: Serves as the base class for other agents. It can provide automated replies based on feedback and/or code executions. It can generate and execute code and converse with other agents.\n\u2022 Assistant Agent: A representative subclass of the conversable agent which is designed to act as an AI assistant. It possesses LLM capabilities and does not require human input. It can receive code execution results and provide feedback for further steps which can guide a complex workflow."}, {"title": "3. Related Works", "content": "\u2022 User Proxy Agent: Acts as a proxy for the user a multi-agent system. It can prompt for human inputs and executes code/function calls automatically based on human or tool feedback.\nBased on the aforementioned types of agents in Autogen, the kind of agents that can be configured in AutoGen can be broadly categorized into LLM-backed agents, human-backed agents, and tool-backed agents. Each type of agent has unique functionalities and can be customized to meet specific application needs. Each type of agent is further described below:\nLLM Backed Agents: LLM-backed agents leverage the capabilities of advanced large language models to perform a variety of functions. These agents can role-play, infer implicit states, provide feedback, adapt based on feedback, and even generate and execute code. The primary advantage of LLM-backed agents is their ability to harness the sophisticated language processing and reasoning capabilities of models like GPT-4. They can be configured to handle complex conversational tasks autonomously. Key capabilities of LLM-backed agents include:\n\u2022 Role Playing: These agents can assume specific roles in a conversation, such as a tutor, assistant, or expert, and provide responses based on their role.\n\u2022 State Inference: They can infer the state of a conversation from the context due to their shared memory and make decisions accordingly.\n\u2022 Feedback Provision and Adaptation: LLM-backed agents can provide feedback on their actions and adapt their responses based on the feedback they receive.\n\u2022 Code Generation and Execution: These agents can generate code to solve problems, execute the code, and interpret the results.\nHuman Backed Agents: Human-backed agents are designed to incorporate human input into the conversation at various stages. This type of agent is crucial for tasks that require human judgment, expertise, or oversight. Human-backed agents can prompt for inputs from human users, ensuring that the conversation or task execution benefits from a human involvement. Configurations and use cases for human-backed agents include:\n\u2022 User Proxy: Acts as a proxy for human users, allowing human input to be incorporated at specific points in the conversation."}, {"title": "Tool Backed Agents:", "content": "\u2022 Human Oversight: These agents can request human oversight to validate outputs and make decisions, or handle exceptions that are beyond the capabilities of LLMs if the tasks are related to coding.\nTool-backed agents extend the functionality of AutoGen by enabling the use of external tools and code execution within the conversation. These agents can execute predefined functions, interact with software tools, and manage data processing tasks. By leveraging existing code bases and tools, tool-backed agents enhance the robustness and reliability of the system while minimizing the risks associated with dynamic code generation. Capabilities of tool-backed agents include:\n\u2022 Code Execution: They can execute code snippets generated by LLM-backed agents or provided by human users.\n\u2022 Function Calls: These agents can make function calls to external tools or services, integrating their outputs into the conversation.\n\u2022 Error Handling and Debugging: Tool-backed agents can handle errors during code execution and provide debugging information to resolve issues\nThe integration of these diverse agents within AutoGen allows for the creation of sophisticated multi-agent systems capable of handling a wide range of tasks and applications. By combining different types of agents, AutoGen can support complex workflows that require diverse capabilities. For instance, an LLM-backed assistant agent can generate a solution, which is then validated by a human-backed user proxy agent or executed by a tool-backed agent. This modular approach ensures that each part of the workflow is handled by the most appropriate agent, enhancing the overall efficiency and effectiveness of the system."}, {"title": "3. Related Works", "content": "This section presents a comprehensive review of prior research and literature in the field of LLM-based autonomous agent applications. By examining the latest advancements in this rapidly evolving domain, this section aims to shed light on the state-of-the-art methodologies and challenges. The objective is to provide a clearer understanding of the current landscape and potential future directions in LLM-based autonomous agents."}, {"title": "3.1. LLM Agents", "content": "As elaborated upon in Section 2.3, LLMs have revolutionized the way we interact with artificial intelligence and have proven to be a disruptive technology. They exhibit or show potential in having near human-like intelligence and are the closest thing to Artificial General Intelligence (AGI). With this surge in LLM capabilities, interest has risen in exploring and utilizing LLMs in complex work-flows to achieve a diverse range of tasks. An LLM based agent can be considered to possess three main conceptual blocks; Brain, Perception and Action [22]. The perception block can be considered to be the \"Input\" for the agent. The \"Brain\u201d represents the LLM model that processes the input and provides an output that can be considered an \"Action\".\nThe development of LLM agents has opened new research avenues across various domains. Given their human-like conversational patterns, there now exists the potential to automate complex processes through simple textual prompts from end users. These LLM agents enable the abstraction of complexities inherent in deconstructing complex events into simpler components and constructing pathways for event resolution. By leveraging the power of LLM agents, it is possible to automate these processes and allow the models to infer the optimal pathways for event resolution.\nLLM agents have captured the attention of researchers across various domains, resulting in a rich body of academic literature. To understand the diverse work-flows and complex event processing facilitated by LLM agents, we will divide the exploration and study of these agents into two broad categories: Single-Agent and Multi-Agent systems"}, {"title": "3.2. Single Agent LLM Systems", "content": "OpenAI's GPT model marked a significant breakthrough in the capabilities of large language models (LLMs). By providing access to their API, OpenAI enabled the development of customized plugins to create domain-specific ChatGPT agents [11]. This innovation facilitated the emergence of specialized Single-Agent Systems (SASs). We define Single-Agent Systems as those that operate independently and lack the capability for multi-agent collaboration. Due to their singular nature, SASs are inherently limited in the complexity of workflows they can manage. The constraints of operating as a single entity restrict these systems from handling more intricate tasks that require coordination and collaboration among multiple agents. However, they are adept at handling domain specific tasks such as planning, assisting, writing etc. Here, we present existing research that present, highlight and evaluate single agent systems."}, {"title": "3.3. Multi-Agent LLM Systems", "content": "As the increasing capabilities of LLMs broadened research horizons, the limitations of single LLM-based agents soon became apparent. LLM agents, by themselves, are limited in what actions they can take since they can only output text. The concept of multi-agents was introduced to harness the collective capabilities of multiple LLM-based agents working in tandem to overcome the constraints of single-agent systems. By distributing tasks among various specialized agents, multi-agent systems can perform more complex and dynamic operations that single agents cannot achieve alone. Here we present some existing applications that utilize multi-agent strategy and highlight their capabilities. Due to the expansive range of work-flows that can now be automated due to LLM-MASs, we divide these systems into two different subsections to provide clarity."}, {"title": "3.3.1. Planning Workflows", "content": "Further exploring the roles and capabilities of MASs, Paper [26] introduces a novel paradigm that integrates LLMs (such as GPT-4) into these MASs to enhance agent communication and autonomy. Their approach is an extension of the MAPE-K (Monitoring, Analyzing, Planning, Executing, and Knowledge) protocol introduced by IBM in 2004 [26]. Nascimento et al. grounded their approach in the \u039c\u0391\u03a1\u0395-\u039a model due to its robustness in system adaptability within dynamic environments. This methodology is validated through a practical application in a marketplace scenario, showcasing improved communicative capabilities and adaptability of the agents. This work represents a significant advancement in MAS self-adaptation, indicating further research opportunities to assess LLMs' applicability in more complex scenarios, potentially leading to enhanced problem-solving capabilities and refined communication within MASS.\nRasal presents a novel multi-agent communication framework named LLM Harmony [27], which is inspired by the CAMEL model [28], to enhance the problem-solving capabilities of Large Language Models (LLMs). The framework employs multiple LLM agents, each with a distinct role, engaged in role-playing communication. For their experiments, paper [27] focused on two metrics: arithmetic reasoning where math problems would be solved and common-sense reasoning. Each agent was given a distinct role to perform and their collaborative performance was measured. Their experiments validate the MAS framework's superior performance and adaptability, showcasing the potential of multi-agent systems in overcoming the limitations of individual LLMs.\nAgentCoord [29] is a novel framework that has been designed to aid in the creation of coordination strategies for LLM-MASs. Liu et al. [29] proposed a three-stage generation method to transform user-provided goals into executable strategies. AgentCoord facilitates user comprehension by visually organizing the generated strategy and providing interactive features to explore alternative strategies. In a similar vein, AgentLite [30] is another research based task-oriented planning framework that allows for the creation of LLM-MASs for researchers to study reasoning strategies, prototype novel LLM-MASs and reproduce agent benchmarks. The framework is validated through experiments conducted on complex reasoning tasks such as Retrieval-Augmented Q/A across a large volume of corpus and e-commerce tasks."}, {"title": "3.3.2. Complex Workflows", "content": "MASs have opened a pathway towards tackling more sophisticated scenarios in technical environments. Autogen, as explained in Section 2.4 is one example of an LLM orchestration framework that leverages the power of LLMs to create au-"}, {"title": "3.4. LLM-based Autonomous Agent: Future Trends and Gap Analysis", "content": "tonomous agents [21] that can work together to solve complex scenarios and automate work-flows. Due to its capabilities, it can work as a research assistant, a programmer and code executor, a critic, reviewer and much more. Autogen is a representative of the current state-of-the-art capabilities of MAS frameworks that utilize the power of LLMs and are able to tackle work-flows in various domains. Besides auto-gen, there are a host of other MASs frameworks that enable complex workflows and event processing.\nMetaGPT [31] presents an innovative approach for leveraging large language models (LLMs) in multi-agent systems. This framework emphasizes the integration of human-like Standard Operating Procedures (SOPs) and a unique executable feedback mechanism to enhance the problem-solving capabilities and code generation quality of autonomous agents. It represents a specialized LLM application that focuses on automated software development. MetaGPT allows for the abstraction of software development roles for each agent (manager, developer, tester etc...) and automate the coding process in a collaborative manner. The experimental results highlighted MetaGPT's efficiency and robustness in real-world programming scenarios, confirming its task completion rate and reduced human revision costs. The primary limitations of the MetaGPT framework include its dependence on predefined SOPs, which might not be adaptable to all possible real-world scenarios. Unlike Autogen, MetaGPT is a specialized MAS LLM application and so is highly domain specific.\nSimilarly to MetaGPT, there are other MASs focused on the domain of software development. ChatDev [32] is one such novel project that utilizes multi-agent communication to contribute to the design, coding, debugging and testing phases in a software development life cycle. Similarly, Feldt et al. [33] presents a taxonomy of LLM-based testing agents that leverage conversational frameworks to automate software testing. This framework, named SOCRATEST, aims to provide varying degrees of autonomy to testing agents, enabling them to assist developers in the software testing process. The proposed system allows developers to specify high-level testing requirements, while the LLM-driven agents manage the detailed execution of tests, thus bridging the gap between testing expertise and domain knowledge. Extensive experiments were conducted to validate the effectiveness of the framework with various software testing tasks, such as unit testing. The experiments demonstrated that LLM-driven agents could generate detailed and actionable testing checklists, suggest relevant testing methodologies, and produce executable test code.\nOther software related MASs have also attributed to ongoing research in this field. One common limitation among LLM-MASs was their inability to execute external tools (API and function calls) which limited complex use-cases from being automated via the use of these systems. Autogen [21], Smurfs [34] and ToolLLM [35] are"}, {"title": "4. System Design and Implementation", "content": "examples of LLM-MASs that provide a framework that allows for the integration of external tool calls. Smurf [34], like Autogen, is a complex LLM-MAS framework that utilizes agents with specialized roles to break down complex prompts from the end-user, create an action plan, execute the created plan by utilizing external tools and finally verify and answer the prompt based on the preceding agents outputs. Unlike Smurf and auto-gen, ToolLLM [35] is a framework designed to specifically enhance tool-use capabilities of LLM-MASs by enabling them to interact with a wide range of of APIs. The framework utilizes a fine-tuned version of the LLaMA model [12] to achieve its functionality. ToolLLM addresses this gap by be being trained on a comprehensive dataset called ToolBench [35], which includes 16,464 real-world APIs across 49 categories. The framework involves a three-stage construction process: API collection, instruction generation, and solution path annotation. The LLM-MAS is paired with a neural API retriever that recommends API calls based on the context provided. The framework was able to demonstrate remarkable performance in handling complex instructions and generalizing unseen APIs and exhibited performances comparable to state-of-the-art models at the time of writing.\nCombining the principles and capabilities introduced by the aforementioned LLM-MASs frameworks that allow for code generation, execution, external tool calls combined with LLM multi-modal capabilities, exciting new pathways have been researched in the field of robotics. Papers [36, 37, 38] highlight the usage of LLM-MASs and how the field of robotics can harness the advanced reasoning and language comprehension capabilities of state-of-the-art LLMs to create precise and functional action plans based on natural language instructions and external observations. Papers [36, 37] explore and demonstrate the capabilities of LLM-MASs for precise task planning and embodied tasks by utilizing vision models and combating LLM hallucinations. Similarly, Kannan et al. [38] proposes a framework titled Smart-LLM that aims to convert high-level task inputs into a multi-robot task plan using a process that includes task decomposition, coalition formation, and task allocation. The framework is validated through extensive experiments in both simulation and real-world settings, demonstrating its effectiveness in generating coherent and logical multi-robot task plans.\nMASs have also been utilized in the field of cyber-security and networks. To demonstrate the capabilities of an MAS in this domain, Fasha et al. [39] proposes a MAS framework to tackle security risks defined in the Open Web Application Security Project (OWASP) Top 10. They focused on mitigating the OWASP Top 10 security risks for LLM applications using intelligent agents. Their proposed model leverages the AutoGen framework and Retrieval Augmented Generation (RAG) technologies to enhance the security of LLM deployments. The model employs autonomous agents to"}, {"title": "4.1. Technical Requirements", "content": "Future research in LLM-based autonomous agents is likely to focus on improving the robustness and adaptability of both single and multi-agent systems. Enhancing the ability of agents to adapt to new tasks without extensive modification, developing more sophisticated communication protocols among agents and further integrating multi-modal LLMs to expand the capabilities and applications of these autonomous agents.\nOne such direction that is already being actively researched is related to augmenting the role of message brokers with LLM agents. Saleh et al. [10] introduces a novel framework for message brokers that are specially designed to cater to the needs of Generative AI (GenAI) applications by utilizing LLM agents. This approach is distinguished by its focus on scalability, efficiency, and the capacity for real-time data management, directly addressing the unique challenges posed by GenAI. The"}, {"title": "4.2. Design Principles", "content": "enforce security policies, validate inputs and outputs, and manage configurations. By integrating multiple intelligent agents working in a collaborative manner, the model can be seen to provide real-time security assessments and proactive countermeasures, enhancing data integrity, confidentiality, and service availability in LLM applications.\nDandoush et al. [40] presents a comprehensive study on integrating Large Language Models (LLMs) with multi-agent systems (MAS) to enhance network slicing management and orchestration. The proposed framework leverages LLMs to translate user intent into precise technical requirements, map network functions to the infrastructure, and manage the entire slice life-cycle. The proposed framework facilitates collaboration across different administrative domains, overcoming challenges such as interoperability, security, resource coordination, and dynamic service orchestration.\nThe multi-modal capabilities of LLM have also proven to be a disruptive technology in the world of art and sciences. Models like GPT-4 [11] and Google Gemini, have the ability to generate images based on textual input. Hence, LLM MASs have also been utilized in the art domain to enhance the designing process. Ding et al. [41] introduced DesignGPT, a multi-agent collaboration framework designed to enhance the product design process by integrating Large Language Models (LLMs) and generative AI tools. The system aims to simulate the roles within a design company, allowing human designers to collaborate with AI agents through natural language interactions. The system leverages the capabilities of GPT-4 for text generation and Stable Diffusion for image generation, providing design inspiration and aiding in the conceptual stage of product design. By integrating these tools, DesignGPT aims to bridge the gap between design thinking and machine thinking, enabling more effective and innovative design solutions\nFuture research in LLM-based autonomous agents is likely to focus on improving the robustness and adaptability of both single and multi-agent systems. Enhancing the ability of agents to adapt to new tasks without extensive modification, developing more sophisticated communication protocols among agents and further integrating multi-modal LLMs to expand the capabilities and applications of these autonomous agents."}, {"title": "4.3. Overview of the System Architecture", "content": "One such direction that is already being actively researched is related to augmenting the role of message brokers with LLM agents. Saleh et al. [10] introduces a novel framework for message brokers that are specially designed to cater to the needs of Generative AI (GenAI) applications by utilizing LLM agents. This approach is distinguished by its focus on scalability, efficiency, and the capacity for real-time data management, directly addressing the unique challenges posed by GenAI. The"}, {"title": "4.4. Complete Data Flow", "content": "paper proposes utilizing an LLM agents and their reasoning ability to dynamically adjust to varying data loads and structures, ensuring that data can be processed and transferred without significant delays or bottlenecks. This adaptability is crucial for maintaining the performance and efficiency of GenAI applications, especially in scenarios where real-time processing and decision-making are essential. \nAnother possible future direction that can utilize LLM agents is the proposal of a new neural pub/sub paradigm in paper [42]. The Neural Publish/Subscribe Paradigm aims integrates artificial intelligence directly into the communication layer, enabling a novel approach to managing AI workflows in distributed systems. It aims to leverage neural networks to facilitate efficient, many-to-many distribution of information, supporting dynamic learning and inference across the network. This can be supported by utilizing the power of LLM agents and presents an opportunity to introduce a more reactive and event-driven process, allowing various stages of the ML pipeline to act as subscribers.\nAnother possible future direction that can utilize LLM agents is the proposal of a new neural pub/sub paradigm (shown in Figure 3) in paper [42]. The Neural Publish/Subscribe Paradigm aims integrates artificial intelligence directly into the communication layer, enabling a novel approach to managing AI workflows in distributed systems. It aims to leverage neural networks to facilitate efficient, many-to-"}, {"title": "5. Experiments", "content": "LLM-empowered infrastructure promises to not only support dynamic analytics and decision-making but also enhance the overall efficiency and effectiveness of operations. The ability to provide seamless data flow and accessibility can empower users to respond swiftly to dynamic conditions. Considering such an objective, this section presents technical requirements, design principles, and the system design of the automated CEP pipeline employing LLM which can be utilized in modern infrastructures."}, {"title": "5.1. Evaluation Criteria and Methodology", "content": "The primary objective of this system is to create a responsive and agile infrastructure that supports near real-time analytics and decision-making. This goal encompasses several key aspects:\n\u2022 Timely Data Availability: Ensuring that data is accessible when needed is crucial for real-time analytics and decision-making processes. The system"}, {"title": "5.1.1. Scoring Methodology", "content": "The proof-of-concept presented in this thesis follows key principles aimed at ensuring the creation of a available and reliable system. These principles are as follows:"}, {"title": "5.2. Experiment 1: Measuring performance with increasing number of Autogen Agents", "content": "4 illustrates the IoT data fabric architecture designed to create an autonomous complex event processing (CEP) pipeline. Central to this architecture is the Autogen Large Language Model (LLM) framework, which acts as the orchestration agent, coordinating data flows and processing tasks across various components. The system ensures seamless data flow, and accessibility, enabling efficient and intelligent decision-making. The diagram consists of three main components; the Central Coordination Unit, The End Users and Cloud and Edge Instances."}, {"title": "5.2.1. Speaker Selection Optimization", "content": "Central Coordination Unit: At the core of the system is the Autogen LLM framework. This central coordination unit orchestrates the interactions between various components, managing agent routing, tool call allocation, and system integrity. Users interact with the system by querying the Autogen LLM agent, which autonomously orchestrates the necessary processes to fulfil the query. The framework leverages advanced GPT capabilities to understand user queries, determine the optimal data sources, and coordinate the processing tasks required to generate the desired outputs.\nA key feature of the AutoGen LLM framework is its capacity to integrate and leverage external tools and functions. AutoGen agents possess the capability to utilize external functions and tools to execute predefined workflows. This functionality is crucial for managing complex workflows that often require interaction with existing files, code bases, and functions rather than developing new code from scratch. Generating dynamic code can introduce breaking changes, increase latency, and pose additional security risks. By providing external tools to the AutoGen agent and customizing the agents to operate strictly within the boundaries of the provided code files, these drawbacks can be effectively circumvented."}, {"title": "5.3. Experiment 2: Evaluating performance with increasing video complexities", "content": "The process of integrating external tools involves creating a bi-directional communication channel between the AutoGen agents and the external tools and/or code files. This interaction allows the agents to execute tasks using pre-existing", "Users": "End users represent different domains that require data access and processing capabilities. They could be individual users, enterprises, or other entities that interact with the Autogen LLM framework to access the data needed for their specific domains. This interaction is facilitated through dedicated communication channels that ensure reliable and timely data delivery tailored to the end users' requirements. The users query the Autogen agents, which then orchestrate the data retrieval and processing operations transparently. One of the key design principles behind this POC is to minimize the complexity for end users in terms of system setup and understanding the internal mechanics of the AutoGen framework.\nThe objective is to enable end users to seamlessly access relevant data based on their queries to the LLM agent without the need for extensive technical knowledge or configuration efforts. To achieve this, the POC is designed to abstract away the complexities involved in setting up connections and configuring the system. End users are not required to understand the intricate details of how the agents operate or how the tools and functions are integrated within the framework. This abstraction ensures that users can focus on their queries and tasks without being burdened by the underlying technicalities. The interaction with the system is streamlined such that the only requirement for the end users is to establish a communication channel with the LLM agent. Once this channel is established, users can issue queries and receive the necessary data through the LLM agent. The agent, in turn, handles all the back-end processes, including interaction with"}]}