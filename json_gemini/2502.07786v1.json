{"title": "Counterexample Guided Program Repair Using Zero-Shot Learning and MaxSAT-based Fault Localization", "authors": ["Pedro Orvalho", "Mikol\u00e1\u0161 Janota", "Vasco Manquinho"], "abstract": "Automated Program Repair (APR) for introductory program-ming assignments (IPAS) is motivated by the large number of student enrollments in programming courses each year. Since providing feedback on programming assignments requires substantial time and effort from faculty, personalized automated feedback often involves suggesting repairs to students' programs. Symbolic semantic repair approaches, which rely on Formal Methods (FM), check a program's execution against a test suite or reference solution, are effective but limited. These tools excel at identifying buggy parts but can only fix programs if the correct implementation and the faulty one share the same control flow graph. Conversely, Large Language Models (LLMS) are used for program repair but often make extensive rewrites instead of minimal adjustments. This tends to lead to more invasive fixes, making it harder for students to learn from their mistakes. In summary, LLMS excel at completing strings, while FM-based fault localization excel at identifying buggy parts of a program.\nIn this paper, we propose a novel approach that combines the strengths of both FM-based fault localization and LLMS, via zero-shot learning, to enhance APR for IPAS. Our method uses MaxSAT-based fault localization to identify buggy parts of a program, then presents the LLM with a program sketch devoid of these buggy statements. This hybrid approach follows a Counterexample Guided Inductive Synthesis (CEGIS) loop to iteratively refine the program. We ask the LLM to synthesize the missing parts, which are then checked against a test suite. If the suggested program is incorrect, a counterexample from the test suite is fed back to the LLM for revised synthesis. Our experiments on 1,431 incorrect student programs show that our counterexample guided approach, using MaxSAT-based bug-free program sketches, significantly improves the repair capabilities of all six evaluated LLMS. This method allows LLMS to repair more programs and produce smaller fixes, outperforming other configurations and state-of-the-art symbolic program repair tools.", "sections": [{"title": "Introduction", "content": "Every year, thousands of students enroll in programming-oriented courses. With the rapid growth of Computer Science courses, providing personalized and timely feedback on introductory programming assignments (IPAS) and software projects has become a significant challenge, requiring substantial time and effort from faculty (Orvalho, Janota, and Manquinho 2024c, 2022b).\nAutomated Program Repair (APR) has emerged as a promising solution to this challenge, aiming to deliver automated, comprehensive, and personalized feedback to students about their programming errors (Gulwani, Radicek, and Zuleger 2018; Ahmed et al. 2022; Wang, Singh, and Su 2018; Hu et al. 2019). Traditional semantic APR techniques based on Formal Methods (FM), while providing high-quality fixes, are often slow and may struggle when the correct implementation diverges significantly from the erroneous one (Contractor and Rivero 2022). These APR approaches do not guarantee minimal repairs, as they align an incorrect submission with a correct implementation for the same IPA. If the alignment is not possible, these tools return a structural mismatch error, leaving the program unrepaired. In the past decade, there has been a surge in Machine Learn-ing (ML) techniques for APR (Gupta et al. 2017; Mesbah et al. 2019; Gupta, Kanade, and Shevade 2019; Yasunaga and Liang 2020; Rolim et al. 2017; Pu et al. 2016; Bhatia, Kohli, and Singh 2018; Orvalho et al. 2023). ML-based approaches require multiple correct implementations to generate high-quality repairs, and need considerable time and resources to train on correct programs. While these approaches generate repairs more quickly, they often produce imprecise and non-minimal fixes (Wang, Singh, and Su 2018).\nMore recently, Large Language Models (LLMs) trained on code (LLMCS) have shown great potential in generating program fixes (Joshi et al. 2023; Xia, Ding, and Zhang 2023; Jin et al. 2023; Wei, Xia, and Zhang 2023; Fan et al. 2023; Xia, Wei, and Zhang 2023; Zhang et al. 2024; Phung et al. 2023). LLM-based APR can be performed using zero-shot learning (Xia and Zhang 2022), few-shot learning (Zhang et al. 2024) or fine-tuned models (Jin et al. 2023). Fine-tuned models are the most commonly used, where the model is trained for a specific task. Conversely, zero-shot learning refers to the ability of a model to correctly perform a task without having seen any examples of that task during training. Few-shot learning refers to the LLMS's ability to perform tasks correctly with only a small number of examples provided. Furthermore, the ability to generalize using zero or few-shot learning enables LLMS to handle a wide range of tasks without the need for costly retraining or fine-tuning. Nonetheless, few-shot learning can lead to larger fixes than necessary, as it is based on a limited number of examples. LLMS do not guarantee minimal repairs and typically rewrite most of the student's implementation to fix it, rather than making minimal adjustments, making their fixes less efficient and harder for students to learn from.\nIn this paper, we propose a novel approach that combines the strengths of both FM and LLMS to enhance APR of IPAS via zero-shot learning. Our method involves using MaxSAT-based fault localization to identify the set of minimal buggy parts of a program and then presenting an off-the-self LLM with a program sketch devoid of these buggy statements. This hybrid approach follows a Counterexample Guided Inductive Synthesis (CEGIS) loop (Solar-Lezama et al. 2005) to iteratively refine the program. We provide the LLM with a bug-free program sketch and ask it to synthesize the missing parts. After each iteration, the synthesized program is checked against a test suite. If the program is incorrect, a counterexample from the test suite is fed back to the LLM, prompting a revised synthesis.\nOur experiments with 1431 incorrect student programs reveal that our counterexample guided approach, utilizing MaxSAT-based bug-free program sketches, significantly boosts the repair capabilities of all six evaluated LLMS. This method enables LLMS to repair more programs and produce superior fixes with smaller patches, outperforming both other configurations and state-of-the-art symbolic program repair tools (Gulwani, Radicek, and Zuleger 2018; Ahmed et al. 2022).\nIn summary, this paper makes the following contributions:\n\u2022 We tackle the Automated Program Repair (APR) problem using an LLM-Driven Counterexample Guided Inductive Synthesis (CEGIS) approach;\n\u2022 We employ MaxSAT-based Fault Localization to guide and minimize LLMS' patches to incorrect programs by feeding them bug-free program sketches;\n\u2022 Experiments show that our approach enables all six evaluated LLMS to fix more programs and produce smaller patches than other configurations and symbolic tools;\n\u2022 Our code is available on GitHub \u00b9 and on Zenodo \u00b2."}, {"title": "Motivation", "content": "Consider the program presented in Listing 1, which aims to determine the maximum among three given numbers. However, based on the test suite shown in Table 1, the program is buggy as its output differs from the expected results. The set of minimal faulty lines in this program includes lines 4 and 8, as these two if conditions are incorrect according to the test suite. A good way to provide personalized feedback to students on their IPAS is to highlight these two buggy lines. However, it is essential to check these faults by fixing the program and evaluating it against the test suite.\nUsing traditional Automated Program Repair (APR) tools for IPAS based on Formal Methods, such as CLARA (Gul-"}, {"title": "Preliminaries", "content": "This section provides definitions used throughout the paper.\nSynthesis Problem. For a given program's specification S (e.g., input-output examples), G a context-free grammar (CFG), and O be the semantics for a particular Domain-specific language (DSL), the goal of program synthesis is to infer a program P such that (1) the program is produced by G, (2) the program is consistent with O and (3) P is consistent with S (Orvalho et al. 2019; Ramos et al. 2020).\nSemantic Program Repair. Given (T, G, O, P), let T be a set of input-output examples (test suite), G be a grammar, O be the semantics for a particular Domain-specific lan-guage, and P be a syntactically well-formed program (i.e. sets of statements, instructions, expressions) consistent with G and O but semantically erroneous for at least one of the input-output tests i.e., {tin, tout}\u2208T : P(tin) \u2260 tout\nThe goal of Semantic Program Repair is to find a program Pf by semantically change a subset S1 of P's statements (S1 \u2286 P) for another set of statements S2 consistent with G and O, such that, Pf = ((P \\ S\u2081) \u222a S2) and \u2200{tin, tout}\u2208T : Pf(tin) = tout\nCounterexample Guided Inductive Synthesis (CEGIS). CEGIS is an iterative algorithm commonly used in Program Synthesis and Formal Methods to construct programs or solutions that satisfy a given specification (Abate et al. 2018; Jha et al. 2010; Solar-Lezama et al. 2005). CEGIS"}, {"title": "Counterexample Guided Automated Repair", "content": "Our approach combines the strengths of both Formal Methods (FM) and LLMS to enhance Automated Program Repair (APR). Firstly, we employ MaxSAT-based fault localization techniques to rigorously identify the minimal set of buggy parts of a program (Ignatiev et al. 2019; Orvalho, Janota, and Manquinho 2024b). Afterwards, we leverage LLMS to quickly synthesize the missing parts in the program sketch. Finally, we use a counterexample from the test suite to guide LLMS in generating patches that make the synthesized program compliant with the entire test suite, thus completing the repair. The rationale of our approach follows a Counterexample Guided Inductive Synthesis (CEGIS) (Solar-Lezama et al. 2006) loop to iteratively refine the program. Figure 1 provides an overview of our APR approach. The input is a buggy program and the specifications for an introductory programming assignment (IPA), including a test suite, a description of the IPA, and the lecturer's reference implementation. We start by using MaxSAT-based fault localization techniques to identify the program's minimal set of faulty statements. Next, the prompt generator builds a prompt based on the specifications of the IPA and a bug-free program sketch reflecting the localized faults, then feeds this information to the LLM. The LLM generates a program based on the provided prompt. After each iteration, the Decider module evaluates the synthesised program against a test suite. If the program is incorrect, a counterexample chosen from the test suite is sent back to the prompt generator, which then provides this counterexample to the LLM to prompt a revised synthesis.\nPrompts. The prompts fed to LLMS can contain various types of information related to the IPA. The typical information available in every programming course includes the description of the IPA, the test suite to check the students' submissions corresponding to the IPA's specifications, and the lecturer's reference implementation. The syntax used in our prompts is similar to that in other works on LLM-driven program repair (Joshi et al. 2023). We have evaluated several types of prompts. Basic prompts are the simplest prompts that can be fed to an LLM without additional computation, including all the programming assignment's basic information. An example of such a prompt is shown below:\nFix all semantic bugs in the buggy program\nbelow. Modify the code as little as possible.\nDo not provide any explanation.\n### Problem Description ###\nWrite a program that determines and\nprints the largest of three integers\ngiven by the user.\n### Test Suite\n#input:\n6 2 1\n#output:\n6\n// The other input-output tests\n# Reference Implementation (Do not copy\nthis program) ``` <c> ###\n\u0e27...\nint main() {\n// Reference Implementation\n}```\n### Buggy Program <c> ###\n\u0e27...\nint main() {\n// Buggy program from Listing 1\n}...\n### Fixed Program <c> ###\n\u0e27...\nIn order to incorporate information about the faults localized in the program using MaxSAT-based fault localization, we utilized two different types of prompts: (1) FIXME annotations and (2) program sketches. FIXME annotated prompts are prompts where each buggy line identified by the fault localization tool is marked with a /* FIXME */ comment. These prompts are quite similar to the basic prompt described previously, with the primary differences being the annotations in the buggy program and the first command given to the LLMS, which is modified as follows:\nFix all buggy lines with '/* FIXME */'\ncomments in the buggy program below.\nIn the second type of prompt, to address program repair as a string completion problem, we evaluated the use of prompts where the buggy program is replaced by an incomplete program (program sketch), with each line identified as buggy by our fault localization module replaced by a hole. The command given to the LLMS is now to complete the incomplete program. Consequently, the sections \u2018Buggy Program' and 'Fixed Program' are replaced by 'Incomplete Program' and 'Complete Program', respectively, as follows:\nComplete all the '@ HOLES N @' in the\nincomplete program below.\n//\n### Incomplete Program <c> ###\n//\n### Complete Program <c> ###\nFeedback. If the candidate program generated by the LLM is not compliant with the test suite, this feedback is provided to the LLM in a new message through iterative querying. This new prompt indicates that the LLM's previous suggestion to fix the buggy program was incorrect and provides a counterexample (i.e., an IO test) where the suggested fixed program produces an incorrect output. Hence, we provide the LLM with a feedback prompt similar to:\n### Feedback ###\nYour previous suggestion was incorrect!\nTry again. Code only. Provide no explanation.\n### Counterexample ###\n#input:\n6 2 1\n#output:\n6\n### Fixed Program <c> ###\n\u0e27..."}, {"title": "Experimental Results", "content": "The goal of our evaluation is to answer the following research questions: RQ1. How effective are state-of-the-art (SOTA) LLMs in repairing introductory programming assignments (IPAS) compared to different SOTA semantic repair approaches? RQ2. How do different prompt configurations impact the performance of LLMS? RQ3. How does FM-based fault localization impact LLM-driven APR? RQ4. How helpful is it to provide a reference implementation for the same IPA to the LLMS? RQ5. What is the performance impact of using a Counterexample Guided approach in LLM-driven APR?"}, {"title": "Experimental Setup", "content": "All LLMS were run using NVIDIA RTX A4000 graphics cards with 16GB of memory on an In-tel(R) Xeon(R) Silver 4130 CPU @ 2.10GHz with 48 CPUs and 128GB RAM. All the experiments related to the program repair tasks were conducted on an Intel(R) Xeon(R) Silver computer with 4210R CPUs @ 2.40GHz, using a memory limit of 10GB and a timeout of 90 seconds.\nEvaluation Benchmark. To evaluate our work, we used C-PACK-IPAS (Orvalho, Janota, and Manquinho 2024a), which is a set of student programs developed during an introductory programming course in the C programming language. Since this work focuses only on semantic program repair, only programs that compile without any errors were selected. C-PACK-IPAS contains 1431 semantically incorrect programs, i.e., fail at least one input-output test.\nLarge Language Models (LLMS). In our experiments, we used only open-access LLMS available on Hugging Face (HuggingFace 2024) with approximately 7 billion parameters for three primary reasons. Firstly, closed-access models like Chat-GPT are cost-prohibitive and raise concerns over student data privacy. Secondly, models with a very large number of parameters (e.g., 70B) need significant computational resources, such as GPUs with higher RAM capacities, and take longer to generate responses, which is unsuitable for a classroom setting. Thirdly, we used these off-the-shelf LLMS to evaluate the publicly available versions without fine-tuning them. This approach ensures that the LLMS used in this paper are available to anyone without investing time and resources into fine-tuning these models. Thus, we evaluated six different LLMS for this study through iterative querying. Three of these models are LLMCs, i.e., LLMs fine-tuned for coding tasks: IBM'S GRANITE (Mishra et al. 2024), Google's CODEGEMMA (Zhao et al. 2024) and Meta's CODELLAMA (Rozi\u00e8re et al. 2023). The other three models are general-purpose LLMS not specifically tailored for coding tasks: Google's GEMMA (Mesnard et al. 2024), Meta's LLAMA3 (latest version of the LLAMA family (Touvron et al. 2023)) and Microsoft's PHI3 (Abdin et al. 2024).\nWe selected specific variants of each model to optimize their performance for our program repair tasks. For Meta's LLAMA3, we utilized the 8B-parameter instruction-tuned variant. This model is designed to follow instructions more accurately, making it suitable for a range of tasks, including program repair. For CODELLAMA, we used the 7B-parameter instruct-tuned version, which is specifically designed for general code synthesis and understanding, making it highly effective for coding tasks. We employed GRANITE model with 8B-parameters, fine-tuned to respond to coding-related instructions. For PHI3, we opted for the mini version, which has 3.8B-parameters and a context length of 128K. This smaller model is efficient yet capable of handling extensive context, making it practical for educational settings. For GEMMA, we used the 7B-parameter instruction-tuned version, optimized to follow detailed instructions. Lastly, for CODEGEMMA, we selected the 7B-parameter instruction-tuned variant, designed specifically for code chat and instruction, enhancing its capability to handle programming-related queries and tasks. To fit all LLMS into 16GB GPUs, we used model quantization of 4bit. Moreover, all LLMS were run using Hugging Face's Pipeline architecture. By using these different LLMS, we aimed to balance computational efficiency with the ability to effectively generate and refine code, facilitating a practical APR approach in an educational environment."}, {"title": "Fault Localization (FL)", "content": "We used CFAULTS (Orvalho, Janota, and Manquinho 2024b) which is a formula-based FL tool, that pinpoints bug locations within the programs. It aggregates all failing test cases into a unified MaxSAT formula. This FL tool can be easily replaced by other FL tools."}, {"title": "Evaluation", "content": "To assess the effectiveness of the program fixes generated by the LLMS under different prompt configurations, we used two key metrics: the number of programs successfully repaired and the quality of the repairs. For assessing the patch quality, we use the Tree Edit Distance (TED) (Tai 1979; Zhang and Shasha 1989) to compute the distance between the student's buggy program and the fixed program returned by the LLMS. TED computes the structural differences between two Abstract Syntax Trees (ASTS) by calculating the minimum number of edit operations (i.e., insertions, deletions, and substitutions) needed to transform one AST into another. Based on this metric for measuring program distances, we computed the distance score, defined by Equation 1. This score aims to identify and penalize LLMS that replace the buggy program with the reference implementation rather than fixing it. The distance score is zero when the TED of the original buggy program (To) to the program suggested by the LLM (Tf) is the same as the TED of the reference implementation (Tr) to To. Otherwise, it penalizes larger fixes than necessary to align the program with the correct implementation.\nds(Tf, To, Tr) = max(0, 1 \u2212TED(Tf, To)/TED(Tr, To))                                                                                                                                             (1)\nBaseline. We used two state-of-the-art traditional semantic program repair tools for IPAS as baselines: VERIFIX (Ahmed et al. 2022) and CLARA (Gulwani, Radicek, and Zuleger 2018). VERIFIX employs MaxSMT to align a buggy program with a reference implementation provided by the lecturer, while CLARA clusters multiple correct implementations and selects the one that produces the smallest fix when aligned with the buggy program. Both tools require an exact match between the control flow graphs (e.g., branches, loops) and a bijective relationship between the variables; otherwise, they return a structural mismatch error. VERIFIX was provided with each buggy program, the reference implementation, and a test suite. CLARA was given all correct programs from different academic years to generate clusters for each IPA. Within a 90-second time limit, CLARA repairs 495 programs (34.6%), times out without producing a repair on 154 programs (10.8%), and fails to repair 738 programs (54.7%). In comparison, VERIFIX repairs 91 programs (6.3%), reaches the time limit on 0.6%, and fails"}, {"title": "Discussion", "content": "To answer our research questions: For RQ1, all six LLMS using different prompt configurations repair more programs than traditional repair tools. For RQ2, prompt configurations with FL-based Sketches, IPA description and test suite yield the most successful repair outcomes. Moreover, for RQ3, it is clear that incorporating FL-based Sketches (or even FIXME annotations) allows the LLMS to repair more programs than only providing the buggy program. For RQ4, including a reference implementation allows for more repaired programs but with potentially less efficient fixes. Finally, for RQ5, employing a Counterexample guided approach significantly improves the accuracy of LLM-driven APR across various configurations. Counterexamples help in the repair process of certain LLMS, such as CODEGEMMA and LLAMA3, across all prompt configurations. For other LLMS, counterexamples are beneficial but only in specific configurations. This difference may be due to variations in the training data used for each LLM. However, a more detailed analysis is necessary.\nFurthermore, we analyzed the effectiveness of LLMS in repairing programs that CLARA fails to address due to control-flow issues, representing 54.7% of C-PACK-IPAS (738 programs). Table 4 presents these results. Among the best-performing configurations, GRANITE with Sk_De-TS achieved the highest repair rate, successfully fixing 290 programs (37.0%) in this subset. This highlights GRANITE's strong capability to handle complex program structures where traditional constraint-based tools fail. CODEGEMMA with Sk_De-TS-CE also performed well, repairing 270 programs (34.5%), demonstrating the advantage of incorporating counterexamples (CE) alongside the Sketches (Sk) configuration. In contrast, models such as LLAMA3 and PHI3 achieved lower success rates, each repairing only 199 programs (25.4%), suggesting limitations in their ability to generalize and address intricate control-flow issues.\nTo gain deeper insights into LLMS' performance across varying levels of program complexity, we evaluated the average cyclomatic complexity of each program in C-PACK-IPAS using lizard (Lizard 2024). Table 5 summarizes"}, {"title": "Conclusion", "content": "Large Language Models (LLMS) excel at completing strings, while MaxSAT-based fault localization (FL) excels at identifying buggy parts of a program. We proposed a novel approach combining MaxSAT-based FL and LLMS via zero-shot learning to enhance Automated Program Repair (APR) for introductory programming assignments (IPAs). Experiments show that our bug-free program sketches significantly improve the repair capabilities of all six evaluated LLMS, enabling them to repair more programs and produce smaller patches compared to other configurations and state-of-the-art symbolic program repair tools. Therefore, this interaction between Formal Methods and LLMS yields more accurate and efficient program fixes, enhancing feedback mechanisms in programming education."}]}