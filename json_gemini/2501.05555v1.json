{"title": "Improving Zero-Shot Object-Level Change Detection by Incorporating Visual Correspondence", "authors": ["Hung H. Nguyen", "Pooyan Rahmanzadehgervi", "Long Mai", "Anh Totti Nguyen"], "abstract": "Detecting object-level changes between two images across possibly different views (Fig. 1) is a core task in many applications that involve visual inspection or camera surveillance. Existing change-detection approaches suffer from three major limitations: (1) lack of evaluation on image pairs that contain no changes, leading to unreported false positive rates; (2) lack of correspondences (i.e., localizing the regions before and after a change); and (3) poor zero-shot generalization across different domains. To address these issues, we introduce a novel method that leverages change correspondences (a) during training to improve change detection accuracy, and (b) at test time, to minimize false positives. That is, we harness the supervision labels of where an object is added or removed to supervise change detectors, improving their accuracy over previous work [25] by a large margin. Our work is also the first to predict correspondences between pairs of detected changes using estimated homography and the Hungarian algorithm. Our model demonstrates superior performance over existing methods, achieving state-of-the-art results in change detection and change correspondence accuracy across both in-distribution and zero-shot benchmarks.", "sections": [{"title": "1. Introduction", "content": "Identifying key changes between two images is a core task that powers many applications [25], e.g., to detect changes across brain scans [6, 22], a missing car in a parking lot (Fig. 1) [13, 31], or a defective product in a manufacturing pipeline [36]. However, existing work has three major limitations. First, most papers did not test on pairs of images where there are no changes [25, 26] and therefore do not measure false positives. Many image-difference captioning benchmarks contain only change cases [34] or only a small subset of no-change examples, e.g., 10% of Spot-the-Diff [13]. Second, prior models are trained to detect only changes; yet, such detected changes are not too usable in the downstream application when there are many changes predicted per image but no correspondence provided (Fig. 1). Third, many image-difference prediction works are specialized for a single domain (e.g., remote sensing [37]) and do not measure zero-shot generalization to unseen datasets [37,40]."}, {"title": "2. Problem formulation", "content": "We define a change to be an addition, absence, or modification of an object in one image compared to the other (see Fig. 2a-b). A major challenge is to detect such object-level changes in the presence of changes in camera viewpoint (COCO-I, KC), colors (COCO-I), or lighting (STD), which we do not aim to detect. The objects that change include humans, animals (COCO-I), man-made objects (COCO-I, STD, KC, OI), and letters (SC).\nIn the case where the same object moves from one location to another (Fig. 2c) across two images, we expect two changes to be detected: (1) An object is removed from the first location in image 1, and (2) an object is added to the second location in image 2. That is, two pairs of corresponding changes are to be predicted."}, {"title": "2.1. Definition of Changes", "content": "Following [25], we train and test both our model and CYWS [25] on COCO-Inpainted. Additionally, we test these models zero-shot on four unseen change-detection benchmarks: STD [13], Kubric-Change [25], and Synthtext-Change [25] and our proposed OpenImages-Inpainted.\nCOCO-Inpainted (COCO-I) [25] contains 57K, 3K, and 4.5K image pairs in the train, validation, and test sets, respectively. In each pair, one image is originally from COCO and the other is a clone with N objects removed (1 \u2264 N \u2264 24) from the image. The test set is divided into three groups based on the size of removed objects: small (38%), medium (39%), and large (23%) (see Fig. 2a). Images are subjected to random affine transformations and color jittering. Combined with cropping, these modifications yield image pairs, where all objects may not appear in both images.\nVIRAT-STD (STD) [13] A random 1,000 pairs of images (see Fig. 2b) is selected from the Spot-the-Difference dataset [13], a dataset of camera surveillance images of street views. Two images in each pair have almost identical views but are taken at different times. Objects being changed are typically humans and cars.\nKubric-Change (KC) [25] comprises 1,605 test cases (see Fig. 2c). The scenes comprise a randomly chosen assortment of 3D objects on a ground plane with a random texture. After applying the change to the scene, the camera's position in the 3D space slightly moves, yielding two different views of the scene.\nSynthtext-Change (SC) [25] consists of 5K pairs of real images with N changes, where 1 \u2264 N \u2264 6. Each change includes an arbitrary letter synthetically placed on one image at random locations.\nOpenImages-Inpainted (OI) To address the view-transformation and inpainting artifacts in and Img-Diff [14], we create ~1.3M pairs of images containing exactly 1 change. We adopt the original images from OpenImages dataset [18], and remove a single object using LaMa [33] inpainter, similar to COCO-I [25]. We filter the object sizes based on their relative bounding box area to the image size and keep the objects that fall within the range of 0.01 to 0.04. This ensures that objects are neither tiny nor overly large. We rotate a random image in the pair within the range of [-10, 10] degrees, and then apply random croppings to"}, {"title": "3. Methods", "content": "CYWS [25], a SOTA change detection model, is a U-Net coupled with CenterNet head [7] to detect changes in two images. That is, they predict 100 boxes per image, assuming the images always contain changes. Here, we summarize the current problems with this method that limit its real-world applications.\n1.  They assume that each image pair always contains \u2265 1 changes. However, there are many cases in the real world where no changes exist.\n2.  Given that they only detect boxes and the correspondence information is not predicted, it is not trivial to understand and relate its predictions with each other across two images when several changes are present (see Fig. 1).\nIn this work, we aim to solve these problems, i.e., we address both change and no-change scenarios and predict a correspondence between the changed objects.\nOur change detection pipeline consists of 3 stages: (1) change detection backbone, (2) alignment, and (3) correspondence prediction. Given a pair of images in Stage 1 (Fig. 3), a change detector e.g., CYWS [25], detects boxes over the changed objects. Then, in Stage 2, we aim to reduce the false positive predictions and remove the boxes that are poor candidates for correspondence prediction via an alignment stage. Finally, we use the Hungarian algorithm with a contrastive matching loss to predict the correspondences in Stage 3."}, {"title": "3.1. Stage 1: Change detection", "content": "The change detection backbone identifies change locations between left and right images using bounding boxes. After applying an optimal detection threshold to filter out predicted boxes with low confidence scores, the remaining bounding boxes are used as input for Stage Two and Stage Three (Fig. 3).\nU-Net encoder The ResNet-50 architecture is employed as the encoder backbone. The input image has a shape of 3 \u00d7 256 \u00d7 256. The output of the last layer (Layer 4.2) has a shape of 8 \u00d7 8 \u00d7 2048.\nCross-attention The cross-attention module facilitates information exchange between left and right images, enabling accurate computation of changes between the two. This process generates three feature maps with shapes 8 \u00d7 8 \u00d7 4096, 16 \u00d7 16 \u00d7 2048, and 32 \u00d7 32 \u00d7 1024, respectively.\nU-Net decoder The decoder utilizes the three feature maps produced by the cross-attention module as input and up-samples them to generate feature maps with a shape of 64 \u00d7 256 \u00d7 256. Skip connections from the encoder and scSE [24] blocks are incorporated into the upsampling process. The decoder output passes to the Bbox head.\nBbox head: The bbox head employs CenterNet [7] to predict bounding boxes for the detected change regions in the two images. CenterNet produces three output maps: center map (1 \u00d7 256 \u00d7 256), offset map (1 \u00d7 256 \u00d7 256), and and height-width map (2 \u00d7 256 \u00d7 256)."}, {"title": "3.2. Stage 2: Alignment", "content": "Thresholding the box predictions with the confidence score significantly reduces the number of false positives (Tab. 6). However, we take an additional step to eliminate boxes that are poor candidates for alignment. The alignment stage is based on the premise that if a predicted change box appears in the left image, there should be a corresponding box in the right image. This helps us refine candidate boxes for the subsequent matching stage in our proposed solution.\nTo identify the alignment box of a candidate box in the left image, we first determine the transformation matrix\u2014an affine transformation between the two images. We use SuperGlue [29] to establish point correspondences between the images, and apply RANSAC to eliminate outliers. We use SuperGlue because of its lightweight and high accuracy point-matching performance [29].\nA candidate box is valid if its alignment overlaps with any box in the other image (IOU > 0). Otherwise, it is invalid and excluded."}, {"title": "3.3. Stage 3: Correspondence prediction", "content": "The alignment stage combined with the confidence thresholding substantially reduces the false positives, yielding improved mAP on five benchmarks (see Tab. 3). Yet, the lack of correspondence information remains unsolved (see the outputs of Stage 2 in Fig. 3).\nHere, we aim to predict the correspondence between the predicted boxes for each image pair given the embeddings of each box. That is, we first extract embeddings from the feature maps in the backbone (Stage 1) for each aligned box of Stage 2. Then, we use the Hungarian bipartite matching algorithm jointly with a contrastive matching loss to predict the final correspondence.\nBox embedding extraction Since each predicted box intersects with \u2265 1 image patches in the feature maps of Stage 1, we use 2 different methods to extract the box embeddings, and choose the best one based on the mAP score in App. B:\n1.  Mean pooling Method: We hypothesize that the mean of patches associated with a predicted box enriches the correspondence embedding vector of the box with contextual information surrounding the object. We input each image (of size 256 \u00d7 256) into the image encoder to obtain a feature volume of 8 \u00d7 8 \u00d7 2048. From the 8 \u00d7 8 = 64 patch embeddings, we select all N embeddings corresponding the patches that overlap with a given bounding box in the input image space. Then, we take the mean of the N embeddings to obtain final embedding of size 2048 (code).\n2.  Region Cropping Method: This method evaluates whether excessive contextual information surrounding an object negatively impacts the quality of the embedding vector. To address this, only the information within the predicted bounding box is utilized. We crop the input image to bounding-box region to create a cropped image (code). We feed the cropped image into a ResNet-50 image encoder and average the 8 \u00d7 8 \u00d7 2048 feature output from layer 4.2 to obtain a 2048 dimensional embedding.\nA key challenge in implementing change detection in real-world scenarios is identifying the correspondence between changes detected in two images. We use the Hungarian algorithm to match the predicted bounding boxes between the two images. Given, $e_i$, and $e_j$, the embeddings of two bounding boxes from (Sec. 3.3) we calculate a cost matrix using the ground distance (Eq. (1)) similar to [23].\n$d_{ij} = 1 - \\frac{(e_i e_j)}{||e_i|| ||e_j||}$ (1)\nwhere i, j are indices of matrix elements. Using the cost matrix the Hungarian algorithm assigns the correspondence between boxes from the first and second image such that the total cost is minimum.\nContrastive matching loss We use the contrastive matching loss to train the model to classify pairs of matched boxes obtained from the Hungarian algorithm. The Hungarian algorithm is not perfectly accurate, i.e., it achieves an F1 score of 91.68% on the  dataset (see App. A) when using the ground truth boxes, and it generates both negative and positive matchings. Specifically, we compare the matched boxes with the correspondence information in the ground truth, classifying them as either: (1) positives, i.e., they match the ground truth, or (2) negatives, i.e., they do not. We leverage this fact and train the model using our contrastive matching loss. First, the embeddings of matched pairs are concatenated and processed through a fully connected layer. Then, we use a binary classification loss (BCELoss), treating the matched pairs as predictions and the ground-truth correspondence as targets.\nThe final training objective in (Eq. (2)) consists of two main components: (1) object detection loss and (2) contrastive matching loss. The detection loss integrates center-based loss components, ensuring precise localization and classification.\n$L_{total} = L_{CenterNet} + \\alpha L_{DETR} + \\beta L_{contrastive}$ (2)\nwhere $L_{CenterNet}$ is the CenterNet detection loss [7], and $L_{DETR}$ [2] is the combination of $L_1$ loss and GIOU loss, $L_{contrastive}$ is our contrastive matching loss. The comprehensive analysis of each loss component shows in (Sec. 4.3)."}, {"title": "3.4. Training hyperparameters", "content": "This section specifies the training hyperparameters. We fine-tuned the CYWS change detector [25] using contrastive matching loss and DETR loss [2], leveraging the pre-trained CYWS model. Transformation estimation followed the method in [26]. The fine-tuning process ran for 200 epochs on four A100 GPUs with a batch size of 16, optimized using the Adam algorithm [16] with a learning rate of 0.0001 and weight decay of 0.0005. The final loss (Eq. (2)) used $\\alpha$ = 3 and $\\beta$ = 2. A detailed analysis of hyperparameter selection is provided in (App. C)."}, {"title": "4. Hyperparameters Tuning", "content": "We analyze the impact of assigning embeddings from ground-truth boxes or predicted boxes as inputs to the Hungarian algorithm on the matching process (Fig. 3).\nExperiments We evaluate three embedding assignment methods for training the contrastive matching loss. The first method assigns embeddings exclusively to predicted bounding boxes. The second method assigns embeddings only to ground-truth bounding boxes. The third method, a hybrid approach, utilizes both ground-truth and predicted bounding boxes, where predicted embeddings are passed to the Hungarian algorithm, and correctly assigned matches replace predicted embeddings with their corresponding ground-truth embeddings, while incorrect assignments retain the original predicted embeddings. In all methods, the output from the Hungarian algorithm is used to compute the contrastive matching loss (Fig. 5).\nResults The method relying solely on only ground-truth boxes achieves the highest mean average precision (mAP) across the,, and datasets. In contrast, combining ground-truth and anticipated boxes results in improved mAP for both the and T datasets. However, due to variations in viewpoint across the,, and datasets, leveraging predicted boxes for feature embeddings proves insufficient for accurately capturing differences between paired images. Consequently, the mAP is reduced when using predicted boxes alone, compared to using solely ground-truth boxes (Tab. 1)."}, {"title": "4.1. Training contrastive matching loss with only ground-truth achieves the highest mAP", "content": "To compute the cost matrix for the Hungarian algorithm in Stage Three, features are extracted from the encoder or decoder in Stage One to generate embeddings for each predicted bounding box (Fig. 3). We hypothesize that using features from different decoder layers allows the extraction of multi-scale information, resulting in embeddings with richer representations compared to those generated solely from the encoder's output. Specifically, we evaluate features obtained from the output of the encoder's final layer and the outputs of the first three initial layers of the decoder.\nExperiments In the first experiment, the output from Layer 4.2 of the encoder (ResNet-50) is used, resulting in an embedding of size 2048 being assigned to each predicted bounding box. In the second experiment, the feature volumes from the first three initial layers of the decoder, with dimensions 8\u00d78\u00d74096, 16\u00d716\u00d72048, and 32\u00d732\u00d71024, respectively, are used. We concatenate embedding extracted from three decoder layers to form the final embedding of size 7168 (code).\nResults Using the embeddings from the decoder layer does not lead to a better mAP score (see Tab. 2) across all datasets. The feature map obtained from the encoder has a higher value of +0.97 in the  dataset, +2.19 in the dataset, and +1.01 in the T dataset. However, on the  dataset, it yields a marginal improvement of only +0.03. Therefore, we use the Encoder feature map for our finetuned model."}, {"title": "4.2. Encoder feature maps yield better localization than decoder feature maps", "content": "Our fine-tuning loss (Eq. (2)) has three components: CenterNet loss, DETR loss, and our novel contrastive matching loss. Here, we run an ablation study to show that all three losses contribute to the final result.\nExperiments We conduct fine-tuning experiments on the CYWS model under various configurations. In the first setup, we used only the CenterNet loss and DETR loss for training. In our ablation study, we fine-tune the model under different configurations to evaluate the impact of the DETR loss and the Contrastive matching loss on change detection performance. Specifically, we experiment with our model fine-tuned with and without the DETR loss, as well as with and without the Contrastive matching loss. For all these experiments, the models were initialized with weights derived from the pre-trained CYWS model."}, {"title": "4.3. Ablation study of loss function", "content": "We find that DETR loss contributes improvements of +7.02, +2.45, +2.95, +0.79, +6.73 in the,,,T, datasets, respectively, compared to using CenterNet alone. Similarly, the Contrastive matching loss leads to enhancements of +6.7, +3.02, +4.94, +1.11, and +3.49 across the same datasets, respectively, compared to CenterNet alone. Adding all three losses results in the highest mAP across 4 out of 5 datasets."}, {"title": "5. Results", "content": "Real-world applications require models to perform well in both the change and no-change cases. We test our hypothesis that visual correspondence (i.e., the binary supervision labels of whether two image patches contain a change or not) improves change detection accuracy. We evaluate change detection performance by applying a detection threshold to ensure that the average number of predicted boxes per image in no-change cases remains below 0.01. This is a critical consideration for practical deployment, which has been overlooked in prior work [25].\nExperiments We initialize our model with the pre-trained CYWS weights and fine-tune it (Sec. 3.4) using the $L_{total}$ loss function (Eq. (2)). We evaluate mean Average Precision (mAP) on five datasets (Sec. 2.2) using both CYWS [25] and our models. We choose the optimal detection threshold at 0.25 for both models, ensuring the average number of predicted boxes per image in the no-change case remains below 0.01.\nResults Our fine-tuned model outperforms CYWS [25] across all THREE post-processing STAGES (see Tab. 4). Since we keep the fine-tuning strategy fixed and repeat the experiment with various post-processing techniques, we contribute the positive delta in mAP score to our contrastive matching loss (Sec. 3.3). That is, our contrastive loss improves change detection performance across both change and no-change pairs compared to CYWS [25]. This performance gap further increases across all five datasets when an optimal threshold is applied. For instance, in the dataset, the margin increases from +9.04 to +10.97."}, {"title": "5.1. Given the same performance on no-change cases, our finetuned detector outperforms state-of-the-art CYWS", "content": "Each additional stage, including detection threshold, alignment, and Hungarian matching, contributes to a monotonic reduction in false positives across all five benchmark datasets for both change cases (Tab. 7) and no-change cases. On the dataset, false positives are reduced by -1.045. Similar reductions are observed across other datasets: a reduction of -1.474 on the dataset, -0.216 on , -1.428 on T, and -0.059 on (Tab. 5)."}, {"title": "5.2. The alignment stage plays a crucial role in the success of the matching algorithm", "content": "Given two sets of boxes of predicted changes [25], our Hungarian-based matching algorithm's goal is to pair up corresponding changes. The alignment stage (Fig. 3) identifies pairs of corresponding boxes between two images and eliminates boxes that do not have a match. We aim to test the matching accuracy with and without the Alignment stage to understand its importance.\nExperiment We repeat our correspondence prediction algorithm (Sec. 3.3) on all five benchmarks with and without the Alignment stage.\nResults We find that the Alignment stage plays a crucial role, responsible for +34.57 in the dataset, +29.27 in the T dataset of CYWS model in matching accuracy. Similarly, in and T datasets, our model's improvement is +38.18 and +30.04, respectively (see Tab. 6). See (Fig. A1) for qualitative results."}, {"title": "5.3. Contrastive matching loss improves change matching accuracy", "content": "The contrastive matching loss directs the model to focus on regions exhibiting changes in both images, filtering out false positives. This approach improves change detection accuracy and boosts the correspondence score relative to the CYWS model.\nExperiment We evaluate our model and the CYWS model under three configurations: using a detection threshold, incorporating an alignment stage, and applying the Hungarian algorithm to detect changes across five datasets (Sec. 2.2). The matching score was computed with and without alignment on these datasets.\nResults Our model, trained with contrastive matching loss, surpasses CYWS across five datasets under both scenarios-with and without alignment. Without alignment, our model yields improvements of +6.05 on, +7.14 on, and +12.24 on. With alignment, it maintains a strong advantage with +6.56 on +3.64 on +5.76 on , and +1.31 on T (Tab. 7)."}, {"title": "6. Related Work", "content": "Change Detection The state-of-the-art model CYWS [25] targets change detection for 2D objects in surveillance images, demonstrating broad applicability without retraining. To extend this capability to 3D objects, CYWS-3D [26] was proposed. However, neither approach identifies corresponding changes between image pairs.\nMethods like Changemamba [4], SCanNet [5], and STADE-CDNet [19] are specifically designed for remote sensing applications. In this domain, images generally exhibit a single change between two images, simplifying the correspondence problem. In contrast, our approach addresses a more complex correspondence problem, involving multiple changes between two surveillance images (see Fig. A4b).\nChange Segmentation Prior research, including [1, 28], has focused on detecting changes in street views, while studies such as [9,27,35,37,38] concentrate on satellite imagery. [40] presents a novel zero-shot change segmentation approach specifically for satellite images. Similarly, our model demonstrates strong performance across four zero-shot benchmarks.\nChange Captioning The Spot-the-Diff (1) change captioning dataset, introduced by [13], contains 13,000 image pairs captured from surveillance cameras. Research in this domain also explores remote sensing image datasets [3] and addresses challenges in datasets such as CLEVR-Change, CLEVR-DC, and Bird-to-Words [8, 10-12, 15, 21, 30, 39], which either simulate or capture real-world changes. These works lack effective change localization, and change captioning becomes more complex when multiple changes occur between two images. Our approach addresses these issues by providing change localization with correspondence, simplifying interpretation. [32] presents STVchrono, a benchmark dataset of 71,900 Google Street View images from 18 years across 50 cities to study long-term changes in outdoor scenes. However, its creation is labor-intensive and time-consuming, limiting scalability. In contrast, our dataset can be efficiently scaled with a simple process."}, {"title": "7. Discussion and Conclusions", "content": "Limitations We observe that the accuracy of Point estimation (Stage 2 in our pipeline) plays a critical role in our pipeline. Specifically, images with significant distortions or detailed textures (see Fig. A4a) poses a challenge to Point Estimate to align two images and leads to estimation accuracy declines, impacting alignment stage effectiveness.\nConclusions This study proposes a novel contrastive matching loss function that improves detector accuracy and matching accuracy, surpassing the CYWS method. The post-processing algorithm ensures accurate pairing of changes, and a new metric is introduced for evaluating matching scores across models."}]}