{"title": "Audio-based Kinship Verification Using Age Domain Conversion", "authors": ["Qiyang Sun", "Alican Akman", "Xin Jing", "Manuel Milling", "Bj\u00f6rn W. Schuller"], "abstract": "Audio-based kinship verification (AKV) is important in many domains, such as home security monitoring, forensic identification, and social network analysis. A key challenge in the task arises from differences in age across samples from different individuals, which can be interpreted as a domain bias in a cross-domain verification task. To address this issue, we design the notion of an \"age-standardised domain\" wherein we utilise the optimised CycleGAN-VC3 network to perform age-audio conversion to generate the in-domain audio. The generated audio dataset is employed to extract a range of features, which are then fed into a metric learning architecture to verify kinship. Experiments are conducted on the KAN_AV audio dataset, which contains age and kinship labels. The results demonstrate that the method markedly enhances the accuracy of kinship verification, while also offering novel insights for future kinship verification research.", "sections": [{"title": "I. INTRODUCTION", "content": "Kinship verification (KV) is an active area of research .[1], which is defined as the process of using deep learning algorithms to identify whether two individuals are related by blood through the extraction and comparison of information from biometric data [2]. It has strong social significance and applies to a variety of fields, including family security monitoring, rapid forensic identification, and social network analysis [3]. The majority of current research has focused on the use of facial images and videos to identify kinship as, in general, genetically related family members exhibit pronounced similarities in terms of appearance [4]. Corresponding similarities, however, can also be observed in voice characteristics as backed up by a recent study [5]. Nevertheless, in line with the common favouritism of the image modality over the audio modality [6], audio-based kinship verification (AKV) has barely been investigated, despite some key advantages of the audio data in the task setting: for instance, in many scenarios, such as phone calls, video footage is unavailable as it is impractical to collect.\nGenerally speaking, KV requires recognising certain similarities between individuals resulting from a family connection while condoning similarities that result from other domain overlaps, such as age or gender. In many cases, related individuals belong to different domains, e.g., a mother-son pair having different age and gender. This adds to additional challenges with respect to variations in appearance or voice characteristics [7], which can be even increased due to emotional states or differences in environments or recording devices [8], [9], [10].\nIn an attempt to mitigate these domain biases resulting from different data distributions [11], one approach is to learn specific latent feature spaces [12]. Most prominently, in AKV, the speech characteristics pitch, timbre, tone, and speech rate, to name but a few, vary largely across age groups [13], [14]. To address countering these domain bias effects, we explore voice conversion techniques in this work to leverage voice recordings of individuals of different ages into the same age domain via generative models. This age-standardised domain refers to a target age range where speech characteristics are unified, minimising the influence of age-related variations in voice. Specifically, young speech data is converted by ageing and projected into a middle-aged standard domain, old speech data is processed by rejuvenation and similarly projected into the middle-aged domain. Subsequently, high-dimensional features are extracted and sent to a triplet network [15] for kinship verification. Our experiments demonstrate that an age-conversion can be helpful in increasing kinship verification accuracy compared to the baseline consisting of the original recordings from different age domains."}, {"title": "II. RELATED WORK", "content": "Voice conversion (VC) is a technique that is commonly used in the field of speech processing and generation to modify and manipulate specific properties of voices [16]. It encompasses the alteration of a speech's gender, age, emotion and timbre [17], [18]. Through the analysis and synthesis of audio features, VC techniques can be used to generate audio with characteristics that closely align with those of the target [19], [20]. It has potential applications across a variety of domains, including personalised voice assistants, voice camouflage, and entertainment [21]. Earlier VC methods are based on Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs) [22], which have been largely replaced by more advanced approaches in recent years. With the development of deep learning techniques, models such as Generative Adversarial Networks (GANs) and Variational Autoencoders"}, {"title": "III. METHODOLOGY", "content": "This section details the design of our AKV experiments. An overview of our overall methodology is given in Figure 1.\nWe use the KAN_AV dataset in this study, which comprises a substantial corpus of audio and video data collected in a natural setting, with a particular focus on the analysis of faces and voices. The data is collected from publicly accessible video sources, including cinematic clips, oratory presentations, and interviews. The dataset consists of over 28,000 audio and video clips, with a total duration of approximately 98.5 hours, encompasses 970 individuals across a diverse age range, from 3 to 100 years old. 645 individuals have been assigned kinship labels, which fall into one of seven categories of first-degree kinship: The kinship labels fall into one of seven categories of first-degree kinship: father and son (FS, 99 pairs), father and daughter (FD, 82 pairs), mother and son (MS, 46 pairs), mother and daughter (MD, 64 pairs), brother and sister (BS, 93 pairs), brother and brother (BB, 111 pairs), or sister and sister (SS, 56 pairs) [31]. The dataset is initially divided into three age-based groups: a young group (<35 years old, 8032 instances), a middle-aged group (35-55 years old, 12170 instances), and an old group (>55 years old, 7798 instances).\nWe use the audio dataset with 16kHz sample rate. For preprocessing, the DC offset of the audio is eliminated, and the audio is converted to single channel. We extract a Mel-spectrogram with a hop length of 256, a window length of 1024, 80 Mel bands in in the frequency range from 0Hz to 8,000 Hz and an FFT size of 1024. We employ the Slaney norm for normalisation and Mel frequency scale conversion. During the training phase, the audio length is adjusted to 64 frames (approximately 1 s). The generated Mel-spectrogram is then standardised according to the mean and standard deviation of the middle-aged group data. The processed data is used to train our voice conversion model."}, {"title": "B. Age Conversion", "content": "Our age VC model is based on the CycleGAN-VC3 architecture [36]. CycleGAN aims to achieve unsupervised image-to-image conversion, or in this case, spectrogram-to-spectrogram conversion between two different domains through bi-directional cyclic consistency loss, which has shown excellent performance in voice conversion tasks. The CycleGAN-VC3 model introduces a Time-Frequency Adaptive Normalisation (TFAN) module, which is an extension of the traditional instance normalisation, allowing for a more fine-grained tuning of the features in the time-frequency dimension while preserving the information of the source spectrogram. The generator first converts the input Mel-spectrogram into 128 channels via a 2D convolutional layer, followed by two downsampling layers, which progressively halve the spatial resolution while increasing the number of channels to 256. During this process, the TFAN module is applied to maintain consistency across time and frequency. Next, six residual blocks are used to extract deep voice features, and a 1D convolution is applied to convert the features back to a 2D representation. During the upsampling phase, the model restores the spatial resolution and increases the number of channels to 1024, with another application of the TFAN module to ensure feature retention. Finally, a 2D convolution generates the output Mel-spectrogram. The discriminator retains the PatchGAN architecture proposed in [36].\nThe generator and discriminator loss functions consist of LSGAN loss, cycle consistency loss, and identity loss. The LSGAN loss improves training stability by replacing the traditional cross-entropy loss with a least-squares objective [37]. The cycle consistency loss ensures that after mapping samples from one domain to another and back, the original data is preserved, enforcing consistency across domain transformations [38]. The identity loss [27] is used to preserve input characteristics when the input already belongs to the target domain, ensuring that the generator does not alter the data unnecessarily. The total loss function is defined as:\n$L(G, F, D_x, D_y) = L_{gan}(G, D_x) + L_{Gan}(F, D_y) + \\lambda_{cycle} L_{cycle} (G, F) + \\lambda_{identity} L_{identity}(G)$,\nwhere $L_{GAN}(G, D_x)$ and $L_{GAN}(F, D_y)$ represent the LSGAN losses for generators G and F against discriminators $D_x$ and $D_y$. $L_{cycle}(G, F)$ represents the cycle consistency loss. $L_{identity}(G)$ represents the identity loss. $\\lambda_{cycle}$ and $\\lambda_{identity}$ are weighting factors for the cycle consistency loss and identity loss, set to 10 and 5, respectively, with $\\lambda_{identity}$ progressively decaying in the later stages of training.\nWe use the Adam optimiser with $Ir = 0.0002$ and the discriminator with $lr = 0.0001$ in our experiments."}, {"title": "C. Feature Extraction", "content": "For the AKV task, we extract the i-vector and x-vector with the Kaldi toolkit [39]. In both cases, the speech signal is first converted to MFCC features with a 25 ms frame length, 10 ms frame shift, 30 Cepstral coefficients, and 40 Mel frequency filters. Voice Activity Detection (VAD) is initially applied to remove non-speech segments. For i-vectors, 400-dimensional features are generated based on a Universal Background Model (UBM) with 512 Gaussian components. For x-vectors, a pre-trained Time-Delay Neural Network (TDNN) model is used to produce 512-dimensional features.\nFinally, we extract 1024-dimensional features (called Wav2Vec below) from the raw audio via the final transformer layer of a pre-trained Wav2Vec 2.0, specifically designed for age and gender recognition tasks [40]. This model is fine-tuned on several large-scale speech datasets using the Wav2Vec2-Large-Robust architecture, which is capable of capturing high-level semantic information within the speech signal and providing rich acoustic feature representations relevant to age."}, {"title": "D. Kinship Verification Model", "content": "We finally employ a triplet network for the AKV task, which projects the extracted features into a new embedding space and minimises the embedding distances between pairs of kinship samples and maximise the corresponding distances between pairs of unrelated samples [41].The model is presented with anchor-positive-negative triplets. When creating the triplets, the anchor and positive samples come from the same familial relationship pair while the negative sample is randomly selected from individuals of the same gender as the positive samples but with no relationship. For example, We apply a custom data partitioning strategy, ensuring the mutual exclusivity of the training, validation, and test sets by dividing the data by individual in a 7:1:2 ratio. After combining, we generate approximately 285,000 sample pairs. the model is trained to optimise embedding distances to distinguish anchor-positive pairs and anchor-negative pairs. During inference, an optimal threshold is determined, which is subsequently used for binary classification on the test set.\nThe triplet model consists of a two-layer fully connected network. The first layer maps the input features to a 256-dimensional hidden space, applying Batch Normalisation, a ReLU activation function and a dropout layer. The second layer compresses the features further into a 128-dimensional vector space for triplet loss computation. We use the classic triplet loss function, defined in [15]. We use SGD with $lr = 0.0001$ and $momentum = 0.9$ during training, ."}, {"title": "IV. EXPERIMENTS", "content": "For the age conversion into a neutral, middle-aged age domain, we train two generators ($G_{y2m}$ and $G_{o2m}$) and corresponding discriminators ($D_{y2m}$ and $D_{o2m}$) to convert young and old audio into middle-aged audio while leaving the data originally in the middle-aged domain untouched. The generated Mel-spectrograms are subsequently converted back into speech using a pre-trained HiFi-GAN vocoder [42]. The length of each generated speech segment is fixed to 3 seconds. Subsequently, the three feature types (i-vector, x-vector, and Wav2Vec) are extracted from both the original dataset and the dataset generated by the age conversion model. The features are then mapped to a low-dimensional space through t-distributed stochastic neighbour embedding (t-SNE) to observe their distribution patterns, similar to [43]."}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed a voice conversion-based kinship verification method. This method mitigated the domain bias caused by age deviation by projecting all audio into the age-standardised domain. We employed the CycleGAN-VC3 architecture to perform age domain conversion of audio and generated a new data set. In addition, we optimised the metric learning method in the baseline and introduced a new feature extraction method based on Wav2Vec 2.0 into the task. The experimental results demonstrated that the method clearly improved the accuracy of audio-based kinship verification task. Future work could consider additional gender conversion to better cope with tasks such as brother and sister, father and daughter, or mother and son."}]}