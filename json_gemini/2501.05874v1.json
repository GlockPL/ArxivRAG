{"title": "VideoRAG: Retrieval-Augmented Generation over Video Corpus", "authors": ["Soyeong Jeong", "Kangsan Kim", "Jinheon Baek", "Sung Ju Hwang"], "abstract": "Retrieval-Augmented Generation (RAG) is a powerful strategy to address the issue of generating factually incorrect outputs in foundation models by retrieving external knowledge relevant to queries and incorporating it into their generation process. However, existing RAG approaches have primarily focused on textual information, with some recent advancements beginning to consider images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing events, processes, and contextual details more effectively than any other modality. While a few recent studies explore the integration of videos in the response generation process, they either predefine query-associated videos without retrieving them according to queries, or convert videos into the textual descriptions without harnessing their multimodal richness. To tackle these, we introduce VideoRAG, a novel framework that not only dynamically retrieves relevant videos based on their relevance with queries but also utilizes both visual and textual information of videos in the output generation. Further, to operationalize this, our method revolves around the recent advance of Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and seamless integration of the retrieved videos jointly with queries. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines.", "sections": [{"title": "1 Introduction", "content": "Recently, large foundation models, such as Large Language Models (LLMs) and their extension to vision modality called Large Vision-Language Models (VLMs), have become the standard choice for addressing diverse tasks due to their remarkable capabilities (Li et al., 2024; Yang et al., 2024; Dai et al., 2024). In particular, these models, trained on extensive textual and multimodal corpora, encode vast amounts of knowledge within their large-scale parameters. However, despite their success, they are still prone to generating factually incorrect outputs, known as hallucination, as their parametric knowledge can be inaccurate, incomplete, or outdated (Lewis et al., 2020; Ram et al., 2023), and this limitation highlights the need for incorporating knowledge from external knowledge sources, with Retrieval-Augmented Generation (RAG) emerging as an essential mitigator for it. Specifically, RAG typically operates by retrieving query-relevant information from an external corpus and then generating answers grounded in the retrieved information (Niu et al., 2024; Ayala and B\u00e9chard, 2024).\nHowever, while existing RAG approaches have been widely adopted for various real-world applications, they have primarily focused on retrieving and incorporating textual content (Ram et al., 2023; Jeong et al., 2024a), with only recent attempts be-"}, {"title": "2 Method", "content": "In this section, we present VideoRAG, which performs retrieval of query-relevant videos over video corpus and generates responses grounded in them.\n2.1 Preliminaries\nWe begin with preliminaries, providing the descriptions of Retrieval-Augmented Generation (RAG) and then Large Video Language Models (LVLMs).\nRetrieval-Augmented Generation RAG aims to enhance the capabilities of foundation models by"}, {"title": "2.3 Auxiliary Text Generation", "content": "In both the retrieval and generation steps, the inclusion of video-associated textual data, such as subtitles, can play a crucial role in enhancing video representation since it provides additional context and semantic cues that complement the visual content. However, not every video in the corpus comes with subtitles since they require additional annotations. Therefore, for such videos, we propose generating auxiliary textual data by extracting audio from the video and converting it into text using off-the-shelf automatic speech recognition techniques. Formally, given a video V, this process can be formalized as follows: $\\tau_{aux}$ = AudioToText(Audio(V)), where Audio(V) extracts the audio track from the video, and AudioToText converts the extracted audio signal into textual content. Therefore, for those videos without subtitles, the auxiliary text $\\tau_{aux}$ is used in place of t in both the retrieval and generation steps."}, {"title": "3 Experimental Setups", "content": "In this section, we describe the datasets, models, evaluation metrics, and implementation details.\n3.1 Datasets\nTo evaluate our VideoRAG in information-seeking question answering (a task particularly suitable and widely used for validating RAG approaches), we use available resources having extensive video corpus and question-answer pairs with diverse topics. Specifically, we adapt WikiHowQA data (Bolotova-Baranova et al., 2023) as the source of queries and answers, as it offers a wide range of instructional questions extracted from the WikiHow webpage\\u00b9, with human-written, step-by-step processes to ensure high-quality ground truths. Also, for the external video corpus, we utilize the HowTo100M dataset (Miech et al., 2019), a comprehensive collection of instruction videos sourced from YouTube, which is also associated with queries from WikiHow based on their search results, therefore, serving as a useful resource for our VideoRAG tasks.\n3.2 Models\nBaselines We evaluate the performance of VideoRAG against four different baselines, as follows:\n1. NA\u00cfVE \u2013 which directly generates answers using input queries without any additional context.\n2. TEXTRAG (BM25) \u2013 which is a text-based RAG model, performing retrieval over documents (from Wikipedia) based on their relevance"}, {"title": "4 Experimental Results and Analyses", "content": "We now present results and various analyses, showing the effectiveness of the proposed VideoRAG.\n4.1 Main Results\nWe provide main results, showcasing the performance of different models with varying types of retrieved knowledge. First, we observe that all RAG models, utilizing external knowledge, clearly outperform the NA\u00cfVE baseline, reaffirming the critical role of external knowledge in enhancing the factual accuracy of generated responses. Also, among these, our VIDEORAG achieves the highest performance, significantly surpassing existing"}, {"title": "4.2 Video Retrieval", "content": "Impact of Video Retrieval We hypothesize that the quality of the retrieved videos is a critical factor in the success of RAG, as it can directly influence the subsequent answer generation process. To confirm this, we compare the performance of VideoRAG with different videos, including randomly selected videos and retrieved videos (relevant to queries), used for augmenting the answer generation step. , we then observe that incorporating query-relevant videos significantly improves the quality of the answers compared to randomly selected videos, demonstrating the importance of retrieval quality. Furthermore, the Oracle setting, which represents an ideal scenario with perfectly relevant video retrieval, achieves the highest performance, highlighting the potential for further improvements through advancements in video retrieval mechanisms within our VideoRAG.\nEffectiveness of Textual and Visual Features\nWhen performing video retrieval, it is questionable how much different modalities, such as textual features, visual features, or a combination of both, contribute to the effectiveness of video representations, and we report retrieval results with varying modalities. From this, we observe that textual features consistently outperform visual features, likely due to their stronger semantic alignment with"}, {"title": "4.3 Analyses and Discussion", "content": "Category-Wise Performance Analysis To evaluate the robustness of our VideoRAG across diverse query types, we breakdown the model performance on 10 distinct categories (annotated within WikiHow). the family of the proposed VideoRAG (such as VIDEORAG-T and VIDEORAG-V) outperforms all other baselines across all categories, which highlights its ability to handle a wide range of queries. Also, it is worth noting that VIDEORAG-V shows notable performance improvement in the Food & Entertaining category, and this is particularly reasonable given that the questions in this category often benefit significantly from visual details, for example, the query: \u201cHow to make a healthy spinach and garlic dish\" requires ingredient preparation or cooking techniques, which are not effectively conveyed through text alone. Thus, the results in this category reaffirm the importance of leveraging video content as external knowledge for RAG.\nAblation Studies To analyze how performance varies with different knowledge sources, we conduct ablation studies. We then find that, while incorporating external knowledge (whether from textual encyclopedic sources or video corpus) consistently improves performance over the NA\u00cfVE baseline, the approach that jointly uses videos with general textual documents achieves slightly degraded performance. This suggests that textual content (retrieved from the encyclopedic knowledge base) may introduce redundant or irrelevant details, which may overlap with or contradict the information provided by video content, leading to diminishing the effectiveness of the VideoRAG framework.\nQualitative Results We now qualitatively analyze the effectiveness of VideoRAG through a case study, with the query: \u201cExplain how to bake cookies on your car dashboard\u201d. the NA\u00cfVE baseline, relying solely on its parametric knowledge, generates a generic response highlighting the impracticality and safety concerns of such a method, failing to provide the step-by-step instructions necessary to address the query. It indicates the limitation of parametric knowledge that is inadequate especially when specific and uncommon information is required. In contrast, VIDEORAG-"}, {"title": "5 Related Work", "content": "Retrieval-Augmented Generation RAG is a strategy that combines retrieval and generation processes to produce accurate answers by grounding them in relevant external knowledge (Lewis et al., 2020; Ram et al., 2023; Zhao et al., 2024). To be specific, during the retrieval step, documents (relevant to queries) are selected from a large corpus by calculating their similarity to the query, which can"}, {"title": "5.  1. Preliminary notations", "content": "Let the probability density function (PDF) be given by \\(f_n(x) = \\frac{1}{{\\sigma \\sqrt{2\\pi}}} e^{-\\frac{1}{2} (\\frac{x-\\mu}{\\sigma})^2}\\) where the parameters are chosen according to:"}, {"title": "5.  2. Preliminary notations", "content": "Let $\\Omega$ be a set, $\\mathcal{A}$ a $\\sigma$-algebra on $\\Omega$, and $\\mu$ a measure on $(\\Omega, \\mathcal{A})$. A function $f: \\Omega \\rightarrow \\mathbb{R}$ is said to be measurable if for every $a \\in \\mathbb{R}$, the set $\\{x \\in \\Omega: f(x) > a\\}$ is in $\\mathcal{A}$."}, {"title": "6 Conclusion", "content": "In this work, we presented VideoRAG, a novel framework that expands the current landscape of RAG systems by leveraging a video corpus as the external knowledge source. Specifically, unlike existing works that use the textual representations of videos or assume the existence of query-relevant videos without retrieval, the proposed VideoRAG retrieves videos based on their relevance to queries but also integrates their multimodal richness (including visual and textual elements) into the RAG pipeline, by harnessing the capabilities of LVLMs. Also, through comprehensive analyses, we demonstrated how the inclusion of visual or textual features, or a combination of both, improves retrieval and generation performance, and, inspired by the critical role of textual features (for retrieval quality)"}]}