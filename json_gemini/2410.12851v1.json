{"title": "VIBECHECK: DISCOVER & QUANTIFY QUALITATIVE DIFFERENCES IN LARGE LANGUAGE MODELS", "authors": ["Lisa Dunlap", "Krishna Mandal", "Trevor Darrell", "Jacob Steinhardt", "Joseph Gonzalez"], "abstract": "Large language models (LLMs) often exhibit subtle yet distinctive characteristics in their outputs that users intuitively recognize, but struggle to quantify. These \"vibes\" - such as tone, formatting, or writing style - influence user preferences, yet traditional evaluations focus primarily on the singular vibe of correctness. We introduce VibeCheck, a system for automatically comparing a pair of LLMs by discovering identifying traits of a model (\u201cvibes\u201d) that are well-defined, differentiating, and user-aligned. VibeCheck iteratively discovers vibes from model outputs, then utilizes a panel of LLM judges to quantitatively measure the utility of each vibe. We validate that the vibes generated by VibeCheck align with those found in human discovery and run VibeCheck on pairwise preference data from real-world user conversations with Llama-3-70b vs GPT-4. VibeCheck reveals that Llama has a friendly, funny, and somewhat controversial vibe. These vibes predict model identity with 80% accuracy and human preference with 61% accuracy. Lastly, we run VibeCheck on a variety of models and tasks including summarization, math, and captioning to provide insight into differences in model behavior. Some of the vibes we find are that Command X prefers to add concrete intros and conclusions when summarizing in comparison to TNGL, Llama-405b often overexplains its thought process on math problems compared to GPT-40, and GPT-4 prefers to focus on the mood and emotions of the scene when captioning compared to Gemini-1.5-Flash.", "sections": [{"title": "INTRO", "content": "How a large language model writes a story, explains a concept, or edits an essay can be evaluated on many different dimensions such as creativity, formatting, and writing style. However, most evaluations focus on one dimension: \"correctness\". State-of-the-art in evaluation methods remain largely focused on measuring accuracy for question answering and analytical reasoning tasks and methods which aim to provide a more holistic view of LLMs rely on predefined concepts like conciseness, clarity, and trustworthiness to measure a model's performance. These evaluation approaches fail to capture the open-ended nature of LLM applications and the critical dependence on subjective user preferences and context of the task. For instance, in creative writing, tone and creativity might be crucial, while in coding tasks, efficiency and readability take precedence. In order to best inform users of which model would be best for their needs, we require flexible evaluation methods that can both discover and measure the relevant axes to evaluate for a given task.\nWhen interacting with a set of LLMs for an extended period, a user can often tell which model generated a particular response by looking at certain traits of the outputs. We define these identifying traits of models as \u201cvibes\u201d. For instance, users have found Llama3 outputs tend to be more friendly compared to outputs from GPT-4 and Claude which tend to be more formal (see Figure 1); in other words, Llama3 ranks high on the friendliness vibe, defined by the axis formal \u2192 friendly.\nUsing these insights, we might select LLama for customer service tasks and Claude for coding tasks.\nUnderstanding these vibes helps inform the development and deployment of models, but discovering and validating them for each model can be time-consuming and difficult. To address this, we outline"}, {"title": "RELATED WORK", "content": "Aspect-based evaluations. The number of benchmarks in the NLP community has exploded in recent years, with a growing body of work on exploring a more holistic evaluation of language models. Several works, aim to improve on automatic metrics like BLEU and ROUGE scores to better measure how well a models output aligns with the ground truth by incorporating more nuanced evaluation criteria like factual accuracy, fluency, and conciseness. Similarly, efforts have been made to standardize model evaluation by evaluating models on many of these metrics.\nMoving away from measuring model outputs on ground truth responses, work evaluate model outputs on criteria like helpfulness and clarity using LLM judges on more open ended tasks like dialogue, role-play, and summarization. While all of these efforts supply an great foundation for measuring correctness, they all define the axes on what makes something correct beforehand. In contrast, VibeCheck aims to automatically discover these axes (vibes) and verify their utility to the user by measuring the correlation between vibes and human preference.\nPairwise comparison of LLMs. HCI tools like Google's AutoSxS and LLMComparator explores the current state of human powered LLM qualitative evaluation through interviews with data analysts. These works find that practitioners often eyeball individual examples to interpret and look at qualitative differences between the outputs of two models, and develop an interactive web based application for users to inspect side-by-side LLM outputs with an LLM based rationale as to why one output is preferred over another. While these works are focused more on software tools rather than a pipeline which can be quantitavely verified, these HCI findings inform VibeCheck's vibe discovery mechanism to align with the human-powered qualitative process. Moreover, many NLP works have explored using LLMs to predict user preference given responses from two models, showing these preference predictions often align with the judgements of human"}, {"title": "VIBE-BASED EVALUATIONS", "content": "We define a vibe as an axis along which a pair of texts can differ (e.g., \"formal \u2192 friendly\") that is perceptible to humans. A vibe v is represented by a text description of the axis along with a definition of what it means to be high or low on this axis (e.g. \"Tone: low = formal, high = friendly\", see Figure 1). Identifying vibes aids users in selecting models that best suit their specific tasks. In this work, we focus on comparing the vibes of two models by discovering the axes on which their outputs differ and quantifying the utility of these vibes.\nConsider a dataset \\(D\\) composed of triples \\((p, o_A, o_B)\\) and preference labels \\(y_p\\), where p is a prompt and \\(o_A, o_B\\) are the outputs from models A and B. For each triple, a judge (human or LLM) assigns a score for vibe v, denoted \\(v(p, o_A, o_B) \\in \\{-1,0, 1\\}\\), which indicates whether model A scores lower (-1), similarly (0), or higher (1) than model B on this vibe. Thus, a vibe imposes an ordering on model outputs.\nWe define 3 key criteria of a useful vibe; it should be well-defined, differentiating, and user-aligned.\nWell-defined: multiple evaluators agree on the ordering of outputs along the vibe. We quantify this by having two different judges (typically LLMs) compute \\(v(p, o_A, o_B)\\) across dataset \\(D\\) and report Cohen's Kappa to assess agreement."}, {"title": "VIBECHECK", "content": "VibeCheck consists of 3 stages: vibe discovery, vibe validation, and vibe iteration. Further details on the method implementation and prompts used are located in the Section D.\nVibe discovery. Similar to how a data scientist would inspect a subset of examples to discover qualitative differences in outputs, we discover vibes by having an LLM (GPT-40) examine the differences seen in a random subset of d prompt triplets. We first split the d prompt triplets into smaller batches of size batch and prompt GPT-40 to find differences between model A and model B across the set \\(\\{(P_1, o_A^1, o_B^1), ..., (P_{batch}, o_A^{batch}, o_B^{batch})\\}\\). To encourage the vibes to be well-defined and user-aligned, we prompt GPT-40 to generate differences that are human-interpretable and informative for understanding the overall behaviors of A and B. Below is a paraphrased system prompt used by the proposer.\nYou are a machine learning researcher trying to understand the\ndifference in behaviors of two LLMS. Given outputs from two different\nlanguage models given the same input, your task is to identify\ndifferences between these outputs along specific axes of variation.\nEach axis should be clearly defined, mutually exclusive, and easily\ninterpretable by humans. For each axis, provide a concise description\nof what it means for an output to be rated as \"Low\" or \"High\".\nAn example axis generated in this step might be 'Tone: Low: formal; High: friendly'. We repeat this proposal step for [d/batch] sets of triplets, obtaining a final set of vibes \\(\\{V_1, \u2026, V_m\\}\\) by taking the union of the vibes generated in each batch. We found that GPT-40 generates 5-10 axes of variation (vibes) for each sample, so we summarize vibes across all samples in \\(D_{discovery}\\) to find a set of K vibes which appear most often in \\(\\{v_1, \u2026, v_M \\}\\).\nVibe validation. Given a vibe v from the discovery phase, we first apply each vibe to a set of validation tuples, then use this validation set to score vibes and compute inter-annotator agreement, model-matching accuracy, and preference prediction accuracy and filter out vibes with low scores.\nTo apply vibes on the validation set, we assign a score to each pair of outputs \\(v_j (p, o_A, o_B) \\in \\{-1,0, 1\\}\\), indicating whether model A scores lower (-1), similarly (0), or higher (1) than model B"}, {"title": "RESULTS", "content": "We validate VibeCheck in two ways: first, we run it in settings where there are known human-discovered axes along which models vary (\u201cgold labels\u201d), and we can compare VibeCheck's output to the gold labels. Second, we run it in more open-ended settings and compute the inter annotator agreement, model matching accuracy, and preference prediction accuracy as described in Section 3.\nComparison to gold labels. The Human ChatGPT Comparison Corpus (HC3;  is a publicly available dataset containing question-answer pairs spanning diverse domains, each answered by both human experts and GPT-3.5. Along with this dataset,  performed a human study where experts were given these pairwise responses and asked to summarize the common differences seen between GPT and human responses. We treat these human-discovered summaries as gold labels, which we use to validate the qualitative axes discovered by VibeCheck.\nOpen-ended metrics. We compute average Cohen's Kappa, model matching accuracy, and preference prediction accuracy on the top 10 vibes generated by VibeCheck on a held-out set of prompt tuples with preference labels. To obtain the top 10 vibes, we apply least-angle regression on the full set of vibes returned by VibeCheck to predict model identity, then sort by the separability score. The full list of vibes discovered, LR coefficients and p-values from the model matching and preference prediction models, Cohen's kappa per vibe, and separability scores are in the Section H.\nExperimental setup. Unless otherwise stated, we run VibeCheck for 3 iterations, use a proposer batch size of 5, and set \\(D_{discovery}\\) to be 20 samples per iteration. Some datasets such as MATH, CN-N/DailyMail, and COCO captions have no pre-computed preference labels; to simulate preferences, we apply LLM-as-a-judge and ensemble GPT-40 and Claude 3.5 Sonnet as a judge using a similar procedure to , removing any samples declared a tie. Additional details on the experimental setup and hyperparameters are given in the Section A.\nBaseline. As a baseline, we prompt GPT-40 to generate a set of 10 vibes shown in Table 2 which represent common axes on which LLM outputs differ."}, {"title": "MEASURING VIBECHECK'S ALIGNMENT WITH HUMAN DISCOVERY", "content": "In this section, we compare the findings from VibeCheck to findings obtained via human discovery. We utilize previous work , which collections responses written by humans and GPT-3.5 and then recruits 200 annotators to look at 100-200 prompt output triples summarize the characteristics of both human answers and ChatGPT answers. This results in a set of 10 insights (vibes) which are listed in detail in Section B.\nIn Table 1 we show a summarization of the top 10 vibes found by VibeCheck along with the corresponding insight found by humans which align with each vibe meaning. We see that VibeCheck uncovers most of the same vibes as the human annotators, aside from (1) GPT fabricates facts and (2) GPT focuses on a literal interpretation of the question while humans address different aspects of the question and can infer hidden meaning. The inability to find these vibes is likely a weakness of our GPT proposer, as these vibes relate to the inherent weaknesses of GPT. Full table of VibeCheck outputs are located in Table 16."}, {"title": "DESCRIBING USER PREFERENCE ON CHATBOT ARENA", "content": "On April 18th 2024, Meta released their open-weight large language model Llama 3. On benchmarks like MMLU, Llama3-70b outperforms Claude-3-Sonnet and Gemini 1.5. It had even stronger results on Chatbot Arena , a popular platform for community-driven LLMs where users submit a prompt, receive responses from 2 anonymous models, and vote on which output they prefer. On this leaderboard, Llama3-70b is ranked similarly to the top proprietary models like GPT-4 and Claude3-Opus. This has led to speculation on whether there are qualitative properties of Llama that make it popular among users .\nWe obtained a subset of battles (pairwise comparisons) between Llama3-70b vs GPT and Llama3-70b vs Claude-3-Opus from the Chatbot Arena maintainers and ran VibeCheck on the combined pairwise battles between Llama3-70b VS GPT-4 and Claude3-Opus. We considered three settings: using the entire dataset, and using 2 subsets of the data: STEM prompts (including coding) and Writing prompts, which include creative writing, humanities questions, and general chatting. We obtain these"}, {"title": "WHAT DO DIFFERENT MODELS FOCUS ON WHEN SUMMARIZING?", "content": "We compare the summary styles of Cohere's Command X large Beta to TNLG v2 (530B) on the CNN/DailyMail dataset. While these models achieve a similar mean win rate on the HELM leaderboard, we see when using LLM as a preference judge, Command X has a win-rate of 71.12%. Looking at the top 5 vibes located in Figure 3, we find that (1) Command X often clearly states an introduction and conclusion while while TNLG utilizes choppy sentences without an either (2) Command provides specific examples or anecdotes to illustrate points and (3) Command is able to capture multiple viewpoints and emotional aspects of a story whole TNLG is more objective. We see in Table 16 that this these qualities are positively correlated with human preference, which may provide explanation as to to disparity between correctness metrics and preference metrics. Using these vibes we are able to achieve a model matching accuracy of 71.29% and a preference prediction accuracy of 61.42%."}, {"title": "HOW DO DIFFERENT LLM SOLVE MATH PROBLEMS?", "content": "Objective tasks like math have a single final answer, but the way a model explains its thought process varies across models. We run VibeCheck on the MATH dataset using chain-of-though prompting to discover how GPT-40 and Llama-405b differ in their thought process and presentation. To reduce the variance seen from incorrect examples, we run VibeCheck only on the questions where both models answered correctly and aim to discover why GPT-40 is favored in 76% of conversations.\nInspecting the top 3 vibes from VibeCheck found in Table 14, we find that Llama-3 structures its answers under markdown headings, uses a more conversational tone, tends to prefer more free from explanation of steps rather than the extensive latex used by GPT-40, and is often overly detailed in its steps, as shown below. When looking at the coefficients for the preference prediction model, we see that speaking in a formal tone and using frequent notation is positively correlated with human preference, while over explaining their thought process is negatively correlated with human preference. With these sets of vibes we are able to achieve a model matching accuracy of 97.09% and preference prediction of 72.79%."}, {"title": "How Do GPT-4V CAPTIONS DIFFER FROM GEMINI CAPTIONS?", "content": "Image captioning is one of the most popular use cases for Vision and Language models, but different captioning models often focus on different image properties. To understand these differences, we run VibeCheck on captions generated by GPT-4V and Gemini-1.5-Flash on 1000 COCO images. Examining the top 5 vibes found in Table 15, we find that GPT-4V uses more poetic language and structures its captions as a dynamic story, inferring the personality and emotions of the subjects in the image while Gemini sticks to more literal descriptions. The top 10 vibes generated by VibeCheck are able to achieve near perfect 99.13% model matching accuracy and 89.02% preference prediction accuracy. Although we compared the captions without the image in this experiment due to cost, the VibeCheck framework can be easily adapted to the multimodal setting."}, {"title": "LIMITATIONS", "content": "VibeCheck is the costly to validate, as each judge will have to evaluate each sample in \\(D_{validation}\\) on each vibe. In order for this to be feasible, our method uses relatively inexpensive models such as GPT-40-mini, but these judge models are often incorrect in their predictions, as shown in Figure 6. LLM judges also have biases , like favoring their own outputs, which may affect the scoring. It can also be hard to disentangle whether a certain vibe directly contributes to human preference/model matching or if there is another confounding variable. For instance, a model may have a vibe of being more detailed but the reason it is preferred is actually because it is more factually correct. Furthermore, the LLM-based vibe discovery process can often to produce similar axes of comparison and may not capture all relevant differences between models. This is particularly problematic when there's a significant discrepancy in model accuracy, as the discovered vibes may focus primarily on accuracy-related aspects. Lastly, running VibeCheck multiple times can lead to different vibes and different results, making it harder to reproduce findings exactly."}, {"title": "CONCLUSION", "content": "It may seem unconventional to focus on vibes instead of concrete metrics, but people often rely on the subtle nuances of communication to form their preferences and judgments about LLM outputs. VibeCheck provides a valuable addition to existing metrics for correctness by capturing these qualitative aspects that influence human preference. As the use of LLMs expands, we anticipate an increased focus on evaluating vibes to better align with user preferences. Moreover, this approach can be extended to other modalities, such as audio or visual content, and can be applied to compare any pairwise set of texts, making it a versatile tool for model evaluation. In future work we hope to explore extending this framework to compare a larger number of models and find more cost efficient ways of validating vibe utility."}, {"title": "EXPERIMENTAL DETAILS & DATASET STATISTICS", "content": ""}]}