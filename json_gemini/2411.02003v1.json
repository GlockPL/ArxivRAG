{"title": "Against Multifaceted Graph Heterogeneity via Asymmetric Federated Prompt Learning", "authors": ["Zhuoning Guo", "Ruiqian Han", "Hao Liu"], "abstract": "Federated Graph Learning (FGL) aims to collaboratively and privately optimize graph models on divergent data for different tasks. A critical challenge in FGL is to enable effective yet efficient federated optimization against multifaceted graph heterogeneity to enhance mutual performance. However, existing FGL works primarily address graph data heterogeneity and perform incapable of graph task heterogeneity. To address the challenge, we propose a Federated Graph Prompt Learning (FedGPL) framework to efficiently enable prompt-based asymmetric graph knowledge transfer between multifaceted heterogeneous federated participants. Generally, we establish a split federated framework to preserve universal and domain-specific graph knowledge, respectively. Moreover, we develop two algorithms to eliminate task and data heterogeneity for advanced federated knowledge preservation. First, a Hierarchical Directed Transfer Aggregator (HiDTA) delivers cross-task beneficial knowledge that is hierarchically distilled according to the directional transferability. Second, a Virtual Prompt Graph (VPG) adaptively generates graph structures to enhance data utility by distinguishing dominant subgraphs and neutralizing redundant ones. We conduct theoretical analyses and extensive experiments to demonstrate the significant accuracy and efficiency effectiveness of FedGPL against multifaceted graph heterogeneity compared to state-of-the-art baselines on large-scale federated graph datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Federated Graph Learning (FGL) has become an attractive research direction for distributed Graph Neural Network (GNN) optimization on isolated graph data without explicit information exposure [20]. Specifically, in FGL, several participants maintain their private graphs collected from different domains, and they aim to collaboratively train GNNs that can provide beneficial knowledge of raw graph data for particular downstream tasks, respectively.\nRecent FGL studies have achieved significant progress mainly in learning more effective graph representation to enhance downstream task prediction [11]. For example, FedSage+ [43] is proposed based on a standard FGL pipeline where subgraphs from different silos are independently distributed, and improve the node classification via missing edge generation. Despite these advancements, the difficulty of graph data heterogeneity remains paramount. Graphs inherently differ in terms of node and edge types, structural configurations, and other characteristics. Traditional graph learning methodologies often operate under the assumption of homogeneity, which can lead to biased or suboptimal models when applied to heterogeneous graph domains. Addressing graph data heterogeneity is crucial for FGL, as it enables the model to capture diverse intrinsic graph characteristics and enhances overall performance. Therefore, FedStar [29] investigates the data heterogeneity of graph features and shared knowledge, which attempts to achieve outperformance in a Non-Independent and Non-Identically Distributed (Non-IID) issue setting. In addition, FedLIT [38] detects link-type heterogeneity to decrease the harmful structure to allow more beneficial message passing. Generally, these works designed effective methods to tackle the graph data heterogeneity in terms of features and structures, where they demand participants to train their models for a consistent downstream task.\nHowever, while attempts have been made to address data heterogeneity, the critical issue of task heterogeneity remains unexplored. FGL participants originate from various sectors, each with its own set of tasks and data characteristics. For instance, healthcare providers may focus on patient data analysis, while financial institutions might prioritize fraud detection. Addressing task heterogeneity allows FGL to adapt to these varied needs, enhancing its applicability and relevance across different applications. Existing federated algorithms, which typically assume a uniform model structure across clients, cannot accommodate the need for diverse architectures in graph learning [28]. This limitation complicates parameters and knowledge sharing between clients working on different tasks, potentially causing inferior effectiveness and efficiency in collaborative optimization. To effectively optimize models against multifaceted heterogeneity, we necessitate a system not only to preserve common knowledge adaptable to various tasks but also to generalize across divergent graph data distributions. Therefore, the core challenge of this work is to simultaneously overcome the multifaceted heterogeneity for more effective and efficient FGL.\nTo address the challenge, we propose a Federated Graph Prompt Learning (FedGPL) framework to federally fine-tune graph models on heterogeneous tasks and data via an efficient prompt-based asymmetric knowledge transfer. Overall, we establish a split framework to simultaneously achieve a universal graph representing and personalized graph prompting to preserve global and local knowledge, respectively. Then, we design tailored algorithms to disentangle and address task and data heterogeneity. On the server side, we develop a Hierarchical Directed Transfer Aggregator (HiDTA) to extract and share asymmetrically beneficial knowledge among task-heterogeneous participants via personalized federated aggregation. On the client side, we devise a lightweight prompting module, called Virtual Prompt Graph (VPG), to adaptively generate augmented graph data by distilling more dominant information with minor data heterogeneity. We provide theoretical analyses that demonstrate the effectiveness in reducing task and data heterogeneity, as well as the significant reduction of memory and communication costs. Extensive experiments validate that FedGPL outperforms baseline methods across three levels of tasks against multifaceted graph heterogeneity on five datasets. Notably, we evaluate FedGPL in a typical large-scale FGL system consisting of 1 million node data. Our method achieves 5.3\u00d7 ~ 6.0\u00d7 GPU memory efficiency, 2.1\u00d7 ~ 3.7\u00d7 communication efficiency, and 1.3\u00d7 ~ 1.9\u00d7 training time efficiency. The efficiency superiority demonstrates its scalability for massive FGL participants and large-scale graph data.\nOur main contributions can be concluded as: (1) To our knowledge, our work is the first to study both task and data heterogeneity in FGL, addressing gaps overlooked by previous research. (2) We propose a federated graph prompt learning framework to effectively enable federated optimization of personalized models among task- and data- heterogeneous participants. (3) We develop an aggregation algorithm to deliver task-specific knowledge based on a transferability-aware hierarchy to asymmetrically enhance model performance. Moreover, we devise a virtual graph prompt to jointly highlight dominant graph structures and alleviate extensive data heterogeneity. (4) We theoretically analyze the mitigation of multifaceted heterogeneity by FedGPL. Besides, we conduct extensive experiments to prove the accuracy and efficiency superiority of FedGPL against FGL with multifaceted heterogeneity on federated graph datasets with million nodes."}, {"title": "2 PRELIMINARIES", "content": null}, {"title": "2.1 Notations and Problem Statement", "content": null}, {"title": "2.1.1 Graph notations.", "content": "Graph is a common data structure representing a set of objects and their relationships. We denote a graph as G = (V, E, X). Here V = {v_1, v_2, ..., v_{N_n}} is the node set with N_n nodes. E = {e_1, e_2,..., e_{N_e}} is the edge set with N_e edges that e_i = (v_{s_i}, v_{t_i}) is an undirected edge from node v_{s_i} to node v_{t_i}. X \u2208 R^{N_n\u00d7d_o} is the node feature matrix that i-th row vector x_i \u2208 R^{d_o} is the feature of v_i, where d_o is the node feature dimension. For graph learning, researchers usually treat three levels (i.e., node-level, edge-level, and graph-level) of tasks in different pipelines [27]. We formulate three levels of tasks in Appendix A.1."}, {"title": "2.1.2 Multifaceted heterogeneity.", "content": "This paper investigates an FGL scenario consisting of multifaceted heterogeneity in terms of task and data heterogeneity, which is defined below.\nDEFINITION 1. Task Heterogeneity. In FGL, task heterogeneity arises when participants aim to optimize models for different downstream tasks, including node-, edge-, and graph- level tasks. Therefore, task heterogeneity will be reflected by the difference of model structures and parameters. Specifically, we measure the heterogeneity \\Delta_a^{T} (\\theta_a, \\theta_b) \u2208 [0, 1] between a-th and b-th tasks as\n\\Delta_a^{T} (\\theta_a, \\theta_b) = \\begin{cases}1, & t_a \\neq t_b, \\\n\\frac{1 - \\exp(-\\|\\theta_a - \\theta_b \\|^2)}{1 + \\exp(-\\|\\theta_a - \\theta_b \\|^2)}, & t_a = t_b,\\end{cases}  (1)\nwhere \\theta_a, \\theta_b denotes the model parameters for the a -th and b -th tasks, \\overline{\\Theta} means the average values of \\Theta.\nDEFINITION 2. Data Heterogeneity. In FGL, data heterogeneity appears when participants keep divergent graph data generated from their domains. The data heterogeneity between graphs is reflected in the difference of their structures and features. Graph representations can incorporate graph structures and features in a high-dimensional vector. We utilize a popular graph readout function [39] to produce graph representations as H = \\{h_1,\\cdots,h_n\\} = G(V,E,X) and h_G = \\overline{H} = \\frac{1}{n} \\sum_{i} h_i, where n is the number of nodes of G and G is a trained GNN to represent nodes. Hence, we measure the data heterogeneity D(h_{G_i}, h_{G_j}) \u2208 [0, 1) between graphs G_i and G_j according to the distance of their representations h_{G_i} and h_{G_j} as\nD^{i,j}(h_{G_i}, h_{G_j}) = \\frac{1 - \\exp((h_{G_i} - h_{G_j})^2)}{1 + \\exp((h_{G_i} - h_{G_j})^2)}  (2)\nwhere (h_{G_i} - h_{G_j})^2 is the average value among d dimensions of (h_{G_i} - h_{G_j})^2."}, {"title": "2.1.3 Problem formulation.", "content": "PROBLEM 1. Federated Graph Learning against Multifaceted Heterogeneity. An FGL system consists of a server S and N clients \\{C_1, C_2,..., C_N\\} corresponding to each participant. Each client aims to learn a model F_i(\u00b7) on its dataset D_i, where t_i \u2208 \\{node, edge, graph\\} and 1 \u2264 i \u2264 N. Models and datasets may be heterogeneous among clients. We aim to optimize the parameters of all models to minimize the average loss as\n\\{\\theta_1,\\cdots, \\theta_N\\} = arg \\min_{\\theta_i}  \\sum_{i=1}^N L_i(F_i(D_i; \\theta)), (3)\nwhere L_i(\u00b7) denotes the loss function of the prediction model F_i(\u00b7)) parameterized with \u03b8_i on dataset D_i."}, {"title": "2.1.4 Threat model.", "content": "We assume all participants are semi-honest, strictly following the Federated Learning (FL) protocol, and only knowing about the knowledge they already keep or receive from others during interactions. We will clarify the objectives of participants, including clients and server: The client aims to obtain the structure and parameters of GNN from the server; the server aims to see the private data of the client."}, {"title": "2.2 Graph Prompt Learning", "content": "We formulates the pipeline of Graph Prompt Learning (GPL), which can enable effective optimization for downstream tasks to inherit knowledge of Graph Neural Network (GNN) and fine-tune on specific domains.\nWe first follow the existing work [27] to reformulate node-level and edge-level tasks as graph-level tasks to train three tasks in a consistent workflow illustrated in Appendix A.2. Then, GNN aims to produce representations by encoding input graphs. Existing works pre-train GNNs on large-scale graph data for efficient fine-tuning and knowledge reusing [25, 37, 45]. In this work, we leverage pre-trained GNNs in GPL for better performance. A GNN G reads an input graph G = (V, E, X) and outputs representations H as G(G) = H \u2208 R^{N_n\u00d7d}, where d is the dimension.\nSpecifically, we can insert a prompt into a graph from a source domain, i.e., source graph, to build a prompted graph, i.e., target graph. GPL follows three steps [27]: (1) Graph prompting: A graph prompting function f_p(\u00b7|\u00b7) is leveraged to transform a source graph G into a target graph G' as G' = f_p(G|P_G) where P_G is a specific prompt for G. (2) Graph representing: The GNN G produces a representation H of the target graph as H = G(G'). (3) Task prediction: A task head f_h(\u00b7) inputs H to predict the label y of the downstream task as y = f_h(H). Usually, the task head is learnable for more accurate downstream prediction. For example, the task head can be a linear classifier [27]."}, {"title": "3 METHODOLOGY", "content": "The sec will illustrate the framework, algorithms, and training. Specifically, we first propose the FedGPL framework to splitly enable privacy-preserving and personalized graph prompt tuning. Then, we develop two algorithms, including a hierarchical directed transfer aggregator and a virtual prompt graph, to eliminate task and data heterogeneity, respectively. Last, we present the training workflow based on algorithms tailored for the FedGPL framework."}, {"title": "3.1 Framework", "content": "For an FGL system with both task and data heterogeneity, learning a parameter-shared model for all participants as traditional federated learning is not applicable. Because it may cause poor generalization ability on different tasks and divergent data. Along with this idea, we propose a framework, namely Federated Graph Prompt Learning (FedGPL), to splitly preserve global and local knowledge by prompt-based privacy-preserving fine-tuning, illustrated in Figure 1. In light of Split Federated Learning [32], the common and domain-specific graph knowledge can be captured separately, allowing global generalizability and local personalization.\n(a) Architecture. Overall, FedGPL consists of a server and several clients corresponding to participants. First, we follow the GPL formulated in Section 2.2 to unify learning procedure of three levels of tasks. Second, we deploy a trainable GNN in the server to deliver the graph representing ability for heterogeneous clients. The GNN contains versatile graph knowledge, which can be efficiently fine-tuned to fit multiple domains in a federated system. Third, a task head and a graph prompt are personally optimized within each client for domain-specific understanding of its task and data.\n(b) Collaborative learning procedure. Based on the GPL pipeline, we describe how to activate forward- and backward- propagation with collaboration between the server and clients via communication. The communication messages between modules include four elements: featured prompted graphs and their representation in forward-propagation, and gradients of the task head and the graph prompt in back-propagation. On the one hand, the forward-propagation procedure follows: (1) A client prompts an input graph and sends it to the server. (2) The server generates the representation of the received prompted graph by GNN and then sends it back to the client. (3) The client takes the prompted graph representation as input of the task head for prediction. On the other hand, the back-propagation procedure follows: (1) A client computes gradients of the task head and sends them to the server. (2) The server computes gradients of the prompted graph and sends it back to the client. (3) The client computes the gradients of the graph prompt. After two-sided propagation, GPL modules can be optimized.\n(c) Differentially privacy preservation. The communication messages remain privacy leakage risks. Specifically, the server may obtain the raw features of prompted graphs from clients by attacks such as DLG [46]. Therefore, to further protect the client-side data privacy, we utilize a Differential Privacy (DP) technique, Laplacian noise, to add it to embeddings and gradients as\nf(X;\\epsilon) = X + \\eta, \\eta \\sim exp(-\\frac{1}{\\lambda}), \\lambda = \\frac{V(X)}{\\epsilon}, (4)\nwhere X is the embedding or gradient to be protected, V(X) determines the variation of the protected data, and \\epsilon is the privacy scale parameter.\nIn summary, based on the split strategy and DP techniques, we build a privacy-preserving federated framework to allow GPL to prevent private data from being leaked or inferred. Besides, state-of-the-art pre-trained GNNs such as GROVER [25] and DVMP [45] have approximately 100 million parameters. Our split strategy makes clients without heavy memory burden to store GNNs, especially when applying FedGPL among resource-constrained devices [4]."}, {"title": "3.2 Algorithms", "content": "The existing federated algorithms [14, 19] developed on FedAvg [23] can only be applied in traditional non-IID settings. However, they still fail in FGL with task and data heterogeneity. Because these algorithms strictly require a consistent model structure across clients. The requirement is infeasible in a task-heterogeneous federation that is more general in FGL. How to tackle the joint heterogeneity in this scenario for more effective federated optimization remains a significant difficulty.\nTherefore, we propose an aggregating and a graph prompting algorithms to eliminate damage from graph task and data heterogeneity by a privacy-preserving and collaborative optimization. Specifically, we first devise a federated aggregation algorithm in the server side to transfer asymmetrically beneficial knowledge between different tasks. Moreover, we equip each client with an graph prompt to retain useful local knowledge with less data divergence."}, {"title": "3.2.1 Server-side federated aggregation.", "content": "Previous methods such as FedAvg mostly aggregate local models based on an assumption that the heterogeneity between clients affects symmetrically. However, cross-task knowledge transfer is asymmetric, which means that the effectiveness of transferring a task to another may differ from the inverse. In fact, measuring the optimal directed knowledge transfer and activating it with learning benefits are both non-trivial problems in task-heterogeneous FGL. Therefore, we develop the Hierarchical Directed Transfer Aggregator (HiDTA) to federally enhance prediction performance of task-heterogeneous clients via directional knowledge transfer that is hierarchically determined by a proposed metric for transferring availability.\n(1) Transferability Definition: a metric for knowledge transfer availability. First, we define a metric, i.e., transferability, to model the directed heterogeneity between clients in Definition 3. For a source client and a target client from their corresponding clients, transferability reflects how much benefit of the target client if it uses the source client's model on the target client's data.\nDEFINITION 3. Transferability. We define the transferability \\tau_{ab} from model a to model b as\n\\tau_{a\\rightarrow b}^{(l)} = \\frac{\\|\\theta_a^{(l)} - \\theta_a^{(l)'}\\| - \\|\\theta_a^{(l)} - \\theta_b^{(l)'}\\|}{\\|\\theta_a^{(l)} - \\theta_a^{(l)'}\\|}, (5)\nwhere l_a is the parameters of a model, \\theta^{(l)} is the parameters at l-th optimization step, \\theta^{(l)'} is the estimated optimal parameters computed on local data for \\theta^{(l)} at the next step.\nTransferability fits the property that knowledge transferring is often asymmetric, i.e., T_{ij} \\neq t_{j\\leftarrow i}, which means that it is more convincing to measure the transferability by a directed value.\n(2) Hierarchical transferability evaluation. Next, we propose to evaluate pairwise transferability to understand the task heterogeneity based on the decoupled hierarchy of clients. Specifically, we construct task-models to identify task-specific knowledge by fused perception on corresponding models. For each task, we measure the multi-dimensional distances of parameters between client-models and task-models. Based on task-models and calculated distances, we extrapolate the transferability for any two client-models by\n\\tau_{ab}^{(l)} = (1 - \\sigma(\\|\\theta_a^{(l)} - \\theta_{p_a}^{(l)}\\|)) \u00b7 (1 - \\sigma(\\|\\theta_b^{(l)} - \\theta_{p_b}^{(l)}\\|)) \u00b7 \\tau_{p_a\\rightarrow p_b},  (6)\nwhere a and b are two client-models, p_a and p_b are a and b's task-models, and \u03c3(\u00b7) is a normalization function (e.g., Sigmoid(\u00b7) or ReLU(\u00b7)).\n(3) Asymmetric knowledge transfer via model aggregation. Last, we weighted aggregate models to transfer knowledge from source clients to target clients based on normalized transferability values. The model parameters of a client i can be compute as\n\\theta_i^{(l+1)} = \\frac{\\sum_{j=1}^N \\tau_{ji}^{(l)} \\theta_j^{(l)}}{\\sum_{j=1}^N \\tau_{ji}^{(l)}} ,  (7)\nwhere \\theta_i^{(l+1)} = (\\theta_p^{(l+1)}, \\theta_h^{(l+1)}) is the parameter set of f_p(\u00b7) and f_h(\u00b7) of client i, and N is the number of participating clients. In short, HiDTA assists positive knowledge transfer across clients and impedes harmful attacks aroused by task heterogeneity, which are both extracted based on the hierarchical transferability assessment. The success of tackling task heterogeneity will be theoretically analyzed in Section 4.1."}, {"title": "3.2.2 Client-side graph prompting.", "content": "Existing works have proposed to construct universal graph prompts for any level of tasks by incorporating task-specific properties [7, 27, 47]. However, there is an underexplored conflict between distilling specific knowledge and eliminating corresponding data heterogeneity. On the one hand, prompting with more task-specific information on graphs brings more severe data heterogeneity. On another hand, extracting more unique patterns requires more distinguished prompted features. Besides, existing graph prompting strategies remain inefficient due to heavy models carrying prior knowledge, which is unsuitable for large-scale FGL scenarios. Therefore, we devise Virtual Prompt Graph (VPG), to model task-specific knowledge as well as minimize data heterogeneity by adaptively indicating intellective graph structures.\n(1) Prompting module construction. First, we insert a positive structure to capture distinctive information in the particular domain. Second, we construct a negative structure to neutralize noisy nodes and edges in terms of relatively useful graph structures. Simultaneously, VPG can assist the GNN to generate more effective yet less heterogeneous graph representations for tailored clients.\nThen, we define VPG in Definition 4, which implies how we can transform a source graph, i.e., the original graph, into a target graph, i.e., the prompted graph, to be input into the GNN.\nDEFINITION 4. VPG. For a graph G = (V, E, X), we denote a VPG as \\hat{G} = ((\\hat{V}^+, \\hat{V}^-), (\\hat{E}^+, \\hat{E}^-), (\\hat{X}^+, \\hat{X}^-)). It is satisfied that \\hat{V}^+ \u2229 \\hat{V}^- = \\emptyset, where \\hat{V}^+ = \\{v_{N_v+1}, v_{N_v+2}, ... ,v_{N_v+ | \\hat{V}^+ |}\\} is the prompted node set and \\hat{V}^- = \\{v_{i_1}, v_{i_2}, ... , v_{i_{|\\hat{V}^-|}\\}\\}, v_{i_1}, v_{i_2}, ... \\in V is the prompted anti-node set. And it is satisfied that \\hat{E}^+ \u2229 \\hat{E}^- = \\emptyset, where \\hat{E}^+ = \\{e_{N_e+1}, e_{N_e+2}, ...,e_{N_e+ | \\hat{E}^+ |}\\} is the prompted edge set and \\hat{E}^- = \\{e_{j_1}, e_{j_2}, ..., e_{j_{|\\hat{E}^-|}\\}\\}, e_{j_1}, e_{j_2}, ... \\in E is the prompted anti-edge set.\n(2) Learning for adaptive prompting on graphs. We define VPG-based prompt function as \\tilde{G} = f_p(G|\\hat{G}), where f_p(\u00b7|\u00b7) follows \\tilde{V} = V \u222a \\hat{V}^+ \\setminus \\hat{V}^-, \\tilde{E} = E \u222a \\hat{E}^+ \\setminus \\hat{E}^-, X = \\{x_i|v_i \u2208 \\tilde{V}\\}. Then, we propose a generative algorithm to build specific VPGs. Specifically, we construct a candidate set of k' nodes, whose features are predefined (default) or learnable. We connect them to the source graph if v_i and v_j satisfies Sigmoid(x_i\u00b7 x_j) < \u03b3, where \u03b3 is a threshold. After that, we have G' = (V', E', X'). We generate node significance as w_i = X' \u00b7 p'/\\|p'\\|, where p' \u2208 R^d is a learnable vector. We also generate edge significance between i-th and j-th node as W_{e,i,j} = \\|w_{n,i} - w_{n,j}\\|. The node features will be also processed by x_i = x_i \\tanh(w_n). Next, we determine the node significance borderline w_n as the k_n-th highest significance score among V', and the edge significance borderline W_e in a similar way. Note that k_n = \u03b1_n |V| and k_e = \u03b1_e |E| and we manually decide \u03b1_n and \u03b1_e. Last, we can create a VPG \\hat{G} by constructing the prompted (anti-) node set and (anti-) edge set, by letting \\hat{V}^+ = \\{v_{w_n > w_n}\\} \\setminus V, \\hat{V}^- = \\{v_w_n < w_n\\}, \\hat{E}^+ = \\{e_{W_{e,i,j} > W_e|w_{n,i} \\ge w_n, W_{n,j} \\ge w_n}\\} \\setminus E, and \\hat{E}^- = \\{e_{W_{e,i,j} < W_e|w_{n,i} < w_n, W_{n,j} < w_n}\\}.\nIn conclusion, VPG indicates useful and useless graph structures to simplify local graph data to reduce data heterogeneity with better representation production, which will be discussed in Section 4.2."}, {"title": "3.3 Training", "content": "We present the training workflow of FedGPL. First, we allow clients to independently optimize their kept modules including prompting and task head modules. Second, the server-side GNN can be optionally optimized. It can be pre-trained or randomly initialized. After that, its parameters will be fine-tuned or frozen during optimization. In this work, we choose to fine-tune a pre-trained GNN by default, because pre-training and fine-tuning GNNs can efficiently utilize common graph knowledge and fit on specific domains. Detailedly, we take the node classification task as an example in Appendix A.4. Specifically, we first initialize the client-side parameters. For each training round, we enable each client in parallel to individually train their modules. The client cooperates with the server for split and privatized forward- and back- propagation. Then the server globally calculates transferability between clients and coordinates them to locally update parameters. The training process will terminate after parameters in clients are converged."}, {"title": "4 THEORETICAL ANALYSIS", "content": "This part investigates FedGPL about (1) the reduction of task heterogeneity, (2) the elimination of data heterogeneity, and (3) the complexity of memory and communication."}, {"title": "4.1 Task Heterogeneity Analysis", "content": "We analyze the effect of our aggregation algorithm, HiDTA, on task heterogeneity by quantifying the heterogeneity values based on Definition 1. We theoretically propose Theorem 1 to demonstrate the success of HiDTA in eliminating task heterogeneity, which is proved in Appendix A.5.\nTHEOREM 1. For a -th and b-th clients in FedGPL, we denote their estimated optimized parameters at l-th step as \\theta_a^{(l+1)'}, \\theta_b^{(l+1)'} (aggregated) or \\theta_a^{(l)', \\theta_b^{(l)'} (non-aggregated). It is satisfied that\n\\Delta_a (\\theta_a^{(l+1)}, \\theta_b^{(l+1)'}) \u2264 \\Delta_a (\\theta_a^{(l)'}, \\theta_b^{(l)'}),  (8)\nwhen their bidirectional transferability are positive. Hence, the task heterogeneity can be reduced by HiDTA."}, {"title": "4.2 Data Heterogeneity Analysis", "content": "We quantify data heterogeneity influenced by VPG based on Definition 2. We compute the theoretical values before and after prompting and propose Theorem 2 which can be proved by Appendix A.6.\nTHEOREM 2. For the a -th and b -th clients in FedGPL, graph representations from two clients are h_{G_a}, h_{G_b}, and h_{G_a}', h_{G_b}' for with and without prompting methods, respectively. And their graph data following uniform distributions of h_{G_a} ~ U^a[0, \u03b7_a] and h_{G_b} ~ U^b[0, \u03b7_b], respectively. We ensure\nE_{\\substack{h_{G_a}' ~ U^a \\ h_{G_b}' ~ U^b}} [\\Delta(h_{G_a}', h_{G_b}')] \u2264 E_{\\substack{h_{G_a} ~ U^a \\ h_{G_b} ~ U^b}} [\\Delta(h_{G_a}, h_{G_b})], (9)\nwhen \u03b1_n \u2248 \u03b1_b. Thus, the data heterogeneity can be reduced by VPG."}, {"title": "4.3 Efficiency Analysis", "content": "4.3.1 Memory efficiency. We assume the input graph has N nodes and M edges, the GNN has L layers with the maximum dimension of a layer being h, and the task head has l layers. Here we compute the theoretical storage complexity. The client has to store the whole GNN in the usual federated prompting setting. Hence, we take a popular GNN variant, GAT [34], as an example of the GNN, the storage complexity will be O(LKh^2 + LKh + Kh). By applying our split strategy, each client only needs to store the prompt and a task head layer, where the storage complexity is O(h + lh)."}, {"title": "5 CONCLUSIONS", "content": "This paper investigated the multifaceted heterogeneity of graph task and data in FGL, which is unexplored in previous studies. We first construct a split framework for the separate preservation of global and local knowledge. Moreover, we developed HiDTA, a federated aggregator to hierarchically share asymmetrically beneficial knowledge with task heterogeneity elimination. Next, we devise a graph prompt that is adaptively generated to emphasize informative structures and denoise useless ones to improve representation efficiency. We provide theoretical analysis to prove the successful mitigation of the graph task and data heterogeneity. We also conduct extensive experiments on our approach and baseline methods to show the accuracy and efficiency superiority of FedGPL on large-scale datasets with millions of nodes."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 Graph Learning Tasks", "content": "Here we define three levels of graph learning tasks.\n(1) Node-level Task. In a graph G = (V, E, X), given a node v_i \u2208 V with label y_i, the node-level task aims to learn a model F_{node}(\u00b7) satisfying F_{node} (v_i, G) = y_i. The node-level dataset is D_{node} = \\{\u00b7\u00b7\u00b7, ((v_i, G), y_i), \u2026\u2026\u2026 \\}.\n(2) Edge-level Task. In a graph G = (V, E, X), given two nodes v_i, v_j \u2208 V with a label y_{i,j}, the edge-level task aims to learn a model F_{edge}(\u00b7) satisfying F_{edge} (v_i, v_j, G) = y_{i,j}. The edge-level dataset is D_{edge} = \\{\u2026\u2026, ((v_i, v_j, G), y_{i, j, \u00b7\u00b7\u00b7 \\}.\n(3) Graph-level Task. For a graph G = (V, E, X) with a label y, the graph-level task aims to learn a model F_{graph}(\u00b7) satisfying F_{graph}(G) = y. The graph-level dataset is D_{graph} = \\{\u00b7\u00b7, ((G_i), y_i), \u2026\u2026\u2026 \\}."}, {"title": "A.2 Graph Inducing", "content": "To align three basic graph learning tasks, we follow the existing work [27] to reformulate node-level and edge-level tasks as graph-level tasks, respectively, by transforming their inputs into induced graphs, a \u03ba-ego network. For node-level tasks, an induced graph includes a node and its neighbors within \u03ba hops. For edge-level tasks, an induced graph includes two nodes and their neighbors within k hops.\nIn our experiments, we follow [27] construct datasets. For edge-level tasks, we specifically choose samples where the endpoints have identical labels, setting the label of the edge-level sample to match that of its endpoints. For the graph-level tasks, we select the majority label of the subgraph nodes as the graph-level sample's label."}, {"title": "A.3 Differential Privacy", "content": "Here we define the Differential Privacy (DP) [36] for information privatization.\nDEFINITION 5. Differential Privacy. A mechanism M : D \u2192 R that maps domain M to range R meets (\u03b5, \u03b4)-differential privacy if it satisfies:\nPr[M(d) \u2208 S] \u2264 e^\u03b5 Pr [M (d') \u2208 S] + \u03b4 (10)\nwhere d, d' \u2208 D are two adjacent inputs, S \u2286 R are any subset of the mechanism's outputs."}, {"title": "A.4 Training Algorithm", "content": "Here we take the node classification task as an example to illustrate the training pipeline in Algorithm 1. We set the GNN as a pre-trained and learnable model, which will be optimized in the server after aggregation."}, {"title": "A.5 Proof of Theorem 1", "content": "PROOF. Basically, we suppose that FedGPL consists of two participants as a-th and b-th clients. Their local parameters are \\theta_a^{(l)} and \\theta_b^{(l)} at l-th step. We can estimate the optimal learning directions by local data as \\theta_a^{(l+1)'} and \\theta_b^{(l+1)'}. ACCording to Definition 3 we can calculate pairwise transferability as T_{a\u2190a}, T_{a\u2190b}, T_{b\u2190a}, T_{b\u2190b}. Moreover, based on Equations 5 and 6, we can calculate their parameters after federated aggregation as\n\\theta_a^{(l+1)} = \\frac{T_{aa}}{T_{aa}+T_{a\\rightarrow b}} \\theta_a^{(l)} + \\frac{T_{a\\rightarrow b}}{T_{aa}+T_{a\\rightarrow b}}, (11)\n\\theta_b^{(l+1)} = \\frac{T_{ba}}{T_{ba}+T_{b\\rightarrow b}"}]}