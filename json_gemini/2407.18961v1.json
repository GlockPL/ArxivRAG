{"title": "MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains", "authors": ["Guoli Yin", "Haoping Bai", "Shuang Ma", "Feng Nan", "Yanchao Sun", "Zhaoyang Xu", "Shen Ma", "Jiarui Lu", "Xiang Kong", "Aonan Zhang", "Dian Ang Yap", "Yizhe zhang", "Karsten Ahnert", "Vik Kamath", "Mathias Berglund", "Dominic Walsh", "Tobias Gindele", "Juergen Wiest", "Zhengfeng Lai", "Xiaoming Wang", "Jiulong Shan", "Meng Cao", "Ruoming Pang", "Zirui Wang"], "abstract": "Recent advances in large language models (LLMs) have increased the demand\nfor comprehensive benchmarks to evaluate their capabilities as human-like agents.\nExisting benchmarks, while useful, often focus on specific application scenar-\nios, emphasizing task completion but failing to dissect the underlying skills that\ndrive these outcomes. This lack of granularity makes it difficult to deeply discern\nwhere failures stem from. Additionally, setting up these environments requires\nconsiderable effort, and issues of unreliability and reproducibility sometimes arise,\nespecially in interactive tasks. To address these limitations, we introduce the\nMassive Multitask Agent Understanding (MMAU) benchmark, featuring com-\nprehensive offline tasks that eliminate the need for complex environment setups.\nIt evaluates models across five domains, including Tool-use, Directed Acyclic\nGraph (DAG) QA, Data Science and Machine Learning coding, Contest-level\nprogramming and Mathematics, and covers five essential capabilities: Understand-\ning, Reasoning, Planning, Problem-solving, and Self-correction. With a total of\n20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU\nprovides a comprehensive framework for evaluating the strengths and limitations\nof LLM agents. By testing 18 representative models on MMAU, we provide\ndeep and insightful analyses. Ultimately, MMAU not only sheds light on the\ncapabilities and limitations of LLM agents but also enhances the interpretability\nof their performance. Datasets and evaluation scripts of MMAU are released at\nhttps://github.com/apple/axlearn/docs/research/mmau.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in the field of AI have been marked by significant progresses in the development\nof LLMs. Particularly, one promising direction along this evolution is the ability of LLMs to perform\nas human-like agents [1], i.e., understand complex contexts, reason and plan with complicated logic [2\u2013\n5], make decisions, and utilize tools effectively [6\u20138]. Consequently, the need for comprehensive\nbenchmarks that evaluate LLMs as intelligent agents has become more and more important.\nWhile existing benchmarks [9\u201312] evaluate LLM agents by focusing on specific application scenarios\nand task completion, they struggle to reveal the underlying capabilities driving these outcomes.\nFor example, as shown in Fig. 3, when an LLM encounters a complex math problem, multiple\ncapabilities are required to solve it. By emphasizing task completion, existing benchmarks often\nobscure whether a failure stems from a lack of comprehension, reasoning, or calculation error.\nConsequently, these evaluation methods blur the distinctions between different types of failures,\nhindering our understanding of where the error originates from and limiting our ability to gain deeper\ninsights into the model's capabilities and make targeted improvements. Additionally, some tasks\nin existing benchmarks require considerable effort to set up the environments, making a thorough\nevaluation both expensive and challenging. Furthermore, we observe that tasks, especially interactive\nones, are sometimes less stable and reproducible due to the stochasticity of the environment feedback\nduring the evaluation process. This randomness can make it difficult to obtain consistent evaluation\nresults and draw solid conclusions.\nTo address such limitations, we introduce the Massive Multitask Agent Understanding (MMAU)\nbenchmark. We develop MMAU by identifing five essential capabilities: Understanding, Reasoning,\nPlanning, Problem-solving and Self-correction across five domains: Tool-use, Directed Acyclic Graph\n(DAG) QA, Data Science & Machine Learning coding, Contest-level programming, and Mathematics.\nAs a result, MMAU comprises a total of 3,220 distinct prompts gathered from diverse data sources.\nThese include our in-house human annotations for tool-use, as well as rewritten and curated prompts\nfrom open-source datasets such as CodeContest [13], Kaggle [14], and DeepMind-Math [15]. Based\non this dataset, we designed 20 tasks across 64 subjects, offering a comprehensive benchmark. To\navoid the complexities of environment setup and issues of unreliability, all tasks in MMAU are\nperformed on our 3K static dataset to eliminate potential issues related to environment instability."}, {"title": "3 The MMAU Benchmark", "content": "To introduce MMAU, we will start with an overview of all included capabilities 3.1. We will then\nprovide detailed explanations of how each task was designed and how the dataset across different\ndomains was constructed 3.2."}, {"title": "3.1 Capabilities in MMAU", "content": "Below, we introduce the capability definitions and the key tasks used to evaluate them. A complete\ntask-capability mapping can be found in the Appendix 6.\nUnderstanding is a fundamental capability required of an intelligent agent. In MMAU, we evaluate\nan agent's understanding in different aspects, including: complex instruction following, user intent\nunderstanding, statistics parsing, and visual grounding.\nReasoning and Planning reflect an agent's thought process and ability to infer logically from complex\nfactors. Although reasoning and planning has been recognized in many works, it is often described\nas a general ability, compounded with other skills, which limits deep investigation. In MMAU,\nwe address this challenge with the task planner-shift, designed to decompose reasoning and\nplanning capabilities from other factors. Unlike standard end-to-end evaluations, planner-shift\ndivides the solution generation into two stages. In stage 1, a planner model generates a high-level\nplan, providing a strategy to solve the given problem without hinting at the final solution. In stage\n2, a solver model is given the original problem along with the plan to solve it. This approach\nisolates the planning and reasoning processes from problem-solving. To test planning and reasoning\ncapabilities, we vary only the planner model while using the same solver model, ensuring that\nperformance differences reflect the planning and reasoning capabilities. The task design diagram is\nshown in Figure 4.\nProblem-solving focuses on measuring an agent's ability to successfully implement or execute a\ntask, assuming it has already understood and planned the strategy well. To address this, we design a\ntask called solver-shift, similar to planner-shift, which also performs a two-stage generation.\nHowever, solver-shift keeps the planner model constant and varies only the solver models\nto reflect differences in problem-solving skills, as shown in Figure 4. In MMAU we use the tasks of\nplanner-shift and solver-shift in domains of Contest-level coding 3.2.3 and Math 3.2.4.\nSelf-correction is another core capability for an intelligent agent. It reflects the agent's ability\nto identify errors, learn from its environment and past behaviors, and correct itself to eventually"}, {"title": "3.2 Dataset Construction", "content": "The construction of MMAU encompasses both breadth and depth of data, as illustrated in Table 4.\nOur dataset is constructed from heterogeneous sources: 1) our in-house tool-use data, used for tasks\nunder tool-use and DAG-QA; 2) Kaggle [14] datasets, which we rewrote to design tasks for DS & ML\ncoding; 3) CodeContest [13], used for tasks under contest-level coding; and 4) DeepMind-math [15],\nused for math tasks. MMAU involves curating and rewriting these data sources. In the following\nsection, we will explain how we leveraged these source data to construct MMAU."}, {"title": "3.2.1 Tool-Use", "content": "We curated an in-house dataset for tool-use with conversation trajectories following the standard\ntool-use (a.k.a. function-calling) protocol. We select from a subset of RapidAPI Hub 3 functions and\nask human annotators to create realistic scenarios with user queries, ground truth function calls and\nactual function returns from the RapidAPI endpoints. In total, our in-house tool-use dataset consists\nof 409 single-step (task:single-tool-use) and 258 multi-step tool-use conversations (task: multi-turn\nmulti-tool-use). Out of the 409 single-step tool use conversations, 225 require making parallel tool\ncalls (task: parallel-tool-use). Figure ?? shows some of these examples. We adapt this dataset for\nthe following tasks.\nTask: Tool-use Benchmarking agent tool-use following the standard protocol requires an interactive\nenvironment. To simplify the evaluation process, we instead evaluate the model's response at each\nassistant turn (i.e., where a function call is expected), conditioning on the ground-truth versions of all\nprevious user or assistant turns. For evaluation, we check if the model's tool call matches that of the\nground truth, i.e. calling the same function and the same parameters.\nTask: DAG QA In this task, a user presents a set of requirements to which the LLM must respond by\nselecting and ordering a sequence of tool invocations from multiple choices provided. This design\nexamines whether the model can identify the relevant tools and deduce the correct dependencies\nbetween them. The prompt enumerates the possible tool use orderings from which the LLM agent is\nasked to pick one, and the label is derived from the ground truth function call sequence. For example,\nwe transform the multi-step example in Figure ?? into a DAG QA task as shown in Figure 14\nTask: Tool-use Self-correction From the tool-use dataset described above, we derive two classes of\nerrors to test the model's self-correction ability:\nTemporary error simulates a tool that is temporarily unavailable. From the ground truth messages\n[user queries, tool-calls and tool responses], we substitute a random temporary error (e.g. \"429:\ntoo many requests\u201d, \u201c504: gateway timeout\") in place of the tool response.\nIncorrect call simulates a previous tool call or response containing an error. We mutate the\nground truth tool-call to be incorrect by changing the arguments or the function name, issue the\nmodified call, and save the updated tool response. Given the message history of [user queries, mutated\ncalls, updated tool responses], the model is expected to retry with the correct call.\nThe evaluation metric is the exact match accuracy of the function name and arguments against the\nground truth.\""}, {"title": "3.2.2 Data Science and Machine Learning", "content": "We leverage the Meta Kaggle Code dataset [14] and curate\n28 Python notebook-style conversations, with 123 conver-\nsation turns. Each turn begins with a user request for code\ngeneration. Among all requests, 83 requests expect text-\nbased outputs from code and 40 requests expect image\noutputs. Due to the open-ended nature of code generation,\nwe created multiple-choice questions that require infor-\nmation from successful code execution to fully address,\nresulting in 207 text-based questions and 121 image-based\nquestions. Figure 5 shows an example turn with multiple\nchoice questions. We report QA accuracy as the main\nmetric and vary the combination of code model and QA\nmodel to produce different evaluation settings.\nTask: E2E Code Generation and QA In this setting, we\naim to gauge the overall capability. the evaluated model\nis responsible for both code generation and QA.\nTask: Code Generation and GPT-4 QA In this setting,\nwe isolate the code generation capability of the model. Af-\nter generating code from the evaluated model, we adopt a\nstrong multimodal model (GPT-4 [41]) to serve as control\nand perform QA based on code execution outputs.\nTask: QA from Oracle Code In this setting, we specif-\nically focus on the textual and visual understanding pro-\nficiency of the model decoupled from code generation.\nWe obtain oracle output by executing ground truth code\nimplementation and then pass to the evaluated model to\nperform QA.\nTask: DS & ML Self-Correction This setting is similar\nto the E2E setting, however, whenever code execution\nfails, we use the execution error message to prompt an\nadditional code generation turn."}, {"title": "3.2.3 Contest-Level Coding", "content": "For contest-level coding problems, we select 261 prob-\nlems from the Valid and Test splits of the CodeContests\ndataset [13] which includes competitive programming\nproblems. We adapt these 261 problems for the following\ntasks.\nTask: E2E Standard In this task, models are challenged\nwith a variety of coding problems. The effectiveness of the\nsolutions is measured by executing the code against all predefined test cases [13]. All CodeContests results reported in this paper are based\non pass@K (K=5) accuracy.\nTask: Planner-shift and Solver-shift As introduced in Section 3.1, we use thses two tasks to\nextensively measure the agent's capability in planning and problem-solving, respectively. We evaluate\nboth of these tasks by generating K Python code solutions and verifying their pass rate.\nTask: Problem Parsing Unlike the other tasks, this task does not require the model to write or\nexecute any code. Instead, given a problem statement and associated test cases, the model is only\ntasked with predicting the outputs for these test cases. An example is shown in Figure 15. Our\nrationale is that if a model truly grasps the problem, including its complex instructions and user intent,\nit should be able to accurately predict the outputs based on its understanding alone. We use match\naccuracy as the evaluate metric.\nTask: CodeContest Self-correction For the E2E standard task above, we collect the error messages\nof each candidate solution if it does not pass some test cases, including 4 types of errors: empty"}, {"title": "3.2.4 Mathematics", "content": "The source data in the domain of math, derived from DeepMind-Math [15], consists of 1,000 carefully\ncurated math problems spanning 56 subjects, including calculus, geometry, statistics, and etc.\nTask: E2E Standard We adhere to standard protocol by incorporating a Chain-of-Thought (CoT) [3]\ninto the prompt to generate the answers end-to-end, and using accuracy as the evaluation metric.\nTask: Planner-shift and Solver-shift As introduced in Sec. 3.1, we use the two tasks to assess\nthe model's planning and problem-solving abilities in a two-stage manner, avoiding confounding\ninfluences from other capabilities. The prompts used for each task can be found in the Appendix B.\nTask: Comprehend+. To better isolate and assess the understanding capability without excessive\ninterference from other skills, we have devised a new task named Comprehend+. Our hypothesis is\nthat problems that are straightforward mathematically but complex in their descriptions rely more\nheavily on understanding capabilities. To test this, we first selected a subset containing only the\nmathematically simpler problems, and then use an LLM to create new math problems that feature\nmore complex descriptions or harder problem statements but retain the same underlying mathematical\nconstructs from each data sample. A rewritten math problem example is shown in Fig. 13. After\ncuration and verification, we finalize 676 newly created problems for Comprehend+. Please refer to\nAppendix A.5 for details of dataset creation."}, {"title": "4 Evaluation", "content": "We comprehensively evaluate 18 models on MMAU. All evaluation model details are listed in Table 5.\nFor easier reading, the main paper presents only the aggregated evaluation results. For the evaluation\nresults over all 20 tasks, please refer to Appendix C."}, {"title": "4.1 Domain-centric Evaluation and Analysis", "content": "As shown in Table 2, there is a clear performance gap between API-based commercial models and\nopen-source models across all evaluation domains. Among the commercial models, the GPT-4 family"}, {"title": "4.2 Capability-centric Evaluation and Analysis", "content": "As introduced in Sec.3.1, we designed tasks to decompose core capabilities from standard evaluations,\nallowing MMAU to offer a unique dimension of evaluation. Each capability includes tasks spanning\ndifferent domains. To provide overall capability-centric evaluation results for each model, we\naggregate tasks under each capability using a weighted average. Detailed task-capability mappings\nand the calculation method can be found in Appendix A. The overall capability-centric evaluation\nresults are shown in Table 3.\nNotably, for the capability of Understanding, GPT-40 significantly outperforms other models, demon-\nstrating its superior capability in handling long contexts, complex user instructions, and capturing\n(sometimes implicit) user intents. Additionally, GPT-4, Gemini-1.5-pro, and Claude3-Opus also\nexhibit reasonably strong understanding capabilities. For the capabilities of Reasoning and Planning,\nthe GPT-4 family shows the strongest performance. When examining the capability of Problem-\nsolving, the performance gap is not significantly large. This trend suggests that when provided with\n\"oracle\" plans, solving a task may be less challenging. While models' problem-solving capabilities\nvary, most can perform these tasks reasonably well, indicating that this capability may be more\nuniversally achievable among different models. On the contrary, for Self-correction, we observe a\nsignificant gap among models. Among open-source models, aside from Mixtral-8x22B, others do\nnot seem to possess the skill to reflect on and correct their own errors effectively. These evaluation\nresults highlight that self-correction is a critical capability needing further research and development\nto advance the field."}, {"title": "5 Analysis and discussion", "content": "How does planning impact the performance? One interesting finding emerges from the results\nof our designed tasks, Planer-Shift and Solver-Shift on Math. As shown in Table 7, we find"}, {"title": "6 Conclusion", "content": "In this paper, we introduce the Massive Multitask Agent Understanding (MMAU) benchmark. By\nevaluating models based on both application scenarios and fundamental capabilities, MMAU provides\na comprehensive and in-depth test bed for reliable and thorough studies. By designing 20 tasks\nto decompose capabilities beyond standard evaluation benchmarks, MMAU offers more granular\ninsights into the strengths and limitations of these models.\nLimitations and future work. The current scope of MMAU, while broad, does however not\nencompass all possible domains relevant to LLM agents, such as interactive environments which are\nalso critical yet challenging. Future iterations of MMAU should aim to include interactive tasks to\nprovide a more holistic evaluation. This expansion will require the development of reliable, stable,\nand user-friendly interactive environments. Moreover, as we expand to include more domains, it\nwill be essential to incorporate additional capabilities such as retrieving, memorizing, sequential\ndecision-making, etc. Our current approach to capability decomposition, though insightful, still faces\nchallenges in disentangling compound capabilities. Future research should focus on developing more\neffective methods for decomposing and evaluating these capabilities to further refine the benchmark.\nEthics and Societal Impacts. Research on LLM agents must consider potential ethical concerns\nand negative societal impacts. The MMAU benchmark aims to provide a thorough and transparent\nevaluation framework, but it is crucial to ensure that these evaluations do not inadvertently reinforce\nbiases or propagate harmful content. We are also careful in detecting and mitigating any personally\nidentifiable information or offensive content within our datasets and prompts."}, {"title": "Acknowledgement", "content": "We acknowledge and thank Mark Lee who helped with our code review and release."}, {"title": "A Experiment and dataset details", "content": "The key statistics of the MMAU dataset are presented in Tab. 4. A list of all evaluated models\n(commercial and open source) is provided in Tab. 5."}, {"title": "A.1 Tool-use", "content": "Data construction protocol: 1) The user sends a query to the agent model along with a list of potential\nfunctions including a description of their purposes and parameters. 2) The agent responds with either\nnatural language or appropriate function use. 3) In case of function-call, the functions are invoked\naccording to the agent's instructions, either by the user or directly by the agent, and the result is\nsubmitted back to the agent model. 4) The agent can then conclude with the given information or\ncontinue the conversation with follow-up questions or additional function calls.\nTool-use evaluation details: To compare the predicted and the ground truth parameter values, we\nperform string normalization including stripping punctuation, white spaces and converting to lower\ncase. In some cases where the parameter value can have open-ended, semantically equivalent\nforms, we define example-specific match rules based on regular expressions to accommodate valid\nalternatives."}, {"title": "A.2 DAG QA", "content": "Data construction protocol: The query includes a description of the task (to choose the appropriate\nplan), a list of potential functions (including a description of their purposes and parameters), an\nenumeration of all possible plans (all possible sequences in which to execute the tools), and the task\ninput (what the task is that the plan is expected to solve). The task is formulated as a multiple choice\ntask, where at the end of the query, the agent is asked to end the reply with which plan it chooses. At\nevaluation, the chosen plan is then extracted from the output by searching for a string match of the\nrequested response format. An example prompt is illustrated in Figure 14.\nThe reasoning and planning benchmarks differ only in the prompt, where for reasoning, the agent\nis requested to \"elaborate on the thought process and reasoning\", while for planning, the agent is\nrequested to \"be concise with a response in the format Chosen Plan: N\"."}, {"title": "A.3 Self-correction tool-use", "content": "To encourage models to retry, we prepend the following system message to the user's first turn\nmessage:"}, {"title": "Kaggle:", "content": "To represent all results on the same scale and reduce confusion, when QA\nmodel is not multimodal, we report its performance as\nratio_of_text_based_questions * accuracy_on_text_based_questions = (207/328) * accuracy_on_text_based_questions. We\nset the temperature to 0 for both code generation and QA."}, {"title": "A.4 Contest-level Coding", "content": "We select 261 valid problems from the Valid and Test splits of the CodeContests dataset [13], each\ncontains a number of test cases, including public tests, private tests, and generated tests.\nE2E Standard. The agent is asked to solve the problem by generating Python code solution. The\nprompt contains a detailed description of the CodeContests problem with public test examples, as\nwell as basic instructions on code formatting following the baseline prompt design in [54]. The\ngenerated code solution will be compiled and executed on all test cases, and is considered correct\nonly if all test cases are passed. Following related works, we report the pass@5 score, which is the\npercentage of problems solved by generating 5 solutions per problem. We set temperature as 0.3 for\nall models.\nProblem Parsing. We design a ProblemParsing task for each CodeContests problem, which provides\nthe agent with the problem description and public test examples, and asks the agent to directly infer\nthe desired output for an unseen test case. To make the unseen test case relatively easy for parsing\nand computation, we select the shorted test case from the union of private test cases and generated\ntest cases. We adopt a chain-of-thought prompt to help the LLM agent understand the problem (see\nAppendix B for detailed prompts). We then compare the answer with the groundtruth output, and\nderive the accuracy over the entire dataset. For reproducibility, we set temperature as 0 for all models.\nPlanner-shift. As introduced in 3.2.3, we divide the code generation process for CodeContests\nproblems into two steps: generating a plan (i.e., planner) and generating code solutions (i.e., solver).\nWe evaluate the planning capability of LLM agents by fixing the solver to be the same strong model.\nFor each LLM model, we generate 1 plan with temperature 0, and use gpt-4-0125-preview as the\nsolver to generate 5 code solutions (with temperature 0.3) and calculate the percentage of solved\nproblems.\nSolver-shift. SolverShift complements PlannerShift by freezing the planner and evaluating the\nproblem-solving capability of various LLM models. Given 1 plan generated by gpt-4-0125-preview,\nwe let LLM agents generate 5 code solutions and calculate the percentage of solved problems."}, {"title": "Self-correction / Retry.", "content": "We also evaluate the self-correct capability of LLM agents on the CodeCon-\ntests problems. For the Regular task above", "errors": "empty solution, compilation error,\nruntim"}]}