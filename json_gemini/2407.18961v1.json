{"title": "MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains", "authors": ["Guoli Yin", "Haoping Bai", "Shuang Ma", "Feng Nan", "Yanchao Sun", "Zhaoyang Xu", "Shen Ma", "Jiarui Lu", "Xiang Kong", "Aonan Zhang", "Dian Ang Yap", "Yizhe zhang", "Karsten Ahnert", "Vik Kamath", "Mathias Berglund", "Dominic Walsh", "Tobias Gindele", "Juergen Wiest", "Zhengfeng Lai", "Xiaoming Wang", "Jiulong Shan", "Meng Cao", "Ruoming Pang", "Zirui Wang"], "abstract": "Recent advances in large language models (LLMs) have increased the demand for comprehensive benchmarks to evaluate their capabilities as human-like agents. Existing benchmarks, while useful, often focus on specific application scenarios, emphasizing task completion but failing to dissect the underlying skills that drive these outcomes. This lack of granularity makes it difficult to deeply discern where failures stem from. Additionally, setting up these environments requires considerable effort, and issues of unreliability and reproducibility sometimes arise, especially in interactive tasks. To address these limitations, we introduce the Massive Multitask Agent Understanding (MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need for complex environment setups. It evaluates models across five domains, including Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine Learning coding, Contest-level programming and Mathematics, and covers five essential capabilities: Understanding, Reasoning, Planning, Problem-solving, and Self-correction. With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents. By testing 18 representative models on MMAU, we provide deep and insightful analyses. Ultimately, MMAU not only sheds light on the capabilities and limitations of LLM agents but also enhances the interpretability of their performance. Datasets and evaluation scripts of MMAU are released at https://github.com/apple/axlearn/docs/research/mmau.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in the field of AI have been marked by significant progresses in the development of LLMs. Particularly, one promising direction along this evolution is the ability of LLMs to perform as human-like agents [1], i.e., understand complex contexts, reason and plan with complicated logic [2-5], make decisions, and utilize tools effectively [6\u20138]. Consequently, the need for comprehensive benchmarks that evaluate LLMs as intelligent agents has become more and more important.\nWhile existing benchmarks [9\u201312] evaluate LLM agents by focusing on specific application scenarios and task completion, they struggle to reveal the underlying capabilities driving these outcomes. For example, as shown in Fig. 3, when an LLM encounters a complex math problem, multiple capabilities are required to solve it. By emphasizing task completion, existing benchmarks often obscure whether a failure stems from a lack of comprehension, reasoning, or calculation error. Consequently, these evaluation methods blur the distinctions between different types of failures, hindering our understanding of where the error originates from and limiting our ability to gain deeper insights into the model's capabilities and make targeted improvements. Additionally, some tasks in existing benchmarks require considerable effort to set up the environments, making a thorough evaluation both expensive and challenging. Furthermore, we observe that tasks, especially interactive ones, are sometimes less stable and reproducible due to the stochasticity of the environment feedback during the evaluation process. This randomness can make it difficult to obtain consistent evaluation results and draw solid conclusions.\nTo address such limitations, we introduce the Massive Multitask Agent Understanding (MMAU) benchmark. We develop MMAU by identifing five essential capabilities: Understanding, Reasoning, Planning, Problem-solving and Self-correction across five domains: Tool-use, Directed Acyclic Graph (DAG) QA, Data Science & Machine Learning coding, Contest-level programming, and Mathematics. As a result, MMAU comprises a total of 3,220 distinct prompts gathered from diverse data sources. These include our in-house human annotations for tool-use, as well as rewritten and curated prompts from open-source datasets such as CodeContest [13], Kaggle [14], and DeepMind-Math [15]. Based on this dataset, we designed 20 tasks across 64 subjects, offering a comprehensive benchmark. To avoid the complexities of environment setup and issues of unreliability, all tasks in MMAU are performed on our 3K static dataset to eliminate potential issues related to environment instability."}, {"title": "2 Related Work", "content": "LLM-based Genelist Agents Researchers have proposed various generalist agent frameworks to create AI assistants that can understand and execute any instruction from users. One pioneering work is Auto-GPT [16], which uses a language model as an AI agent that can break down goals into action-able steps with the aid of auxiliary tools. Integrating language models into multi-agent collaboration systems [17] is a cutting-edge research area. Frameworks like AutoGen [18], LangChain [19], Camel [20], AGENTS [21], AutoAgents [22], and XAgent [23] have explored different approaches to enable communicative agents to collaborate autonomously, facilitate practical applications, ensure control and customization, dynamically generate specialized agents, and manage complex tasks effectively.\nAgent Benchmarks The need for rigorous benchmarks also arises in response to the surge in LLM-based agents. Varying benchmarks are created to gauge agent capability on tasks inspired by real-world use cases. The Berkeley Function Calling Leaderboard [24], NexusRaven V2 Function Calling Benchmark [25], ToolBench [26], StableToolBench [27], and API-BLEND [28] seek to evaluate the capability of LLM agent to plan and perform function calls. Webshop [29], WebArena [30], Mind2Web [31], MiniWoB++ [32], and VisualWebArena [33] focus on the agent's ability to browse and interact with a web environment. A line of benchmarks consider the universal presence of user interface (UI) and envision UI automation agents, including PixelHelp [34], MetaGUI [35], MOTIF [36], AITW [37], and OmniACT [38]. SWE-bench [39] tests agent capability to solve real-world software engineering problems.\nWhile each benchmark tends to focus on a specific application, a generalist agent should be able to perform well on a wide range of tasks. AgentBench [12] consolidates tasks covering coding, game, and math into a single systematic benchmark. AgentBoard [11] evaluates agent capabilities under web browsing, tool use, embodied AI, and game domains. However, both benchmarks require containerized environments to run and need involved effort to implement new tasks. As a result, simply stacking tasks would lead to diminishing returns in comprehensiveness. MMAU takes a step"}, {"title": "3 The MMAU Benchmark", "content": "To introduce MMAU, we will start with an overview of all included capabilities 3.1. We will then provide detailed explanations of how each task was designed and how the dataset across different domains was constructed 3.2."}, {"title": "3.1 Capabilities in MMAU", "content": "Below, we introduce the capability definitions and the key tasks used to evaluate them. A complete task-capability mapping can be found in the Appendix 6.\nUnderstanding is a fundamental capability required of an intelligent agent. In MMAU, we evaluate an agent's understanding in different aspects, including: complex instruction following, user intent understanding, statistics parsing, and visual grounding.\nReasoning and Planning reflect an agent's thought process and ability to infer logically from complex factors. Although reasoning and planning has been recognized in many works, it is often described as a general ability, compounded with other skills, which limits deep investigation. In MMAU, we address this challenge with the task planner-shift, designed to decompose reasoning and planning capabilities from other factors. Unlike standard end-to-end evaluations, planner-shift divides the solution generation into two stages. In stage 1, a planner model generates a high-level plan, providing a strategy to solve the given problem without hinting at the final solution. In stage 2, a solver model is given the original problem along with the plan to solve it. This approach isolates the planning and reasoning processes from problem-solving. To test planning and reasoning capabilities, we vary only the planner model while using the same solver model, ensuring that performance differences reflect the planning and reasoning capabilities. The task design diagram is shown in Figure 4.\nProblem-solving focuses on measuring an agent's ability to successfully implement or execute a task, assuming it has already understood and planned the strategy well. To address this, we design a task called solver-shift, similar to planner-shift, which also performs a two-stage generation. However, solver-shift keeps the planner model constant and varies only the solver models to reflect differences in problem-solving skills, as shown in Figure 4. In MMAU we use the tasks of planner-shift and solver-shift in domains of Contest-level coding 3.2.3 and Math 3.2.4.\nSelf-correction is another core capability for an intelligent agent. It reflects the agent's ability to identify errors, learn from its environment and past behaviors, and correct itself to eventually"}, {"title": "3.2 Dataset Construction", "content": "The construction of MMAU encompasses both breadth and depth of data, as illustrated in Table 4. Our dataset is constructed from heterogeneous sources: 1) our in-house tool-use data, used for tasks under tool-use and DAG-QA; 2) Kaggle [14] datasets, which we rewrote to design tasks for DS & ML coding; 3) CodeContest [13], used for tasks under contest-level coding; and 4) DeepMind-math [15], used for math tasks. MMAU involves curating and rewriting these data sources. In the following section, we will explain how we leveraged these source data to construct MMAU."}, {"title": "3.2.1 Tool-Use", "content": "We curated an in-house dataset for tool-use with conversation trajectories following the standard tool-use (a.k.a. function-calling) protocol. We select from a subset of RapidAPI Hub 3 functions and ask human annotators to create realistic scenarios with user queries, ground truth function calls and actual function returns from the RapidAPI endpoints. In total, our in-house tool-use dataset consists of 409 single-step (task:single-tool-use) and 258 multi-step tool-use conversations (task: multi-turn multi-tool-use). Out of the 409 single-step tool use conversations, 225 require making parallel tool calls (task: parallel-tool-use). Figure ?? shows some of these examples. We adapt this dataset for the following tasks.\nTask: Tool-use Benchmarking agent tool-use following the standard protocol requires an interactive environment. To simplify the evaluation process, we instead evaluate the model's response at each assistant turn (i.e., where a function call is expected), conditioning on the ground-truth versions of all previous user or assistant turns. For evaluation, we check if the model's tool call matches that of the ground truth, i.e. calling the same function and the same parameters.\nTask: DAG QA In this task, a user presents a set of requirements to which the LLM must respond by selecting and ordering a sequence of tool invocations from multiple choices provided. This design examines whether the model can identify the relevant tools and deduce the correct dependencies between them. The prompt enumerates the possible tool use orderings from which the LLM agent is asked to pick one, and the label is derived from the ground truth function call sequence. For example, we transform the multi-step example in Figure ?? into a DAG QA task as shown in Figure 14\nTask: Tool-use Self-correction From the tool-use dataset described above, we derive two classes of errors to test the model's self-correction ability:\nTemporary error simulates a tool that is temporarily unavailable. From the ground truth messages [user queries, tool-calls and tool responses], we substitute a random temporary error (e.g. \"429: too many requests\u201d, \u201c504: gateway timeout\") in place of the tool response.\nIncorrect call simulates a previous tool call or response containing an error. We mutate the ground truth tool-call to be incorrect by changing the arguments or the function name, issue the modified call, and save the updated tool response. Given the message history of [user queries, mutated calls, updated tool responses], the model is expected to retry with the correct call.\nThe evaluation metric is the exact match accuracy of the function name and arguments against the ground truth."}, {"title": "3.2.2 Data Science and Machine Learning", "content": "We leverage the Meta Kaggle Code dataset [14] and curate 28 Python notebook-style conversations, with 123 conversation turns. Each turn begins with a user request for code generation. Among all requests, 83 requests expect text-based outputs from code and 40 requests expect image outputs. Due to the open-ended nature of code generation, we created multiple-choice questions that require information from successful code execution to fully address, resulting in 207 text-based questions and 121 image-based questions. Figure 5 shows an example turn with multiple choice questions. We report QA accuracy as the main metric and vary the combination of code model and QA model to produce different evaluation settings.\nTask: E2E Code Generation and QA In this setting, we aim to gauge the overall capability. the evaluated model is responsible for both code generation and QA.\nTask: Code Generation and GPT-4 QA In this setting, we isolate the code generation capability of the model. After generating code from the evaluated model, we adopt a strong multimodal model (GPT-4 [41]) to serve as control and perform QA based on code execution outputs.\nTask: QA from Oracle Code In this setting, we specifically focus on the textual and visual understanding proficiency of the model decoupled from code generation. We obtain oracle output by executing ground truth code implementation and then pass to the evaluated model to perform QA.\nTask: DS & ML Self-Correction This setting is similar to the E2E setting, however, whenever code execution fails, we use the execution error message to prompt an additional code generation turn."}, {"title": "3.2.3 Contest-Level Coding", "content": "For contest-level coding problems, we select 261 problems from the Valid and Test splits of the CodeContests dataset [13] which includes competitive programming problems. We adapt these 261 problems for the following tasks.\nTask: E2E Standard In this task, models are challenged with a variety of coding problems. The effectiveness of the solutions is measured by executing the code against all predefined test cases [13]. All CodeContests results reported in this paper are based on pass@K (K=5) accuracy.\nTask: Planner-shift and Solver-shift As introduced in Section 3.1, we use thses two tasks to extensively measure the agent's capability in planning and problem-solving, respectively. We evaluate both of these tasks by generating K Python code solutions and verifying their pass rate.\nTask: Problem Parsing Unlike the other tasks, this task does not require the model to write or execute any code. Instead, given a problem statement and associated test cases, the model is only tasked with predicting the outputs for these test cases. An example is shown in Figure 15. Our rationale is that if a model truly grasps the problem, including its complex instructions and user intent, it should be able to accurately predict the outputs based on its understanding alone. We use match accuracy as the evaluate metric.\nTask: CodeContest Self-correction For the E2E standard task above, we collect the error messages of each candidate solution if it does not pass some test cases, including 4 types of errors: empty"}, {"title": "3.2.4 Mathematics", "content": "The source data in the domain of math, derived from DeepMind-Math [15], consists of 1,000 carefully curated math problems spanning 56 subjects, including calculus, geometry, statistics, and etc.\nTask: E2E Standard We adhere to standard protocol by incorporating a Chain-of-Thought (CoT) [3] into the prompt to generate the answers end-to-end, and using accuracy as the evaluation metric.\nTask: Planner-shift and Solver-shift As introduced in Sec. 3.1, we use the two tasks to assess the model's planning and problem-solving abilities in a two-stage manner, avoiding confounding influences from other capabilities. The prompts used for each task can be found in the Appendix B.\nTask: Comprehend+. To better isolate and assess the understanding capability without excessive interference from other skills, we have devised a new task named Comprehend+. Our hypothesis is that problems that are straightforward mathematically but complex in their descriptions rely more heavily on understanding capabilities. To test this, we first selected a subset containing only the mathematically simpler problems, and then use an LLM to create new math problems that feature more complex descriptions or harder problem statements but retain the same underlying mathematical constructs from each data sample. A rewritten math problem example is shown in Fig. 13. After curation and verification, we finalize 676 newly created problems for Comprehend+. Please refer to Appendix A.5 for details of dataset creation."}, {"title": "4 Evaluation", "content": "We comprehensively evaluate 18 models on MMAU. All evaluation model details are listed in Table 5. For easier reading, the main paper presents only the aggregated evaluation results. For the evaluation results over all 20 tasks, please refer to Appendix C."}, {"title": "4.1 Domain-centric Evaluation and Analysis", "content": "As shown in Table 2, there is a clear performance gap between API-based commercial models and open-source models across all evaluation domains. Among the commercial models, the GPT-4 family"}, {"title": "4.2 Capability-centric Evaluation and Analysis", "content": "As introduced in Sec.3.1, we designed tasks to decompose core capabilities from standard evaluations, allowing MMAU to offer a unique dimension of evaluation. Each capability includes tasks spanning different domains. To provide overall capability-centric evaluation results for each model, we aggregate tasks under each capability using a weighted average. Detailed task-capability mappings and the calculation method can be found in Appendix A. The overall capability-centric evaluation results are shown in Table 3.\nNotably, for the capability of Understanding, GPT-40 significantly outperforms other models, demonstrating its superior capability in handling long contexts, complex user instructions, and capturing (sometimes implicit) user intents. Additionally, GPT-4, Gemini-1.5-pro, and Claude3-Opus also exhibit reasonably strong understanding capabilities. For the capabilities of Reasoning and Planning, the GPT-4 family shows the strongest performance. When examining the capability of Problem-solving, the performance gap is not significantly large. This trend suggests that when provided with \"oracle\" plans, solving a task may be less challenging. While models' problem-solving capabilities vary, most can perform these tasks reasonably well, indicating that this capability may be more universally achievable among different models. On the contrary, for Self-correction, we observe a significant gap among models. Among open-source models, aside from Mixtral-8x22B, others do not seem to possess the skill to reflect on and correct their own errors effectively. These evaluation results highlight that self-correction is a critical capability needing further research and development to advance the field."}, {"title": "5 Analysis and discussion", "content": "How does planning impact the performance? One interesting finding emerges from the results of our designed tasks, Planer-Shift and Solver-Shift on Math. As shown in Table 7, we find"}, {"title": "6 Conclusion", "content": "In this paper, we introduce the Massive Multitask Agent Understanding (MMAU) benchmark. By evaluating models based on both application scenarios and fundamental capabilities, MMAU provides a comprehensive and in-depth test bed for reliable and thorough studies. By designing 20 tasks to decompose capabilities beyond standard evaluation benchmarks, MMAU offers more granular insights into the strengths and limitations of these models.\nLimitations and future work. The current scope of MMAU, while broad, does however not encompass all possible domains relevant to LLM agents, such as interactive environments which are also critical yet challenging. Future iterations of MMAU should aim to include interactive tasks to provide a more holistic evaluation. This expansion will require the development of reliable, stable, and user-friendly interactive environments. Moreover, as we expand to include more domains, it will be essential to incorporate additional capabilities such as retrieving, memorizing, sequential decision-making, etc. Our current approach to capability decomposition, though insightful, still faces challenges in disentangling compound capabilities. Future research should focus on developing more effective methods for decomposing and evaluating these capabilities to further refine the benchmark.\nEthics and Societal Impacts. Research on LLM agents must consider potential ethical concerns and negative societal impacts. The MMAU benchmark aims to provide a thorough and transparent evaluation framework, but it is crucial to ensure that these evaluations do not inadvertently reinforce biases or propagate harmful content. We are also careful in detecting and mitigating any personally identifiable information or offensive content within our datasets and prompts."}]}