{"title": "Speech-Forensics: Towards Comprehensive Synthetic Speech Dataset Establishment and Analysis", "authors": ["Zhoulin Ji", "Chenhao Lin", "Hang Wang", "Chao Shen"], "abstract": "Detecting synthetic from real speech is increasingly crucial due to the risks of misinformation and identity impersonation. While various datasets for synthetic speech analysis have been developed, they often focus on specific areas, limiting their utility for comprehensive research. To fill this gap, we propose the Speech-Forensics dataset by extensively covering authentic, synthetic, and partially forged speech samples that include multiple segments synthesized by different high-quality algorithms. Moreover, we propose a TEmporal Speech LocalizaTion network, called TEST, aiming at simultaneously performing authenticity detection, multiple fake segments localization, and synthesis algorithms recognition, without any complex post-processing. TEST effectively integrates LSTM and Transformer to extract more powerful temporal speech representations and utilizes dense prediction on multi-scale pyramid features to estimate the synthetic spans. Our model achieves an average mAP of 83.55% and an EER of 5.25% at the utterance level. At the segment level, it attains an EER of 1.07% and a 92.19% F1 score. These results highlight the model's robust capability for a comprehensive analysis of synthetic speech, offering a promising avenue for future research and practical applications in this field.", "sections": [{"title": "Introduction", "content": "Speech forensics analysis constitutes a crucial component in the field of forensic science, focusing on the verification of the authenticity and integrity of digital audio recordings. With the advent of advanced artificial intelligence, deep learning technologies have significantly progressed, particularly in the realm of content generation with notable examples including WaveNet [Oord et al., 2016], Tacotron [Wang et al., 2017], or VITS [Kim et al., 2021]. The rapid evolution of text-to-speech (TTS) synthesis and voice conversion (VS) technology has markedly narrowed the perceptual gap between deepfake audio and authentic audio recordings, making them virtually indistinguishable from the human ear. This development poses substantial challenges in the domain of speech forensics, as forensic experts increasingly face difficulties in ascertaining the authenticity and integrity of audio recordings, which are often critical pieces of evidence.\nExtensive research has been conducted to address the forensic challenges posed by deepfake speech, and several benchmarks have been established. The ASVspoof databases [Wang et al., 2020; Yamagishi et al., 2021] stand out as a pivotal resource, designed to bolster research in developing utterance-level countermeasures against deceptions in automatic speaker verification systems. ADD [Yi et al., 2022; Yi et al., 2023] represents another significant dataset series for detecting fake audio, encompassing a range of detection tasks under various scenarios, which includes the localization of tampered regions in partially forged speech and the recognition of utterance-level forgery algorithms. Simultaneously, a diverse range of studies have been proposed to navigate the intricate scenarios of speech analysis in practical settings. Initially, the research relied on identifying artifacts to verify the authenticity and integrity of audio recordings [Zhao and Malik, 2013]. Traditional methods predominantly focused on the acoustic and statistical disparities between forged and genuine audio to facilitate differentiation [Kajstura et al., 2005; Yang et al., 2008; Malik and Farid, 2010]. As the complexity of forensic scenarios escalated, machine learning, especially deep learning techniques, began to receive heightened attention from researchers, leading to significant breakthroughs in numerous instances [Villalba et al., 2015; Alzantot et al., 2019; Li et al., 2023a; Zeng et al., 2023].\nWhile previous datasets in this field offer valuable insights, they also come with their own set of limitations and focuses. The majority of ASVspoof datasets, for instance, are primarily geared towards binary classification at the utterance level. The ADD database, although it encompasses scenarios of partially forged speech, tends to oversimplify these scenarios by typically involving alterations in just a single region of the speech. The PartialSpoof dataset [Zhang et al., 2021] has made strides by updating its segmentation labels to enable the localization of multiple forged regions within a speech sample. However, this dataset does not provide corresponding labels for the forgery algorithms used, which restricts our ability to glean vital forensic insights regarding the origins of the forgery. Consequently, the algorithms [Tak et al., 2021; Cai et al., 2023b; Lu et al., 2023] developed based on these datasets often analyze synthesized audio from a singular perspective. This approach substantially limits their broader applicability and impedes the progression toward a more comprehensive and multi-dimensional forensic analysis.\nTo tackle these challenges, we develop a sophisticated data pipeline for creating an extensive synthetic speech analysis dataset, which we have named Speech-Forensics. This dataset is meticulously crafted to maintain the semantic integrity of each audio sample. We achieve this by randomly manipulating multiple regions within a single audio and meticulously recording the specific forgery algorithms applied to these regions. This approach enables our dataset to support a thorough evaluation of speech analysis algorithms, encompassing aspects such as authenticity detection, regional localization, and algorithmic recognition.\nIn addition, we introduce the Temporal Speech localizaTion network called TEST. With a strong emphasis on temporal localization, this framework facilitates the seamless execution of complex tasks, including authenticity detection, regional localization, and forgery algorithm recognition, in a single and streamlined process. TEST negates the need for intricate post-processing steps, thereby allowing for a more flexible and efficient analysis of diverse synthesized speech forgeries. Drawing inspiration from the realm of temporal action localization, TEST utilizes a one-dimensional feature pyramid network, along with the combination of masked difference convolutions, LSTM, and transformers, enabling it to perform dense prediction tasks effectively. To our knowledge, this is the first instance of such a comprehensive dataset and framework being implemented in the field of synthetic speech analysis. The dataset and code are available at https://github.com/ring-zl/Speech-Forensics. Our contributions can be summarized as follows:\n\u2022 We implement a rational data pipeline and establish a comprehensive synthetic speech analysis dataset, encompassing multi-regional forgeries and their corresponding algorithms.\n\u2022 We propose a framework designed for simultaneous authenticity detection, localization of multiple fake segments, and recognition of synthesis algorithms, without any complex post-processing.\n\u2022 We demonstrate the robust capability of our proposed framework to perform comprehensive, multi-dimensional analyses of synthetic speech."}, {"title": "Related Work", "content": "Table 1 summarizes key datasets of synthetic speech analysis, illustrating the evolution and focus of current research. The inaugural ASVspoof Challenge in 2015, based on the SAS database [Wu et al., 2015], concentrated on detecting spoofed speech. ASVspoof 2019 [Wang et al., 2020] marked a significant development by introducing VC and TTS algorithms, establishing a critical baseline dataset for synthetic speech detection. In 2020, Reimao employed deep-learning synthesizers to create the FoR [Reimao and Tzerpos, 2019] dataset. In 2021, the In-the-Wild dataset [M\u00fcller et al., 2022] emerged as the first to represent real-world scenarios in the field. In the same year, the Partialspoof dataset [Zhang et al., 2021] was introduced, becoming the first dataset to propose the concept of partial forgery. It established the groundwork for the localization of forged regions, setting a precedent in the field. The ASVspoof 2021 challenge [Yamagishi et al., 2021] added a track for detecting synthetic and compressed tampered audio to enhance system robustness. The Wave-Fake [Frank and Sch\u00f6nherr, 2021] dataset employs a broader range of advanced synthesis algorithms, significantly increasing the challenge of distinguishing forged audio. Building upon these developments, SASV Challenge 2022 [Jung et al., 2022b] concentrated on enhancing both Countermeasure (CM) and Automatic Speaker Verification (ASV) systems, aiming to achieve dependable detection of spoofed speech.\nThe ADD series, notably the ADD2022 challenge [Yi et al., 2022], introduced tasks such as Low-quality and Partially Fake Audio Detection, and the Audio Fake Game, broadening the scope of forged audio analysis. ADD2023 [Yi et al., 2023] further expanded this with tasks like Manipulation Region Location and Deepfake Algorithm Recognition, enabling a more thorough analysis of synthetic speech.\nWhile these datasets provide a diverse landscape for spoofed audio analysis, each has its own focus and limitations. The ASV series, for instance, focuses on utterance-level analysis, whereas the ADD series primarily addresses single-segment localization. Although the PartialSpoof dataset has made strides, it lacks information on the spoofing algorithms used. This diversity in focus and the lack of comprehensiveness in existing datasets present challenges in conducting holistic forensic research in speech analysis."}, {"title": "Synthetic Speech Analysis", "content": "Detection. Forged speech detection has advanced significantly, initially relying on various speech signal processing algorithms. Malik [2010] utilized spectral decay phenomena for forensic audio analysis. In early deceptive audio detection, machine learning techniques, such as SVM, were prevalent, with Alegre [2012] proposing an SVM-based robust voice spoofing detection strategy. The advent of deep learning brought about extensive use of these algorithms in anti-spoofing efforts. For instance, LCNN [Lavrentyeva et al., 2019] and RawNet2 serve as baseline models in the ASVspoof competition [Todisco et al., 2019], achieved top performance in single-system detection.\nLocalization. Research has progressed beyond utterance-level speech forgery detection to address the more complex task of localizing specific forgery regions in partially fake speech. Li [2023b] viewed audio as a sequence of frames, using a CRNN network for frame classification. However, this indirect localization method often results in less intuitive audio analysis outcomes. Cai [2023b] introduced a boundary prediction-based model focusing on clipped segment boundaries, though it depends on dataset specifications and a sophisticated post-processing procedure. Despite these advancements, a simple, intuitive, and flexible method for accurate localization remains a pressing need in the field.\nRecognition. With the increasing use of deepfake algorithms for speech synthesis, the field has moved beyond basic authenticity classification to include detailed forgery information analysis, like identifying specific generating algorithms. Qin [2022] drew parallels between deepfake algorithm recognition and speaker verification, achieving notable results. To address the challenge of unknown algorithms, Zeng [2023] extended their closed-set experiments using the ECAPA-TDNN model and data augmentation, successfully generalizing to new algorithms. However, most existing work focuses on utterance-level recognition, leaving the challenge of identifying multiple forgery algorithms within a single audio sample largely unexplored."}, {"title": "Speech-Forensics Dataset", "content": "The research community has developed various datasets to tackle synthetic speech analysis tasks, each contributing uniquely to the field. However, comprehensive forensic analysis, especially in scenarios requiring multifaceted evidence evaluation, demands a dataset that transcends conventional limitations. Our dataset, Speech-Forensics, is designed to address this need. It aims to facilitate studies within a comprehensive analytical framework, catering to the nuanced requirements of forensic speech analysis. Speech-Forensics distinguishes itself by offering a diverse range of speech samples, encompassing various types of forgeries, and is structured to support both broad and detailed analyses."}, {"title": "Dataset Construction", "content": "To emulate the intricacies of real-world voice forensics, our dataset construction prioritizes semantic coherence in the fabricated samples. Based on insights from previous studies [Yi et al., 2021], we employed a multi-step data pipeline consisting of: (1) Content Selection: Choosing appropriate content for editing. (2) Audio Synthesis: Generating audio from the edited text. (3) Audio-Text Alignment: Creating alignments between the audio and text. (4) Forgery Splicing: Using these alignments for splicing in speech forgeries.\nWe utilized the LJ Speech dataset*, a public domain collection of 13,100 audio clips with transcripts, read by a single speaker. This dataset, spanning approximately 24 hours, was chosen for its diversity in content and audio length (1 to 10 seconds per clip). To ensure semantically coherent modifications, we implemented a keyword replacement strategy. This involved using a Named Entity Recognition (NER) algorithm to identify and replace various entities (e.g., persons, organizations) within the texts. To diversify our entity pool, additional entities were extracted from a news dataset. Table 2 details the distribution of these entities. Moreover, we integrated an antonym replacement strategy for adjectives in the transcripts, randomly substituting them with their antonyms to mirror the varied scenarios of voice forgery encountered in practical applications.\nTo enhance the naturalness of the fabricated samples, we avoid directly synthesizing individual keyword segments. Instead, we generate complete synthetic audio for the entire edited transcript. Then we extract the specifically forged keyword phrases for final assembly. This method ensures a seamless integration of the forged segments with the original audio context. In selecting algorithms, we prioritized models pretrained on the LJ Speech dataset. This choice aims to retain as much of the original speaker's characteristics as possible, thereby preserving the authenticity of the synthesized audio. The specific configurations of the TTS and VC algorithms used in our dataset construction are detailed in Table 3.\nUpon obtaining the original audio and its corresponding transcription, along with the series of synthesized audio files and their edited transcripts, we employed the Montreal Forced Aligner (MFA) tool. This tool utilizes a trained speech recognition system to produce accurate timestamp alignments between the audio and text. These timestamps are instrumental in seamlessly integrating the synthesized segments into the original audio. This meticulous process results in the creation of realistic voice forgeries, which are accurately labeled and closely mirror real-world scenarios. The integration strategy ensures that the fabricated samples are not only natural-sounding but also maintain coherent and reasonable semantics, thus enhancing their applicability in forensic analysis."}, {"title": "Dataset Description", "content": "Task Comprehensiveness. Unlike existing datasets that typically analyze synthesized audio from a single perspective, our dataset provides comprehensive Ground Truth data. This includes the veracity of speech at the utterance level, timestamps for each forged span, and the specific synthesis algorithms used. This allows for more nuanced tasks in forensics.\nDataset Size. We have extensively utilized the LJ Speech dataset, generating 4,323 records using NER and 3,129 records through antonym replacement strategies. To create a more realistic forensic environment, the remaining samples are labeled as bonafide. Figure 1a illustrates the distribution of sample durations in the Speech-Forensics dataset.\nMultiple Spans Per Speech. Enhancing the dataset's utility, we manipulated multiple spans within each audio sample using different synthesis algorithms. This detailed approach provides granular timestamp information and corresponding algorithm data. The length distribution of forged spans within the dataset is presented in Figure 1b.\nAppropriate Algorithm. A variety of advanced voice synthesis algorithms were selected for creating the voice forgeries. These algorithms, known for their natural-sounding synthetic outputs, were mostly trained on the LJ Speech dataset to mimic intricate forgery scenarios. The dataset includes both TTS and VC algorithms, with the number of algorithms used per sample depicted in Figure 1c."}, {"title": "Method", "content": "In forensic scenarios involving synthetic speech, conducting a multifaceted analysis is essential. This analysis must extend beyond merely detecting authenticity to include localizing tampered regions and identifying the methods used for forgery. Traditional localization methods, which focus only on identifying the true and false boundaries, prove inadequate for handling multiple forged intervals. These methods, along with frame-by-frame classification techniques, fall short as they do not offer a clear representation of the extent of forgeries and often require complex post-processing. Similarly, in the realm of algorithm recognition, most existing methods operate at the utterance level, presenting challenges in cases where speech forgeries involve multiple forging algorithms. This limitation hinders the complete identification of manipulations. To overcome the limitations of existing methods and enhance practicality, we have developed a comprehensive framework for synthesized speech analysis. Our approach diverges from existing methods by defining the task objective as a structured output:\nX \u2192 Y = {\u00d8,\n{Y1,Y2,...,YN}, if N = 0,\nif N > 0.\n\nwhere X represents a potential synthetic speech, Y is our target for forensic speech analysis. N denotes the total number of forged spans in the entire audio. When N = 0, it indicates that the audio is bonafide. Each span represented by Yi = (Si, ei, Ci), which is defined by its starting time si, ending time ei and the class of the algorithm used ci. It is evident that si \u2208 [0, T] and ei \u2208 [0, T'], where T denotes the total duration of the audio, and that si < ei.\nTo achieve this structured output objective, we were inspired by an anchor-free representation [Yang et al., 2020; Zhang et al., 2022]. The core idea is the dense regression of the current timestamp relative to the start and end distances of the forged span. This implies that we have transformed the output objective of the model into the following form:\nX \u2192 Y = {Y1, Y2, \u2026\u2026\u2026, YM }"}, {"title": "Embedding Module", "content": "Due to the sparsity of speech data (typically sampled at 16,000 points per second), researchers often opt to filter the original speech data [Tak et al., 2021], extract spectrograms [Li et al., 2023b], or employ deep features [Cai et al., 2023b] for experiments to achieve a more robust and effective representation of speech.\nFor a given speech X of duration D, it is transformed into a sequence of features X = {x1, X2, ..., XT} via the embedding module. Here, each x\u2081 has E embedding dimensions and can represent a time segment of frame length fl. The temporal offset between each x\u2081 is defined as the frameshift fs, the length of the sequence T is given by the following equation:\nT =\nD\u2212fl\nfs\n+1\n\nwhere [*] indicates round down operation.\nOur embedding module consists of a feature extractor and a projection layer. The feature extractor can either perform mathematical operations for extracting handcrafted features, or it can be a trained self-supervised learning network that extracts deep features from speech signals. In this work, we have employed both approaches. We use Linear Frequency Cepstral Coefficients (LFCC) and Mel-Frequency Cepstral Coefficients (MFCC) for their proven efficacy in capturing speech characteristics. Additionally, we leverage models like Wav2Vec2 [Baevski et al., 2020] and WavLM [Chen et al., 2022], trained on diverse multilingual and unlabeled language data, to acquire high-level speech representations. Rather than fine-tuning these models for a specific task, we extract features from various layers. This method allows the model to benefit from a range of speech features, potentially enhancing task performance, as previous studies [Yang et al., 2021] have shown.\nThe projection layer is another component, consisting of a one-dimensional masked difference convolution. This design is intended to not only adjust the input dimensions for the backbone but also to enrich the temporal information captured from the speech data. Traditionally, in handcrafted feature extraction, derivatives like first and second-order differences are included as supplementary parameters to cepstral coefficients. Our projection layer seeks to emulate this enriching process by using the differences convolution, a concept inspired by a temporal enhanced network [Yu et al., 2021]. For a standard one-dimensional difference convolution with a mask, the feature output can be expressed as:\nMDC(to) = \u2211 w(tn) x(to+tn)\ntn\u2208D\n+0\u22c5(\u2212x(to)\u22c5\u2211w(tn))\ntn\u2208D\n\nwhere to denotes current timestamp while tn enumerates the timestamps in D, w represents learnable weights, hyperparameter \u03b8 \u2208 [0,1] tradeoffs the contribution between intensity-level and gradient-level information."}, {"title": "Backbone", "content": "The backbone of our model, as illustrated in Figure 2, is composed of L R-Transformer blocks. These blocks incorporate the LSTM module into Transformer architecture, aimed at achieving more robust and powerful speech representations [Sun et al., 2021]. Drawing inspiration from hierarchical feature extraction methodologies [Lin et al., 2017; Long et al., 2019], we segment speech into various levels, facilitating the capture of features across different time scales.\nThis integration of R-Transformers with one-dimensional downsampling convolutions forms the Bottom-up process of our feature map construction. For the Top-down process, we employ nearest-neighbor upsampling, complemented by Lateral connections, resulting in a layered feature pyramid F with L levels, denoted as F = {F1, F2, ..., FL }.\nGiven a virtual timestamp + at level i of the feature pyramid, and considering its cumulative scaling stride si, we can map this virtual timestamp back to the corresponding real timestamp t in the original speech sequence. This mapping is defined by the equation:\nt = [Si/2]+T\u22c5Si\n\nwhere [*] indicates round down operation."}, {"title": "Prediction Head", "content": "The role of our prediction head is to decode the feature pyramid F, generated by the backbone, into our desired output \u0176. This decoding involves two primary components of one-dimensional convolutional networks: the classification head and the localization head.\nClassification Head. For each virtual timestamp T on the feature pyramid F, the classification head is designed to output a probability vector of length C, where C represents the maximum number of synthetic algorithm categories. This vector reflects the probability p(c+) of timestamp 7 belonging to each category. The parameters for this process are consistently applied across all levels of the pyramid.\nLocalization Head. The objective of our localization head is to determine the distance between the timestamp T and the start and end times of the forged interval, corresponding to onset d and offset d. The output is valid only when timestamp T is within a forged interval. To ensure accurate distance estimation, we use the ReLU activation function.\nThe conversion of the virtual timestamp 7 to the real-time t in the corresponding audio is a critical step in our backbone design. For a given time moment t in the audio, we can determine the most likely forgery algorithm ct, and the start st and end et points of the forged spans using the following formula:\nct = argmax p(ct),\nst = t \u2212d,\net=t+d"}, {"title": "Loss Function", "content": "We employed a sigmoid focal loss to measure the loss of the predicted categories and a DIoU loss to evaluate the onset and offset. To balance these components effectively, we utilize the following loss function:\nL = \u2211(1\u03bbLcls + (1 \u22121\u03bb)ItLloc)/T+\nt\n\nwhere A serves as a balancing ratio between the two functions. It is an indicator function, determining whether the timestamp t falls within a specified forgery region. T+ represents the total number of positive samples."}, {"title": "Experiments", "content": "In the embedding process, we employed MFCC and LFCC as the primary handcrafted features. We set the frame length to 25 ms and the frameshift to 20 ms for these features. A Fourier transform, utilizing 256 filters and 2,048 FFT, was applied to produce 256-dimensional features. To enrich our cepstral representation, we concatenated the first and second-order delta spectra, resulting in a 768-dimensional feature. For deep feature extraction, we used Wav2Vec2 and wavLM as representatives of self-supervised learning models. Under the BASE configuration, both models transform every 20 ms of audio into a 768-dimensional vector. Under the LARGE configuration, these models output a 1,024-dimensional vector for every 20 ms of audio. In the experiment, we sequentially selected six layers of varying depths for comparison."}, {"title": "Implemental Details", "content": "Our experiments were conducted on a NVIDIA 4090 GPU. We adopted mini-batch training using the AdamW [Loshchilov and Hutter, 2017] optimizer. The training included a warmup phase of 5 epochs, followed by a cosine decay schedule for the learning rate. The initial learning rate was set at 1e \u2013 3, with a weight decay of 1e - 3. We employed Non-Maximum Suppression (NMS) to effectively filter out redundant and less effective spans from our model's output."}, {"title": "Evaluation Metrics", "content": "To ensure a comprehensive assessment of our method's performance across different scenarios, we have employed a multifaceted approach to evaluation metrics.\nFirstly, we utilize the average Mean Average Precision (mAP) across various temporal intersections over union (tIOU) thresholds to gauge the overall capability of our model in accurately locating forged regions and identifying the synthetic algorithm used. This metric effectively evaluates both the precision and recall of our model in a singular framework.\nFor utterance-level performance analysis, we adopted the Equal Error Rate (EER) metric, as used in ASVspoof 2019 LA. This threshold-independent measure reflects the point at which the false acceptance rate and false rejection rate are equal. Additionally, we extend this evaluation to segment-level analysis. Here, for each 0.01s audio segment, we assess our model's ability to accurately classify the segment as either bonafide or synthetic, offering a more granular view of performance.\nThis comprehensive set of evaluation metrics allows for a thorough assessment of our method's effectiveness in various aspects of synthetic speech analysis."}, {"title": "Main Results", "content": "In this section, we will demonstrate comprehensive capability in synthesizing speech of our model TEST, focusing on its performance in three key areas: authenticity detection, forged segment localization, and synthesis algorithm recognition. Additionally, we will examine the impact of the various configurations on performance."}, {"title": "Discussion", "content": "We present a viable benchmark for addressing comprehensive synthetic speech analysis and propose an innovative solution approach. Nonetheless, the Speech-Forensics dataset is constrained by limited resources, requiring further expansion in both data volume and speaker diversity. Additionally, while we have introduced TEST as a foundational method for comprehensive speech analysis, further exploration into tailored research methodologies specific to this domain remains imperative. These aspects represent potential avenues for refinement in future research endeavors."}, {"title": "Conclusion", "content": "This study has developed an extensive synthetic speech analysis dataset, named Speech-Forensic. This dataset comprises authentic, synthetic, and various high-quality algorithm-generated, multiple-segment partially forged speeches, effectively bridging the gap in comprehensive datasets within the synthetic speech analysis field. Furthermore, we introduce TEST, a temporal speech localization network to effectively integrate difference convolution, LSTM, and transformers, leveraging multi-scale pyramid features for dense prediction. This method can simultaneously perform authenticity detection, localization of multiple forged segments, and synthesis algorithm identification, without requiring complex post-processing. Experimental results demonstrate that TEST exhibits robust capabilities across these comprehensive tasks and achieves exceptional performance in each sub-task."}]}