{"title": "LLMS4OL 2024 Overview: The 1st Large Language Models for Ontology Learning Challenge", "authors": ["Hamed Babaei Giglou", "Jennifer D'Souza", "S\u00f6ren Auer"], "abstract": "This paper outlines the LLMs4OL 2024, the first edition of the Large Language Models for Ontology Learning Challenge. LLMs4OL is a community development initiative collocated with the 23rd International Semantic Web Conference (ISWC) to explore the potential of Large Language Models (LLMs) in Ontology Learning (OL), a vital process for enhancing the web with structured knowledge to improve interoperability. By leveraging LLMs, the challenge aims to advance understanding and innovation in OL, aligning with the goals of the Semantic Web to create a more intelligent and user-friendly web. In this paper, we give an overview of the 2024 edition of the LLMs4OL challenge\u00b9 and summarize the contributions.", "sections": [{"title": "Introduction", "content": "The Semantic Web aims to enrich the current web with structured knowledge and metadata, enabling enhanced interoperability and understanding across diverse systems. At the core of this endeavor is Ontology Learning (OL), a process that automates the extraction of structured knowledge from unstructured data [18], essential for constructing dynamic ontologies that underpin the Semantic Web. The emergence of Large Language Models (LLMs) like GPT-3 [7] and GPT-4 [25] has revolutionized natural language processing (NLP), demonstrating remarkable performance across tasks such as language translation, question answering, and text generation. These models are particularly adept at crystallizing existing textual knowledge from a vast array of sources, making them potentially valuable for OL, where the goal is to extract a shared conceptualization of concepts and relationships from diverse inputs [13]. The introduction of LLMs has thus opened up new avenues of research, including the exploration of their potential in automating the OL process."}, {"title": "LLMs4OL 2024 Tasks", "content": "In the LLMs4OL 2024 challenge, we have organized three main tasks which are centered around the ontology primitives [22] that comprise the following: 1. a set of strings that describe terminological lexical entries L for conceptual types; 2. a set of conceptual types T; 3. a taxonomy of types in a hierarchy H\u0442; 4. a set of non-taxonomic relations R described by their domain and range restrictions arranged in a heterarchy of relations HR; and 5. a set of axioms A that describe additional constraints on the ontology and make implicit facts explicit.\nTo address these primitives, the tasks for OL [23] are: 1) Corpus preparation \u2013 collecting source texts for building ontology. 2) Terminology extraction \u2013 extracting relevant terms from the texts. 3) Term typing \u2013 grouping similar terms into conceptual types. 4) Taxonomy construction - establishing \"is-a\" hierarchies between types. 5) Relationship extraction - extracting semantic relationships beyond \u201cis-a\" between types. 6) Axiom discovery - finding constraints rules for the ontology. These tasks constitute the LLMs4OL task paradigm as depicted in Figure 1. Assuming the corpus preparation step is done by reusing ontologies publicly released in the community, we introduced the following three main tasks for the first edition of the LLMS4OL challenge."}, {"title": "Task A \u2013 Term Typing", "content": "The Table 1 shows 10 subtasks for Task A across 6 distinct domains such as lexicosemantics, geographical locations, biomedical, biological, general knowledge, and food domains. This task is defined as \"discover the generalized type for a given lexical term\". For this task, for each ontology, participants are given training instances defined as following formalism.\n$f_{prompt}^{Task(L)} := [S?]. ([L], [T])$"}, {"title": "Task B \u2013 Taxonomy Discovery", "content": "After grouping terms under a conceptual type, in Task B, the goal is for given types \"discover the taxonomic hierarchy between types\", where the hierarchy between types is defined with an \"is-a\" relationship. Participants receive training instances for 6 distinct subtasks (described in Table 1) as :\n$f_{prompt}^{TaskBt(a, b)} := (Ta, T\u044c)$\nWhere Ta is the parent (superclass) of T\u266d, and Ty is the child (subclass) of Ta. The goal is to train a system to correctly identify the taxonomy between type. The training dataset will include term types and taxonomically related type pairs. In the test phase, participants work with just term types and must use their trained models to identify correct taxonomic relationships (type pairs). The types for the training and test phases are mutually exclusive. Furthermore, for the testing phase participants are required to post-process their outputs to return type pairs that follow the order of superclass-subclass related types."}, {"title": "Task C \u2013 Non-Taxonomic Relation Extraction", "content": "Nonetheless, the \"is-a\" relations are not the only relations in ontologies. So, Task C aims to \"identify non-taxonomic, semantic relations between types\". Training instances are given for three subtasks C.1 - UMLS, C.2 - GO, and C.3 - FoodOn as:\n$f_{prompt}^{(h, r,t)} := (Th, r, Tt)$\nWhere, Th and Tt are head and tail taxonomic types, respectively, and r is the non-taxonomic semantic relation between them, chosen from a predefined set R of semantic relations. Participants aimed to train a system to identify pairs of types, and then classify pairs of types into semantic relations. The training phase involves types, relations, and triples of semantic relations; the test phase requires applying the trained system to predict semantically related triples from given types and the set of relations.\nThe caveat here is that we do not expect participant systems to infer a semantic relation but rather establish semantically related types and classify their relation from a known set of predetermined relations. This implies that any manual ontology specification task predetermines which semantic relations hold for the given ontology. In an alternative scenario, where participants might have had to infer the semantic relation, we realize that the possibilities of semantic relations might have been rather vast. Hence we posit a more realistic task design by predetermining the possible set of semantic relations."}, {"title": "Evaluation", "content": "There are two main evaluation phases for the challenge, which are the following:\nFew-shot testing phase. Each ontology selected for system training will be divided into two parts: one part will be released for the training of the systems and another part will be reserved for the testing of systems in this phase.\nZero-shot testing phase. New ontologies that are unseen during training will be introduced. The objective is to evaluate the generalizability and transferability of the LLMs developed in this challenge.\nFor evaluation, we used the challenge datasets [5] \u2013 available at challenge GitHub2 repository with standard evaluation metrics used for all tasks. Given G(i) as a set of ground truth labels for sample i, and P(i) as a set of predicted labels for sample i, the precision P, recall R, and F1-score F1 are being calculated as follows:\n$P = \\frac{\\Sigma_{i} |G(i) \\cap P(i)|}{\\Sigma |P(i)|}, R = \\frac{\\Sigma_{i} |G(i) \\cap P(i)|}{\\Sigma |G(i)|}, F1 = \\frac{2 \\times P \\times R}{P+R}$\nWith precision, we assessed the percentage of the returned related pairs, while recall was used to measure the proportion of correct pairs that were accurately retrieved. In the end, the Fl-score was calculated as the harmonic mean of precision and recall, serving as a comparison metric for the participants' submissions. We used Codalab\u00b3 [26] submission platform to organize participants submissions and scoring."}, {"title": "Participant Systems and Results", "content": "The LLMs4OL 2024 challenge has inspired diverse solutions, showcasing the growing potential of LLMs for OL tasks. Using the Codalab submissions platform, for this challenge we set a limit of 10 submissions per day and a total of 30 submissions per subtask. We received 272 total submissions from 14 participants. In final, this challenge attracted the interest of the final eight research teams, as demonstrated by the various approaches they submitted for the subtasks. Each subtask of the competition depicted a rigorous field inherent to OL, which helped facilitate breakthroughs in finding generalized types (Task A), identifying taxonomic hierarchies (Task B), and extracting non-taxonomic relations (Task C), further scaffolding future AI advancements. Notably, teams employed varied strategies to tackle subtasks, such as fine-tuning, prompt-tuning, and retrieval-augmented generation (RAG). These approaches were used to analyze OL tasks across domains like lexicosemantics, geographical locations, biomedical concepts, and more (see Table 1 for subtasks and domains involved in this challenge). The summary of explored LLMs and subtasks are presented in Table 2 and in the following we will detail contributions and findings."}, {"title": "Participants Contributions", "content": "The results for Task A are presented in Table 3, for Task B in Table 5, and for Task C in Table 4.\nDSTI [1]. DSTI fine-tuned Flan-T5-Small [8] model for SubTasks A.1 - WordNet and A.2 GeoNames. Obtained Fl-score of 0.9716 for SubTask A.1 and ranked as a second team. But for GeoNames they did not submit the model to the leaderboard due to the larger nature of GeoNames dataset that required more computational resources. They introduced two approaches for OL. The first approach is fine-tuning LLMs using the zero-shot prompting method, the second approach is using a RAG pipeline using the General Text Embeddings (GTE)-Large [21] model as a retriever and fine-tuned LLM as a retriever. Due to the computational resources they preferred to use the Flan-T5-small model, and the results showed the effectiveness of their approach.\nRWTH-DBIS [27]. This team participated in tasks A and B (12 subtasks in total). For both tasks, they proposed a domain-specific continual training, fine-tuning, and knowledge-enhanced prompt-tuning approach. The models are firstly enriched with conceptual information related to terms and types. This is followed by CausalLM manner and task-specific fine-tuning using LLaMA-3-8B [11]. The proposed approach performs well on several subtasks, showcasing that incorporating domain-specific information and providing a list of classification types enhances inference performance. They concluded that in Task A, GPT-3.5-Turbo [24] outperformed fine-tuned open-source LLM, and incorporating domain-specific information and providing a list of types at prompt significantly enhances the performance.\nDaSeLab [6]. The DaSeLab team participated in UMLS, GeoNames, and WordNet subtasks. This team approach is based on fine-tuning a GPT-3.5-Turbo model. The result of fine-tuning on UMLS and GeoNames domains showed that fine-tuning of such model can achieve superior performance. The DaSeLab ranked first place in NCI (0.8249), GeoNames (0.5906), and SNOMEDCT_US (0.8829) subtasks (scores inside practices are F1-scores).\nTheGhost [28]. The TheGhost team investigated a variety of LLMs with a prompt-tuning approach. They are the first team in the challenge that explored 11 LLMs (the LLM list depicted in Table 2) for 8 subtasks of term typing tasks within a few-shot testing evaluation scenario. They showed the viability of soft prompt tuning for OL and the challenge of imbalanced class prompt tuning. Their finding supports the complexity of geographical and biological domains at the term typing task of OL.\nsilp nlp [19]. The silp_nlp team participated in all three tasks with a total of 15 subtasks. They ranked in first place in several subtasks including A.3 (FS) UMLS - MEDCIN ( 0.9382), A.4 (FS) - GO - Cellular Component (0.2726), A.4 (FS) - GO - Biological Process (0.2691), A.4 (FS) - GO - Molecular Function (0.2970), B.2 (FS) - Schema.org (0.6157), B.3 (FS) - UMLS, B.5 (FS) DBO (0.2109), and C.1 (FS) - UMLS (0.0783). They employed several machine learning techniques, such as Random Forest, Logistic Regression, and XGBoost, alongside advanced generative models like LLaMA-3-8B, Mixtral [16], and GPT-40 [25]. The results revealed that prompt-based methods were effective in some domains but not universally applicable. Notably, Random Forest models excelled in subtasks A.1 through A.4, while GPT-40 dominated the zero-shot tasks A.5 and A.6, as well as relation extraction tasks B and C. This team obtained in first-place in six subtasks and second place in five subtasks.\nTSOTSALearning [31]. The TSOTSALearning team focused on LLMs such as BERT [9] and GPT-4. Through experimentation on SubTask A.1 - WordNet dataset, they achieved an F1-score of 0.9264 with GPT-4, but significantly improved results when they combined BERT with rule-based strategies, leading to an F1-score of 0.9938 and ranked first place in WordNet dataset. Their findings showed the importance of incorporating rules into LLMs for enhanced accuracy in OL. However, they highlight the challenge of identifying appropriate rules, suggesting that future work should focus on automating rule detection and integrating them seamlessly into LLMs. The WordNet dataset is being considered as a low number of types and having a higher number of types makes it challenging to obtain highly accurate rules.\nSKH-NLP [14]. Team SKH-NLP participated in SubTask B.1 - GeoNames, where they developed a fine-tuning approach using the LLaMA-3-70B and BERT-Large [10]. This team obtained the first place in SubTask B.1 - GeoNames with an F1-score of 0.6557. Their comprehensive analysis demonstrates that BERT-Large, when fine-tuned, achieves performance close to the larger LLaMA-3-70B model.\nPhoenixes [29]. The Phoenixes team explored the application of a Retrieval Augmented Generation (RAG) approach within the 12 subtaks of the challenge. They introduced a promising RAG-specific formulation over all three tasks of OL, where a RAG system with minor changes was developed for both tasks A and B, later can be used as a two-step approach for task C. Task C consists of the following steps: Step 1 - runs the Task B approach for finding child-parent pairs and step 2 - applying the Task A approach for assigning the relations to the pairs. They incorporated Mistral-7B [15] as LLM and Dense Passage Retrieval (DPR) [17] model as the retriever model in the RAG framework. However, their results in both zero-shot and few-shot fall shorter than the fine-tuned models and this suggests that still fine-tuning is the key to obtain a high performance within OL."}, {"title": "Large Language Models", "content": "The participants in the challenge utilized a diverse array of LLMs, each bringing distinct strengths to the tasks. We detailed a breakdown of the key strengths of the prominent LLMs used.\nGPT FAMILY \u2013 GPT-3.5-Turbo, GPT-4, and GPT-40: GPT based LLMs, developed by OpenAI, are renowned for their advanced natural language understanding and generation capabilities. These models excel in context comprehension and can handle a variety of queries effectively, making them particularly suitable for tasks that require deep semantic understanding and detailed generation. Their ability to generalize from a wide range of training data allows them to perform well across various knowledge domains relevant ontologies [4,12]. GPT-3.5-Turbo was a popular choice among participants, with teams such as DaSeLab, RWTH-DBIS, and silp_nlp using the model and demonstrating its high adaptability and effectiveness across the various challenge subtasks. Furthermore, GPT-4 and GPT-40 as more advanced models over GPT-3, were explored by the teams: TSOTSA Learning and silp_nlp.\nLLAMA FAMILY \u2013 LLAMA-7B, LLaMA-2-7B, LLaMA-3-8B, and LLaMA-3-70B: The LLaMA models were another prominent choice among participants. With models like LLaMA-2 and LLaMA-3 featured by TheGhost, RWTH-DBIS, SKH-NLP, and silp_nlp, their popularity stems from their open-source, efficiency, and scalability. These models' strengths in handling large-scale data and intricate details made them well-suited for comprehensive multi-dimensional data interpretation.\nBLOOM FAMILY \u2013 BLOOM-1B7, BLOOM-3B, and BLOOM-7B1: BLOOM [2] models, featured in our original research work [4], gained traction due to their open-access nature and collaborative development. TheGhost, in particular, utilized a range of BLOOM models for their flexibility and multilingual capabilities.\nBIOMEDICAL FAMILY \u2013 BioMistral-7B and OpenBioLLM-8B: BioMistral-7B [20], as a domain-specific fine-tuned variant of Mistral-7B, and OpenBioLLM-8B [3], as a domain-specific fine-tuned variant of LLaMA-3-8B, were utilized for their domain-specific strengths in biomedical contexts. TheGhost's use of these models highlights their importance in tasks requiring detailed biomedical terminology and concepts, emphasizing their significance in the specialized subfields of the challenge.\nMISTRAL FAMILY Mistral-7B and Mixtral-8x7B: Mistral-7B, part of the Mistral family of models, was noted for its performance in the challenge by teams like Phoenixes and TheGhost. Moreover, Mixtral-8x7B was utilized by the team silp_nlp.\nOTHERS - Flan-T5, GTE-Large, Sentence-BERT, and DPR: Flan-T5 and GTE-Large were chosen for their adaptability and fine-tuning capabilities. DSTI recognized their potential in fine-tuning and handling diverse NLP tasks efficiently when there are limited computational resources. Sentence-BERT was prominently used for tasks involving semantic similarity and sentence-level embeddings. Its popularity among participants like SKH-NLP and Phoenixes. Phoenixes used DPR for the retrieval model of the RAG approach."}, {"title": "Trade-offs Between Precision and Recall", "content": "Across the tasks, a clear trend emerges among the participating teams. Teams like silp_nlp often exhibit high precision but lower recall, particularly in subtasks related to GO and UMLS ontologies. This suggests that while silp_nlp is adept at avoiding false positives and making accurate predictions, it frequently misses relevant instances, indicating a more conservative approach. However, teams such as RWTH-DBIS and Phoenixes display a different trend, where recall is relatively higher than precision. These teams retrieve a larger number of relevant results but at the cost of precision, indicating that they tend to capture a broad set of possible answers, including many false positives. Their approach may be useful in tasks where coverage is prioritized over accuracy, but it also introduces challenges in filtering out noise.\nTeams that manage to balance both precision and recall, such as DaSeLab and SKH-NLP, stand out for their well-rounded performance. These teams perform consistently across different subtasks by finding a middle ground between retrieving enough relevant results and minimizing false positives. DaSeLab, for example, shows balanced performance across multiple subtasks, especially in UMLS-related tasks, suggesting a more effective strategy. Meanwhile, SKH-NLP stands out in the GeoNames taxonomy discovery task, where it achieves high precision and recall, demonstrating its capability to capture relevant information without sacrificing accuracy.\nIn more challenging tasks, such as non-taxonomic relation extraction, the disparity between precision and recall becomes particularly pronounced. For example, both silp_nlp and Phoenixes struggle, with silp_nlp showing low precision but managing to retrieve more relevant results than Phoenixes, which has very low recall. This suggests that these tasks may require more sophisticated models or techniques to achieve higher performance. Overall, the results reflect that teams vary significantly in how they prioritize precision and recall, depending on the specific subtask, with some teams excelling in precision-oriented tasks while others show better results in recall-sensitive subtasks."}, {"title": "Discussion", "content": "Performance Analysis. As the participating teams navigated through the zero-shot and few-shot testing phases of the LLMs4OL 2024 challenge, notable variations in performance underscored the importance of model adaptability and data-specific adjustments. Few-shot tasks, particularly those involving geographical, biological, and biomedical domains, highlighted the critical need for specialized model tuning and the strategic use of training data to achieve high precision and recall rates. This indicates that achieving optimal performance in real-world ontology challenges requires not only selecting the right LLMs but also fine-tuning them to align with the specific characteristics of the domains and tasks at hand. Additionally, studies show that for Task A, even smaller models like Flan-T5-Small with 80M parameters can perform well when there are fewer types. However, as the number of types increases, larger models, such as those with 7B parameters, tend to perform better. One reason for the popularity of 7B models is that Parameter-Efficient Fine-Tuning (PEFT) [30] fine-tuning requires less memory compared to traditional fine-tuning methods. Many participants also incorporated external knowledge, such as type definitions, synthesis data using LLMs, or general knowledge graphs (KGs) to build answer sets. These strategies have demonstrated a positive impact on fine-tuning performance.\nComplexity Across Domains and Tasks. The results indicated that certain domains and tasks, such as biomedical term typing and non-taxonomic relation extraction, were more challenging than others. The variation in performance across tasks, particularly in relation to term complexity (e.g., Gene Ontology), highlights the complexity of certain knowledge domains. This still requires specialized approaches. The Phoenixes (on all three tasks) and DSTI (on task A only) teams introduced a formulation based on Retrieval-Augmented Generation (RAG) approaches with success, indicating that combining LLM generation capabilities with retrieval mechanisms can enhance accuracy in OL tasks. This approach is particularly suitable due to the hybrid framework with high adaptability to be extended with different components.\nFew-Shot and Zero-Shot Testing Phases. While many models performed well in the few-shot phase, the zero-shot testing phase exposed limitations in the generalization capabilities of LLMs. Models like GPT-3.5 and GPT-4 demonstrated strong performance, but there were notable drops when transitioning from few-shot to zero-shot testing phases. More research is needed to improve the transferability and robustness of LLMs across unseen domains and ontologies.\nTask A vs Task C. From a task perspective, Task C attracted only two teams, indicating it was perceived as highly challenging. Non-taxonomic relation extraction requires identifying complex relationships between terms that go beyond hierarchical (taxonomy-based) relations, which is a significantly more intricate task. Unlike simple is-a relationships, non-taxonomic relations are more diverse, context-dependent, and require a deeper understanding of the subject matter. Extracting these relations often involves dealing with ambiguous or implicit connections, requiring models to infer meanings that might not be explicit. This complexity might have discouraged more teams from participating, as success in this task requires advanced techniques, often combining deep semantic understanding with domain-specific knowledge. On the other hand, Task A, term typing, had much higher participation compared to Task C. This task involves classifying terms into predefined categories, a more familiar task for many researchers. Term typing is conceptually simpler because it involves assigning a label to a term, which is something that even general-purpose LLMs can do relatively well. There is a clear, finite set of categories or types, and many participants experimented with text classification approaches."}, {"title": "Conclusion", "content": "The 1st Large Language Models for Ontology Learning Challenge at ISWC 2024 has revealed the emerging potential of LLMs beyond previous studies of OL tasks. The diverse range of participant systems, including fine-tuning, prompt-tuning, and retrieval-augmented generation approaches, demonstrated how adaptable LLMs can be when handling complex ontological data across various domains. The integration of diverse LLMs like GPT-40, GPT-3.5, LLaMA-3, and Mistral underscored the versatility of LLMs.\nThrough this challenge, key insights were garnered regarding the strengths and limitations of current LLMs for OL. Notably, while LLMs have shown a remarkable capacity to generalize across unseen tasks (as evidenced by their performance in few-shot and zero-shot scenarios), certain domains such as biomedical and geographical ontologies posed unique challenges, particularly in terms of class imbalance and complex taxonomies. These challenges opened pathways for future research, emphasizing the need for scalable LLM training and the refinement of prompt-based methods to handle highly specialized ontologies.\nMoreover, the variety of approaches suggests that hybrid methods combining LLMs with domain-specific knowledge are particularly effective. Moving forward, research should focus on improving the interpretability and scalability of LLM-based OL systems to enable even more accurate and dynamic knowledge extraction. This challenge has laid the groundwork for expanding LLM capabilities in the context of the Semantic Web, fostering innovation and collaboration in building the next generation of intelligent web technologies."}]}