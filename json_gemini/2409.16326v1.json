{"title": "AUTOMATED SPATIO-TEMPORAL WEATHER MODELING FOR LOAD FORECASTING", "authors": ["Julie Keisler", "Margaux Br\u00e9g\u00e8re"], "abstract": "Electricity is difficult to store, except at prohibitive cost, and therefore the balance between generation and load must be maintained at all times. Electricity is traditionally managed by anticipating demand and intermittent production (wind, solar) and matching flexible production (hydro, nuclear, coal and gas). Accurate forecasting of electricity load and renewable production is therefore essential to ensure grid performance and stability. Both are highly dependent on meteorological variables (temperature, wind, sunshine). These dependencies are complex and difficult to model. On the one hand, spatial variations do not have a uniform impact because population, industry, and wind and solar farms are not evenly distributed across the territory. On the other hand, temporal variations can have delayed effects on load (due to the thermal inertia of buildings). With access to observations from different weather stations and simulated data from meteorological models, we believe that both phenomena can be modeled together. In today's state-of-the-art load forecasting models, the spatio-temporal modeling of the weather is fixed. In this work, we aim to take advantage of the automated representation and spatio-temporal feature extraction capabilities of deep neural networks to improve spatio-temporal weather modeling for load forecasting. We compare our deep learning-based methodology with the state-of-the-art on French national load. This methodology could also be fully adapted to forecasting renewable energy production.", "sections": [{"title": "Introduction", "content": "The cost of large-scale electricity storage remains high, and the current systems in use remain inefficient. Furthermore, the secure and smooth operation of the power grid depends on maintaining a constant and precise balance between electricity production and demand. The aforementioned equilibrium is achieved via the adaptability of programmable power plants, which modify their production in accordance with load forecasts. It is therefore essential to have accurate forecasts of both electricity demand and the output of renewable energy sources in order to schedule power plants and maintain grid stability. The two signals depend on meteorological variables, specifically temperature, wind speed, and solar radiation, which vary in both space and time. As consumer demand and renewable energy generation facilities are not evenly distributed across a given area, variations in meteorological conditions at a particular location will affect these signals. In addition, temporal weather variations can have a delayed effect, particularly with regard to the load, due to the thermal inertia of buildings and the reactivity of consumers. It can be assumed that the manner in which temporal and spatial variations in weather patterns are modelled has a significant impact on the efficacy of the forecasting models.\nThis article concentrates on short-term load forecasting with a forecast time horizon of one day. Such forecasts enable power system operators to make adjustments to production and spot market prices. This signal is challenging to forecast due to its dependence on a multitude of variables, including meteorological factors (temperature, wind, etc.) and calendar-related elements (holidays, weekdays, etc.). Consequently, the models employed in the industry and those that have been successful in load forecasting competitions (see Farrokhabadi et al. [2022], for a recent example) are regression-based models, such as Generalized Additive Models (GAMs) or tree-based models. In general, lagged load is not employed. While this variable offers valuable insight, it can also limit the model's interpretability by reducing the importance of other variables. Furthermore, the model would become unusable in case of data stream issues. To address this challenge while leveraging the insights offered by lagged load, a promising approach is to construct a static model"}, {"title": "Related Work", "content": "The load signal can be explained almost entirely by a set of explanatory variables that do not include the past target data. Consequently, the majority of performing models are based on regression rather than time series techniques. Multiple Linear Regressions (MLRs) can be used to calculate the relationships between multiple variables. However, the relationships between load and some exogenous variables are not linear, and thus, these models require the specification of functional forms for these variables. For instance, Generalized Additive Models (GAMs) employ a spline basis to model the nonlinear effects, as detailed in [Pierrot and Goude, 2011]. These models, which are highly accurate for load forecasting, are used in industry and have been the winners of several competitions (see, for example, Nedellec et al. [2014]).\nDNNs have dominated the fields of computer vision and natural language processing for some years now. They offer the ability to process data in a variety of formats - e.g., text, images, graphs - make them particularly interesting for load forecasting, which depends on a large number of explanatory variables that may come from data sets in a variety of formats. Recently, they have also revolutionized the field of weather forecasting, proving more effective than Numerical Weather Prediction (see for example [Pathak et al., 2022] and [Lam et al., 2022]). While the initial work was based on gridded reanalysis data, McNally et al. [2024] have shown that DNNs could also be effective in extracting spatio-temporal features directly from raw weather data. In the field of load forecasting, they are currently less widely used than multilinear regression models. However, Keisler et al. [2024a] have demonstrated that by optimizing the structure and hyperparameters of DNNs, it is possible to develop models that surpass the current state of the art. In their article, the authors optimize DNNs using the DRAGON package\u00b9 (see Keisler et al. [2024b]). The models are represented using Directed Acyclic Graphs (DAGs). The search space is defined as \\( \\Omega = (A \\times {\\Lambda(\\alpha), \\alpha\\in A}) \\), where A is the set of all considered architectures and \\( \\Lambda(a) \\) is the set of all considered hyperparameters induced by the architecture a. Each architecture a \u2208 A is represented by a DAG \u0393, where the nodes are the DNN layers and the edges are the connections between them. See Keisler et al. [2024b] for more information about this search space.\nFinally, this paper deals with short-term forecasting of electricity consumption. The COVID crisis and recent european energy crises have highlighted the importance of models that can rapidly adapt to new contexts. This is why research in the field has focused on different techniques for online model adaptation. These include the Kalman filter adaptation of Generalized Additive Models (GAMs), which won the post-covid electricity load forecasting competition (see Farrokhabadi et al. [2022] and De Vilmarest and Goude [2022]). The adaptation is done by multiplying the GAM"}, {"title": "Weather Modeling with Deep Neural Networks for load forecasting", "content": "In this work, we aim to forecast at each time step \\( t\\in [1, . . ., T] \\), with \\( T \\in N^* \\) a daily load variable \\( y_t \\in R^H \\), using a features vector \\( x_t = (W_t, C_t) \\in (R^{H\\times V\\times I} \\times R^{H\\times F}) \\), where T represents the number of days in the data set and H the number of time steps within a day. The features vector \\( x_t \\) is made of two elements: \\( w_t \\) gathering the spatio-temporal weather data and \\( c_t \\) containing the other \\( F\\in N \\) explanatory variables such as calendar data (e.g., months, years holidays). The vector \\( w_t \\in R^{H\\times V\\times I} \\) contains the forecasts at time t from different weather stations, or to a weather forecast grid, produced by, for example, a NWP model. The dimension \\( I \\in N^* \\) corresponds to the number of spatial points (i.e., the number of weather stations in the first case and the number of grid points in the second) and \\( V \\in N^* \\) to the number of weather variables present in \\( w_t \\) (e.g., temperature, wind speed, solar radiation)."}, {"title": "Spatio-temporal weather modeling", "content": "In order to be integrated into load forecasting models, spatio-temporal weather is deterministically transformed into \"electrical\" weather. Several functions are applied in order to reduce the information and extract what will be most useful for load forecasting. These functions have been defined with industry expertise, but are not adapted to a particular dataset or period. An example of such functions for the french load signal are given by the French Transmission System Operator called RTE.\nPonderation The first step is to switch back from the multi-variate, spatial signal to an aggregated univariate signal. The I spatial locations are not necessarily evenly distributed throughout the considered region and do not contribute equally to the electrical weather. For instance locations in densely-populated parts have more weights than others located in isolated areas. Let's denote \\( w_i^v \\in R \\) the forecast of the weather variable v (e.g. temperature) at time step h of the location i, and \\( a_i^v \\in [0, 1] \\) the weight of the location i. The weights are shared accross the variables v. The aggregated signal at time h can then be written as:\n\\( w^v_h = \\sum_{i=1}^I a_i^v w_{h,i}^v \\), with: \\( \\sum_{i=1}^I a_i^v = 1 \\).\nThis behavior can easily be reproduced with a Multi-Layer Perceptron (MLP):\n\\( w^v = Aw^v_h + b \\), with \\( w^v_h = [w_{h,i}^v]_{i=1}^I, A = [a_i^v]_{i=1}^I \\) and \\( b = 0\\)\nHowever, a Deep Neural Networks requires the scaling of the input data. Each v variable is scaled independently, so that variables with large amplitudes (e.g. temperature) don't override the others. We scale each location i independently and denote \\( \\tilde{w}_{h,i}^v = (w_{h,i}^v - \\min_{h\\in [1,...,H]} w_{h,i}^v) / (\\max_{h\\in [1,...,H]} w_{h,i}^v - \\min_{h\\in [1,...,H]} w_{h,i}^v) \\) the min-max scaled version of \\( w_{h,i}^v \\), with \\( \\min_i^v = \\min_{h\\in [1,...,H]} w_{h,i}^v \\) and \\( \\max_i^v = \\max_{h\\in [1,...,H]} w_{h,i}^v \\in R \\). If we consider the aggregated target to also be scaled, with \\( \\min^v = \\min_{h\\in [1,...,H]} w_h^v \\) and \\( \\max^v = \\max_{h\\in [1,...,H]} w_h^v \\in R \\), we have:\n\\( \\tilde{w}_h^v = (w^v_h - \\min^v)/( \\max^v - \\min^v)\n= (\\sum_{i=1}^I a_i^v w_{h,i}^v - \\min^v)/( \\max^v - \\min^v)\n= (\\sum_{i=1}^I a_i^v ((\\tilde{w}_{h,i}^v(\\max_{h,i}^v - \\min_{h,i}^v) + \\min_{h,i}^v) - \\min^v)/( \\max^v - \\min^v)\n= \\sum_{i=1}^I a_i^v ((\\max_{h,i}^v - \\min_{h,i}^v)/ ( \\max^v - \\min^v) \\tilde{w}_{h,i}^v + (\\min_{h,i}^v - \\min^v)/( \\max^v - \\min^v)\n= A_v \\tilde{w}^v_h + b_v\\)\nwith \\( \\tilde{w}_h^v = [\\tilde{w}_{h,i}^v]_{i=1}^I, A_v = [a_i^v(\\max_{h,i}^v - \\min_{h,i}^v)/ ( \\max^v - \\min^v)]_{i=1}^I\\) and \\( b_v = \\sum_{i=1}^I (\\min_{h,i}^v - \\min^v)/( \\max^v - \\min^v) \\).\nTherefore, we need V MLP layers to aggregate the data variable by variable."}, {"title": "Temperature smoothing", "content": "Load does not respond instantaneously to changes in the weather. In particular, temperature effects are more gradual due to the thermal inertia of buildings. This is why the concept of smoothed temperature is useful for understanding the factors that influence electricity consumption. Exponential smoothing is typically employed in this context. We denote, for a day t, \\( T_t = [T_{t,1},...,T_{t,H}] \\in R^H \\) the aggregated temperature and \\( \\overline{T}_t = [\\overline{T}_{t,1},...,\\overline{T}_{t,H}] \\in R^H \\) the smoothed version. We define:\n\\( \\overline{T}_{t,1} = (1 - \\alpha)T_{t,1} + \\alpha \\overline{T}_{t-1,H}, \\) and, \\( \\forall i \\in [2, H] : \\overline{T}_{t,i} = (1 - \\alpha)T_{t,i} + \\alpha \\overline{T}_{t,i-1}, \\)\nwhere \\( \\alpha \\in [0, 1] \\) is the smooth coefficient, which can be optimized.\nRecurrent Neural Networks Smoothed temperature requires at each time step t to pass \\( \\overline{T}_{t,H} \\in R \\) to the next time step t + 1. Such information passing can be reproduced by Recurrent Neural Networks (RNNs), which are designed with a memory vector. The equations of a recurrent neural network with input \\( T_t \\in R^H \\) and output \\( \\overline{T}_t \\in R^H \\) can be written as:\n\\( \\overline{T}_t = \\phi(T_tW_1 + b_1 +\\overline{T}_{t-1}W_2 + b_2), \\)\nwhere \\( \\phi \\) is an activation function (typically non-linear), and \\( W_1 \\in R^{H\\times H}, W_2 \\in R^{H\\times H}, b_1 \\in R^H \\) and \\( b_2 \\in R^H \\) are some parameters which can be learned through gradient descent. For writing simplicity, we now index our temperature by t*, such that, if \\( t^* = {t, i} \\), we have, if i < H:\n\\( t^* = {t, i + 1} \\)\nelse, \\( t^* + 1 = {t + 1,1} \\).\nLet \\( \\tau > 0 \\). To compute \\( \\overline{T}_{t^*} \\) based on the smoothed temperature at instant \\( t^* - \\tau \\), \\( \\overline{T}_{t^* - \\tau} \\), and the sequence of temperatures \\( \\overline{T}_{t^* - \\tau+1},..., \\overline{T}_{t^*} \\), we have:\n\\( \\overline{T}_{t^*-\\tau+1} = \\overline{T}_{t^*-\\tau} \\)\n\\( \\overline{T}_{t^*-\\tau+1} = (1 - \\alpha)T_{t^*-\\tau+1} + \\alpha \\overline{T}_{t^* - \\tau} \\)\n: :\n\\( \\overline{T}_{t^*} = \\sum_{s=0}^{\\tau-1} \\alpha^s(1 - \\alpha)T_{t^*-s} + \\alpha \\overline{T}_{t^*-\\tau}.\\)"}, {"title": "Online learning", "content": "The last layer of the search space from Keisler et al. [2024a] is a linear layer, transforming an input \\( h_t \\in R^{H\\times D} \\) into an output \\( y_t \\in R^{H} \\), where \\( y_t \\) is the load consumption for the day t, H the number of instants during the day and \\( D \\in N^* \\) is the hidden dimension within the network before the last layer. Let's name \\( A_F \\in R^D \\) and \\( b_F \\in R \\) respectively the weights and bias matrices of this last MLP layer, we have:\n\\( y_t = A_Fh_t + b_F = \\sum_{i=1}^{D} a^i_Fh_t + b_F.\\)\nTo adapt our DNN, we use a daily Kalman state vector \\( \\theta_t \\in R^{D} \\) to adapt the coefficients of equation 8:\n\\( y_t = \\theta^T (A_Fh_t + b_F) = \\sum_{i=1}^{D} (\\theta^i a^i_Fh_t + b_F) + \\epsilon_t \\)\n\\( \\theta_{t+1} = \\theta_t + \\eta_t,\\)"}, {"title": "Automated weather modeling", "content": "This Section presents the integration of the various elements presented in Section 3, namely the weather modeling and Kalman adaptation modules, into EnergyDragon with the objective of optimizing them."}, {"title": "Objective function", "content": "The objective is to identify the optimal DNN \\( f \\in \\Omega \\) with the lowest forecast error on a given load signal with a short forecast horizon (e.g., 24 hours). The load dataset, denoted by D, contains the spatio-temporal W data, the explanatory variables C and the target (the load signal) Y. For any subset \\( D_o = ((W_o, C_o), Y_o) \\), the forecast error \\( l_{MSE} \\) is defined as:\n\\( l_{MSE}: \\Omega \\times D \\rightarrow R \\)\n\\( f \\times D_o \\rightarrow l_{mse}(f(D_o)) = l_{mse}(Y_o, f (W_o, C_o)) = MSE(Y_o, f (W_o, C_o)) .\\)\nWhere MSE is the Mean Squared Error. Each DNN \\( f \\in \\Omega \\) is parameterized by:\n\u2022 \u03b1 \u2208 A, its architecture, optimized by the framework.\n\u2022 \\( \\lambda \\in \\Lambda(\\alpha) \\), its hyperparameters, optimized by the framework, where \\( \\Lambda(\\alpha) \\) is induced by \u03b1. The hyperparam-eters include Q and \u03c3 from the Kalman adaptation. It should be noted that the shape of Q depends on the architecture and hyperparameters of the networks.\n\u2022 \\( \\delta \\in \\Delta(\\alpha, \\lambda) \\), the DNN weights, where \\( \\Delta(\\alpha, \\lambda) \\) is generated by \u03b1 and \u03bb and optimized by gradient descent when training the model.\nThe optimization process is done in several steps. First, the optimal DNN weights \\( \\delta \\in \\Delta(\\alpha, \\lambda) \\) for a given ar-chitecture \u03b1 \u2208 A and set of hyperparameters \\( \\lambda \\in \\Lambda(\\alpha) \\) are found using gradient descent over the training set \\( D_{train} = ((W_{train}, C_{train}), Y_{train})) = (X_{train}, Y_{train}) \\):\n\\( \\delta \\in \\underset{\\delta \\in \\Delta(\\alpha, \\lambda)}{argmin}(MSE (f(X_{train}, Y_{train}))). \\)\nOnce the DNN is trained, the performance of the selected \u03b1 and \u03bb are evaluated on \\( D_{valid} \\). As Q and \u03c3 are part of \\( \\lambda \\), the evaluation is done using the- Kalman recalibration of the model First, the state vector \\( \\theta \\in R^{T \\times D} \\) is estimated on the last MLP layer of the trained DNN \\( \\hat{f} \\) using Equations 9 and 10. Let's have \\( \\Theta_{\\alpha, \\lambda} (\\hat{f}(X_{valid})) \\) the recalibration of \\( \\hat{f}^{\\alpha, \\lambda} (X_{valid}) \\) as defined Equation 9. The architecture \u03b1 and hyperparameters \u03bb are optimized as:\n\\( (\\alpha, \\lambda) \\epsilon \\underset{(\\alpha, \\lambda) \\in (A \\times \\Lambda(\\alpha))}{argmin} l_{MAPE} (\\Theta_{\\alpha, \\lambda} (\\hat{f}(X_{valid}))), Y_{valid} \\) .\nThe framework output will be \\( l_{MAPE} \\), the Mean Absolute Percentage Error. Given a load series Y = (y1... yn) and the predictions \\( \\hat{Y} = (\\hat{y}_1, \u2026\u2026 \\hat{y}_n) \\), \\( MAPE(Y,\\hat{Y}) = 1/n \\sum_{i=1}^{n} | (y_i - \\hat{y}_i)/y_i | \\). The MAPE is computed using the DNN with the best architecture, hyperparameters, weights and calibration using Kalman on the test dataset:\n\\( MAPE(\\Theta_{\\alpha, \\lambda} (\\hat{f}(X_{test}))), Y_{test}). \\)\nIn the following section (4.2), we explicit our search space, defined by A and \\( \\Lambda(\\alpha) \\)."}, {"title": "Search space", "content": "The search space used in this work extends the one from Keisler et al. [2024a] by adding a weather modeling and a Kalman module, and is depicted Figure 1. Each DNN \\( f \\in \\Omega \\) maps two batched inputs: \\( w_b \\in R^{B\\times H\\times V\\times I} \\) containing the spatio-temporal weather and \\( c_b \\in R^{B\\times H\\times F} \\) containing the other explanatory variables into a target \\( y_b \\in R^{B\\times H} \\), where \\( B \\in N^* \\) represents the size of the batch."}, {"title": "Experiments", "content": "In this section, we evaluate the efficacy of our weather modeling and Kalman adaptation techniques on the French load dataset from January 2023 to May 2024. In contrast with the paper by Keisler et al. [2024a], which compares data from a relatively stable and distant period, our analysis focuses on a more dynamic and operational context. The training period spans from 2018 to 2022 and encompasses both the global pandemic caused by the SARS-CoV-2 virus and the subsequent energy crisis at the end of 2022. The test period encompasses the winter of 2023, during which consumers were encouraged to voluntarily reduce their consumption, a period commonly referred to as the \"sobriety period\"."}, {"title": "Conclusion", "content": "In conclusion, this article builds upon the work initiated by Keisler et al. [2024a] on automated deep learning for load forecasting. In this initial article, a framework, designated as EnergyDragon, was proposed for the optimization of both the architecture and hyperparameters of Deep Neural Networks, specifically designed for load forecasting. This article improves upon the previous work by incorporating an automated spatio-temporal weather modeling approach based on DNN and a recalibration module based on Kalman filtering. The efficacy of our approach is evaluated in a more dynamic and operational context, namely the national French load during the sobriety period."}]}