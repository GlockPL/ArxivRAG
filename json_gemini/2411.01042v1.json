{"title": "Introduction to AI Safety, Ethics, and Society", "authors": ["Dan Hendrycks"], "abstract": "This is an introduction to the risks and societal impact of artificial intelligence (AI) from an ethical perspective. It provides an overview of the core concepts and debates in AI safety, ethics, and society. The text covers topics such as malicious use, AI arms race dynamics, organizational risks, rogue AIs, complex systems, monitoring, robustness, alignment, systemic safety, safety and general capabilities, governance, utility functions, law, fairness, the economic engine, wellbeing, preferences, happiness, social welfare functions, moral uncertainty, collective action problems, evolutionary pressures, and compute governance. This serves as a fundamental introduction to the broader field of AI safety, ethics, and society.", "sections": [{"title": "Introduction", "content": "Artificial Intelligence is rapidly embedding itself within militaries, economies, and societies, reshaping their very foundations. Given the depth and breadth of its con-sequences, it has never been more pressing to understand how to ensure that AI systems are safe, ethical, and have a positive societal impact.\nThis book aims to provide a comprehensive approach to understanding AI risk. Our primary goals include consolidating fragmented knowledge on AI risk, increasing the precision of core ideas, and reducing barriers to entry by making content simpler and more comprehensible. The book has been designed to be accessible to readers from diverse backgrounds. You do not need to have studied AI, philosophy, or other such topics. The content is skimmable and somewhat modular, so that you can choose which chapters to read. We introduce mathematical formulas in a few places to specify claims more precisely, but readers should be able to understand the main points without these.\nAI risk is multidisciplinary. Most people think about problems in AI risk in terms of largely implicit conceptual models which significantly affect how they approach these challenges. We aim to replace these implicit models with explicit, time-tested models. A full understanding of the risks posed by AI requires knowledge in several disparate academic disciplines, which have so far not been combined in a single text. This book was written to fill that gap and adequately equip readers to analyze AI risk, and moves beyond the confines of machine learning to provide a holistic understanding of AI risk. We draw on well-established ideas and frameworks from the fields of engineering, economics, biology, complex systems, philosophy, and other disciplines that can provide insights into AI risks and how to manage them. Our aim is to equip readers with a solid understanding of the technical, ethical, and governance challenges that we will need to meet in order to harness advanced AI in a beneficial way.\nIn order to understand the challenges of AI safety, it is important to consider the broader context within which AI systems are being developed and applied. The de-cisions of and interplay between AI developers, policy-makers, militaries, and other actors will play an important role in shaping this context. Since AI influences many different spheres, we have deliberately selected time-tested, formal frameworks to provide multiple lenses for thinking about AI, relevant actors, and AI's impacts. The frameworks and concepts we use are highly general and are useful for reasoning about various forms of intelligence, ranging from individual human beings to corporations, states, and AI systems. While some sections of the book focus more directly on AI"}, {"title": "Overview of Catastrophic AI Risks", "content": "In this chapter, we will give a brief and informal description of many major societal-scale risks from AI, focusing on AI risks that could lead to highly severe or even catastrophic societal outcomes. This provides some background and motivation before we discuss specific challenges with more depth and rigor in the following chapters.\nThe world as we know it today is not normal. We take for granted that we can talk instantaneously with people thousands of miles away, fly to the other side of the world in less than a day, and access vast mountains of accumulated knowledge on devices we carry around in our pockets. These realities seemed far-fetched decades ago, and would have been inconceivable to people living centuries ago. The ways we live, work, travel, and communicate have only been possible for a tiny fraction of human history.\nYet, when we look at the bigger picture, a broader pattern emerges: accelerating development. Hundreds of thousands of years elapsed between the time Homo sapiens appeared on Earth and the agricultural revolution. Then, thousands of years passed before the industrial revolution. Now, just centuries later, the artificial intelligence (AI) revolution is beginning. The march of history is not constant\u2014it is rapidly accelerating.\nWe can capture this trend quantitatively in Figure 1.1, which shows how es-timated gross world product has changed over time [Roodman2020OnTP,Davidson2021]. The hyperbolic growth it depicts might be explained by the fact that, as technology advances, the rate of technological advancement also tends to increase. Empowered with new technologies, people can innovate faster than they could before. Thus, the gap in time between each landmark development narrows.\nIt is the rapid pace of development, as much as the sophistication of our technol-ogy, that makes the present day an unprecedented time in human history. We have reached a point where technological advancements can transform the world beyond recognition within a human lifetime. For example, people who have lived through the creation of the internet can remember a time when our now digitally-connected world would have seemed like science fiction.\nFrom a historical perspective, it appears possible that the same amount of develop-ment could now be condensed in an even shorter timeframe. We might not be certain that this will occur, but neither can we rule it out. We therefore wonder: what new technology might usher in the next big acceleration? In light of recent advances, AI seems an increasingly plausible candidate. Perhaps, as AI continues to become more powerful, it could lead to a qualitative shift in the world, more profound than any we have experienced so far. It could be the most impactful period in history, though it could also be the last.\nAlthough technological advancement has often improved people's lives, we ought to remember that, as our technology grows in power, so too does its destructive potential. Consider the invention of nuclear weapons. Last century, for the first time in our species' history, humanity possessed the ability to destroy itself, and the world suddenly became much more fragile.\nOur newfound vulnerability revealed itself in unnerving clarity during the Cold War. On a Saturday in October 1962, the Cuban Missile Crisis was cascading out of control. US warships enforcing the blockade of Cuba detected a Soviet submarine and attempted to force it to the surface by dropping low-explosive depth charges. The submarine was out of radio contact, and its crew had no idea whether World War III had already begun. A broken ventilator raised the temperature up to 140\u00b0F in some parts of the submarine, causing crew members to fall unconscious as depth charges exploded nearby.\nThe submarine carried a nuclear-armed torpedo, which required consent from both the captain and political officer to launch. Both provided it. On any other submarine in Cuban waters that day, that torpedo would have launched and a nuclear third world war may have followed. Fortunately, a man named Vasili Arkhipov was also on the submarine. Arkhipov was the commander of the entire flotilla and by sheer luck happened to be on that particular submarine. He talked the captain down from his rage, convincing him to await further orders from Moscow. He averted a nuclear war and saved millions or billions of lives and possibly civilization itself.\nCarl Sagan once observed, \u201cIf we continue to accumulate only power and not wisdom,we will surely destroy ourselves\" [sagan1994pale]. Sagan was correct: The power ofnuclear weapons was not one we were ready for. Overall, it has been luck rather thanwisdom that has saved humanity from nuclear annihilation, with multiple recordedinstances of a single individual preventing a full-scale nuclear war.\nAI is now poised to become a powerful technology with destructive potential similar to nuclear weapons. We do not want to repeat the Cuban Missile Crisis. We do not want to slide toward a moment of peril where our survival hinges on luck rather than the ability to use this technology wisely. Instead, we need to work proactively to mitigate the risks it poses. This necessitates a better understanding of what could go wrong and what to do about it.\nLuckily, AI systems are not yet advanced enough to contribute to every risk we discuss. But that is cold comfort in a time when AI development is advancing at an unprecedented and unpredictable rate. We consider risks arising from both present-day Als and Als that are likely to exist in the near future. It is possible that if we wait for more advanced systems to be developed before taking action, it may be toolate.\nIn this chapter, we will explore various ways in which powerful Als could bring about catastrophic events with devastating consequences for vast numbers of people. We will also discuss how Als could present existential risks catastrophes from which humanity would be unable to recover. The most obvious such risk is extinction, but there are other outcomes, such as creating a permanent dystopian society, which would also constitute an existential catastrophe. As further discussed in this book's Introduction, we do not intend to cover all risks or harms that AI may pose in an exhaustive manner, and many of these fall outside the scope of this chapter. We outline many possible scenarios, some of which are more likely than others and some of which are mutually incompatible with each other. This approach is motivated by the principles of risk management. We prioritize asking \u201cwhat could go wrong?\" rather than reactively waiting for catastrophes to occur. This proactive mindset enables us to anticipate and mitigate catastrophic risks before it's too late.\nTo help orient the discussion, we decompose catastrophic risks from Als into four risk sources that warrant intervention:\n\u2022 Malicious use: Malicious actors using Als to cause large-scale devastation.\n\u2022 AI race: Competitive pressures that could drive us to deploy Als in unsafe ways, despite this being in no one's best interest.\n\u2022 Organizational risks: Accidents arising from the complexity of AIs and the or-ganizations developing them.\n\u2022 Rogue Als: The problem of controlling a technology more intelligent than we are.\nThese four sections-Malicious Use, AI Race, Organizational Risks, and Rogue Als-describe causes of AI risks that are intentional, environmental/structural, acciden-tal, and internal, respectively [Yampolskiy2016TaxonomyOP]. The risks that arebriefly outlined in this chapter are discussed in greater depth in the rest of this book.\nIn this chapter, we will describe how concrete, small-scale examples of each risk mightescalate into catastrophic outcomes. We also include hypothetical stories to helpreaders conceptualize the various processes and dynamics discussed in each section.We hope this survey will serve as a practical introduction for readers interested inlearning about and mitigating catastrophic AI risks.\""}, {"title": "Artificial Intelligence Fundamentals", "content": "To reduce risks from AI systems, we need to understand their technical foundations. Like many other technologies, AI presents benefits and dangers on both individual and societal scales. In addition, AI poses unique risks, as it involves the creation of autonomous systems that can intelligently pursue objectives without human assistance. This represents a significant departure from existing technologies, and we have yet to understand its full implications, especially since the internal work-ings of AI systems are often opaque and difficult to observe or interpret. Nevertheless, the field is progressing at a remarkable speed, and AI technologies are being increas-ingly integrated into everyday life. Understanding the technical underpinnings of AIcan inform our understanding of what risks it poses, how they may arise, and howthey can be prevented or controlled.\nOverview. This chapter mostly focuses on machine learning (ML), the approachthat powers most modern AI systems. We provide an overview of the essential el-ements of ML and discuss some specific techniques. While the term \"AI\" is mostcommonly used to refer to these technologies and will be the default in most of thisbook, in this chapter we distinguish between AI, ML, and their subfields.\nArtificial Intelligence. We will begin our exploration by discussing AI: the over-arching concept of creating machines that perform tasks typically associated withhuman intelligence. We will introduce its history, scope, and how it permeates ourdaily lives, as well as its practical and conceptual origins and how it has devel-oped over time. Then, we will survey different \"types\" or \"levels\" commonly usedto describe AI systems, including narrow AI, artificial general intelligence (AGI),human-level AI (HLAI), transformative AI (TAI), and artificial superintelligence(ASI) [Bostrom2014].\nMachine Learning. Next, we will narrow our discussion to machine learning(ML), the subfield of AI focused on creating systems that learn from data, makingpredictions or decisions without being explicitly programmed. We will present funda-mental vocabulary and concepts related to ML systems: what they are composed of,how they are developed, and common tasks they are used to achieve. We will surveyvarious types of machine learning, including supervised, unsupervised, reinforcement,and deep learning, discussing their applications, nuances, and interrelations.\nDeep Learning. Then, we will delve into deep learning (DL), a further subset ofML that uses neural networks with many layers to model and understand complexpatterns in datasets. We will discuss the structure and function of deep learningmodels, exploring key building blocks and principles of how they learn. We will presenta timeline of influential deep learning architectures and highlight a few of the countlessapplications of these models.\nScaling Laws. Having established a basic understanding of AI, ML, and DL, wewill then explore scaling laws. These are equations that model the improvements inperformance of DL models when increasing their parameter count and dataset size.We will examine how these are often power laws equations in which one variableincreases in proportion to a power of another, such as the area of a square andexamine a few empirically determined scaling laws in recent AI systems.\nSpeed of AI Development. Scaling laws are closely related to the broader ques-tion of how fast the capabilities of AI systems are improving. We will discuss some ofthe key trends that are currently driving increasing AI capabilities and whether weshould expect these to continue in coming years. We will relate this to the broaderdebate around when we might see AI systems that match (or surpass) human per-formance across all or nearly all cognitive tasks.\nThroughout the chapter, we focus on building intuition, breaking down technicalterms and complex ideas to provide straightforward explanations of their core prin-ciples. Each section presents fundamental principles, lays out prominent algorithmsand techniques, and provides examples of real-world applications. We aim to demys-tify these fields, empowering readers to grasp the concepts that underpin AI systems.By the end of this chapter, we should have a basic understanding of machine learningand be in a stronger position to consider the complexities and challenges of AI sys-tems, the risks they pose, and how they interact with our society. This will providethe technical foundation we need for the following chapters, which will explore therisks and ethical considerations that these technologies present from a wide array ofperspectives."}, {"title": "Single-Agent Safety", "content": "To understand the risks associated with artificial intelligence (AI), we begin by ex-amining the challenge of making single agents safe. In this chapter, we review corecomponents of this challenge including monitoring, robustness, alignment and sys-temic safety.\nMonitoring. To start, we discuss the problem of monitoring AI systems. Theopaqueness of machine learning (ML) systems their \"black-box\" nature hindersour ability to fully comprehend how they make decisions and what their intentions,if any, may be. In addition, models may spontaneously develop qualitatively newand unprecedented \u201cemergent\u201d capabilities as they become more advanced (for ex-ample, when we make them larger, train them for longer periods, or expose themto more data). Models may also contain hidden functionality that is hard to detect,such as backdoors that cause them to behave abnormally in a very small number ofcircumstances.\nRobustness. Next, we turn to the problem of building models that are robustto adversarial attacks. AI systems based on deep learning are typically vulnerableto attacks such as adversarial examples, deliberately crafted inputs that have beenslightly modified to deceive the model into producing predictions or other outputsthat are incorrect. Achieving adversarial robustness involves designing models thatcan withstand such manipulations. Without this, malicious actors can use attacks tocircumvent safeguards and use AI systems for harmful purposes. Robustness is relatedto the more general problem of proxy gaming. In many cases, it is not possible toperfectly specify our idealized goals for an AI system. Inadequately specified goalscan lead to systems diverging from our idealized goals, and introduce vulnerabilitiesthat adversaries can attack and exploit.\nAlignment. We then pivot to the topic of alignment, focussing primarily on controlof AI systems (another key component of alignment, the choice of values to whichan AI system is to be aligned, is discussed in the Beneficial AI and Machine Ethicschapter).\nWe start by exploring the issue of deception, categorizing the varied formsthat this can take (some of which already observed in existing AI systems), andanalyze the risks involved in AI systems deceiving human and AI evaluators. Wealso explore the possible conditions that could give rise to power-seeking agents andthe ways in which this could lead to particularly harmful risks. We discuss sometechniques that have potential to help with making AI systems more controllableand reducing the inherent hazards they may pose, including representation controland unlearning specific capabilities."}, {"title": "Safety Engineering", "content": "In developing an AI safety strategy, it might be tempting to draw parallels with otherhazardous technologies, from airplanes to nuclear weapons, and to devise analogoussafety measures for AI. However, while we can learn lessons from accidents and safetymeasures in other spheres, it is important to recognize that each technology is unique,with its own specific set of applications and risks. Attempting to map safety protocolsfrom one area onto another might therefore prove misleading, or leave gaps in ourstrategy where parallels cannot be drawn.\nInstead of relying on analogies, we need a more general framework for safety, fromwhich we can develop a more comprehensive approach, tailored to the specific casein question. A good place to start is with the field of safety engineering: a broaddiscipline that studies all sorts of systems and provides a paradigm for avoidingaccidents resulting from them. Researchers in this field have identified fundamentalsafety principles and concepts that can be flexibly applied to novel systems.\nWe can view AI safety as a special case of safety engineering concerned with avoidingAI-related catastrophes. To orient our thinking about AI safety, this chapter willdiscuss key concepts and lessons from safety engineering.\nRisk decomposition and measuring reliability. To begin with, we will lookat how we can quantitatively assess and compare different risks using an equationinvolving two factors: the probability and severity of an adverse event. By furtherdecomposing risk into more elements, we will derive a detailed risk equation, andshow how each term can help us identify actions we can take to reduce risk. We willalso introduce a metric that links a system's reliability to the amount of time wecan expect it to function before failing. For accidents that we would not be able torecover from, this expected time before failure amounts to an expected lifespan.\nSafe design principles and component failure accident models. The fieldof safety engineering has identified multiple \"safe design principles\" that can be builtin a system to robustly improve its safety. We will describe these principles and consider how they might be applied to systems involving AI. Next, we will outline sometraditional techniques for analyzing a system and identifying the risks it presents."}, {"title": "Complex Systems", "content": "Artificial intelligence systems and the societies they operate within belong to the classof complex systems. These types of systems have significant implications for thinkingabout and ensuring AI safety. Complex systems exhibit surprising behaviors and defyconventional analysis methods that examine individual components in isolation. Todevelop effective strategies for AI safety, it is crucial to adopt holistic approaches thataccount for the unique properties of complex systems and enable us to anticipate andaddress AI risks.\nThis chapter begins by elucidating the qualitative differences between complex andsimple systems. After describing standard analysis techniques based on mechanisticor statistical approaches, the chapter demonstrates their limitations in capturingthe essential characteristics of complex systems, and provides a concise definitionof complexity. The \"Hallmarks of Complex Systems\" section then explores sevenindications of complexity and establishes how deep learning models exemplify eachof them.\nNext, the \"Social Systems as Complex Systems\" section shows how various humanorganizations also satisfy our definition of complex systems. In particular, the sectionexplores how the hallmarks of complexity materialize in two examples of social sys-tems that are pertinent to AI safety: the corporations and research institutes pursu-ing AI development, and the decision-making structures responsible for implementingpolicies and regulations. In the latter case, there is consideration of how advocacyefforts are affected by the complex nature of political systems and the broader socialcontext.\nHaving established that deep learning systems and the social systems surroundingthem are best described as complex systems, the chapter moves on to what this meansfor AI safety. The \"General Lessons\" section derives five learnings from the chapter'sexamination of complex systems and sets out their implications for how risks mightarise from AI. The \"Puzzles, Problems, and Wicked Problems\" section then reframesthe contrasts between simple and complex systems in terms of the different kinds of"}, {"title": "Beneficial Al and Machine Ethics", "content": "As we continue todevelop powerful AI systems, it is crucial to ensure they are safe and beneficial forhumanity. In this chapter, we discuss the challenges of specifying appropriate valuesfor AI systems to pursue. Some of these questions are already relevant for AI systemsthat exist today in the healthcare or automotive sectors, where artificial systemsmay be making decisions in situations that can harm or benefit humans. They willbecome even more critical for future systems that may be integrated more broadlyacross economies, governments, and militaries, making high-stakes decisions withmany important moral considerations. As discussed in section 3.4, these questionsare core components of the overall challenge of \"AI alignment\".\nMany people have incoherent views on embedding values into AIs. Peo-ple often talk about what Als should do to promote human values. They may agreewith many of the following:\n1. Als should do what you tell them to do.\n2. Als should promote what you choose to do.\n3. Als should do what's fair.\n4. Als should do what a democratic process tells them to do.\n5. Als should figure out what is moral, then do that.\n6. Als should do what is objectively good for you.\n7. Als should do what would make people happy.\nAll of these seem like reasonable answers. At least at first glance, these all seemlike excellent goals for Als. However, many of these are incompatible, because theymake different normative assumptions and require different technical implementa-tions. Other suggestions such as \"Als should follow human intentions\" are highly"}, {"title": "Collective Action Problems", "content": "In this chapter, we have been exploring the risks generated or exacerbated by theinteractions of multiple agents, both human and AI. In the previous section, weexplored a variety of mechanisms by which agents can achieve stable cooperation. Inthis section we address how, despite the fact that cooperation can be so beneficial toall involved, a group of agents may instead enter a state of conflict. To do this, wediscuss bargaining theory, commitment problems, and information problems, usingtheories and examples relevant both for conflict between nation-states and potentiallyalso between future AI systems.\nHere, we use game theory to present natural dynamics in biological and social systemsthat involve multiple agents. In particular, we explore what might cause agents tocome into conflict with one another, rather than cooperate. We show how these multi-agent dynamics can generate undesirable outcomes, sometimes for all the agentsinvolved. We consider risks created by interactions within and between human andAI agents, from human-directed companies and militaries engaging in perilous racesto autonomous Als using threats for extortion.\nWe start with an overview of the fundamentals of game theory. We begin this sectionby setting out the characteristics of game theoretic agents. We also categorize thedifferent kinds of games we are exploring.\nWe then focus on the Prisoner's Dilemma. The Prisoner's Dilemma is a simple ex-ample of how an interaction between two agents can generate an equilibrium statethat is bad for both, even when each acts rationally and in their own self-interest. Weexplore how agents may arrive at the outcome where neither chooses to cooperate.We use this to model real-world phenomena, such as negative political campaigns.Finally, we examine ways we might foster rational cooperation between self-interestedAI agents, such as by altering the values in the underlying payoff matrices. The keyupshot is that intelligent and rational agents do not always achieve good outcomes."}, {"title": "Governance", "content": "AI companies need sound corporate governance: it is essential that thesefirms are guided in directions that enable the creation of safe and beneficial Als. Inthis section, we explain what corporate governance is, differentiating between share-holder and stakeholder theory. Then, we give an overview of different legal structures,ownership structures, organizational structures, as well as internal and external as-surance mechanisms.\nA company's legal structure refers to its legal form and place of incorporation, itsstatutes and bylaws.\nAI development is not being aligned with human values but rather with naturalselection."}]}