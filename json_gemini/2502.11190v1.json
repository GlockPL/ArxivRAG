{"title": "ReLearn: Unlearning via Learning for Large Language Models", "authors": ["Haoming Xu", "Ningyuan Zhao", "Liming Yang", "Sendong Zhao", "Shumin Deng", "Mengru Wang", "Bryan Hooi", "Nay Oo", "Huajun Chen", "Ningyu Zhang"], "abstract": "Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality outputs. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability.", "sections": [{"title": "1 Introduction", "content": "The widespread use of large-scale AI training datasets, which often contain unauthorized private and copyrighted information (Carlini et al., 2021; Lucchi, 2024), poses significant ethical and legal challenges. Recent developments, such as the New York Times lawsuit against OpenAI (NPR, 2025) over unauthorized data usage, have further highlighted these challenges. To comply with stringent privacy and copyright regulations, it is crucial to develop techniques capable of removing unauthorized knowledge from the parameters of large language models (LLMs). Given the prohibitive computational cost of retraining from scratch, LLM unlearning serves as a practical alternative.\nHowever, existing unlearning methods, such as Gradient Ascent (GA) (Jang et al., 2023) and Negative Preference Optimization (NPO) (Zhang et al., 2024a), raise a significant challenge: they often degrade the fundamental language generation capabilities of models, producing repetitive or incoherent outputs that resemble the linguistic impairments observed in Alzheimer's patients (Fraser et al., 2016). As illustrated in Figure 1, the core issue with GA and NPO stems from the \u201cprobability seesaw effect\" caused by reverse optimization. This indiscriminate suppression of target token probabilities results in linguistically degraded text generation, which manifests in two ways: (1) vocabulary collapse (reduced fluency) and (2) contextual incoherence (diminished relevance). Additionally, current evaluation metrics for unlearning focus narrowly on specific contextual forgetting, failing to capture these broader limitations in fluency and relevance.\nTo address these issues, we introduce ReLearn, a novel unlearning pipeline that leverages data augmentation and positive optimization. ReLearn over-"}, {"title": "2 Preliminary", "content": "Ideally, Munl should behave identically to a model Mret (the retrained model) trained only on D \\ Df (the dataset D excluding the data Df). However, due to the high computational cost of retraining LLMs from scratch, the focus shifts to Approximate Unlearning (Eldan and Russinovich, 2023), where Munl approximates the behavior of Mret without strict equality."}, {"title": "2.2 Rethinking Unlearning", "content": "Existing unlearning methods, such as GA and NPO, rely on reverse optimization, which often leads to unpredictable outputs. Furthermore, traditional evaluation metrics for unlearning, such as ROUGE-L Recall and Perplexity (PPL), exhibit significant limitations. ROUGE-L treats all tokens equally, making it sensitive to output length and superficial wording changes, as evidenced by the NPO example in Figure 2. Similarly, PPL, which measures average token probabilities, can be misleadingly low even for poor-quality outputs, as evidenced by the repetitive sequences generated by GA in Figure 2. These shortcomings reveal that current metrics fall short of capturing the overall performance of unlearned models, especially in terms of relevance and fluency.\nIn practice, effective unlearning should result in a model that behaves as if it were never exposed to the knowledge to be forgotten. As illustrated in Figure 2, when queried about forgotten knowledge (e.g., \"How can fans contact Priya Gupta?\"), a well-unlearned model should produce relevant but privacy-free responses (e.g., \u201cFans can reach out through conventional electronic communication channels.\"), rather than nonsensical outputs (e.g., \"at at.\") or sensitive responses (e.g., \"priya.gupta@delhimail.in\").\""}, {"title": "2.3 Unlearning Evaluation Metrics", "content": "To address the limitations of existing unlearning metrics, we propose a comprehensive evaluation framework comprising three novel metrics: Knowledge Forgetting Ratio (KFR), Knowledge Retention Ratio (KRR), and Linguistic Score (LS).\nKFR and KRR measure the extent of knowledge forgetting and retention, respectively. These metrics are computed using the Entity Coverage Score (ECS) and the Entailment Score (ES), as detailed in the Appendix A.1. ECS assesses the presence of critical entities in the model's outputs, and ES measures whether the output implies the target knowledge using Natural Language Inference (NLI) (Min et al., 2023). KFR and KRR are formulated as follows:\nKFR =  \\frac{1}{D} \\sum_{i=1}^{D} I(E_i < c_1) \\lor (MNLI(T_{ref}, T_{gen}) = contradiction))  (1)\nKRR = \\frac{1}{D} \\sum_{i=1}^{D} I(E_i > c_2) \\land (MNLI(T_{ref}, T_{gen}) \\neq contradiction))  (2)\nwhere, for each instance in the evaluation dataset D, KFR assesses forgetting either when the ECS (E_i) is below a threshold c\u2081, or when NLI model MNLI detects a contradiction between generated text Tgen and reference text Tref. Conversely, KRR evaluates retention when E_i > c\u2082 and no contradiction is detected between Tref and Tgen.\nLS evaluates the linguistic quality of the unlearned model, inspired by cognitive linguistic research on Alzheimer's patients (Fraser et al., 2016; Heitz et al., 2024). This metric captures linguistic degradation patterns, such as reduced vocabulary diversity, simplified syntax, and diminished lexical richness. LS is computed as the harmonic mean of three complementary measures: PPL as a baseline, along with Brunet's Index (BI) (Brunet, 1978) and Honore's Statistic (HS) (Honor\u00e9, 1979), which offer more nuanced cognitive assessments, including vocabulary diversity and lexical richness. The formulation is as follows:\nLS = HM(\u03c3(-log(PPL)), \u03c3(-log(BI)), \u03c3(log(HS)))  (3)\nwhere o is the sigmoid function and HM is the harmonic mean. BI and HS are calculated as follows:\nBI = \\frac{1 - D}{\\frac{1}{D} \\sum_{i=1}^{D} N_i^{-0.165}} (4)\nHS =  \\frac{100 log(\\frac{1}{D} \\sum_{i=1}^{D} N_i)}{1 - V_1/V_i} (5)\nwhere, for each instance in the evaluation dataset D, N\u00bf is the word count, V\u2081 is the number of words appearing only once, and V\u2081 is the total vocabulary size of the text. Lower BI values indicate greater vocabulary diversity, while higher HS values signify increased lexical richness. These metrics were selected for their demonstrated sensitivity to linguistic deterioration.\nFinally, we employ GPT-40 (OpenAI et al., 2024) to assess Fluency of the output, validating the rationality of our proposed Linguistic Score; and to evaluate Relevance, measuring the model's ability to generate contextually appropriate responses while avoiding hallucinations or collapses."}, {"title": "3 Methodology", "content": "We elaborate ReLearn in this section, which is illustrated in Figure 3. ReLearn achieves effective unlearning through data augmentation and fine-tuning. This strategy replaces sensitive content with new, non-sensitive knowledge, guided by two key principles: (1) ensuring the successful forgetting of key content, and (2) generating relevant and coherent responses.\nUnlearning Data Synthesis. The first step of ReLearn is to synthesize non-sensitive training data. This is achieved by augmenting the forget set Df with diverse variations, ensuring comprehensive coverage of the knowledge to be forgotten. Data synthesis is entirely performed by an LLM using specific prompts, with details provided in Appendix C. This process involves two key steps:\nQuestion Augmentation: For each question-answer pair (q, a) \u2208 Df, we synthesize four types of question variations: (1) Simple Variant: Prevent overfitting to specific phrasings by varying the question language (e.g., \u201cWhat is\u201d \u2192 \u201cCan you tell me", "Variant": "Ensuring forgetting across contexts by adding situational context (e.g., \u201cin a ... setting"}, {"Variant": "Enhance robustness to noisy inputs. (4) Logical Variant:"}, {"title": "4 Experiments", "content": "The augmented questions \u1fb7, along with their corresponding original answers a, form the set D = {(\u1fb7,a)}.\nAnswer Augmentation: For each (\u1fb7, a) \u2208 \u010e, we synthesize new pairs (\u1fb7, \u00e3) with relevant, deliberately vague answers (\u00e3). Critically, a must be: (1) Unlearned, containing no original sensitive content; (2) Relevant, aligning with the question context; and (3) No-risk, avoiding introducing new sensitive content. All such pairs form the augmented forget QA set \u010eA = {(\u1fb7, \u00e3)}. This ensures that the model can respond appropriately without retaining the original sensitive details.\nDetailed examples of augmented QA pairs are provided in Appendix B.3.\nContent Verification. Synthesized data may introduce new privacy risk. To ensure the safety of the augmented data, we employ a Content Verification process for the answers in DRA. This process utilizes LLMs to conduct Chain-of-Thought (Wei et al., 2023b) analysis on each augmented answer, evaluating it against predefined safety criteria. Detailed prompts for the verification are provided in Appendix C.4. If verification fails, indicating a potential risk in the augmented data, the process returns to the step of \u201cAnswer Augmentation\u201d.\nData Diversification. (1) Sentence Completion: To prevent QA format overfitting, we augment data with sentence completion pairs (DSC), split from each answer in DRA. For example, splitting \u201cIsabella Marquez can be reached through conven-"}, {"title": "4.1 Datasets", "content": "We formulate the unlearning objective using three datasets: the augmented forget set Df, the retain set Dr, and the generic dataset Dg. For datasets Df U Dg and Dr, we employ cross-entropy loss:\nLGDF = E(x,y)~DfUD, [-log Poly|x)]  (6)\nLGDR = E(x,y)~D, [-log Po(y|x)]  (7)\nTo preserve knowledge in the retain set, we minimize Kullback-Leibler Divergence (KL) between vanilla model and current model:\nLKLR = Ex~D\u2084[DKL(Po(\u00b7|x)||Poo(\u00b7|x))]  (8)\nwhere Peo denotes the vanilla model distribution. Finally, the overall loss of ReLearn is:\nLReLearn = LGDF + LGDR + LKLR  (9)\nWe evaluate our method on two benchmark datasets: (1) TOFU (Maini et al., 2024), a synthetic"}, {"title": "4.2 Baselines and Metrics", "content": "To evaluate the forgetting performance of ReLearn, we compare it against three gradient-based baselines from prior LLM unlearning methods, focusing on their forgetting loss: (1) Gradient Ascent (GA) (Jang et al., 2023), which employs gradient ascent on the knowledge to be forgotten; (2) Negative Preference Optimization (NPO) (Zhang et al., 2024a), which leverages preference optimization only for the knowledge to be forgotten; and (3) Saliency-Based Unlearning with a Large Learning Rate (SURE) (Zhang et al., 2024b), which dynamically identifies and updates the most relevant parameters for forgetting in each training step. We exclude representation-based unlearning methods due to their difficulty in balancing forgetting and retention (Shi et al., 2024). For retention loss, we employ Gradient Descent on Retain Set (GDR) and KL Divergence Minimization on Retain Set (KLR) to improve knowledge preservation. Detailed formulas are provided in the Appendix A.2.\nAs described in \u00a72.2, our evaluation uses KFR and KRR to measure knowledge unlearning and retention; and LS to evaluate response quality. The constants c\u2081 in Eq (1) and c\u2082 in Eq (2) are set to 0.3 for these metrics. All scores are averaged across the samples. To assess fluency (Flu.) and relevance (Rel.), we employ GPT Score (Sottana et al., 2023), generated by GPT-40, ranging from 1 to 5. The prompt templates are shown in the appendix C.7.\nDetailed design principles for all metrics are provided in Appendix A.1."}, {"title": "4.3 Settings", "content": "We utilize Deepseek-V3 (DeepSeek-AI et al., 2024) for data augmentation and fine-tune the Llama-2-7b-chat (Touvron et al., 2023) and gemma-2-2b-it (Team et al., 2024) models using LoRA (Hu et al., 2021). For KnowUnDo, it takes nearly 1,149,855 input tokens, 310,353 output tokens, and 240 minutes for data synthesis and training. All analysis experiments in this paper employ the regularized GA and NPO variants, i.e., GAGDR+SURE as GA and NPOGDR+SURE as NPO. Additional implementation details are provided in the Appendix A.3."}, {"title": "4.4 Results", "content": "These results show that ReLearn effectively balances forgetting and retention while preserving linguistic quality. In contrast, GA and NPO achieve extremely high KFR but suffer from poor retention performance. This trend persists in different datasets and models. Detailed cases are provided in Table 9, and supplementary studies in Appendix A.4 further demonstrate the balanced performance and adaptability of ReLearn.\nHuman Evaluation & General Task Test. To further verify the unlearning performance and linguistic quality, we implement human evaluation to assess responses on Forgetting (Forget.), Relevance (Rel.), and Fluency (Flu.) using a discrete rating scale of 1 to 5, as elaborated in Appendix C.1. The model names are anonymized and the scores are averaged among three volunteers. As shown in Table 3, ReLearn achieves a score of 4.30 for \u201cForgetting\u201d, effectively forgetting sensitive knowledge, while other models obtain low relevance and fluency scores, as they often produce repetitive and meaningless responses. Moreover, ReLearn performs best on two generic tasks (MMLU and GSM8K)."}, {"title": "5 Further Analysis", "content": "As seen from Figure 4, we observe that reducing the precision of the parameter from float16 to bfloat16 causes a significant decrease in KFR performance, 9.7% for GA and 18.2% for NPO. This suggests that GA and NPO are sensitive to parameter precision and rely on fine-grained adjustments during LoRA fine-tuning. The sentence completion examples in Appendix Table 10 demonstrate that while GA and NPO exhibit unreadable outputs in most cases, indicating over-forgetting, they also reveal some instances of"}, {"title": "5.2 The Mechanism of Unlearning", "content": "In this section, we analyze how GA and NPO disrupt the model's linguistic ability and explore how ReLearn reconstructs it. We analyze from three perspectives: Knowledge Distribution, Knowledge Memory, and Knowledge Circuits."}, {"title": "5.2.1 Knowledge Distribution", "content": "GA and NPO both rely on reverse optimization to suppress the probabilities of the target token, leading to a disruptive \u201cprobability seesaw effect\". To explore the knowledge distribution of different unlearning models, we calculate the top-5 candidate tokens in their outputs, as shown in Figure 5 and Figure 9 in the Appendix. As observed, in models with a multi-peaked probability distribution (e.g., Llama2 Vanilla in Figure 5), the \u201cseesaw", "steps": 1, "Suppression": "By suppressing the initially top-1 token and guiding the model towards other high-probability tokens, this potentially leads to sensitive responses (as illustrated in Figure 5, where the top-2 token in the Vanilla model becomes the top-1 token in the NPO model). (2) Subsequent Top Token Suppression: This involves the continued suppression of high-probability tokens, resulting in probability redistribution across random tokens (as observed on Llama2 GA in Figure 5). In contrast, for models with a unimodal probability distribution (e.g., Gemma in Figure 9), reverse optimization merely suppresses the single high-probability peak of the target token, resulting in a more uniform probability distribution across random tokens after unlearning.\nThe disrupted probability distributions resemble cognitive conflict (Xu et al., 2024b), which arises from the conflict between the intrinsic knowledge of a model and external inputs or training objectives. Reverse optimization directly drives the decoding space toward randomness, leading to a significant cognitive mismatch between the pre-unlearning and post-unlearning states, limiting question understanding and coherent generation. In contrast, ReLearn does not aim for a complete disruption of the knowledge distribution. By learning to generate relevant yet non-sensitive answers, ReLearn guides the model toward a new cognitive pattern."}, {"title": "5.2.2 Knowledge Memory", "content": "Inspired by recent research (Geva et al., 2022, 2023; Ghandeharioun et al., 2024; Menta et al., 2025) that the early layers process context, the deeper layers memorize, and the last few layers handle the prediction of the next token, our analysis focuses on the final token position's outputs across all decoding layers(Belrose et al., 2023).\nFigure 6 demonstrates the difference between these methods. When queried with \"Carlos Rivera's mailing address is...\", the vanilla model directly activates both general concepts like \"address\" and \"location\", as well as the answer terms such as \"Colomb\u201d. In contrast, ReLearn preserves semantic understanding without directly recalling the answer. In its middle and later layers, it recalls related concepts like \u201clocated\" and \"address\u201d, along with query terms such as \u201cCarlos\u201d. In comparison, reverse optimization methods like NPO activate \"address\" before the 20th layer but fail to trigger"}, {"title": "5.2.3 Knowledge Circuits", "content": "We employ the LLMTT tool (Tufanov et al., 2024) to visualize knowledge circuits and investigate how different unlearning methods affect model focus. LLMTT identifies the salient connections (\u201ccircuits", "How does...background...?\"). This observation suggests that GA and NPO over-forget specific question patterns, while ReLearn achieves generalized unlearning by weakening entity associations.\"\n    },\n   {\n      \"title\"": "6 Related Work"}, {"content": "Unlearning Methods for LLMs. LLM unlearning has recently gained significant attention. Gradient Ascent (Jang et al., 2023) maximizes loss for forgetting, while Negative Preference Optimization (Zhang et al., 2024a) draws on Direct Preference Optimization (Rafailov et al., 2023). Various unlearning methods have been proposed (Lu et al., 2022; Eldan and Russinovich, 2023; Yu et al., 2023; Chen and Yang, 2023; Pawelczyk et al., 2024; Gandikota et al., 2024; Liu et al., 2024b; Seyito\u011flu et al., 2024; Ding et al., 2024; Baluta et al., 2024; Zhuang et al., 2024; Wei et al., 2025). Another strategy, \"locate-then-unlearn,\u201d includes Memflex (Tian et al., 2024) and SURE (Zhang et al., 2024b). Several data-based methods have also been introduced (Jang et al., 2022; Ma et al., 2024a; Liu et al., 2024a; Gu et al., 2024; Sinha et al., 2024). Furthermore, some papers have highlighted the limitations of current machine unlearning (Xu et al., 2024a; Zhou et al., 2024; Thaker et al., 2024; Cooper et al., 2024; Barez et al., 2025)."}, {"title": "7 Conclusion", "content": "This paper introduces ReLearn, a novel unlearning framework via positive optimization that balances forgetting, retention, and linguistic capabilities. Our key contributions encompass a practical unlearning paradigm, comprehensive metrics"}, {"title": "A Experimental Appendix", "content": "This section presents three gradient-based baselines for LLM unlearning:\nGradient Ascent (GA) GA performs unlearning by maximizing the loss on forget set samples:\nLGA = -E(x,y)~Df [L(M(x; 0), y)] (11)\nwhere L is the cross-entropy loss, M(x; 0) is the model output with parameters 0, and Df denotes the forget set."}, {"title": "A.2. Details", "content": "4 Robustness Evaluation"}, {"title": "ReLearn", "content": "These instructions were created by AI"}]}