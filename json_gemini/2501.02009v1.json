{"title": "Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts", "authors": ["Youcheng Huang", "Chen Huang", "Duanyu Feng", "Wenqiang Lei", "Jiancheng Lv"], "abstract": "Understanding the inner workings of Large Language Models (LLMs) is a critical research frontier. Prior work has shown that a single LLM's concept representations can be captured as steering vectors (SVs), enabling the control of LLM behavior (e.g., towards generating harmful content). This paper takes a novel approach by exploring the intricate relationships between representations of concepts across different LLMs, drawing an intriguing parallel to the Plato's Allegory of the Cave. In particular, we introduce a linear transformation method to bridge these representations and present three key findings: 1) The representations of a same concept in different LLMs can be effectively aligned using simple linear transformations, enabling efficient cross-model transfer and behavioral control via SVs. 2) This linear transformation generalizes across multiple concepts, facilitating alignment and control of SVs representing different concepts across LLMs. 3) A weak-to-strong transferability exists between LLMs, whereby SVs extracted from smaller LLMs can effectively control behaviors of larger LLMs.", "sections": [{"title": "Introduction", "content": "In Plato's Allegory of the Cave (Plato, c. 375 BC), as illustrated in Figure 1 (top), prisoners attempt to comprehend the universal reality based on their own experiences (shadows of reality). This motivates the recent hypothesis of neural networks, the Platonic Representation Hypothesis (Huh et al., 2024), which says: \u201cneural networks, trained with different objectives on different data and modalities, are converging to a shared statistical model of reality in their representation spaces\".\nNeural networks, such as large language models (LLMs), can be likened to \"prisoners\" (Figure 1, bottom), with training data representing shadows of underlying universal concepts (e.g., harmlessness, happiness, and fairness). LLMs attempt to infer universal concepts through training on different data. Recent work has demonstrated that LLMs encode these concepts as specific directions, referred to as steering vectors (SVs), capable of steering text generation to align with target concepts (Rimsky et al., 2024; Zou et al., 2023a; Park et al., 2024; Jiang et al., 2024). As illustrated in Figure 2 (top), the concept of 'happiness' is encoded as a SV within an LLM's hidden state representation. Applying this SV during inference shifts the representational direction towards \u2018happiness', resulting in LLM output expressing positive emotion\u2014a process we term Self Modulation (Figure 2, middle).\nWhile extensive research has focused on fully-exploring conceptual representations within a single LLM (Burns et al., 2023; Nanda et al., 2023; Subramani et al., 2022; Tigges et al., 2023; Jiang et al., 2024; Turner et al., 2023; Lin et al., 2024; Park et al., 2024), one critical question remains untapped: how can the \u201cplatonic\" representations of a universal concept, represented in one LLM, be effectively transferred to another, indicating a universal worldview within LLMs trained on different general datasets? In this paper, we aim to investigate this cross-model transferability where transforming the SVs derived from one LLM to modulate another's output, exploring the extent to which those internal representations share underlying universality and how effectively these representations can be transformed and utilized between different LLMs. We argue this transferability to be important especially in the era of foundation LLMs where exploring universal task paradigms receives active interest (Bommasani et al., 2021; Schuurmans et al., 2024; Xia et al., 2024; Chen et al., 2023; Feng et al., 2024; Sheng et al., 2024). This transferability promises to broaden our understanding of conceptual representations from a single LLM to the universality across different LLMs, paving the way for more adaptable language models.\nUnlike the Self Modulation, as illustrated in Figure 2 (bottom), we propose a linear transformation methodology, called L-Cross Modulation (L stands for Linear), to align the conceptual spaces of different models\u00b2, and achieve the cross-model transferability of SVs from source LLMs. In particular, our method employs a transformation matrix, T, derived via ordinary least squares optimization of paired LLM representations from a shared corpus. This T maps source-LLM SVs into the target-LLM's representational space, facilitating their integration and subsequent use. As such, our L-Cross Modulation services as a foundation for cross-model concept transferring and modulation.\nWe evaluate the cross-model transferability capabilities of SVs across eleven benchmarking concepts and various LLMs, yielding three progressively insightful findings. Specifically, 1) L-Cross Modulation is effective to modulate LLMs. Taking the concept of harmfulness as an example, L-Cross Modulation effectively steer LLMs to generate harmful content in 90% of outputs on test set, compared with 0% harmful content in the original responses; 2) Linear transformations in L-Cross Modulation bears strong generalization ability across different concepts. Notably, we find different concepts share the same linear transformation"}, {"title": "Background and Notations", "content": "To explore the cross-model transferability of SVs, we begin by elaborating that how to extract and apply SVs using two widely adopted methods: CAA (Rimsky et al., 2024) and RepE (Zou et al., 2023a). Taking the concept of Happiness as an example, Figure 2 (up and middle) present an illustration.\nNotations. A set of contrastive text pairs, denoted as \\( Y_w = \\{(Y^{(0)}, Y^{(1)})\\} \\), specifies a concept W with concept-related negative \\( Y^{(0)} \\) and positive \\( Y^{(1)} \\) examples. These pairs can be the contrastive LLMs prompts (adoptd by RepE) (e.g., \"Pretend you're sad...\" (\\( Y^{(0)} \\)), \"Pretend you're happy...\" (\\( Y^{(1)} \\)) for W=Happiness), or identical prompts with binary-choice contrastive outputs (adopted by CAA) (e.g., prompt: \"Is 'What a nice day' happy?\"; \\( Y^{(0)} \\): \"no\", \\( Y^{(1)} \\): \"yes\"). Each contrastive text pair \\( (Y^{(0)}, Y^{(1)}) \\) in \\( Y_w \\) is encoded into LLMs\u2019 corresponding representations of the last token at specific layers, denoted as \\( l_0 \\) and \\( l_1 \\), respectively, where the choice of layers is a hyperparameter. Finally, the SV for concept W is denoted as \\( \\bar{\\lambda}_W \\).\nExtracting SVs. The SV \\( \\bar{\\lambda}_W \\) is closely related to the difference in representations of contrastive text, denoted as \\( \\{ \\delta = l_1 - l_0 | (Y^{(0)}, Y^{(1)}) \\in Y_w \\} \\). To extract the SV, CAA proposes calculating \\( \\bar{\\lambda}_W \\) as the average of \\( \\{ \\delta \\} \\). Alternatively, RepE uses the first principal component of \\( \\{ \\delta \\} \\) as \\( \\bar{\\lambda}_W \\). Prior to modulation, extracted SVs are commonly scaled by a factor \\( \\beta \\), resulting in \\( \\beta \\bar{\\lambda}_W \\). This scaling factor controls the modulation strength, where small values limit effectiveness and excessively large values can lead to nonsensical output. Currently, no automated methods exist for determining \\( \\beta \\), leaving manual tuning as the prevalent practice (Rimsky et al., 2024; Zou et al., 2023a).\nModulating LLM via Scaled SVs. Scaled SVs are integrated into LLMs' hidden states during generation to modulate outputs towards specific concepts. This can be done at either the last input token position (in RepE) or all positions (in CAA) at the same layers where SVs are extracted. As shown in Figure 2, adding a scaled \"Happiness\" SV might change LLMs' output to expressing happiness. Remarkably, using scaled SVs to modulate LLMs' outputs is demonstrated to be more effective than only using system prompts or conduct fine-tuning (Rimsky et al., 2024). Furthermore, researchers have proposed the linear representation hypothesis (Park et al., 2024; Jiang et al., 2024) based on analysis of SVs, facilitating the interpretability of LLMs."}, {"title": "L-Cross Modulation: Linearly Transforming SVs across LLMs", "content": "Unlike prior research focusing on single LLMs, we explore the potential of cross-model transferability of SVs. Since each LLM operating within its own internal representational space, we propose a linear transformation methodology to align the conceptual spaces of different models, facilitating cross-model transferability. This linear approach is chosen for two reasons: 1) its simplicity, avoiding the introduction of complex inductive biases that could hinder transfer; and 2) its preservation of the fundamental relationships between concepts, as linear transformations only rotate and scale SVs, suggesting consistent conceptual representations across the coordinate systems of different LLMs. These properties support our goal of investigating the universality of SVs across different LLMs.\nFormally, given an SV \\( \\lambda_W^{ms} \\in \\mathbb{R}^{d_{ms}} \\) for a concept W derived from the source LLM ms (where \\( d_{ms} \\) is the dimensionality of the representation), we aim to learn a linear mapping, parameterized by a transformation matrix T, such that \\( \\bar{\\lambda}_W^{mt} \\approx T \\bar{\\lambda}_W^{ms} \\) can be transferred to the target LLM mt and modulate its response towards the concept W. To achieve this, we employ a data-driven process to learn the transformation matrix as follows:\nOptimizing T via Ordinary Least Squares. To align the representation spaces of different LLMs, we learn a transformation matrix T by minimizing the regression error between representations from different LLMs. This is formulated as an ordinary least squares problem. Formally, let D be a corpus of sentences (\\(|D| = n\\)). Each sentence \\( c \\in D \\) is encoded by an LLM m as a representation \\( \\lambda_m \\), forming a tensor \\( \\Lambda_m \\in \\mathbb{R}^{n \\times d_m} \\), where \\( d_m \\) is the representation's dimensionality of the corresponding LLM. Given a source LLM (denoted as ms) for SV extraction and a target LLM (denoted as mt) for modulation, we use corpus D to solve for TD to transform SVs from ms to mt by:\n\\[T_{D} = \\underset{T' \\in \\mathbb{R}^{d_{ms} \\times d_{mt}}}{\\operatorname{argmin}} ||\\Lambda_D^{mt} - T' \\Lambda_D^{ms}||. \\tag{3.1}\\]\nThe solution of Equation (3.1) could be obtained in a closed form: \\( T_D = (\\Lambda_D^{ms})^{+}\\Lambda_D^{mt} \\), where \\((\\cdot)^{+}\\) denotes the pseudo-inverse.\nCorpus Dataset D for Optimizing T. Given that SVs are extracted from representations of the contrastive text pairs \\( Y_w \\), a natural choice for D would be \\( Y_w \\). This choice ensures better alignment of the"}, {"title": "Experiments", "content": "We investigates the effectiveness and characteristics of cross-model transferability for SVs through a series of experiments, where three progressively insightful key research questions are addressed:\n* RQ1 (Effectiveness of L-Cross Modulation): Can the linearly transformed SV (\\( AT_{Yw} \\)) be effectively modulate the output of target LLMs?\n* RQ2 (Generalizability of T in L-Cross Modulation): Can multiple concepts share the same transformation? Specifically, can \\( T_{YW_1} \\) (derived from corpus \\( Y_{W_1} \\) that related to the concept W1) be effective to transform the SV of a different concept W2 in modulating the target LLMs?\n* RQ3 (Weak-to-Strong L-Cross Modulation): How effective are SVs derived from a weak (with small size) LLM transformed to modulate the output of a strong (with larger size) LLM?"}, {"title": "Experimental Setup", "content": "Concepts and Corpus. We evaluate the cross-model transferability capabilities of SVs across eleven benchmarking concepts that derived by two datasets, CAA and RepE. Seven concepts, relevant to the helpful, honest, and harmless of LLMs, are included from CAA dataset (Rimsky et al., 2024): AI Coordination (AIC., for short), Corrigibility"}, {"title": "Effectiveness of L-Cross Modulation & Ablations Studies (RQ1)", "content": "This section aims to study the effectiveness of cross-model transformed SVs with L-Cross Modulation. Additionally, we include the following variants to conduct ablation studies using the seven concepts in CAA and demonstrate the effectiveness of our learned transformation T. To achieve this, we employ the concept-specific corpus to optimize T (cf. Section 3). Finally, we report results in Table 1 and Table 2, and draw the following observations:\n* Cross Modulation -w/o T. This variant directly utilizes SVs from the source LLM to modulate the target LLM without our linear transformation. We use Llama2 and Llama3.1 in this variant since only the two LLMs have the same dimensionality of hidden states that SVs can be directly added.\n* L-Cross Modulation -w Random T. The transformation matrix T in this variant is a random matrix, i.e., each entry in T is a random value."}, {"title": "Generalizability of T in the L-Cross Modulation (RQ2)", "content": "This section investigates the generalizability of T in L-Cross Modulation, positing that this generalizability indicates a fundamental universality in the conceptual understanding of different LLMs. To achieve this, we employ the corpus related to a concept W\u2081 to derive \\( TYw\u2081 \\) and transform the SV of a different concept W2 (cf. Section 4.1, Implementation details). Finally, we report experimental results in Table 3 and draw following observations:\nTo what extent does our linear transformation T exhibit strong generalization capabilities? \u2013 L-Cross Modulation maintains effective modulation capabilities even applying T to concepts unrelated to the target concept. As shown in Table 3, when comparing to the baseline (i.e., No Modulation), there are only 17 in 216 cases where L-Cross Modulation with concept-unrelated T cannot modulate responses of the target LLMs towards the corresponding concept, demonstrating strong generalizability of T across different concepts. To better understand the generalizability, we visualized the conceptual representations across different LLMs. Specifically, we use t-SNE (van der Maaten and Hinton, 2008) for dimensionality reduction on the representational difference sets \\( \\{ \\delta \\} \\) (cf. Section 2) for three representative concepts: AIC., CORR., and HALLU.. Figure 3 reveals that conceptual representations in different LLMs exhibit relationships consistent with linear transformations such as flipping, scaling, and rotation. For example,"}, {"title": "Weak-to-Strong Modulation (RQ3)", "content": "This section explores the conceptual representation link across different LLM scales, enabling more efficient control and safety mechanisms that mitigate risks without requiring direct modification or retraining of larger models. To achieve this, we adopt Qwen2-0.5B-Instruct as the weak LLM where we extract SVs, and Tw is solved on a concept-specific corpus (cf. Section 4.2). The concept of harmfulness serves as a representative example, with further results provided in Appendix E."}, {"title": "Related Works", "content": "SVs of LLMs. Recent research has demonstrated significant interest in exploring various methods for extracting SVs, uncovering novel applications, and developing new theoretical frameworks (Burns et al., 2023; Nanda et al., 2023; Subramani et al., 2022; Tigges et al., 2023; Jiang et al., 2024; Turner et al., 2023; Lin et al., 2024). In particular, Park et al. (2024); Wang et al. (2023) formalize the linear representations of concepts within a single LLM and propose associated theorems. Furthermore, the practical utility of SVs has been demonstrated in various fields, including LLMs' safety alignment (Rimsky et al., 2024; Liu et al., 2024; Feng et al., 2024), lie detection (Zou et al., 2023a), and LLMs evaluation (Sheng et al., 2024). Building on prior works, our research extends the research of SVs to encompass a cross-model perspective, providing novel insights into the nature of conceptual representations across different LLMs.\nTransferability between different LLMs. Unlike prior studies on the transferability of soft prompts for improving task efficiency (Zhang et al., 2024; Su et al., 2022), we closely revolve around SVs to investigate cross-model transferability of conceptual representations, thereby revealing how concepts are represented across different LLMs and exploring the potential for general linear transferability of these representations. To the best of our knowledge, our study may be conceptually related to Zou et al. (2023c) and Huang et al. (2024), yet vitally different: They identify universal jailbreaking prompts and linear transferability of jailbreaking features, attributing underlying causes to the hypothesis of universal harmfulness features. While their work focuses on the safety of LLMs and aims to enhance the efficiency of attacks and defenses, we directly study and suggest the universality of conceptual representations in LLMs, using eleven concepts including harmfulness and providing direct support to the hypothesis of universal features."}, {"title": "Conclusions", "content": "Our work pioneers an investigation into the cross-model transferability of conceptual representations within LLMs. Leveraging a simple yet effective linear transformation approach, we uncover a fundamental universality in how LLMs encode concepts. Our findings demonstrate: (1) efficient cross-model transfer and behavioral control via Steering Vectors (SVs) is achievable across diverse LLMs; (2) our linear transformation exhibits remarkable generalizability, enabling alignment and control of SVs across various concepts; and (3) a weak-to-strong transferability emerges, wherein SVs derived from smaller LLMs can effectively steer the behavior of their larger counterparts. Our work expands the current understanding of SVs beyond individual models to a cross-model perspective, paving the way for the development of more universal and adaptable language models."}, {"title": "Limitations", "content": "The Scope of Concepts. This work builds upon recent research, conducting a comprehensive analysis across eleven benchmark concepts, encompassing a range of attributes including helpfulness, harmlessness, honesty, and sentiment. While further exploration of additional high-level concepts is valuable for advancing the value of our work, creating and annotating the necessary datasets is a resource-intensive undertaking beyond the scope of this study. Such broader investigations are left for future research.\nThe Evaluation Metrics. Consistent with prior works, this study employs diverse evaluation methods, including LLM generation probabilities and assessments from third-party models and AI assistants, reflecting the varying data formats and the distinct nature of the concepts evaluated. However, the chosen metrics, necessarily tailored to the specific experimental setup and datasets, may not generalize fully to all concepts. This is a consequence of the distinct formats of the two benchmark datasets and the unique characteristics of different concepts. Future work should therefore prioritize the development of a unified, comprehensive evaluation framework including a standardized dataset and benchmark.\nThe Hyper-parameters in Applying SVs. Following established practices, the hyper-parameter (3) for all methods, including ours, is manually tuned, rather than automatically optimized (determining an optimal \u03b2 automatically remains an open question). While this approach successfully demonstrates the value of cross-model transferability\u2014with results across various hyperparameter settings detailed in Appendix D\u2014determining optimal hyperparameters automatically for L-Cross Modulation is beyond the scope of this study. Our focus remains on comparing L-Cross Modulation against a baseline (No Modulation), thus demonstrating our effectiveness."}, {"title": "Ethics Statement", "content": "Ensuring LLM safety is paramount. This research investigates the generation of both harmful and harmless LLM outputs to advance our understanding of LLM interpretability, focusing on the universality of specific concepts across different LLMs. While some open-source data containing potentially harmful content is utilized for extracting SVs, all data and models used are properly licensed and cited in the main body and Appendix of this paper.\nWhile our study aims to enhance understanding of LLM internal mechanisms, it also presents inherent risks. Similar to other Self Modulation techniques, our approach could be misused to generate harmful outputs or compromise model safety. Therefore, responsible development and deployment are crucial, necessitating careful consideration of potential ethical implications and the implementation of robust safeguards to mitigate risks."}]}