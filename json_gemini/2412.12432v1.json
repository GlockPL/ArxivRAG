{"title": "Three Things to Know about Deep Metric Learning", "authors": ["Yash Patel", "Giorgos Tolias", "Ji\u0159\u00ed Matas"], "abstract": "This paper addresses supervised deep metric learning for open-set image retrieval, focusing on three key aspects: the loss function, mixup regularization, and model initialization. In deep metric learning, optimizing the retrieval evaluation metric, recall@k, via gradient descent is desirable but challenging due to its non-differentiable nature. To overcome this, we propose a differentiable surrogate loss that is computed on large batches, nearly equivalent to the entire training set. This computationally intensive process is made feasible through an implementation that bypasses the GPU memory limitations. Additionally, we introduce an efficient mixup regularization technique that operates on pairwise scalar similarities, effectively increasing the batch size even further. The training process is further enhanced by initializing the vision encoder using foundational models, which are pre-trained on large-scale datasets. Through a systematic study of these components, we demonstrate that their synergy enables large models to nearly solve popular benchmarks.", "sections": [{"title": "1 Introduction", "content": "Deep metric learning (DML) is a representation learning task with deep models, typically aiming to retrieval or classification with non-parametric, i.e. nearest neighbor, classifiers. For retrieval, the task is considered open-set with evaluation performed on classes that are not seen during training. Therefore, the ability to generalize well is crucial to achieve good performance, measured using established information retrieval metrics, such as mean Average Precision (mAP) or recall@k. Due to the common limitation of having limited training data on particular domains, it's essential to initialize the model properly rather than training from scratch. This work focuses on DML for images, aiming to bridge the gap between learning and evaluation objectives, while exploring the significance of model initialization.\nMinimization of a loss that is a function of the test-time evaluation metric has shown to be beneficial in deep learning for numerous computer vision and natural language processing tasks. Examples include intersection-over-union as a loss that boosts performance for object detection [1, 2] and semantic segmentation [3], and structural similarity [4], peak signal-to-noise ratio [5] and perceptual [6] as reconstruction losses for image compression that give better results according to the respective evaluation metrics. Training deep networks via gradient descent on the evaluation metric is not possible when the metric is non-differentiable. To address this, existing DML methods use proxy loss functions like contrastive [7], triplet [8], and margin [9]. For a distance measure, these loss functions encourage pulling together samples from the same class while pushing apart samples from different classes. As differentiable functions, they provide a workaround, which empirically leads to a reasonable performance, but may not align well with the evaluation metric as shown in Figure 1."}, {"title": "2 Related work", "content": "In this section, the related work is reviewed for two different families of deep metric learning approaches regarding the type of loss that is optimized, namely classification losses and pairwise losses. Given an embedding network that maps input images to a high dimensional space, in the former, the loss is a function of the embedding and the corresponding category label of a single image, while in the latter, the loss is a function of the distance, or similarity, between two embeddings and the corresponding pairwise label. Prior work for mixup [26] techniques related to embedding learning is reviewed too.\nClassification losses. The work of Zhai and Wu [27] supports that the standard classification loss, i.e. cross-entropy (CE) loss is a strong approach for deep metric learning. Their finding is supported by the use of layer normalization and class-balanced sampling. In the domain of metric learning for faces, several different classification losses are proposed, such as SphereFace [28], CosFace [29] and ArcFace [30], where contributions are in the spirit of large margin classification. Despite the specificity of the domain, such losses are applicable beyond faces. Another variant is the Neighborhood Component Analysis (NCA) loss that is used in the work of Movshovitz-Attias et al. [31], which is later improved [32] by temperature-based scaling and faster update of the class prototype vectors, also called proxies in their work. The restriction of a single prototype vector per class is dropped by Qian et al. [33] who stores multiple representatives per category. Instead of jointly processing the example to all class vectors similarities, Kim et al. [34] jointly normalize the class vector to all batch examples similarities. Their interpretation is that such a formulation allows examples to interact with each other, indirectly resembling pairwise losses.\nClassification losses, in contrast to pairwise losses, perform the optimization independently per image. An exception is the work of Elezi et al. [35] where a similarity propagation module captures group interactions within the batch. Then, cross-entropy loss is used, which now comes with significant improvements by taking into account such interactions. This is later improved [36, 37] by replacing the propagation module with an attention model. The relation between CE loss and some of the widely used pairwise losses is studied from a mutual information point of view [38]. CE loss is viewed as approximate bound-optimization for minimizing pairwise losses; CE maximizes mutual information, and so do these pairwise losses, which are reviewed in the following.\nPairwise losses. The first pairwise loss introduced is contrastive loss [7], where embeddings of relevant pairs are pushed as close as possible, while those of non-relevant ones are pushed far enough. Since the target task is typically a ranking one, the triplet loss [8], a popular and widely used loss, improves that by forming training triplets in the form of anchor, positive and negative examples. The loss is a function of the difference between anchor-to-positive and anchor-to-negative distances and is zero if such a difference is"}, {"title": "3 Method", "content": "We present the task, the relevant background and the proposed approaches."}, {"title": "3.1 Background", "content": "Task. We are given a query example $q \\in \\mathcal{X}$ and a collection of examples $\\Omega \\subset \\mathcal{X}$, also called database, where $\\mathcal{X}$ is the space of all images. The set of database examples that are positive or negative to the query are denoted by $P_q$ and $N_q$, respectively, with $\\Omega = P_q \\cup N_q$. Ground-truth information for the positive and negative sets per query is obtained according to discrete class labels per example, i.e. if two examples come from the same class, then they are considered positive to each other, otherwise negative. This is the case for all (training or testing) databases used in this work. Terms example and image are used interchangeably in the following text. In image retrieval, all database images are ranked according to similarity to the query $q$, and the goal is to rank positive examples before negative ones.\nDeep image embeddings. Image embeddings, oth-erwise called descriptors, are generated by function $f_\\theta : \\mathcal{X} \\rightarrow \\mathbb{R}^d$. In this work, function $f_\\theta$ is a convolu-"}, {"title": "3.2 Recall@k surrogate loss", "content": "The computation of recall in (3) involves the use of the Heaviside step function. The gradient of the Heaviside step function is a Dirac delta function. Hence, direct optimization of recall with back-propagation is not feasible. A common smooth approximation of the Heaviside step function is provided by the logistic function [60-62], a common sigmoid function $\\sigma_\\tau :$ $\\mathbb{R} \\rightarrow \\mathbb{R}$ controlled by temperature $\\tau$, which is given by\n$\\sigma_\\tau(u) = \\frac{1}{1+e^{-u / \\tau}} $\nwhere large (small) temperature value leads to worse (better) approximation and denser (sparser) gradient. This approximation is common in the machine learning literature for several tasks [63-65] and also appears in the approximation of the Average Precision evaluation metric [49], which is used for the same task as ours. By replacing the step function with the sigmoid function, a smooth approximation of recall is obtained as\n$ \\tilde{R}_\\tau^k(q) = \\frac{\\sum_{x \\in P_q} \\sigma_{\\tau_1} (k-1 - \\sum_{z \\in \\Omega, z\\neq x} \\sigma_{\\tau_2} (s_{qz} - s_{qx}))}{|P_q|}$\nwhich is differentiable and can be used for training with back-propagation. The two sigmoids have different function domains and, therefore, different temperatures (see Figure 3). The minimized single-query loss in a mini-batch B, with size M = |B|, and query q\u2208 B is given by\n$L^k(q) = 1 - \\tilde{R}_\\tau^k(q)$.\nwhile incorporation of multiple values of k is performed in the loss given by\n$L^K(q) = \\frac{1}{|K|} \\sum_{k \\in K}L^k(q).$\nAll examples in the mini-batch are used as queries and the average loss over all queries is minimized during the training. The proposed loss is referred to as Recall@k Surrogate loss, or RS@kloss for brevity.\nTo allow for 0 loss when k is smaller than the number of positives (note that exact recall@k is less than 1 by definition), we slightly modify (5) during the training. Instead of dividing by |Pq|, we divide by min(k, |Pq|), and, consequently, we clip values larger than k in the numerator to avoid negative loss values."}, {"title": "3.3 Similarity mixup (SiMix)", "content": "Given original batch B, virtual batch B is created by mixing all pairs of positive examples in the original"}, {"title": "3.4 Large batch size", "content": "Since recall@k is a non-decomposable objective function, which needs to be computed on the entire training dataset. To better reflect this objective function during training with the proposed surrogate loss, use of large batch sizes is required for the training. Additionally, the larger the batch the high the chance for hard"}, {"title": "3.5 Pre-training method", "content": "We briefly present the different pre-training methods that we use as model initialization for performing DML. The pre-training methods explored in this paper span supervised learning [17], weakly supervised [18], self-supervised [21] and text-supervised [19, 20]. The pre-trained models of those methods are open source and widely used for a variety of tasks. We consider the following methods.\nImageNet-21k [17]. This variant corresponds to supervised training with cross-entropy loss on"}, {"title": "3.6 Overview", "content": "An overview of the training process with the proposed loss and SiMix is given in Algorithm 2. In case SiMix is not used, then lines 11, 13, 14 and 15 are skipped. It is assumed that each image in training is labeled to a class. Mini-batches of size M are generated by randomly sampling m images per class out of M/m randomly sampled classes."}, {"title": "4 Experiments", "content": "The training and evaluation is performed on four widely used deep metric learning benchmarks, namely iNaturalist [13], PKU VehicleID [70], Stanford Online Products [14] (SOP), and Stanford Cars [15] (Cars196). Recall at top k retrieved images, denoted by r@k, is one of the standard evaluation metrics in these benchmarks. Metric r@k is 1 if at least one positive image appears in the top k list, otherwise 0. The metric is averaged across all queries. Note that this is different from the standard definition of recall in (1).\niNaturalist [13] is firstly used by Brown et al. [49], whose setup we follow: 5, 690 classes for training and 2, 452 classes for testing. For VehicleID, according to"}, {"title": "4.2 Implementation Details", "content": "We experiment with CNN and ViT based vision encoders. The implementation details differ based on the vision encoder used for an experiment. The implementation details are identical across the metric learning benchmarks but differ for ROxford/RParis to follow and compare to prior work [25]. These differences are clarified when applicable.\nArchitectures. For CNN based experiments, an ImageNet [16] pre-trained ResNet-50 [59] is used as the backbone for deep image embeddings. Building on the standard implementation of [40], the BatchNorm parameters are kept frozen during the training. After the convolutional layers, Generalized mean pooling [74] and layer normalization [75] are used, similar to [32]. The last layer of the model is a d dimensional fully connected (FC) layer with L2 normalization. In the case of ROxford/RParis, ResNet-101 [59] is used, layer normalization is not added, while the FC layer is initialized with the result of whitening [74]. For vision transformers [17] (ViT) based setup, we experiment with different initialization based on the encoder pre-training. For model implementation and initialization, we rely on PyTorch hub and timm libraries [76]. The details of explored initialization for ViT encoders are presented in Section 3.5. For CLIP and DiHT based models, we only use the vision encoders, while the text"}, {"title": "4.3 Results", "content": "Comparison to SoTA. We present the performance comparison to existing methods with a variety of backbones in Figure 6. A major focus is on performing a direct comparison with SAP, which is a very similar"}]}