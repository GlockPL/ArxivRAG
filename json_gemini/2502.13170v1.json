{"title": "UNVEILING THE MAGIC OF CODE REASONING THROUGH REFLECTIVE HYPOTHESIS DECOMPOSITION AND AMENDMENT", "authors": ["Yuze Zhao", "Tianyun Ji", "Wenjun Feng", "Zhenya Huang", "Qi Liu", "Zhiding Liu", "Yixiao Ma", "Kai Zhang", "Enhong Chen"], "abstract": "The reasoning abilities are one of the most enigmatic and captivating aspects of large language models (LLMs). Numerous studies are dedicated to exploring and expanding the boundaries of this reasoning capability. However, tasks that embody both reasoning and recall characteristics are often overlooked. In this paper, we introduce such a novel task, code reasoning, to provide a new perspective for the reasoning abilities of LLMs. We summarize three meta-benchmarks based on established forms of logical reasoning, and instantiate these into eight specific benchmark tasks. Our testing on these benchmarks reveals that LLMs continue to struggle with identifying satisfactory reasoning pathways. Additionally, we present a new pathway exploration pipeline inspired by human intricate problem-solving methods. This Reflective Hypothesis Decomposition and Amendment (RHDA) pipeline consists of the following iterative steps: (1) Proposing potential hypotheses based on observations and decomposing them; (2) Utilizing tools to validate hypotheses and reflection outcomes; (3) Revising hypothesis in light of observations. Our approach effectively mitigates logical chain collapses arising from forgetting or hallucination issues in multi-step reasoning, resulting in performance gains of up to 3x. Finally, we expanded this pipeline by applying it to simulate complex household tasks in real-world scenarios, specifically in Virtu- alHome, enhancing the handling of failure cases. We release our code and all of results at https://github.com/TnTWoW/code_reasoning.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs), which are trained on billions of tokens, have demonstrated impressive reasoning abilities in complex tasks Brown et al. (2020); Wei et al. (2022); Kojima et al. (2022); OpenAI (2023). However, it is evident that as potential fuzzy retrieval systems or parameterized knowledge compression systems Xie et al. (2021), LLMs perform better on System 1 tasks than on System 2 tasks Kahneman (2011); Bengio (2019); Yao et al. (2023a); Weston & Sukhbaatar (2023); Christakopoulou et al. (2024). Specifically, LLMs excel in intuitive, memory-retrieval tasks, but continue to face significant challenges with tasks requiring rational reasoning Kambhampati (2024).\nFrom the perspective of human cognitive psychology, reasoning can be viewed as a process of memory retrieval, in which people retrieve relevant information from memory and use it to make inferences Kyllonen & Christal (1990); S\u00fc\u00df et al. (2002); Hayes et al. (2014); Feeney & Thompson (2014); Hardman & Cowan (2015). For example, Haidt (2001) proposed that when individuals engage in moral reasoning, they typically draw upon their prior knowledge from social and cultural contexts. Similarly, studies involving animal lesions and human neuroimaging have confirmed that the hippocampus, which is primarily associated with memory, also plays a crucial role in reasoning"}, {"title": "2 META-BENCHMARK", "content": "We describe the general process of code reasoning as the transformation from Input I and Program P to Output O, represented as I P\u2192 O. Inductive code reasoning is concretized as the Programming by Example (PBE) task. In this task, a neural program synthesis model M searches the execution space to find a program that best satisfies all given input-output specifications. We donate this meta- benchmark as M(I,O) \u2192 P. Deductive code reasoning is exemplified in tasks that simulate the program execution process. In this task, a neural simulation compiler model M tracks the program's execution and records intermediate states, gradually deriving the final valid output. We denote this meta-benchmark as M(I,P) \u2192 \u00d5. Abductive code reasoning is concretized as input prediction tasks. This task requires the neural understanding model M to form an abstract-level understanding of function's behavior and perform abductive inference based on the given program and output. We represent this meta-benchmark as M(O,P) \u2192 \u0128. The details of the benchmarks are provided in the Appendix C."}, {"title": "2.1 INDUCTIVE CODE REASONING", "content": "Inductive code reasoning can be represented as M(I, O) \u2192 P and is concretized as a PBE task Qiu et al. (2024); Shi et al. (2024). PBE is a program synthesis task designed to help end-users, par- ticularly non-programmers, create scripts to automate repetitive tasks Gulwani (2016). Based on input-output specifications, PBE systems can synthesize a program in either a general-purpose lan- guage (GPL) or a domain-specific language (DSL). Inductive code reasoning encompasses four challenging PBE tasks, two of which are GPL tasks: List Function Rule (2020) and MiniARC Kim et al. (2022), while the other two are DSL tasks: RobustFill Devlin et al. (2017) and DeepCoder Ba- log et al. (2016). GPL tasks are relatively complex, allowing the model to solve problems in a more flexible manner. In contrast, DSL tasks require the model to quickly learn the syntax of DSL through few-shot learning and address relatively simpler problems.\nList Function. The List Function task was originally designed to investigate how humans learn the concept of computable functions that map lists to lists. Given input and output specifications in the form of lists, the model generates GPL rules that conform to these specifications. For example, with an input specification of [2, 4, 8, 10] and an output specification of [3, 5, 9, 11], we expect the resulting rule to be lambda x: x + 1\u00b9.\nMiniARC. MiniARC is a compressed 5x5 version of the Abstraction and Reasoning Corpus Chol- let (2019); Moskvichev et al. (2023), designed to assess imaginative and reasoning abilities. MiniARC balances the length of the input-output pairs with the difficulty of the problems. The specifications are 5x5 2D grids, where the numbers represent blocks of specific colors. The model must find valid problem-solving paths (such as color swapping, row flipping) to achieve the trans- formation from input to output.\nRobustFill. RobustFill is a string manipulation task where the model is expected to perform a combination of atomic operations, such as extracting a substring from position k\u2081 to k\u2082 us- ing SubString(k\u2081, k\u2081), to achieve generalization. As an example, a program ToCase(Lower, SubStr(1,3)) converts full month names (January, April) to their abbreviations (jan, apr).\nFor conciseness while maintaining generality, we will use lambda expressions to represent a program."}, {"title": "2.2 DEDUCTIVE CODE REASONING", "content": "Deductive code reasoning refers to the process of deriving a sound inference O by reasoning from the given premise I, assuming the validity of the argument P. Deductive code reasoning can be instantiated as an output prediction task Gu et al. (2024). Based on the given premise, out- put prediction requires the LLM to simulate a compiler Kim et al. (2024b), executing step by step until it arrives at a valid conclusion. For example, given a program P = lambda text, value: ''.join(list(text) + [value]) and inputs text = 'bcksrut', b = 'q', the output prediction from LLM should be 'bcksrutq\u02b9."}, {"title": "2.3 ABDUCTIVE CODE REASONING", "content": "Starting from existing facts P and O, deriving the most reasonable and optimal explanation I is referred to as abductive code reasoning. This meta-benchmark can be framed as an input prediction task. Given the provided facts, input prediction requires the LLM to backtrack through the program's execution process to recover the potential inputs. In cases where multiple possible inputs exist, the model should apply Occam's Razor and return the simplest input. For example, given a program P = lambda nums: nums + [nums[i % 2] for i in range(len(nums))] and out- puts [-1, 0, 0, 1, 1, -1, 0, -1, 0, -1], the input prediction from LLM should be [-1, 0, 0, 1, 1].\nDeductive code and abductive code reasoning can be regarded as opposite processes; therefore, we selected two identical and representative datasets, CRUXEval Gu et al. (2024) and Live- CodeBench Jain et al. (2024), as benchmarks to validate these two capabilities."}, {"title": "3 CODE REASONING WITH HYPOTHESIS DECOMPOSITION AND AMENDMENT", "content": "We aim to generate a reliable reasoning process for problem-solving by establishing a problem- solving pathway f : X \u2192 Y. For a given task 7 and the seen specifications/observations X, the pathway f, should lead to a seen valid solution Y through a chain of reasoning. We expect this pathway f to have sufficient generalization capabilities to handle unseen specifications/observations X. To this aim, we employ a process involving hypothesis decomposition, execution verification, and amendment submission to iteratively explore and refine the reasoning pathway. We first establish an initial hypothesis h\u00ba \u2208 \u03a3* based on the observations x \u2208 X, where \u03a3* is closure form of LLM's vocabulary. This initial hypothesis h\u00ba serves as a preliminary solution pathway to the problem. Given the complexity of many problems, we decompose the hypothesis h\u00ba into simpler sub-hypotheses h\u00ba {ho, ho, ho, h,...}. A translator function g : \u03a3* \u2192 \u03a3\u1f14, which maps the hypothesis space \u2211* into an executable function space 2, is then used to \u2018compiled' the sub-hypotheses h\u00ba into an executable function e\u00ba. This executable function is directly applicable to the"}, {"title": "$\\tilde{y} = g(h^o)(x)$", "content": "observations x, allowing for the derivation of conclusions \u1ef9, that is:\n$\n(1)\nFeedback $F(y, \u1ef9)$ is used to evaluate the conclusions drawn from the current hypothesis, guiding the LLM to reflect on its sub-hypotheses. Through this iterative process of reflection, the model gen- erates a new hypothesis h\u00b9 for the next iteration. Finally, the problem-solving pathway f is applied to unseen observations X, and the model's generalization performance is assessed by measuring its accuracy:\n$\\operatorname{acc}_{X_u} \\triangleq \\frac{1}{|X_u|} \\sum_{x \\in X_u} \\mathbb{1} \\left[f(\\mathbf{x})=y\\right].$\n(2)\nThe preceding section presents a unified framework for the hypothesis decomposition and amend- ment method. However, the implementation specifics differ across various tasks. In the following sections, we will introduce these task-specific variations in detail.\nHypothesis Decomposition. We recognize that complex logical reasoning problems are difficult to encapsulate in a single reasonable hypothesis, which can adversely affect the performance of LLMs. Therefore, we require the LLM to decompose its hypotheses. Specifically, given an obser- vation x, the LLM gradually presents corresponding hypotheses step by step. For inductive code reasoning, ho represents the step-by-step hypothesis of the input-to-output transformation rules. For deductive and abductive code reasoning, ho refers to the step-by-step hypothesis regarding the func- tionality of the program.\nExecution Verification. After obtaining the hypothesis, we need to apply it to the observations. However, hypotheses are often not directly usable, so we need to convert the decomposed hypothesis into an executable function e through a translator g. For inductive code reasoning, the executable function is a program; for deductive and abductive code reasoning, the executable function is the predicted output and input, respectively. These three types of tasks are then sent to a compiler to obtain the actual execution results, and the feedback generated by the compiler is provided to the LLM to help it further refine and adjust the sub-hypotheses."}, {"title": "4 EXPERIMENTS", "content": "Experimental Setup. We utilize the latest and most advanced model, gpt-4o-2024-08-06, as the backbone LLM for all our experiments. We report the results using Llama-3.1-70B-Instruct, Qwen- max (qwen-max-2024-09-19) Bai et al. (2023), Claude 3.5 (claude-3-5-sonnet-20240620) in Ap- pendix B. Following the methodology of Qiu et al. (2024), we set the temperature to 0.7. We report results using several methods: input-output (IO) prompting, standard prompting, Chain of Thought (CoT) Wei et al. (2023), Program of Thought (PoT) Chen et al. (2023), Chain of Code (CoC) Li et al. (2024), Self-Consistency (SC) Wang et al. (2023c) and Self-Refine (SR) Madaan et al. (2024), all implemented with 2-shot learning.\u00b2 For our proposed process, we employ 0-shot prompts, allow- ing the LLM to explore problem-solving pathways in a more flexible manner. We provide detailed prompt templates in Appendix H."}, {"title": "4.1 INDUCTIVE CODE REASONING", "content": "For inductive code reasoning, we establish four baseline methods. The Input-Output (IO) prompt- ing requires the LLM to predict outputs based on all seen observations and an unseen input. The Program of Thought (PoT) method generates and executes programs to derive outputs. The CoC method prompts the LLM to utilize pseudocode for reasoning in output prediction. The SC method builds upon PoT by sampling multiple programs and selecting the one that demonstrates optimal performance on seen observations. Furthermore, since each example may contain multiple unseen observations, we adopt the approach from Qiu et al. (2024) to define task accuracy externally. An example is deemed passed only when all unseen observations within it pass; thus, the proportion of passed examples reflects the task accuracy. The experimental results are presented in Table 1.\nThe results demonstrate that the RHDA method achieves optimal performance across four bench- marks, with task accuracy exceeding that of the second-best methods by 18.45%, 5.89%, 33.31%, and 12.02%, respectively. However, we observe that RHDA appears to underperform compared to IO prompting. This is because the IO prompt does not generate a hypothesis that satisfies all ob- servations but instead predicts the output for a single input. A successful prediction for a single instance does not generate a hypothesis that satisfies all observations, resulting in a high prediction accuracy but a relatively low task accuracy."}, {"title": "4.2 DEDUCTIVE CODE REASONING", "content": "For deductive code reasoning, we select standard prompting, CoT, SC, SR and CoC as benchmark methods. The experimental results are presented in Table 2. These results indicate that the CoT and CoC methods significantly enhanced the accuracy of reasoning outcomes by guiding the model to think step-by-step about function capabilities. Our proposed method advances this further, achiev- ing optimal performance with a single round of amendments, resulting in an improvement of up to 104.37% compared with baseline method. A horizontal comparison of the two datasets revealed that, due to the absence of LiveCodeBench data in internet corpora, the performance with standard prompts showed a marked advantage, with the SC method amplifying this gap. Notably, the com- bination of CoT, CoC, and hypothesis decomposition and amendment enabled the LLM to exhibit a substantial degree of reasoning and generalization ability, nearly solving all presented problems."}, {"title": "4.3 ABDUCTIVE CODE REASONING", "content": "For abductive code reasoning, we employ the same baseline methods as those used for de- ductive reasoning. The experimental results are presented in Figure 3. Compared to deduc- tive reasoning, abductive reasoning involves a reverse thinking process, which presents sig- nificant challenges. The LLM cannot derive the program's intermediate states through de- duction and must first establish an abstract- level understanding of the function's behav- ior before proceeding with abduction. On the CRUXEval dataset, the performance decline for abductive reasoning ranged from 8.20% to 25.52%. However, the hypothesis decompo- sition and amendment approach demonstrated robustness, as the shift in reasoning modes re- sulted in only minimal performance degrada- tion (8.20%) while still outperforming baseline methods by 10.02% to 31.89% on the CRUX- Eval dataset and 7.35% to 40.39% on the LiveCodeBench dataset. A horizontal comparison of the two datasets revealed a trend similar to that observed in deductive reasoning, with an overall perfor- mance decline on the LiveCodeBench dataset, suggesting a complex relationship between reasoning and recall."}, {"title": "4.4 QUALITATIVE ANALYSE", "content": "We select some cases to conduct an in-depth exploration of the quality of RHDA."}, {"title": "4.5 RHDA IS A FLEXIBLE AND SCALABLE PROBLEM-SOLVING PATHWAY", "content": "We consider extending the RHDA pipeline to more complex scenarios. To this end, we select Vir- tualHome Puig et al. (2018; 2020), a sophisticated multi-agent platform for simulating household activities, as our new exploration subject. VirtualHome comprises a set of predefined atomic ac- tions and objects that can be combined into high-level instructions. For example, \u2018<char0) [walk] (salmon)' describes character 0 walking to the salmon. Given a specific scenario, the LLM is tasked with completing concrete housework using a series of high-level instructions. As depicted in Fig- ure 4, and guided by the RHDA process, we demonstrate how the LLM successfully accomplishes the task of storing pie in the fridge through the methods of hypothesis decomposition, execution verification (offloading to VirtualHome engine), and reflection. we show another example in App- neidx D."}, {"title": "5 LIMITATION AND DISCUSSIONS", "content": "Benchmark Selection. This paper represents the first systematic exploration of the code reasoning task, focusing on the analysis of three forms of logical reasoning: inductive, deductive, and abduc- tive. Due to time and cognitive constraints, we were unable to collect all benchmarks for testing. Our aim is to stimulate in-depth discussion on this topic and inspire meaningful follow-up research. While several excellent studies utilize code to address logical reasoning tasks Zelikman et al. (2023); Hu et al. (2023); Srivastava et al. (2024); Liu et al. (2024), we did not include them here due to their differing starting points from this paper.\nHyperparameters. The goal of this paper is to explore the potential of LLMs in code reasoning, rather than solely improving the performance of a specific code reasoning task. The RHDA frame- work serves as a preliminary exploration process; therefore, we didn't fully optimized the prompt"}, {"title": "6 RELATED WORK", "content": "Reasoning with LLMs. LLMs such as GPT OpenAI (2023), LLaMA Touvron et al. (2023), and Claude Anthropic (2024), demonstrate impressive reasoning capabilities across various NLP tasks Zhang et al. (2024). However, due to the problems of direct reasoning with LLMs such as hallucinations Ji et al. (2023), researchers have proposed several methods to enhance the reasoning power of LLMs. For example, Zhou et al. (2023); Xue et al. (2025) decompose complex tasks into sequential subproblems, while Sun et al. (2024) refine reasoning through environment feedback. Moreover, intermediate representations, such as graphs Jiang et al. (2024), planning domain defini- tion languages (PDDL) Guan et al. (2023), and triples Wang et al. (2023a), have been employed to enhance LLM's reasoning. Most recently, OpenAI 01 OpenAI (2024) demonstrates strong reason- ing capabilities and broad world knowledge. Upon further contemplation, it is capable of reasoning through complex tasks and addressing challenges that exceed those faced by previous scientific, coding, and mathematical models.\nSimultaneously, domain-specific reasoning with LLMs has gained attention. Kim et al. (2024a) enhance reasoning outputs in computer tasks through recursive critique. In a case study using Minecraft, Wang et al. (2023d) introduce a Describe, Interpret, Plan, and Select framework for open-world multitasking. In computer vision, Gupta & Kembhavi (2023) employ Python-like mod- ular programs to tackle complex tasks. Nonetheless, reasoning in code remains an area yet to be thoroughly explored.\nImprovement with Reflection. Reflective ability is regarded as a crucial metric for evaluating LLMs as agents. Reflection can be categorized into internal and external based on its feedback source Pan et al. (2024). Internal reflection relies feedback from the model's own knowledge and parameters Huang et al. (2022), while external feedback comes from various sources, including humans Wang et al. (2023b), other models Paul et al. (2024), external tools Gou et al. (2024); Chen et al. (2024), or knowledge bases Yao et al. (2023b); Asai et al. (2024). Huang et al. (2024) find that LLMs struggle to self-correct their responses without external feedback, and in some cases, their performance may even decline following self-correction. Our work focuses on leveraging external tools, such as compilers, to generate feedback and enhance the performance of LLMs."}, {"title": "7 CONCLUSION", "content": "In this paper, we emphasized that the reasoning capabilities of LLMs still depend on recalling prior knowledge and highlighted that code reasoning has not been sufficiently explored as a novel perspec- tive for examining the boundaries of LLM capabilities. Based on this consideration, we designed three meta-benchmarks\u2014inductive code reasoning, deductive code reasoning, and abductive code reasoning-drawing on established forms of logical reasoning, and instantiated these benchmarks into eight specific tasks. Experimental results indicated that these benchmarks present significant challenges for current state-of-the-art LLMs. To initially explore code reasoning tasks, we proposed a method involving Reflective Hypothesis Decomposition and Amendment (RHDA). This method was iterative: LLMs need to generate decomposed initial hypotheses based on observations and employ a translator to interpret these into executable functions that can be directly applied to the observations. After obtaining the executable functions, we performed execution verification and submit amendments, allowing for reflection and refinement of the sub-hypotheses. Experimental results demonstrated that this approach, which integrated the principles of divide-and-conquer and reflection, can flexibly solve complex code reasoning problems, achieving performance improve- ments of 2 to 3 times compared to baseline methods. Finally, we extended this process to simulate household tasks in real-world complex scenarios to validate its scalability and transferability."}, {"title": "A DSL GRAMMARS", "content": "RobustFill is a string manipulation task using the DSL. Figure 5 illustrates the DSL syntax for RobustFill. Our implementation is based on the works of ExeDec Shi et al. (2024) and Robust- Fill Devlin et al. (2017).\nDeepcoder is a list transformation task using the DSL. Figure 6. This implementation is based on the works of ExeDec Shi et al. (2024) and DeepCoder Balog et al. (2016)."}, {"title": "B EXPERIMENTAL RESULTS USING MORE LLMS", "content": "We report the performance of Llama3.1-70B-Instruct, Qwen-max (qwen-max-2024-09-19), Claude 3.5 (claude-3-5-sonnet-20240620) using the RHDA method and compare them with GPT-40 (gpt- 40-2024-0806). The results for inductive code reasoning are shown in Table 5. The experimental results indicate that GPT-40 performs better in solving DSL problems, while Claude 3.5 excels in General Propose Language (GPL) tasks. Compared to closed-source models, the open-source model Llama still exhibits relatively limited reasoning capabilities. However, in list manipulation tasks (List Function and Deepcoder), Llama demonstrates stronger programming abilities."}, {"title": "D RHDA ACTING AS AN AGENT IN VIRTUALHOME", "content": "We utilized the RHDA framework to drive agent actions in the VirtualHome environment powered by LLMs. Figure 7 illustrates a task of cleaning the bathroom.\nWe also provided some quantitative metrics to validate the potential of RHDA as a agent in Vir- tualHome. Specifically, we selected a total of 52 tasks across two scenarios in VirtualHome and manually tested their execution error rates. The test results are shown in Table 8, which indicate that native GPT-40 struggles to handle simulated real-world scenarios effectively. The primary cause of failure lies in generating scripts that, while semantically similar to correct actions, are not executable within the environment (e.g., 'open the tap' is invalid action, whereas 'touch the tap' is valid action). By employing the RHDA method, which incorporates step-by-step solutions and effective feedback mechanisms, the error rate was significantly reduced."}, {"title": "E EXAMPLES ANALYSES", "content": "E.1 EFFECTIVE CASE STUDY"}, {"title": "E.2 FAILURE ANALYZE", "content": "We analyze RHDA's performance in numerous failure cases and summarize the underlying causes of these failures. Our findings suggest that the primary reason can be attributed to the insufficient"}, {"title": "F COSTS", "content": "In Table 15, we present the average number of API calls and the total cost for each task. We used GPT-40, with an input cost of $0.0025/1K tokens and an output cost of $0.01/1K tokens. The results indicate that our approach still demonstrates high cost-effectiveness for certain tasks."}, {"title": "G TRADE OFF BETWEEN NUMBER OF ITERATIONS AND PERFORMANCE GAIN", "content": "In this section, we investigate the impact of iteration count on the performance of three types of reasoning tasks, with experimental results illustrated in Figure 8 and Figure 9. For inductive and abductive code reasoning tasks, performance consistently improved as the number of iterations in- creased. However, the rate of improvement diminished, with marginal gains becoming less sig- nificant at higher iteration counts. Conversely, for deductive code reasoning tasks, performance followed a rise-and-fall trend, initially improving but declining with excessive iterations. These findings suggest that while increasing the number of iterations can enhance performance for general code reasoning tasks, it is crucial to balance iterative gains against potential performance instability."}, {"title": "H PROMPTS", "content": ""}]}