{"title": "Performance Metric for Multiple Anomaly Score\nDistributions with Discrete Severity Levels", "authors": ["Wonjun Yi", "Wonho Jung", "Yong-Hwa Park"], "abstract": "The rise of smart factories has heightened the\ndemand for automated maintenance, and normal-data-based\nanomaly detection has proved particularly effective in\nenvironments where anomaly data are scarce. This method,\nwhich does not require anomaly data during training, has\nprompted researchers to focus not only on detecting anomalies\nbut also on classifying severity levels by using anomaly scores.\nHowever, the existing performance metrics, such as the area\nunder the receiver operating characteristic curve (AUROC), do\nnot effectively reflect the performance of models in classifying\nseverity levels based on anomaly scores. To address this\nlimitation, we propose the weighted sum of the area under the\nreceiver operating characteristic curve (WS-AUROC), which\ncombines AUROC with a penalty for severity level differences.\nWe conducted various experiments using different penalty\nassignment methods: uniform penalty regardless of severity\nlevel differences, penalty based on severity level index\ndifferences, and penalty based on actual physical quantities that\ncause anomalies. The latter method was the most sensitive.\nAdditionally, we propose an anomaly detector that achieves\nclear separation of distributions and outperforms the ablation\nmodels on the WS-AUROC and AUROC metrics.", "sections": [{"title": "I. INTRODUCTION", "content": "With the proliferation of smart factories, the demand for\nautomated maintenance has increased significantly. Normal-\ndata-based anomaly detection is effective in environments\nwhere anomaly data are scarce because it does not require\nsuch data for training and validation. Recently, this approach\nhas evolved not only to detect anomalies but also classify their\nseverity levels based on anomaly scores [1], [2]. Accordingly,\nsystems with higher anomaly scores indicate greater severity\nlevels.\nOne method to measure performance is by setting a\nthreshold for anomaly scores to classify anomalies by severity\nlevel, and then, as in [3], applying penalties for discrepancies\nbetween the ground truth and predicted severity levels. This\napproach assumes that thresholding is a valid method for\ndelineating severity levels. However, setting thresholds\ninherently involves referencing test data's anomaly scores,\nwhich contradicts the goal of avoiding reliance on test data for\nperformance evaluation.\nTo address these issues, we propose the weighted sum of\nthe area under the receiver operating characteristic curve (WS-\nAUROC). This method integrates penalties based on severity\nlevel differences between two distributions under evaluation\nwith AUROC. We demonstrate that WS-AUROC effectively\nseparates anomaly scores depending on severity levels and is\nconsiderably more sensitive than AUROC. Additionally, we\ncompare different penalty assignment methods: uniform\npenalty regardless of severity level differences, penalty based\non severity level index differences, and penalties based on\nactual physical quantities that cause anomalies. The results\nindicate that the method based on actual physical quantities is\nthe most sensitive in practice.\nTo demonstrate the application of WS-AUROC and\nAUROC, we propose an anomaly detection model that\nperforms well on a vibration dataset [4], which simulates\nanomalies in a test bed. In this model, the final anomaly score\nis calculated by summing the anomaly scores obtained\nthrough application of the K-nearest neighbor (KNN) method\nto rotation frequency harmonics and ball pass frequency outer\nrace (BPFO) frequency harmonics and those obtained by\ninputting fast Fourier transform (FFT) features into a one-\ndimensional convolutional neural network (1DCNN).\nAccording to this model, the anomaly score increases in\nproportion to the severity level. Additionally, in ablation\nstudies, the proposed anomaly detector outperforms other\nmethods on the AUROC and WS-AUROC metrics. The code\nis available in [5]."}, {"title": "II. PROPOSED METRIC", "content": "The WS-AUROC metric assigns penalties based on the\ndiscrepancy between two distributions of anomaly scores\ncorresponding to different severity levels. The rationale is that\nthe greater the discrepancy, the higher should be the penalty.\nLet A represent the WS-AUROC metric. For the\nanomaly severity level index $i\\in \\{0,1,...,n\\}$, where 0\ndenotes the normal condition, and numbers greater than 0\ndenote the severity level of an anomaly condition, the anomaly\nscore distribution $S_i$ can be measured. Next, the AUROC $a_{ij}$\nbetween two anomaly score distributions $S_i$ and $S_j$ can be\nmeasured. Then, by using multiple $a_{ij}$, WS-AUROC can be\ncalculated as follows:\n$A=1-\\sum_{i=0}^{n-1}\\sum_{j=i+1}^{n} p_{ij}(1-a_{ij}) = \\sum_{i=0}^{n-1}\\sum_{j=i+1}^{n} p_{ij}a_{ij}$ (1)\nwhere $P_{ij}$ is the penalty assigned based on the severity level\ndifference between $S_i$ and $S_j$.\nPij can be defined in three different ways: uniform (U),\nindex (I), and physics (P). First, a uniform penalty can be\napplied regardless of severity level differences, as follows:\n$P_{ij} = \\frac{2}{n(n+1)}$ (2)\nSecond, index penalty can be defined in proportion to the\nseverity level index difference j-i, which ensures that\nlarger differences in severity levels lead to higher penalties:\n$P_{ij} = \\frac{6(j-i)}{n(n+1)(n+2)}$ (3)\nThird, physics penalty can be based on actual physical\nquantities that cause anomalies. Here, penalties are assigned\nin proportion to the physical differences that cause an\nanomaly, ensuring that the penalties are more reflective of\nreal-world conditions:\n$P_{ij} = \\frac{W_i-W_j}{\\sum_{a=0}^{n-1}\\sum_{b=i+1}^{n} (W_a-W_b)}$ (4)\nIn (4), $w_i$ represents the physical quantity corresponding\nto the severity level index i. For instance, in the dataset used\nherein [4], the physical quantities are the mass that causes\nunbalance and the defect length that causes misalignment. For\nbearing defects, the physical quantity is the defect length\nassociated with the characteristic frequencies of ball pass\nfrequency inner race (BPFI) and BPFO. The physics penalty,"}, {"title": "III. PROPOSED ANOMALY DETECTION MODEL", "content": "The vibration dataset [4] simulates anomalies such as\nunbalance, misalignment, and bearing faults, which cause\ncracking of the inner and outer bearing races. For unbalance,\nthe magnitude of the rotation frequency component $f$\nincreases compared to that under normal conditions. For\nmisalignment, generally, the magnitudes of the $2f,4f,...$\ncomponents increase. For BPFI and BPFO, the magnitudes of\nthe harmonics of their characteristic frequency increase. In the\nexperiment conducted in [4], the rotation frequency was 50.17\nHz, BPFI characteristic frequency was 272.07 Hz, and BPFO\ncharacteristic frequency was 179.43 Hz. In the presence of\nthese anomalies, we constructed vectors by using the\nmagnitude and phase information of up to the fourth harmonic\nof $f_r$. Because we used four accelerometers, the length of\neach vector was 32. Additionally, we created separate vectors\nof length 32 for BPFI and BPFO by using their characteristic\nfrequencies.\nWe applied the KNN method with k = 1 to calculate the\nminimum distance to the nearest neighbor. Then, we\ncalculated the minimum distance of validation data and test\ndata. Subsequently, we normalized the test-data distances by\nusing the minimum and maximum validation-data distances."}, {"title": "C. FFT and 1DCNN as feature extractor", "content": "We used FFT and 1DCNN to extract feature vectors and\nused KNN to score the test data. FFT components of up to 1\nkHz were input into the 1DCNN, which distinguished torque\nloads (0 Nm, 2 Nm, 4 Nm) based on the data obtained using\nfour accelerometers. To enhance feature representation\nlearning, we used MixUp [6] with a uniform distribution. The\nmodel was trained over 20 epochs by employing categorical\ncross entropy loss, a learning rate of 1e-3, a batch size of 16,\nand the Adam optimizer [7]. The model with the lowest\nvalidation loss was used for anomaly detection. For the\ndownstream task, we extracted feature vectors from the\npenultimate layer of the 1DCNN and trained a KNN-based\nmodel on the training-data feature vectors. For the validation\nand test datasets, we used the KNN model to calculate the\nminimum distances among the data obtained using four\naccelerometers, which yielded four anomaly scores per event.\nThese scores were summed to obtain the final anomaly scores.\nThe test-data anomaly scores were normalized using the\nminimum and maximum validation-data anomaly scores."}, {"title": "D. Ensemble", "content": "By combining the four test anomaly scores, we created\nvarious combinations of anomaly detectors by using ensemble\nmethods.\nFor convenience, we refer to the three anomaly scores\nobtained in sections III-A and III-B as Rotation, BPFI, and"}, {"title": "IV. EXPERIMENT", "content": "The data used in the experiment consist of 432 normal\nsamples for training, 54 normal samples for validation, and\n270 samples for testing; the test samples contained 54 samples\neach for the normal, unbalance, BPFI, BPFO, and\nmisalignment conditions. The unbalance anomaly samples\nhad five severity levels (583 mg, 1169 mg, 1751 mg, 2239 mg,\nand 3318 mg); the BPFI and BPFO anomaly samples had three\nseverity levels each (0.3 mm, 1.0 mm, and 3.0 mm); and the\nmisalignment anomaly samples had three severity levels (0.1\nmm, 0.3 mm, 0.5 mm). When the 1DCNN was not utilized,\nKNN, a non-parametric method, was run only once. However,\nwhen the 1DCNN was used, the experiment was conducted\nsix times with different random seeds. AUROC and WS-\nAUROC were used as the performance metrics. By using 54\nnormal test samples and 54 anomaly test samples for each\nanomaly type, we calculated the AUROC and three types of\nWS-AUROC (Uniform, Index, and Physics) for four pairs of\ndatasets, resulting in 16 individual performance metrics.\nAdditionally, we calculated the averages of AUROC and three\ntypes of WS-AUROC."}, {"title": "V. RESULT", "content": "Tables I-III summarize the performance metrics for\ndifferent combinations of anomaly detection methods applied\nto various anomaly types. According to Table III, the\ncombination of Rotation, BPFO, and DL yielded the highest\naverage WS-AUROC anomaly score for the Uniform, Index,\nand Physics penalties, outperforming the other combinations.\nThe anomaly score distributions of this combination are\nplotted in Fig. 1. The results show a clear distinction between\nthe normal and anomaly conditions and separate the anomaly\nscore distributions by severity levels.\nAccording to Table I, the performance of BPFI at index 1\nindicates that a high AUROC does not guarantee a high WS-\nAUROC. To achieve a high WS-AUROC, the model must\ndistinguish between the normal and anomaly conditions and\nensure that the anomaly score distribution is well ordered by\nseverity levels. Fig. 2 illustrates this requirement, where the\nnormal and anomaly conditions are well separated but the\nseverity level anomaly score distributions are in reverse order,\nresulting in a lower WS-AUROC.\nWe plotted scatter plots and calculated biases for all results\nand cases with AUROC = 1, as shown in Fig. 3, to identify the\nmost sensitive performance metric. A negative bias indicates\nhigher sensitivity for the y-axis metric. The AUROC vs. WS-\nAUROC plots and bias calculations revealed WS-AUROC'S\nsensitivity, as it did not consistently show high values near\nAUROC = 1. Among WS-AUROC metrics, the physics\npenalty was the most sensitive. Although the uniform penalty\nalso showed sensitivity, it misaligned with the intended\npenalty philosophy by assigning constant penalties to low\nseverity level differences. The physics penalty maintained this\nphilosophy and proved to be the most sensitive."}, {"title": "VI. CONCLUSION", "content": "We proposed WS-AUROC, which incorporates penalties\nbased on severity level differences, to facilitate more refined\nevaluations than that possible with AUROC. In the\nexperiments, the highest WS-AUROC score was obtained by\nusing a combination of Rotation, BPFO, and DL anomaly\nscores. However, a high AUROC did not always correspond\nto a high WS-AUROC, highlighting the issues caused by\nreversed order of anomaly scores between different severity\nlevels. We experimented with different penalty assignment\nmethods for WS-AUROC, including the uniform, index, and\nphysics-based penalties. The physics-based method\ndemonstrated the highest sensitivity, adhering to the\nphilosophy that greater severity level differences should incur\nhigher penalties.\nIt should be noted that implementing WS-AUROC\nrequires more detailed information than simpler metrics like\nAUROC, specifically regarding severity levels and associated\npenalties. This requirement can present challenges, especially\nwhen such data is not readily available, but it allows for a more\nprecise and sensitive assessment of anomaly detection\nperformance.\nIn future work, WS-AUROC can be applied to datasets\nbeyond vibration, such as audio data [8], or even current data\n[1], [2]. Additionally, defining and utilizing other threshold-\nindependent metrics, such as Partial Area Under the ROC\nCurve (pAUC), Area Under the Precision-Recall Curve (PR\nAUC), and F1-EV [9], can provide a more nuanced evaluation\nof models."}]}