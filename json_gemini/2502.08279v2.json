{"title": "What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations", "authors": ["Dongqi Liu", "Chenxi Whitehouse", "Xi Yu", "Louis Mahon", "Rohit Saxena", "Zheng Zhao", "Yifu Qiu", "Mirella Lapata", "Vera Demberg"], "abstract": "Transforming recorded videos into concise and accurate textual summaries is a growing challenge in multimodal learning. This paper introduces VISTA, a dataset specifically designed for video-to-text summarization in scientific domains. VISTA contains 18,599 recorded AI conference presentations paired with their corresponding paper abstracts. We benchmark the performance of state-of-the-art large models and apply a plan-based framework to better capture the structured nature of abstracts. Both human and automated evaluations confirm that explicit planning enhances summary quality and factual consistency. However, a considerable gap remains between models and human performance, highlighting the challenges of scientific video summarization.", "sections": [{"title": "1 Introduction", "content": "Large multimodal models (LMMs), which integrate components from different modalities through cross-modal alignment training (Koh et al., 2023; Cheng et al., 2023; Li et al., 2024a; Ahn et al., 2024; Fu et al., 2025; Wu et al., 2025), have achieved considerable progress in video-to-text summarization tasks for general-purpose content such as YouTube, movies, and news videos (Li et al., 2020; Lin et al., 2023; Krubi\u0144ski and Pecina, 2023; Hua et al., 2024; Chen et al., 2024a; Zhang et al., 2024a; Qiu et al., 2024; Patil et al., 2024; Mahon and Lapata, 2024a,b). However, many recent studies have highlighted that these LMMs exhibit reduced performance in scientific contexts, particularly when processing technical terminology and scientific visual elements like figures and tables (Li et al., 2024c; Lu et al., 2024; Yue et al., 2024; Hu et al., 2024a; Bai et al., 2024; Liang et al., 2024; Patil et al., 2024; Huang et al., 2024). This performance gap might be largely attributed to the absence of specialized training datasets for multimodal scientific content (Chen et al., 2024c; Hu et al., 2024b; Pramanick et al., 2024; Zhang et al., 2024b).\nThus, we introduce VISTA (Video to Scientific Abstract), an English dataset for video-to-text summarization in scientific domains. VISTA consists of 18,599 aligned pairs of conference presentation recordings and their corresponding paper abstracts, collected from leading conferences in computational linguistics (ACL Anthology including ACL, EMNLP, NAACL, EACL, Findings of *ACL) and machine learning (ICML and NeurIPS)."}, {"title": "2 Related Work", "content": "Video-to-Text Summarization generates coherent summaries by integrating multimodal information (Hua et al., 2024), supported by datasets like MSS (Li et al., 2017), VideoXum (Lin et al., 2024b), MMSum (Qiu et al., 2024), Hierarchical3D (Papalampidi and Lapata, 2023), and LfVS-T (Argaw et al., 2024), spanning tasks from instructional videos to general web content (Li et al., 2017; Zhou et al., 2018; Li et al., 2019, 2020; Liu and Wan, 2021; Fu et al., 2021; Krubi\u0144ski and Pecina, 2023; Han et al., 2023; He et al., 2023; Hua et al., 2024; Islam et al., 2024; Qiu et al., 2024). Technical advancements include hierarchical attention models (Sanabria et al., 2018), extractive methods using multimodal features (Cho et al., 2021; Krubi\u0144ski and Pecina, 2023), and hybrid extractive-abstractive frameworks (Ramakrishnan and Ngan, 2022; Papalampidi and Lapata, 2023). Transformer-based systems have further improved performance (Krubi\u0144ski and Pecina, 2023; Li et al., 2020; Shang et al., 2021; Mahon and Lapata, 2024a). However, challenges in summarizing academic videos remain under-explored.\nScientific Text Summarization condenses complex scholarly content into concise formats (Cachola et al., 2020; Ju et al., 2021; Sotudeh and Goharian, 2022; Liu and Demberg, 2023), supported by datasets like TalkSumm (Lev et al., 2019) for academic video transcripts, SumSurvey (Liu et al., 2024b) for survey papers, ACLSum (Takeshita et al., 2024) for ACL discourse, and SciNews (Liu et al., 2024a) for simplifying research for broader audiences. M\u00b3AV (Chen et al., 2024c) supports tasks like ASR, TTS, and slide-script generation. Methods like HAESum (Zhao et al., 2024) and SAPGraph (Qi et al., 2022) improve discourse and structural summarization, while CiteSum (Mao et al., 2022) and SSR (Fatima and Strube, 2023) focus on scalability and audience-specific customization. Despite these efforts, scientific summarization remains a challenging domain due to the inherent complexity and diversity of scholarly texts.\nPlan-based Summarization employs structured representations to improve summary quality and reduce hallucinations (Narayan et al., 2021; Amplayo et al., 2021; Wang et al., 2022; Narayan et al., 2023). Research focuses on text-only planning with elements like entities (Narayan et al., 2021; Liu and Chen, 2021; Huot et al., 2024), keywords prompts (Creo et al., 2023), and question-answer pairs (Narayan et al., 2023). Examples include PlanVerb (Canal et al., 2022), which converts task plans into natural language via semantic tagging, and domain-specific approaches in dialogue summarization that align with knowledge structures for improved quality (Srivastava et al., 2024). Blueprint-based frameworks utilize intermediate plans such as question-answer pairs to create coherent narratives for visual storytelling (Liu et al., 2023). However, plan-based strategies for multimodal tasks, particularly video-to-text summarization, have received limited attention."}, {"title": "3 VISTA Dataset", "content": "Data Acquisition and Cleaning VISTA is derived from computational linguistics and machine learning conferences, including ACL Anthology (ACL, EMNLP, NAACL, EACL, Findings of *ACL), ICML, and NeurIPS, covering content from 2020 to 2024. All materials (paper abstracts and video recordings) are contributed by the respective paper authors, ensuring narrative consistency. Since these metadata are stored in XML/JSON files on their respective websites, no further preprocessing (e.g. extracting abstracts from PDFs) is required. We collect paper titles, author lists, paper abstracts, links to papers, and presentation videos, in accordance with platform terms for academic research purposes (or obtain written confirmation).\nTo maintain one-to-one video-to-text alignments, we exclude samples that may cover multiple papers (e.g., tutorials, invited talks) and videos shorter than one minute or longer than 30 minutes.\nQuality Control The data are sourced directly from official proceedings websites, including textual summaries and presentation videos authored/recorded by corresponding researchers, eliminating the need for additional annotations. We verify the data quality through both human and automated checks.\nData Splits After quality control, our dataset comprises 18,599 samples"}, {"title": "4 Benchmarking VISTA", "content": "Task Overview We formalize the task of summarizing recorded scientific videos as follows: Let $v$ and $s$ denote a video (or its transcript/audio) and its paired summary from dataset $D = \\{(v_1, s_1), (v_2, s_2), ..., (v_n, s_n)\\}$, where $n$ signifies the number of video-abstract pairs. The objective is to train a (multimodal) model $M$ to learn the conditional probability distribution $P(s | v)$. Given a new video, the trained model $M$ is expected to generate an appropriate summary.\nA challenge in video-to-text summarization is structuring the generated summaries in a coherent and faithful manner. Directly learning the mapping from $v$ to $s$ could lead to inadequate outputs, as the model lacks explicit guidance on how to organize and present the extracted information. Scientific abstracts often follow a relatively well-defined structure, making them suitable for a more structured generation approach. We follow previous work in adopting a plan-based framework that introduces an intermediate representation to capture latent structure more effectively than simpler end-to-end approaches. Specifically, given input video $v$, we first generate plan $p$, which consists of a sequence of automatically generated questions $\\{q_1, q_2,..., q_m\\}$, each corresponding to a sentence to be verbalized in the summary. The plan explicitly controls the structure of the summary as a whole and the content of each of its sentences (which are meant to answer the questions in the plan). The model is then trained to learn the extended conditional probability distribution $P(s | v, p)$, ensuring that the generated summaries follow the structure and flow of plan $p$."}, {"title": "Plan Generation", "content": "We hypothesize that summary sentences can be viewed as responses to plan questions directly associated with them. This idea is inspired by the theory of Questions Under Discussion (QUD), which posits that discourse often revolves around a set of questions that guide the structure and interpretation of the conversation.\nWe leverage GPT-01 to generate silver-standard plans based on reference summary sentences and their preceding context. As shown in Figure 4, for example, question q3 is generated based on target sentence t3 and the summary sentences preceding it (i.e., t1 and t2), and so on. As a result, the question sequence preserves the order of sentences in the reference summaries, ensuring that the plan maintains a natural and coherent flow consistent with the structure of reference summaries.\nSummarization Model We train two independent models corresponding to Plan Generation (PG) and Summary Generation (SG). The PG module is trained on pairs of (v,p) samples, where v represents the input and p is the silver-standard plan. The SG module is trained on tuples ([v; p], s), where [v; p] is the concatenation of the input v and its plan p. During inference, the trained PG module predicts plan p\u0302 for input v, and the tuple [v; p\u0302] is fed into the SG module to generate the final summary. Both modules have the same backbone but are trained independently."}, {"title": "5 Experiments", "content": "Baseline Models We benchmark our dataset using three settings: zero-shot learning, QLoRA fine-tuning and full-parameter fine-tuning. For zero-shot, we test closed-source multimodal models, including GPT-01, Gemini 2.0, Claude 3.5 Sonnet as well as open-source video LMMs such as Video-LLaMA, Video-ChatGPT, Video-LLaVA, LLAMA-VID, LLaVA-NeXT-Interleave, and mPLUG-Owl3. These open-source video LMMs process videos by extracting multimodal features, such as visual and/or audio components, using cross-modal attention mechanisms to align and integrate information across modalities.\nWe also assess LLaMA-3.1 and Qwen2-Audio to examine if text- or audio-based models can accomplish the summarization task without taking video information into account. For LLAMA-3.1, we explore two variants: in LLAMA-3.1transcript, we extract audio from video files using moviepy and transcribe it with OpenAI's Whisper-1 to generate text input for the model. In LLAMA-3.1OCR, we apply EasyOCR to extract on-screen text from video frames and use the OCR-generated text as input for summarization. Similarly, for Qwen2-Audio, we use moviepy to convert video files into audio and treat the audio as input."}, {"title": "6 Results and Analysis", "content": "General Results Table 3 illustrates the performance differences between closed-source and open-source models. In the zero-shot setting, closed-source models generally outperform their open-source counterparts. Among open-source models, mPLUG-Owl3 stands out, particularly in semantic alignment (BERTScore) and video-text consistency (VideoScore). Fine-tuning on in-domain data yields noticeable improvements for open-source models with both QLoRA and full-parameter fine-tuning. QLoRA shows overall lower performance than full parameter fine-tuning.\nPlan-mPlug-Owl3 is the plan-based approach built on mPLUG-Owl3, outperforming all open-source baselines in both zero-shot and fine-tuned settings. For zero-shot inference, the Plan-mPlug-Owl3 variant, which fine-tunes only the Plan Generation (PG) module, surpasses other models in summary quality, factual consistency, and semantic alignment. With full-parameter fine-tuning, Plan-mPlug-Owl3 achieves the highest overall scores across models, showing improvements in factual accuracy (+3.47 in FactVC) and quality (+0.34 in RLsum) compared to mPLUG-Ow13. However, all models (including the plan-based method) exhibit hallucinations (FactVC) and alignment (VideoScore) issues, and there are still significant differences (p-value of the paired t-test is less than 0.05) between the human performance in this task, with reference abstracts scoring 88.54 on FactVC and 4.62 on VideoScore.\nImpact of Plan Generation Strategy We analyze the plan generation ablation strategy by comparing it with simpler baselines: Lead-3Q, Tail-3Q, and Random-3Q. In these ablation baselines, plans are generated by selecting the first three, last three, or three randomly chosen summary sentences, respectively. Each selected sentence serves as a target for generating a question, with its preceding sentences providing the context. For instance, in the Lead-3Q setting, the first sentence is used as the target (without any preceding context), prompting the first question in the plan, while subsequent sentences incorporate earlier ones as context. Additionally, we compare the case where QUD is not considered. That is, we directly let GPT-01 generate all planning questions at once only based on the reference summary (NoQUD).\nImpact of Plan Quality We assess how the quality of the plan questions affects model performance. We applied GPT-01 as a question generator in a zero-shot setting in our previous experiments. For comparative analysis, we additionally incorporate Llama-3.1 and a state-of-the-art question generation algorithm (RAST) from Gou et al. (2023) to generate the plan questions. In addition, we apply a Random Replacement (RR) method, where questions generated by GPT-01 are randomly replaced with irrelevant ones. The number of replaced questions per summary ranges from one to the entire set. We also introduce full random replacement (FRR), where questions generated by GPT-01 are all replaced with randomly irrelevant questions."}, {"title": "7 Human Evaluation", "content": "We conduct a human evaluation on 50 randomly selected instances from the VISTA test set. Annotators include master's and doctoral students in computer science or computational linguistics with advanced English proficiency. They receive compensation per our university's standard rate and are blind to the source of each summary to ensure impartial assessment. We compare Plan-mPlug-Owl3, mPLUG-Ow13, LLAVA-NeXT-Interleave, and GPT-01 against human reference summaries/abstracts. Three independent annotators are asked to review the source video and evaluate corresponding model summaries (and the human upper bound) on a 1\u20135 Likert scale for Faithfulness, Relevance, Informativeness, Conciseness, and Coherence (higher scores indicate better quality). They are also asked to provide an overall ranking. In total, participants rated 750 samples (50 \u00d7 5 \u00d7 3).\nFigure 6 presents the performance of each model, along with the proportion of instances where models are rated best or worst. Fleiss' Kappa scores for Faithfulness (\u03ba = 0.767), Relevance (\u03ba = 0.842), Informativeness (\u03ba = 0.721), Conciseness, and Coherence (\u03ba = 0.813) indicate a substantial level of agreement, with an average agreement score of \u03ba = 0.787. Overall, human-written summaries outperform all neural summarization models in quality, as they are perceived as substantially more faithful, coherent, concise, and informative. Human-written summaries are 81.7% more likely to be rated as best compared to model-generated summaries.\nAmong the four neural models, GPT-01 performs worst, being rated as worst 63.2% of the time. LLAVA-NeXT-Interleave follows suit, with a 17.8% chance of receiving the worst ranking. The plan-based model, Plan-mPLUG-Ow13, outperforms mPLUG-Owl3 and demonstrates superior performance across all metrics. Paired t-tests show that human answers are considered significantly better than all neural models in all metrics (p < 0.05), revealing a clear gap between automatic systems and human performance on the VISTA dataset. The plan-based method is significantly better (p < 0.05) than other neural models in faithfulness, coherence, and informativeness, although it falls short of human performance."}, {"title": "8 Conclusion", "content": "This paper introduces VISTA, a dataset for summarizing scientific video presentations into concise textual summaries. Comprehensive evaluations across multiple large models demonstrate that the summarization task is challenging, relying on the interplay of multiple modalities (video, text, and audio). We further introduce a plan-based approach, which yields improvements in summary quality and factual accuracy. Beyond dataset creation, our work also confirms that current leading large models still exhibit a noticeable gap compared to human performance."}, {"title": "Ethical Considerations", "content": "All data in our dataset are sourced from publicly accessible resources, strictly adhering to relevant copyright regulations. Each data sample explicitly includes the corresponding source URL and author attribution. Throughout the processes of data processing, experimental analysis, model training, and evaluation, no instances of privacy infringement were identified. In human evaluations, all participants volunteered willingly and were fairly compensated. We provided a safe and comfortable environment for our participants and complied with ACL's Policy on Publication Ethics throughout our studies."}, {"title": "Limitations", "content": "Data All the summary and video data used in this study are open source. While our sources are generally of high quality and exhibit a broad range of diversity, we have not investigated inherent biases in the data. Moreover, as these data represent only a small fraction of real-world data, our findings may not extend to all video-to-text summarization scenarios. In addition, our dataset is restricted to English, which limits its generalizability to other languages.\nTask In our task, we consider the paper abstract as the summary of the corresponding video. This hypothesis has been supported by our two-stage quality control process, which ensures a strong alignment. However, we acknowledge that there may be nuanced differences between the abstract and a textual summary derived solely from the video. That said, authors often present the abstract as a summary of the video, as it conveys the key contributions, objectives, and findings of the research, which are typically central to the content discussed.\nModel We use several state-of-the-art large models in our experiments and select the best-performing model, mPLUG-Ow13, to demonstrate the effectiveness of the planning strategy. These large models may carry biases introduced during pretraining. We have not assessed the extent of these biases, as they lie beyond the scope of this study. Furthermore, we have not tested the plan-based approach on all model variants presented in our experiments (e.g., text-based large models and audio-based large models). Our work does not aim to prove that the plan-based method is effective in all models of different modalities, but rather to demonstrate that the plan-based method can improve the performance of video-based models on our dataset.\nModality In our experiments, we explore the performance of individual modalities on the downstream summarization task (e.g., text-to-text model and audio-to-text model). However, we do not conduct an in-depth analysis of how different modality combinations impact the final summarization results.\nData Contamination and Prompt Selection It is worth noting that we have not found evidence in the original papers describing the open-source models we use to suggest that the contents of the VISTA dataset are included in their pretraining stage.\nScope Our study focuses on video-to-text summarization within scientific domains."}, {"title": "Automated Evaluation", "content": "While we employ a suite of automated metrics and hallucination detection methods to assess model performance on the test set, these metrics have inherent limitations and may fail to capture all aspects of model quality."}, {"title": "Human Evaluation", "content": "Similar to many earlier studies, we only evaluate 50 video-summary pairs, a subset that may not represent the entire dataset. Additionally, while all evaluators are graduate students, they are not necessarily experts in video-to-text summarization and possess varying levels of reading and assessment skills."}, {"title": "LMM-as-Judge Evaluation", "content": "Although the LMM-based judge paradigm (GPT-01) enables large-scale and relatively consistent evaluations, it may inherit biases from its pretraining data, and its black-box nature makes the rating process difficult to interpret. Data contamination also remains a concern if GPT-01 is trained on overlapping data. Results should be interpreted with caution and supplemented by human judgment where possible."}, {"title": "Case Study and Error Analysis", "content": "For our case study, we randomly select a sample from the test split. The analysis reveals differences in summary quality across models, and against the human-written text. Specifically, GPT-01 often produces concise summaries but at the cost of precision. For example, it incorrectly claims that \u201cdata splitting helps control test thresholds,\u201d which is a hallucination. In general model-generated outputs consistently struggle with informativeness, coherence, and factual accuracy."}]}