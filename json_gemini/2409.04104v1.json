{"title": "MixNet: Joining Force of Classical and Modern Approaches toward The Comprehensive Pipeline in Motor Imagery EEG Classification", "authors": ["Phairot Autthasan", "Rattanaphon Chaisaen", "Huy Phan", "Maarten De Vos", "Theerawit Wilaiprasitporn"], "abstract": "Recent advances in deep learning (DL) have significantly impacted motor imagery (MI)-based brain-computer interface (BCI) systems, enhancing the decoding of electroencephalography (EEG) signals. However, most studies struggle to identify discriminative patterns across subjects during MI tasks, limiting MI classification performance. In this paper, we propose MixNet, a novel classification framework designed to overcome this limitation by utilizing spectral-spatial signals from MI data, along with a multi-task learning architecture named MIN2Net, for classification. Here, the spectral-spatial signals are generated using the filter-bank common spatial patterns (FBCSP) method on MI data. Since the multi-task learning architecture is used for the classification task, the learning in each task may exhibit different generalization rates and potential overfitting across tasks. To address this issue, we implement adaptive gradient blending, simultaneously regulating multiple loss weights and adjusting the learning pace for each task based on its generalization/overfitting tendencies. Experimental results on six benchmark datasets of different data sizes demonstrate that MixNet consistently outperforms all state-of-the-art algorithms in subject-dependent and -independent settings. Finally, the low-density EEG-MI classification results show MixNet's superiority over state-of-the-art algorithms, offering promising implications for Internet of Thing (IoT) applications such as lightweight and portable EEG wearable devices based on low-density montages.", "sections": [{"title": "I. INTRODUCTION", "content": "Brain-computer interfaces (BCIs) are a transformative technology that establishes a direct communication channel between the human brain and external devices [1]. Among the numerous neural acquisition techniques used in BCIs, electroencephalography (EEG) has attracted considerable interest due to its non-invasiveness, affordability, and high temporal resolution [2], [3]. Motor imagery (MI)-based BCI systems are increasingly popular in BCI paradigms due to their advantage of not requiring external stimuli [4], [5]. Within such an MI-based BCI system, participants are instructed to mentally imagine the movement of different body parts, evoking neural activity in specific cerebral regions associated with those movements. Throughout the MI process, the co-occurrence of event-related desynchronization (ERD) and event-related synchronization (ERS) patterns in EEG signals reflects event-related modulations in brain activity within particular frequency bands, such as mu (9\u201313 Hz) and beta (22-29 Hz), over the motor cortex region [6], [7]. The ERD/ERS patterns generated by imagined movements are matched with actual movements and then decoded to determine the user's intent. Furthermore, the output signal can be used to control external devices [8].\nNevertheless, MI-based BCI systems encounter notable challenges. Firstly, the EEG signals have a low signal-to-noise ratio (SNR), making them highly sensitive to noise interference [9], [10]. Another limitation is the considerable variability observed in EEG signals across different subjects, posing a substantial challenge to accurately decoding these oscillatory neural activities. Recently, researchers have developed calibration-free or subject-independent methods to tackle the issue of EEG data variability across subjects [11], [12]. These methods are trained and tested using data from different groups of subjects, enabling new users to utilize the BCI system without needing a calibration phase. For the MI-based BCI system to help capture and learn generalized MI-EEG features across subjects, it is essential to develop powerful feature extraction and classification algorithms. Deep learning (DL) has recently emerged as an up-and-coming technique and demonstrated remarkable success in numerous fields, including computer vision, bio-signals, speech recognition, and natural language processing [13]. In contrast to conventional methods, which rely on manually hand-crafted features, DL models can learn complex patterns directly from multi-dimensional data. This capability has attracted numerous researchers' interest in BCIS, resulting in the development of advanced DL architectures that have demonstrated significant improvements in EEG-based motor imagery (MI) classification [14].\nConvolutional neural networks (CNNs) have been extensively applied in EEG-MI classification due to their ability to learn temporal and spatial features from EEG signals effectively [15]-[17]. By leveraging the power of 2D-CNNs, these models can capture local and global connectivity patterns within EEG data. The combination of CNNs and long short-term memory units (LSTMs) has been extensively employed to capture long-term dependencies in temporal features [18], [19]. This fusion permits extracting relevant temporal and spatial features, enhancing the overall performance of EEG-MI classification. Despite the notable successes of existing deep learning approaches in decoding EEG signals in various MI datasets, their performance is limited to the subject-dependent task, where the methods are trained and tested using data from the same subject. In other words, these models have difficulty adapting and performing well when evaluated using data from unseen users. This limitation hinders their applicability and potential real-world deployment in BCI systems.\nIn recent years, our previous work [12] introduced a deep learning architecture known as MIN2Net, explicitly developed to capture generalized MI-EEG features across subjects in the subject-independent task. This architecture demonstrated acceptable performance in subject-independent learning over a large-scale dataset. However, its effectiveness was compromised when attempting to recognize generalized EEG-MI features with limited training data, such as using a small data size and training data from only a single subject in subject-dependent learning. Another drawback is incorporating multi-task learning into MIN2Net (e.g., unsupervised, supervised, and deep metric learning) for EEG-MI classification proved challenging, given the differing generalization rates and potential overfitting across tasks. MIN2Net was the reliance on an exhaustive parameter search to identify the optimal set of loss weights. This time-consuming process yielded varying optimal weight configurations when applied to different datasets.\nThis paper proposes MixNet, a novel framework that builds upon the foundational principles of MIN2Net, addressing limitations in network learning with small data sizes and time-consuming loss weights optimization. MixNet utilizes filter-bank common spatial patterns (FBCSP) for data pre-processing, extracting discriminative patterns from EEG-MI data. This results in spatially filtered or spectral-spatial signals, preserving meaningful information across EEG classes. We adjusted and adopted the adaptive gradient blending concept, previously applied in sleep staging tasks by Huy et al. [20], which is incorporated to regulate multiple loss weights concurrently. This approach alleviates the need for manual parameter tuning, leading to a more efficient and effective optimization process. The proposed method achieves remarkable performance in subject-dependent and subject-independent MI classification tasks by leveraging the spectral-spatial signals and employing an adaptive gradient blending approach for loss weight optimization.\nAdditionally, MixNet outperforms state-of-the-art algorithms in handling MI classification on low-density EEG-MI signals. As the MixNet developed for improving low- and high-density EEG classification, this advancement has promising implications for Internet of Thing (IoT) applications such as the EEG classification on EEG signals that are from lightweight and portable EEG wearable devices based on low-density montages; most of these devices are more comfortable for users, reducing the effort to set up and also expanding their usage in the real world, including stroke identification [21] and robot control [22].\nThe remainder of this paper is organized as follows. Section II presents an overview of related work on MI classification topics. Section III details the pre-processing steps applied to EEG data and outlines the architecture of the proposed method. Section IV presents the experimental results, indicating the performance of the proposed method. Section V provides a comprehensive discussion of the experimental results and their implications. Finally, the main findings and potential avenues for future research are summarized in Section VI."}, {"title": "II. RELATED WORK", "content": "This section thoroughly reviews the evolution and progress in EEG-based MI classification. Subsequently, we address the existing limitations in the current research on MI-based BCI. Furthermore, we provide a detailed exposition of the concepts of autoencoders (AE) and deep metric learning (DML), both of which bear relevance to our study. Lastly, we outline the rationale behind undertaking this research endeavor."}, {"title": "A. The Progressive Development of MI based on BCI", "content": "The progressive evolution of machine learning has inspired BCI researchers to propose more advanced intelligent algorithms employing subject-dependent paradigms to improve motor imagery (MI) decoding performance based on EEG signals. Common spatial pattern (CSP) is one of the most prominent approaches widely used for feature extraction in MI-based BCI [23]. CSP efficiently extracts discriminative features by maximizing the variance differences between the different classes of EEG signals. Moreover, several methods are built based on extensions of CSP, such as common spatiospectral pattern (CSSP) [24], filter bank common spatial pattern (FBCSP) [25], and bayesian spatio-spectral filter optimization (BSSFO) [26].\nAmong advanced CSP algorithms, FBCSP is the most widely used, which extends the approach by incorporating multiple frequency bands instead of being limited to a specific band. It has demonstrated state-of-the-art performance in EEG-based MI classification as seen in [27]. Upon processing EEG signals through FBCSP, meaningful brain features are extracted. Subsequently, a feature selection method such as mutual information-based best individual feature (MIBIF) [25] is employed to identify the most discriminative features. In the realm of classification, numerous conventional algorithms, such as support vector machine (SVM) and linear discriminant analysis (LDA), have been extensively utilized to decode the extracted features, as seen in [11], [27]. Furthermore, several algorithms have extended the CSP paradigm, promising performance improvements [28]\u2013[30]. Despite the significant achievements of existing CSP and its modified algorithms in EEG decoding for various Motor Imagery (MI) datasets, it is evident that their efficacy is primarily restricted to the subject-specific task. Regrettably, their effectiveness in the subject-independent task is a topic of ongoing research and requires additional refinement to improve their performance and generalizability."}, {"title": "B. Deep Metric Learning in MI based on BCI", "content": "Deep metric learning (DML) is an advanced and effective technique based on the fundamental concept of distance metrics [33]. Its primary objective is to acquire highly informative data representations that enable precise data similarity measurement. This is accomplished using embedded features obtained from a metric learning network. Classical similarity metric functions, such as Euclidean distance, Mahalanobis distance, and Cosine distance, are explicitly engaged as the distance metric between pairs of data points within the DML framework.\nIn recent years, there has been significant advancement in developing several loss functions designed explicitly for DML. These loss functions are created with the primary objective of improving the discrimination of features. Several loss functions have been identified as significant in this context, including contrastive loss [34], triplet loss [35], quadruplet loss [36], and multi-similarity loss [37]. By adeptly utilizing these loss functions, similarity measures are computed on correlated samples, compelling samples belonging to the same class to converge while concurrently inducing samples from distinct classes to diverge. One notable differentiation between DML losses and traditional loss functions, such as cross-entropy loss, is their dependency on contrastive pairs, triplets, or quadruplets of data for gradient determination.\nMore recently, DML has been extensively utilized in EEG-BCI research, providing novel opportunities for decoding human brain activity more precisely and efficiently [12], [38]\u2013[40]. Specifically, integrating the DML technique into advanced deep learning architectures, such as autoencoders and convolutional neural networks (CNNs), enables the model to automatically learn and extract meaningful features from high-dimensional embeddings and optimize the entire system for the task of capturing similarities and differences between EEG classes. The combination of deep metric learning in multi-task autoencoder-based CNN architectures indicates great promise for advancing EEG-BCI technology and has demonstrated promising results in addressing the long-standing problem of inter-subject variability, as seen in our previous work [12]."}, {"title": "C. Autoencoders in MI based on BCI", "content": "The Autoencoder (AE) concept, a widely recognized component in unsupervised learning algorithms, was first introduced in 1986 [41]. Autoencoders (AEs) have been widely used and have demonstrated their versatility in various domains, such as data compression, denoising, dimensionality reduction, and feature extraction [42]\u2013[44]. The AE network architecture can efficiently extract crucial features from labeled and unlabeled input data, forming a latent representation. The latent representation plays a fundamental role in the later reconstruction of the original input data. The training objective for this network architecture is centered on reducing the reconstruction loss corresponding to the input data. Moreover, significant research findings suggest that the efficacy of the obtained latent representation is much enhanced when the reconstructed data closely mirrors the prominent features of the original input data.\nOne such advanced AE architecture is the denoising sparse autoencoder (DSAE) [45], which is proposed to improve EEG-based epileptic seizure detection. The main objective of DSAE is to improve the accuracy of epileptic seizure detection by efficiently reconstructing the original EEG signals from corrupted inputs. The sparsity constraint integrated into the DSAE is crucial in achieving this efficiency, offering significant potential for more reliable and precise results. Another area where AE-based methods have shown remarkable potential is in handling biopotentials and telemonitoring systems. A technique based on compressed sensing (CS) and AE [46] has demonstrated outstanding results in data compression and accurate classification of electrocardiogram (ECG) and EEG signals.\nIn more recent papers, the utilization of DL-based AE architectures has grown in popularity. In the works of Parashiva et al. [47] and Mammone et al. [48], researchers have shown this trend through integrating AEs with FBCSP technique. These studies have proven that combining AEs and FBCSP can extract crucial features, resulting in excellent EEG-based MI classification results. One limitation of these approaches is that they do not offer end-to-end learning paradigms. Even though latent representations were successfully compressed during the encoding-decoding process, further classification is still required to accurately distinguish and classify various classes or types of EEG patterns.\nIn our previous work [12], we introduced MIN2Net, a novel end-to-end neural network architecture based on a multi-task autoencoder. MIN2Net simultaneously learns deep features from unsupervised EEG-MI reconstruction and supervised EEG-MI classification. It demonstrated excellent performance in extracting discriminative features for MI decoding. However, the model cannot extract discriminative patterns from raw EEG signals when the number of training samples is relatively scarce. Therefore, from all angles, this study aims to develop an enhanced version of MIN2Net capable of achieving high performance regardless of the size of the training dataset."}, {"title": "III. METHODS", "content": "The proposed method, along with all baseline methods, was assessed using various datasets, including BCIC IV 2a [49], BCIC IV 2b [50], BNCI2015-001 [51], SMR-BCI [52], High-Gamma [15], and OpenBMI [53] datasets. The BCIC IV 2a, BCIC IV 2b, BNCI2015-001, and SMR-BCI datasets are widely recognized as benchmark datasets for MI classification, originating from Graz University of Technology. The High-Gamma dataset, which has been made available by the University of Freiburg, is recognized as the second largest MI dataset. Lastly, the evaluation also included the OpenBMI dataset, which is the largest public MI dataset to date, provided by Korea University.\nIn the high-density EEG motor imagery (EEG-MI) classification task, we considered all EEG channels from the BCIC IV 2a, BNCI2015-001, and SMR-BCI datasets, as these datasets directly provide comprehensive EEG channels associated with the motor cortex area. Following previous studies [11], [12], we selected 20 EEG channels in the motor cortex region from the High-Gamma and OpenBMI datasets for our analysis. We utilized the BCIC IV 2b dataset for the low-density EEG-MI classification task, which provides EEG data from 3 channels (C3, Cz, and C4) placed over the motor cortex region. Further details about these databases are provided in Table I.\nAdditionally, all EEG data in the considered datasets were downsampled to a sampling frequency of 100 Hz, and the MI period for all datasets was selected as the time interval between 0 s and 4 s after stimulus onset. The EEG signals utilized in this study are time-domain signals with a duration of four seconds."}, {"title": "A. Databases", "content": null}, {"title": "B. Generation of Spectral-spatial Signals", "content": "In this study, the raw EEG signals are time-domain signals that change over time with 4 seconds long. Formally, we denote \\(X_i \\in \\mathbb{R}^{N_e \\times t}\\) as a single-trial EEG data from the \\(i\\)th trial and let \\(y_i \\in \\{1, 2, ..., N_{class}\\}\\) denote its corresponding label, where \\(N_e\\) is the number of channels, \\(t\\) is the number of sampled time points, and \\(N_{class}\\) is the number of classes.\nUsing a fifth-order Butterworth bandpass filter, a filter bank was used to decompose the single-trial EEG data \\(X\\) into multiple frequency bands. Here, the filter bank is defined as \\(B = \\{b_k\\}_{k=1}^{N_b} \\in \\{\\[4,8], \\[8, 12], ..., \\[36,40]\\}\\), where \\(N_b\\) is the number of frequency bands, is predefined from 4 to 40 Hz. Each bandpass filter has a range of 4 Hz. While some variations in the filter bank configurations are effective, these specific bandpass frequency ranges are used due to their ability to provide stable frequency response, spanning the range of 4\u201340 Hz, as observed in the work by [27]. Filtered EEG signals will be denoted as \\(E_{b_k,i} \\in \\mathbb{R}^{N_c \\times t}\\).\nAfter filtering the EEG signals by the filter bank, the CSP algorithm was used for spatial filtering [25], [27]. The CSP algorithm is well-known for its efficacy in estimating and applying a projection matrix to transform EEG signals linearly, maximizing the variance of signals belonging to one class and minimizing the variance of signals belonging to the other. The spatial filtering procedure involves linearly transforming EEG data using the CSP algorithm as follows:\n\\(Z_{b_k,i} = W_{b_k}E_{b_k,i}\\)   (1)\nWhere \\(Z_{b_k,i} \\in \\mathbb{R}^{N_e \\times t}\\) denotes \\(E_{b_k,i}\\) after spatial filtering, \\(W_{b_k} \\in \\mathbb{R}^{N_c \\times N_c}\\) denotes the CSP projection matrix; \\(T\\) denotes the transpose operator.\nThe CSP algorithm computes the transformation matrix \\(W_{b_k}\\) through the solution of the eigenvalue decomposition problem as shown in Equation 2, thereby producing features with optimized variances to discriminate between the two classes of EEG signals effectively:\n\\(E_{b_{k,1}}W_{b_k} = (\\Sigma_{b_{k,1}} + \\Sigma_{b_{k,2}})W_{b_k}D_{b_k},\\)   (2)\nWhere \\(E_{b_{k,1}}\\) and \\(E_{b_{k,2}}\\) are the estimation of the covariance matrices of the filtered EEG signals for motor imagery class 1 and class 2, respectively. \\(D_{b_k}\\) is the diagonal matrix that contains the eigenvalues of \\(E_{b_{k,1}}\\). By considering all the filtered EEG trials \\(N_i\\), the covariance matrix for each class of the \\(b_k\\) filtered EEGs is then given by:\n\\(\\Sigma_{b_k}^{y} = \\frac{1}{N_i} \\sum_{i=1}^{N_i} \\frac{E_{b_k,i}^{(y)} E_{b_k,i}^{(y)T}}{tr(E_{b_k,i}^{(y)} E_{b_k,i}^{(y)T})}\\)  (3)\nThe spatially filtered EEG signal or spectral-spatial EEG signal \\(Z_{b_k,i}\\) in Equation 1 using first and last 2 columns of spatial filter \\(W_{b_k}\\) from Equation 2 can maximize the differences in the variance of the two classes of filtered EEG signals, thus denoted as \\(W_{bh}\\). For example, the number of spatial filters U = 4 means that the two largest and the two smallest eigenvalues of the spatial filter \\(W_{b_k}\\) are selected. The spectral-spatial signals are finally computed as follows:\n\\(\\tilde{Z}_{b_k,i} = W_{b_k}^T E_{b_k,i}\\)  (4)\nWhere \\(\\tilde{Z}_{b_k,i} \\in \\mathbb{R}^{U \\times t}\\) and \\(W_{b_k} \\in \\mathbb{R}^{N_c \\times U}\\)."}, {"title": "C. Classification With MixNet", "content": "In this study, the proposed method builds upon the the idea of our previous paper presented MIN2Net and still keeps the network architecture the same as the original MIN2Net. MixNet consists of three main modules: autoencoder, deep metric learning, and supervised learning, and its architecture is optimized by minimizing three different loss functions simultaneously: reconstruction, cross-entropy, and triplet loss functions.\n1) Autoencoder: The autoencoder module, which is a part of the MixNet framework, has two main components: the encoder, expressed as \\(z = q(x)\\), and the decoder, represented as \\(\\hat{x} = p(z)\\). The input signal \\(x\\) is subjected to encoding within the encoder component, transforming \\(x\\) into a latent vector \\(z\\) via dimensionality reduction. In contrast, the decoder module receives a given latent vector \\(z\\) and continues to decode it, resulting in the reconstruction of the input signal \\(\\hat{x}\\). The primary objective of the AE module is to learn how to map data into a latent space that retains pertinent information for reconstructing the data. By utilizing spectral-spatial EEG signals from FBCSP as inputs for the AE module in this study, the AE can effectively recognize instances and preserve instances' discriminative patterns corresponding to different classes.\nThe encoder module consists of two separate CNN blocks. Each block is built of a Conv2D layer, followed by a batch normalization (BN) layer, an exponential linear unit (ELU), and an average pooling layer (AveragePooling2D). The final output of the last CNN block serves as the input to a fully connected (FC) layer. This FC layer aids in the process of mapping the latent representation. The decoder module has a symmetrical construction to that of the encoder module. The latent vector is passed via an FC layer to preserve consistency in the input dimensions of the CNN blocks. It then proceeds to a reshape layer that arranges the data into a suitable dimension. The decoder module consists of two CNN blocks, each using a Conv2DTranspose layer with a stride value of 4. Subsequently, an ELU layer is used to achieve its intended purpose. More details on the dimensions of the input size at each layer in the architecture of MixNet can be found in Table II.\nThe primary goal of the AE module's training objective is to minimize the difference in reconstruction between the input and the reconstructed output. This study uses the mean square error (MSE) as the objective function for this module, which is given by:\n\\(L_{MSE}(x, \\hat{x}) = \\frac{1}{N_c} \\sum_{j=1}^{N_c} ||x_j - \\hat{x}_j||^2\\).  (5)\nWhere \\(x_j\\) denotes the input signals and \\(\\hat{x}_j\\) is the reconstructed signals of the channel \\(j\\).\n2) Deep Metric Leaning: Deep metric learning (DML) has proven its efficacy in retaining discriminative patterns for the latent representation of the autoencoder (AE), as indicated by our prior research [12]. Hence, this research article employs a triplet loss approach, specifically using a semi-hard triplet constraint as proposed in the work of Schroff et al. [35], inside the DML module. The aim is to maximize the relative distances between distinct classes of latent vectors. During the training procedure, a triplet set comprising three samples, represented as \\(\\{x^a, x^p, x^n\\}\\), is randomly selected from the training dataset. The relationship between the anchor sample \\(x^a\\) and the positive sample \\(x^p\\) is ensured to be larger than the distance between the anchor sample \\(x^a\\) and the negative sample \\(x^n\\). Subsequently, the encoder component processes the set of three input signals concurrently, generating their corresponding latent vectors \\(z^a, z^p, and z^n\\). The loss function can, therefore, be expressed as:\n\\(L_{triplet}(z^a, z^p, z^n) = \\[||z^a - z^p||^2 - ||z^a - z^n||^2 + \\alpha\\]_+ \\)  (6)\nwhere \\(\\[z\\]_+ = max(z,0)\\). The parameter \\(\\alpha\\) regulates as the margin threshold that imposes a constraint on the Euclidean distance \\(||z^a - z^p||^2\\) between positive pairings, ensuring that it is less than the Euclidean distance \\(||z^a - z^n ||^2\\) between negative pairs. Determining the optimal value for the triplet loss margin is crucial to training the DML module.\n3) Supervised Learning: The supervised learning module employs a conventional soft-max classifier to classify the latent representations of the input spectral-spatial EEG signals. The latent representation \\(z\\) is inputted into the fully connected (FC) layer using the softmax activation function to calculate the importance weight for each class. This can be represented as follows:\n\\(\\hat{y}(z) = softmax(Wz + b)\\) (7)\nWhere \\(W\\) and \\(b\\) are the weight matrix and the bias vector, respectively. Then, the cross-entropy (CE) loss is used to measure the difference between the predicted value and true label, given by:\n\\(L_{CE}(y, \\hat{y}) = - \\frac{1}{N_i} \\sum_{i=1}^{N_i} y_i log \\hat{y}_i\\) (8)\nwhere \\(y\\) represents the true label, \\(\\hat{y}\\) represents the classification probabilities, and \\(N_i\\) represents the total number of input signals. The class with the highest classification probability is the predicted class of the single-trial EEG signal."}, {"title": "D. Training Procedure for MixNet Using Adaptive Gradient Blending", "content": "The training objective of the proposed method is optimized through the combination of the three loss functions: \\(L_{MSE}\\) in Equation 5, \\(L_{triplet}\\) in Equation 6, and \\(L_{CE}\\) in Equation 8. The final loss function of MixNet \\(L_{MixNet}\\) is represented as:\n\\(L_{MixNet}(x, \\hat{x}, z^a, z^p, z^n, y, \\hat{y}) = \\frac{1}{N_i} \\sum_{i=1}^{N_i} \\{w^{(1)} L_{MSE}(x_i, \\hat{x}_i) + w^{(2)} L_{triplet}(z^a, z^p, z^n) + w^{(3)} L_{CE}(y_i, \\hat{y}_i)\\}\\).  (9)\nWhere \\(N_i\\) denotes the total number of input signals, \\(w^{(1)}\\), \\(w^{(2)}\\), and \\(w^{(3)}\\) represent the hyperparameters to weigh the contribution of each loss function. As a result of integrating the three loss functions, both the unsupervised and the DML can influence the learning process when supervised learning occurs.\nManually tuning these weights \\(\\{w^{(1)}, w^{(2)}, w^{(3)}\\}\\) is a problematic and expensive operation, making multi-tasking learning impractical. Inspired by the adaptive gradient blending concept, previously demonstrated in work by Huy et al. [20], it has shown great success in regulating multiple loss weights simultaneously for joint multi-view learning. This study presents a systematic solution for multi-task learning, where the weighting of multiple loss functions is determined based on the adaptive gradient blending of each task. The concept of multi-task learning is designed to tackle the challenge of optimizing a model when faced with multiple objectives, which is a significant problem in many deep learning architectures. Acquiring access to the gradient flows of network streams becomes crucial for effectively regulating the learning pace. MixNet has three main learning tasks: autoencoder, deep metric learning, and supervised learning, optimizing three objective functions simultaneously. Conducting a thorough investigation of the tendencies towards generalization and overfitting in the learning tasks allows us to provide suitable weights to the gradient flows of these tasks. This means providing a higher weight to the task that exhibits excellent generalization while a lower weight to the task with tendencies towards overfitting. In this approach, the gradients are blended in a way that considers the overall behavior of multi-task learning in terms of generalization and overfitting. Additionally, the learning progress of the network tasks is adjusted for each individual task.\nThis paper applies adaptive loss weighting to regulate the learning process across the three learning tasks to reach a balance in the generalization and overfitting rates inside the multi-task learning of MixNet. We determine the loss weights using a mechanism similar to that described in reference [20] based on the ratio of generalization and overfitting metrics (a theoretical justification is provided in the supplementary material), as can be seen below:\n\\(w^{(m)} = \\frac{\\frac{1}{Z} G_m}{\\Omega}\\)  (10)\nwhere \\(m \\in \\{1,2,3\\}\\) and \\(Z\\) is a normalization factor. The generalization metric is the gap between the gain in information learned about the target distribution. We define the overfitting metric as the gap between the gain on the training set and the target distribution. The weighted loss used through the training process at epoch n is then expressed as follows:\n\\(L(n) = \\sum_{m \\in \\{1,2,3\\}} w^{(m)}(n) L^{(m)}(n)\\)  (11)\nThe learning behavior of MixNet can be influenced by the weighted loss mentioned above, relying on how \\(G_m\\) and \\(\\Omega_m\\) are approximated.\nGenerally, in the work [54], \\(G_m\\) and \\(\\Omega_m\\) at the training process at epoch n can be simply approximated as follows:\n\\(G_m(n) \\approx L^{(m)}(n_0) - L^{(m)}(n),\\)\n\\(\\Omega_m(n) \\approx \\Delta L^{(m)}(n) - \\Delta L^{(m)}(n_0)\\)  (12)\n\\(\\sim (L_{train}^{(m)}(n) - L_{val}^{(m)}(n)) - (L_{train}^{(m)}(n_0) - L_{val}^{(m)}(n_0))\\) (13)\nWhere \\(L_{train}^{(m)}\\) and \\(L_{val}^{(m)}\\) are defined as the training loss and true loss, respectively. Epoch n is the training step and \\(n_0\\) is reference step, where epoch \\(n_0 < n\\). Nonetheless, the approximation above approach has two major drawbacks: the loss curves of training and validation are highly frustrated due to minibatch training, and the references \\(L_{train}^{(m)}(0)\\) and \\(L_{val}^{(m)}(0)\\) may vary considerably with various random initializations. Alleviate these drawbacks; the powerful approximation approach is proposed to alleviate these drawbacks based on the tangents of losses over the line fitting of loss curves (training and validation) [20]. This approximation method utilizes two fundamental observations: the smooth yet noisy pattern of loss curves and the informative characteristics of loss tangent directions at particular training iterations to determine the generalization or overfitting tendencies of the network. The approximation approach based on the tangents of losses throughout our experiments where \\(G_m\\) and \\(\\Omega_m\\) at the training process at epoch n can be given by\n\\(G_m(n) \\approx tan \\theta_{val}^{(m)}(n) - tan \\theta_{val}^{(m)}(n_0),\\)\n\\(\\Omega_m(n) \\approx (tan \\theta_{val}^{(m)}(n) - tan \\theta_{train}^{(m)}(n)) - (tan \\theta_{val}^{(m)}(n_0) - tan \\theta_{train}^{(m)}(n_0))\\)  (14)\nWhere \\(tan \\theta_{train}^{(m)}(n)\\) and \\(tan \\theta_{val}^{(m)}(n)\\) are the training and true loss tangents through the training process at epoch n and \\(tan \\theta_{train}^{(m)}(n_0)\\) and \\(tan \\theta_{val}^{(m)}(n_0)\\) are their references, respectively. The relationship between all above variables can be summarized in Figure 2. Moreover, let \\(\\theta_{train}^{(m)}(n)\\), and \\(\\theta_{val}^{(m)}(n)\\), where -90\u00b0 < \\(\\theta_{train}^{(m)}(n)\\) \u2264 0\u00b0 and -90\u00b0 \u2264 \\(\\theta_{val}^{(m)}(n)\\) \u2264 90\u00b0, denotes the angles made by the tangent lines of the training and true loss curves with the horizontal axis, respectively. The network is generalizing when -90\u00b0 < \\(\\theta_{val}^{(m)}(n)\\) < 0\u00b0 (i.e., negative tangent) and overfitting when 0\u00b0 < \\(\\theta_{val}^{(m)}(n)\\) \u2264 90\u00b0 (i.e., positive tangent).\nThe process of determining the adaptive loss weight, denoted as \\(w^{(m)}\\), for a specific network task labeled as m in MixNet, is detailed in Algorithm 1. This involves estimating the loss tangent (\\(tan \\theta_{train}^{(m)}(n)\\) and \\(tan \\theta_{val}^{(m)}(n)\\)) at epoch n. This is done by analyzing the slope of lines fitted to a loss curve segment spanning length \\(l\\) epoch, starting from epoch \\(n-l\\) and extending to epoch n. This results in initiating the network's training with a warm-up period lasting W training epochs, during which the task weights are uniformly set, where \\(l<W\\). Furthermore, for each learning task, the training epoch corresponding to the smallest authentic loss tangent (i.e., the steepest slope of the fitted line) is identified as the reference epoch \\(n_0\\). In the practical context of MixNet, the true loss \\(tan \\theta_{val}^{(m)}(n)\\) remains unknown. Therefore, we approximate it with the loss measured on a held-out validation set \\(tan \\theta_{val}^{(m)}(n)\\)."}, {"title": "E. Network Parameters", "content": "The proposed method was implemented using TensorFlow framework version 2.8.2. An NVIDIA Tesla V100 GPU with 32GB of memory was utilized during the training process. We optimized the loss function in each training iteration using the Adam optimizer, with a learning rate schedule ranging from \\(10^{-3}\\) to \\(10^{-4}\\) throughout our experiments. The learning rate was reduced by a factor of 0.5 when no improvement in validation loss was observed for five consecutive epochs. A batch size of 32 samples was employed for the subject-dependent classification setting, while 100 samples were used for the subject-independent classification setting. The parameter \\(l\\) in Algorithm 1, which defines the window size for line fitting, was established at 1.5 epochs. Additionally, an \\(l\\)-point moving average was applied to the loss curves for smoothing purposes prior to conducting line fitting. Finally, we employed the early stopping strategy to determine the number of training iterations. The training process was halted if there was no reduction in validation loss for 20 consecutive epochs."}, {"title": "F. Baseline Approaches"}]}