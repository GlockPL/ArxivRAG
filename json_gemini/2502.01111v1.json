{"title": "A GENERATIVE FOUNDATION MODEL FOR AN ALL-IN-ONE\nSEISMIC PROCESSING FRAMEWORK", "authors": ["Shijun Cheng", "Randy Harsuko", "Tariq Alkhalifah"], "abstract": "Seismic data often face challenges in their utilization due to noise contamination, incomplete ac-\nquisition, and limited low-frequency information, which hinder accurate subsurface imaging and\ninterpretation. Traditional processing methods rely heavily on task-specific designs to address these\nchallenges and fail to account for the variability of data. To address these limitations, we present a\ngenerative seismic foundation model (GSFM), a unified framework based on generative diffusion\nmodels (GDMs), designed to tackle multi-task seismic processing challenges, including denoising,\nbackscattered noise attenuation, interpolation, and low-frequency extrapolation. GSFM leverages a\npre-training stage on synthetic data to capture the features of clean, complete, and broadband seismic\ndata distributions and applies an iterative fine-tuning strategy to adapt the model to field data. By\nadopting a target-oriented diffusion process prediction, GSFM improves computational efficiency\nwithout compromising accuracy. Synthetic data tests demonstrate GSFM surpasses benchmarks with\nequivalent architectures in all tasks and achieves performance comparable to traditional pre-training\nstrategies, even after their fine-tuning. Also, field data tests suggest that our iterative fine-tuning ap-\nproach addresses the generalization limitations of conventional pre-training and fine-tuning paradigms,\ndelivering significantly enhanced performance across diverse tasks. Furthermore, GSFM's inherent\nprobabilistic nature enables effective uncertainty quantification, offering valuable insights into the\nreliability of processing results.", "sections": [{"title": "1 Introduction", "content": "Seismic processing is an essential step for raw data acquisition to produce high-quality subsurface images [Yilmaz, 2001]. It involves a series of complex and diverse procedures aimed at revealing detailed information about subsurface formations and their physical properties. Due to the highly intricate nature of seismic wave propagation in subsurface media and the interference of acquisition environments, raw acquired data are often degraded by various factors. For example, environmental noise reduces the signal-to-noise ratio, making it challenging to extract valuable signals. Damaged geophones can lead to bad traces in the data, compromising the consistency and completeness of subsequent processing. Low-frequency signals are often week, resulting in the loss of crucial signal components that are vital"}, {"title": "2 Review of conventional neural network-based seismic processing", "content": "Traditional seismic processing methods often rely on explicit physical models and assumptions, which may not be fully applicable in complex media. In contrast, the advantage of NNs lies in their ability to automatically approximate such complex mapping relationships by extracting features from data, without relying heavily on prior assumptions.\nCommonly, NN-based seismic processing methods can be viewed as a parameterized function approximator, which adjusts its internal weights through a training process to learn the nonlinear relationship that maps seismic data, xi, (such as raw noisy data) to the desired output products, Yi, (such as denoised data). During this process, the network optimizes its parameters by minimizing the error between the NN output (prediction) and the target, yi, to capture key features in the seismic data, which can be represented by the following loss function:\n$\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} ||NN_{\\theta}(x_i) - Y_i ||^2,$"}, {"title": "3 Method", "content": "In this section, we will first introduce the fundamental concepts of GDMs. Following that, we will present the framework for a GSFM based on GDMs. We will provide detailed illustrations on how we perform multi-task encoding, pre-training, fine-tuning, and prediction. Finally, we will introduce our network architecture."}, {"title": "3.1 Generative diffusion models", "content": "GDMs are gaining attention for its strong capability to produce highly realistic samples. These models initially convert data into pure noise through a forward process and then progressively denoise it to recover the data in the reverse process.\nWithin the denoising diffusion probabilistic model (DDPM), the forward process is defined as [Ho et al., 2020]:\n$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_{t-1}, (1-\\alpha_t)I).$\nIn the forward process, the data sample xo gradually transforms into a noisy version xt at each step t, controlled by the parameter at, with at = 1 \u2212 \u03b2t, where \u03b2t is a small constant specifying the noise variance incrementally introduced at each step. Thus, at each step, noise is added according to the Gaussian distribution N. This process adds isotropic Gaussian noise, facilitated by the identity matrix I, and progressively diffuses the original data into a Gaussian noise distribution from step t = 1 to t = T.\nTo express xt directly in terms of 20 and a noise term, we can use the reparameterization trick:\n$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon,$\nwhere \u2208 ~ N(0, I) is Gaussian noise, and \u0101t = \u03a0\u22121(1 \u2212 \u03b2\u03b5).\nTo learn how to reverse this process, the network is trained to predict the noise e added to the sample at each step t. Given a noisy sample xt, the network predicts the noise term eo(xt, t). The objective of the network is to minimize the difference between the actual noise \u20ac (added during the forward process) and the predicted noise 60 (xt, t). This is achieved by minimizing the mean squared error (MSE) between the added noise and the predicted noise:\n$\\mathcal{L}(\\theta) = E_{x_0,\\epsilon,t} [||\\epsilon - \\epsilon_{\\theta}(x_t, t) ||^2]$\nThis objective trains the network to accurately predict the noise added at each step, and to do so, the network needs to store the features of the signal information diffused by this added noise.\nOnce the network is trained, the reverse process, starting for a noise sample drawn from the Gaussian distribution, uses the network's noise predictions ee(x, t) to iteratively inject the stored signal and gradually remove noise to reconstruct a sample from the distribution it was trained on. Specifically, starting from pure noise xr, each step in the reverse process estimates the previous sample xt-1 from xt by subtracting the predicted noise component. The reverse sampling equation can be expressed as:\n$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_{\\theta}(x_t,t)) + \\sigma_t z,$\nwhere 60 (xt, t) is the noise predicted by the trained network, \u03c3\u03c4z is an optional noise term for stochastic sampling, and z ~ N(0, I).\nIn DDPM, the step-by-step denoising process is implemented through a Markov chain, which requires numerous time steps to gradually remove noise. As a result, the sampling speed of DDPM is relatively slow. In contrast, the denoising diffusion implicit model (DDIM) improves the sampling process of DDPM by removing the dependence on the Markov chain [Song et al., 2020]. Actually, DDIM and DDPM share the same forward process, which can also control noise introduction through Equation 4. The reverse process of DDIM can be represented by the following sampling equation:\n$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}\\hat{x}_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\epsilon (x_t, t) + \\sigma_t z,$\nwith\n$\\hat{x}_0 = \\frac{x_t}{\\sqrt{\\bar{\\alpha}}} - \\frac{\\sqrt{1 - \\bar{\\alpha}}}{\\sqrt{\\bar{\\alpha}}} \\epsilon_{\\theta}(x_t, t),$\ngiving an estimate of the original data 20 directly, 60(xt, t) is the noise term predicted by the neural network, and z ~ N(0, I) is a random noise term, and ot controls the level of this randomness. When ot = 0, the sampling process becomes deterministic because the random noise term otz is removed. If ot \u2260 0, it introduces a small random term"}, {"title": "3.2 Generative seismic foundation model: Pre-training", "content": "Our GSFM is adapted from a GDM and employs multi-task simultaneous pre-training on synthetic data, followed by direct fine-tuning on real data. However, in traditional GDMs, the model's input consists of a noisy version of a clean single-channel image, which is used to train the model for stepwise denoising.\nTo accommodate the needs of multi-task seismic processing, we extend the input of our GSFM to a dual-channel structure. During the pre-training and fine-tuning phases, the dual-channel inputs may contain different content. In this section, we first explain how the dual-channel network inputs are configured during the pre-training phase. Since the network is optimized on synthetic data during the pre-training phase, we can access the labels for different tasks. Therefore, in this phase, the first channel contains a noisy version of the labels (the target complete clean data), while the second channel is used for the corresponding data to be processed, i.e., the degraded data specific to the task. The content of the second channel varies depending on the SPT, enabling the model to adapt flexibly to different tasks based on the input data. Specifically, the dual channels for different specified tasks are as follows:\n\u2022 Denoising: The second channel contains the data contaminated with noise we want the network to learn to remove.\n\u2022 Backscattered noise attenuation: As a special case, the second channel contains data contaminated with backscattered noise.\n\u2022 Interpolation: The second channel contains data with missing traces.\n\u2022 Low-frequency extrapolation: The second channel contains data lacking low-frequency components.\nIn the pre-training phase, the forward process of our GSFM shares the noise injection formulation of the conventional diffusion model, as shown in Equation 4, thereby constructing the content for the first channel of the dual-channel input. For the second channel, we can see that we essentially use the same input data as that used in conventional NN-based seismic processing methods.\nTo enable simultaneous training for different tasks, we introduce a task encoding label c, allowing the network to identify and distinguish between various SPTs. For the tasks considered in this paper, including denoising, backscattered noise attenuation, interpolation, and low-frequency extrapolation, their class labels c are defined as 0, 1, 2, and 3, respectively."}, {"title": "3.3 Generative seismic foundation model: Fine-tuning", "content": "After completing pre-training on synthetic data, our GSFM is directly fine-tuned on real data to enhance its generalization capability for practical applications. During the fine-tuning phase, due to the lack of labels, we employ an SSL-based optimization approach, maintaining the model's adaptability and stability across multi-task seismic processing. To ensure consistency, the fine-tuning process retains the embedding methods for the task encoding label e during pre-training, enabling the model to continue supporting multi-task learning on real data and improving task transfer efficiency. The prediction target during fine-tuning remains set to xo (pseudo-labels), which represents the ideal output for each task.\nTo accomplish fine-tuning, we perform this process independently for each SPT, using the pre-trained network as the starting point for each task and setting the task encoding label c to the value corresponding to the desired task. We propose the following three fine-tuning strategies for each SPT:\n\u2022 1. The pre-trained model on synthetic data is used directly on the raw field data to generate preliminary processing products, which are then used as pseudo-labels during the fine-tuning phase. In this case, the first channel of the network input is a noisy version of the predicted pseudo-labels, while the second channel takes in shot gathers from the field data.\n\u2022 2. The second channel of the network input differs from that of strategy 1. Instead of using the field data, we use a corrupted version (similar to the corruptions applied to the synthetic data) of the pseudo-labels. For example, for the denoising task, additional noise is added to the predicted pseudo-labels. For the backscattered noise attenuation task, backscattered noise is added to the pseudo-labels. For the interpolation task, traces are removed from the pseudo-labels. For the low-frequency extrapolation task, the low-frequency components are filtered out from the pseudo-labels.\n\u2022 3. The third strategy is based on strategy 2 and involves iteratively updating the training dataset during the fine-tuning process. The complete workflow is detailed in Algorithm 1. Specifically, the fine-tuning process is divided into multiple stages, with each stage consisting of several iterations. In the first stage, we maintain the configuration of strategy 2. In each subsequent stage, the model fine-tuned from the previous stage is used directly on the field data, generating new pseudo-labels. Diffusion process is added to these pseudo-labels to create the input for the first channel, while the second channel contains a further corrupted version of the newly generated pseudo-labels, consistent with strategy 2.\nIn subsequent experiments, we will test these three fine-tuning strategies to determine which one performs better in enhancing the model's generalization ability and processing performance. Based on our test results, the fine-tuned network induced by strategy 3 provides superior performance. This outcome is expected, as the multi-stage strategy with gradual optimization allows the model to achieve a smooth transition between the feature distributions of synthetic and real data. By updating and further degrading the pseudo-labels at each stage, the model progressively shifts from the synthetic domain to the real data domain during the fine-tuning process. This stepwise adjustment not only makes the model more robust in handling the complexity of real data but also effectively reduces the distribution gap between synthetic and real data, thereby improving the model's generalization capability in real-world tasks."}, {"title": "3.4 Generative seismic foundation model: Predicting", "content": "After completing the fine-tuning process, our GSFM is ready to perform predictions for each seismic processing task independently. For each task-specific fine-tuned network, we obtain predictions tailored to the corresponding task. Unlike conventional NN-based seismic processing methods, which typically use a direct mapping approach, our GSFM leverages a generative prediction process due to its foundation in GDM.\nSpecifically, for each SPT, we begin by assigning the corresponding task encoding label e to indicate the target task. The network input is structured as follows: The first channel is initialized with random noise e, while the second channel contains seismic data x that needs processing. Using the reverse process of GDM, we iteratively denoise the input to generate the desired output 20. At each step t in the reverse process, the model estimates xo based on the current noisy input xt, and the reverse step is given by:\n$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}\\hat{x}_{0, \\theta}(x_t, x, t, c) + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\epsilon(x_t, x, t, c),$\nwhere x0,0 (xt, x, t, c) is the model's prediction of the clean data xo given the noisy input xt at step t and task label c. Here, \u00ea(xt, x, t, c) represents an estimate of the noise component, which can be computed as in Equation 11.\nThe conventional prediction process continues iteratively, with the model starting from a high level of noise and gradually refining the input. The final output at last step, xo, represents the processed data for the specified task, having been transformed from noise to the desired form through a series of denoising steps. However, considering the efficiency requirements in actual processing, we will only use one time step for the sampling process here. Specifically, we only use the last sampling step, that is, t is set to 0 to get our final prediction product."}, {"title": "3.5 Network architecture", "content": "Our GSFM adopts an enhanced U-Net-based architecture tailored for multi-task seismic processing. This architecture incorporates multi-scale feature extraction, task-specific embeddings, and attention mechanisms to deliver accurate and robust predictions. The main components of the network are illustrated in Figure 1, including convolutional layers, residual blocks, attention blocks, downsampling and upsampling layers, and embeddings for time and task-specific information.\nThe GSFM processes dual-channel inputs (xt, x), where xt represents in training the target data input at timestep t, and x contains the data to be processed specific to the task. These inputs are first passed through an initial 3 \u00d7 3 convolutional"}, {"title": "4 Synthetic data examples", "content": "In this section, we first introduce the pre-training details of the GSFM, including dataset preparation and training configuration. We then evaluate the pre-trained GSFM's performance on denoising, backscattered noise attenuation, interpolation, and low-frequency extrapolation tasks using synthetic test data.\nTo assess the effectiveness of the pre-trained GSFM, we provide two comparative experimental benchmarks:\n\u2022 Benchmark 1: Traditional NN-based processing paradigm.\nThis benchmark utilizes conventional NN-based seismic processing methods, employing the networks to approximate the nonlinear relationship between input data and target data. To ensure a fair comparison,"}, {"title": "4.1 Pre-training configuration", "content": "Creating synthetic subsurface models that represent the real Earth remains a challenge. For our purposes, we closely follow the workflow introduced by Ovcharenko et al. [2022] to generate random velocity models, which have been shown to effectively generalize to real data. Specifically, first, we randomly create 1D compressional wave velocity (Vp) profiles using velocity values within our expected range of 1,500 to 4,500 m/s. These 1D profiles are then spread laterally to build 2D laterally homogeneous layered velocity models. Lastly, we apply random elastic transforms to the velocity models to distort them and introduce structures resembling realistic geological phenomena (folding, intrusion, etc.)"}, {"title": "4.2 Denoising", "content": "We first test the denoising performance of the pre-trained GSFM on synthetic data contaminated by random noise. Figure 2 illustrates the denoising products of the three methods. Panel (a) shows the clean data, while panel (b) represents the noisy test data, which is generated by injecting Gaussian noise with a noise level of 30%, as follows:\n$y = x + \\epsilon \\cdot std(x) \\cdot rand(0, 1),$\nwhere e is the noise level, std(x) represents the standard deviation of the clean data x, and rand(0, 1) is the standard normal distribution. The denoised results for GSFM, Benchmark 1, and Benchmark 2 are displayed in panels (c), (d), and (e) (Figure 2), respectively. The difference between the denoised results and the clean data are presented in panels (f), (g), and (h), respectively.\nVisually, the denoised results from our GSFM and two benchmarks appear very similar, with each method successfully suppressing the random noise and preserving the main seismic reflection events. The differences between the methods are subtle and difficult to evaluate qualitatively, as all methods produce results with comparable reflection continuity and noise suppression.\nTo provide a more objective assessment of their performance, Table 2 shows a quantitative evaluation of the denoising performance in terms of the MSE metric across different noise levels (10% to 60%). The results reveal that GSFM consistently outperforms Benchmark 1 across all noise levels, demonstrating lower MSE values. It is worth noting that Benchmark 1 and GSFM share almost identical architectures, with the only difference being that Benchmark 1 excludes the time encoding module used in the diffusion process. Despite this, GSFM consistently outperforms Benchmark"}, {"title": "4.3 Backscattered noise attenuation", "content": "We, then, evaluate the performance of our pre-trained GSFM in attenuating backscattered noise. Figure 3 displays the backscattered noise attenuation results for the three methods. Panel (a) shows the clean seismic data, while panel (b) displays the input data contaminated with backscattered noise. The denoised products from GSFM, Benchmark 1, and Benchmark 2 are presented in panels (c), (d), and (e), respectively, and the corresponding residuals are shown in panels (f), (g), and (h), respectively.\nSimilar to the denoising case, the visual differences among the results produced by the three methods are minimal. All methods successfully suppress the backscattered noise and preserve the primary seismic reflections. The residuals reveal that all methods reduce the noise effectively. We further compute the MSE metric for the predicted results of each method. GSFM achieves the lowest MSE of 9.59e - 07, outperforming Benchmark 1 (1.10e - 06) and Benchmark 2 (1.26e - 06). This demonstrates GSFM's superior ability to attenuate backscattered noise while preserving the seismic signal."}, {"title": "4.4 Interpolation", "content": "Furthermore, we evaluate the interpolation performance of our pre-trained GSFM. Figure 4 shows the interpolation results for synthetic data with 50% randomly missing traces. Panel (a) displays the complete labeled data, while panel (b) shows the input data with missing traces. The interpolated results from GSFM, Benchmark 1, and Benchmark 2 are presented in panels (c), (d), and (e), respectively. The corresponding differences between the interpolated results and the complete data are shown in panels (f), (g), and (h), respectively.\nWe can see that all three methods achieve visually similar interpolated products, successfully reconstructing the missing traces with less signal leakage. More quantitatively, Table 3 summarizes the MSE metrics of the interpolated results across different missing data levels (10% to 60%). The results reveal the following trends: 1. At low missing data levels (10%), GSFM achieves the lowest MSE (1.45e \u2013 08), outperforming both benchmarks. 2. At intermediate missing data levels (20% to 50%), Benchmark 2 slightly outperforms GSFM, demonstrating its effectiveness in handling moderate missing levels due to its task-specific learning on pre-training stage. However, GSFM consistently performs better than Benchmark 1, highlighting its robustness. 3. At the highest missing data level (60%), GSFM significantly outperforms both benchmarks, achieving an MSE of 3.65e \u2013 07. This result demonstrates GSFM's superior ability to handle highly missing input data.\nOnce again, these results demonstrate that the diffusion model boosts the performance of the networks, enabling GSFM to outperform Benchmark 1 consistently. This trend was also observed in the denoising tests, confirming the effectiveness of the diffusion-guided training paradigm."}, {"title": "4.5 Low-frequency extrapolation", "content": "Finally, we focus on assessing the capability of our pre-trained GSFM in low-frequency extrapolation, a critical SPT which is particularly beneficial for full-waveform inversion. The extrapolation results for a test data, which miss low-frequencies below 4 Hz, are illustrated in Figure 5. The reference data, including low frequencies, is shown in"}, {"title": "4.6 Understanding performance differences among the methods", "content": "The evaluations of our pre-trained GSFM, Benchmark 1, and Benchmark 2 across the four SPTs on synthetic data offer insights into their strengths, limitations, and fundamental differences.\nBenchmark 1, based on the conventional NN paradigm, consistently underperforms compared to GSFM and Benchmark 2. While its performance gap is less pronounced in the first three tasks, it becomes significantly evident in the low-frequency extrapolation task. We know that Benchmark 1 approximates the nonlinear relationship between the input and target data. For the first three tasks, the target data (clean, complete seismic data) remains consistent across tasks. This consistency enables the network to learn a more generalized mapping that performs adequately across these tasks. However, in the low-frequency extrapolation task, the target data shifts to clean, full-band seismic data, including low frequencies that are absent in the input. This change introduces a more specific and challenging relationship to learn, which the traditional paradigm struggles to approximate effectively. As a result, the network is biased towards learning a more generalized mapping, leading to insufficient focus on the specific relationship required for accurate low-frequency extrapolation.\nIn contrast, GSFM, despite sharing the similar architecture as Benchmark 1, leverages the GDMs to capture and learn a more unified distribution. By modeling the joint distribution of clean, complete, and full-band seismic data, GSFM is able to bridge the gap between the input and target data more effectively, enabling it to achieve more accurate and robust results across a broader range of tasks.\nBenchmark 2 consistently demonstrates strong performance across tasks even slightly outperforms GSFM in terms of MSE for certain tasks. However, this slight advantage is achieved through task-specific fine-tuning, which relies heavily on labeled datasets and requires additional computational resources. Although additional fine-tuning can improve performance on synthetic data, our ultimate goal is field data. Since Benchmark 2 conducts fine-tuning using synthetic data, it still faces generalization challenges. In contrast, our GSFM does not rely on task-specific fine-tuning with labeled data. Instead, it undergoes fine-tuning directly on field data in an SSL manner, enabling it to address the generalization challenges faced by Benchmark 2. In the following section, we will share our field data test to highlight these advantages, showcasing GSFM's effectiveness in addressing the complexities of diverse SPTs."}, {"title": "5 Field data examples", "content": "In this section, we will go forward to fine-tune the pre-trained GSFM on real data and, then, evaluate the performance of our fine-tuned GSFM on field data across denoising, interpolation, and low-frequency extrapolation tasks. Also, we will examine the effectiveness of three different fine-tuning strategies, as outlined in the Fine-tuning subsection of the Method section. Finally, we discuss how GSFM can be leveraged for uncertainty quantification in seismic processing and, also, illustrate how use uncertainty quantification to guide our fine-tuning process."}, {"title": "5.1 Field data and Fine-tuning configuration", "content": "We use a marine field dataset to test our method. This dataset was acquired using a steamer survey in North West Australia. The original dataset consists of 1824 shot gathers activated with air gun sources, with an approximate horizontal spacing of 18.75 m and a sampling rate of 1 ms. Each shot gather contains 648 receivers, spaced 12.5 m apart. For testing purposes, we select every third shot gather starting from the left, resulting in a total of 200 shot gathers. To reduce the computational burden during training, the number of receivers in the field data was reduced to 324, and the time samples are downsampled from 6016 to 376, following the preprocessing used in Harsuko and Alkhalifah [2024].\nDuring fine-tuning on field data using the three different strategies, we ensure a fair comparison by using the same total number of iterations, set to 30000. However, in Strategy 3, these 30000 iterations are divided into 10 stages. Specifically, in Algorithm 1, the total stages S are set to 10, and the iterations per stage Nstage are set to 3000. We emphasize that all additional fine-tuning configurations remain consistent. Specifically, the learning rate is fixed at 5e - 5, the batch size is set to 4, and the EMA rate is configured to 0.999. Furthermore, during the sampling process, the pseudo-label generation utilized a diffusion step respace of 1 from the original 1000 diffusion steps. This means that only a single sampling step is used for field data predictions, which significantly improves fine-tuning and processing efficiency to meet the demands of practical applications.\nWe also emphasize that, since the field data does not include random noise, we do not fine-tune the pre-trained GSFM for the denoising task in this case. Instead, we independently optimize the pre-trained GSFM for the backscattered noise attenuation, interpolation, and low-frequency extrapolation tasks. During fine-tuning, we adopt a sequential workflow similar to the traditional seismic processing paradigm. Specifically:\n\u2022 1. Backscattered noise attenuation: Fine-tuning is first applied to this task to address inherent noise in the field data. Here, the noise added to pseudo labels is extracted from the area outside the first arrival.\n\u2022 2. Interpolation: After obtaining denoised results, since the denoised data does not contain bad traces, we artificially remove 50% of the seismic traces from the data to construct the incomplete seismic data, serving as the original fine-tuning dataset for interpolation task. To better reflect practical scenarios, where certain fixed"}, {"title": "5.2 Backscattered noise attenuation", "content": "we first evaluate the performance of our fine-tuned GSFM for backscattered noise attenuation on field data. We compare the outputs of our method against those from Benchmark 1 and Benchmark 2, which is detailed in last section. Furthermore, we explore the impact of various fine-tuning strategies and highlight the iterative improvements achieved with Strategy 3.\nFigure 6 illustrates the comparison of the denoised results between our fine-tuned GSFM and the two benchmarks. Panel (a) shows the field data contaminated with backscattered noise, while panels (b), (c), and (d) present the denoised outputs produced by our fine-tuned GSFM, Benchmark 1, and Benchmark 2, respectively. The corresponding differences between the denoised results and the noisy field data are shown in panels (e), (f), and (g), respectively. It is evident that both benchmarks fail to suppress the backscattered noise effectively, leading to severe signal leakage in their outputs. Specifically, compared with Benchmarks 1, Benchmarks 2 exhibit more significant signal leakage. In contrast, our fine-tuned GSFM demonstrates superior performance, successfully preserving the true signal while significantly reducing the noise. This highlights GSFM's capability to adapt to field data through SSL fine-tuning, addressing the generalization challenges faced by the benchmarks."}, {"title": "5.3 Interpolation", "content": "We, then, explore the interpolation capabilities of our pre-trained GSFM and its fine-tuned versions when applied to field data. Different fine-tuning strategies are assessed to identify the most effective approach. Moreover, we analyze how the iterative refinement process enhances interpolation performance at successive stages.\nThe interpolation results obtained using the pre-trained GSFM and the fine-tuned GSFM under different strategies are depicted in Figure 9. Panel (a) presents the denoised product from the backscattered noise attenuation task (Figure 6b), while panel (b) displays the incomplete data created by removing 50% of the seismic traces. Panels (c), (d), (e), and (f) show the outputs of the pre-trained GSFM and the models fine-tuned using strategies 1, 2, and 3, respectively. The differences for each result relative to the original denoised data are illustrated in panels (g), (h), (i), and (j). The results show noticeable variations in performance across the strategies. Strategy 1, which minimally adjusts the pre-trained model, produces results that closely resemble the pre-trained GSFM's output, leaving significant interpolation gaps unaddressed. Strategy 2 improves interpolation performance by reducing gaps, yet some residual inaccuracies remain. Strategy 3, in contrast, delivers the most refined results, reconstructing the missing traces with superior accuracy and significantly reducing errors. This progression highlights the advantage of iterative fine-tuning for aligning the model with field data.\nTo better understand the iterative refinement enabled by Strategy 3, Figure 10 displays the interpolated results at different stages of fine-tuning. The initial result from the pre-trained GSFM is presented in panel (a), while panels (b), (c), and (d) show the outputs after stages 1, 5, and 10 of fine-tuning, respectively. The corresponding differences are displayed in panels (e), (f), (g), and (h). With each fine-tuning stage, the quality of interpolation improves. At stage 1, the model addresses some missing traces but leaves substantial residuals. By stage 5, the interpolation becomes more accurate, with reduced signal leakage. Stage 10 achieves optimal results, effectively reconstructing the missing traces with minimal discrepancies. This progression demonstrates the power of Strategy 3 in leveraging iterative updates to adapt the model's predictions to the field data distribution.\nSince the incomplete field data is generated by artificially removing 50% of the seismic traces from the denoised data, we can obtain corresponding labeled data. As a result, we can provide a quantitative perspective to evaluate the interpolation performance across all 200 shot gathers at different fine-tuning stages. The mean squared error (MSE) for interpolation is plotted in Figure 11. The curve labeled \"Original Prediction\" represents the pre-trained GSFM, while the curves for stages 1, 5, and 10 correspond to the respective fine-tuning iterations. The MSE trends reveal a clear improvement as fine-tuning progresses. Compared to the pre-trained GSFM, stage 1 achieves a noticeable reduction in MSE. By stage 5, the interpolation accuracy improves further, with MSE values continuing to decline. Stage 10 marks the culmination of the process, yielding the lowest MSE across all shot gathers. This trend underscores the effectiveness of the iterative refinement process in progressively aligning the model with the complexities of field data."}, {"title": "5.4 Low-frequency extrapolation", "content": "We, finnaly, assess the performance of our pre-trained and fine-tuned GSFM models on the low-frequency extrapolation task for field data. By analyzing results across different fine-tuning strategies, we demonstrate how the iterative refinement process enables the model to progressively capture missing low-frequency components in real data.\nFigure 12 presents a detailed comparison of the low-frequency extrapolation results produced by the pre-trained GSFM and models fine-tuned with Strategies 1, 2, and 3. Panel (a) displays the denoised product obtained from the backscattered noise attenuation task (see Figure 6b), which serves as the input for extrapolation. Panels (b), (c), (d), and (e) show the results from the pre-trained GSFM and the fine-tuned models using Strategies 1, 2, and 3, respectively. To provide a focused view of the low-frequency content, the second and third columns highlight frequency components below 4 Hz and 2 Hz, respectively, where we use the low-pass filter to leave the low-frequency component. The results clearly illustrate the progression in reconstruction quality across the strategies. Strategy 1 offers only negligible improvements over the pre-trained GSFM. Strategy 2 delivers better results, reconstructing slight more of the missing low-frequency details. However, it struggles to effectively recover frequencies below 2 Hz. In contrast, Strategy 3 achieves the most substantial enhancement, recovering nearly complete low-frequency components both below 4 Hz and 2 Hz. This demonstrates that Strategy 3 is effective in adapting the model to field data and recovering challenging low-frequency details.\nAgain, to investigate the refinement process under Strategy 3, Figure 13 illustrates the extrapolated outputs at different fine-tuning stages. Panel (a) represents the prediction from the pre-trained GSFM, while panels (b), (c), and (d) show the results at fine-tuning stages 1, 5, and 10, respectively. The corresponding frequency components below 4 Hz and 2 Hz are highlighted in the second and third columns. The progression across stages demonstrates the strength of iterative refinement. In the early stages, the model begins to reconstruct low-frequency content, but the frequencies below 2 Hz remain less apparent. By stage 5, improvements become evident. At stage 10, the model delivers its best performance, with nearly complete recovery of low-frequency components, including those below 2 Hz. This gradual enhancement further underscores how Strategy 3 enables the GSFM to iteratively align with the field data distribution and, thus, improve the performance of GSFM on real data"}]}