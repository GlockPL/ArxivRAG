{"title": "Harnessing Scale and Physics: A Multi-Graph Neural Operator Framework for PDEs on Arbitrary Geometries", "authors": ["Zhihao Li", "Haoze Song", "Di Xiao", "Zhilu Lai", "Wei Wang"], "abstract": "Partial Differential Equations (PDEs) underpin many scientific phenomena, yet traditional computational approaches often struggle with complex, nonlinear systems and irregular geometries. This paper introduces the AMG method, a Multi-Graph neural operator approach designed for efficiently solving PDEs on Arbitrary geometries. AMG leverages advanced graph-based techniques and dynamic attention mechanisms within a novel GraphFormer architecture, enabling precise management of diverse spatial domains and complex data interdependencies. By constructing multi-scale graphs to handle variable feature frequencies and a physics graph to encapsulate inherent physical properties, AMG significantly outperforms previous methods, which are typically limited to uniform grids. We present a comprehensive evaluation of AMG across six benchmarks, demonstrating its consistent superiority over existing state-of-the-art models. Our findings highlight the transformative potential of tailored graph neural operators in surmounting the challenges faced by conventional PDE solvers. Our code and datasets are available on https://github.com/lizhihao2022/AMG.", "sections": [{"title": "1 INTRODUCTION", "content": "Partial Differential Equations (PDEs) underlie critical phenomena across diverse fields, from fluid dynamics to quantum mechanics, showcasing their ability to model complex variable relationships. Traditional approaches, while foundational, often struggle with the complexities and non-linearities of these systems, particularly in irregular geometries. Operator Learning has emerged as a transformative approach, leveraging deep learning architectures like Deep Operator Networks (DeepONet) and Fourier Neural Operators (FNO) to directly map input conditions to PDE solutions [16, 18, 25]. This method stands out for its versatility, requiring no retraining for different conditions, thus enabling efficient model adaptation across diverse settings.\nHowever, applying these deep learning techniques to real-world problems poses significant challenges, especially with irregular geometries where traditional methods like FNO [18], using Fast Fourier Transform, and U-Net [32], employing convolutions, are inherently limited to uniform grids. Innovations such as Graph Neural Operators (GNO) [22], and geometric adaptations like Geo-FNO [17], attempt to address these limitations by projecting irregular domains into more manageable latent meshes. Moreover, GINO combines these approaches to enhance modeling across various scales [23]. However, the inherent limitations of Fourier bases, particularly under the periodic boundary assumption, lead to significant performance degradation in complex geometries [9]. Similarly, graph kernels often fail to capture global information effectively [13, 37, 38].\nThe challenge extends to the domain's frequency spectrum; high-frequency areas require more intensive learning efforts compared to the predominantly flat, low-frequency regions. This disparity in learning demand across different frequencies, akin to techniques used in computer vision for tasks such as super-resolution, underscores the potential of tailoring neural network structures to leverage these differences for enhanced performance [10, 34].\nIn PDE learning, multi-scale methods have shown promise; however, they are generally limited to uniform grids and often require dimensions to be powers of two, particularly in methods utilizing wavelets [11, 19, 20]. Furthermore, transformer-based models, which distribute sample points across equal-sized attention grids, struggle with the highly variable frequency behaviors seen in phenomena like wave propagation [4, 13, 37, 38]. Existing graph-based methods, which often employ a k-Nearest-Neighbor approach to construct graphs, treat all nodes equally, neglecting the diverse learning needs inherent in PDE dynamics. This one-size-fits-all approach to node degrees can significantly impede the efficacy of graph-based models.\nTo address the challenges associated with solving PDEs on arbitrary geometries and capturing multi-scale features, we introduce AMG-a method employing a Multi-scale Graph neural operator tailored for Arbitrary geometries. AMG leverages three distinct types of graphs, constructed from the given coordinates: two are designed for multi-scale processing is dedicated to capturing the underlying physical properties. At the heart of AMG is the GraphFormer, a novel architecture equipped with dynamic graph attention mechanisms. This design allows the model to compute the hidden representations of each node by dynamically attending to its neighbors, effectively handling the complex interdependencies within the data. Furthermore, we theoretically establish that graph attention can be seen as a learnable integral, enhancing our method's ability to model continuous spaces. Extensive experiments were conducted on six benchmarks, including four well-established ones and two custom-designed to test our method's efficacy across diverse geometries and dynamic mesh configurations. AMG consistently outperforms existing solutions, achieving a remarkable relative gain on six benchmarks.\nThe contributions of this paper are summarized as follows:\n\u2022 We use local sampling (Section 3.2.3) and global sampling (Section 3.2.4) to construct multi-scale graphs for capturing different frequencies of features and a physical graph (Section 3.2.5) to encode inherent physical properties effectively.\n\u2022 We introduce a GraphFormer (Section 3.4) architecture with dynamic graph attention mechanisms, providing a scalable and flexible encoder-processor-decoder framework (Section 3.1) for learning operators tailored to arbitrary geometries.\n\u2022 AMG demonstrates superior performance, achieving consistent state-of-the-art results with significant relative gains across a variety of benchmarks."}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 Problem Formulation", "content": "We consider Partial Differential Equations (PDEs) defined over a domain $D \\subset \\mathbb{R}^d$. Define $\\mathcal{A} = \\mathcal{A}(D; \\mathbb{R}^{d_a})$ and $\\mathcal{U} = \\mathcal{U}(D; \\mathbb{R}^{d_u})$ as two Sobolev spaces $H^{s,p}$, with parameters $s > 0$ and $p \\geq 1$. Our aim is to learn an operator $\\mathcal{G}: \\mathcal{A} \\rightarrow \\mathcal{U}$, mapping from the input function space $\\mathcal{A}$ to the solution function space $\\mathcal{U}$. Specifically, we select $s > 0$ and $p = 2$ to take advantage of the Hilbert space structure, which facilitates the definition of projections.\nThe operator $\\mathcal{G}$ is characterized as an integral operator with a kernel $\\kappa$, where $\\kappa: D \\times D \\rightarrow L^2$, and is formalized by the integral equation:\n$\\mathcal{G}a(x) = \\int_{D} \\kappa(x, y)a(y) dy$,\n enabling $\\mathcal{G}$ to operate within the Hilbert space framework and leveraging the properties of $L^2$ spaces for enhanced analytical and computational efficiency."}, {"title": "2.2 Graph Neural Operators", "content": "A directed graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ includes nodes $\\mathcal{V} = \\{1, ..., n\\}$ and edges $\\mathcal{E} \\subset \\mathcal{V} \\times \\mathcal{V}$, with $(j, i) \\in \\mathcal{E}$ representing an edge from node $j$ to node $i$. Each node $i \\in \\mathcal{V}$ initially possesses a representation $h_i^{(0)} \\in \\mathbb{R}^{d_n}$. An undirected graph is depicted using bidirectional edges between nodes.\nA Graph Neural Network (GNN) layer updates the representation of each node by aggregating information from its neighbors. The input to a GNN layer consists of a set of node representations $\\{h_i \\in \\mathbb{R}^{d_h} | i \\in \\mathcal{V}\\}$ and the set of edges $\\mathcal{E}$. The output is a new set of node representations $\\{h_i' \\in \\mathbb{R}^{d_h} | i \\in \\mathcal{V}\\}$, where each node's updated state is calculated as:\n$h_i' = f_{\\Theta}(h_i, AGGREGATE(\\{h_j | j \\in \\mathcal{N}_i\\}))$\nThe function $f_{\\Theta}$ and the aggregation method AGGREGATE largely define the distinctions among various GNN architectures.\nIf we accurately construct the graph on the spatial domain $D$ of the PDE, the kernel integration can be interpreted as an aggregation of messages [22]. In the edge-conditioned aggregation mechanism [8, 35] utilized at the $k^{th}$ layer, the output feature $h_i^k$ for node $i$ is calculated based on its previous feature $h_i^{k-1}$ as follows:\n$u(x) = h_i^k = \\frac{\\sum_{j \\in \\mathcal{N}(i)} \\exp(\\chi(x_j, x_i))h_j^{k-1}}{\\Sigma_{j \\in \\mathcal{N}(i)} \\exp(\\chi(x_j, x_i))}$."}, {"title": "3 METHODOLOGY", "content": "This section outlines the AMG method's architecture for solving PDEs on arbitrary geometries. We start with an architecture overview in Section 3.1, followed by the construction of multi-graphs in Section 3.2. The processing module is detailed in Section 3.3, and the GraphFormer Block is discussed in Section 3.4."}, {"title": "3.1 Overview of Model Architecture", "content": "Our model architecture is designed to effectively handle the complexities involved in processing partial differential equations (PDEs) on irregular geometries using graph neural networks. The architecture is illustrated in Figure 1.\nInput and Encoder: The process begins with the input dataset $\\{a_i\\}_{i=1}^N$, which consists of the initial conditions or parameters for the PDEs. These inputs are first processed by an Encoder, which transforms the raw data into a preliminary feature representation suitable for further processing within the neural network.\nGraph Construction: Two distinct types of graphs, the Physics Graph and the Multi-Scale Graph, are constructed from the encoded features. The Physics Graph encapsulates the underlying physical laws governing the phenomena being modeled, while the Multi-Scale Graph captures interactions at various scales, crucial for accurately modeling complex systems.\nMulti-Graph Processing Layers: The core of the architecture comprises multiple processing layers, each consisting of a GraphFormer, Message Passing, and Layer Normalization components followed by a Feed Forward network. The GraphFormer component applies transformations specific to graph data, facilitating the propagation and update of node features. Message Passing enables the exchange of information between nodes, enhancing the model's ability to learn from the topology of the graph. Layer Normalization is employed to stabilize the learning process, and the Feed Forward networks provide additional transformation capabilities to the node features.\nDecoder and Output: Following the processing layers, a Decoder reverts the graph-based features back into the spatial domain, generating the output $\\{u_i\\}_{i=1}^N$ which represents the solution to the PDE at the discretized points. This output effectively demonstrates the model's capability to predict complex phenomena governed by the PDEs.\nThis architecture leverages the strengths of graph neural networks to process data over irregular domains, ensuring robustness and accuracy in capturing the dynamics of the system modeled."}, {"title": "3.2 Graph Construction", "content": ""}, {"title": "3.2.1 High-Frequency Indicator", "content": "To efficiently and effectively pinpoint regions within feature maps that contain detail-rich information, we introduce a high-frequency indicator. This indicator is designed to rapidly identify high-frequency areas that are crucial for accurate PDE solutions in complex geometries. Given a feature map of point set $F \\in \\mathbb{R}^{N\\times C}$,where C is the number of feature channels and a specific down-sampling ratio s, the high frequency indicator per node, $H_F \\in \\mathbb{R}^N$, is computed as follows:\n$H_F = \\sum_{c=1}^C |F - (F^{(c)})_{ss}|$"}, {"title": "3.2.2 Partitioning of Point Sets", "content": "To efficiently manage the challenge of generating overlapping partitions within a point set, we define each partition as a neighborhood ball within Euclidean space, characterized by centroid location and scale. We utilize a farthest point sampling (FPS) algorithm [31] to select centroids in a manner that ensures even coverage across the entire point set. This method starts with an arbitrary initial point and iteratively selects subsequent points that are the farthest in metric distance from all previously selected points. This approach not only guarantees a superior coverage compared to random sampling but also takes into account the spatial distribution of the points, contrasting with methods like convolutional neural networks (CNNs), that process data agnostically of spatial distributions. Consequently, our graph construction strategy establishes receptive fields in a data-dependent manner, significantly enhancing the model's capability to capture and process complex spatial relationships. Detailed description of the FPS algorithm is provided in Appendix A.1."}, {"title": "3.2.3 Local Sampling", "content": "Local sampling is essential for constructing graphs that precisely capture the immediate neighborhood dynamics of a node, particularly valuable in systems characterized by complex local interactions, such as in solving PDEs with intricate local behaviors. This method selects each node $v$ and its neighbors $\\mathcal{N}(v)$ based on a high-frequency indicator (Eq.(4)), identifying nodes that reside within areas of high detail and information density.\nIn practice, nodes are chosen for their rich detail using the high-frequency indicator to determine the composition of each local graph. For every node $v$, its neighbors $\\mathcal{N}(v)$ are selected to ensure that only nodes within areas of substantial detail and information content are included. This targeted selection strategy guarantees that the connections within the graph are meaningful and representative of significant local interactions. By focusing on nodes with high information content, the model can adapt more effectively to variations in data density and local structural complexities. This approach is invaluable in scenarios demanding high precision in local detail, enabling the model to accurately capture complex dynamics. To maintain local coherence, we use the Euclidean distance between two nodes as the metric for establishing connections:\n$d_{local}(i, j) = ||p_i - p_j||_2,$\nwhere $p_i$ and $p_j$ are the position vector of nodes $i$ and $j$, respectively. This metric also allows for connecting nodes that belong to different domains.\nAn illustrative example of this local sampling process and the resultant graph structure is shown in Figure 2(b). This visualization underscores how local sampling concentrates on areas rich in detail, thereby substantially enhancing the model's capacity to learn nuanced behaviors and interactions, which are crucial for solving complex PDEs. Detailed description of the local sampling algorithm is provided in Appendix A.2."}, {"title": "3.2.4 Global Sampling", "content": "Conversely, global sampling aims to capture the broader structure of the dataset by establishing connections across wider spatial extents. This sampling strategy selects nodes that span the entire domain in a pattern designed to maximize coverage and minimize redundancy, typically implemented using a dilated sampling technique as introduced in Section 3.2.2. By increasing the intervals between selected nodes, this method provides a comprehensive overview of global interactions essential for understanding large-scale trends and phenomena.\nConnectivity among nodes is established through a metric that measures the similarity in node features, employing cosine similarity:\n$d_{global}(i, j) = \\frac{h_i \\cdot h_j}{||h_i|| ||h_j||}$\nwhere $h_i$ and $h_j$ are the feature vectors of nodes $i$ and $j$, respectively. This metric ensures that nodes with similar features are linked, regardless of their physical placement, thus enhancing the graph's ability to model phenomena accurately.\nAn illustrative example of this global sampling process is shown in Figure 2(b). The combination of both local and global sampling strategies in graph construction allows the model to effectively balance detail-oriented and holistic learning objectives."}, {"title": "3.2.5 Physics Graph", "content": "In our model, node attributes are embedded into higher dimensions, allowing us to interpret them as high-level physical attributes, which are often the solutions sought in complex physical systems. These high-level attributes derive from more fundamental, lower-level physical properties.\nTo effectively represent these relationships, we construct a physics graph where each node corresponds to a fundamental physical attribute. In this graph, nodes from the standard operational graph are linked to all nodes in the physics graph, emphasizing the foundational contributions of these attributes to higher-level phenomena. The edges within the physics graph symbolize the interactions between each of these lower-level physical attributes.\nIn scenarios where explicit physical information about the connections between attributes is available, the physics graph is constructed based on this empirical data. However, in cases where such specifics are lacking, we opt for a fully-connected graph configuration. This approach is justified by the typically small number of lower-level nodes and the frequent interactions among them, ensuring comprehensive coverage of potential influences and interactions within the system.\nAn illustration of the construction of the physics graph is shown in Figure 2(a), and the detailed propagation mechanisms between the physics graph and the multi-scale graph are defined in Section 3.3.2."}, {"title": "3.3 Multi-Graph Processor", "content": "Multi-Scale Graph Blocks integrate both local and global-scale information crucial for effective operator learning. These blocks compute local and global graphs per block based on the inputs, allowing for dynamic graph updates throughout the model's processing."}, {"title": "3.3.1 Cross-Scale Graph Aggregation", "content": "After constructing two graphs at different scales, as shown in Figure 2(b), nodes are processed through the GraphFormer Block, defining local or global edges accordingly:\n$\\mathcal{N}_{local} = GraphFormer(\\{h_i^{local}\\}_{i=1}^N, \\mathcal{E}_{local}),$\n$\\mathcal{N}_{global} = GraphFormer(\\{h_i^{local}\\}_{i=1}^N, \\mathcal{E}_{global}),$\nwhere $h_i$ represents the input features for node $i$, and $\\mathcal{E}_{local}$ and $\\mathcal{E}_{global}$ are the sets of edges in the local and global graphs, respectively."}, {"title": "3.3.2 Physics Graph Propagation", "content": "As depicted in Figure 2(a), virtual physics nodes are initialized by aggregating information from the original graph nodes. Specifically, the value of virtual physics nodes $\\{h_i^{phy}\\}_{i=1}^N$ is calculated as:\n$h_i^{phy} = \\sum_{i=1}^N e_i h_i'$\nwhere $h_i'$ are the nodes from the original graph, and $e_i$ are the edge weights defined by:\n$e_i = \\frac{W h_i'}{\\sum_i \\mathcal{N}}$\nwith $W \\in \\mathbb{R}^{d_h \\times d_h}$ being a trainable linear layer. Subsequently, the transformed physical nodes are processed through the GraphFormer:\n$\\{h_i'\\}_{i=1}^N = GraphFormer(\\{h_i^{phy}\\}_{i=1}^N, \\mathcal{E}_{phy}),$\nFinally, these physical nodes are reintegrated into the graph nodes through a message-passing scheme, effectively blending the computed physical node representations back into the global node features:"}, {"title": "3.4 GraphFormer Block", "content": "Inspired by the general MetaFormer architecture [42], we introduce the GraphFormer block, which replaces traditional attention mechanisms with graph attention, thereby tailoring the approach to handle graph-based data more effectively. As depicted in Figure 3, this block employs graph aggregation techniques as token mixers, facilitating efficient information processing across the network."}, {"title": "3.4.1 Graph Attention Mechanism", "content": "A scoring function $f: \\mathbb{R}^{d_h} \\times \\mathbb{R}^{d_h} \\rightarrow \\mathbb{R}$ evaluates the importance of features from neighbor $j$ to node $i$ in every edge $(j, i)$:\n$f(h_i, h_j) = LeakyReLU(\\alpha^T \\cdot [Wh_i || Wh_j])$\nwhere $\\alpha \\in \\mathbb{R}^{2d_h}$ and $W \\in \\mathbb{R}^{d_h \\times d_h}$ are trainable parameters, and $||$ denotes vector concatenation. The attention scores are then normalized using a softmax function across all neighbors $j \\in \\mathcal{N}_i$:\n$\\alpha_{ij} = softmax_j(f(h_i, h_j)) = \\frac{exp(f(h_i, h_j))}{\\Sigma_{j' \\in \\mathcal{N}_i} exp(f(h_i, h_{j'}))}.$\nThe Graph Attention (GA) then compute the new representation for node $i$ as a weighted average of transformed features of its neighbors, applying a nonlinearity $\\sigma$:\nh_i' = \\sigma(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} Wh_j)\nFollowing [2], we apply the $\\alpha$ layer after the LeakyReLU non-linearity and $W$ after concatenation, essentially using an MLP to compute the score for each query-key pair.\nThe overall complexity is $O(|\\mathcal{V}|d^2 + |\\mathcal{E}|d_h)$, where $|\\mathcal{V}|$ represents the number of nodes and $|\\mathcal{E}|$ the number of edges in the graphs used, either multi-scale or physics. Since $|\\mathcal{V}|< N$ and $|\\mathcal{E}| = k|\\mathcal{V}| \\leq |\\mathcal{V}|^2$, the computational complexity remains linear with respect to the number of mesh points $N$. This ensures that the proposed method scales efficiently even as the mesh complexity increases."}, {"title": "3.4.2 Properties of GraphFormer", "content": "Prior methods have approached PDE learning as an iterative update process, demonstrating that canonical attention mechanisms can approximate integral operators over the input domain $\\Omega$ [4, 16]. To deepen our theoretical understanding of the GraphFormer, we propose that it similarly acts as a learnable integral on $\\Omega$:\nTheorem 1 (GraphFormer as a Learnable Integral on $\\Omega$). Given an input function $a: \\Omega \\rightarrow \\mathbb{R}^d$ and a mesh point $x \\in \\Omega$, GraphFormer seeks to approximate the integral operator $\\mathcal{G}$, defined by:\n$\\mathcal{G}a(x) = \\int_{\\Omega} \\kappa(x, \\xi) a(\\xi) d\\xi,$"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 General Setting", "content": "In practice, to learn a neural operator, we use a dataset $\\mathcal{D} = \\{(a, u)\\}$, where $u = \\mathcal{G}(a)$. Due to challenges in directly representing functions, we discretize the input and solution functions over the domain $\\mathcal{D}$ using an irregular mesh, as per a specified mesh generation algorithm [27]. Thus, we consider a set of function pairs $(a_i, u_i)$, where $a_i = a(x_i)$ and $u_i = u(x_i)$ at discretized points $x_i$ within the domain $\\mathcal{D} \\subset \\mathbb{R}^d$.\nOur goal is to approximate $\\mathcal{G}$ by optimizing the network parameters $\\Theta$, through the following optimization problem:\n$\\min_{\\Theta \\in \\Theta} \\mathcal{L}(\\Theta) := \\min_{\\Theta \\in \\Theta} \\frac{1}{N} \\sum_{i=1}^N [||\\mathcal{G}_\\Theta(a_i) - u_i||_2]$"}, {"title": "4.1.1 Benchmarks", "content": "We conducted comprehensive analyses across a variety of benchmark scenarios to demonstrate the superiority of our method, as detailed in [17, 18, 29]. These benchmarks were carefully selected to encompass a wide range of PDE problems, from fluid dynamics to structural deformations, using both static and dynamic conditions. Specifically, we developed datasets for the Poisson equation and Cylinder Flow, testing our model on both fixed and dynamic unstructured meshes, as well as standard structured meshes. We also expand our experimental scope to include global weather forecasting, utilizing data from the European Centre for Medium-Range Weather Forecasts (ECMWF). This new experiment involves climate data from the first quarter of 2018, focusing specifically on the atmospheric conditions at a pressure level of 50hPa. This breadth of testing highlights our model's adaptability and precision across various computational environments. For ease of reference, these benchmarks are summarized in Table 1, with visual examples provided and detailed configurations discussed in Appendix C."}, {"title": "4.1.2 Baselines", "content": "We conducted a comprehensive evaluation against a variety of established methods to validate our approach. Traditional models like MLP [33] and U-Net [32] were included for their foundational roles in computational tasks. We also assessed against specialized graph-based methods such as MeshGraphNets (MGN) [29] and Graph Neural Operator (GNO) [22], which are designed for complex spatial data. Additionally, our model was compared with advanced transformer-based methods, including LSM [37], GNOT [13], ONO [39] and Transolver [38], to benchmark against the latest approaches in handling high-dimensional data complexities."}, {"title": "4.1.3 Implementation", "content": "To ensure fairness in performance comparison, all models were uniformly trained across 500 epochs using the L2 loss function [18]. We utilized the Adam [15] and AdamW [24] optimizers, initiating with a learning rate of 10-3 and implementing a decay factor of $\\gamma = 0.5$ to halve the rate every 100 epochs. For handling dynamic mesh sizes, which transformer-based models inherently struggle with, we adopted a padding strategy similar to that used in GNOT [13] to accommodate these models. All experiments were conducted using a single Nvidia A800 80GB GPU, and unless specified, default hyperparameters were employed for the baseline models. This setup facilitated a controlled environment to accurately assess the comparative effectiveness of each method."}, {"title": "4.2 Main Results", "content": "AMG's robustness and accuracy across diverse PDE benchmarks are highlighted in the performance comparison (Table 2), demonstrating significant enhancements in both structured and unstructured mesh environments."}, {"title": "4.2.1 Structured Mesh Performance", "content": "In the Navier-Stokes benchmark with structured 2D geometry, AMG achieved a notable reduction in errors by 19.70%. This illustrates AMG's effectiveness in structured environments where regular geometries predominate, requiring high precision to accurately model dynamic interactions."}, {"title": "4.2.2 Fixed Unstructured Mesh Analysis", "content": "In the Poisson benchmark, AMG improved prediction by 6.44%, demonstrating its effectiveness in modeling static spatial variations in simpler systems. In the Shape-Net Car benchmark involving complex 3D automobile geometries, AMG achieved improvements of 11.64% in pressure accuracy of surface and 23.81% in velocity accuracy of surrounding air, showcasing its capability to handle intricate 3D shapes and multiple attributes in a large unstructured mesh environment. For the Airfoil benchmark, AMG excelled by enhancing velocity predictions by 51.44% for velocity X and 47.45% for velocity Y, along with increases of 39.97% in density and 38.01% in pressure accuracy. This demonstrates its proficiency in dynamic simulations, capturing detailed airflow dynamics and interactions. Overall, these results affirm AMG's robust performance across diverse fixed unstructured mesh scenarios, illustrating its versatility and high precision in handling complex physical phenomena across different dimensions and conditions."}, {"title": "4.2.3 Dynamic Unstructured Mesh Challenges", "content": "AMG's prowess was distinctly displayed in the Cylinder Flow and Deforming Plate benchmarks, both demanding high adaptability in dynamic environments. In the 2D Cylinder Flow scenario, AMG significantly enhanced performance metrics, improving pressure accuracy by 45.33%, velocity X by 35.44%, and velocity Y by 32.94% over 100 timesteps. These enhancements confirm AMG's exceptional ability to handle temporal changes and adaptive mesh akin to real-world fluid dynamics. The 3D Deforming Plate benchmark posed additional challenges with both spatial and temporal dynamics. Here, AMG achieved a 2.35% improvement in pressure accuracy, demonstrating its competency in managing the intricacies of 3D dynamic systems that necessitate synchronized predictions across varied scales. Collectively, these benchmarks validate AMG's effectiveness in dynamic unstructured meshes, underlining its high precision and versatility in complex, real-world simulation scenarios. This robust performance underscores AMG's sophisticated capacity to navigate intricate interactions and continual changes."}, {"title": "4.2.4 Three-Dimensional Benchmark Insights", "content": "The Shape-Net Car benchmark involves predicting two attributes across a large mesh size of 32,186 points spread over three dimensions. The Deforming Plate benchmark presents a dynamic three-dimensional challenge, where the mesh evolves over time, adding another layer of complexity. In addressing these challenges, AMG leverages a combination of a graph specifically designed to handle arbitrary geometries, and multi-scale and physics graphs that effectively manage features across different scales. This integrated approach significantly enhances AMG's performance, leading to a substantial reduction in prediction errors by 35.44%. This improvement not only highlights the model's scalability but also its adeptness in processing volumetric data, effectively handling the intricacies of varied spatial and temporal scales, thereby markedly reducing prediction errors."}, {"title": "4.2.5 Comparison with Graph-Based Methods", "content": "Our analysis reveals that graph-based models such as MeshGraphNets (MGN) and Graph Neural Operator (GNO) generally underperform in static benchmarks like Shape-Net Car and Poisson, where the scenarios are dominated by low-frequency features. These models are better suited to dynamic benchmarks such as Deforming Plate, which involve varying shapes; however, their performance still falls short of expectations. To overcome these limitations, AMG incorporates global and physics graphs. The global graph helps in capturing broader spatial relationships that are crucial for static environments, while the physics graph integrates fundamental physical laws and principles, enhancing the model's ability to accurately predict behaviors in dynamic scenarios. This strategic use of multiple graph types enables AMG to significantly outperform traditional graph-based methods across a range of benchmarks, effectively addressing both static and dynamic challenges."}, {"title": "4.2.6 Comparison with Transformer-Based Methods", "content": "Transformer-based models such as Transolver and GNOT previously set the standard for state-of-the-art performance in handling complex data structures. Despite their strengths, AMG has demonstrated superior performance by integrating specialized graph techniques that enhance feature processing capabilities across various scales. Utilizing the graph neural operator's ability to effectively manage local high-frequency features, combined with the global graph and physics graph's adeptness at capturing global low-frequency features, AMG has significantly outperformed these transformer-based models. This remarkable improvement showcases AMG's enhanced capability to address a broader range of dynamic and static phenomena more accurately than the existing state-of-the-art models."}, {"title": "4.2.7 Experiment on Real-World Dataset", "content": "Our objective was to predict the next hour's temperature and wind components (U-component and V-component). The data was sampled from a reduced grid of the original, capturing 11,088 gridpoints, to balance computational demand with meaningful analysis. As illustrated in Table 3, AMG outperforms other methods across all metrics, particularly excelling in dynamic conditions, such as predicting wind directions and speeds. This success underscores AMG's integration of multiscale graphs and dynamic graph attention mechanisms, enhancing its predictive accuracy in complex, variable scenarios.\nThese results affirm the robustness of AMG across diverse computational settings, from static and dynamic meshes to two-dimensional and three-dimensional spaces. The model's adeptness at navigating various mesh types and geometries, combined with its capability to manage intricate local and global interactions, establishes AMG as a powerful solution for solving PDEs in a wide range of applications. This comprehensive performance, particularly evident in real-world dynamic scenarios like weather forecasting, demonstrates AMG's potential to deliver precise, reliable predictions across different scales and conditions."}, {"title": "4.3 Model Analysis", "content": ""}, {"title": "4.3.1 Ablation Studies on Graph Configurations", "content": "We conducted ablation studies to determine the contribution of different graph configurations-Local, Global, Multiscale, and Physics-to our model's performance. These studies, summarized in Table 4, show the impact of each graph type on the accuracy of predictions for pressure, velocity X, and velocity Y.\n\u2022 Without Local Graph: Removing this graph increased L2 errors, especially in velocity Y, underscoring its role in capturing local interactions.\n\u2022 Without Global Graph: The absence led to higher L2 errors, highlighting its importance for contextual relationship understanding.\n\u2022 Without Multiscale Graph: Its removal caused minor performance drops, indicating its role in linking local and global insights.\n\u2022 Without Physics Graph: This had the most significant impact, emphasizing its necessity for incorporating physical principles.\nThe baseline AMG configuration outperformed all variations, illustrating the critical need for an integrated approach to handle complex simulations effectively."}, {"title": "4.3.2 Hyperparameter Analysis", "content": "Our comprehensive hyperparameter study, summarized in Table 5, identifies optimal configurations that significantly enhance model performance across various metrics. The analysis revealed that simply increasing the number of layers, node numbers, or head numbers does not consistently reduce L2 errors, indicating that optimal hyperparameter settings are crucial for maximizing performance. These results demonstrate that effective hyperparameter tuning is crucial for deploying AMG in diverse computational environments. By strategically selecting settings tailored to specific tasks, we can refine the model's architecture, thereby enhancing both performance and operational efficiency. The insights gained from this study are instrumental in fine-tuning the model to achieve superior performance across different applications."}, {"title": "4.3.3 Multi-Scale Sampling", "content": "Multi-Scale Sampling is a critical technique in our methodology, enabling the AMG model to effectively capture and integrate diverse spatial information. This approach involves two distinct sampling strategies: global sampling and local sampling. Global sampling (Figure 4(a,c) focuses on capturing broad-scale data integration across the entire domain, while local sampling (Figure 4(b,d)) targets high-resolution detail capture within specific regions of interest. These techniques collectively enhance the model's ability to process and analyze data from complex PDE systems with varying dynamics and geometric irregularities."}, {"title": "5 RELATED WORK", "content": ""}, {"title": "5.1 Neural Operators", "content": "Neural Operators utilize deep neural networks to efficiently solve complex PDE systems by learning mappings between two functional spaces [16]. Typically modeled as kernel integral operators, various parameterization strategies have been developed, including the widely recognized Fourier Neural Operators (FNO) which integrate Fast Fourier Transform (FFT) [18]."}, {"title": "5.1.1 Graph-based Neural Operators", "content": "Graph-based neural operators such as GNO [22] utilize Message Passing Graph Networks to process data in irregular domains but face challenges like prolonged training and difficulty in capturing global low-frequency features [19, 23, 36, 37, 41]. MGNO [19] attempts to overcome these by using multi-scale graphs, yet its reliance on pre-defined matrix decomposition restricts flexibility with dynamic meshes and complex geometries. Recent models like GINO and GeoFNO [17, 23] combine graph-based strategies with Fourier Neural Operators to adapt irregular data into uniform grids, aiming to resolve some of the enduring challenges of GNO. However, these advancements still contend with foundational issues inherent in the original graph-based methods."}, {"title": "5.1.2"}]}