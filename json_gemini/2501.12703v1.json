{"title": "HEPPO: Hardware-Efficient Proximal Policy Optimization\nA Universal Pipelined Architecture for Generalized Advantage Estimation", "authors": ["Hazem Taha", "Ameer M. S. Abdelhadi"], "abstract": "This paper introduces HEPPO, an FPGA-based\naccelerator designed to optimize the Generalized Advantage\nEstimation (GAE) stage in Proximal Policy Optimization (PPO).\nUnlike previous approaches that focused on trajectory collection\nand actor-critic updates, HEPPO addresses GAE's computational\ndemands with a parallel, pipelined architecture implemented\non a single System-on-Chip (SoC). This design allows for the\nadaptation of various hardware accelerators tailored for different\nPPO phases. A key innovation is our strategic standardization\ntechnique, which combines dynamic reward standardization and\nblock standardization for values, followed by 8-bit uniform\nquantization. This method stabilizes learning, enhances per-\nformance, and manages memory bottlenecks, achieving a 4x\nreduction in memory usage and a 1.5x increase in cumula-\ntive rewards. We propose a solution on a single SoC device\nwith programmable logic and embedded processors, delivering\nthroughput orders of magnitude higher than traditional CPU-\nGPU systems. Our single-chip solution minimizes communication\nlatency and throughput bottlenecks, significantly boosting PPO\ntraining efficiency. Experimental results show a 30% increase\nin PPO speed and a substantial reduction in memory access\ntime, underscoring HEPPO's potential for broad applicability in\nhardware-efficient reinforcement learning algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement Learning (RL) is a subset of machine learn-\ning where agents learn best behaviors by interacting with the\nenvironment. RL agents do not receive correct input/output\npairs like in supervised learning; they find strategies through\ntrial and error, guided by rewards. This approach has been\ncrucial in addressing intricate decision-making challenges in\nvarious fields such as robotics [1] and strategic games like\nchess and Go [2].\nProximal Policy Optimization (PPO) is a widely used\nreinforcement learning (RL) algorithm that optimizes policy\ndirectly through gradient ascent to maximize expected cumu-\nlative reward [3]. PPO enhances the stability of policy gradient\nmethods by using a clipped objective function to ensure that\npolicy updates are not excessively large, effectively addressing\nhigh gradient variance and instability. This approach elimi-\nnates the need for the computationally expensive second-order\noptimization step required by algorithms such as Trust Region\nPolicy Optimization (TRPO), making PPO easier to implement\nand more computationally efficient [4]. By preventing large\nupdates, PPO maintains robustness and improves training sta-\nbility of TRPO [3]. At the core of PPO, it effectively balances\nthe need for exploration and exploitation by preventing large\npolicy updates, ensuring stable and efficient learning. This\nmakes PPO a robust and practical choice for a wide range\nof RL tasks [3]."}, {"title": "A. Background on Generalized Advantage Estimation (GAE)", "content": "GAE addresses the variance-bias tradeoff in policy gradient\nmethods for RL by using value functions to estimate the\nadvantage function more accurately, at the cost of introducing\nsome bias. The key idea is to use an exponentially-weighted\nestimator of the advantage function, analogous to the TD(1)\nmethod [6]. An estimation can be derived by utilizing the\ntemporal-difference (TD) residual, $\\delta V_t$,\n$\\delta V_t = r_t + V(s_{t+1}) \u2013 V(s_t)$,\nwhereas the GAE is an exponentially-weighted average of k-\nstep advantage estimators:\n$A^{GAE(\\gamma,\\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta V_{t+l}$.\nAlternatively, this can be computed sequentially, namely,\n$A^{GAE}_t = \\delta_t + (\\lambda \\gamma) A^{GAE}_{t+1}$.\nGAE allows for direct computation of advantages, handling\nreward delays and noisy rewards more effectively, where\nRewards-to-Go are defined as\n$Rewards\\text{-}to\\text{-}Go = V + A^{GAE}$.\nGAE is used in PPO for policy updates as it provides low-\nvariance and high-bias advantage estimates. This involves\n1) Collecting trajectories using the current policy.\n2) Computing the advantages and rewards-to-go for each\nstate-action pair.\n3) Using the computed advantages to update the policy using\nthe PPO objective.\nThis combination allows PPO to leverage GAE's strengths,\nresulting in improved performance on complex RL tasks."}, {"title": "B. Challenges and Limitations in PPO Acceleration", "content": "Table I and Figure 1 highlight the time taken by different\ncomponents of the PPO algorithm in both CPU-only and\nCPU-GPU systems. The profiling was conducted on a high-\nperformance system with 32 Intel(R) Xeon(R) Silver 4216\nCPU cores @ 2.10GHz and a Tesla V100-SXM2-32GB GPU,\nusing the Humanoid environment by Gymnasium.\nThe data reveals that the environment run and the GAE com-\nputation phase consume a significant portion of the processing\ntime (47% and 30% respectively in CPU-GPU systems).\nKnowing that environments are typically high-level code com-\npiled to run on commodity CPUs and are independent of the\nRL algorithm, it's not feasible to build custom hardware that\ncan accelerate different environments.\nIn our work, we focused on exploiting the PPO character-\nistics to accelerate the algorithm itself. Developing custom\nhardware for the GAE phase and potentially other heavy\ncomponents of PPO and executing these computations on\nan SoC would reduce the communication overhead between\ndifferent systems like CPU, GPU, and DRAM. Replacing\nDRAM with on-chip memory for these operations would\nfurther decrease latency and improve data throughput, leading\nto significant performance gains in PPO training."}, {"title": "C. Related Work", "content": "This section discusses key contributions in hardware ac-\nceleration for RL, aligning with our work on HEPPO by\naddressing the computational demands of RL algorithms\nthrough innovative hardware designs, quantization techniques,\nand memory management strategies.\nKrishnan et al. introduced an RL training paradigm using\n8-bit quantized actors to accelerate data collection without\ncompromising learning convergence, achieving a 1.5x to 5.41\u00d7\nspeedup and reducing carbon emissions by 1.9x to 3.76x\ncompared to full-precision training [7]. Yang et al. employed\nfixed-point data types and arithmetic units for both training\nand inference, demonstrating a training throughput of 25293.3\ninferences per second (IPS), 2.7 times higher than a CPU-GPU\nplatform, and an energy efficiency of 2638.0 IPS/W, 15.4 times\nmore energy-efficient than a GPU [8]. These studies highlight\nthe effectiveness of quantization techniques in enhancing RL\ntraining speed and energy efficiency.\nMeng et al. (2020) targeted the inference and training\nphases of the PPO algorithm on a CPU-FPGA heterogeneous\nplatform, achieving throughput improvements of 2.1x-30.5\u00d7\nover CPU-only implementations and 2x-27.5\u00d7 over CPU-GPU\nimplementations [9]. Specific benchmarks showed a 23.5%\nincrease in throughput for Hopper and a 21.2% increase\nfor Humanoid with data layout optimization. Load balancing\noptimization led to improvements ranging from 9.3% to 28.3%\nin overall running average throughput.\nWeng et al.'s EnvPool addresses the bottleneck of slow envi-\nronment execution in RL training systems using a C++ thread\npool-based executor engine, achieving 1 million frames per\nsecond for Atari environments and 3 million frames per second\nfor MuJoCo environments on a NVIDIA DGX-A100 with 256\nCPU cores [10]. Dalton et al.'s CuLE platform leverages GPU\nparallelization to run thousands of Atari game environments\nsimultaneously, achieving up to 155 million frames per hour\n[11]. Liang et al. introduced a GPU-accelerated RL simulator\nusing NVIDIA Flex, achieving substantial improvements in\ntraining complex RL tasks and offering significant scaling\nbenefits with multi-GPUs [12]. These works underscore the\ncritical role of efficient environment simulation in enhancing\nRL training performance.\nTo the best of our knowledge, we are the first to specifically\ntarget the optimization of the critical GAE step in PPO, which"}, {"title": "D. Paper Contribution", "content": "Major contributions and innovations of this paper are\n\u2022 Enabling the integration of multiple custom hardware\ncomponents, memory, and CPU cores on a single system-on-chip (SoC) architecture, accommodating all phases of\nPPO from environment simulation to GAE computation.\nThis reduces communication overhead and enhances data\nthroughput and system performance.\n\u2022 Introducing dynamic standardization for rewards and\nblock standardization for values. This technique stabilizes\nlearning, enhances training performance, and manages\nmemory efficiently, reducing memory usage by 4x and\nincreasing cumulative rewards by 1.5x.\n\u2022 A parallel processing system that processes trajectories\nconcurrently, employing a k-step lookahead approach for\noptimized advantage and rewards-to-go calculations. Our\npipelined Processing Element (PE) can handle 300M\nelements per second, decimating the delay of GAE cal-\nculation and reducing PPO time by approximately 30%.\n\u2022 A memory layout system that organizes rewards, values,\nadvantages, and rewards-to-go on-chip for faster access.\nUsing dual-ported Block RAM (BRAM) to implement\na FILO storage mechanism, this system provides the\nrequired throughput each cycle, allowing overwriting of\nthe same memory locations for efficient data handling.\n\u2022 In-depth time profiling for the PPO algorithm revealing\nthat GAE computation is a major contributor to process-\ning time, accounting for 30% in CPU-GPU systems."}, {"title": "II. ALGORITHM MODIFICATION", "content": "We aim to achieve an optimally-reduced version of the\nPPO algorithm that can closely resemble the training behavior\nof the original algorithm, while allowing rescaling the input\ndata to the GAE calculation phase. This guarantees that any\ncomputation done to the input data will be independent of the\nused environments and hyperparameters as all inputs are re-\ndistributed evenly. To achieve our goal, several modifications\nhave been proposed and investigated as follows."}, {"title": "A. Dynamic Standardization of Rewards", "content": "The motivation behind standardizing the rewards (and later\nvalues) is to have a consistent and predictable distribution\nin which we can perform quantization. Applying traditional\nstandardization techniques has experimentally shown to cause\ntraining divergence. This is mainly because these methods\nindependently alter the distribution of rewards within each\ntraining epoch, disrupting the relative differences in reward\ndistributions between epochs and equalizing short-term and\nlong-term rewards, misleading the training.\nTo solve this problem, a novel standardization technique\nhas been developed and coined the name Dynamic Stan-\ndardization. The idea is that at each training epoch, reward\nstandardization shall be conducted while accounting for all\npreviously attained rewards. As it will be computationally and\nmemory intensive to store and reprocess all the rewards across\ntraining, a more efficient way is to store a running mean and\nrunning standard deviation that gets updated every epoch with\nthe new reward.\nTo update the running mean with every new reward, we\nfollow the equation\n$RunningMean_n = RunningMean_{n-1} + \\frac{r_n - RunningMean_{n-1}}{n}$,\nwhere n is the total number of rewards processed so far, $r_n$\nis the n-th reward, and RunningMean is the running mean\ncalculated up to the n-th reward.\nAs for the running standard deviation, inspired by Welford's\nalgorithm [13] [14] for dynamically calculating variance over\nmultiple iterations, the running variance for each new data\npoint has been computed as follows.\n1) Initialize $M_0$ and $S_0$ to 0.\n2) For each new reward $r_n$\n$M_n = M_{n-1} + \\frac{(r_n - M_{n-1})}{n}$,\nand\n$S_n = S_{n-1} + (r_n - M_{n-1}) \\times (r_n \u2013 M_n)$.\n3) The running standard deviation after n rewards is then\n$RunningSTD_n = \\sqrt{\\frac{S_n}{n}}$\nwhere $M_n$ is the running mean after n rewards and $S_n$ is\nthe cumulative value used for calculating variance."}, {"title": "B. Block Standardization of Values", "content": "Unlike rewards, the values are outputs of a trainable Neural\nNetwork (critic) that evolves differently over time and exhibits\nvarying distributions. This observation is illustrated in Fig-\nure 2, which shows the distribution of values across a selected\nset of trajectories during training.\nDynamic standardization of values was unsuccessful as\nit affected the loss calculations. Instead, a more adaptable\nstandardization method is required to handle these variations\neffectively while keeping a history of their original distribution\nto project them back in place. To address this, we propose\na block standardization technique that quantizes values in\nbatches. The steps involved in this process are as follows:\n1) Batch Collection: Collect a batch of values from multiple\ntrajectories.\n2) Compute Statistics: Calculate the mean ($\\mu_v$) and stan-\ndard deviation ($\\sigma_v$) for each batch.\n3) Standardization: Scale values to have a mean of zero\nand a standard deviation of one by subtracting each the\nmean $\\mu_v$ from each element in the block and then dividing\nby standard deviation $\\sigma_v$.\n4) Uniform Quantization: Quantize standardized values\nuniformly, storing them with $\\mu_v$ and $\\sigma_v$.\n5) Reconstruction: De-quantize and convert values back to\nthe original scale using the stored statistics.\nThis method leverages the similar distribution of trajectories\ncollected at the same point in training, allowing for adaptive\nquantization based on the actual mean and standard deviation\nat that moment. The effectiveness of this method was validated\nthrough a series of experiments, demonstrating its robustness\nto shifts in training dynamics and its efficiency in utilizing\nmemory bandwidth."}, {"title": "C. Quantization of Rewards and Values", "content": "Due to the memory bottlenecks discussed in Section IV, it\nis impractical to use 32-bit floating-point representation for\neach element in the Rewards and Values vectors. Instead, we\nadopt a quantization strategy tailored separately for rewards\nand values.\n1) Quantization of Rewards: After applying dynamic stan-\ndardization, rewards are centered around zero with a unit\nstandard deviation. They are then uniformly quantized using\nn-bit codeword, mapping continuous values to discrete levels.\nDuring reconstruction, rewards are retrieved from memory, de-\nquantized, and used in their standardized form. Experimental\ntesting showed that leaving the rewards in their standardized\nform enhances the cumulative rewards by around 50% as\nshown in section Section V.\n2) Quantization of Values: Similar to the Quantization of\nRewards, After applying block standardization, the values are\nuniformly quantized using n-bit codeword. During reconstruc-\ntion, we also fetch and de-quantize the values. However, the\nmain difference is that we have to do a final de-standardization\nstep shifting the distribution back to its original form. This is\ndone by multiplying the elements in v back by the stored\nstandard deviation $\\sigma_v$ and then adding the mean $\\mu_v$."}, {"title": "III. HEPPO ARCHITECTURE DETAILS", "content": "The proposed architecture integrates the whole PPO pipeline\nin a single SoC, reducing latency and communication overhead\ncompared to traditional CPU-GPU systems. Figure 3 illus-\ntrates the connections between the Processing System (PS),\nProgrammable Logic (PL), and BRAM within the SoC."}, {"title": "A. Data Flow, Processing, and Efficiency", "content": "The PS access the BRAM for reading and writing via the\nAXI Interconnect, ensuring seamless data exchange with the\ncustom logic in the PL. This integration keeps critical data\non-chip, reducing the need to access external DRAM and\nenhancing throughput. Unlike traditional CPU-GPU systems\nwhich require frequent data transfers between the CPU, GPU,\nand DRAM, leading to higher latency and communication\noverhead. Detailed Processing Stages are as follows.\n\u2022 Data Preparation and Initiation: Processed data is stored\nin BRAMs, and the PS communicates to the FPGA with\nan initiate signal.\n\u2022 Advantages and RTGs Calculation: The FPGA fetches\nthe data, performs de-quantization, calculates advantages\nand rewards-to-go (RTGs), writes back, and sends a\ncompletion signal back to the PS.\n\u2022 Actor-Critic Losses Calculation: The PS retrieves the\ncomputed advantages and RTGs from the BRAMS and\ncalculates actor-critic losses.\n\u2022 Back Propagation and Networks Update: The PS sends\nthe losses to the FPGA to perform backpropagation to\nupdate the neural networks. Updated network parameters\nare then used for subsequent iterations."}, {"title": "B. Advantage Estimate Decomposition and k-step Lookahead", "content": "In RL, the advantage estimate A(t) is a critical component\nfor policy improvement. Using Equation 4, we can decompose\nthe advantage estimate calculation as shown in Table II.\nwhere C is a constant defined as $\\gamma \\cdot \\lambda$.\nThis decomposition shows how each advantage estimate\ndepends on future values. An efficient way that developers\nuse is to compute the estimates in revere order from t = N\nto t = 1 to avoid recalculating the same terms multiple times,\nthus saving computational resources.\nSingle Cycle Implementation and Pipelining: The single-\ncycle GAE unit can be represented with potential pipelines\nhighlighted in dashed green, as shown in Figure 4. In this im-\nplementation, various stages of the computation are pipelined\nto improve efficiency. However, when we attempt to pipeline\nthe feedback loop (highlighted in red), it introduces bubbles\ninto the system. These bubbles represent idle states where\nthe pipeline must wait for data from previous stages, severely\nreducing efficiency.\nk-step Lookahead Solution: The k-step lookahead method\naddresses this inefficiency by introducing registers (delays) in\nthe feedback loop. This approach can be explained through the\nmodified advantage estimate calculations for 2-step lookahead\n$\\hat{A}_t = C^2\\hat{A}_{t+2} + C\\delta_{t+1} + \\delta_t$,\nand for 3-step lookahead,\n$A_t = C^3A_{t+3} + C^2\\delta_{t+2} + C\\delta_{t+1} + \\delta_t$.\nFigure 4 illustrates the implementation of a 3-step looka-\nhead. As shown, three registers are added to the feedback loop\nto apply 3-step lookahead transformation (highlighted in yel-\nlow). The added registers on the feedback loop can be moved\ninside the multiplier, enabling embedding pipelined multiplier\nthrough DSP blocks. This solution enables a fully pipelined\nprocessing, eliminating compute bubbles in the feedback loop.\nThe following is a general equation for the k-step lookahead,\n$A_t = C^k A_{t+k} + \\sum_{i=0}^{k-1} c^{(k-1)-i}\\delta_{t+i}$,\nfacilitating the incorporation of additional registers on the\nfeedback loop, and a more profound pipelining of the multi-\nplier. Although the 2-step lookahead transformation is satisfac-\ntory for enabling our system to operate at the highest frequency\nand attain the peak performance, alternative systems, notably\nthose with wider data formats, might necessitate more pipeline\nstages to operate at the maximum achievable frequency."}, {"title": "C. System Architectue", "content": "Figure 5 (a) shows the micro-architecture of HEPPO, which\nconsists of Rewards Loaders (ReLs), Values Loaders (VaLs),\nand compute Processing Elements (PEs) forming a one-\ndimensional systolic array with N rows.\nParallelization: Rows in the systolic array run concurrently\nand independently, each processing distinct vectors from dif-\nferent agents assigned by a round-robin fashion. When one\nrow finishes, it gets a new set of vectors. This parallel archi-\ntecture enhances HEPPO's efficiency and scalability. While\nthe BRAM stack memory enables substantial data transfers,\na crossbar network ensures robust connections between ReLs,\nVaLs, and PEs to the BRAM stack memory.\nData flow: Each ReL reads element Ri from the rewards\nvector and sends it with index i and the signal Done to VaL.\nVaL fetches the corresponding i-th value Vi and sends Ri,\nVi, i, and Done to the PEs. The PE calculates the Advantage\nEstimate (Adv) and Rewards-to-Go (RTG) and writes them\nback to the main memory at index i."}, {"title": "IV. DATA LAYOUT", "content": "To enhance the efficiency of the Proximal Policy Optimiza-\ntion (PPO) algorithm, we propose a memory layout that orga-\nnizes rewards, values, advantages, and rewards-to-go on-chip\nfor faster access. This layout groups data from different tra-\njectories with the same timestep into memory blocks, enabling\nsimultaneous retrieval and processing. Additionally, it employs\na First-In-Last-Out (FILO) storage mechanism to align with\nthe backward iteration required for GAE calculations. This\nsection details the memory organization, access patterns, and\nbandwidth considerations."}, {"title": "A. Memory Bandwidth Bottleneck", "content": "In a typical large RL setup with 64 trajectories and 1024\ntimesteps, both rewards and values are stored in 32-bit\nfloating-point format. The required memory per timestep for\n64 trajectories (128 elements) is 512 bytes.\nFor parallel processing, these 512 bytes need to be fetched\nfrom memory per clock cycle. Assuming a typical DDR4\n3200 bandwidth of 25 GB/s and a clock frequency of\n300 MHz, the available bandwidth per cycle is calculated as\n$Bandwidth \\text{ per cycle } = \\frac{25 \\times 10^9 \\text{ bytes/s}}{300 \\times 10^6 \\text{ cycles/s}} = 83.3 \\text{ bytes/cycle}$.\nThis results in a shortfall of 428.7 bytes per cycle. Clearly,\nDRAM cannot supply enough data to sustain 64 parallel\nprocessing elements (PEs), severely limiting parallelization. To\novercome this bottleneck, we store data in on-chip dual-port\nBlock RAM (BRAM), which meets the required 512 bytes per\ncycle, ensuring high-throughput parallel processing.\n1) Memory Block Layout: The memory layout is structured\nas 2D arrays, with dimensions representing timesteps and\ntrajectories. Each memory block stores specific data (rewards,\nvalues, advantages, or rewards-to-go) indexed by timestep and\ntrajectory. This layout enables parallel processing of different\ntrajectories using the same fetched block which improves\nefficiency and lower memory accesses."}, {"title": "Algorithm 2: PPO/GAE Memory Layout and Processing", "content": "1 Initialization\n2 Initialize memory blocks RMB, VMB, AMB, RTGMB\n3 Data Insertion\n4 foreach timestept do\n5 foreach trajectory i do\n6 Push reward[i][t] into RMB[t][i]\n7 Push value[i][t] into VMB[t][i]\n8 GAE Calculation and In-Place Update\n9 foreach timestep t, backward do\n10 foreach trajectory i do\n11 Retrieve reward from RMB[t][i]\n12 Retrieve value from VMB[t][i]\n13 Compute advantage and reward-to-go\n14 Store advantage in AMB[t+1][i]\n15 Store reward-to-go in RTGMB[t+1][i]\n2) FILO Storage Mechanism: The FILO storage mecha-\nnism uses a stack-based structure to store rewards and values:\n\u2022 Push Operation: Rewards and values are pushed onto the\nstack at each timestep.\n\u2022 Pop Operation: Rewards and values are popped from\nthe stack during GAE calculation, starting from the last\ntimestep and iterating backward.\n3) In-Place Updates and Dual-Port Memory: The sys-\ntem uses dual-port memory for simultaneous read and write\noperations, enabling in-place updates where advantages and\nrewards-to-go can overwrite the original rewards and values\nreducing memory usage by half.\nAlgorithm 2 is implemented to manage the FILO stack\nstructure in BRAM, ensuring efficient data access patterns\ncompatible with the hardware architecture.\nThis proposed design ensures fast data retrieval and process-\ning that allows continuous feeding of data to the PEs, keeping\nthem always busy."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "The results of our work are divided across multiple axes.\nIn this section, we will discuss each axis and how they can\naffect the overall acceleration of the PPO algorithm."}, {"title": "A. Dynamic Rewards Standardization", "content": "It's important to note that, in most PPO implementations, the\nfinal calculated advantage vector is standardized to stabilize\ngradient updates and ensure smoother and more consistent\ntraining. This practice has become widely adopted due to its\npositive impact on training dynamics, as highlighted in various\nimplementations and community discussions [15], [16].\nFigure 7 shows training outcomes in the Humanoid environ-\nment (Gymnasium toolkit). Our PPO version (with and with-\nout standardized advantages) achieved over 1.5x increase in\ncumulative rewards compared to the original PPO, continuing\nto improve after the original PPO plateaued. This improvement\nwas consistent across MuJoCo and Atari environments, con-\nfirming our modification benefits both hardware and training."}, {"title": "B. Quantization of Rewards and Values", "content": "Optimal Quantization Size: Detailed investigation, shown\nin Figure 8 and Figure 9 shows that quantizing using 5 and\n7 bits performed the poorest followed by 3, 4 which are near\nto the baseline (PPO + DS). Finally, quantizing with 6, 8 to\n10 performed equally higher than the baseline. The reason\nwhy using 5 and 7 bits performed worse than 3 and 4 in\nsome of the trials and better in others is most likely due to\nthe inherent variance of the policy gradient algorithm and the\nprobabilistic nature of RL. To avoid this unstable region, it was\nconcluded through all trials that 8 bits and above can be seen\nas a threshold for a stable uniform quantization that archives\nhigh performance."}, {"title": "C. Summary of Rewards and Values Quantization Approaches", "content": "To summarize the effects of various quantization approaches\non PPO performance, we conducted several experiments with\ntheir configurations shown in Table III.\n\u2022 Experiment 1: Baseline PPO without quantization.\n\u2022 Experiment 2: Dynamic standardization of rewards.\n\u2022 Experiment 3: Both rewards and values are standardized\nand uniformly and quantized by block to 8-bit codewords.\n\u2022 Experiment 4: Both rewards and values are standardized\nand uniformly quantized by block, with rewards kept\nin standardized form throughout computations.\n\u2022 Experiment 5: Dynamic standardization applied to re-\nwards and block approach to values.\nThese experiments highlight the importance of dynamic\nstandardization and appropriate quantization techniques in"}, {"title": "D. Hardware Implementation of HEPPO", "content": "A parameterized Verilog model of HEPPO's proposed\npipelined architecture, as described in Subsection III-C, has\nbeen developed with a data width of 32 bits (after fetching\nand de-quantizing the elements). The AMD-Xilinx_Zynq\u00ae\nUltraScale+TM MPSoC ZCU106 Evaluation Kit was chosen\nto host our implementation. This device integrates a quad-\ncore Arm\u00ae Cortex\u2122-A53 processing system (PS) and a\ndual-core Arm Cortex-R5 real-time processor, providing the\nnecessary computing for running the environment. The FPGA\nfabric within the Programmable Logic (PL) provides extensive\nresources for custom logic implementation, including neural\nnetwork inference and GAE computation.\nFor the DNN inference within the PL, we adapt the systolic\narray implementation introduced by Meng et al. (2020) [9].\nTheir design achieves a clock frequency of 285 MHz, whereas\nour overall system is designed to run at 300 MHz. As all\ndesign subsystems operate sequentially (processing does not\noverlap in time), it's advantageous to enable each subsystem\nto run at its highest frequency. While this creates multiple\nclock domains, data synchronization is not required because\nall subsystems operate sequentially and communicate through\nBRAMS. However, control signals across domains, such as\nthose indicating that processing has ended and data is ready,\nstill need to be synchronized.\n1) Area Utilization: Figure 11 illustrates the resource uti-\nlization percentages for 1-step, 2-step, and 3-step lookahead\nimplementations per Processing Element (PE). As seen, there\nis a quadratic increase in resource usage with each increase in\nn. This figure highlights how the increase in lookahead steps\n(n) impacts the utilization of various resources (LUTs, FFs,\nand DSPs), demonstrating a clear quadratic trend. Based on\nour implementation, we found that n > 1 allows the system to\noperate at a maximum frequency of 300 MHz. This is due to\nthe intensive pipelining and absence of pipeline stalls. Hence,\na single PE is estimated to handle 300 million elements per\nsecond with the continuous data flow supported by the efficient\ndesign of the FILO BRAM memory system which ensures\nthe required throughput every cycle. This is in contrast to a\nnormal CPU-GPU system that suffers from DRAM memory\naccess latency, buffering, and scheduling, all of which are a\ngreat bottleneck.\nWe choose to work with 2-step lookahead. The resource"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "We introduced HEPPO (Hardware-Efficient Proximal Policy\nOptimization), an FPGA-based implementation designed to\naccelerate the GAE stage of the PPO algorithm. HEPPO\nutilizes dynamic reward standardization and 8-bit uniform\nquantization, reducing memory usage by 4x and increasing\ncumulative rewards by 50%.\nOur innovative memory layout system, using FILO storage\nand dual-port memory, efficiently handles rewards, values,\nand advantages, ensuring high throughput and compact data\nmanagement. The ultra-pipelined Process Element (PE) unit,\noperating at 300 MHz, greatly enhances throughput and ef-\nficiency, outperforming conventional CPU-GPU systems and\nimproving PPO training efficiency by an estimated 30%.\nHEPPO leverages AMD-Xilinx Zynq UltraScale+ MPSoC's\ncapabilities, integrating environment simulation, neural net-\nwork inference, backpropagation, and GAE computation on\na single SoC. This minimizes communication latency and\noptimizes data handling which originally accounted for around\n11% of the training.\nFurther incremental optimizations of HEPPO's SoC are pos-\nsible. Overclocking techniques [18] and bit-serial computation\n[19] in FPGAs can be employed to accelerate overall process-\ning. To mitigate power consumption, multiple clock domains\ncan be implemented for the ARM cores, the DNN, and the\nGAE calculations. High-performance clock-domain crossing\n(CDC) FIFOs can facilitate faster data transfers [20]\u2013[24].\nAdditionally, hardware-efficient data compression methods,\noptimized for deep learning workloads, can be leveraged to\nminimize data transfers [25]\u2013[27].\nFuture work should focus on optimizing custom hardware\nfor other phases of the PPO algorithm, particularly in ac-\ncelerating environment simulation, which consumes 47% of\nthe training time. Investigating techniques for dynamic High-\nLevel Synthesis of environments on FPGA and implementing\nloss calculation on FPGA could eliminate the need for CPU\ncores, significantly boosting the computational efficiency of\nthe algorithm."}]}