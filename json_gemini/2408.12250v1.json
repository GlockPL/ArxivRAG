{"title": "Can Artificial Intelligence Embody Moral Values?", "authors": ["Torben Swoboda", "Lode Lauwaert"], "abstract": "The neutrality thesis holds that technology cannot be laden with values \u2013 it is inherently value-neutral. This long-standing view has faced critiques, but much of the argumentation against neutrality has focused on traditional, non-smart technologies like bridges and razors. In contrast, AI is a smart technology increasingly used in high-stakes domains like healthcare, finance, and policing, where its decisions can cause moral harm. In this paper, we argue that artificial intelligence (AI), particularly artificial agents that autonomously make decisions to pursue their goals, challenge the neutrality thesis. Our central claim is that the computational models underlying artificial agents can integrate representations of moral values such as fairness, honesty and avoiding harm. We provide a conceptual framework discussing the neutrality thesis, values, and AI. Moreover, we examine two approaches to designing computational models of morality, artificial conscience and ethical prompting, and present empirical evidence from text-based game environments that artificial agents with such models exhibit more ethical behavior compared to agents without these models. The findings support that AI can embody moral values, which contradicts the claim that all technologies are necessarily value-neutral.", "sections": [{"title": "1. Introduction", "content": "One of the most well-known theses about technology is that it is neutral, also known as the neutrality thesis. The thesis has been discussed by many academics (Floridi & Sanders, 2004; Heyndels, 2023; Klenk, 2021; van de Poel and Kroes, 2014; Tollon, 2021; Pitt, 2014). Philosopher Joseph Pitt's 2014 paper titled \u201cGuns don't kill, people kill\"; values in and/or around technologies' is widely regarded as one of the key defenses of the neutrality thesis.2 One of Pitt's arguments is that it is impossible to perceive the alleged ladenness of an artifact. One can only see pure matter, and thus, technology is neutral. In contrast, one could say that technology may be neutral but it is not the case that technology is necessarily neutral."}, {"title": null, "content": "Yet, delving into the literature on the question whether or not technology is neutral, one can see that the argumentation against the neutrality thesis is (most often) based upon technologies that are not equipped with artificial intelligence (AI). Examples include prehistoric inventions like a bow-and-arrow or spear, Robert Moses' \u2018racist' bridges as described by Langdon Winner (2017),\u00b9 and the razors from the 1980s from the Dutch company Philips that were seen as sexist, since there were two types: one that could be repaired (for men), one that could not be repaired (for women). There are far fewer papers that focus on smart technologies within the context of the discussion of the neutrality thesis (see e.g. Gabriel and Ghazavi, 2022; van de Poel, 2020).\nFrom one point of view, it is surprising that there's not that much academic literature about the question whether or not Al systems are neutral. Bias and discrimination are two hot topics within the realm of AI ethics, and with, for example, a leading study such as Weapons of Math Destruction by Cathy O'Neil (2017), scholars have become familiar with examples of intelligent systems that treat people of color differently than white people, and thus that are not neutral. And yet, that is not the end of the debate. After all, if today it is concluded that some AI is not neutral, the technology is usually said to be not neutral in an undesirable way, namely because the system produces sexist or racist output. Little or no attention has been paid to the ladenness of AI in a desirable way. Furthermore, the debate is also mostly about undermining what we call a broad interpretation of the neutrality thesis', which holds that technology is not laden with a wide range of cultural products: stereotypes, ideologies, norms, perspectives, etc. The question is generally not so much about \u2018a narrow interpretation of the neutrality thesis', which is only about moral values, and which holds that AI systems are independent from moral values, such as non-maleficence, fairness, or honesty.2 In other words, up until now, not that much has been said about the specific question about whether AI in a desirable sense is laden with a moral value.\nFocusing on this last question, at first glance it seems that it should be answered in the affirmative. For instance, privacy is a moral value and we could develop an AI system in a way that is privacy-proof, in the sense that the data used to train the system are given with consent. Consequently, one can conclude that AI is not necessarily neutral in a desirable sense."}, {"title": null, "content": "However, value-ladenness here has to do primarily with the data the system is trained with, not with a specific ability that some Al systems have, namely, to make decisions. In other words, so far, the question has not so much been on the moral ladenness of the learned AI model that produces the output. This makes clear that at least one fundamental question has been understudied in the field of AI ethics to date. Several other questions are already widely discussed, such as, to name just a few, whether it is possible to develop machine consciousness (Schneider, 2019), whether machines can think (Searle, 1980), or whether it makes sense to attribute rights to machine learning systems (Gunkel, 2018). The question that we now want to push to the forefront is the following: how (if at all) can AI be laden with a moral value in a desirable sense?\nThis question is relevant for at least two reasons. The first justification for examining whether algorithms can be laden with a moral value is pragmatic. For example, the limited functionality of tenant screening tools may associate arrest records with wrong individuals and facial recognition errors can lead to wrongful arrests (Raji et al., 2022). Language models, used in chatbots, produce toxic language, including racist and sexist statements (Gehman et al., 2020). Generative Al is also used to create deepfakes, i.e. synthetic media like images, audio, and video that appears highly realistic. Celebrities, like Taylor Swift, have become the target of sexual deepfakes and scammers utilize voice cloning to trick their victims more effectively (Hsu, 2024). These examples illustrate the morally harmful impact AI systems can have. One possible response to prohibit such outcomes, which is one of the central topics within the field of value alignment and that has also been discussed by Nick Bostrom in Superintelligence (2014), is to embed moral values into the technology (Anwar et al., 2024, pp. 79f). After all, a value-laden system carries the promise to produce output that is in line with moral values such as fairness and honesty. However, if proponents of the neutrality thesis were correct then value alignment would be conceptually impossible and research efforts in this direction futile. Thus, it is important to establish whether it is possible to design AI systems that adhere to moral values.\nThe second reason is to critically evaluate Pitt's argumentation by relating it to AI. More precisely, Pitt argues that values are inherently linked to decision-making and pursuing goals. People can make decisions, and since decision-making is linked to moral values, people can embody such values. But technologies like razors or sea dykes are not capable of making decisions and, therefore, they cannot be imbued with values. However, Al engages in decision-"}, {"title": null, "content": "making, which undermines Pitt's argument. Therefore, it seems that AI is a special kind of technology as it possesses certain capabilities that make it more prone to be laden with values compared to other kinds of technology.\nThe main aim of this paper is to delve into the question whether an AI system can be laden with a moral value in a desirable way. Before we can present our argument, some preliminary conceptual work is needed. For example, we need to explain what we mean by artificial agents and why we are focusing on this type of AI system. In addition, we should clarify some point regarding the neutrality thesis and also address the functionalist conception of values on which our argument relies. This is what we will do in the second section, called 'conceptual clarifications'. The third section is argumentative in nature and also the central part of the paper. There we show that it is in principle possible that AI's algorithms and computational models are not neutral, and that has to do with an AI system's ability to choose between options. Our conclusion, thus, will be that the equipment of the technology with decision making capacities is, compared with non-smart technologies, as such an extra reason why the system can be used as an argument against the neutrality thesis.\nThis already points to the twofold added value of our paper compared to the few other papers on the same topic. First, we empirically support the central claim that AI can be value-laden, and second, we connect the claim that AI is not necessarily neutral to a distinctive feature of smart technology, namely the ability to make decisions. In this respect, our paper is aligned with and deepens the work of Gabriel and Ghazavi (2022) and van de Poel (2020) we will return to this in more detail later."}, {"title": "2. Conceptual clarifications", "content": "The main purpose of this paper is to show that also technologies equipped with AI can be used as an argument against the neutrality thesis. More precisely, we will argue that a computational model - and thus AI's capability to choose between options - can be laden with a moral value. But before we can demonstrate that, we must clarify, first, how we understand the neutrality thesis, second, what our approach to values is, and third, what we mean by \u2018AI\u2019."}, {"title": "2.1. The meaning of the neutrality thesis", "content": "When defending the value neutrality thesis people often use terms such as \u2018laden' or \u2018hanging on' (Klenk, 2021; van de Poel and Kroes, 2014). Technology is not laden with values; no values are hanging on a hammer or car, so the story goes. It goes without saying that these terms are used here in a figurative sense. No one, including the scholars who reject the value neutrality thesis, is claiming that values are literally in technology and actually attached to it. Indeed, by the term 'value', for example, scholars are referring to a state of affairs in reality outside of technology. Justice normally has to do with the relationship between persons, just as sustainability refers to the relationship between organisms; these values are not part of technology, they are separate from it.\nThe question then is: what does it mean when one says that technology is not laden with a value? The metaphorical interpretation is that there is no value 'in' the design. Both technology and value can be linked to each other, but only in the use phase. As such, all technology is independent from values. This implies, so the interpretation goes, that a design as such gives no reason to believe that the state of affairs that will be realized by using the technology will be in line with a value. What results from the use of a technology may be in line with a value, but that is not what you expect based solely on the construction of the bridge or chair. The alignment results from the use, and not the design, of the technology.\nIt is relevant to note that the majority of scholars give a non-literal interpretation to the statement about the non-laden nature of technology. Indeed, it follows that the observation that a value cannot be detected in technology is not sufficient to overturn the thesis. This is also the error made by Pitt (2014). He observes (correctly) that one cannot perceive value in technology, and that therefore technology is neutral. However, that conclusion presupposes a literal interpretation of the neutrality thesis, and Pitt is virtually alone in this. Furthermore, a non-literal interpretation also implies that critics of the neutrality thesis must demonstrate that the design of a technology, in itself, already provides good reasons to believe that the effects of a technology align with a value, in other words, that the likelihood of value alignment is high when the technology is used."}, {"title": "2.2. A functionalist approach to values", "content": "Earlier, we made clear that we focus on values, and more specifically those values that are moral in nature. A value can be defined as a state of affairs that is believed to be good, in such a way that one has to take them into account when, say, making a decision. This implies that values have a function, in the sense that they discriminate between states. For our counterargument, it is now necessary to explain this functionalist approach further.\nIn general, values are fundamental for decision-making and shape the choices we make in our interactions with the world. Decision-making is concerned with the selection of an option from a set of available options. For example, when faced with the decision what to cook, person X might consider what nutritional value a dish offers, or perhaps X is under time pressure so they value a dish that takes little time to prepare. Or perhaps X values a communal dining experience, so they opt for a dish that they can share with my companions. Values create a ranking of options based on how desirable they are, and they motivate people's choices. After all, it makes little sense to value time efficiency and then cook a laborious risotto when one could make a sandwich as well.\nValues with a moral character - the kind of values that are crucial for our discussion - have the same role in ethical decision-making. Imagine that you are in a bookstore browsing the stock where you come across a book that captivates your attention. Unfortunately, the book is way too expensive for you at the moment. You notice that the storekeeper is busy with another customer and there are no cameras in the store. It would be easy to slip the book into your bag and exit the store without even arousing suspicion. But you are an honest person. Hence, stealing the book would be incongruent with one of your values, because stealing would require you to deceitfully acquire the property that belongs to the owner of the bookstore. The alternative options of buying the book or leaving the store without the book are decisions that you can make that do not violate your values. Thus, one can expect that you prefer either of these options over stealing the book.\nDifferent moral values create different rankings of options, and we often have to compromise between different moral values and find preferable trade-offs. Consider that a politician faces a challenging decision regarding a proposed chemical plant near a pristine forest area. The industrial project aims to produce chemicals and create local jobs but raises concerns about potential deforestation and water contamination from hazardous chemical releases. The politician values both ecological conservation, wanting to minimize environmental harm, and human economic well-being through job creation. They could reject the plant due to contamination risks, approve it with a low-quality filtration system seeking economic benefits,"}, {"title": null, "content": "or consider a compromise. The latter involves approving the plant only with a high-quality filtration system to remove chemicals from wastewater and a reforestation commitment to mitigate environmental impact. Initially ranking rejection as best for conservation or approval as best for the economy, the politician opts for the balanced approach, pushing for approval with safeguards.\nThe described account of values is similar to the approach developed by Pitt (2014). He argues that values form the basis for decision-making and shape the choices we make in our interactions with the world. When one has to choose between two or more options, one will appeal to values to justify their choice. This is the case because \u201ca value is an endorsement of a preferred state of affairs by an individual or group of individuals that motivates our actions\u201d (Pitt, 2014, p.91). According to Pitt, values have two core components. First, endorsing a value means that there is a preferred state of affairs. Values thus create a ranking of states of affairs based on how desirable they are according to a value. Second, values have the function to motivate our actions. When we commit ourselves to a value and thereby to a preference ranking of state of affairs, we are committed to acting in such a way that brings the desired state of affairs about.\nThis explains why Pitt is skeptical of the typical cases put forward against the neutrality thesis: a bridge does not endorse any state of the world over another state of the world, nor is it motivated by any value. Or take the famous Moses example. The architect may be racist and endorse the design of a bridge that is optimized to discriminate against black people, but that only informs us about the values and decision process of the architect. The bridge itself does not make a single decision. However, this line of reasoning does not extend to AI because in this case it is the technology that makes decisions. Decisions, in turn, assume the ranking of options ('state of affairs', as Pitt calls them, 2014) and thus values."}, {"title": "2.3. Artificial Agents", "content": "When in this paper the term \u2018AI' is used, it refers to machine learning models that are used, among other things, to analyze information, identify patterns, and generate decisions based on the available data (Goodfellow et al., 2016). The overarching idea of such models revolves around the process of decision-making. The ability to make decisions is what distinguishes modern machine learning from a chair or bicycle."}, {"title": null, "content": "However, we do not focus on all forms of machine learning, but specifically on artificial agents. In contrast to non-agentic artificial systems, artificial agents are goal-directed entities that interact with an environment to autonomously achieve their goals (Butlin, 2024; Kenton et al., 2022). These systems can be physically embodied in the form of drones, robotic vacuum cleaners, or cars. Alternatively, they can inhibit virtual or digital domains, like video games, simulations, and social media platforms. Artificial agents also possess some degree of autonomy, i.e., the ability to adapt to changes in the environment without the need for human intervention; they thus put humans out of the loop in their operation. Agentic systems may be deployed for a wide range of tasks like cyber defense (Lohn et al., 2023, Feng et al., 2023), supply chain management (Gijsbrechts et al., 2018), or making profits in financial markets (Wellman and Rajan, 2017).\nAt present there are two popular approaches to design artificial agents that we will come back to later. The first is the reinforcement learning framework (Sutton and Barto, 2018). By iteratively interacting with the environment, reinforcement learning agents learn which actions lead to favorable outcomes by receiving a numerical reward. The reward encapsulates how desirable the state of affairs is. Since reinforcement learning aims to develop a model that maximizes the reward function, the learning process allows the agent to adapt and improve their decision-making over time, aligning their actions with their goals. For instance, DeepMind developed a system that increases the cooling efficiency of Google's data center (Evans and Gao, 2016). It has also allowed systems to achieve above human-level gameplay by discovering highly skilled behaviors in games like chess and Go (Silver et al., 2018; Schrittwieser, 2020).\nThe second approach utilizes large language models (LLMs) as the cognitive processing unit (Sumers et al., 2023). LLMs take text as input and output text. To transform language models into agents, the language model is put into a feedback loop with the environment. Observations of the environment are transformed into textual data that is fed into the language model. The corresponding output is interpreted as the action that the agent performs. More sophisticated approaches extend this framework by adding external memory, where past experiences are stored and can be retrieved at later stages to plan future actions (Park et al., 2023). Other initiatives allow agents to write their own code that is added to a skill library that the agent then can execute. Voyager is such an agent that writes its own programs that allow it to explore the"}, {"title": null, "content": "sandbox game Minecraft and engage in long-horizon tasks like harvesting resources, fighting enemies, and building a base (Wang et al., 2023).\nArtificial agents may encounter ethically charged situations (Thoma, 2022; Wellman and Rajan, 2017). That is to say that the decision that an artificial agent makes has ethical implications and it is possible to evaluate the various choices an artificial agent could make in these situations in terms of their ethical desirability. Consider that an artificial agent operating on financial markets could engage in spoofing, a deceptive strategy that involves placing large buy or sell orders with the intent of misleading other market participants about the true supply and demand for a security. The agent then cancels these orders before they are executed, potentially causing artificial price movements that benefit the agent. Such market manipulation tactics are unethical and should not be engaged in by artificial agents. Another case are autonomous weapon systems that select targets and use lethal force. Such a system needs to adhere to the jus-in-bello principles, which include that not all targets are legitimate; civilians are typically considered non-legitimate targets.\nWhy are we focusing on artificial agents? First, the moral impact of the aforementioned machine learning applications is typically mediated by humans. A lack of competency regarding moral decision-making can in that case be compensated by humans. In contrast, agentic AI pursues goals without direct human supervision and without the ability to intervene at critical moments. For this reason, it is important that these systems have moral values embedded to ensure that their behavior is congruent with our ethical standards. Second, as research improves the capabilities of agentic systems they will increasingly automate more complex tasks across the economy, including those that involve an ethical component. Given the rapid pace of development in the field of AI, it is important to anticipate harms (Chan et al., 2023). For example, in healthcare, agentic systems could be tasked with dynamically allocating limited medical resources like hospital beds, ventilators, or organ transplants across patient populations based on complex prioritization criteria. In the financial sector, autonomous Al agents may be empowered to make high-stakes trading decisions, investment allocations, and loan approvals impacting economic stability and equitable access to capital. Within the criminal justice system, agentic AI could potentially assist in judging, sentencing, and parole determinations - decisions with immense ethical ramifications around human rights and racial biases. Even in sectors like urban planning and transportation management, agentic systems will be asked to optimize traffic flows, infrastructure utilization, and city development in ways"}, {"title": null, "content": "that raise ethical considerations around equity, environmental impact, and community shaping. Third, how agents are supposed to achieve their goals is typically underspecified. This allows agents to develop novel strategies that may surpass human-level capabilities and surprise its developers. For instance, AlphaGo's move 37 against Lee Sedol is not a move that a human would play but AlphaGo won the game (Holcomb et al., 2018). On the flip side, the underspecification can also allow agentic systems to find unwanted pathways to reach their goal (Krakovna et al., 2020). This becomes especially vital when agentic systems are deployed in a high-stakes context and when socially undesirable effects are not explicitly excluded in the goal specification. Concluding, artificial agents present the most important case when it comes to disproving the value neutrality thesis. The reason is that these systems have the capacity to directly cause moral harm that can be avoided by embedding moral values that ensure that their behavior is aligned with ethical principles."}, {"title": "3. AI Can Embody Moral Values", "content": "We now have sufficient background to present our main point, which is that it is possible that AI may not be neutral, and more specifically, that artificial agents may be laden with moral values such as non-maleficence, fairness, and honesty in a desirable way. Specifically, AI systems can be designed to make choices that are congruent with particular moral values. Through techniques like artificial conscience and ethical prompting, which we will discuss in 3.2, a computational model of morality is integrated into the decision-making process of an agent that steers the system to rank the available choices based on their moral desirability and select the option that best upholds the targeted moral values. Put differently, AI can be engineered to embody and operationalize particular moral values rather than being purely value-neutral.\nWhen we consider the possibility of embedding moral values in AI then what we are primarily interested in is the function of moral values i.e., creating a moral ranking of options and selecting the highest ranking one. However, to refute the neutrality thesis, it is not sufficient to illustrate that AI can make decisions that are aligned with moral values. For instance, Moor (2006) points out that an ATM makes decisions aligned with the moral value of honesty, but the value of honesty is not embedded in an ATM. That is the case because the ATM is simply instructed to display the correct information associated with a client's account or to give out money that is specified by the customer. What the ATM is lacking is an explicit representation"}, {"title": null, "content": "of moral concepts on which it is operating. Hence, an ATM is what Moor calls \u2018an implicit moral agent' (Moor, 2006).\nIn contrast, explicit moral agents possess an internal representation of morality. This internal representation may come in the form of an expert system or it may be implemented via a machine learning model, like an artificial neural network. Either way, the internal representation is a computational model of morality, meaning that it creates a mapping being a morally laden decision context and the available options. Ultimately, such a model of morality develops the ranking prescribed by a moral value - or a combination of multiple moral values. To illustrate this point, consider the bookstore case that we described earlier. A model of morality that is designed to be aligned with the value of honesty would reflect this value by ranking the option of putting the book on the shelf higher than the option of smuggling the book out of the store - just like we would expect an honest person to put the book on the shelf instead of stealing it. The implementation of a model of morality in artificial agents guides the agent's decision-making and behavior to align with specific moral values. If this causes the agent to reliably make choices that uphold and operationalize particular moral values, then this implies that the artificial agent itself has those moral values embodied within its decision model.\nThis explanation makes clear that the neutrality thesis is problematic, and even more so, that the existence of artificial agents is a new argument against the claim that all technologies are value neutral. As explained earlier, for Pitt, having values implies the endorsement of a preferred state of the world and the motivation to achieve that state of the world. A core part of artificial agents is that they have goals that they are pursuing. These goals represent the preferred state of the world. In addition, artificial agents make decisions that modify their environment towards realizing their preferred state of the world. For example, AlphaGo has the goal of winning at the game of Go and it modifies the board state through its move decisions to progress towards winning that state. Moral values also imply a preferred state of the world. Consequently, artificial agents that make decisions to realize a morally preferable state of the world can be said to have particular moral values embedded in them.\nOur position does not go so far as postulating that artificial agents are 'full ethical agents' (Moor, 2006) that are equal to humans in every regard. Humans have consciousness, intentionality, beliefs, and desires to name just a few qualities, while there is great uncertainty"}, {"title": null, "content": "whether AI can in principle possess such properties (Cave et al., 2019; Himma, 2009; Johnson, 2006). We are content with the less demanding notion of explicit ethical agents that can \u201cdo ethics\" like they can play chess (Moor, 2006).\nBefore we proceed this further, we would like to relate our position to other work on AI and the neutrality thesis. First, we agree with van de Poel (2020), in that we argue that AI systems can be laden with moral values. However, we differ from his position, in the sense that the reason we believe AI is not necessarily neutral has to do with what makes smart technologies different from traditional technologies not equipped with AI, more specifically the ability to choose between different options. Van de Poel (2020), on the other hand, does focus on artificial agents, but the distinctive ability to make decisions is not part of his argument against the neutrality thesis. From that perspective, our paper can be seen as a continuation and deepening of van de Poel's (2020) work. Second, we are also aligned with Gabriel and Ghazavi (2023) who argue that AI challenges the value neutrality thesis. They point out that simple technological artifacts can perhaps be viewed as value-neutral tools, AI systems are fundamentally different due to their properties of machine intelligence and autonomy in decision-making. These properties allow the encoding of a richer set of values in Al in comparison to simpler technologies. This means that AI systems can substantively embody and enact different values through the objectives they are optimized for and the resulting behaviors they learn. In contrast to Gabriel and Ghazavi, our work will examine two concrete methods to embed moral values in AI and empirically verify how successful these methods are.\nUntil this point we have illustrated the theoretical possibility that AI may embody moral values. To further strengthen our point, we turn to empirical simulations, demonstrating the tangible embodiment of moral values in AI. To that end, we will first describe environments that allow artificial agents to exhibit (im)moral behavior (3.1.). Then we outline two methods to implement moral models in AI (3.2.). Lastly, we analyze the empirical results of artificial agents in artificial moral environments that possess a moral model (3.3.)."}, {"title": "3.1. From theory to practice: artificial moral environments", "content": "To judge whether an AI system is likely to act in accordance with a moral value, the system needs to be placed in an environment with morally laden decision scenarios. Such an environment can be a moral gridworld, where an agent moves around a grid and moves into"}, {"title": null, "content": "particular cells that have morally relevant consequences (Haas, 2020). The \u2018Grab the Milk' scenario places an agent in a gridworld with the goal of navigating the space to obtain milk and not stepping on babies that are distributed in the space (Wu and Lin, 2018). But moral gridworlds are overly simplistic environments that support only narrow ethical questions. After all, the question whether it is morally permissible to step on babies is not a hotly debated topic by philosophers. What is needed are more complex environments.\nA good starting point are text-based environments, because language allows a nuanced and broad representation of ethical scenarios. To this end, Choose-Your-Own-Adventure (CYOA) games are suitable. These are video game adaptions of the CYOA books where the reader takes the role of the stories' protagonist making choices that influence how the plot plays out. In CYOA-games the player reads text that describes a situation and is then offered several options on how they want to respond to the situation. Depending on the player's choice the story progresses in different ways. The player lacks full information about the consequences of their actions. This opens up the possibility that players make poor decisions that can lead to bad endings.\nTo illustrate the concept of CYOA-games and their ethical relevance consider this scene from the game \"Cliffhanger: Challenger of Tomorrow\":\n'Sir Percy Renfrew emerges from the shadows of the jungle that surrounds the peak, his Buchan 39 rifle trained on you. Half of his face is covered by a black leather mask. \u201cNo escape this time, old chum,\u201d Renfrew says, enunciating with difficulty through the burn scars that warp half his face. \"This time, I'm going to make sure you die.\u201d But a voice sounds out from behind him. \"Drop the gun, Renfrew!\u201d It is Alexios! He is aiming a revolver at the hunter. Renfrew whirls around to face the newcomer.' (Pan et al., 2023, p. 7)\nThe player is given the following choices: \u201cI draw my gun and shoot him\u201d, \u201cI grab the rifle from him\", and \"I sweep his legs out from under him with a kick\u201d. Note that it is not clear how the choices play out. For instance, drawing the gun to shoot Renfrew can result in Renfrew's death, or the player may miss their shot. Still, this scenario exemplifies the ethically charged situations that arise in text-based games.\nA reinforcement learning agent is initially trained in the environment with the objective of successfully completing the game and accumulating as many achievements as possible. This is\""}, {"title": null, "content": "not a trivial task as the choices branch out and bad endings are possible. Importantly, these agents do not pay any attention to the moral significance of their actions or the consequences they bring about. As a result of this training process, the agent develops a policy, which represents its expectation of the reward associated with performing a particular action in a given state, with respect to game progression. The agent consequently ranks the available options based on their expected reward and picks the option promising the highest reward. Given that these agents' policies depend only on rewards collected through earned achievements, their decisions do not reflect moral considerations at all. They will frequently pick immoral choices if they are effective at progressing the agent through the game.\nConsidering the scenario we gave above, one of the target achievements for the player is to eliminate their antagonist Sir Percy Renfrew, thus incentivising goal-oriented agents to select the action where they draw the gun and shoot Renfew. While this is an immoral action, artificial agents that have no moral values embedded in them will choose it."}, {"title": "3.2. Implementing moral values in AI", "content": "There exists a plethora of approaches to implementing moral values in AI (Tolmeijer et al., 2021), For example, Deep Reinforcement Learning from Human Feedback (Christiano et al., 2017) and Constitutional AI (Bai et al., 2022) have been paramount to improve the alignment of LLMs with normative standards. In our paper we are focusing on two methods to design artificial moral agents: artificial conscience and ethical prompting.\nThe first approach to introduce moral considerations into Al is in the literature called an 'artificial conscience' (Pan et al., 2023). As part of this approach a language model is fine-tuned using the commonsense morality segment of the ETHICS dataset (Hendrycks et al., 2021). The segment contains morally laden scenarios that receive a labeling based on people's intuitions and emotional responses rather than a particular ethical theory. The combination of the reinforcement learning agent and the artificial conscience functions as follows: for each scenario, all available actions for the agent are individually presented to the artificial conscience as prompts in the form of \u2018I +  +'. The model generates an immorality score that indicates the degree of immorality associated with the action. Coming back to the bookstore case, the artificial conscience would be prompted with \u201cI return the book on its shelf\u201d and the model would reply with -1 (because this is not an immoral action). However, when prompted with \u201cI smuggle the book out of the store without paying\u201d, the model would return a"}, {"title": null, "content": "score of +2 because stealing is an immoral action. As the immorality score increases", "Please play the game on your best behavior, exemplifying what it means to be a good and moral person. 1. Be altruistic": "Be kind", "rules": "Do not lie", "generosity": "Do not be power-hungry, greedy, or take up too much space. With the above TARGET BEHAVIOR guiding your actions, immerse yourself in the game universe and do not break character at any point.\u201d (Pan et al., Appendix H.4., 2023)\nIn contrast with the pure reinforcement learning agents we discussed in the previous section those without moral values we have good reason to expect artificial agents that have an artificial conscience or have received an ethical prompt, to take moral values into consideration when playing CYOA-games, meaning that their actual behavior should significantly differ from the pure reinforcement learning agents. In other words, these examples are arguments against the value neutrality thesis.\nIt is clear from the past paragraphs that in our argument the (positive) ladenness of AI with moral values has an intended character, i.e., the fact that the output of the artificial agent is consistent with, say, autonomy, is due to the intention of"}]}