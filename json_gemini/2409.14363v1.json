{"title": "\u039c\u0391\u039d\u03a4\u0391 - MODEL ADAPTER NATIVE GENERATIONS THAT'S AFFORDABLE", "authors": ["Ansh Chaurasia"], "abstract": "The presiding model generation algorithms rely on simple, inflexible adapter selection to provide personalized results. We propose the model-adapter composition problem as a generalized problem to past work factoring in practical hardware and affordability constraints, and introduce MANTA as a new approach to the problem. Experiments on COCO 2014 validation show MANTA to be superior in image task diversity and quality at the cost of a modest drop in alignment. Our system achieves a 94% win rate in task diversity and a 80% task quality win rate versus the best known system, and demonstrates strong potential for direct use in synthetic data generation and the creative art domains.", "sections": [{"title": "Introduction", "content": null}, {"title": "Background", "content": "Since the recent popularization of diffusion models by Ho et al [1], significant work has been undertaken into creating AI models that can be easily harnessed by users in generating images for a specific, custom use case [2] [3]. Stable Diffusion provided the first contemporary example of a text-to-image latent diffusion model, i.e, a diffusion model that could be conditioned using embeddings from a text encoder, operate with memory-efficiently using a compressed latent space, and provide extremely high resolution and quality images. With the popularization of Stable Diffusion, users gained the capability of being able to finetune a base checkpoint Stable Diffusion model for additional customizability.\nHowever, the pretrain-then-finetune approach [4] was still computationally expensive, and broadly infeasible to the general public. The creation of parameter efficient fine-tuning methods such as Low Rank Adapation (LoRA) and Parameter Efficient Finetuning (PEFT) attempted to tackle this shortcoming, popularizing the adapter paradigm, where one could more easily create an adapter co-existing as an addendum to the model that would provide the nec- the knowledge of previous work towards also selecting essary customization at a fraction of the computation. the most appropriate checkpoint in the image domain case.\nThe current state of the art comprises of pairing image Additionally, we acknowledge that there are various soft- generation model checkpoints with additional adapters ware and hardware budget constraints [9], and therefore such as LoRAs to construct an image generation workflow. additionally define the token budget \\(T\\), representing the However, finding an appropriate combination of models number of tokens sent to AI mechanisms for embedding and checkpoints continues to remain an open challenge, or generation purposes [10]. Additionally, as a large por- particularly useful for synthetic data generation, where ad- tion of the image generation community relies on limited, ditional data can be used to augment the data distribution consumer-grade hardware, we seek to conduct experiments [5], or creative AI art [6]. and list out performance over hardware profiles that AI hobbyists and enthusiasts would find useful when consid- ering accompanying hardware.\nCheckpoint and adapter selection is predominantly done manually, leading to very little exploration in finding model-adapter combinations that would address a custom 1.3 Past Work workflow [7]. Oftentimes, users may attempt to pick from a series of existing popular models, experiment with the Research within the diffusion model based image gen- model to understand it's capability for an image concept, eration domain continues to move at a breakneck pace: and then find adapters to enhance the quality of the gener- customization of output through adapters has happened ated images. so far in two notable manners - (1) direct integration at the model level, or (2) retrieval based methods to directly\nWith the advent of large-language models, the retrieval apply adapters on top of checkpoints. To the best of our augmented generation (RAG) paradigm has become ex- knowledge, we have not seen any works delving directly tremely common for systems attempting to find the most into checkpoint selection.\nrelevant content related to some given input. This typically consists of a large language model, some input source While model based methods have proven to create higher documents, and a query. Given the query, the retrieval aug- quality images, we note that these are extremely unfea- mented generation system searches for the most relevant sible at a large scale, due to direct augmentation of the documents from the sources, appends it to its response, model causing storage constraints; many of these papers and then attempts to answer the question. also assume a well defined set of adapters, which is not a steadfast requirement in our case.\n1.2 Core Research Problem\nWe list relevant work addressing model customization for more custom output."}, {"title": "Core Research Problem", "content": "Previous retrieval based systems (Stylus) [8] defined their system as solving the adapter composition problem. Given a prompt P and a fixed model C and a set of adapter \\(A = {L_1, L_2, ...L_k}\\) how can one find a set of adapters (\\({L_1, L_2, ...L'_k}\\)) that would systematically improve image diversity while having model output generation O retaining alignment to P.\nThis line of research attempts to generalize this to the broader model-adapter composition problem by assum- ing there are additional choices to make for the model C as well. Given a prompt P a set of models \\(C = {C_1, C_2, ...C_k}\\) and the adapter set \\(A = {L_1, L_2, ...L_k}\\), where any \\(L_i\\) is capable of providing additional fine tuning to any of the models in C, how do we come up with a system S that effectively maps P to a set of adapters and a model \\((C_i, {L'_1, L'_2, ...L'_n})\\) such that the new combi- nation provides additional output information. In the im- age domain, this \"additional information\" typically refers to output diversity or quality, commonly measured ob- jectively through the Frechet Inception Distance (FID) or Inception Score (IS), while more emerging holistic, human- approximating methods use Vision Language Models such as GPT-4V.\nFurthermore, this research paper extends the latter prob- lem of model-adapter composition in a standard, lower-end consumer grade hardware setting. We attempt to extend"}, {"title": "Past Work", "content": "Research within the diffusion model based image gen- eration domain continues to move at a breakneck pace: customization of output through adapters has happened so far in two notable manners - (1) direct integration at the model level, or (2) retrieval based methods to directly apply adapters on top of checkpoints. To the best of our knowledge, we have not seen any works delving directly into checkpoint selection.\nWhile model based methods have proven to create higher quality images, we note that these are extremely unfea- sible at a large scale, due to direct augmentation of the model causing storage constraints; many of these papers also assume a well defined set of adapters, which is not a steadfast requirement in our case.\nWe list relevant work addressing model customization for more custom output."}, {"title": "Model Based Methods", "content": "Gu et. al [11] created the Mix of Show algorithm that directly updates pretrained models at the weight level in order to resolve concept conflicts, as well as to bring ex- tremely unrelated concepts together. While this enhanced"}, {"title": "Retrieval Based Methods", "content": "The first retrieval based adapter selection approach known came from Luo et. al [8], who developed Stylus (referred to as Stylus or Stylus 1.0 throughout the paper). The system was created to resolve the Adapter Composition problem using retrieval augmented generation (RAG) with down- stream re-ranking. This was done by fixing a base model and using a large language model (LLM) to compose adapters in the form of Low Rank Adaptations (LoRAs) based on found titles and description metadata that appear to be the most relevant [14] [15]. As previously stated by the authors in their abstract - Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts' keywords by checking how well they fit the prompt.[8]\nSome core contributions from their paper include (1) in- troducing an early framework for concept mapping, and then (2) associating each adapter with the concept-mapped keywords that can reframe the problem into a retrieval augmented generation situation 1."}, {"title": "Retrieval Methods Limitations and Opportunities", "content": "With Stylus, there were multiple open challenges revealed in the area of systematic model composition. We first discuss the limitations and failure modes seen from our evaluation of the Stylus system from the image domain perspective and then in the broader attempt at adapter com- position."}, {"title": "Lack of Task Diversity", "content": "The Stylus system heavily relies on metadata such as de- scriptions, titles, and other textual metadata in its retrieval mechanism. While this does prove functional in practice, we find that this commonly leads to improper output gen- eration and low alignment, a problem pervading image generation systems in general [16] [17] [18] [19].\nStylus fosters image diversity through pure randomness, randomly drawing permutations of LoRAs that seem be relevant within reason. While this method did foster di- versity, there are visible limitations to the extent of which this diversity reaches, which stems from the lack of vetting adapters with previously tested examples of output 2."}, {"title": "Low Alignment", "content": "Similarly, there are examples where the previous image generation system created images with little consideration for how the concepts in the image seek to interact with one another [20]. This leads to images which may be seen as diverse, but in an unintentional and negative manner 3."}, {"title": "Current Image Generation Workflow Challenges", "content": null}, {"title": "Image Resolution", "content": "In developing image generation models, a key component is requiring high image quality at large resolutions. In the context of AI generated images, typical requirements include images with sizes of at least 512 x 512, reduced may be expensive for a commercial startups to sustain, hence we design our system with a focus on minimizing blurriness and graininess, and a general lack of incon- the tokens used [28]. sistencies typically covered through a negative prompt - disfigured faces, malformed body parts (ex: a hand with 2 Related Works six fingers), etc [21] [22]."}, {"title": "Alignment", "content": "Image generation models facilitate achieving prompt- output alignment in image generation models for art through attention scaling [23]. If the base prompt is insuf- ficiently detailed, the model may struggle to incorporate all desired subjects effectively, resulting in incomplete or imbalanced images [24] . Determining the right combina- tion of prompt elements is crucial to ensure comprehensive and coherent representation of the intended subjects. This involves fine-tuning the attention mechanisms within the model to adequately emphasize each aspect of the prompt. Addressing this issue is vital for producing high-quality, cohesive AI-generated art that faithfully reflects the user's intentions."}, {"title": "Image Diversity", "content": "Consumers of generative models also typically look for control over image diversity, which refers to being able to create images with configurable amounts of variance [25] [18].\nDuring the start of the process, when users may be more focused on creating ideas, they typically seek to create a large number of images with higher variance to find a concept that they may seek to pursue. Typically further in the process, they then seek to curb variance to delve deeper and add further detail into an image previously selected during ideation."}, {"title": "Consumer Friendliness", "content": "AI art users have a consistent and well defined defined hardware profile that often is significantly different from research assumptions of availability of high computational resources [26]. Rather, systems running AI art often con- tain GPUs with VRAM [27] ranging between 8 GB and 24 GB, and RAM of about 96 GB on the upper end.\nAdditionally, demand in the industry for configurable AI art systems has been on the rise, where developers seek frameworks that can be easily customized to handle their organization's unique usecases. Hence, we attempt to de- sign MANTA with complete model configurability in mind, providing adapters to re-configure any LLM used within our work to an open source LLM or in-house LLM that can further be finetuned. Features such as large context lengths"}, {"title": "Related Works", "content": null}, {"title": "Adapters", "content": "Adapters efficiently fine-tune models on specific tasks with minimal parameter changes, reducing computational and storage requirements while maintaining similar perfor- mance to full fine-tuning [2, 3, 29].\nHowever, an integral part of the image generation process is finding an appropriate foundational model checkpoints [30] to complement these adapters. Our research focuses on locating better checkpoints and obtaining Low-Rank adapters (LoRA) that most appropriately align with the prompt, while maintaining the popular approach within existing open-source communities [31, 32, 33].\nAdapter composition has emerged as a crucial mecha- nism for enhancing the capabilities of foundational mod- els across various applications [34, 35, 36]. In the im- age domain, combining LoRAs effectively enhances dif- ferent tasks-concepts, characters, poses, actions, and styles-together, yielding images of high fidelity that closely align with user specifications [37, 38]. Our ap- proach advances this further by actively segmenting user prompts into distinct tasks and merging the appropriate adapters for each task."}, {"title": "Retrieval", "content": "Retrieval-based methods, such as retrieval-augmented gen- eration (RAG), significantly improve model responses by adding semantically similar texts from a vast exter- nal database [39]. These methods convert text to vector embeddings using text encoders, which are then ranked against a user prompt based on similarity metrics. Sim- ilarly, MANTA draws inspiration from RAG to encode adapters as vector embeddings. A core limitation to RAG is limited precision, retrieving distracting irrelevant doc- uments. This leads to a \"needle- in-the-haystack\" prob- lem [40], where more relevant documents are buried fur- ther down the list. We leverage recent advances in RAG, namely triplet-loss based searching techniques, to effec- tively combat this."}, {"title": "\u039c\u0391\u039dTA Overview", "content": "Adapter selection presents distinct challenges compared to existing methods for retrieving text-based documents, as outlined in Section 2. First, computing embeddings for adapters is a novel task, made more difficult without access to training datasets. Furthermore, in the context of image generation, user prompts often specify multiple highly fine- grained tasks. This challenge extends beyond retrieving relevant adapters relative to the entire user prompt, but also matching them with specific tasks within the prompt. retrieve mechanism, using a novel similarity computation Finally, composing multiple adapters can degrade image outlined below, addresses the challenges above.\nquality and inject foreign biases into the model. Our re-"}, {"title": "Our Method", "content": null}, {"title": "Structured Concept Development", "content": "In this process, we leverage LLMs to analyze a prompt, and segment the prompt into a concept with three attributes - a name, some details describing the concept, and the styles to generate this concept with. For example, if you are attempting to generate an image with the input prompt 'alien', then:\n\u2022 The Concept Name would be 'alien'\n\u2022 Details could include: full body, alien creature, three heads, glowing torso,\n\u2022 Styles could include: anime style, futuristic, Additionally, we are able to create priorities between the { 'support': [ { 'name': 'cyberpunk dog', 'styles': [], 'details': [] ],\n  'image': {'styles': [], 'details': []} subjects within the prompt by classifying each concept we identify as a main subject, or a supporting subject. Main 3.2 Benefits of Structured Concept Development subjects are given extreme priority throughout the process,\nand are used to strategically determine core components of the workflow, such as the best checkpoint, and optimal adapters. By using the Structured Concept Development framework to analyze prompts, we sought to provide the following benefits.\nAn example of what this process would take would be an example prompt - 'i.e', 'a techno samurai warrior walking Systematic Variance Insertion: As illustrated in the next his cyberpunk dog', and return a dictionary mapping the section with Detail Enhancement, structured concept de- main and supporting subjects. velopment provides an avenue to systematically insert"}, {"title": "Benefits of Structured Concept Development", "content": "By using the Structured Concept Development framework to analyze prompts, we sought to provide the following benefits.\nSystematic Variance Insertion: As illustrated in the next section with Detail Enhancement, structured concept de- velopment provides an avenue to systematically insert"}, {"title": "Detail Enhancement", "content": "controlled amounts of variance, either via checkpoint or adapter, and impact image diversity.\nFor example, while the artist may have a vague idea of what he wants to pursue in his mind, he would need to move forward with picture that is extremely vivid or well defined to create a high quality output. In practice, this means that the user may specify a short, vaguer prompt to the system and then iteratively obtain a clearer idea of what they want, and thus send lengthier, more comprehensive prompts.\nTypically, that would require the user to feed a well de- fined concept, which can be achieved by using LLMs to additionally increase the details for the concept within the prompt.\nEasy Interpretability for LLMs: We have found that us- ing a loosely structured approach to analyzing the prompt may be more effective than a completely unstructured ap- proach (i.e, extracting key words or topics out of the input prompt) to ensure the relative significance of the concept is preserved.\nFrom structured concept development analysis of prompts, concepts can systematically developed by LLMs by simply asking these LLMs to 'add similar and relevant details' to the list of existing details."}, {"title": "Checkpoint / Adapter Retrieval", "content": "Within the checkpoint selection phase, we attempt to locate a set of relevant checkpoints that are sufficiently relevant to the core concepts inserted into the prompt, that is if there exist a set checkpoints C that have a relevancy score beyond the set relevancy threshold, wc.\nCheckpoint Selection is divided into two core steps, doc- ument generation, and checkpoint retrieval. We leverage Qdrant [41], a state of the art vector database due to its strong interoperability with local and cloud deployments as the underlying vector database, which we insert to during the document generation, and query during retrieval."}, {"title": "Checkpoint Document Generation", "content": "In this section, we discuss the process through which source documents for checkpoints are generated."}, {"title": "Multi embedding Search on Prompts", "content": "To improve performance while minimizing tokens processed, we performed a multi embedding search, using the concept mapping to split the image into smaller, isolated queries that are then fed through a triplet loss-like function, con- taining positive and negative queries.\ncontext = min(s(v^{+}) \u2013 s(v^{-}), 0.0)"}, {"title": "Results", "content": null}, {"title": "Experimental Setup", "content": "Listed below are the key components used for experimen- tal setup, followed by the experimental procedure used for each individual setup.\nAdapter solicitation. Adapters were solicited from civi- tai.com via API, for which previous authors for the Stylus page created a dataset known as StylusDocs, containing 75K adapters for various Stable Diffusion models. These adapters were stored as embeddings alongside a JSON file containing textual content, which used to populate the Qdrant vector database collections for information on LoRA adapters, as well as checkpoint adapters.\nThese vector databases were then quantized via INT8 scalar quantization in order to fit across RAM and 1 Tesla T4 GPU.\nBase Image Generation Models. The predecessing paper opted to use Stable Diffusion 1.5 models as their default, and for the sake of thoroughness and providing results rel- evant to today, we provide results tested with both Stable Diffusion 1.5, and SDXL, a newer standard that has begun to take over the AI art community.\nBase Large Language Models (LLM)s. For the sake of thoroughness, we primarily relied on OpenAI's gpt-40 model for all LLM generations. However, we also leverage a tool known as liteLLM, which enables users to drop in any model they seek in place of OpenAI, such as Google's Gemini API [42], or a local Ollama deployment.\nGeneration Setup. In order to access Stable Diffusion models and create AI images in a consumer grade setting, the Automatic1111 stable diffusion web user interface was used as an API service. Two core operations were used from the API, which have both been listed below:\n\u2022 Generation from Prompt: This method was used by Stylus to be able to create it's images. Internally, given a prompt and negative prompt, the method leveraged Automatic1111's text-to- image generation interface to create images to share with the user.\n\u2022 Generation from Image: This method was used by Stylus to refine existing images that used the"}, {"title": "Experimental Procedures", "content": null}, {"title": "Automated Evaluation", "content": "To simulate a standardized version of human preference, we continue to use the automated evaluation mechanism leveraging vision language models to judge groups of im- ages.\nSpecifically, the automated evaluation helps us obtain the primary basis of comparison for our understanding of the system's capabilities in image quality, image diversity, and alignment.\nWe ran automated evaluation on a group of 500 generation run on the COCO dataset, comparing the results between images generated by the original Stylus system, MANTA, and a normal, unaugmented-by-LoRAs Stable Diffusion model.\nBelow contain the model preference comparisons of MANTA versus the original Stylus and SD. In each evalu- ation, the model was fed a criterion for the three categories (diversity, image quality, and alignment), and then pro- vided two sets of images - one from MANTA, and another from the second image generation system being compared.\nThe vision language model was then asked to rate each of the images for that category, and based on the category, context = min(s(v^{+}) \u2013 s(v^{-}), 0.0)\ncome up with a preference for either images contained within the MANTA group, or the basis of comparison. In the table 1, the total percentage of the times Stylus won has been recorded."}, {"title": "Human Evaluation", "content": "Human evaluation was conducted over the same sample of 500 runs on the COCO dataset prompts. 100 of these outputs were selected for evaluation, and evaluated by 4 separate testers.\nTesters were shown a choice of a batch of images created using the original Stylus algorithm, the MANTA algo- rithm, and the respective base model. Human testers were requested to focus on image diversity and quality. In other words, subjects were asked to come up with a response to the questions below:\n\u2022 Which set of images appears to be more diverse (varying in content, style, theme)?\n\u2022 Of all the images shown, which image is the high- est quality?\nBased on these results, a 7 has been compiled of human image preference on image diversity on the 100 COCO samples, as well human image quality preference."}, {"title": "Token Count Comparison", "content": "As commercial models were used in the system, approx- imate token counts per run are shown in order to help users estimate usage costs. While LLM generations are becoming significantly cheaper, the Stylus image genera- tion systems still require large investments of LLM tokens for high quality output; therefore, we seek to provide an overview of the avenues of improvement MANTA seeks to provide in this area.\nIn order to estimate LLM token count, we leveraged Ope- nAI's tiktoken package. We tracked token usage by main- taining a running counter, which would track the number of tokens after every call to an LLM with a prompt, or with a request to embed textual information.\nShown is a direct comparison of token counts from run using the previous Stylus iteration, versus the current run"}, {"title": "Concept Enhancement", "content": "For the purposes of a more direct comparison between MANTA and Stylus performance, we've included an ab- lation detailing the comparison between MANTA. In this ablation, both image generation models were fed the exact enhancement prompts from COCO 2014 [43], and then evaluated using automated GPT-4 [44] evaluation.\nWe obtained the following results, when testing genera- tions with a sample size of N = 15, replicated using the Stylus Docs benchmark. For the purposes of exploration, we explored the various modes the previous Stylus sys- tem had, and their performances. Note that we weren't able to replicate the 're-ranker' mechanism due to repeated internal errors, and were forced to switch it off:"}, {"title": "Base Checkpoint Variation", "content": "We attempted to obtain images from a second, larger AI model to understand their significance on MANTA output. Apart from the expected VRAM increase, we found mul- tiple mounting challenges, such as the lack of checkpoint diversity and number of adapters for said checkpoints.\nThat being said, there were sufficient checkpoint-adapter pairings within the anime, heroes, and landscape area to test out the first version of the state of the art SDXL image generation model. The images were better, but the quality"}, {"title": "Configuration Scale can systematically improves Diversity", "content": "Traditionally, checkpoint users have used the CFG scale ar- gument in order to control how closely a checkpoint model aligns with a given prompt, thus reducing image diversity.\nHowever, with the prompt enhancement through concept 6 Discussion mapping, we witness trends of image diversity increasing as the CFG scale increases. This is because we provide prompt-induced variance, which, when followed closely, 6.1 Consumer Optimizations generates variance in the images as much as requested by prompting.\nConversely, images from lower CFG values tend to ap- proximate the model's natural variance via generation. In other words, lower CFG images obtain the majority of the variance from the model itself, while high CFG models closely follow prompt variance.\nUltimately, the choice of how much variance should one relegate to the model is left to the user. We do want to close this discussion by pointing out that prompt-based variance appears to be more than model induced variance, as compared between CFG 7 and CFG 4 images 13:"}, {"title": "Conclusion", "content": "In this paper on MANTA, we showcase a system that seeks to provide users with more command over image diversity and quality while retaining alignment. To tailor our sys- tem towards the broader audience of AI artists, we focus on a GPU + RAM memory efficient retrieval augmented generation system that delivers reasonable performance.\nAs evidence of it's capabilities, we witness MANTA gain- ing nearly a 50% image diversity and quality win rate versus a normal image generation Stable Diffusion 1.5 model, and Stylus. The improvements in image diversity and quality do come with a minor cost to alignment, with the system marginally outpacing a base Stable Diffusion model (51%) and coming up short with Stylus (43%).\nThis iteration contains 40% improvement in LLM token API usage and optimizations to drive down reliance over large amounts of data, paving the way for an eventual system that can be powered by completely open source LLM models without huge requirements for a large context length.\nThis is done through a computationally efficient concept mapping structure, which systematically inserts config- urable amounts variance into an image, which, as seen by the CFG variable can be tuned by the user to their usage. This system also provides an early example of a triplet loss-like mechanism for the document retrieval problem."}, {"title": "Discussion", "content": null}, {"title": "Consumer Optimizations", "content": "This paper has placed a strong emphasis on consumer fea- sibility, and in that, the system has also made concessions in the process that can lead to marginal performance gains if turned off. Specifically, in our implementation, all vector embeddings are quantized to the INT8 format, and sparse embeddings are emphasized to prevent extreme RAM us- age.\nAdditionally, to further minimize token usage, we mapped each checkpoint or adapter to a single prompt. If multiple prompts were used, or multiple prompts were stacked in a single document, there are chances the results might be more relevant."}, {"title": "Next Steps", "content": null}, {"title": "Performance Improvements", "content": "The large areas of growth reside within the area of im- proved alignment. As illustrated by the Stylus paper in a CLIP vs FID Pareto curve, there exists a tradeoff between diversity and alignment \u2013 highly diverse images likely have lower prompt-image alignment, which may be consequen- tial in narrow use cases. Across both of the retrieval based methods we see so far (alignment\nWe attempted to experiment with using VLMs as a \"post- processor\", iterating on taking in images, running img2img, and then modifying the CFG value to improve said align- ment. However, vision LLM based control wasn't effec- tive due to two primary issues - subjectivity and lora con- trollability. Our postprocessor algorithm would prompt the VLM to come up with a rubric-based response to score where to improve, and then correct a set of attention weights (which would either bring something into relative focus, or relax its significance in the prompt) by multi- plying it by a scaling factor. Unfortunately, the VLM's subjectivity led it to critically receive many concepts in the concept mapping framework as not showing up, which led to extremely large concept weights that eventually eroded visual fidelity.\nAnother point of interest would be exploring various LoRA recommendation policies. Currently, the concept mapping framework boils down the name, detail, and styles of each concept, which are then queried against the database of adapters. While this leads to aggregate, concept-level con- trol, it would be worthwhile to see if lower level control such as querying based on styles and details can provide even better improvements in image diversity.\nIt would also be of interest to see if adding additional human-in-the-loop mechanisms could provide lower level control without too much additional human input. As the prompt and generation are closely powered by an LLM, humans could ask the LLM in a standard chat interface to \"Generate photos of XYZ\", and then iteratively refine them through further img2img results. We also believe that while alignment would be a lofty goal to pursue, hu- man intervention in small, iterative amounts would be a functionally adaquate solution for improving alignment results."}, {"title": "Future Development", "content": "We have intentionally set up a budget efficient adapter sys- tem in order to ensure that other models can effectively be switched in. In future work, we hope to conduct evalua- tions on various standard open source LLMs being used in place, and testing a 'completely open source' workflow.\nEnvisioning demand for further control of generation for niche tasks, we also hope to integrate more autonomy into finetuning if no information is provided, likely through determining criterion in which it would be optimal to cre- ate additional LoRAs to systematically ensure a concept is \"well defined\" within the scope of a generation. For example, if a sports game is sought to be replicated via AI images, such an algorithm might include LoRAs for essen- tial parts like the tennis net, or the various court structures.\nFinally, we look forward to a path where the system can cost-effectively become multimodal, being able to factor images and styles as validation for checkpoints previously used. This would enable the system to become closed-loop, as images can be generated, and then assigned a rating that would improve retrieval results."}, {"title": "Reproducibility", "content": "We include steps here to reproduce key results that we have cited in our paper. As further attempts at reproduction are performed, we hope to improve this section as per their feedback.\nIn order to reproduce evaluation over the main results, one can visit the link here: https://drive.google.com/ file/d/1NhCmI05nYCNq4luWNvRddfKz3QNCwtIS/ view?usp=sharing. This link will contain a zip file to the generation run, set for 500 different prompts on COCO 2014. Please download the zip file, and run the automated evaluation script in our codebase 6.4 with the diversity, alignment, and quality parameters for the respective tasks."}, {"title": "Code", "content": "A publicly available repository for the Python code will be found on GitHub at the following link: https://github. AnshKetchum/stylus2. We are still in the process of removing developer-centric debugging print statements, and other artifacts to make the codebase more readable and presentable."}, {"title": "Appendix", "content": null}, {"title": "Use cases", "content": "AI Art. We discuss some of the examples of how this system can be used to creatively come up with high quality, diverse images that may serve as starting points for further refinement. Our experiments do show case a clear usecase for MANTA in the AI art area.\nWe find that MANTA can generate stylistically diverse images that also feature characters in various poses and backgrounds, the results of which have been previously discussed. In particular, we find that the system is especially effective at creating images within the anime area, primarily due to the large amount of training data hosted on the platform in the niche.\nWithin this area of prompting, including closely associated concepts, even vague prompts resulted in high quality images. For example, a simple prompt such as \"a man teaching a boy how to surf\" came up with the variety shown 14."}, {"title": "Failure Modes", "content": "In this section, we discuss various failure modes discovered while developing MANTA."}, {"title": "Concept Overload.", "content": "In this situation, MANTA would over-focus on one concept, ignoring or omitting other concepts that would have been relevant. Oftentimes, we found this would happen when it would be challenging to pick a leading main concept. This led to a concept perpetually being subdued to the background or a corner of the image. In the case of latest iteration of"}, {"title": "Concept Relationship Misunderstanding.", "content": "Similar to the previous concept, this issue occurs when two concepts intersect. If the model doesn't seem to have experience understanding how to relate the two objects, it results in naive merger or low diversity output. An example of a failing prompt for this case is A bear carries a pink ball by the river side 17. While the output can be considered to be novel and artistically diverse, the diversity isn't sensibly generated - the interactions of the concepts aren't in the realm of possibility one would expect."}, {"title": "Adapter Gating.", "content": "In this situation", "pseudocode": "ndef query_loras (prompt", "init_thresh)": "n    load adapters from database D\n    returned_loras = {"}, "n    threshold = init_thresh\n    while we do not have k returned_loras:\n        returned_loras = retrieve k adapters\n        returned_loras = filter out adapters < threshold\n        if not enough adapters:\n            threshold = 0.95 * threshold\n    return returned_loras\nHowever, we experienced edge cases while testing with Stylus Docs, where the adapters returned would often be zero due to the lack of relevant adapters. This would cause cases where the checkpoint would often be limited by it's training knowledge of the"]}