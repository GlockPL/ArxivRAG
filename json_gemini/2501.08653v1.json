{"title": "Fine-grained Spatio-temporal Event Prediction with Self-adaptive Anchor Graph", "authors": ["Wang-Tao Zhou", "Zhao Kang", "Sicong Liu", "Lizong Zhang", "Ling Tian"], "abstract": "Event prediction tasks often handle spatio-temporal data distributed in a large spatial area. Different regions in the area exhibit different characteristics while having latent correlations. This spatial heterogeneity and correlations greatly affect the spatio-temporal distributions of event occurrences, which has not been addressed by state-of-the-art models. Learning spatial dependencies of events in a continuous space is challenging due to its fine granularity and a lack of prior knowledge. In this work, we propose a novel Graph Spatio-Temporal Point Process (GSTPP) model for fine-grained event prediction. It adopts an encoder-decoder architecture that jointly models the state dynamics of spatially localized regions using neural Ordinary Differential Equations (ODEs). The state evolution is built on the foundation of a novel Self-Adaptive Anchor Graph (SAAG) that captures spatial dependencies. By adaptively localizing the anchor nodes in the space and jointly constructing the correlation edges between them, the SAAG enhances the model's ability of learning complex spatial event patterns. The proposed GSTPP model greatly improves the accuracy of fine-grained event prediction. Extensive experimental results show that our method greatly improves the prediction accuracy over existing spatio-temporal event prediction approaches.", "sections": [{"title": "1 Introduction", "content": "The prediction of spatial-temporal events has become an important task in many applications, such as earthquake prediction [2], crime prevention [14], spacecraft anomaly detection [24], and epidemic control [8]. By accurately anticipating the time and location of future events, we can avoid potential risks or dangers and maximize benefits. With the rapid development of deep learning, event prediction techniques have been exten-"}, {"title": "2 Related Works", "content": "2.1 Spatio-temporal Event Prediction Extensive research has been conducted on the prediction of spatio-temporal events. Jointly modeling the continuous spatio-temporal event pattern is challenging; thus, it is a common practice to discretize the time and space in event prediction tasks. Works like LASSO [41], MITOR [11], and SIMDA [12] treat spatio-temporal event prediction as a multi-task learning problem, where the time is cut into windows and each location is taken as a separate event prediction task. Other works like STAPLE [26], STCGNN [33], and STEP [37] propose to learn the spatial correlations by organizing the different locations into a graph, where each node represents a country or city. Although these works jointly consider the times and locations of events, they can only model the locations as discrete labels instead of Euclidean coordinates and hence fail to fit in scenarios where fine-grained spatial prediction is required. Traffic-related event prediction models either divide the Euclidean space into boxes [39, 18] or construct a fixed road network [10, 23]. However, the accuracy of the spatio-temporal prediction still depends on the granularity they choose and cannot adapt to sparse distributions.\n2.2 Point Process Models Point processes [6] are useful tools for continuous spatio-temporal event prediction, unaffected by granularity issues. By specifying continuous functions representing event occurrence rates, point process models can generate the distribution of future events. For this purpose, traditional point process models assume fixed functional forms with tunable parameters. Poison processes [22], Hawkes processes [13], and self-correcting processes [16] formulate different forms of intensity functions conditioned on past events, accounting for the triggering effects between events. However, simple functional forms fail to capture complex event dependencies. Thus, neural networks have been widely applied to point process modeling problems. Most neural point process methods are merely sequential prediction models, including RNN-based methods [9, 25, 28, 30], transformer-based methods [45, 40, 36], CNN-based methods [42, 43], etc. Sequential models treat spatial features as discrete labels, lacking the ability to accurately predict spatial coordinates. Hence, STPP models have become a research hotspot in recent years. The core issue of STPP modeling is the normalization of the multi-dimensional spatio-temporal distribution. Most recent STPP methods adopt generative approaches to avoid the intractable normalization problem, including variational auto encoders (VAEs) [44], conditional normalising flows [4], and diffusion-based models [38]."}, {"title": "3 Preliminary", "content": "3.1 Spatio-temporal Point Processes Spatio-temporal Point Processes (STPPs) are useful tools for modeling discrete event occurrences in continuous time and space. An STPP event sequence can be given as S = {(t\u1d62, S\u1d62)}\u1d62=1, where each event is characterized by a timestamp and a spatial coordinate. An STPP prediction problem can be formulated as fitting the joint distribution p(t, s|H\u209c), where H\u209c = {(t\u2c7c, s\u2c7c)|t\u2c7c \u2264 t} represents the history events that occurred before time t. For simplicity, we use p* to represent distributions that depend on the history H\u209c hereafter.\n3.2 Neural Ordinary Differential Equations Neural Ordinary Differential Equations (ODEs) [5] have become a popular technique for modeling continuous dynamics. Using a neural network to specify the gradients of the dynamic variable at any point in its domain, we can establish a vector field to extrapolate the future evolution of the state by solving initial value problems."}, {"title": "4 Methodology", "content": "4.1 Overview As shown in Fig. 1, the model has an encoder-decoder architecture. The encoder simulates the global-local state evolution. Unlike previous works [4, 17] that use a single global state vector, we propose to incorporate global and local dynamics by maintaining two types of dynamic states, including a location-independent global state z\u1d33 \u2208 \u211d and K region-specific local states Z\u1d38 = [z\u2081\u1d38, z\u2082\u1d38,..., z\u2096\u1d38] representing the dynamics of different spatial regions. We define two types of state evolution, namely extrapolations and jumps. The ex-"}, {"title": "4.2 Self-Adaptive Anchor Graph", "content": "An SAAG G consists of a set of K anchor nodes and the correlation edges between them. Unlike the nodes in normal graphs, the anchor nodes are localized in the Euclidean space of event occurrences. Each of the anchor nodes is associated with a coordinate indicating its spatial location, that is, C = [c\u2081, ...c\u2096]. The use of anchor nodes eliminates the need for explicit borders to split the space into regions. The anchor nodes are representatives of their nearby regions and store the event dynamics of the local areas. The locations of the anchor nodes are trainable in order to adaptively find the best localization points. Fig.2 is an example of an anchor graph.Inspired by [35, 1, 34, 3, 15, 29], we propose to use a self-adaptive adjacency learning approach to learn the hidden correlations between anchor nodes. We use a double-headed adjacency approach to represent the inter-region correlations, specifically a distance adjacency head and a latent adjacency head. The distance adjacency A\u1d48 is built on the intuition that locations close in space should have a stronger correlation. We define the distance adjacency with the RBF kernel:\n(4.3) A\u1d48[i, j] = \\begin{cases} 0, & i = j \\\\ exp(-\\frac{||c\u1d62 - c\u2c7c||\u00b2}{\u03b3}), & i\u2260j \\end{cases}\nwhere \u03b3 is a hyperparameter controlling the decay rate. However, there often exist hidden connections between locations that could not be expressed by Euclidean distance or any other prior knowledge we have. Thus, we also need to learn a latent adjacency to capture such hidden correlations. Inspired by [34], we propose to use a uni-directional adjacency learning approach defined as follows.\n(4.4) A\u02e1 = softplus(E\u2081E\u2082\u1d40 - E\u2082E\u2081\u1d40)\nwhere E\u2081, E\u2082 \u2208 \u211d\u1d37\u00d7d\u2091 are the trainable node embedding matrices. This design guarantees that all relations are approximately uni-directional, i.e., if A\u02e1[i, j] has a relatively big value, A\u02e1[j, i] is guaranteed to be a small value close to zero. We use softplus activa-"}, {"title": "4.3 Global-local State Evolution", "content": "The global-local state evolution is modelled with neural ODEs with jumps. The evolution of the global state z\u1d33 \u2208 \u211d\u1d48\u1d50\u1d52\u1d48\u1d49\u02e1 and the local states Z\u1d38 \u2208 \u211d\u1d37\u00d7\u1d48\u1d50\u1d52\u1d48\u1d49\u02e1 are modeled simultaneously with two types of encoders. The extrapolation encoders model the smooth transition of the states within event intervals, while the jump encoders simulate the abrupt state changes induced by event occurrences.\n4.3.1 State Extrapolations The extrapolation encoders model the drift functions of ODEs. Specifically, given the global extrapolation encoder f\u1d33 and the local extrapolation encoder f\u1d38, the global-local state extrpolation can be formulated as follows.\n(4.9) dz\u1d33(t) = f\u1d33(z\u1d33(t), t) dt\n(4.10) dZ\u1d38(t) = f\u1d38(Z\u1d38(t),t,G)dt\nGiven the initial state [z\u1d33(t\u2080), Z\u1d38(t\u2080)], we can compute the state at any time t before the next event occurrences by solving an initial value problem. Note that the evolution of the local states depends on the anchor graph G, because we associate each of the local states with an anchor node in the graph, and thus the local dynamics rely on the spatial correlations obtained by SAAG.\nThe internal structures of the global and local\ninformation learnt by SAAG is exploited by the local extrapolation encoder with the L-GCN network that encodes inter-region correlations.\n4.3.2 State Jumps The jump encoders simulate the abrupt state changes induced by event occurrences. The global-local dynamic states are updated with the information contained in the new event occurrences. The internal structures of the jump encoders are shown in Fig.1. Specifically, the global jump encoder J\u1d33 is simply a T-GRU network that takes the current location as input. The local jump encoder J\u1d38 first transforms the event location into feature vectors corresponding to each anchor node using RLE, before passing them to T-GRU as input."}, {"title": "4.4 Spaio-temporal Distribution Generation", "content": "The joint distribution of the target event p*(s,t) can be decomposed into temporal and spatial components as given in Eq.4.1. Thus, we propose to use two decoders to generate the two components, respectively. The temporal decoder predicts the conditional intensity function \u03bb*(t) using the global state z\u1d33. We formulate the temporal decoding network as follows.\n(4.11) \u03bb*(t) = softplus(MLP(z\u1d33(t)))\nwhere the softplus activation guarantees the non-negativity of the function. The conditional temporal pdf p*(t) can be approximated with Eq. 4.2.\nThe spatial decoder predicts the conditional spatial distribution of the event at the given time t, i.e., p*(s|t), from the local states Z\u1d38(t). The spatial distribution is formulated as a mixture distribution as follows.\n(4.12) p*(s|t) = \u2211\u1d62\u208c\u2081\u1d37 \u03b3\u1d62p\u1d62(s|t)\nwhere \u03b3\u1d62 is the mixture coefficient obtained as:\n(4.13) \u03b3\u1d62 = softmax (MLP(z\u1d62\u1d38(t)))\nThe i-th mixture component p\u1d62(s|t) is a Gaussian distribution generated from the i-th anchor node in SAAG.\n(4.14) p\u1d62(s|t) = \ud835\udca9(s; \u03bc(z\u1d62\u1d38(t), c\u1d62), \u03c3\u00b2(z\u1d62\u1d38(t)))\nwhere the mean network takes into account the current local state and location of the corresponding anchor node.\n(4.15) \u03bc(z\u1d62\u1d38(t), c\u1d62) = MLP(z\u1d62\u1d38(t)) + c\u1d62"}, {"title": "4.5 Maximum Log-likelihood Estimation", "content": "We propose to train our GSTPP with a Maximum Log-likelihood Estimation (MLE) approach. The training objective is the log-likelihood of the spatio-temporal event sequence S = {(t\u1d62, s\u1d62)}\u1d62=1\n(4.17) log p\u0398(S) = \u2211\u1d62\u208c\u2081\u1d3a log p\u0398(t\u1d62, s\u1d62)\nwhere \u0398 represents all the trainable parameters of the encoder-decoder network. The training process hence generalizes to the following optimization problem.\n(4.18) max\u0398 log p\u0398(S)\nIn practice, we use the back propagation and gradient descent methods to learn the model parameters. The adjoint sensitivity method is also adopted to address the efficiency issue of ODEs."}, {"title": "5 Experiments", "content": "5.1 Datasets We adopt three real-world spatio-temporal datasets for model evaluation and comparison, namely Earthquakes [32], COVID-19 [31], and CitiBike.Fig.3 shows the total sequence numbers and average sequence lengths of the three datasets. Using datasets of different scales helps us to understand our model's performance in different situations. Please refer to Appendix A for an introduction of the three datasets.\n5.2 Training Details The proposed GSTPP is implemented using Python with the Google JAX frame-"}, {"title": "6 Conclusion", "content": "In this work, we propose a novel GSTPP framework that promotes the performance of fine-grained spatio-temporal event prediction. We address the issue of spatial heterogeneity and correlations between different regions that greatly affect event occurrences, which has never been considered by state-of-the-art methods. The proposed framework incorporates global and local state dynamics with a novel encoder-decoder architecture. We introduce a novel self-adaptive anchor graph to capture the complex spatial dependencies within the continuous spatial area. By leveraging the spatial patterns learned by the graph, the model can make more accurate predictions of future events. Extensive experiments demonstrate the effectiveness of the proposed framework and validate its advantages over state-of-the-art approaches."}]}