{"title": "LAUGH NOW CRY LATER: CONTROLLING TIME-VARYING EMOTIONAL STATES OF FLOW-MATCHING-BASED ZERO-SHOT TEXT-TO-SPEECH", "authors": ["Haibin Wu", "Xiaofei Wang", "Sefik Emre Eskimez", "Manthan Thakker", "Daniel Tompkins", "Chung-Hsien Tsai", "Canrun Li", "Zhen Xiao", "Sheng Zhao", "Jinyu Li", "Naoyuki Kanda"], "abstract": "People change their tones of voice, often accompanied by non-verbal vocalizations (NVs) such as laughter and cries, to convey rich emotions. However, most text-to-speech (TTS) systems lack the capability to generate speech with rich emotions, including NVs. This paper introduces EmoCtrl-TTS, an emotion-controllable zero-shot TTS that can generate highly emotional speech with NVs for any speaker. EmoCtrl-TTS leverages arousal and valence values, as well as laughter embeddings, to condition the flow-matching-based zero-shot TTS. To achieve high-quality emotional speech generation, EmoCtrl-TTS is trained using more than 27,000 hours of expressive data curated based on pseudo-labeling. Comprehensive evaluations demonstrate that EmoCtrl-TTS excels in mimicking the emotions of audio prompts in speech-to-speech translation scenarios. We also show that EmoCtrl-TTS can capture emotion changes, express strong emotions, and generate various NVs in zero-shot TTS.", "sections": [{"title": "1. INTRODUCTION", "content": "Humans express a wide range of emotions by changing their tone of voice, often accompanied by nonverbal vocalizations (NVs) such as laughter and crying. While current emotional text-to-speech (TTS) systems have made significant advancements [1-12], they still lack the ability to generate emotional speech with fine-grained control (e.g. changing the emotion states within a single generated utterance) and with various types of NVs like laughter and crying. In addition, current emotional TTS systems [1-12] are typically trained on staged datasets with a limited number of speakers; in extreme cases, some are trained on only one speaker. These TTS models often lack the ability to generate emotional speech for any speaker, a feature critical for applications like speech-to-speech translation systems that need to retain both the emotion and speaker characteristics of the source audio when generating the translated speech.\nIn this paper, we introduce EmoCtrl-TTS, an emotion-controllable zero-shot TTS system that can generate highly emotional speech with NVs for any speaker. EmoCtrl-TTS generates the speech by mimicking the voice characteristics and emotion presented by an audio sample, referred to as an audio prompt. EmoCtrl-TTS is based on the flow-matching-based zero-shot TTS [13] and utilizes valence and arousal values to mimic the time-varying characteristics of emotions. In addition, it also utilizes laughter embeddings [14], which we find to be effective for generating not only laughter but also other NVs, including crying. Furthermore, by leveraging over 27k hours of highly expressive real-world data through careful data mining, EmoCtrl-TTS achieves significant enhancements in robustness. Comprehensive evaluations demonstrate that EmoCtrl-TTS excels in reproducing the emotions of audio prompts across multiple languages in speech-to-speech translation scenarios. We also show that EmoCtrl-TTS can capture emotion changes, express strong emotions, and generate various types of NVs in zero-shot TTS."}, {"title": "2. RELATED WORK", "content": "Emotional TTS has undergone substantial advancements in recent years. Table 1 lists various TTS systems from different perspectives.\nThe first point is whether the TTS systems can control the fine-grained emotional attributes within one utterance. Such fine-grained control is ideal for many applications, for example, speech-to-speech translation where nuanced emotional changes need to be transferred to the translated speech. However, as shown in the table, most prior works focused on controlling the utterance-level emotion, and only a few works tackled the control of time-varying emotional status. MsEmoTTS [7] used a local emotional strength predictor to estimate syllable-level emotion strength, leveraging it as a condition to control the emotion strength of the generated speech. ELaTE [14] leveraged laughter representation to condition the flow-matching-based zero-shot TTS, and showed superior controllability of laughter generation. However, these works still lack full controllability of the emotional status.\nThe second point is the capability to generate NVs. As far as we investigated, most prior emotional TTS works were not able to generate NVs. While ELaTE [14] can generate natural laughter, it was not investigated with other NVs such as cries. Our work aims to generate arbitrary types of NVs, including laughter and cries.\nThe third point is the size of the training data. Due to the difficulty in developing high-quality emotional training data with su-"}, {"title": "2.1. Controlling emotion in TTS", "content": null}, {"title": "3. \u0415\u041c\u041e\u0421TRL-TTS", "content": null}, {"title": "3.1. Overview", "content": null}, {"title": "3.1.1. Model training", "content": "Figure 1 (a) illustrates the training procedure of EmoCtrl-TTS. Given a training audio sample s with transcription y, we extract its mel-filterbank features $\\bm{\\hat{s}} \\in \\mathbb{R}^{F \\times T}$, where F denotes the feature dimension and T represents the sequence length. Additionally, we employ force alignment and a phoneme embedding layer to obtain a frame-wise phoneme embedding $\\bm{a} \\in \\mathbb{R}^{D_{phn} \\times T}$, where $D_{phn}$ is the phoneme embedding dimension. The phoneme embedding layer is a part of the audio model and is jointly trained. Furthermore, we extract frame-wise embeddings that represent NV $\\bm{h} \\in \\mathbb{R}^{D_{NV} \\times T}$ and emotion $\\bm{e} \\in \\mathbb{R}^{D_{emo} \\times T}$, where $D_{NV}$ and $D_{emo}$ denote the dimensions of NV and emotion embeddings respectively. The embeddings h and e are extracted by using pre-trained NV and emotion detector, respectively, which are discussed in Section 3.2 and 3.3. We leverage the speech infilling task introduced in [13] to train the audio model, focusing on training a conditional flow-matching model to estimate the distribution $P(\\bm{m} \\odot \\bm{\\hat{s}} | (1 - \\bm{m}) \\odot \\bm{\\hat{s}}, \\bm{a}, \\bm{h}, \\bm{e})$, where $\\bm{m} \\in \\{0, 1\\}^{F \\times T}$ represents a binary temporal mask, and $\\odot$ is the Hadamard product."}, {"title": "3.1.2. Inference", "content": "Figure 1 (b) illustrates the inference procedure of EmoCtrl-TTS. During inference, the model takes four inputs: text prompt $\\bm{y}_{text}$, speaker prompt audio $\\bm{s}_{spk}$, NV prompt audio $\\bm{s}_{NV}$, and emotion prompt audio $\\bm{s}_{emo}$. The text prompt represents the content of the generated speech. Meanwhile, the speaker, NV, and emotion prompts control the characteristics of the speaker, NV, and emotion"}, {"title": "2.2. Flow-matching-based TTS", "content": null}, {"title": "2.2.1. Conditional flow matching", "content": "Conditional flow-matching [15] is an objective for training generative models. Continuous normalizing flows [16] is employed to convert a simple prior distribution $p_0$ into a complex distribution $p_1$ that aligns with the data. To be specific, for a given data point x, a neural network parameterized by $\\theta$ models a time-dependent vector field $v_t(\\bm{x}; \\theta)$. This vector field constructs a flow $\\phi_t$, which reshapes the prior distribution into the target distribution. Lipman et al. [15] proposed to train such a neural network with the conditional flow-matching objective:\n$L_{CFM}(\\theta) = \\mathbb{E}_{t, q(\\bm{x}_1), p_t(\\bm{x} | \\bm{x}_1)} [|| \\bm{u}_t(\\bm{x} | \\bm{x}_1) - v_t(\\bm{x}; \\theta) ||^2]$,                                                                                                                                                                                                                            (1)\nwhere q denotes the training data distribution, $\\bm{x}_1$ represents the random variable for the training data, $p_t$ denotes the probability path at time step t, and $\\bm{u}_t$ is the vector field associated with $p_t$. Also, Lipman et al. [15] present a conditional flow known as the optimal transport path, defined by the equations $p_t(\\bm{x} | \\bm{x}_1) = \\mathcal{N}(\\bm{x} | t \\bm{x}_1, (1 - (1 - \\zeta_{min})t)^2 I)$ and $\\bm{u}_t(\\bm{x} | \\bm{x}_1) = (\\bm{x}_1 - (1 - \\zeta_{min}) \\bm{x}) / (1 - (1 - \\zeta_{min})t)$."}, {"title": "2.2.2. Voicebox", "content": "Voicebox [13] is the first to utilize conditional flow matching for training zero-shot TTS. It is designed to perform speech-infilling tasks given audio context and frame-wise phoneme sequence as conditions. Given the promising performance of Voicebox, we also employ conditional flow matching to develop our TTS model."}, {"title": "2.2.3. ELaTE", "content": "ELaTE [14] was proposed to generate natural laughing speech with fine-grained controllability. It utilized a frame-level laughter representation derived from the laughter detector [17,18] to condition the flow-matching-based zero-shot TTS, showing significantly higher quality and better controllability in generating laughing speech compared to conventional models. However, ELaTE was only tested with laughing speech, and the effects on other NVs, such as crying, have not been investigated. In our preliminary experiment, we found that ELaTE sometimes generates laughing speech even when the audio prompt contains different NVs, such as crying. Our work can be regarded as an extension of ELaTE, where we aim to achieve better emotion controllability as well as the generation of various NVs."}, {"title": "3.2. NV embeddings", "content": "For our proposed framework, it is essential to figure out a suitable embedding that can represent the characteristics of various NVs. In ELaTE [14], an embedding obtained from an off-the-shelf laughter detection model [17,18] was used to control laughter in the zero-shot TTS.\nOne of our findings in this work is that this laughter detector-based embedding actually captures a broader range of NV types than just laughter. By appropriately using the laughter-detector-based embedding, we have successfully generated various NVs such as crying and moaning. Therefore, throughout this paper, we use a 32-dimensional embedding from the laughter detection model as the NV embedding."}, {"title": "3.3. Emotion embeddings", "content": "Based on the Russell's circumplex model of emotion, emotions can be represented in two major ways [19]: Firstly, emotions can be categorized into different emotion classes, such as happiness or sadness, reflecting distinct emotional states. Secondly, emotions can be described using two attributes, arousal, and valence, sometimes with the third attribute of dominance. Arousal refers to the level of intensity or activation of the emotion, ranging from calm to highly stimulated. Valence refers to how pleasant or unpleasant an emotion is, ranging from very positive to very negative. Dominance relates to how much control one feels over the situation.\nFinding an effective emotion embedding is crucial for our framework. In our preliminary experiment, we used the eight emotion categories determined in [20] where each emotion category was represented by a learnable embedding, which is then used as $\\bm{e}_{spk}$. However, we found that the TTS model struggled to generate emotionally expressive speech with this approach. We also explored using the prosody encoder of the FACodec [21]. However, we found that the output of the prosody encoder contains phonetic information, resulting in the speech generation following the contents of the emotion prompt audio rather than the text prompt.\nUltimately, we identify a promising representation: arousal and valence values predicted by a pre-trained arousal-valence-dominance extractor [22]. This extractor is initialized with a wav2vec 2 model [23] and fine-tuned on MSP-PODCAST data [20] to predict arousal, valence, and dominance values. Chunk-wise arousal-valence values ($D_{emo} = 2$) are extracted using a sliding window with a window size of 0.5 seconds and a hop size of 0.25 seconds. Because the extractor outputs each value in the range of 0.0 to 1.0, we subtract 0.5 from the estimated value to adjust the range from -0.5 to 0.5. We align the length of the extracted values with the phoneme embedding through linear interpolation. This representation allows for capturing more nuanced emotional variations within each utterance. Note that our preliminary investigations revealed that the additional use of the dominance value hurt the audio quality; therefore, we omitted the dominance value."}, {"title": "3.4. Collecting large-scale emotional data with pseudo-labeling", "content": "The quantity and quality of training data is a crucial factor for achieving high-quality TTS. However, either the recording of emotional speech or manual annotation of the recording is costly, making it difficult to scale the data size to more than 100 hours.\nIn this work, we curate 27k hours of highly emotional data, referred to as In-house Emotion Data (IH-EMO) in this paper, from 200k hours of in-house unlabeled anonymized English audio [24]. The data curation procedure is as follows. We first employ the emotion2vec model [25] to obtain predicted emotion confidence scores. We retain the samples if the predicted emotion is {angry, disgusted, fearful, sad, surprised} or the predicted emotion is {neutral, happy} with a confidence score of 1.0. We further apply DNSMOS [26] and retain only samples whose OVLR score is greater than 3.0. Finally, we also apply an in-house speaker change detection model and discard the sample whenever a speaker change is detected. As a result, 27k hours of emotional audio are collected. We use an off-the-shelf speech recognition model to obtain the transcription."}, {"title": "4. EXPERIMENTS", "content": null}, {"title": "4.1. Data", "content": null}, {"title": "4.1.1. Training data", "content": "We used three training datasets: Libri-light, LAUGH, and IH-EMO. Libri-light was used for pre-training the audio model without NV and emotion embeddings. On the other hand, LAUGH and IH-EMO were used for fine-tuning the audio model with NV and emotion embeddings. During fine-tuning, we also used the Libri-light data with a certain probability as suggested in [14]. The overview of each dataset is as follows.\nLibri-light [27]: 60k hours of untranscribed English audio-books from over 7,000 speakers. We transcribed the data by using a pre-trained Kaldi ASR model, which was trained on the 960-hour LibriSpeech dataset [28].\nLAUGH: 460 hours of laughing speech collected from the AMI meeting corpus [29], Switchboard corpus [30], and Fisher corpus [31]. We gather all the utterances marked with laughter from the transcriptions of each corpus. Note that the dataset still contains a substantial amount of neutral speech, as laughter tends to occur at certain parts of the speech.\nIH-EMO: 27k hours of collected emotional speech as described in Section 3.4."}, {"title": "4.1.2. Evaluation data", "content": "We used four evaluation datasets as presented in Table 2.\nJVNV speech-to-speech translation (S2ST): To evaluate the emotion transferability of zero-shot TTS models, we established an experimental setting based on the Japanese-to-English S2ST scenario. In the evaluation, we used a Japanese speech from the JVNV corpus [32]. The JVNV corpus includes speeches from four speakers (two males and two females) with six emotions: anger, disgust, fear, happiness, sadness, and surprise with various NVs. With its intense emotional expressions and various NVs, the JVNV dataset offers comprehensive coverage of expressive emotions, making it ideal for our emotion transfer testing. In the evaluation process, we first applied speech-to-text translation to the Japanese speech to obtain the English translations. We then used a zero-shot TTS model, using the English translations as a text prompt and the Japanese speech as an audio prompt, from which we extracted both NV and emotion embeddings. The generated speech is expected to be English-translated speech with the original speaker's voice and emotional characteristics. We assessed the similarity of the speaker and emotion between the source Japanese speech and the translated English speech using various metrics described in the next section.\nEMO-change: To test the model's capacity for fine-grained emotional speech generation, we created the EMO-change dataset based on the RAVDESS dataset [33], which contains English emotional speech data with emotions such as calm, happy, sad, angry, fearful, surprised, and disgusted. The RAVDESS dataset includes two transcriptions: \"kids are talking by the door\" and \"dogs are sitting by the door\" with intense emotional expressions. To generate the EMO-change dataset, we randomly select two utterances with different emotions with the transcription \"kids are talking by the door,\" remove the silence from each, and concatenate them to create the audio prompts. During testing, the concatenated emotion change sample serves as the audio prompt, and a repeated sentence \"dogs are sitting by the door dogs are sitting by the door\" serves as the text prompt for the zero-shot TTS model. This setup evaluates the model's ability to generate speech that mimics the emotional transitions in the audio prompt while following the given text prompt, and speaker characteristics.\nLaughter-test: To evaluate the zero-shot TTS capability in generating laughing speech, we employed a Chinese-to-English S2ST experiment by following the protocol presented in [14]. Specifically, we used 154 Chinese utterances containing laughter from the evaluation subset of the DiariST-AliMeeting test set [34]. We applied zero-shot TTS, where we used the ground-truth English transcription as the text prompt and the Chinese speech as the audio prompt to extract both NV and emotion embeddings. The generated speech is expected to be English speech with the original speaker's voice and laughter characteristics.\nCrying-test: To evaluate the zero-shot TTS capability in generating crying speech, we further conducted a Chinese-to-English S2ST experiment using 33 Chinese crying speech samples collected from publicly available data sources. We followed the same procedure with the Laughter-test, and the generated speech is expected to be English speech with the original speaker's voice and crying characteristics."}, {"title": "4.2. Evaluation metrics", "content": null}, {"title": "4.2.1. Objective evaluation metrics", "content": "We utilized the following objective metrics. Among them, AutoPCP, EMO SIM, and Aro-Val SIM are closely related to the emotion controllability.\nWord error rate (WER): To evaluate the intelligibility of the generated audio, we applied a Whisper-Large [35] to the generated audio and computed the WER. In all our tables, we express the WER in percentage.\nSpeaker SIM-o: To evaluate the speaker similarity between the generated audio and the audio prompt, we computed a cosine similarity between speaker embeddings of the two audios. We used a WavLM-large-based speaker verification model [36], following previous works [13,37].\nAutoPCP: AutoPCP [38] is an utterance-level estimator to quantify the prosody similarity between two speech samples. We computed the score between the generated audio and the audio prompt. We leveraged AutoPCP_multilingual_v2.\nEmo SIM: To evaluate the similarity of time-varying emotion states, we applied the emotion2vec model [25] to extract the emotion embeddings. We performed interpolation to ensure the embeddings of the audio prompt and the generated audio have the same length. We then computed the cosine similarity between these two embedding sequences for each frame and took the average to obtain the EMO SIM score.\nAro-Val SIM: As another metric for the similarity of time-varying emotion states, we computed the arousal-valence values based on [22] using a sliding window with a window size of 0.5 sec and a hop size of 0.25 sec. Similar to EMO SIM, we computed the cosine similarity between the audio prompt and the generated audio for every frame and took the average as the Aro-Val SIM."}, {"title": "4.2.2. Subjective evaluation metrics", "content": "We used the following subjective evaluation metrics.\nSMOS: Speaker similarity mean opinion score, which is the similarity between the speaker prompt and the generated speech from 1 (not at all similar) to 5 (extremely similar).\nNMOS: Naturalness MOS, which is the naturalness of the generated speech from 1 (bad) to 5 (excellent).\nEMOS: Emotion MOS, which measures the similarity of emotion between the audio prompt and the generated speech from 1 (not at all similar) to 5 (extremely similar)."}, {"title": "4.3. Model configuration", "content": "The architecture of the EmoCtrl-TTS audio model closely followed the configurations of the Voicebox [13]. Specifically, we used a Transformer [39] with 24 layers, featuring 16 attention heads, a 1024-dimensional embedding, and a feed-forward layer with a dimension of 4096.\nThe model was pre-trained using Libri-light data, without NV or emotion embedding. This resulted in the reproduction of the Voice-box model (B2 in Table 3). The model was trained for 390K steps with an effective mini-batch size of 307,200 audio frames. A linear-decay learning rate scheduler with a peak learning rate at 7.5e-5 was used, along with 20K steps of linear warmup. After the pre-training, we further fine-tuned the model by combining Libri-light, LAUGH, and IH-EMO. During fine-tuning, the effective mini-batch size was set to 307,200 audio frames. A linear-decay learning rate scheduler was used with a peak learning rate of 7.5e-5. Unless otherwise stated, we fine-tuned the model with 40k steps.\nDuring the inference, we used classifier-free guidance with a guidance strength of 1.0, and the number of function evaluations was set to 32. A MelGAN-based vocoder [40] was used to convert the mel spectrogram into waveforms."}, {"title": "4.4. S2ST pipleline", "content": "For the JVNV S2ST evaluation data, we leveraged the Whisper large-v3 model [35] to transcribe the Japanese utterances with time stamps. We then employed GPT-4 [41] to translate the time-stamped Japanese text into English by keeping the time stamps. We then used a total-duration-aware (TDA) duration model [42] to obtain a frame-wise phoneme alignment. For the Laughter-test and Crying-test, we used the ground-truth English text translation, and the same TDA duration model to obtain the frame-wise phoneme alignment."}, {"title": "4.5. Results and discussion", "content": null}, {"title": "4.5.1. Objective evaluation", "content": "Table 3 presents the objective evaluation results on the JVNV and EMO-change datasets. During the evaluation, we generated speech with three different random seeds, and the average of the scores was taken to report.\nBaseline analysis: For the JVNV S2ST test set, we first observed that the ELaTE (B3), trained with LL and LAUGH data, achieved superior performance over the SeamlessExpressive model [38] (B1) and the reproduced VoiceBox model (B2) across all the metrics, except for WER. The comparison between B2 and B4 highlights the effect of LAUGH data, showing that the latter improved performance in SIM-o and AutoPCP while maintaining WER, EMO SIM, and Aro-Val SIM. By replacing LAUGH with the IH-EMO data (B4 vs. B5), SIM-o and WER were improved from 0.410 to 0.479, and from 2.5% to 2.2%, respectively. However, this came at the cost of a slight degradation of AutoPCP (3.07 2.96), Emo SIM (0.645 0.641), and Aro-Val SIM (0.438 0.397). Combining all training data (B6) gave us a balanced result. However, even after these trials, no Voicebox model achieved better AutoPCP, EmoSIM, and Aro-Val SIM compared to ELaTE. The same trends were also observed for the EMO-change test set. These results suggest that simply adding emotional data does not improve the emotion transferability of the zero-shot TTS models.\nResults of EmoCtrl-TTS: From JVNV S2ST results, we first observed (P1) showed a significant improvement in AutoPCP, Emo SIM, and Aro-Val SIM compared to the baselines. By executing a longer fine-tuning (P2), EmoCtrl-TTS achieved further better SIM-0, WER, AutoPCP, and Emo SIM while almost preserving the high Aro-Val SIM. For the EMO-change test set, EmoCtro-TTS achieved the best SIM-o and Aro-Val SIM and the second-best AutoPCP. It is also noteworthy that SIM-o score was significantly improved from (B6) to (P1), which suggests the effectiveness of the use of NV and emotion embeddings not only for the emotion-related metrics but also for the speaker similarity. Note that we have trained more variants of EmoCtrl-TTS to analyze the impact of each data and each embedding, which will be discussed in Section 4.5.3 with Table 5."}, {"title": "4.5.2. Subjective evaluation", "content": "We performed the subjective evaluation for selected models. We randomly picked 24 samples from JVNV S2ST data and 28 samples from EMO-change data, and asked native English testers to rate SMOS, NMOS and EMOS. For JVNV S2ST test set, each sample was judged by 9, 11, and 10 testers for SMOS, NMOS, and EMOS, respectively. For EMO-change test set, each sample was judged by 12 testers for all metrics.\nThe results are presented in Table 4. From the JVNV S2ST result, the Voicebox model (B6), leveraging the IH-EMO data, demonstrated significant improvements of SMOS and EMOS from the vanilla Voicebox model (B2), which illustrates the importance of the training data. Meanwhile, ELaTE (B3) excelled in NMOS, suggesting that the integration of NV features significantly boosts the naturalness of the audio by generating an appropriate NV. Finally, EmoCtrl-TTS (P2) achieved even better NMOS while keeping the high SMOS and EMOS scores.\nIn the EMO-change dataset, Voicebox (B6) showed significantly worse EMOS compared to ELaTE (B3) and EmoCtlr-TTS (P2). This result demonstrated that the conventional zero-shot TTS cannot mimic the time-varying emotional states presented in the audio prompt. The proposed EmoCtrl-TTS (P2) achieved best SMOS and comparable NMOS and EMOS to other top baselines, again showcasing the efficacy of the proposed approach."}, {"title": "4.5.3. Impact of training configurations", "content": "Table 5 presents the results of the JVNV S2ST test set with various training data configurations.\nThe first four rows compare the performance of models trained by Libri-light and LAUGH data with a 0.5:0.5 ratio. By comparing the first two rows, we observe that the NV embedding h significantly improved SIM-o, AutoPCP, Emo SIM, and Aro-Val SIM with the expense of degradation of WER. By introducing emotion embedding e, AutoPCP, EmoSIM, and Aro-Val SIM were further improved. However, it came with a cost of further degradation of WER as well as a significant drop in SIM-o.\nIn the 5th to 9th rows, we trained models using the Libri-light and IH-EMO data. Similar to the case with LAUGH data, we observed that the emotion embedding e improved all the emotion-related metrics while keeping SIM-o and WER nearly intact. However, in contrast to the results with the LAUGH data, we observed a severe degradation in WER when the NV embedding h was introduced. Upon examination, we found that the NV embedding often resulted in unwanted NV generation, such as laughter from multiple speakers.\nBoth observations suggest that including both NV and emotion embeddings in model training is not always beneficial. Based on these findings, we opted to use only emotion embeddings for IH-EMO and only NV embeddings for LAUGH when combining all training data.\nThe last four rows in Table 5 show the impact of data ratio and training steps on model performance with the combination of all training data. We first observed that the data ratio of 0.5:0.4:0.1 for Libri-light, IH-EMO, and LAUGH provided us with a better SIM-o and WER than the ratio of 0.5:0.25:0.25 while keeping the emotion-related metrics similar. By fine-tuning the model longer, further marginal improvement was obtained for all evaluation metrics."}, {"title": "4.5.4. Results on real laughter and crying data", "content": "Table 6 and 7 present a comparison between the EmoCtrl-TTS model and selected baselines on the Laughter-test and Crying-test datasets, both of which are composed of real data. Compared to the baselines, the EmoCtrl-TTS consistently achieved the best performance for Emo SIM and Aro-Val SIM while achieving either the best or the second-best Auto PCP and SIM-o. It demonstrated EmoCtrl-TTS's robustness for the real data. On the other hand, we observe a moderate degradation of WERS."}, {"title": "5. CONCLUSIONS", "content": "In this work, we presented EmoCtrl-TTS, an emotion-controllable zero-shot TTS model that can generate highly emotional speech with NVs for any speaker. EmoCtrl-TTS leveraged arousal and valence values as well as the laughter embeddings to control the time-varying characteristics of emotional speech including NVs. Our comprehensive experiments demonstrated that EmoCtrl-TTS can closely mimic the voice characteristics and nuances of the source audio prompts by generating emotional speech with NVs."}]}