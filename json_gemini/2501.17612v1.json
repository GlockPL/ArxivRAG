{"title": "VoicePrompter: Robust Zero-Shot Voice Conversion with Voice Prompt and Conditional Flow Matching", "authors": ["Ha-Yeong Choi", "Jaehan Park"], "abstract": "Despite remarkable advancements in recent voice conversion (VC) systems, enhancing speaker similarity in zero-shot scenarios remains challenging. This challenge arises from the difficulty of generalizing and adapting speaker characteristics in speech within zero-shot environments, which is further complicated by mismatch between the training and inference processes. To address these challenges, we propose VoicePrompter, a robust zero-shot VC model that leverages in-context learning with voice prompts. VoicePrompter is composed of (1) a factorization method that disentangles speech components and (2) a DiT-based conditional flow matching (CFM) decoder that conditions on these factorized features and voice prompts. Additionally, (3) latent mixup is used to enhance in-context learning by combining various speaker features. This approach improves speaker similarity and naturalness in zero-shot VC by applying mixup to latent representations. Experimental results demonstrate that VoicePrompter outperforms existing zero-shot VC systems in terms of speaker similarity, speech intelligibility, and audio quality. Our demo is available at https://hayeong0.github.io/VoicePrompter-demo/.", "sections": [{"title": "I. INTRODUCTION", "content": "Zero-shot voice conversion (VC) systems [1]\u2013[8] have gained significant attention with the advancement of deep generative models. In particular, diffusion-based VC models [9] have demonstrated high performance in zero-shot speaker adaptation through iterative sampling processes. Recent works, such as Diff-HierVC [10] and DDDM-VC [11], have further improved zero-shot VC performance by adopting source-filter disentanglement and disentangled denoising processes. NaturalSpeech 3 [12] enhanced voice style transfer performance by disentangling the speech into timbre, prosody, content, and residual details. However, diffusion-based models are limited by slow inference speed due to their iterative sampling. Additionally, these models are vulnerable to noisy speech, often generating noisy sound when conditioned on global style embedding extracted from noisy target speech.\nMeanwhile, recent advancements in text-to-speech models have shifted from global style conditioning [13]\u2013[16] to voice prompting methods [17]\u2013[21] for target speaker adaptation. VALL-E [17] was the first to adopt in-context learning for speaker adaptation by concatenating the audio codec into the input sequences. Similarly, VoiceBox [18] was trained using a masking and infilling speech, where speech was generated by infilling masked input sequences with conditional flow matching (CFM). Speech generation with prompting mechanisms can endow the model with in-context learning capability, enabling them to follow the style of a given voice prompt. However, VC models with voice prompts have not yet been thoroughly investigated, primarily due to the challenges of speech disentanglement.\nIn this paper, we present VoicePrompter, a robust zero-shot VC model with in-context learning ability using voice prompt. We first adopt a diffusion transformer with CFM as the backbone model. For speech perturbation, we train the model to estimate the vector field based on features extracted by a speech disentangle encoder, augmented with latent mixup. Then, the model is trained by masking sequences and infilling speech to emerge in-context learning ability. The results demonstrate that it is essential to improve the robustness by prompting the target voice when infilling speech from the augmented speech presentation using mixup. By guiding the target voice style with prompts during conversion, our model achieves better speaker similarity compared to recent powerful baselines. Our main contributions are summarized as follows:\n\u2022 We propose VoicePrompter, a robust zero-shot VC system that leverages in-context learning with voice prompts to achieve high speaker similarity.\n\u2022 We improve the robustness of VC by incorporating latent mixup and speech infilling approaches.\n\u2022 Thanks to the integration of the CFM and adaLN-sep within the DiT backbone, VoicePrompter achieves successful VC and high audio quality in a single step.\n\u2022 The results show that our model outperforms recent powerful baselines in terms of speaker similarity, speech intelligibility, and audio quality."}, {"title": "II. VOICE PROMPTER", "content": "In this section, we introduce our proposed system, VoicePrompter. As depicted in Fig. 1, our model consists of two main components: (1) a speech factorizing encoder that effectively disentangles and embeds the input speech, and (2) a DiT-based CFM decoder that conditions on factorized speech features and voice prompts. The details are described in the following subsection."}, {"title": "A. Speech Factorizing Encoder", "content": "1) Content Encoder: To extract linguistic information from input audio, we utilize the seventh layer of a pre-trained MMS [22] model. Given that MMS embeddings include acoustic information, we apply signal perturbation to the input audio to isolate linguistic information independent of speaker characteristics. The extracted MMS embeddings are then modeled alongside speaker information using an 8-layer WaveNet-based [23] content encoder.\n2) Pitch Encoder: For pitch extraction, we employ Praat [24] to obtain FO values. These extracted FO values are embedded according to the encoder's hidden layer configuration, and the embedded pitch information is processed through a temporal bottleneck layer. Subsequently, a pitch encoder, utilizing the same WaveNet architecture as the content encoder, models the pitch information.\n3) Speaker Encoder: We extract speaker information by applying spectral feature extraction on the Mel-spectrogram using 1D convolutional layers. Temporal features are refined with a Conv1dGLU layer, and long-term dependencies are captured through multi-head attention. A final 1D convolutional layer generates the style representation, which is then used for speaker adaptation across all encoders and decoders."}, {"title": "B. Conditional Flow Matching Decoder", "content": "To generate high-quality Mel-spectrograms, we employ a conditional flow matching [25], [26] structure utilizing an optimal transport (OT) path. FM starts with a noise sample $x_0$ drawn from a standard Gaussian distribution and learns the time-conditioned transformation $t$ that maps it to the target sample $x_1$, with this flow controlled by an ordinary differential equations (ODEs). The time-conditioned vector field $u_t$ can be chosen as an OT path, and the corresponding vector field is estimated by the vector field estimator network $v_\\theta$. In this process, the network is conditioned on the factorized speech feature $z$, voice prompt $p$, and speaker embedding $e_{spk}$ to predict the vector field. The computation of the CFM loss is detailed in Algorithm 1."}, {"title": "C. Voice Prompt for In-Context Learning", "content": "Drawing inspiration from previous work [18], [21] that leveraged masking strategies for in-context learning, we explore a method to incorporate direct target voice prompts into the VC task. Unlike prior approaches, which used only encoded feature from the encoder as conditioning, we introduce a method where the input speech is masked by 70-100% and utilized alongside the decoder's embedding as conditioning input. Specifically, we adopt the same masking strategy as VoiceBox [18], computing the CFM loss only on the masked segments. In the inference phase, as shown in Fig. 1-(b), we perform an in-filling task where the masked portions are predicted using the source content information and the factorized feature z, which encodes the target's timbre."}, {"title": "D. DiT with AdaLN-Sep", "content": "We employ DiT as the backbone for the CFM decoder and introduce adaLN-Sep, a conditioning method derived from a modification of adaLN-Zero. Fig. 2-(a) depicts the adaLN-Zero approach, which demonstrated superior performance in previous DiT [27] research by exploring various methods of integrating conditioning into transformer blocks. The adaLN-Zero can be described as follows:\n$AdaLN\\text{-}Zero(h, c) = \\alpha_c (c \\cdot LN(h) + \\beta_c),$\nThe adaLN-Zero block accelerates training by zero-initializing the scaling parameters at the end of each residual block and applying dimension-wise scaling parameters before the residual connections. However, the conventional adaLN-zero has a limitation in that it processes the conditioning information together, which does not fully reflect the independent characteristics of each feature. To address this and enhance both speaker similarity and training efficiency, we propose adaLN-Sep, which separates speaker and time embeddings, as illustrated in Fig. 2-(b). This method independently integrates each conditioning information into the transformer block, and adaLN-Sep is defined as follows:\n$AdaLN\\text{-}Sep(h, s,t) = \\begin{cases} \\alpha_s (s LN(h) + \\beta_s), & (\\text{for SA Block}) \\\\ \\alpha_t (t LN(h) + \\beta_t), & (\\text{for FFN Block}) \\end{cases}$"}, {"title": "E. Latent Mixup", "content": "Although the speaker adaptation performance has improved through the integration of the voice prompt and the powerful DiT backbone network, train-inference mismatch problem still remains in zero-shot VC. Following [11], during the training phase, we perform latent mixup by randomly combining representations from different speakers to perturb the speech components. Specifically, latent mixup is conducted on 50% of the batch size. When mixup is not applied, the process can be represented as follows:\n$E_{cont}(z_{cont,x}, z_{spk,x}) + E_{F0}(z_{F0,x}, z_{spk,x}) = z$\nwhere $E_{cont}$ denotes the content encoder, $E_{F0}$ denotes the FO encoder, and $z_{spk,x}$ represents the style information of speaker $x$. In the case where mixup is applied, the formulation is expressed as follows:\n$E_{cont}(z_{cont,x}, z_{spk,y}) + E_{F0}(z_{F0,x}, z_{spk,y}) = z_{mix}$\nwhere $z_{cont,x}$ and $z_{F0,x}$ refer to the content and F0 information of speaker x, and $z_{spk,y}$ represents the style information of speaker y. We use $z_{mix}$ as a condition for the CFM decoder, and is employed for the encoder's reconstruction. This strategy allows us to disentangle and embed the relevant factors more robustly, ultimately leading to improved generalization in the zero-shot VC task."}, {"title": "III. EXPERIMENT AND RESULT", "content": "Dataset We trained our model using the multi-speaker LibriTTS [28], specifically the train-clean-100 and train-clean-360 subsets, which include 245 hours of speech from 1,151 speakers. For validation, we used the dev-clean subset. To evaluate zero-shot VC, we selected random sentences from the VCTK [29].\nPreprocessing We resampled the audio to 16,000 Hz using the Kaiser-best algorithm from torchaudio [30]. The Mel-spectrogram was generated with a hop size of 320, a window size of 1280, an FFT size of 1280, and a bin size of 80.\nImplementation Details We trained the model for 300K steps with a batch size of 64 on two NVIDIA A100 GPUs, and applied the same setup for training the ablation models. The learning rate was set to $2 \\times 10^{-4}$, and the AdamW optimizer was used. For the vocoder, we trained BigVGAN [31] on LibriTTS, adapting it to our 16 kHz Mel settings."}, {"title": "B. Zero-shot Voice Conversion", "content": "We conduct various subjective and objective evaluation on the zero-shot VC task with four strong VC baseline: DiffVC 1, Diff-HierVC 2, DDDM-VC 3, and NaturalSpeech (NS) 3's VC model, FACodec 4. Each model was evaluated using official checkpoints, with a consistent sampling of 6 steps applied to all baselines except NS 3 for fair comparison. For subjective evaluation, we conduct the naturalness (NMOS) and similarity mean opinion score (SMOS), and UTMOS [32]. For objective evaluation, we utilized four key metrics: character error rate (CER), word error rate (WER), equal error rate (EER), and speaker encoder cosine similarity (SECS). To evaluate the accuracy of intelligibility, we used Whisper-large-v2 [33] to measure CER and WER, and evaluated the EER using an automatic speaker verification model. We also calculated SECS with Resemblyzer. The results in Table I show that our model delivers strong performance in terms of speaker similarity, speech clarity, and overall audio quality."}, {"title": "C. Ablation Study", "content": "We conducted ablation studies for latent mixup, voice prompt, and AdaLN-Sep to demonstrate the effectiveness of the proposed methods. We first followed the mixup of DDDM-VC [11] to perturb the speaker information before fed to the CFM decoder. However, as shown in Table II, while latent mixup improved speaker similarity, it led to a decrease in audio quality. This reduction in quality is likely due to the perturbed representation, which can affect the model's robustness. To address this issue, we utilized voice prompts along with latent mixup to guide the voice information and enhance the model's robustness during training. Table II shows that using both mixup and voice prompts mechanism significantly improves the performance in terms of speaker similarity and audio quality. Furthermore, AdaLN-Sep could enhance the adaptation performance by conditioning speaker and time embeddings separately, and we also observed that AdaLN-Sep accelerates the training process."}, {"title": "D. Sampling Steps", "content": "We compared the performance of VoicePrompter according to sampling steps. We found that our model could convert the speech even with a single step generation. However, increasing the sampling steps consistently increase the speaker similarity in terms of SECS. Although the UTMOS of each result showed a similar score, we found that increasing the sampling step could improve the perceptual quality. We have added the audio samples based on the sampling steps on the demo page5."}, {"title": "E. Scaling Down Model Size", "content": "We scale down model size to evaluate the robustness of our structure. The details of model hyperparameter are described in TABLE IV. Table V reveals that VoicePrompter-S still showed lower CER and WER compared to baseline models. Furthermore, both models has better speaker similarity than other baselines. The results also demonstrated that increasing the model size could enhance the capability for voice style transfer. Moreover, scaling up model size could significantly improve the audio quality. In future work, we will further scale up both model size and data size for better generalization."}, {"title": "IV. CONCLUSION", "content": "In this paper, we proposed VoicePrompter, a zero-shot VC model designed to enhance in-context learning capabilities through the voice prompts. Our model adopts a DiT as its backbone, incorporating adaLN-sep, and estimates vector fields using a flow matching conditioned on factorized speech features. This design allowed the model to achieve robust speaker adaptation performance. Notably, we introduced a voice prompt method that combined latent mixup with sequence masking, which significantly improved the robustness. Experimental results demonstrated that using target voice prompts during inference process could maximize speaker similarity. VoicePrompter showed outstanding performance in zero-shot domain, proving that prompting techniques offered new possibilities in VC tasks. The findings suggested that prompting could greatly enhance perceptual performance in VC and highlighted the potential of high-quality backbone models to maintain superior audio quality."}]}