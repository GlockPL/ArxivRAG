{"title": "Watch Less, Feel More: Sim-to-Real RL for Generalizable Articulated Object Manipulation via Motion Adaptation and Impedance Control", "authors": ["Tan-Dzung Do", "Nandiraju Gireesh", "Jilong Wang", "He Wang"], "abstract": "Articulated object manipulation poses a unique challenge compared to rigid object manipulation as the object itself represents a dynamic environment. In this work, we present a novel RL-based pipeline equipped with variable impedance control and motion adaptation leveraging observation history for generalizable articulated object manipulation, focusing on smooth and dexterous motion during zero-shot sim-to-real transfer (Fig. 1). To mitigate the sim-to-real gap, our pipeline diminishes reliance on vision by not leveraging the vision data feature (RGBD/pointcloud) directly as policy input but rather extracting useful low-dimensional data first via off-the-shelf modules. Additionally, we experience less sim-to-real gap by inferring object motion and its intrinsic properties via observation history as well as utilizing impedance control both in the simulation and in the real world. Furthermore, we develop a well-designed training setting with great randomization and a specialized reward system (task-aware and motion-aware) that enables multi-staged, end-to-end manipulation without heuristic motion planning. To the best of our knowledge, our policy is the first to report 84% success rate in the real world via extensive experiments with various unseen objects. Webpage: https://watch-less-feel-more.github.io/", "sections": [{"title": "I. INTRODUCTION", "content": "A generalist robot represents a big milestone for the robot learning community, with the potential to revolutionize our daily life. With the ubiquity of articulated objects in both household and industry settings, learning how to efficiently manipulate them is one of the main challenges to achieving this goal. Amid the great progress in the embodied AI field in these couple of years [1]\u2013[4], generalizable articulated object manipulation remains an open question due to various reasons. One major challenge is that the true articulation characteristics (e.g. pivot center, friction, stiffness) could only be identified after physical contact is made. For instance, two objects might appear identical but their physical properties differ significantly. As a result, in order to achieve a generalizable articulated object manipulation pipeline that can seamlessly interact with unseen objects, it necessitates a closed-loop pipeline that can adaptively infer these characteristics during the manipulation stage. Another difficulty lies in the joint constraints of objects which require the applied actions to comply with the actual object joint motion. If the robot actions do not tolerate object joint motion and prioritize completing the given commands, it could result in large forces and damage to both objects and the robot.\nRecent articulated object manipulation works often rely on visual information as the dominant input for their pipelines. Some prior works leverage vision input in the first frame, either in the form of pointcloud [5]\u2013[7] or RGB images [6], [8]\u2013[12], to predict actionable parts followed by a sequence of actions or a waypoint trajectory. This sequence or way-"}, {"title": "II. RELATED WORK", "content": "Manipulating articulated objects is highly challenging due to the wide variety of object geometries and physical properties. Recent works on articulated object manipulation can be broadly categorized into affordance-based and RL-based methods. Affordance-based approaches rely on visual affordance heatmaps [18] where each point corresponds to the success rate of manipulation to choose contact points and predict actions [11], [14], [17], [19]. However, this approach often neglects physical interaction and suffers from large sim-to-real gap [5], [11], [12], which limits their generalizable capability to novel scenes. On the other hand, RL-based methods [13], [15], [16] with closed-loop feedback have shown better generalization capability. Nevertheless, they utilize point-cloud features as an input to the policy, which makes the exploration space vast and complicates the task. These pipelines also leverage visual input for each inference step which inherently introduces more sim-to-real gap. Our work only leverages low-dimensional vision information captured in the first frame and incorporates history observation during the manipulation stage for better object motion understanding with RL."}, {"title": "B. Impedance control for learning-based methods", "content": "Impedance control belongs to the position-force control family where position and force are not decoupled but simultaneously processed, thus enhancing tolerance to feed-back force while maintaining a good tracking state. Many contact-rich robotic tasks such as object placement [20] or tool assembly [21]\u2013[25] have successfully demonstrated the compatibility of this type of controller for tasks that consider both position setpoint tracking and object-robot force con-straints. For learning-based methods, many works [8], [26], [27] introduce impedance control as an off-the-shelf low-level controller for downstream command execution guided by a policy. Some directly incorporate impedance control parameters as learnable variables for RL [21], [28], inverse RL [29], or analytical optimization methods [30]. These works also showcase that variable impedance control can be more generalizable to different task settings and less labor-expensive than manually tuned impedance control. In this work, we extend the application of impedance control for"}, {"title": "III. PROBLEM STATEMENT", "content": "Given an articulated object O and a manipulation task 0, we train a policy \u03c0 to output one dexterous action at a time to finish the task in a closed-loop manner.\nOur task definition is a more challenging and realistic adaptation of VAT-MART [17] and subsequent affordance works [11], [12]. Our pulling task (open doors, drawers) requires the policy to reach, grasp actionable parts, and then open untill the object's joint position reaches at least 80% of the joint limit instead of about half-way [8], [17]. This criterion, especially when applied to revolute joints, necessitates much dexterous and long-horizon motions since the robot needs to follow the actual SE(3) movements of objects. Moreover, in our settings, we allow only realistic IK configuration of robots (a fixed-base Franka) and do not assume the absolute feasibility of predicted motions as with other waypoint prediction pipelines using a flying gripper or suction cup [11], [12], [14]."}, {"title": "IV. PROPOSED METHOD", "content": "We design our framework to facilitate one dexterous action prediction at a time instead of short-horizon primitive actions. Our action for each step af \u2208 R11 includes the target delta position \u2206xyz \u2208 R\u00b3, target 6D orientation R\u02bb \u2208 R6, gripper action G\u02bb \u2208 R\u00b9, and impedance control parameter k', \u2208 R\u00b9. Our raw robot action a' is later converted into robot commands c\u2208 R using an action scaler.\nOur observation of consists of desired grasping pose g\u00b9 \u2208 R7, robot joint configuration q \u2208 R7, robot-object relative distance \u03b4' \u2208 R\u00b9, end-effector pose eet \u2208 R9 with three-dimensional position and 6D rotation, and graspability 1'grasp \u2208 R\u00b9. Here, desired grasping poses are directly in-ferred from the handle bounding box in the simulation and from off-the-shelf grasp prediction modules in the real world. Our graspability signal is a distance-based and contact-aware condition, rather than a direct command for open/close gripper. In terms of task-aware observation, for instance, with DoorOpen task, we incorporate noisy pivot center pivot \u2208 R3, noisy pivot radius radius \u2208 R\u00b9, and right-hinged boolean Fn \u2208 R\u00b9. These motion-related arguments serve as high-level guidance for smoother implementation.\no' = [g',q', 8', ee', 1'grasp, p grasp pivot, radius,rh] E IR30\nOur privileged observation o\u02bbpriv, including values that are difficult to track in real-world settings, is used only in simulation for better environment understanding. These values are: pivot center rpivot \u2208 R\u00b3, pivot radius rradius \u2208 R1, object stiffness rstiff \u2208 R1, object mass rm \u2208 R\u00b9, object joint position dobj \u2208 R\u00b9, handle grasped signal 1 1'grasped \u2208 R1.\nOpriv = [rpivot, radius, m, stiff, dobj, grasped] \u2208 R8"}, {"title": "B. Online policy distillation with Observation History", "content": "Articulated object manipulation poses a unique challenge compared to rigid object manipulation because the object itself is a dynamic environment. The fact that object motion can only be observed via physical interactions or that joint ground-truth position is hidden inside the object resembles locomotion tasks where environment parameters (e.g. ter-rain friction, slope) are difficult to predict. To this end, we adopt the online policy distillation pipeline, which is widely applied for locomotion tasks [3], [4], [31], and learn two separate modules: Adaptation Module \u03c3 and Privileged Observation Encoder $ (Fig. 2).\nPrivileged Observation Encoder is a shallow MLP, which is utilized during training to learn the latent repre-sentation z' of privileged observations. This 20-dimensional vector is then concatenated with an (observation, action) pair p\u00b2 = (o* \u2295af\u22121) at the current timestep to form actor inputs. We design the Adaptation Module \u03c3 to be a temporal archi-tecture to extract latent information about the environment from H = 10 p\u00b9 pairs. We keep only parts of action history as inputs for \u03c3: position command Axyz, gripper command G', and controller gain k\u2081.\nAs the conventional two-staged teacher-student pipeline might result in realizability gap and sim-to-real gap [31], we simultaneously train Adaptation Module and Privileged Observation Encoder in a single training. Specifically, when jointly train the Adaptation Module with our RL backbone, we also learn to extract similar privileged information Z from history buffer by formulating a supervision-regularization loss ||z - sg[2] ||2+ ||sg[z] - \u017e||2 on top of PPO objectives (sg[.] denotes stop gradient operator). We apply a linear schedule for A to prevent our policy from conservative actions in the beginning phase."}, {"title": "C. Reward Design and Domain Randomization", "content": "While the proposed framework is adopted widely for locomotion tasks, it remains non-trivial how to transfer this pipeline for fine-manipulation tasks like articulated object manipulation. To facilitate a single end-to-end policy that can efficiently perform multi-staged motions, we introduce stage-conditioned rewards, including task-aware rewards and motion-aware rewards (see Table I).\nTask-aware rewards focus on executing a proper motion sequence, complying A-then-B order, rather than cheating to gain success rewards immediately. For instance, at timestep t, state s with the door opened and the door handle grasped firmly by the gripper is rewarded significantly more than state s without the grasped handle.\nMotion-aware rewards encourage our policy to generate smooth motions while maintaining a high success rate. These terms are often activated after the policy is trained to complete the main task, thus acting as a fine-tuning incentive for smoother execution. We argue that incorporating these regularization terms is crucial and helps bridge the sim-to-real gap by preventing unnecessary motion or non-achievable"}, {"title": "D. Variable Impedance Control", "content": "The goal of impedance control is to follow a desired trajectory xd considering the external force Fext resulted from the interaction between the robot and the environment. The design of impedance control follows a mass-spring-damper system that can dynamically adjust target setpoints based on feedback force as well as the stiffness of the environment. The dynamics model of impedance control is:\nM(xc-xa) + D(xc \u2013 xa) + K(xc - xd) = Fext\nwhere M is the mass-inertia matrix of the robot, D is the damping matrix, K is the stiffness matrix, and [xc,Xc,Xc] is impedance trajectory outputs.\nIn our pipeline, we learn to predict the stiffness factor kp of our Cartesian impedance controller and expand it into a six-dimensional diagonal matrix K. Following [21], [30], we assume that M, K, D are positive definite diagonal matrices to ensure system stability. To this end, we scale actor prediction kp by:\nckp = clip(akp,-1,1)*40+100\nWe find this value range generates reasonable motions in"}, {"title": "V. EXPERIMENTS", "content": "To verify the effectiveness of the proposed method, we conduct extensive evaluations in both simulation and real-world settings."}, {"title": "A. Data and Task Settings", "content": "In the simulation, following the settings of PartManip [13], we conduct our experiments in the IsaacGym simulator and the large-scale PartNet-Mobility dataset [32]. We use a fixed-base Franka and a total of 346 articulated 3D objects covering both doors and drawers (modified StorageFurniture subset), to carry out the simulation experiments.\nIn the real-world setting, we perform experiments with a variety of household objects using the Franka Emika robotic arm equipped with an on-hand RealSense D415 camera to capture RGBD images. We leverage Segment Anything (SAM) [33] for actionable part pointcloud extraction using a first-framed RGBD image and GSNet [34] for grasp prediction.\nWe evaluate our proposed pipeline with two following tasks: OpenDoor/OpenDoor+ and OpenDrawer/OpenDrawer+.\nOpenDoor/OpenDoor+: A door is initially closed, the agent needs to open the door larger than 15%/80% of the maximum door swing. The key requirement for our task setting is that the gripper should firmly grasp the handle while opening the door without cheating by opening from the side or with the robot body.\nOpenDrawer/OpenDrawer+: A drawer is initially closed, the agent needs to open the drawer larger than 20%/80% of the maximum opening length. Similar to the OpenDoor task, we require the gripper to firmly grasp the handle while opening the drawer.\nFor simulation and real-world settings, we adopt Success Rate (SR) as the major evaluation metric."}, {"title": "B. Baselines and Ablation Study Design", "content": "We compare our proposed method with articulated-object manipulation pipelines that follow sim-to-real RL paradigm.\nPPO. We directly use the PPO algorithm to learn a state-based policy to handle each task. The detailed PPO parameters and training strategy are similar to our method.\nWhere2Act [11]. An affordance learning framework pre-dicting the visual actionable affordance using a partial point cloud. We include the part mask as an additional dimension in our task, while keeping other aspects unchanged.\nPartManip [13]. A vision-based policy learning method that first trains a state-based expert with part-based canon-icalization and part-aware rewards, and then distills the knowledge to a vision-based student policy."}, {"title": "C. Results and Findings", "content": "Results of simulation experiments are shown in Table II, from which we can see that while most baselines perform reasonably well on the training set, their performance tends to decline significantly on the testing set. In contrast, our method maintains consistently strong performance on the evaluation set, without a sharp drop, highlighting the excel-lent generalization ability of our approach. We also find our controller learns to adapt to different manipulation stages, even without any direct gain rewards (Fig. 4). Specifically, when the gripper is far from the object, it turns stiffer by setting the controller gain to a higher kp. On the other hand, when the distance is reduced, to minimize the collision penalty, it becomes softer with a lower kp.\nOur policy rollout performance in real world can be found in Table III. We conduct 50 experiments for our pipeline and each ablated model (500 runs in total) on diverse objects (Fig. 3). We further investigate our success rate by decoupling the failure cases due to grasp pose estimation in Grasping Stage and due to our pipeline in Opening Stage. For OpenDoor+,\nWith the ablation study results demonstrated in Table III, apart from SR drop in both simulation and the real world, we aim to highlight the non-smooth motions of real-world executions. For W/o Impedance Control, we find the main reason for failure cases (40% drop) is the low flexibility of position control, which requires each predicted action to be executed precisely. This would generate large joint torque to overcome the feedback force of objects, resulting in the robot"}, {"title": "VI. CONCLUSIONS", "content": "In this work, we introduce a novel RL framework equipped with variable impedance control for end-to-end articulated object manipulation, which adaptively learns the object movement from observation and action history instead of naively executing a trajectory predicted before any robot-object contact. We demonstrate great sim-to-real transfer ca-pability on diverse test objects in the real world and achieve 80% and 84% success rate for OpenDoor+ and OpenDrawer+ tasks, respectively, outperforming all existing works. Along with quantitative results, our policy can generate smooth and dexterous motion thanks to our well-designed training settings and reward functions. We hope our work can suggest an alternative way to leverage vision information, as well as other potential modalities (e.g. tactile grasp signal), to better bridge the sim-to-real gap for future RL-based manipulation works."}]}