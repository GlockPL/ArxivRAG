{"title": "Obtaining Optimal Spiking Neural Network in\nSequence Learning via CRNN-SNN Conversion", "authors": ["Jiahao Su", "Kang You", "Zekai Xu", "Weizhi Xu", "Zhezhi He"], "abstract": "Spiking neural networks (SNNs) are becoming a promising\nalternative to conventional artificial neural networks (ANNs) due to\ntheir rich neural dynamics and the implementation of energy-efficient\nneuromorphic chips. However, the non-differential binary communica-\ntion mechanism makes SNN hard to converge to an ANN-level accuracy.\nWhen SNN encounters sequence learning, the situation becomes worse\ndue to the difficulties in modeling long-range dependencies. To over-\ncome these difficulties, researchers developed variants of LIF neurons\nand different surrogate gradients but still failed to obtain good results\nwhen the sequence became longer (e.g., >500). Unlike them, we obtain\nan optimal SNN in sequence learning by directly mapping parameters\nfrom a quantized CRNN. We design two sub-pipelines to support the\nend-to-end conversion of different structures in neural networks, which\nis called CNN-Morph (CNN \u2192 QCNN \u2192 BIFSNN) and RNN-Morph\n(RNN\u2192 QRNN \u2192 RBIFSNN). Using conversion pipelines and the s-\nanalog encoding method, the conversion error of our framework is zero.\nFurthermore, we give the theoretical and experimental demonstration of\nthe lossless CRNN-SNN conversion. Our results show the effectiveness\nof our method over short and long timescales tasks compared with the\nstate-of-the-art learning- and conversion-based methods. We reach the\nhighest accuracy of 99.16% (0.46 \u2191) on S-MNIST, 94.95% (3.95 \u2191) on\nPS-MNIST (sequence length of 784) respectively, and the lowest loss of\n0.057 (0.013\u2193) within 8 time-steps in collision avoidance dataset.", "sections": [{"title": "1 Introduction", "content": "Spiking Neural Networks (SNNs), known as third-generation neural networks\n[24], are inspired by the biological structure of the brain. Recent studies have"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Relation of RNN and SNN", "content": "Spiking neural networks have similarities to vanilla RNN and its variants in\nthe same form. Since the change of membrane potential is related to time, an\nSNN can be understood as an RNN without recurrent connection [25]. Recurrent\nneural networks (RNNs) are powerful models for processing sequential data while\nspiking neural networks (SNNs) show huge potential for processing sequential\nevent-based data. To address the vanishing and exploding gradient problems\nduring the training of RNN, long short-term memory (LSTM) [14] is proposed.\nIn addition to adding the gate units in recurrent neurons, other works address the\nproblem by weight initialization like IRNN [17] or changing the form of recurrent\nneurons like indRNN [21]. Similar to RNNs, many efforts have been made to\nhelp SNNs learn long-term patterns. Variants of LIF (e.g., Adaptive LIF [2,33],\nGLIF [32]) are proposed to enlarge the representation of neuronal behaviors.\nThe RSNN that contains recurrent connections is adopted by [34,31], resulting\nin better performance compared with feedforward-only connections. However, it\nstill remains challenges to obtain an SNN with RNN-level performance in the\ndataset that RNNs are good at, such as sequential image classification and time\nseries forecasting."}, {"title": "2.2 ANN-to-SNN conversion", "content": "The ANN-to-SNN conversion algorithm was first introduced in [7] by changing\nthe activation function to ReLU. [9] presented two ways to normalize the net-\nwork weights (i.e., data-based and model-based normalization) to prevent the\noverestimating output activation. [28,10] took threshold into consideration and\nproposed different normalization methods. By theoretically analyzing the con-\nversion error between the source ANN and the converted SNN, [6,22] achieved\nthe high-performance ANN-SNN conversion with ultra-low latency. To mitigate\nthe sequential error, a neuron that can trigger both positive and negative spikes\nwas proposed, which has been widely used in recent works [15,20,30]."}, {"title": "2.3 Quantization in ANN Compression", "content": "Quantization refers to techniques for performing computations and storing ten-\nsors at lower bit-widths than floating point precision. The mathematics of quan-\ntization for neural networks is as follows:\n$x_q = clip(round(\\frac{x}{s} + z), a, b)$.\nwhere s and z denote quantization scale and zero point respectively. clip(\u00b7, a, b)\nfunction sets the the lower bound a and upper bound b. There are two main\nclasses of algorithms: post-training quantization (PTQ) and quantization-aware\ntraining (QAT). Compared with PTQ, QAT usually leads to a more robust\nmodel. It inserts some fake modules in the computational graph of the model\nto simulate the effect of the quantization during training, where the straight-\nthrough estimator (STE) [3] is a typical adoption to approximate the gradient of\nthe quantization function. To further mitigate the quantization error, LSQ [11,4]\nmakes s as learnable as other network parameters (i.e., z, a, b). We adopt LSQ\nas our quantization method, following the approach of previous works [5,15]."}, {"title": "3 Method", "content": ""}, {"title": "3.1 SNN Model", "content": "Bipolar Integrate-and-Fire Neuron To mitigate the sequential error (the\nphenomenon that spikes are generated in spiking neurons where they should not"}, {"title": "3.2 Conversion Pipelines", "content": "As illustrated in fig. 3, QCRC can simultaneously convert different layers to\ntheir corresponding SNN layers via two sub-pipelines, which is versatile and\nsuitable for the compound model (i.e., model that contains different types of\nlayers), such as CRNNs. We design two conversion pipelines for different types of\nlayers in networks, which we call CNN-Morph (CNN \u2192 QCNN \u2192 BIFSNN)\nand RNN-Morph (RNN \u2192 QRNN \u2192 RBIFSNN). In brief, the conversion\npipeline can be divided into two steps: the quantization process and the Neuron-\nMorph process.\nQuantization. (1) Operator Substitution: The first step is to make sure all\noperators in the original ANN are compatible with the SNN. For example, all\nactivation functions should be ReLU based on equivalence requirements before\ntraining at full precision. In addition, max-pooling should be replaced by average-\npooling because computing maxima with spiking neurons is non-trivial [27]. (2)\nActivation Substitution: In this step, the ReLU function is replaced by the quan-\ntized ReLU function, where the lower bound a is set to 0 and upper bound b set\nto L. After the configuration, the quantized ANN is trained using the protocols\ndefined in [11,4].\nNeuron-Morph. (1) Neuron Substitution: Benefiting from neuronal equiva-\nlence (section 3.3), the synaptic weights of a quantized ANN can be directly"}, {"title": "3.3 Theoretical Equivalence in QCRC", "content": "Theorem 1. Assume a quantized CNN with ReLU activation function param-\neterized by W\u00b9 is converted to a BIFSNN based on CNN-Morph and s-analog\nencoding is adopted, then the accumulated outputs of the SNN are equal to the\nquantized CNN outputs when T is long enough that remaining membrane poten-\ntial is insufficient to fire a spike.\nProof. Theorem 1 proof is in the appendix.\nTheorem 2. Suppose an RNN with ReLU activation function, parameterized\nby $W_{ih}$ and $W_{hh}$, is quantized into n quantization level by quantization scale s:\n$h_k = s \\cdot clip([\\frac{W_{ih}x_k + b_{ih} + W_{hh}h_{k-1}+ b_{hh}}{s}], 0, n)$.\nIf an RBIFSNN is converted from the QRNN with $V_k(0) = 0.5s$, $S_{max} = n$,\n$X' = s$ and the s-analog encoding is adopted, then for any k-th input of the RNN\nsequence, the accumulated outputs of the SNN is equal to the QRNN output:\n$X(T) = h_k$,\nwhen T is long enough that the remaining membrane potential is not sufficient\nto fire a spike."}, {"title": "4 Experiments", "content": "In this section, we obtain optimal SNNs in sequence learning via CRNN-to-\nSNN conversion. We validate the effectiveness of our method with other state-\nof-the-art learning-based approaches and conversion-based approaches, demon-\nstrating the advantages of our method on different datasets (i.e., benchmark"}, {"title": "4.1 Implementation details", "content": "The experiments exactly follow the quantization and conversion stages as intro-\nduced in section 3.2. Both ANN quantization training and SNN implementation\nare carried out with PyTorch. Unless otherwise specified, the optimizer is Adam\n[16], the learning rate scheduler is the cosine annealing schedule [23].\nS-MNIST and pS-MNIST We only apply normalization transform to the\ndataset. The main hyper-parameters of the models follow their corresponding\npapers [2,33,12]. Training epoch and batch size are 200 and 256 for all models.\nThe learning rate of our model is 0.0002. The cross-entropy loss (CE) is used to\nevaluate the difference between the estimated value and the actual value.\nObstacle detection and avoidance The total dataset (including 20 training,\n5 validation traces) is split into multiple sub-sequences of length 32 and fed into\nthe model sequentially. The input of the LIDAR scanner will be fed to the main\nstructure, while the estimated robot pose will be firstly clipped to the range of\n-1.0 to 1.0 and then concatenated with the output of layer 5 before sent to the\nnext layer. We follow a similar network as [18], consisting of an RNN preceded\nby a set of convolutional layers. The epoch and batch size are set to 1000 and\n32 respectively. We use a fixed learning rate of 0.0001 to train the model with\nthe mean square error (MSE) loss function."}, {"title": "4.2 Sequential MNIST", "content": "The sequential- and permuted-sequential MNIST (S/PS-MNIST) [19] are widely\nused benchmarks to verify the learning ability for long-term dependencies. The\nimage will be divided into 784 pixels and sent to the network pixel by pixel.\nThe networks are asked to predict the class of MNIST image only when all 784\npixels are fed sequentially to the recurrent network. Therefore, achieving high\naccuracy on the \"pixel-by-pixel MNIST\" problem is not easy because neurons\nmust have the ability to learn from the long contexts.\nBenefiting from the high scalability of our method, we use indRNN cell [21]\nas the original RNN model. We set the quantization step and time-steps to 128\nand 512. A performance comparison is given in Table 3. RBIF reads the image\npixel by pixel without any extra encoding process, just as the same as the LSTM.\nIt outperforms all models, achieving 99.16% and 94.95% classification accuracy\non S-MNIST and PS-MNIST respectively. Note that, the accuracy of pr-ALIF\n(94.3%) on PS-MNIST is not included for comparison because the adoption of a\nsliding window is unfair to other models. We also compare our method with the\nconversion-based method. It turns out that performance deteriorates rapidly as"}, {"title": "4.3 Obstacle detection and avoidance", "content": "To explore the application of SNNs in sequential robotic tasks, we conduct robot\nnavigation experiments using the dataset proposed in [18]. The objective of this\ntask is to navigate a Pioneer 3-AT mobile robot safely through obstacles. Specif-\nically, the network input comprises data streams from a 270-degree 2D LiDAR\nscanner and a time series of estimated robot poses sampled at 10Hz. By generat-\ning a decision in the form of a target angular velocity, the network can maneuver\nthe robot safely around the obstacles."}, {"title": "4.4 Ablation Study", "content": "Conversion Error Analysis We perform two-fold validation on the equiva-\nlence of QCRC. We use the dataset and network in section 4.3. The choice of\nCRNN network can make the analysis more comprehensive since it contains three\ncommonly used layers (i.e., linear, convolutional, recurrent layers). To measure\nthe conversion error straightforwardly, we use a batch of data to visualize the\nL1 Norm (a.k.a. Manhattan distance) between QANN and its counterpart SNN\nfor intermediate activation layers, as shown in the left of fig. 4. It is shown that\nthe use of IF neurons makes the L1 Norm in QCFS remain at a large value due"}, {"title": "5 Discussion and Conclusion", "content": "This paper proposes a comprehensive QCRC framework to help SNNs overcome\nthe challenge of not achieving ANN-level results in sequence learning, enabling\nSNNs to achieve results comparable to RNNs. To overcome the incompatibility\nproblem of RNN cell, we propose RBIF neuron. Based on this, we further demon-\nstrate the lossless CRNN-SNN conversion with the design of conversion pipelines\nand s-analog encoding. The framework includes two sub-pipelines (i.e., CNN-\nMorph and RNN-Morph), which can support end-to-end conversion of complex\nmodels with both recurrent and convolutional structures into SNN and is not\nlimited by the type of dataset. We are the first work to implement lossless RNN-\nSNN conversion on time series tasks. Our results show promising advantages\ncompared to the state-of-the-art conversion- and learning-based methods. Our\nresults answer the question in section 1: we can easily achieve ANN-level per-\nformance for SNNs in sequence learning via CRNN-SNN conversion. We believe\nour work paves the way for the application of SNNs in time series tasks."}]}