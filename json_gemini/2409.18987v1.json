{"title": "Efficient and Personalized Mobile Health Event Prediction via Small Language Models", "authors": ["Xin Wang", "Ting Dang", "Vassilis Kostakos", "Hong Jia"], "abstract": "Healthcare monitoring is crucial for early detection, timely intervention, and the ongoing management of health conditions, ultimately improving individuals' quality of life. Recent research shows that Large Language Models (LLMs) have demonstrated impressive performance in supporting healthcare tasks. However, existing LLM-based healthcare solutions typically rely on cloud-based systems, which raise privacy concerns and increase the risk of personal information leakage. As a result, there is growing interest in running these models locally on devices like mobile phones and wearables to protect users' privacy. Small Language Models (SLMs) are potential candidates to solve privacy and computational issues, as they are more efficient and better suited for local deployment. However, the performance of SLMs in healthcare domains has not yet been investigated. This paper examines the capability of SLMs to accurately analyze health data, such as steps, calories, sleep minutes, and other vital statistics, to assess an individual's health status. Our results show that, TinyLlama, which has 1.1 billion parameters, utilizes 4.31 GB memory, and has 0.48s latency, showing the best performance compared other four state-of-the-art (SOTA) SLMs on various healthcare applications. Our results indicate that SLMs could potentially be deployed on wearable or mobile devices for real-time health monitoring, providing a practical solution for efficient and privacy-preserving healthcare.", "sections": [{"title": "1 INTRODUCTION", "content": "Healthcare monitoring is crucial in our daily lives as it allows for the early detection of hidden diseases, enables timely interventions, and helps sustain well-being by continuously tracking physical status and other health metrics [1-6]. The growing development of artificial intelligence has led to significant attention on large language models (LLMs), which demonstrate the capability to comprehensively understand and analyze vast amounts of unstructured data, such as time-series data collected by wearable sensors, thereby facilitating health status predictions [7].\nLLMs, such as Health-LLMs[7], are particularly valuable in healthcare due to their ability to process complex data and generate insights. However, they come with several challenges, particularly heavy computational requirements. For example, state-of-the-art (SOTA) LLMs can require up to 175 billion parameters, necessitating powerful GPUs for training and inference. Additionally, their heavy memory usage can reach up to 700 GB, making them impractical for deployment on local mobile devices, which typically have only 4 to 6 GB of memory. Furthermore, LLMs often exhibit high latency, with response times that can exceed several seconds, which is unacceptable for real-time healthcare applications. Privacy concerns also arise, as the use of cloud-based models can lead to data leakage [8], risking user privacy; for instance, employers could misuse health data to terminate employees' contracts.\nIn contrast, small language models (SLMs) present an ideal solution to these challenges. They typically have fewer training parameters, making them easier to deploy on mobile devices with limited computational resources. However, the performance of SLMs in the context of healthcare monitoring has yet to be investigated. Additionally, the system performance of these models is unknown.\nIn this paper, we evaluate SOTA SLMs for healthcare monitoring applications using text. Specifically, we examine fives SOTA SLMs including Phi-3-mini-4k-Instruct [9], TinyLlama-1.1B [10], Gemma2-2b [11], SmolLM-1.7B [12], and Qwen2-1.5B [13] on benchmark healthcare datasets. We also implement an on-device LLMs app on the iPhone 15 Pro Max to evaluate these models' performance and computational requirements. Extensive experiments demonstrate that SLMs are ideal solutions for on-device healthcare monitoring. Our contributions are outlined as follows:\n\u2022 We benchmarked a variety of SOTA SLMs on healthcare monitoring tasks, highlighting that SLMs surprisingly outperformed many SOTA LLMs such as Gemini-Pro [14], GPT-4 [15], and MedAlpaca [7] in some tasks. In suboptimal tasks, SLMs also achieve on-par accuracy compared with SOTA LLMs."}, {"title": "2 RELATED WORKS", "content": "This section briefly introduce SOTA LLMs for heathcare applications \u00a72.1 and SOTA SLMs \u00a72.2."}, {"title": "2.1 LLMs for Heathcare Applications", "content": "Powered by their generalization capabilities, recent LLMs have shown great success in the healthcare domain and are an area of rapidly growing research. For example, Health-LLM [7] and MultiEEG-GPT [16] leverages LLM capabilities for healthcare monitoring using text. LLMs for Mental health predictions [17, 18], powered by few-shot and fine-tuned strategies, shows great promise in predicting mental health status. PaLM2 [19] demonstrated the effectiveness of LLMs by combining various strategies, achieving superior performance across different datasets in medical domains. Recent medical reasoning evaluations on GPT-4 [20] have demonstrated the great promise of LLMs in recognizing medical events without significant training efforts. However, all these models require significant computational resources, which is impractical for privacy-sensitive and real-time mobile healthcare monitoring."}, {"title": "2.2 Small Language Models", "content": "Due to the significant number of parameters in LLMs, which makes it challenging to deploy them on hardware devices, recent SLMs have emerged, showing comparable effectiveness in various tasks. For instance, Microsoft's Phi-3-mini-4k-Instruct [9] has 3.8 billion parameters and is trained on a combination of synthetic data and selected publicly available website data, emphasizing high-quality and reasoning-dense properties. Similarly, TinyLlama-1.1B [10], a distilled version of Llama 2 with 1.1 billion parameters, was fine-tuned on the UltraChat dataset, which contains a wide range of synthetic dialogues generated by ChatGPT. Google's state-of-the-art (SOTA) model, Gemma2-2b [11], was built from the same research and technology used to create the Gemini models and is well-suited for text generation tasks such as question answering, summarization, and reasoning. HuggingFace's SmolLM-1.7B [12], with 1.7 billion parameters, was trained on synthetic textbooks, stories, educational Python, and web samples, while Qwen2-1.5B [13], a"}, {"title": "3 METHODOLOGY", "content": "In this section, we will discuss how to construct the prompts inputted into the SLMs for healthcare monitoring reasoning \u00a73.1, how we design the answer template in the prompt for reliable text responses generated via SLMs \u00a73.2, and how we deploy SLMs in mobile devices \u00a73.3."}, {"title": "3.1 Zero-shot Learning", "content": "Zero-shot prompting is an effective method for evaluating the performance of state-of-the-art language models (SLMs) by assessing their ability to generate responses based solely on the instructions given, without the aid of example inputs. This approach aims to demonstrate the inherent capabilities of modern SLMs in analyzing and interpreting healthcare-related data within the context of our study.\nIn designing our prompting strategy, we follow the typical zero-shot prompt design in healthcare domain [7], while introducing a customized answer instruction tailored specifically for SLMs. As illustrated in Table 1, the prompt consists of three main components: an instruction that clearly defines the task, a main query that integrates relevant physiological data and user information, and an answer prompt designed to ensure consistent output from each SLM.\nFor instance, as shown in Table 1, consider the example where recent sensor readings include steps taken, calories burned, resting heart rate, sleep minutes, and mood ratings. Our prompt begins with a clear instruction that delineates the task at hand. Then, main query synthesizes this information to predict fatigue levels on a scale from 0 to 5. By providing a structured input that encapsulates these diverse data points, the SLM is guided to focus on the relevant metrics that contribute to fatigue assessment."}, {"title": "3.2 Answer Prompt for SLMs", "content": "While the original answer template* suggested by [7] aimed to provide a structured format, it was not recognized effectively by the SLMs. This can be attributed to the limitations of the reduced training parameters, which hindered the SLMs' ability for in-context learning. Specifically, the original answer template is designed to guide LLMs in producing specific numerical values by analyzing the entire prompt and understanding long-range contextual information. This process inevitably leverages the LLMs' capabilities in context comprehension, which is enhanced by their large number of parameters to a certain extent. However, SLMs, with significantly fewer parameters, may exhibit reduced abilities in understanding long-range context. The original template appeared to confuse the SLMs, as they struggled to grasp both the task and the answer template simultaneously. Therefore, a more direct answer template is required for SLMs.\nTo simplify prompt complexity and alleviate the inference workload, we propose a new answer prompt, as illustrated in Table 1. This new approach leverages the generative capabilities of language models by guiding SLMs to naturally complete the sentence with the expected answer, rather than requiring them to fully understand the implications of the answer template. By doing so, we align more closely with the SLMs' training paradigm of predicting the next tokens, enhancing their performance in generating the desired responses."}, {"title": "3.3 Mobile Hardware Deployment", "content": "To investigate the efficiency and resource utilization, we deployed SOTA SLMs that achieve the best results in health dataset prediction on a real iPhone 15 Pro Max with a total RAM of 8 GB. In detail, all models are downloaded in HF version from Hugging Face [21] and converted to GGUF format. To ensure smooth deployment on mobile devices, quantization is found to effectively reduce computational costs while maintaining good performance, as evidenced in recent studies [22]. Due to the tight memory allocation on mobile devices, 4-bit quantization supported by Llama.cpp [23] is applied to all models to further reduce model size and maximize efficiency. We evaluate latency, memory, and other hardware overhead via the mobile app MobileAIBench [22]."}, {"title": "4 EXPERIMENT", "content": "To verify SLMs' capability on healthcare domains, we tested a variety of SOTA SLMs use a large scale PMData[24] dataset. Specifically, PMData consists of the life-logging of 16 participants in time-series format over a period of 5 months using the Fitbit Versa 2 smartwatch, the PMSys sports logging app, and Google Forms. The Fitbit Versa 2 smartwatch primarily records physical activity and physiological status in time-series format (e.g., burned calories, resting heart rate, step counts, sleep minutes, etc.), while the PMSys sports logging application records the users' self-reported measurements of their health status, such as fatigue, readiness, sleep quality, and stress level. In data preparation, as a common practice, all relevant data is extracted from CSV files, shuffled, and split into training and evaluation subsets in a ratio of 8:2. For health event prediction, we will first integrate the time-series data collected by the smartwatches into the previously mentioned query prompts, then utilize self-reported data to assess the answers generated by SLMs. The associated tasks of this dataset remain identical to those discussed in [7], which include stress, readiness, fatigue, and sleep quality."}, {"title": "4.2 Small Language Models", "content": "We considered the 5 most state-of-the-art SLMs, including Phi-3-mini-4k-Instruct, TinyLlama-1.1B, Gemma2-2B, SmolLM-1.7B, and Qwen2-1.5B.\n\u2022 Phi-3-mini-4k-Instruct[9]: Microsoft's smallest model in the Phi-3 family. It has 3.8 billion parameters, trained on a combination of synthetic data and selected publicly available website data, with an emphasis on high-quality and reasoning-dense properties.\n\u2022 TinyLlama-1.1B[10]: Distilled version of Llama 2 remains the same architecture and tokenizer but is compact"}, {"title": "5 RESULTS AND DISCUSSION", "content": "During experiments, we run tests on five SOTA SLMS across four tasks created with PMData and compare the results with those of 12 SOTA LLMs, as shown in Table 2. In addition to investigating the efficiency and computational expenses in real deployment, we conduct another experiment with two models that achieve the best results on a real iPhone 15 Pro Max and compare them with Llama 2 for further analysis, as presented in Table 3."}, {"title": "5.1 Zero-Shot Performance", "content": "For performance evaluation, Mean Absolute Error (MAE) and Accuracy are utilized to assess model performance. Accuracy measures the hit rate, which defined as the number of true positives (correctly predicted instances) divided by the total number of actual positive instances, while MAE measures the loss of error when they miss. In Table 2, \u2193refers to tasks evaluated with MAE, where lower values/errors are better, while\u2191 refers to Accuracy, indicating that higher values/accuracy are better, consistent with [7]. For better visualization in both LLMs and SLMs, the best value in each task is indicated in bold, while the second-best value is underlined.\nAs shown in Table 2, on average the SLMs are able to achieve similar or superior performance to SOTA LLMs in all four health conditions including stress, readiness, fatigue, and sleep quality. Specifically, the mean MAE for stress level using SLMs is 0.615, which outperforms that of LLMs at 0.639. Similar superiority is also observed for readiness, with SLMs showing a mean MAE of of 1.91 and LLMs of 2.56. For fatigue and sleep quality, despite not outperforming LLMs, SLMs still exhibit similar range of performance compared to LLMs, indicating its effectiveness with much higher efficiency.\nIn detail, SLMs demonstrate a competitive edge in certain aspects, particularly in stress and readiness metrics. For"}, {"title": "5.2 Efficiency and Utilization Evaluation", "content": "In order to determine the actual latency in real cases, we deployed state-of-the-art (SOTA) SLMs that show strong promise in processing healthcare field data on a real iPhone 15 Pro Max. To better demonstrate the importance of efficiency on mobile devices, the widely used LLM, Llama 2, is selected and serves as a comparison to the best SLMs. All the results in Table 3 are tested on a standard NLP task, the HotPotQA dataset [25], a standard benchmark dataset to test hardware overhead, consists of question-answering tests across different fields. The following metrics suggested by MobileAIBench [22] are adopted to evaluate both efficiency and utilization:\n\u2022 Time-to-First-Token (TTFT, sec): The time of the first token generated to respond to the prompt - assess the latency.\n\u2022 Input Token Per Second (ITPS, tokens/sec): The number of input tokens being processed per second - access understanding speed.\n\u2022 Output Token Per Second (OTPS, tokens/sec): The number of tokens produced per second after starting to produce tokens - access inference speed.\n\u2022 Output Evaluation Time (OET, sec): The time model takes to complete a response - assess the overall efficiency of generating an entire response.\n\u2022 Total Time: The total time it takes to produce a complete response after receiving a prompt is a comprehensive efficiency metric for how long a model takes to complete a given task from start to finish.\n\u2022 CPU (%): An amount of computational resources used in the inference process.\n\u2022 RAM (GB): An amount of memory needed to run a model during the inference process.\nAccording to Table 3, the comparative analysis of system overhead between SLMs and LLMs reveals significant advantages for SLMs, specifically Phi-3-mini-4k and TinyLlama-1.1B, when tested on an iPhone 15 Pro Max with an 8GB RAM memory limit. TinyLlama-1.1B demonstrates exceptional efficiency with a TTFT of 0.48 seconds and an ITPS rate of 552.58, significantly outperforming Llama-2-7b, which has a TTFT of 4.75 seconds and an ITPS of 56.17. This represents an improvement of approximately 9.9 times in TTFT and an increase of roughly 884% in ITPS. Moreover, TinyLlama-1.1B achieves a notable OET of 1.16 seconds and an OTPS rate of 45.22, in stark contrast to Llama-2-7b's OET of 18.04 seconds and OTPS of 7.04, indicating an improvement of about 15.5 times in OET and an increase of approximately 542% in OTPS. Phi-3-mini-4k also shows commendable performance with a TTFT of 2.01 seconds and an ITPS of 102.46, alongside an OET of 8.52 seconds and an OTPS of 13.70.\nBoth SLMs exhibit reduced CPU usage and RAM consumption compared to LLMs, with Phi-3-mini-4k and TinyLlama-1.1B utilizing 66.48% and 74.55% CPU, and 6.32GB and 4.31GB of RAM, respectively, whereas Llama-2-7b demands 262.84% CPU and 6.82GB of RAM. This reflects a reduction of approximately 75% in CPU usage for Phi-3-mini-4k and a reduction of around 82% for TinyLlama-1.1B. These findings underscore the efficiency and suitability of SLMs for deployment in resource-constrained environments."}, {"title": "6 CONCLUSION", "content": "In this paper, we evaluate the performance of the current SOTA SLMs in healthcare event prediction. Our results show that SLMs, although they have fewer model parameters and representation capabilities, achieve surprisingly on par or even better performance in some healthcare tasks compared to SOTA LLMs, while also demonstrating significantly higher efficiency. This indicates the feasibility of deploying SLMs on local mobile devices to prevent privacy leakage. However, we recognize some limitations in our study. Our efficiency evaluation results are limited to standard NLP datasets. Additionally, we validated only a limited health dataset and focused solely on zero-shot performance, without exploring few-shot learning or instruction tuning. Therefore, in future studies, we will evaluate SLMs on a broader range of health datasets to ensure generalizability. We will also conduct further tests, combined with few-shot learning and instruction tuning, to examine how quantization affects model performance, aiming to find an optimal balance."}]}