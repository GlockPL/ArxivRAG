{"title": "Llama Guard 3-1B-INT4: Compact and Efficient Safeguard for Human-Al Conversations", "authors": ["Igor Fedorov", "Kate Plawiak", "Lemeng Wu", "Tarek Elgamal", "Naveen Suda", "Eric Smith", "Hongyuan Zhan", "Jianfeng Chi", "Yuriy Hulovatyy", "Kimish Patel", "Zechun Liu", "Changsheng Zhao", "Yangyang Shi", "Tijmen Blankenvoort", "Mahesh Pasupuleti", "Bilge Soran", "Zacharie Delpierre Coudert", "Rachad Alao", "Raghuraman Krishnamoorthi", "Vikas Chandra"], "abstract": "This paper presents Llama Guard 3-1B-INT4, a compact and efficient Llama Guard model, which has been open-sourced to the community during Meta Connect 2024. We demonstrate that Llama Guard 3-1B-INT4 can be deployed on resource-constrained devices, achieving a throughput of at least 30 tokens per second and a time-to-first-token of 2.5 seconds or less on a commodity Android mobile CPU. Notably, our experiments show that Llama Guard 3-1B-INT4 attains comparable or superior safety moderation scores to its larger counterpart, Llama Guard 3-1B, despite being approximately 7 times smaller in size (440MB).", "sections": [{"title": "1 Introduction", "content": "Safety is a major concern for products powered by Generative AI. It is essential to have robust safeguards in place to protect against the generation of high-risk or policy-violating content (Inan et al., 2023). To address this issue, large language models (LLMs) are often paired with a guard model that checks both user input and model output for unsafe content. The guard model can be an LLM itself, but does not need to share the same architecture or weights as the generative LLM it is paired with.\nAnother important consideration in deploying LLMs is the inference cost, especially when targeting resource constrained hardware systems like a mobile device. For many such devices, the model's memory usage becomes the primary deployment bottleneck since mobile systems have limited DRAM and allocating large chunks of memory can cause runtime issues due to operating system throttling.\nRecently, much work has gone into compressing LLMs to make them significantly smaller. Nvidia's Minitron models (Muralidharan et al., 2024) and Google's Gemma models (Team et al., 2024) extensively use pruning and distillation to achieve smaller versions of their larger parent models. This paper further adds to the discourse and evidence of the efficacy of these methods. On top of pruning and distillation, we also include quantization in the compression process and show that several compression techniques can work well together.\nAs part of the Llama 3.2 1B release at Meta Connect 2024, we delivered Llama Guard 3-1B-INT4 (Llama Team, 2024), a lightweight Llama Guard model which:\n\u2022 Consumes only ~ 440MB, 7\u00d7 less than Llama Guard 3-1B (Llama Team, 2024)\n\u2022 Leverages 4-bit per-channel groupwise weight quantization and 8-bit per token dynamic activation quantization for model compression\n\u2022 Is compatible with the ExecuTorch runtime (Pytorch Team, 2024a) and XNNPACK backend delegation (Pytorch Team, 2024b), which accelerate the quantization schema we used via specialized ARM CPU kernels, achieving \u2265 30 token/s on a commodity Android mobile device CPU"}, {"title": "2 Llama Guard Models", "content": "Llama Guard consists of a series of high-performance moderation models designed to support developers to detect various common types of violating content. The input to the Llama Guard model can consist of only user input (prompt classification) or both user input and generative model output (prompt + response"}, {"title": "2.1 Training and evaluation", "content": "Training for Llama Guard models consists of taking a pretrained LLM and conducting finetuning on content safety classification data. The finetuning stage consists of minimizing the next-token prediction (cross-entropy) loss over tokens corresponding to the target model outputs (shown in red in Fig. 1). Both Llama Guard 3-1B and Llama Guard 3-1B-INT4 start from the Llama 3.2 1B pre-trained model (lla, 2024), but Llama Guard 3-1B-INT4 proceeds through a series of model compression steps before / during the finetuning stage (see Sec. 3).\nWe finetune Llama Guard 3-1B-INT4 using the English data used by Llama Guard (Inan et al., 2023), which are obtained by labelling Llama 2 and Llama 3 generations on prompts from the hh-rlhf dataset (Bai et al., 2022). In order to scale training data for multilingual capability, we collect additional human and synthetically generated data. Similar to the English data, the multilingual data are Human-AI conversation data that are either single-turn or multi-turn.\nWe evaluate the performance of Llama Guard 3-1B-INT4 on our internal test dataset based on MLCommons (2024) hazard taxonomy and compare it across languages with Llama Guard 3-1B, Llama Guard 3-8B and GPT4 (with zero-shot prompting)."}, {"title": "2.2 Running the model on a mobile device", "content": "We employ ExecuTorch to demonstrate viability of running Llama Guard 3-1B-INT4 on widely available mobile devices. ExecuTorch is a PyTorch native runtime framework for running PyTorch models on edge devices, including smart phones. The ExecuTorch stack also enables leveraging neural network accelerators available on modern mobile/edge devices, although we did not take advantage of this capability for running Llama Guard 3-1B-INT4. This enables efficient on-device execution of floating point and quantized models. Furthermore, the lightweight nature of the runtime results in very small runtime memory footprint, significantly reducing runtime overhead. ExecuTorch also provides LLM specific extensions and optimizations, including optimized scaled dot product attention (SDPA) and KV-cache quantization."}, {"title": "3 Model compression", "content": "We began with a pre-trained Llama 3.2 1B model (lla, 2024) and applied several techniques to achieve our compression goals. Note that Llama 3.2 1B has about 1.2B parameters and uses embedding / unembedding (i.e. output layer) weight sharing. ExecuTorch's XNNPACK delegate does not support weight sharing with embedding layer and as a result Llama 3.2 1B effectively has 1.5B parameters from an on-device deployment point of view. Assuming bf16 weights for the original Llama 3.2 1B, which occupies ~ 2.8GB, we achieved ~7\u00d7 compression by applying our compression pipeline to yield a 440MB model. In the following, we describe the techniques we used to achieve our compression goal. Fig. 3 provides a high level overview of the compression pipeline, alongside the model sizes at each step."}, {"title": "3.1 Pruning", "content": "To reduce the number of model parameters, we prune the model along two dimensions: The number of decoder blocks and the MLP hidden dimension. The methodology is similar to Muralidharan et al. (2024) and proceeds in 3 stages: 1) pruning metric calibration; 2) model pruning; 3) fine-tuning the pruned model.\nIn the pruning metric calibration step, we calculate importance scores for the decoder blocks and neurons to determine what to prune. For decoder block pruning, we employ the following importance metric from Men et al. (2024):\n$ED_D \\frac{(X_{in}, X_{out})}{||X_{in}||_2 ||X_{out}||_2}$     (1)\nwhere\n\u2022 (,) is the inner product\n\u2022 $X_{in}$ and $X_{out}$ refer to the decoder block (Llama Team, 2024a) input and output vectors, respectively, for a particular input token position\n\u2022 $E_D[\\cdot]$ is the expectation over the training data distribution D\nThe metric in (1) calculates the cosine similarity between the input and output of a decoder block. Blocks which have low cosine similarity between input and output make a relatively small update to the residual stream (Elhage et al., 2023) and can therefore be cut from the network.\nFor the MLP neurons, we estimate\n$E_D [v_q h_k^2] \\forall 1 \\leq k \\leq K$     (2)\nwhere $h_k$ denotes the k'th neuron feeding into the output linear layer inside the Llama 3.2 MLP and K is the MLP hidden dimension. Both (1) and (2) are estimated using Monte-Carlo approximation over a few thousand batches, where each batch contains 32 (GPUs) x 8 (per-GPU batch size) elements.\nAfter calibrating the pruning metrics, we prune the model to 12 layers (from 16) and 6400 MLP hidden dimension (from 8192), such that the pruned model has 1123 million parameters."}, {"title": "3.2 Quantization", "content": "Next, we quantized the model using quantization-aware training (QAT) (Krishnamoorthi, 2018; Nagel et al., 2021; Liu et al., 2023), where the network is fine-tuned for accuracy with quantization operations in-the-loop. QAT was implemented using the torchao (torchao maintainers and contributors, 2024) and torchtune (torchtune maintainers and contributors, 2024) libraries. The weights of all the linear layers are quantized to INT4, symmetrically with the range [-8, 7], and then de-quantized to enable backpropagation through the quantization operation:\n$Q(o_g) = s_e \\times clip\\big(round(\\frac{o_g}{s_e}), -8, 7 \\big), s_e = \\frac{1}{7.5} \\max \\vert o_g \\vert$     (3)\nwhere $o_g$ refers to a group of weights for a particular output neuron and $s_e$ is the corresponding scaling factor. In order to backpropagate through (4), we use the straight-through estimator for the round(.) operator (Bengio et al., 2013). Our quantization scheme uses a group-size of 256 values per-channel, meaning for a linear layer with weight shape $[M_{out}, M_{in}]$, there are corresponding $[M_{out}, M_{in}//256]$ scaling factors. The inputs to each linear layer are quantized to INT8, with asymmetric dynamic quantization with a scaling factor for each token:\n$Q(x_{in}) = s_x \\times clip \\big(round(\\frac{x_{in}}{s_x} - z), 0, 255\\big) + z, s_x = \\frac{1}{255}(max x_{in} - min x_{in}), z = min x_{in}$     (4)\nwhere $x_{in}$ is an activation vector corresponding to a particular token index. Dynamic quantization means the tensor is quantized using the per-token min/max right before executing the matrix-multiply operation. Apart from the inputs to each linear layer, and the linear weights, the rest of the network is executed in BF16. Quantizing the linear layers to 4 bits per weight, we are able to reduce the model size from 2.1GB to 0.9GB (Fig. 3).\nThe embedding layer in our pruned model contains a surprisingly large percentage of the total model weights: 23%. Therefore, quantizing the embedding layer weights to 4 bits represents a huge compression opportunity: the model size goes from 0.9GB to 0.5GB. We observed that the model accuracy is largely unaffected by simply rounding the embedding layer weights to their closest 4-bit values, so we did not apply QAT for the embedding quantization. The embeddings were quantized with a group-size of 32."}, {"title": "3.3 Unembedding layer pruning", "content": "Llama Guard 3-1B-INT4 is trained to generate 128k output tokens, out of which only 20 tokens are used:\n\u2022 Moderation tokens: 'safe', 'unsafe'\n\u2022 Unsafe categories: '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14'\n\u2022 Extras: '\\n', 'S', ',', '<|eot_id|>'\nBy keeping only the model connections corresponding to those 20 tokens in the unembedding linear layer, we can reduce the output layer size significantly without impacting the model. Using this unembedding layer pruning, we reduced the output layer size from 262.6M parameters (2048 \u00d7 128k) to 40.96k parameters (2048 \u00d7 20), giving us a total savings of 131.3MB, assuming 4-bit weight quantization, and a final model size of 0.4GB. Although the pruned output layer only generates 20 tokens, they are expanded back to produce the original 128k outputs in the model such that the model interface does not change from the developer's point of view."}, {"title": "3.4 Distillation", "content": "We employ Llama Guard 3-8B(Llama Team, 2024a) as a teacher to distill Llama Guard 3-1B-INT4 (Hinton, 2015):\n$\\min E_D [L_{cross-entropy}(y_s, y_t)]$     (5)"}, {"title": "4 Results", "content": "As described in 2.1, we evaluate models using an internal dataset based on MLCommons (2024) hazard taxonomy. Table 1 provides a quantitative comparison between our Llama Guard 3-1B-INT4, Llama Guard 3-1B, the much larger Llama Guard 3-8B, and GPT4. The metrics used in the table are F1 and FPR, which stand for F1 score and False Positive Rate. F1 is a harmonic mean of precision and recall, which means it balances the importance of both metrics and FPR measures the proportion of false positives among all negative instances. For all models other than Llama Guard 3-1B-INT4, we assume the bf16 data format when calculating the model size. Llama Guard 3-1B-INT4 achieves better F1/FPR scores for English relative to Llama Guard 3-1B despite having 7\u00d7 smaller model size. In addition, Llama Guard 3-1B-INT4 achieves on par or higher F1 score than Llama Guard 3-1B on 5 out of 8 non-English languages. Compared to GPT4, Llama Guard 3-1B-INT4 achieves considerably better F1/FPR for English as well as 7/8 non-English languages. Note that GPT4 was tested in a zero-shot manner (using an internal dataset without any additional fine-tuning or adaptation).\nTo validate that Llama Guard 3-1B-INT4 can be run on a commodity mobile device, we deployed the model to a Moto-Razor phone and observed >= 30 token/s and <=2.5s time-to-first-token."}, {"title": "5 Limitations", "content": "There are some limitations associated with Llama Guard 3-1B-INT4. First, Llama Guard 3-1B-INT4 itself is an LLM fine-tuned from Llama 3.2. Thus, its performance (e.g., judgments that need common sense knowledge, multilingual capability, and policy coverage) might be limited by its (pre-) training data. Llama Guard performance varies across model size and languages. When possible, developers should consider Llama Guard 3-8B which may provide better safety classification performance but comes at a higher deployment cost. Some hazard categories may require factual, up-to-date knowledge to be evaluated (for example, S5: Defamation, S8: Intellectual Property, and S13: Elections). We believe more complex systems should be deployed to accurately moderate these categories for use cases highly sensitive to these types of hazards, but Llama Guard 3-1B-INT4 provides a good baseline for generic use cases. Lastly, as an LLM, Llama Guard 3-1B-INT4 may be susceptible to adversarial attacks or prompt injection attacks that could bypass or alter its intended use. Please report vulnerabilities and we will look to incorporate improvements in future versions of Llama Guard."}, {"title": "6 Summary", "content": "In conclusion, the pruned and quantized Llama Guard 3-1B-INT4 model is a significant improvement over its predecessors in terms of both safety and efficiency. Additionally, the model's small size and low latency make it well-suited for deployment on mobile devices."}]}