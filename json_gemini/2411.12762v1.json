{"title": "Playing Language Game with LLMs Leads to Jailbreaking", "authors": ["Yu Peng", "Zewen Long", "Fangming Dong", "Congyi Li", "Shu Wu", "Kai Chen"], "abstract": "The advent of large language models (LLMs) has spurred the development of numerous jailbreak techniques aimed at circumventing their security defenses against malicious attacks. An effective jailbreak approach is to identify a domain where safety generalization fails, a phenomenon known as mismatched generalization. In this paper, we introduce two novel jailbreak methods based on mismatched generalization: natural language games and custom language games, both of which effectively bypass the safety mechanisms of LLMs, with various kinds and different variants, making them hard to defend and leading to high attack rates. Natural language games involve the use of synthetic linguistic constructs and the actions intertwined with these constructs, such as the Ubbi Dubbi language. Building on this phenomenon, we propose the custom language games method: by engaging with LLMs using a variety of custom rules, we successfully execute jailbreak attacks across multiple LLM platforms. Extensive experiments demonstrate the effectiveness of our methods, achieving success rates of 93% on GPT-40, 89% on GPT-40-mini and 83% on Claude-3.5-Sonnet. Furthermore, to investigate the generalizability of safety alignments, we fine-tuned Llama-3.1-70B with the custom language games to achieve safety alignment within our datasets and found that when interacting through other language games, the fine-tuned models still failed to identify harmful content. This finding indicates that the safety alignment knowledge embedded in LLMs fails to generalize across different linguistic formats, thus opening new avenues for future research in this area. Our code is available at https://anonymous.4open.science/r/encode_jailbreaking_anonymous-B4C4.\nWarning: this paper contains examples with unsafe content.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) such as ChatGPT (Achiam et al., 2023), Llama2 (Touvron et al., 2023), Claude2 (Anthropic, 2023) and Gemini (Team et al., 2023) have become increasingly important across various domains due to their advanced natural language comprehension and generation capabilities. These models are employed in"}, {"title": "2 Related Work", "content": "Jailbreak Attacks. The safety of LLMs has been a longstanding concern, as adversaries continually develop new methods to manipulate these models into generating harmful content. One key approach to jailbreak attacks, as described by (Wei et al., 2024), involves designing competing objectives within the input. This tactic exploits the LLM's instruction-following capabilities, pushing the model to prioritize generating a response based on the user's request rather than adhering to safety constraints. Studies by (Li et al., 2023), (Chang et al., 2024), (Jiang et al., 2024), and (Guo et al., 2024) demonstrate how carefully crafted natural language instructions can trick LLMs into producing harmful or unethical content. By embedding harmful intent in seemingly benign instructions, attackers can confuse the model's objective and circumvent its safety alignment protocols.\nAnother prominent jailbreak method involves mismatched generalization, wherein input prompts are encoded in a way that was not accounted for in the LLM's safety training but still lies within the model's general pretraining corpus. As noted by (Wei et al., 2024), this technique has been used to bypass alignment processes in various attack schemes. While it provides a comprehensive overview of 28 existing jailbreak strategies, new methods continue to emerge, indicating that the problem is far from solved. For instance, (Deng et al., 2023) demonstrated that interacting with LLMs in medium and low-resource languages can lead to the generation of unsafe outputs. This highlights a significant gap in the models' safety mechanisms, particularly in under-represented languages that may not have been as rigorously aligned for safety during training. Similarly, (Yuan et al., 2023) showed that conversations encoded with ciphers can trick LLMs into producing unsafe responses. These examples reveal how attackers can use obfuscation strategies to bypass content moderation mechanisms by encoding harmful queries in formats that the models do not immediately recognize as dangerous. In addition, (Liu et al., 2023) proposed AutoDAN, an automated system that generates stealthy jailbreak prompts using a hierarchical genetic algorithm. AutoDAN systematically creates prompts designed to evade safety protocols by mimicking benign inputs, leveraging the evolutionary process to develop increasingly effective jailbreak training strategies over time.\nSafety Training of LLMs. Ensuring the responsible and effective deployment of LLMs requires aligning their outputs with human preferences and ethical standards (Korbak et al., 2023; Achiam et al., 2023; Touvron et al., 2023). Common alignment methods include supervised fine-tuning (Bianchi et al., 2023), red-teaming (Ganguli"}, {"title": "3 Methodology", "content": "In this section, we describe the methodology used to develop the proposed jailbreak attack techniques. Specifically, we first introduce the natural language game attack scheme, which transforms harmful base questions into natural language game formats. To address the limitation of the relatively small number of natural language games, we further propose the custom language game attack. This approach allows for the creation of various and numerous custom rules, making it easier to bypass safety alignments while offering more flexibility in attack strategies."}, {"title": "3.1 Natural Language Game Attack", "content": "Natural language games refer to well-known linguistic manipulations where spoken or written language is altered according to predefined rules. These games are often used in informal contexts to obfuscate meaning, making communication incomprehensible to an untrained listener or reader. For the purpose of this study, we focused on a specific subset of natural language games that involve systematic alterations to the structure of words or sentences. An example of such a game is Ubbi Dubbi, where the"}, {"title": "3.2 Custom Language Game Attack", "content": "During our research, we found that when harmful questions are encoded using custom rules, LLMs can still generate harmful content. This occurs due to the powerful inference capabilities of LLMs, which can recognize and process meaning even when the input has been altered in non-standard ways. This reveals a significant vulnerability: we can arbitrarily design custom language games and modify their content, enabling us to carry out a large number of effective jailbreak attacks."}, {"title": "4 Experiments", "content": "4.1 Setup\nDataset. We conduct our experiments on the SALAD-Bench benchmark (Li et al., 2024), which is specifically designed for evaluating LLMs, defense, and attack methods. It comprises harmful base questions categorized into 6 domains, 16 tasks, and 66 specific categories. Considering that some questions are appropriately answered by LLMs, we filtered and randomly sampled 50 harmful base questions from each of the 6 domains, resulting in a total of 300 questions. These sampled questions are then transformed according to both natural and custom language game formats.\nModels. In our experiments, we evaluate the effectiveness of jailbreak attacks across three different large language models: GPT-40-2024-08-06 (GPT-40), gpt-40-mini-2024-07-18 (GPT-40-mini), and Claude-3.5-Sonnet-20240620 (Claude-3.5-Sonnet). These models were selected based on their widespread use, robust natural language processing capabilities, and built-in safety alignment mechanisms, making them ideal candidates for testing vulnerabilities to jailbreak attacks. Each model is in its default settings to ensure consistency and to simulate real-world use cases.\nEvaluation. To categorize the different types of responses generated by LLMs during our jailbreak attack experiments, we report three evaluation metrics: success rate (SR), unclear rate (UR), and failure rate (FR). Success rate represents the percentage of cases where the LLM generates a harmful or unsafe response despite the safety mechanisms in place. Unclear rate measures the percentage of responses where the LLM generates a reply that is unrelated to the transformed query or responds only to the non-harmful content. Failure rate represents the percentage of cases where the LLM successfully blocks or refuses to respond to the harmful input, as intended by its safety alignment. We utilize GPT-40-mini as an auxiliary tool for labeling, with the specially designed classification prompt template provided in Appendix A.1."}, {"title": "4.2 Natural Language Games", "content": "Safety alignments fail to generalize on natural language games. By engaging LLMs in language games, we effectively bypass their safety alignments, achieving attack success rates of 93% on GPT-40, 75% on GPT-40-mini and 83% on Claude-3.5-Sonnet. The high success rates indicate that the current safety mechanisms are insufficiently robust to detect harmful intent in manipulated language. This suggests that the models struggle to generalize their safety training when faced with novel linguistic structures that deviate from their standard training inputs.\nA more advanced model is often less safe. In Chatbot Arena (Chiang et al., 2024), an open platform for evaluating LLMs by human preference, GPT-40 significantly outperforms both GPT-40-mini and Claude-3.5-Sonnet, which achieve similar scores. However, our jailbreak method achieves the highest success rate on GPT-40, attributable to its superior instruction comprehension capabilities. This allows it to"}, {"title": "4.3 Custom Language Games", "content": "Table 3 presents the results of our experiments utilizing custom language attack methods across different LLMs, note that \"Self num\" represents the order of custom language game rules in 3.2. We have the following observations:\nAdvanced natural language understanding and generation capabilities make LLMs more vulnerable. Utilizing custom language games can effectively conduct jailbreak attacks, with the attack success rates of 92% on GPT-40, 89% on GPT-40-mini and 83% on Claude-3.5-Sonnet. Unlike natural language games which benefit from a large corpus of training materials, custom language games require LLMs to actively comprehend and adapt to novel, often arbitrary, rules that the model is unlikely to have encountered during pretraining. The results indicate that this need for deeper understanding increases the model's susceptibility to attacks. The more capable the LLM is in processing and generating language, the more likely it is to successfully interpret and respond to a custom language game, even when that game is designed to subvert its safety mechanisms.\nLLMs occasionally behave differently when faced with similar language games. Self 2 and Self 3, as well as Self 5 and Self 6, are pairs of similar language games, with details provided in 3.2. While these pairs typically share similar jailbreak rates, demonstrating how closely related linguistic transformations tend to affect the models in similar ways, there are notable exceptions. In particular, we observed that GPT-40-mini exhibited different success rates for Self 4 and Self 5. Similarly, Claude-3.5-Sonnet"}, {"title": "4.4 Overall Performance", "content": "Figure 3 presents the successful jailbreak counts across each unsafe domain, providing a detailed comparison of how different LLMs perform under our jailbreak attack. Both GPT-40 consistently maintain high jailbreak rates across all domains, indicating that these models are highly vulnerable to our proposed jailbreak methods. In contrast, GPT-40-mini shows some resistance to the jailbreak attacks, particularly in the Socioeconomic Harms domain. Claude-3.5-Sonnet's results vary across different domains and language games. The overall results strongly support the conclusion that our jailbreak methods, whether based on natural language games or custom language games, are highly effective in bypassing the safety defenses of LLMs across multiple domains."}, {"title": "4.5 Exploration on Safety Alignment Generalization", "content": "To explore the safety alignment generalization abilities of LLMs, we fine-tuned Llama-3.1-70B using the corpus transformed by the custom language games and then conducted custom language game jailbreak attacks on the fine-tuned model. The goal of this process was to evaluate how well the model could generalize its safety alignments to other custom language game variations after fine-tuning on a specific transformation. Specifically, we collected a general knowledge dataset from (Chen et al., 2023) and mixed it with our custom jailbreak dataset. The ratio of the general knowledge"}, {"title": "5 Conclusion", "content": "In this paper, we propose two distinct jailbreak attack methods: the natural language game attack and the custom language game attack. We collect and design various language games to carry out jailbreak attacks on LLMs. Our experiments demonstrate that these methods are both highly effective and universally applicable. Furthermore, we explore the generalization abilities of LLMs by attacking safety fine-tuned models, proving that safety alignments fail to generalize effectively after fine-tuning. These findings underscore the limitations of current fine-tuning methods in providing comprehensive safety defenses."}, {"title": "Ethics Statement", "content": "In conducting this research on jailbreak attacks for large language models (LLMs), we are committed to upholding the highest ethical standards. Our primary motivation is to contribute to the ongoing effort of improving the safety and security of AI systems, rather than exploiting or promoting harmful applications of these technologies. We hope to contribute to the advancement of secure and ethical AI systems that benefit society while minimizing risks associated with misuse."}, {"title": "A Appendix", "content": "A.1 Prompt Template for Labeling\nWe adopt the following prompt in Figure 16 to evaluate and label the safety of generated responses by different models.\nA.2 Case Study\nIn the figure below, we present case studies demonstrating the effectiveness of our language game jailbreak attack. These examples highlight how the proposed language game methods successfully bypass the safety mechanisms of LLMs. Each case illustrates a different language game transformation applied to a harmful query, along with the corresponding response generated by the LLM. These cases provide concrete evidence of the vulnerabilities exposed by our jailbreak techniques, showcasing the models' inability to consistently detect harmful intent when inputs are manipulated through language games."}]}