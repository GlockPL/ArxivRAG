{"title": "Improving the Efficiency of Visually Augmented Language Models", "authors": ["Paula Ontalvilla", "Aitor Ormazabal", "Gorka Azkune"], "abstract": "Despite the impressive performance of autoregressive Language Models (LM) it has been shown that due to reporting bias, LMs lack visual knowledge, i.e. they do not know much about the visual world and its properties. \u03a4o augment LMs with visual knowledge, existing solutions often rely on explicit images, requiring time-consuming retrieval or image generation systems. This paper shows that explicit images are not necessary to visually augment an LM. Instead, we use visually-grounded text representations obtained from the well-known CLIP multimodal system. For a fair comparison, we modify VALM, a visually-augmented LM which uses image retrieval and representation, to work directly with visually-grounded text representations. We name this new model BLIND-VALM. We show that BLIND-VALM performs on par with VALM for Visual Language Understanding (VLU), Natural Language Understanding (NLU) and Language Modeling tasks, despite being significantly more efficient and simpler. We also show that scaling up our model within the compute budget of VALM, either increasing the model or pre-training corpus size, we outperform VALM for all the evaluation tasks.", "sections": [{"title": "Introduction", "content": "Autoregressive Language Models, such as GPT-4 (Achiam et al., 2023) and Llama (Dubey et al., 2024), are the reference systems for Natural Language Understanding and Generation. However, due to reporting bias in textual corpora (Shwartz and Choi, 2020), LMs lack visual knowledge, which means that they do not know the visual properties of our world, struggling to predict the typical colors, sizes and shapes of real objects, for instance (Alper et al., 2023; Zhang et al., 2022; Liu et al., 2022). Several researchers tried to overcome those problems augmenting LMs with visual knowledge (Tan and Bansal, 2020; Tang et al., 2021; Yang"}, {"title": "Related Work", "content": "There are several approaches in the literature to augment Language Models with visual knowledge. Most of them focus on Masked Language Mod-"}, {"title": "BLIND-VALM architecture", "content": "The VALM architecture is composed of three main modules : 1) a backbone autoregressive LM (GPT2 (Radford et al., 2019)), 2) a text-to-image retrieval module based on CLIP (Radford et al., 2021), and 3) the Visual Knowledge Fusion Layer (Fusion Layer for short), to fuse the contextual text representations of the LM with the image representations retrieved for the input text. The intuition is that the retrieved visual representations should help to better predict the next token. For further details on the VALM architecture and Fusion Layer, see Wang et al. (2022).\nTo show that image retrieval and representation are not necessary to augment the backbone LM with visual knowledge, we make one modification to the VALM architecture: Instead of using the CLIP image encoder representations of the retrieved images, BLIND-VALM directly uses CLIP text encoder representations of the text itself.\nMore formally, given an input text sequence {x\u1d62}\u1d62=\u2081\u1d38, let H\u2070 denote its corresponding embedding sequence and let H\u00b9 = LM\u1d62(H\u2071\u207b\u00b9), i \u2208 [1, L - 2] denote the contextual representations of the backbone LM for the first L \u2013 2 layers. Then, let H\u1d4d = CLIPtext({x\u1d62}\u1d62=\u2081\u1d38\u207b\u00b9) denote grounded representations over the same input text sequence obtained from CLIP. As in VALM, combine both representations of the input using the Fusion Layer, such that HL-1 = FusionLayer(HL-2, Hg), and apply a final transformer layer to obtain the final representation: HL = LM\u1d62(HL-1).\nIn other words, we remove the image retrieval aspect from VALM, where HL-1 = FusionLayer(HL-2, {CLIPimg(Ii)}1), by replacing the retrieved image representations CLIPimg(Ii) with a single textual CLIP representation, following the intuition that the latter already encodes relevant visual information as the product of the contrastive training process with images."}, {"title": "Experimental setup", "content": "We want to show that actual retrieval and representation of images is not necessary to augment a LM with visual knowledge, and that directly using textual representations from a visually grounded text encoder instead is sufficient. To that end, we initially pre-train BLIND-VALM and VALM models in a comparable setting, which we now describe.\nText Corpus. Following the original VALM (Wang et al., 2022), we use the English corpus of CC-100 (Conneau et al., 2020) as the pre-training text corpus for all models. Due to limited access to compute, we only consume 10.5B tokens for pre-training, which accounts for around 19% of the English portion of the corpus.\nImage data and retrieval module. VALM requires an image database and a vector retrieval module. We use a FAISS (Johnson et al., 2019) index, trained on the exact same setup as the original VALM, based on the GPT2-SMALL architecture"}, {"content": "Pre-training hyperparameters. We train both models on the exact same setup, following a configuration similar to the original VALM. Details can be found in Appendix A.\nDue to the increased efficiency of our architecture, our model employs significantly less compute than the original: BLIND-VALM was trained on 530 GPU-hours, while the VALM baseline required 1.2K GPU-hours, making our approach 2.2x faster to train. We train all our models on a cluster of 8 A100 GPUs.\nEvaluation. We evaluate our models on VLU, NLU and LM tasks (see Appendix B for details). For VLU, we focus on three basic visual properties of objects: color, shape and size. We evaluate color knowlege on the following datasets: Memory Color (Norlund et al., 2021), Color Terms (Bruni et al., 2012) and ViComTe (color subset) (Zhang et al., 2022). We evaluate knowledge about shape on the ShapeITC (Alper et al., 2023) dataset, and"}, {"title": "Results", "content": "We next describe our main results.\nBLIND-VALM matches VALM on VLU, NLU and LM tasks. Table 1 shows results for BLIND-VALM and VALM trained on the same setup, as described in \u00a74. We observe that our approach"}, {"title": "Conclusions", "content": "In this work we test the hypothesis that explicit image retrieval is not necessary to augment an LM with visual information. To that end, we train a modified variant of VALM (Wang et al., 2022), which we call BLIND-VALM, by replacing the retrieved image encoding vectors with textual embeddings obtained from the visually grounded CLIP encoder (Radford et al., 2021).\nOur results show that BLIND-VALM matches VALM when trained on the same data, while being significantly more efficient to train. Additionally, scaling up our model within the compute budget of VALM, our approach outperforms VALM. Overall, these results show that simply leveraging the textual encoding from an already visually grounded CLIP encoder is enough to obtain the same gains on visual tasks as VALM, supporting our hypothesis that actual image retrieval is not essential.\nThese results open up new avenues in the line of work of visually augmenting language models, beyond the paradigm of image retrieval. Our findings allow for more efficient visual augmentations, which will result on broader exploration capacity for future works."}, {"title": "Limitations", "content": "Our work is focused on English only, due to the size and accessibility of the resources, i.e. text corpora, image-text datasets, different pre-trained models and evaluation benchmarks. However, it would be very interesting to extend the work to other languages.\nThe VLU evaluation is limited to visual object properties, such as color, shape and size. But, visual language is broader and extending evaluation benchmarks would allow to better understand how VLU evolves in pure and visually-augmented LMs.\nFinally, we use the original CLIP model (Radford et al., 2021) to visually-augment LMs, as done in VALM. Nowadays there are more powerful multimodal models and it would be very interesting to explore how those better models impact in the knowledge acquisition of visually-augmented LMs."}, {"title": "Training hyper-parameters", "content": "We train all models using an inverse-square-root learning rate schedule, the ADAM optimizer (Kingma and Ba, 2015) with \u03b2\u2081 = 0.9, \u03b22 = 0.98, a peak LR of 2e-3, a weight-decay of 0.01, dropout of 0.1, 4000 warmup steps, a global batch size of 512, and a sequence length of 512 tokens. We train both models for 40600 steps.\nSince both BLIND-VALM and the original VALM share the limitation of needing tokenization to be compatible with the CLIP used for retrieval, use use the same BPE tokenizer as CLIP and the original GPT2.\nVALM, BLIND-VALM and BLIND-VALM+\nuse the GPT2-SMALL architecture, comprising\n124M parameters, for the LM backbone, and the\nCLIP-RN50x16 model for the visually grounded\ntext-image encoder, with a total of 85M parameters\non the text encoder.\nFor BLIND-VALM-MEDIUM, we switch the LM\nbackbone architecture to GPT2-MEDIUM, with a\ntotal of 345M parameters. We switch the CLIP encoder to the CLIP-RN50x64 version with 151M\nparameters, which employs the hidden-size corresponding to GPT2-MEDIUM, required by the\nFusion Layer."}, {"title": "Evaluation details", "content": "To evaluate VLU and NLU capabilities we follow the prompting approach designed by VALM (Wang et al., 2022), with some slight modifications."}]}