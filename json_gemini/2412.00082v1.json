{"title": "Dual Prototyping with Domain and Class Prototypes for Affective Brain-Computer Interface in Unseen Target Conditions", "authors": ["Guangli Li", "Zhehao Zhou", "Tuo Sun", "Ping Tan", "Li Zhang", "Zhen Liang"], "abstract": "Abstract-EEG signals have emerged as a powerful tool in affective brain-computer interfaces, playing a crucial role in emotion recognition. However, current deep transfer learning-based methods for EEG recognition face challenges due to the reliance of both source and target data in model learning, which significantly affect model performance and generalization. To overcome this limitation, we propose a novel framework (PL-DCP) and introduce the concepts of feature disentanglement and prototype inference. The dual prototyping mechanism incorporates both domain and class prototypes: domain prototypes capture individual variations across subjects, while class prototypes represent the ideal class distributions within their respective domains. Importantly, the proposed PL-DCP framework operates exclusively with source data during training, meaning that target data remains completely unseen throughout the entire process. To address label noise, we employ a pairwise learning strategy that encodes proximity relationships between sample pairs, effectively reducing the influence of mislabeled data. Experimental validation on the SEED and SEED-IV datasets demonstrates that PL-DCP, despite not utilizing target data during training, achieves performance comparable to deep transfer learning methods that require both source and target data. This highlights the potential of PL-DCP as an effective and robust approach for EEG-based emotion recognition.", "sections": [{"title": "I. INTRODUCTION", "content": "Motion, as a fundamental physiological signal, plays a critical role in human communication. Extensive research has established a close link between emotional states and mental health disorders [1]. Accurately recognizing emotions is crucial for daily life, psychological health management, and human-computer interaction. Recently, electroencephalography (EEG)-based emotion recognition has become a focal point for understanding and modulating human emotions, attracting growing interest from researchers in affective computing [2], [3]. This trend is further fueled by advancements in machine learning and deep learning, which have enabled the development of increasingly efficient and accurate models for emotion recognition [4]. Despite this progress, two critical challenges remain that limit the practical application of EEG-based emotion recognition systems. First, EEG signals are highly individualized, and current models rely heavily on target domain data (transfer learning models), which reduces their adaptability in real-world settings. How can we create emotion recognition models that accommodate substantial individual variability without requiring target-specific data, thereby improving real-world usability? Second, label noise continues to be a pressing issue, affecting model reliability. Developing models with increased resilience to label noise is essential for enhancing the robustness of emotion recognition systems. Addressing these challenges is crucial for advancing the field and enabling more adaptable, accurate, and reliable emotion recognition technologies.\nPrevious studies have emphasized that EEG signals are highly subject-dependent [5], [6], and that the way individuals perceive and express emotions can vary significantly [7]. These individual differences extend to neural processes involved in emotional regulation, further complicating the task of emotion recognition [8]. A fundamental assumption in many machine learning and deep learning methods is that training and testing data share the same feature space and follow the same distribution, thus satisfying the independent and identically distributed (IID) condition. However, individual variability in EEG signals often disrupts this assumption, leading to substantial performance degradation or even model failure when traditional emotion recognition models are applied to new subjects. This variability presents a significant challenge for the effectiveness and generalizability of existing models, underscoring the need for approaches that can adapt to diverse individual EEG patterns without compromising performance.\nTo address the challenges posed by individual variability in EEG signals, a growing number of researchers are employing transfer learning methodologies, which have shown promising results [9]\u2013[16]. Transfer learning accommodates variations in domains, tasks, and data distributions between training and testing phases. By considering the feature distributions of both the source domain (with labeled data and known distribution)"}, {"title": "II. RELATED WORK", "content": "To overcome the limitations of the IID assumption, which is challenging to uphold due to significant individual variability in EEG signals, an increasing number of researchers are turning to transfer learning methods. In transfer learning, the labeled training data is source domain, and the unknown test data is target domain. Current transfer learning algorithms for EEG-based emotion recognition can generally be divided into two categories: non-deep transfer learning models and deep transfer learning models."}, {"title": "A. Non-deep transfer learning models", "content": "To facilitate knowledge transfer between the source and target domains, Pan et al. [20] proposed the Transfer Component Analysis (TCA) method, which minimizes the maximum mean discrepancy (MMD) to learn transferable components across domains. Fernando et al. [21] introduced the Subspace Alignment (SA) method, which learns a mapping function to align source and target subspaces. The results showed SA could reduce the domain gap and enhance the adaptability of the model. Zheng et al. [22] proposed two transfer learning approaches specifically for EEG signals. The first combines TCA with kernel principal component analysis (KPCA) to identify a shared feature space. The second approach, Transductive Parameter Transfer (TPT), constructs multiple classifiers in the source domain and transfers knowledge to the target subject by learning a mapping from distributions to classifier parameters, which are then applied to the target domain. Additionally, Gong et al. [23] introduced the Geodesic Flow Kernel (GFK), which maps EEG signals to a kernel space that captures domain shifts using geodesic flow. This approach enhances feature alignment by integrating multiple subspaces and identifying domain-invariant directions, thereby supporting more robust cross-domain adaptation."}, {"title": "B. Deep transfer learning models", "content": "Non-deep transfer models would be limited in complexity and capacity, constraining their ability to fully meet the practical demands of emotion recognition. With advances in deep learning theories and technologies, deep learning-based transfer algorithms have been introduced, offering enhanced model capabilities in terms of performance and generalizability. These algorithms have been widely applied in EEG-based emotion recognition, and most contemporary models now leverage deep transfer learning.\nFor example, Tzeng et al. [24] proposed the Deep Domain Confusion (DDC) architecture, a CNN model incorporating an adaptation layer and a domain confusion loss based on MMD. This approach learns representations that are both semantically rich and domain-invariant. Zhang et al. [25] introduced a cross-subject emotion recognition method that utilizes CNNS with DDC. This method constructs an Electrode-Frequency Distribution Map (EFDM) from EEG signals, using a CNN to extract emotion-related features while employing DDC to minimize distribution differences between source and target domains. Jin et al. [26] applied the Domain-Adversarial Neural Network (DANN) framework to EEG-based emotion recognition. DANN eliminates distribution discrepancies between source and target subjects, improving cross-domain robustness. Many recent deep transfer learning methods for EEG emotion recognition have been developed based on the DANN structure, leveraging its capacity for effective domain adaptation. For example, Li et al. [18] introduced the Bi-Domain Adversarial Neural Network (BiDANN), which accounts for asymmetrical emotional responses in the left and right hemispheres of the brain, leveraging neuroscientific insights to improve emotion recognition performance. Li et al. [27] developed the Region-Global Spatial-Temporal Neural Network (R2G-STN), which incorporates neuroscientific insights by considering emotional response variations across different brain regions. Ye et al. [16] introduced a semi-supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive learning framework (DS-AGC) to enhance feature representation in scenarios with limited labeled data. This framework includes a graph contrastive learning method to extract effective graph-based feature representations from multiple EEG channels. Additionally, it incorporates a self-attentive fusion module for feature fusion, sample selection, and emotion recognition, focusing on EEG features more relevant to emotions and identifying data samples in the labeled source domain that are closer to the target domain. These deep transfer learning methods has provided valuable insights and strategies for addressing individual differences in EEG signals and achieving significant results in EEG-based emotion recognition. However, a key challenge remains: the reliance on target domain data during training, which increases practical application costs and limits the scalability of these models in real-world scenarios."}, {"title": "C. Prototype Learning", "content": "The core concept of prototype learning is that each class is represented by a prototype (a feature vector that acts as a central, representative feature for that class). Data points belonging to a specific class are clustered around this prototype, enabling classification by evaluating the proximity or similarity of data points to their respective class prototypes. For example, Snell et al. [28] proposed prototypical networks, which learn a metric space where samples from the same category are clustered around their respective class prototypes. In Pinheiro et al. [29]'s work, prototype representations are computed for each category, and target domain images are classified by comparing their feature representations to these prototypes, assigning the label of the most similar prototype. Ji et al. [30] tackled proposed Semantic-guided Attentive Prototypes Network (SAPNet) framework to address the challenges of extreme imbalance and combinatorial explosion in Human-Object Interaction (HOI) tasks. Liu et al. [31] developed a refined prototypical contrastive learning network for few-shot learning (RPCL-FSL), which combines contrastive learning with few-shot learning in an end-to-end network for enhanced performance in low-data scenarios. Yang et al. [32] introduced the Two-Stream Prototypical Learning Network (TSPLN), which simultaneously considers the quality of support images and their relevance to query images, thereby optimizing the learning of class prototypes. These studies demonstrate that prototype learning is particularly effective in few-shot learning and unsupervised tasks, offering a potential solution by enabling models that do not rely on target domain data. In the application of EEG-based emotion recognition, Zhou et al. [9] proposed a prototype representation-based pairwise learning framework (PR-PL), where sample features interact with prototype features through a bilinear transformation. Experimental results demonstrated that these interaction features with prototype representations can significantly enhance model performance. However, PR-PL considers only class prototypes, assuming that source domain data follow a uniform distribution, a limitation that does not align with real-world variability. Additionally, PR-PL requires both source and target data during training to align sample features, which restricts its practical applicability in scenarios where target data may not be accessible."}, {"title": "III. METHODOLOGY", "content": "The source domain is defined as $S = \\{S_1, S_2, S_3, ..., S_n\\}_{n=1}^{N_d}$, where $N_d$ denotes the number of subjects in the source domain. For each individual subject in the source domain, we have $S_n = \\{(x_i^n, y_n)\\}_{i=1}^{N_s}$, where $x_i^n$ denotes the i-th sample of n-th subject, $y_n$ represents the corresponding emotion label, and $N_s$ is the sample size for the n-th subject. The target domain is represented as $T = \\{(x_i, y_i)\\}_{i=1}^{N_t}$, where $N_t$ denotes the number of EEG samples in the target domain. For clarity, Table I summarizes the commonly used notations."}, {"title": "A. Feature Disentanglement", "content": "Inspired by the work of Peng et al. [33] and Cai et al. [34], we hypothesize that EEG features involves two types of deep features: domain-invariant class features and class-invariant domain features. The domain-invariant class features capture semantic information about the class to which a sample belongs, while the class-invariant domain features convey the domain or subject-specific information of the sample. Original EEG features can be viewed as an integration of these two types of features.\nThe distributional differences in EEG signals across subjects can be attributed to variations in the domain features, causing a shift in the distribution of class features. Since EEG classification adheres to a common standard, the class features from different subjects should ideally occupy the same feature space and follow a consistent distribution. Traditional methods assume that all test data come from a single, unified domain, effectively focusing only on class features while overlooking the presence of domain features. This assumption limits model generalization across subjects. Feature extraction methods such as DANN can be interpreted as an attempt to remove the domain-specific components from sample features, thereby aligning the class features across subjects and improving the generalization performance of the model. In this paper, we approach EEG feature extraction with consideration for both domain-specific and class-specific components to improve robustness and generalization across diverse subjects.\nWe start with a shallow feature extractor $f_g$ to obtain shallow features from the EEG samples x. Then, we introduce a class feature disentangler $f_c(.)$ and a domain feature disentangler $f_d(.)$ to disentangle the semantic information within these shallow features, resulting in class features $x_c$ and domain features $x_d$, expressed as:\n$x_c = f_c(f_g(x))$\n$x_d = f_a(f_g(x))$\nTo improve the effectiveness of the disentanglers in separating the two types of features, we introduce a domain discriminator $D_d(.)$ and a class discriminator $D_c(.)$. The domain discriminator is designed to determine the domain of the input features, while the class discriminator ascertains the class of the input features. Our goal is for the domain discriminator to accurately identify the domain of the input when given domain features, while the class discriminator should be unable to identify the class based solely on domain features. This inability to classify based on domain features indicates successful disentanglement, where domain features contain only domain-specific information and no class-related information. Similarly, we aim for class features to contain only class-related information, free from domain-specific information. To achieve this, we draw on ideas from DANN [35]. Before the class features are passed into the domain discriminator and the domain features into the class discriminator, they pass through a Gradient Reversal Layer (GRL) to facilitate adversarial training. We use a binary cross-entropy loss function to optimize the discriminators. The output from each discriminator is first passed through a sigmoid layer to obtain probability values, which are then compared to the true labels. This approach converts the multi-class problem into several independent binary classification tasks. The binary cross-entropy loss function is defined as follows:\n$L = \\frac{1}{N} \\sum_{i=1}^{N}(y_i \\cdot log(z_i) + (1 - y_i) \\cdot log(1 \u2013 z_i))$\nHere, $y_i$ represents the true class labels, and $z_i$ denotes the predicted class labels from the discriminator. Specifically, for the class discriminator, the binary cross-entropy loss can be defined as:"}, {"title": "B. Prototype Inference", "content": "For domain features, we assume that each domain has a prototype representation, which we refer to as the domain prototype. This prototype represents the key characteristics of that domain, with the distribution of domain features centered around it. For each domain, the domain prototype can be considered the \"centroid\u201d of all its domain features. Similarly, for each category within a domain, we derive class prototypes through prototype inference. The class prototypes capture the essential properties of each class within the domain and serve as the \"centroid\" of the class features. Both types of prototypes, domain and class, can be computed as the average value of their respective sample features, denoted as $\u00b5$. Specifically, the estimation of domain prototypes for each domain is given by:\n$\\mu_d = \\frac{1}{|X_n|} \\sum_{x_i^n \\in X_n} x_d^i$\nwhere $X_n = \\{(x_i^n, y_i^n)\\}$ represents the collection of domain features for all samples from the n-th subject in the source domain. Here, $|X_n|$ denotes the number of samples from this subject, $x_d^i$ is the the domain feature of the i-th sample, and $y_i^n$ is the corresponding domain label for that sample's domain feature. For data from the same domain, the domain labels $y_d^i$ are identical. For the class features within a single domain $d_n$, the class prototype is given as\n$\\mu_{d_n,c^*} = \\frac{1}{|X_{d_n}^{c^*}|} \\sum_{x^i \\in X_{d_n}^{c^*}} x_c^i$\nwhere $X_{d_n}^{c^*}$ represents the set of class features for samples that classified as $c^*$ from the n-th subject's samples. Here, $|X_{d_n}^{c^*}|$ denotes the number of samples classified as $c^*$ in the n-th subject's data. $x_c^i$ is the class feature of the i-th sample, and $y_i$ is the class label for that sample. In summary, for each sample, we will obtain the corresponding domain prototype $\u00b5_d$ and class prototypes $[\\mu_{d,1}, \\mu_{d,2},..., \\mu_{d,N_c}]$. Here, $N_c$ represents the number of classes. During training, each subject's prototypes are calculated based on the features of all their samples and are updated throughout the training process to better capture the feature distribution. In the testing phase, these prototypes are fixed.\nAfter obtaining the domain prototypes and class prototypes, we proceed with prototype inference. For each sample, after feature disentanglement and extraction of the corresponding domain and class features, we first perform domain prototype inference to identify the most suitable domain. This is followed by class prototype inference within the selected domain to determine the class label. Specifically, for the domain feature $x_d$, we compare its similarity with each class domain prototype using a bilinear transformation $h(.)$ as\nh(x_d^i, \\mu_d) = (x_d^i)^T S \u00b5_d$\nwhere $S \u2208 R^{d \\times d}$ is a trainable, randomly initialized bilinear transformation matrix that is not constrained by positive definiteness or symmetry. The model updates the weights of this bilinear matrix through backpropagation, with the purpose of enhancing the feature representation capability for downstream tasks. We compare the similarity between the sample's domain feature and each domain prototype, as follows:\nDsim = softmax([h(x_d^i, \u00b5_{d,1}), ..., h(x_d^i, \u00b5_{d,N_d})])\nHere, $\u00b5_{d,n}(n = 1,..., N_d)$ represents the domain prototype of the n-th subject.The most similar domain $d^*$ is determined based on the $\u00b5_n$ corresponding to the maximum value in the vector Dsim. For each training epoch, once the most similar domain $d^*$ for the sample is identified, the class prototypes $\u00b5_{d^*,k} (k = 1,..., N_c)$ for that domain are used to measure the similarity between the sample's class features and each class prototype. When comparing class features with class prototypes, we use cosine similarity, as follows:\nl_i = softmax([d_{cos} (x_c^i, \\mu_{d^*,1}), ..., d_{cos}(x_c^i, \\mu_{d^*,k})])\nwhere $d_{cos}()$ denotes the cosine similarity computation."}, {"title": "C. Pairwise Learning", "content": "To address this issue and enhance the model's resistance to label noise, we employ a pairwise learning strategy to replace pointwise learning. Unlike pointwise learning, pairwise learning takes into account the relationships between pairs of samples, capturing their relative associations through pairwise comparisons. The pairwise loss function used is defined as follows.\n$L_{class} = \\frac{1}{N_b} \\sum_{i, j \\in N_b} L(r_{ij}, g(x_i^c, x_j^c; \\theta))$\nwhere $L(.)$ is the binary cross-entropy function, defined in Eq. 3. $N_b$ represents the number of samples in a batch. $r_{ij}$ is determined based on the class labels of samples i and sample j. For class labels $y_i$ and $y_j$ of samples i and j, if $y_i = y_j$, then $r_{ij} = 1$; otherwise, $r_{ij} = 0$. The $r_{ij}$ derived from the sample labels enhances the model's stability during the training process as well as its generalization capability. The term $g(x_i^c, x_j^c; \\theta)$ represents the similarity measure between the class features of samples $x_i^c$ and $x_j^c$, given as:\ng(x_i^c, x_j^c; \\theta) = \\frac{l_i l_j}{||l_i|| \\cdot ||l_j||}$\nHere, $l_i$ and $l_j$ are the feature vectors of the class feature of samples $x_i^c$ and $x_j^c$, obtained through prototype inference ( Eq. 9). The symbol \u2219 represents the dot product operation. The result of $g(x_i^c, x_j^c; \\theta)$ falls within the range [0, 1], representing the similarity between the two feature vectors $l_i$ and $l_j$. In summary, the objective function for the pairwise learning is defined as follows:\n$L_{pairwise} (\\theta) = \\frac{1}{N_b} \\sum_{i, j \\in N_b} L(r_{ij}, g(x_i^c, x_j^c; \\theta)) + \\beta R$\nCompared to pointwise learning, pairwise learning has a stronger resistance to label noise. Furthermore, a soft regularization term R is introduced to prevent the model from overfitting, with its weight parameter $\\beta$ as:\nR = ||P^T P \u2013 I||_F\nwhere each row of the matrix P represents the domain prototype belonging to a source domain subject, $||.||_F$ denotes the Frobenius norm of the matrix, and I represents the identity matrix."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We validate the proposed PL-DCP using the widely recognized public databases SEED [36] and SEED-IV [37]. The SEED dataset includes 15 subjects, each participating in three experimental sessions conducted on different dates, with each session containing 15 trials. During these sessions, video clips were shown to evoke emotional responses (negative, neutral, and positive) while EEG signals were simultaneously recorded. For the SEED-IV dataset, it includes 15 subjects, each participating in three sessions held on different dates, with each session consisting of 24 trials. In this dataset, video clips were used to induce emotions of happiness, sadness, calmness, and fear in the subjects.\nThe acquired EEG signals undergo preprocessing as follows. First, the EEG signals are downsampled to a 200 Hz sampling rate, and noise is manually removed. The denoised data is then filtered using a bandpass filter with a range of 0.3 Hz to 50 Hz. For each experiment, the signals are segmented using a 1-second window, and differential entropy (DE) features, representing the logarithmic energy spectrum of specific frequency bands, are extracted based on five frequency bands: Delta (1-3 Hz), Theta (4-7 Hz), Alpha (8-12 Hz), Beta (14-30 Hz), and Gamma (31-50 Hz), resulting in 310 features for each EEG segment (5 frequency bands \u00d7 62 channels). Finally, a Linear Dynamic System (LDS) is applied to smooth all obtained features, leveraging the temporal dependency of emotional changes to filter out EEG components unrelated to emotions and those contaminated by noise [38]. The EEG preprocessing procedure adheres to the same standards as previous studies to enable fair comparisons with models presented in previous literature."}, {"title": "B. Experiment Protocols", "content": "To thoroughly evaluate the model's performance and enable a comprehensive comparison with existing methods, we adopt two different cross-validation protocols. (1) Cross-Subject Single-Session Leave-One-Subject-Out Cross-Validation. This is the most widely used validation method in EEG-based emotion recognition tasks. In this approach, data from a single session of one subject in the dataset is designated as the target, while data from single sessions of the remaining subjects serve as the source. To ensure consistency with other studies, we use only the first session for the cross-subject single-session cross-validation. (2) Cross-Subject Cross-Session Leave-One-Subject-Out Cross-Validation. To more closely simulate practical application scenarios, we also assess the model's performance for unknown subjects and unknown sessions. Similar to the previous method, all session data from one subject in the dataset is assigned as the target domain, while data from all sessions of the remaining subjects serve as the source domain."}, {"title": "V. DISCUSSION", "content": "To thoroughly evaluate the model's performance and assess the impact of each module in the proposed PL-DCP, we conduct an ablation study. The ablation results, based on the SEED dataset under cross-subject single-session leave-one-subject-out cross-validation, are presented in Table VI. (1) Removing the domain discriminator loss. A decrease of 7.53% in model performance is observed. (2) Removing the class discrimination loss. The model's performance drops from 82.88% to 79.18%, resulting in a decrease of 3.70%. (3) Removing both domain discriminator loss and class discrimination loss. It causes a significant performance decline of 8.39%. This indicates that the combined presence of both domain and class discriminators enhances the extraction of relevant features, substantially improving model recognition performance in the target domain. (4) Removing the bilinear transformation matrix S. The bilinear transformation matrix S in Eq. 8 contributes to model performance, increasing accuracy by 2.24%. (5) Removing soft regularization R. The soft regularizer R also improves accuracy, raising it by 1.15%. These results demonstrate the effectiveness of each component within the PL-DCP model and their combined impact on overall performance."}, {"title": "B. Visualization of Domain and Class Features", "content": "To intuitively understand the extracted domain and class features, we use T-SNE [49] to visualize these features for the respective samples, enabling us a clear observation of how the features and prototypes evolve over training. The visualizations of domain and class features at different training stages are shown in Fig. 4 (a)-(c) and Fig. 4 (d)-(f), respectively. These figures capture the features at the beginning of training (first column), after 50 training epochs (second column), and at the end of training (third column). In the domain feature visualizations (upper row), different colors represent the domain features for each domain, while diamonds in corresponding colors denote the domain prototypes for each domain. The target data are represented as semi-transparent black crosses (x) to avoid excessive overlap with other domain features. Comparing the feature distribution from Fig. 4 (a) to (c), it becomes evident that the domain features for the same subject are closely clustered, forming separate groups with the domain prototypes located at the center of each cluster. This differentiation in domain feature distributions across subjects supports our hypothesis that domain features, derived through feature disentanglement of shallow features, can effectively distinguish between subjects. A similar trend is observed in the class feature visualizations. Here, different colors represent different classes, with the semi-transparent black crosses (\u00d7) again indicating the target data. As training progresses, a more defined class boundary among different classes becomes apparent from Fig. 4 (d) to (f), illustrating the model's ability to learn clear class separability over the course of training. To further illustrate the relationships between domain prototypes and class prototypes (Fig. 5), we analyze both close pairs and distant pairs of domain samples, visualizing their respective representations in the class prototype space. The results reveal that samples closer in the domain prototype space tend to remain closer in the class prototype space, indicating consistency and coherence in the mapping across the two prototype spaces. This observation reinforces the effectiveness of the proposed framework in preserving the intrinsic relationships between samples during dual prototype learning."}, {"title": "C. Effect of Noisy Labels", "content": "We further evaluate the model's performance under label noise to assess the robustness and noise-resistance capability introduced by pairwise learning. In this process, we randomly replace a proportion (7%) of labels in the source domain data with incorrect labels, simulating real-world scenarios where data labels may contain noise. The model is then trained using this noisy source domain data, and its performance is validated on the unseen target domain data. We vary the \u03b7% values to 5%, 10%, 20%, and 30%, respectively, yielding model accuracies and standard deviations of 81.46% \u00b1 5.54%, 80.32% \u00b1 6.39%, 79.79% \u00b1 6.09%, and 79.01% \u00b1 7.46%. These results indicate that as the label noise rate \u03b7% gradually increases from 5% to 30%, the model's performance decreases only slightly, with a steady downward trend resulting in an overall performance drop of just 3.87%. This suggests that the proposed model demonstrates strong robustness against label noise, as its performance does not exhibit significant declines in the presence of noisy labels."}, {"title": "VI. CONCLUSION", "content": "This study proposes a novel pairwise learning framework with domain and class prototypes (PL-DCP) for EEG-based emotion recognition in unseen target conditions. Unlike existing transfer learning methods that require both source and target data for feature alignment, PL-DCP relies solely on source data for model training. Experimental results show that the proposed method achieves promising results even without using target domain data for training, with performance approaching or even surpassing some deep transfer learning models that heavily rely on target domain data. This suggests that combining feature disentanglement with domain and class prototypes helps generalize more reliable and stable characteristics of individual subjects. Additionally, the introduction of pairwise learning enhances the model's resilience to label noise. These findings underscore the potential of this method for practical applications in aBCIs."}]}