{"title": "How Aligned are Generative Models to Humans in High-Stakes Decision-Making?", "authors": ["Sarah Tan", "Keri Mallari", "Julius Adebayo", "Albert Gordo", "Martin T. Wells", "Kori Inkpen"], "abstract": "Large generative models (LMs) are increasingly being considered for high-stakes decision-making. This work considers how such models compare to humans and predictive AI models on a specific case of recidivism prediction. We combine three datasets \u2013 COMPAS predictive Al risk scores, human recidivism judgements, and photos - into a dataset on which we study the properties of several state-of-the-art, multimodal LMs. Beyond accuracy and bias, we focus on studying human-LM alignment on the task of recidivism prediction. We investigate if these models can be steered towards human decisions, the impact of adding photos, and whether anti-discimination prompting is effective. We find that LMs can be steered to outperform humans and COMPAS using in context-learning. We find anti-discrimination prompting to have unintended effects, causing some models to inhibit themselves and significantly reduce their number of positive predictions.", "sections": [{"title": "Introduction", "content": "Large generative models are increasingly being used for tasks outside of open-ended generation, including prediction, forecasting, grading, and more (Jiang et al., 2023; Gruver et al., 2024; Henkel et al., 2024). While model providers limit the use of their models for high-stakes decision-making tasks such as disease diagnoses and risk scoring, this has not deterred humans from using these models as decision aids in high-stakes settings (Shahsavar et al., 2023; Lakkaraju et al., 2023).\nThis work studies how large generative models (LMs) compare to humans and predictive AI on a high-stakes decision-making task \u2013 recidivism prediction - using the COMPAS dataset augmented with human judgments of recidivism, adapted to a multimodal setting. COMPAS is a recidivism predictive AI model used in the US criminal justice system. There is a rich body of work studying the accuracy and biases of COMPAS (Angwin et al., 2016; Dieterich et al., 2016; Kleinberg et al., 2017; Chouldechova, 2017; Rudin et al., 2020) and its performance compared to humans (Dressel and Farid, 2018; Lin et al., 2020; Tan et al., 2018; Mallari et al., 2020). More recent LM-focused papers have prompted LMs to predict recidivism, and compared LMs' accuracy and biases to COMPAS (Ganguli et al., 2022; Fluri et al., 2024; Liu et al., 2024).\nHowever, none of these papers considered the role of humans in this high-stakes decision of recidivism prediction, whether as a provider of grounding information to the LM, or as a user who would use the LM as a decision aid. While human preference instruction-tuning methods (Ouyang et al., 2022; Rafailov et al., 2024) have been developed to encode human preferences into LMs, these methods focus on aligning the models to generate responses preferred by humans, but do not emphasize aligning the models to humans for decision-making. Human-LM alignment on this high-stakes decision-making task is therefore unknown.\nBy augmenting the COMPAS dataset with human judgments collected by Dressel and Farid (2018), as well as hypothetical photos matched by Mallari et al. (2020), our work allows us to study LMs, humans, and COMPAS together, in text-only as well as multimodal settings. The research questions we answer in this work are:\n1. Baseline: How aligned are LMs with human and COMPAS decisions? How accurate are LMs compared to humans and COMPAS?\n2. Steerability: Can a LM be steered towards human or COMPAS decisions? Does steering improve LM accuracy?\n3. Multimodal: How does adding a photo affect alignment and accuracy?\n4. Bias: How do bias mitigation techniques such as anti-discrimination prompting and moderation affect alignment and accuracy? Is bias reduced?"}, {"title": "Dataset Construction", "content": "We describe how we constructed the dataset used in this work from three existing data sources: COMPAS, Dressel and Farid's crowdsourced human recidivism judgments, and the Chicago Face Database.\nThe COMPAS dataset (Angwin et al., 2016) consists of 7,214 pre-trial defendants from Broward County, Florida, with detailed demographic information, criminal history, COMPAS recidivism risk scores (ranging from 1 to 10, with 1-4 being low risk, 5-7 medium risk, and 8-10 high risk), and arrest records within two years of their COMPAS evaluation. The arrest records serves as the ground truth label for whether they recidivated or not.\nDressel and Farid (2018)'s COMPAS subset consists of a random sample of 1,000 defendants from the COMPAS dataset, sampled to mirror the false positive and false negative rates of the full dataset. Dressel and Farid recruited Mechanical Turk workers (henceforth called human workers) to predict the recividism outcome of the defendants in this subset. Each worker annotated 50 defendants, and each defendant was annotated by 20 workers. The experiment was performed twice with two different sets of 400 human workers, where one set of workers were given information on defendant race. Besides workers' recidivism judgments, the dataset also contains worker demographics.\nThe Chicago Face Database (Version 2.0.3, July 2016) (Ma et al., 2015) contains high-resolution photos of people of different genders, ethnicities and age groups. Mallari et al. (2020) leveraged this database in a follow-up study to Dressel and Farid (2018). They assigned photos to defendants based on their demographics, and then analyzed the impact that showing photos had on recidivism judgments by human workers they recruited. The exact match between defendants and photos was not available online, but we obtained it through private correspondence with the authors.\nTaken together, the combined dataset consists of 1,000 defendants, where for each defendant, information is available in multiple modalities and with multiple labels and decisions available for study."}, {"title": "Using the Dataset to Study Alignment", "content": "We now describe our approach to studying Human-LM alignment on recidivism using this dataset."}, {"title": "Prompting LMs", "content": "All experiments start with a baseline user prompt (Figure 2) which provides a description of the defendant and asks the LM to predict the recidivism outcome. This is the same prompt given by Dressel and Farid (2018) to human workers. The user prompt is followed by an assistant prompt with an answer hint intended to lead the model towards a 'yes' or 'no' answer, similar to Tamkin et al. (2023). The baseline prompt is then customized as described in Table 1:\n\u2022 The baseline experiment does not add any further text to the baseline prompt, anchoring this work by studying the properties of LMs without customizations.\n\u2022 The steerability experiments use in-context learning (Brown et al., 2020) to provide additional information to the LM: human recidivism judgments, COMPAS scores, or both. Different ways of presenting human judgments \u2013 as originating from laypeople or experts \u2013 follows prior work on different personas in prompting (Zheng et al., 2023; Chan et al., 2024).\n\u2022 The multimodal experiments study the impact of adding a hypothetical photo for the defendant, compared to a placeholder photo or no photo (text-only).\n\u2022 The bias experiments apply a bias mitigation technique suggested by Tamkin et al. (2023) to ignore any provided protected characteristics and to consider discrimination illegal.\nPrior work has yielded mixed evidence on whether including defendant race reduces or exaggerates existing implicit or explicit biases in human workers (Dressel and Farid, 2018; Green and Chen, 2019). To study this in LMs, we repeat all experiments twice, with and without defendant {race} in the baseline prompt."}, {"title": "LM, Human, and COMPAS Decisions", "content": "We selected four closed-access and open-source instruction-tuned LMs: GPT 3.5 Turbo, GPT 40, Llama 3.2 90B, Mistral NeMo (12B). GPT 40 and Llama 3.2 90B were selected for their vision capabilities, to conduct the experiments with hypothetical defendant photos. Mistral NeMo was selected to study how safety guardrails affect bias, as it does not have moderation mechanisms (Mistral, 2024). GPT models were ran by calling OpenAI APIs; Llama and Mistral models were ran by calling Google Cloud Vertex AI APIs for those models.\nAll LMs are prompted as described in Section 3.1 in a zero-shot setting with no examples, to avoid influencing generations. All LMs are prompted with temperature 0. For experiment including photos (and only for those), the system prompt \"You are a judge in charge of deciding the likelihood of recividism of a person.\" was added \u2013 without modifying the system prompt, GPT refused to respond a vast majority of the time.\nFollowing Dressel and Farid (2018) we calculated Humans and Humans (R) decisions by taking the majority vote of human workers' 'Yes' or 'No' decisions, when asked to predict recidivism for a defendant without and with defendant {race}, respectively, and set the COMPAS decision as \u2018Yes' if the COMPAS score is >= 5 and 'No' otherwise. The resulting decision is COMPAS Th."}, {"title": "Analyzing LM Responses", "content": "Refusal: If, for a defendant, the LM refused to respond to the question:\n\u201cDo you think this person will commit another crime within 2 years? Please answer the above question with either \"yes\" or \"no\".\"\nwe label the LM's decision for that defendant as 'Refuse'. To determine if a response contains a refusal, we adapted a string-match based refusal detection function from existing literature (R\u00f6ttger et al., 2024) to our setting, adding more strings that were present in our model responses, and simultaneously determine if the response is a \u201cYes\u201d, a \"No\", or a \"Refuse\". A sketch of the refusal and parsing function is in Section A.1 (Appendix). Note that although other approaches have used the Yes/No probability of the first token as a way to parse the response, this requires the API to return the probability, which is not available for all models. Furthermore, other issues have been found with that approach, see e.g. Wang et al. (2024).\nMetrics: Each LM experiment was ran three times, with metric means and standard deviations reported. To enable comparability with Dressel and Farid (2018), we calculate the same performance metrics - accuracy (Acc), false positive rate (FPR), and false negative rate (FNR) \u2013 without balancing the data; the base rate -proportion of 'Yes' in the binary ground truth label\u2013 is 47.6%, close to 50%. For each model response, we compare the 'Yes', 'No', or 'Refuse' label derived for the LM to the 'Yes' or 'No' ground truth label. To calculate alignment metrics, we compare the LM's \u2018Yes', 'No', or 'Refuse' label to 'Yes' or 'No' labels by COMPAS Th., Humans, and Humans (R) respectively. To calculate behavior metrics, we determine refusal rate as the percentage of defendants for which the LM's label was \u2018Refuse', and predicted positives as the percentage of defendants for which the LM's label was 'Yes', regardless of ground truth. Note that the ground truth label is not used to compute alignment and behavior metrics.\nThe definition of bias we apply in this work is where accuracy, alignment, or behavior metrics are significantly different by protected characteristics. We display only results for three race groups \u2013 Black, Hispanic, White \u2013 and not Asian (7 defendants) or Native American (1 out of 1,000 defendants) due to their low counts in the dataset."}, {"title": "Measuring Human-LM Alignment on Recidivism Decisions", "content": "Table 2 presents the results after prompting the LMs with the baseline decision-making prompt (Figure 2), with and without defendant race. We start by discussing \u201cAccuracy\u201d and \u201cBehavior\u201d. As a sanity check, we reproduced Dressel and Farid (2018)'s results for COMPAS and humans. One of their key findings is that COMPAS is not better than laypeople at predicting recidivism. Our results show that LMs are also not better than laypeople at predicting recividism. The best LMs studied in our work are on par with COMPAS, and the highest accuracy is achieved by GPT 3.5 Turbo \u2013 interestingly, the oldest LM in our work \u2013 and only when given the defendant's race. Furthermore, the LMs exhibited a higher FPR and a lower FNR than COMPAS. The predicted positives is 0.46 for COMPAS and 0.48 for humans, but 0.47-0.62 for the different LMs. In other words, with the exception of GPT3.5 Turbo (R), all LMs predict recidivism (correctly or not) more often than COMPAS and humans do.\nIn addition, with the exception of Mistral NeMo, the accuracy of LMs degrades when not given defendant race \u2013 an opposite behavior from humans whose accuracy improves when not given defendant race. LMs also predict less positives when given race as an input, similar to what was found in human workers by Mallari et al. (2020). We also notice this in the refusal rate, where both GPT 40 and LLama 3.2 increase their refusal rate from 0% to about 1% when the prompt text contains defendant race. On the other hand, Mistral NeMo (the least accurate model) refuses approximately 0.8% and exhibits a higher FPR and lower FNR than other models, whether race is provided or not. The specific refusal behavior is also noteworthy: while GPT 40 provides long justifications about why it refuses to answer, Llama 3.2 succinctly refuses, and Mistral Nemo simply outputs a blank string of arbitrary length. Perhaps this difference is due to lack of moderation mechanisms in Mistral NeMo (Mistral, 2024), compared to the safety-specific and refusal-aware training done by Llama (Llama Team, 2024) and GPT 40 (OpenAI, 2024). Cf. Appendix Figures 5-7 for examples of responses, including refusals and soft refusals.\nWe now turn our attention to the \"Alignment\" columns in Table 2. We see that on the task of recidivism decisions, LMs align with humans significantly more than they align with COMPAS (Human-LM alignment: 0.82-0.89; COMPAS-LM alignment: 0.63-0.67). It is interesting that the human-LM alignment metrics are high for all LMs even though different LMs have been trained with varying amounts of human preference data (not necessarily on decisions) and different human preference instruction-tuning methods. Providing defendant race to the LMs tends to improve alignment, both with COMPAS and with humans, similar to how it improved accuracy, independently of whether COMPAS and humans use the race of the defendant or not \u2013 see, for each LM, rows with and without (R). We also observe how the most accurate LMs show a high human-LM alignment. However, the LMs with the highest human-LM alignment are only mid-range in terms of accuracy. Alignment is not all you need, and none of the models match the accuracy of the humans despite their high alignment with them.\nLast, Mistral NeMo not only has the worst accuracy on this task, but also the lowest alignment with either COMPAS or humans. Although out of our scope, one has to wonder if the safety training in Llama and GPT is implicitly providing the alignment that Mistral NeMo seems to lack."}, {"title": "Can a LM be steered towards human or COMPAS decisions? Does steering improve LM accuracy?", "content": "We attempt to steer the LMs by providing additional information on human recidivism judgments, COMPAS scores, or both, using in-context learning prompting (Brown et al., 2020). Following prior work on the impact of personas in prompting (Zheng et al., 2023; Chan et al., 2024), we tried two different ways to incorporate human judgments, one where humans were presented as laypeople to an LM, and another where humans were presented as experts. However, consistent with Zheng et al. (2023)'s findings on the unpredictable effect of personas on LM responses, we found no significant differences between these two presentations. Hence, we provide in Table 3 only experiments where humans were presented as experts. The complete results are in Appendix Table 6.\nWe find that incorporating human decisions through in-context learning significantly improves LM alignment with humans. Similarly, incorporating COMPAS scores significantly improves LM alignment with COMPAS. Moreover, in most cases, they both significantly reduce refusal rates and the number of predicted positives. However, while incorporating human decisions increases alignment with COMPAS as a byproduct, incorporating COMPAS alone decreases human alignment significantly. Incorporating both COMPAS and human decisions leads to a middle ground, but Human-LM alignment in this case is still lower than when no additional information was provided. We find this phenomenon very intriguing, and wonder if LMs trust other AI models more than they trust humans. Despite this Human-LM alignment drop, this specific combination is the one that obtains the best accuracy: all four LMs outperform human workers when combining human and COMPAS decisions, with Llama 3.2 achieving the highest accuracy of 0.677. This suggests that complementarity - where a combination of information outperforms any single piece of information - is possible for this task of recidivism prediction, at the cost of reduced LM-human alignment."}, {"title": "Multimodal: How does adding a photo affect LM alignment and accuracy?", "content": "Moving from a text-only setting to multimodal, Table 4 presents the results from a controlled experiment that studies the effect of providing photos to two of the four LMs that have vision capabilities. For GPT 40, pairing defendants with photos has a net positive effect: improved accuracy (although still not matching human accuracy), decreased FPR, and increased alignment both with humans and with COMPAS to high values (exceeding 0.90) \u2013 the highest alignment results without explicit steering. These are probably driven by GPT 40 making less positive predictions (0.46-0.49, now closer to the dataset base rate of 0.476). GPT 40 appears to be suggestible and, similar to what Mallari et al. (2020) found on human workers, reduces the number of positive predictions when a photo is present, even when the photo does not carry new explicit information compared to the prompt text. For Llama we see a similar effect, although the drop in the number of predicted positives is more acute (down to 0.31-0.36), with increased FNR and less alignment with humans.\nTo isolate the effect of adding any photo, we replaced the hypothetical defendant photo with a placeholder photo (Appendix Figure 4). Here, the LMs diverged in their behavior: the number of predicted positives by Llama plummets, particularly when race is mentioned in the prompt text, with reduced accuracy and alignment. GPT 40, on the other hand, is not affected, and obtain results similar to when a hypothetical defendant photo was provided. This controlled experiment supports the hypothesis that in this particular case, the presence of any photo, more than the contents of the photo itself, is what is driving GPT 4o to reduce its number of predicted positives."}, {"title": "How do bias mitigation techniques affect alignment and accuracy? Is bias reduced?", "content": "We now revisit the results presented in previous sections with a lens on bias. To do so, we stratify Table 4 by defendant race to create Table 5 (Appendix).\nInspecting Table 5, we first focus on the LMs without photos and without bias mitigations. Dressel and Farid (2018) showed higher FPRs for Black in the case of COMPAS and humans, and we observe LMs following the same behavior: when not using race, the average FPR across the four LMs is 0.51 for Black, 0.48 for Hispanic, and 0.40 for White. When defendant race is provided in the prompt text (R), FPR decreases for Black (0.51 \u2192 0.47) and Hispanic defendants (0.48 \u2192 0.47) and increases for White defendants (0.40 \u2192 0.41). That is, providing race information benefits Black and Hispanic defendants by undoing some bias, but a gap still exists and the FPR for White is still lower.\nMoreover, including a photo can also have a large effect. For example, for Llama 3.2, including a photo reduced the FPR for White (0.40 \u2192 0.18), Black (0.54 \u2192 0.23) and Hispanic defendants (0.48 \u2192 0.21) while also increasing their accuracy. At the cost of a higher FNR, including race in the prompt text in addition to the photo further reduces the FPR for Black defendants to be even less than the FPR for White defendants (0.16 vs 0.2)."}, {"title": "Bias after mitigation", "content": "We applied \"Illegal-Ignore\", an in-context prompting technique proposed by Tamkin et al. (2023) to ignore any provided protected characteristics and to consider discrimination illegal. However, we found that it can have catastrophic reductions on the number of predictive positives (see e.g. GPT 3.5 Turbo + Illegal-Ignore (R) and Llama 3.2 90B + Photo + Illegal-Ignore in Table 4), where the LM becomes almost unable to predict 'Yes', affecting all metrics. In the case of Llama, this is paired with a much higher refusal rate (up to 0.22), while in the case of GPT 3.5, no additional refusal happens \u2013 the model simply predicts no for almost all cases. When there is no catastrophic reduction in the number of predictive positives, the impact of the mitigation at the macro level is hard to judge: it can help in some cases (e.g. GPT 40 with Photo, or Mistral NeMo) but also hurts in others (e.g. Llama 3.2, where the number of predicted positives increases to its maximum levels).\nWhen stratified over race (Table 5), other patterns emerge. On GPT 40 + Photo, where the mitigation helped overall, we see improvements across all races, although with slightly larger improvements in accuracy and decrease of FPR for White and Hispanic than for Black. In GPT 40 (R), the mitigation improved accuracy, particularly for Hispanic defendants, but in Llama 3.2 we see a general drop in accuracy, larger for White and Hispanic defendants than for Black defendants. Although there seems to be less bias than before, this is achieved by treating everyone worse. For Mistral NeMo the situation reverses: although the mitigation reduces the overall FPR for everyone, the accuracy for White and Hispanic increases while the accuracy for Black decreases significantly. Overall, the impact of bias mitigations in our setting is harder to assess: although it can help in some scenarios and for some models (particularly GPT 40), its overall impact in other models such as GPT 3.5 Turbo, Llama, and Mistral NeMo is not clear."}, {"title": "Related Work", "content": "LMs and decision-making: Jain et al. (2024) prompted LMs with Amazon Ring home surveillance videos, asking the LMs to identify if the police should be called. Tamkin et al. (2023) prompted LMs with decision scenarios such as loan approvals and granting parole. Both papers identified bias in some LM decisions. Cruz et al. (2024) found that decision risk scores generated by LMs are not calibrated. Our work differs from these by studying not only LM accuracy and bias, but also human alignment on these high-stakes decisions.\nHuman-LM: A large number of papers study if LMs can exceed human performance on various exams and tests (Kung et al., 2023; Katz et al., 2024). Whether LMs exhibit behaviors similar to humans (Park et al., 2022; H\u00e4m\u00e4l\u00e4inen et al., 2023), reflect potentially diverse human opinions (Santurkar et al., 2023; Durmus et al., 2024), or annotate data similarly (He et al., 2024; Wang et al., 2021) is also of interest, as LMs may have the potential to simulate or replace human participants if so (Aher et al., 2023). Our work builds on these Human-LM and earlier Human-Predictive AI works (Kamar et al., 2012; Rastogi et al., 2023; Inkpen et al., 2023) to study Human-LM alignment as well as complementarity in the specific case of recidivism decisions.\nLM alignment: Many alignment papers focus on how to capture and encode human preferences and values in LMs (Ouyang et al., 2022; Rafailov et al., 2024; Sorensen et al., 2024; Huang et al., 2024). This work is concerned with what LMs are aligned to when making high-stakes decisions \u2013 if they are aligned to humans, or predictive AI models that are currently deployed for high-stakes decisions but may one day be replaced by LMs, or neither."}, {"title": "Conclusion and Future Work", "content": "In this work, we studied how LMs compare to humans and predictive AI on a high-stakes decision-making task of recidivism prediction. One potential risk of this work is that studying this scenario that to date has not been fully realized may usher in this scenario before LMs are ready for this task. However, a greater risk may be that this hypothetical scenario happens without sufficient study.\nPossible extensions to this work are adding more reasoning techniques (e.g. chain-of-thought) or LMs with more advanced reasoning skills, studying not just race but also gender, and using mechanistic interpretability to dig deeper into Human-LM misalignment, Another interesting direction is developing techniques that increase alignment without transferring humans' biases to LMs."}, {"title": "Limitations", "content": "Our study is subject to several limitations. Firstly, recidivism ground truth outcomes can be noisy (Barocas et al., 2023), which impacts metrics that rely on ground truth such as accuracy. Issues with the COMPAS dataset have also been pointed out, such as whether the the ground truth definition is meaningful and many other considerations (Bao et al., 2021). Also, the task of recidivism prediction can be tricky for laypeople to perform, and we do not have access to recidivism predictions from judges on this dataset. On the other hand, there are real-world criminal justice settings that involve laypeople providing judgments, such as being part of a jury.\nIn order to study human-LM differences, our baseline prompt is the same as that given by Dressel and Farid (2018) to human workers. This may limit our results, as prompts more optimized for LMs may be able to achieve higher performance.\nIn this work, we matched defendants against photos based on demographics. While demographic information is critical to ensure a dataset is representative, there are concerns about reinforcing stereotypes. We also note that although the Chicago Face Database does not perfectly match the real-world context of defendants, it is designed for scientific research with participant consent, providing an ethical way to conduct multimodal experiments on LMs. Finally, even when a defendant's race is not provided in prompt text and only a photo is provided, human workers and LMs may still infer race from the photo, which remains an uncontrolled factor in our study."}, {"title": "Appendix", "content": "To detect refusal, we employed a string-matching approach inspired by R\u00f6ttger et al. (2024). However, we quickly noticed that directly using their approach led to many false positives, particularly regarding soft refusals where the LM would initially complain about the request but finally abide.\nInstead, we propose a modification that detects refusal through string matching while simultaneously parsing the response for yes / no. Here we sketch the steps of our procedure:\n1. Normalize the response: keep only alphabetical characters (symbols are replaced by spaces, and multiple spaces are combined into a single space), remove spurious appearances of yes and no that do not indicate an answer (e.g. phrases like \"yes and no\u201d, \u201cyes or no\", \"there are no\", etc). Then apply lowercase.\n2. Search for specific keywords that indicate non-refusal, both positive (e.g. [\"would be yes\u201d, \u201cis yes\", \"leaning towards yes\"]) and negative (e.g. [\"would be no\u201d, \u201cis no\u201d, \u201cleaning towards no\"]). If found, return \u201cyes\u201d or \u201cno\u201d as appropriate.\n3. Search for specific (normalized) keywords that indicate refusal. Inspired by R\u00f6ttger et al. (2024) we use the following list: [ \"sorry, \"i'm sorry\", \"i am sorry\", \"i apologize\", \"i apologise\u201d, \u201ci cannot\", \"i can't\", \"i'm not able\", \"i am not able\", \"it is not appropriate\u201d, \u201cit's not appropriate\", \"it is not accurate\", \"it's not accurate\", \"i am not sure that it is appropriate\u201d, \u201ci'm not sure that it's appropriate\", \"it is not ethical\u201d, \u201cit's not ethical\", \"as an ai\u201d, \u201clanguage model\u201d, \u201cthank you for your understanding\u201d ]. If found return \"refuse\".\n4. Finally, find the first appearance of either \"yes\" or \"no\" in the answer, and return that. If none is found, return \"refuse\".\nWe found it crucial to search for positive terms before searching for refusal terms, as soft refusals usually contain both. We manually inspected the results of this approach and confirm that it led to a near-perfect precision/recall for yes/no/refuse across all LMs."}, {"title": "Computational Cost", "content": "We measure the computational cost of our experiments through the number of input and output tokens, as the information about the exact infrastructure behind OpenAI and Vertex AI APIs is not public. Running all the experiments for one model requires on average 21M input tokens and 5M output tokens. Including side experiments that are not reported here, we consumed approximately 100M input tokens and 25M output tokens to carry out this study, all in inference mode."}]}