{"title": "CUAL: Continual Uncertainty-aware Active Learner", "authors": ["Amanda Rios", "Ibrahima Ndiour", "Parual Datta", "Jerry Sydir", "Omesh Tickoo", "Nilesh Ahuja"], "abstract": "AI deployed in many real-world use cases should be capable of adapting to novelties encountered after deployment. Here, we consider a challenging, under-explored and realistic continual adaptation problem: a deployed AI agent is continuously provided with unlabeled data that may contain not only unseen samples of known classes but also samples from novel (unknown) classes. In such a challenging setting, it has only a tiny labeling budget to query the most informative samples to help it continuously learn. We present a comprehensive solution to this complex problem with our model \"CUAL\" (Continual Uncertainty-aware Active Learner). CUAL leverages an uncertainty estimation algorithm to prioritize active labeling of ambiguous (uncertain) predicted novel class samples while also simultaneously pseudo-labeling the most certain predictions of each class. Evaluations across multiple datasets, ablations, settings and backbones (e.g. ViT foundation model) demonstrate our method's effectiveness. We will release our code upon acceptance.", "sections": [{"title": "1 Introduction and Related Work", "content": "Real-world artificial intelligence (AI) systems often contend with evolving data distributions, due to changes in operating conditions and the introduction of new classes after deployment. To enforce a robust response, AI systems should ideally be able to continuously learn and adapt to detected novelties with minimal computing and labeling costs. Nonetheless, most current adaptive continual learning (CL) solutions [1, 2, 3] still necessitate fully labeled data, a requirement that overlooks the feasibility and high cost of data labeling in practical AI applications. Unsupervised and semi-supervised CL solutions are scarce [4, 5, 6] and furthermore, operate with a highly simplified oracle assumption: that in unlabeled post-deployment data, old (known) classes cannot appear in conjunction with newly introduced (novel) classes. Yet, in many realistic use cases this assumption will not hold, i.e. new and old classes may co-occur. One reason current models continue to adopt this oversimplification is to limit error propagation: when unseen novel-class samples co-occur with old-classes, misclassifications can occur not only among new classes but also between novel and old classes. In this scenario, the CL model may also face prediction overconfidence [7], equivocally mapping novel-class samples to old-class labels. The question then becomes how to design a robust semi-supervised CL model that can properly function in a non-overly constrained setting? In what is still a parallel research area, cost-effective active labeling (AL) [8, 9, 10, 11] has been applied extensively on closed-world learning to minimize labeling costs to a human-in-the-loop. However, only a few works have incorporated Active Labeling concepts within continual learning [12, 13, 14] as a strategy to minimize labeling costs in challenging scenarios where data distributions are evolving. Our contribution is as follows: We eliminate the commonly used oracle assumption employed by most semi-supervised CL techniques. This creates a more realistic continual learning setting where old and new classes can co-occur, while maintaining a minimal labeling budget. In this case, AL querying must be judiciously designed not only to prioritize novel-class sampling but also to select the most informative (ambiguous) samples among them [15]. We introduce our model, CUAL (Continual Uncertainty-aware Active Learner), which learns to classify new classes as they are introduced while"}, {"title": "2 Our Method", "content": "Consider an agent A(x, t) that is expected to learn to classify from a sequence of continual tasks. At each continual task t, the model is presented with an initially unlabeled set of samples U(t) which consists of a mixture of unseen samples of old classes Uold(t) and unseen samples of new (novel) classes Unew(t):\n\n$U(t) = Uold(t) \\cup Unew(t)$, where $Uold(t) = \\{x|x \\sim \\cup_{k=1}^{t-1} Dk\\}$, $Unew(t) = \\{x|x \\sim Dt\\}$.                                                   (1)\n\nHere Dt comprises samples from the set of new classes Chew introduced at task t, while $\\cup_{k=1}^{t-1} Dk$ are samples belonging to all the old classes Cold that have been learned up to and including task t - 1. Samples in Uold(t) are \u201cunseen\", meaning they were never used, neither in the initial training nor during prior tasks' learning. Note that addressing data drifts in Uold is beyond the scope of this work and will be addressed in future research.\nWe introduce a \u2018short-term' active learner As(x, i; t) whose goal is to acquire a sufficient number of samples from Unew(t) via a combination of uncertainty-guided active-labeling and pseudo-labeling (i being the index for inner-loop AL iterations). These will be used to update the parameters of A(x, t) via continuous learning procedures such as experience replay. Both modules, A and As, contain trainable classification heads, $A_{cl}^{t}$ and $A_{cl}^{s}$ respectively, that operate on features extracted from a frozen large/foundation vision model pre-trained on a separate dataset with no class overlap with the test dataset (implementation details in Appendix 4.2.2). While $A_{cl}^{t}$ predicts labels for the main continual classification task, $A_{cl}^{s}$ will perform pseudo-labeling by predicting among only new-class labels and will use an uncertainty metric $\\Omega$ to guide AL and confident pseudo-labeling.\nCUAL's uncertainty estimation $\\Omega$ uses the feature reconstruction error (FRE) metric introduced in [16]. FRE has been shown to effectively estimate novelty in the closed-world and continuous setting [17]. In multi-class settings, FRE learns a dimensionality-reducing PCA (principal component analysis) transform $T_m$ and its inverse $T_m^{-1}$ for each class m. For scoring, a test-feature u = g(x) is first transformed into a lower-dimensional subspace by applying $T_m$ and then re-projected back into the original higher dimensional space via $T_m^{-1}$. FRE for the mth class is calculated as the 12 norm of the difference between the original and reconstructed vectors.\nAt the outset (task t = 0), we assume that A(x, t = 0) has been previously trained to classify among a fixed set of pre-deployment classes $C_{new}^{0}$. Simultaneously, a set of FRE transforms, Tm have been learnt $\\forall m \\in C_{new}^{0}$. For each subsequent task t > 0, CUAL invokes its active learning inner-loop, indexed by i. At the first iteration i = 0 of a given task t, a first cycle of AL is performed by querying samples with the highest uncertainty scores defined as $S^{0}(u) = min_{j \\in C_{old}^{0}} FRE_j(u)$.\nAt this point, novel classes are identified (denoted by |$C_{new}^{t}$| in section 2.1, assuming |$C_{new}^{t}$| > 0) and the actively labeled samples are used to: (1) Initialize and train As(x, i = 0, t) to learn an imperfect initial mapping to the |$C_{new}^{t}$| novel classes ($A_{cl}^{s}$ contains output nodes only w.r.t novel classes); (2) compute rough estimates of per-novel-class PCA transforms ${T_m^{t,0}}$, m \u2208 $C_{new}^{t}$.\nFor subsequent iterations i > 0, given an unlabeled sample x \u2208 U(t), As(x, i, t) predicts a pseudo-label m, m \u2208 $C_{new}^{t}$ which then routes the selection of the corresponding PCA transform $T_{m}^{-1}$ resulting in that iteration's uncertainty score Si(u) $\\Omega$:\n\n$S^{i}(u) = min \\frac{FRE_{A_{cl}^{s}(u,i-1)}}{\\min_{j \\in C_{old}} FRE_{j}(u)}$   ; i > 0, $m=A_{cl}^{s}(u, i-1, t) \\in C_{new}^{t}$                                                        (2)\n\nSi(u) can be used to robustly categorize samples in U(t) as: (1) Novel with high-confidence: These are samples with the highest score values, which will occur for a high numerator relative to the"}, {"title": "3 Experiments", "content": "Implementations for CUAL and baselines: All use a large/foundation frozen feature extractor, e.g. ResNet50 [19] pre-trained on ImageNet1K via SwAV [20] or ViTs16 [21] pre-trained on Imagenet1K via DINO [22]. $A_{cl}^{s}$ (the pseudo-labeling head) is a fully connected layer and $A_{cl}^{t}$'s is a perceptron of size 4096 (the latter $A_{cl}^{t}$ is same for all baselines). We test on 4 diverse datasets: Imagenet21K-OOD (Im21K-OOD) [23], Places365-OOD (Places) [24], Eurosat [25], iNaturalist-Plants-20 (Plants) [26] and Cifar100-superclasses [27]. All of the aforementioned datasets were constructed to have class orthogonality (be out-of-distribution) with respect to Imagenet1K with the exception of Cifar100. Our replay buffer stores a fixed amount of pre-logit deep-embeddings and labels/pseudo-labels. We set the maximum coreset size to 5000 for Im21K-OOD, Places and 2500 for Cifar100, Eurosat. While Cifar100 is not orthogonal to Imagenet1K, we include it for direct comparison with our main benchmarking approach, incDFM, which uses Cifar100 Superclasses as its hardest experimental dataset. Details on datasets are in the supplementary. At each incoming unlabeled pool, we fix a mixing ratio of 2:1 of old to new classes per task, with old classes drawn from a holdout set (0.35% of each dataset). We set pseudo-labeling selection to a = 20% of samples predicted as novel (appendix 4.1.1). For experiments not purposely varying the tiny supervision budget, we fix a labeling budget of 1.25% for Im21K-OOD, Places, Plants and 0.5% for Eurosat, Cifar100 as guided by Fig 1 right which varies the AL budget from 0.5% to 5%. We compare CUAL to: (1) CCIC [5] a semi-supervised CL model that adapts MixMatch [28] to the CL setting; (2) Experience-Replay \"ER\" [18, 3]. ER is adapted to learn with active labeling via Entropy or Margin similarly to other continual AL works [13]; (3) PseudoER [29], same as ER, but uses the AL heuristic (Entropy or margin) to iteratively pseudo-label the most confident samples during the inner loop. Other baselines are constructed (Fig 2 right table) from removing elements of CUAL such as the iterativeness (i.e. doing AL/Pseudo-labeling in one shot during ER/PseudoER), etc. More details in appendix sections."}, {"title": "4 Conclusion", "content": "In this work, we presented CUAL, a solution to the still under-explored field of continual active learning. To the best of our knowledge, CUAL is the first continual active learner to propose and demonstrate that querying based on the ambiguity of being novel or non-novel may be more informative than just selecting the samples that are most likely to be novel. Further, this work shows that assigning pseudo-labels with trustworthy confidence can significantly alleviate labeling costs without degrading continual AL performance. Overall, the proposed approach outperforms"}, {"title": "4 Appendix", "content": "The inner-loop in CUAL is guided by two simple thresholds: (1) Threshold $T_{inner}$ \"roughly\" estimates if there are any possible novel-class samples in the unlabeled task input data pool and is controlled by a single hyper-parameter, the number of standard deviations above the mean of an in-distribution validation set (2 STDs in our experiments). If no samples are found to be above $T_{inner}$, we reach the stopping criterion for our iterations. Our in-distribution validation set is conventionally defined to"}, {"title": "4.1.2 How to define Ambiguity", "content": "CUAL selects the most ambiguous samples at each AL inner-loop iteration, i.e. scores Si(u) which are neither too high or too low. Our mathematical formulation uses the threshold $T_{inner}$ defined in the previous section: we formulate ambiguousness as the inverse squared distance $||S^{i}(u)-T_{inner} ||^{2}$ of scores to $T_{inner}$. Intuitively, this formula favors selecting samples that cannot be unambiguously predicted as either old or new since $T_{inner}$ represents this rough decision boundary. Active selection is stopped when the tiny labelling budget is exhausted. The only exception to this Ambiguity formulation is at the first iteration i = 0 where we select homogeneously from samples above $T_{inner}$. This is the case because at i = 0 only old classes are used to compute the score function, $S^{0}(u) = min_{j \\in C_{old}^{0}} FRE_j(u)$ and so ambiguity cannot be defined in the same way as for the remainder of iterations."}, {"title": "4.1.3 Measuring per-class uncertainty in CUAL S'(u) formulation", "content": "CUAL is agnostic to the elemental uncertainty metric used in its uncertainty scoring function ($S^{i} (u)$ Eq. 2 in section 2) as long as it can reliably estimate uncertainty w.r.t each novel class or old class. However, this is not an easy feat since many existing static uncertainty quantification approaches are not fully reliable [16, 17]. As discussed in the main text, CUAL currently leverages the feature reconstruction error (FRE) metric introduced in [16] to build Eq 2. For each in-distribution class, FRE learns a PCA (principal component analysis) transform {Tm} that maps high-dimensional features u from a pre-trained deep-neural-network backbone g(x) onto lower-dimensional subspaces. During inference, a test-feature u = g(x) is first transformed into a lower-dimensional subspace by applying Tm and then re-projected back into the original higher dimensional space via the inverse $T_m^{-1}$. The FRE measure is calculated as the 12 norm of the difference between the original and reconstructed vectors:\n\n$FRE_m(u) = ||f(x) - (T_m^{-1} \\circ T_m)u||_2$.                                                   (3)\n\nIntuitively, FREm measures the distance of a test-feature to the distribution of features from class m. If a sample does not belong to the same distribution as that mth class, it will usually result in a large reconstruction score FREm. FRE is particularly well suited for the continual setting since for each new class discovered at test-time, an additional principle component analysis (PCA) transform can be trained without disturbing the ones learnt for previous classes."}, {"title": "4.1.4 Continual Model Update", "content": "After the end of CUAL's inner loop, i = I with I corresponding to the last iteration, CUAL's short-term As has accumulated a final set of actively labeled and pseudo-labeled novel samples. This final set is then used to update the continual agent A(x, t): (1) obtain the final estimates of the PCA transformations corresponding to the novel classes, Tm, m \u2208 Crew, for task t and to update long-term Acl (long term classification head of A(x, t)) via experience replay. From then on, in subsequent tasks, these novel classes m, m \u2208 Crew will no longer be flagged as \u201cnovel\u201d and will be treated as \"old\".\nHere we expand upon the technique of \"Experience Replay\" (ER) [18, 3] used to update long-term Acl at the end of each task. Note that the same ER algorithm and hyper-parameters are used for baselines ER and PseudoER. In our implementation of ER a limited number of deep-embeddings belonging to old classes (from the frozen foundation model) must be stored in a Buffer of fixed size (Bt = $U_{old,k},k : 1 : t \u2212 1$). For a given task, samples Bt are interleaved with those in $U_{new,t}$ to train A(x, t) without catastrophic forgetting. After updating $A_{cl}^{t}$ the replay buffer itself must also be updated. We use a fixed-size memory buffer Bt with the same building strategy as in [31]: a buffer of fixed size and prioritizing homogeneous distribution among classes. That is, an equivalent number of samples of each class are removed if room is required for new classes and the buffer is full. Formally,"}, {"title": "4.2.2 Datasets:", "content": "Since the employed large/foundation feature extractor were pretrained on Imagenet1K, we evaluate CUAL on datasets that either do not contain class overlap with Imagenet1K (out-of-distribution w.r.t Imagenet1K [36]), or curated them by excluding any overlapping classes. The exception is cifar100, which was included due to it being a very popular and widespread dataset.\nImagenet21K-OOD (Im21K-OOD) [23]: We curated a subset of Imagenet21K containing the top-most populous 50 classes and that do not overlap with the classes present in Imagenet1K. We use a random set of 500 samples from each of the 50 classes. Because Imagenet21K is a superset of Imagenet1K, by excluding any overlapping class we guarantee orthogonality in our curated subset. We will release the full list of images chosen in this curation for reproducibility.\nPlaces365-OOD (Places) [24, 37]: is a subset of Places365 also originally curated by [37] to contain 51 \"environment\" categories orthogonal to Imagenet1K, containing a total of 9822 images. It has also been used as a test OOD dataset in [37, 38, 39] with respect to Imagenet1K.\nEurosat [25]: An RGB dataset of 10 classes and 27K images of Sentinel-2 satellite images, which is also orthogonal to Imagenet1K.\nCifar100-Superclasses (Cifar100) [27]: We use the super-label granularity of Cifar100 dataset. This totals 20 labels (super) and 50K images. While Cifar100 is not orthogonal to Imagenet1K, we decided to showcase its results since it is a widespread dataset in CL."}, {"title": "4.2.3 Baselines", "content": "As discussed, we use a range of methods for comparison: (1) CCIC [5] leverages experience replay, a MixMatch [28] derived label sharpening technique and nearest neighbor classification to solve semi-supervised CL but does not differentiate between old and new classes which may be unlabeled. we used the author's publicly released code to generate results. CCIC, similar to other semi-supervised CL models, assumes that new and old classes will not co-occur (discussed in Section 1) which ultimately explains its performance degradation when the oracle assumption (of only novel classes present in an unlabeled pool) is no longer valid, as in our experimental scenario; (2) ER [18, 3], originally proposed for supervised CL is adapted in a similar fashion to continual active learning work [13] to only use actively labeled samples (as embeddings) for replay; (3) We also adapt PseudoER [29] to continual AL by further incorporating pseudo-labeling for high confidence unlabeled samples in addition to actively labeled samples to be used for training. In both ER and PseudoER, we utilize the cumulative classification entropy as an uncertainty score to actively-label and Pseudo-Label (PseudoER). Similar to CUAL, we actively label \"ambiguous\u201d samples according to the same formula as outlined in appendix 4.1.2 for superior results, then sampling according to the TOP heuristic (see section 3 discussion). We also tested with other common uncertainty metrics such as margin [7] but with inferior results."}, {"title": "4.2 Experimental Methodology", "content": "We use two large-scale/foundation models as feature extraction backbones, kept frozen throughout CUAL and baselines' training: (1) Most results use ResNet50 [19] unsupervisedly pre-trained on ImageNet1K via SwAV [20]. We extract features from the pre-logit AvgPool layer of size 2048 as deep-embeddings. We also experimented with other feature extraction points [16] but those under-performed w.r.t the pre-logit layer. (2) We also show results (Fig 2 Left Table) using ViTs16 [21] pretrained on Imagenet1K via DINO [22]. For ViTs16 we tried several extraction points, e.g. head, last norm later, different transformer block outputs with different pool factors (e.g. 2,4). Best results were obtained with Block 9 features with pooling range of 2, yielding deep features of size 18816. Note that learning on frozen deep features is commonplace in vision CL and domain-adaptation fields [1, 17, 32]. It is theoretically based on the principle that low-level visual features from a large-scale/foundation frozen model are task nonspecific and do not need to be constantly re-learned. Rather, learning may happen upstream by utilizing the extracted deep features (at the last or inner-layers, or a combination thereof - an active research area) [33, 34, 32].\nCUAL and baseline parameters are trained using the extracted deep features as outlined above. The classification heads are trained with ADAM [35] and learning rate of 0.001. For long-term Acl we use a batchsize of 50 and an average of 20 epochs for each task during ER. CUAL $A_{cl}^{s}$ is trained at each inner-loop for an average of 5 epochs and mini-batch ranging from 10-20. As mentioned in the main text, we implement $A_{cl}^{s}$ as a fully-connected layer (we also tried 1-layer perceptrons ut with marginal performance gain). Baselines' long term classifier and CUAL's $A_{cl}^{t}$ are implemented as a one layer perceptron of size 4096 (also tested variations with marginal variations in results). The ER replay buffer is set to a size 5000 deep-embeddings for Places365 [24], Imagenet21K-OOD [23] and 2500 for eurosat and cifar100. Lastly, baseline CCIC [5] was trained using same hyperparameters proposed by the authors and their open-source code."}]}