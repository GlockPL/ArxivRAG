{"title": "Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction", "authors": ["Jhon A. Castro-Correa", "Jhony H. Giraldo", "Mohsen Badiey", "Fragkiskos D. Malliaros"], "abstract": "Reconstructing time-varying graph signals (or graph time-series imputation) is a critical problem in machine learning and signal processing with broad applications, ranging from missing data imputation in sensor networks to time-series forecasting. Accurately capturing the spatio-temporal information inherent in these signals is crucial for effectively addressing these tasks. However, existing approaches relying on smoothness assumptions of temporal differences and simple convex optimization techniques have inherent limitations. To address these challenges, we propose a novel approach that incorporates a learning module to enhance the accuracy of the downstream task. To this end, we introduce the Gegenbauer-based graph convolutional (GegenConv) operator, which is a generalization of the conventional Chebyshev graph convolution by leveraging the theory of Gegenbauer polynomials. By deviating from traditional convex problems, we expand the complexity of the model and offer a more accurate solution for recovering time-varying graph signals. Building upon GegenConv, we design the Gegenbauer-based time Graph Neural Network (GegenGNN) architecture, which adopts an encoder-decoder structure. Likewise, our approach also utilizes a dedicated loss function that incorporates a mean squared error component alongside Sobolev smoothness regularization. This combination enables GegenGNN to capture both the fidelity to ground truth and the underlying smoothness properties of the signals, enhancing the reconstruction performance. We conduct extensive experiments on real datasets to evaluate the effectiveness of our proposed approach. The experimental results demonstrate that GegenGNN outperforms state-of-the-art methods, showcasing its superior capability in recovering time-varying graph signals.", "sections": [{"title": "I. INTRODUCTION", "content": "The accumulation of complex unstructured data has experienced a tremendous surge due to the noteworthy advancements in information technology. Undertaking the task of representing and analyzing such data can present a formidable challenge. Nevertheless, Graph Signal Processing (GSP) and Graph Neural Networks (GNNs) have emerged as promising areas of research that have demonstrated remarkable potential for unstructured data in recent years [1]\u2013[4]. GSP and GNNs adopt a data modeling approach wherein data is represented as signals or vectors residing on a collection of graph nodes. This framework encompasses the incorporation of both feature information and the inherent relational structure of the data. This approach offers novel insights into data manipulation, effectively bridging the domains of machine learning and signal processing [5], and has profound implications across diverse fields, including semi-supervised learning [3], node classification, link prediction, graph classification [6]\u2013[9], clustering [10], computer vision [11]\u2013[13], recommendations in social networks [14], [15], influence propagation [16] and misinformation detection [17], materials modeling [18], and drug discovery [19], among others.\nSampling and reconstructing (or imputing) graph signals have become crucial tasks that have attracted considerable interest from both the signal processing and machine learning fields in recent times [1], [20]-[26]. However, there is a lack of research on the reconstruction of time-varying graph signals\u00b9 despite its numerous applications in sensor networks, time-series forecasting, and infectious disease prediction [23], [27]\u2013[29]. Prior research has primarily concentrated on expanding the concept of smoothness from static graph signals to those that evolve over time, as evidenced by Qiu et al. [30]. Furthermore, the rate of convergence of optimization techniques employed in reconstruction has been analyzed in several works [23], [28]. Nevertheless, these optimization-based methods heavily depend on rigid assumptions about the underlying time-varying graph signals, which can pose limitations in real-world applications. For example, some previous approaches in GSP assume that the graph Fourier transform of the signals are bandlimited [1], i.e., the projection of the signal into the spectrum of the graph can be represented with few components. However, in real-world scenarios, this bandlimitedness assumption is often not satisfied; the signals typically consist of components spanning the entire spectrum of the graph and are often corrupted by noise. This non-bandlimitedness fact also has profound implications regarding the sample complexity in problems of semi-supervised node classification for example [12], [31].\nFrom the perspective of GNNs, their applications to the reconstruction of time-varying signals is a relatively unexplored area that holds immense potential. The ability of GNNs to capture both spatial and temporal dependencies within graph-structured data makes them well-suited for handling time-varying signals observed over interconnected entities, where the temporal evolution is as crucial as the spatial relationships. However, existing GNN works lack simultaneous exploration of both spatial and temporal relationships in time-varying\n\u00b9The recovery or regression of time-varying graph signals can be viewed as a matrix completion problem where each column (or row) corresponds to a specific time and each row (or column) corresponds to a vertex of a graph."}, {"title": "II. RELATED WORK", "content": "The problem of sampling and reconstruction of static signals has been addressed from both the GSP [21], [22], [33]\u2013[39] and machine learning [2], [40] perspectives. In the GSP context, Pesenson [41] introduced the concept of Paley-Wiener spaces in graphs, which establishes that a graph signal can be uniquely determined by its samples in a specific set of nodes known as the uniqueness set. Consequently, if a graph signal is sampled according to its uniqueness set, a bandlimited graph signal can be reconstructed perfectly. However, in real-world datasets, graph signals are typically approximately bandlimited instead of strictly bandlimited, making the assumption of strict bandlimitedness unrealistic. To overcome this limitation, several approaches have been proposed that leverage the smoothness assumption of graph signals [42]\u2013[44], where smoothness is quantified using a Laplacian function. Similarly, other studies have explored the use of Total Variation [45] or extensions of the concept of stationarity in graph signals [46], [47] for reconstruction purposes.\nIn the realm of time-varying graph signals, researchers have investigated the concept of joint harmonic analysis to establish connections between time-domain signal processing techniques and GSP [48]. Additionally, some studies have put forward reconstruction algorithms that assume the bandlimited nature of signals at each time instance [45], [49]. However, these methods often fail to fully exploit the inherent temporal correlations present in time-varying graph signals. In an effort to address this limitation, Qiu et al. [30] introduced an approach that captures temporal correlations by utilizing a temporal difference matrix applied to the time-varying graph signal."}, {"title": "III. PRELIMINARIES", "content": "In this paper, sets are denoted by calligraphic letters, such as V, with their cardinality represented as |V|. Matrices are denoted by uppercase boldface letters, such as A, while vectors are represented by lowercase boldface letters, such as x. The identity matrix is denoted as I, and 1 represents a vector consisting of ones with appropriate dimensions. The pseudo-inverse of a matrix A is defined as At, while A\u2265 0 denotes a positive semidefinite matrix. The Hadamard and Kronecker products between matrices are respectively denoted by and . Transposition is indicated by (\u00b7)T. The vectorization of matrix A is represented as vec(A), and diag(x) denotes the diagonal matrix with entries {x1,x2,...,xN} as its diagonal elements. The l2-norm of a vector is expressed as ||x||2. The maximum and minimum eigenvalues of matrix A are respectively denoted as \u03bbmax(A) and \u03bbmin(A), while the Frobenius norm of a matrix is represented by ||A||F."}, {"title": "B. Graph Signals", "content": "We use the notation G = (V,E) to represent a graph, where V = {1,2,...,N} denotes the set of nodes, and E\u2286 {(i, j) | i, j \u2208 V; and i \u2260 j} represents the set of edges. Each element in E indicates a connection between vertices i and j. The graph structure is represented by the adjacency matrix A \u2208 R^{N\u00d7N}. For any (i, j) \u2208 E, a positive value A(i, j) signifies the weight associated with the connection between nodes i and j. This study focuses on connected, undirected, and weighted graphs. The degree matrix D \u2208 R^{N\u00d7N} can be described as a diagonal matrix denoted as D = diag(A1), where each element D(i, i) on the diagonal represents the sum of edge weights connected to the ith node. For the purpose of this study, we define the combinatorial Laplacian matrix as L = D \u2013 A. The Laplacian matrix L is a positive semi-definite matrix with eigenvalues 0 = \u03bb1 \u2264 \u03bb2 \u2264 \u2026 \u2264 \u03bbN, along with their corresponding eigenvectors u1,u2,...,uN. A graph signal is a function that assigns real values to a set of nodes, represented as x : V \u2192 R. In the case of a static graph signal, it can be expressed as a vector x \u2208 RN, where x(i) corresponds to the value of the graph signal at the ith node. The graph Fourier operator is defined by the eigenvalue decomposition of the Laplacian matrix L = U\u039bUT, where U = [u1,u2, ...,un] and \u039b = diag(\u03bb1, \u03bb2, . . ., \u03bb\u03bd). Each eigenvalue \u03bbi corresponds to a frequency associated with the ith eigenvalue [1]. The Graph Fourier Transform (GFT) of a graph signal x is defined as x = U\u00afx, while the inverse GFT is given by x = Ux."}, {"title": "C. Reconstruction of Smooth Time-varying Graph Signals", "content": "The reconstruction of graph signals plays a fundamental role in the field of GSP [20], [21]. To address the challenges of signal reconstruction and sampling in graph domains, smoothness assumptions have been widely employed. The concept of smoothness in graph signals has been formalized through the notion of local variation [53]. To capture the idea of global smoothness, we can introduce the discrete form of the p-Dirichlet operator [53]. It characterizes smoothness by defining Sp(x)iev\u2207ix, where \u2207ix represents the local variation of a graph signal. Therefore, we have the following expression:\n$$S_p(x) = \\frac{1}{p} \\sum_{i \\in V} \\left( \\sum_{j \\in N_i} A(i,j) [x(j)-x(i)]^2 \\right)^{\\frac{p}{2}}$$\nwhere Ni represents the set of neighbors of node i. When p = 2, we obtain the graph Laplacian quadratic form given by S2(x) = \u2211(i,j)\u03b5\u03b5 \u0391(i, j)[x(j) \u2013 x(i)]\u00b2 = xTLx [53].\nFor time-varying graph signals, some studies assumed that the temporal differences of the signals are smooth [23], [30]. Let X = [X1, X2, \u2026\u2026\u2026,XM] \u2208 RN\u00d7M be a time-varying graph signal, where xs \u2208 RN is a graph signal in G at time s. The smoothness of X is given by:\n$$S_2(X) = \\sum_{s=1}^M x_s^T L x_s = tr(X^T L X).$$"}, {"title": "D. Learning Graphs from Data", "content": "When the graph structure is not readily available for the given task, we need to infer a meaningful graph from the data. The classical approach for this problem is the k-Nearest Neighbors (k-NN) method with a Gaussian kernel [1]. Learning graphs from data has been extensively studied in the literature, with contributions from the signal processing and machine learning communities [54]\u2013[57]. In this paper, we either use the k-NN approach or adopt the smoothness assumption to infer the underlying graph structure from the data. We employ Graph-Based Filters (GBFs) within a regularized maximum-likelihood framework, as defined in [58].\nLet h(L) = Uh(\u039b)UT be a GBF such that (h(\u039b))ii = h(\u03bbi), for all i. By selecting h(\u03bb) as a monotonically decreasing function, such that h(\u03bb1) \u2265 \u2265 h(\u03bbN) > 0, we can learn L by solving the following optimization problem:\n$$L = \\arg \\min_L tr \\left( h_\\beta(L)^{\\dagger} S \\right) - \\log |h_\\beta(L)^{\\dagger}| + \\gamma ||L||_1$$\n$$L \\succeq 0, \\beta$$\nsubject to\n$$L1 = 0, \\quad (L(i, j)) \\leq 0 \\quad i \\neq j,$$\nwhere \u03b3 denotes a regularization parameter, \u03b2 represents the (unknown) parameter for a specific type of GBF hp(\u00b7), and S is the sample covariance calculated using n samples xi for i = 1, 2, ...,n. Readers are referred to Table I in [58] for details of different GBFs."}, {"title": "IV. GEGENBAUER GRAPH NEURAL NETWORK", "content": "In this section, we introduce the GegenGNN architecture, which leverages Gegenbauer polynomials for graph convolutions. Our main objective is to reconstruct time-varying graph signals, and the overall framework is illustrated in Figure 1. The input to our architecture is the graph Laplacian matrix L, which can be constructed using the k-NN algorithm or learned from data, as discussed in Section III-D. We process a sampled version of the time difference signal (JoX)Dh as input, encode it using Gegenbauer-based convolutions, and then decode the reconstructed signal X. To facilitate the reconstruction process, we incorporate a specialized regularization term that accounts for the time dependency of the data, as elaborated upon in subsequent sections."}, {"title": "A. Spectral Graph Convolution", "content": "The spectral approach in GSP offers a precise localization operator on graphs by employing convolutions that involve a Kronecker delta in the spectral domain [1]. The convolution theorem [59] states that convolutions are linear operators that can be diagonalized in the Fourier basis, which is represented by the eigenvectors of the Laplacian operator. As described in [2], a graph signal x \u2208 RN can be filtered by a non-parametric filter g\u03b8 as follows:\n$$y = g_\\theta(L)x = U g_\\theta(\\Lambda) U^T x,$$\nwhere \u03b8 \u2208 RN represents a vector of Fourier coefficients. However, due to their intrinsic properties, non-parametric filters are not localized in space and have a computational complexity of O(N3).\nTo overcome the limitations of non-parametric filters, localized filters can be constructed using K-order polynomials, such that g\u03b8 (\u039b) = \u2211k=0 \u03b8k\u039bk. However, even with these localized filters, the complexity remains high at O(N2) when multiplying with the Fourier basis U to filter the signal x, as expressed by y = U g\u03b8 (\u039b) UTx [2]. Fortunately, this issue can be overcome by directly parameterizing g\u03b8(L) using the truncated expansion\n$$g_\\theta(L) = \\sum_{k=0}^{K-1} \\Theta_k P_k^{(\\alpha)}(L),$$\nwhere \u0398k \u2208 RS is a vector of polynomial coefficients, and P(\u03b1) \u2208 RN\u00d7N is a polynomial of k-order evaluated at the scaled Laplacian L = 2L/\u03bbmax(L) I. For the computation"}, {"title": "B. Gegenbauer Polynomials", "content": "The Gegenbauer polynomials $C_k^{(\\alpha)}(z)$ of degree k are orthogonal on the interval z \u2208 [-1,1] with respect to the weight function (1 \u2212 z2)\u03b1\u22121/2 and are solutions to the Gegenbauer differential equation (1 \u2013 z\u00b2)y'' \u2013 2(\u03bc + 1)y' + (\u03c5 \u2212 \u03bc)(\u03c5 + \u03bc + 1)y = 0 [63]. Gegenbauer polynomials are a generalization of Chebyshev and Legendre polynomials to a (2\u03b1 + 2)-dimensional vector space and are proportional to the ultraspherical polynomials $P_k^{(\\lambda)}(z)$. We can represent Gegenbauer polynomials in terms of the Jacobi polynomials $J_k^{(\\alpha,\\beta)}(z)$ when \u03b1 = \u03b2 = \u03b1 \u2212 1/2 by:\n$$C_k^{(\\alpha)}(\u03b1)(z) = \\frac{\\Gamma(\\alpha+1) \\Gamma(k+2\\alpha)}{\\Gamma(2\\alpha) \\Gamma (k+\\alpha+ \\frac{1}{2})}J_k^{(\\alpha-1/2,\\alpha-1/2)}(2),$$\nwhere \u0393 is the gamma function \u0393(k) = (k \u2212 1)! \u2200 k > 0. Thus, we can define the basis functions for the Gegenbauer polynomials using the following recurrence relation\n$$C_0^{(\\alpha)}(z) = 1, \\quad C_1^{(\\alpha)}(z) = 2\\alpha z.$$\nFor k\u2265 2, we have that:\n$$C_k^{(\\alpha)}(z) = \\frac{2z(k + \\alpha - 1)}{(k + 2\\alpha - 2)} C_{k-1}^{(\\alpha)}(z) - \\frac{k}{C_{k-2}^{(\\alpha)}(z),$$\nwhere k \u2208 N is the coefficient representing the kth-term Gegenbauer polynomial and \u03b1 > \u22121/2. By setting the parameter \u03b1 in $C_k^{(\\alpha)}$ to a positive integer, we can compute Chebyshev (kinds I & II) and Legendre polynomials, as these are special cases of the Gegenbauer polynomials. Chebyshev polynomials of the kind I used for the spectral convolution operator introduced in [2] can be derived in terms of $C_k^{(\\alpha)}$ as:\n$$T_k(z) = \\lim_{\\alpha \\to 0} \\frac{1}{2} \\frac{k + \\alpha}{\\alpha} C_k^{(\\alpha)}(z) \\text{ if } k \\neq 0$$\n$$\\lim_{\\alpha \\to 0} C_0^{(\\alpha)}(z) = 1 \\text{ if } k = 0,$$\nwhereas, Chebyshev polynomials of kind II can be easily computed by setting \u03b1 = 1 as\n$$U_k(z) = C_k^{(1)}(z)$$\nAnalogously, the Legendre polynomials can be derived by assigning to \u03b1 a value of 1/2 as follows\n$$L_k(z) = C_k^{(1/2)}(z).$$\nSimilar to the work in [2] with the Chebyshev basis, we can define a polynomial filtering operation (Eq. (7)) for spectral convolutional using Gegenbauer polynomials basis for graphs using the normalized Laplacian matrix as $g_\\theta(L) = \\sum_k \\Theta_k C_k^{(\\alpha)}(L)$ [62]."}, {"title": "C. Computation of the Gegenbauer Basis", "content": "By utilizing the recursive formula for the Gegenbauer basis, we can efficiently calculate all of the basis vectors in O(\u03be|E|) time and perform \u03da message-passing operations. The recurrence relations in Eqs. (9)-(10) can be used to quickly compute the $C_k^{(\\alpha)}(L)$ basis as follows:\n$$C_0^{(\\alpha)}(L) XW = XW,$$\n$$C_1^{(\\alpha)}(L) XW = 2 \\alpha LXW.$$\nFor k\u2265 2, we have:\n$$C_k^{(\\alpha)}(L) XW = \\frac{2\\alpha}{(k + 2\\alpha - 2)}\\left( k - \\frac{1}{2} \\right) (L C_{k-1}^{(\\alpha)}(L) X) W.$$"}, {"title": "D. Graph Neural Network Architecture", "content": "GegenGNN is a generalization of the Chebyshev spectral graph convolutional operator defined by Defferrard et al. [2] (Eq. (11)), as the parameter \u03b1 enables the use of orthogonal basis in more complex domains. The propagation rule for our Gegenbauer-based convolutional operator is defined as follows:\n$$Z_\\mu^{(l)} = \\sum_{k=0}^{\u03b6-1} \u0398_\\mu^{(l)} C_k^{(\\alpha)}(L) X_\\mu^{(l)} W^{(l)},$$\nwhere W(l) is the matrix of trainable parameters for layer l, \u0398(l) is the vector of Gegenbauer coefficients for layer l, and $C_k^{(\\alpha)}(L)$ is computed recursively using the relations in (14)-(15). To fully harness the power of the Gegenbauer polynomial filters, we employ the filtering operation in (16) to propose a novel cascaded-type convolutional layer for our architecture. This layer consists of two components: 1) a cascade of Gegenbauer graph filters with increasing order, and 2) a linear combination layer, as depicted in Fig. 2.\nTo provide a more detailed description, we precisely outline the propagation rule for each layer of GegenGNN as follows:\n$$H^{(l+1)} = \\sum_{\\rho=0}^{\\zeta - 1} \\mu_\\rho Z_\\rho^{(l)},$$\nwhere H(l+1) is the output of layer l+1, \u03c2 is a hyperparameter, \u03bc(l) is a learnable parameter, and Z(l) is recursively computed for each branch \u03c1 as in (16). A single layer in the GegenGNN architecture is composed by a cascade of Gegenbauer filters of increasing order 0,1,..., \u03c2 \u2212 1 as in (17), where the input of the first layer is (JoX)Dh. Finally, our loss function is such that:\n$$\\mathcal{L} = \\frac{1}{|T|} \\sum_{(i, j) \\in \\mathcal{T}} (X(i, j) - \\hat{X}(i, j))^2 + \\lambda tr\\left( (X D_h)^T (L + C I) X D_h \\right),$$\nwhere X is the reconstructed graph signal, T is the training set, with Ta subset of the spatio-temporal sampled indexes given by the sampling matrix J, and \u0454 \u2208 R+ is a hyperparameter. The term $tr ((X D_h)^T (L + C I) X D_h)$ is the Sobolev smoothness [23]. Our GegenGNN is designed as an encoder-decoder network, utilizing a loss function that combines mean squared error (MSE) and Sobolev smoothness regularization. The initial layers of GegenGNN encode the term (JoX)Dh into an H-dimensional latent vector, which is then decoded to reconstruct the time-varying signal using the final layers. This architecture enables the extraction of spatio-temporal information by leveraging a combination of GNNs, temporal encoding-decoding structure, and the regularization term tr ((X Dh)T (L + CI)XD), where the temporal operator Dh is employed. The parameter A in (18) controls the trade-off between the Sobolev smoothness term and the MSE loss."}, {"title": "V. EXPERIMENTAL EVALUATION", "content": "We compare GegenGNN with Natural Neighbor Interpolation (NNI) [65], Time-Varying Graph Signal Reconstruction (TGSR) [30], Time-varying Graph signal Reconstruction via Sobolev Smoothness (GraphTRSS) [23], Graph Convolutional Networks (GCN) [3], ChebNet [2], Graph Attention Networks (GAT) [66], Transformer [67], GCN powered by Hierarchical Layer Aggregations and Neighbor Normalization (GCN-DHLA) [6], Graph Neural Networks with High-Order Polynomial Spectral Filters (FFK-GCNII) [8], and Multiresolution Reservoir Graph Neural Network (MRGNN) [68]."}, {"title": "A. Implementation Details", "content": "In this study, we implemented GegenGNN, GCN, ChebNet, GAT, and Transformer architectures using the PyTorch and PyG libraries [69]. Similarly, we adapted GCN-DHLA, FFK-GCN, and MRGNN using the same libraries, relying on the original implementations by the authors. For the implementation of NNI, TGRS, and GraphTRSS, we utilized the MATLAB\u00ae 2023a (code provided in [23]). To ensure fair comparisons, we extensively optimized the hyperparameters"}, {"title": "B. Datasets", "content": "GegenGNN, along with state-of-the-art algorithms, undergoes evaluation on a diverse set of datasets comprising four real-world datasets, including 1) the Shallow Water Experiment 2006 (SW06) [70], 2) the mean concentration of Particulate Matter (PM) 2.5 [30], 3) the Sea-surface temperature, and 4) the Intel Berkeley Research lab dataset. A summary of the datasets is presented in Table I\nSW06 dataset: The data utilized in this study were obtained from the Shallow Water acoustic and oceanographic experiment 2006 (SW06), conducted off the coast of New Jersey from mid-July to mid-September in 2006 [32]. During the experiment, a network of acoustic and oceanographic moorings was deployed in two intersecting paths: one along the 80-meter isobath, parallel to the shoreline, and another across the shelf starting from a depth of 600 meters and extending towards the shore to a depth of 60 meters. A cluster comprising 16 moorings, each equipped with sensors, was placed at the intersection of these two paths. These sensors captured the three-dimensional temperature changes in the water column and detected the presence of internal waves (IWs) during the experiment. The variations in water column density caused by these waves had an impact on the propagation of acoustic sound speed [70]. The majority of environmental sensors deployed in the study area consisted of temperature, conductivity, and pressure sensors, enabling the measurement of physical oceanographic parameters throughout the water column. For this research, temperature data in Celsius obtained from 59 thermistors located at the cluster in the intersection were utilized. The specific time frame of interest spanned from August 6, 17:39, to August 15, 00:00 UTC, 2006.\nCalifornia daily mean PM2.5 concentration. In our analysis, we utilized a publicly available dataset provided by the US Environmental Protection Agency (EPA) that contains information on the daily mean PM2.5 concentration in California.\nSea surface temperature dataset. The sea surface temperature dataset utilized in our study was obtained from the Earth System Research Laboratory.\nIntel Lab dataset. The dataset used in this study was obtained from the Intel Berkeley Research Laboratory, where 54 sensors were deployed. The dataset consists of temperature readings recorded between February 28 and April 5, 2004. It includes timestamped topology information and provides measurements of humidity, temperature, light, and voltage values. The data was captured at a frequency of once every 31 seconds. For the purposes of this research, only the temperature data measured in Celsius within the research laboratory were utilized."}, {"title": "C. Evaluation Metrics", "content": "In this study, we employ different metrics to compare and evaluate the performance of the algorithms presented here. For a ground truth vector x with N time steps and its reconstruction x, we compute the Root Mean Square Error (RMSE), which measures the overall error magnitude as\n$$RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\hat{x}_i)^2}.$$\nWe also calculate the Mean Absolute Error (MAE), which considers error without accounting for the direction and is given by $$MAE = \\frac{1}{N} \\sum_{i=1}^{N} |x_i - \\hat{x}_i|.$$\nAdditionally, we use the Mean Absolute Percentage Error (MAPE) to assess errors relative to the magnitude of estimated values, and it is computed as $$\\frac{1}{N} \\sum_{i=1}^N \\frac{|x_i - \\hat{x}_i|}{x_i}$$"}, {"title": "D. Experiments", "content": "This section provides details on the experimental framework. In this work, we follow a random sampling strategy in all experiments using different sampling densities m for each dataset. We compute the reconstruction error metrics on the non-sampled nodes for a set of sampling densities m. In our study, we divided each dataset into separate development and testing sets. The sampled values were used for the development set, while the non-sampled values were reserved for testing. For instance, if m = 0.1, the development set corresponds to 10% of the data and the testing set to 90%. To tune the hyperparameters, we employed a Monte Carlo cross-validation setting with 5 different folds over the development set. We performed 300 repetitions for Monte Carlo on each fold. For each combination of hyperparameters in the Monte Carlo setting, we split the development set into 70% for training and 30% for validation. This allowed us to train the model with different hyperparameter settings and evaluate the model performance. After identifying the optimal model based on the validation results, we proceeded to train the resulting model using the selected hyperparameters. The trained model was then evaluated on the testing data for 15 repetitions with different seeds, ensuring a thorough assessment of its performance.\nFor GAT and transformer architectures, we used a fixed graph construction method. We connected all nodes in the dataset (excluding self-loops) and allowed the networks to learn the attention coefficients during the training stage. This means that the connectivity pattern of the graph was not predetermined but learned by the models themselves. By adopting this approach, we aimed to leverage the expressive power of attention mechanisms in capturing the dependencies between nodes in the graph by adaptively assigning weights to different nodes based on their relevance to the task at hand.\nFor all datasets except for the SW06 experiment, we constructed graphs G using the k-NN approach with the Euclidean distance between node locations and a Gaussian kernel as in [23]. Regarding the PM 2.5 concentration, the sampling densities m are 0.1, 0.15, 0.2, ..., 0.45. For the sea-surface temperature, the sampling densities are specified as 0.1, 0.2, ..., 0.9, while for the Intel Lab dataset, they are set to 0.1, 0.3,0.5,0.7. In all three datasets, we establish a connection between the graph's nodes by setting k = 5 in the k-NN algorithm. For the SW06 experiment, we also consider the sampling densities in the set 0.1, 0.2,..., 0.9. However, the SW06 experiment involved sensors placed in a three-dimensional (3D) environment, where the horizontal distances between sensors were in the order of kilometers and the vertical distances were in meters. In such a scenario, a k-NN approach alone does not capture the temperature fluctuations in the underwater sensor network. To address this issue, we applied a graph inference approach that tackles the Graph System Identification (GSI) problem using a regularized maximum likelihood (ML) criterion, as explained in Section III-D. This approach allows us to incorporate temperature fluctuations."}, {"title": "E. Results and Discussions", "content": "Figure 4 illustrates the performance of GegenGNN compared to baseline methods across all four datasets, using the root mean squared error (RMSE) metric for different sampling densities m. The range of m varies depending on the dataset. It can be observed that GegenGNN consistently outperforms existing methods in all cases. Moreover, Table II provides a quantitative comparison by averaging the performance metrics across all sampling densities for each dataset. To ensure a fair comparison, all the networks in this study were implemented with the same input and loss functions as GegenGNN. Notice that by introducing a trainable GNN module, we relax the prior assumption of smooth evolution and achieve better performance.\nSeveral factors contribute to the success of GegenGNN on real-world datasets. First, its ability to capture spatio-temporal information enables it to effectively model the dynamics of graph signals. Second, the encoding-decoding structure allows for effective data representation and reconstruction. Lastly, the powerful learning module provided by the cascade of Gegenbauer graph convolutions enhances the model's performance. The experimental results demonstrate the superiority of GegenGNN in reconstructing time-varying graph signals in real-world scenarios where ideal conditions of smoothness are not assured. Appendix E provides a comparison of the convergence between GegenGNN and the baselines."}, {"title": "F. Ablation Studies", "content": "We have performed ablation studies to evaluate the performance of our architecture and illustrate how the inclusion of additional parameters improves its performance in comparison to the baseline architectures. Specifically, we examined the"}, {"title": "G. Limitations", "content": "Based on the experiments conducted, we observe that the Gegenbauer layers used in GegenGNN incur a comparable computational overhead to Chebyshev convolutions, outperforming both GCN and attention-based models in terms of efficiency when reconstructing time-varying signals. However, it is important to note that GegenGNN has a primary limitation related to hyperparameter tuning. This limitation arises from the introduction of the additional Gegenbauer parameter, \u03b1. While the inclusion of this parameter allows the network to search for richer orthogonal basis functions compared to the Chebyshev polynomial-based architecture proposed in [2], it also introduces an added burden during the hyperparameter optimization process due to the presence of extra parameters."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we proposed a new GNN architecture called GegenGNN, which utilizes cascaded Gegenbauer polynomial filters in its convolutional layers. In Section IV, we provided a formal introduction to GegenGNN and presented implementation details. GegenGNN incorporates a specialized loss function to capture the temporal relationship of time-varying graph signals. We apply our architecture to the task of reconstructing time-varying graph signals and evaluate its performance on four real-world datasets that deviate from conventional smoothness assumptions. Our experimental results demonstrate that GegenGNN outperforms other state-of-the-art methods from both the GSP and machine learning communities when it comes to recovering time-varying graph signals. This highlights GegenGNN's ability to extract high-order information from data using a cascade of Gegenbauer filters. The superior performance of our method on real-world datasets suggests its potential for addressing practical challenges such as missing data recovery in sensor networks or weather forecasting. To assess the contribution of the Gegenbauer parameter \u03b1 and the specialized temporal loss function in our architecture, we conducted several ablation studies. Our findings indicate that the additional components introduced by GegenGNN play a crucial role in the recovery of time-varying signals.\nExploring the potential of GegenGNN opens up numerous promising research avenues for graph-based signal forecasting, multimodal learning, and fusing information from diverse sources. Another possible research direction is exploring novel types of efficient graph filters that enhance the model performance without adding excessive computational complexity. Moreover, GegenGNN is a generalizable architecture that can be extended to different domains, including traditional graph machine learning tasks such as node and graph classification. Beyond its temporal analysis focus demonstrated in this work, GegenGNN can find practical applications in other fields, including recommender systems, computational biology, or temporal analysis."}]}