{"title": "LIGHTWEIGHT SAFETY CLASSIFICATION USING PRUNED\nLANGUAGE MODELS", "authors": ["Mason Sawtell", "Tula Masterman", "Jim Brown", "Sandi Besen"], "abstract": "In this paper, we introduce a novel technique for content safety and prompt injection classification for\nLarge Language Models. Our technique, Layer Enhanced Classification (LEC), trains a Penalized\nLogistic Regression (PLR) classifier on the hidden state of an LLM's optimal intermediate transformer\nlayer. By combining the computational efficiency of a streamlined PLR classifier with the sophisti-\ncated language understanding of an LLM, our approach delivers superior performance surpassing\nGPT-40 and special-purpose models fine-tuned for each task. We find that small general-purpose\nmodels (Qwen 2.5 Instruct sizes 0.5B, 1.5B, and 3B) and other transformer-based architectures like\nDeBERTa v3 are robust feature extractors allowing simple classifiers to be effectively trained on\nfewer than 100 high-quality examples. Importantly, the intermediate transformer layers of these\nmodels typically outperform the final layer across both classification tasks. Our results indicate that\na single general-purpose LLM can be used to classify content safety, detect prompt injections, and\nsimultaneously generate output tokens. Alternatively, these relatively small LLMs can be pruned to\nthe optimal intermediate layer and used exclusively as robust feature extractors. Since our results are\nconsistent on different transformer architectures, we infer that robust feature extraction is an inherent\ncapability of most, if not all, LLMs.", "sections": [{"title": "Introduction", "content": "Since the introduction of LLMs, a primary concern has been detecting inappropriate content in both the user's input and\nthe generated output of the LLM. Establishing language model guardrails is a critical requirement of responsible AI\npractices. A robust set of guardrails extends beyond the detection of hate speech and sexual content, also detecting when\nan LLM strays away from its intended purpose. There have been numerous troubling instances where chat bots have\nbeen coerced into responding to inappropriate requests and therefore produced damaging outputs. Existing solutions for\nidentifying inappropriate content range in complexity from traditional text classification methods to using an LLM to\nclassify the text. Our approach combines the computational efficiency of a simple machine learning based classifier\nwith the robust language understanding provided by an LLM for optimal performance.\nTwo primary concerns related to LLM use include content safety and prompt injection detection. Content safety involves\nidentifying inputs and outputs that are harmful, offensive, or otherwise inappropriate [10, 15, 34, 20]. Prompt injections\nare attempts by the user to manipulate the language model to behave in unintended ways or respond outside of ethical\nguidelines [16, 10]. This is important because prompt injections can compromise the ethical integrity of AI systems,\npotentially leading to vulnerabilities in AI-driven applications.\nOur contributions to content safety and prompt injection detection are:\n\u2022 We prove the intermediate hidden state between transformer layers are robust feature extractors. A penalized\nlogistic regression classifier with the same number of trainable parameters as the size of the hidden state, with\nas few as 769 parameters, achieves state-of-the-art performance surpassing GPT-40 \u2013 the presumed leader.\n\u2022 We show that for both content safety and prompt injection classification tasks there exists an optimal interme-\ndiate transformer layer that produces the necessary features. With only a minimal set of training examples,\nthe classifier generalizes extremely well to unseen examples. This is particularly valuable in cases where few\nhigh-quality examples are available, making our approach adaptable for a wide variety of custom classification\nscenarios.\n\u2022 We demonstrate that the hidden states of general-purpose LLMs (Qwen 2.5 0.5B Instruct, 1.5B, and 3B)\nand special-purpose models fine-tuned for content safety (Llama Guard 3 1B and 8B) or prompt injection\ndetection (DeBERTa v3 Base Prompt Injection v2), produce features achieving similar classification results.\nThis indicates LEC generalizes across model architectures and domains. Special-purpose models require even\nfewer examples to surpass GPT-40 level performance on both tasks.\n\u2022 Special-purpose content safety and prompt injection models when pruned and used as feature extractors\noutperform their non-pruned versions on their respective tasks."}, {"title": "Related Work", "content": "Previous work has demonstrated that combining transformer-based language models with penalized logistic regression\non model embeddings can create effective classifiers using a small number of examples (e.g., 10 to 1,000 examples)[5].\nBy using penalized logistic regression on the model embeddings, Buckmann et al. created classifiers that are robust to\nmodel size as well as quantization. This technique, called linear probing, has traditionally been used to investigate the\nhidden states of language models in order to understand what they represent[5, 1, 7]. These works have also shown that\nprompting has a significant impact on the performance of these classifiers[5].\nOur approach builds on this foundation by introducing layer-specific insights instead of only using the embeddings\nfrom the final layer of the model before the prediction head. We demonstrate that different layers are better suited for\ndifferent classification tasks. This layer-focused analysis combined with model pruning enables highly accurate and\nefficient classification models for responsible AI focused classification tasks."}, {"title": "Language Model Pruning", "content": "Model pruning techniques aim to reduce the computational requirements of language models by removing non-critical\ncomponents of the model to reduce its overall size while maintaining performance[23, 6]. Researchers have found that\nin many cases up to half the model layers can be removed and that earlier layers tend to play a more important role\nin knowledge understanding compared to later layers[12]. These pruned models are typically fine-tuned to recover\nlost performance on tasks like zero-shot classification, generation, and question-answering[23, 12]. Research by D K,\nFischer et al. shows that layer-based pruning can be used to reduce large language models by 30-50% while maintaining\nnearly all of their performance as text encoders[17]. They found that using the model as a text-encoder does not require\nfine-tuning to recover performance, and that in some cases model performance actually increases when pruned.\nOur approach expands on this by using the hidden state from intermediate layers as input to train a classification\nmodel. Unlike other approaches, we do not need to fine-tune the model to recover performance since we find that the\nintermediate layers provide better performance than the original model. Our focus is on identifying which of the pruned\nlayers create the best inputs to use for downstream classification tasks."}, {"title": "Intermediate Layers", "content": "Recent research analyzes the effectiveness of language models' intermediate layers. Each intermediate layer block\nencompasses 2 stages: Multi-Head Self-Attention and a Feedforward Network (MLP) with normalization occurring\nbefore and after the MLP step. After each hidden layer there is an updated contextually aware hidden state that is\nproduced.\nValeriani et al.'s work demonstrates that in the early layers of the transformer model, the data manifold expands\nbecoming highly dimensional, then contracts significantly in the intermediate layers, and continues to remain constant\nuntil the later layers of the model where it has a shallow second peak. Their experimentation suggests that the semantic\ninformation of the dataset is better expressed at the end of the first peak \u2013 and therefore in the intermediate layers [30].\nSkean et al. found that the intermediate layers of SSMs and transformer-based models can yield better performance on a\nrange of downstream tasks, including classification of embeddings [28]. They show that intermediate model layers have\nlower levels of prompt entropy, suggesting that these layers more efficiently eliminate redundancy during training [33].\nOur approach further supports the claims made in these papers and goes beyond to practically demonstrate how utilizing\nthe hidden state of the intermediate layers is beneficial for training a highly effective and computationally efficient\nclassification model across various tasks."}, {"title": "Model Explainability", "content": "Explainability for deep learning is a critical yet challenging field of research. Due to their complexity and scale, language\nmodels are particularly opaque. Model hallucinations and the generation of harmful content are just some of the many\nexamples that result from the lack of interpretability within a transformer model. Increased explainability offers more\nthan the ability for researchers to improve downstream tasks, it also offers the end user clarity and confidence in the\nmodel's response.\nThere are several techniques to improve transformer interpretability which Luo et al. categorizes into the two broad\ntopics of \"local analysis\" and \"global analysis\". In local analysis, researchers aim to understand how models generate"}, {"title": "Responsible AI Classification Tasks", "content": "Content safety and prompt injections are two of the most well-researched and high-priority use cases related to the\nresponsible use of Generative AI. Without effective mitigation strategies, these issues can compromise model integrity,\nuser trust, and overall system security. Numerous methods have been developed to address these classification tasks,\nwith public leaderboards available to benchmark their performance. Notable public leaderboards include the AI Secure\nLLM Safety Leaderboard on HuggingFace[2] and the Lakera PINT Benchmark for Prompt Injection [18].\nOne detection method for prompt injections is proposed by Hung et al. where they analyze patterns in the attention\nheads and introduce a concept called the \"distraction effect\". The \"distraction effect\" is where select attention heads shift\nfocus from the original instruction to the newly introduced instruction. They propose Attention Tracker, a training-free\ndetection method that monitors attention patterns on instruction to detect prompt injection attacks[14].\nAnother content safety classification approach is presented in the work of Mozes et al., where they show that LLM-based\nparameter-efficient fine-tuning (PEFT) can produce high-performance classifiers using small datasets (as few as 80\nexamples) across three domains: offensive dialogue, toxicity in online comments, and neutral responses to sensitive\ntopics. This method offers a cost-effective alternative to large-scale fine-tuning[24].\nOur approach distinguishes itself from these existing approaches as it prunes the LLM's hidden layers and uses just\nthe optimal number of parameters to be the most efficient yet performant classifier for the task. Additionally, it can be\nimplemented in two distinct ways: (1) integrated directly into the LLM's forward pass, similar to the work of Hung\net al. [14], or (2) as a separate component in the model pipeline, akin to the approach used by Mozes et al. [24]. By\noffering flexibility in deployment, our method provides a versatile and scalable solution for content safety and prompt\ninjection detection."}, {"title": "Experiments", "content": "Our experiments explore the effectiveness of training a classifier on the hidden states of intermediate transformer layers\nand identify which intermediate layer(s) provide the best performance for both content safety and prompt injection\nclassification. We compare our approach to baseline models using task-specific datasets.\nFor each task, we evaluate performance against two types of baseline models, GPT-40 and a task-specific, special-\npurpose model. We use GPT-4o as a baseline for both classification tasks since it is widely considered one of the most"}, {"title": "Experiment Implementation", "content": "For both general-purpose Qwen 2.5 Instruct models and special-purpose models, we prune individual layers and capture\nthe hidden state of the transformer layer to train a classification model. Our implementation uses the Python package\n13prune from D K et al.[17] to load the models from their respective HuggingFace repositories and remove the LM\nHead. We iterate through each layer of the model, pruning a single layer each time and capturing the hidden state at the\ntransformer layer. This allows us to understand the impact of individual layers on the task.\nAfter pruning, we train a PLR classifier with L2 regularization on the output vector of our pruned model. Our PLR\nclassifier uses the RidgeClassifier class from scikit-learn with a=10. All other settings are left to their default values.\nEach model was run on a single A100 GPU with 220GB of memory in an Azure Databricks environment. For our\nGPT-40 baseline we used an Azure OpenAI deployment with version \"2024-06-01\".\nWe used task-specific datasets, each containing 5,000 examples with 66% allocated to training and 33% to testing.\nWhile previous work suggests that our classifiers will only see small improvements after a few hundred examples\n[5], we randomly sampled 5,000 examples to ensure enough data diversity and minimize compute time. For each\nexperiment, we trained multiple classifiers on a random sample of our training set from sizes 5 to 3,000. Then, we\ncalculated the weighted F1-score on our 1,700 example test set for each unique training size, layer count, and model.\nTo establish the baseline models' performance we calculated the weighted F1-score on the 1,700 example test set."}, {"title": "Datasets", "content": "Content Safety: Our content safety dataset is designed for both binary and multi-class classification. For binary\nclassification content is either \"safe\" or \"unsafe\". For multi-class classification, content is either categorized as \"safe\" or\nassigned to a specific fine-grained category under the broader \"unsafe\" category. To create a balanced dataset of safe\nand unsafe content, we combine two existing datasets, the \"SALAD Data\" dataset from OpenSafetyLab to represent\nunsafe content and the \"LMSYS-Chat-1M\" dataset from LMSYS, to represent safe content[20, 34]. We randomly\nsample 2,500 records from each source to create a combined balanced dataset with 5,000 examples."}, {"title": "Results Summary", "content": "Our results indicate that for both content safety and prompt injection classification tasks, using the transformer layer's\nhidden states with PLR classifiers consistently outperforms the baseline models, GPT-40 and the special-purpose models.\nFurthermore, applying LEC to the special-purpose models outperforms the models' own baseline performance by\nidentifying the most task-relevant layer for the classification task. This ultimately results in a significant improvement in\nthe F1-score compared to the full model's performance on the same task. Overall, we find that LEC results in improved\nperformance across all evaluated tasks, models, and number of training examples, often achieving better performance\nthan the baselines in fewer than 100 examples.\nWe find that content safety and prompt injection classification are largely driven by local features that are represented\nearly on in the transformer network, allowing the middle layers of the model to perform better on the classification\ntasks than later layers. These middle layers tend to attain the highest F1-scores within the fewest training examples for\ncontent safety and prompt injection classification tasks. For both tasks, LEC enables the special-purpose models to\ngeneralize to new tasks in a similar domain using fewer than 100 training examples. We infer this capability because, to\nour knowledge, the base special-purpose models were not trained on the datasets used in this evaluation.\nIn each model, we observe that the performance appears to mimic a continuous right-skewed function that is concave\ndown, with one or two local maximums near 50%-75% of the original number of layers. This is consistent with Gromov\net al.'s assertion that the layers of each model are largely dependent on the previous layer [12]. These findings suggest\nthat the optimal layer for classification tasks is not the LM head or final encoding, but instead one of the intermediate\nlayers of the model. \nWe also find that intermediate transformer layers tend to show the largest improvement in F1 compared to the final\ntransformer layer when trained on fewer examples. This suggests that low-resource or low-data cases can especially\nbenefit from LEC. In each result summary, we show the performance of LEC on 3 selected layers per model. These\nlayers include the full model's layers, the best-performing layer, and the smallest layer that achieves similar performance\nto the full model.\nIn summary, LEC provides a more computationally efficient and better performing solution (higher F1-scores with few\ntraining examples) for these classification tasks compared to GPT-40 and the unmodified special-purpose models."}, {"title": "Content Safety Results", "content": "In this section, we present our results for both binary and multi-class classification on the content safety task. Of\nthe baseline models used for this task \u2013 Llama Guard 3 1B, Llama Guard 3 8B, and GPT-40 \u2014 GPT-40 consistently\nachieved the highest weighted F1-scores for both binary and multi-class content safety classification. However, all"}, {"title": "Prompt Injection Results", "content": "In this section, we present our results for the prompt injection classification task. We find that both general-purpose and\nspecial-purpose models trained using LEC consistently outperform all the baseline models. As can be seen in Figure"}, {"title": "Conclusion", "content": "In conclusion, our method, LEC, which uses the hidden state between intermediate transformer layers as robust feature\nextractors for content safety and prompt injection classification outperforms all other current methods tested including\nGPT-40. The classification model is easily trained using penalized logistic regression and only a small number of\ntraining examples are needed. Most importantly, our results demonstrate that high-performing content classification is\npossible without modifying the LLM's weights or modifying the input prompt in any way. This classification approach\nhas trivial computational complexity at inference time because the classifier contains the same number of parameters as\nthe size of the LLM's hidden state. At most a few thousand new parameters are needed for the classification.\nWe believe that such a lightweight approach allows guardrails to be efficiently established around an LLM's input and\noutput. This approach also unlocks the ability for other use case specific classifiers to be created. We are intrigued by\nthe possibility that content classification can be baked into the LLM inference code providing real-time monitoring of\nthe LLM's input and output as tokens are generated.\nOur results also show that tiny LLMs can be pruned and used only as robust feature extractors for computationally\nefficient text classifiers. These tiny pruned models may be run virtually anywhere depending on the use case complexity."}, {"title": "Limitations and Future Work", "content": "Limitations: These experiments did not fine-tune the baseline models on our datasets, we instead left them unmodified\nand focused on training our classifiers. We chose to use the static transformer layers to leave the possibility open that\nLEC could be integrated into the inference code during token generation. Fine-tuning standalone lightweight feature\nextractors may perform even better, but we did not explore this possibility.\nOur findings are highly task dependent. More work is needed to directly compare the general ability of our method in\nother unexplored classification domains. Other classification domains may require more robust models, but we limited\nour exploration to a single linear model.\nAdditionally, we were unable to retrieve a small fraction of results from GPT-40 since sensitive content was blocked by\nbuilt-in safety filters. Although this can affect our results since the blocked content is more likely to contain content\nlabeled 'unsafe', this accounted for less than 1% of our dataset in all cases. Regardless of GPT-40's performance on\nthese examples, our results are conclusive enough to show that our method outperforms it.\nFuture Work: LEC has numerous implications when it comes to the deployment of NLP classifiers. First, we show\nthat models with as little as 100 million parameters and fewer than 100 training examples can make accurate predictions\non a range of classification tasks. This speed and flexibility allows data scientists to test very specific use cases without\nsignificant investment in time, compute, or hardware. We also show that the hidden state of small, general-purpose\nmodels like Qwen 0.5B can be used to train a wide range of effective PLR classifiers. By using smaller models and\nconcentrating resources into one or a handful of LM deployments, organizations can drastically reduce the amount of\ncomputational power devoted to these tasks.\nOur findings also have potential implications in the field of NLP explainability. As language models grow more and\nmore complex, there is increasing interest in interpreting and understanding their outputs [35]. This is especially\nimportant for fields where automated decision-making can cause direct harm to people. Despite this, many frameworks\nfor understanding, visualizing, or explaining transformer-based models rely on local explainability through attention\nlayer activations on individual phrases [19, 31, 9, 29, 13]. In comparison, our approach uses a model that is explainable\nat both the local and global level. By utilizing techniques such as SHAP values [21], we can better understand which\ncomponents are most important for individual classifier predictions as well as for the model as a whole.\nOur results suggest that a general-purpose model can be adapted to classify content safety violations and prompt\ninjections while simultaneously generating output tokens. Applying LEC to a general-purpose model allows us to\nidentify which model layers are important for each of the classification tasks. We believe it is possible to take the\noutputs from each of these layers, run them through their associated classification model to generate the predictions\nand based on the results either continue generating output tokens or stop the output from generating if a violation has\noccurred. Alternatively, pruning a very small language model and using its relevant layers for classification would work\nwell and incredibly quickly, allowing for immediate identification of violations before sending the prompt inputs to a\nseparate LLM for generation."}, {"title": "Appendix", "content": "We observe that each model had large variations in performance, especially on a very small number of training examples.\nIn the prompt injection task, there is a sharp drop in performance with a training set of around 40, which can be\nobserved in Figures 12, 13. Interestingly, these changes were most present in the intermediate layers of each model,\nwith the fine-tuned model having the most variability overall. Because this drop in performance is present in each\nmodel and layer trained, our assumption is that the higher variability inherent in small datasets causes sharp changes\nin estimated performance. We perform 10-fold cross-validation on the DeBERTa model on the prompt injection task.\nFrom Figure 6, we observe that cross-validation appears to stabilize the performance across all layers of the model. We\nconclude that model validation metrics such as cross-validation help provide more reliable estimates of the performance\nof low-resource classifiers, especially when the number of examples is less than 100."}, {"title": "Layer Concatenation", "content": "Since the layer representation of text can be significantly different between layers as the number of layers between them\nincreases, we performed experiments on whether the model could benefit from using the encoding from multiple layers.\nIn the experiment, we evaluated Qwen 2.5 0.5B using our prompt injection dataset. Rather than using a single layer's\nencoding, we set the features for our Ridge Classifier to be the concatenation of all previous layers. We then plotted and\ncompared the concatenated performance to performance using single layers. From Figure 7 and Gromov's assertion\nthat model layers are largely dependent on the previous ones[12], we conclude that layer concatenation has little to no\neffect on model performance."}]}