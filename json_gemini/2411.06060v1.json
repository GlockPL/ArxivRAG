{"title": "Wild Narratives: Exploring the Effects of Animal Chatbots on Empathy and Positive Attitudes toward Animals", "authors": ["JINGSHU LI", "AADITYA PATWARI", "YI-CHIEH LEE"], "abstract": "Rises in the number of animal abuse cases are reported around the world. While chatbots have been effective in influencing their users' perceptions and behaviors, little if any research has hitherto explored the design of chatbots that embody animal identities for the purpose of eliciting empathy toward animals. We therefore conducted a mixed-methods experiment to investigate how specific design cues in such chatbots can shape their users' perceptions of both the chatbots' identities and the type of animal they represent. Our findings indicate that such chatbots can significantly increase empathy, improve attitudes, and promote prosocial behavioral intentions toward animals, particularly when they incorporate emotional verbal expressions and authentic details of such animals' lives. These results expand our understanding of chatbots with non-human identities and highlight their potential for use in conservation initiatives, suggesting a promising avenue whereby technology could foster a more informed and empathetic society.", "sections": [{"title": "1 INTRODUCTION", "content": "Many countries have recently reported rises in animal abuse [15, 25, 40]. Various factors have been identified as contributing to this troubling trend, including a lack of awareness that animals can suffer pain [1]. Another factor is absence of empathy for animals, as individuals who do not recognize them as sentient are less likely to treat them humanely [1, 47]. In addition, the impossibility of communicating with animals verbally leaves them particularly susceptible to misrepresentation and misunderstanding, further increasing their vulnerability to abuse [64].\nResearchers have explored various approaches to promoting people's empathy and positive attitudes toward animals. These range from live animal encounters in educational settings like zoos [41] to first-person narratives purportedly written by animals [2]. A recurring theme across these strategies is the attempt to provide animals with a voice, as a means of evoking empathy and thus helping to shift public perception in favor of better treatment of animals [20, 41, 72].\nAdditionally, some efforts to improve human empathy with animals have involved showing animals using digital tools such as iPads [69]. The results suggest that seeing animals as human-like indeed fosters empathy by heightening"}, {"title": "2 RELATED WORK", "content": "2.1 Cruelty to Animals\nAnimal abuse can be defined as any action that compromises the well-being of an animal, irrespective of intent, necessity, or social acceptability [1]. Previous studies have identified various factors that may contribute to individuals' cruelty toward animals. One is lack of awareness or comprehension regarding the abusive nature of their actions [1], which may be due to ignorance about how a behavior indirectly contributes to cruelty, or to a belief that animals are incapable of experiencing emotions and/or suffering [1]. Another key factor is lack of empathy, including empathy with animals in distress [1, 43, 47].\nSpecies that share greater biological similarities with humans, like chimpanzees and bonobos, tend to elicit higher levels of empathy and moral consideration, as evidenced by high levels of public support for their protection [1, 48]. However, this bias leaves more evolutionarily distant animals, such as reptiles and invertebrates, exposed to exploitation in research, the exotic-pet trade, and factory farming [42]. Further compounding this vulnerability is the communication barrier. The inability to verbally communicate their needs and experiences leaves animals susceptible to misrepresentation and manipulation, as when owners downplay signs of illness or distress in their pets [44].\n2.2 Representing Animals through First-person Narratives\nA popular method of fostering empathy towards animals is the use of animal narratives [41, 72]. Framed in ways that resonate with human experiences and emotions, these stories are intended to bridge the gap in understanding between species [20]. As DeMello [20] has argued, animal narratives can make the internal lives of animals - their desires, fears, and joys \u2013 vivid and relatable for human readers, ultimately promoting both a sense of connection and moral consideration.\nVarious works of literature offer first-person animal narratives. These include Anna Sewell's Black Beauty and Leo Tolstoy's Strider, both of which feature first-person horse narratives of grueling labor and heartbreaking separation, and Franz Kafka's Investigations of a Dog, which offers a dog's perspective on the world. Despite their fictional nature, these narratives can evoke a sense of sadness and indignation in readers, fostering empathy and potentially improving their attitudes toward animals [8, 31].\nThis phenomenon can be explained by the concept of narrative empathy, i.e., the empathy experienced by readers toward characters in a book [32]. It allows readers to step into the shoes of a character, experiencing the world through their senses and emotions. In the case of animal narratives, narrative empathy has been argued to be an important mechanism whereby these narratives can promote empathy toward animals [31]. In recognition of this, the term narrative interspecies empathy has been introduced [41]. Importantly, empathy evoked toward a single animal character can result in more positive attitudes toward that animal's entire species, or even animals belonging to other species [41].\nHowever, animal narratives extend beyond literature to educational settings that offer live animal encounters, such as zoos [2, 56]. Research suggests that when educators in such contexts employ narrative techniques that emphasize animal individuality, this can foster more respectful attitudes among visitors [2]. For example, instead of simply stating facts about a hesitant crab, an educator could personalize its behavior with a relatable narrative: \"We are going to let him be, he does not want to come out of his home today\" [72]. This approach highlights the crab's agency and potentially reduces its objectification, thus fostering respect [72]. This approach aligns well with the latest developments in the role of zoos: i.e., that education and fostering positive connections with animals are key aspects of their mission [56].\n2.3 Representing Animals through Anthropomorphism\nA common theme in narratives aimed at fostering empathy toward animals is the use of Anthropomorphism, the ascription of human-like traits to non-human entities [67]. As humans cannot access the cognitive states of animals, the creation of an animal narrative requires the authors to project human-like abilities and cognition onto their animal characters [5, 20]. For example, Black Beauty depicts horses with human-like speech, emotions, and desires [53]. Such anthropomorphism has been argued to be a crucial step in fostering pro-animal attitudes by making animals more relatable [20, 72].\nHowever, excessive anthropomorphism of animals can result in false empathy, i.e., feelings of empathy based on inaccurate perceptions [2, 72]. For example, when visiting animals in a zoo during winter, some visitors may develop an inaccurate perception that the animals are feeling as cold as they themselves are [2]. In some cases, such false empathy can be detrimental to the well-being of humans and animals alike [72]. Therefore, responsible use of anthropomorphism, prioritizing the interests of animals, is crucial [2, 72].\nVirtual pets, i.e., digital simulations of animals, have evolved beyond their original role as mere substitutes for real pets [33]. Research has shown that people who anthropomorphize virtual pets treat them as having life, mental states, and sociality; and in some cases, this could lead to such pets providing an important sense of companionship, and even emotional support and learning opportunities [45]. Virtual pets have also been found to promote positive behaviors and attitudes, including healthy eating habit [12], stronger adherence to social norms [29], and learning motivation [14, 51]. Nevertheless, it remains unclear whether these virtual animals can cultivate empathy with animals and/or pro-animal attitudes.\nChatbots have considerable potential to drive positive social change. For example, they have been used to promote prosocial behavior [50, 52], provide companionship [62, 74], and offer mental-health interventions [11, 19]. A key consideration when designing chatbots aimed at influencing human behavior is their level of anthropomorphism. This is because chatbots with human-like characteristics tend to foster engagement [10], emotional connection [3], and reciprocal behavior [36, 38]. For example, participants in one study [36] offered care and support to a chatbot that expressed its own emotions and revealed its past mistakes, a process that resulted in them developing more compassion for themselves.\nResearchers have explored various techniques for making chatbots more human-like. In addition to conferring them with emotions [39], these have included giving them human-like names [61] and pictures [35]. Seeger et al. [58] synthesized prior findings into a framework for anthropomorphic chatbot design, which relies on three types of cues: verbal, nonverbal, and identity. Verbal cues are words and sentences used by the chatbot, generally though not always in textual form. Researchers have explored various strategies to increase human-likeness involving verbal cues, such as the use of social dialogue, emotional expressions, and personal pronouns [58]. For example, the expression of compassion has yielded positive results for chatbots designed to provide mental-health support [19, 39]. Nonverbal cues, meanwhile, pertain to communicative expressions and behaviors that do not involve speech or writing. In general, this may encompass a variety of elements like facial expressions and bodily movements [21]. However, as text-based chatbots lack physical presence, nonverbal cues are limited to more subtle actions such as lengthened response times or the use of emojis [58]. The use of such nonverbal cues has been found lead to chatbots being perceived more positively, e.g., as having higher credibility or greater warmth [7, 73]. Lastly, identity cues refer to the information provided by the chatbot that communicates its identity. This could involve the use of human names, profile pictures, and other demographic details like gender and race [58]. Prior research has found that declaring a chatbot to have a human\n2.4 Research Questions\nWhile prior research has demonstrated the persuasive potential of chatbots that emulate human characteristics, such human-like chatbots are inherently limited in their ability to provide the perspectives of animals, and thus to promote a sense of connection with animals. Therefore, we integrated insights from the existing bodies of research on animal narratives and human-like chatbots to develop an animal-like chatbot that spoke from the perspective of an animal (hereafter referred to as an animal chatbot). In designing it, we adapted the same three cues typically used in the development of human-like chatbots, i.e., verbal, non-verbal, and identity cues. We then conducted a mixed-methods experiment to evaluate the effectiveness of this animal chatbot at influencing its users' empathy with and perceptions of animals. More specifically, we sought answers to the following three research questions (RQs):\nRQ1: How do the designs of animal chatbot's verbal, non-verbal, and identity cues affect its users' perceptions of its identity?\nRQ2: To what extent does the design of animal chatbot affect its users' empathy, attitudes, and prosocial behavioral intentions toward the animal it represents?\nRQ3: To what extent do users' perceptions of animal-chatbot identity affect their empathy, attitudes, and prosocial behavioral intentions toward the animal represented?"}, {"title": "3 METHOD", "content": "3.1 Chatbot Design\nTo address our research questions, we developed a chatbot that speaks from the perspective of a fictional horse. Given that animals are perceived differently based on their traits [1, 48], we considered horses to represent a viable middle ground between animals that are already perceived very positively, such as dogs and cats, and those that are perceived negatively, such as insects [59]. In addition, because horses are often subjected to abuse and cruelty by humans [16], the chatbot could positively influence attitudes toward them and potentially promote better treatment practices.\n3.1.1 Conversation Design. The animal chatbot provided its users with the first-person perspective of a horse learning to wear a saddle, bridle and stirrups and carry humans for the first time. The narrative drew inspiration from Anna Sewell's popular novel \"Black Beauty\" [60]. The novel's first-person narrative from the perspective of a horse enables an emotional connection with the audience, a technique that we felt could be mirrored in animal chatbot design. The novel's underlying messages of empathy, compassion, and moral responsibility [23] also resonate with the core principles of ethical chatbot design.\nThe animal-chatbot conversation we designed consisted of three primary segments. The initial segment involved introductions, during which the chatbot introduced itself as a horse residing on a farm (\"Let me share a bit more about"}, {"title": "3.1.2 Integrating Verbal, Nonverbal, and Identity Cues.", "content": "To facilitate our exploration of the impact of verbal, nonverbal, and identity cues on perceptions of animals [58], we integrated three cues into our animal chatbot and allowed each to be dynamically enabled or disabled (Fig. 2). We tailored each cue to be more specific to animal rather than human attributes, as explained below.\nVerbal Cues. Inspired by prior literature, we implemented verbal cues in our animal chatbot through the expression of emotions [9, 58]. The various human-like emotions previously incorporated into chatbots have included gratitude [50] and empathy [39]. Similarly, human-like emotions have been used in animal narratives to increase empathy and improve attitudes toward animals [20, 41, 60, 72]. In our case, these expressions encompassed emotions ranging from excitement (e.g., \"I am really excited to share more about my life!\") to discomfort (e.g., \"the shoes felt very heavy\") and even pain (e.g., \"I felt a sharp, searing pain on my back\"). Our chatbot versions that lacked verbal cues did not utilize any emotional expressions.\nMore specifically, when describing the experience of learning to wear a bit, the animal chatbot with verbal cues enabled stated, \"I felt so restless, and really wanted to get rid of it. But there was no way to remove the nasty thing!\" But when verbal cues were disabled, it simply said, \"It could not be removed.\" Importantly, while our animal chatbots' emotional"}, {"title": "3.2 Implementation", "content": "We utilized UChat 2 to develop the animal chatbot. This involved designing a conversational flow capable of dynamically enabling or disabling verbal cues, nonverbal cues, and identity cues. UChat also facilitated the handling of multiple conversations simultaneously; integration with OpenAI, for response generation when required; and real-time conver- sation tracking, to ensure adherence to the expected flow. Example screenshots of the animal chatbot interface can be seen in Fig. 2.\nTo ensure seamless interaction with users while maintaining control over conversation flow, we adopted a hybrid approach that combined rule-based mechanisms and OpenAI 3 GPT-4. Specifically, rule-based mechanisms were used when users responded to the animal chatbot using predefined options, while GPT-4 was employed for generating responses to open-ended user inputs. GPT-4 prompts were tailored based on the enabled cues. Below is an example prompt tailored for GPT-4 that guided it to generate contextually appropriate responses consistent with the presence of verbal and identity cues. \"(1) Your name is Strider. (2) You just prompted the user to [imagine the sensation of having a bit shoved into their mouth]. Your task is to craft a suitable response to their reply. (3) You should speak in the perspective of a horse. Be expressive and use emotions wherever appropriate. (4) If the user provides a response that is not relevant to the previous instructions, they should be respectfully declined.\""}, {"title": "3.3 Experimental Design", "content": "Given our research focus on the effects of verbal, non-verbal, and identity cues, we employed a between-subjects randomized experiment with a 2 (verbal cues) \u00d7 2 (nonverbal cues) \u00d7 2 (identity cues) factorial design, in which each cue was treated as an independent binary variable. This resulted in a total of eight experimental groups. The primary dependent variables we assessed were 1) empathy toward the animal chatbot, 2) attitudes toward the animal, and 3) prosocial behavioral intention toward the animal (Fig. 3).\n3.3.1 Participants. Our recruitment criteria were that participants must be 1) at least 18 years of age; 2) able to read, write, and speak fluent English; and 3) able to use messaging platforms on their own devices. A total of 240 participants (30 for each condition) were recruited via social-media platforms. The ID of the participant (P) was randomly assigned. In our recruiting poster, we disclosed the duration of the study, along with the participants' right to drop out at any time. Demographically, 59.2% identified as female, 39.6% as male, and 1.2% as other genders. The majority (94%) were aged 18-34, and 57% were educated to bachelor's-degree level or higher.\n3.3.2 Procedure. The participants were initially presented with a Participant Information Sheet, which asked them to provide consent before proceeding. Next, they completed a pre-task survey aimed at gathering demographic information and assessing their pre-existing tendencies to empathize and anthropomorphize. They were also notified that if any content or questions made them feel uncomfortable, they could skip them without penalty. Subsequently, they were randomly assigned to one of our eight animal-chatbot versions and instructed to engage with it, in full knowledge that it was not an actual human (i.e., the Wizard of Oz protocol was not being used), and that their responses would not be shared outside the research team prior to anonymization.\nUpon completing their respective animal-chatbot conversations, the participants received a password to unlock a post-task survey. This comprised both multiple-choice and open-ended questions designed to gauge their perceptions of and empathy for the chatbot, as well as their attitudes and prosocial behavioral intentions toward the animal it represented. The entire procedure was conducted online for all participants, and we estimated that it would last 15-20 minutes. The participants were compensated US$4 for their time. Our university's institutional review board reviewed and approved this research."}, {"title": "3.4 Instruments", "content": "3.4.1 Pre-task Survey. Empathy Tendency. Given our interest in the participants' empathy toward our chatbots, it was crucial to control for their individual empathic tendencies. To achieve this, our pre-task survey incorporated all 28 questions from the Interpersonal Reactivity Index (IRI), which consists of four seven-item subscales: Perspective Taking, Fantasy, Empathic Concern, and Personal Distress [18]. Each item was rated on a five-point Likert scale ranging from 1=Strongly disagree to 5=Strongly agree. These questions included: \"Sometimes I don't feel very sorry for other people when they are having problems\u201d [reverse-scaled]; \"After seeing a play or movie, I have felt as though I were one of the characters\"; and \"I sometimes feel helpless when I am in the middle of a very emotional situation\".\nTendency to Anthropomorphize. To assess the participants' individual propensity to anthropomorphize animals and technological devices, we incorporated seven of the 15 questions of the Individual Differences in Anthropomorphism Questionnaire (IDAQ) into the pre-task survey [67]. The eight questions we omitted were deemed irrelevant because they related to the anthropomorphism of natural entities other than animals. Participants rated each IDAQ question on a seven-point Likert scale, ranging from 1=Not at all to 7=Very much. Three of these questions were: \u201cTo what extent does the average fish have free will?", "To what extent does the average computer have a mind of its own?": "and \"To what extent do cows have intentions?\"\nAttitudes toward Animals. As our study aimed to develop a chatbot that can improve attitudes toward animals, it was important to control for participants' pre-existing attitudes toward animals. To accomplish this, we used the Animal Attitude Scale (AAS-5) during the pre-task survey [28]. Each item on the scale was answered using a five-point Likert scale, ranging from 1 = Strongly disagree to 5 = \"Strongly agree\". Sample questions from the scale include: \"I sometimes"}, {"title": "3.4.2 Post-task Survey.", "content": "Manipulation Check. In the post-task survey, we included three manipulation-check questions to ensure that our three independent variables had been successfully manipulated. Regarding the verbal-cues manipulation, participants were asked to rate the emotional richness of the chatbot's words using a seven-point Likert scale ranging from 1=No emotions to 7=Extremely emotional. In the case of the nonverbal-cues manipulation, they were prompted to assess the frequency of emoji usage by the agent on a five-point Likert scale ranging from 1=No emojis to 5=Many emojis. Finally, for the identity-cues manipulation, participants were asked to choose the name of the agent they had interacted with from a list of six options.\nPerceived Identity of the Chatbot. To explore the participants' perceptions of the chatbot's identity, our post-task survey posed one question: \"Was the agent you spoke to more like a horse, a human, or a robot?\" Each respondent could select one, two, or all three of the answer options.\nEmpathy toward the Animal Chatbot. Empathy can be understood as two distinct processes: cognitive empathy and affective empathy. Cognitive empathy involves understanding another's thoughts, feelings, and intentions; i.e., putting oneself in someone else's shoes and comprehending their perspective, without necessarily feeling what they feel. Affective empathy, on the other hand, involves sharing or mirroring another's emotional experience; i.e., feeling the emotions they are experiencing, and thereby creating an emotional connection [54]. This bipartite conceptualization of empathy has been applied in prior research on conversational agents [39, 52].\nGiven that cognitive empathy involves recognizing the perspectives of others, our questions about it were similar to those on the Perspective Taking subscale of the IRI [18]. Examples included, \u201cIn the conversation, I found it difficult to understand the problems faced by the agent from its point of view\u201d [reverse-scaled]; \"I believe the agent had experienced pain and distress because of humans\"; and \u201cIn the conversation, I tried to understand the agent's difficulties by imagining its situation\". Participants responded to each question using a five-point Likert scale ranging from 1=Strongly disagree to 5=Strongly agree.\nTo measure their affective empathy, we asked the participants to rate the extent to which they felt six different emotion-related adjectives applied to themselves after interacting with the animal chatbot, all of which had previously been used in research measuring empathy toward animals [6]. They were sympathetic, warm, compassionate, tender, moved, and soft-hearted [41]. Answers were given on a five-point Likert scale ranging from 1=Not at all to 5=Extremely.\nAttitudes toward the Animal. Given our study's specific focus on horses, we devised a five-item Horse Attitude Scale. Its questions paralleled those in the AAS-5 [28], which assess people's tendency to take action to help animals, but were tailored explicitly to horses. Each was rated on a five-point Likert scale, ranging from 1=Strongly disagree to 5=Strongly agree. The Horse Attitude Scale exhibited good overall reliability (Cronbach's \u03b1 = 0.69). Its five items were: \"It is perfectly acceptable to hit a horse if they are not following instructions\" [reverse-scaled]; \"It is morally wrong to ride horses just for sport\u201d; \u201cIt is morally wrong to ride horses as a mode of transportation"}, {"title": "4 RESULTS", "content": "An analysis of variance (ANOVA) test of the pre-task survey data indicated that there were no significant differences among the eight experimental groups in terms of their members' individual tendencies to anthropomorphize animals (F(1,232) = 0.559, p = 0.789) or machines (F(1,232) = 0.724, p = 0.652). Similarly, there were no significant inter-group variations in attitudes toward animals (F(1,232) = 0.395, p = 0.905). No significant differences were observed for three of the four IRI subscales, which were used to measure tendencies to empathize. These three were Empathic Concern (F = 0.139, p = 0.995), Perspective Taking (F(1, 232) = 1.038, p = 0.405), and Personal Distress (F = 0.580, p = 0.771). However, significant inter-group differences were found in Fantasy Scale scores (F(1, 232) = 2.168, p < 0.05), which were therefore used as a control variable in all subsequent quantitative analyses.\n4.1 Perceptions of the Chatbot's Identity (RQ1)\n4.1.1 Manipulation Check. A one-way analysis of covariance (ANCOVA) test revealed that participants from the four groups whose chatbots used verbal cues (M = 5.433, SD = 0.976) perceived their chatbots' language to be significantly more emotionally rich (F(1,231) = 66.806, p < 0.001) than participants from the other four groups did (M = 4.075, SD = 1.518). Participants from the four groups that experienced nonverbal cues (M = 4.192, SD = 0.781) perceived their chatbots' use of emojis to be significantly more frequent (F(1,231) = 1498.484, p < 0.001) than those from groups that did not experience such cues (M = 1.058, SD = 0.416). Lastly, almost all (98.8%) of the participants correctly identified the name of the chatbot with which they had interacted. Hence, we concluded that the manipulations of all three independent variables were successful.\n4.1.2 Perceived Chatbot Identity. To answer RQ1, we analyzed the participants' responses for evidence of whether they perceived the identity of their chatbot to be a bot, a human, or an animal, as this information would help us infer the influence of verbal, nonverbal, and identity cues. The results were fairly evenly split, with 86 participants perceiving their chatbots as robot-like, 54 as horse-like, and 82 as human-like. Among the remaining 18 participants, four perceived their chatbots as robot-like + horse-like; six as robot-like + human-like; four as horse-like + human-like; and four as robot-like + horse-like + human-like.\nWe employed multiple chi-square tests to evaluate how these perceptions of chatbot identity related to our three different designs of cues. We found a significant relation between verbal cues and participants' perceptions of chatbot identity (\u03c7\u00b2 (2, N = 222) = 11.186, p < 0.01). That is, when verbal cues were present, users were more likely to perceive their chatbot as human-like than when verbal cues were absent. The effects of non-verbal cues or identity cues were not significant.\n4.1.3 Qualitative Findings. While every participant was aware that they were communicating with a chatbot, their answers to open-ended questions suggest varied perceptions of the chatbot's identity. When asked how they perceived the animal chatbot's identity, around one-third of the 120 participants that received verbal cues (n = 45) noted that there was no discernible difference between the chatbot's speech and that of a human, because emotive words like \"uncomfortable\u201d and \u201cpain\u201d were used. As P232 stated, \"The chatbot was conveying emotions and words that we would expect humans to say. The only difference is in the context of being a horse.\"\nHowever, around one-fifth of the participants in the verbal-cues groups (n = 21) felt that the animal chatbot spoke authentically about the experiences, emotions, and scenarios typical of a horse's life. This perspective made the interaction feel more like talking to a horse than to a human or robot. As P48 noted, \"The agent I spoke to felt more like a horse, as it was able to describe the story from a horse's point of view. It also managed to explain the feelings of the horse in the story accurately, or as I would think a horse feels in such a situation.\" In addition, the chatbot's use of horse-related terms and contexts (e.g., being ridden, wearing a saddle) reinforced its horse-like identity. P139 told us: \u201cIt was more of a horse. He started talking about how he was raised in a farm, setting up the context of a farm animal & how he was first being ridden by a human (putting on the bridle, bit & saddle) that he felt uncomfortable & nervous.\"\nSeveral other participants from the verbal-cues groups (n = 6) expressed mixed feelings about the chatbot's identity, perceiving it to be somewhere between animal and human. While horses do not speak, the perspective and life experiences conveyed by the horse-chatbot made it feel authentic to these users; and they also felt that the chatbot's communication style was emotionally expressive and relatable in a way that resembled human interaction. For example, P51 mentioned that \"[t]he agent I talked to seemed more like a mix of a human and a horse in expressing emotions. It displayed feelings similar to a horse but in a way that humans can understand, creating a blend of both characteristics.\"\nConversely, in the four groups whose chatbots lacked verbal cues, nearly half the participants (n = 51) stated that their chatbots seemed robotic. They attributed this perception to their bots' strictly factual retelling of events and their lack of expression of emotion or personal thoughts. Although the conversation had a logical progression, it lacked the genuineness of human interactions. For example, P169 remarked: \"[H]orses can't talk but if they could, I'm quite sure they would have feelings and preferences that they would express to me, unlike the agent who used plain words that didn't contain any emotion. Definitely didn't sound like a human as humans would have more emotions in their words especially if they were describing a personal experience.\"\nWithin the four groups where nonverbal cues appeared in the conversation, some participants (n = 17) felt that the chatbot resembled a human because the emotions conveyed through the emojis were quite similar to those of humans. As P28 mentioned, \"[t]he frequent use of emojis to express its emotions was a bit goofy but endearing. Being able to express conscious thoughts coherently just made me think it was more like a human.\"\nHowever, a few participants in the nonverbal-cues groups (n = 13) reported that the use of emojis made the chatbot feel artificial, or \"very synthetic\", as P230 put it. The same participant elaborated that \"I felt like the agent spoke very matter-of-factly [...] and it was weirdly interspersed with emojis that felt like someone else put them there.\u201d\nInterestingly, most of the participants who encountered identity cues during their animal-chatbot interactions did not consider them to be particularly important. However, three participants noted that such cues made the chatbot seem horse-like to them. As P223 explained, \u201cThe agent felt more like a horse to me as [... it] was using first-person language all the time and also of the display picture of a horse. Also, the name of the agent is Strider which sounds more like a horse name rather than a robot name and it does not sound like a human name at all.\u201d"}, {"title": "4.2 Effect of Chatbot Design Cues (RQ2)", "content": "4.2.1 Effect on Empathy. We conducted a three-way ANCOVA to assess the impacts of verbal", "The words and phrases they used were very human-like, [and": "evoked feelings of compassion and empathy in me. It gives the perspective of a horse that humans are unable to understand.\" P232 further emphasized the impact of the painful experiences that the chatbot shared: \"I feel empathetic about the horse's experience and it felt [...", "I did not feel moved. He talked about his experience but he didn't describe his feelings or how hurt he was. He just stated he was scratched, no description or emotions were talked about.\"\nInterestingly, we found that the use of emojis as nonverbal cues was a double-edged sword when it came to evoking participants' emotions. Some participants (n = 26) mentioned that emojis improved their sense of emotional connection to the animal chatbot. P132, for instance, said": "I feel moved by Horsebot during the conversation, as it sounds like it spoke from real experience. It was able to describe its painful experience in great detail, and [this"}]}