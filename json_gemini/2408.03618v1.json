{"title": "A Logical Fallacy-Informed Framework for Argument Generation", "authors": ["Luca Mouchel", "Debjit Paul", "Shaobo Cui", "Robert West", "Antoine Bosselut", "Boi Faltings"], "abstract": "Despite the remarkable performance of Large Language Models (LLMs), they still struggle with generating logically sound arguments, resulting in potential risks such as spreading misinformation. An important factor contributing to LLMs' suboptimal performance in generating coherent arguments is their oversight of logical fallacies. To address this issue, we introduce FIPO, a fallacy-informed framework that leverages preference optimization methods to steer LLMs toward logically sound arguments. FIPO includes a classification loss, to capture the fine-grained information on fallacy categories. Our results on argumentation datasets show that our method reduces the fallacy errors by up to 17.5%. Furthermore, our human evaluation results indicate that the quality of the generated arguments by our method significantly outperforms the fine-tuned baselines, as well as prior preference optimization methods, such as DPO. These findings highlight the importance of ensuring models are aware of logical fallacies for effective argument generation.", "sections": [{"title": "1 Introduction", "content": "Argument generation is crucial in daily life and has many online and offline applications. For instance, legislative bodies often use persuasive arguments to ensure votes for bills to pass. However, generating logically coherent arguments is a challenging task and requires an appropriate combination of reliable evidence and effective logical reasoning (Walton et al., 2008; Wachsmuth et al., 2017). Humans are prone to misconstruing logical argumentation in the real world and often unknowingly adopt flawed reasoning in discussions and decision-making processes (Evans, 2002). Similar to humans, Large Language Models (LLMs) have demonstrated limitations in their logical reasoning capabilities, suffering from logical inconsistencies (Chen et al., 2023b; Jin et al., 2022; Sourati et al., 2023) and producing logically incorrect arguments (Chen et al., 2023a).\nIn this work, we hypothesize that LLMs generate logically incorrect arguments because they lack an understanding of logical fallacies. A logical fallacy is an error in reasoning that undermines the validity of an argument (Tindale, 2007). For example, \"I've never had the flu because I take my vitamins every day.\" is an instance of a false causality fallacy. In our pioneer study, we generate 100 arguments on different topics with ChatGPT and find that 21% of the arguments contain fallacies. We observe that several types of logical fallacy arguments, including false causality, faulty generalization or circular reasoning are commonly generated by different LLMs (such as Llama-2 or Mistral).\nOur study explores the relationship between logical fallacy understanding and argument generation. We view models generating logically coherent arguments as a logical alignment problem, i.e., aligning the model responses (arguments) to the given topic and stance. Recent methods, such as Reinforce-"}, {"title": "2 Related Work", "content": "Logical Fallacies. Logical fallacies are errors in reasoning that can undermine the validity of an argument (Tindale, 2007). In argumentative discourse, identifying fallacies is crucial to measure the quality of argumentation (Wachsmuth et al., 2017; Nakpih and Santini, 2020). In particular, identifying fallacies can be useful for disinformation detection systems and critical thinking tools. Prior works have shown that LLMs struggle to classify logical fallacies, with the F1 scores being around 66% and merely reaching 27% during transfer learning (Jin et al., 2022; Sourati et al., 2023). More recently, however, Li et al. (2024) demonstrated GPT-4's ability to identify and classify fallacies, achieving over 86% of accuracy in both tasks. Nevertheless, previous works have not explored how a nuanced understanding of logical fallacies might influence argument generation.\nArgument Generation. Argument generation is an important task in natural language processing that requires generating coherent and persuasive arguments on a given topic. Existing argument generation frameworks have made significant strides: Hua and Wang (2018) introduced a generator that generates arguments from key phrases, followed by a separate decoder to produce the final argument text. Hua et al. (2019) developed Candela, a style-controlling counter-argument generation framework. Schiller et al. (2021) presented Arg-CTRL a model that uses control codes for topic, stance, and aspect in sentence-level argument generation. More recently, Saha and Srihari (2023) introduced an argument generator for factual arguments across a limited set of topics. Despite these advances, no study has yet addressed generating arguments from the lens of logical fallacies.\nData Generation and Automatic Evaluation with LLMs. The employment of LLMs like ChatGPT in data generation is supported by their proven effectiveness in a broad spectrum of text generation tasks, including the creation of instructional and relational datasets (Peng et al., 2023; Sun et al., 2023; Wang et al., 2023; Shao et al., 2023; Chia et al., 2022). Notably, Schick and Sch\u00fctze (2021) have demonstrated the utility of LLMs in producing datasets that significantly enhance the training of smaller models. With respect to evaluating automatically generated text, Liu et al. (2023) show that traditional reference-based metrics such"}, {"title": "3 Problem Formulation", "content": "In this work, we address the argument generation task, where a model needs to generate a logically correct argument given a topic and a stance. We denote a dataset as $\\mathcal{D} = \\{(x^{(i)}, y_w^{(i)})\\}_{i=1}^N$, in which x is the input topic and the stance (either supporting or countering), and $y_w$ is the argument. N is the number of instances. One naive approach to address the problem of logical argument generation is prompting LLMs. We use GPT-3.5, Llama-2 (7B), and Mistral (7B) in the zero-shot setting on a set of 100 topics. We also implement a Retrieval Augmented Generation (RAG) model on top of Llama-2 using the wiki-dpr database (Karpukhin et al., 2020). A short description and example are provided in Appendix C.2.\nObservations. In Table 1, we report the performance of all the models in argument generation. We use GPT-4 to assess the logical fallacies of the generated arguments. Among the models evaluated, ChatGPT demonstrated the best performance; however, it still generates arguments containing logical fallacy errors in 21% of the cases. Further, we also observe that open-source models generate higher rates of fallacious arguments. The detailed distributions of the fallacy types across the different approaches in the zero-shot setting are discussed in Table 5."}, {"title": "4 Methodology", "content": "To address the problem of generating fallacy-free and logically sound arguments, we propose using preference learning methods to generate arguments that are logically aligned with the given topic and stance. This involves making models aware of various logical fallacies and teaching them to generate logically correct arguments, by rewarding valid arguments and rejecting dispreferred samples.\nThe process of preference learning typically involves three main steps: (i) supervised fine-tuning (SFT) step (Section 4.1), (ii) preference data collection step (Section 4.2), (iii) reinforcement learning step (Section 4.3). In Section 4.4, we introduce our method (FIPO), which introduces fine-grained information about fallacies in the alignment process. A comprehensive overview of the methodology is presented in Figure 2. To justify our current design and methodology, we perform an ablation study with different training approaches, described in Appendix E. The results show that our design achieves the best results."}, {"title": "4.1 Supervised Fine-Tuning", "content": "We begin by fine-tuning a pre-trained language model ($\\pi_{\\beta}$) on the argumentation dataset $\\mathcal{D}$ and use maximum likelihood estimation to obtain a model $\\pi_{SFT}$.\n$\\mathcal{L}_{SFT}(\\pi_{\\beta}) = -E_{(x, y_w) \\sim \\mathcal{D}}[log \\pi_{\\beta}(y_w | x)]$\t\t\t\t\t\t\t\t\t(1)"}, {"title": "4.2 Preference Data Collection", "content": "Conventionally, after the SFT phase, $\\pi_{SFT}$ is prompted with input x to produce pairs of outputs ($y_1$, $y_2$) ~ $\\pi_{SFT}(y|x)$, which are then presented to human annotators to rank as preferred and dispreferred responses. However, our objective is to reduce logical fallacy errors in the model's outputs. Therefore, it is essential to include a diverse range of fallacy types in the preference data, as these may not be sufficiently represented in the model's outputs.\nDrawing inspirations from the history of logic and logical fallacies (Aristotle, 2006; Russell, 2013), we define 13 categories of logical fallacy errors (see Figure 3). However, there are two key challenges: (i) determining the appropriate distribution of logical fallacy errors in the preference data, and (ii) automatically collecting such fallacy arguments.\nTo address the first concern, we follow the LOGIC dataset Jin et al. (2022), which was carefully curated through extensive web crawling and data collection from diverse online sources. This dataset reflects the distribution of fallacies in real-world scenarios, providing a realistic foundation for mitigating fallacies in everyday argumentative discourse.\nFor the second concern, we collect fallacy arguments using ChatGPT, following the distribution of fallacies in (Jin et al., 2022). To ensure arguments generated by ChatGPT are indeed fallacies, we provide a definition of the specific fallacy being generated as well as examples of that fallacy type. To populate our preference dataset and ensure it spans across the most types of fallacies, We generate four fallacious arguments with different fallacy types for each instance $x^{(i)}$. To emphasize certain fallacy types more than others, we sample the fallacy types to generate according to the LOGIC dataset (Jin et al. (2022), Figure 3).\nThe original dataset $\\mathcal{D}$ is now augmented with fallacies, denoted as $y_l^{(i)}$, and we define the training dataset as $\\mathcal{D}' = \\{(x^{(i)}, y_w^{(i)}, y_l^{(i)}, k^{(i)})\\}_{i=1}^{M}$, where we have M pairs of preferred ($y_w$) and dispreferred ($y_l$) samples, with $k^{(i)}$ being the fallacy type of the dispreferred argument $y_l^{(i)}$. The test set is not augmented with fallacies, as we use only the topics and stance $x^{(i)}$ at inference time to evaluate the quality and logical soundness of the arguments generated with respect to the topic and stance. More details on the generations and our prompt designs for Chat-"}, {"title": "4.3 Reinforcement Learning Phase", "content": "Following the preference data collection phase, we refine and optimize the SFT policy ($\\pi_{SFT}$) based on the trained reward model (explicit feedback) or the preference data (implicit feedback). In this work, we use four different learning algorithms: PPO, DPO, KTO, and CPO to mitigate the logical fallacy errors. Among these algorithms only PPO requires explicit feedback from a trained reward model. For the other methods, we apply the preference optimization methods directly by using $\\pi_{SFT}$ as a reference model, and the preference data $\\mathcal{D}'$.\nExplicit Reward Modelling. We use the dataset $\\mathcal{D}'$ to train the Electra model (Clark et al., 2020) to learn to predict reward values.\nImplicit Reward Modeling. Methods like DPO, KTO, and CPO employ contrastive loss to derive implicit rewards from preference datasets. Note that CPO (Xu et al., 2024) is a reference-free method that does not require a reference policy."}, {"title": "4.4 Fallacy-Informed Preference Optimization (FIPO)", "content": "Despite the reinforcement learning fine-tuning, models persistently generate specific types of logical fallacies, particularly faulty generalization and false causality arguments. This can be attributed to the fact that the models do not explicitly learn about the fallacy types. Hence, we propose a fallacy-informed preference optimization method. This is achieved by attaching a classification head on top of the generative model, which calculates a weighted cross-entropy loss for the preferred and dispreferred samples. Recall $\\mathcal{D}' = \\{(x^{(i)}, y_w^{(i)}, y_l^{(i)}, k^{(i)})\\}$ where k \u2208 [1,13] is the fallacy type. We also label the preferred samples $y_w$ as class 0, 'Not a Fallacy'. Secondly, after a forward pass through the language model, we extract the hidden state from the last layer, denoted as $\\pi_{\\theta}(y|x)L$, where L represents the number of layers in the base models. This hidden state is then fed into the classification head. The resulting output defines the probability of each fallacy type k:\n$p_k(\\pi_{\\theta}(y|x)) = Softmax(W \\pi_{\\theta}(y|x) + b)_k$ (2)\nwhere W is the linear layer's weight matrix, and b is the corresponding bias term. To avoid penalizing the model equally for misclassifying different types of fallacies, we propose to guide the model to prioritize the most frequent fallacy types. This approach ensures that the model accurately identifies common fallacies, aligning its learning process with the real-world distribution (Figure 3). Hence, we define the weight $w_k$ for each fallacy type k as its frequency in $\\mathcal{D}'$:\n$w_k = \\frac{1}{|\\mathcal{D}'|} \\sum_{i=1}^{|\\mathcal{D}'|} 1_{k^{(i)}=k}$ (3)\nwhere 1 is the indicator function. The Fallacy-Informed loss $\\mathcal{L}_{FI}$ is defined as a weighted cross-entropy loss:\n$\\mathcal{L}_{FI} = -E_{(x, y_w, y_l, k) \\sim \\mathcal{D}'}[w_0 log p_0(\\pi_{\\theta}(y_w|x)) + w_k log p_k(\\pi_{\\theta}(y_l|x))]$ (4)\nwhere $w_0$ is the minimum frequency of all of these fallacy types, which is designed to let the policy focus more on the fallacies in the dispreferred samples rather than the non-fallacy samples during the preference optimization process.\nThe resulting loss function, termed Fallacy Informed Preference Optimization loss, combines the loss from the preference optimization with our classification loss ($\\mathcal{L}_{FI}$). In our work, CPO (Xu et al., 2024) is the method with which we combine our loss. The resulting loss is:\n$\\mathcal{L}_{FIPO} = \\mathcal{L}_{CPO} + \\lambda \\mathcal{L}_{FI}$ (5)\nwhere $\\lambda$ is a weighting parameter to adjust the fallacy-informed loss with respect to the preference optimization loss. A more detailed description of $\\mathcal{L}_{FIPO}$ is described in Appendix F."}, {"title": "5 Experimental Setup", "content": "We denote $\\pi_{SFT}$ and $\\pi_{\\theta}$ as the policies obtained after the SFT phase and the aligned policy respectively. $\\pi_{\\theta}$ is aligned using method $\\theta$ which is one of PPO (Schulman et al., 2017), DPO (Rafailov et al., 2023), CPO (Xu et al., 2024), KTO (Ethayarajh et al., 2024) and FIPO."}, {"title": "5.1 Datasets", "content": "We evaluate argument generation based on the EXPLAGRAPHS dataset (Saha et al., 2021) where samples contain a Topic, a Stance, and an Argument, spanning across a wide range of topics such as: Cannabis should be legalized or The government controls people's money. We augment this dataset using ChatGPT by generating equivalent arguments in the form of fallacies using the distribution of the LOGIC dataset (Jin et al., 2022), as described in Section 4.2 and depicted in Figure 2."}, {"title": "5.2 Settings", "content": "Base Models. We use Llama-2 (7B) (Touvron et al., 2023) and Mistral (7B) (Jiang et al., 2023) as our base models. For each alignment method, we leverage Low-Rank Adaptation (LoRA) (Hu et al., 2021), which is a Parameter Efficient Fine-Tuning method (Xu et al., 2023) that decomposes a large matrix into two smaller low-rank matrices in the attention layers. This drastically reduces the number of parameters that need to be fine-tuned. We reduce"}, {"title": "Hyperparameter Selection for FIPO.", "content": "To optimize our custom loss function, defined in Equation 5, we conduct a series of experiments manipulating the hyperparameter $\\lambda$ (Equation 5) and the weights for the cross-entropy loss (Equation 3). Our initial step involved tuning the weights for the loss function. Given that our dataset consists of n pairs of preferred and dispreferred arguments as logical fallacies, it is crucial to differentiate different fallacy classes. It is thus more suitable to set the weights for each fallacy type as their frequency in the dataset, given by Equation 3 and the weight for the preferred responses as little as possible, which we set as the minimum of all the fallacy frequencies. We also evaluate different settings of $\\lambda$, testing values of 0.1, 0.3, and 0.6. Our findings indicate that a higher $\\lambda$ effectively reduced the number of fallacies produced by the policies but adversely impacted the argument quality (win-rate). Conversely, a $\\lambda$ of 0.1 had minimal impact on improving the fallacy-rate. After assessing the trade-offs, we determined that a $\\lambda$ value of 0.3 provided the optimal balance between minimizing fallacies and maintaining a reasonable win-rate."}, {"title": "5.3 Evaluation", "content": "In this work, we use two metrics to evaluate argument quality, the win-rate and the fallacy-rate. The win-rate is the proportion of instances where one option is superior or more successful compared to another option in a given set of comparisons. We include 3 options for the win-rate: win, tie, lose. The fallacy-rate is the proportion of fallacies detected in the generated arguments.\nHuman Evaluation. We conduct a human evaluation to validate the relevance and quality of the generated arguments. We randomly select 200 topics and stances (either supporting or opposing), and a pair of arguments generated by different methods. More concretely, human annotators are requested to perform a comparative study by determining which argument is superior or whether both arguments are equally good or equally bad. Despite the subjective nature of this task, as certain arguments may appeal differently to different individuals, we provide instructions to annotators including selecting the"}, {"title": "6 Experimental Results", "content": "As outlined in Section 5.3, our evaluation of the generated arguments focuses on two primary aspects: (i) pairwise comparison of argument quality between the reference policy $\\pi_{SFT}$ and the aligned policies $\\pi_{\\theta}$, detailed in Section 6.1; and (ii) the analysis of fallacy-rates and types across different preference optimization methods in Section 6.2."}, {"title": "6.1 Pairwise Comparison of Different Preference Optimization Methods", "content": "Each argument sample undergoes a manual and automatic (GPT-4-based) comparative evaluation, whose results are shown in Figure 4 and Table 6. For the human evaluation, each sample receives three assessments. Samples lacking majority consensus among annotators are excluded from further analysis. From the human annotated win-rates, depicted in Figure 4, we address the following research questions:\nRQ1: Are preference optimization methods better than SFT? The aligned policies outperform SFT by several percentage points in terms of win-rates, indicating an improvement in overall argument quality. DPO and CPO are the only methods achieving over 40% win-rates, demonstrating a better ability to generate arguments. $\\pi_{FIPO}$ stands out"}, {"title": "6.2 Results of Fallacy-Rates and Types", "content": "Evaluating text segments to identify logical fallacies poses inherent challenges for humans. Detecting such fallacies demands an extensive understanding of logical principles and argumentative structures. Without a robust grasp of logical fallacies, differentiating between valid and flawed reasoning becomes difficult. Additionally, personal biases and preconceptions can also cloud judgment, leading to overlooked fallacies or biased interpretations of arguments.\nRQ3: Do preference optimization methods mitigate logical fallacy errors? The aligned policies produce fewer fallacies compared to the SFT baseline. Specifically, every alignment method outperforms SFT by several percentage points for Llama-2. For both Llama-2 and Mistral, DPO is the method which improves the least, and even produces more fallacies than SFT with Mistral, having a lower frequency of Not a fallacy arguments. The other methods (PPO, CPO and KTO) consistently outperform SFT and produce fewer fallacies.\nRQ4: Does FIPO further reduce logical fallacy errors? The least fallacy producing policy is FIPO, achieving 83% of Not a fallacy arguments with Llama-2 and 80.5% with Mistral, outperforming the previous best score by 9% for Llama-2 (PPO: 74%) and 8.25% for Mistral (KTO: 72.25%), (Figure 5, Table 3). More specifically, FIPO, based on CPO, beats CPO by 11% and 10.5% for Llama-2 and Mistral, respectively. This highlights the utility of the classification loss, indicating the policies have a better understanding of logical fallacies compared to regular preference optimization.\nRQ5: What is the most observed fallacy type? The most frequently observed fallacy produced across all of the policies is Faulty Generalization."}, {"title": "6.3 Out-of-Domain Analysis", "content": "To showcase the effectiveness of alignment methods in argument generation, we evaluate the fallacy-rate and win-rate of 100 arguments generated with each policy on a subset of (Cabrio and Villata, 2012). Topics include Social security should be privatized or Airport security profiling is a good idea. The backbone model for these alignment methods is Llama-2. The results are detailed in Table 4, and we find that $\\pi_{FIPO}$ is the second policy to produce the least amount of fallacies (55%), behind $\\pi_{KTO}$ (56%). Although it is second, it still outperforms all the other policies by several percentage points. We also find that FIPO achieves the highest win-rate, winning 62% of the times against SFT (Table 4)."}, {"title": "7 Conclusion", "content": "In this work, we investigate the impact of logical fallacies on argument generation and introduce FIPO, a novel framework designed to improve the"}, {"title": "Limitations", "content": "Although various preference optimization strategies have shown improvement over the SFT baseline in reducing fallacious arguments, the margin remains modest. This may be attributed to several factors: our assumption that the original dataset (Saha et al., 2021) was free of fallacies, the inherent complexity and diversity of fallacies which complicates effective detection, and the variability in model performance, particularly the weaker results from the Mistral model compared to Llama-2. Additionally, the limited size of our dataset and the brevity of arguments present further challenges, as the lack of contextual cues can hinder the models' ability to identify and avoid fallacies consistently."}, {"title": "Ethics Statement", "content": "In this paper, we experiment with existing datasets that are well-acknowledged. Our framework is designed to improve argument generation in LLMs, which have been shown to potentially encode biases about race, gender, and other demographic attributes (Weidinger et al., 2021; Sheng et al., 2020). Since our work does not offer a way to mitigate these biases, models could still possibly reflect the same harmful behaviors. We recommend anyone deploying our model off-the-shelf should first check whether the model is harmful towards any protected group, and appropriate mitigation should be taken. Moreover, our annotation task is based on arguments generated by these models, which are complex to evaluate in terms of identifying logical fallacies because of the complex nature of fallacious reasoning. Hence, our task was done with English-speaking AMT workers who were paid adequately for the time it took to solve the tasks."}, {"title": "A Data Augmentation and Evaluation with LLMs", "content": ""}, {"title": "A.1 Generating Arguments with ChatGPT", "content": "Our prompt design for ChatGPT to generate fallacies follows a similar heuristic to (Liu et al., 2023), by introducing the task, defining the fallacy type it must generate, along with two examples of that particular fallacy type. Following the distribution in Figure 3, we generate four fallacies for the same topic and feed the following prompt to ChatGPT to generate fallacies as negative preference data:\nYou are given a topic T. Your task is to generate a {'supporting' or 'counter'} argument in the form of a f-type\u201c logical fallacy in the context of the topic. It should not be longer than 25 words.\nf-type fallacy is defined as: {definition}\nexamples of fallacy type are:\nexample 1\nexample 2\nHere is an example of a supporting f-type fallacy:\n{example of an argumentative fallacy}\n{\n\"topic\": T, \"fallacy\": f-type, \"argument\": <...>}\n\"Fallacy type that can be any of the thirteen types described in Table 8\nSome examples of generated logical fallacies include: \"I know someone who smoked cannabis and became successful. Therefore, everyone who smokes cannabis will be successful.\", \"I know a few people who spend too much time on social media and have no real-life friends. Therefore, social media is terrible for society.\" Table 9 presents examples of samples in our preference dataset. Our augmented fallacy argument dataset consists of the train-test split in Table 2."}, {"title": "A.2 Prompting GPT-4 for Evaluation", "content": "To evaluate whether $\\pi_{\\theta}$ for a given method $\\theta$ generates logical arguments compared to $\\pi_{SFT}$, we use GPT-4 and evaluate the win-rate (e.g., how often does $\\pi_{\\theta}$ produce better arguments) by prompting GPT-4 with:\nWhich of these arguments is better for the topic x:\n1. $\\pi_{SFT}(y|x)$\n2. $\\pi_{\\theta}(y|x)$\nIf both arguments are equally good, return 3 (Tie). The better argument is: <response>\nWe also evaluate how often both models produce logical fallacies, which we call fallacy-rate, by prompting ChatGPT with:\nConsider the following topic and {supporting or counter} argument:\nTopic: T\nArgument: t\nOut of all the following logical fallacy types {list of types from Table 8}\nwould you qualify t as one of these logical fallacies? If not - return \"None\".\nIf yes, which logical fallacy type is it? Let f-type be your answer. Return {\"topic\": T, \"text\": t, \"fallacy type\": f-type }"}, {"title": "B Preference Optimization", "content": "Preference optimization is a crucial step in aligning language models to generate outputs that meet user preferences and objectives effectively. It involves the process of ensuring that the goals and preferences of AI systems align with those of their human users. To demonstrate models are capable of learning to distinguish logically sound text from logical fallacies, we assess the performance of four different preference optimization techniques, including PPO (Schulman et al., 2017), DPO (Rafailov et al., 2023), CPO (Xu et al., 2024) and KTO (Ethayarajh et al., 2024).\nPPO. One widely used reinforcement learning optimization algorithm within RLHF is Proximal Policy Optimization (PPO). PPO (Schulman et al., 2017) is particularly favored due to its stability and efficiency. It iteratively updates the model's policy parameters by maximizing the expected cumulative reward while constraining the policy updates to a proximity threshold, preventing large policy changes that could destabilize learning.\nDPO. More recently, Rafailov et al. (2023) introduced Direct Preference Optimization, which skips the reward modelling part that is necessary for PPO. DPO leverages an analytical mapping from reward functions to optimal policies, to transform a loss function over reward functions into a loss function over policies and avoids fitting a reward model, while still optimizing under given preferences."}, {"title": "C Details of Experimental Setup", "content": ""}, {"title": "C.1 Hyperparameters", "content": "Training Parameters. For both base models, we train $\\pi_{SFT}$ and $\\pi_{\\theta}$ on 2 A100 GPUs for 3 epochs, using an Adam optimizer, with a learning rate of $2^{-4}$. For the LORA configuration, we select a rank r = 16, a = 32 and a dropout of $5^{-2}$. Regarding the specific training details of each method, we use \u03b2 = 0.25 for DPO (Rafailov et al., 2023), which controls how much $\\pi_{\\theta}$ deviates from the reference model $\\pi_{SFT}$. For both CPO (Xu et al., 2024) and KTO (Ethayarajh et al., 2024), we use \u03b2 = 0.1, which controls the implicit reward. Regarding the reward model for PPO (Schulman et al., 2017), we use a binary logical fallacy classifier we trained using the chosen responses as non-fallacies and the rejected responses as fallacies. The classifier achieves accuracy and F1 over 95% in the detection of fallacy arguments, which makes us confident in using the model as a reward model for this particular task and we use the logits as rewards. Decoding Parameters. At inference time, we generate arguments with various LLMs using the same decoding strategy across all models. We use nucleus-sampling with p = 0.75 and top-k sampling with k = 10."}, {"title": "C.2 Context for Retrieval Augmented Generation (RAG)", "content": "We use the wiki-dpr database to add extra contextual information to prompts. RAG (Lewis et al., 2020) ensures that arguments are grounded in factual accuracy and contextually relevant information by retrieving relevant information from pre-existing knowledge sources. We retrieve relevant documents with the topics and add them to the prompts as context. For example, given the topic Factory farming should not be banned, the knowledge extracted includes \"The practice of dairy production in a factory farm environment has been criticized by animal welfare activists. The U.S. Food and Drug Administration states that no 'significant difference' has been found between milk from treated and non-treated cows. [...]\""}, {"title": "C.3 Licenses of Artifacts", "content": "The artifacts we employ, including datasets, packages, and models, are detailed in Table 11. Our usage of these artifacts is consistent with their intended purposes."}, {"title": "C.4 Detailed Setup of Human Annotations in Amazon Mechanical Turk", "content": "The privacy of these collected annotations for the pairwise comparison is under the university policy, and we use Amazon Mechanical Turk's services. The webpage we use for the comparative study by human annotators is provided in Figure 6. The workers are paid fairly for their annotating work."}, {"title": "D Additional Experimental Results", "content": ""}, {"title": "D.1 Fallacy-Rates in Zero-Shot Setting", "content": "Table 5 presents the distribution of fallacies detected with GPT-4 in the zero-shot setting using Llama-2 and Mistral."}, {"title": "D.2 Results Regarding Win-Rates", "content": "Table 6 presents the results of win-rates computed by GPT-4 by comparing the policies with the corresponding SFT models and comparing which models win more often."}, {"title": "D.3 Examples of Fallacies Produced", "content": "Table 10 shows a few examples of arguments and fallacies produced by the models on a set of three topics."}, {"title": "E Ablation Study", "content": "To validate the effectiveness of our design, we conducted two ablation studies:\n(i) Dataset Uniformity: The first study involves modifying the training dataset to include an equal number of samples for each fallacy type. Our original data exhibits a natural distribution where some fallacy types are more prevalent. For this study, we created a uniformly distributed dataset by down-sampling, resulting in a dataset comprising 2,522 samples (194 per fallacy type)."}, {"title": "4.4 Fallacy-Informed Preference Optimization: Loss Definition", "content": "In our work", "as": "n$\\mathcal{L"}, {"right": "nThe CPO loss first approximates the DPO loss using a uniform reference model:\n$\\mathcal{L"}, "pi_{\\theta}, U) = -E_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ log \\sigma \\left( \\beta (log \\pi_{\\theta}(y_w | x) - log \\pi_{\\theta}(y_l | x))\\right) \\right"], "mathcal{L}_{CPO}$": "n$\\mathcal{L}_{CPO} (\\pi_{\\theta}) = min \\left( \\mathcal{L}(\\pi_{\\theta}, U) - E_{(x, y_w) \\sim \\mathcal{D}}[log \\pi_{\\theta}(y_w | x)] \\right)$ (6)\nwhich combines the preference loss with a negative log-likelihood (NLL) term. The min term ensures $\\pi_{\\theta}$ does not deviate from the preferred data distribution.\nFIPO Loss\nRecall the definitions for our classification loss in Section 4.4. We define the probability $p_k"}