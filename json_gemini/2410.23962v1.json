{"title": "Image Synthesis with Class-Aware Semantic Diffusion Models for Surgical Scene Segmentation", "authors": ["Yihang Zhou", "Rebecca Towning", "Zaid Awad", "Stamatia Giannarou"], "abstract": "Surgical scene segmentation is essential for enhancing surgical precision, yet it is frequently compromised by the scarcity and imbalance of available data. To address these challenges, semantic image synthesis methods based on generative adversarial networks and diffusion models have been developed. However, these models often yield non-diverse images and fail to capture small, critical tissue classes, limiting their effectiveness. In response, we propose the Class-Aware Semantic Diffusion Model (CASDM), a novel approach which utilizes segmentation maps as conditions for image synthesis to tackle data scarcity and imbalance. Novel class-aware mean squared error and class-aware self-perceptual loss functions have been defined to prioritize critical, less visible classes, thereby enhancing image quality and relevance. Furthermore, to our knowledge, we are the first to generate multi-class segmentation maps using text prompts in a novel fashion to specify their contents. These maps are then used by CASDM to generate surgical scene images, enhancing datasets for training and validating segmentation models. Our evaluation, which assesses both image quality and downstream segmentation performance, demonstrates the strong effectiveness and generalisability of CASDM in producing realistic image-map pairs, significantly advancing surgical scene segmentation across diverse and challenging datasets.", "sections": [{"title": "Introduction:", "content": "Accurate segmentation of surgical scenes\nis critical for assisting intraoperative navigation and enhancing\nsurgical accuracy, directly influencing surgical outcomes. For\nexample, precise delineation of anatomical structures [1] and\naccurate detection of tumors and polyps [2] can guide the surgeon\nduring the execution of surgical tasks. Recently developed deep\nlearning models have demonstrated clinically relevant near-real-\ntime segmentation capabilities, some even outperforming human\nsurgeons [3].\nAmong these approaches, convolutional neural network (CNN)-\nbased methods like DeepLab [4] have advanced the field by using\natrous convolution to effectively capture detailed features across\nvarying scales, making them adept at detecting both large and\nsubtle structures. U-Net [5], with its encoder-decoder architecture\nand skip connections, is particularly effective at preserving spatial\nresolution, which is crucial for the precise segmentation of small\nanatomical features. More recently, transformer-based models\nsuch as SegFormer [6] and Mask2Former [7] have emerged,\noffering significant advantages in capturing global context and\nmulti-scale information through self-attention mechanisms. These\ntransformer-based models have proven particularly valuable in\nhandling complex and diverse surgical scenes, achieving notable\nimprovements in accuracy [3, 8, 9].\nHowever, two major challenges persist across all these\nsegmentation techniques. First, most multi-class segmentation\nmethods are highly data-hungry, requiring large amounts of\nannotated data for training. Obtaining such data is labor-intensive\nand time-consuming, and even professional surgeons often struggle\nto accurately annotate low-contrast areas and unclear edges.\nSecond, while these methods excel at segmenting large and distinct\nanatomical structures or surgical tools, they frequently struggle\nwith underrepresented classes, particularly in datasets where certain\ncategories are significantly smaller or less prevalent than others.\nThis imbalance can lead to overfitting on these infrequent classes,\nresulting in poor generalization during testing. Such challenges\nare particularly critical in surgical applications, where the accurate\nidentification of subtle anomalies can be life-saving.\nTo address these issues, generative adversarial networks (GANS)\n[10, 11] have been proposed for data synthesis, particularly to\naugment underrepresented classes. However, GANs often suffer"}, {"title": null, "content": "from mode collapse, producing only a limited variety of data\npatterns, which limits their effectiveness [12].\nMore recently, diffusion models have demonstrated superior\nabilities in generating high-fidelity and diverse images, setting\na new standard in image synthesis [13, 14, 15, 16]. Semantic\ndiffusion models have been proposed to synthesize images from\nsegmentation maps, which can then be directly employed in\ndownstream segmentation tasks [17]. For instance, Yuhao et al.\ndeveloped a framework for colonoscopy image synthesis [18],\nwhile Yan et al. focused on abdominal CT image synthesis\n[19]. However, most diffusion models use the pixel-wise mean\nsquared error (MSE) loss, which inadequately represents smaller\nclasses and leads to their underrepresentation. Moreover, MSE loss\ndoes not effectively capture higher-level perceptual features such\nas textures and structures, often resulting in visually unrealistic\nimages. Additionally, synthetic images generated by semantic\ndiffusion models must adhere to the structure and class distribution\npredefined by the segmentation maps. This further limits the\nmodel's ability to generate diverse variations.\nTo overcome the above limitations, we propose the Class-\nAware Semantic Diffusion Model (CASDM) tailored for surgical\nscene generation. (2) Multi-class segmentation map generation: For\nthe first time, segmentation maps containing multiple classes are\ngenerated to guide semantic image synthesis. (3) Text-prompted\nsegmentation map generator: We propose a model that uniquely\ngenerates segmentation maps using separate text prompts for\neach class, specifying class names, quantities, and locations.\nThis method, when integrated with CASDM, yields pairs of\nhigh-quality segmentation maps and images, markedly improving\ndataset diversity. (4) Empirical Validation: Through extensive\nqualitative and quantitative assessments, our pipeline demonstrates\nsubstantial improvements in both image quality and segmentation\nperformance. We applied our approach to the CholecSeg8K dataset\n[20], focusing on scarce and challenging classes such as the\nhepatic vein, blood, and cystic duct. To demonstrate the broader\napplicability of our model, we also tested it on well-represented\nclasses within the same dataset, confirming that our approach is\nparticularly beneficial for small and rare classes. Additionally, to\nevaluate the generalisability of our method, we applied it to the\ngastrectomy SISVSE dataset [21]. Across different datasets and"}, {"title": "2. Method:", "content": "Our pipeline, illustrated in Fig. 1, features two\nprimary components: the semantic image generator (CASDM) and\nthe text-guided segmentation map generator. Both components\nutilize unconditional denoising diffusion probabilistic models\n(DDPMs) [13].\nUnconditional Denoising Diffusion Probabilistic Models\n(DDPMs): DDPMs are generative models that synthesize images\nby progressively adding Gaussian noise in a forward process and\nsubsequently reversing this noise addition in a reverse process,\nthereby learning to reconstruct data.\nIn the forward process, the model gradually introduces noise to\nthe data through a Markov chain at each timestep t, transitioning\nthe data xt-1 to a noisier state xt. This process is mathematically\ndescribed by:\n$$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t\\mathbf{I})$$\nwhere, $\\beta_t$ are the predefined noise schedule parameters\nincrementally increasing the noise level, and $\\mathcal{N}$ represents\nthe Gaussian distribution. The reverse diffusion process aims to\nreconstruct the original data by reversing the noise added during the\nforward process. At each reverse timestep t, the model estimates\nthe less noisy previous state $x_{t-1}$ from the current state $x_t$ using a\nconditional distribution parameterized as:\n$$p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t\\mathbf{I})$$\nwhere $\\mu_\\theta$ and $\\sigma_t$ are functions of the neural network parameters\n$\\theta$ (with the network typically being a U-Net architecture), and t,\npredicting the mean and variance necessary for effective denoising.\nThe training of DDPM primarily focuses on learning the\nparameters $\\theta$ that effectively reverse the forward diffusion process.\nThis is achieved by minimizing the pixel-wise MSE between the\nreal noise $\\epsilon$ used in the forward process and the noise predicted by\nthe model $\\hat{\\epsilon}_\\theta(x_t, t)$ during the reverse process:\n$$L_{MSE} = \\mathbb{E} [||\\epsilon - \\hat{\\epsilon}_\\theta(x_t, t)||^2]$$\nwhere the expectation is over all data points and all noise levels,\noptimizing the model to regenerate new data $x_0$ accurately by\ntraversing backward along the Markov chain.\nSemantic Conditioning: The unconditional DDPM does not\nincorporate semantic conditions into the image generation process,\nresulting in unpredictable and random images. Additionally, the\ngenerated images lack class annotation information, limiting their\nusefulness in downstream tasks such as segmentation and object\ndetection. To advance the traditional unconditional DDPMs, we\ndesigned the CASDM which is based on semantic image synthesis\n[17]. CASDM employs a dual-pathway architecture, as shown"}, {"title": null, "content": "in Fig. 2. As part of the forward diffusion process, noise\nis added to the input image which is then processed by the\nencoder, while the decoder is conditioned by the segmentation\nmap. Semantic information is integrated at multiple stages of\nthe decoder using spatially-adaptive normalization layers [17],\nensuring that the generated images maintain their semantic structure\nand integrity throughout the generation process. This architecture\nallows CASDM to produce images that have corresponding\nclass annotations (segmentation maps), useful for downstream\nsegmentation tasks.\nAt the reverse diffusion process, the reverse diffusion equation is\nbased on Equation (2), incorporating semantic guidance as:\n$$p_\\theta(x_{t-1}|x_t, s) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, s, t), \\sigma_t\\mathbf{I})$$\nwhere, s is the segmentation map. The MSE loss from Equation (3)\nis adapted to:\n$$L_{Semantic MSE} = \\mathbb{E} [||\\epsilon - \\hat{\\epsilon}_\\theta(x_t, s, t)||^2]$$\nClass-Aware MSE Loss: Traditional pixel-wise MSE loss\nin diffusion models tends to overemphasize larger areas like\nbackgrounds, neglecting small but crucial classes such as blood\nvessels in surgical scenes. To enhance the fidelity of CASDM\nto minor classes, we designed the novel class-aware MSE loss\nfunction $L_{CAMSE}$. This function ensures all classes, regardless of\ntheir size, are accurately represented in the generated images and it\nis defined as:\n$$L_{CAMSE} = \\sum_{c=1}^C w_c \\cdot \\mathbb{E} [||\\epsilon_c - \\hat{\\epsilon}_\\theta(x_t, s, t)_c||^2]$$\nwhere, the class weight $w_c$ is calculated based on the inverse of\nthe pixel count for each class within the segmentation conditioning\nmap, normalized across all classes:\n$$w_c = \\frac{(H \\cdot W/n_c)}{\\sum_{j=1}^C(H \\cdot W/n_j)}$$\nwhere H. W is the total number of pixels in the image, and $n_c$ is\nthe number of pixels for class c.\nOur approach gives adequate focus to smaller but important\ntissue classes. This is beneficial for generating pairs of\naligned images and segmentation maps that enhance downstream\nsegmentation performance on imbalanced datasets, crucial for\nsurgical imaging applications.\nClass-Aware Self-Perceptual Loss: While $L_{CAMSE}$ significantly\nadvances the precision of class representation in semantic image\nsynthesis by ensuring equal attention to all classes, it still primarily\nfocuses on reducing pixel-level discrepancies between the ground\ntruth noise and predicted noise. This function does not capture\ncomplex perceptual qualities such as textures and contours, which\nare essential for producing realistic and clinically applicable\nimages. Moreover, although $L_{CAMSE}$ addresses class imbalance, it\nmight not fully represent the contextual and spatial relationships\nbetween classes, which are key for high-fidelity surgical imaging.\nPrevious approaches have used external segmentation models\nto refine the training process of semantic image synthesis [18],\nbut these can introduce additional complexity and potential\nimperfections. classifier-free guidance (CFG) [22] has also been\nemployed to enhance image quality by interpolating between\nconditional and unconditional outputs, improving sharpness,\nfidelity, and diversity. However, CFG can disrupt the coherence\nbetween the generated images and the segmentation map\nconditions. Recent findings [23] suggest that self-perceptual loss\ncan improve the quality of synthesized images by focusing on high-\nlevel perceptual features like textures and structures, thus enhancing\nvisual realism without affecting the constraints of conditions.\nTo build on these advancements, we introduce the novel class-\naware self-perceptual loss $L_{CASP}$ for semantic image synthesis to"}, {"title": null, "content": "enrich the perceptual quality of images. As shown in Fig. 2 (right\nrefinement side), this loss is computed by first adding identical\nrandom noise to both the input image and the image predicted\nby the diffusion model. These images are then processed through\nthe encoder of the U-Net in the CASDM framework to generate\ncorresponding feature maps from both images. $L_{CASP}$ then uses\nthese feature maps to measure perceptual discrepancies. $L_{CASP}$\nspecifically amplifies the representation of minor but critical classes\nby incorporating class-aware considerations into the calculation. It\nis defined as follows:\n$$L_{CASP} = \\sum_{c=1}^C ||(\\Phi(x_o) - \\Phi(\\hat{x}_o + \\epsilon))_c \\odot pool(w_c)||^2$$\nwhere, $\\Phi$ represents the feature maps extracted by the U-Net\nencoder, and $\\odot$ signifies element-wise multiplication. The class\nweights $w_c$ are derived from Equation (7). The function $pool(w_c)$\nadapts the class weights to the dimensions of the feature maps. The\npredicted image $\\hat{x}_o$ is obtained by first predicting the noise output\nusing the core U-Net of the diffusion model. This is given by:\n$$\\epsilon = \\epsilon_\\theta(x_t, t)$$\nUsing this estimated noise, the original clean image is reconstructed\nby reversing the noise process applied during the diffusion model's\nforward process.:\n$$x_o = \\frac{x_t - \\sqrt{1 - \\beta_t}\\hat{\\epsilon}_\\theta}{\\sqrt{a_t}}$$\nwhere, $a_t = \\prod_{i=1}^t(1 - B_i)$ and $\\beta_t$ is the noise schedule.\nThe advantages of using this novel $L_{CASP}$ is that we avoid the\nuse of additional modulation models, ensuring that the generated\nimages align with the conditions while enhancing image details\nrelated to textures, colors, and structures, especially for smaller\nclasses.\nText-Prompted Segmentation Map Generator: Using ground\ntruth segmentation maps to generate images inherently limits\ndiversity. To address this issue, we are the first to propose a novel\npipeline that uses text prompts in a unique way to control the\nsynthesis of multi-class segmentation maps. These maps are then\nused in CASDM to create diverse pairs of images and segmentation\nmaps.\nThe process starts by inputting class names separately for\neach class into a frozen text encoder based on the contrastive\nlanguage-image pre-training (CLIP) model [24], with the option to\ninclude directional and quantitative information for each class. This\ninnovative method of separate text input for each class allows for the\nisolated encoding of class characteristics, significantly enhancing"}, {"title": null, "content": "the specificity and relevance of the generated text embeddings.\nThese individual embeddings are then concatenated to form a\ncomprehensive control vector. The CLIP model, pretrained on\n400 million image-text pairs, effectively captures detailed visual\nconcepts including locations and quantities from raw text.\nThe integrated text embeddings modify the conditional\ndistribution in the reverse diffusion steps of this text-guided DDPM:\n$$p_\\theta(x_{t-1}|x_t, e) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, e, t), \\sigma_t\\mathbf{I})$$\nwhere, e represents the concatenated text embedding.\nBy leveraging text prompts in this manner, we can produce highly\ndiverse and controlled segmentation maps. These maps allow the\nCASDM to generate corresponding synthetic images, significantly\nenhancing the dataset's diversity. Our proposed segmentation map\ngenerator is illustrated in Fig. 3."}, {"title": "3. Experiments and Results:", "content": "Our experiments aim to show that\nthe proposed framework is able to alleviate the poor segmentation\nperformance caused by data scarcity and class imbalance. We\ndemonstrate that CASDM and our text-prompted segmentation map\ngeneration pipeline can produce high-quality synthetic data for\nunderrepresented classes, and that our approach generalizes well\nacross different datasets. The code and synthetic data for CASDM\nare available at https://github.com/YihangZhou123/\nCASDM-Code-and-Synthetic-Data."}, {"title": null, "content": "To evaluate the quality of the synthesised images, we use the\nFr\u00e9chet inception distance (FID), the mean multi-scale structural\nsimilarity index measure (Mean MS-SSIM) and the mean peak"}, {"title": null, "content": "mean dice coefficients (mDice) for blood, cystic duct, and hepatic\nvein on the testing set. As shown in Table 2, the naive data\naugmentation method surpasses the original dataset's performance\nonly for the hepatic vein class with the SegFormer model, and for\nthe cystic duct class with the Mask2Former model. In the other\ncases, it shows a deterioration in the performance. This is expected\nas increasing the sample size without enhancing diversity leads to a\nsignificant risk of overfitting. When the training data is augmented\nwith synthesised images generated by segmentation maps only\nfrom the training set, CASDM achieves the highest mIoU and\nmDice, demonstrating its strict adherence to segmentation map\nguidance to synthesise meaningful images. CASDM showed an\naverage improvement across these three classes of 1.4% in mIoU\nand 0.6% in mDice for SegFormer, and 1.3% in mIoU and\n0.9% in mDice for Mask2Former. Additionally, the combination\nof CASDM with our text-prompted segmentation map generator\n(CASDM + Synthetic Maps) achieved the highest mIoU and mDice\non both SegFormer and Mask2Former, demonstrating the practical\nutility of the proposed pipeline. On SegFormer, this combination\nimproved the mIoU by 1.8% and mDice by 1.4% across the three\nclasses. On Mask2Former, it improved the mIoU by 2% and mDice\nby 1.5%. Furthermore, on Mask2Former, it enhanced the mIoU for\nthe cystic duct by 3.2%, significantly proving the effectiveness of\nthis approach.\nAs shown in Fig. 6, segmentation models trained on data\naugmented by CASDM and synthetic maps perform significantly\nbetter in segmenting small classes, even in the presence of\nocclusions. Specifically, in case (a), SegFormer achieves more\ndetailed segmentation in the yellow cystic duct's top-left and\nbottom-right regions, which is crucial for successful surgeries.\nAdditionally, in case (b), using the Mask2Former model, our\npipeline significantly enhances the segmentation of tiny, barely\nvisible blood clots. In case (c), where slight smoke is present,\nour method is particularly effective, accurately segmenting the\nsecond blue hepatic vein area at the bottom of the image where"}, {"title": null, "content": "other approaches failed. Moreover, in case (d), in a highly smoke-\nobscured scenario, our result closely aligns with the ground truth,\nespecially in capturing the contours of blood, demonstrating the\nrobustness of our approach in maintaining critical detail even under\nsevere visual impairment.\nThe success in addressing underrepresented classes lies in\nCASDM's enhanced diversity and realism, improving textural and\nstructural details vital for accurate diagnostics. The integration of\nsynthesized segmentation maps further enables the model to output\nimages with diverse structures, increasing alignment with unseen\ntesting data. This pipeline enriches the training set for downstream\nsegmentation models with novel patterns, significantly boosting\ntheir generalization capabilities for underrepresented classes.\nIn our evaluation of well-represented classes using several\ndata expansion methods, as shown in Table 3, we observed\nless significant improvements. For the neither scarce, nor small\nconnective tissue class, adding synthetic data had minimal impact\non the segmentation performance. In the case of the rare, but\nhighly uniform and large liver ligament class, the best result came\nfrom CASDM alone, with a 0.9% increase in mIoU and a 0.5%\nincrease in mDice on SegFormer compared to the original dataset.\nFor the common and moderately sized L-hook electrocautery class,\nCASDM on Mask2Former yielded a 0.8% increase in mIoU and a\n0.4% increase in mDice. These results suggest that the downstream\nsegmentation models have already learned sufficient features of"}, {"title": null, "content": "respectively improves the quality of synthesised images in terms\nof FID, Mean MS-SSIM and Mean PSNR. When combined in\nour CASDM, they achieve the best performance, indicating the\nefficiency of both modules.\nGeneralisability Across Datasets: We extended our experiments\nto include another surgical procedure, gastrectomy, using the\npublicly available SISVSE dataset [21]. Our pipeline was retrained\non this dataset, focusing on three less-represented classes: Suction\nIrrigation, Endotip, and Spleen. These classes appear in 8.36%,\n8.76%, and 9.29% of the images, respectively, and occupy 3.85%,\n4.57%, and 2.47% of the image area in those images. All settings\nwere kept consistent with those used on the cholecystectomy\ndataset. Using the original and augmented gastrectomy datasets, we\ntrained and tested the SegFormer and Mask2Former models. The\nresults, detailed in Table 4, show an average mIoU improvement\nof 3.6% and mDice improvement of 4.3% for these three classes\non SegFormer when using our full pipeline, which includes\nboth segmentation map generation and image generation. On\nMask2Former, the mIoU and mDice for these three classes\nimproved by 5.1% and 4.4%, respectively. Our pipeline achieved\nmore significant improvements on the gastrectomy dataset, due to\ndataset's greater diversity and imbalance, with 32 classes where\nthe selected classes appeared in a smaller percentage of images\nand occupied a smaller area. This outcome demonstrates the strong\ngeneralisability of our pipeline, particularly in more challenging\nand diverse datasets."}, {"title": "4. Conclusions:", "content": "In this work, we proposed CASDM, a\nnovel image synthesis approach designed to address data scarcity\nand imbalance. We also introduced a pipeline that generates\nsegmentation maps and corresponding images from text prompts.\nCASDM leverages class-aware MSE and self-perceptual losses\nto create diverse, high-quality images that closely adhere to\nsegmentation maps, enhancing the representation of less visible\nclasses. Our evaluation confirms that this approach significantly\nimproves both the quality of synthesized images and the\nperformance of segmentation models, demonstrating its potential\nto enhance surgical outcomes by providing robust and balanced\ntraining data, particularly for endoscopic procedures. Additionally,\nour results highlight the strong generalisability of our pipeline\nacross different surgical procedures, showcasing its broader\napplicability.\nWhile our method is not real-time, the fact that data augmentation\nis performed offline makes real-time inference not a concern for\napplications. Our next step is to develop a model capable of\ndirectly generating segmentation maps and closely aligned images\nfrom text prompts in a single step, further enhancing applicability\nand ultimately supporting better surgical decision-making and\noutcomes."}]}