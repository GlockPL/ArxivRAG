{"title": "Video Diffusion Alignment via Reward Gradients", "authors": ["Mihir Prabhudesai*", "Russell Mendonca*", "Zheyang Qin*", "Katerina Fragkiadaki", "Deepak Pathak"], "abstract": "We have made significant progress towards building foundational video diffusion models. As these models are trained using large-scale unsupervised data, it has become crucial to adapt these models to specific downstream tasks. Adapting these models via supervised fine-tuning requires collecting target datasets of videos, which is challenging and tedious. In this work, we utilize pre-trained reward models that are learned via preferences on top of powerful vision discriminative models to adapt video diffusion models. These models contain dense gradient information with respect to generated RGB pixels, which is critical to efficient learning in complex search spaces, such as videos. We show that backpropagating gradients from these reward models to a video diffusion model can allow for compute and sample efficient alignment. We show results across a variety of reward models and video diffusion models, demonstrating that our approach can learn much more efficiently in terms of reward queries and computation than prior gradient-free approaches. Our code, model weights, and more visualization are available at https://vader-vid.github.io.", "sections": [{"title": "1 Introduction", "content": "We would like to build systems capable of generating videos for a wide array of applications, ranging from movie production, creative story-boarding, on-demand entertainment, AR/VR content genera-tion, and planning for robotics. The most common current approach involves training foundational video diffusion models on extensive web-scale datasets. However, this strategy, while crucial, mainly produces videos that resemble typical online content, featuring dull colors, suboptimal camera angles, and inadequate alignment between text and video content.\nContrast this with the needs of an animator who wishes to bring a storyboard to life based on a script and a few preliminary sketches. Such creators are looking for output that not only adheres closely to the provided text but also maintains temporal consistency and showcases desirable camera perspectives. Relying on general-purpose generative models may not suffice to meet these specific requirements. This discrepancy stems from the fact that large-scale diffusion models are generally trained on a broad spectrum of internet videos, which does not guarantee their efficacy for particular applications. Training these models to maximize likelihood across a vast dataset does not necessarily translate to high-quality performance for specialized tasks. Moreover, the internet is a mixed bag when it comes to content quality, and models trained to maximize likelihood might inadvertently replicate lower-quality aspects of the data. This leads us to the question: How can we tailor diffusion models to produce videos that excel in task-specific objectives, ensuring they are well-aligned with the desired outcomes?\nThe conventional approach to aligning generative models in the language and image domains begins with supervised fine-tuning [22, 4]. This involves collecting a target dataset that contains expected behaviors, followed by fine-tuning the generative model on this dataset. Applying this strategy to video generation, however, presents a significantly greater challenge. It requires obtaining a dataset of target videos, a task that is not only more costly and laborious than similar endeavors in language or image domains, but also significantly more complex. Furthermore, even if we were able to collect a video target dataset, the process would have to be repeated for every new video task, making it prohibitively expensive. Is there a different source of signal we can use for aligning video diffusion, instead of trying to collect a target dataset of desired videos?\nReward models play a crucial role [24, 32, 17] in aligning image and text generations. These models are generally built on top of powerful image or text discriminative models such as CLIP or BERT [21, 1, 29]. To use them as reward models, people either fine-tune them via small amounts of human preferences data [24] or use them directly without any fine-tuning; for instance, CLIP can be used to improve image-text alignment or object detectors can be used to remove or add objects in the images [20].\nThis begs the question, how should reward models be used to adapt the generation pipeline of diffusion models? There are two broad categories of approaches, those that utilize reward gradients [20, 6, 33], and others that use the reward only as a scalar feedback and instead rely on estimated policy gradients [2, 18]. It has been previously found that utilizing the reward gradient directly to update the model can be much more efficient in terms of the number of reward queries, since the reward gradient contains rich information of how the reward function is affected by the diffusion generation [20, 6]. However, in text-to-image generation space, reward gradient-free approaches are still dominant [23], since these methods can be easily trained within 24 hours and the efficiency gains of leveraging reward gradients are not significant.\nIn this work, we find that as we increase the dimensionality of generation i.e transition from image to video, the gap between the reward gradient and policy gradient based approaches increases. This is because of the additional amount and increased specificity of feedback that is backpropagated to the model. For reward gradient based approaches, the feedback gradients linearly scale with respect to the generated resolution, as it yields distinct scalar feedback for each spatial and temporal dimension. In contrast, policy gradient methods receive a single scalar feedback for the entire video output. We test this hypothesis in Figure 4, where we find that the gap between reward gradient and policy gradient approaches increases as we increase the generated video resolution. We believe this makes it crucial to backpropagate reward gradient information for video diffusion alignment.\nWe propose VADER, an approach to adapt foundational video diffusion models using the gradients of reward models. VADER aligns various video diffusion models using a broad range of pre-trained vision models. Specifically, we show results of aligning text-to-video (VideoCrafter, OpenSora, and"}, {"title": "2 Related Work", "content": "Denoising diffusion models [26, 11] have made significant progress in generative capabilities across various modalities such as images, videos and 3D shapes [10, 12, 19]. These models are trained using large-scale unsupervised or weakly supervised datasets. This form of training results in them having capabilities that are very general; however, most end use-cases of these models have specific requirements, such as high-fidelity generation [24] or better text alignment [32].\nTo be suitable for these use-cases, models are often fine-tuned using likelihood [3, 4] or reward-based objectives [2, 20, 6, 33, 18, 7, 9]. Likelihood objectives are often difficult to scale, as they require access to the preferred behaviour datasets. Reward or preference based datasets on the other hand are much easier to collect as they require a human to simply provide preference or reward for the data generated by the generative model. Further, widely available pre-trained vision models can also be used as reward models, thus making it much easier to do reward fine-tuning [2, 20]. The standard approach for reward or preference based fine-tuning is to do reinforcement learning via policy gradients [2, 30]. For instance, the work of [18] does reward-weighted likelihood and the work of [2] applies PPO [25]. Recent works of [20, 6], find that instead of using policy gradients, directly backpropagating gradients from the reward model to diffusion process helps significantly with sample efficiency.\nA recent method, DPO [22, 30], does not train an explicit reward model but instead directly optimizes on the human preference data. While this makes the pipeline much simpler, it doesn't solve the sample inefficiency issue of policy gradient methods, as it still backpropagates a single scalar feedback for the entire video output.\nWhile we have made significant progress in aligning image diffusion models, this has remained challenging for video diffusion models [3, 31]. In this work, we take up this challenging task. We find that naively applying prior techniques of image alignment [20, 6] to video diffusion can result in significant memory overheads. Further, we demonstrate how widely available image or video discriminative models can be used to align video diffusion models. Concurrent to our work, InstructVideo [34] also aligns video diffusion models via human preference; however, this method requires access to a dataset of videos. Such a dataset is difficult to obtain for each different task, and becomes difficult to scale especially to large numbers of tasks. In this work, we show that one can easily align video diffusion models using pre-trained reward models while not assuming access to any video dataset."}, {"title": "3 Background", "content": "Diffusion models have emerged as a powerful paradigm in the field of generative modeling. These models operate by modeling a data distribution through a sequential process of adding and removing noise.\nThe forward diffusion process transforms a data sample x into a completely noised state over a series of steps T. This process is defined by the following equation:\n$$x_t = \\sqrt{\\bar{\\alpha}_t} x + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, \\epsilon \\sim N(0,1),$$"}, {"title": "4 VADER: Video Diffusion via Reward Gradients", "content": "We present our approach for adapting video diffusion models to perform a specific task specified via a reward function R(.).\nGiven a video diffusion model $$p_\\theta(.),$$ dataset of contexts $$D_c$$, and a reward function R(.), we seek to maximize the following objective:\n$$J(\\theta) = \\mathbb{E}_{c \\sim D_c,x_0 \\sim p_\\theta(x_0|c)}[R(x_0, c)]$$\nTo learn efficiently, both in terms of the number of reward queries and compute time, we seek to utilize the gradient structure of the reward function, with respect to the weights \u03b8 of the diffusion model. This is applicable to all reward functions that are differentiable in nature. We compute the gradient $$\\nabla_\\theta R(x_0, c)$$ of these differentiable rewards, and use it to update the diffusion model weights \u03b8. The gradient is given by :\n$$\\nabla_\\theta R(x_0, c) = \\sum_{t=0}^T \\frac{\\partial R(x_0, c)}{\\partial x_t} \\frac{\\partial x_t}{\\partial \\theta}$$"}, {"title": "Algorithm 1 VADER", "content": "VADER is flexible in terms of the denoising schedule, we demonstrate results with DDIM [27] and EDM solver [14]. To prevent over-optimization, we utilize truncated backpropaga-tion [28, 20, 6], where the gradient is back prop-agated only for K steps, where K < T, where T is the total diffusion timesteps. Using a smaller value of K also reduces the memory burden of having to backpropagate gradients, making train-ing more feasible. We provide the pseudocode of the full training process in Algorithm 1. Next, we discuss the type of reward functions we con-sider for aligning video models.\nReward Models: Consider a diffusion model that takes conditioning vector c as input and generates a video $$x_0$$ of length N, consisting of a series of images $$i_k$$, for each timestep k from 0 to N. Then the objective function we maximize is as follows:\n$$J_\\theta = \\mathbb{E}_{c,i_{0:N}}[R([i_0, i_1...i_k...i_{N-1}], c)]$$\nWe use a broad range of reward functions for aligning video diffusion models. Below we list down the distinct types of reward functions we consider.\nImage-Text Similarity Reward - The generations from the diffusion model correspond to the text provided by the user as input. To ensure that the video is aligned with the text provided, we can define a reward that measures the similarity between the generated video and the provided text. To take advantage of popular, large-scale image-text models such as CLIP[21], we can take the following approach. For the entire video to be well aligned, each of the individual frames of the video likely need to have high similarity with the context c. Given an image-context similarity model $$\\mathcal{G}_{img}$$, we have:\n$$R([i_0, i_1...i_k...i_{N-1}], c) = \\sum_k R(i_k, c) = \\sum_k \\mathcal{G}_{img} (i_k, c)$$\nThen, we have $$J_\\theta = \\mathbb{E}_{k \\in [0,N]} [\\mathcal{G}_{img}(i_k, c)]$$, using linearity of expectation as in the image-alignment case. We conduct experiments using the HPS v2 [32] and PickScore [16] reward models for image-text alignment. As the above objective only sits on individual images, it could potentially result in a collapse, where the predicted images are the exact same or temporally incoherent. However, we don't find this to happen empirically, we think the initial pre-training sufficiently regularizes the fine-tuning process to prevent such cases.\nVideo-Text Similarity Reward - Instead of using per image similarity model $$\\mathcal{G}_{img}$$, it could be beneficial to evaluate the similarity between the whole video and the text. This would allow the model to generate videos where certain frames deviate from the context, allowing for richer, more diverse expressive generations. This also allows generating videos with more motion and movement, which is better captured by multiple frames. Given a video-text similarity model $$\\mathcal{G}_{vid}$$ we have $$J_\\theta = \\mathbb{E} [\\mathcal{G}_{vid}([i_0, i_1...i_k...i_{N-1}], c)]$$. In our experiments, we use a VideoMAE[29] fine-tuned on action classification, as $$\\mathcal{G}_{vid}$$, which can classify an input video into one of a set of action text descriptions. We provide the target class text as input to the text-to-video diffusion model, and use the predicted probability of the ground truth class from VideoMAE as the reward.\nImage Generation Objective - While text similarity is a strong signal to optimize, some use cases might be better addressed by reward models that only sit on the generated image. There is a prevalence of powerful image-based discriminative models such as Object Detectors and Depth Predictors. These models utilize the image as input to produce various useful metrics of the image, which can be used as a reward. The generated video is likely to be better aligned with the task if the reward obtained on each of the generated frames is high. Hence we define the reward in this case to be the mean of the rewards evaluated on each of the individual frames, i.e $$R([i_0, i_1...i_k...i_{N-1}], c) = \\sum_k R(i_k)$$. Note that given the generated frames, this is independent of the text input c. Hence we have, $$J_\\theta =$"}, {"title": "5 Results", "content": "In this work, we focus on fine-tuning various conditional video diffusion models, including VideoCrafter [5], Open-Sora [35], Stable Video Diffusion [3] and ModelScope [31], through a comprehensive set of reward models tailored for images and videos. These include the Aesthetic model for images [24], HPSv2 [32] and PickScore [16] for image-text alignment, YOLOS [8] for object removal, VideoMAE for action classification [29], and V-JEPA [1] self-supervised loss for temporal consistency. Our experiments aim to answer the following questions:\n\u2022 How does VADER compare against gradient-free techniques such as DDPO or DPO regard-ing sample efficiency and computational demand?\n\u2022 To what extent can the model generalize to prompts that are not seen during training?\n\u2022 How do the fine-tuned models compare against one another, as judged by human evaluators?\n\u2022 How does VADER perform across a variety of image and video reward models?\nThis evaluation framework assesses the effectiveness of VADER in creating high-quality, aligned video content from a range of input conditioning.\nBaselines. We compare VADER against the following methods:\n\u2022 VideoCrafter [5], Open-Sora 1.2 [35], and ModelScope [31] are current state-of-the-art (publicly available) text-to-video diffusion models. We serve them as base models for fine-tuning and comparison in our experiments in text-to-video space.\n\u2022 Stable Video Diffusion [3] is the current state-of-art (publicly available) image-to-video diffusion model. In all our experiments in image-to-video space, we use their base model for fine-tuning and comparison.\n\u2022 DDPO [2] is a recent image diffusion alignment method that uses policy gradients to adapt diffusion model weights. Specifically, it applies PPO algorithm [25] to the diffusion denoising process. We extend their code for adapting video diffusion models.\n\u2022 Diffusion-DPO [30] extends the recent development of Direct Preference Optimization (DPO) [22] in the LLM space to image diffusion models. They show that directly modeling the likelihood using the preference data can alleviate the need for a reward model. We extend their implementation to aligning video diffusion models, where we use the reward model to obtain the required preference data.\nReward models. We use the following reward models to fine-tune the video diffusion model.\n\u2022 Aesthetic Reward Model: We use the LAION aesthetic predictor V2 [24], which takes an image as input and outputs its aesthetic score in the range of 1-10. The model is trained on top of CLIP image embeddings, for which it uses a dataset of 176,000 image ratings provided by humans ranging from 1 to 10, where images rated as 10 are classified as art pieces.\n\u2022 Human Preference Reward Models: We use HPSv2 [32] and PickScore [16], which take as input an image-text pair and predict human preference for the generated image. HPSv2 is trained by fine-tuning CLIP model with a vast dataset that includes 798,090 instances of human preference rankings among 433,760 image pairs, while PickScore [16] is trained by fine-tuning CLIP model with 584,000 examples of human preferences. These datasets are among the most extensive in the field, offering a solid foundation for enhancing image-text alignment.\n\u2022 Object Removal: We design a reward model based on YOLOS [8], a Vision Transformer based object detection model trained on 118,000 annotated images. Our reward is one minus the confidence score of the target object category, from which video models learns to remove the target object category from the video.\n\u2022 Video Action Classification: While the above reward models sit on individual images, we employ a reward model that takes in the whole video as input. This can help with getting gradients for the temporal aspect of video generation. Specifically, we consider VideoMAE"}, {"title": "5.1 Sample and Computational Efficiency", "content": "Training of large-scale video diffusion models is done by a small set of entities with access to a large amount of computing; however, fine-tuning of these models is done by a large set of entities with access to a small amount of computing. Thus, it becomes imperative to have fine-tuning approaches that boost both sample and computational efficiency.\nIn this section, we compare VADER's sample and computational efficiency with other reinforcement learning approaches such as DDPO and DPO. In Figure 7, we visualize the reward curves during training, where the x-axis in the upper half of the figure is the number of reward queries and the one in the bottom half is the GPU-hours. As can be seen, VADER is significantly more efficient in terms of sample and computation than DDPO or DPO. This is mainly due to the fact that we send dense gradients from the reward model to the diffusion weights, while the baselines only backpropagate scalar feedback."}, {"title": "5.2 Generalization Ability", "content": "We split the prompts into train and test sets, such that the prompts in the test set do not have any overlap with the ones for training. We find that VADER achieves the best on both metrics."}, {"title": "5.3 Human Evaluation", "content": "We carried out a study to evaluate human preferences via Amazon Mechanical Turk. The test consisted of a side-by-side comparison between VADER and ModelScope. To test how well the videos sampled from both the models aligned with their text prompts, we showed participants two videos generated by both VADER and a baseline method, asking them to iden-tify which video better matched the given text. For evaluating video quality, we asked participants to compare two videos generated in response to the same prompt, one from VADER and one from a baseline, and decide which video's quality seemed higher. We gathered 100 responses for each comparison. The results, illustrated in Table 2, show a preference for VADER over the baseline methods."}, {"title": "5.4 Qualitative Visualization", "content": "In this section, we visualize the generated videos for VADER and the respective baseline. We conduct extensive visualizations across all the considered reward functions on various base models.\nHPS Reward Model: In Figure 3, we visualize the results before and after fine-tuning VideoCrafter using both HPSv2.1 and Aesthetic reward function together in the top three rows. Before fine-tuning, the raccoon does not hold a snowball, and the fox wears no hat, which is not aligned with the text description; however, the videos generated from VADER does not result in these inconsistencies. Further, VADER successfully generalizes to unseen prompts as shown in the third row of Figure 3, where the dog's paw is less like a human hand than the video on the left. Similar improvements can be observed in videos generated from Open-Sora V1.2 and ModelScope as shown in the second and third rows of Figure 6.\nAesthetic Reward Model: In Figure 3, in the top three rows we visualize the results before and after fine-tuning ModelScope using a combination of Aesthetic reward function and HPSv2.1 model. Also, we fine-tune ModelScope via Aesthetic Reward function and demonstrate its generated video in the last row in Figure 6. We observe that Aesthetic fine-tuning makes the generated videos more artistic.\nPickScore Model: In the bottom three rows of Figure 3, we showcase videos generated by PickScore fine-tuned VideoCrafter. VADER shows improved text-video alignment than the base model. In the last row, we test both models using a prompt that is not seen during training time. Further, video generated from PickScore fine-tuned Open-Sora is displayed in the first row of Figure 6.\nObject Removal: Figure 5 displays the videos generated by VideoCrafter after fine-tuning using the YOLOS-based objection removal reward function. In this example, books are the target objects for removal. These videos demonstrate the successful replacement of books with alternative objects, like a blanket or bread.\nVideo Action Classification: In Figure 8, we visualize the video generation of ModelScope and VADER. In this case, we fine-tune VADER using the action classification objective, for the action specified in the prompt. For the prompt, \"A person eating donuts\", we find that VADER makes the human face more evident along with adding sprinkles to the donut. Earlier generations are often misclassified as baking cookies, which is a different action class in the kinetics dataset. The addition"}, {"title": "6 Conclusion", "content": "We presented VADER, which is a sample and compute efficient framework for fine-tuning pre-trained video diffusion models via reward gradients. We utilized various reward functions evaluated on images or videos to fine-tune the video diffusion model. We further showcased that our framework is agnostic to conditioning and can work on both text-to-video and image-to-video diffusion models. We hope our work creates more interest towards adapting video diffusion models."}]}