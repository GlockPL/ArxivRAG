{"title": "NeuroAI for AI Safety", "authors": ["Patrick Mineault", "Niccol\u00f2 Zanichelli", "Joanne Zichen Peng", "Anton Arkhipov", "Eli Bingham", "Julian Jara-Ettinger", "Emily Mackevicius", "Adam Marblestone", "Marcelo Mattar", "Andrew Payne", "Sophia Sanborn", "Karen Schroeder", "Zenna Tavares", "Andreas Tolias"], "abstract": "As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.", "sections": [{"title": "Introduction", "content": "AI systems have made remarkable advances in fields as diverse as game-playing [1, 2, 3], vision [4, 5], autonomous driving [6], medicine [7], protein folding [8, 9], natural language processing and dialogue [10, 11], mathematics [12], weather forecasting [13, 14] and materials science [15]. The advent of powerful AI has raised concerns about AI risks, ranging from issues with today's systems like climate impact [16, 17], systematic bias [18, 19], and mass surveillance [20], to future-looking issues like the misuse of powerful Al by malicious agents [21], accidents stemming from the misspecification of objectives [22, 23], catastrophic risk from autonomous systems [24, 25], and race dynamics among AI companies promoting the release of unsafe agents [26].\nAl safety is a field of research aimed at developing AI systems that are helpful to humanity, and not harm- ful. Because AI is a general-purpose technology [27], AI safety research is fundamentally interdisciplinary, cutting across research in computer science and machine learning, mathematics, psychology, economics, law, and social science. Technical AI safety is a subset of AI safety that focuses on finding technical solutions to safety problems, distinguishing it from other areas such as policy work [23].\nWe can broadly categorize these safety issues into two levels, depending on the capability of AI systems and time horizon:\n\u2022 Immediate safety concerns from today's prosaic AI systems. By prosaic AI systems, we mean non- autonomous systems with limited capacity. This includes widely deployed systems like large language models and image generators [28], as well as more specialized systems used in medicine, policing, and military applications, among others. Bias, amplification of societal issues like algorithmic policing, unequal access, interference in the political process and climate impacts, and the misuse of generative AI for creating fake content are commonly cited short-term safety concerns [29].\n\u2022 Long-term safety concerns from future agentic AI systems. By agentic AI systems, we mean partially or fully autonomous systems with a broad range of capabilities. This may include future systems with physical embodiments, including robotics, autonomous vehicles, aerial drones and wet-lab autonomous scientists [30], as well as purely digital agents such as virtual agents interacting in sandboxes [31], virtual research assistants [32], virtual scientists [33], and software engineering agents [34, 35]. These future agents, far more capable than current systems, could be far more valuable for society. Yet they also present dual-use concerns, including malicious use by state and non-state agents, use in military applications, organizational risks, as well as the possibility of losing control over advanced agents which pursue objectives and goals that can be harmful for humanity [36]. Highly capable agentic Al systems are sometimes referred to as general-purpose artificial intelligence, or artificial general intelligence (AGI).\nAlthough immediate safety is critical for AI to benefit society [37], our primary focus here is on safety concerns related to future agentic AI. As stated in a recent report chaired by Yoshua Bengio [24]:\n\"The future of general-purpose AI technology is uncertain, with a wide range of trajectories appearing possible even in the near future, including both very positive and very negative outcomes. But nothing about the future of AI is inevitable.\"\nLong-term Al safety is a sufficiently important societal problem [25] that it deserves multidisciplinary consideration, including from neuroscientists. In this roadmap, we aim to evaluate and draw a path for how inspiration, data, and tools from neuroscience can positively impact AI safety."}, {"title": "1.1 What can a neuroscientist do about AI safety?", "content": "Animals navigate, explore, and exploit their environments while maintaining self-preservation. Among these, mammals, birds, and cephalopods exhibit particularly flexible perceptual, motor, and cognitive systems [38] that generalize well to out-of-distribution inputs, meaning they can effectively handle situations or stimuli that differ significantly from what they have previously encountered.\nHumans have evolved additional capacities for cooperation and complex social behavior [39], organizing themselves into societies that promote prosocial conduct and discourage harmful actions. These capacities emerge, in part, from the neural architecture of the brain. Evolution has shaped the brain to impose strong constraints on human behavior in order to enable humans to learn from and participate in society. By understanding what those constraints are and how they are implemented, we may be able to transfer those lessons to Al systems. We could build systems that exhibit familiar human-like intelligence, which we have experience dealing with. It follows that studying the brain and understanding the biological basis of this natural alignment is a promising route toward AI safety.\nWe use the technical framework introduced by Deepmind in 2018 [40] to more concretely categorize how studying the brain could positively impact AI safety (Figure 1).\n1. Robustness: specifying how an agent can safely respond to unexpected inputs. This includes perform- ing well or failing gracefully when faced with adversarial and out-of-distribution inputs, and safely exploring in unknown environments. This can also mean learning compositional representations that generalize well out-of-distribution. Robustness further implies knowing what you do not know, by maintaining a representation of uncertainty, to ensure safe and informed decision-making in novel or uncertain scenarios.\n2. Specification: specifying the expected behavior of an Al agent. A pithy way of expressing this is that we want AI systems to \u201cdo what we mean, not what we say\". This includes correctly interpreting instructions specified in natural language despite ambiguity; preventing learning shortcuts that gen- eralize poorly [41]; ensuring that agents solve the real task at hand rather than engaging in reward hacking [23] (i.e. Goodhart's law); and so on.\n3. Assurance (or oversight): being able to verify that AI systems are working as intended. This includes opening the black box of AI systems using interpretability methods; scalably overseeing the deployment of AI systems and detecting unusual or unsafe behavior; or detecting and correcting for bias.\nThere have been a few examples where neuroscience has already positively impacted AI safety, which fit neatly into this framework. For example, interpretability methods inspired by neuroscientific approaches [42] are a form of assurance, while methods which seek inspiration from the brain to find solutions to adversarial attacks can enhance robustness [43, 44, 45, 46, 47, 48, 49, 50, 51, 52].\nThroughout this roadmap, we'll encounter several more proposals which are aimed at solving specific technical issues within AI safety under one of these rubrics. Some proposals, however-for example, detailed biophysical simulations of the human brain or top-down simulations from representations learned from neural data-seek to benefit AI safety by emulating human minds and all their safety-relevant properties, thus affecting all the relevant rubrics: robustness, specification and assurance. We'll note them as primarily aiming to build simulations of the human mind in the following."}, {"title": "Box 1: Marr's levels for AI safety", "content": "At what level should we study the brain for the purpose of AI safety? The different proposals we evaluate make very different bets on which level of granularity should be the primary focus of study. Marr's levels [53] codify different levels of granularity in the study of the brain:\n1. The computational level. What is the high-order task that the brain is trying to solve? Reverse engineering the loss functions of the brain and building better cognitive architectures map onto this level.\n2. The algorithmic (or representation) level. What is the algorithm that the brain uses to solve that problem? Alternatively, what are the representations that the brain forms to solve that problem? Approaches including brain-informed process supervision and building digital twins of sensory systems map onto this level.\n3. The implementation level. How is this problem solved by the brain? Biophysically detailed whole- brain simulation falls into that category, while embodied digital twins straddle the algorithmic and implementation levels.\nFor the purpose of AI safety, any one level is unlikely to be sufficient to fully solve the problem. For example, solving everything at the implementation level using biophysically detailed simulations is likely to be many years out, and computationally highly inefficient. On the other hand, it is very difficult to forecast which properties of the brain are truly critical in enhancing AI safety, and a strong bet on only the computational or algorithmic level may miss crucial details that drive robustness and other desirable properties.\nThus, we advocate for a holistic strategy that bridges all of the relevant levels. Importantly, we focus on scalable approaches anchored in data. All of these levels add constraints to the relevant problem, ultimately forming a safer system."}, {"title": "1.2 Proposals for neuroscience for AI safety", "content": "There have been several proposals for how neuroscience can positively impact AI safety. These span from emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems by imitating brain and body structure, activity, and behavior; fine-tuning AI systems on brain data or learning loss functions from it; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We list them in Table 1, along with which aspect of AI safety they propose to affect.\nMany of these proposals are in embryonic form, often in grey literature-whitepapers, blog posts, and"}, {"title": "2 Reverse engineer the representations of sensory systems", "content": "AI systems need robust sensory systems to be safe. Sensory systems in current AI systems are brittle: they are susceptible to adversarial examples [43], they learn slowly [62], they can fail catastrophically out-of-distribution [63], they tend to rely on shortcuts that generalize poorly [45], and they don't display compositionality [64]. By contrast, sensory systems in the brain are robust [65]. If we could reverse engineer how sensory systems form these robust representations, we could embed them in AI systems, enhancing their safety. Sensory digital twins are large-scale neural networks trained to predict neural responses across a wide range of sensory inputs [66, 67, 68]. We evaluate reverse engineering the representations of sensory systems in model systems using sensory digital twins as an intermediate."}, {"title": "2.2 Why does it matter for AI safety and why is neuroscience relevant?", "content": ""}, {"title": "2.2.1 Adversarial robustness", "content": "Adversarially crafted examples can cause AI systems to misbehave, and create an attack vector for malicious actors. Small perturbations to input data that are imperceptible to humans can cause otherwise highly capable models to make catastrophic errors in their predictions. While the most potent adversarial attacks leverage access to model internals, vulnerabilities still remain in the black-box setting [69]. Attackers can leverage decision-based boundary attacks, which iteratively adjust inputs based on model outputs to approximate decision boundaries in black-box settings [70]. These methods expose AI to vulnerabilities even without internal access, highlighting the risks of adversarial exploitation. Vulnerabilities of this kind have justifiably raised serious concerns, particularly at a time when AI systems are increasingly deployed in sensitive real-world contexts, including in autonomous settings [71].\nThe problem of adversarial robustness is not confined to any single domain of AI. Early results focused on image classification models, following the discovery that deep neural networks could be fooled through"}, {"title": "2.2.2 Out-of-distribution (OOD) generalization", "content": "Out-of-distribution inputs refer to data that are significantly different from what a machine learning model has been trained on. These inputs are especially prevalent in autonomous agents that navigate and explore environments on their own. If not properly designed, such agents can fail catastrophically when encountering unfamiliar inputs. In contrast, humans display adaptive behaviors even in entirely new situations, including those requiring zero-shot (no prior exposure) or few-shot (minimal prior exposure) learning. This adaptability is often attributed to our ability to recognize and combine familiar components in new ways, a capability known as finding and exploiting compositional representations [77], and to learn representations of the world that disentangle its causal variables. For example, an agent that has disentangled texture, color, and shape can correctly classify an object with unique combinations of variables it has never encountered before, such as a pink elephant.\nFoundation models have alleviated some of these concerns by pretraining on massive, internet-scale datasets, effectively incorporating previously unseen scenarios into their training distribution. However, this approach is impractical for covering all possible out-of-distribution scenarios because it would require training on an exponentially large number of cases [78]. For example, training a self-driving car to handle every possible road condition, weather scenario, pedestrian behavior, and vehicle type would require simulating or collecting data from an unimaginably vast number of situations. Rare edge cases, such as a child running into the street during a snowstorm while an autonomous car encounters a malfunctioning traffic light, are extremely costly and difficult to capture comprehensively in a dataset. Similar to the challenge of achieving adversarial robustness, ensuring robustness to out-of-distribution inputs remains an unsolved problem with important safety implications."}, {"title": "2.2.3 Specification alignment and simulating human sensory systems", "content": "Current systems view the world and sense the world in different ways than humans. This can pose a safety risk in out-of-distribution situations, where specifications written by a human are misinterpreted by an AI that lacks the primitives of the human mind (e.g., understanding causality, context, or social norms) [79]. Autonomous AI agents will need to be able to simulate how their actions affect the world to safely explore the world and act inside of it [80]. AI agents will thus need to engage in perspective-taking, simulating how a human would react to a particular set of sensory inputs in a model-based fashion [81]. Again, building good models of human sensory systems is a stepping stone toward safe human-AI interactions."}, {"title": "2.2.4 Bridging neuroscience and AI through digital twins", "content": "To solve the problem of reverse engineering sensory processing, a natural intermediate milestone is to build a model that can account for responses of neurons to arbitrary stimuli. A sensory digital twin is trained to learn the relationship between stimuli and the resulting neural response as well as how these sensory representations are modulated by motor variables and brain states [82, 68, 83]. If trained with enough data, the model can be used to simulate the neural response to data never seen by the animal [66], allowing researchers to simulate, parallelize and scale experiments in silico that would be impossible in vivo. Sensory digital twins have been used to uncover how neurons in the visual cortex adapt their tuning selectivity to changing brain states [83]; how they integrate local and contextual information to optimize information processing [84]; and to systematically characterize single-neuron invariances [85].\nCritically, digital twins are built using artificial neural networks, allowing the use of mechanistic inter- pretability (Section 8) to understand how they function. For example, if we were to create a digital twin of the entire primate visual brain, we could investigate how it constructs adversarially and distributionally robust representations that are useful for behavior. In the following, we turn our attention to the feasibility of building digital twins given current technology."}, {"title": "2.3 Details", "content": ""}, {"title": "2.3.1 Sensory digital twins", "content": "Foundation models of the brain, whole-brain simulations, and digital twins are sometimes conflated. For the purpose of this discussion, we use the following definitions (Figure 4):\n1. Sensory digital twins seek to model an animal's sensory systems, potentially with auxillary task- relevant inputs such as cognitive and motor state, at the level of representations. These are the topic of this section.\n2. Embodied digital twins seek to model an entire animal, including its sensory, cognitive and motor systems, its body, and its relationship to the environment. An embodied digital twin could contain a sensory digital twin. We cover embodied digital twins in Section 3.\n3. Biophysically detailed models seek to model nervous systems from the bottom-up, with detailed simulations that may include biophysically detailed neuron models and connectomes. These are covered in Section 4.\n4. Foundation models of the brain are Al models trained at a large scale, usually using self-supervised or unsupervised learning, that seek to find good representations of neural data\u2013which may include"}, {"title": "2.3.3 Scaling laws for cores vs. readouts", "content": "The scaling laws in the previous section focused on the feasibility of building a digital twin of a single neuron, which depends on effectively learning the readout. However, the artifact we really care about is the core, a distillation of the processing within an area, which can be learned by stitching data together across neurons, animals and sessions. In the particular implementation of digital twins we discussed so far, the tuning of the neurons is embedded in a latent space that learns the non-linear relationships between neural responses and sensory input and other variables like motor responses. Each neuron's response is then modeled as a linear readout of the core, where the readout is learned separately for each neuron. How do cores scale with data? In particular, how much data do we need to learn good cores, and how does this affect the scaling laws for learning accurate models of individual neurons?\nTo disentangle core and readout scaling, we turn to simulations. We start with a simulation where the correct core is known a priori: it is simply the identity function over the inputs. In this case, all we need to learn are the weights of the readouts, and we're in a situation equivalent to multivariate Poisson regression. We generate random design matrices and random weights for linear-nonlinear-Poisson (LNP) neurons, estimate maximum a posteriori weights (MAP) under Tikhonov regularization constraints, and estimate the FEVE in a validation dataset.\nWe find that scaling laws for these models are qualitatively and quantitatively well explained by a sigmoid as a function of log recording time and log number of parameters in the readout:\n$$FEVE = \\rho (a \\cdot log(t) + b \\cdot log(readout \\ params) + c)$$\nOn a log-linear scale, increasing the dimensionality of the core, and consequently of the readout weights, shifts the curves to the right, as more data is needed to fit the readout weights. We derive a mathematical expression for the functional form of scaling laws for linear regression which matches the log-sigmoid scaling laws we empirically find here (see Appendix).\nWhat happens when the core is incorrect? Imagine, for example, that a neuron in V1 is selective for the sign of an edge, but displays some translation invariance. In other words, this neuron displays properties that place it somewhere between the classic orientation-and-phase-selective simple cell and a phase-invariant complex cell. If we tried to learn a linear mapping from an image's luminance values to this neuron's responses, we would not be able to fully account for the responses of the neuron no matter how much data we train on; we'd need a nonlinear mapping.\nWe simulate this effect by partitioning the design matrix into two components: one part which is known, and for which we can estimate linear weights as before; and a second component which is unknown, and for which we cannot estimate weights by construction. The effect is shown in Figure 7 (right). The takeaway is that using the wrong core scales down the entire curve, capping the maximum attainable FEVE. A secondary effect is that using a bad core delays learning, as the unaccounted component of a neuron's response decreases its effective signal-to-noise ratio."}, {"title": "2.3.4 Feasibility of transferring robustness from brains to models", "content": "We have seen that it's practically feasible to build digital twins that accurately predict neural responses within their training distribution. Since primates and humans are robust to out-of-distribution shifts, and they are not sensitive to adversarial examples [44, 129, 130, 131, 46, 47], it would stand to reason that distilling neural data should lead to adversarially and distributionally robust neural networks. If this was the case, one could potentially use these robust digital twins either as adversarially robust networks for classification purposes, or as a means of reverse engineering adversarial robustness.\nTo the best of our knowledge, a direct approach-simply training a neural network to imitate neural data at scale and testing its adversarial robustness\u2013has not been tried. However, several references instead use neural data to regularize networks trained for image classification. They report that regularization with neural data leads to higher adversarial or distributional robustness in the domain of natural images (Table 2 with summary of results). We refer to these methods collectively as neural data augmentation (NDA). These prior results give valuable insight into whether distilling robust representations from neural data to digital twins is feasible."}, {"title": "2.4 Evaluation", "content": "Digital twins of perceptual systems could potentially contribute towards safer AI systems by:\n1. Allowing one to predict a subject's perception in reaction to a physical stimulus, which could facilitate human-Al interactions\n2. Distilling more robust representations than is currently feasible with data augmentation and adversarial training alone\n3. Enabling virtual and closed-loop experiments that tease apart the circuit mechanisms, inductive biases, and representation geometry underlying robust perception (see also Section 8 for a discussion on mechanistic interpretability)\nScaling laws indicate that building digital twins that account for a large fraction of the explainable variance in a particular area is likely feasible with current technology in model species, while the path to scale up to humans remains more speculative. The data used to train digital twins may be useful to enhance adversarial robustness in existing neural networks; furthermore, digital twins as an intermediate step to denoise activations in neural data augmentation may lead to more effective regularization than the direct approach of using noisy neural data directly as a regularizer. To move beyond proofs of concepts, we identify one bottleneck, the measurement of neural responses to adversarial stimuli. We also single out better understanding the geometry of human robustness to adversarial stimuli using large-scale psychophysics as an important bottleneck to improving adversarial robustness [47]."}, {"title": "2.5 Opportunities", "content": "\u2022 Create large-scale neural recordings from sensory areas in response to rich spatiotemporal stimuli\n* Focus on bringing down the required single-neuron recording times to reach high FEVE within a 3-4 hour recording session\n* Improve efficiency of transfer function estimation by learning compact cores, or by using sparse readout mechanisms\n* Scale promising chronic recording technologies to allow recording beyond the 3-4 hour single- session limit\n\u2022 Build sharable, composable digital twins of sensory systems\n* Measure and report scaling laws on an apples-to-apples basis, for both single-neuron transfer function estimation and the ceiling attainable by a core\n* Share pretrained models in a standardized format to bootstrap the creation of fine-tuned models\n* Expand beyond vision to other modalities, including audition and somatosensation and more motor variables\n\u2022 Build robust digital twins\n* Measure in-vivo neural responses to adversarial stimuli in a closed-loop fashion\n* Build robust digital twins through adversarial training and anchoring to measured neural re- sponses to adversarial stimuli\n* Estimate the geometry of adversarial robustness in humans through large-scale, online psy- chophysics\n* Track and report robustness of digital twins to adversarial and out-of-distribution stimuli across a range of relevant attack dimensions (e.g. L\u221e and L2 robustness, distributional shifts)\n* Compare against state-of-the-art defenses rather than against non-robust training"}, {"title": "Box 3: Scaling trends for neural recordings", "content": "Many of the approaches to NeuroAI safety that we discuss are dependent on the continued creation of ever-larger datasets with more capable recording techniques. For instance, under log-linear scaling laws, to obtain linear improvements in performance over time, one would need to obtain datasets that grow exponentially over the same time. Stevenson [137] first documented a Moore's-law-like relationship for recording capabilities in electrophysiology, estimating that simultaneously recorded neurons double every 7 years; Stevenson recently updated his estimate at 6 years. Urai et al. [138] extended these results to calcium imaging; while the growth was not quantified, the trend pointed toward exponentially faster improvements in calcium imaging.\nWe extended these results to update these scaling trends using an LLM-based pipeline to identify relevant papers (see methods for details) and joining them with previous databases collected by Stevenson and Kording and Urai et al. Fitting a Bayesian linear regression to the log number of neurons for recordings after 1990, we obtained an estimated 5.2\u00b10.2 year doubling time for electrophysiology and 1.6\u00b10.2 year doubling time for imaging.\nOut of the 10 electrophysiology studies reporting the largest simultaneous neuron numbers recorded, 9 were performed with multi-shank Neuropixel recordings. Light bead microscopy is currently the state-of-the-art for calcium imaging [139]. If current trends continue, we predict that in 2035, 10,000 neuron electrophysio- logical recordings will be commonplace, whereas calcium imaging should reach 10M neurons. A concerted investment in neurotechnology could break these incremental trends, in particular for electrophysiology. These trends are important to contextualize how digital twins will evolve; indeed, scaling laws for digital twins suggest linear improvements in our ability to predict neural activity with exponential increases in channel count. However, this analysis does not assess the signal-to-noise ratio (SNR) of different recording technologies. While this is more or less constant in electrophysiology, SNR can vary widely in imaging depending on the amount of out-of-focus light and the size of the focus area (e.g. one-photon vs. multiphoton imaging approaches). Thus, it's likely that we'll see optimizations in terms of signal-to-noise ratio rather than increasing channel count as we approach whole-brain coverage with imaging."}, {"title": "3 Build embodied digital twins", "content": ""}, {"title": "3.1 Core idea", "content": "In the previous section, we saw a path toward building sensory digital twins that learn the transfer function between sensory inputs and brain state. Could we leverage similar ideas to build a digital twin of the entire brain and body at the functional level? Embodied digital twins aim to model human behavior, embodiment and neural activity at a coarse level without necessarily replicating intricate neural circuits. If they are accurate models of the humans which they aim to imitate, we may derive relevant safety properties from them. This includes the ability to safely explore the world and control their bodies, as well as display the same inductive biases as humans.\nAn embodied digital twin could be built with a patchwork of approaches: leveraging a pre-existing sensory digital twin; learning a model for the brain and body through self-supervised learning on neural activity and behavior, acting as a base controller; embodying that model in a virtual environment; and fine-tuning the controller in silico in virtual environments. Embodied digital twins would be built with many of the same building blocks as conventional AI systems, which could facilitate synergistic interactions between AI and neuroscience [140]. We evaluate the technical feasibility of embodied digital twins here."}, {"title": "3.2 Why does it matter for AI safety and why is neuroscience relevant?", "content": "Building a biophysically detailed simulation of people at the brain and body level has long been entertained as a speculative, brute-force path toward artificial general intelligence [54, 22]. In the AI safety community, biophysically detailed bottom-up simulations of neural activity are often termed WBEs-whole brain emula- tions [54, 22]. Bostrom (2014) lists three reasons why biophysically detailed simulations are a safer path to AGI than alternatives:\n1. Mutual understanding. Because simulations are derived from humans, with whom we have extensive experience, we should be able to reason about their behavior, in the same way we leverage theory- of-mind and empathy to reason about humans. Furthermore, they should be able to reason about humans.\n2. Inheriting human values. Because simulations are derived from humans, they should have similar motivations and values as humans.\n3. Slower takeoff. Because biophysically detailed simulation is a brute-force approach to AGI, it requires massive investments in neural recording and scanning hardware. We should be better able to forecast when biophysically detailed simulations are likely to arrive and prepare accordingly, compared with alternative paths which could be massively accelerated through scientific insight.\nIt's easy to find flaws in each of these lines of reasoning. We may not be able to reason about simulations that are run at much faster speeds than humans. Inheriting human motivations and values can be a double- edged sword, as these may include undesirable properties like aggression and power-seeking. And takeoff could be faster than anticipated if coarser-grained simulations can accelerate subsequent steps in building the next generation of simulations.\nHowever, simulations need only be relatively safer than alternative paths toward artificial general intelli- gence for them to represent an improvement over the status quo [141]. Other desirable properties of human"}, {"title": "3.3 Details", "content": ""}, {"title": "3.3.1 Defining embodied digital twins", "content": "What do we mean by an embodied digital twin? In a footnote, Bostrom and Sandberg [54] define an effective simulation of an embodied nervous system as being able to predict the future state of the brain and body x(t) at all future times t > To given the state of the system at the current time, x(To), within an e bound. The state can be defined at different levels of abstractions, but can include:\n\u2022 Neural activity, e.g. membrane voltage or spike rate\n\u2022 Auxiliary state variables related to neural activity, e.g. a neuron's relative refractory state, the state of neuromodulators, etc.\n\u2022 The position of the limbs, the load on each muscle, and their velocity; collectively, behavior\n\u2022 The system's connectome, reflecting the accumulated memories of the organism\nAn alternative implementation conditions the simulation not just on the current time step To but on several time steps in the past t \u2264 To, which is attractive from the point of view of Takens' theorem [143]. By that yardstick, both embodied digital twins and biophysically detailed models attempt to build simulations of the brain and body at different levels of granularity. An embodied digital twin primarily leverages behavior, functional neural recordings and body measurements to build a simulation from the top-down, while a biophysically detailed model primarily leverages structural recordings and detailed biophysical modeling to build a simulation from the bottom-up.\nSeveral recent proposals and papers help clarify what an embodied digital twin might look like:\nAI animal models and the embodied Turing Test Zador et al. [144] propose to build a simulation of an entire animal in silico\u2013an AI animal model. Comparisons between real and virtual animals would provide a readout of how well a simulation performs. A virtual animal would pass the 'embodied Turing test' if it is indistinguishable from its living counterpart when observed in a virtual environment.\nThe virtual rat Merel et al. [145] demonstrated that training a virtual rat using reinforcement learning in a virtual environment could display rich behavior. Aldarondo, et al. [146] trained an ANN to actuate a biomechanical model of a more advanced version of this virtual rat. When the virtual agent was tasked with"}, {"title": "3.3.4 Bridging the causal and real-to-sim gap", "content": "Even with observational data from brains and behavior, as well as virtual bodies, measurements can only offer partial observations of a system. Not all neurons can be recorded simultaneously, and those that are recorded may not capture all the relevant neural dynamics. Simulations of completely transparent neural networks that implement the functions of interest, e.g., sensorimotor control of complex bodies, can help guide us on how to best sample from real brains [216] and capture the posture and movement of bodies.\nA related issue is that datasets rarely cover all possible situations or behaviors an organism can exhibit. Just as we have seen with self-driving cars, rare but critical scenarios might be underrepresented, leading models to fail in unanticipated ways [217]. Since graceful out-of-distribution failure is one of the desiderata of safe AI systems, the generalizability and robustness of embodied digital twins are critical. Several groups have demonstrated empirical results that show that more constraints\u2014e.g. using higher entropy data or constraining the data to be consistent with a connectome [218]\u2014lead to more robust generalization. A mathematically precise theory of the stability of simulations, however, is currently lacking.\nOne avenue toward better generalization is to build causal models of neural activity [219, 220]. Researchers have begun to explore this approach with C. elegans [154], stimulating all neurons and measuring the resulting outputs, in an effort to characterize neural dynamics and behavior. Unlike models that build a regressor from observational data\u2014which can confound cause and effect\u2014these approaches predict how neural activity and behavior is causally related to changes in the organism's senses or neural activity. While this is a promising direction, applying causal analysis to larger organisms is highly non-trivial.\nEven in the best of circumstances, we will likely need to deal with a sim-to-real gap [221]: the virtual environment can never fully replicate the real world, leading to discrepancies in simulated sensory inputs and motor outputs. To mitigate this, models can be adapted or fine-tuned within the virtual environment. This may involve training the virtual animal through supervised learning, imitation learning, or reinforcement learning to perform tasks that are underrepresented in the observational data. This is conceptually similar to the route by which LLMs are built for chat: pretraining, supervised fine-tuning, and reinforcement learning from human feedback [222]. Designing appropriate training curricula will be essential to expose the model to a wide range of scenarios, improving its robustness and generalizability."}, {"title": "3.3.5 Defining success criteria for embodied digital twins", "content": "How would we know if we achieved success in building an embodied digital twin? One possibility, taking a page from Bostrom & Sandberg, is to use autoregressive prediction as a target. For instance, we could measure a real animal's behavior and neural activity, virtualize the animal, and predict, in the virtual environment, its future activity conditioned on the past [146]. Appropriate metrics could include, e.g. root-mean-square error (RMSE) in predicting the position and angles of the limbs, or the neural activity.\nWhile this straightforward approach seems intuitively appealing, it has some significant drawbacks. There are many reasons why a simulation could fail on a metric basis, not all of which are due to model failure. This includes the difficulty in digitizing the animal and its environment with perfect accuracy, as well as the fact that the systems to be simulated sit at the edge of chaos [223]. These could lead to the divergence of even very capable models. Like the weather, neural activity and behavior might not be predictable in the RMSE sense over a long time horizon [224]."}, {"title": "3.4 Evaluation", "content": "Embodied digital twins intersect with multiple ongoing trends in neuroscience and AI:\n\u2022 Foundation models: training large-scale models for autoregressive generation\n\u2022 Foundation models for neuroscience: training foundation models specifically on neuroscientific data", "neuroscience": "measuring the activity of a significant proportion of the"}]}