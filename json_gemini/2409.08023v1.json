{"title": "Edge-Wise Graph-Instructed Neural Networks", "authors": ["Francesco Della Santa", "Antonio Mastropietro", "Sandra Pieraccini", "Francesco Vaccarino"], "abstract": "The problem of multi-task regression over graph nodes has been recently approached through Graph-Instructed Neural Network (GINN), which is a promising architecture belonging to the subset of message-passing graph neural networks. In this work, we discuss the limitations of the Graph-Instructed (GI) layer, and we formalize a novel edge-wise GI (EWGI) layer. We discuss the advantages of the EWGI layer and we provide numerical evidence that EWGINNs perform better than GINNs over graph-structured input data with chaotic connectivity, like the ones inferred from the Erdos-R\u00e9nyi graph.", "sections": [{"title": "1. Introduction", "content": "Graph Neural Networks (GNNs) are powerful tools for learning tasks on graph-structured data [1], such as node classification [2], link prediction, or graph classification. Their formulation traces back to the late 2000s [3, 4, 5]. In the last years, GNNs have received increasing attention from the research community for their application in biology [6], chemistry [7, 8], finance [9], geoscience [10], computational social science [11], and particle physics [12], to name a few. Yet, the community has neglected the applications concerning the Regression on Graph Nodes (RoGN) learning task. Indeed, to the best of the authors' knowledge, the most used benchmarks do not include datasets for this task [13, 14]. In particular, RoGN can be stated as multi-task regression, where the input data are endowed with a graph structure.\nThe benchmark models for multi-task regression are Fully Connected Neural Networks (FCNNs). Recently, a new type of layer for GNNs has been developed in [10], belonging to the class of message-passing GNNs [8]. From now on we will refer to these layers as Graph-Instructed (GI) layers; Graph-Instructed NNs (GINNs) are built by stacking GI layers. GINNs have demonstrated good performance on RoGN, showing better results than FCNNs, as illustrated in [10]. Although the GINN architecture has been specifically designed for RoGN, the usage of GI layers has been recently extended to supervised classification tasks (see [15]).\nWe point the reader to the fact that in [10] GI layers and GINNs are denoted as Graph-Informed layers and Graph-Informed NNs, respectively. In [16], in a different framework from the one addressed in [10], a homonymous but different model is presented; therefore, to avoid confusion with [16], we have changed the names of both layers and NNs.\nGI layers are based on a weight-sharing principle, such that their weights rescale the outgoing message from each node. In this paper, to improve the generalization capability of their inner-layer representation, we introduce Edge-Wise Graph-Instructed (EWGI) layers, characterized by additional weights (associated with graph nodes) that enable the edge-wise customization of the passage of information to each receiving node.\nWe compare the Edge-Wise GINN (EWGINN) with the GINN in the experimental settings originally used in [10] for validating the models; these settings are two RoGN tasks on two stochastic flow networks based on a Barab\u00e1si-Albert graph and an Erdos-R\u00e9nyi graph, respectively. In particular, we show that EWGINNs perform better on the Erdos-R\u00e9nyi connectivity structure, with a small increment of the number of learning weights.\nThe work is organized as follows: in Section 2 the GI layers are introduced, recalling their inner mechanisms. Section 3 formally defines EWGI layers and theoretically discusses their properties. Then, in Section 4, we analyze the experiment results for the RoGN tasks, comparing with the previous literature [10]. Finally, Section 5 summarizes our work and discusses future improvements and research directions."}, {"title": "2. Graph-Instructed Layers", "content": "This section briefly reviews previous GINNs to establish the framework for introducing our main contribution. Graph-Instructed (GI) Layers are NN layers defined by an alternative graph-convolution operation introduced in [10]. Given a graph G (without self-loops) and its adjacency matrix A \u2208 R^{n\u00d7n}, a basic GI layer for G is a NN layer with one input feature per node and one output feature per node described by a function L^{GI} : R^n\u2192R^n such that\nL^{GI}(x) = \u03c3 ((diag(w)(A + I_n)) x + b), (1)\nfor each vector of input features x \u2208 R^n and where:\n\u2022 w\u2208 R^n is the weight vector, with the component w_i associated to the graph node v_i, i = 1, . . ., n.\n\u2022 diag(w) \u2208 R^{n\u00d7n} is the diagonal matrix with elements of w on the diagonal and I_n \u2208 R^{n\u00d7n} is the identity matrix. For future reference, we set W:= diag(w)(A + I_n);\n\u2022 \u03c3: R^n \u2192 R^n is the element-wise application of the activation function \u03c3;\n\u2022 b\u2208 R^n is the bias vector.\nIn brief, Eq. (1) is equivalent to the action of a Fully-Connected (FC) layer where the weights are the same if the connection is outgoing from the same unit, whereas it is zero if two units correspond to graph nodes that are not connected; more precisely:\nW_{ij} = { w_i, if a_{ij} \u2260 0 or i = j\n       0, otherwise\nwhere a_{ij}, W_{ij} denote the (i, j)-th element of A, W, respectively.\nOn the other hand, from a message-passing point of view, the operation described in (1) is equivalent to having each node v_i of G sending to its neighbors a message equal to the input feature x_i, scaled by the weight w_i; then, each node sum up all the messages received from the neighbors, add the bias, and applies the activation function. In a nutshell, the message-passing interpretation can be summarized by Figure 1 and the following node-wise equation\nx'_i = \\sum_{j \u2208 N_{in}(i) \u222a {i}} x_j w_j + b_i, (2)\nwhere x'_i is the output feature of the GI layer corresponding to node v_i and N_{in}(i) is the set of indices such that j\u2208 N_{in}(i) if and only if e_{ij} = {v_i, v_j} is an edge of the graph. We dropped the action of the activation function \u03c3 for simplicity.\nLayers characterized by (1) can be generalized to read any arbitrary number K \u2265 1 of input features per node and to return any arbitrary number F \u2265 1 of output features per node. Then, the general definition of a GI layer is as follows.\nDefinition 2.1 (GI Layer - General form [10]). A GI layer with K\u2208 \\mathbb{N} input features and F\u2208 \\mathbb{N} output features is a NN layer with nF units connected to a layer with outputs in R^{n\u00d7K} and having a characterizing function L^{GI} : R^{n\u00d7K} \u2192 R^{n\u00d7F} defined by\nL^{GI}(X) = \u03c3 (W^T vertcat(X) + B), (3)\nwhere:\n\u2022 X \u2208 R^{n\u00d7K} is the input matrix (i.e., the output of the previous layer) and vertcat(X) denotes the vector in R^{nK} obtained concatenating the columns of X;\n\u2022 tensor W \u2208 R^{nK\u00d7F\u00d7n} is the concatenation along the 2nd dimension (i.e., the column-dimension) of the matrices W^{(1)},..., W^{(F)}, such that\nW^{(l)} :=  W^{(1,l)}\n  \u22ee\n  W^{(K,l)}  =  diag(w^{(1,l)}) A\n  \u22ee\n  diag(w^{(K,l)}) A  \u2208 R^{nK\u00d7n}, (4)\nfor each l = 1,..., F, after being reshaped as tensors in R^{nK\u00d71\u00d7n}. Vector w^{(k,l)} \u2208 R^n is the weight vector characterizing the contribution of the k-th input feature to the computation of the l-th output feature of the nodes, for each k = 1,...,K, and l = 1, ..., F; matrix A denotes A + I_n.\n\u2022 the operation W^T vertcat(X) is a tensor-vector product and B \u2208 R^{n\u00d7F} is the matrix of the biases.\nAdditionally, pooling and mask operations can be added to GI layers (see [10] for more details).\nFrom now on, we call Graph-Instructed Neural Network (GINN) a NN made of GI layers [10]. We point out that the number of weights of a GI layer is equal to nKF + nF. On the other hand, the number of weights of a FC layer of n units, reading the outputs of a layer of m units, is equal to mn + n; therefore, if we consider the case of m = n and KF+F < n+1 (typically satisfied for sufficiently large graphs), GI layers have fewer weights to be trained compared with the FC layer. Moreover, we observe that adjacency matrices are typically sparse and, therefore, the tensor W in (7) is typically sparse too. Then, it is possible to exploit the sparsity of this tensor to reduce the memory cost of the GINN implementation."}, {"title": "3. Edge-Wise Graph-Instructed Layers", "content": "A possible drawback of GI layers is that their weights rescale only the outgoing information of the nodes. For example, if nodes v_j and v_k are connected to node v_i in a graph G = (V, E) such that (v_i, v_j), (v_i, v_k) \u2208 E, then the units corresponding to v_j and v_k in a GI layer based on G receive the same contribution from the input features corresponding to node v_i; moreover, if nodes v_j, v_k have the same neighbors, the GI layer's outputs corresponding to these nodes are the same except for the contribution of the bias. This property is useful to reduce the number of weights per layer and, depending on the complexity of the target function defined on the graph nodes, it is not necessarily a limitation. Nonetheless, it surely limits the representational capacity of the model. Therefore, some target functions can be too complicated to be modeled by GI layers.\nGiven the observation above, it is useful to define a new GI layer capable of improving the capacity of the model at a reduced cost in terms of the total number of trainable weights. In this work, we propose to modify the classic GI layers by adding an extra set of weights associated with the nodes to rescale their incoming information. In brief, given the node-wise equation (1), we change it into\nx'_i = \\sum_{j \u2208 N_{in}(i) \u222a {i}} x_j w^{out}_j w^{in}_i + b_i, (5)\nwhere w^{out}_j denotes the (old) weights for rescaling the outgoing information from node v_j, while w^{in}_i denotes the (new) weights for rescaling the incoming information to node v_i (see Figure 2).\nA NN layer based on (5) is a layer with one input feature per node and one output feature per node, described by a function L : R^n \u2192 R^n such that\nL(x) = \u03c3 ((diag(w^{out})(A + I_n) diag(w^{in})) x + b) (6)\nfor each vector of input features x \u2208 R^n and where w^{out}, w^{in} \u2208 R^n are the weight vectors, where the components w^{out}_i, w^{in}_i with w^{in}_i are associated to the graph nodes v_i, for each i = 1,...,n. For future reference, we set W:= diag(w^{out}) (A + I_n) diag(w^{in}).\nIn brief, (6) is equivalent to a FC layer where the weights are zero if two distinct units correspond to graph nodes that are not connected, otherwise w_{ij} = w^{out}_j w^{in}_i if e_{ij} \u2208 E or i = j. Therefore, we observe that each weight w_{ij} = w^{out}_j w^{in}_i is associated with the edge e_{ij} = (v_i, v_j) in the graph or the self-loop added by the layer (if i = j). Given the above observations, we can interpret (6) as the operation of a NN layer with weights associated with edges instead of nodes. Then, we define the new layer as Edge-Wise GI (EWGI) Layer.\nRemark 3.1 (EWGI Layers - Advantages of the Formulation). Note that in principle EWGI layers could be defined by associating an independent weight w_{ij} to each edge of G and each added self-loops. Nonetheless, the approach here proposed exhibits the following advantages:\n\u2022 If G is a directed graph, we have that n -1 < |E| < n^2 \u2013n; therefore, for the independent weight formulation the total number of weights is in the range [2n-1,n^2] (biases excluded). On the other hand, in (6) the number of weights is always equal to 2n (biases excluded).\n\u2022 If G is an undirected graph, we have n\u22121 < |E| < (n^2 - n)/2; therefore, for the independent weight formulation the total number of weights is in the range [2n \u2013 1,n + (n^2 \u2013 n)/2] (biases excluded). On the other hand, in (6) the number of weights is always equal to 2n (biases excluded).\nThe advantage of using formulation (6) is therefore evident: independently of the number of graph edges, the number of weights is always 2n, which is essentially the lower bound of the number of weights in the other formulation.\nAnalogously to classic GI layers, EWGI layers can be generalized to read any arbitrary number K > 1 of input features per node and to return any arbitrary number F\u2265 1 of output features per node. Then, the general definition of a EWGI layer is as follows.\nDefinition 3.1 (EWGI Layer - General form). An EWGI layer with K\u2208 \\mathbb{N} input features and F\u2208 \\mathbb{N} output features is a NN layer with nF units connected to a layer with outputs in R^{n\u00d7K} and having a characterizing function L^{EWGI} : R^{n\u00d7K} \u2192 R^{n\u00d7F} defined by\nL^{EWGI}(X) = \u03c3 (W^T vertcat(X) + B), (7)\nwhere the tensor W \u2208 R^{nK\u00d7F\u00d7n} is defined as the concatenation along the 2nd dimension of the matrices W^{(1)},..., W^{(F)}, such that\nW^{(l)} :=  diag(w^{out(1,l)}) A diag (w^{in(1,l)})\n  \u22ee\n  diag(w^{out(K,l)}) A diag (w^{in(K,l)})  \u2208 R^{nK\u00d7n}, (8)\nfor each l = 1,...,F, after being reshaped as tensors in R^{nK\u00d71\u00d7n}, and where:\n\u2022 w^{out(k,l)} \u2208 R^n is the weight vector characterizing the contribution of the k-th input feature to the computation of the l-th output feature of the nodes, for each k = 1,..., K, and l = 1,..., F, with respect to the outgoing information;\n\u2022 w^{in(k,l)} \u2208 R^n is the weight vector characterizing the contribute of the k-th input feature to the computation of the l-th output feature of the nodes, for each k = 1,..., K, and l = 1, . . ., F, with respect to the incoming message."}, {"title": "4. Preliminary Results", "content": "From the definition above, we observe that the number of weights of a general EWGI layer is 2nKF + nF. Therefore, if we consider a FC layer of n units, reading the outputs of a layer of m = n units, the EWGI layers have a smaller number of weights to be trained if 2KF + F < n + 1.\nFrom now on, we call Edge-Wise Graph-Instructed Neural Network (EWGINN) a NN made of EWGI layers.\nIn this section, we illustrate the results of a preliminary experimental study about the representational capacity of the new EWGI layers and EWGINNs. We compare the performances of a set of EWGINNs with the results obtained in [10] by a set of GINNs for a RoGN task of a stochastic maximum flow problem. In particular, we train the new EWGINN models using the same hyperparameters, training options, and number of nodes, output features and input features used for the models trained in [10]; we only replace GI layers with EWGI layers.\nConcerning the regression problem, we recall that a stochastic maximum-flow problem is a problem where the edge capacities in a flow network are modeled as random variables and the target is to find the distribution of the maximum flow (e.g., see [17]). The task is to approximate with a NN model the function\n\u03a6: R^n\u2192R^m\nc \u2192 \u03a6(c) = \u03c6 (9)\nwhere c := (c_1,...,c_n) \u2208 R^n is the vector of the capacities of all the n edges of the network and \u03c6 := (\u03c6_1,...,\u03c6_m) \u2208 R^m is the flow vector corresponding to the m incoming edges of the network's sink that generate the maximum flow; in other words, the maximum flow corresponding to c is \u03c6 := ||\u03a6(c)||_1 = \\sum_{j=i}^m \u03c6_j.\nTo address this regression task, we build the GINNs and the EWGINNs with respect to the adjacency matrix of the line graph of the flow network; i.e., on the graph where the vertices correspond to edges of the network and two vertices are connected if the corresponding edges in the network share at least one vertex. We refer to [10] for more details about the formulation of this RoGN task for learning the maximum flow of a stochastic flow network (SFN)."}, {"title": "4.2. Performance Measures", "content": "Let \\Theta denote a NN model trained for learning (9) and let P be a test set used for measuring the performances of the model. Then, denoted by \\hat{\u03c6} := \\Theta(c) \u2208 R^m, we define the following performance measures:\n\u2022 Average Mean Relative Error (MRE) of sink's incoming flows, with respect to the max-flow:\nMRE_{av}(P) := (\\frac{1}{m} \\sum_{j=1}^{m} \\frac{| \u03c6_j - \\hat{\u03c6}_j |}{\u03c6})  (10)\nThis error measure describes the average quality of the NN in predicting the single elements \u03c6_1,..., \u03c6_m.\n\u2022 Average max-flow MRE:\nMRE_{\u03c6}(P) :=  (\\frac{ \\sum_{(c,\u03c6) \u2208 P}| \u03c6 - \\hat{\u03c6} |}{\u03c6}) (11)\nThis error measure describes the NN capability to predict the vector of fluxes \\hat{\u03c6} such that the corresponding maxflow \\hat{\u03c6} approximates the true maxflow \u03c6."}, {"title": "4.3. Data, Model Architectures, and Hyperparameters", "content": "We run our experiments on the same data reported in [10] for two randomly generated SFNs: a network based on a Barab\u00e1si-Albert (BA) graph and a network based on an Erdos-R\u00e9nyi (ER) graph. Each of the datasets D_{BA} and D_{ER} consists of 10000 samples of capacity vectors and corresponding flow vectors.\nIn this work, we focus on the harder case illustrated in [10]: for each network, we train the EWGINN models on 500 samples (20% used as validation data), measuring the errors MRE_{av} and MRE_{\u03c6} on a test set of 3500 samples. Then, we compare these performances with the performances of the GINNs trained in [10], excluding the ones with ReLU activation function, due to their low performances.\nFor a fair comparison, the architectures and hyperparameters of the EWGINN models are the same used for the GINN models in [10]. Specifically, we build 60 EWGINN model configurations for each SFN, varying among these parameters: hidden layers' activation function \u03c3\u2208 [ELU, swish, softplus], depth H \u2208 [3,5,7,9] for D_{BA} and H \u2208 [4,9,14,19] for D_{ER}, output features of each EWGI layer F\u2208 [1, 5, 10], output layer's pooling operation (if F > 1) pool \u2208 [reduce_max, reduce_mean]. Also, the training options are the same used in [10]: Adam optimizer [18] (learning rate e=0.002, moment decay rates \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.999), early stopping regularization [19, 20] (200 epochs of patience, restore best weights), reduction on plateau for the learning rate [20] (reduction factor \u03b1 = 0.5, 100 epochs of patience, minimum e = 10^{-6})."}, {"title": "4.4. Analysis of the Results", "content": "Figures 3 and 4 compare the errors between classical GINNs (blue dots) and EWGINN (orange crosses). The error plane shows the MRE_{av} error on the x-axis and the MRE_{\u03c6} on the y-axis.\nWe observe that the performance of GINNs and EWGINNs are comparable in D_{BA}, whereas EWGINNs exhibit better performance in the D_{ER} network. The explanation for the first case is that BA graphs are generated using preferential attachment criteria because they are used to describe many natural and human systems where few nodes are characterized by a higher degree compared to the other nodes of the network. Therefore, this structural property suggests that classical GINNs operate efficiently by rescaling only the outgoing information from the nodes toward their neighbors; the additional property of EWGINNs of rescaling the incoming information of nodes does not seem to offer significant benefits.\nIn contrast, the network D_{ER} has a more chaotic connection structure compared to D_{BA}; therefore, EWGINNs show a clear advantage in this scenario. Indeed, many EWGINNs perform better than the best classical GINNs. More precisely, we observe two clusters of EWGINNs: one cluster with performance almost equal to or better than the best GINNs, and one cluster with performance almost equal to or worse than the worst GINNs. The cluster of EWGINNs with poor performance, in this case, is attributed to an issue with early stopping. Specifically, for these models, at the beginning of the training phase, we observe a temporary overfitting phenomenon that induces a large increment of the validation loss and subsequently interrupts the training to reach the early stopping patience. Nonetheless, by increasing the early stopping patience from 200 epochs to 1000 epochs, we observe that the overfitting phenomenon tends to disappear (see Figure 5). From these observations, we can deduce that the larger representational capacity of EWGINNs can be an advantage but requires more careful tuning of the training hyperparameters.\nWe defer to future work an in-depth analysis of EWGINNs by varying the training hyperparameters, such as the early stopping patience."}, {"title": "5. Conclusion", "content": "In this work, we have proposed a novel type of GI layer: the Edge-Wise GI layer. Compared with the original GI layers, each node of an EWGI layer is equipped with an additional weight for rescaling the incoming message. This enables improved representational capacity and breaks the symmetry of GI layers, where nodes with the same neighborhood invariably receive the same message from the previous layer.\nTo analyze the performance of the newly proposed layers, we compared EWGINNs and GINNs on two benchmark RoGN tasks based on two SFNs, respectively, and where the graph connectivity is either concentrated on a few more central nodes (Barab\u00e1si-Albert) or shows a random structure (Erdos-R\u00e9nyi). The experiments indicate that EWGINNs exhibit improved performance in the second scenario, whereas in the first they are on par with GINNs.\nIn addition, we showed that the EWGINNS can reveal a non-monotone behavior in the validation loss that could make their training more unstable. Nonetheless, a careful choice of the hyperparameters can easily solve the problem. Overall, we confirm that RoGN tasks benefit from accounting for the graph connectivity.\nFuture works can provide a theoretical analysis of the convergence properties of the two compared architectures and applications to real-world problems."}]}