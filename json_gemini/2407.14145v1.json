{"title": "PassTSL: Modeling Human-Created Passwords through Two-Stage Learning", "authors": ["Haozhang Li", "Yangde Wang", "Weidong Qiu", "Shujun Li", "Peng Tang"], "abstract": "Textual passwords are still the most widely used user authentication mechanism. Due to the close connections between textual passwords and natural languages, advanced technologies in natural language processing (NLP) and machine learning (ML) could be used to model passwords for different purposes such as studying human password-creation behaviors and developing more advanced password cracking methods for informing better defence mechanisms. In this paper, we propose PassTSL (modeling human-created Passwords through Two-Stage Learning), inspired by the popular pretraining-finetuning framework in NLP and deep learning (DL). We report how different pretraining settings affected PassTSL and proved its effectiveness by applying it to six large leaked password databases. Experimental results showed that it outperforms five state-of-the-art (SOTA) password cracking methods on password guessing by a significant margin ranging from 4.11% to 64.69% at the maximum point. Based on PassTSL, we also implemented a password strength meter (PSM), and our experiments showed that it was able to estimate password strength more accurately, causing fewer unsafe errors (overestimating the password strength) than two other SOTA PSMS when they produce the same rate of safe errors (underestimating the password strength): a neural-network based method and zxcvbn. Furthermore, we explored multiple finetuning settings, and our evaluations showed that, even a small amount of additional training data, e.g., only 0.1% of the pretrained data, can lead to over 3% improvement in password guessing on average. We also proposed a heuristic approach to selecting finetuning passwords based on JS (Jensen-Shannon) divergence and experimental results validated its usefulness. In summary, our contributions demonstrate the potential and feasibility of applying advanced NLP and ML methods to password modeling and cracking.", "sections": [{"title": "1 Introduction", "content": "Textual passwords are currently the most common authentication scheme [2] and will continue to be widely used in the foreseeable future [13]. However, human-created passwords are often vulnerable to attacks based on data-driven probabilistic models [33,32,34]. Current state-of-the-art (SOTA) modeling approaches include the Markov chain based models (modeling n-gram transfer probabilities) [24,6,4,22,8], pattern-based models (modeling password semantic structures) [36,19,32,15,41,38], recurrent neural network (RNN) based models (learning to predict transfer distributions using complete preceding context) [23], and generative adversarial network (GAN) based models (adversarially learning the overall representation of a password set) [14,21,26].\nFor human-created textual passwords have to be memorized by the human users, they are often created partly or even fully based on natural languages so that there are elements reflecting natural language semantics. However, there are also substantial differences between human-created textual passwords and natural languages, e.g., the former do not usually include any white spaces or other obvious separator characters like in natural languages and are much shorter. Despite the differences, many natural language processing (NLP) and machine learning (ML) techniques can still be applied to password modeling and prediction, e.g., the widely used sequence2sequence prediction in NLP can be easily generalized to human-created textual passwords so that the masked part of a password like 'q1w2e[MASK]' can be predicted to be more likely 'q1w2e3' rather than other characters.\nInspired by the pretraining-finetuning framework that have been widely used for NLP and deep learning (DL) models in recent years, this paper presents our work about PassTSL (modeling human-created Passwords through Two-Stage Learning), a deep learning based password model powered by the self-attention mechanism in transformers [31] under a two-staged learning process: pretraining based on a large and more general database, and finetuning based on a smaller and more specific database for the target password database.\nWe conducted extensive experiments on the impact of the network size, the training data used, and the training data size on the password distribution modeling ability in the pretraining phase. We implemented PassTSL and applied it to six large leaked password databases. Our performance evaluation results manifested the effectiveness of PassTSL, compared against five SOTA password guessing models, including the 6-gram Markov model [22], Ma et al.'s backoff Markov model [22], the latest released implementation of the original PCFG-based password cracking method [36], Houshmand et al.'s semantic PCFG [15], and the RNN-based FLA [23]. In our experiments, we utilized the Monte Carlo method [5] to evaluate the performance with a large number of guesses, up to 1020.\nBased on PassTSL, we further designed a lightweight password strength meter (PSM) that can estimate the strength of a password in real time. Experiments proved that our PSM was more accurate than the FLA-based PSM [23] and the SOTA PSM zxcvbn [37] for password strength estimation."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Pretrained Models", "content": "To overcome the limitation about the lack of available large-scale datasets, machine learning researchers proposed transfer learning [25] and a two-staged learning framework including the pretraining and finetuning stages [17]. When used for NLP problems, the two-staged framework first encodes linguistic knowledge from a large-scale corpus and then transfers the captured knowledge about the underlying language to a more specified task, which avoids training from scratch but uses a smaller more targeted corpus to finetune the pretrained model [3].\nNLP researchers have proposed various pretrained language models (PLMs) using large unlabeled corpora to learn contextual word embeddings [17]. In 2017 Vaswani et al. [31] proposed Transformer to capture higher-level concepts in context like polysemous disambiguation and syntactic structures. Radford et"}, {"title": "2.2 Password Modeling Methods", "content": "Password guessing attacks are probably as old as the history of passwords. Much research has been done in this area, and we would only discuss methods and contributions that are most relevant to our work.\nMarkov. Narayanan et al. [24] proposed to guess passwords using an n-gram Markov model. The model was further extended by Ma et al. [22] and D\u00fcrmuth et al. [8]. Ma et al. [22] explored normalization techniques and smoothing strategies for Markov models, proposing Laplace smoothing and the backoff mechanism to prevent overfitting of higher-order Markov models. D\u00fcrmuth et al. [8] optimized the enumeration process by proposing an ordered Markov enumerator (OMEN) based on approximate sorting. However, all Markov model based methods are commonly limited by memory resources. The hyperparameter n is usually taken as 5 or 6 and thus longer-distance contextual information cannot be considered.\nPCFG. Weir et al. [36] explored the use of the probabilistic context-free grammar (PCFG) for password analyses and cracking. They considered passwords as instances of templates based on character types with different terminals (i.e., strings of the same type). For example, the password 'alice123!@' is an instance of the template L5D3S2, and its probability is calculated as\n$P(alice123!@) = P(L5D3S2) \\times P(alice|L5) \\times P(123|D3) \\times P(!@|S2)$. Li et al. [19] introduced Pinyin as a new type of password segments to improve Chinese password guessing. Houshmand et al. [15] introduced keyboard patterns and multi-word detection into PCFG to enhance its power. Veras et al. [32] used NLP methods to extract semantic information in passwords, and proposed a semantic PCFG that incorporates more types of password segments with different semantic meanings.\nNeural network based methods. Melicher et al. [23] proposed to model passwords using a long short-term memory (LSTM) network, which is referred by other researchers as FLA (derived from three words in their paper's title: \u2018Fast', \u2018Lean', and 'Accurate'). They also designed a PSM based on a highly compressed FLA network. Hitaj et al. [14] proposed PassGAN, which guesses passwords via a GAN. However, they reported that PassGAN required more guesses in order to achieve the same cracking performance as FLA. Dario et al. [26] demonstrated the potential of representation learning in improving PassGAN. In particular, they proposed to dynamically control the latent space of PassGAN through the feedback from correct guesses during password guessing to mimic the unknown distribution of the target passwords.\nMore recently, some researchers working on password cracking have started recognizing the potential of transformer-based deep learning models. He et al. [12]"}, {"title": "3 PassTSL: Pretraining", "content": "We first report the architecture and pretraining method of PassTSL. Then, we discuss the effects of different pretraining settings on PassTSL's performance. Finally, we demonstrate advantages of the pretrained (without finetuning) PassTSL over SOTA methods via Monte Carlo simulation from two aspects, password cracking and password strength estimation."}, {"title": "3.1 Method", "content": "Here we propose PassTSL's pretrained model, a base line model of our two-staged password guessing method, in order to learn universal representations of passwords. Formally, password modeling is constructed as an unsupervised distribution estimation of passwords (x = {C1, C2,...,Cm}), where x stands for a password and ci is the i-th character in x. We use the standard language model object to maximize a likelihood function L(x):\n$L(x) = \\sum_{i=1}^{m}logP(C_i | C_1, ..., C_{i-1}; \\theta)$,\nwhere \u03b8 represents parameters of PassTSL. An overview of the PassTSL architecture is illustrated in Figure 1 with an example.\nVocabulary and tokenization. We focus on character-level tokens instead of commonly used word-level tokens in NLP or pattern-level tokens in PCFG. Texts (especially Western texts) are naturally separated by spaces, while passwords rarely include any white spaces or other uniformly defined separators so characters are what we have to start with to analyse passwords.\nInput. The input is a single password since PassTSL aims to learn character-level transfer probability distributions. Each password x will be preprocessed to a sequence of characters after character-level tokenization. A special start-of-sequence ([SOS]) token is added to the beginning of x as the initial input when generating a candidate or calculating the probability of a given password. A special end-of-sequence ([EOS]) token is also added as the end symbol to help PassTSL learn when to stop decoding.\nEmbedding. The tokenized sequence will be encoded by the text embedding layer and the position embedding layer. The resulting vectors are then summed up to get the representation of x, and to construct the self-attention mask matrix M for each password, with element Mij satisfying\n$M_{ij} = \\begin{cases}\n0 & i \\geq j, \\text{need attention}, \\\\\n-\\infty & i < j, \\text{need to be masked}.\n\\end{cases}$\nBackbone network. Our model is a multi-layered network based on the architecture of a transformer decoder [31]. The embedding vector R0 of sequence x is encoded as a contextual representation RL by the L-block transformer decoder after normalization. RL is fed into a linear classifier to compute the distribution over target tokens. Formally, in the ith block (0 < i < L), a self-attention operation is implemented as:\n$Q = R_{i-1}W^Q, K = R_{i-1}W^K, V = R_{i-1}W^V$,\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}} + M)V$,\nwhere Ri\u22121 is the output of the i \u2013 1-th block, while $W^Q, W^K, W^V$ are the parameter matrices for linearly mapping Ri\u22121 to a triple. M is the self-attention mask matrix fed into PassTSL together with the context tokens.\nHyperparameters. PassTSL is defined by the following hyperparameters."}, {"title": "3.2 Effect of Different pretraining Settings", "content": "Experimental settings We randomly selected 100 million passwords and one million passwords from COMB, one million passwords from CSDN, and one million passwords from Gmail as the pretraining data, denoted by COMB_100M, COMB_1M, CSDN_1M, and Gmail_1M. These passwords consist of only 95 ASCII printable characters and contain no less than 6 characters.\nTable 3 gives detailed settings. Models were pretrained for 10 epochs with a batch size of 256. In Monte Carlo simulations, one million passwords were sampled from each pretrained model to provide valid and accurate estimates. We"}, {"title": "Analyses and Discussions", "content": "Figures 2 and 3 show the performance of PassTSL on six testing databases under different pretraining settings. We have the following findings and conclusions.\nThe increase of the model size plays a positive but has a limited effect on the guessing performance. As shown in Figure 2, the curves COMB 100M CSDN 1M of PassTSLBase and PassTSLSmall largely overlap. Figure 3 indicates Gmail 1M that the simulation curves of PassTSL Base are a bit higher than those of PassTSL Small, specifically 0.92% on average at 1020 guesses. However, such improvement is not surprising considering that the number of parameters for PassTSLBase is 20 times larger than that for PassTSLsmall. For sequences like passwords with shorter length and less logical information than texts in general NLP tasks, a small-scale model has been already strong enough to sufficiently learn the inner linguistic features. Larger-scale models, while still likely to improve the effectiveness of password modeling, have to bear the risk of overfitting."}, {"title": "3.3 Evaluations on Password Cracking and Password Strength Estimation", "content": "COMB 100M In this subsection, we compare PassTSL Small with other SOTA password guessing models and PSMs on the testing sets shown in Table 2. We briefly describe the models for comparison, and then report their performance.\nPassword Cracking Tools for Comparison The password guessing models compared with PassTSL and their settings are as follows: 1) the 6-gram Markov model used in [22] as a benchmark; 2) the backoff Markov model proposed in [22]; 3) PCFGse - Veras et al.'s semantic PCFG [32]; 4) PCFGori \u2013 the latest released (v4.0-rc3) of the original PCFG [35] developed and maintained by Matt Weir, the lead author of the original PCFG paper [36]; and 5) FLA [23] with the default configurations recommended by their authors. We did not consider GAN-based models such as PassGAN [14] because past research suggested that they need"}, {"title": "4 PassTSL: Finetuning", "content": "As explained in the previous section, our PassTSL base-line model pretrained using the COMB has already demonstrated superior performance over some SOTA methods. In this section, we show that even better performance can be achieved by finetuning the pretrained model. We explore methods for finetuning PassTSL and report the impact of various finetuning strategies."}, {"title": "4.1 The Effects of Database Properties", "content": "In general, attackers know the language most users speak and the service type of a target website. We were interested in knowing if these two properties can"}, {"title": "4.2 The Size of Finetuning Passwords", "content": "The results in the previous subsection are based on one million finetuning passwords. This amount of data may still considered too high, so we also investigated if a smaller amount of finetuning data can achieve a sufficiently good performance."}, {"title": "4.3 How to Select Finetuning Password Database", "content": "Based on the experimental results in Section 3, it is recommended that COMB be used as the pretraining database. Heuristically, we suggest selecting the finetuning database so that it is more similar to the target database than the pretraining database is, so that the finetuning process can add more specific information about target passwords to the finetuned PassTSL model. To measure the similarity between two password databases, we propose to use the JS (Jensen-Shannon) divergence [20] calculated based on the union of all 3-grams in the two password databases."}, {"title": "5 Conclusion", "content": "This paper presents PassTSL, a deep learning model for password modeling and guessing, based on the pretraining-finetuning two-staged learning framework. Our model aims to extract universal password features through a self-attention mechanism. Extensive experiments have proved that PassTSL is superior to other SOTA password modeling and cracking methods and is also practical to"}]}