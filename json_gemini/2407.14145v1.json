{"title": "PassTSL: Modeling Human-Created Passwords through Two-Stage Learning", "authors": ["Haozhang Li", "Yangde Wang", "Weidong Qiu", "Shujun Li", "Peng Tang"], "abstract": "Textual passwords are still the most widely used user authentication mechanism. Due to the close connections between textual passwords and natural languages, advanced technologies in natural language processing (NLP) and machine learning (ML) could be used to model passwords for different purposes such as studying human password-creation behaviors and developing more advanced password cracking methods for informing better defence mechanisms. In this paper, we propose PassTSL (modeling human-created Passwords through Two-Stage Learning), inspired by the popular pretraining-finetuning framework in NLP and deep learning (DL). We report how different pretraining settings affected PassTSL and proved its effectiveness by applying it to six large leaked password databases. Experimental results showed that it outperforms five state-of-the-art (SOTA) password cracking methods on password guessing by a significant margin ranging from 4.11% to 64.69% at the maximum point. Based on PassTSL, we also implemented a password strength meter (PSM), and our experiments showed that it was able to estimate password strength more accurately, causing fewer unsafe errors (overestimating the password strength) than two other SOTA PSMS when they produce the same rate of safe errors (underestimating the password strength): a neural-network based method and zxcvbn. Furthermore, we explored multiple finetuning settings, and our evaluations showed that, even a small amount of additional training data, e.g., only 0.1% of the pretrained data, can lead to over 3% improvement in password guessing on average. We also proposed a heuristic approach to selecting finetuning passwords based on JS (Jensen-Shannon) divergence and experimental results validated its usefulness. In summary, our contributions demonstrate the potential and feasibility of applying advanced NLP and ML methods to password modeling and cracking.", "sections": [{"title": "1 Introduction", "content": "Textual passwords are currently the most common authentication scheme [2] and will continue to be widely used in the foreseeable future [13]. However, human-created passwords are often vulnerable to attacks based on data-driven probabilistic models [33,32,34]. Current state-of-the-art (SOTA) modeling approaches include the Markov chain based models (modeling n-gram transfer probabilities) [24,6,4,22,8], pattern-based models (modeling password semantic structures) [36,19,32,15,41,38], recurrent neural network (RNN) based models (learning to predict transfer distributions using complete preceding context) [23], and generative adversarial network (GAN) based models (adversarially learning the overall representation of a password set) [14,21,26].\nFor human-created textual passwords have to be memorized by the human users, they are often created partly or even fully based on natural languages so that there are elements reflecting natural language semantics. However, there are also substantial differences between human-created textual passwords and natural languages, e.g., the former do not usually include any white spaces or other obvious separator characters like in natural languages and are much shorter. Despite the differences, many natural language processing (NLP) and machine learning (ML) techniques can still be applied to password modeling and prediction, e.g., the widely used sequence2sequence prediction in NLP can be easily generalized to human-created textual passwords so that the masked part of a password like 'q1w2e[MASK]' can be predicted to be more likely 'q1w2e3' rather than other characters.\nInspired by the pretraining-finetuning framework that have been widely used for NLP and deep learning (DL) models in recent years, this paper presents our work about PassTSL (modeling human-created Passwords through Two-Stage Learning), a deep learning based password model powered by the self-attention mechanism in transformers [31] under a two-staged learning process: pretraining based on a large and more general database, and finetuning based on a smaller and more specific database for the target password database.\nWe conducted extensive experiments on the impact of the network size, the training data used, and the training data size on the password distribution modeling ability in the pretraining phase. We implemented PassTSL and applied it to six large leaked password databases. Our performance evaluation results manifested the effectiveness of PassTSL, compared against five SOTA password guessing models, including the 6-gram Markov model [22], Ma et al.'s backoff Markov model [22], the latest released implementation of the original PCFG-based password cracking method [36], Houshmand et al.'s semantic PCFG [15], and the RNN-based FLA [23]. In our experiments, we utilized the Monte Carlo method [5] to evaluate the performance with a large number of guesses, up to 1020.\nBased on PassTSL, we further designed a lightweight password strength meter (PSM) that can estimate the strength of a password in real time. Experiments proved that our PSM was more accurate than the FLA-based PSM [23] and the SOTA PSM zxcvbn [37] for password strength estimation."}, {"title": "2 Related Work", "content": "Password guessing attacks are probably as old as the history of passwords. Much research has been done in this area, and we would only discuss methods and contributions that are most relevant to our work.\nPCFG. Weir et al. [36] explored the use of the probabilistic context-free grammar (PCFG) for password analyses and cracking. They considered passwords as instances of templates based on character types with different terminals (i.e., strings of the same type). For example, the password 'alice123!@' is an instance of the template L5D3S2, and its probability is calculated as\nP(alice123!@) = P(L_5D_3S_2) \\times P(alice|L_5) \\times P(123|D_3) \\times P(!@|S_2).\nLi et al. [19] introduced Pinyin as a new type of password segments to improve Chinese password guessing. Houshmand et al. [15] introduced keyboard patterns and multi-word detection into PCFG to enhance its power. Veras et al. [32] used NLP methods to extract semantic information in passwords, and proposed a semantic PCFG that incorporates more types of password segments with different semantic meanings."}, {"title": "3 PassTSL: Pretraining", "content": "We first report the architecture and pretraining method of PassTSL. Then, we discuss the effects of different pretraining settings on PassTSL's performance. Finally, we demonstrate advantages of the pretrained (without finetuning) PassTSL over SOTA methods via Monte Carlo simulation from two aspects, password cracking and password strength estimation."}, {"title": "3.1 Method", "content": "Here we propose PassTSL's pretrained model, a base line model of our two-staged password guessing method, in order to learn universal representations of passwords. Formally, password modeling is constructed as an unsupervised distribution estimation of passwords (x = {C_1, C_2,...,C_m}), where x stands for a\nL(x) = \\sum_{i=1}^{m}logP(C_i C_1, ..., C_{i-1}; \\theta),\nwhere \\theta represents parameters of PassTSL. An overview of the PassTSL architecture is illustrated in Figure 1 with an example.\nVocabulary and tokenization. We focus on character-level tokens instead of commonly used word-level tokens in NLP or pattern-level tokens in PCFG. Texts (especially Western texts) are naturally separated by spaces, while passwords rarely include any white spaces or other uniformly defined separators so characters are what we have to start with to analyse passwords.\nInput. The input is a single password since PassTSL aims to learn character-level transfer probability distributions. Each password x will be preprocessed to a sequence of characters after character-level tokenization. A special start-of-sequence ([SOS]) token is added to the beginning of x as the initial input when generating a candidate or calculating the probability of a given password. A special end-of-sequence ([EOS]) token is also added as the end symbol to help PassTSL learn when to stop decoding.\nEmbedding. The tokenized sequence will be encoded by the text embedding layer and the position embedding layer. The resulting vectors are then summed up to get the representation of x, and to construct the self-attention mask matrix M for each password, with element M_{ij} satisfying\nM_{ij} =\n\\begin{cases}\n0 & i \\geq j, \\text{ need attention}, \\\\\n-\\infty & i < j, \\text{ need to be masked}.\n\\end{cases}\nBackbone network. Our model is a multi-layered network based on the architecture of a transformer decoder [31]. The embedding vector R_0 of sequence x is encoded as a contextual representation R_L by the L-block transformer decoder after normalization. R_L is fed into a linear classifier to compute the distribution over target tokens. Formally, in the ith block (0 < i < L), a self-attention operation is implemented as:\nQ = R_{i-1}W_Q, K = R_{i-1}W_K, V = R_{i-1}W_V,\nAttention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}+ M)V,\nwhere R_{i-1} is the output of the i \u2013 1-th block, while W_Q, W_K, W_V are the parameter matrices for linearly mapping R_{i-1} to a triple. M is the self-attention mask matrix fed into PassTSL together with the context tokens.\nHyperparameters. PassTSL is defined by the following hyperparameters."}, {"title": "3.2 Effect of Different pretraining Settings", "content": "The increase of the model size plays a positive but has a limited effect on the guessing performance. As shown in Figure 2, the curves of PassTSLBaseCSDN 1M and PassTSLSmallCSDN 1M largely overlap. Figure 3 indicates that the simulation curves of PassTSLBaseGmail 1M are a bit higher than those of PassTSLSmallGmail 1M, specifically 0.92% on average at 1020 guesses. However, such improvement is not surprising considering that the number of parameters for PassTSLBase is 20 times larger than that for PassTSLsmall. For sequences like passwords with shorter length and less logical information than texts in general NLP tasks, a small-scale model has been already strong enough to sufficiently learn the inner linguistic features. Larger-scale models, while still likely to improve the effectiveness of password modeling, have to bear the risk of overfitting.\nPretraining PassTSL with mixed-language passwords is able to help better modeling English passwords. Figure 3 shows that mixed-language based PassTSL instances obviously outperformed those pretrained using English passwords only. Particularly, the cracking rate of PassTSLSmallCOMB 1M is on average 3.38% higher than that of PassTSLBaseGmail 1M and 3.46% higher than that of PassTSLSmallGmail 1M, while PassTSLSmallCOMB 1M is on average 3.48% and 3.55% higher than the same two benchmarks, respectively. This significant performance gain does not exist when such PassTSL instances are used to attack Chinese databases, as shown in Figure 2. We believe that it is because hybrid databases like COMB are more dominated by English passwords, making the pretrained model more biased towards English passwords. Such a bias also implies that using Chinese passwords for finetuning can potentially improve a COMB-pretrained PassTSL model's performance against Chinese databases, which was proved in our experimental results reported in Section 4.2.\nThe increase of the training data size will significantly improve the performance. PassTSLSmallCOMB 100M has noticeable improvement over PassTSLSmallCOMB 1M on each test set as shown in Figures 2 and 3. Besides, although the mixed database COMB is more biased towards English passwords, the performance of PassTSLSmallCOMB 100M on Chinese testing sets is still better, likely because the amount of training data from COMB is much larger than the amount of CSDN. As shown in Figure 2, the simulated cracking rates of PassTSLSmallCOMB 100M compared to PassTSLBaseCSDN 1M differ by only 0.004% (17173), 0.06% (178), and 0.35% (Tianya) at 1020, respectively. These results suggest that the data-driven PassTSL model possesses the potential to further improve the accuracy of password modeling given more training resources.\nConsidering the resources required for pretraining, password generation and probabilities calculation, we believe that PassTSLSmallCOMB 100M is a good balanced representation of PassTSL's ability to model passwords."}, {"title": "3.3 Evaluations on Password Cracking and Password Strength Estimation", "content": "In this subsection, we compare PassTSLSmallCOMB 100M with other SOTA password guessing models and PSMs on the testing sets shown in Table 2. We briefly describe the models for comparison, and then report their performance.\nPassword Cracking Tools for Comparison The password guessing models compared with PassTSL and their settings are as follows: 1) the 6-gram Markov model used in [22] as a benchmark; 2) the backoff Markov model proposed in [22]; 3) PCFGse - Veras et al.'s semantic PCFG [32]; 4) PCFGori \u2013 the latest released (v4.0-rc3) of the original PCFG [35] developed and maintained by Matt Weir, the lead author of the original PCFG paper [36]; and 5) FLA [23] with the default configurations recommended by their authors. We did not consider GAN-based models such as PassGAN [14] because past research suggested that they need\nFrom Figure 5, PassTSL-Small-int8z is more accurate than the other two PSMs: it has the lowest unsafe errors (over-estimated password strength), while safe errors are aligned with others. It makes significantly fewer (almost half for all red cells) unsafe errors than FLA. Moreover, although zxcvbn is more accurate when evaluating strong passwords (> 1013), its unsafe errors often appear at lower and mid-ranged guessing numbers, which could mislead users to choose weaker passwords more often."}, {"title": "4 PassTSL: Finetuning", "content": "As explained in the previous section, our PassTSL base-line model pretrained using the COMB has already demonstrated superior performance over some SOTA methods. In this section, we show that even better performance can be achieved by finetuning the pretrained model. We explore methods for finetuning PassTSL and report the impact of various finetuning strategies."}, {"title": "4.1 The Effects of Database Properties", "content": "In general, attackers know the language most users speak and the service type of a target website. We were interested in knowing if these two properties can\nTogether with the findings in Section 3.2, we consider the attack on a specific password database as a downstream task on PassTSLSmallCOMB 100M. We expect that the password cracker could enhance password modeling performance with small finetuning costs while preserving common password knowledge learned in the pretraining phase. Intuitively, passwords taken from a website sharing the same language and service type properties as the target website are ideal sources for finetuning. We designed three scenarios to investigate the effects of both properties, where one million passwords were randomly selected from a finetuning database to attack a target database.\nThe same service type (social media), but different languages (Chinese to English) (Scenario A). PassTSLSmallCOMB 100M is finetuned by Tianya_1M to attack RockYou and Twitter.\nThe same language (English), but different service types (Scenario B). PassTSLSmallCOMB 100M is finetuned by Twitter_1M to attack MyHeritage (social media to information service) and by Tianya_1M to attack 178 (social media to gaming website).\nThe same language (English) and the same service type (social media) (Scenario C). PassTSLSmallCOMB 100M is finetuned by 17173_1M to attack 178 (Chinese gaming websites) and by Twitter_1M to attack RockYou.\nAs shown in Figure 6, the curves in sub-figure (a) show that it will lead to a decrease on performance if finetuning only based on the service type of the target website, while results in sub-figures (b-c) prove that the user language has a positive effect in the finetuning stage: the coverage rate of PassTSLSmallFTCOMB 100M (FT = finetuning) is on average 3.33% higher than that of PassTSLSmallCOMB 100M in sub-figure (b), and 4.07% in sub-figure (c)."}, {"title": "4.2 The Size of Finetuning Passwords", "content": "The results in the previous subsection are based on one million finetuning passwords. This amount of data may still considered too high, so we also investigated if a smaller amount of finetuning data can achieve a sufficiently good performance.\nUsing the Tianya database as an example, 10K and 100K passwords were randomly selected, satisfying the requirements in Section 3.2, denoted as Tianya_10K and Tianya_100K. They were new password sets to finetune PassTSLSmallCOMB 100M respectively, while target databases were 17173 and 178. The finetuned models, denoted by PassTSLSmallFT 10K and PassTSLSmallFT 100K, were compared with PassTSLSmallCOMB 100M and PassTSLSmallFT 1M.\nExperimental results are provided in Figure 7. For both target databases, curves of PassTSLSmallFT 100K and PassTSLSmallFT 1M are approximately identical, while visible gaps exist for PassTSLSmallFT 10K. In particular, PassTSLSmallFT 10K is on average 0.13% lower than PassTSLSmallFT 100K on 17173 and 0.14% lower on 178, while PassTSLSmallFT 1M is 0.77% and 0.91% lower. However, coverage rates of PassTSLSmallFT 10K is still 3.4% and 3.7% higher than PassTSLSmallCOMB 100M, respectively. The results show that, for the finetuning stage, only 0.1%\nof the pretraining data size can be sufficient to obtain a good level of performance improvement. The finding echoes the observation in Section 3.2 that using COMB for pretraining can lead to a pretrained model biased towards English passwords.\nFrom the experiments in Sections 4.1 and 4.2, we conclude that:\nThe finetuning process can enhance PassTSL's ability to guess passwords.\nThe user language of the target website can be used to achieve better fine-tuning results, if the finetuning data share the same language as the target website. Considering that this property of a website is mostly public knowledge, an immediate advice to users is that they should try to diversify ways to define their passwords, particularly to avoid using elements that are more typical in the dominating language of a website.\nTo achieve a good performance, the finetuning stage just needs as little as 0.1% of the amount of pretrained passwords."}, {"title": "4.3 How to Select Finetuning Password Database", "content": "Based on the experimental results in Section 3, it is recommended that COMB be used as the pretraining database. Heuristically, we suggest selecting the finetuning database so that it is more similar to the target database than the pretraining database is, so that the finetuning process can add more specific information about target passwords to the finetuned PassTSL model. To measure the similarity between two password databases, we propose to use the JS (Jensen-Shannon) divergence [20] calculated based on the union of all 3-grams in the two password databases.\nTo validate the above heuristic idea, some experiments were conducted based on JS divergence values between selected pairs of password databases given in Table 5. The finetuned PassTSL in each experiment is denoted as PassTSLSmallFT. Figure 8 presents experimental results. As shown in in Sub-figure (a), compared with the pretrained model PassTSLSmallCOMB 100M, the model PassTSLSmallFT finetuned using the Tianya database performs significantly and consistently better when attacking 17173 and 178 (the coverage rate increases by 12.13% on 109 guesses on 17173 and by 15.77% on 109 guesses on 178). Such results can be predicted from the JS divergence values: JS(Tianya, 17173) < JS(COMB_100M, 17173)\nand JS(Tianya, 178) < JS(COMB_100M, 178). When the Twitter database was used for finetuning, PassTSLSmallFT also beats PassTSLSmallCOMB 100M on MyHeritage and RockYou as shown in Sub-figure (b). The coverage rate increases by 3.9% on 108 guesses and 4.96% on 107 guesses. Such results could also be predicted from the JS divergence values: JS(Twitter, MyHeritage) < JS(COMB_100M, MyHeritage) and JS(Twitter, RockYou) < JS(COMB_100M, RockYou). The lower improvement may be explained by the fact that the JS divergence values between the Twitter database and the two target databases are just slightly smaller than the values between COMB_100M and the two target databases. When the RockYou database was used for finetuning, the performance of the finetuned model PassTSLSmallFT behaves very similar to PassTSLSmallCOMB 100M, which can be explained by the fact that the RockYou database does not have a smaller JS divergence value with the target databases than COMB_100M, as shown in Table 5, therefore the contribution of the finetuning becomes more marginal (if any).\nIn summary, the results in Figure 8 largely validate the effectiveness of using the JS divergence as a quantitative metric to guide the selection of the finetuning database."}, {"title": "5 Conclusion", "content": "This paper presents PassTSL, a deep learning model for password modeling and guessing, based on the pretraining-finetuning two-staged learning framework. Our model aims to extract universal password features through a self-attention mechanism. Extensive experiments have proved that PassTSL is superior to other SOTA password modeling and cracking methods and is also practical to support a password strength meter. It is also shown that the finetuning phase can help improve the performance even further if the finetuning database is properly selected, and the amount of finetuning data needed is very light (e.g., 0.1% of the pretraining data)."}]}