{"title": "Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks", "authors": ["Yixiao Xu", "Binxing Fang", "Rui Wang", "Yinghai Zhou", "Shouling Ji", "Yuan Liu", "Mohan Li", "Zhihong Tian"], "abstract": "Developing high-performance deep learning models is resource-intensive, leading model owners to utilize Machine Learning as a Service (MLaaS) platforms instead of publicly releasing their models. However, malicious users may exploit query interfaces to execute model extraction attacks, reconstructing the target model's functionality locally. While prior research has investigated triggerable watermarking techniques for asserting ownership, existing methods face significant challenges: (1) most approaches require additional training, resulting in high overhead and limited flexibility, and (2) they often fail to account for advanced attackers, leaving them vulnerable to adaptive attacks.\nIn this paper, we propose Neural Honeytrace, a robust plug-and-play watermarking framework against model extraction attacks. We first formulate a watermark transmission model from an information-theoretic perspective, providing an interpretable account of the principles and limitations of existing triggerable watermarking. Guided by the model, we further introduce: (1) a similarity-based training-free watermarking method for plug-and-play and flexible watermarking, and (2) a distribution-based multi-step watermark information transmission strategy for robust watermarking. Comprehensive experiments on four datasets demonstrate that Neural Honeytrace outperforms previous methods in efficiency and resisting adaptive attacks. Neural Honeytrace reduces the average number of samples required for a worst-case t-Test-based copyright claim from 12,000 to 200 with zero training cost. The code is available at https://github.com/NeurHT/NeurHT.", "sections": [{"title": "1 Introduction", "content": "With the growing scale of training dataset and model parameter, the development of high-performance deep learning models has become increasingly expensive. For example, excluding the labor costs for data collection and processing, training a GPT-3 [4] model on 1,024 NVIDIA Ampere A100 GPUs takes approximately 34 days and incurs a cost of around $500,000. As a result, instead of releasing their models publicly, model owners usually build up Machine Learning as a Service (MLaaS) platforms for paid intelligent services (e.g., ChatGPT 1, Midjourney 2), where users interact with black-box models via a query interface.\nHowever, malicious users can still extract valuable information by executing carefully-designed queries to the service interface, enabling them to locally reconstruct the functionality of the victim model with minimal overhead [3,9,15,31,40,41], namely model extraction attacks (MEAs). In recent years, model extraction attacks have been extensively studied across various information channels [3, 40], leveraging different data sources [9, 41], and employing diverse learning strategies [6, 13]. Additionally, recent research has also investigated adaptive attacks targeting potential defenses [7,23,37]. These mechanisms use effective label recovery strategies to remove misleading perturbations or watermarks embedded in the output predictions, causing severe challenges to the copyright protection of MLaaS applications.\nTo mitigate the risk of model extraction attacks, a variety of defense mechanisms have been proposed, including model extraction detection [15, 18], prediction perturbation [17,20,27,37], and model watermarking [8,14,21,25,36]. Compared to the other two passive defenses, model watermarking aims to implant triggerable watermarks into the stolen model, allowing the model owner to assert ownership by activating the watermarks during inference. Recently, backdoor-like watermarks [14, 21, 25] have showed great potential for watermark embedding and capability retention. Fig. 1 provides an overview of the workflow of model extraction attacks and triggerable watermarking methods, where the watermark is embedded in the output predictions and then transmitted to the stolen model.\nDespite the success of previous solutions, existing watermarking strategies still face challenges in terms of flexibility and robustness. For example, the watermark embedding process in existing methods [14, 21, 25] requires extensive model retraining. Once embedded, these watermarks cannot be easily removed or modified. Some methods, including the state-of-the-art one, MEA-Defender [25], are designed for soft-label black-box scenarios, resulting in significant performance degradation in hard-label settings\u2014the most common scenario in MLaaS. Moreover, existing approaches are only evaluated against naive attacks, where attackers have no knowledge about potential defenses. As shown in Sec. 5.2, these methods exhibit limited robustness against adaptive model extraction attacks. Tab. 1 compares the capability of different watermarking strategies.\nTherefore, we propose Neural Honeytrace, a robust plug-and-play watermarking framework against model extraction attacks. We begin by developing a watermark transmission model from an information-theoretic perspective. Based on this framework, we analyze the embedded watermark information, its transmission, and its robustness in terms of information entropy, channel capacity, and noise resilience. We discovered that triggered watermarks face challenges against adaptive attacks due to the limited channel capacity and additional noises. Building on these insights, we introduced: (1) a training-free watermarking method for plug-and-play and flexible watermarking, and (2) a multi-step watermark information transmission strategy for robust watermarking. The main contributions of this paper are summarized as follows:\n1. We propose Neural Honeytrace, the first training-free triggerable watermarking framework. It is designed as a plug-and-play manner, offering the flexibility for seamless removal or modification post-deployment.\n2. We establish a watermark transmission model using the information theory, which addresses several open questions"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Model Extraction Attack", "content": "The goal of model extraction attacks is to rebuild the functionality of the victim model F locally. To achieve this object, attackers first collect or synthesize an unlabeled surrogate dataset Ds = {X1,X2, ...,Xn}, and then utilize the victim model F to label the surrogate dataset Ds and get the corresponding label set Ys = {Y1, Y2, ..., Yn}. The type of label Y is determined by the setting of MLaaS (e.g., scores, probabilities, and hard-labels). Subsequently, attackers can train a surrogate model Fs following the optimization problem:\n$\\arg \\min\\limits_{F_s} E_{(X,Y)\\in(D_s, Y_s)} [L(F_s(X),Y)]$\nwhere L is the loss function for evaluating the distance between the outputs of F and Fs. After training, the surrogate model should have similar functionality with the victim model, thus attackers can provide stolen services as shown in Fig. 1.\nThe basic model extraction attack defined above may fail under perturbation-based defenses [17, 20, 27, 37]. Therefore, attackers introduced more advanced adaptive attacks to bypass potential defenses. Denote the perturbed prediction set as \u0176s = {Y1 + P1, Y2 + P2, ..., Yn + Pn}, where P; represents the perturbation added on Y; by defense strategies. Adaptive model extraction attacks adopt different recovery mechanisms R(.) to recover Ys from \u0176s. For example, Smoothing"}, {"title": "2.2 Triggerable Watermarking", "content": "Triggerable watermarking strategies [8, 14,21,25] aim at implanting watermarks that can survive during model extraction attacks into the target model. Similar to backdoor activation in backdoor attacks, model owners can use predefined triggers to activate watermarks in stolen models and utilize special model outputs for ownership declaration. Existing triggerable watermarking methods follow the injection of backdoors, for a predefined watermark trigger T, the watermarking process can be described as the following optimization problem:\n$\\arg \\min\\limits_{F_w} E_{(X,Y)\\in(X,Y)} [L(F_w(X),Y) + \\alpha L(F_w(\\tau(X,T)),\\hat{Y}) ]$\nwhere Fw is the watermarked model, (X, Y) are the sets of training data and corresponding labels, \u03b1 is the weight parameter which balances the watermarking success rate and clean accuracy, \u03c4(...) represents the watermark injection mechanism, and \u0176 denotes the special output for IP declaration.\nA fundamental challenge for triggerable watermarks is they need to survive during model extraction, where the surrogate dataset may not contain the watermark trigger predefined by defenders. Existing methods introduced different assumptions and addressed this challenge empirically by adding regularization terms to Eq. 1 [8, 14] or designing specialized triggers [21, 25]. However, these solutions often introduce additional computational overhead and are not robust against adaptive model extraction attacks."}, {"title": "2.3 Hypothesis Test", "content": "Triggerable watermarking enables defenders to claim ownership of the stolen model using hypothesis test [14]. The sample size N required for ownership claim can be calculated using the following equation:\n$N = \\frac{2(Z_{\\alpha/2}+Z_{\\beta})^2}{d^2}$\nwhere Z\u03b1/2 is the critical value of the significance level, Z\u03b2 is the power, and d is the effect size. For watermarking scenarios, the effect size d is the watermark success rate (WSR). As depicted in Fig. 2, the number of samples required grows exponentially as the success rate of watermarking decreases."}, {"title": "2.4 Threat Model", "content": "Attacker's goal. As defined in Sec. 2.1, the primary goal of model extraction attackers is to reconstruct the functionality of the victim model locally. We assume the victim model is deployed as a black-box service, which means that attackers can only access output predictions through the interface provided by the model owner.\nAttacker's capability. We consider attackers who lack direct access to the training dataset but possess knowledge of the task domain (e.g., image classification, face recognition etc.) and utilize open-source datasets from the corresponding domains to perform attacks.\nTo account for varying levels of sophistication, we classify attackers into three categories based on their knowledge of potential defenses: 1) naive attackers who have no prior knowledge, 2) adaptive attackers who know the type of deployed defenses but do not know the implementation details, and 3) oracle attackers who have full knowledge about the deployed defense mechanisms.\nDefender's Capability. We assume that the defender does not have control over the model training process and provides defense services after the target model has been deployed. The defender possesses a watermark dataset along with the corresponding watermark features extracted by the target model. During the watermark embedding process, the defender can modify the predictions of the target model. For watermark activation, the defender interacts with the suspicious model under black-box constraints with limited query numbers."}, {"title": "3 Watermark Transmission Model", "content": "Previous studies [8, 14, 21, 25] have empirically tackled challenges associated with triggerable watermarks, such as achieving a high watermark success rate. However, some fundamental questions remain unexplored:\n1. Why are watermarks transmitted to stolen models even when they are not explicitly activated?\n2. Why do existing methods struggle against adaptive attacks? Specifically, what aspects of watermarking strategies affect the watermark success rate?\nThe first question explores whether we can interpretably design watermarking methods that operate without relying on"}, {"title": "3.1 Elements", "content": "Initially, we modeled the watermark transmission process following the classic message transmission model. As illustrated in Fig. 3, the model owner designs a watermark represented by the message W. This message is then encoded by an encoder, denoted as fn, according to a predefined strategy. The encoded message is transmitted via the model outputs, which serve as the communication channel. If the attacker successfully receives and decodes the watermark message, it becomes embedded in the stolen model.\nAccording to the model, three elements determine the performance of watermark transmission:\nSource. The format of watermark message W determines the amount of information contained in it, which can be quantified using information entropy:\n$H(W) = -P(W)\\log(P(W))$\nwhere P(W) is the probability distribution of W.\nEncoding. In watermark transmission scenarios, the encoding process determines how watermarks change model outputs, which can described as:\n$\\hat{O} = O \\oplus f_n(W,A)$\nwhere A is the alphabet for encoding, \\hat{O} and O denote the modified and the original output, respectively.\nChannel. According to Shannon's theorem [34], the channel capacity C can be calculated by:\n$C = \\arg \\max \\limits_{P(X)} I(X,Y)$\nFor watermark transmission, X and Y both represent the output probability, thus we have X = Y = O, and C = H(O)."}, {"title": "3.2 Theoretical Analysis", "content": "Given the three elements, we can answer the following questions by analyzing the features of these elements in the triggerable watermark transmission scenario.\n1. What is the message generated by the source in the transmission of different watermarks?\nFor non-triggerable watermarking method DAWN [36], it randomly flips 0.5% of the labels of input samples and utilize these samples as watermarks. Therefore, the watermark message for DAWN is \"Does the sample belong to the watermark set?\", and H(W) \u2248 0.05 bit.\nHowever, for triggerable watermarks [8, 14, 21, 25], the source information can not be \"Does the sample contain the trigger?\" because triggers are not distributed in the surrogate dataset, making H(W) \u2248 0.0. Instead, we observe that the similarities of input samples and watermark triggers are transmitted. As depicted in Fig. 4, by gradually replacing the corresponding region with the watermark trigger, the probability of the target class gradually increases (gray line). A similar trend can be observed on extraction query examples(blue plots), too. Previous work uses the long-tailed effect of backdoors to characterize this phenomenon [44]. As the result, the stolen model will learn the relationship between trigger similarity and output probability of the target class.\n2. Why existing triggerable watermarks fail to transmit in adaptive attack scenarios?\nBased on the observations in Question 1, we address Question 2 by analyzing and comparing the channel capacity and transmission rate under different model and attack settings. Some adaptive attacks narrows the channel capacity (e.g., Top-1 Attack), while others introduce noise in watermark transmission [7, 13, 23, 37]. Initially, we provide an running example following Fig. 4 to show why existing methods are effective against naive attacks:\nWe begin by estimating the information entropy of the transmitted similarities. Assuming the similarity distribution follows a normal distribution, N(\u03bc, \u03c3\u00b2), the information entropy is given by H(W) = \u00bd ln(2\u03c0\u03b5\u03c3\u00b2), where \u03c3\u00b2 = E[(X \u2212 \u03bc)2]"}, {"title": "4 Neural Honeytrace Design", "content": "In this section, we introduce our watermarking framework, Neural Honeytrace. We first provide the brief workflow of Neural Honeytrace in Sec. 4.1. Then we provide detailed description of training-free watermark embedding and multi-step watermark transmission in Sec. 4.2 and Sec. 4.3."}, {"title": "4.1 Overview", "content": "Different from existing methods which implant watermarks into the target model via retraining or finetuning, Neural Honeytrace aims at implanting triggerable watermarks into the stolen model during model extraction attacks without introducing additional training cost.\nMotivated by the watermark transmission model, we have observed that the watermark information transmitted by triggerable watermarking is the similarity of query features and watermark triggers. Therefore, Neural Honeytrace directly embeds the similarity into the predictions. Meanwhile, model extraction attackers may utilize label recover strategies to remove watermark information encoded in the predictions, which can be considered as noises introduced in the channel of watermark transmission. The channel capacity may also be limited in several conditions (e.g., quantized output and hard-label output), making it difficult to transmit the full watermark information in a single query. To address this challenge, Neural Honeytrace utilizes similarity-based label flipping to achieve multi-step watermark transmission, which is robust against different prediction recover strategies and applicable for limited channel capacity.\nFig. 5 provides an overview of the workflow of generic model extraction attacks and Neural Honeytrace. For model extraction attacks, attackers first use an unlabeled surrogate dataset to query the target model through interfaces provided by the model owner. After getting the predictions, attackers may adopt different label recover strategies to erase potential watermark information and reconstruct a retraining dataset using the recovered predictions and original inputs. This dataset is then used to train the surrogate model and reproduce the functionality of the target model locally.\nFor Neural Honeytrace, defenders initially select watermarks to be transmitted from the watermark pool (e.g., random watermark, semantic watermark, and composite watermark etc.). Then, the target model extracts the watermark features and query features respectively and makes predictions on the query samples. Neural Honeytrace calculates the similarity of these two features and embeds the similarity into the predictions by mixing the reference logits (target logits of watermarking) and the original logits following the similarity value. Subsequently, Neural Honeytrace establishes a probability-guided label-flipping matrix according to the similarity. By encoding similarity values into the prediction distribution of different queries, this matrix enables multi-step watermarking transmission.\nIn stolen model detection and ownership claim, defenders use watermarked samples to query the suspicious model and perform a hypothesis test utilizing the predictions. According to Eq. 2, given the watermark success rate (i.e., the ratio that suspicious model makes certain predictions on watermarked samples), if the total number of watermarked samples is lager than the lower bound, then the suspicious model can be considered statistically significant as having a watermark."}, {"title": "4.2 Training-free Watermark Embedding", "content": "As we analyzed in Sec. 3.2, the information transmitted by triggerable watermarks is the similarity of query features and watermark features, and the message channel is the output predictions. Therefore, we can directly calculate and encode the similarity into predictions without additional training. So the initial questions become watermark selection, similarity calculation, and similarity embedding.\nWatermark generation. The form of watermarks will influence the cost of watermarking process. Previous methods have introduced different watermark generation strategies, e.g., EWE [14] utilized white pixel blocks as triggerable watermarks, while Composite Backdoor [21] and MEA-Defender [25] used spliced in-distribution samples as triggers. According to the information bottleneck theory [39], the training process of neural networks is solving a min-max mutual information problem. For any i \u2265 j it holds that:\n$I(X;Y) > I(Y; f_j) \\ge I(Y; f_i) \\ge I(Y;\\hat{Y})$\nwhere X, Y are input samples and corresponding labels, \u0176 is the prediction, and fi, f; denotes the latent features of the i-th layer and the j-th layer, respectively. Intuitively, label-independent information will gradually lost during the forward process of neural networks. And if the watermark only contains out-of-distribution features (e.g., white pixel blocks), it will introduce additional information for the stolen model to learn and make the watermark transmission process harder to converge. Therefore, Neural Honeytrace adopts composite in-distribution samples as watermarks in the default configuration. We also evaluated Neural Honeytrace with different watermark forms in Sec. 5.3.\nSimilarity calculation. After selecting the watermark forms, Neural Honeytrace calculates the distances between input queries and registered watermarks. This process is also performed in the last latent space following the information bottleneck theory. Specifically, the similarity of input query X and registered watermarks can be calculated as:\n$s = d - \\frac{1}{N} \\sum\\limits_{i=1}^{N} [f_{-1}(X) - f_{-1}(W_i)]^2$\nwhere d is a hyperparameter which balances the watermark success rate and model usability, f\u22121(.) represents the last layer of the target model, and W; denotes the i-th watermark. Intuitively, Eq. 6 calculates the average similarity of the input query and registered watermarks.\nAdditionally, considering that the attackers probably will not have a large amount of in-distribution data for querying the target model, we adopt a simple algorithm to minimize the impact of watermarking on model availability as follows:\n$s = \\begin{cases}s^2, & \\text{if } Max(SoftMax(F(X))) \\ge 0.95\\\\s, & \\text{else wise}\\end{cases}$"}, {"title": "4.3 Multi-step Watermark Transmission", "content": "Training-free watermark embedding enables plug-and-play watermarking, however, the message channel is not always ideal. As analyzed in Sec. 3.2, in hard-label black-box scenarios, the channel capacity may not able to effectively transmit watermark information. Also, adaptive attackers may adopt different label recover strategies, which can be considered as noises in watermark transmission. Therefore, we propose the multi-step watermark transmission strategy to enhance the robustness of Neural Honeytrace.\nEmbedding watermarks in label distribution. Note that a well-trained neural network will establish a mapping from the input distribution to the label distribution, Neural Honeytrace proposes to embed watermark information in the distribution of predictions. Specifically, for a query sample Xq and the corresponding watermark similarity s calculated using Eq. 6, Neural Honeytrace flips the label by probability according to the following equation:\n$\\mathbb{I}_{flip} = \\begin{cases} \\mathbb{I}_{ref} + \\epsilon, & \\text{if Bernoulli}(s) = 1\\\\ \\mathbb{I}_{flip}, & \\text{if Bernoulli}(s) = 0\\end{cases}$\nwhere \u03b2 > 1 is used to balance model availability and watermark success rate, Bernoulli(.) randomly samples with probability to decide whether to flip the label, and \u03b5 is a small random value to maintain randomness. Intuitively, for s \u2192 1, the input sample will be labeled as the target class with a high probability. And by controlling \u03b2, defenders can determine the flipping ratio for samples with s < 1. Eq. 9 links the predicted labels to the watermark similarities. As a results, Neural Honeytrace transmits watermark information to the label distribution of the surrogate dataset owned by attackers.\nCombining training-free watermark embedding and multi-step watermark transmission, Algorithm 1 summarizes the workflow of Neural Honeytrace."}, {"title": "5 Experiments", "content": "In this section, we evaluate Neural Honeytrace against different model extractions and compare it with previous watermarking methods. We first introduce the experiment setup in Sec. 5.1. Then we compare the overall performance of different watermarking strategies against adaptive attacks in Sec. 5.2. In Sec. 5.3, we provide ablation study to further analyze Neural Honeytrace."}, {"title": "5.1 Experiment Setup", "content": "Datasets. We use four different image classification datasets to train the target model: CIFAR-10 [19], CIFAR-100 [19], Caltech-256 [11], and CUB-200 [43].\nWe use another two datasets as surrogate datasets used by model extractions attackers, TinyImageNet-200 [29] for querying target models trained on CIFAR-10 and CIFAR-100, ImageNet-1K [10] for querying target models trained on Caltech-256 and CUB-200.\nModels. We use two model architectures to train target models: VGG16-BN [35] for CIFAR-10 and CIFAR-100, and ResNet50 [12] for Caltech-256 and CUB-200. Following previous model extraction defenses [32, 37], the same architectures are used for surrogate models.\nMetrics. We use three metrics to evaluate the effectiveness of different watermarking methods:\n1. Protected Accuracy indicates the accuracy of the protected model on clean samples, i.e., clean accuracy."}, {"title": "5.2 Experimental Results", "content": "Overall performance comparison. We begin by comparing Neural Honeytrace with four baseline methods across four different datasets and six model extraction attack strategies. The experimental results for target models trained on CIFAR-10, CIFAR-100, Caltech-256, and CUB-200 are presented in Tab. 2, Tab. 3, Tab. 4, and Tab. 5, respectively. For each defense, we report both the average and maximum extraction accuracy, as well as the average and minimum Watermark Success Rate (WSR). This approach highlights the effectiveness of various watermarking strategies under both optimal and worst-case scenarios, simulating an average attacker and more sophisticated attackers who select the most effective attack methods. We make the following additional observations based on the experiment results:\nExisting watermarking methods are sensitive to dataset scale and data complexity. As shown in Tab. 2 and Tab. 3, for smaller datasets and simpler data, existing watermarking methods are effective against Naive Attacks. However, for larger and more complex datasets, as demonstrated in Tab. 4, in four baseline methods, only MEA-Defender maintains acceptable success rates against Naive Attack. This is due to the increased complexity of the data, which leads to an increase in the information entropy of the output prediction. As a result, the watermark transmission process is subject to more channel noise and requires more robust watermark embedding strategies. Intriguingly, sometimes the sensitivity of dataset may also lead to higher watermark success rate. As shown in"}, {"title": "5.3 Ablation Study", "content": "In this section, we comprehensively evaluate the performance of Neural Honeytrace with different defense settings and against different attack settings.\nDifferent Watermark Triggers. We first compare the watermark success rate of Neural Honeytrace using different watermark triggers. As discussed in Sec.4.2, Neural Honeytrace supports the transmission of various watermarks, and the form of these watermarks influences the cost of the watermarking process. In Tab.6, we evaluate three different watermark triggers: white pixel blocks (used in EWE [14]), a semantic object (e.g., a specific copyright logo), and a composite trigger (the default configuration). For the first two trigger types, we adjust the hyperparameters to ensure that the protected model maintains accuracy similar to that of the default configuration.\nAs shown in Tab.6, the composite trigger achieves the highest average watermark success rate among the three triggers, which aligns with the analysis in Sec.4.2. Compared to the semantic object trigger, white pixel blocks yield a higher average watermark success rate due to their simpler features, making them easier to learn. However, even the semantic object trigger remains robust against various adaptive attacks. Given its interpretable semantics (e.g., the owner's logo), it could still have potential applications in real-world scenarios.\nDifferent Query Datasets. Depending on the capabilities and prior knowledge, attackers may use different surrogate datasets to query the target model. In Tab.6, we compare"}, {"title": "6 Related Work", "content": ""}, {"title": "6.1 Model Extraction Attack", "content": "Existing research reveals that machine learning models have different security risks throughout their lifecycle, threatening the availability [46, 49], integrity [26, 50], and confidentiality [2, 5]. Among these malicious attacks, model extraction attacks aim to breach the confidentiality of closed-source models for two goals: (1) rebuilding the functionality of the target model and use it without payment [40], or (2) conducting black-box attacks on the target model via surrogate models [22]. In this paper, we focus on model extraction attacks that attempt to steal functionality.\nNaive Attacks. Previous work have explored using different query strategies to perform model extraction attacks. For example, KnockoffNet [31] and ActiveThief [33] select representative natural samples for querying. Subsequent research found that samples close to the decision boundary contain more parameter information, motivating utilizing synthetic (adversarial) samples to perform attacks. In practice, JBDA-TR [15] utilized the feedback of the target model to guide the synthesis process, while FeatureFool [47] used different adversarial attacks to generate query samples. With the development of data-free distillation techniques [48], some methods utilize generative models to generate query samples (e.g., MAZE [16] and MEGEX [28].)\nAdaptive Attacks. To mitigate the threat of model extraction attacks, various defense strategies have been proposed, including detection-based defenses [15, 17] and perturbation-based defenses [45]. In response, several adaptive model extraction attacks have been developed to bypass these defenses. The S4L Attack [13] combines cross-entropy loss with a semi-supervised loss to extract more information with a limited number of queries. The Smoothing Attack [23] augments each sample multiple times and averages the predictions to train the model. D-DAE [7] trains both a defense detection model and a label recovery model to detect and bypass potential defenses. The p-Bayes Attack [37] utilizes independent and neighborhood sampling for real label estimation."}, {"title": "6.2 Model Watermarking", "content": "White-box watermarking. White-box watermarking methods aim at defending direct model stealing and embed watermark information in parameters or architectures. For example, Uchida et al. [42] and Adi et al. [1] both embed watermarks in the parameter space and utilize embedding matrix and meta classifier to verify watermarks, respectively. However, these methods require a white-box access to the stolen model, making them less practicable.\nBlack-box watermarking. In black-box conditions, defenders can only query the suspicious model through certain interfaces. Namba et al. [30] use a set of sample-label pairs to embed a backdoor-like watermark in the target model and verify the ownership by activating the backdoor in the stolen model. The subsequent method SSL-WM [24] migrated this method to protect pretrained models by injecting task-agnostic backdoors. Nevertheless, these methods are ineffective against model extraction attacks, because the watermarks fail to transmit from protected models to stolen models.\nSzyller et al. [36] proposed a simple watermarking strategy against model extraction attacks by randomly mislabeling some input queries, but it relied on a strong assumption to achieve ownership declaration that malicious queries can not be far fewer than benign queries. Other methods attempted to enhance the success rate of watermark transmission [8, 14,25]. For example, EWE [14] links watermarking learning to main task learning closely by adding additional regular terms. As a result, the stolen model will effectively inherit the watermark from the protected model while learning the main task. On the basis of EWE, MEA-Defender [25] adopted the composite backdoor [21] as watermarks, further enhancing the success rate of watermark transmission. However, there is still no effective theoretical framework for watermarking against model extraction attacks."}, {"title": "7 Conclusion, Limitations, and Future Work", "content": "In this paper, we propose Neural Honeytrace, a plug-and-play watermarking framework. We first model watermark transmission problem from the perspective of information theory. Based on the theoretical analysis, we propose two watermarking strategies: training-free watermark embedding and multi-step watermark transmission, to achieve training-free watermarking and enhance the robustness of watermarking. Experimental results show that Neural Honeytrace is significantly more robust against adaptive attacks. It is also more flexible and can be adjusted or removed in time if needed.\nAlthough Neural Honeytrace mostly maintains the availability of the protected model, it will still introduce slight performance decrement on the target model, which poses the need for distinguishing benign and malicious queries. Neural Honeytrace adopts a simple algorithm to detect potential malicious attacks, which will fail when attackers have access to the training dataset of the protected model. In this case, Neural Honeytrace requires a larger sample size for stolen model detection and ownership declaration. One potential solution is to adaptively and dynamically adjust the parameters of Neural Honeytrace for suspicious and trusted users based on their historical behavior.\nThe most relevant future work is to implement Neural Honeytrace on generative models such as Stable-Diffusion. Although we mainly evaluated Neural Honeytrace on classification tasks following existing methods, our watermark transmission model suggests that the larger output space of generative models may be able to effectively transmit more watermark information in one query."}, {"title": "A Theoretical Justification of Equation 5", "content": "Denote the watermark information as s1, noise signals introduced by Smoothing Attack [23] as s2, s3,..., sk. Then the transmitted message signal y and the corresponding noise signal n can be represented by:\ny = \\frac{s_1+s_2 + ... + s_k}{k}, n = \\frac{s_2 + s_3 + ... + s_k}{k-1}\nDefine an error as a received signal that deviates from the transmitted signal by more than a certain threshold \u03b5, then the error event is:\n|y - s_1| = | \\frac{s_1+s_2 + ... + s_k}{k} - s_1 | > \\epsilon\nLet X = s2 + s3+..+ sk denote the sum of all noises, assume that s2, s3,.., sk are i.i.d and with mean \u00b5s and variance \u03c32, according to the central limit theorem, the distribution of X approximately obeys the normal distribution:\nX ~ N ((k \u2212 1) \u00b7 \u00b5s, (k \u2212 1))\u00b7 \u03c32)\nThus, the distribution of y - s1 approximately obeys:\ny-s1 ~ N(\\frac{k-1}{k} \u00b7 (\u00b5_s - s_1), \\frac{(k-1)}{k^2} \u00b7 \u03c3^2)\nTherefore, the error probability (i.e., the probability that |y - s1| > \u03b5) satisfies:\nP(y-s1 > \u03b5) = P(N(\\frac{k-1}{k} \u00b7 (\u00b5_s - s_1), \\frac{(k-1)}{k^2} \u00b7 \u03c3^2) > \u03b5)\nLet the standardized z-distribution be:\nz = \\frac{y-s_1 - \\frac{(k-1)}{k} \u00b7 (\u00b5_s - s_1)}{ \\sqrt{\\frac{(k-1)}{k^2} \u00b7 \u03c3^2} }\nThen the error probability satisfies:\nP(y-s1 > \u03b5) = 2\u00b7Q(\\frac{\u03b5 - \\frac{(k-1)}{k} \u00b7 (\u00b5_s - s_1)}{ \\sqrt{\\frac{(k-1)}{k^2} \u00b7 \u03c3^2}})\nwhere Q(\u00b7) is the tail probability function of the standard normal distribution and is a monotonically decreasing function."}, {"title": "B Additional Experimental Results", "content": ""}, {"title": "B.1 Overhead of Different Defenses", "content": "Fig. 10 compares the defense overhead of different triggerable watermarking methods for protecting target models trained on various datasets. During the training phase, previous triggerable watermarking methods introduce additional time costs"}]}