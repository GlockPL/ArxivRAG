{"title": "LEARNING MULTIPLE INITIAL SOLUTIONS TO OPTIMIZATION PROBLEMS", "authors": ["Elad Sharony", "Heng Yang", "Tong Che", "Marco Pavone", "Shie Mannor", "Peter Karkus"], "abstract": "Sequentially solving similar optimization problems under strict runtime constraints is essential for many applications, such as robot control, autonomous driving, and portfolio management. The performance of local optimization methods in these settings is sensitive to the initial solution: poor initialization can lead to slow convergence or suboptimal solutions. To address this challenge, we propose learning to predict multiple diverse initial solutions given parameters that define the problem instance. We introduce two strategies for utilizing multiple initial solutions: (i) a single-optimizer approach, where the most promising initial solution is chosen using a selection function, and (ii) a multiple-optimizers approach, where several optimizers, potentially run in parallel, are each initialized with a different solution, with the best solution chosen afterward. We validate our method on three optimal control benchmark tasks: cart-pole, reacher, and autonomous driving, using different optimizers: DDP, MPPI, and iLQR. We find significant and consistent improvement with our method across all evaluation settings and demonstrate that it efficiently scales with the number of initial solutions required.", "sections": [{"title": "INTRODUCTION", "content": "Many applications, ranging from trajectory optimization in robotics and autonomous driving to portfolio management in finance, require solving similar optimization problems sequentially under tight runtime constraints (Paden et al., 2016; Ye et al., 2020; Mugel et al., 2022). The performance of local optimizers in these contexts is often highly sensitive to the initial solution provided, where poor initialization can result in suboptimal solutions or failure to converge within the allowed time (Michalska & Mayne, 1993; Scokaert et al., 1999). The ability to consistently generate high-quality initial solutions is, therefore, essential for ensuring both performance and safety guarantees.\nConventional methods for selecting these initial solutions typically rely on heuristics or warm-starting, where the solution from a previously solved, related problem instance is reused. More recently, learning-based solutions have also been proposed, where neural networks are used to predict an initial solution. However, in more challenging cases, where the optimization landscape is highly non-convex or when consecutive problem instances rapidly change, predicting a single good initial solution is inherently difficult.\nTo this end, we propose Learning Multiple Initial Solutions (MISO) (Figure 1), in which we train a neural network to predict multiple initial solutions. Our approach facilitates two key settings: (i) a single-optimizer method, where a selection function leverages prior knowledge of the problem instance to identify the most promising initial solution, which is then supplied to the optimizer; and (ii) a multiple-optimizers method, where multiple initial solutions are generated jointly to support the execution of several optimizers, potentially running in parallel, with the best solution chosen afterward.\nMore specifically, our neural network receives a parameter vector that characterizes the problem instance and outputs K candidate initial solutions. The network is trained on a dataset of problem instances paired with (near-)optimal solutions and is evaluated on previously unseen instances. Crucially, the network is designed not only to predict good initial solutions-those close to the optimal-but also to ensure that these solutions are sufficiently diverse, potentially spanning all underlying modes of the problem in hand. To actively encourage this multimodality, we implement"}, {"title": "RELATED WORK", "content": "Learning for optimization.\nAdvancements in machine learning have introduced numerous learning-based approaches to optimization problems (Sun et al., 2019). Early work by Gregor &\nLeCun (2010) replaced components of classical convex optimization algorithms with neural networks. More recent works aim to replace optimization methods entirely with end-to-end neural networks (OpenAI et al., 2020; Mirowski et al., 2017) or generate new optimization algorithms (Chen et al., 2022b) for specific classes of problems. Other works enhance optimization-based control"}, {"title": "INITIALIZING OPTIMIZERS", "content": "Problem setup. In the most general form, we need to solve instances of a parameterized optimization problem,\n$x^{*}(\\psi) = \\underset{x}{\\arg \\min} J(x; \\psi) \\text{ s.t. } g(x; \\psi) \\leq 0, h(x; \\psi) = 0,$\nwhere $x \\in \\mathbb{R}^{n}$ is the variable vector to be optimized, $J$ is the objective function, $g$ and $h$ are collections of inequality and equality constraints, and $\\psi \\in \\mathbb{R}^{m}$ is a parameter vector that defines the problem instance, e.g., parameters of the objective function and constraints that differ across problem instances. A local optimization algorithm, Opt, attempts to find an optimum of $J$, namely,\n$x^{*} = \\text{Opt}(J, \\psi, t_{lim}; x_{init}),$\nwhere $x_{init}$ is initial solution provided to the optimizer, and $t_{lim}$ is the runtime limit.\nHeuristic methods. A common choice of the initial solution, $x_{init}$, is the solution to a previously solved similar problem instance, referred to as a warm-start. For example, in optimal control the warm-start is typically the solution from the previous timestep, shifted and padded with zeros,\n$x_{w.s.} := \\{\\{x_{cand}\\}^{H-1}_{t=2}, 0\\}$ (Otta et al., 2015). This heuristic often works well in practice, however, it can struggle when large changes in the problem instance, $\\psi$, occur between consecutive time steps, leading to significant shifts in the optimal solution. For example, in autonomous driving, abrupt events like a traffic light switch or the sudden appearance of a pedestrian might drastically alter the reference trajectory or constraints. In such cases, the previous solution becomes a poor initialization, and the optimizer may fail to find a good solution within the allocated time frame."}, {"title": "LEARNING MULTIPLE INITIAL SOLUTIONS ( MISO)", "content": "The main idea of MISO is to train a single neural network to predict multiple initial solutions to an optimization problem, such that the initial solutions cover promising regions of the optimization landscape, eventually allowing a local optimizer to find a solution close to the global optimum. The key questions are then how to design a multi-output predictor; how to utilize multiple initial solutions in existing optimizers; and how to train the predictor to output a diverse set of initial solutions. In the following, we discuss our proposed solutions to these questions, illustrate the need for multimodality with a toy example, and discuss applications to optimal control."}, {"title": "MULTI-OUTPUT PREDICTOR", "content": "Our multi-output predictor is a neural network that takes the problem instance, $\\psi$, as input and outputs K initial solutions for the optimization problem,\n$\\left\\{x_{init}^{k}\\right\\}_{k=1}^{K} = f(\\psi; \\theta),$\nwhere $\\theta$ are the learned parameters of the network. We train the network on a dataset of problem instances and their corresponding (near-)optimal solutions, $\\left\\{(\\psi^{i}, x^{*i})\\right\\}_{i=1}^{N}$. Such dataset can be generated offline, for example, by running a slow yet globally optimal solver, or allowing the same local optimizer to run with longer time limits, potentially many times from different initial solutions."}, {"title": "OPTIMIZATION WITH MULTIPLE INITIAL SOLUTIONS", "content": "We propose two distinct settings to leverage multiple initial solutions: single-optimizer and multiple-optimizers. The resulting frameworks are illustrated in Fig. 1.\nSingle optimizer. In the single-optimizer setting we run a single instance of the optimizer with the most promising initial solution, $x^{*} = \\text{Opt}(J, \\psi, t_{limit}; x_{init}^{*})$. We introduce a selection function, $\\Lambda$, which, given a set of candidate solutions and the problem instance $\\psi$, returns the most promising candidate, $x_{init}^{*} = \\Lambda(\\left\\{x_{init}^{k}\\right\\}_{k=1}^{K}, \\psi)$. A reasonable choice for $\\Lambda$ used in our experiments is selecting the candidate that minimizes the objective function the optimizer aims to minimize, i.e., $\\Lambda := \\underset{k}{\\arg \\min} J(x_{init}^{k}; \\psi)$. Other possibilities include risk measures, metrics based on performance stability, robustness, exploration, or domain-specific metrics that align with the objectives of the overall task.\nMultiple optimizers. In the multiple-optimizers setting, we assume multiple instances of the optimizer can be executed in parallel. We then initialize each optimizer with a different initial solution, $x_{k}^{*} = \\text{Opt}_{k}(J, \\psi, t_{limit}; x_{init}^{k}), k \\in \\{1, ..., K\\}$. To select a single solution from the outputs of the optimizers, we can use the same selection function $\\Lambda$, as in the previous case, e.g., the solution that minimizes the objective function.\nOur framework can be trivially generalized to allow a different number of optimizers and initial solution predictions, as well as using a heterogeneous set of optimization methods. Further, to maintain performance guarantees, one may include the default, e.g., warm-start, solution as one of the considered initial solutions, which ensures that even with poor predictions, the final solution quality does not degrade."}, {"title": "TRAINING STRATEGIES", "content": "The ultimate goal is to predict multiple initial solutions so that the downstream optimizer can find a solution close to the global optima, i.e., $J(x^{*}; \\psi) \\approx J(x^{*};\\psi)$. Training a neural network directly for this objective is not feasible in general. Instead, we propose proxy training objectives that combine two terms: a regression term that encourages outputs to be close to the global optimum, e.g., $L_{reg}(x_{init}^{k}, x^{*}) = ||x_{init}^{k} - x^{*}||$, where $|| . ||$ is a distance metric; along with a diversity term that promotes outputs being different from each other, thereby covering various regions of the solution space. An illustrative example is in Sect. 4.4. In the following, we present three simple training strategies promoting diversity and preventing mode collapse. We discuss alternative formulations, with probabilistic modeling and reinforcement learning, in Sect. 7."}, {"title": "ILLUSTRATIVE EXAMPLE", "content": "To illustrate the advantage of using a single model with multimodal outputs compared to regression models or ensembles of regressors, we examine a straightforward one-dimensional optimization problem aimed at minimizing the cost function c(x) shown in Fig. 2 (top). The function features two global minima, denoted as A and C, with a local minimum located between them at B.\nApplying our learning framework to this simple problem, the dataset of optimal solutions includes instances of A and C. A single-output regression model has no means to distinguish the two modes and inevitably learns to predict the mean of examples in the dataset, somewhere near B. Consequently, the local optimizer is likely to converge to the suboptimal local minimum at B. Constructing an ensemble of such models to generate multiple initial solutions does not mitigate this issue, as each ensemble member tends to be biased toward the mean of the two modes near B. We implemented the optimization problem and showed the predictions for different training strategies in Fig. 2 (bottom). Details are in Appendix A.5. Indeed, an ensemble of single-output predictors fails to predict a global optimum, while our multi-output predictor succeeds with winner-takes-all and mixture losses.\nWhile the problem considered here is purposefully simplistic, the existence of local minima is the key challenge in most optimization problems."}, {"title": "APPLICATION TO OPTIMAL CONTROL", "content": "MISO is applicable to a broad class of sequential optimization problems; however, for the sake of evaluation, we focus on optimal control problems. Optimal control has a wide range of applications, e.g., in robotics, autonomous driving, and many other domains with strict runtime requirements, and due to the complexity induced by constraints and non-convex costs, local optimization algorithms are highly sensitive to the initial solution.\nIn optimal control the optimization variable a represents a trajectory defined as a sequence of states and control inputs over discrete time steps: $\\tau = \\{s_{t}, u_{t}\\}_{t=1:H}$. Here, $s_{t} \\in \\mathcal{S}$ and $u_{t} \\in \\mathcal{U}$ denote the state and control input at time step $t \\in \\mathbb{Z}_{+}$, and $H \\in \\mathbb{Z}_{+}$ is the optimization horizon. The constraints involve adhering to the system dynamics $f_{d}(s_{t+1}, s_{t}, u_{t}) = 0$, starting from an initial state $s_{0} = s_{curr}$, where $s_{curr}$ represents the system's current state. The problem instance parameters $\\psi$ encompass the initial state $s_{0}$, and other domain-specific variables that parameterize the objective function or constraints, such as target states, reference trajectories, obstacle positions, friction coefficients, temperature, etc.\nA specific property of optimal control problems is that the relationship between optimization variables, states $s_{t}$ and controls $u_{t}$, are defined by the dynamics constraint $f_{d}$; and the initial state $s_{0}$ is given. Therefore, a sequence of controls uniquely defines an (initial) solution. We can leverage this property by learning to predict only a sequence of controls instead of the full optimization variable of state-control sequences. Further, one can define the training loss over either control, state, or state-control sequences and backpropagate gradients through the dynamics constraint as long as it is differentiable. In our experiments, we use state-control loss by default as we found it to improve both our and baseline learning methods. In Appendix A.7, we show that our conclusions hold with control-only loss as well."}, {"title": "EXPERIMENTAL SETUP", "content": "Tasks. We evaluated our method on the three robot control benchmark tasks shown in Fig. 3, each employing a distinct local optimization algorithm. Cart-pole. This task involves balancing a pole upright while moving a cart toward a randomly selected target position (Barto et al., 1983), using a first-order box Differential Dynamic Programming (DDP) optimizer (Amos et al., 2018). Reacher. In this task, a two-link planar robotic arm needs to reach a target placed at a random positon (Tassa et al., 2018), using a Model Predictive Path Integral (MPPI) optimizer (Williams et al., 2015). Autonomous Driving. Based on the nuPlan benchmark (Caesar et al., 2021), this task focuses on trajectory tracking in complex urban environments by following a reference trajectory generated by a Predictive Driver Model (PDM) planner (Dauner et al., 2023), using the Iterative Linear Quadratic Regulator (iLQR) optimizer (Li & Todorov, 2004). Further details are in Appendix A.1 and Appendix A.2.\nBaselines. We compare MISO to a range of alternative methods to provide single or multiple initial solutions. For a single initial solution, we considered: Warm-start, the default method that uses the optimizer output from the last problem instance; Regression, a single-output regression model (the K = 1 version of MISO); Oracle Proxy, optimization with unlimited runtime, which we also used to generate our training data. For methods that generate multiple initial solutions, we considered: Warm-start with perturbations, which extends the warm-start approach by adding Gaussian noise to the optimizer output from the last problem instance; Regression with perturbations, where Gaussian noise is introduced to the predictions of the single-output regression model; Multi-output"}, {"title": "RESULTS", "content": "Our main results for optimization with different initial solutions are reported in Table 1 and Table 2 for single optimizer and multiple optimizers settings, respectively. Figure 4 shows the effect of the number of predicted initial solutions. Figure 5 provides qualitative results. More detailed results, including inference times, are in the Appendix.\nSingle optimizer. In the single-optimizer setting, Table 1, we first observe that even one learned initialization outperforms heuristic solutions (regression vs. warm-start), in almost all settings, and in particular in the most challenging autonomous driving task. We then examine the impact of generating multiple initial solutions. Perturbations-based methods show some improvement over their single-initialization counterparts in most cases, and ensembles of independently learned models perform consistently better than single models. Finally, our proposed multi-output methods demonstrate substantial improvements over all baselines because they can learn to predict diverse multimodal initial solutions. Specifically, MISO winner-takes-all or MISO mix achieve the lowest mean costs across all tasks. Considering the pairwise distance term alone proves insufficient to ensure adequate diversity, whereas incorporating it with MISO winner-takes-all often boosts performance, yet, its effectiveness varies, which underscores the challenge of selecting optimal hyperparameters. As expected, improvements are consistently larger in the more important sequential optimization setting, where errors over time compound.\nMultiple optimizers. When considering the multiple-optimizers setting, we observe the same trend. Learning-based methods outperform heuristic ones, and multi-output approaches yield further enhancements. As expected, the use of multiple optimizers leads to consistently better results compared to the single-optimizer setting due to increased exploration of the solution space.\nScaling with the number of initial solutions. Figure 4 shows that our method scales effectively and consistently with the number of predicted initial solutions K, and outperforms other approaches across varying values of K. Importantly, as K increases, the inference time for ensemble approaches grows, whereas MISO remains almost constant (see Appendix A.6). We further evaluate mode diversity in Appendix A.8, and find that, in line with our conclusions, all MISO outputs remain useful even when K increases.\nQualitative results. Figure 5 (left) depicts the optimizer's output trajectories with different initial solutions for the autonomous driving task. In this scenario, the high-level planner abruptly alters the reference path, which could happen, e.g., because of a newly detected pedestrian. The change in reference path makes the previous solution (warm-start) a poor initialization, and the optimizer converges to a local minimum that minimizes control effort but is far from the desired path. Regression and model ensemble also fail to predict a good initial solution. In contrast, MISO winner-takes-all adapts to this sudden reference change and closely follows the reference path. Figure 5 (right) depicts MISO's initial solutions for the cart-pole task. The different outputs capture different modes of the solution space (moving upright, moving with left swing, moving with right swing), showing MISO's ability to generate diverse and multimodal solutions."}, {"title": "CONCLUSIONS AND FUTURE WORK", "content": "We introduced Learning Multiple Initial Solutions (MISO), a novel framework for learning multiple diverse initial solutions that significantly enhance the reliability and efficiency of local optimization algorithms across various settings. Extensive experiments in optimal control demonstrated that our method consistently outperforms baseline approaches and scales efficiently with the number of initializations.\nLimitations. Our approach is not without limitations. First, to train a useful model, we rely on the coverage and quality of the training data, as the method does not directly interact with the optimizer or the underlying objective function. Second, the underlying assumption of our regression loss is that initial solutions closer to the global optimum increase the likelihood of successful optimization may not hold in complex optimization landscapes with intricate constraints. Third, in highly complex optimization problems where each solution constitutes a high-dimensional and intricate structure, accurately learning initial solution candidates can become exceedingly challenging, potentially diminishing the effectiveness of our approach.\nFuture work. There are several promising directions for future research. To address the aforementioned limitations, one may simply incorporate the optimization objective into the model training loss, thus creating a direct link to the final optimization goal. Alternatively, using reinforcement learning (RL) to train MISO is a particularly exciting opportunity. By framing the problem in an RL context, e.g., where the reward is the negative cost of the optimizer's final solution, models would be directly trained to maximize the probability of the optimizer finding the global optima and may learn to specialize to the specific optimizer. One challenge would be computational, as RL would require running the optimizer numerous times during training.\nOther extensions of our approach include probabilistic modeling, e.g., Gaussian mixture models, variational autoencoders, or diffusion models; however, preventing mode collapse and promoting diversity would remain a challenge. Future work may explore alternative selection functions, such as risk measures or criteria based on stability, robustness, exploration, or other domain-specific metrics; as well as using a heterogeneous set of parallel optimizers. Finally, we are excited about various possible applications in optimal control and beyond, where sequences of similar optimization problems need to be solved, for example, localization and mapping in robotics, financial optimization, traffic routing optimization, or even training neural networks with different initial weights, e.g., for meta-learning, or scene representation learning with Neural Radiance Fields or 3D Gaussian splatting."}, {"title": "APPENDIX", "content": "DETAILED DESCRIPTIONS OF BASELINE OPTIMIZERS\nThis subsection provides detailed descriptions of the optimization algorithms used in our evaluations: First-order Box Differential Dynamic Programming (DDP), Model Predictive Path Integral (MPPI), and the Iterative Linear Quadratic Regulator (iLQR). These algorithms were selected due to their widespread use and effectiveness in solving optimal control problems across various domains.\nFirst-order Box Differential Dynamic Programming (DDP). Building on the work of Tassa et al. (2014), Amos et al. (2018) introduced a simplified version of Box-DDP that utilizes first-order linearization instead of second-order derivatives. This approach, termed \"first-order Box-DDP,\" reduces computational complexity while maintaining the ability to handle box constraints on both the state and control spaces.\nModel Predictive Path Integral (MPPI). MPPI (Williams et al., 2015) is a sampling-based model predictive control algorithm that iteratively refines control inputs using stochastic sampling. Starting from the current state and a prior solution, it generates a set of randomly perturbed control sequences, simulates their trajectories, and evaluates them using a cost function. The control inputs are then updated based on a weighted average, favoring lower-cost trajectories. We use the implementation from https://github.com/UM-ARM-Lab/pytorch_mppi.\nIterative Linear Quadratic Regulator (iLQR). iLQR (Li & Todorov, 2004) is a trajectory optimization algorithm that refines control sequences iteratively by linearizing system dynamics and approximating the cost function quadratically around a nominal trajectory. It alternates between a forward pass, simulating the system trajectory, and a backward pass, computing optimal control updates. We use the implementation provided in the nuPlan simulator."}, {"title": "DETAILED TASK DESCRIPTIONS AND HYPERPARAMETERS", "content": "This subsection provides detailed descriptions of the tasks used in our experiments: cart-pole, reacher, and autonomous driving. For each task, we outline the system dynamics, control inputs, and the specific hyperparameters employed in our evaluations.\nCart-pole. The cart-pole task (Barto et al., 1983) involves a cart-pole system tasked with swinging the pole upright while moving the cart to a randomly selected target position along the rail. The goal is to balance the pole vertically and simultaneously reach the target cart position. The system is characterized by the state vector $s_{t} \\in \\mathbb{R}^{4}$, which includes the pole angle $\\theta$, pole angular velocity $\\dot{\\theta}$, cart position $x$, and cart velocity $\\dot{x}$. The control input is a single force applied to the cart, $u_{t} \\in \\mathbb{R}$.\nHyperparameters. The mass of the cart is $m_{c} = 1.0\\text{kg}$, the mass of the pole is $m_{p} = 0.3\\text{kg}$, and the length of the pole is $l = 0.5\\text{m}$. Gravity is set to $g = -9.81 \\text{m/s}^{2}$. Control inputs are bounded by $U_{min} = -5.5 \\text{N}$ and $U_{max} = 5.5 \\text{N}$, with a time step of $\\Delta t = 0.1 \\text{s}$ and $N_{sub\\_steps} = 2$ physics sub-steps per control step. Each episode has a maximum length of $T_{env} = 50$ steps. Both optimizers use goal weights of $[0.1, 0.01, 1.0, 0.01]$ for the state variables and a control weight of 0.0001. The prediction horizon is set to $H = 10$. For the online optimizer, we set $1qr\\_iter = 2$ and $max\\_linesearch\\_iter = 1$. In the oracle optimizer, we use $lqr\\_iter = 10$ and $max\\_linesearch\\_iter = 3$. The initial state $s_{0} \\in \\mathbb{R}^{4}$ is sampled as follows: $x_{0} \\sim \\mathcal{U}(-2, 2) \\text{m}$, $\\dot{x_{0}} \\sim \\mathcal{U}(-1, 1) \\text{m/s}$, $\\theta_{0} \\sim \\mathcal{U}(-\\frac{\\pi}{3}, \\frac{\\pi}{3}) \\text{rad}$, and $\\dot{\\theta_{0}} \\sim \\mathcal{U}(-\\frac{\\pi}{3}, \\frac{\\pi}{3}) \\text{rad/s}$. The goal state is defined by a target cart position $X_{goal} \\sim \\mathcal{U}(-2, 2) \\text{m}$, while the rest of the state variables (pole angle and velocities) are set to zero, ensuring the goal is to bring the pole upright and bring the system to rest.\nReacher. The Reacher task (Tassa et al., 2018) involves a two-link planar robot arm tasked with reaching a randomly positioned target. The goal is to move the end-effector to the target position in the plane. The system is characterized by the state vector $s_{t} \\in \\mathbb{R}^{4}$, which includes the joint angles $\\theta_{1}, \\theta_{2}$ and angular velocities $\\dot{\\theta_{1}}, \\dot{\\theta_{2}}$. The control inputs, $u_{t} \\in \\mathbb{R}^{2}$, are torques applied to each joint.\nHyperparameters. The simulation uses a time step of $\\Delta t = 0.02\\text{s}$, with joint damping set to 0.01 and motor gear ratios of 0.05. The control inputs are constrained by $U_{min} = [-1, -1]$ and $U_{max} = [1, 1]$. The wrist joint has a limited range of $[-160^{\\circ}, 160^{\\circ}]$. Each episode is limited to $T_{env} = 250$ steps. Both optimizers are set with a control noise covariance $\\sigma^{2} = 1 \\times 10^{-3}$, a"}, {"title": "NETWORK ARCHITECTURE AND TRAINING DETAILS", "content": "This section provides a brief overview of the network architecture, the data collection process, and the training procedures used in our experiments. We summarize the design of our base Transformer model, outline the methods used to generate and preprocess the training data, and detail the key training methodologies and hyperparameters employed.\nNetwork Architecture. The base model is a standard Transformer architecture with absolute positional embedding, a linear decoder layer, and output scaling. The core Transformer architecture remains standard, with task-specific configurations. The input to the network is the concatenated sequence of the warm-start state trajectory error, $\\tau_{WS}^{e}$, defined as the difference between the reference trajectory, $\\psi = \\tau_{r}$, (in the autonomous driving task) or the goal state, $\\psi = x_{G}$ (in the cart-pole and reacher tasks), and the warm-start trajectory, $\\tau_{WS}$. The warm-start control trajectory is $\\tau_{W.S.} = \\{\\{u_{cand}\\}_{t=2}^{H}, 0\\}$. The network predicts K control trajectories, $\\{x_{init}^{funk}\\}_{k=1}^{K}$, for the next optimization step. Each environment's configuration is described in Table 3."}, {"title": "DESCRIPTIONS OF BASELINE METHODS", "content": "This section introduces the baseline methods used in our experiments in more detail and discusses their implementation specifics. These baselines serve as reference points to evaluate our proposed methods' performance and understand the benefits and limitations of different initialization strategies in optimization algorithms.\nWarm-start (K = 1). A common technique involves shifting the previous solution forward by one time step and padding it with zeros. Assuming the system does not exhibit rapid changes in this interval, the previous solution should retain local, feasible information.\nOracle Proxy (K = 1). This proxy serves two purposes: (1) estimating the gap between a real-time-constrained optimizer and an unrestricted one and (2) providing a proxy for a mapping worth learning. For each optimization algorithm, a suitable heuristic is defined. In DDP, the oracle is allowed more iterations to converge; in MPPI, it has a larger sample budget; and in iLQR, it is given more time to perform optimization iterations.\nRegression (K = 1). This approach involves training a neural network to approximate the oracle's mapping. Unlike the oracle heuristic, which is impractical for real-time use, the trained neural network requires only a single forward pass.\nWarm-start with Perturbations (K >1). We utilize the warm-start technique as another baseline by duplicating the proposed initial solution K times and adding Gaussian noise. While this introduces some form of dispersion, the resulting initial solutions are neither guaranteed to be feasible nor to ensure any level of optimality.\nRegression with Perturbations (K >1). Similar to the warm-start with perturbation, after predicting an initial solution using the neural network, we duplicate it K times and add perturbations.\nEnsemble (K >1). An ensemble of K separate neural networks leverages the idea that networks initialized with different weights during training will often produce different predictions. The main drawbacks of this approach are (1) the need to train K neural networks and (2) the requirement to run K forward passes, which may be impractical for real-time deployment.\nMulti-Output Regression (K >1). A naive approach to predicting K initializations from a single network involves calculating the loss for each prediction and summing the losses. However, since there is no explicit multimodality objective, these models are prone to mode collapse.\nMISO Pairwise Distance (K >1). One way to mitigate mode collapse is to introduce an additional term in the loss function, such as the pairwise distance between predictions. While this approach requires weight tuning and selecting an appropriate norm, a significant challenge lies in understanding how effectively this term promotes multimodality in practice."}, {"title": "STATE LOSS", "content": "An additional challenge in learning control policies is addressing compounding errors\u2014small inaccuracies in the predicted control trajectory $T_{U}$ that, when unrolled, cause significant deviations in the state trajectory $T_{S}$. Even if most elements $T_{U}$ are accurate, errors in the initial steps can cause the state $T_{S}$ to drift, leading to further divergence as the system evolves.\nTo mitigate compounding errors, we introduce a regression loss not only over the control trajectory $T_{U}$ but also over the resulting state trajectory $T_{S}$. In a supervised learning setting, this requires a model of the system dynamics, which can either be known or learned. As we show in Table 6, incorporating state trajectory loss helps mitigate these types of error accumulation and improve long-horizon trajectory accuracy. More specifically, we see that (1) as the prediction horizon increases, from H = 9 (Cart-pole) to H = 40 (Driving), so does the difference between using and not using state loss, (2) The gap between One-Off and Sequential also increases thus we do not generalize as well, (3) For the single-output regression model, the difference is even greater."}]}