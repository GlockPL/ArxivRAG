{"title": "Articulated Object Manipulation using Online Axis Estimation with SAM2-Based Tracking", "authors": ["Xi Wang", "Tianxing Chen", "Qiaojun Yu", "Tianling Xu", "Zanxin Chen", "Yiting Fu", "Cewu Lu", "Yao Mu", "Ping Luo"], "abstract": "Articulated object manipulation requires precise object interaction, where the object's axis must be carefully considered. Previous research employed interactive perception for manipulating articulated objects, but typically, open-loop approaches often suffer from overlooking the interaction dynamics. To address this limitation, we present a closed-loop pipeline integrating interactive perception with online axis estimation from segmented 3D point clouds. Our method leverages any interactive perception technique as a foundation for interactive perception, inducing slight object movement to generate point cloud frames of the evolving dynamic scene. These point clouds are then segmented using Segment Anything Model 2 (SAM2), after which the moving part of the object is masked for accurate motion online axis estimation, guiding subsequent robotic actions. Our approach significantly enhances the precision and efficiency of manipulation tasks involving articulated objects. Experiments in simulated environments demonstrate that our method outperforms baseline approaches, especially in tasks that demand precise axis-based control.", "sections": [{"title": "I. INTRODUCTION", "content": "Robotic manipulation has a wide range of applications, such as industrial automation, medical surgery, and warehouse logistics. Among various manipulation tasks, those involving articulated objects, such as doors and drawers, are particularly challenging. This is because manipulating articulated objects requires not only a deep understanding of their overall geometry but also knowledge of their part compositions and the kinematic relationships between those parts [1].\nTraditional manipulation techniques, which typically rely on predefined kinematic models [2], [3] or open-loop control [4], have shown limitations in dynamically adapting to real-world interactions, especially in the presence of uncertainties. One significant drawback of open-loop approaches is their lack of feedback regulation, preventing the robot from adjusting its actions based on real-time interaction dynamics. Without continuous feedback, the system cannot adapt to changes in the object's state, leading to inefficiencies and inaccuracies in manipulation tasks.\nRecently, interactive perception has emerged as a promising approach to address these challenges [1], [5], [4], [6]. By actively interacting with the object, robots can gather real-time sensory data that provides insights into the object's structure and kinematics. Interactive perception aims to bridge the gap between perception and action by using sensory feedback from these interactions. While this method is effective in providing information about the object's structure and state, it lacks a critical component the evolving dynamic interactions between the robot and the articulated object over time. This omission limits the robot's ability to adapt its manipulation policy in real time as the object's state changes, leading to inefficiencies in tasks that require precise control. To address this limitation, in this paper, we propose a novel closed-loop pipeline that integrates interactive perception with online refined axis estimation, providing the robot with helpful guidance for axis-aware manipulation. The basic idea is illustrated in Fig. 1. Our approach builds on interactive perception and enhances it by using the guidance of an online"}, {"title": "II. RELATED WORK", "content": "A. Vision-based Robotic Manipulation\nVisual perception is crucial for robotic manipulation, whose quality heavily depends on the visual modalities. Existing works on vision-based robotic manipulation have adopted different types of visual input, with various visual modalities demonstrating strong potential in different scenarios, while also exhibiting limitations specific to their applications.\nRGB images, despite their richness in texture, lack depth information, which is critical for object manipulation and grasping tasks. To address this, UMPNet [9] and Grasp Proposal Networks [10] turned to depth sensors and combined RGB images with depth maps. RGB-D information provides a more direct representation of the 3D world, which, however, can be noisy and affected by transparent and reflective surfaces [11].\nTo circumvent the above challenge, unlike the recent approaches that switch between different visual modalities [12], [13], [14], RGBManip utilizes RGB-only visual input and directly estimates the 6D pose of objects from multi-view RGB images. It relies on a single RGB camera mounted on its end-effector as its primary visual input. As a result, during the pre-grasping phase, the camera predominantly captures only the immediate vicinity of the handle, inevitably missing crucial information about the interaction dynamics that occur throughout the manipulation process, which is critical for the robot to understand how articulated objects should be manipulated.\nOn the other hand, point-cloud-based methods have also demonstrated promising performance in vision-based robotic manipulation. Several studies such as Where2Act [15] and SAGCI-System [16] have integrated point cloud information to predict action affordances. Both methods are particularly well-suited for handling articulated objects, but RLAfford [10] emphasizes interaction dynamics which helps to predict manipulation policies effectively. Furthermore, Flowbot3D [17] leverages point cloud motion flows to predict how objects should be manipulated, whose strength is in handling dynamic articulated objects.\nOur approach strives to integrate the strengths of RGB-based methods and point cloud-based techniques by tracking RGB inputs and segmenting the point clouds corresponding to the motion part of articulated objects. In order to address the limitation of opened-loop methods, we integrate an additional RGB-D camera into the scene, specifically aimed at capturing the manipulation process.\nB. 3D Point Cloud Segmentation Powered by 2D Foundation Models\nSegmentation techniques for 2D images have reached a high level of sophistication, with models such as Segment Anything Model (SAM) [18] standing out as prominent examples. However, the limitations of segmentation methods for 3D point clouds have hindered the development of their application in robotic manipulation.\nDeepLabv3 [19] has adapted 2D image segmentation techniques to the 3D point cloud domain by projecting 3D data onto 2D planes and performing segmentation within the transformed space. It exhibits robustness when handling large-scale data, but the projection from 3D point clouds onto 2D planes can lead to a loss of information, which in turn compromises segmentation accuracy.\nTo overcome this challenge, researchers have proposed some SAM-based methods for 3D point cloud segmentation. Point-SAM [20] extended SAM to 3D domain, which can"}, {"title": "III. METHOD", "content": "We introduce a novel approach for manipulating articulated objects, guided by axis estimation derived from SAM2-based tracking. As depicted in Fig. 2, our pipeline consists of 3 core modules: (1) Interactive Perception & Init-Manipulation Module, (2) Tracking & Segmentation Module and (3) Axis Estimation & Manipulation Module.\nInitially, the robot applies any interactive perception methods to manipulate the articulated object, creating a slight displacement. Then, using a SAM2-based approach, the moving part of the object's point cloud in each frame is captured, followed by further manipulation guided by the estimated axis.\nA. Interactive Perception & Init-Manipulation Module\nThe Interactive Perception & Init-Manipulation Module employs flexible interactive perception methods to initiate"}, {"title": "IV. EXPERIMENT", "content": "A. Task Settings\nOur method specifically focuses on door-opening and drawer-opening tasks. We evaluate the performance of these tasks in scenarios with large ranges of door opening and drawer extension.\nIn each task, the articulated objects, placed with limited random position and rotation, begins in a closed state, and the robotic arm is initially positioned randomly in front of it. The arm must then accomplish the designated manipulation goal - either opening the drawer or door to a specific degree or range.\nThe task settings are detailed as follows:\n\u2022 Open Door: The agent needs to open the door larger than 8.6\u00b0, 10\u00b0, 20\u00b0, 30\u00b0, 40\u00b0, 45\u00b0, 50\u00b0, 55\u00b0, 60\u00b0, 65\u00b0, 70\u00b0.\n\u2022 Open Drawer: The agent needs to open the drawer larger than 10 cm, 15 cm, 20 cm, 25 cm, 30 cm, 35 cm, 40 cm, 45 cm.\nTo ensure fairness in comparison, all RGBManip components employ task-specific adapose as the pose estimator and heuristic pose as the controller among different tasks. All methods are compared under equivalent total step sizes, though different methods may allocate step sizes differently based on their respective policies.\nFor each task, we evaluate our methods compared with RGBManip and other baselines separately on RGBManip's training set and testing set. Success rates of the first 100 experiments are used as metrics for comparison respectively."}, {"title": "B. Baselines and Quantitative Results", "content": "We benchmark our approach against seven existing techniques (including RGBManip). Below is a brief summary of each comparison method.\n\u2022 DrQ-v2 [27]: Based on reinforcement learning (RL), it takes in the robot's state and RGB image to determine the desired 6D pose of the robot's end-effector.\n\u2022 LookCloser [28]: A multi-perspective RL model that leverages multi-view inputs and visual transformers to amalgamate data from various angles.\n\u2022 RGBManip [5]: Utilizes RGB-only visual input to directly estimates the 6D pose of objects from multi-view images.\n\u2022 Where2Act [15]: Processes point-cloud data to estimate the best point of interaction for manipulation.\n\u2022 Flowbot3D [17]: Predicts point-wise motion, or \u201cflow\u201d, within the point cloud. The point with the highest motion magnitude is selected for interaction.\n\u2022 UMPNet [9]: Utilizing RGB-D images, it predicts an action point in the image and projects it into 3D space using depth data.\n\u2022 GAPartNet [29]: A pose-centric method which predicts the pose of an object's part from point-cloud data.\nQuantitative results of basic tasks are summarized in Table. I, where both our method and RGBManip almost out-"}, {"title": "V. ANALYSIS", "content": "The analysis section delves into the specific experimental results to elucidate the practical advantages of our online axis estimation approach in the context of articulated object manipulation. Our method's performance is grounded in the empirical data obtained from the experiments, which are analyzed below to provide a detailed understanding of the improvements achieved.\nA. Online Axis Estimation vs. Traditional Methods\nOur experiments clearly demonstrate the superiority of our online axis estimation approach over traditional methods, especially in tasks that demand precise control such as door and drawer opening. As illustrated in Table I, our method achieves an impressive 87.0% success rate in the training set and 88.0% in the test set for the \"Open Door\" task, significantly outperforming RGBManip, which records 75.0% and 82.0% respectively. This notable enhancement stems directly from the continuous refinement of the axis estimation, empowering the robot to adapt its actions in response to the most current interaction dynamics.\nFurthermore, in the more demanding versions of these tasks, depicted in Figures 3 and 4, our method's reliance on ongoing online axis estimation reveals enhanced robustness in handling large-amplitude movements of articulated objects. The experimental data consistently show that our method sustains higher success rates as the complexity of tasks escalates, whether it involves opening doors to wider angles or drawers to greater extents. The online axis estimation process is pivotal in this regard, enabling our system to dynamically adjust to real-time interaction feedback and ensuring precise control over articulated objects. The continuous axis refinement, aligned with the object's changing state, is essential for the manipulation's accuracy and efficiency, particularly in tasks with substantial state variations. The real-time feedback loop is crucial, enabling the robot to make swift and accurate operational adjustments that accommodate the subtleties of the interaction, thereby achieving a higher success rate in task completion. This robust performance underscores the strength of online axis estimation in providing reliable and responsive control in sophisticated robotic manipulation scenarios.\nB. Consistency Across Various Manipulation Scenarios\nOur method's consistent performance across a range of manipulation scenarios highlights its robustness and versatility. The success rates in both door and drawer opening tasks, as depicted in Tables II and III, consistently show higher rates for our method, indicating that the online axis estimation is effective regardless of the specific manipulation task. This consistency is a significant advantage over methods that may perform well in one scenario but falter in others."}, {"title": "VI. CONCLUSION", "content": "In this research, we introduced a novel approach to articulated object manipulation that employs online axis estimation integrated with SAM2-based tracking. Our method utilizes segmented 3D point cloud data and motion axis estimation to enhance the precision and control of robotic actions. The experimental results suggest that our method shows promising improvements in handling large manipulation movements, indicating the potential utility of axis-based guidance in robotic manipulation tasks.\nThe online axis estimation technique has demonstrated beneficial in dynamically adjusting to the real-time interaction feedback, thereby enabling the robot to maintain better control over articulated objects. While our method has shown enhancements over traditional approaches, particularly in complex manipulation scenarios, it is important to acknowledge that there is still room for further refinement and optimization.\nOur findings indicate that the online axis estimation process contributes positively to the accuracy and efficiency of robotic manipulation. However, this study represents an initial step, and future work is necessary to fully explore the long-term reliability and scalability of this approach across a broader spectrum of tasks and environments."}, {"title": "C. Axis Estimation & Manipulation Module", "content": "The Axis Estimation & Manipulation Module calculates the axis of motion based on the segmented point cloud representing the articulated object's moving parts identified by the Tracking & Segmentation Module. Task-specific geometric priors play an essential role in achieving accurate axis estimation, i.e., for a prismatic joint, the moving parts translate along the axis, whereas for a revolute joint, the moving parts rotate around the axis. In our simulation settings, the drawer is designed to translate parallel to the xy-plane, while the door rotates about a hinge aligned with the z-axis. By leveraging these task-based priors, we are able to explicitly calculate the joint's axis. Consider a single action of robotic manipulation, where the OBBs of the initial and final point cloud of the motion components of the articulated object are denoted as $obb_{st}$ and $obb_{ed}$, with their centers designated as $O_{st}$ and $O_{ed}$ respectively. On the one hand, for a prismatic joint, the axis pivot point is defined as the center of $obb_{st}$, and the axis direction is estimated as the direction $\\overrightarrow{d} = \\overrightarrow{O_{st}O_{ed}}$. On the other hand, for a revolute joint, the axis pivot point is derived by identifying the intersection of the mid perpendiculars along the longer edges in the top-down view of $obb_{st}$ and $obb_{ed}$. With the axis point $P$ established, the axis direction is ascertained by evaluating the sign of the dot product $\\overrightarrow{d} \\cdot \\overrightarrow{t}$, where $\\overrightarrow{t} = \\overrightarrow{O_{st}P} \\times \\overrightarrow{z}$, among which $\\overrightarrow{z}$ represents the positive direction of the z-axis.\nWe propose a closed-loop axis estimation refinement policy predicated on a straightforward observation: as the manipulation of the joint assembly increases, the point cloud of its moving component becomes progressively more amenable to accurate reconstruction. The enhancement in the fidelity of the point cloud data, in turn, furnishes the axis estimation process with inputs that are increasingly precise and dependable. For implementation, after each manipulation, we invoke the Tracking & Segmentation Module and the Axis Estimation Module to ascertain the current axis estimation. The Manipulation Module then guides the robot's subsequent actions according to this estimation. The interactive process reiterates until the robot has executed all designated actions, with the axis estimation being continuously refined throughout the procedure."}]}