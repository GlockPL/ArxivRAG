{"title": "Leveraging Invariant Principle for Heterophilic Graph Structure Distribution Shifts", "authors": ["Jinluan Yang", "Zhengyu Chen", "Teng Xiao", "Wenqiao Zhang", "Yong Lin", "Kun Kuang"], "abstract": "Heterophilic Graph Neural Networks (HGNNs) have shown promising results for\nsemi-supervised learning tasks on graphs. Notably, most real-world heterophilic\ngraphs are composed of a mixture of nodes with different neighbor patterns, ex-\nhibiting local node-level homophilic and heterophilic structures. However, existing\nworks are only devoted to designing better HGNN backbones or architectures for\nnode classification tasks on heterophilic and homophilic graph benchmarks simul-\ntaneously, and their analyses of HGNN performance with respect to nodes are only\nbased on the determined data distribution without exploring the effect caused by\nthis structural difference between training and testing nodes. How to learn invariant\nnode representations on heterophilic graphs to handle this structure difference or\ndistribution shifts remains unexplored. In this paper, we first discuss the limitations\nof previous graph-based invariant learning methods from the perspective of data\naugmentation. Then, we propose HEI, a framework capable of generating invariant\nnode representations through incorporating heterophily information to infer latent\nenvironments without augmentation, which are then used for invariant prediction,\nunder heterophilic graph structure distribution shifts. We theoretically show that\nour proposed method can achieve guaranteed performance under heterophilic graph\ndistribution shifts. Extensive experiments on various benchmarks and\nbackbones can also demonstrate the effectiveness of our method compared with\nexisting state-of-the-art baselines.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) have emerged as prominent approaches for learning graph-structured\nrepresentations through the aggregation mechanism that effectively combines feature information\nfrom neighboring nodes [11, 40, 13, 14]. Previous GNNs primarily dealt with homophilic graphs,\nwhere connected nodes tend to share similar features and labels [41]. However, growing empiri-\ncal evidence suggests that these GNNs' performance significantly deteriorates when dealing with\nheterophilic graphs, where the majority of nodes connect with others from different classes, even\nworse than the traditional neural networks [23]. An appealing way to address this issue is to tailor the\nheterophily property to GNNs, extending the range of neighborhood aggregation and reorganizing\narchitecture [40], known as the heterophilic GNNs (HGNNs)."}, {"title": "Heterophilic Graph Structure distribution Shift(HGSS): A novel data distribution shift perspec-tive to reconsider existing HGNNs works.", "content": "Despite promising, most previous HGNNs assume the\nnodes share the determined data distribution [23, 22], we argue that there is data distribution disparity\namong nodes with different labels. As illustrated in Figure 1(a1), heterophilic graphs are composed\nof a mixture of nodes that exhibit local homophilic and heterophilic structures, i.e, the nodes have\ndifferent neighbor patterns 1. Here, we identify their varying neighbor patterns between train and test\nnodes as the Heterophilic Graph Structure distribution Shift (Figure 1(a2)). This kind of shift was\nneglected by previous works and truly affected GNN's performance. As shown in Figure 1(a3), we\nvisualize the HGSS between training and testing nodes on the Squirrel dataset. Compared with test\nnodes, the train nodes are more prone to be categorized into groups with high homophily, which may\nyield a test performance degradation. More statistical results on other heterophilic graph datasets\ncan be shown in the Appendix A.4. Notably, we acknowledge that some work [30] also discusses\nhomophilic and heterophilic structural patterns, but until now they don't provide a clear technique\nsolution for this problem. Exactly, apart from [30], our work is orthogonal to most HGNNs works\n[40] that focus on designing backbones or architectures without considering the HGSS issue.\nThus, it's extremely urgent to seek solutions from the perspective of data distribution to address the\nHGSS in the context of node-level tasks within heterophilic graphs."}, {"title": "Existing graph-based invariant learning methods perform badly for HGSS due to the envi-ronments construction strategy.", "content": "In the context of general distribution shifts, the technique of\ninvariant learning [32] is increasingly recognized for its efficacy in mitigating these shifts. The\nfoundational approach involves learning node representations to facilitate invariant predictor learning\nacross various constructed environments (Figure 1(b1)), adhering to the Risk Extrapolation (REx)\nprinciple [37, 12, 27]. Unfortunately, previous graph-based invariant learning methods may not\neffectively address heterophilic graph structure distribution shifts, primarily due to explicit environ-\nments that may be ineffective for invariant learning. As illustrated in Figure 1(c1), within HGSS\nsettings, altering the original structure does not consistently affect the node's neighbor patterns. In\nessence, obtaining optimal and varied environments pertinent to neighbor patterns is challenging. Our\nobservation (Figure 1(c2)) reveals that EERM [37], a pioneering invariant learning approach utilizing\nenvironment augmentation to tackle graph distribution shifts in node-level tasks, does not perform\nwell under HGSS settings. At times, its enhancements are less effective than simply employing the\noriginal V-Rex [20], which involves randomly distributing the train nodes across various environmen-"}, {"title": "HEI: A Heterophil-inspired Environment inference framework for invariant prediction.", "content": "Recent\nfindings confirm that obtaining an accurate environment partition without prior knowledge is unfea-\nsible [24]. Therefore, our initial step should be to quantify the nodes' neighbor pattern properties\nrelated to the HGSS, which are central to the issue at hand. Consequently, a critical question emerges:\nDuring the training phase, how can we identify an appropriate metric to estimate the node's\nneighbor pattern and leverage it to deduce latent environments to manage this HGSS issue? As\npreviously mentioned, node homophily or edge homophily can assess neighbor patterns for dataset\ndescription [23]. Unfortunately, this requires the actual labels of the node and its neighbors, rendering\nit inapplicable during the training stage due to the potential unlabeled status of neighbor nodes. To\ncope with this problem, several evaluation metrics pertinent to nodes' neighbor patterns, including\nlocal similarity [10], post-aggregation similarity [29], and SimRank [26], have been introduced.\nThese metrics aim to facilitate node representation learning on heterophilic graphs during the training\nphase. However, while these studies primarily concentrate on employing these metrics to enhance\nneighbor aggregation for improved HGNN architectures, our objective is to introduce framework-\nagnostic backbones, augmented by estimated neighbor pattern insights, to tackle distribution shifts.\nA thorough analysis is essential to evaluate these metrics' efficacy as dependable indicators of the\nnode's environments, both from causal and graph-theoretical perspectives. Therefore, we propose\nHEI, a framework capable of generating invariant node representations through incorporating\nheterophily information to inference latent environments,as shown in Figure 1 (b2), which are\nthen used for downstream invariant prediction, under heterophilic graph structure distribution\nshifts. Moreover, we adapt this framework to the latest SOTA HGNN backbones to further verify its\northogonality to previous HGNN works. Simultaneously, the comparison experiments with previous\nmethods designed for mitigating agnostic distribution shifts on various graph benchmarks can verify\nits effectiveness in addressing this neglected HGSS issue."}, {"title": "Our Contributions", "content": "(i) We highlight an important yet often neglected form of heterophilic graph\nstructure distribution shift, which is orthogonal to most HGNN works that focus on backbone\ndesigns; (ii) We propose HEI, a novel graph-based invariant learning framework to tackle the HGSS\nissue. Unlike previous efforts, our method emphasizes leveraging a node's inherent heterophily\ninformation to deduce latent environments without augmentation, thereby significantly improving the\ngeneralization and performance of HGNNs;(iii) We demonstrate the effectiveness of our proposed\nmethod on several benchmarks and backbones compared with existing methods."}, {"title": "2 Preliminaries", "content": "Notations. Given an input graph G = (V, X, A), we denote V \u2208 {v1, ..., vn} as the nodes set,\nX \u2208 \\mathbb{R}^{N \\times D} as node features and A \u2208 {0,1}^{N \\times N} as an adjacency matrix representing whether the\nnodes connect, where the N and D denote the number of nodes and features, respectively. The node\nlabels can be defined as Y \u2208 {0,1}^{N \\times C}, where C represents the number of classes. For each node v,\nwe use Av and X to represent its adjacency matrix and node feature.\nProblem Formulation. We provide the formulation for the node-level OOD problems on graphs.\nFrom the perspective of data generation, we can get train data (Gtrain, Ytrain) from train distribution\np(G, Y)|e = e), the model should handle the test data (Gtest, Ytest) from a different distribution\np(G, Y)|e = e'). Considering the specific neighbor aggregation process, the general optimized\nobject of node-level OOD problem on graphs can be formulated as follows:\nmin max E_{y\\sim p(y/A_v=A_v, X_v=X_v,e=e)}l(f_w (f_{\\Phi} (A_v, X_v)), Y_v)  (1)\n\u03c9,\u03a6 \u0395\u03b5\nVEV"}, {"title": "3 Methodology", "content": "In this section, we present the details of the proposed HEI. Firstly, on heterophilic graphs, we verify\nthat the similarity can serve as a neighbor pattern indicator and then review existing similarity-based\nmetrics to estimate the neighbor patterns during training stages. Then, we elaborate the framework to\njointly learn environment partition and invariant node representation on heterophilic graphs without\naugmentation, assisted by the estimated neighbor patterns. Finally, we clarify the overall training\nprocess of the algorithm and discuss its complexity. Moreover, we provide a detailed theoretical\nanalysis in Appendix A.3 to clarify the effectiveness of HEI."}, {"title": "3.1 Neighbor Patterns Estimation", "content": "The node homophily and edge homophily are commonly used metrics to evaluate the node's neighbor\npatterns[20], but unfortunately, both of them need the true labels of the node and its neighbors, which\nmeans they can not be used in the training stage because the neighbor nodes may be just the test nodes\nwithout labels. To cope with it, we achieve inspiration from previous studies that focus on HGNN's\nbackbone designs [29, 10, 26] and verify that the neighbor pattern can be estimated by similarity.\nSimilarity: An Indicator of Neighbor Patterns. Though previous works have shown there may\nexist somewhat relationship between similarity and homophily from the experimental analysis [10],\nit can not be guaranteed to work well without a theory foundation. Thus, we further investigate its\neffectiveness from the node cluster view and verify the similarity between nodes can be exploited to\napproximate the neighbor pattern without the involvement of label information. For simplicity, we\ntake K-Means as the cluster algorithm. For two nodes v and u, let v belong to the cluster centroid c1\nand denote the distance between v and u as \u03b4 = ||u \u2013 v||2, where we can get c\u2081 = arg min ||v - ci||2,\nwhere ci represent the i-th cluster centroid. Then the distance between u and cluster centroid c\u2081 can\nbe acquired by the Eq. 5. Exactly, the neighbor pattern describes the label relationship between the\nnode and its neighbors. From the Eq 5, we can find the smaller \u03b4, the more likely the v and u belong\nto the same cluster and own the same label. Therefore, the similarity between nodes can be exploited\nto serve as a neighbor pattern indicator without using label information.\n||\u0438 - \u04411 ||2 = ||(\u0438 \u2212 v) + (v \u2013 C1)||2\n= || (u \u2013 v) ||\u00b2 + 2 ||u \u2013 v|| ||v - C1 || + ||v - C1 ||2\n= \u03b4 + 2\u221a\u03b4 ||v \u2013 C1 || + ||v - C1 ||2\n= (||v - c1|| + \u221a\u03b4)\u00b2 \u2265 \u03b4  (5)\nExisting Similarity-based Metrics. Existing similarity-based metrics on heterophilic graphs can be\nshown as Eq.6.\nSim(Xv, Xu)\nSim(AvXv, \u00c2u Xu)\nSimilarity(u, v)\n, Local Sim\n, AggSim\n(6)\n[NS(u)[[NS(v) \u03a3 Sim(Xu', Xu), SimRank\nu'ENS(u)\nv' \u2208NS(v)"}, {"title": "Estimated Neighbor Patterns", "content": "Thus, as shown by the Eq.7, we can obtain the estimated neighbor\npatterns zu for the node v during the training stage by averaging the node's similarity with neighbors.\n1\n\u0396\u03c5 =\n\u2211 Similarity(u, v)  (7)\nNS(v)| u\u2208NS(v)\nNotably, we further strengthen our object of using similarity metrics is indeed different from\nprevious HGNN works that also utilize the similarity metrics[29, 10, 26]. We focus on utilizing\nthe neighbor pattern to infer the node's environment for invariant prediction(separate and\nweaken the effect of spurious feature as shown in Figure 4, when given the neighbors). But\nprevious HGNN works mainly aim to help the node select proper neighbors and then directly\nutilize full neighbor features as aggregation targets for better HGNN backbone designs. Our\nwork is exactly orthogonal to previous HGNN works."}, {"title": "3.2 Our Proposed Invariant Learning Framework", "content": "Our work aims to utilize the estimated neighbor patterns Z \u2208 \\mathbb{R}^{G-} for nodes as auxiliary information\nto jointly learn nodes' environment partition and invariant node representation without augmentation.\nSimilar ideas can be also shown in [4, 24] that we can rely on some auxiliary information for invariant\nprediction without environmental labels,\nSpecifically, assisted by the estimated neighbor patterns for nodes, we can train an environment\nclassifier p(\u00b7) : \\mathbb{R}^{G_z} \u2192 \\mathbb{R}^{K} that softly assigns the train nodes to K environments. The K is a\npre-defined number, p is a two-layer MLP and the p(k) (\u00b7) is denoted as the k-th entry of p(\u00b7), with\n\u03c1(\u0396) \u2208 [0,1]^{\u039a} and \u03a3\u03ba\u03c1(k) (Z) = 1. Denote the ERM loss as R(w, \u03a6), which is calculated on all\ntrain nodes. Then, as shown in Eq. 8, the ERM loss in the k-th inferred environment can be defined\nas R_{p(k)} (\u03c9, \u03a6), which only calculates the loss on the nodes belonging to the k-th environment.\nR(k) (\u03c9, \u03a6) = \u03a3\u03c1(k) (v)l (fw (fo (Av, Xo), yv)  (8)\n1\nEV\nTherefore, the training framework can be defined as Eq. 9.\nmax\nmin\n\u03c9, \u03a6\u03c1, \u03c91,...,\u03c9\u03ba}\nL(\u03a6, \u03c9, \u03c91,...,\u03c9\u03ba,\u03c1) =\nR(\u03c9, \u03a6) + \u03bb\u03a3 [R(k) (\u03c9, \u03a6) \u2013 Rp(k) (Wk, \u03a6)] (9)\nK\nk=1\ninvariance penalty\nCompared with Eq. 3 and Eq. 4, our framework mainly differs in the maximization process. Thus,\nwe clarify the effectiveness of our framework from two aspects: (i) The invariance penalty that\nintroduces a set of environment-dependent GNN classifiers {fwh}K=1, which are only trained on the\ndata belonging to the inferred environments; (ii) The optimization of environmental classifier p(\u00b7);\nInvariance Penalty Learning. As shown by Eq.1, the ideal GNN classifier fw is expected to be\noptimal across all environments. After the environment classifier p(k)(\u00b7) assigns the train nodes into\nk inferred environments, we can adopt the following criterion to check if f is already optimal in\nall inferred environments: Take the k-th environment as an example, we can additionally train an\nenvironment-dependent classifier fwn on the train nodes belonging to the k-th environment. If fwk\nachieves a smaller loss, it indicates that fo is not optimal in this environment. Moreover, we can\nfurther train a set of classifiers {fwk}k=1, }K1, each one with a respective individual environment, to assess\nwhether fw is simultaneously optimal in all environments. Notably, all these classifiers share the\nsame encoder fo, if f extracts spurious features that are unstable across the inferred environments,\nR_{p(k)} (\u03c9, \u03a6) will be larger than R_{p(k)} (Wk, \u03a6), resulting in a non-zero invariance penalty, influencing\nmodel optimization towards achieving optimality across all environments. In other words, as long as"}, {"title": "Adaptive Environment Construction", "content": "As shown in Figure 1(c), the effectiveness of previous\nmethods is only influenced by environmental construction strategy. A natural question arises: What is\nthe ideal environment partition for invariant learning to deal with the HGSS? We investigate it from\nthe optimization of environment classifier p(\u00b7). Specifically, a good environment partition should\nconstruct environments where the spurious features exhibit instability, incurring a large penalty if fo\nextracts spurious features. In this case, we should maximize the invariance penalty to optimize the\npartition function p(\u00b7) to generate better environments, which is also consistent with the proposed\nstrategy. Though previous works [37, 12, 27] also adopt the maximization process to construct diverse\nenvironments, they just focus on directly optimizing the masking strategy to get augmentation graphs.\nDuring the optimization process, these methods lack guidance brought by auxiliary information Z\nrelated to environments, ideal or effective environments are often unavailable in this case. That's\nwhy we propose to introduce the environment classifier to infer environments without augmentation,\nassisted by the Z. Exactly, to make sure the guidance of Z has a positive impact on constructing\ndiverse and effective environments for the invariant node representation learning, there are also two\nconditions for Z from the causal perspective. We will further clarify it in Appendix A.3."}, {"title": "3.3 Overall Algorithm and Complexity Analysis", "content": "Our algorithm can be concluded as Algorithm 1: Given a heterophilic graph input, we first estimate\nthe neighbor patterns for each train node by Eq. 7. Then, based on Eq. 9, we collectively learn\nenvironment partition and invariant node representation, assisted by the estimated neighbor patterns,\nto address the HGSS issue. For the complexity, given a graph with N nodes, the average degree\nis d. GNN with l layers calculate embeddings in time and space O(Nld\u00b2). HEI assigns N nodes\ninto k inferred environments Ne=1 + \u00b7\u00b7\u00b7 + Ne=k = N and executes k + 1 classifier computations,\nwhere the k corresponds to the k environment-independent classifiers, and 1 refers to the basic GNN\nclassifier. Denote N' as the average number of nodes belonging to an inferred environment, the\noverall time complexity is O(Nld\u00b2 + kN'ld\u00b2), which is linear to the scale of the graph."}, {"title": "4 Experiments", "content": "In this section, we investigate the effectiveness of HEI to answer the following questions.\n\u2022 RQ1: Does HEI outperform state-of-art methods to address the HGSS issue?\n\u2022 RQ2: How robust is the proposed method? Can HEI solve the problem that exists severe\ndistribution shifts?\n\u2022 RQ3: How do different similarity-based metrics influence the neighbor pattern estimation,\nso as to further influence the effect of HEI?\n\u2022 RQ4: What is the sensitivity of HEI with respect to the pre-defined number of training\nenvironments?\n\u2022 RQ5: How efficient is the proposed HEI compared with previous methods?"}, {"title": "4.1 Experimental Setup", "content": "Due to the page limit, we provide the detailed Experimental Setup in Appendix A.4."}, {"title": "4.2 Experimental Results and Analysis", "content": "Handling Distribution Shifts under Standard Settings (RQ1). We first evaluate the effectiveness\nof HEI under standard settings, where we follow the previous dataset splits and further evaluate the\nmodel on more fine-grained test groups with low and high homophily, respectively. The results can\nbe shown in Table 1 and Table 2. We have the following observations.\nFirstly, the impact brought by the HGSS issue is still apparent though we adopt the existing SOTA\nHGNNs backbones for training. As shown by the base results in Table 1 and Table 2, for most of the\ndatasets, there are significant performance differences between the High Hom Test and Low Hom"}, {"title": "Handling Distribution Shifts under Simulation Settings where exists severe distribution shifts (RQ2)", "content": "As shown in Figure 3 and Figure 6 in Appendix A.5, for each dataset, we mainly report\nresults under severe distribution shifts between training and testing, which include Train High on\nTest Low and Train Low on Test High. We can observe that HEI achieves better results than other\nbaselines apparently, with up to 5 ~ 10 scores on average. The results further verify the robustness\nand effectiveness of our method for handling graph structure distribution shifts on heterophilic graphs.\nIn contrast, previous methods with environment augmentation only achieved minor improvements\ncompared with the base results. This is because it's difficult to construct diverse environments by\ndata augmentation on the ego-graph of train nodes to help the model adapt to diverse distribution\nespecially where there exists a huge gap between train and test distribution."}, {"title": "The effect of different similarity metrics as neighbor pattern indicators for HEI (RQ3)", "content": "As\ndepicted in Table 3 and Table 6 in Appendix A.5, we can draw two conclusions: (i) Applying any\nsimilarity-based metrics can outperform previous SOTA strategy FLOOD. This verifies the flexibility\nand effectiveness of HEI and help distinguish HEI with previous HGNNs works that also utilize\nthe similarity; (ii)Applying SimRank to HEI can acquire consistently better performance than other\nmetrics. This can be explained by previous HGNN backbone designs [29, 10, 26], which have verified\nthat SimRank has a better ability to distinguish neighbors patterns compared with Local Sim and\nAgg-Sim, so as to design a better HGNN backbone, SIMGA. Moreover, from the perspective of\ndefinitions as Eq. 6, the SimRank is specifically designed considering structural information, which\nis more related to our problem considering structure-related distribution shifts."}, {"title": "Sensitivity Analysis (RQ4)", "content": "We further evaluate the sensitivity of HEI concerning the pre-defined\nenvironmental numbers. As shown in Figure 7 in Appendix A.5, we report the results on Chameleon,\nSquirrel, and Actor datasets under Standard Settings. Specifically, we vary the hyper-parameter\nenvironment numbers k in Equation 9 within the range of [2, 12], and keep all other configurations\nunchanged to explore its impact on the HEI. From the results, we can observe that it has a stable\nimpact on the performance of HEI, especially when K \u2265 6."}, {"title": "Efficiency Studies (RQ5)", "content": "Apart from theoretical complexity analysis in Section 3.3, we further\ndiscuss the efficiency of HEI on large-scale heterophilic datasets. As shown in Table 5 in Appendix\nA.5, referring to [22], we provide the time(seconds) to train the model until converges that keep the\nstable accuracy score on the validation set. From the results, we can conclude that the extra time cost\ncan be acceptable compared with the base(backbone itself)."}, {"title": "5 Conclusion", "content": "In this paper, we emphasize an overlooked yet important variety of graph structure distribution shifts\nthat exist on heterophilic graphs. We verify that previous node-level solutions with environment\naugmentation are ineffective in addressing this problem due to the irrationality of constructing\nenvironments. To mitigate the effect of this distribution shift, we propose HEI, a framework capable of\ngenerating invariant node representations by incorporating the estimated neighbor pattern information"}, {"title": "A Appendix / supplemental material", "content": "In this appendix, we provide the details omitted in the main text due to the page limit, offering\nadditional experimental results, analyses, proofs, and discussions.\n\u2022 A.1: We provide a detailed literature review related to our work and further distinguish our\nwork with these works to clarify our contribution.\n\u2022 A.2: We provide the pseudo-code ofHEI (3.3 of the main paper).\n\u2022 A.3: We provide detailed theoretical analysis from the causal perspective to clarify the rea-\nsonability of HEI: Why we can utilize the estimated neighbor pattern to infer environments\nto address the HGSS issue?\n\u2022 A.4: We provide the detailed experimental setup (4.1 of the main paper).\n\u2022 A.5: We provide the full experimental results on small and large-scale datasets adopting\nLINKX and GloGNN as backbones (4.2 of the main paper).\n\u2022 A.6: We provide detailed implementation details to reproduce our experiments."}, {"title": "A.1 Related Work", "content": "Graph Neural Networks with Heterophily. Existing strategies for mitigating graph heterophily\nissues can be categorized into two groups [40]: (i) Non-Local Neighbor Extension, aimed at\nidentifying suitable neighbors through mixing High-order information [1, 41, 18] or discovering\npotential neighbors assisted by various similarity-based metrics [18, 19, 35, 36, 17, 26]; (ii) GNN\nArchitecture Refinement, focusing on harnessing information derived from the identified neighbors,\nthrough selectively aggregating distinguishable and discriminative node representations, including\nadapting aggregation scheme [28, 34, 22], separating Ego-neighbor [41, 34, 28] and combining\ninter-layer [41, 7, 15]. However, these efforts share the common objective of designing more\neffective HGNN backbones adapted to heterophilic graphs. But we instead consider from an\nidentifiable neighbor pattern distribution perspective and propose a framework adapted to most\nHGNN backbones. Moreover, we also find that there exists an arxiv paper called INPL[39] that\ncoincides with our work, though it also mention the distribution shifts of neighborhood patterns, it still\nfocuses on proposing an adaptive Neighborhood Propagation (ANP) module to optimize the HGNN\nbackbone or architecture without a thorough analysis for previous graph-based invariant-learning\nmethods. And their evaluation is still only on previous full test performance. In contrast, we provide a\nframework that can be integrated with existing backbones to address the HGSS issue after discussing\nthe limitations of existing graph-based invariant learning methods, and our extensive evaluations\ncan provide a more in-depth understanding of the HGSS issue. Besides, the most related work is\n[30], but it mainly focuses on providing theoretical and empirical analysis to discuss homophilic and\nheterophilic structural patterns without providing a specific technique solution. In contrast, from the\ninvariant learning view, we provide in-depth theoretical analysis and technique solutions to further\nimprove HGNNs generalization.\nGeneralization on GNNs. Many efforts have been devoted to exploring the generalization ability\nof GNNs. (i) For graph-level tasks, it assumes that every graph can be treated as an instance for\nprediction tasks [37]. Many works propose to identify invariant sub-graphs that decide the label Y and\nspurious sub-graphs related to environments, such as CIGA [9], GIL [21], GREA [25] and DIR [38].\n(ii) However, for node-level tasks that we focus on in this paper, the nodes are interconnected\nin a graph as instances in a non-iid data generation way, it is not feasible to transfer graph-level\nstrategies directly. To address this issue, EERM [37] proposes to regard the node's ego-graph\nwith corresponding labels as instances and assume that all nodes in a graph often share the same\nenvironment, so it should construct different environments by data augmentation, e.g., DropEdge\n[32]. Based on these findings, BA-GNN [12] and FLOOD [27] inherit this assumption to improve\nmodel generalization. Apart from these environments-augmentation methods, the SR-GNN [42]\nand ReNode [6] are two works that address agnostic distribution shifts on node-level tasks from the\ndomain adaption and graph conflict perspectives respectively. We also compare them in experiments.\nUnlike these works, we highlight a special variety of structure-related distribution shifts for\nnode classification tasks on heterophilic graphs. Under this circumstance, previous node-level"}, {"title": "A.2 The pseudo-code of the HEI", "content": "Our algorithm can be concluded as Algorithm 1: Given a heterophilic graph input, we first estimate the\nneighbor patterns for each node by Eq. 7. Then, based on Eq. 9, we collectively learn environment\npartition and invariant representation on heterophilic graphs, assisted by the estimated neighbor\npatterns, to address the HGSS issue."}, {"title": "A.3 Theoretical Analysis", "content": "To help understand our framework well, we first provide the comparison between our work and\nprevious graph-based invarinat learning works as shown in Figure 4. Specifically, the definitions of\nrandom variables can be defined as follows: We define G as a random variable of the input graph, A\nas a random variable of node's neighbor information, X as a random variable of node's features, and"}, {"title": "Casual conditions of Z", "content": "From the casual perspective, some conditions are also needed for Z to\nmake sure our framework to address the heterophilic graph structure distribution shifts well [24].\nDenote the H(Y|X, A) as the expected loss of an optimal classifier over (X, A, and Y), we can\nclarify the reasonability of utilizing the estimated neighbor patterns as Z auxiliary information to\ninfer environments for invariant prediction based on the following two conditions.\nCondition 1 (Invariance Preserving Condition). Given invariant feature (X\u00b9 and A\u00b9) and any\nfunction p(\u00b7), it holds that\nH(Y|(\u03a7\u00b9, A\u00b9), \u03c1(Z)) = H(Y\\(X\u00b9, A\u00b9)).  (10)\nCondition 2 (Non-invariance Distinguishing Condition). For any feature XSk \u2208 XS or Ask \u2208 AS\n,there exists a function p(\u00b7) and a constant C > 0 satisfy:\nH(Y|(XSk, ASk)) \u2013 H(Y|(XSk, ASk), p(Z)) \u2265 C.  (11)\nCondition 1 requires that invariant features X\u00b9 and A\u00b9 should keep invariant under any environ-\nment split obtained by p(Z). Otherwise, if there exists a split where an invariant feature becomes\nnon-invariant, then this feature would introduce a positive penalty as shown in Eq. 9 to further\npromote the learning of invariant node representation. Exactly, Condition 1 can be met only if\nH(Y\\(X\u00b9, A\u00b9), Z) = H(Y|(X1, A\u00b9)), which means the auxiliary variable Z should be d-separated\nby invariant feature X\u00b9 and A\u00b9. We provide a detailed proof in the appendix A.3. Exactly, the\nestimated neighbor pattern just describes the similarity between the node and its neighbors as Eq. 7,\nwhile the label Y only has the direct causal relationship with X\u00b9 and A\u00b9 from the causal perspective.\nThis means the Condition 1 can be met by adopting the estimated neighbor pattern as auxiliary\ninformation Z to construct environments.\nCondition 2 reveals that for each spurious feature XS and AS, there exists at least one environment\nsplit where this feature demonstrates non-invariance within the split environment. If a spurious\nfeature doesn't cause invariance penalties in all environment splits, it can't be distinguished from\ntrue invariant features. As shown in Figure 1(c2), the results of V-Rex are better than ERM, which\nmeans even randomly split environments with seeds can have a positive effect on making spurious\nfeatures produce effective invariance penalty, further promoting the learning of invariant features. It's\nmore likely to construct comparable or better environments than random seeds under the guidance\nof the estimated neighbor patterns Z. Thus, Condition 2 can also be guaranteed under our defined\nheterophilic graph structure distribution shit.\nProof of Meeting Condition 1. We show that for all p(\u00b7), if H(Y|(X\u00b9, A\u00b9), Z) = H(Y|(X1,1 A\u00b9))\nholds, then there will exist that H (Y|(X1, A\u00b9), \u03c1(Z)) = H(Y\\(X\u00cc, A\u00b9)).\nProof. On one hand, because p(Z) contains less information than Z, we have\nH(Y\\(X\u00b9, A\u00b9), \u03c1(\u0396)) > H(Y|(X\u00b9, A\u00b9), Z) = H(Y\\(X\u00b9, A\u00b9)).\nOn the other hand, (X1, A\u00b9) and p(Z) contain more information than (XI, AI), so we can get\nH(Y\\(\u03a7\u00b9, \u0391\u00b9), \u03c1(Z)) \u2264 H(Y\\(X\u00b9, A\u00b9)).\nThus, we conclude H(Y|(X1, A\u00b9), \u03c1(Z)) = H(Y|(X1, A\u00b9)).\nAssumptions and Theorem to Identify Invariant Features. In particular, our assumption is\nconsistent with previous invariant learning [24]. So we provide the previous version to support our\nframework, where X = [Xv; X5], where the X refers to the invariant feature and the X, refers to\nthe spurious feature.\nAssumption 1. For a given feature mask I and any constant \u20ac > 0, there exists f \u2208 F such that\nE[l(f(\u03a6(X)), Y)] \u2264 H(Y|\u03a6(X)) + \u0454."}, {"title": "Assumption 2", "content": "If a feature violates the invariance constraint, adding another feature would not\nmake the penalty vanish, i.e., there exists a constant delta > 0 so that for spurious feature X1 C X\u2083\nand any feature X2 C X,\nH(Y|X1, X2) \u2013 H(Y|\u03c1(Z), X1, X2) \u2265 \u03b4 (H(Y|X\u2081) \u2013 H(Y|\u03c1(Z), X1)).\nAssumption 3. For any distinct features X1, X2, H(Y|X1, X2) <\u2264 H(Y|X\u2081) \u2013 y with fixed \u03b3 > 0.\nExactly, Assumption 1 is a common assumption that requires the function space F be rich enough\nsuch that, given \u03a6, there exists f \u2208 F that can fit P(Y|\u03a6(X)) well. Assumption 2 aims to ensure a\nsufficient positive penalty if a spurious feature is included. Assumption 3 indicates"}]}