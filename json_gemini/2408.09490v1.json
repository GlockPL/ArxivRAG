{"title": "Leveraging Invariant Principle for Heterophilic Graph Structure Distribution Shifts", "authors": ["Jinluan Yang", "Zhengyu Chen", "Teng Xiao", "Wenqiao Zhang", "Yong Lin", "Kun Kuang"], "abstract": "Heterophilic Graph Neural Networks (HGNNs) have shown promising results for semi-supervised learning tasks on graphs. Notably, most real-world heterophilic graphs are composed of a mixture of nodes with different neighbor patterns, exhibiting local node-level homophilic and heterophilic structures. However, existing works are only devoted to designing better HGNN backbones or architectures for node classification tasks on heterophilic and homophilic graph benchmarks simultaneously, and their analyses of HGNN performance with respect to nodes are only based on the determined data distribution without exploring the effect caused by this structural difference between training and testing nodes. How to learn invariant node representations on heterophilic graphs to handle this structure difference or distribution shifts remains unexplored. In this paper, we first discuss the limitations of previous graph-based invariant learning methods from the perspective of data augmentation. Then, we propose HEI, a framework capable of generating invariant node representations through incorporating heterophily information to infer latent environments without augmentation, which are then used for invariant prediction, under heterophilic graph structure distribution shifts. We theoretically show that our proposed method can achieve guaranteed performance under heterophilic graph structure distribution shifts. Extensive experiments on various benchmarks and backbones can also demonstrate the effectiveness of our method compared with existing state-of-the-art baselines.", "sections": [{"title": "Introduction", "content": "Graph Neural Networks (GNNs) have emerged as prominent approaches for learning graph-structured representations through the aggregation mechanism that effectively combines feature information from neighboring nodes [11, 40, 13, 14]. Previous GNNs primarily dealt with homophilic graphs, where connected nodes tend to share similar features and labels [41]. However, growing empirical evidence suggests that these GNNs' performance significantly deteriorates when dealing with heterophilic graphs, where the majority of nodes connect with others from different classes, even worse than the traditional neural networks [23]. An appealing way to address this issue is to tailor the heterophily property to GNNs, extending the range of neighborhood aggregation and reorganizing architecture [40], known as the heterophilic GNNs (HGNNs)."}, {"title": "Preliminaries", "content": "Notations. Given an input graph G = (V, X, A), we denote V \u2208 {v1, ..., vn} as the nodes set, X \u2208 \\mathbb{R}^{N\u00d7D} as node features and A \u2208 {0,1}^{N\u00d7N} as an adjacency matrix representing whether the nodes connect, where the N and D denote the number of nodes and features, respectively. The node labels can be defined as Y \u2208 {0,1}^{N\u00d7C}, where C represents the number of classes. For each node v, we use Av and X to represent its adjacency matrix and node feature.\nProblem Formulation. We provide the formulation for the node-level OOD problems on graphs. From the perspective of data generation, we can get train data (Gtrain, Ytrain) from train distribution p(G, Y)|e = e), the model should handle the test data (Gtest, Ytest) from a different distribution p(G, Y)|e = e'). Considering the specific neighbor aggregation process, the general optimized object of node-level OOD problem on graphs can be formulated as follows:\n\\min_{\\omega,\\Phi} \\max_{E \\epsilon} \\frac{1}{|V|} \\sum_{v \\epsilon V} E_{y \\sim p(y/A_v=A_v, X_v=X_v,e=e)} l(f_\\omega (f_{\\Phi} (A_v, X_v)), Y_v)  (1)"}, {"title": "Methodology", "content": "In this section, we present the details of the proposed HEI. Firstly, on heterophilic graphs, we verify that the similarity can serve as a neighbor pattern indicator and then review existing similarity-based metrics to estimate the neighbor patterns during training stages. Then, we elaborate the framework to jointly learn environment partition and invariant node representation on heterophilic graphs without augmentation, assisted by the estimated neighbor patterns. Finally, we clarify the overall training process of the algorithm and discuss its complexity. Moreover, we provide a detailed theoretical analysis in Appendix A.3 to clarify the effectiveness of HEI."}, {"title": "Neighbor Patterns Estimation", "content": "The node homophily and edge homophily are commonly used metrics to evaluate the node's neighbor patterns[20], but unfortunately, both of them need the true labels of the node and its neighbors, which means they can not be used in the training stage because the neighbor nodes may be just the test nodes without labels. To cope with it, we achieve inspiration from previous studies that focus on HGNN's backbone designs [29, 10, 26] and verify that the neighbor pattern can be estimated by similarity.\nSimilarity: An Indicator of Neighbor Patterns. Though previous works have shown there may exist somewhat relationship between similarity and homophily from the experimental analysis [10], it can not be guaranteed to work well without a theory foundation. Thus, we further investigate its effectiveness from the node cluster view and verify the similarity between nodes can be exploited to approximate the neighbor pattern without the involvement of label information. For simplicity, we take K-Means as the cluster algorithm. For two nodes v and u, let v belong to the cluster centroid c1 and denote the distance between v and u as d = ||u \u2013 v||2, where we can get c\u2081 = arg min ||v - ci||2, where ci represent the i-th cluster centroid. Then the distance between u and cluster centroid c\u2081 can be acquired by the Eq. 5. Exactly, the neighbor pattern describes the label relationship between the node and its neighbors. From the Eq 5, we can find the smaller \u03b4, the more likely the v and u belong to the same cluster and own the same label. Therefore, the similarity between nodes can be exploited to serve as a neighbor pattern indicator without using label information.\n||\u0438 - \u04411 ||2 = ||(\u0438 \u2212 v) + (v \u2013 C1)||2\n= || (u \u2013 v) ||\u00b2 + 2 ||u \u2013 v|| ||v - C1 || + ||v - C1 ||2\n= \u03b4 + 2\u221a\u03b4 ||v \u2013 C1 || + ||v - C1 ||2\n= (||v - c1|| + \u221a\u03b4)\u00b2 \u2265 \u03b4 (5)\nExisting Similarity-based Metrics. Existing similarity-based metrics on heterophilic graphs can be shown as Eq.6.\nSim(Xv, Xu)\nSim(AvXv, \u00c2v Xu)\nSimilarity(u, v) =  Local Sim ,AggSim\n\u03a3u'\u2208NS(u) \u03a3v' \u2208NS(v) Sim(Xu', Xu),SimRank (6)"}, {"title": "Our Proposed Invariant Learning Framework", "content": "Our work aims to utilize the estimated neighbor patterns Z \u2208 \\mathbb{R}^{G-} for nodes as auxiliary information to jointly learn nodes' environment partition and invariant node representation without augmentation. Similar ideas can be also shown in [4, 24] that we can rely on some auxiliary information for invariant prediction without environmental labels,\nSpecifically, assisted by the estimated neighbor patterns for nodes, we can train an environment classifier p(\u00b7) : \\mathbb{R}^{Gz} \u2192 \\mathbb{R}^{K} that softly assigns the train nodes to K environments. The K is a pre-defined number, p is a two-layer MLP and the p(k) (\u00b7) is denoted as the k-th entry of p(\u00b7), with \u03c1(Z) \u2208 [0,1]^{K} and \u03a3_{\u03ba}\u03c1^{(k)} (Z) = 1. Denote the ERM loss as R(\u03c9, \u03a6), which is calculated on all train nodes. Then, as shown in Eq. 8, the ERM loss in the k-th inferred environment can be defined as R_{\u03c1^{(k)}} (\u03c9, \u03a6), which only calculates the loss on the nodes belonging to the k-th environment.\nR^{(k)} (\u03c9, \u03a6) = \\frac{1}{N} \\sum_{v \\epsilon V}\u03c1^{(k)} (v)l (f_\u03c9 (f_{\\Phi} (A_v, X_o), y_v) (8)\nTherefore, the training framework can be defined as Eq. 9.\nmin_{\\omega, \\Phi,\\{\u03c9_1,...,\u03c9_K\\}}  max_\u03c1 L(\u03a6, \u03c9, \u03c9_1,...,\u03c9_K,\u03c1) =\nR(\u03c9, \u03a6) + \u03bb \u03a3_{k=1}^K [R^{(k)} (\u03c9, \u03a6) \u2013 R_{\u03c1^{(k)}} (\u03c9_k, \u03a6)] (9)\ninvariance penalty\nCompared with Eq. 3 and Eq. 4, our framework mainly differs in the maximization process. Thus, we clarify the effectiveness of our framework from two aspects: (i) The invariance penalty that introduces a set of environment-dependent GNN classifiers {f_{wh}}^K_{h=1}, which are only trained on the data belonging to the inferred environments; (ii) The optimization of environmental classifier p(\u00b7);\nInvariance Penalty Learning. As shown by Eq.1, the ideal GNN classifier f\u03c9 is expected to be optimal across all environments. After the environment classifier \u03c1^{(k)}(\u00b7) assigns the train nodes into k inferred environments, we can adopt the following criterion to check if f\u03c9 is already optimal in all inferred environments: Take the k-th environment as an example, we can additionally train an environment-dependent classifier f_{\u03c9k} on the train nodes belonging to the k-th environment. If f_{\u03c9k} achieves a smaller loss, it indicates that f\u03c9 is not optimal in this environment. Moreover, we can further train a set of classifiers {f_{wk}}_{k=1}, }K1, each one with a respective individual environment, to assess whether f\u03c9 is simultaneously optimal in all environments. Notably, all these classifiers share the same encoder f\u03a6, if f extracts spurious features that are unstable across the inferred environments, R^{(k)} (\u03c9, \u03a6) will be larger than R_{\u03c1^{(k)}} (\u03c9_k, \u03a6), resulting in a non-zero invariance penalty, influencing model optimization towards achieving optimality across all environments. In other words, as long as"}, {"title": "Overall Algorithm and Complexity Analysis", "content": "Our algorithm can be concluded as Algorithm 1: Given a heterophilic graph input, we first estimate the neighbor patterns for each train node by Eq. 7. Then, based on Eq. 9, we collectively learn environment partition and invariant node representation, assisted by the estimated neighbor patterns, to address the HGSS issue. For the complexity, given a graph with N nodes, the average degree is d. GNN with l layers calculate embeddings in time and space O(Nld\u00b2). HEI assigns N nodes into k inferred environments Ne=1 + \u00b7\u00b7\u00b7 + Ne=k = N and executes k + 1 classifier computations, where the k corresponds to the k environment-independent classifiers, and 1 refers to the basic GNN classifier. Denote N' as the average number of nodes belonging to an inferred environment, the overall time complexity is O(Nld\u00b2 + kN'ld\u00b2), which is linear to the scale of the graph."}, {"title": "Experiments", "content": "In this section, we investigate the effectiveness of HEI to answer the following questions.\n\u2022 RQ1: Does HEI outperform state-of-art methods to address the HGSS issue?\n\u2022 RQ2: How robust is the proposed method? Can HEI solve the problem that exists severe distribution shifts?\n\u2022 RQ3: How do different similarity-based metrics influence the neighbor pattern estimation, so as to further influence the effect of HEI?\n\u2022 RQ4: What is the sensitivity of HEI with respect to the pre-defined number of training environments?\n\u2022 RQ5: How efficient is the proposed HEI compared with previous methods?"}, {"title": "Experimental Setup", "content": "Due to the page limit, we provide the detailed Experimental Setup in Appendix A.4."}, {"title": "Experimental Results and Analysis", "content": "Handling Distribution Shifts under Standard Settings (RQ1). We first evaluate the effectiveness of HEI under standard settings, where we follow the previous dataset splits and further evaluate the model on more fine-grained test groups with low and high homophily, respectively. The results can be shown in Table 1 and Table 2. We have the following observations.\nFirstly, the impact brought by the HGSS issue is still apparent though we adopt the existing SOTA HGNNs backbones for training. As shown by the base results in Table 1 and Table 2, for most of the datasets, there are significant performance differences between the High Hom Test and Low Hom Test, sometimes even higher than 20 scores. These results further verify the necessity to seek methods from the perspective of data distribution rather than backbone designs to deal with this problem.\nSecondly, HEI can outperform previous methods in most circumstances. Specially, compared with invariant learning methods, EERM, BAGNN, and FLOOD, though HEI does not augment the training environments, utilizing the estimated neighbor patterns to directly infer latent environments still benefits invariant prediction and improves model generalization on different test distributions. In contrast, directly adopting a reweight strategy (Renode) or evaluating the difference between the train domain and target domain (SRGNN) without environment augmentation can't acquire superior results than invariant learning methods. This is because the Renode and SRGNN need to acquire accurate domain knowledge in advance to help them make adjustments to model training. However, as for the HGSS issue, the nodes' environments on heterophily graphs are unknown and difficult to split into the invariant and spurious domains, like the GOOD dataset [16] which has clear domain and distribution splits. This also further verifies our problem is more complex and challenging.\nThirdly, we also find that Renode, SRGNN, and EERM don't attain superior performance than Base results on Actor datasets. This phenomenon also can be observed in [2, 30], presenting the challenge of achieving invariant prediction in non-Euclidean graph settings, especially for some datasets with complex and unidentifiable data distribution. Moreover, there also exists slight performance degrada-tion on the partly High Hom Test. For instance, when we use the LINKX as the backbone, compared with Base results, adopting the HEI strategy will lead to performance degradation on the High Hom Test of the twitch-gamer dataset. This can be attributed to the over-smoothing problem as stated by [5].\nHandling Distribution Shifts under Simulation Settings where exists severe distribution shifts (RQ2). As shown in Figure 3 and Figure 6 in Appendix A.5, for each dataset, we mainly report results under severe distribution shifts between training and testing, which include Train High on Test Low and Train Low on Test High. We can observe that HEI achieves better results than other baselines apparently, with up to 5 ~ 10 scores on average. The results further verify the robustness and effectiveness of our method for handling graph structure distribution shifts on heterophilic graphs. In contrast, previous methods with environment augmentation only achieved minor improvements compared with the base results. This is because it's difficult to construct diverse environments by data augmentation on the ego-graph of train nodes to help the model adapt to diverse distribution especially where there exists a huge gap between train and test distribution."}, {"title": "Conclusion", "content": "In this paper, we emphasize an overlooked yet important variety of graph structure distribution shifts that exist on heterophilic graphs. We verify that previous node-level solutions with environment augmentation are ineffective in addressing this problem due to the irrationality of constructing environments. To mitigate the effect of this distribution shift, we propose HEI, a framework capable of generating invariant node representations by incorporating the estimated neighbor pattern information"}, {"title": "Appendix / supplemental material", "content": "In this appendix, we provide the details omitted in the main text due to the page limit, offering additional experimental results, analyses, proofs, and discussions.\n\u2022 A.1: We provide a detailed literature review related to our work and further distinguish our work with these works to clarify our contribution.\n\u2022 A.2: We provide the pseudo-code ofHEI (3.3 of the main paper).\n\u2022 A.3: We provide detailed theoretical analysis from the causal perspective to clarify the rea-sonability of HEI: Why we can utilize the estimated neighbor pattern to infer environments to address the HGSS issue?\n\u2022 A.4: We provide the detailed experimental setup (4.1 of the main paper).\n\u2022 A.5: We provide the full experimental results on small and large-scale datasets adopting LINKX and GloGNN as backbones (4.2 of the main paper).\n\u2022 A.6: We provide detailed implementation details to reproduce our experiments."}, {"title": "Related Work", "content": "Graph Neural Networks with Heterophily. Existing strategies for mitigating graph heterophily issues can be categorized into two groups [40]: (i) Non-Local Neighbor Extension, aimed at identifying suitable neighbors through mixing High-order information [1, 41, 18] or discovering potential neighbors assisted by various similarity-based metrics [18, 19, 35, 36, 17, 26]; (ii) GNN Architecture Refinement, focusing on harnessing information derived from the identified neighbors, through selectively aggregating distinguishable and discriminative node representations, including adapting aggregation scheme [28, 34, 22], separating Ego-neighbor [41, 34, 28] and combining inter-layer [41, 7, 15]. However, these efforts share the common objective of designing more effective HGNN backbones adapted to heterophilic graphs. But we instead consider from an identifiable neighbor pattern distribution perspective and propose a framework adapted to most HGNN backbones. Moreover, we also find that there exists an arxiv paper called INPL[39] that coincides with our work, though it also mention the distribution shifts of neighborhood patterns, it still focuses on proposing an adaptive Neighborhood Propagation (ANP) module to optimize the HGNN backbone or architecture without a thorough analysis for previous graph-based invariant-learning methods. And their evaluation is still only on previous full test performance. In contrast, we provide a framework that can be integrated with existing backbones to address the HGSS issue after discussing the limitations of existing graph-based invariant learning methods, and our extensive evaluations can provide a more in-depth understanding of the HGSS issue. Besides, the most related work is [30], but it mainly focuses on providing theoretical and empirical analysis to discuss homophilic and heterophilic structural patterns without providing a specific technique solution. In contrast, from the invariant learning view, we provide in-depth theoretical analysis and technique solutions to further improve HGNNs generalization.\nGeneralization on GNNs. Many efforts have been devoted to exploring the generalization ability of GNNs. (i) For graph-level tasks, it assumes that every graph can be treated as an instance for prediction tasks [37]. Many works propose to identify invariant sub-graphs that decide the label Y and spurious sub-graphs related to environments, such as CIGA [9], GIL [21], GREA [25] and DIR [38]. (ii) However, for node-level tasks that we focus on in this paper, the nodes are interconnected in a graph as instances in a non-iid data generation way, it is not feasible to transfer graph-level strategies directly. To address this issue, EERM [37] proposes to regard the node's ego-graph with corresponding labels as instances and assume that all nodes in a graph often share the same environment, so it should construct different environments by data augmentation, e.g., DropEdge [32]. Based on these findings, BA-GNN [12] and FLOOD [27] inherit this assumption to improve model generalization. Apart from these environments-augmentation methods, the SR-GNN [42] and ReNode [6] are two works that address agnostic distribution shifts on node-level tasks from the domain adaption and graph conflict perspectives respectively. We also compare them in experiments.\nUnlike these works, we highlight a special variety of structure-related distribution shifts for node classification tasks on heterophilic graphs. Under this circumstance, previous node-level"}, {"title": "The pseudo-code of the HEI", "content": "Our algorithm can be concluded as Algorithm 1: Given a heterophilic graph input, we first estimate the neighbor patterns for each node by Eq. 7. Then, based on Eq. 9, we collectively learn environment partition and invariant representation on heterophilic graphs, assisted by the estimated neighbor patterns, to address the HGSS issue."}, {"title": "Theoretical Analysis", "content": "To help understand our framework well, we first provide the comparison between our work and previous graph-based invarinat learning works as shown in Figure 4. Specifically, the definitions of random variables can be defined as follows: We define G as a random variable of the input graph, A as a random variable of node's neighbor information, X as a random variable of node's features, and"}]}