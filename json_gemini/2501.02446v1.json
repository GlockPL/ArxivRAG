{"title": "RTLMarker: Protecting LLM-Generated RTL Copyright via a Hardware Watermarking Framework", "authors": ["Kun Wang", "Kaiyan Chang", "Mengdi Wang", "Xingqi Zou", "Haobo Xu", "Yinhe Han", "Ying Wang"], "abstract": "Recent advances of large language models in the field of Verilog generation have raised several ethical and security concerns, such as code copyright protection and dissemination of malicious code. Researchers have employed watermarking techniques to identify codes generated by large language models. However, the existing watermarking works fail to protect RTL code copyright due to the significant syntactic and semantic differences between RTL code and software code in languages such as Python. This paper proposes a hardware watermarking framework RTLMarker that embeds watermarks into RTL code and deeper into the synthesized netlist. We propose a set of rule-based Verilog code transformations, ensuring the watermarked RTL code's syntactic and semantic correctness. In addition, we consider an inherent tradeoff between watermark transparency and watermark effectiveness and jointly optimize them. The results demonstrate RTLMarker's superiority over the baseline in RTL code watermarking.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent progress in Large Language Models (LLMs) has shown considerable promise in the synthesis of RTL code, attracting significant interest in the Electronic Design Automation (EDA) community[9, 11, 15]. These models can autonomously produce Verilog code from natural language inputs, thus greatly enhancing the efficiency of chip design processes. However, these developments come with issues related to copyright ownership and security. The irresponsible generation of seemingly valid but flawed or insecure code by LLMs introduces significant hazards, which may compromise the integrity of the information environment. Therefore, implementing robust copyright tracking systems for LLM-generated RTL code is essential to prevent unauthorized or harmful applications.\nWatermarking techniques are crucial for protecting the intellectual property of text produced by large language models (LLMs). Coding, a unique subset of text, introduces specific challenges for these algorithms. Proper watermarking of code requires careful preservation of its syntax and semantics to maintain its operational functionality. Even slight modifications introduced through watermarking can cause syntactic or semantic issues, potentially hindering the code's execution. During the model inference phase, SWEET[7] embeds watermarks by assessing token entropy and classifying high-entropy tokens into red and green groups. In the post-inference phase, ACM[8] incorporates watermarks through equivalent substitutions, thus ensuring the integrity and correctness of the watermarked code.\nCurrent approaches have failed to adequately protect the intellectual property rights related to LLM-generated RTL code for several reasons. Firstly, RTL code operates at a lower level of abstraction in comparison to high-level programming languages like Python and Java, resulting in lower information entropy and thus presenting more significant challenges for hardware watermark embedding. Secondly, ensuring the durability of these watermarks requires their integration and detection not only in the RTL code but also at the gate-level netlist. However, this embedding task is challenging since conventional watermarking methods, such as variable name replacement and semantic equivalence modifications, often lose their effectiveness after logic synthesis. Moreover, there is a trade-off between the watermark's transparency and its effectiveness. Previous research has prioritized the watermark's efficacy over its transparency, producing watermarked code with unusual styles that developers are not likely to use, thus making the watermarks easy to remove.\nTo address the shortcomings in existing watermarking systems, this study presents RTLMarker, an advanced hardware watermarking framework designed to protect the copyright of RTL generated by LLM. Maintaining the integrity of the watermarked code is crucial, as any reduction in code correctness would undermine the watermark's effectiveness. Therefore, RTLMarker utilizes semantic-preserving transformations[8] to embed the watermark, ensuring the code's integrity remains unaffected. We have created various Verilog-specific code transformers, totaling 15 distinct types, and implemented these through the abstract syntax tree (AST) framework provided by Pyverilog [14]. RTLMarker includes a watermark embedding module, a feature representation module, and a watermark detection module. The watermark embedding module is tasked with choosing and performing specific code transformations to insert the watermark. The watermark detection network identifies the watermark's presence within the code. To enhance watermark transparency while preserving its effectiveness, we jointly optimize the watermark embedding and detection modules. This is done by using the output logits of the detection network to adjust the watermark embedding intensity, avoiding unnecessary watermarking that might reduce transparency. Nonetheless, the non-differentiable nature of AST-based code transformations poses a significant challenge to this process. The feature representation module overcomes this obstacle by using a transformer-based neural network to recognize the intrinsic properties of the watermarked code.\nWe assess the performance of our RTLMarker, and the findings indicate that RTLMarker surpasses the baseline models in accuracy. Our main contributions are highlighted as follows:\n\u2022 To our knowledge, this research is the pioneering effort to introduce a practical and efficient watermarking framework designed to safeguard the copyright of RTL generated by large language models.\n\u2022 We propose a comprehensive suite of Verilog-centric code transformations and concurrently create a state-of-the-art tool powered by Pyverilog to facilitate these transformations.\n\u2022 Our study introduces an advanced framework for embedding and identifying hardware watermarks, functional at both the Register Transfer Level (RTL) and the logic netlist level.\n\u2022 The evaluation results reveal that our proposed watermarking framework significantly improves precision compared to conventional LLM watermark generation methods."}, {"title": "2 BACKGROUND & MOTIVATION", "content": "Large Language Models For Chip Design. Large language models (LLMs) have emerged as a highly promising methodology for chip design, particularly within the realm of RTL code generation. Several studies [3, 5, 12, 16] focused on the enhancement of RTL code generation via prompt engineering. For example, ChipChat[3] successfully architected an 8-bit accumulator-based microprocessor and achieved tape-out. Nonetheless, the models employed in these pursuits are proprietary and closed-source commercial large language models. Consequently, parallel research endeavors [4, 9-11, 15] are devoted to the fine-tuning of open-source models such as LLama2[17], thereby advancing the democratization of chip design through domain-specific customization.\nLarge Language Models Text Watermarking. Text watermarking pertains to the incorporation of unique, imperceptible identifiers within textual content. The embedding of text watermarks occur at three distinct stages: during the training of LLM, during LLM inference, and post-inference. In the context of the LLM training phase, watermarking cannot be applied to already trained LLM, which imposes certain constraints. During the inference phase, WLLM [6] embeds watermarks into the text by categorizing tokens into red and green lists and subsequently detects the watermark based on the token distribution within the green list. SWEET [7] enhances WLLM by eliminating low-entropy segments within the watermarking code, as embedding watermarks in low-entropy text presents significant difficulties and may adversely affect code functionality. During the post-inference stage, AWT [1] and Remark-LLM [20] deploy learning-based techniques to embed watermarks, leveraging the superior feature representation capabilities of deep neural networks to improve watermark efficacy. ACM [8] employs rule-based code equivalence transformations to embed watermarks, thereby ensuring functional correctness of the watermarked code. However, all of the aforementioned watermarking frameworks are tailored for natural language text and high-level programming language code, such as Python and Java. No prior research has investigated the embedding of watermarks into LLM-generated RTL code. Our proposal introduces a hardware watermarking framework designed to surmount the challenges identified in prior studies.\nChallenge I: LLM-based RTL code generation requires soft and firm IP level copyright protecting machanism. Figure 2 illustrates an example of LLM-generated insecure RTL code[13]. The power consumption of the output XOR gate is contingent on the state of the output bits. By monitoring the power consumption during the module's output, adversaries can infer the secret key bits, which poses a risk of compromising confidential user information. Consequently, it is imperative to trace the origins of insecure RTL code to mitigate its malicious dissemination. Moreover, in the circuit design process, logic synthesis represents a pivotal stage wherein RTL code is transformed into a logic gate netlist. To ensure the robustness of watermarks, it is crucial to implement protection mechanisms for both LLM-generated soft IP and firm IP.\nChallenge II: LLM-generated RTL copyright cannot be well protected by a general watermarking framework. In the LLM inference phase, tokens are generated sequentially, inherently lacking comprehensive syntactic and semantic information about the code, thereby complicating the assurance of the watermarked code's functional correctness. In the post-inference phase, rule-based watermarking methods preserve the functional correctness of the watermarked code; however, these methodologies are unsuitable for Verilog code, which possesses distinct syntactic and semantic properties. Learning-based watermarking methods can augment watermark efficacy through the integrated optimization of watermark embedding and detection networks. Nonetheless, these methods cannot be directly applied to code domains necessitating high syntactic and semantic accuracy. Moreover, extant research on code watermarking has predominantly concentrated on watermark effectiveness, frequently neglecting the aspect of watermark transparency. Diminished transparency increases the likelihood of the watermark's detection and modification."}, {"title": "3 PROBLEM FORMULATION", "content": "In this section, we present a general formulation of hardware watermarks. The embedding of hardware watermarks can be viewed as a constrained optimization problem. Specifically, we aim to find a suitable watermark $\\delta$ to embed into the LLM-generated RTL code $x$, while satisfying constraints on effectiveness, robustness, and transparency, as follows:\n$\\min (D(x, x + \\delta) \u2013 Det_{rtl}(F(x + \\delta)) \u2013 Det_{gate}(F(x + \\delta)))$ (1)\nD(x, x + \u03b4) represents the discrepancy between the LLM-generated code x and the watermarked code x+d. For transparency, this discrepancy should be minimized. F(x + \u03b4) represents watermark attacks, e.g., variable name replacements. For robustness, the watermark should withstand such attacks. $Det_{rtl}$ and $Det_{gate}$ represent the watermark detection accuracy at the RTL and netlist level, respectively."}, {"title": "4 METHOD", "content": "4.1 Overview\nRTLMarker's global flow is depicted in Figure 1. In the training phase, RTLMarker comprises three principal modules: watermark embedding module, feature representation module, and watermark detection module. In the watermark embedding module, LLM-generated RTL code x is fed into an embedding network, which subsequently produces selected transformation set T (Line 6 in Algorithm 1). Feature representation network receives the RTL code x and the selected transformation set T as inputs, employing a learning-based approach to approximate the code transformed by the rules from T (Line 9). The approximate code xa is then fed into a detection network to obtain the watermark confidence P (Line 12) and provide feedback to the embedding network. In the deployment phase, after obtaining the set T from the embedding network, we use the AST-based method with the rules in T to transform the code and obtain the watermarked code xw, ensuring that the watermarked code is fully correct.\n4.2 Watermark Embedding\nRule-Based Verilog Code Transformations. we propose a set of Verilog-specific code transformations, based on equivalence transformation watermarking methodologies[8]. The set includes 15 code transformation rules, categorized into two distinct granularities: token level and statement level. In curating this set, we make efforts to avoid transformations [8] characterized by atypical styles that developers are unlikely to adopt, such as transforming n <= 2 to 2 >= n. Furthermore, the applicability of these rules is constrained by specific contextual factors. For example, the state variable encoding rule can only be applied when Finite State Machine (FSM) is present in the code. Nevertheless, the process of watermark embedding does not necessitate the application of all transformation rules; rather, applying an adequate number to satisfy detection requirements suffices. Indeed, excessive utilization of transformation rules can adversely compromise the watermark's transparency.\nEmbedding watermarks at the Register Transfer Level (RTL) presents substantial challenges in achieving deep integration within the gate-level netlist, as most RTL watermarks tend to dissipate during the logic synthesis process. We discover that watermarks which modify the code's data flow and control flow exhibit greater resilience, often persisting in the gate-level netlist. However, modifications to the data flow often compromise the correctness of the code, thereby hindering the embedding of watermarks. In light of these observations, we consider integrating redundant control logic into the code's control flow to facilitate watermark embedding into the gate-level netlist. For example, as shown in Figure 3, we implement a redundant assignment logic that is activated exclusively when the watermark_trigger signal is asserted to 1. We encrypt the watermark information, which includes model and developer signatures, and use it as the right-hand side of the assignment statement. In this implementation, 8'hA5 represents the encrypted watermark information.\nLearning-Based Watermark Embedding The goal of the watermark embedding network is to select suitable code transformations set from Table 1, based on the LLM-generated code. The embedding network architecture incorporates multiple encoder layers to extract code features. The network's terminal segment consists of two fully connected layers, which are utilized for the regression prediction of the selected transformation set T. The network's loss function $L_{embed}$ comprises three parts: $L_{detect}$, $L_{mse}$ and $L_{trans}$, as described in Equ.2, where $L_{detect}$ represents the confidence score output by the detection network, $L_{mse}$ represents the mean squared error between the selected code transformations and the applicable code transformations, $L_{trans}$ represents the number of selected code transformations. The coefficients m and n modulate the relative contributions of the various loss components in the optimization process.\n$L_{embed} = L_{detect} + m \\cdot L_{trans} + n \\cdot L_{mse}$ (2)\n4.3 Feature Represention\nRule-based code transformations are non-differentiable, posing a challenge for joint optimization between the watermark embedding and detection networks. To overcome this challenge, we develop a feature representation network which accepts code x and the selected transformation set T and generates feature approximate watermarked code xa. The network comprises six encoder layers and six decoder layers, following the transformer architecture [18]. The objective function employs cross-entropy loss, computed by comparing the network's output code tokens against those generated through Abstract Syntax Tree (AST)-based code transformations. While the network-generated code may occasionally exhibit character-level discrepancies, these anomalies seldom compromise the salient features of the embedded watermark.\n4.4 Watermark Detection\nWatermark Detection at the Register Transfer Level (RTL). The detection network ingests the code generated by the feature representation network. It computes a confidence score indicating the probability of a watermark's presence in the code. This score is then fed back to the watermark embedding network, creating a feedback loop. We include non-watermarked samples in training to enhance discrimination of unwatermarked code. The network optimizes the process using cross-entropy loss.\nWatermark Detection at Netlist Level. We employ synthesis tools such as Yosys[19] to perform logic synthesis on the LLMs-generated RTL code, generating the gate-level netlist. Subsequently, we trace the connections of the embedded watermark variable in the gate-level netlist. Through analysis of these connections, we identify the watermark's presence and characteristics. As shown in Figure 3, we analyze the connections associated with processed_data variable in the gate-level netlist. Through this analysis, we extract the embedded watermark information, designated as 8'hA5, which allows us to further extract critical information, such as digital signatures of the model and developer."}, {"title": "5 EVALUATION", "content": "5.1 Experiment Setup\nTarget Model We use the general purpose LLM, GPT-4[2] along with the hardware-specific LLMs ChipGPT-FT[4] and RTLCoder[11], as our target models. We use RTLLM [12] and VerilogEval [10]as benchmarks, which provide natural language descriptions for the generation of Verilog code.\nBaselines We compare RTLMarker with WLLM[6], which is tailored for text watermarking, along with SWEET[7] which is specifically designed for general code watermarking.\nEvaluation Metrics We evaluate our watermarking framework from three aspects as follows.\n\u2022 Effectiveness. The watermark should be successfully embedded and detected. To comprehensively evaluate our watermark framework, we employ Accuracy (ACC), True Positive Rate (TPR) and False Positive Rate (FPR) as effectiveness metrics.\n\u2022 Robustness. The watermark should be robust against typical attacks. We evaluate the robustness by measuring detection accuracy after variable name substitution attacks.\n\u2022 Transparency. Transparency encompasses two metrics: the correctness rate of syntax and semantics in the watermarked code, and the number of code transformations utilized. Fewer transformations indicate better transparency.\nImplement Details Our implementation of rule-based code transformations relies primarily on Pyverilog. Initially, verilog code is parsed into an abstract syntax tree (AST). We then perform replacements of tokens and statements on the AST. The final step involves generating the modified code from the updated AST. As Pyverilog lacks support for certain token-level rules, we supplement these transformations with methods based on regular expressions.\n5.2 Effectiveness\nThis experiment aims to assess RTLMarker's efficacy in discriminating between watermarked and non-watermarked code. The watermarked code is generated by RTLMarker, while the unwatermarked code is human-written and provided by the benchmark. Table 2 illustrates the evaluation results for both RTLMarker and the baseline methods. Since WLLM and SWEET necessitate access to model-specific outputs, particularly logits, they are inherently unable to evaluate the closed-source GPT-4 model. To ensure the baselines achieve the best accuracy, we set the hardness parameter y to 3, the watermark tokens ratio 8 to 0.25, the z-statistic score threshold to 4 and the entropy threshold to 0.6.\nRTLMarker achieves an accuracy of over 95% on the RTLLM benchmark and over 92% on the VerilogEval benchmark at the Register Transfer Level (RTL), significantly outperforming the baseline methods WLLM and SWEET. In the VerilogEval benchmark, the 156 Verilog problems sourced from the Hdlbits website encompass various complexities, including some simple combinational circuits, which pose significant challenges for watermark embedding. Consequently, the True Positive Rate (TPR) observed on VerilogEval tends to be lower compared to that on RTLLM. Specifically, the TPR of RTLMarker on VerilogEval decreased by an average of 2.48%, in contrast to reductions of 28.78% and 42.56% for WLLM and SWEET, respectively. At the netlist level, RTLMarker achieves an accuracy of over 76% on the RTLLM benchmark and over 59% on the VerilogEval benchmark. Although the accuracy of watermarking at the netlist level may not be exceptional, combining it with the RTL-level watermarking can enhance both watermark embedding and detection performance.\n5.3 Robustness\nIn this experiment, we aim to evaluate whether RTLMarker can effectively withstand typical variable name replacement attack. We consider renaming 25%, 50%, 75% and 100% of the variables in the watermarked code, with new variable names ranging in length from 3 to 10 characters. Table 3 presents the evaluation results of RTL-Marker. When the string replacement percentage is set to 100%, the VerilogEval benchmark, due to its generally shorter lines of code, suffers significant impacted, resulting in a decrease in accuracy to 80.45%. In contrast, the RTLLM is less affected, maintaining an accuracy of 91.67%. Additionally, variable name replacement does not affect the accuracy of watermarking at the netlist level, but it does increase the complexity of watermark detection. Since we cannot rely on variable names which are replaced to analyze and extract watermark information, we need to analyze all variables with the same bit width and check for the presence of watermark.\n5.4 Transparency\nTo evaluate the transparency of the watermark, we consider two aspects: the impact of the watermark on the syntactic and semantic correctness of the RTL code and the number of code transformations utilized. Firstly, since the code transformation rules we designed are semantically equivalent, RTLMarker does not compromise the correctness of syntax and semantics. Moreover, employing an excessive number of code transformations implies substantial modifications to the code, potentially compromising the watermark's transparency. Consequently, it is crucial to apply the minimal number of code transformation required to maintain effective watermark detectability. Figure 4 illustrates that the average number of applicable code transformations in the RTLLM benchmark is 6.42, while the number of code transformations that RTLMarker utilizes is 4.25, effectively enhancing the transparency of the watermark. The x-axis (index) in the figure 4 represents different cases within the RTLLM benchmark."}, {"title": "6 CONCLUSION", "content": "We present RTLMarker, a hardware watermarking framework to protect LLM-generated RTL Copyright. This paper proposes a rule-based Verilog code transformation set that ensures the functional correctness of the watermarked code while embedding the watermark into the RTL code and the synthesized netlist. This paper considers an inherent tradeoff between watermark transparency and watermark effectiveness through the joint optimization of the watermark embedding and detection network. RTLMarker demonstrates superior performance over existing watermarking framework in RTL code watermarking."}]}