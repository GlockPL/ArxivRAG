{"title": "STGformer: Efficient Spatiotemporal Graph Transformer for Traffic Forecasting", "authors": ["Hongjun Wang", "Jiyuan Chen", "Tong Pan", "Zheng Dong", "Lingyu Zhang", "Renhe Jiang", "Xuan Song"], "abstract": "Traffic forecasting is a cornerstone of smart city management, enabling efficient resource allocation and transportation planning. Deep learning, with its ability to capture complex nonlinear patterns in spatiotemporal (ST) data, has emerged as a powerful tool for traffic forecasting. While graph neural networks (GCNs) and transformer-based models have shown promise, their computational demands often hinder their application to real-world road networks, particularly those with large-scale spatiotemporal interactions. To address these challenges, we propose a novel spatiotemporal graph transformer (STGformer) architecture. STGformer effectively balances the strengths of GCNs and Transformers, enabling efficient modeling of both global and local traffic patterns while maintaining a manageable computational footprint. Unlike traditional approaches that require multiple attention layers, STG attention block captures high-order spatiotemporal interactions in a single layer, significantly reducing computational cost. In particular, STGformer achieves a 100x speedup and a 99.8% reduction in GPU memory usage compared to STAEformer during batch inference on a California road graph with 8,600 sensors. We evaluate STGformer on the LargeST benchmark and demonstrate its superiority over state-of-the-art Transformer-based methods such as PDFormer and STAEformer, which underline STGformer's potential to revolutionize traffic forecasting by overcoming the computational and memory limitations of existing approaches, making it a promising foundation for future spatiotemporal modeling tasks. Codes are available at GitHub", "sections": [{"title": "1 INTRODUCTION", "content": "SPATIOTEMPORAL graph neural networks [1], [2], [3], [4] have shown exceptional potential and have become a preferred method for making precise traffic predictions by leveraging graph neural networks to capture spatial relationships between sensors and utilize sequential models to learn temporal patterns. Recently, the emergence of Transformer-based architectures [5], [6], [7] greatly challenges the dominance of GCNs and become state of the arts architecture in traffic forecasting. However, recent work [8] have prove that most of ST-model, including Transformer, have failed when meeting large scale graph because current traffic datasets used for benchmarking are relatively small when compared to the complexity and scale of actual traffic networks. For instance, popular benchmark datasets like PEMS series [9], MeTR-LA, PEMS-Bay [1] consist of only a few hundred nodes and edges. In contrast, real-world traffic systems, such as Caltrans Performance Measurement System in California, USA [10], incorporate nearly 20,000 active sensors. Consequently, as traffic forecasting models are predominantly developed using these limited datasets, They often do not consider the computational overhead of the model fail to scale up to larger sensor networks, presenting a significant challenge in the field.\nAs depicted in Figure 2, we conducted a comparative analysis of leading methods on the San Diego dataset within the LargeST framework [8]. Our evaluation included performance, model parameters, and computational costs. While Transformer-based models, such as STAEformer [5], demonstrated superior performance, they exhibited significantly higher computational demands compared to GCN-based approaches like STGCN [11]. Based on these findings, we posit that the success of GCN and Transformer methods can be attributed to their respective strengths in capturing global spatial interactions and input-adaptive long-range dependencies [12]. Standard graph convolution operations [13] effectively aggregate local neighbor features with high orders interaction [11]. However, their reliance on local information limits their ability to consider global context in traffic scenarios, potentially hindering performance. In contrast, self-attention mechanisms, while disregarding explicit graph structures [5], implicitly capture global spatial interactions through successive matrix multiplications. This, however, introduces substantial computational overhead, making it challenging to scale to large-scale real-world traffic networks.\nPreviously, the overhead associated with global attention mechanisms remained within an acceptable range due to the use of only small-scale datasets for validation. However, the recent introduction of LargeST has exacerbated this issue. The global attention mechanism typically exhibits quadratic time and space complexity as the number of nodes increases, while the computational graph experiences exponential growth with an increasing number of layers. A potential compromise is to employ advanced techniques to partition interconnected nodes into smaller mini-batches, thereby reducing computational overhead [14], [15], [16], [17]. Nonetheless, this strategy results in longer training times due to the smaller mini-batches. In this paper, we propose a more efficient model, which demonstrates exceptional competitiveness across seven traffic benchmarks, ranging from small-scale PEMS [9] datasets to large-scale datasets like LargeST [8], utilizing only a single layer with linear spatiotemporal global attention. Despite the model's simplicity, it retains the full expressive capability necessary to capture all interactions among graph convolutions and attention mechanisms. Furthermore, our findings indicate that using fewer parameters can enhance generalization capabilities. Our research demonstrates that STGformer maintains strong performance even when tested on data from LargeST one year later.\nIn Figure 1, we specifically examine the differences between STGformer and STAEformer. Firstly, STAEformer predominantly relies on stacked 2L layers and utilizes a spatiotemporal separable attention mechanism to achieve higher-order interactions by deepening the model. As previously noted, this approach incurs substantial computational overhead. In contrast, we introduce a more efficient attention module that integrates both graph and spatiotemporal attention mechanisms. Specifically, we conceptualize the temporal and spatial dimensions as a unified entity, employing the same query, key, and value in the attention mechanism to facilitate efficient spatiotemporal attention computation, which markedly reduces computational overhead compared to the separate treatment of these dimensions. STGformer surpasses current state-of-the-art models by leveraging graph information, allowing for efficient computation using only a single layer of the attention module. Furthermore, we adopt linear attention [18], [19], [20], which replaces the softmax operation of the standard attention mechanism with decomposed inner products, thereby reducing the computational complexity from quadratic to linear. This significantly alleviates memory consumption and computational burden when handling large-scale spatiotemporal datasets. The main contributions of this paper are summarized as follows:\n\u2022 We propose a novel STG-attention that efficiently captures high-order spatiotemporal interactions for both global and local patterns in a single layer, unlike previous methods requiring multiple stacked layers.\n\u2022 STGformer combines the advantages of GCNs and Transformers while maintaining low computational and memory costs, significantly improving efficiency in processing large-scale traffic graphs compared to existing methods.\n\u2022 STGformer outperforms state-of-the-art Transformer-based methods on the LargeST benchmark, demonstrating remarkable efficiency by being 100\u00d7 faster and us-"}, {"title": "2 RELATED WORK", "content": "2.1 Traffic Forecasting\nDeep neural networks have become the predominant approach for traffic forecasting [21], [22], [23], [24], [25], typically combining graph neural networks (GNNs) with either Recurrent Neural Networks (RNNs) or Temporal Convolutional Networks (TCNs) to capture complex spatio-temporal patterns in traffic data. RNN-based models, such as DCRNN [21], incorporate diffusion convolution with GRU layers to model spatial and temporal dependencies. Extensions of this approach include ST-MetaNet [27], which employs meta-learning with graph attention networks, and AGCRN [28], which introduces node-specific adaptive parameters in graph convolution. To improve computational efficiency, TCN-based models like STGCN [30] and GWNet [2] have adopted dilated causal convolutions for temporal modeling. These architectures demonstrate faster training times and competitive performance on various benchmarks. Attention mechanisms have been integrated into models such as AST-GCN [9] and STAEformer [5] to better capture long-range dependencies and complex spatio-temporal interactions. These approaches have shown improved performance in handling diverse traffic patterns. Recent research directions include the integration of GNNs with neural ordinary differential equations for continuous modeling of spatio-temporal dependencies [33], [34], and the development of dynamic adjacency matrices to reflect changing relationships over time [29], [35]. These emerging approaches aim to address limitations of previous models and enhance the adaptability and interpretability of traffic forecasting systems.\n2.2 Graph Transformer\nGraph Transformers have emerged as a powerful class of models for learning on graph-structured data, with several surveys reviewing different aspects of these models. The incorporation of graph structure into Transformer architectures has been explored through various graph inductive biases, as discussed by Dwivedi et al. [36] and Ramp\u00e1\u0161ek et al. [37], who provided comprehensive overviews of node positional encodings, edge structural encodings, and attention bias. In terms of graph attention mechanisms, Velickovic et al. [38] introduced graph attention networks (GAT) leveraging multi-head attention for node classification, while subsequent works like GATv2 [39] addressed limitations in GAT's expressiveness. The literature has explored various types of graph Transformers, including shallow models like GAT and GTN [40], deep architectures stacking multiple attention layers [41], [42], scalable versions addressing efficiency challenges for large graphs [37], [43], and pre-trained models leveraging self-supervised learning on large graph datasets [44], [45]. Graph Transformers have demonstrated promising results across various domains, including protein structure prediction in bioinformatics [46], [47], entity resolution in data management [48], [49], and anomaly detection in temporal data [50], [51]. Despite their success, recent surveys [37], [52] have highlighted ongoing challenges in scalability, generalization, interpretability, and handling dynamic graphs, indicating that addressing these issues remains an active area of research in the graph learning community. At the same time, the Transformer based on spatiotemporal graph correlation has not yet been explored in the field of traffic prediction."}, {"title": "3 PRELIMINARIES", "content": "3.1 Problem Statement\nIn this paper, we formalize the representation of a graph as G = (V,E, A), where V denotes the set of nodes with cardinality N = |V|, E \u2286 V \u00d7 V defines the set of edges, and A \u2208 \u211d^{N\u00d7N} represents the adjacency matrix. The dynamic nature of the graph is captured by a time-dependent feature matrix X_t \u2208 \u211d^{|V|\u00d7C} at each discrete time step t, where C denotes the dimensionality of node features (e.g., traffic flow, vehicle speed, and road occupation). Traffic forecasting can be formally expressed as a function:\nf: [X_{(t-T):t}, G] \u2192 X_{(t+1):(t+S)},\nwhere T and S represent the input and output sequence lengths, respectively, which encapsulates the temporal dependency by considering a historical window of T time steps and predicting future states for S time steps ahead.\n3.2 Spatiotemporal Graph Convolution\nGraph neural networks (GNNs) have the advantage of aggregating node neighborhood contexts to generate spatial representations, which is earliest method introduced to spatiotemporal graph modeling [1], [3], [11]. Formally, let *g is graph convolution operator [13], which is reformulated as\n\u0398 *g h = \u0398(\u00a3)x \u2248 \u2211_{k=0}^{K-1} \u03b8_k T_k(\u00a3)h,\nwhere 0 = [\u03b8_0,\u2026\u2026,\u03b8_{K-1}] \u2208 \u211d^K is a vector of polynomial coefficients K denotes the kernel size of the graph convolution T_k(\u00a3) \u2208 \u211d^{N\u00d7N} represents the k-th order Chebyshev polynomial evaluated at the rescaled Laplacian \u00a3 = 2L/\u03bb_{max} \u2212 I_N L = D^{-1/2}(D \u2013 A)D^{-1/2} is the normalized graph Laplacian D is the degree matrix of the graph \u03bb_{max} is the largest eigenvalue of L I_N is the N \u00d7 N identity matrix. This approach enables the efficient computation of K-localized convolutions by leveraging polynomial approximation, effectively capturing the local structure of the graph within a K-hop neighborhood.\n3.3 Spatiotemporal Self-Attention Layer\nInstead of GCNs [2], [9], [11], which aggregate neighboring features using static convolution kernels, Transformers [5], [6] employ multi-head self-attention to dynamically generate weights that mix spatial and temporal signals. Formally, given a hidden spatiotemporal representation h \u2208 \u211d^{T\u00d7N\u00d7C}, where T is the number of time frames, N = |V| is the number of graph nodes, and C denotes the channel dimension, we can formulate the spatiotemporal self-attention mechanism as follows: For spatial self-attention, the query, key, and value matrices are derived as: Q_s = hW_q, K_s = hW_k, and V_s = hW_v, where W_q, W_k, W_v \u2208 \u211d^{C\u00d7C} are learnable parameter matrices."}, {"title": "3.4 Analysis of GCN and Transformer Flops", "content": "In analyzing the computational complexity of spatiotemporal graph modeling techniques, we observe distinct characteristics between graph convolution and self-attention mechanisms. The spatiotemporal graph convolution, utilizing Chebyshev polynomials, exhibits a computational complexity of O(K|E|C), where K represents the kernel size, |E| the number of edges, and C the number of channels. This complexity arises primarily from the polynomial approximation of the graph Laplacian. In contrast, the spatiotemporal self-attention layer demonstrates a more intricate computational profile, with a complexity of O(TN^2C+NT^2C), where T denotes the number of frames. This increased complexity stems from the dynamic weight generation in multi-head self-attention, encompassing operations such as query-key interactions, softmax computations, and attention-weighted aggregations across both spatial and temporal dimensions. The self-attention approach, while more computationally intensive, offers enhanced flexibility in capturing complex spatiotemporal dependencies, particularly when dealing with lengthy sequences or high-dimensional feature spaces. The choice between these methods thus presents a trade-off between computational efficiency and model expressiveness, contingent upon the specific requirements of the spatiotemporal modeling task at hand."}, {"title": "4 METHODOLOGY", "content": "4.1 Overview\nThe overall architecture of our proposed STGformer is illustrated in Figure 4. The key feature of our model is its efficiency, as it achieves joint spatiotemporal graph attention using only a single attention module. Our model is divided into two branches: the graph propagation module and the attention module, with specific details shown in Figure 3. First, the spatiotemporal data undergoes graph propagation and is then fed into the attention module separately. Subsequently, a 1x1 convolution is applied to interact with the outputs of different-order attentions, which are finally aggregated.\n4.2 Data Embedding Layer\nTo transform the input data into a high-dimensional representation, we adopt a data embedding layer consistent with the STAEformer. Specifically, the raw input X is first projected into X_{data} \u2208 \u211d^{T\u00d7N\u00d7d} through a fully connected layer, where d is the embedding dimension. Recognizing the inherent periodicity of urban traffic flow influenced by human commuting patterns and lifestyles, such as rush hours, we introduce two embeddings to capture weekly and daily cycles, denoted as t_w(t), t_d(t) \u2208 \u211d^d, respectively. Here, w(t) and d(t) are functions that map time t to the corresponding week index (1 to 7) and minute index (1 to 288, with a 5-minute interval). The temporal cycle embeddings X_w, X_d \u2208 \u211d^{T\u00d7d} are obtained by concatenating the embeddings of all T time steps. Following [5], we also incorporate spatiotemporal positional encoding X_{ste} \u2208 \u211d^{N\u00d7T\u00d7d} to introduce positional information into the input sequence. Finally, the output of the data embedding layer is obtained by simply concatenating the aforementioned embedding vectors: X_{emb} = X_{data} || X_{w} || X_{d} || X_{ste}.\n4.3 Spatiotemporal Graph Transformer\nAs previously elucidated, GCNs excel in modeling locally high-interaction information, whereas Transformers"}, {"title": "4.4 Computational Cost Analysis.", "content": "From Eq. (5), it is evident that the computational cost of the spatiotemporal softmax attention mechanism scales with O (TN^2 + NT^2). The memory requirements similarly increase, as the complete attention matrix must be stored to compute the gradients for the queries, keys, and values. In contrast, the linear transformer we adopt in Eq. (7) has both time and memory complexities of O(N + T).\nWe will divide the computation of our into 3 parts, and calculate the FLOPs for each part.\n\u2022 Graph Propagation. Graph propagation, employing Chebyshev polynomials, owns a computational complexity of O(K|E|C), where K denotes the order, |E| represents the number of edges, and C signifies the number of channels. This complexity is predominantly attributed to the polynomial approximation of the graph Laplacian.\n\u2022 Spatiotemporal Linear Attention. As previously mentioned, the time complexity of Spatiotemporal Linear Attention is O(N + T), where N is the number of spatial nodes and T is the temporal length. Since the process needs to be performed K times, the overall computational complexity becomes O(K(N + T)).\n\u2022 Recursive Interaction. We consider the FLOPs of the element-wise multiplication with 1\u00d71 convolution. Therefore, the computational cost is KNTC^2.\nTherefore, the total FLOPs with spatiotemporal attention are:\nFLOPS (STGformer) = KC(|E| + N + T + NTC).\nBecause STAEFormer performs self-attention operations on spatial and temporal stacked with L layers respectively, we can easily calculate its FLOPs as:\nFLOPS (STAEFormer) = L(TN^2C + NT^2C).\nFor instance, assuming input lengths of 12, California graph with 8600 nodes, a hidden dimension of 32, an interaction order K of 3, |E| = 201,363 and L = 3, the ratio of FLOPs between STGformer and STAEFormer can be calculated as follows:\nFLOPS(STGformer)/FLOPS(STAEFormer) \u2248 0.00131,\nwhich STGformer significantly reduces 99.869% computational burden compared to STAEFormer."}, {"title": "5 EXPERIMENT", "content": "Datasets. We experimented with LargeST [8], which aggregated traffic readings from 5-minute intervals into 15-minute windows, aiming to predict future 12-step outcomes based on historical 12-step data [55]. LargeST comprises three California sub-datasets constructed from three representative regions within the state. The first is Los Angeles, encompassing 3,834 sensors installed across five counties in the Los Angeles region: Los Angeles, Orange, Riverside, San Bernardino, and Ventura. The second sub-dataset, the Bay Area, includes 2,352 sensors located in 11 counties: Alameda, Contra Costa, Marin, Napa, San Benito, San Francisco, San Mateo, Santa Clara, Santa Cruz, Solano, and Sonoma. The smallest sub-dataset, San Diego, contains 716 sensors. To further verify the performance of our method, we also conducted experiments on the widely-used PEMS-series benchmarks i.e., PEMS03, PEMS04, PEMS07, PEMS08 [9]. PEMS-series, representing the four major districts in California, are aggregated into 5-minute intervals, resulting in 12 data points per hour and 288 data points per day.Implementation Details. Our experiments ran on a GPU server with eight GeForce GTX 3090 graphics cards, employing the PyTorch 2.0.3 framework. Raw data have been standardized using z-score normalization [56]. If validation error stabilized within 15-20 epochs or ceased after 200 epochs, training halted prematurely, preserving the best model based on validation data [57]. We maintained fidelity to the original paper's model parameters and settings, while also conducting multiple parameter tuning iterations to enhance experimental outcomes. Data were partitioned chronologically into training, validation, and test sets at a 6:2:2 ratio across all sub-datasets. In our experiments, we assess model performance using the Mask-Based Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE) as metrics, wherein zero values (indicating noisy data) are disregarded.Baselines. In this study, we conduct a comprehensive evaluation of traffic forecasting methodologies, encompassing a diverse array of baselines with publicly available implementations. These baselines span traditional approaches, contemporary deep learning techniques, and state-of-the-art models, providing a thorough representation of the field's progression.\n\u2022 HA (Historical Average): Conceptualizes traffic flows as periodic processes, utilizing weighted averages from antecedent periods for future predictions."}, {"title": "5.1 Performance Comparisons.", "content": "Performance on LargeST. We further evaluated the performance of STGformer on the LargeST datasets. Experimental results in Table 1 demonstrate that STGformer consistently outperforms STAEformer across all evaluated datasets, exhibiting significant performance improvements. Specifically, in the San Diego dataset, STGformer achieved the most remarkable advancements in average metrics, with improvements of 3.61%, 2.83%, and 6.73% in MAE, RMSE, and MAPE, respectively. While the improvement margins were relatively smaller for the Bay Area and Los Angeles datasets, STGformer maintained a consistent advantage, with average metric improvements ranging from 1.92% to 2.12%. Notably, STGformer not only excels in prediction accuracy but also demonstrates superior model efficiency. Taking the Los Angeles dataset as an example, STGformer achieved performance superior to STAEformer, which has 4.7M parameters, while utilizing only 705K parameters, highlighting its significant advantage in parameter efficiency. These results collectively indicate that STGformer can effectively enhance the accuracy of spatiotemporal sequence forecasting while maintaining a lower computational complexity, providing a valuable new direction for research in this field.\nPerformance on Cross Year Scenario. Table 2 presents a comparative analysis of STGformer against other spatiotemporal baseline models in cross-year scenarios, including three major urban regions: San Diego, Bay Area, and Los Angeles. We compared the performance of spatiotemporal models trained on 2019 data when applied to 2020 data to evaluate model generalization ability. In the San Diego dataset, STGformer achieved a significant 14.14% reduction in RMSE for 3-hour predictions, decreasing from 31.55 to 27.09. Similarly, on the Bay Area dataset, RMSE for the same forecast horizon decreased from 32.66 to 28.20, representing a substantial 13.66% improvement. The Los Angeles dataset exhibited a similar trend, with a notable 13.10% reduction in RMSE for 3-hour predictions, from 33.97 to 29.52. These"}, {"title": "5.2 Ablation Study", "content": "To comprehensively evaluate the significance of various components within the STGformer, we conducted an ablation study using the LargeST dataset. Four scenarios were considered in Figure 5: without self-attention (W/o SA), without temporal self-attention (W/o TSA), without spatiotemporal self-attention (W/o SSA), and without graph high-order interaction (W/o Graph). Our findings indicate that the absence of any component led to a substantial decline in performance. Notably, the removal of spatiotemporal self-attention (W/o SA) resulted in the most significant performance degradation across all metrics (MAE, RMSE, and MAPE) and datasets, as it reduced the model to a feed-forward module. Higher-order interactions (W/o Graph) also played a crucial role, with their absence having a relatively larger impact compared to removing only spatial or temporal self-attention. Furthermore, spatial self-attention (W/o SSA) and temporal self-attention (W/o TSA) contributed significantly to performance, as their removal led to substantial performance drops. These results underscore the critical importance of each component, particularly global spatiotemporal information and higher-order interactions, for achieving optimal performance with the STGformer."}, {"title": "5.3 Case Study", "content": "Figure 6 illustrates the results of traffic flow prediction, including STGformer, STAEformer, AGCRN, D2STGNN, and ST-GCN, comparing the performance of multiple models across different nodes. (a) Node 24 in San Diego: The STGformer model closely adheres to the ground truth traffic flow, demonstrating superior accuracy particularly during transitional periods between peak and off-peak hours. In contrast, other models exhibit deviations from the actual values at certain intervals. (b) Node 64 in San Diego: STG-former's predictive curve aligns closely with the ground truth, notably outperforming other models in the afternoon period. Alternative models such as STAEformer and D2STGNN display greater fluctuations or discrepancies. (c) Node 93 in Los Angeles: STGformer's performance is particularly noteworthy during morning and evening peak hours, with predictions nearly coinciding with the ground truth. Other models show varying degrees of overestimation or underestimation during these critical periods. (d) Node 208 in Los Angeles: Despite all models exhibiting some bias in predictions during the afternoon traffic peak, STG-former demonstrates the closest approximation to actual flow trends. This is especially evident in the post-18:00 timeframe, where it significantly outperforms other models."}, {"title": "6 CONCLUSION AND LIMITATION", "content": "In conclusion, this study introduces STGformer, a novel spatiotemporal graph Transformer model that addresses the computational challenges faced by existing GCN and Transformer-based methods in adapting to real-world road networks. STGformer demonstrates superior performance across various traffic benchmarks, from small-scale PEMS datasets to the large-scale LargeST dataset, utilizing only a single layer and linear spatiotemporal global attention. ST-Graph attention block enables efficient high-order spatiotemporal interactions for both global and local patterns, significantly reducing computational costs and memory usage compared to state-of-the-art methods like STAEformer.Furthermore, STGformer exhibits remarkable generalization capabilities, maintaining robust performance even when tested on data from a year later. These results position STGformer as a promising backbone for spatiotemporal models in large-scale traffic forecasting applications."}]}