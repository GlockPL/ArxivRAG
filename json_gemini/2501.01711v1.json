{"title": "LLMs & Legal Aid: Understanding Legal Needs Exhibited Through User Queries", "authors": ["Michal Kuk", "Jakub Harasta"], "abstract": "The paper presents a preliminary analysis of an experiment conducted by Frank Bold, a Czech expert group, to explore user interactions with GPT-4 for addressing legal queries. Between May 3, 2023, and July 25, 2023, 1,252 users submitted 3,847 queries. Unlike studies that primarily focus on the accuracy, factuality, or hallucination tendencies of large language models (LLMs), our analysis focuses on the user query dimension of the interaction. Using GPT-40 for zero-shot classification, we categorized queries on (1) whether users provided factual information about their issue (29.95%) or not (70.05%), (2) whether they sought legal information (64.93%) or advice on the course of action (35.07%), and (3) whether they imposed requirements to shape or control the model's answer (28.57%) or not (71.43%). We provide both quantitative and qualitative insight into user needs and contribute to a better understanding of user engagement with LLMs.", "sections": [{"title": "1. Introduction", "content": "This paper reports on the experiment running between May 3, 2023, and July 25, 2023, which utilized GPT-4 to answer legal questions submitted to Frank Bold Legal Counseling Center in the Czech Republic. The paper does not address the accuracy and factuality of the model's answers. Instead, we focus on analyzing queries, allowing a unique insight into legal needs demonstrated by users intuitively interacting with GPT-4.\nChatGPT was launched on November 30, 2022, drawing significant attention to the natural language capabilities of large language models (LLMs). Its accessibility immediately became a major disruptive force, raising questions about its potential impact on legal services and access to justice. Various organizations worldwide have become interested in leveraging LLMs to support, scale, or restructure their operations.\nFrank Bold (FB), a Czech expert group offering for-profit and non-profit services, has experimented with ChatGPT since early 2023. FB ran a public experiment between May 3, 2023, and July 25, 2023, mediating access to ChatGPT to the general public seeking legal aid. Throughout the experiment, 1,252 users submitted 3,847 queries, to which GPT-4 responded. Various studies focused on capabilities displayed by models [1, 2, 3, 4, 5, 6, 7], and prevalence of hallucinations [8, 9]. However, the issue of AI/GAI/LLM's ability to perform human tasks is largely detached from user expectations and perceptions related to interactions with LLMs. We aim to support surveys by Hagan [10] and Cheong et al. [11] and provide a better understanding of user legal needs by analyzing larger queries arising from their intuitive use of GPT-4.\nWe report on statistics of the user queries and offer insight into the legal needs the users intuitively manifested throughout the experiment. We outline the experiment (Section 2), offer quantitative statistics of the user queries (Section 3), and qualitative analysis of the observed trends (Section 4). The"}, {"title": "2. Experimental set-up", "content": "In 2023, Frank Bold conducted an experiment to explore and evaluate the potential applications of the newly accessible GPT-4 model in the context of legal aid. Frank Bold is an expert group established in 2013 as a collective of entities offering both for-profit and non-profit services in law and other areas. The services include a commercial law firm and a non-profit online Legal Counseling Center (Pr\u00e1vn\u00ed poradna in Czech). The latter provides the Czech public with legal information and assistance in areas of public interest, including environmental law, whistleblowing and corruption-related issues, civic rights, municipal laws, and civic engagement issues. Additionally, the counselling centre offers legal technology tools such as interactive interviews on legal matters and document generators.\nThe experiment utilizing GPT-4 was initiated on May 3, 2023, and was accessible via the now (December 2024) defunct online platform at www.ai.frankbold.org. The effort was presented as an exploration of the experimental application of artificial intelligence to address legal questions. Initially, given the internal funds available, the limit was set at providing answers to 3,000 user queries. On June 19, 2023, the limit was increased to 4,000 user queries. The limit was reached on June 10, 2023, when the experiment was concluded. The experiment was made public through FB's internal mailing lists. As the first effort of its kind, information about the project was disseminated through several prominent online media outlets.\nUsers interested in submitting their queries needed to create a user account and provide a valid e-mail address and a full name. Additionally, users could, as non-required information, provide additional information such as their profession, the organization to which they were affiliated, and their phone number. Once the registration and login processes were complete, users could submit their queries via an interface comprising a single input form. The tool operated on a single question-single answer basis. It did not facilitate chat-like interaction, which is now well-known to users of ChatGPT, and did not allow follow-up questions. Users were limited to 10 questions per day. Users could select whether to wait for the answer (being informed that it might take up to 3 minutes to provide it) or prefer the answer sent to their e-mail (the e-mail option was added later in the experiment on May 25). The delay was caused by the set-up presenting the user with the final answer created by the LLM instead of having it in a well-known streaming text format. 24% of the users preferred receiving answers via e-mail. Subsequently, the answer was enriched by links to relevant articles from FB's Legal Counseling Center and the FB Law Firm's blog, which could be accessed to gather further, more detailed information. The final element was a voluntary option to rate the provided answer and provide textual feedback.\nThe LLM-generated answer employed retrieval-augmented generation (RAG) comprising the follow- ing steps:\n1. Retrieval of user query.\n2. Identification of the relevant context through a similarity search between the user's query and FB's guidelines, blog posts, and articles. Later in the experiment, on May 25, selected legal acts were added as a context source.\n3. Selection of the relevant context based on similarity search.\n4. Combination of the relevant context with user query to form a single prompt.\n5. Retrieval of answer generated by GPT-4.\n6. Selection of links to guidelines, blog posts, and articles based on similarity search of the answer.\n7. Presentation of the answer to the user on the website or via e-mail.\nTo provide users with guidance and to manage expectations, a comprehensive set of instructions and a detailed disclaimer were displayed to the user. Before submitting a query, the user was informed that the tool performs optimally in the subject areas where FB's Legal Counseling Centre has developed a"}, {"title": "3. Numbers...", "content": "Throughout the experiment, a total of 4,045 queries were submitted by 1,262 registered users. The total number of users who registered for the experiment was 1,543. However, 281 (18%) of the registered users never submitted any query. 72% of the queries were submitted in the first half of the 13 weeks of the experiment running. Before the analysis, we removed duplicities in queries and queries that were out-of-scope of the experiment (such as users asking for flight duration between New York and Prague, etc.). The preprocessing brought the final tally to 3,847 queries submitted by 1,252 users.\nOne of the benefits of online legal assistance tools, including those that employ AI, is their accessibility at any time. In this experiment, most queries were submitted on Friday, with 61% of the queries submitted between 9:00 and 17:00 and 76% submitted during workdays. Therefore, a substantial part of queries were submitted during standard work hours, but with a clear preference for days later in the week"}, {"title": "4. ... and needs", "content": "It is safe to assume that there are human needs behind every submitted query. The users facing legal issues and having legal needs approached the interface with some level of expectation that their needs would be met. Our understanding of user interaction with the LLM is often shaped by expert-driven [11] or community-driven findings [10] based on workshops and interviews. To our knowledge, no analysis provides insight into user needs by observing an intuitively used set of queries submitted by users trying to obtain legal information or legal help.\nOne of the main concerns about using GPT-4 for legal help and self-help is the blurry line between obtaining legal information and seeking legal advice. Providing legal opinions is associated predomi- nantly with human experts, who are, as members of the legal profession, accountable and liable for their advice.\nCheong et al. [11] proposed a framework describing the issues related to potential risks and avenues to consider when developing responsible LLM policies for legal advice. One of the dimensions identified in their paper were considerations about the user queries dissipating into three interconnected parts: assessment of facts, identification of relevant laws, and the nature of desired answers. Their analysis serves as our starting point for classifying user queries submitted throughout the experiment.\nWe randomly selected 200 queries from the dataset of user-submitted queries and developed descrip- tive codes to help us understand their nature.\nThrough an iterative process, and based on Cheong et al., we developed the following categories 1:\n1. Facts in User Query: Queries can describe facts about a specific situation the user encountered. Users tend to either explain the facts or submit a specific document. Alternatively, queries can contain no facts.\n2. Information about the Law: Queries can seek retrieval of specific legal information or iden- tification and lookup of a specific act or case law. Alternatively, queries seek advice on further course of action and possible solutions.\n3. User Grants Control: Queries can pose open-ended questions, which provide no guidance in structuring the answer and grant control over the answer to the model. Alternatively, queries can be formulated to impose requirements on the answer's structure or format.\nWe pose that queries providing facts, seeking advice, and containing open-ended questions manifest intuitive users' expectations of obtaining personalized and actionable advice about the further course"}, {"title": "5. Limitations and Discussion", "content": "Before we engage in the discussion, it is necessary to outline the limitations of our analysis.\nFirst, the experiment was primarily designed to explore and evaluate the potential applications of the GPT-4 model in the context of legal aid. The experiment was an internal effort of Frank Bold to bolster their non-profit activities and scale their legal clinic efforts. As such, no testable hypotheses were developed before the experiment. There are many variables which were not controlled. Arguably, our paper offers a great insight into actual and genuine legal issues people face and how they intuitively queried the GPT-4. At the same time, it means there is no demographic or other background available about the individual users, which could shed more light on the motivations and expectations of individual users.\nSecondly, we developed the codes based on the existing literature and on iterative analysis of 200 randomly selected queries. The approach is reasonable within the confinement of our preliminary analysis. However, it gave us only limited validity of the results. Arguably, a more rigorous approach will lead to more nuanced categories, which will provide further (more detailed) insight into users' intuitive use of LLMs for legal aid.\nThirdly, we opted for zero-shot classification based on our codes and related definitions, which we used as prompts for GPT-40. Zero-shot classification, while, as Savelka and Ashley [12] put it, arguably 'unreasonably effective', has its limitations. We did not engage in any validation of the zero-shot classification provided by GPT-40. The trends in user queries are clear. However, the precise numbers and ratios may be significantly different should the classification be done in other settings (a few-shot, manual, etc.)\nDespite these limitations, our paper offers interesting-and, to our knowledge, previously unavail- able-insights into intuitive user interaction with LLMs within legal aid. Our results provide several key takeaways.\nThe often-claimed risk of users disclosing their personal or other potentially sensitive information to LLMs or LLM-based applications is real and severe. When preparing the dataset for sharing between the researchers through the removal of personally identifiable information, the number of details disclosed by participants encountered by the first author (M.K.) was staggering. Especially within longer queries, users did not hesitate to include anything they deemed relevant as a factual context for their query. Within the nearly 30% of queries containing facts, some users went to great lengths to describe anything"}, {"title": "6. Conclusion and Future Work", "content": "While LLMs pose risks when misused, especially in high-stake contexts, they are here to stay. It does not seem reasonable to assume that laypeople will not use these tools for legal self-help just because they are informed they should avoid using them. Personalized legal services are prohibitively expensive even to the middle class, not to mention low-income and marginalized people. Warnings may lead users to be more careful when dealing with LLMs but will not prevent them from querying various models when seeking information. LLMs are here to stay and will be used - that much is clear from anecdotal evidence and the experiment introduced in the paper.\nTwo strategies can be leveraged to address these issues - one on the side of users and one on the side of providers. There must be a significant increase in AI literacy Ng et al. [13] throughout the society. As basic computer literacy became part of citizens' regular background knowledge, AI literacy must also become the norm. Future work should investigate laypeople's and professionals' intuitive use of LLMs, provide further insight, and offer strategies to increase their literacy to manage expectations and encourage responsible behaviour. On the other hand, users cannot be the only ones bearing the grunt of the widespread use of AI. Specific safeguards should be developed and implemented to mitigate the risks reasonably. These two efforts must go hand in hand.\nAdditionally, the experiment employed retrieval-augmented generation. FB's internal evaluation of the results suggests that users expressed varying satisfaction levels when receiving answers with different augmentation levels or using different augmentation methods. More robust experiments in this direction should follow. A layered approach forcing LLMs to consider specific-either expertly prepared or selected-documents and contexts could be an answer to some of the issues related to hallucinations and factuality.\nThe paper analysed user queries submitted by users Frank Bold, the Czech expert group providing for-profit and non-profit legal services. The experiment led 1,252 users to submit 3,847 queries to GPT-4. We classified queries based on whether or not they contained facts of the case, whether users required legal information or advice, and whether users posited open-ended questions or imposed further requirements on the structure of the model's answer. We used zero-shot classification by GPT-40 to classify the user queries. We have shown that users mosly do not provide LLMs with facts (70.05% over 29.95%), request information (as opposed to advice; 64.93% over 35.07%), and ask open-ended questions, granting control over the reply to the model (71.43% over 28.57%). We offered a unique insight into the intuitive use of LLMs. However, more detailed analysis and more rigorous experiments and surveys are required."}]}