{"title": "What if Eye...? Computationally Recreating Vision Evolution", "authors": ["Kushagra Tiwary", "Aaron Young", "Zaid Tasneem", "Tzofi Klinghoffer", "Akshat Dave", "Tomaso Poggio", "Dan-Eric Nilsson", "Brian Cheung", "Ramesh Raskar"], "abstract": "Vision systems in nature show remarkable diversity, from simple light-sensitive patches to complex camera eyes with lenses [1, 2]. While natural selection has produced these eyes through countless mutations over millions of years, they represent just one set of realized evolutionary paths [3, 4]. Testing hypotheses about how environmental pressures shaped eye evolution remains challenging since we cannot experimentally isolate individual factors [5]. Computational evolution offers a way to systematically explore alternative trajectories [6-10]. Here we show how environmental demands drive three fundamental aspects of visual evolution through an artificial evolution framework that co-evolves both physical eye structure and neural processing in embodied agents. First, we demonstrate computational evidence that task specific selection drives bifurcation in eye evolution - orientation tasks like navigation in a maze leads to distributed compound-type eyes while an object discrimination task leads to the emergence of high-acuity camera-type eyes. Second, we reveal how optical innovations like lenses naturally emerge to resolve fundamental tradeoffs between light collection and spatial precision. Third, we uncover systematic scaling laws between visual acuity and neural processing, showing how task complexity drives coordinated evolution of sensory and computational capabilities. Our work introduces a novel paradigm that illuminates evolutionary principles shaping vision by creating targeted single-player games where embodied agents must simultaneously evolve visual systems and learn complex behaviors. Through our unified genetic encoding framework, these embodied agents serve as next-generation hypothesis testing machines while providing a foundation for designing manufacturable bio-inspired vision systems [11]. Website: eyes.mit.edu", "sections": [{"title": "1 Introduction", "content": "What if vision was only used for navigation or detection? What if eyes never evolved optical elements like lenses? What if animal brains stayed small throughout evolution? Operating over millions of years and culminating in millions of unique perception systems [1], natural evolution has followed specific evolutionary trajectories in its development of vision. What if there was a tool to instead simulate alternative paths that evolution didn't take? By computationally recreating the evolutionary dynamics (i.e., mutation, selection, adaptation) which gave rise to"}, {"title": "2 Results", "content": "Our computational framework tests hypotheses about how specific environmental pressures shape eye morphologies, neural architectures, and behavior. While previous work has used evolutionary algorithms to independently design optical systems [40, 41], embodied agent morphologies [8, 42, 43], or visually-guided behaviors [44], our approach uniquely evolves embodied agents with both eyes and behaviors together. This enables the automatic task-driven discovery of diverse vision-based embodied agents."}, {"title": "2.1 A What if ...? World", "content": "We model the world in which embodied agents interact as a deep reinforcement learning environment, where agents must evolve appropriate vision capabilities such that they can learn effective behavior. It's infeasible, however, to model all factors which contribute to the evolution of vision; thus, we model each environment as corresponding to a specific function of vision, such as orientation or object discrimination. In this way, each environment represents a single task which models the functional pressures hypothesized to garner the emergence of vision. We focus on modeling three distinct tasks to isolate their effects on vision evolution [2, 31]: NAVIGATION, DETECTION, and TRACKING. We create these tasks in a MuJoCo simulation environment [35, 45] with custom changes to support complex imaging models and evolutionary search. Each agent in this environment is modeled as a point mass (the green sphere in Figure 2) with a heading and forward velocity. For more technical details, please see Section 4."}, {"title": "2.2 Genetic Encoding for Vision", "content": "Vision in nature has co-evolved as a function of sensing and the underlying neural circuitry; subsequently constraining behavior that an animal learns during its lifetime [2, 4, 46]. At a population scale, this continuous feedback loop between evolution and learning ensures the Baldwin Effect [12] where learned behaviors affect the evolution and selection of genetic traits of future generations. Similarly, we create a genotype that directly encodes both the physical (morphological and optical) and the neurological components of an agent's vision. Rather than incorporating neural network weights directly in the genomic encoding [47-49], we train each agent from scratch to learn behaviors specific to the mutated eye design.\nSimilar to nature, our genetic encoding scheme needs to be general enough to allow the emergence of a diverse set of eyes and cognitive capabilities while being physically realizable."}, {"title": "2.3 Co-Evolution of Vision and Behavior", "content": "Our approach computationally simulates nature's co-evolution approach to vision innovation: changes in sensory capabilities directly influence behavioral performance, which in turn guides the evolution of future eye morphologies, optics and behaviors. We implement co-evolution through two nested loops that mirror the interplay between evolutionary timescales and lifelong adaptation. Over generations, the outer evolutionary loop utilizes the Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [50, 51] to enable efficient selection and mutation of populations of agents. Within each generation, the inner learning loop, an agent with the selected genotype is instantiated and trained to solve a visual task through reinforcement learning via Proximal Policy Optimization (PPO) [52]. During the agent's lifetime, the agent's performance is evaluated within the same visual task. The fitness of each agent is then used in the following generation to selectively adapt the populations genotypes. We discuss the evolution and learning loops in more detail in Section 4."}, {"title": "2.4 What if the goals for vision were different?", "content": "Understanding how specific visual tasks shaped the evolution of eyes remains a major challenge because animals are required to solve multiple visual tasks simultaneously. For instance, honey-bees have evolved compound eyes with around 5,000 individual receptors, balancing trade-offs between extracting optic flow to maintain equidistance from obstacles and regulate flight speed, and sufficient spatial resolution to discern body movements of other bees in their colony [34]. This coupling of tasks in nature makes it difficult to understand how individual visual demands influence eye evolution. For instance, dragonflies have evolved compound eyes with high-resolution regions, making it challenging to identify which visual adaptation was a result of which environmental pressure. Thus, would evolution converge on similar eye morphologies as found in nature"}, {"title": "2.5 What if eyes could bend light?", "content": "Early visual systems faced a fundamental trade-off between light collection and acuity, progressing from simple light-sensitive patches to cup-shaped eyes with smaller apertures [58, 59]. While decreasing the size of the aperture and creating pinhole-like designs is a straightforward way to improve image formation, they severely limit light collection. This trade-off creates a performance ceiling, where further improvements in spatial resolution through pinhole designs are limited by the lack of light. This inherent limitation ultimately restricts the visual capabilities of such systems, causing a saturation in performance. We see this manifested in our results where agents with pinhole eyes plateau in fitness and are not able to achieve the performance benefits of"}, {"title": "2.6 What if the brain became larger?", "content": "Biological visual intelligence emerges from the interplay and scaling between sensory hardware, morphology, and neural processing [63, 64]. While artificial intelligence relies on fixed sensors (RGB cameras) and scales with the number of parameters, nature has evolved diverse eye-brain"}, {"title": "3 Discussion", "content": "Similar to natural evolution, we follow a function over form approach, where we code the desired function through fitness and let evolutionary search discover a variety of forms that are optimal for the fitness. This results in our agent's form (eye design and learned behavior) to emerge solely from functional pressures from the environment such as orientation, obstacle avoidance, or object discrimination. The emergent features resemble principles of real biological evolution. These results affirm our central claim that embodied agents trained with reinforcement learning can serve as hypothesis-testing machines for vision and vision evolution. The evolutionary outcomes we present are a result of the co-evolution of vision- hardware (physical eye morphology and structure) and software (learned behavior of the agent). Lastly, in our current approach, we evolve our agents under isolated environmental pressures i.e. cases where agents are heavily biased to evolve to solve a single task. However, in the natural world animals have evolved to jointly solve diverse tasks found in their ecological niches. While our framework can be easily extended for diverse visual tasks, isolated scenarios help us understand the extreme cases.\nSince our work is the first in this space, our results point towards open technical challenges that will enable a wider variety of hypothesis to be tested. For instance, future research can be extended to incorporate explore multi-agent interactions where multiple species evolve in shared environments, applying gradient-based methods, or incorporating richer light properties like spectral, polarization, or temporal sampling. Additionally, future work could include incorporating bio-physical models of vision [38] or replace neural networks with mechanistic circuits derived from fly connectomes [17, 67].\nOur framework provides a discovery tool by enabling large scale computational evolution of vision in embodied artificial agents. For biologists and cognitive scientists, this approach allows systematic manipulation of key variables to test alternative hypothesis or counterfactuals such as isolating the effects of optical elements from neural processing, or testing how specific environmental pressures drive eye morphology. Much like natural and artificial evolution [68],"}, {"title": "4 Methods", "content": "Learning loop. The learning loop is the mechanism for which we score each agent. Via reinforcement learning, we train the brain of the agent (i.e., neural network parameters). Reinforcement learning serves as a mechanism for learning representations of the environment through interactions with it. The subsequent score, or fitness, of the agent is determined from the average reward it receives over six evaluation episodes after training. We utilize an open-source implementation of the Proximal Policy Optimization (PPO) algorithm [52, 71]. Each agent is trained up to 1 million total steps, though training may be terminated early if no improvement is found after five evaluations.\nReinforcement learning algorithms have been shown to have a strong dependence on the random seed used to initialize the environment [72]. Thus, during the evolution loop, we allow the same agent genotype to be sampled multiple times. Additionally, each agent's training loop is initialized with a unique random seed such that configurations sampled with the same genotype are not subject to the same seed. This allows for a more robust evaluation of the agent's performance.\nObservations. An agent interacts with its environment through actions based on its observations. The observations are created by compositing the images captured from each eye. For example, if an agent's vision system consists of 5 eyes with four photoreceptors in each eye, the resulting observation by the full agent \"eye\" at each time step will be a tensor of size 5 x 4 x 1 x 3. Furthermore, for each eye, the previous observations are stacked in a memory buffer. If the memory buffer is of size 10, then a tensor if size 10 x 5 x 4 x 1 x 3 is provided to the underlying agent network. In addition to the visual stimuli, we provide a single boolean that describes whether an agent is in physical contact with an object at the current time step and the previous action that was taken. Although not needed for the agent to solve the tasks, we have found that providing contact information and previous action as observations led to convergence nearly twice as fast; in an evolutionary search context, this speed-up significantly improves the overall optimization time.\nReward function. The reward function is used in RL to drive policy optimization towards some desired observation and action mapping. In our case, each task has a unique reward function:\n $R_{NAVIGATION} = \\lambda (||x_t - x_o || - ||x_{t-1} - x_o||) + w_g + w_c$ (1)\n$R_{DETECTION} = -\\lambda (||x_t - x_f || - ||x_{t-1} - x_f ||) + w_g + w_a + w_c$ (2)\n$R_{TRACKING} = -\\lambda (||x_t \u2013 x_f|| - ||x_{t\u22121} - x_f ||) + w_g + w_a + w_c$ (3)\nwhere $R_x$ is the reward at time t for each task, $\\lambda$ is a scaling factor, $x_t$ and $x_{t-1}$ is the position of the agent at time t and t - 1 respectively, $x_o$ is the initial position of the agent, and $x_f$ is the position of the goal (i.e., end of maze for NAVIGATION, goal object in DETECTION). The w variables are non-zero when certain conditions are met. $w_g$ and $w_a$ indicates the reward/penalty given for reaching the goal and adversary, respectively. $w_c$ is the penalty for contacting a wall. In essence, in the NAVIGATION task, the agent is incentivized to move from it's initial position as fast as possible. In the DETECTION and TRACKING tasks, the agent is incentivized to navigate to the goal as quickly as it can. During training, $\\lambda$ = 0.25, $w_g$ = 1, $w_a$ = -1, and $w_c$ = \u22121. Additionally, when an agent reaches the goal or adversary, the episode terminates."}, {"content": "Fitness function. As compared to the reward function, the fitness function is used to evaluate the current performance of an agent. Where the reward function is used to inform the RL algorithm for it's weight optimization, the fitness function informs the evolutionary search algorithm for further selection and mutation. For each task, the fitness function $F_x$ in generation g is identical to $R_x$, except with different weights to emphasize the relative performance difference between agents. During fitness evaluation, $\\lambda$ = 1.5, $w_g$ = 10, $w_a$ = -10, and $w_c$ = -2. Instead of terminating when the goal or adversary is reached, in evaluation, the object is respawned and the agent continues to solve the task.\nEvolution loop. Evolving the full agent visual genotype, with a total of >~1020 possible combinations, necessitates \"intelligent\" optimization. The vast size of the search space alone means all combinations cannot be tested in a timely fashion. Strategically selecting morphologies that simultaneously explore new configurations and exploit previously gained knowledge is imperative to not waste resources on suboptimal solutions. We accomplish this intelligent search mechanism through the integration of evolutionary strategies (ES) [48]. ES is a broad optimization technique that is inspired by natural evolution and operates by iteratively refining a population of candidate solutions through processes such as mutation, selection, and adaptation. Unlike traditional genetic algorithms, ES emphasizes mutation over crossover and is particularly well-suited for optimizing continuous, high-dimensional spaces.\nWe use a population size of 16 agents, and evolve for 50 to 100 generations depending on the experiment. The specific ES algorithm we use is the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [73]. CMA-ES is a variant of ES that adapts the mutation distribution based on the covariance matrix of the population. This adaptation allows for faster convergence and better exploration of the search space. We use the open-source implementation of CMA-ES provided by nevergrad [51]. Hyperparameters for CMA-ES can be found in the Supplementary Material.\nAgent phenotype. An agent's phenotype is the physical manifestation of its genotype. The phenotype is the realized form that interacts within the environment that acquires and acts on observed stimuli. An agent in this work is represented as a fixed radius sphere with eyes facing outward and distributed uniformly along its equator. The agent is embodied therefore can control its direction and speed using its underlying policy, which are used to actuate the joints to move the agent in the simulation environment. Our framework also allows for more complex dynamical systems and controller to be used as well. Lastly, in the case of the TRACKING task, we assign computed action profiles to the goal and adversary to move to random locations within the environment.\nAgent's genotype. The genotype encodes the instructions to create the agent's eye and cognitive system that learns task behavior. The vision genotype are further divided into three clusters that are mutated independently, and incorporate both continuous and discrete parameters. The clusters are: morphological, optical, and neural Figure 2. Rather than modeling complex biological mechanisms like photoreceptor dynamics, we implement these genes to encode capture of light from a physics-based rendering perspective. We believe that this model captures the essential functional properties needed to evolve vision while remaining computationally tractable in modern embodied simulators. Our full encoding scheme is capable of representing approximately 1020 unique agent vision types.\nMorphological genes. The morphological genes defines properties used to spatially sample the environment, such as the number of eyes, their placement (determined by placement range), and field of view (FOV). We model the agent as a sphere of fixed radius of 0.2 units with eyes distributed uniformly along its equator. Thus, the placement of each eye is also dependent on both the number of eyes, and a placement range (i.e., the maximum angle from latitude 0\u00b0) that"}, {"title": "Appendix A Analysis of Evolved Agents", "content": "Our analysis quantifies the optical performance of evolved vision systems using three metrics. The Point Spread Function (PSF) represents the system's response to a point source of light how a perfect point gets \u201cspread out\" by the optical system. In Figure A1, high-performing agents develop compact, symmetric PSFs indicating precise light focusing, while poor performers show diffuse, irregular patterns suggesting inefficient light management. A perfect PSF would appear as an infinitesimally small point, while real optical systems produce some degree of spread due to diffraction and optical imperfections.\nMTF for Spatial Precision Analysis The Modulation Transfer Function (MTF), mathematically derived as the Fourier transform of the PSF, quantifies how well different spatial frequencies are preserved by the optical system. On the MTF plots in Figure A1, the y-axis represents contrast preservation (from 0 to 1) while the x-axis shows spatial frequency in cycles/mm. The area above the noise floor (10-2) represents useful spatial information frequencies where the signal can be reliably distinguished from noise. Early-generation agents show erratic MTF curves with sharp dips below this noise floor, while later-generation agents"}, {"title": "Appendix B Genotype and its relation to the Plenoptic Design Space", "content": "The vision genotype of the agent can be understood as operating on the plenoptic function, which describes the complete flow of light in a scene [80]. We can conceptually think about biological vision evolution as directly evolving to capture differnet dimensions of the plenoptic function. Our current implementation demonstrates that computational evolution can also sample from the Plenoptic function [80]: we allowing evolution to explore a subset of plenoptic dimensions such as placement, orientaotin, optical constraints, movement of the agent etc. We believe this framework can naturally extend to encompass the full plenoptic representation of light - including spectral sensitivity, polarization detection, and varied spatiotemporal resolutions. Just as our computational experiments have shown evolution discovering diverse and creative solutions within a limited set of visual parameters, expanding the genotype to sample from the complete plenoptic dimensions would enable the discovery of even more sophisticated visual systems, analogous to those found in nature. For instance, the mantis shrimp (stomatopods) evolved 16 different photoreceptor types that can detect both linear and circular polarized light [81], while jumping spiders (Salticidae: Dendryphantinae) developed a unique combination that provide both high acuity and wide-field motion detection [82]. Our genotype enables co-evolution of eyes, neural circuitry and subsequent behavior (learned through reinforcement learning) and provides a unified way to think about vision evolution as a creative optimization process operating directly on the fundamental properties of light. As we expand the genptype of the available plenoptic dimensions, we expect to see the emergence of increasingly sophisticated and novel visual systems that may parallel, or even exceed, the remarkable diversity found in biological evolution."}, {"title": "Appendix C Acuity-Neural Processing Trade-offs and Task-Specific Scaling", "content": "In our framework, we systematically explore how visual task performance emerges from the interplay of three key components. The first component is the eye's physical characteristics, measured in cycles per degree (CPD), which determines the ability to resolve spatial detail. The second is neural capacity, where we vary the number of parameters in the vision-processing layers.\nOur parameter sweep reveals emergent power law scaling relationships between sensory acuity and neural capacity Figure A5. The relative fitness plots (top row) demonstrate that navigation achieves high performance (>0.8) at lower CPDs ( 0.05) with modest neural capacity (8000 parameters). Detection and tracking tasks show a distinct scaling pattern, requiring both higher CPDs (>0.3) and larger networks (>40,000 parameters) for comparable fitness levels.\nThe error plots (bottom row) reveal fundamental constraints in how these capabilities emerge. At fixed CPD values, increasing neural capacity follows characteristic power law improvements until hitting task-specific performance ceilings. These ceilings are particularly evident in the scattered error distributions, where higher CPDs enable lower minimum error rates across all tasks. This demonstrates that poor visual acuity creates a fundamental bottleneck that cannot be overcome by simply scaling neural capacity. Notably, detection and tracking display continuous improvements in error rates as both CPD and network size increase, suggesting these tasks benefit from simultaneous scaling of both sensory and neural resources."}]}