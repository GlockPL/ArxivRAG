{"title": "Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models", "authors": ["Yuyan Chen", "Chenwei Wu", "Songzhou Yan", "Panjun Liu", "Haoyu Zhou", "Yanghua Xiao"], "abstract": "Teachers are important to imparting knowledge and guiding learners, and the role of large language models (LLMs) as potential educators is emerging as an important area of study. Recognizing LLMs' capability to generate educational content can lead to advances in automated and personalized learning. While LLMs have been tested for their comprehension and problem-solving skills, their capability in teaching remains largely unexplored. In teaching, questioning is a key skill that guides students to analyze, evaluate, and synthesize core concepts and principles. Therefore, our research introduces a benchmark to evaluate the questioning capability in education as a teacher of LLMs through evaluating their generated educational questions, utilizing Anderson and Krathwohl's taxonomy across general, monodisciplinary, and interdisciplinary domains. We shift the focus from LLMs as learners to LLMs as educators, assessing their teaching capability through guiding them to generate questions. We apply four metrics, including relevance, coverage, representativeness, and consistency, to evaluate the educational quality of LLMs' outputs. Our results indicate that GPT-4 demonstrates significant potential in teaching general, humanities, and science courses; Claude2 appears more apt as an interdisciplinary teacher. Furthermore, the automatic scores align with human perspectives.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated great performance in various natural language processing (NLP) tasks, including question answering (Saad-Falcon et al., 2023; Chen et al., 2024f,e,d), information retrieval (Chen et al., 2022; Liu et al., 2023b), reasoning (Kojima et al., 2022; Chen et al., 2023c), and generation (Chung et al., 2023; Chen et al., 2023e, 2024b), etc. Beyond these general NLP applications, LLMs are also widely used in other domains, such as education. In the educational field, LLMs can now be used as substitutes for teachers. They can help automated teaching or assisted learning applications, thereby alleviating the pressure on human teachers. Additionally, LLMs can recommend appropriate elective courses based on a student's knowledge state, learning style, and interests, automatically generating practice problems of corresponding difficulty levels, and identifying areas where a student is struggling to provide targeted improvement.\nHowever, the capability of questioning is a crucial aspect in the educational field. As LLMs take on the role of teachers, can they pose high-quality questions like human educators? Therefore, evaluating what constitutes a high-quality question in education becomes necessary. According to Anderson and Krathwohl's educational taxonomy (Anderson and Krathwohl, 2001; Elkins et al., 2023), we consider that high-quality questioning in the educational field must meet the following characteristics: i) achieve a higher level across the six domains including memory, understanding, application, analysis, evaluation, and creation; ii) be relevant to the given context; iii) comprehensively cover the content of the context, and iv) also reflect the important knowledge of this context. We consider that questions meeting these characteristics can effectively assess students' knowledge levels, and LLMs capable of posing such questions can assume the role of competent human educators. The first characteristic is the most basic requirement for LLMs to act as human teachers, while the following three characteristics measure the excellence of LLMs in their role as a teacher.\nEvaluating and enhancing the capability of LLMs to generate questions of high quality standards in the educational domain requires a benchmark. However, previous studies have mainly viewed LLMs from a student's perspective, focusing on tasks like reading comprehension (Bai et al., 2023; Tran and Kretchmar, 2023; Chen et al., 2023a; Cheng et al., 2023; Izacard and Grave, 2020; Kawabata and Sugawara, 2023; Zhou et al., 2023b,a) and exam evaluations (Zhang et al., 2023b; Huang et al., 2023; Zhong et al., 2023; Li et al., 2023; Wang et al., 2023; Zeng, 2023; Wei et al., 2023). However, these tasks focus on adopting contexts to passively answer questions or make reasoning, and these tests treat LLMs as students, assessing their abilities by how they answer questions, while the LLM's questioning capability through generating educational questions is understudied. Current education-related research is far from adequate to determine LLMs' question raising capability as a teacher, and there isn't a benchmark that studies the overall teaching abilities of LLMs, seeing them as teachers. Although some role-playing tasks (Shao et al., 2023) mimic professional dialogues but don't truly assess the LLMs' teaching capabilities. Therefore, if we want LLMs to assist in teaching effectively, we need to evaluate and enhance their teaching abilities, as possessing knowledge and guiding others to learn are distinct skills.\nTherefore, in this paper, we have developed a benchmark for assessing whether LLMs generate high-quality questions in the field of education, guided by professional educational theories. Unlike general questioning, as shown in Fig. 1 (a), our benchmark requires that the generated questions not only be fluent and readable but also meet the fundamental characteristics proposed earlier (i.e. the first characteristic), as shown in Fig. 1(b).\nSpecifically, we draw on Anderson and Krathwohl's educational taxonomy (Anderson and Krathwohl, 2001) to prompt LLMs to generate questions at six levels for each context. We select tasks from three domains, including general, single-discipline, and interdisciplinary domains, to more comprehensively assess the strengths of LLMs as teachers in various fields. Based on the four characteristics proposed earlier, we have also designed four evaluation metrics: consistency, relevance, coverage, and representativeness, to assess the value of questions posed by LLMs in the educational domain, thereby comprehensively evaluating the questioning capability of LLMs as teachers in education through evaluating their generated educational questions. Our experiments reveal that LLMs like GPT-4, Claude2, and GPT-3.5 demonstrate good questioning capability across domains as teachers in education through evaluating their generated educational questions. In summary, our contributions are threefolds:\n\u2022 We introduce the problem of evaluating questioning capability in education as a teacher for LLMs through evaluating their generated educational questions, building a framework based on educational theory that includes six cognitive levels and tasks from three different domains.\n\u2022 We establish four evaluation metrics to assess the questioning capability in education as a teacher of LLMs through evaluating their generated educational questions.\n\u2022 We conduct experimental evaluations of 11 LLMs, providing quantitative standards and subject orientations for each LLM's questioning capability as a teacher."}, {"title": "2 Datasets and Task Setups", "content": "We propose a benchmark named Dr.Academy, which has tasks from three domains. The first two request LLMs to generate questions in the general and monodisciplinary domain, respectively, based on the six levels of Anderson and Krathwohl's educational taxonomy (Anderson and Krathwohl, 2001), including memory, understanding, application, analysis, evaluation and creation. The third one requests LLMs to generate questions that intersect multiple subjects. The overview of Dr.Academy is shown in Fig. 2."}, {"title": "2.1 Context Construction", "content": "Initially, we collect 10,000 contexts from the general domain and produce an additional 10,000 contexts specifically for the monodisciplinary domain. In the general domain, the contexts are sourced from the SQUAD dataset (Rajpurkar et al., 2016), an extractive reading comprehension dataset derived from Wikipedia articles, and are utilized as the foundation for the LLMs to generate questions. In the monodisciplinary domain, we generate corresponding contexts for each of the multiple-choice questions in the MMLU dataset (Hendrycks et al., 2020), which covers a broad spectrum of subjects, with GPT4 1. These contexts include essential information related to the question and all candidate choices. The prompt for generating contexts is shown in Table 1. We also conduct manual evaluations on the generated contexts for the MMLU questions. In this process, we engage three graduate students from different disciplines to perform the evaluations. For each discipline, we randomly select 1% of the questions to undergo manual assessment. If these entries do not achieve a manual evaluation score of 4 (on a scale of 1-5), we will regenerate the contexts."}, {"title": "2.2 Task Setup", "content": "We have designed three tasks and each task requires LLMs to generate questions catering for the corresponding domain. Finally, these generated questions will be used to evaluate the questioning capability in education as a teacher of LLMs. The prompt for generating questions is shown in Table 8 (row \"Generation\").\nGeneral domain tasks request an LLM to generate questions with the collected contexts from SQUAD based on the six levels of Anderson and Krathwohl's educational taxonomy (Anderson and Krathwohl, 2001), including memory, understanding, application, analysis, evaluation and creation. For instance, in Fig. 2 (a), at the memory level, a question might ask specific details like \u201cWhat are the religious features of a school building?", "Why is a school considered to have Catholic characteristics?": "at the creating level, questions could be more open-ended, involving imagination and design, etc. This task is designed to evaluate \"which LLM is more suitable to be a general course teacher\".\nMonodisciplinary domain tasks request an LLM to generate questions with the generated contexts from MMLU, focusing on either humanities (like history, geography) or sciences (like physics, chemistry), based on the same six educational levels. In science, for instance, a memory-level question might ask about element symbols and formulas, such as \"What is the chemical formula for ammonium sulfate?\u201d; an application-level question related to real-world phenomena, like \u201cchoose a substance that reacts with hydrochloric acid to produce carbon dioxide\u201d. This task is designed to evaluate \"which of the two LLMs is more suitable to act as a humanities teacher and a science teacher.\"\nInterdisciplinary domain tasks request an LLM to generate questions that cross multiple subject areas, reflecting each subject's characteristics."}, {"title": "2.3 Evaluation metrics", "content": "We adopt consistency, relevance, coverage, and representativeness to evaluate LLMs' generated questions in the general and monodisciplinary domains, respectively, while using relevance and representativeness to evaluate questions in the interdisciplinary domain. The difference of metrics selection is because questions in interdisciplinary domain lack a comprehensive contextual framework. For instance, in reality, there is no distinct academic discipline like \"historical geography.\u201d This absence of a well-defined, unified context means that metrics such as coverage and consistency do not apply.\nTo validate the effectiveness of these metrics, we consult ten experts in education to rate the effectiveness of these metrics within the field of education on a scale of 1 to 5. They consistently award these metrics scores of 4 and above, which leads us to believe that these metrics are meaningful for evaluating questions in education. We also align these metrics with manual evaluations in Figure 6, indicating that our metrics are indeed significant within the field of education. Additionally, experiments are conducted to compare these metrics with human scoring in order to corroborate the validity and reasonableness of them (see Fig. 6). The prompt for evaluating questions is shown in Table 8 (see row \u201cEvaluation\u201d). Specifically, consistency is to assess whether the question accurately corresponds to a pre-defined level of the educational taxonomy, relevance is to assess whether the question is related to the provided text content or theme, coverage is assessed by determining if all generated questions based on a context encompasses a major portion (over 50%) of this given context, representativeness evaluates whether a question captures the main content or core ideas of the text. Metrics are rated on a binary scale, with 1 for criteria met and 0 for not met, as shown in Fig. 3 and Table 2. We adopt GPT-4 to score each question three times. A question that scores 1 in two out of three instances meets the metric's requirement (Chen et al., 2023b,d)."}, {"title": "3 Experiments", "content": "In this section, we conduct extensive experiments to evaluate different LLMs' questioning capability through evaluating their generated educational questions in the proposed Dr.Academy."}, {"title": "3.1 Experimental Setups", "content": "Our experiments are conducted on 8 Nvidia A100 GPUs, each with 80GB of memory, and we use PyTorch 2 in Python 3. We set the maximum sequence length for both input and output sequences to maximum 1000 tokens."}, {"title": "3.3 Main results", "content": "Question 1: Which LLM is more suitable to be a general course teacher? Answer 1: GPT4! The term \"suitable\" is used to assess the effectiveness of each LLM in different academic subjects. This evaluation helps in identifying which LLMs perform best in specific educational disciplines. In the general domain tasks, various LLMs demonstrate diverse performances as shown in Table 3 and Table 6. Specifically, GPT4 achieves a perfect score of 100% in both consistency and relevance, indicating its strong capability in understanding task requirements and generating relevant questions. However, its coverage score of 54.5% suggests there is room for improvement in generating questions that encompass more content. In representativeness, with a score of 80.1%, GPT-4 shows a good grasp of the context's core content and viewpoints, crafting questions with depth and breadth. BLOOM-176B and Claude2 also score perfectly in relevance, reflecting their excellent performance in linking questions to the context's themes and content. However, their lower scores in coverage and representativeness indicate potential for improvement in capturing the full extent and core insights of the texts. Moreover, \u201cAver\u201d in Table 3, Table 4 and Table 5 represent the average result of the corresponding dimensions under each domain, which are obtained using in-context learning (i.e. ICL). ICL is to introduce a human-written sample into the prompt which typically improves LLMs' performance across all metrics, while most LLMs show a decline in the 0-shot setting, demonstrating the critical role of ICL in enhancing the quality of question generation.\nQuestion 2: Which of the two LLMs is more suitable to act as a humanities teacher and a science teacher? Answer 2: Both are GPT4!\nIn the monodisciplinary domain tasks, LLMs are compared based on their performance in humanities and sciences as illustrated in Table 4 and Table 6. The results reveal that the majority of the LLMs perform marginally better in the scientific disciplines compared to the general domain. Specifically, GPT4 excels across all metrics, particularly in the science disciplines, where it scores higher than in the humanities, indicating great capability in handling science content. Following closely is Claude2, which nearly matches or equals GPT4 in Relevance and representativeness in the humanities, demonstrating a deep understanding and effective processing of humanities content. Claude2 also maintains a high performance in the science disciplines. GPT3.5 shows competitive strength across the four metrics, especially in relevance and representativeness within the science subjects, approaching the leading performance of GPT4. BLOOM-176B scores significantly higher in consistency within science compared to the humanities, and also demonstrates good capability in coverage and representativeness, suggesting its strengths in processing logical and scientific data.\nQuestion 3: Which LLM is more suitable to be a interdisciplinary teacher? Answer 3: Claude2!\nResults of LLMs in the interdisciplinary domain tasks are shown in Table 5 and Table 6. It shows that Claude2 outperforms other LLMs with scores of 89.1% in relevance and 93.3% in representativeness. Following closely is GPT4, with scores of 87.8% in relevance and 91.2% in representativeness, also indicating strong performance. GPT3.5 and LLaMA2-70B also show high scores, particularly in representativeness, suggesting their capability in understanding key textual content and generating in-depth questions. On the other hand, BLOOM-7B, Falcon-7B, and Vicuna-7B perform relatively poorly on both metrics. Specifically, BLOOM-7B scores below 40% in both relevance and representativeness, which may suggest a need for further enhancement in understanding interdisciplinary content and generating high-quality questions.\nQuestion 4: Which LLM is more suitable to be a all-around teacher? Answer 4: GPT4!\nWe also comprehensively compare the performance of LLMs in three tasks as shown in Table 6 and Fig. 4. Specifically, in the general domain tasks, GPT4 scores the highest and ranks first. In the monodisciplinary domain tasks, including humanities and science, GPT4 also has the best performance. In the interdisciplinary domain tasks, Claude2 occupies the first rank. Finally, for the comprehensive rating, GPT4 ranks first again with the highest score. Overall, GPT4 shows the best performance in most tasks, while Claude2 also demonstrates strong capability in certain tasks. Overall, GPT4, Claude2, and GPT3.5 perform well in these assessments, demonstrating their versatility and adaptability as high-performance models. On the other hand, BLOOM-7B and Falcon-7B tend to perform weaker in most fields, which may make them more suitable for specific application scenarios.\nQuestion 5: What's the relationship among metrics for various LLMs? Answer 5: Pairwise positive correlations!\nWe also analyze the relationship among four metrics in three tasks as shown in Fig. 5. Fig. 5 (a) and Fig. 5 (b) represent the relationship between four metrics of question quality generated by different LLMs in the general and monodisciplinary domains, respectively. The size of the circle indicates coverage, with larger circles covering more content of the text. The darker the color of the circle, the higher the relevance of the questions to the text. The \"Average1\" in the first graph represents a group of LLMs, which are zoomed in on the second graph, and the \"Average2\" in the second graph represents a subset of these LLMs, which are further examined in the third graph. In Fig. 5 (a), analyzing the general domain tasks, we see a positive correlation between relevance and consistency across all three graphs. Representativeness also shows a positive correlation with relevance and consistency, but the correlation is weaker. As relevance and consistency increase, the LLMs have darker colors and larger circles, indicating higher relevance and coverage. Although larger LLMs like BLOOM-176B show good coverage, not all models with large coverage have high relevance. For example, Falcon-180B does not perform as well as BLOOM-176B in relevance, suggesting a need for balance between the breadth of text coverage and the accuracy of question generation. In the third graph, LLMs like GPT4 maintain high relevance while also achieving good coverage. In Fig. 5 (b), for the monodisciplinary domain tasks, the correlations between the four metrics are not as pronounced as in the general domain. The third graph shows little color variation, indicating that representativeness does not change much with increased relevance and consistency. However, there are LLMs like GPT4 that stand out in all metrics, shown in the top right corner with a dark color and large size. But GPT3.5, while showing good representativeness and relevance, has only average consistency. In Fig. 5 (c), analyzing the interdisciplinary domain tasks, generally shows a positive correlation between relevance and representativeness, although it's not as clear in the second graph. Overall, LLMs like GPT4, Claude2, BLOOM-176B, and GPT3.5 perform well across all four metrics, while the 7B series models tend to perform less well. The metrics also tend to show positive correlations with each other.\nQuestion 6: Is the automatic scores generated by GPT4 agree with human perspectives? Answer 6: Yes, the Pearson correlation coefficient reaches 0.947 and Spearman rank correlation reaches 0.87!\nWe adopt Pearson correlation coefficient that normalized to a 1-100 scale to investigate the difference between automatic and human scores for different LLMs as shown in Fig. 6. The human scores for each metric and the corresponding agreement of human annotators on these metrics are listed in Table 7. We find that automatic and human evaluations for LLMs generally agree, show- ing a high positive correlation with the Pearson correlation coefficient reaching 0.947 and Spearman rank correlation reaching 0.870. Specifically, GPT4 performs excellently in both automatic and human scoring, with minimal difference, indicating widespread recognition of its capability. Similarly, Claude2 has close scores in both evaluations, indicating balanced performance in assessment tasks. It's important to clarify that the process of generating questions and scoring them is separate. During the scoring phase, there is no knowledge of which LLM generates which question. We believe that even if other LLMs are used to evaluate GPT-4's performance against a comparatively weaker 7b model, the results would still favor GPT-4, a conclusion also supported by human evaluations. The findings suggests that automatic scoring has the potential to partially replace human scoring for evaluating questioning capability in education as a teacher of LLMs through evaluating their generated educational questions."}, {"title": "3.4 Case study", "content": "We present a set of general domain questions generated by GPT4, identified as the leading LLM in this area, in Fig.7. The top-performing LLMs in monodisciplinary (Humanities), monodisciplinary (Science), and interdisciplinary domains have their questions displayed in Fig.9, Fig.10, and Fig.8, respectively. Additionally, further examples from baseline LLMs are shown in figures ranging from Fig.11 to Fig.26. In Fig.7, the first question tests memory by asking about the true ruler of the Twilight Realm. The second requires understanding the reasons behind the Mirror of Twilight's durability. The third applies Link's transformation ability to his quest. The fourth analyzes the contrasting rules of Midna and Zant. The fifth evaluates the use of the Mirror as a punishment tool. Finally, the sixth encourages creating an alternative ending for the saga. These questions showcase GPT-4's range from simple recall to creative thinking, and the generated questions can be adopted to make students quickly grasp the key content, indicating its strong questioning capability in education as a teacher through evaluating their generated educational questions. We also show a good case in the interdisciplinary tasks generated by Claude2, which shows outstanding comprehensive capability that can effectively combining different subject knowledge to pose accurate and challenging questions. In chemistry and physics combination questions, it understands the basic principles of chemical reactions and the physical properties of bubble motion in liquids, demonstrating analysis and synthesis ability in interdisciplinary questions. Claude2 shows high adaptability and understanding in complex tasks involving multiple disciplines."}, {"title": "4 Related Work", "content": ""}, {"title": "4.1 Question Generation", "content": "The potential of LLMs in generating questions for educational purposes garners significant academic attention. Chen et al. (2019) have developed a reinforcement learning approach specifically for generating natural questions. Elkins et al. (2023) evaluate the educational utility of questions produced by LLMs, organizing them according to their levels of difficulty. Tavares et al. (2023) investigate the methods LLMs use to create questions, focusing on the tracking of dialogue states. Kai et al. (2021) introduce a method involving double hints for the generation of questions about visuals. Uehara et al. (2022) emphasize the importance of sub-questions in improving the effectiveness of primary visual questions. Arora et al. (2022) examine various prompting techniques for LLMs and analyze the differences in model responses. Abdelghani et al. (2022) utilize the capabilities of GPT-3 to foster curiosity-driven questioning among children. Collectively, these studies underline the evolving role of LLMs in reshaping question generation instead of searching valuable questioning points."}, {"title": "4.2 Test-based Benchmark", "content": "There has been an increasing focus on evaluating the capability of LLMs in the context of standardized exams and academic benchmarks. For example, Zhang et al. (2023b) introduce GAOKAO Benchmark to evaluate the intuitive benchmark of Chinese college entrance examination questions; Huang et al. (2023) propose the first comprehensive Chinese evaluation package C-EVAL; Zhong et al. (2023) present a human-centric benchmark AGIEval designed for evaluating foundation models; Li et al. (2023) introduce CMMLU, a comprehensive Chinese benchmark covering multiple disciplines; Wang et al. (2023) introduce SciBench to systematically investigate the reasoning ability required to solve complex scientific problems;; Liu et al. (2023a) propose M3KE, a large-scale multilayer and multi-disciplinary knowledge assessment benchmark; Zeng (2023) propose a method to evaluate the multi-task accuracy of large Chinese language models across various domains; Zhang et al. (2023a) introduce FinEval, a benchmark designed for financial knowledge evaluation in LLMs; Wei et al. (2023) focus on the Chinese Elementary School Math Word Problems dataset to evaluate reasoning capability; Dao et al. (2023) explore ChatGPT's potential to complete the Vietnam National High School Graduation Exam; Raina and Gales (2022) propose performance criteria to assess the generated multiple-choice questions; Chen et al. (2018) present LearningQ, an educational question generation dataset containing over 230K document- question pairs. Chen et al. (2024a) introduce a novel game named BrainKing for evaluating LLM capabilities under incomplete information scenarios. Chen et al. (2024c) presents a novel framework named EmotionQueen for evaluating the emotional intelligence of LLMs. However, these tests treat LLMs as students, assessing their abilities by how they answer questions instead of seeing them as teachers."}, {"title": "5 Conclusions and Future Work", "content": "In conclusion, our study presents a pioneering investigation into the questioning capability in education as a teacher of large language models (LLMs) through evaluating their generated educational questions, shifting the traditional role of LLMs from learners to educators. We have developed a comprehensive benchmark, named Dr.Academy, based on educational taxonomies that assesses LLMs' abilities to generate questions across various domains with four evaluation metrics. Our findings indicate that models like GPT4, Claude2, and GPT3.5 demonstrate promising teaching potential. Looking ahead, the future directions of this research include refining the evaluation metrics for even more nuanced assessments of teaching effectiveness and expanding the range of subjects and domains covered."}, {"title": "Limitations", "content": "One limitation of our study is that it primarily focuses on the ability of large language models (LLMs) to generate questions, which is just one aspect of teaching. Actual teaching involves more complex interactions, including providing feedback, adapting to students' needs, and fostering critical thinking, areas not fully captured by our current benchmark. Additionally, our approach relies heavily on the textual content, which may not comprehensively represent the nuances of human teaching methods that include non-verbal cues and personalized interactions. Therefore, while our findings offer valuable insights into the potential of LLMs as teaching aids, they should be viewed as a starting point for more in-depth research into the multifaceted nature of teaching and learning processes."}, {"title": "Acknowledgements", "content": "This work is supported by Science and Technology Commission of Shanghai Municipality Grant (No. 22511105902), Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103), the National Natural Science Foundation of China (No.62072323), Shanghai Science and Technology Innovation Action Plan (No. 22511104700), and the Zhejiang Lab Open Research Project (NO. K2022NB0AB04)."}]}