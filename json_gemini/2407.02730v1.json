{"title": "MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context", "authors": ["Zishan Gu", "Changchang Yin", "Fenglin Liu", "Ping Zhang"], "abstract": "Large Vision Language Models (LVLMs) have recently achieved superior performance in various tasks on natural image and text data, which inspires a large amount of studies for LVLMS fine-tuning and training. Despite their advancements, there has been scant research on the robustness of these models against hallucination when fine-tuned on smaller datasets. In this study, we introduce a new benchmark dataset, the Medical Visual Hallucination Test (MedVH), to evaluate the hallucination of domain-specific LVLMs. MedVH comprises five tasks to evaluate hallucinations in LVLMs within the medical context, which includes tasks for comprehensive understanding of textual and visual input, as well as long textual response generation. Our extensive experiments with both general and medical LVLMs reveal that, although medical LVLMs demonstrate promising performance on standard medical tasks, they are particularly susceptible to hallucinations, often more so than the general models, raising significant concerns about the reliability of these domain-specific models. For medical LVLMs to be truly valuable in real-world applications, they must not only accurately integrate medical knowledge but also maintain robust reasoning abilities to prevent hallucination. Our work paves the way for future evaluations of these studies.", "sections": [{"title": "Introduction", "content": "Recent advancements in large language models (LLMs) have stimulated the development of domain-specific LLM applications in various sectors, including healthcare. Building on this, researchers have further introduced large vision language models (LVLMs) that combine the robust capabilities of LLMs with the processing of visual inputs. However, despite the promising performance, both LLMs and LVLMs encounter this critical issue known as \u201challucination\", where they produce seemingly correct yet unverified responses with great confidence. Numerous studies have been trying to identify, evaluate, and mitigate the occurrence of hallucinations of large-scale models. However, despite the recent emergence of medically specialized LVLMs, research specifically targeting hallucinations in the medical context remains limited. On the one hand, the fine-tuning of LVLMs for domain-specific tasks, such as interpreting chest X-ray images, has demonstrated significant performance improvements . These advances suggest the potential for a more accessible image analysis system that could not only empower patients with vital information about their health conditions but also provide physicians with a reliable second opinion to support more informed clinical decisions. On the other hand, the susceptibility of these systems to hallucinations poses a serious risk, potentially leading to adverse effects on healthcare decisions, diagnoses, and treatment plans. Developing a test to assess this would necessitate extensive domain expertise and the creation of specifically curated input data, such as images with hard negative diagnostic results. This underscores the urgent need for focused research to evaluate and enhance the robustness and proficiency of medical LVLMs.\nThis paper aims to bridge this gap by introducing a novel benchmark dataset, Medical Visual Hallucination Test (MedVH), to evaluate LVLMs' capabilities in dealing with hallucinations in the medical context from two facets. We demonstrate"}, {"title": "Related Work", "content": "With the advent of LLMs, researchers have advanced to developing multimodal large-scale models, or LVLMs. Several efforts have also been made to adapt such LVLMs for use in the medical field, such as LLaVA-med and CheXagent. However, numerous efforts have highlighted the risk of hallucinations in large models, casting doubt on their reliability in critical fields such as healthcare. M\u00fcndler et al. (2024) have identified and suggested methods to address self-contradiction in LLMs. Umapathi et al. (2023) introduced Med-Halt to assess reasoning and memory-based hallucinations with medical entrance exams, finding that no model achieved satisfactory accuracy across most tasks. Yifan Li and Wen (2023) developed POPE to evaluate visual hallucinations in object detection in general images, noting LVLMs often identify objects that frequently appear or co-occur in their training datasets. Despite these efforts, research into hallucinations in medical vision-language tasks is still limited."}, {"title": "Hallucination Evaluation", "content": "In this section, we introduce our evaluation framework for assessing hallucinations in LVLMs within the medical domain. The overview of this framework is illustrated in Figure 1. We have developed a new benchmark dataset, MedVH, designed to evaluate the models across two distinct facets through five tasks that probe key functionalities. The following sections will offer a detailed explanation of the framework, the tasks associated with each facet of evaluation, and the metrics used for assessment."}, {"title": "Overall Evaluation Framework", "content": "As demonstrated in Figure 1, we evaluate seven state-of-the-art LVLMs from two facets, each corresponding to a different type of hallucination in the medical context. The first facet examines the models' robustness against hallucinations in comprehensive understanding of medical visual information and textual input through MC-VQA tasks, such as disease identification and severity assessment. The second facet focuses on hallucinations occurring in long text generation, particularly with false confidence justification and medical report generation. We detail each task within the MedVH dataset and provide examples of prompts used in these tasks. The models' robustness against hallucinations will be evaluated considering their ability to leverage the medical knowledge base and their model size."}, {"title": "Medical Visual and Text Understanding", "content": "We begin by assessing the presence of hallucinations in LVLMs with visual and textual comprehension. Specifically, we evaluate the models' capability to discern irrelevant or incorrect inputs and detect misleading instructions. To achieve this, we introduce three MC-VQA tasks, which involves multi-modal input comprising both an image and a textual question. The models are tested in the following settings.\nWrongful Image This task is designed to evaluate the model's capability to recognize inconsistencies between the image content and the associated question, in which we replace the corresponding images with unrelated ones. We either randomly select a wrongful medical image from a different genre or choose an adversarial X-ray image of a different organ. For instance, in the task of disease identification using chest X-ray images, a randomly chosen image could be a retinal image or a picture of cells, while an adversarial image would be an X-ray image of another organ that does not exhibit the targeted disease.\nNone Of The Above In this task, models are presented with a multi-choice question where the correct answer is explicitly listed as 'None of the above'. This setup requires the model to recognize"}, {"title": "Medical Text Generation", "content": "We also evaluate the appearance of hallucination in the long textual response of the LVLMs under the following two settings.\nFalse Confidence Justification This task presents a question and a randomly suggested wrong answer to the language model, and then asks the model to provide detailed explanations for its correctness or incorrectness. The model is supposed to suggest an alternative answer if it decides the suggested answer is incorrect. This test specifically examines the language model's propensity to express answers with unwarranted certainty in the input text.\nGeneral Report Generation In this task, we prompt the LVLMs to generate medical reports based on CXR images. The objective is for the models to accurately identify diseases visible in the image. Any mention of diseases not present in the image will be considered a hallucination. This setup evaluates the models' precision in recognizing and reporting medical conditions from visual inputs while generating long textual responses."}, {"title": "Data Synthesis and Statistics", "content": "For each of the MC-VQA tasks and the False Confidence Justification task with multi-choice questions, we establish our benchmark by randomly sampling 500 questions from four publicly available medical VQA datasets: RAD-VQA, SLAKE, PMC-VQA, and MIMIC-Diff-VQA. As for the unrelated medical images and adversarial X-ray images in the Wrongful Imgae task, we randomly select the images Path-VQA and Med-VQA-2021 respectively. Among these datasets, RAD-VQA, SLAKE, and PMC-VQA mainly focus on medical"}, {"title": "Evaluation", "content": "Multi-choice VQA. For each multi-choice question, there is a designated correct answer. We quantify the model's success rate in selecting this answer using the metric $acc_h$. A higher $acc_h$ score indicates greater resistance of the model to hallucinations. Additionally, we also assess the model's performance on regular MC-VQA tasks as baseline experiments, which involve standard CXR images, correct answers among the options, and questions based on accurate clinical assumptions, serving to evaluate the model's medical knowledge. We represent the models' accuracy on this baseline task with $acc_b$. Ideally, an LVLM should demonstrate both a broad medical knowledge base and the ability to generate responses free from hallucinations.\nCharacterization score. In this study, we introduce the characterization score as a comprehensive evaluation metric, which is designed to effectively balance the requirements of robustness against hallucinations with the accuracy of medical knowledge. Analogous to the way precision and recall are combined in the Micro-F1 metric, the characterization score, $char\\_score$, is calculated as the weighted harmonic mean of $acc_h$ and $acc_b$:\n$char\\_score = \\frac{w_h + w_b}{\\frac{w_h}{acc_h} + \\frac{w_b}{acc_b}} = \\frac{(w_h + w_b) \\times acc_h \\times acc_b}{w_h \\times acc_h + w_b \\times acc_b}$,\nwhere $w_h, w_b \\in [0, 1]$ are weights for hallucination test accuracy $acc_h$ and baseline test accuracy $acc_b$ respectively, satisfying $w_h + w_b = 1$. Naturally, the characterization score, with assigned equal weights to $acc_h$ and $acc_b$, typically exhibits a low value when either of these scores is low, as demonstrated. This observation underscores the significant concurrent dependence of the characterization score on both metrics. Moreover, the weights can be tailored to suit the specific requirements of different applications, allowing for flexibility in adapting the model to varied use cases.\nFalse Confidence Justification. For evaluation, we will measure the propensity of LVLMs to disagree with a suggested incorrect answer, denoted as $r_{disagree}$. Additionally, we will calculate $r_{correct}$, the ratio indicating how often the alternative answer proposed by the LVLMs is correct. We will also establish a baseline, $r_{baseline}$, which represents the accuracy of the LVLMs when responding to the same set of questions without any suggested incorrect answers.\nGeneral Report Generation. We incorporate CHAIR to calculate the proportion of diseases that appear in the report but not the CXR image. Specifically, we utilize CheXpert to label the generated reports, and measure both instance-level hallucination CHAIR$_1$, and the sentence-level hallucination CHAIR$_S$ as defined in the following equations:\n$CHAIR_1 = \\frac{|\\{hallucinated \\, diseases\\}\\}|}{|\\{all \\, mentioned \\, diseases\\}|}$,\n$CHAIR_S = \\frac{|\\{sentences \\, with \\, hallucinated \\, diseases\\}\\}|}{|\\{all \\, sentences\\}|}$"}, {"title": "Main Results", "content": "We visualize the evaluation results of the Medical Visual and Text Understanding test in the left plots of Figure 3, which includes three MC-VQA tasks"}, {"title": "Long Text Generation", "content": "We present the models' performance on the False Confidence Justification in Table 2. CheXagent once again showcases the most reliable medical knowledge base in baseline experiments of the False Confidence Justification task without suggested answers. However, it exhibits a significantly higher tendency to disagree when an answer is suggested. Notably, the probability of disagreement drops when the correct answer is suggested, indicating that it can recognize the correct answer to a certain degree. MiniGPT also shows a consistent pattern of disagreement across all suggested answers, but with no reduction in disagreement when the correct answer is provided. This performance, coupled with an incompatible $r_{baseline}$, indicates a lack of both medical knowledge and reasoning capabilities. In contrast, LLM-CXR performs optimally when the correct answer is suggested. However, its performance drops with incorrect or no suggested answers, which indicates that it may possess the requisite medical knowledge, but lacks the reasoning capabilities to independently identify the correct answer, possibly due to the limited number of parameters and fine-tuning tasks. Notably, LLaVa-Med displays an even higher propensity to disagree with the correct answer and achieves the lowest scores when no answer is suggested, even falling below LLaVA's performance. This indicates that its fine-tuning not only failed to develop a coherent medical knowledge base but also impaired its original reasoning abilities.\nThe performance of the Report Generation task is demonstrated in Table 3. General LVLMs, including chat-GPT4V, fail to achieve meaningful performance with a compatible F1 score, indicating that this is indeed the task that requires the most medical knowledge and domain fine-tuning. On the other hand, since there is no misleading input in this task, CheXagent again outperforms the others, but still has a nearly 50% instance-level hallucination. In the meantime, LLM-CXR can also generate meaningful reports with a compatible F1 score, but with a much higher CHAIR score."}, {"title": "Instruction Fine-tuning", "content": "Based on our experimental findings, there is still significant potential for improvement in the robustness of LVLMs against hallucinations within the medical domain. Our experiments illustrate a notable trade-off between the reasoning capabilities developed from extensive general-domain training and the specialized knowledge obtained through domain-specific fine-tuning. The reasoning ability of a model is critical for its robustness against inputs that may induce hallucinations. Potential enhancements include increasing the model size and conducting comprehensive training with a wide variety of general images. Additionally, the source and volume of medical training data are crucial factors. Specifically, LLaVA-Med does not demonstrate competitiveness in any task, indicating that reliance solely on general PMC data to capture medical concepts is insufficient. On the other hance, the inclusion of diverse domain-specific training tasks and data sources is vital for enriching the medical knowledge base of LVLMs. This point is exemplified by CheXagent, whose superior performance highlights the benefits of instruction-based fine-tuning in endowing models with the necessary knowledge. However, despite its strong performance in regular medical tasks, CheXagent's tendency to produce hallucinated outputs poses significant concerns for its deployment in real-life settings. Future research should aim to preserve the model's reasoning ability throughout the fine-tuning process, thus developing a more reliable expert system."}, {"title": "Exploratory Analysis", "content": "Effects of Temperature Parameter\nWe examine the impact of the hyperparameters, temperature, on model-induced hallucinations. Specifically, we employed the Chat-GPT4V and assessed its performance over various temperature settings on the False Confidence Justification task, which did not provide a suggested answer. The results, depicted in Figure 4, show minimal variation in accuracy across different temperature values. These findings suggest that while temperature adjustments do influence the model's accuracy, their"}, {"title": "Sensitivity to Prompt", "content": "In Figure 5, we replaced the original options in the Wrongful Image and Clinically Incorrect Question tasks with \"None of the above\", which originally were \"This is not a suitable question for the image\" and \"The question contains a clinically incorrect premise\u201d, respectively. As the revised choices are integral to the input textual prompts for these models, our objective is to evaluate LVLMs\u2019 sensitivity to the nuances of prompt wording. Although both the substituted and original options serve to negate the correctness of other available choices, they do not convey the same message. Consequently, the observed decrease in accuracy for Chat-GPT4V is both understandable and anticipated. Conversely, the notable performance improvement in LLaVA once again underscores its propensity to select 'None of the above'. Additionally, the slight improvement in CheXagent suggests that simpler expressions of incorrectness are more easily interpreted by this model, which also points to a limitation in its reasoning ability.\nHowever, this sensitivity to prompt wording should not be viewed exclusively as a negative attribute. In Figure 6, we incorporated a hint within the prompt that suggests the possibility of an incorrect response, which led to improved performance across all models, except MiniGPT. This indicates that careful prompt design can enhance model robustness\u2014a critical aspect in real-world applications involving both patients and physicians. By incorporating user-specific information either in the prompt or even during training, the model can be tailored to handle misleading inputs more effectively. For example, while there is a potential for a patient to upload an incorrect image, the likelihood of such an error by a physician is significantly lower. Acknowledging these user-specific scenarios during model training or in the prompt structure could substantially increase the model's resilience and accuracy in practical settings."}, {"title": "Conclusion", "content": "This research investigates hallucination phenomena in domain-specific large vision-language models (LVLMs) after fine-tuning on small datasets. We introduce the MedVH benchmark dataset, which includes five types of tasks designed to evaluate hallucinations, and we compare the performance of both general and medical LVLMs using this dataset. The experimental results indicate that medical LVLMs experience more hallucinations than general LVLMs, despite achieving better performance on standard medical tasks. This inconsistency between hallucination and medical task performance raises significant concerns about the reliability of these domain-specific models, particularly in critical settings like the medical field. By releasing MedVH, we aim to encourage extensive ex-"}, {"title": "Limitations", "content": "Despite the comprehension of our proposed benchmark dataset, there are still some limitations.\nFirstly, even though our benchmark dataset incorporates multiple public datasets from various sources, there may still be potential for data bias. This is a prevalent challenge in the medical field due to the naturally unbalanced distribution of diagnosis results. Secondly, all datasets used to construct MedVH are publicly available, which may result in an overlap with the training data of some Large Vision-Language Models (LVLMs), such as Chat-GPT, which could affect the fairness and accuracy of our evaluations. Future studies could benefit from assessing these models on a private dataset that more closely mirrors real-world scenarios."}, {"title": "Ethics Statement", "content": "In this study, we introduce an evaluation framework for hallucination in Large Vision Language Models (LVLMs) within the medical domain and develop a benchmark dataset. Our framework aims to enhance the understanding of LVLMs' capabilities and improve their evaluation prior to implementation in real-world medical applications. We constructed our dataset from multiple publicly accessible sources, including MIMIC-Diff-VQA and MIMIC-CXR. To adhere to the Health Insurance Portability and Accountability Act (HIPAA) standards, all protected health information has been thoroughly anonymized. Consistent with strict privacy protocols, we refrained from directly sharing raw data with the OpenAI API and instead conducted our experiments via Azure OpenAI, per the recommendations by PhysioNet\u2074. Furthermore, we will not distribute the raw data from MIMIC-CXR through any unauthorized channels, such as GitHub. The benchmark dataset will be made available on PhysioNet following the publication of this work."}]}