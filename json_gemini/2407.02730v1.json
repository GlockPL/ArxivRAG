{"title": "MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context", "authors": ["Zishan Gu", "Changchang Yin", "Fenglin Liu", "Ping Zhang"], "abstract": "Large Vision Language Models (LVLMs) have recently achieved superior performance in various tasks on natural image and text data, which inspires a large amount of studies for LVLMS fine-tuning and training. Despite their advancements, there has been scant research on the robustness of these models against hallucination when fine-tuned on smaller datasets. In this study, we introduce a new benchmark dataset, the Medical Visual Hallucination Test (MedVH), to evaluate the hallucination of domain-specific LVLMs. MedVH comprises five tasks to evaluate hallucinations in LVLMs within the medical context, which includes tasks for comprehensive understanding of textual and visual input, as well as long textual response generation. Our extensive experiments with both general and medical LVLMs reveal that, although medical LVLMs demonstrate promising performance on standard medical tasks, they are particularly susceptible to hallucinations, often more so than the general models, raising significant concerns about the reliability of these domain-specific models. For medical LVLMs to be truly valuable in real-world applications, they must not only accurately integrate medical knowledge but also maintain robust reasoning abilities to prevent hallucination. Our work paves the way for future evaluations of these studies.", "sections": [{"title": "Introduction", "content": "Recent advancements in large language models (LLMs) have stimulated the development of domain-specific LLM applications in various sectors, including healthcare. Building on this, researchers have further introduced large vision language models (LVLMs) that"}, {"title": "Related Work", "content": "With the advent of LLMs, researchers have advanced to developing multimodal large-scale models, or LVLMs. Several efforts have also been made to adapt such LVLMs for use in the medical field, such as LLaVA-med and CheXagent. However, numerous efforts have highlighted the risk of hallucinations in large models, casting doubt on their reliability in critical fields such as healthcare. M\u00fcndler et al. (2024) have identified and suggested methods to address self-contradiction in LLMs. Umapathi et al. (2023) introduced Med-Halt to assess reasoning and memory-based hallucinations with medical entrance exams, finding that no model achieved satisfactory accuracy across most tasks. Yifan Li and Wen (2023) developed POPE to evaluate visual hallucinations in object detection in general images, noting LVLMs often identify objects that frequently appear or co-occur in their training datasets. Despite these efforts, research into hallucinations in medical vision-language tasks is still limited."}, {"title": "Hallucination Evaluation", "content": "In this section, we introduce our evaluation framework for assessing hallucinations in LVLMs within the medical domain. The overview of this framework is illustrated in Figure 1. We have developed a new benchmark dataset, MedVH, designed to evaluate the models across two distinct facets through five tasks that probe key functionalities. The following sections will offer a detailed explanation of the framework, the tasks associated with each facet of evaluation, and the metrics used for assessment."}, {"title": "Overall Evaluation Framework", "content": "As demonstrated in Figure 1, we evaluate seven state-of-the-art LVLMs from two facets, each corresponding to a different type of hallucination in the medical context. The first facet examines the models' robustness against hallucinations in comprehensive understanding of medical visual information and textual input through MC-VQA tasks, such as disease identification and severity assessment. The second facet focuses on hallucinations occurring in long text generation, particularly with false confidence justification and medical report generation. We detail each task within the MedVH dataset in Figure 2, and provide examples of prompts used in these tasks in Figure 9 of Appendix E. The models' robustness against hallucinations will be evaluated considering their ability to leverage the medical knowledge base and their model size."}, {"title": "Medical Visual and Text Understanding", "content": "We begin by assessing the presence of hallucinations in LVLMs with visual and textual comprehension. Specifically, we evaluate the models' capability to discern irrelevant or incorrect inputs and detect misleading instructions. To achieve this, we introduce three MC-VQA tasks, which involves multi-modal input comprising both an image and a textual question. The models are tested in the following settings."}, {"title": "Wrongful Image", "content": "This task is designed to evaluate the model's capability to recognize inconsistencies between the image content and the associated question, in which we replace the corresponding images with unrelated ones. We either randomly select a wrongful medical image from a different genre or choose an adversarial X-ray image of a different organ. For instance, in the task of disease identification using chest X-ray images, a randomly chosen image could be a retinal image or a picture of cells, while an adversarial image would be an X-ray image of another organ that does not exhibit the targeted disease."}, {"title": "None Of The Above", "content": "In this task, models are presented with a multi-choice question where the correct answer is explicitly listed as 'None of the above'. This setup requires the model to recognize and select this option, effectively testing its ability to discern irrelevant or incorrect options presented in the choices."}, {"title": "Clinically Incorrect Questions", "content": "This task assesses the ability of LVLMs to correctly align the specific clinical findings visible in images with the descriptions provided in the questions. In this scenario, the proposed question inquires about a specific feature that, contrary to what is suggested, does not appear in the corresponding image. This task not only tests the model's capability for interpreting medical images with domain-specific knowledge but also demands a strong reasoning ability to identify the contradiction."}, {"title": "Medical Text Generation", "content": "We also evaluate the appearance of hallucination in the long textual response of the LVLMs under the following two settings."}, {"title": "False Confidence Justification", "content": "This task presents a question and a randomly suggested wrong answer to the language model, and then asks the model to provide detailed explanations for its correctness or incorrectness. The model is supposed to suggest an alternative answer if it decides the suggested answer is incorrect. This test specifically examines the language model's propensity to express answers with unwarranted certainty in the input text."}, {"title": "General Report Generation", "content": "In this task, we prompt the LVLMs to generate medical reports based on CXR images. The objective is for the models to accurately identify diseases visible in the image. Any mention of diseases not present in the image will be considered a hallucination. This setup evaluates the models' precision in recognizing and reporting medical conditions from visual inputs while generating long textual responses."}, {"title": "Data Synthesis and Statistics", "content": "For each of the MC-VQA tasks and the False Confidence Justification task with multi-choice questions, we establish our benchmark by randomly sampling 500 questions from four publicly available medical VQA datasets: RAD-VQA, SLAKE, PMC-VQA, and MIMIC-Diff-VQA. As for the unrelated medical images and adversarial X-ray images in the Wrongful Imgae task, we randomly select the images Path-VQA and Med-VQA-2021 respectively. Among these datasets, RAD-VQA, SLAKE, and PMC-VQA mainly focus on medical knowledge-based questions, with only a small portion of general diagnosis-level questions like \u201cWhat is abnormal about the lung?\u201d. On the other hand, MIMIC-Diff-VQA, derived from de-identified patient data in MIMIC-CXR, includes a larger proportion of specific diagnostic-level questions, like \"Where in the image is the pleural effusion located?\" The details and statistics of these datasets are presented in Table 4 of subsection C.1.\nExcept for PMC-VQA, the other three datasets do not provide options for each question. For MedVH, we therefore generate answer choices for the MC-VQA questions by randomly sampling from the answers associated with the same questions. In this manner, all the datasets would be eligible being the source of the Wrongful Imgae task and the False Confidence Justification task. However, due to the limited number of repeated questions in RAD-VQA and SLAKE, excluding the ground truth answer to create a None Of The Above option would often leave only one plausible answer, reducing it to a true-or-false question. In this case, only PMC-VQA and MIMIC-Diff-VQA are utilized in the None Of The Above task. Similarly, due to the limited availability of diagnosis-level questions and the absence of hard-negative images related to the specified diseases, only MIMIC-Diff-VQA is included in the Clinically Incorrect Question task. We demonstrate the distribution of question sources in Figure 8 of subsection C.1. As for the medical report generation, we randomly sampled 200 CXR images from MIMIC-CXR."}, {"title": "Evaluation", "content": "Multi-choice VQA. For each multi-choice question, there is a designated correct answer. We quantify the model's success rate in selecting this answer using the metric $acc_h$. A higher $acc_h$ score indicates greater resistance of the model to hallucinations. Additionally, we also assess the model's performance on regular MC-VQA tasks as baseline experiments, which involve standard CXR images, correct answers among the options, and questions based on accurate clinical assumptions, serving to evaluate the model's medical knowledge. We represent the models' accuracy on this baseline task with $acc_b$. Ideally, an LVLM should demonstrate both a broad medical knowledge base and the ability to generate responses free from hallucinations.\nCharacterization score. In this study, we introduce the characterization score as a comprehensive evaluation metric, which is designed to effectively balance the requirements of robustness against hallucinations with the accuracy of medical knowledge. Analogous to the way precision and recall are combined in the Micro-F1 metric, the characterization score, $char\\_score$, is calculated as the weighted harmonic mean of $acc_h$ and $acc_b$:\n$char\\_score = \\frac{w_h + w_b}{\\frac{w_h}{acc_h} + \\frac{w_b}{acc_b}} = \\frac{(w_h + w_b) \\times acc_h \\times acc_b}{w_h \\times acc_h + w_b \\times acc_b}$,\nwhere $w_h, w_b \\in [0, 1]$ are weights for hallucination test accuracy $acc_h$ and baseline test accuracy $acc_b$ respectively, satisfying $w_h + w_b = 1$. Naturally, the characterization score, with assigned equal weights to $acc_h$ and $acc_b$, typically exhibits a low value when either of these scores is low, as demonstrated in Figure 7 within Appendix A. This observation underscores the significant concurrent dependence of the characterization score on both metrics. Moreover, the weights can be tailored to suit the specific requirements of different applications, allowing for flexibility in adapting the model to varied use cases.\nFalse Confidence Justification. For evaluation, we will measure the propensity of LVLMs to disagree with a suggested incorrect answer, denoted as $r_{disagree}$. Additionally, we will calculate $r_{correct}$, the ratio indicating how often the alternative answer proposed by the LVLMs is correct. We will also establish a baseline, $r_{baseline}$, which represents the accuracy of the LVLMs when responding to the same set of questions without any suggested incorrect answers.\nGeneral Report Generation. We incorporate CHAIR to calculate the proportion of diseases that appear in the report but not the CXR image. Specifically, we utilize CheXpert to label the generated reports, and measure both instance-level hallucination CHAIR, and the sentence-level hallucination CHAIRS as defined in the following equations:\n$CHAIR_1 = \\frac{|\\{hallucinated diseases\\}|}{|\\{all mentioned diseases\\}|}$,\n$CHAIR_S = \\frac{|\\{sentences with hallucinated diseases\\}|}{|\\{all sentences\\}|}$"}, {"title": "Main Results", "content": "We visualize the evaluation results of the Medical Visual and Text Understanding test in the left plots of Figure 3, which includes three MC-VQA tasks along with their averaged performance in the subplots. Additionally, the numeric results are detailed in Table 5 of Section D. It is observed that CheXagent excels in the baseline test\u2014where the input image accurately matches the question and the correct answer is provided among the options\u2014yet it lacks robustness when faced with inputs that could lead to hallucination. In contrast, Chat-GPT4V exhibits the most robustness against misleading inputs but falls short in displaying medical knowledge, particularly for diagnosis-level queries in the Clinically Incorrect Question task. It shows exceptional performance in handling wrongful images, likely because this task primarily tests the model's ability to differentiate between images of various organs and modalities, which demands minimal medical knowledge. The overall characterization scores of the LVLMs are also evaluated against their model size. The right plot of Figure 3 shows that CheXagent, despite having a smaller parameter size, performs comparably to ChatGPT-4V by achieving higher scores in both the None Of The Above and Clinically Incorrect Question tasks.\nAs for the rest of the models, LLaVa appears somewhere in the middle of CheXagent and ChatGPT-4V in terms of average performance (left subplot) and third in characterization score (right subplot). This is attributed to its strong performance in the None Of The Above task, a result of its propensity to select \"None of the above\". This behavior will be discussed further in Section E. Although LLaVa achieves the second highest $acc_b$ scores in all tasks, this is primarily due to its tendency to ignore distractor options such as \"This is not a suitable question for the image\", opting instead for a random choice among the remaining options. In contrast, models like MiniGPT find all options equally reasonable due to a lack of medical knowledge. Both LLaVa-Med and LLM-CXR also fail to show competitive performance, underscoring that instruction tuning based solely on general medical knowledge, or a limited amount of tasks and fine-tuning data, does not just compromise robustness against hallucination but also fails to establish a solid medical knowledge base. Note that we exclude the performance of Med-Flamingo from this analysis, as it cannot process MC-VQA tasks in a zero-shot setting, and its performance under the few-shot learning is highly dependent on the provided content, which could be unfair competition for the other models."}, {"title": "Long Text Generation", "content": "We present the models' performance on the False Confidence Justification in Table 2. CheXagent once again showcases the most reliable medical knowledge base in baseline experiments of the False Confidence Justification task without suggested answers. However, it exhibits a significantly higher tendency to disagree when an answer is suggested. Notably, the probability of disagreement drops when the correct answer is suggested, indicating that it can recognize the correct answer to a certain degree. MiniGPT also shows a consistent pattern of disagreement across all suggested answers, but with no reduction in disagreement when the correct answer is provided. This performance, coupled with an incompatible $r_{baseline}$, indicates a lack of both medical knowledge and reasoning capabilities. In contrast, LLM-CXR performs optimally when the correct answer is suggested. However, its performance drops with incorrect or no suggested answers, which indicates that it may possess the requisite medical knowledge, but lacks the reasoning capabilities to independently identify the correct answer, possibly due to the limited number of parameters and fine-tuning tasks. Notably, LLaVa-Med displays an even higher propensity to disagree with the correct answer and achieves the lowest scores when no answer is suggested, even falling below LLaVA's performance. This indicates that its fine-tuning not only failed to develop a coherent medical knowledge base but also impaired its original reasoning abilities.\nThe performance of the Report Generation task is demonstrated in Table 3. General LVLMs, including chat-GPT4V, fail to achieve meaningful performance with a compatible F1 score, indicating that this is indeed the task that requires the most medical knowledge and domain fine-tuning. On the other hand, since there is no misleading input in this task, CheXagent again outperforms the others, but still has a nearly 50% instance-level hallucination. In the meantime, LLM-CXR can also generate meaningful reports with a compatible F1 score, but with a much higher CHAIR score."}, {"title": "Instruction Fine-tuning", "content": "Based on our experimental findings, there is still significant potential for improvement in the robustness of LVLMs against hallucinations within the medical domain. Our experiments illustrate a notable trade-off between the reasoning capabilities developed from extensive general-domain training and the specialized knowledge obtained through domain-specific fine-tuning. The reasoning ability of a model is critical for its robustness against inputs that may induce hallucinations. Potential enhancements include increasing the model size and conducting comprehensive training with a wide variety of general images. Additionally, the source and volume of medical training data are crucial factors. Specifically, LLaVA-Med does not demonstrate competitiveness in any task, indicating that reliance solely on general PMC data to capture medical concepts is insufficient. On the other hance, the inclusion of diverse domain-specific training tasks and data sources is vital for enriching the medical knowledge base of LVLMs. This point is exemplified by CheXagent, whose superior performance highlights the benefits of instruction-based fine-tuning in endowing models with the necessary knowledge. However, despite its strong performance in regular medical tasks, CheXagent's tendency to produce hallucinated outputs poses significant concerns for its deployment in real-life settings. Future research should aim to preserve the model's reasoning ability throughout the fine-tuning process, thus developing a more reliable expert system."}, {"title": "Exploratory Analysis", "content": "We examine the impact of the hyperparameters, temperature, on model-induced hallucinations. Specifically, we employed the Chat-GPT4V and assessed its performance over various temperature settings on the False Confidence Justification task, which did not provide a suggested answer. The results, depicted in Figure 4, show minimal variation in accuracy across different temperature values. These findings suggest that while temperature adjustments do influence the model's accuracy, their overall effect is relatively minor, which underscores the importance of other factors in mitigating hallucinations within medical vision language tasks."}, {"title": "Sensitivity to Prompt", "content": "In Figure 5, we replaced the original options in the Wrongful Image and Clinically Incorrect Question tasks with \"None of the above\", which originally were \"This is not a suitable question for the image\" and \"The question contains a clinically incorrect premise\u201d, respectively. As the revised choices are integral to the input textual prompts for these models, our objective is to evaluate LVLMs\u2019 sensitivity to the nuances of prompt wording. Although both the substituted and original options serve to negate the correctness of other available choices, they do not convey the same message. Consequently, the observed decrease in accuracy for Chat-GPT4V is both understandable and anticipated. Conversely, the notable performance improvement in LLaVA once again underscores its propensity to select 'None of the above'. Additionally, the slight improvement in CheXagent suggests that simpler expressions of incorrectness are more easily interpreted by this model, which also points to a limitation in its reasoning ability.\nHowever, this sensitivity to prompt wording should not be viewed exclusively as a negative attribute. In Figure 6, we incorporated a hint within the prompt that suggests the possibility of an incorrect response, which led to improved performance across all models, except MiniGPT. This indicates that careful prompt design can enhance model robustness\u2014a critical aspect in real-world applications involving both patients and physicians. By incorporating user-specific information either in the prompt or even during training, the model can be tailored to handle misleading inputs more effectively. For example, while there is a potential for a patient to upload an incorrect image, the likelihood of such an error by a physician is significantly lower. Acknowledging these user-specific scenarios during model training or in the prompt structure could substantially increase the model's resilience and accuracy in practical settings."}, {"title": "Conclusion", "content": "This research investigates hallucination phenomena in domain-specific large vision-language models (LVLMs) after fine-tuning on small datasets. We introduce the MedVH benchmark dataset, which includes five types of tasks designed to evaluate hallucinations, and we compare the performance of both general and medical LVLMs using this dataset. The experimental results indicate that medical LVLMs experience more hallucinations than general LVLMs, despite achieving better performance on standard medical tasks. This inconsistency between hallucination and medical task performance raises significant concerns about the reliability of these domain-specific models, particularly in critical settings like the medical field. By releasing MedVH, we aim to encourage extensive exploration of hallucination tasks in future research, ultimately advancing the development of reliable medical LVLMs."}, {"title": "Limitations", "content": "Despite the comprehension of our proposed benchmark dataset, there are still some limitations. Firstly, even though our benchmark dataset incorporates multiple public datasets from various sources, there may still be potential for data bias. This is a prevalent challenge in the medical field due to the naturally unbalanced distribution of diagnosis results. Secondly, all datasets used to construct MedVH are publicly available, which may result in an overlap with the training data of some Large Vision-Language Models (LVLMs), such as Chat-GPT, which could affect the fairness and accuracy of our evaluations. Future studies could benefit from assessing these models on a private dataset that more closely mirrors real-world scenarios."}, {"title": "Ethics Statement", "content": "In this study, we introduce an evaluation framework for hallucination in Large Vision Language Models (LVLMs) within the medical domain and develop a benchmark dataset. Our framework aims to enhance the understanding of LVLMs' capabilities and improve their evaluation prior to implementation in real-world medical applications. We constructed our dataset from multiple publicly accessible sources, including MIMIC-Diff-VQA and MIMIC-CXR. To adhere to the Health Insurance Portability and Accountability Act (HIPAA) standards, all protected health information has been thoroughly anonymized. Consistent with strict privacy protocols, we refrained from directly sharing raw data with the OpenAI API and instead conducted our experiments via Azure OpenAI, per the recommendations by PhysioNet\u2074. Furthermore, we will not distribute the raw data from MIMIC-CXR through any unauthorized channels, such as GitHub. The benchmark dataset will be made available on PhysioNet following the publication of this work."}, {"title": "Model Implementation", "content": "In our experimental setup, we utilized ChatGPT-4V, accessed via the OpenAI Azure API \u2075, specifically employing the turbo-2024-04-09 version with the temperature parameter set to 0.2. Additionally, we integrated several local large vision language models (LVLMs): MiniGPT-v2, LLaVA v1.5, LLaVA-Med v1.5, Med-Flamingo, LLM-CXR, and CheXagent, all configured according to their default settings. We conducted all model evaluations on an NVIDIA A100 GPU, equipped with 80GB of memory."}, {"title": "Dataset Statistics", "content": "In Table 4, we present the statistics for all datasets used to develop the MC-VQA benchmark of MedVH. Of these datasets, only PMC-VQA features multiple-choice options for its questions. For the other datasets, we had to generate options ourselves. Notably, MIMI-Diff-VQA, based on MIMIC-CXR, is the only one with a considerable amount of detailed diagnosis-level questions like \"where in the image is the pleural effusion located?\" or \"what level is the cardiomegaly in the image?\", as well as hard negative CXR samples of pleural effusion and cardiomegaly. Thus, we specifically utilize MIMI-Diff-VQA to construct the Clinically Incorrect Question task."}, {"title": "MedVH Benchmark Dataset", "content": "We visualize the distribution of question sources in Figure 8 of subsection C.1. Due to the limited number of repeated questions in RAD-VQA and SLAKE, we only utilize PMC-VQA and MIMIC-Diff-VQA in the None Of The Above task. Similarly, due to the limited availability of diagnosis-level questions and the absence of hard-negative images related to the specified diseases, only MIMIC-Diff-VQA is included in the Clinically Incorrect Question task."}, {"title": "Numeric Results", "content": "We present the numeric results of MC-VQA tasks in Table 5"}, {"title": "Prompts", "content": "We exhibit example prompts in Figure 9. We change the questions, choices, and suggested answers accordingly at runtime."}]}