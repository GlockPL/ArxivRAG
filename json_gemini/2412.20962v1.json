{"title": "Conservation-informed Graph Learning for Spatiotemporal Dynamics Prediction", "authors": ["Yuan Mi", "Pu Ren", "Hongteng Xu", "Hongsheng Liu", "Zidong Wang", "Yike Guo", "Ji-Rong Wen", "Hao Sun", "Yang Liu"], "abstract": "Data-centric methods have shown great potential in understanding and predicting spatiotemporal dynamics, enabling better design and control of the object system. However, pure deep learning models often lack interpretability, fail to obey intrinsic physics, and struggle to cope with the various domains. While geometry-based methods, e.g., graph neural networks (GNNs), have been proposed to further tackle these challenges, they still need to find the implicit physical laws from large datasets and rely excessively on rich labeled data. In this paper, we herein introduce the conservation-informed GNN (CiGNN), an end-to-end explainable learning framework, to learn spatiotemporal dynamics based on limited training data. The network is designed to conform to the general conservation law via symmetry, where conservative and non-conservative information passes over a multiscale space enhanced by a latent temporal marching strategy. The efficacy of our model has been verified in various spatiotemporal systems based on synthetic and real-world datasets, showing superiority over baseline models. Results demonstrate that CiGNN exhibits remarkable accuracy and generalization ability, and is readily applicable to learning for prediction of various spatiotemporal dynamics in a spatial domain with complex geometry.", "sections": [{"title": "1 Introduction", "content": "The principled and accurate modeling and simulation of spatiotemporal dynamical systems play an important role in many science and engineering applications, e.g., physics, meteorology, ecology, social science, material science, etc. Classical approaches are primarily rooted in the use of well-posed physics-based models, governed by a system of partial differential equations (PDEs) under certain initial conditions (ICs) and boundary conditions (BCs), where the solutions could be achieved by a properly chosen numerical solver [3]. Nevertheless, the complexity of real-world problems poses grand challenges that traditional physics-based techniques struggle to address, especially when the prior knowledge of physics is incomplete or repeated forward analyses are required to assimilate data.\nThe ever-growing availability of data opens up a new avenue to tackle these challenges, resulting in data-centric methods leveraging the power of machine learning. Tremendous efforts have been recently placed on developing statistical or deep learning approaches for modeling [10, 32, 40], predicting [6, 49, 50, 65], discovering [11, 16, 55], and controlling [5, 64] the complex behavior"}, {"title": "2 Related Works", "content": "Over the past few decades, classical numerical methods [14, 27] were dominated in the forward and inverse physics problems. However, the development of this field reaches a bottleneck until the emergence of deep learning methods, which changes the status quo.\nPhysics-informed learning. As one of the most popular framework, the seminal development of the physics-informed neural network (PINN) [29, 45] has enabled learning in small data regimes for forward and inverse analyses of PDEs, where the loss function is regularized by the PDE residual. Such a differentiable paradigm has kindled enthusiastic attention in the past few years, which, together with its variant schemes, has been demonstrated effective in modeling and discovering a wide range of complex systems including fluid flows [46], subsurface transport [23], cardiovascular systems [4], deformation of solid continua [39], material constitutive relations [52], governing equation discovery [12], among many others. However, PINN generally has the need of explicit governing equations and the issue of scalability, resorting to the use of fully-connected neural networks and mesh-free point-wise training.\nNeural operators. Another framework, neural operator, is designed to learn the underlying Green's functions of PDEs, offering an alternative to model spatiotemporal dynamics, which possess generalizability and do not require a prior knowledge of the explicit PDE formulation [61, 62]. Neural operators, including DeepONet [37], Fourier neural operator (FNO) [35], and their variant algorithms [34, 57], have taken remarkable leaps in approximating the solution of PDE systems. However, the generalizability of these models largely depends on the richness of the labeled training data. When the complexity of the considered system remains high (e.g., multi-dimensional spatiotemporal dynamics in an irregular domain geometry), these methods might suffer from scalability issues with deteriorated performance.\nGeometric learning. Recently, geometric learning of system dynamics has drawn great attention [22, 43, 58], where graphs, regardless of Euclidean or non-Euclidean meshes, are employed to represent physical features and provide naturally structured relationships (e.g., spatial proximity) with interpretability [9, 36, 60]. For example, message-passing neural networks (MPNNs) have been regarded as an effective form of graph neural network (GNN) in simulating dynamics of particle assembly [53], continuum media [42] and rigid bodies [21], as well as forecasting global weather [33]. In particular, when the explicit PDE formulation is completely unknown or partially missing, a recent study [25] shows that forcibly encoding the law of thermodynamics into the GNN architecture can boost the generalizability of the model for simulation of quasi-static deformable bodies. Moreover, the model in [19] is designed to deal with lattice meshes and the graph solver in [28] is rooted in the assumption of conservative systems to update the discretized conservation equation in the latent space via hard encoding."}, {"title": "3 Preliminary and Problem Statement", "content": "Learning to predict the dynamics of complex spatiotemporal systems based on limited training data (e.g., sparsely, and perhaps non-uniformly, sampled in space and time) is regarded as a great challenge [7]. Our objective is to tackle this challenge via creating a new explainable and generalizable learning framework adaptable to a broad category of spatiotemporal dynamics. Drawing from the fundamental insights of physics, we recognize the existence of numerous conservation laws within the physical realm. In general, the strong form of these conservation laws necessitates that, in order for a conserved quantity at a specific point to alter, there must exist a flow or flux of that quantity either into or out of that point. For example, the conservation law with a source term for"}, {"title": "4 Methodology", "content": "We develop a learning scheme that conforms to the conservation law shown in Eq. 1. More details are described in Appendix Section B. We will release the source code and data at https://www.github.com after peer review."}, {"title": "4.1 Conservation-informed graph neural network", "content": "The CiGNN architecture is composed of five key blocks, as shown in Figure 1a, including the space block, the encoder, the SymMPNN block, the time block, and the decoder. Here, the encoder lifts the low-dimensional quantities to a high-dimensional representation, and in the meantime the decoder projects the updated high-dimensional latent features to the evolved physical state. The space block (see Figure 1b) defines a multi-mesh graph to account for different scales and transforms the physical system state to a low-dimensional graph representation (e.g., node features, graph topology). Given the fact that the disturbance of existing source, such as internal reaction, damping or external input, breaks down the conservation, the system will be subjected to entropy inequality. We introduce a multi-layer SymMPNN module (see Figure 1c-d) as the core component for high-dimensional feature representation learning, where we construct symmetric and asymmetric matrices to pass non-conservative information in graph edges. In the next work, instead of treating the SymMPNN module as a step-by-step learning and acquisition of higher-order information in space, we view the multi-layer message-passing mechanism of SymMPNN as a process of cascaded transmission over time. This implies that implicit latent time intervals, either fixed or non-fixed, can be generated between the SymMPNN layers. Hence, we propose a simple but effective latent time integrator (see the time block depicted in Figure 1e) to march the sequenced high-dimensional features, which improves the stability and accuracy of long-range multi-step rollout prediction. Moreover, we provide a pseudo-code describing our proposed graph operator, as shown in Algorithm 1. More details of design motivation are shown in Appendix Subsection B.3."}, {"title": "4.2 Network architecture", "content": "4.2.1 Encoder. The Encoder lifts the physical state into latent features represented in a high-dimensional space. The encoded initial node features $h_i^0 \\in \\mathbb{R}^{c}$ imply the physical state $u_i$ at the $i$th node, the corresponding position feature $x_i$ and the node type, where $c$ denotes the channel size (aka, the dimension of the latent space). The encoded initial edge features $e_{ij} \\in \\mathbb{R}^{e}$ capture the relative positional information between the connected $i$th and $j$th nodes. Additionally, a variety of variables, such as angles, are encoded to enhance the expressiveness of the edge feature representation. The corresponding formulations are given by\n$$h_i^0 = \\eta_n (u_i || x_i || v_i || ...),$$\n$$e_{ij} = \\eta_{en} ((x_j - x_i) || d_{ij} || \\theta_{ij} || \\varphi_{ij} || ...),$$\nwhere $\\eta_{en}(\\cdot)$ and $\\eta_{en}(\\cdot)$ represent the learnable functions (e.g., MLPs); $x_j-x_i$ a vector that reflects the relative information between the $i$th and $j$th nodes; $d_{ij}$ the relative distance in the physical space; $x_i$ the $i$th node type; and $\\theta_{ij}, \\varphi_{ij}$ the angle information of edge in different directions. In addition, $(||)$ denotes the concatenation.\n4.2.2 Processor. The Processor employs a graph with exclusively inter-node connections, where spatial and channel information from the local neighborhood of each node is aggregated through connections to adjacent nodes [53]. As shown in Figure 1a, it iteratively processes the high-dimensional latent features using a"}, {"title": "4.2.3 Decoder", "content": "The Decoder maps updated high-dimensional latent features back to the original physical domain, which receives input from the Processor and generates the predicted increment for the last time step as output. This increment is then added as residue to the initial state to determine the new state. The learning process at the Decoder is given by\n$$\\hat{u_i} (t + \\Delta t) = \\eta_{ode} (h_i^L) + u_i (t),$$\nwhere $\\eta_{ode}(\\cdot)$ is a differentiable function (e.g., MLP) and L the total number of SymMPNN layers."}, {"title": "4.3 Spatial and temporal learning strategies", "content": "4.3.1 Space Block with multi-mesh strategy. Consider a graph with $G = (V, E)$, where $V$ denotes the node set and $E$ the edge set. The multi-mesh strategy involves three steps: (1) defining $G^k$ as the initial graph; (2) randomly discarding a proportion of nodes in $G^k$ to generate a new graph $G^{k+1}$ as the subsequent initial graph; (3) iteratively repeating the second step, and aggregating all graphs into $G$. We organize the generated graphs by layers, designating the graph with the most node count as the topmost layer and the one with the fewest nodes as the bottom layer. In other words, as $k$ increases (k = 0, 1, 2, ...), the resulting graph structure $G^{k+1}$ contains fewer nodes. Then, we define a refinement ratio $r \\in [0, 1]$ to characterize the relationship between node numbers in the context graph. For instance, if $G^k$ has N nodes, then the subsequent graph, $G^{k+1}$, will have $rN$ nodes. We specifically set the refinement ratio $r$ to 0.1 in this study. Experimental results concerning the selection of $r$ are discussed in Appendix Table S7. The overall graph $G$ can be conceptualized as the union of all edges and nodes:\n$$G = \\bigcup_k G^k, G^k = (V^k, E^k), k = 0, ..., m.$$\nNoting that the nodes in $G^{k+1}$ are always the node subset of $G^k$, and the edge reconstruction process can capture information across extended distances. This leads to a multi-scale mesh distribution, enabling the network to capture both local and non-local relationships and learn complex detailed information. Meanwhile, the"}, {"title": "4.3.2 Time Block with trainable temporal integration", "content": "We treat the multi-layer message-passing mechanism of SymMPNN as a process of cascaded transmission over time. A series of fixed or non-fixed sub-time intervals $\\Delta t$ are generated implicitly, e.g., $\\Delta t^j \\in [0, \\Delta t_k]$, where $j = 0, 2, ..., L-1$ and $\\Delta t_k$ denotes the physical time step. Given this assumption, each SymMPNN layer can be regarded as the evolution of information over one time interval. This is equivalent to the sub-stepping process commonly used in numerical integration methods. Additionally, a Taylor series expansion shows that the method is consistent if and only if the sum of the coefficients $a_j$ is equal to one, described as\n$$h^L = h^0 + \\sum_{j=0}^{L-1} \\alpha_j K_j$$\ns.t.\n$$\\sum_{j=0}^{L-1} \\alpha_j = 1,$$\n$$K_{j+1} = \\varphi_{j+1} (t + \\Delta t, h^0 + \\beta_jK_j),$$\nwhere $\\alpha_j, \\beta_j$ are scalar, $K_1 = h^0$ the initial node features, $\\varphi_{j+1}$ the learnable function in the $(j + 1)$th SymMPNN block, and $L$ the total number of SymMPNN layers.\nNotably, this temporal integration strategy is realized on the high-dimensional space, so the coefficients $\\alpha_j, \\beta_j$ are also learnable latent features. $K_1$ denotes the output of final SymMPNN layer, which is the L-order approximation of solution. Each increment in our method can be viewed as the product of the coefficients $\\beta_j$ and an estimated slope specified by each function $\\varphi_j$. The total increment is the sum of the weighted increments. To this end, we employ a trainable 1D convolutional filter to approximate the coefficients in Eqs. 8a and 8b, as shown in Figure 1d. This temporal integration strategy does not lead to extra cost for network training and inference."}, {"title": "5 Experiments", "content": "5.1 Datasets and Baselines.\nTo verify the efficacy of CiGNN for learning complex spatiotemporal dynamics on graphs, we consider four different systems shown in Appendix Table S2, e.g., the 2D Burgers equation, a 3D Gray-Scott (GS) reaction-diffusion (RD) system, the 2D incompressible flow past a cylinder (CF), and the Black Sea (BS) hydrological observation (see Figure 2). The datasets of the first three systems were spatiotemporally down-sampled (e.g., 5-fold in time) from high-fidelity data generated by direct numerical simulations considering different ICs, BCs, and meshes, while the last one was compiled from field measurements. Additional information, including data generation statistics and example trajectories, can be found in Appendix Table S3. We selected four baseline models for comparison (see Appendix Subsection C.3 for more details), including deep operator network (DeepONet) [38], Fourier neural operator (FNO) [35], mesh-based graph network (MGN) [42], and message-passing"}, {"title": "5.2 Loss function", "content": "The training procedure is to minimize the discrepancy between the labeled and the predicted data. The loss function of our model is defined as\n$$\\mathcal{L}(\\theta) = \\frac{1}{S} \\sum_{\\alpha=1}^{S} \\| \\| \\text{vec}(\\bar{U}^{(\\alpha)}) - \\text{vec}(U^{(\\alpha)}) \\| \\|_2^2,$$\nwhich calculates the aggregated mean square errors (MSE) between the rollout-predicted data$\\bar{U}^{(\\alpha)} \\in \\mathbb{R}^{N \\times d \\times T}$ and the training labels $U^{(\\alpha)} \\in \\mathbb{R}^{N \\times d \\times T}$ to optimize the trainable parameters $(\\theta)$ in our model. Here, vec() denotes the vectorization operation; $d$ the system dimension; $N$ the total number of mesh grid points; $T$ the"}, {"title": "5.3 Evaluation metrics", "content": "The root mean squared error (RMSE) and the Pearson correlation coefficient (PCC) are utilized as the evaluation metrics, given by\n$$\\text{RMSE}(U, \\hat{U}) = \\sqrt{\\frac{1}{dNT} \\| \\text{vec}(U) - \\text{vec}(\\hat{U}) \\| ^2} $$\n$$PCC_s(U, \\hat{U}) = \\frac{\\text{cov} [\\text{vec}(U), \\text{vec}(\\hat{U})]}{\\sigma_U \\sigma_{\\hat{U}}},$$\nwhere $\\hat{U} \\in \\mathbb{R}^{N \\times d \\times T}$ and $U \\in \\mathbb{R}^{N \\times d \\times T}$ denote the predicted and true system states, respectively; cov [,] the covariance function; $\\sigma_U$ and $\\sigma_{\\hat{U}}$ the standard deviations of U and \\hat{U}, respectively; and T the total number of considered time steps. In addition, introducing noise to the training data can stabilize the model and improve the generalization ability for learning spatiotemporal dynamics [42, 53]. Therefore, we add a small amount of Gaussian noise in the training data to improve the CiGNN's performance."}, {"title": "5.4 Training settings and computational resources", "content": "For a fair comparison, we train each model ten times and select the one with the median performance according to the evaluation of the validation datasets. We train all models for 1,000 epochs and set early stopping with a patience of 100 epochs. We utilize the GELU activation function [41] for all MLP hidden layers, while linear activation is considered for the output layers. Additionally, layer normalization is applied after each MLP to improve the training convergence, except for those in the Decoder. All models are trained using the Adam Optimizer [31] and the ReduceLROnPlateau learning scheduler [41] which decays the learning rate by a factor of 0.8. The encoder module consists of a 2-layer MLP with a hidden size of 128, and the decoder has a 2-layer MLP with a hidden size of 128. We only use 4 SymMPNN blocks for the processor, where 2 layer"}, {"title": "5.5 Results", "content": "5.5.1 2D Convection-Diffusion system. The results listed in Table 1 show that CiGNN consistently outperforms other baseline models in the context of prediction accuracy. Figure 3a displays the distribution of prediction errors as well as the predicted snapshots at a typical time step (0.995 s) by different methods. The CiGNN model lifts the prediction accuracy by 1~2 orders of magnitude in terms of MSE metric compared with all other baselines. Due to the mesh-less and point-wise nature of DeepONet, it falls short in scalable modeling of higher dimensional spatiotemporal dynamics. Given the squared domain with structured grid mesh, FNO produces a moderate prediction result in the situation with limited training data. The performance of MP-PDE aligns with that of MGN. Note that MP-PDE is a derivative of MGN with the primary distinction in their decoder components, e.g., MGN employs multilayer perceptron (MLP) while MP-PDE utilizes convolution for decoding. Despite discrepant performance of other models, CiGNN generalizes well to ICs and maintains a high prediction accuracy, which demonstrates notable strength and shows clear superiority over the baselines.\n5.5.2 3D Reaction-Diffusion system. The results reported in Table 1 once again show our CiGNN model achieves the best performance. Figure 3b displays the prediction error distribution and typical snapshots for the GS RD system at 713.75 s. Similar to the previous example, FNO, renowned for its potency in the realm of operator learning, appears to inadequately grasp the underlying dynamics of the GS RD equation in the regime of small training data. Unfortunately, DeepONet exhibits little progress in learning the intricate evolution of the 3D system states, e.g., the prediction over time still deviates clearly from actual values yielding large errors. The results serve to substantiate that both MGN and MP-PDE, based on the multi-step rollout training strategy, maintain robustness. However, CiGNN still outperforms other models, showing its superior potential in learning spatiotemporal dynamics in a 3D space.\n5.5.3 2D Flow past a cylinder. Our model CiGNN generalizes well over Re and BCs as well as graph meshes in the CF example, which again outperforms the baselines as depicted in Table 1. Figure 3c displays the snapshots of the predicted flow field at time 8.55 s in the generalization test, where the cylinder size and position as well as the domain mesh remain different from those in the training datasets. Notably, generalizing the learning model on various CF datasets poses grand challenges due to the varying mesh node counts and graph structures, exacerbated by the introduction of random cylinder positions that lead to variant Reynolds numbers. Since FNO struggles with irregular mesh structures, we opted not"}, {"title": "5.5.4 2D Hydrological dynamics forecast of the Black Sea", "content": "We employ the trained CiGNN model to conduct a 20-day-long forecasting of the hydrological dynamics of the Black Sea. Figure 4a-b show the results of the predicted water flow velocities and sea surface temperature. It is evident that our model effectively captures the evolution patterns of the hydrological dynamics, especially for low-frequency information that dominates the overall trend (as can be seen from the predicted snapshots). The correlation values for both the velocity and temperature fields consistently lie above 0.8, indicating an accurate prediction even in the presence of unknown uncertainties. The correlation curve in Figure 4c-d illustrates that the prediction of the temperature exhibits relatively scattered with a larger deviation. This might be because the temperature data, captured at a depth of 12 meters below sea level, remains less susceptible to the influence of other external factors. Yet, this real-world dataset considered in this example encompasses numerous elusive variables that are absent from our model's training data which consists solely of velocity and temperature variables. Consequently, this limitation hampers accurate predictions of localized high-frequency information, deteriorating the performance of CiGNN. Commonly, this issue can be ameliorated by augmenting the training data with pertinent variables to enhance the model's predictive capability."}, {"title": "5.6 Ablation study", "content": "To gain deeper insights into how each component of the model affects its overall performance, we conducted an ablation study based on the aforementioned datasets. The results show that the removal of each module leads to deteriorated performance of the model as shown in Table 2. We summarize the observations as follows:\n\u2022 Results in Task #1 demonstrate that, while the promotion may vary, both the conservative and inequality principles play significant roles across different datasets.\n\u2022 Results in Task #2 indicate that a larger number of edges in the global context leads to more accurate prediction. Our proposed trade-off strategy of constructing sparse graphs at different scales greatly improves performance.\n\u2022 Results in Task #3 show that our temporal learning strategy effectively mitigates error propagation issues with a high-order scheme. In particular, we observe that the performance of the temporal strategy is sensitive to the size of the single-step time interval."}, {"title": "6 Conclusion", "content": "This paper introduces an end-to-end graph-based deep learning model (namely, CiGNN) to predict the evolution of complex spatiotemporal dynamics. The CiGNN model is designed to preserve"}, {"title": "A Background", "content": "The study of spatiotemporal dynamics aims to explore and elucidate the dynamic behavior, pattern formation, instability, and predictive capabilities of complex systems. Understanding spatiotemporal dynamics is crucial for addressing practical challenges in various domains, including physics, chemistry, engineering, climate science, etc. Recently, machine learning (ML) has shown great promise in modeling domain-specific scientific problem governed by partial differential equations (PDEs). In this paper, we focus on graph neural networks (GNNs) to learn spatiotemporal PDE systems."}, {"title": "A.1 Conventional GNNS", "content": "A classic GNN [59] is composed of three main components: an embedding layer for the input features, a stack of GNN layers, and a final task-based layer. In a graph G = (V, E) where V contains n = |V| nodes and E is the set of edges, the connectivity of the graph is represented by the adjacent matrix $A \\in \\mathbb{R}^{n \\times n}$. Here, Aij = 1 if there exists an edge between the nodes i and j, otherwise Aij = 0. The degree matrix is denoted as $D \\in \\mathbb{R}^{n \\times n}$."}, {"title": "A.2 Conventional MPNNS", "content": "For simplicity, we describe the process of message-passing neural networks (MPNNs) [17] on the undirected graph G with node features $h_i$ and edge features $e_{ij}$. The forward pass has two phases, a message-passing phase and an update phase. The message-passing phase is expressed by message function $\\varphi_e$ and the update phase is described by vertex update function $\\varphi_u$. The network layers are indexed by $l$ and $h^{l=0}$ denotes the node features of the initial input layer. During these two phase, the hidden state $h_i^{l+1}$ at node $i$ in the graph are updated by the messages $e_{ij}^{l,*}$ and its last hidden state $h_i^{l}$ according to\n$$e_{ij}^{l,*} = \\varphi_e(h_i^{l} || h_j^{l} || e_{ij}^{l}),$$\n$$h_i^{l+1} = \\varphi_u( h_i^l, \\sum_{j \\in N_i} e_{ij}^{l,*} )$$"}, {"title": "B Methodology", "content": "This section provides the supplementary information for the methodology described in the Main Text."}, {"title": "B.1 Preliminary of Graph Calculus", "content": "Let G = (V, E) be a connected finite graph, which includes |V| nodes and |E| edges containing all edges between vertices in V. In the scalar or vector field, the sum of degrees is formulated as $\\sum_{u_i\\in V} deg(u_i) = 2|E|$, where the symbol $deg(u_i)$ denotes the number of edges that are incident to the vertex $u_i$. Under the aforementioned conditions, we furnish the relevant definitions below.\nDefinition 1: With a vertex function f: V \u2192 C, the symbol $\\nabla_{ij}f$ for $j \\in N_i$ is described as follows:\n$$\\nabla_{ij}f = f (u_j) \u2013 f (u_i),$$\nwhere Ni denotes all other nodes within the neighborhood of node $u_i$ and we simplify the subscript $u_i u_j$ as $ij$.\nDefinition 2: With a vertex function f: V \u2192 C, the symbol $\\nabla_{ij}f^2$ for $j \\in N_i$ is defined as\n$$\\nabla_{ij}f^2 = f (u_j)^2 \u2013 f (u_i)^2 .$$\nDefinition 3: With any vertex function f: V \u2192 C and g: V \u2192 C, the symbol $\\nabla_{ij} (fg)$ for $j \\in N_i$ is formulated as follows:\n$$\\nabla_{ij} (fg) = f (u_j) g (u_j) \u2013 f (u_i) g (u_i)$$\n$$\\hspace{1.2cm}= f (u_j) (g (u_j) \u2013 g (u_i)) + (f (u_j) \u2013 f (u_i)) g (u_i)$$\n$$\\hspace{1.2cm}= f (u_j) \\nabla_{ij} (g) + \\nabla_{ij} (f) g (u_i)$$\n$$\\hspace{1.2cm}= (\\nabla_{ij} (f) + f (u_i)) \\nabla_{ij} (g) + \\nabla_{ij} (f) g (u_i)$$\n$$\\hspace{1.2cm}= \\nabla_{ij} (f) \\nabla_{ij} (g) + f (u_i) \\nabla_{ij} (g) + \\nabla_{ij} (f) g (u_i) .$$\nDefinition 4: For $j \\in N_i$, the gradient of vertex function f at any $u_i \\in V$ is given by\n$$\\nabla f (u_i) = (\\nabla_{ij}f).$$\nDefinition 5: For $j \\in N_i$ on G, the scalar product of any vector field $W (u_i) = (w_{ij})$ and $U (u_i) = (\\bar{u}_{ij})$ is given by\n$$W (u_i). U (u_i) = \\sum_{j \\in N_i} W_{ij}\\bar{u}_{ij}$$\nSpecifically, we have\n$$|W (u_i) |^2 = \\sum_{j \\in N_i} w_{ij}$$\nDefinition 6: If f is a scalar function (i.e., a real-valued or complex-valued function) and W (u_i) is a vector field, their pointwise product can be written as follows:\n$$(fW) (u_i) = f (u_i) \u00b7 W (u_i),$$\nwhere the function f scales each component of the vector field W (u_i) at every point $u_i$ in space.\nDefinition 7: For $u_i \\in V$, the directional derivative of function f along a vector field W (u_i) is defined by\n$$\\nabla_W f (u_i) = W \u00b7 \\nabla f (u_i) = \\sum_{j \\in N_i} w_{ij} \\nabla_{ij}f.$$\nThen, the directional derivative of a function f along \\nabla f has the form of\n$$\\nabla_{\\nabla f} f (u_i) = \\sum_{j \\in N_i} \\nabla_{ij}f \\nabla_{ij}f = |\\nabla f (u_i) |^2.$$"}, {"title": "B.2 Conservative Property of CiGNN", "content": "Despite the complex behavior exhibited in spatiotemporal dynamic systems, conservation laws provide a means to simplify the analysis of these systems. For instance, in fluid dynamics, the conservation of mass (also named continuity equation) is the key to the problem-solving. Our proposed graph approximator is inspired by this conservation principle linking the flux of a vector field through a closed surface to its divergence within the enclosed volume. Suppose volume V is a compact subset of R\u00b3 and S (also indicated with V) is its piecewise smooth boundary, we have the detailed description shown as follows.\nTheorem 1: If there is a continuously differentiable tensor field F on a neighborhood of V, then we have that a volume integral over the volume V is equal to the surface integral over the boundary of the volume V:\n$$\\int_{V} (\\nabla F)dv = \\oint_{S} (F.\\hat{n})dS,$$\nwhere $ \\hat{n} $ is the outward-pointing unit normal. The conservation principle (e.g., the divergence-free condition) formulated in Theorem 1 implies that while the flow quantity within a local region changes over time, the global net flow in and out remains nearly constant. For further explanation, within the graph G, we denote $flow_{ij}$ as the flow that leaves from the node $i$, and $flow_{ji}$ the flow that enters to the node $i$. In a scalar or vector field, we consider that $flow_{ij} \\geq 0$ and $flow_{ji} \\leq 0$. Here, a key assumption underlying the physics law is that $flow_{ij} = -flow_{ji}$, for $(i, j) \\in G$. Then, the conservation principle on the edge, or the divergence measurement on one vertex reads\n$$flux_i = \\sum_{(i,\\cdot) \\in G} flow_{i, \\cdot} + \\sum_{(\\cdot, i) \\in G} flow_{\\cdot,i}$$\nwhere $flux_i$ denotes the total flow quantity through the $i$th node, and $flow_{i,.}$ and $flow_{.,i}$ the flows that leave and enter the node"}, {"title": "B.3 Spatial and Temporal Strategies", "content": "The motivations of our network design are two-fold in terms of feature extraction of spatial patterns and temporal evolution, respectively.\nB.3.1 Spatial strategy. Firstly, GNNs have shown great potential for modeling spatiotemporal systems thanks to their inherent capacity to capture the dynamics in the physical world using graph structures. However, a critical observation reveals that current GNN models tend to overemphasize node features while neglecting the crucial aspect of edge features [20]. In particular, leveraging GNNs for modeling spatiotemporal dynamics should be able to include both explicit temporal changes in node states and implicit interactions via edges. The existing GNN models focus on processing edge features to align with data distribution without obeying a prior knowledge of physics principles. Hence, we introduce two forms of prior knowledge to ensure the GNN model conforming to the general conservation law, namely, symmetry (e.g., conservation) and asymmetry (e.g., entropy inequality), and design the SymMPNN block as a core graph learning module. These concepts draw inspiration from the well-known conservation law and principle of symmetry. Imposing hard constraints on GNN models via symmetry can facilitate the network convergence, enhance the model's interpretability, and improve the ability of spatial feature representation learning."}, {"title": "B.3.2 Temporal strategy", "content": "Predicting the temporal evolution of dynamics can be cast as a Markov process. A simple approach to model such an evolution is resorted to numerical discretization and integration over time. Explicit time integration schemes are favorable and preferred in deep learning (e.g., MPNNs [42"}]}