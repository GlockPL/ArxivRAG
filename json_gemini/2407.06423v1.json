[{"title": "Prompt 3: Prompt used for Generating Diverse Follow-up Questions based on the previous answer", "authors": ["Gaurav Sahu", "Abhay Puri", "Juan Rodriguez", "Alexandre Drouin", "Perouz Taslakian", "Valentina Zantedeschi", "Alexandre Lacoste", "David Vazquez", "Nicolas Chapados", "Christopher Pal", "Sai Rajeswar Mudumba", "Issam Hadj Laradji"], "abstract": "Data analytics is essential for extracting valuable insights from data that can assist or-\nganizations in making effective decisions. We introduce InsightBench, a benchmark\ndataset with three key features. First, it consists of 31 datasets representing diverse\nbusiness use cases such as finance and incident management, each accompanied by a\ncarefully curated set of insights planted in the datasets. Second, unlike existing bench-\nmarks focusing on answering single queries, InsightBench evaluates agents based on\ntheir ability to perform end-to-end data analytics, including formulating questions,\ninterpreting answers, and generating a summary of insights and actionable steps.\nThird, we conducted comprehensive quality assurance to ensure that each dataset\nin the benchmark had clear goals and included relevant and meaningful questions\nand analysis. Furthermore, we implement a two-way evaluation mechanism using\nLLaMA-3-Eval as an effective, open-source evaluator method to assess agents' abil-\nity to extract insights. We also propose AgentPoirot, our baseline data analysis agent\ncapable of performing end-to-end data analytics. Our evaluation on InsightBench\nshows that AgentPoirot outperforms existing approaches (such as Pandas Agent)\nthat focus on resolving single queries. We also compare the performance of open-\nand closed-source LLMs and various evaluation strategies. Overall, this benchmark\nserves as a testbed to motivate further development in comprehensive data analytics\nand can be accessed here: https://github.com/ServiceNow/insight-bench.", "sections": [{"title": "1 Introduction", "content": "Businesses frequently leverage vast datasets to perform data analytics to uncover insights, discover\npatterns, and analyze trends in order to support effective decision-making [McAfee and Brynjolfsson,\n2012, Colson, 2019, Bean, 2022]. The task of end-to-end data analysis starts with stating a high-level\ngoal; the analyst then alternates between identifying key questions to explore, and extracting valuable\ninsights from their answers, working towards the goal. This iterative process continues until they have\na comprehensive summary of insights and recommended actions (shown in Figure 1).\nPreprint. Under review."}, {"title": "2 InsightBench \u2013 An Enterprise Data Analysis Benchmark", "content": "InsightBench's data were derived from the ServiceNow [ServiceNow, 2024] platform, which is used\nto manage business workflows crucial for enterprise operations. It consists of tables that store and\nmanage records and data entities relevant to various organizational functions. Common tables in\nServiceNow include incident tables (for incident records), user tables (for user information), and asset\nmanagement tables (to oversee infrastructure). InsightBench consists 31 tabular datasets acquired\nfrom the ServiceNow platform covering five distinct themes (see Figure 3).\nOverview of Benchmark Creation. InsightBench is an automated data analysis benchmark that\ncan rigorously assess the performance of LLM-based data analysis agents in real-world scenarios.\nBuilding InsightBench consisted of four stages: 1) selecting relevant data tables from the ServiceNow\ndata tables and extracting a list of relevant columns to define the schema (Section 2.1.1), 2) formulating\ntrends to inject in the data (Section 2.1.2), 3) creating synthetic data entries to populate the tables that\nfollow the trend formulated in the previous step (Section 2.1.3), and 4) creating a ground-truth analysis\nnotebook for the generated data (Section 2.1.4). See Figure 2 for an overview of the multi-stage process.\nA detailed list of the datasets sorted by themes is presented in Appendix B.4. We propose a multi-step\nevaluation mechanism to measure the performance of an agents on InsightBench (See Section 2.2)"}, {"title": "2.1 Creating InsightBench Datasets", "content": "Each dataset consists of 500 synthetically generated entries, stored as a CSV file. We choose a size\nof 500 entries to keep the volume of data manageable for analysis and substantial enough to simulate\ntypical enterprise data loads. To emulate a realistic enterprise environment, InsightBench uses a hybrid\napproach combining actual data structures with synthetic insights."}, {"title": "2.1.1 Data Tables", "content": "To create the datasets, we first select relevant data tables from ServiceNow system tables. As a\nguiding example, consider an incident table, which contains records of any disruption to normal\nservice operations. The incidents can range from server outages to hardware malfunctions, such as\na non-operational printer. This table is structured with fields (aka columns) relevant to managing the\nlifecycle of an incident, such as the time it was opened, the description of the incident, and assigned"}, {"title": "2.1.2 Planting Insights", "content": "We embed systematic anomalies and trends into the datasets to certain types of columns in the\ndatasets, referred to as controllable columns. These trends were implemented using standard\nmathematical models or algorithms, such as regression, to ensure consistency and detectability\nof the planted insights. For instance, we manipulated the time-to-resolution (TTR) trend for\nincidents in a service management dataset to reflect an increasing trend over time. This was\nachieved by using a linear model to generate the TTR based on the creation date of each incident:\nTTR = 1+ slope. (incident_open_date - data_start_date), where (slope) is a parameter that controls\nthe rate of increase in resolution time (See Figure 2(b)).\nTypes of Insights. Every insight in InsightBench can be categorized into four types, where each type\nserves a specific function in data analysis, contributing uniquely to the utility of the insights derived.\n1. Descriptive: These insights describe what has happened and are vital in summarizing large datasets\ninto understandable plots. E.g., a plot of the distribution of incident categories over the past year.\n2. Diagnostic: These insights explain the why or the cause behind observed trends using tools such as\nsegmentation and correlation. E.g., a wordcloud of the most common words in incident description.\n3. Predictive: These insights use statistical methods to forecast future outcomes based on past data.\nE.g., a plot forecasting the future volume of incidents based on current trends in resolution times.\n4. Prescriptive: These types of insights suggest actions to tackle the current issues. E.g., an insight\nrecommending strategies to mitigate this projected increase in incident volume."}, {"title": "2.1.3 Synthetic Data Generation", "content": "We employed two primary methods to populate the non-controllable columns-columns that are not\ndirectly manipulated to embed synthetic insights:\n\u2022 Random Sampling: Fields such as IDs, categories of incidents or assets, transaction dates, and\nstatus codes were populated by sampling from a curated list of plausible values. This approach\nmimics the random yet plausible variations in actual operational data.\n\u2022 Context Generation Using Large Language Models (LLMs): To introduce complexity and\nrealism, certain text-based fields like incident or goal descriptions and user feedback were generated\nusing LLMs such as GPT-4 [OpenAI, 2023]. These models were tasked with creating coherent\nand contextually relevant entries that align with the data schema, enhancing the datasets with\nnatural-looking and appropriate text data. The prompt structure used can be found in Appendix D."}, {"title": "2.1.4 Ground-Truth Analysis Notebooks", "content": "A key component of constructing InsightBench was developing 31 expert-annotated Jupyter notebooks,\neach tailored for a specific context, such as incident management or financial operations. The notebook\nstructure and contents are as follows (see Figure 2(c)):\n\u2022 Dataset overview and a SMART Goal: Each notebook begins with a comprehensive dataset\noverview, outlining its relevance and structure. It is accompanied by a SMART (Specific, Measurable,\nAttainable, Relevant, and Timely) goal (E.g., \"Analyze the discrepancy and imbalance in the\ndistribution of incidents assigned across categories,\" aimed at identifying the primary causes of\nhardware failures in an organization.)\n\u2022 Sequential Analysis: The notebook contains a series of questions designed to uncover layers\nof insights gradually. Each question has a Python code block that generates a plot to answer\nthe question, and each plot's data is summarized as JSON metadata outlining the insights. We ensure\nthat each question builds sequentially onto the previous one, leading up to the final insight.\n\u2022 Insight Summary: The final section of each notebook summarizes the findings and proposes\nactionable steps. This might include recommendations like \u201cOpen a Ticket\" for further investigation."}, {"title": "2.2 Evaluating Agents on InsightBench", "content": "To evaluate agents on InsightBench, we compare the ground-truth annotations (GT) in Jupyter\nnotebooks against the list of insights (A) provided by an agent. Formally speaking, we compare two\nnatural language texts. While multiple metrics have been proposed in the past, like ROUGE [Lin, 2004],\nMETEOR [Banerjee and Lavie, 2005], and BERTSCore [Zhang et al., 2019], they do not align well\nwith human preferences. Recently, G-Eval [Liu et al., 2023] was shown to be highly aligned with human\npreferences for a variety of tasks, including text summarization. Since our task involves comparing two\nfree-form sentences, we adopt an LLM-based evaluator as well. Specifically, we use LLaMA-3-Eval,\na variant of G-Eval that uses LLAMA-3-70b instead of a GPT model. We use LLaMA-3-Eval as the\nprimary metric to measure the correctness of agent-provided insights. We chose LLaMA-3-Eval\nover G-Eval because LLAMA-3-70b is open-sourced, allowing us to a) avoid API costs for using GPT\nmodels from OpenAI and b) fix the model weights to obtain a stable and reliable evaluation metric,\nunlike G-Eval, whose output scores can change due to periodic updates to GPT endpoints.\nInsightBench performs a two-way evaluation:\n1. Summary-level. To obtain the summary-level score, we simply compute the LLaMA-3-Eval score\nbetween the agent-provided summary of insights, and the ground-truth summary.\n2. Insight-level. First, we select the most appropriate insight a \u2208 A to evaluate against a given\nground-truth insight (gt \u2208 GT), and then average the scores for all the ground-truth insights. The\ninsight-level score can be expressed as in Equation 1, where |GT| is the number of ground-truth\ninsights in a dataset, and M represents LLaMA-3-Eval evaluator:\nscore= \n\\frac{\\sum_{gteGT}argmax_{a\u2208A}M(gt,a)}{|GT|} \n                                                                 (1)\nWe average the summary-level and insight-level scores for all 31 datasets in InsightBench to obtain a\nmeasure of the agent's performance. Additionally, we compute scores using ROUGE-1 for comparison."}, {"title": "2.3 Dataset Statistics", "content": "InsightBench covers a distinct range of business analytics themes and varying difficulty lev-\nels, ensuring a comprehensive evaluation of the agents. The benchmark includes 31 datasets\nspread across five key topics. These topics were chosen to reflect typical scenarios encoun-\ntered in enterprise settings, each requiring clear and well-defined insights. The distribution\nacross the topic is shown in Figure 3, and the data tables used are outlined in Appendix B.3\nDistribution by Difficulty. The datasets are assigned a dif-\nficulty level on a scale from 1 (easy) to 4 (hard). Easy (Level\n1-2) comprises 9 datasets that primarily involve direct data\nretrieval and basic analysis. Medium (Level 3) includes\n12 datasets that may require the integration of multiple"}, {"title": "2.4 Quality Assurance", "content": "To ensure our datasets are of high quality, we designed a questionnaire and an interface that displays our\ndataset to users. We had 20 volunteers with basic data science skills evaluate whether our datasets were\naccurately described, contained relevant and interesting questions, and provided substantial insights.\nThe exact questions used, the interface design, and the results are shown in B.5. The average rating was\n4 out of 5 across these questions, and we received helpful comments that improved parts of the dataset."}, {"title": "3 Experimental Setup", "content": "The main experimental setup in our benchmarking process involves inputting the dataset schema and\nan overarching goal for each dataset in InsightBench and letting the agent perform exploratory data\nanalysis. As discussed in Section 2, the goals are carefully designed to provide a meaningful signal to\nthe agent without revealing the ground-truth answer.\nIn addition to the main experiments, we conduct the following ablation studies to understand the\nimportance of different parameters of InsightBench and their effect on agent performance.\nEffect of Using Generic Goals. Instead of using the carefully designed goal from the ground-truth\nannotations, we use the generic goal \"I want to find interesting trends in this dataset.\" We aim to study\nthe effectiveness of our goals in steering the agents toward performing meaningful data analysis.\nEffect of Trend Intensity. We generate variations of Dataset 2 (see Table 4 for the complete list),\nwhich has the planted insight, \"Time to resolution increases linearly over time.\" We vary the slope of\nthe linear curve for that insight from -0.1 to 0.9. Through this experiment, we study whether an agent\nneeds a trend to be blatantly obvious (high intensity) to retrieve or if the agent can retrieve nuanced\ntrends (low intensity) as well. Notably, we also consider negative slopes in this experiment to test if the\nagents output a false positive (discovering a trend even when it is not present).\nEffect of Question Diversity. We restrict the agent to generate only \u201cdescriptive\" questions as\nfollow-ups instead of letting it generate different types of questions.\nEffect of LLM Sampling Temperature. We vary the sampling temperature of the LLM from 0 to 1.0\nto study its effect on the agent's performance."}, {"title": "3.1 Baselines", "content": "We run the following data analytics agents on InsightBench:\na) Pandas Agent (PA) [LangChain, 2024]. A data science agent by LangChain optimized for question-\nanswering 1. Given a data frame and a question related to it, Pandas Agent first generates Python code\nand then executes it to produce the answer. In our experiments, we input the goal to the agent and let it\niteratively generate question and their answers. We then pass the list of answers to the agent again and\nask it to generate a summary of those answers.\nb) AgentPoirot (Ours). We propose AgentPoirot that, given a dataset (D) in InsightBench and a\ngoal (G), performs systematic data exploration aimed at achieving G. Figure 7 shows the working of\nAgentPoirot:"}, {"title": "3.2 Implementation Details", "content": "We use Python's openai package to access the family of GPT models for our experiments and v11m to\nhost LLAMA-3-70b 2 model on 4 A100 GPUs. We use different LLMs as backbones in our experiments\nto benchmark, including gpt-40, gpt-4-turbo, gpt-3.5-turbo, and llama-3-70b 3. All results\nare reported for the sampling temperature of 0.0, unless otherwise stated. We use the evaluate\nPython package to compute ROUGE-1 scores and Prompt 9 in Appendix D for computing LLaMA-\n3-Eval scores. We also fix the temperature of LLaMA-3-70b to 0 during evaluation. We repeat all our\nexperiments for 5 seeds and report the mean and standard deviation in our results."}, {"title": "4 Benchmark Results", "content": "Main Results. Table 1 shows the performance of different data analytics agents on InsightBench.\nFirst, we note that AgentPoirot outperforms Pandas Agent in terms of ROUGE-1 and LLaMA-3-Eval\nscores. We note that using gpt-40 consistently achieves the best overall performance. We further see\nthat AgentPoirot with LLAMA-3-70b outperforms AgentPoirot with gpt-3.5-turbo and is close to\ngpt-4-turbo in terms of LLaMA-3-Eval scores.\nFigure 4a shows the performance of the agents by different dataset categories. Agents achieve the\nhighest scores in \u201cAsset Management,\u201d \u201cGoal Management,\" and \"Finance Management\" categories.\nNotably, AgentPoirot with LLaMA-3-70b struggles when we combine two datasets (\u201cFinance Manage-"}, {"title": "Effect of Using Generic Goals", "content": "Table 1 also shows the performance of AgentPoirot that uses a\ngeneric goal. We notice a drastic overall decrease in the ROUGE-1 and LLaMA-3-Eval scores for all\nthe backbones. For instance, the insight-level LLaMA-3-Eval score for AgentPoirot (gpt-40) drops\nfrom 0.59 (\u00b1 0.01) to 0.39 (\u00b1 0.02) and the summary-level LLaMA-3-Eval score drops from 0.41\n(\u00b10.02) to 0.31 (\u00b10.13) (see \u201cOurs (gpt-4o)\u201d v/s \u201cOurs (gpt-4o) w/ generic goal\" rows in Table 1).\nThis highlights the importance of including a SMART goal in the agent's performance."}, {"title": "Effect of Exploring Diverse Questions", "content": "Table 1 includes the performance of AgentPoirot when\ngenerating only a single type of follow-up questions (see \u201cOurs (llama-3-70b) non-diverse follow-ups\").\nWe observe a notable decrease in ROUGE-1 scores and a slight decrease in LLaMA-3-Eval scores,\nconfirming that discovering a diverse set of questions during data analysis leads to better performance."}, {"title": "Effect of Trend Intensity", "content": "Figure 8b shows the performance of AgentPoirot with different backbones\non variations of dataset 2 in InsightBench. First, we note that all the models fail to discover insight when\nthe slope is less than 0.1 (including negative slopes). This suggests that our model did not generate\nfalse positives (we assume the flag is discovered if LLaMA-3-Eval is greater than 0.5. Overall, for\nslope greater than 0.1, gpt-40 and gpt-4-turbo discovered the insight every time, LLAMA-3-70b\ndiscovered it nearly half the times, and gpt-3.5-turbo had a particularly hard time discovering\nthe insight. While both LLAMA-3-70b and gpt-3.5-turbo have difficulty discovering the trend\nsometimes (as showcased by the fluctuations in LLaMA-3-Eval scores), LLaMA-3-70b shows a better\noverall discovery rate of the insight compared to gpt-3.5-turbo."}, {"title": "Effect of Sampling Temperature", "content": "Figure 8a in Appendix C shows the performance of AgentPoirot with\nLLAMA-3-70b for different sampling temperatures. The performance of the agent peaks at temperature\n0.2 before it starts to decrease. Higher temperatures also lead to unstable performance, as indicated\nby bigger standard deviations in the plot. Overall, we note that the temperature should be low but not\ncompletely 0 for optimal agent performance on InsightBench."}, {"title": "G-Eval v/s LLaMA-3-Eval", "content": "Table 6 in Appendix C shows results for a small-scale study where we\ncompare G-Eval and LLaMA-3-Eval. We note that both scores are similar in range and follow the\nsame trend for all agents. This suggests that LLaMA-3-70b is a good alternative to using gpt-4o for\nevaluation."}, {"title": "One-to-Many v/s Many-to-Many Evaluation", "content": "We conduct another study to compare our approach\nwith an alternative many-to-many version, where, instead of computing a score for each ground\ntruth and prediction pair, we first prompt the LLM to match every ground truth response with an\nappropriate prediction. We perform this experiment because the one-to-many evaluation approach can"}, {"title": "5 Related Work", "content": "Our work intersects with the literature on Data Science Benchmarks, LLM Evaluation Frameworks,\nand Text-to-Analytics Agents."}, {"title": "Data Science Benchmarks", "content": "With enhanced abilities in code generation, utilization of LLM-based data\nanalytics assistants is becoming increasingly prevalent. This has led to the development of numerous\ndata science benchmarks [Lai et al., 2022, Chandel et al., 2022, Zan et al., 2022, Hu et al., 2024,\nZhang et al., 2024b]. DS-1000 [Lai et al., 2022] focuses on code generation for diverse data science\nquestions sourced from StackOverflow, emphasizing functional correctness in solutions. Similarly,\nInfiAgent-DABench [Hu et al., 2024] assesses the end-to-end problem-solving ability of LLMs by\nrequiring them to interact with an execution environment to answer questions based on provided CSV\nfiles. DSP [Chandel et al., 2022], evaluates a model's proficiency through a code-infilling task within\nJupyter Notebooks. DSEval [Zhang et al., 2024b] focuses on the overall behavior of data science agents\nwithout getting into the nuances of code generation techniques. On the other hand, Text2SQL and\ntabular data reasoning benchmarks assess the ability of models to parse queries, extract information,\nand synthesize the retrieved data to formulate responses [Katsogiannis-Meimarakis and Koutrika,\n2021, Zhong et al., 2018, Chen et al., 2021b]. While existing benchmarks primarily evaluate agents\non single-step code generation, InsightBench shifts the focus to multi-step analysis[Delen and Ram,\n2018]."}, {"title": "LLM Evaluation Frameworks", "content": "Current LLM evaluation frameworks primarily focus on measuring\nthe competence of LLM-based agents in handling structured outputs Zhang et al. [2023]. Existing\nevaluations predominantly rely on pre-formatted prompts to assess code completion [Wu et al., 2023,\nZhang et al., 2024a, Yao et al., 2023]. While recent advancements have seen autonomous agents\nspecializing in intricate data science tasks, including analysis, visualization, and modeling [Qian et al.,\n2023, 2024, Zhang et al., 2023], evaluations for these methods often depend on extensive human\neffort [Cheng et al., 2023] or use more powerful LLMs to assess the output [Dubois et al., 2023, Belyi\net al., 2024]. G-Eval [Liu et al., 2023] is a recent technique used to evaluate the quality of free-form\ntexts in terms of factuality and coherence. In this work, we use a variation of G-Eval to score how well\nthe predicted insights are aligned with the ground-truth insight."}, {"title": "Text-to-Analytics Agents", "content": "Chen et al. [2023] explore the application of GPT variants within a data\nvisualization context, highlighting the strengths and limitations of these models. More recent LLM-\nbased data analysis agents include Code Interpreter [OpenAI, 2024] and Pandas Agent [LangChain,\n2024] that are capable of processing multiple data formats and answering questions about them. Ma\net al. [2023] propose InsightPilot, an advanced automated tool that leverages LLMs to enhance data\nexploration by automatically identifying goals and generating targeted intentional queries. Vacareanu\net al. [2024] showed that LLMs can also perform regression tasks, enhancing their predictive abilities.\nAdditional studies assess the data analysis capabilities of GPT-4 and propose an end-to-end framework\nfor automating data processes [Cheng et al., 2023, Wang et al., 2023, 2024, Hong et al., 2024]. Inspired\nby InsightPilot, we propose AgentPoirot that can perform end-to-end data analysis that includes\nextracting descriptive, diagnostic, predictive, and prescriptive insights."}, {"title": "6 Conclusion", "content": "We have introduced InsightBench, a benchmark with 31 diverse datasets that evaluates agents on their\nability to perform end-to-end data analytics, including suggesting questions, interpreting answers, and\nsummarizing insights. We ensured each dataset has clear goals and meaningful analysis as ground-\ntruth to evaluate agents reliably. Using LLaMA-3-Eval, an open-source evaluator, we assessed how\nwell the agents extracted insights compare to the ground-truth. We showed how our proposed agent,\nAgentPoirot, can perform data analytics from high-level goals using both open-source and closed-\nsource models and showed that it outperforms existing approaches like Pandas Agent. We believe\nInsightBench will significantly drive advancements in data analytics. For future work, we look into\nexpanding this benchmark to include more categories of data such as healthcare data, social media\ntrends, environmental data, e-commerce analytics, and educational statistics."}, {"title": "7 Limitations", "content": "Benchmarks have the risk of reinforcing existing biases or oversimplifying complex decision processes.\nThe design of InsightBench necessitates continuous reviews to ensure that the insights generated by\nagents do not perpetuate biases or lead to misinformed decisions. Additionally, agents' performance on\nthis benchmark should be continually assessed against evolving business practices and technological\nadvancements to maintain relevance and effectiveness. More discussions on limitations are presented\nin Appendix A."}, {"title": "A Limitations and Potential Societal Impact", "content": "InsightBench is designed to advance the field of Exploratory Data Analysis (EDA) by providing a\nchallenging framework for evaluating open-ended multi-step data analytics tools and models. The\ndataset evaluates and helps build conversational data agents with significant potential to enhance\ndecision-making and operational efficiencies in organizations, particularly for end-users who may not\nhave deep technical expertise in data science. Yet, there are important considerations and limitations\nconcerning the current data construction and the benchmark's application in the real-world setting.\nDiversity and Scope of Data Simulation. While InsightBench covers a spectrum of business\nareas and themes, the datasets are synthesized based on patterns observed in widely-used ServiceNow.\nConsequently, the synthetic \"flags\" are designed to mimic real-world anomalies and trends but may not\ncapture genuine enterprise data's full complexity or unexpected behavior.\nExpanding the Benchmark. The current structure of InsightBench allows for scalable expansions to\ninclude more nuanced scenarios, themes, and more datasets that could involve unstructured data like\nreports, logs, or emails. Crucial information and actionable insights can also be derived from visual\nrepresentations such as charts and plots. Further development could also integrate diverse business\nnorms, the ability to analyze the plots and graphs from a visual context alongside language by the\nagents. Moreover, incorporating feedback from real-world deployments could help refine the datasets\nto simulate underlying business data better. Lastly, InsightBench allows interpreting data types and\nevaluating outputs for practical utility. Its adaptive framework, which evolves through multi-step\ninteractive feedback, may align with agile development practices. This positions our InsightBench a\nfuture setup for integrating intelligence into the end-to-end software development process."}, {"title": "B Appendix: Overview", "content": "Our supplementary material includes the following sections:\n\u2022 Section A: Contains details of data accessibility, generation process, and a breakdown of\ndomains covered by InsightBench.\n\u2022 Sectoin B: Contains results of some ablation experiments and qualitative examples.\n\u2022 Section C: Discusses relevant literature for our work.\n\u2022 Section D: Contains all the prompts used in our experiments."}, {"title": "B.1 Accessibility and Maintenance", "content": "Table 3: Following the dataset track guidelines, we share the following availability of InsightBench dataset\nArtifact          Link                                              Licence\nHome Page         https://insightbench.github.io/                       CC BY-NC 4.0\nCode Repository   https://github.com/ServiceNow/insight-bench          MIT License\nDatasets          https://huggingface.co/datasets/ServiceNow/insight_bench CC BY-NC 4.0\nDataset Maintenance Authors are committed to ensuring the dataset's regular upkeep and relevance,\nwe will place a system for users to report issues or suggest updates. A feedback form will be available\nfor users to contribute their input. We commit to actively reviewing these suggestions and making the\nnecessary adjustments to the dataset. We also invite contributions from the community through pull\nrequests on InsightBench's GitHub repository."}, {"title": "B.2 Example Protocol for Data Generation and Insight Planting", "content": "Here, we outline a practical case of how we created a flag dataset for an incidents table with a trend in\nwhich the duration of incident resolution increases over time:\n1. Schema Exporting and Standardization: Initially, the schema for the incidents table\nis exported and standardized by inspecting a demo instance. Fields such as \"number\",\n\"opened_at\", and \"closed_at\", among others, are defined."}, {"title": "B.3 Data Tables across themes", "content": "Here is an exhaustive and descriptive outline of the topics covered in the benchmark:\n\u2022 Incidents Management: Focuses on tracking, analyzing, and resolving workplace incidents. An\nextensive description of this data table is discussed afore-mentioned Sec 2.1.1.\n\u2022 User Management: Datasets in this theme are derived from sys_user system table of ServiceNow.\nThis table tracks all user profiles within the platform. It contains information about each user, such as\ntheir roles, department affiliations, contact details, and activity status. Key fields include the user's\nID, name, email, department, and last login time. This table is used in InsightBench for insights\nrelated to roles and permissions of employee agents, focusing on schedules and activity patterns.\n\u2022 Finance Expenses: Datasets in this topic examine detailed records of expenses to uncover patterns\nand optimize budget allocations and prioritization. Datasets are built from fm_expense_list, a system\ntable that logs detailed entries of financial transactions and expenses as part of the financial manage-\nment module. It includes data points like the expense amount, date, associated user, department,\ncategory, and processing status.\n\u2022 Inventory Management: Manages data regarding hardware assets and procurement, aiming to\nunderstand patterns in inventory systems. Datasets are derived from alm_hardware, a system table\nthat manages records of all physical assets, particularly IT hardware. Fields in this table detail each\nasset's tag number assigned to the user, status, location, purchase date, and warranty expiration. It\nsupports asset tracking, lifecycle management, and maintenance activities within an organization.\n\u2022 Enterprise Goal Management: Datasets in this theme evaluate the alignment of departmental\nperformance with overarching goals, focusing on the effectiveness and achievement rates. Datasets\nare derived from sn_gf_goal system table of ServiceNow, which is essential for evaluating goal\nmanagement efficiency and aligning departmental outputs with organizational objectives. Fields\nin this table consist of goal description, start and end dates, current status, owner, priority, and\npercentage completion."}, {"title": "B.4 List of Datasets and Problems", "content": "Table 4 shows the names of all the datasets in InsightBench along with their difficulty and category.\nTable 5 presents an example of a question-insight pair along with corresponding plots for each category."}, {"title": "B.5 Quality-check Interface", "content": "To ensure the high standards of data quality and relevance, InsightBenchhas implemented a compre-\nhensive quality-check process involving volunteer contributors. This process is facilitated through a\nspecifically developed gradio interface, which guides the expert reviewers through a structured review\nof the ground-truth analysis notebooks included in the benchmark (Figure 5). The evaluation is divided\ninto three main sections:"}, {"title": "B.6 Details of the proposed AgentPoirot", "content": "Here we describe the proposed baseline AgentPoirot in further detail. Figure 7 depicts our pipeline.\nInitially, AgentPoirot uses the dataset and its schema to obtain a set of root questions about the data,\naligning with the user's Goal or Role. Each of the root questions can be injected into a Code Generation\nprompt to obtain and execute code, as well as obtain textual descriptions of the insight found. From\nthese outputs, we generate follow-up questions and start the process again, obtaining a model that\nnavigates the data in depth and in breadth to find interesting insights."}, {"title": "D Prompt Design", "content": "In this section, we illustrate the use of tailored prompts to enhance the analytical capabilities of our\nmodels for extracting insights and conducting advanced analytics."}, {"title": "Prompt 1: Prompt used for Extracting Questions from Data", "content": "SYSTEM MESSAGE:\nYou the manager of a data science team whose goal is to help stakeholders within your\ncompany extract actionable insights from their data.\nYou have access to a team of highly skilled data scientists that can answer complex\nquestions about the data.\nYou call the shots and they do the work.\nYour ultimate deliverable is a report that summarizes the findings and makes\nhypothesis for any trend or anomaly that was found.\nPROMPT:\n### Instruction:\nGiven the following context:\n{context"}, "nGiven the following goal:\n{goal}\nGiven the following schema:\n{schema}\nInstructions:\n* Write a list of questions to be solved by the data scientists in your team to\nexplore my data and reach my goal.\n* Explore diverse aspects of the data, and ask questions that are relevant to my goal.\n* You must ask the right questions to surface anything interesting (trends, anomalies,\netc.)\n* Make sure these can realistically be answered based on the data schema.\n* The insights that your team will extract will be used to generate a report.\n* Each question should only have```json\n{\n  \"title\": \"Prompt 2: Prompt used for Generating and Executing code for answering a Question\",\n  \"content\":", "PROMPT:\nGiven the goal:\\n\n{goal}\nGiven the schema:\\n\n{schema}\nGiven the data path:\\n\n{database_path}\nGiven the list of predefined functions in cba.tools module and their example usage:\\n\\\nn\n{function_docs}\nGive me the python code required to answer this question \"{question}\" and put a\ncomment on top of each variable.\\n\\n\nMake a single code block for starting with '''python\nDo not produce code blocks for languages other than Python.\nImport cba.tools at the beginning.\nYou must only use the predefined functions mentioned above to make the plot.\nYou must generate one single simple plot and save it as a jpg file.\nFor the plot, save a stats json file that stores the data of the plot.\nFor the plot, save a x_axis.json and y_axis.json file that stores a maximum of 50 of\nthe most important x and y axis data points of the plot, respectively.\nSave each json file using the cba.save_json function\nFor the json file must have a \"name\", \"description\", and \"value\" field that describes\nthe data.\nThe content of the json file should be less than 4500 characters\nCall the fix_fnames function in cba.tools at the end of your code.\nEnd your code with\nOutput code:\\n"], "content": "SYSTEM:\nYou the manager of a data science team whose goal is to help stakeholders within your\ncompany extract actionable insights from their data.\nYou have access to a team of highly skilled data scientists that can answer complex\nquestions about the data.\nYou call the shots and they do the work.\nYour ultimate deliverable is a report that summarizes the findings and makes\nhypothesis for any trend or anomaly that was found.\nPROMPT:\nHi, I require the services of your team to help me reach my goal.\n{context}\n{goal}\n{schema}\n{question}\n{answer}\nInstructions:\n* Produce a list of follow up questions explore my data and reach my goal.\n* Note that we have already answered  and have the answer at , do\nnot include a question similar to the one above.\n* Explore diverse aspects of the data, and ask questions that are relevant to my goal.\n* You must ask the right questions to surface anything interesting (trends, anomalies,\netc.)\n* Make sure these can realistically be answered based on the data schema.\n* The insights that your team will extract will be used to generate a report.\n* Each question that you produce must be enclosed in  tags\n* Each question should only have one part, that is a single '?' at the end which only\nrequire a single answer.\n* Do not number the questions.\n* You can produce at most {max_questions} questions."}, {"title": "Prompt 4: Prompt used for Generating only one type of Follow-up Questions based on the previous answer", "content": "SYSTEM:\nYou the manager of a data science team whose goal is to help stakeholders within your\ncompany extract actionable insights from their data.\nYou have access to a team of highly skilled data scientists that can answer complex\nquestions about the data.\nYou call the shots and they do the work.\nYour ultimate deliverable is a report that summarizes the findings and makes\nhypothesis for any trend or anomaly that was found.\nPROMPT:\nHi, I require the services of your team to help me reach my goal.\n{context}\n{goal}\n{schema}\n{question_type}\n{question}\n{answer}\nInstructions:\n* Produce a list of follow-up questions to explore my data and reach my goal.\n* Note that we have already answered  and have the answer at , do\nnot include a question similar to the one above.\n* Explore diverse aspects of the data, and ask questions that are relevant to my goal.\n* You must ask the right questions to surface anything interesting (trends, anomalies,\netc.)\n* Make sure these can realistically be answered based on the data schema.\n* The insights that your team will extract will be used to generate a report.\n* The question has to adhere to the type of question that is provided in the <\nquestion_type> tag\n* The type of question is either descriptive, diagnostic, prescriptive, or predictive.\n* Each question that you produce must be enclosed in  tags\n* Each question should only have one part, that is a single '?' at the end which only\nrequire a single answer.\n* Do not number the questions.\n* You can produce at most {max_questions} questions."}, {"title": "Prompt 5: Prompt used for Interpreting the Solution", "content": "PROMPT:\n### Instruction:\nYou are trying to answer a question based on information provided by a data scientist.\nGiven the context:\n\nYou need to answer a question based on information provided by a data scientist.\n\nGiven the goal:\n{goal}\nGiven the question:\n{question}\nGiven the analysis:\n\n{message}\n\n{insights}\n\nInstructions:\n* Based on the analysis and other information provided above, write an answer to the\nquestion enclosed with  tags.\n* The answer should be a single sentence, but it should not be too high level and\nshould include the key details from justification.\n* Write your answer in HTML-like tags, enclosing the answer between   tags, followed by a justification between   tags.\n* Refer to the following example response for the format of the answer and\njustification.\nExample response:\nThis is a sample answer\nThis is a sample justification\n### Response:"}, {"title": "Prompt 6: Prompt used for Selecting the best Question based on previously asked questions and the goal", "content": "SYSTEM MESSAGE:\nYou the manager of a data science team whose goal is to help stakeholders within your\ncompany extract actionable insights from their data.\nYou have access to a team of highly skilled data scientists that can answer complex\nquestions about the data.\nYou call the shots and they do the work.\nYour ultimate deliverable is a report that summarizes the findings and makes\nhypothesis for any trend or anomaly that was found.\nPROMPT:\nHi, I require the services of your team to help me reach my goal.\n{context}\n{goal}\n{prev_questions}{prev_questions_formatted}\n{followup_questions}{followup_questions_formatted}\nInstructions:\n* Given a context and a goal, select one follow up question from the above list to\nexplore after prev_question that will help me reach my goal.\n* Do not select a question similar to the prev_questions above.\n* Output only the index of the question in your response inside \n.\n* The output questions id must be 0-indexed."}, {"title": "Prompt 7: Prompt used for Summarizing the Insights", "content": "SYSTEM MESSAGE:\nYou the manager of a data science team whose goal is to help stakeholders within your\ncompany extract actionable insights from their data.\nYou have access to a team of highly skilled data scientists that can answer complex\nquestions about the data.\nYou call the shots and they do the work.\nYour ultimate deliverable is a report that summarizes the findings and makes\nhypothesis for any trend or anomaly that was found.\nPROMPT:\nHi, I require the services of your team to help me reach my goal.\n{context}\n{goal}\n{history}{history}\nInstructions:\n* Given a context and a goal, and all the history of  pairs from\nthe above list generate the 3 top insights that will help me reach my goal.\n* Output each insight within this tag ."}, {"title": "Prompt 8: Prompt used for generating the synthetic data for non-controllable elements of an incidents dataset.", "content": "gpt_fields = [\"short_description\", \"assignment_group\"]\nfield_info = \"\\n\".join(\n[\n \"<field>\"\n + f\"{f}</name>{col_info[f]['type']}</type>\"\n + (\n f\" The  categories. AgentPoirot with LLaMA-3-70b struggles when we combine two datasets (\u201cFinance Manage-"}]