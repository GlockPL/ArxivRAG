{"title": "InsightBench: Evaluating Business Analytics Agents Through Multi-Step Insight Generation", "authors": ["Gaurav Sahu", "Abhay Puri", "Juan Rodriguez", "Alexandre Drouin", "Perouz Taslakian", "Valentina Zantedeschi", "Alexandre Lacoste", "David Vazquez", "Nicolas Chapados", "Christopher Pal", "Sai Rajeswar Mudumba", "Issam Hadj Laradji"], "abstract": "Data analytics is essential for extracting valuable insights from data that can assist or- ganizations in making effective decisions. We introduce InsightBench, a benchmark dataset with three key features. First, it consists of 31 datasets representing diverse business use cases such as finance and incident management, each accompanied by a carefully curated set of insights planted in the datasets. Second, unlike existing bench- marks focusing on answering single queries, InsightBench evaluates agents based on their ability to perform end-to-end data analytics, including formulating questions, interpreting answers, and generating a summary of insights and actionable steps. Third, we conducted comprehensive quality assurance to ensure that each dataset in the benchmark had clear goals and included relevant and meaningful questions and analysis. Furthermore, we implement a two-way evaluation mechanism using LLaMA-3-Eval as an effective, open-source evaluator method to assess agents' abil- ity to extract insights. We also propose AgentPoirot, our baseline data analysis agent capable of performing end-to-end data analytics. Our evaluation on InsightBench shows that AgentPoirot outperforms existing approaches (such as Pandas Agent) that focus on resolving single queries. We also compare the performance of open- and closed-source LLMs and various evaluation strategies. Overall, this benchmark serves as a testbed to motivate further development in comprehensive data analytics and can be accessed here: https://github.com/ServiceNow/insight-bench.", "sections": [{"title": "1 Introduction", "content": "Businesses frequently leverage vast datasets to perform data analytics to uncover insights, discover patterns, and analyze trends in order to support effective decision-making [McAfee and Brynjolfsson, 2012, Colson, 2019, Bean, 2022]. The task of end-to-end data analysis starts with stating a high-level goal; the analyst then alternates between identifying key questions to explore, and extracting valuable insights from their answers, working towards the goal. This iterative process continues until they have a comprehensive summary of insights and recommended actions (shown in Figure 1).\nPreprint. Under review."}, {"title": "2 InsightBench \u2013 An Enterprise Data Analysis Benchmark", "content": "InsightBench's data were derived from the ServiceNow [ServiceNow, 2024] platform, which is used to manage business workflows crucial for enterprise operations. It consists of tables that store and manage records and data entities relevant to various organizational functions. Common tables in ServiceNow include incident tables (for incident records), user tables (for user information), and asset management tables (to oversee infrastructure). InsightBench consists 31 tabular datasets acquired from the ServiceNow platform covering five distinct themes (see Figure 3).\nOverview of Benchmark Creation. InsightBench is an automated data analysis benchmark that can rigorously assess the performance of LLM-based data analysis agents in real-world scenarios. Building InsightBench consisted of four stages: 1) selecting relevant data tables from the ServiceNow data tables and extracting a list of relevant columns to define the schema (Section 2.1.1), 2) formulating trends to inject in the data (Section 2.1.2), 3) creating synthetic data entries to populate the tables that follow the trend formulated in the previous step (Section 2.1.3), and 4) creating a ground-truth analysis notebook for the generated data (Section 2.1.4). See Figure 2 for an overview of the multi-stage process. A detailed list of the datasets sorted by themes is presented in Appendix B.4. We propose a multi-step evaluation mechanism to measure the performance of an agents on InsightBench (See Section 2.2)"}, {"title": "2.1 Creating InsightBench Datasets", "content": "Each dataset consists of 500 synthetically generated entries, stored as a CSV file. We choose a size of 500 entries to keep the volume of data manageable for analysis and substantial enough to simulate typical enterprise data loads. To emulate a realistic enterprise environment, InsightBench uses a hybrid approach combining actual data structures with synthetic insights."}, {"title": "2.1.1 Data Tables", "content": "To create the datasets, we first select relevant data tables from ServiceNow system tables. As a guiding example, consider an incident table, which contains records of any disruption to normal service operations. The incidents can range from server outages to hardware malfunctions, such as a non-operational printer. This table is structured with fields (aka columns) relevant to managing the lifecycle of an incident, such as the time it was opened, the description of the incident, and assigned"}, {"title": "2.1.2 Planting Insights", "content": "We embed systematic anomalies and trends into the datasets to certain types of columns in the datasets, referred to as controllable columns. These trends were implemented using standard mathematical models or algorithms, such as regression, to ensure consistency and detectability of the planted insights. For instance, we manipulated the time-to-resolution (TTR) trend for incidents in a service management dataset to reflect an increasing trend over time. This was achieved by using a linear model to generate the TTR based on the creation date of each incident:\n TTR = 1+ slope. (incident_open_date - data_start_date), where (slope) is a parameter that controls the rate of increase in resolution time (See Figure 2(b)).\nTypes of Insights. Every insight in InsightBench can be categorized into four types, where each type serves a specific function in data analysis, contributing uniquely to the utility of the insights derived.\n1. Descriptive: These insights describe what has happened and are vital in summarizing large datasets into understandable plots. E.g., a plot of the distribution of incident categories over the past year.\n2. Diagnostic: These insights explain the why or the cause behind observed trends using tools such as segmentation and correlation. E.g., a wordcloud of the most common words in incident description.\n3. Predictive: These insights use statistical methods to forecast future outcomes based on past data. E.g., a plot forecasting the future volume of incidents based on current trends in resolution times.\n4. Prescriptive: These types of insights suggest actions to tackle the current issues. E.g., an insight recommending strategies to mitigate this projected increase in incident volume."}, {"title": "2.1.3 Synthetic Data Generation", "content": "We employed two primary methods to populate the non-controllable columns-columns that are not directly manipulated to embed synthetic insights:\n\u2022 Random Sampling: Fields such as IDs, categories of incidents or assets, transaction dates, and status codes were populated by sampling from a curated list of plausible values. This approach mimics the random yet plausible variations in actual operational data.\n\u2022 Context Generation Using Large Language Models (LLMs): To introduce complexity and realism, certain text-based fields like incident or goal descriptions and user feedback were generated using LLMs such as GPT-4 [OpenAI, 2023]. These models were tasked with creating coherent and contextually relevant entries that align with the data schema, enhancing the datasets with natural-looking and appropriate text data. The prompt structure used can be found in Appendix D."}, {"title": "2.1.4 Ground-Truth Analysis Notebooks", "content": "A key component of constructing InsightBench was developing 31 expert-annotated Jupyter notebooks, each tailored for a specific context, such as incident management or financial operations. The notebook structure and contents are as follows (see Figure 2(c)):\n\u2022 Dataset overview and a SMART Goal: Each notebook begins with a comprehensive dataset overview, outlining its relevance and structure. It is accompanied by a SMART (Specific, Measurable, Attainable, Relevant, and Timely) goal (E.g., \"Analyze the discrepancy and imbalance in the distribution of incidents assigned across categories,\" aimed at identifying the primary causes of hardware failures in an organization.)\n\u2022 Sequential Analysis: The notebook contains a series of questions designed to uncover layers of insights gradually. Each question has a Python code block that generates a plot to answer the question, and each plot's data is summarized as JSON metadata outlining the insights. We ensure that each question builds sequentially onto the previous one, leading up to the final insight.\n\u2022 Insight Summary: The final section of each notebook summarizes the findings and proposes actionable steps. This might include recommendations like \u201cOpen a Ticket\" for further investigation."}, {"title": "2.2 Evaluating Agents on InsightBench", "content": "To evaluate agents on InsightBench, we compare the ground-truth annotations (GT) in Jupyter notebooks against the list of insights (A) provided by an agent. Formally speaking, we compare two natural language texts. While multiple metrics have been proposed in the past, like ROUGE [Lin, 2004], METEOR [Banerjee and Lavie, 2005], and BERTSCore [Zhang et al., 2019], they do not align well with human preferences. Recently, G-Eval [Liu et al., 2023] was shown to be highly aligned with human preferences for a variety of tasks, including text summarization. Since our task involves comparing two free-form sentences, we adopt an LLM-based evaluator as well. Specifically, we use LLaMA-3-Eval, a variant of G-Eval that uses LLAMA-3-70b instead of a GPT model. We use LLaMA-3-Eval as the primary metric to measure the correctness of agent-provided insights. We chose LLaMA-3-Eval over G-Eval because LLAMA-3-70b is open-sourced, allowing us to a) avoid API costs for using GPT models from OpenAI and b) fix the model weights to obtain a stable and reliable evaluation metric, unlike G-Eval, whose output scores can change due to periodic updates to GPT endpoints.\nInsightBench performs a two-way evaluation:\n1. Summary-level. To obtain the summary-level score, we simply compute the LLaMA-3-Eval score between the agent-provided summary of insights, and the ground-truth summary.\n2. Insight-level. First, we select the most appropriate insight a \u2208 A to evaluate against a given ground-truth insight (gt \u2208 GT), and then average the scores for all the ground-truth insights. The insight-level score can be expressed as in Equation 1, where |GT| is the number of ground-truth insights in a dataset, and M represents LLaMA-3-Eval evaluator:\nscore=$\\frac{\\sum_{gt \\in GT}argmax_{a \\in A}M(gt,a)}{|GT|}$ (1)\nWe average the summary-level and insight-level scores for all 31 datasets in InsightBench to obtain a measure of the agent's performance. Additionally, we compute scores using ROUGE-1 for comparison."}, {"title": "2.3 Dataset Statistics", "content": "InsightBench covers a distinct range of business analytics themes and varying difficulty lev- els, ensuring a comprehensive evaluation of the agents. The benchmark includes 31 datasets spread across five key topics. These topics were chosen to reflect typical scenarios encoun- tered in enterprise settings, each requiring clear and well-defined insights. The distribution across the topic is shown in Figure 3, and the data tables used are outlined in Appendix B.3\nDistribution by Difficulty. The datasets are assigned a dif- ficulty level on a scale from 1 (easy) to 4 (hard). Easy (Level 1-2) comprises 9 datasets that primarily involve direct data retrieval and basic analysis. Medium (Level 3) includes 12 datasets that may require the integration of multiple"}, {"title": "2.4 Quality Assurance", "content": "To ensure our datasets are of high quality, we designed a questionnaire and an interface that displays our dataset to users. We had 20 volunteers with basic data science skills evaluate whether our datasets were accurately described, contained relevant and interesting questions, and provided substantial insights. The exact questions used, the interface design, and the results are shown in B.5. The average rating was 4 out of 5 across these questions, and we received helpful comments that improved parts of the dataset."}, {"title": "3 Experimental Setup", "content": "The main experimental setup in our benchmarking process involves inputting the dataset schema and an overarching goal for each dataset in InsightBench and letting the agent perform exploratory data analysis. As discussed in Section 2, the goals are carefully designed to provide a meaningful signal to the agent without revealing the ground-truth answer.\nIn addition to the main experiments, we conduct the following ablation studies to understand the importance of different parameters of InsightBench and their effect on agent performance.\nEffect of Using Generic Goals. Instead of using the carefully designed goal from the ground-truth annotations, we use the generic goal \"I want to find interesting trends in this dataset.\" We aim to study the effectiveness of our goals in steering the agents toward performing meaningful data analysis.\nEffect of Trend Intensity. We generate variations of Dataset 2 (see Table 4 for the complete list), which has the planted insight, \"Time to resolution increases linearly over time.\" We vary the slope of the linear curve for that insight from -0.1 to 0.9. Through this experiment, we study whether an agent needs a trend to be blatantly obvious (high intensity) to retrieve or if the agent can retrieve nuanced trends (low intensity) as well. Notably, we also consider negative slopes in this experiment to test if the agents output a false positive (discovering a trend even when it is not present).\nEffect of Question Diversity. We restrict the agent to generate only \u201cdescriptive\" questions as follow-ups instead of letting it generate different types of questions.\nEffect of LLM Sampling Temperature. We vary the sampling temperature of the LLM from 0 to 1.0 to study its effect on the agent's performance."}, {"title": "3.1 Baselines", "content": "We run the following data analytics agents on InsightBench:\na) Pandas Agent (PA) [LangChain, 2024]. A data science agent by LangChain optimized for question- answering 1. Given a data frame and a question related to it, Pandas Agent first generates Python code and then executes it to produce the answer. In our experiments, we input the goal to the agent and let it iteratively generate question and their answers. We then pass the list of answers to the agent again and ask it to generate a summary of those answers.\nb) AgentPoirot (Ours). We propose AgentPoirot that, given a dataset (D) in InsightBench and a goal (G), performs systematic data exploration aimed at achieving G. Figure 7 shows the working of AgentPoirot:"}, {"title": "3.2 Implementation Details", "content": "We use Python's openai package to access the family of GPT models for our experiments and v11m to host LLAMA-3-70b 2 model on 4 A100 GPUs. We use different LLMs as backbones in our experiments to benchmark, including gpt-40, gpt-4-turbo, gpt-3.5-turbo, and llama-3-70b 3. All results are reported for the sampling temperature of 0.0, unless otherwise stated. We use the evaluate Python package to compute ROUGE-1 scores and Prompt 9 in Appendix D for computing LLaMA-3-Eval scores. We also fix the temperature of LLaMA-3-70b to 0 during evaluation. We repeat all our experiments for 5 seeds and report the mean and standard deviation in our results."}, {"title": "4 Benchmark Results", "content": "Main Results. Table 1 shows the performance of different data analytics agents on InsightBench. First, we note that AgentPoirot outperforms Pandas Agent in terms of ROUGE-1 and LLaMA-3-Eval scores. We note that using gpt-40 consistently achieves the best overall performance. We further see that AgentPoirot with LLAMA-3-70b outperforms AgentPoirot with gpt-3.5-turbo and is close to gpt-4-turbo in terms of LLaMA-3-Eval scores.\nFigure 4a shows the performance of the agents by different dataset categories. Agents achieve the highest scores in \u201cAsset Management,\u201d \u201cGoal Management,\" and \"Finance Management\" categories. Notably, AgentPoirot with LLaMA-3-70b struggles when we combine two datasets (\u201cFinance Manage-"}, {"title": "Effect of Using Generic Goals", "content": "Table 1 also shows the performance of AgentPoirot that uses a generic goal. We notice a drastic overall decrease in the ROUGE-1 and LLaMA-3-Eval scores for all the backbones. For instance, the insight-level LLaMA-3-Eval score for AgentPoirot (gpt-40) drops from 0.59 (\u00b1 0.01) to 0.39 (\u00b1 0.02) and the summary-level LLaMA-3-Eval score drops from 0.41 (\u00b10.02) to 0.31 (\u00b10.13) (see \u201cOurs (gpt-4o)\u201d v/s \u201cOurs (gpt-40) w/ generic goal\u201d rows in Table 1). This highlights the importance of including a SMART goal in the agent's performance."}, {"title": "Effect of Exploring Diverse Questions", "content": "Table 1 includes the performance of AgentPoirot when generating only a single type of follow-up questions (see \u201cOurs (llama-3-70b) non-diverse follow-ups\"). We observe a notable decrease in ROUGE-1 scores and a slight decrease in LLaMA-3-Eval scores, confirming that discovering a diverse set of questions during data analysis leads to better performance."}, {"title": "Effect of Trend Intensity", "content": "Figure 8b shows the performance of AgentPoirot with different backbones on variations of dataset 2 in InsightBench. First, we note that all the models fail to discover insight when the slope is less than 0.1 (including negative slopes). This suggests that our model did not generate false positives (we assume the flag is discovered if LLaMA-3-Eval is greater than 0.5. Overall, for slope greater than 0.1, gpt-40 and gpt-4-turbo discovered the insight every time, LLAMA-3-70b discovered it nearly half the times, and gpt-3.5-turbo had a particularly hard time discovering the insight. While both LLAMA-3-70b and gpt-3.5-turbo have difficulty discovering the trend sometimes (as showcased by the fluctuations in LLaMA-3-Eval scores), LLaMA-3-70b shows a better overall discovery rate of the insight compared to gpt-3.5-turbo."}, {"title": "Effect of Sampling Temperature", "content": "Figure 8a in Appendix C shows the performance of AgentPoirot with LLAMA-3-70b for different sampling temperatures. The performance of the agent peaks at temperature 0.2 before it starts to decrease. Higher temperatures also lead to unstable performance, as indicated by bigger standard deviations in the plot. Overall, we note that the temperature should be low but not completely 0 for optimal agent performance on InsightBench."}, {"title": "G-Eval v/s LLaMA-3-Eval", "content": "Table 6 in Appendix C shows results for a small-scale study where we compare G-Eval and LLaMA-3-Eval. We note that both scores are similar in range and follow the same trend for all agents. This suggests that LLaMA-3-70b is a good alternative to using gpt-4o for evaluation."}, {"title": "One-to-Many v/s Many-to-Many Evaluation", "content": "We conduct another study to compare our approach with an alternative many-to-many version, where, instead of computing a score for each ground truth and prediction pair, we first prompt the LLM to match every ground truth response with an appropriate prediction. We perform this experiment because the one-to-many evaluation approach can"}, {"title": "5 Related Work", "content": "Our work intersects with the literature on Data Science Benchmarks, LLM Evaluation Frameworks, and Text-to-Analytics Agents."}, {"title": "6 Conclusion", "content": "We have introduced InsightBench, a benchmark with 31 diverse datasets that evaluates agents on their ability to perform end-to-end data analytics, including suggesting questions, interpreting answers, and summarizing insights. We ensured each dataset has clear goals and meaningful analysis as ground- truth to evaluate agents reliably. Using LLaMA-3-Eval, an open-source evaluator, we assessed how well the agents extracted insights compare to the ground-truth. We showed how our proposed agent, AgentPoirot, can perform data analytics from high-level goals using both open-source and closed- source models and showed that it outperforms existing approaches like Pandas Agent. We believe InsightBench will significantly drive advancements in data analytics. For future work, we look into expanding this benchmark to include more categories of data such as healthcare data, social media trends, environmental data, e-commerce analytics, and educational statistics."}, {"title": "7 Limitations", "content": "Benchmarks have the risk of reinforcing existing biases or oversimplifying complex decision processes. The design of InsightBench necessitates continuous reviews to ensure that the insights generated by agents do not perpetuate biases or lead to misinformed decisions. Additionally, agents' performance on this benchmark should be continually assessed against evolving business practices and technological advancements to maintain relevance and effectiveness. More discussions on limitations are presented in Appendix A."}, {"title": "A Limitations and Potential Societal Impact", "content": "InsightBench is designed to advance the field of Exploratory Data Analysis (EDA) by providing a challenging framework for evaluating open-ended multi-step data analytics tools and models. The dataset evaluates and helps build conversational data agents with significant potential to enhance decision-making and operational efficiencies in organizations, particularly for end-users who may not have deep technical expertise in data science. Yet, there are important considerations and limitations concerning the current data construction and the benchmark's application in the real-world setting.\nDiversity and Scope of Data Simulation. While InsightBench covers a spectrum of business areas and themes, the datasets are synthesized based on patterns observed in widely-used ServiceNow. Consequently, the synthetic \"flags\" are designed to mimic real-world anomalies and trends but may not capture genuine enterprise data's full complexity or unexpected behavior.\nExpanding the Benchmark. The current structure of InsightBench allows for scalable expansions to include more nuanced scenarios, themes, and more datasets that could involve unstructured data like reports, logs, or emails. Crucial information and actionable insights can also be derived from visual representations such as charts and plots. Further development could also integrate diverse business norms, the ability to analyze the plots and graphs from a visual context alongside language by the agents. Moreover, incorporating feedback from real-world deployments could help refine the datasets to simulate underlying business data better. Lastly, InsightBench allows interpreting data types and evaluating outputs for practical utility. Its adaptive framework, which evolves through multi-step interactive feedback, may align with agile development practices. This positions our InsightBench a future setup for integrating intelligence into the end-to-end software development process."}, {"title": "B Appendix: Overview", "content": "Our supplementary material includes the following sections:\n\u2022 Section A: Contains details of data accessibility, generation process, and a breakdown of domains covered by InsightBench.\n\u2022 Sectoin B: Contains results of some ablation experiments and qualitative examples.\n\u2022 Section C: Discusses relevant literature for our work.\n\u2022 Section D: Contains all the prompts used in our experiments."}, {"title": "B.1 Accessibility and Maintenance", "content": "Authors are committed to ensuring the dataset's regular upkeep and relevance, we will place a system for users to report issues or suggest updates. A feedback form will be available for users to contribute their input. We commit to actively reviewing these suggestions and making the necessary adjustments to the dataset. We also invite contributions from the community through pull requests on InsightBench's GitHub repository."}, {"title": "B.2 Example Protocol for Data Generation and Insight Planting", "content": "Here, we outline a practical case of how we created a flag dataset for an incidents table with a trend in which the duration of incident resolution increases over time:\n1. Schema Exporting and Standardization: Initially, the schema for the incidents table is exported and standardized by inspecting a demo instance. Fields such as \"number\", \"opened_at\", and \"closed_at\", among others, are defined."}, {"title": "2. Data Generation Parameters", "content": "Parameters for generating the data include factors that influence and define the trend. For this example, the parameters include a slope to model the trend in resolution time. Dates ranging from start date to end date are used to set the temporal context of the data. In the experiments section, we ablate and study the effect of the key parameters."}, {"title": "3. Synthetic Data Creation", "content": "Incident numbers are generated sequentially, caller IDs are randomly assigned, and opening times for incidents are sampled randomly within the date range."}, {"title": "4. Trend Implementation", "content": "The time to resolution is calculated using a linear model where the resolution time increases over the duration of the dataset. The parameter chosen in step 2 is used to model the linear function. The closing time of an incident is determined based on the resolution time and its opening time."}, {"title": "5. LLM Utilization", "content": "For fields that require diverse and realistic inputs like short descriptions of the incidents, an LLM generates descriptions based on the incident category, ensuring that the data retains an authentic and varied narrative quality."}, {"title": "B.3 Data Tables across themes", "content": "Here is an exhaustive and descriptive outline of the topics covered in the benchmark:\n\u2022 Incidents Management: Focuses on tracking, analyzing, and resolving workplace incidents. An extensive description of this data table is discussed afore-mentioned Sec 2.1.1.\n\u2022 User Management: Datasets in this theme are derived from sys_user system table of ServiceNow. This table tracks all user profiles within the platform. It contains information about each user, such as their roles, department affiliations, contact details, and activity status. Key fields include the user's ID, name, email, department, and last login time. This table is used in InsightBench for insights related to roles and permissions of employee agents, focusing on schedules and activity patterns.\n\u2022 Finance Expenses: Datasets in this topic examine detailed records of expenses to uncover patterns and optimize budget allocations and prioritization. Datasets are built from fm_expense_list, a system table that logs detailed entries of financial transactions and expenses as part of the financial manage- ment module. It includes data points like the expense amount, date, associated user, department, category, and processing status.\n\u2022 Inventory Management: Manages data regarding hardware assets and procurement, aiming to understand patterns in inventory systems. Datasets are derived from alm_hardware, a system table that manages records of all physical assets, particularly IT hardware. Fields in this table detail each asset's tag number assigned to the user, status, location, purchase date, and warranty expiration. It supports asset tracking, lifecycle management, and maintenance activities within an organization.\n\u2022 Enterprise Goal Management: Datasets in this theme evaluate the alignment of departmental performance with overarching goals, focusing on the effectiveness and achievement rates. Datasets are derived from sn_gf_goal system table of ServiceNow, which is essential for evaluating goal management efficiency and aligning departmental outputs with organizational objectives. Fields in this table consist of goal description, start and end dates, current status, owner, priority, and percentage completion."}, {"title": "B.4 List of Datasets and Problems", "content": "Table 4 shows the names of all the datasets in InsightBench along with their difficulty and category. Table 5 presents an example of a question-insight pair along with corresponding plots for each category."}, {"title": "B.5 Quality-check Interface", "content": "To ensure the high standards of data quality and relevance, InsightBenchhas implemented a compre- hensive quality-check process involving volunteer contributors. This process is facilitated through a specifically developed gradio interface, which guides the expert reviewers through a structured review of the ground-truth analysis notebooks included in the benchmark (Figure 5). The evaluation is divided into three main sections:"}, {"title": "B.6 Details of the proposed AgentPoirot", "content": "Here we describe the proposed baseline AgentPoirot in further detail. Figure 7 depicts our pipeline. Initially, AgentPoirot uses the dataset and its schema to obtain a set of root questions about the data, aligning with the user's Goal or Role. Each of the root questions can be injected into a Code Generation prompt to obtain and execute code, as well as obtain textual descriptions of the insight found. From these outputs, we generate follow-up questions and start the process again, obtaining a model that navigates the data in depth and in breadth to find interesting insights."}, {"title": "D Prompt Design", "content": "In this section, we illustrate the use of tailored prompts to enhance the analytical capabilities of our models for extracting insights and conducting advanced analytics."}, {"title": "PROMPT:", "content": "### Instruction:\nYou are trying to answer a question based on information provided by a data scientist."}, {"title": "Given the analysis:", "content": "<analysis>\n   <message>\n     {message}\n   </message>\n   {insights}\n </analysis>\nInstructions:\n* Based on the analysis and other information provided above, write an answer to the question enclosed with <question></question> tags.\n* The answer should be a single sentence, but it should not be too high level and should include the key details from justification.\n* Write your answer in HTML-like tags, enclosing the answer between <answer></answer> tags, followed by a justification between <justification></justification> tags.\n* Refer to the following example response for the format of the answer and justification."}]}