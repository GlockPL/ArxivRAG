{"title": "APS-USCT: Ultrasound Computed Tomography\non Sparse Data via AI-Physic Synergy", "authors": ["Yi Sheng", "Hanchen Wang", "Yipei Liu", "Junhuan Yang", "Weiwen Jiang", "Youzuo Lin", "Lei Yang"], "abstract": "Ultrasound computed tomography (USCT) is a promising\ntechnique that achieves superior medical imaging reconstruction resolu-\ntion by fully leveraging waveform information, outperforming conven-\ntional ultrasound methods. Despite its advantages, high-quality USCT\nreconstruction relies on extensive data acquisition by a large number\nof transducers, leading to increased costs, computational demands, ex-\ntended patient scanning times, and manufacturing complexities. To mit-\nigate these issues, we propose a new USCT method called APS-USCT,\nwhich facilitates imaging with sparse data, substantially reducing de-\npendence on high-cost dense data acquisition. Our APS-USCT method\nconsists of two primary components: APS-wave and APS-FWI. The APS-\nwave component, an encoder-decoder system, preprocesses the waveform\ndata, converting sparse data into dense waveforms to augment sample\ndensity prior to reconstruction. The APS-FWI component, utilizing the\nInversionNet, directly reconstructs the speed of sound (SOS) from the\nultrasound waveform data. We further improve the model's performance\nby incorporating Squeeze-and-Excitation (SE) Blocks and source encod-\ning techniques. Testing our method on a breast cancer dataset yielded\npromising results. It demonstrated outstanding performance with an av-\nerage Structural Similarity Index (SSIM) of 0.8431. Notably, over 82%\nof samples achieved an SSIM above 0.8, with nearly 61% exceeding 0.85,\nhighlighting the significant potential of our approach in improving USCT\nimage reconstruction by efficiently utilizing sparse data.", "sections": [{"title": "1 Introduction", "content": "Ultrasound Computed Tomography (USCT) is valued in the medical imaging\nlandscape for its non-invasive nature and the absence of harmful radiation. This\ntechnique harnesses the potential of ultrasound data, which can be interpreted\nusing either time-of-flight measurements (ray-based approaches) [13] or full wave-\nform data [18]. While ray-based USCT offers swift computational processing, it"}, {"title": "2 Method", "content": "A. Framework Overview: Fig. 2 shows the proposed framework, which is\ncomposed of three components: (1) APS-wave is an AI model that enhances\nsparse waveform data, increasing the density of samples (i.e., the sources or\nreceivers), called \u201cdense waveform\"; (2) APS-FWI is the other AI model for SOS\nmap reconstruction from the dense waveform; (3) the underline APS-physics is\nthe key enabler, which converts the SOS map label to dense waveform label. As\nsuch, we have an AI-Physics Synergy USCT framework, denoted as APS-USCT.\nIn the following, we will introduce each component in detail.\nB. APS-wave: The objective of APS-wave is to generate dense waveforms (i.e.,\nhigher sample density) from sparse ones obtained by fewer sources or receivers.\nAPS-wave contains two steps: (1) interleave the sparse waveform by inserting 0;\nas such, the interleaved sparse waveform will have the same dimension as the\ndense waveform; and (2) the interleaved waveform will be processed by a learn-\nable encoder-decoder system which converts sparse measurements into dense\nwaveforms to augment sample density.\nThe encoder-decoder system contains the forward propagation and backward\npropagation, as shown in Fig. 2. For forward propagation, the interleaved wave-\nform will go through a 15-layer encoder-decoder neural network to generate the"}, {"title": "C. APS-FWI", "content": ": APS-FWI is to reconstruct the SOS map from the augmented\nwaveform data, which contains two steps. First, we integrate a learnable source\nencoding model, as described by [19], at the forefront of the InversionNet, and\nany other baseline methods for fair comparison. This model is tasked with encod-\ning the predicted dense waveform through a random encoding vector. It aims to\napproximate the sound speed distribution via stochastic optimization with gra-\ndient descent. This process not only leverages the imaging operator's linearity\nfor computational efficiency but also enables a significant reduction in compu-\ntational demands. Second, the encoded results will be fed to InversionNet [22],\nwhich aims to solve the minimization problem from a given waveform data.\n\nmin 1/2N \u2211||\u03c6(D_n^w) \u2013 C_n||^2 .\n(1)\nwhere \u03c6_\u03b5(D^w_n) is the predicted SOS map on the nth sample by InversionNet \u03c6\nwith weights \u03b5 on input waveform data (D^w_n), and C_n is the label of the nth\nsample. The input D^w_n is a 3-D tensor D \u2208 R^{I\u00d7K\u00d7 J} and d_{ijk} \u2208 D corresponds to\nthe measurement data from the ith source, jth receiver, and kth time step. The\noutput \u03c6_\u03b5(D^w_n) and label C_n are 2-D tensor R^{x\u00d7y} corresponding to pixel values\nof SOS estimates over the field of view.\nKindly note that the InversionNet was originally designed for seismic recon-\nstruction in the geophysics domain, where the SOS maps describe the subsurface\nstructures. Although the fundamental FWI problem is the same, applying In-\nversionNet to the medical domain encounters new challenges in reconstructing\nthe details in the structure (e.g., tissue). We propose to bring attention to Inver-\nsionNet. Specifically, we optionally add the Squeeze-and-Excitation (SE) Blocks\nin each layer of the decoder in InversionNet. The experimental results will show\nthat the InversionNet with attention (i.e., SE blocks) can better reconstruct\ntissue, particularly for sparse data."}, {"title": "D. APS-physics", "content": ": To support the training process in APS-wave, the APS-\nphysics is applied to generate high-dimensional waveform data from the SOS\nlabels in the training dataset by using the acoustic wave equation [5]:\n\n\u2202^2u/\u2202t^2 = c(x)^2 \u2202^2u/\u2202x^2 + S(t) \u00b7 \u03b4(x \u2013 x_i).\n(2)\nwhere u(x, t) represents the pressure wave field, c is the speed of sound (i.e.,\nSOS) in the medium, t is time, and x is the spatial coordinates in the two-\ndimensional space. The term S represents the source function, for which we\nemploy a Ricker wavelet with 1MHz peak frequency, x_i represents the ith (i =\n1, 2, ..., N) source location coordinates, and \u03b4(\u00b7) is a spatial Dirac delta function."}, {"title": "3 Experiment", "content": "3.1 Dataset, Implementation, and Evaluation Protocol\nTo evaluate APS-USCT, we adopt 2D cross-sectional slices of SOS maps ex-\ntracted from anatomically realistic numerical breast phantoms (NBPs), which\nare constructed in [14] by adapting and extending validated tools from the Vir-\ntual Imaging Clinical Trial for Regulatory Evaluation (VICTRE) project [6] for\nuse in USCT virtual imaging studies. A few examples of 2D cross-sectional slices\nextracted from these NBPs are available from [1]. Each NBP corresponds to a\nspeed of sound (SOS) map, where the values were assigned randomly within re-\nalistic ranges, varying spatially. The training dataset contains 1,353 NBPs while\nthere are 41 NBPs in the testing set.\nThe AI and APS-physics in APS-USCT are implemented by using PyTorch\nand Python, respectively. Specifically, the encoder-decoder architecture is adopted\nfor APS-wave, InversionNet [22] with additional SE-blocks for APS-FWI, and\nForward Modeling algorithm [20] for APS-physics. Please refer to the supple-\nmentary for detailed training hyperparameters.\nWe apply two metrics for quantitative analysis: (1) structural similarity index\nmeasure (SSIM) [21] that reflects the structure of objects in the scene, and\n(2) peak signal-to-noise ratio (PSNR) [9] that is the approximate estimation of\nhuman perception of reconstruction quality.\nWe compare the quality of APS-USCT over three state-of-the-art method-\nologies: (1) InverstionNet [17], (2) USCT-Net [15], and (3) SRSS-Net [16]. Both\nUSCT-Net and SRSS-Net are based on U-Net, and they are designed for sparse\ndata. Unlike APS-USCT which generates dense waveforms, USCT-Net ensem-\nbles multiple low-quality SOS maps from waveforms taken by different sources.\nSRSS-Net uses an additional neural network to process the low-quality SOS\nmaps, which is similar to the concept of superresolution in computer vision. As\nInverstionNet did not consider data sparsity, we applied data interpolation meth-\nods for it, including (1) bicubic interpolation [8] (denoted Bicubi+InversionNet)\nand (2) nearest neighbor [12] (denoted Nearest+InversionNet)."}, {"title": "3.2 Main Results", "content": "A. Quantitative and Qualitative Comparison: Table 1 reports the results\nfor the waveform captured by 32 sources and 32 receivers, where the data sparsity"}, {"title": "B. Results Visualization", "content": ": We visualize the SOS maps of obtained by differ-\nent image reconstruction approaches in Fig. 3. We selected two breast types (in\nterms of the percentage of fibroglandular tissue [4,2]) for comparison: (1) fatty\nbreast and (2) dense breast. According to [16], dense breast tissue poses greater\nchallenges in image reconstruction. We draw a crucial observation from the visu-"}, {"title": "3.3 Ablation Studies", "content": "A. Validity of APS-wave: Fig. 4(a) reports the comparison of APS-wave\nover the interpolation methods. The column D_{in} \u2192 D_{out} shows the change of\n(#sources, # receivers) pair for data acquisition. The column \"similarity\" is the\ncosine similarity between the obtained data and ground truth by APS-physics.\nWe made several observations. First, traditional interpolation methods demon-\nstrate limited efficiency. Both Bicubic and Nearest have similarity lower than 0.4,\nexcept (32, 32) \u2192 (32, 64), while APS-wave can achieve over 0.9 of similarity\nwhen D_{in} is (32, 32). Second, the higher similarity can be achieved by increasing\nD_{in} or decreasing D_{out}. For all methods, the best performance is achieved by\n(32, 32) \u2192 (32, 64), and the worst one is by (4, 4) \u2192 (32, 512). Third, when D_{in}"}, {"title": "B. Effectiveness of SE-Block with APS-FWI", "content": ": Results in Fig. 4 (b) show\nthat for input data with extremely high sparsity, say over 75%, InversionNet with\nSE-Block can outperform the one without SE-block. However, with high-density\ndata, it becomes counterproductive. Consequently, in APS-FWI, this module is\noptional activated for sparse data. This approach provides more options for\ndesigners when they need to handle data with different sparsities."}, {"title": "C. Resource-Performance Co-Exploration for APS-USCT", "content": ": Last, we\nconduct a set of experiments for resource-performance co-exploration on APS-\nUSCT. As for comparison, we implement InversionNet [17] using dense data for\nboth training and inference, indicating more hardware resources for data acqui-\nsition. Here, the hardware cost is formulated as the number of elements used in\nthe system, which is the sum of source numbers and receiver numbers.\nThe exploration results are reported in Table 2. We have several interesting\nobservations from the table. First, with the same hardware cost, APS-USCT can\ncontinuously improve SSIM from 0.7455 to 0.8431 when APS-wave generates a\nmore dense intermediate waveform. This calls back to the results in Fig. 4(a),\nwhere APS-wave shows high stability. On the other hand, for InversionNet, such\nimprovements rely on increasing the number of hardware elements. Second, the\ncomparison between APS-USCT and InversionNet shows that APS-USCT can\nachieve 2.5x hardware cost reduction with merely 0.0007 SSIM degradation.\nWhen the hardware cost reduction increases to 8.5\u00d7, the SSIM degradation is"}, {"title": "4 Conclusion", "content": "We developed the APS-USCT framework to improve image reconstruction from\nsparse ultrasound waveform, integrating AI with physical principles. APS-USCT\nfeatures two key modules: APS-wave and APS-FWI. APS-wave improves sample\ndensity while maintaining waveform integrity. Following APS-wave, the APS-\nFWI module, enhanced by source coding and SE-Blocks, significantly improves\nreconstruction accuracy. This dual-module USCT method not only ensures an\naccurate tissue characterization with minimal hardware cost but also underscores\nAPS-USCT's broad applicability in advanced ultrasound imaging technologies."}]}