{"title": "REVISITING ATTENTION FOR MULTIVARIATE TIME SERIES FORECASTING", "authors": ["Haixiang Wu"], "abstract": "Current Transformer methods for Multivariate Time-Series Forecasting (MTSF) are all based on the conventional attention mechanism. They involve sequence embedding and performing a linear projection of Q, K, and V, and then computing attention within this latent space. We have never delved into the attention mechanism to explore whether such a mapping space is optimal for MTSF. To investigate this issue, this study first proposes Frequency Spectrum attention (FSatten), a novel attention mechanism based on the frequency domain space. It employs the Fourier transform for embedding and introduces Multi-head Spectrum Scaling (MSS) to replace the conventional linear mapping of Q and K. FSatten can accurately capture the periodic dependencies between sequences and outperform the conventional attention without changing mainstream architectures. We further design a more general method dubbed Scaled Orthogonal attention (SOatten). We propose an orthogonal embedding and a Head-Coupling Convolution (HCC) based on the neighboring similarity bias to guide the model in learning comprehensive dependency patterns. Experiments show that FSatten and SOatten surpass the SOTA which uses conventional attention, making it a good alternative as a basic attention mechanism for MTSF. The codes and log files will be released at:\nhttps://github.com/Joeland4/FSatten-SOatten.", "sections": [{"title": "1 Introduction", "content": "Multivariate Time Series Forecasting (MTSF) is extensively applied in real-world scenarios such as finance, electricity, and transportation. Benefiting from the attention mechanism's [1] ability to effectively capture both long- and short-term dependencies, many Transformer-based methods have demonstrated remarkable performance. These methods mainly include the Temporal Transformer, which evolves from applying attention between time steps [2] [3] [4] to applying attention between subseries [5] [6] [7], and the Variate Transformer, which explicitly models the correlations between variates though attention [8] [9].\nTemporal and Variate Transformers, as shown in Figure 2, primarily apply attention mechanisms to time series sequences and have become mainstream, with many subsequent studies [10] [11] building upon these architectures. We aim to understand where these correlations between sequences manifest. In the attention mechanism, sequences are mapped to a learnable space by embedding and linear projection, and then the correlations are calculated within this latent space. Although we cannot provide physical interpretations for the learned characteristics of black-\nbox neural networks, it is worth considering whether the dependency capturing within this latent space is optimal."}, {"title": "2 Preliminaries", "content": "To explore the interpretability of time-series attention and make further improvements for MTSF, we propose Frequency Spectrum attention (FSatten), which is based on the frequency domain space. The consideration is that dependencies between non-stationary sequences are complex and can be synchronous or asynchronous at different frequencies. It is appropriate to consider these dependencies from the frequency domain perspective, as previous works have made improvements [6] [12]. In FSatten, Fourier transform is utilized for the embedding, and Query and Key are projected by a proposed Multi-head Spectrum Scaling (MSS) instead of the conventional linear projection. MSS scales amplitude for different frequency components under each of the multiple heads, identifying clear frequency spectral relationships between sequences.\nExperimental results in Figure 1 and Table 1 show that without modifying the architecture, simply replacing the conventional attention with FSatten yields significant improvement over the state-of-the-art (SOTA), especially for scenarios with significant periodicity. This suggests that the conventional attention mechanism is not optimal for MTSF. However, the frequency domain space cannot meet all the characteristics of different scenes, indicating that there is more to explore. Also, FSatten is good for capturing same-frequency correlations between variates but may not be highly appropriate for Temporal Transformers, as sequences split from one variate naturally tend to exhibit the same periodic frequency.\nTo find a more general method for various scenarios, we propose Scaled Orthogonal attention (SOatten), which creates a learnable orthogonal transformation beyond the Fourier transform. In SOatten, we propose a Head Coupling Convolution (HCC) to guide the updating of learnable orthogonal spaces by leveraging the similarity between adjacent sequences. Experiments show that SOatten enhances overall performance compared to FSatten when applied to Variate Transformer, iTransformer [9] and makes significant improvements when applied to a general Temporal Transformer, PatchTST [7], showcasing stronger adaptability. We hope the proposed methods may inspire future work in time series analysis and offer contributions to other deep learning fields.\nThe main contributions of this work are as follows:\n\u2022 We propose FSatten, a more interpretable and effective model than conventional attention for MTSF, which replaces the learnable latent space by Frequency domain.\n\u2022 Through the proposed MSS mapping for Query and Key, FSatten accurately identifies the frequency correlations between sequences. This specific dependency is more effective than what is provided by conventional linear projections.\n\u2022 We propose SOatten, a more general attention than FSatten which provides a learnable orthogonal latent space facilitated by a designed HCC module for capturing comprehensive dependencies.\n\u2022 On six real-world long-term forecasting benchmarks, our FSatten and SOatten outperform the SOTA method which utilizes conventional attention by an overall of 8.1% and 21.8% on MSE, demonstrating their superior effectiveness for MTSF.\nA Multivariate Time Series (MTS) sampling with look back window L is denoted by $X = \\{X_1, ..., X_L \\} \\in R^{C\\times L}$, where each $x_1$ at time step $l$ is a vector of dimension C. The task is to forecast T future values $\\{X_{L+1},..., X_{L+T}\\}$. In this work, the proposed FSatten and SOatten are applied to two SOTA Transformers to compare with conventional attention,"}, {"title": "2.1 Temporal Transformer", "content": "as shown in Figure 2 Left: (1) Variate Transformer, iTransformer [9], and (2) Temporal Transformer, PatchTST [7]. Many subsequent approaches [10] [11] are based primarily on these two mainstream architectures. Detailed illustrations are as follows:\nThe initial Transformer-based MTS models take time steps as tokens and apply temporal attention between them. Subsequent works demonstrated that temporal attention at a sub-series level with fewer tokens is more effective and can greatly reduce the complexity. PatchTST [7] provides a general paradigm of the Temporal Transformer at the sub-series level. In PatchTST, each of the C variates $X^{(i)} = \\{x_1^{(i)}, ... x_L^{(i)} \\} \\in R^{1 \\times L}$ is converted to sub-series Patches $X^{(i)} = \\{x_1^{(i)}, ...x_N^{(i)} \\} \\in R^{P \\times N}$, where $N = \\frac{L-P}{S}+1$, P is length of patches and S is the stride - the non-overlapping region between two consecutive patches. Temporal attention is applied to capture the dependencies between patches of each variate. The $X^{(i)}$ is first embedded to tokens $Z^{(i)}$, $Z^{(i)} = W_FX^{(i)}$, Where $W_F \\in R^{D \\times P}$ and D is the number of dimensions. Then the attention weight is calculated:\n$A_h^{(i)} = Sofmax(\\frac{((Z^{(i)}W_Q)^T)((Z^{(i)}W_K))}{\\sqrt{d_k}})(Z^{(i)}W_V)$ \nWhere $\\{W_Q,W_K,W_V\\} \\in R^{D\\times \\frac{D}{H}}$ and $H$ is number of attention heads. The mapping to latent space is $\\Theta_P = \\{W_F, \\{W_Q,W_K,W_V\\} \\}$."}, {"title": "2.2 Variate Transformer", "content": "The Temporal Transformer directly follows the paradigm in NLP. But unlike natural language, MTS has multiple parallel sequence inputs. Variate Transformer explicitly models the complex correlations between variable sequences. Typically, iTrasformer [9] embed the whole time series of each variate $X^{(i)}$ independently into a (variate) token as $Z_v = XW_V$, where $W_v \\in R^{L\\times D}$. Then it adopts attention to multivariate correlations as follows:\n$A_h = Sofmax(\\frac{((Z_vW_Q)^T)((Z_vW_K))}{\\sqrt{d_k}})(Z_vW_V)$\nThe mapping to latent space of variate attention is $\\Theta_V = \\{W_v, \\{W_Q,W_K,W_V\\} \\}$.\nWhether temporal or variate as mentioned above, both transform sequences of MTS to a latent space to provide the dependency pattern between sequences. The point of our research is to demonstrate whether the mapping to latent space under conventional attention is optimal or if we can find a better one for MTSF."}, {"title": "3 FSatten", "content": "FSatten is an innovative attention mechanism that we propose to explore the effectiveness of conventional attention. The intuitive difference from the conventional attention, as depicted in Figure 3, is that FSatten replaces the embedding by a Fourier transform and the linear projection for the query and key by a proposed MSS."}, {"title": "3.1 Workflow", "content": "We apply the FSatten to Variate Transformer for MTSF. As shown in Figure 3 right, each discrete variate sequence of the input X is first transformed by the Fast Fourier Transform (FFT) [13], which efficiently computes the Discrete Fourier Transform (DFT) from the time domain to the complex frequency domain as:\n$F_k = \\sum_{t=0}^{L} X_te^{-i(2\\pi/L)kt}, 0 \\leq k \\leq F$\nHere, i is the imaginary unit, and the exponential term represents the Fourier basis associated with the different k frequencies. The value of F is typically half the number of data points L in FFT. According to our consideration,"}, {"title": "3.2 MSS", "content": "correlations can be made up of associated frequency components with different phases. Thus, we extracted the amplitudes of different frequencies from the complex domain as follows:\n$A_k = |X_F| = \\sqrt{Re(X_F)^2 + Im(X_F)^2}$\nWhere $Re$ represents the real part of $X_F$, and $Im$ represents the imaginary part. We then apply the MSS module for the projection of queries and keys, replacing the conventional linear projection. The aim is to compare the learnable latent space for generating attention weights with the fixed frequency domain space. Since predictions are made in the time domain, the embedding and linear projection for the value remain unchanged.\n$Q = MSS_Q(A_k), K = MSS_K(A_k), V = Linear_V (Emb(X))$\nAfter the multi-head dot product, the subsequent Feed-Forward Network (FFN) provides complicated representations by adding random noise for each variate token [14]."}, {"title": "4 SOatten", "content": "Motivation. Indeed, experiments show that FSatten outperforms conventional attention, but its performance across six real-world datasets exhibits significant variance, as shown in Figure 1 and Table 1. This suggests that a fixed frequency domain mapping may not be universally applicable. Determining the optimal configuration manually for each scenario is challenging, given the numerous unexplained physical transformations. Therefore, we aim to further develop FSatten to design a method with better generalization capabilities. Furthermore, FSatten may not be fully compatible with Temporal Transformers, considering that sequences derived from a single variate naturally exhibit identical periodic frequencies."}, {"title": "4.1 Method", "content": "This is a research direction with many possible approaches, but the most straightforward idea is to extend by leveraging the orthogonality of the Fourier transform. Therefore, we propose SOatten, as depicted in Figure 5, which improves the frequency domain to a more general orthogonal domain through a learnable orthogonal transformation, described as:\n$Orth(X) = W^T X, where W^TW = I$"}, {"title": "4.2 HCC", "content": "In fact, the orthogonality of the learnable space is not guaranteed during backpropagation for parameter updates. We could apply a measure such as QR decomposition, but this could result in the loss of some gradient information. We made a different trade-off for MTSF, considering that the scale of models that match the size of the dataset is usually not very large, with relatively few layers. Therefore, we only perform orthogonal initialization for the embedding, rather than enforcing a completely orthogonal space during backpropagation. Subsequently, SOatten also applies the MSS projection for query and key and applies linear projection for the value. Additionally, in dot product attention, we propose a Head Coupling Convolution (HCC) module operating on the heads of attention weights, which serves as an important 'guidance' for the mapping space learning of SOatten.\nFSatten provides sequence dependencies based on explicit spectral information, but extending this to a learnable orthogonal space makes it difficult to effectively determine valid characteristics as a defined periodicity in FSatten. In other words, data-driven approaches that learn an effective orthogonal space without any restrictions have requirements for the size and distribution of the dataset.\nI propose a general enhancement method called Head Coupling Convolution (HCC), which leverages the constraint of similarity between neighboring sequences to guide the model in exploring feature spaces. Specifically, HCC involves performing convolution operations on the attention weights within the dot product attention mechanism as:\n$Atten = Softmax(\\frac{Q K^T}{\\sqrt{dk}})$\n$HCC(Atten) = ReLU \\{Conv_{H\\rightarrow H}(Atten, stride = 1, kernel\\_size = K)\\}$\nWhere $Conv_{H\\rightarrow H}$ is channel fusion convolution that maps from H heads to H heads and padding is necessary for keeping the size of the weight matrix.\nFor most time-series data, contrastive learning methods [15] [16] [17] [18] have demonstrated the effectiveness of assumption: neighboring similarity, the similarity between sequences of the same time series decreases as the time lag increases. In fact, similar variates are arranged together in most datasets (see details in Appendix A.2).\nBy applying a convolution operation to the attention weights, more critical correlated patterns between local neighboring sequences are extracted, guiding the parameter updates in the feature space during backpropagation. The diverse features extracted by H heads are all predicated on neighboring similarity, multi-head coupling helps to obtain more precise associative features than single-channel convolution."}, {"title": "5 Experiments", "content": "Navigation. In section 5.1, We show the forecasting results of FSatten and SOatten. In section 5.2, I conduct ablation studies to verify the effectiveness of MSS and HCC. In section 5.3, We make a visualization analysis for FSatten and SOatten, comparing them with conventional attention. Hyperparameter sensitivity analysis is presented in Section 5.4. Moreover, model efficiency analysis is presented in Appendix C.3.\ncSetting. We extensively evaluate the proposed FSatten and SOatten on six real-world datasets, including ECL, ETT (4 subsets), Exchange, Traffic, Weather [5], and Solar-Energy [19]. Detailed dataset descriptions are provided in Appendix B.1. We choose 9 well-known forecasting models as our baselines, including (1) Transformer-based methods: Autoformer [5], FEDformer [6], Crossformer [8], PatchTST [7], and iTransformer [9]; (2) Linear-based methods: DLinear [20], TiDE [21]; and (3) CNN-based methods: SCINet [22], TimesNet [23]. The experimental setting is the same as in iTransformer [9], that the input length L = 96 and the output length and T = {96, 192, 336, 720}."}, {"title": "5.1 Long-term MTSF", "content": "Compared to the baselines presented in Table 1, FSatten, based on the Variate Transformer, shows overall better forecasting performance than the SOTA, which uses conventional attention mechanisms. Particularly for datasets with more pronounced periodicity, such as on ECL, FSatten significantly improves performance by an overall 9.0% compared to SOTA and exhibits greater stability for longer prediction sequences. These improvements demonstrate that FSatten effectively captures the accurate correlation at the same frequency, which is more suitable for application in Transformers for MTSF.\nPeriodicity is one of the most fundamental characteristics of time series, but not all datasets exhibit strong periodicity. Thus, as a more general approach that can be adapted to both Temporal and Variate transformers, SOatten achieves more"}, {"title": "5.2 Ablation Studies", "content": "To validate the effectiveness of the components of FSatten and SOatten, detailed ablation experiments are conducted. The effectiveness of the MSS mapping module, used in both methods, is compared with that of applying a linear mapping to FSatten and SOatten, as shown in Figure 6. MSS significantly outperforms the fully connected layer, corroborating its ability to identify more accurate associated components. In Section 5.3, a visual interpretation is provided to explain in detail.\nSecondly, we validate the effectiveness of the HCC module in SOatten in Figure 7. The HCC is an important design for SOatten, significantly enhancing forecasting performance. Especially under the Temporal Transformer, the HCC demonstrates better generalizability. These results prove that neighboring similarity is crucial for the formation of an effective orthogonal mapping space and the generation of accurate attention weights. In Section 5.4, we conduct further hyperparameter analysis on the kernel size K of HCC."}, {"title": "5.3 Visualized Analysis", "content": "First, we make visualizations of generated attention matrices and analyze the advantages of the proposed two attention mechanisms. In the upper part of Figure 8, under the Variate Transformer, SOatten and FSatten generate smaller ranges but more refined weight values than the conventional attention applied by the SOTA, iTransformer (compare the value range on the right side of heatmaps). There are three main points of analysis:\n(1) In the generated attention weight maps, the patterns of the conventional attention and FSatten show similarities, presenting dependencies that are based on the sequence periodicity. However, FSatten significantly reflects complex"}, {"title": "5.4 Hyperparameter Sensitivity", "content": "Compared to conventional attention, the main hyperparameter differences are in the dimension size of the orthogonal mapping space F for MSS and the kernel size K for HCC, as shown in Figures 3 and 5. For FSatten, the F frequency components are orthogonal and are typically set to $F = \\frac{(L+1)}{2}$. In contrast, SOatten orthogonally initializes the mapping weights, and the dimension size F of the mapping spaces for Query and Key are set separately from that of the Value (same with conventional attention). Sensitivity experiments on F, depicted in Figure 9, demonstrate that the performance of the proposed attention mechanisms is not coincidental. When optimized, SOatten's F is significantly smaller than the do and dk of conventional attention, thereby enhancing the model's efficiency to some extent (detailed efficiency analysis is presented in Appendix C.3)."}, {"title": "6 Disscussion", "content": "For the MTSF problem, this paper proposes two innovative attention that are superior to conventional attention. We started from the frequency domain and made preliminary explorations based on the mainstream Temporal and Variate Transformers. We will soon supplement comprehensive experimental results of applying FSatten and SOatten to new architectures released this year, such as TimeXer [24], and the Cross Spatio-Temporal architecture (another work of ours). Constrained by our resources, we encourage the community to explore solutions to these problems together.\nwe outline three promising directions for future work:\nMapping spaces based on other physical properties, as there exist trend, seasonality, and other characteristics in addition to periodicity.\nCombine with state-space models, especially for datasets with a large number of variates, The advantage of current state-space models such as Mamba [25] [26] [27] and TTT [28] is that they can compress the larger variable background into a fixed hidden state, selectively retaining the most important information under global variates, which is very important for scenarios with multiple classes of variable information, like in Traffic. Of course, this is an improved method that we will launch soon.\nApply to other fields, Such as CV (Computer Vision), NLP (Natural Language Processing), and motion planning, which are all directions that can explore the proposed FSatten and SOatten."}, {"title": "A Related Work", "content": "A.1 Transformer-based MTSF methods\nAs the Transformer continues to make breakthroughs in the fields of NLP and CV, applying the attention mechanism to time series analysis outperforms TCN- and RNN-based methods. LogTrans [3] proposes LogSparse attention, which focuses on the previous step at exponential intervals in each step to break the memory bottleneck. Reformer [29] employs position-sensitive hashing (LSH) to divide tokens into several buckets and perform attention within each bucket. Informer [2] introduces ProbSparse attention, a mechanism that calculates the top-u leading queries based solely on the measured query sparsity. Autoformer [5] constructs a series-level connection based on the process similarity derived by series periodicity and introduces a decomposition forecasting architecture. FEDformer [6] starts from the frequency domain. It selects a random subset of frequency components and multiplies them by learnable complex parameters to learn a sparse representation of time series. PatchTST [7] adopts a general patch-based series representation inspired by the Vision Transformer [30]. Crossformer [8] explicitly captures the cross-time and cross-variate dependencies through two-stage attention and a renovated architecture. iTransformer[9] embeds the time points of individual series into variate tokens which are utilized by the attention mechanism to capture multivariate correlations. These methods have continuously improved from the model architecture and the application of Attention. However, none of them have delved into the internal workings of the attention mechanism to explore whether conventional attention is optimal for MTSF.\nA.2 Neighboring Similarity\nFor the sub-sequence patches segmented from the same variate, in most cases, they meet the assumption of neighboring similarity: the similarity between sequences of the same time series decreases as the time lag increases. We found that most MTS datasets such as widely used ETT, ECL, and Weather, also exhibit certain degrees of neighboring similarity between variates. As shown in Figure 11, In the Weather dataset, adjacent variates have similar curve shapes. The attention weights and forecasting performance have verified the significance of the neighboring similarity bias for SOatten."}, {"title": "B Experimental Details", "content": "B.1 Dataset Description\nWe conducted experiments on six real-world datasets to evaluate the performance of the proposed FSatten and SOatten. The ETT series includes data on seven oil and load features of electricity transformers. Traffic contains hourly road occupancy rates recorded by San Francisco freeway sensors from 2015 to 2016. Weather contains meteorological observations including temperature, humidity, wind speed, and precipitation. Exchange contains panel data on daily exchange rates from 8 countries, spanning from 1990 to 2016. ECL contains hourly electricity consumption data (in kWh) for 321 clients from 2012 to 2014. Solar-Energy contains hourly solar power output data collected from 137 PV plants in Alabama in 2007. We provide the dataset details in Table 3."}, {"title": "B.2 Implementation Detials", "content": "All the experiments are implemented in PyTorch [31] and conducted on a single NVIDIA RTX 3090 24GB GPU. To validate our approach, we only replace the conventional attention with FSatten and SOatten without changing any original parameter settings in iTransformer and PatchTST. PatchTST includes 3 encoder layers with a head number of H = 16 and a latent space dimension of D = 128 and a prediction head dimension F = 256. For ETT, parameters of a reduced size are used (H = 4, D = 16, and F = 128) to reduce the risk of overfitting. A dropout rate of 0.2 is applied in the encoders for all experiments. iTransformer contains \\{2, 3, 4\\} layers with a head number of H = 8, The dimension of series representations D = \\{256, 512\\}.The only difference is the addition of two hyperparameters F = \\{32, 49\\} and kernel size K = \\{1,3\\} in HCC."}, {"title": "C Full Results", "content": "C.1 Long-term Forecasting\nWe give the full forecasting results in Table 4, we compare SOatten and FSatten based on Variate Transformer with benchmarks, and we can see that SOatten and FSatten are in the top two on all six real-world datasets. Table 5 shows the full results of directly comparing SOatten with the conventional attention based on both Variate Transformer and Temporal Transformer, SOatten achieved a comprehensive surpass.\nC.2 Ablations\nTable 6 shows the complete results of ablation for MSS. It can be seen that replacing MSS with a Linear map significantly reduced the forecasting performance of FSatten and SOatten, indicating that MSS is an essential design. Secondly, Table 7 shows the complete results of ablation for HCC. The HCC module is very important for the orthogonal space learning and accurate generation of attention weights in SOatten."}, {"title": "C.3 Efficiency", "content": "Figure 12 shows the efficiency results of FSatten and SOatten, compared with conventional attention under both the Temporal Transformer and the Variate Transformer. Since FSatten and SOatten were only replaced in both Transformers, with only the dimension size of MSS F and an additional convolutional operation, there is no significant difference in efficiency compared to conventional attention. It can be observed that FSatten has slightly improved efficiency by replacing the original linear mapping of Query and Key with a Fast Fourier Transform [13]. Secondly, because MSS is a Hadamard product, it can enhance the efficiency relative to fully connected layers to some extent. Although SOatten has an additional convolution layer, which significantly improves predictive performance, its memory usage and running time are slightly inferior to conventional attention."}, {"title": "D Showcases", "content": "Figure 13 shows the attention weight values and corresponding predictions using various attention methods under the Variate Transformer. FSatten captures more accurate inter-variable frequency dependencies. SOatten learns association patterns based on physical characteristics in addition to sequence periodicity. Furthermore, by comparing the results of SOatten without HCC, it can be intuitively found that although it can learn various patterns, it cannot integrate multiple patterns accurately. The corresponding predictions are like a collage based on two patterns. This reversely proves the effectiveness of HCC. In addition, through numerical analysis of the attention weight matrix, the weight matrix of conventional attention is of low rank (19), while the FSatten and SOatten we proposed are full rank (21), showing the diversity when aggregating features from values. Moreover, by calculating the condition number, it is found that the condition numbers of the weight matrices generated by FSatten and SOatten are 1,519 and 1,480, respectively, which are much smaller than the conventional attention's condition number 78,596,560. This indicates that FSatten and SOatten can effectively capture more stable and reliable correlations between sequences than conventional attention. Condition Number Calculation Formula: The condition number of a matrix A is calculated as the ratio of the maximum singular value to the minimum singular value:\n$\\kappa(A) = \\frac{\\sigma_{max}(A)}{\\sigma_{min}(A)}$\nFigure 15 demonstrates the attention weight values and corresponding forecasting results using SOatten under the Temporal Transformer. For the sub-sequences within the same variate, it can be observed that the attention weights generated by conventional attention are very uniform on ETTh1, and exhibit a very singular association on ETTm1. In contrast, SOatten is able to capture more comprehensive and precise dependency patterns compared to conventional attention."}]}