{"title": "REVISITING ATTENTION FOR MULTIVARIATE TIME SERIES FORECASTING", "authors": ["Haixiang Wu"], "abstract": "Current Transformer methods for Multivariate Time-Series Forecasting (MTSF) are all based on\nthe conventional attention mechanism. They involve sequence embedding and performing a linear\nprojection of Q, K, and V, and then computing attention within this latent space. We have never\ndelved into the attention mechanism to explore whether such a mapping space is optimal for MTSF.\nTo investigate this issue, this study first proposes Frequency Spectrum attention (FSatten), a novel\nattention mechanism based on the frequency domain space. It employs the Fourier transform\nfor embedding and introduces Multi-head Spectrum Scaling (MSS) to replace the conventional\nlinear mapping of Q and K. FSatten can accurately capture the periodic dependencies between\nsequences and outperform the conventional attention without changing mainstream architectures. We\nfurther design a more general method dubbed Scaled Orthogonal attention (SOatten). We propose\nan orthogonal embedding and a Head-Coupling Convolution (HCC) based on the neighboring\nsimilarity bias to guide the model in learning comprehensive dependency patterns. Experiments show\nthat FSatten and SOatten surpass the SOTA which uses conventional attention, making it a good\nalternative as a basic attention mechanism for MTSF. The codes and log files will be released at:\nhttps://github.com/Joeland4/FSatten-SOatten.", "sections": [{"title": "Introduction", "content": "Multivariate Time Series Forecasting (MTSF) is extensively applied in\nreal-world scenarios such as finance, electricity, and transportation. Ben-\nefiting from the attention mechanism's [1] ability to effectively capture\nboth long- and short-term dependencies, many Transformer-based meth-\nods have demonstrated remarkable performance. These methods mainly\ninclude the Temporal Transformer, which evolves from applying attention\nbetween time steps [2] [3] [4] to applying attention between subseries\n[5] [6] [7], and the Variate Transformer, which explicitly models the\ncorrelations between variates though attention [8] [9].\nTemporal and Variate Transformers, as shown in Figure 2, primarily\napply attention mechanisms to time series sequences and have become\nmainstream, with many subsequent studies [10] [11] building upon these\narchitectures. We aim to understand where these correlations between\nsequences manifest. In the attention mechanism, sequences are mapped\nto a learnable space by embedding and linear projection, and then the\ncorrelations are calculated within this latent space. Although we cannot\nprovide physical interpretations for the learned characteristics of black-\nbox neural networks, it is worth considering whether the dependency capturing within this latent space is optimal."}, {"title": "Preliminaries", "content": "A Multivariate Time Series (MTS) sampling with look back window L is denoted by $X = {X_1, ..., X_L } \\in R^{C\\times L}$, where\neach x1 at time step l is a vector of dimension C. The task is to forecast T future values ${X_{L+1},..., X_{L+T}}$. In this\nwork, the proposed FSatten and SOatten are applied to two SOTA Transformers to compare with conventional attention,"}, {"title": "Temporal Transformer", "content": "The initial Transformer-based MTS models take time steps as tokens and apply temporal attention between them.\nSubsequent works demonstrated that temporal attention at a sub-series level with fewer tokens is more effective\nand can greatly reduce the complexity. PatchTST [7] provides a general paradigm of the Temporal Transformer at\nthe sub-series level. In PatchTST, each of the C variates $X = {x^{(i)}_1,...x^{(i)}_L } \\in R^{1\\times L}$ is converted to sub-series\nPatches $X^{(i)} = {x^{(i)}_{1:P} ,...x^{(i)}_{N:L} } \\in R^{P\\times N}$, where $N = \\frac{(L-P)}{S} + 2$, P is length of patches and S is the stride - the\nnon-overlapping region between two consecutive patches. Temporal attention is applied to capture the dependencies\nbetween patches of each variate. The $X^{(i)}$ is first embedded to tokens $Z^{(i)}$ , $Z = W_FX^{(i)}$, Where $W_F \\in R^{D\\times P}$ and D\nis the number of dimensions. Then the attention weight is calculated:\n$A^{(i)}_h = Sofmax(\\frac{((Z^{(i)}_p)W^Q_Q)((Z^{(i)}_p)W^K_K)^T)}{\\sqrt{d_k}})((Z^{(i)}_p)W^V_V)$\nWhere ${W^{Q,K,V}} \\in R^{D\\times \\frac{D}{H}}$ and His number of attention heads. The mapping to latent space is $\\Theta_p$\n=${W_F, {W_Q,K,V}}$."}, {"title": "Variate Transformer", "content": "The Temporal Transformer directly follows the paradigm in NLP. But unlike natural language, MTS has multiple\nparallel sequence inputs. Variate Transformer explicitly models the complex correlations between variable sequences.\nTypically, iTrasformer [9] embed the whole time series of each variate $X^{(i)}$ independently into a (variate) token as\n$Zv = X^{(i)}W_V$, where $W_V \\in R^{L\\times D}$. Then it adopts attention to multivariate correlations as follows:\n$A_h = Sofmax(\\frac{(Z_vW^Q)(Z_vW^K)^T}{\\sqrt{d_k}})(Z_vW^V)$\nThe mapping to latent space of variate attention is $\\Theta_V =  {W_V, {W_Q,K,V}}$.\nWhether temporal or variate as mentioned above, both transform sequences of MTS to a latent space to provide the\ndependency pattern between sequences. The point of our research is to demonstrate whether the mapping to latent space\nunder conventional attention is optimal or if we can find a better one for MTSF."}, {"title": "FSatten", "content": "FSatten is an innovative attention mechanism that we propose to explore the effectiveness of conventional attention.\nThe intuitive difference from the conventional attention, as depicted in Figure 3, is that FSatten replaces the embedding\nby a Fourier transform and the linear projection for the query and key by a proposed MSS."}, {"title": "Workflow", "content": "We apply the FSatten to Variate Transformer for MTSF. As shown in Figure 3 right, each discrete variate sequence\nof the input X is first transformed by the Fast Fourier Transform (FFT) [13], which efficiently computes the Discrete\nFourier Transform (DFT) from the time domain to the complex frequency domain as:\n$F_k(X) = \\sum_{t=0}^{L-1} Xe^{-i(2\\pi/L)kt}, 0 \\leq k \\leq F$\nHere, i is the imaginary unit, and the exponential term represents the Fourier basis associated with the different k\nfrequencies. The value of F is typically half the number of data points L in FFT. According to our consideration,"}, {"title": "MSS", "content": "correlations can be made up of associated frequency components with different phases. Thus, we extracted the\namplitudes of different frequencies from the complex domain as follows:\n$A_k = |X_F| = \\sqrt{Re(X_F)^2 + Im(X_F)^2}$\nWhere Re represents the real part of $X_F$ and Im represents the imaginary part. We then apply the MSS module for the\nprojection of queries and keys, replacing the conventional linear projection. The aim is to compare the learnable latent\nspace for generating attention weights with the fixed frequency domain space. Since predictions are made in the time\ndomain, the embedding and linear projection for the value remain unchanged.\n$Q = MSS_Q(A_k), K = MSS_K(A_k), V = Linear_V(Emb(X))$\nAfter the multi-head dot product, the subsequent Feed-Forward Network (FFN) provides complicated representations\nby adding random noise for each variate token [14]."}, {"title": "SOatten", "content": "Motivation. Indeed, experiments show that FSatten outperforms conventional attention, but its performance across six\nreal-world datasets exhibits significant variance, as shown in Figure 1 and Table 1. This suggests that a fixed frequency\ndomain mapping may not be universally applicable. Determining the optimal configuration manually for each scenario\nis challenging, given the numerous unexplained physical transformations. Therefore, we aim to further develop FSatten\nto design a method with better generalization capabilities. Furthermore, FSatten may not be fully compatible with\nTemporal Transformers, considering that sequences derived from a single variate naturally exhibit identical periodic\nfrequencies."}, {"title": "Method", "content": "This is a research direction with many possible approaches, but the most straightforward idea is to extend by leveraging\nthe orthogonality of the Fourier transform. Therefore, we propose SOatten, as depicted in Figure 5, which improves the\nfrequency domain to a more general orthogonal domain through a learnable orthogonal transformation, described as:\n$Orth(X) = W^T X, where W^TW = I$\nIn fact, the orthogonality of the learnable space is not guaranteed during backpropagation for parameter updates. We\ncould apply a measure such as QR decomposition, but this could result in the loss of some gradient information. We\nmade a different trade-off for MTSF, considering that the scale of models that match the size of the dataset is usually\nnot very large, with relatively few layers. Therefore, we only perform orthogonal initialization for the embedding,\nrather than enforcing a completely orthogonal space during backpropagation. Subsequently, SOatten also applies the\nMSS projection for query and key and applies linear projection for the value. Additionally, in dot product attention, we\npropose a Head Coupling Convolution (HCC) module operating on the heads of attention weights, which serves as an\nimportant 'guidance' for the mapping space learning of SOatten."}, {"title": "HCC", "content": "FSatten provides sequence dependencies based on explicit spectral information, but extending this to a learnable\northogonal space makes it difficult to effectively determine valid characteristics as a defined periodicity in FSatten. In\nother words, data-driven approaches that learn an effective orthogonal space without any restrictions have requirements\nfor the size and distribution of the dataset.\nI propose a general enhancement method called Head Coupling Convolution (HCC), which leverages the constraint of\nsimilarity between neighboring sequences to guide the model in exploring feature spaces. Specifically, HCC involves\nperforming convolution operations on the attention weights within the dot product attention mechanism as:\n$Atten = Softmax(\\frac{Q K^T}{\\sqrt{d_k}})$ \n$HCC(Atten) = ReLU \\{Con\u0475_{H\u2192H}(Atten, stride = 1, kernel\\_size = K)\\}$\nWhere $Conv_{H\u2192H}$ is channel fusion convolution that maps from H heads to H heads and padding is necessary for\nkeeping the size of the weight matrix.\nFor most time-series data, contrastive learning methods [15] [16] [17] [18] have demonstrated the effectiveness of\nassumption: neighboring similarity, the similarity between sequences of the same time series decreases as the time lag\nincreases. In fact, similar variates are arranged together in most datasets (see details in Appendix A.2).\nBy applying a convolution operation to the attention weights, more critical correlated patterns between local neighboring\nsequences are extracted, guiding the parameter updates in the feature space during backpropagation. The diverse\nfeatures extracted by H heads are all predicated on neighboring similarity; multi-head coupling helps to obtain more\nprecise associative features than single-channel convolution."}, {"title": "Experiments", "content": "Navigation. In section 5.1, We show the forecasting results of FSatten and SOatten. In section 5.2, I conduct ablation\nstudies to verify the effectiveness of MSS and HCC. In section 5.3, We make a visualization analysis for FSatten and\nSOatten, comparing them with conventional attention. Hyperparameter sensitivity analysis is presented in Section 5.4.\ncSetting. We extensively evaluate the proposed FSatten and SOatten on six real-world datasets, including ECL, ETT\n(4 subsets), Exchange, Traffic, Weather [5], and Solar-Energy [19]. Detailed dataset descriptions are provided in\nAppendix B.1. We choose 9 well-known forecasting models as our baselines, including (1) Transformer-based methods:\nAutoformer [5], FEDformer [6], Crossformer [8], PatchTST [7], and iTransformer [9]; (2) Linear-based methods:\nDLinear [20], TiDE [21]; and (3) CNN-based methods: SCINet [22], TimesNet [23]. The experimental setting is the\nsame as in iTransformer [9], that the input length L = 96 and the output length and T = {96, 192, 336, 720}."}, {"title": "Long-term MTSF", "content": "Compared to the baselines presented in Table 1, FSatten, based on the Variate Transformer, shows overall better\nforecasting performance than the SOTA, which uses conventional attention mechanisms. Particularly for datasets\nwith more pronounced periodicity, such as on ECL, FSatten significantly improves performance by an overall 9.0%\ncompared to SOTA and exhibits greater stability for longer prediction sequences. These improvements demonstrate that\nFSatten effectively captures the accurate correlation at the same frequency, which is more suitable for application in\nTransformers for MTSF.\nPeriodicity is one of the most fundamental characteristics of time series, but not all datasets exhibit strong periodicity.\nThus, as a more general approach that can be adapted to both Temporal and Variate transformers, SOatten achieves more"}, {"title": "Ablation Studies", "content": "To validate the effectiveness of the components of FSatten and SOatten, detailed ablation experiments are conducted.\nThe effectiveness of the MSS mapping module, used in both methods, is compared with that of applying a linear\nmapping to FSatten and SOatten, as shown in Figure 6. MSS significantly outperforms the fully connected layer,\ncorroborating its ability to identify more accurate associated components. In Section 5.3, a visual interpretation is\nprovided to explain in detail.\nSecondly, we validate the effectiveness of the HCC module in SOatten in Figure 7. The HCC is an important design\nfor SOatten, significantly enhancing forecasting performance. Especially under the Temporal Transformer, the HCC\ndemonstrates better generalizability. These results prove that neighboring similarity is crucial for the formation of an\neffective orthogonal mapping space and the generation of accurate attention weights. In Section 5.4, we conduct further\nhyperparameter analysis on the kernel size K of HCC."}, {"title": "Visualized Analysis", "content": "First, we make visualizations of generated attention matrices and analyze the advantages of the proposed two attention\nmechanisms. In the upper part of Figure 8, under the Variate Transformer, SOatten and FSatten generate smaller ranges\nbut more refined weight values than the conventional attention applied by the SOTA, iTransformer (compare the value\nrange on the right side of heatmaps). There are three main points of analysis:\n(1) In the generated attention weight maps, the patterns of the conventional attention and FSatten show similarities,\npresenting dependencies that are based on the sequence periodicity. However, FSatten significantly reflects complex"}, {"title": "Hyperparameter Sensitivity", "content": "Compared to conventional attention, the main hyperparameter differences are in the dimension size of the orthogonal\nmapping space F for MSS and the kernel size K for HCC, as shown in Figures 3 and 5. For FSatten, the F frequency\ncomponents are orthogonal and are typically set to $F = \\frac{(1+1)}{2}$. In contrast, SOatten orthogonally initializes the\nmapping weights, and the dimension size F of the mapping spaces for Query and Key are set separately from that of\nthe Value (same with conventional attention). Sensitivity experiments on F, depicted in Figure 9, demonstrate that the\nperformance of the proposed attention mechanisms is not coincidental. When optimized, SOatten's F is significantly\nsmaller than the do and dk of conventional attention, thereby enhancing the model's efficiency to some extent (detailed"}, {"title": "Disscussion", "content": "For the MTSF problem, this paper proposes two innovative attention that are superior to conventional attention. We\nstarted from the frequency domain and made preliminary explorations based on the mainstream Temporal and Variate\nTransformers. We will soon supplement comprehensive experimental results of applying FSatten and SOatten to new\narchitectures released this year, such as TimeXer [24], and the Cross Spatio-Temporal architecture (another work of\nours). Constrained by our resources, we encourage the community to explore solutions to these problems together.\nwe outline three promising directions for future work:\nMapping spaces based on other physical properties, as there exist trend, seasonality, and other characteristics in\naddition to periodicity.\nCombine with state-space models, especially for datasets with a large number of variates, The advantage of current\nstate-space models such as Mamba [25] [26] [27] and TTT [28] is that they can compress the larger variable background\ninto a fixed hidden state, selectively retaining the most important information under global variates, which is very\nimportant for scenarios with multiple classes of variable information, like in Traffic. Of course, this is an improved\nmethod that we will launch soon.\nApply to other fields, Such as CV (Computer Vision), NLP (Natural Language Processing), and motion planning,\nwhich are all directions that can explore the proposed FSatten and SOatten."}]}