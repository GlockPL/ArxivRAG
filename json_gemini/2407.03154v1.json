{"title": "Reinforcement Learning for Sequence Design Leveraging Protein Language Models", "authors": ["Jithendaraa Subramanian", "Shivakanth Sujit", "Niloy Irtisam", "Umong Sain", "Derek Nowrouzezahrai", "Samira Ebrahimi Kahou", "Riashat Islam"], "abstract": "Protein sequence design, determined by amino acid sequences, are essential to protein engineering problems in drug discovery. Prior approaches have resorted to evolutionary strategies or Monte-Carlo methods for protein design, but often fail to exploit the structure of the combinatorial search space, to generalize to unseen sequences. In the context of discrete black box optimization over large search spaces, learning a mutation policy to generate novel sequences with reinforcement learning is appealing. Recent advances in protein language models (PLMs) trained on large corpora of protein sequences offer a potential solution to this problem by scoring proteins according to their biological plausibility (such as the TM-score). In this work, we propose to use PLMs as a reward function to generate new sequences. Yet the PLM can be computationally expensive to query due to its large size. To this end, we propose an alternative paradigm where optimization can be performed on scores from a smaller proxy model that is periodically finetuned, jointly while learning the mutation policy. We perform extensive experiments on various sequence lengths to benchmark RL-based approaches, and provide comprehensive evaluations along biological plausibility and diversity of the protein. Our experimental results include favorable evaluations of the proposed sequences, along with high diversity scores, demonstrating that RL is a strong candidate for biological sequence design. Finally, we provide a modular open source implementation can be easily integrated in most RL training loops, with support for replacing the reward model with other PLMs, to spur further research in this domain. The code for all experiments is provided in the supplementary material.", "sections": [{"title": "1 Introduction", "content": "Proteins are indispensable for virtually every biological function, from providing structural integrity to managing immune responses and facilitating molecular transport within organisms. Despite their diverse roles, our current knowledge of proteins remains incomplete, representing merely a fraction of the vast protein landscape. Generating novel protein sequences holds promise to deepen our understanding of cellular mechanisms, physiological responses, disease pathways, and can significantly accelerate drug development [Stanton et al., 2022].\nThe goal of de novo protein design is to find sequences x that maximize an objective function f(x), scored by an oracle. This is a challenging discrete black box optimization problem over a combinatorial search space, that grows according to |A|L where L is the sequence length of the protein, and A = 20 is the number of most commonly appearing a-amino acids to build proteins, although"}, {"title": "2 Related Work", "content": "Several methods have been explored for designing biological sequences.\nEvolutionary Algorithms. Designing sequences by directed evolution [Arnold, 1998] was one of the earliest attempts to solve the protein design problem, wherein random mutations are performed on a set of sequences and the best performing ones are greedily chosen to be mutated further. AdaLead [Sinai et al., 2020] proposes selecting the sequences that are within a fraction k of the best scoring sequence for further mutation. The fraction k controls the greediness of the algorithm. Setting the threshold based on the score rather than the number of candidate sequences allows the algorithm to adapt to the optimization landscape; it can select many candidates in regions where the landscape is flat, thereby promoting diversity, while being able to focus on the best sequences when there are prominent peaks in the landscape. Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [Hansen and Ostermeier, 2001] generates a population of candidate sequences from a"}, {"title": "3 Preliminaries", "content": "Reinforcement Learning In RL, we model a decision making task through an environment that describes a Markov Decision Process (MDP) < S, A, R, P >, where S, A, R and P represent the state space, action space, rewards and transition dynamics respectively. At every timestep, the agent receives a state st \u2208 S and predicts an action at \u2208 A, to transition to a new state according to the transition probability p(st+1 | St, at), and receives a reward rt \u2208 R. The agent chooses an action using a policy \\( \\pi_{\\theta}(a_t|s_t) \\), parameterized by 0. The objective is to learn an optimal policy \\( \\pi_{\\theta}(a_t|s_t) \\) that maximises the expected sum of rewards from the environment. Concretely, optimization problem"}, {"title": "4 Reinforcement Learning for Generating Protein Sequences", "content": "4.1 Approach\nIn this paper, we provide a modular approach to leverage existing protein language models for sequence design, in an RL framework. Our work relies on using pretrained LMs and then being able to train a RL based agent for mutating the sequences over long horizon lengths, for generating amino acid sequences. To this end, we first provide an overview of the different RL algorithms being tests for the protein design tasks. We note that even though prior works have explored this, we provide the first approach for RL based algorithms to leverage existing protein language models, such as the well known ESMFold model Lin et al. [2023], for these tasks.\nBaselines: We provide an overview of the baselines that were tested on this benchmark. We experiment with both value based and policy based deep RL algorithms for learning mutation policies for protein sequences: (1) Deep Q Network (DQN) [Mnih et al., 2013] (2) Rainbow [Hessel et al., 2018] (3) Proximal Policy Optimization (PPO) [Schulman et al., 2017] (4) Soft Actor-Critic (SAC) [Haarnoja et al., 2018] for the base RL algorithms. We also try an additional exploration based baseline, namley (5) PPO with Random Network Distillation (RND) [Burda et al., 2019]. We compare our approach with several works that have previously been proposed for similar tasks, namely (6) GFlowNets [Bengio et al., 2021, 2023] and (7) Metropolis Hastings MCMC with simulated annealing"}, {"title": "4.2 Learning the Reward Model", "content": "In this section, we outline our approach for learning a proxy to the reward function. As outlined in section 4.1, the RL based mutation process tries to maximize the reward function at each step. However, it is computationally expensive to evaluate the reward, especially for protein sequences of longer lengths.\nInspired by knowledge distillation [Hinton et al., 2015, Polino et al., 2018], we distill the 3B ESM model (teacher) into a smaller transformer model [Vaswani et al., 2017], with 15 \u00d7 103 parameters, to serve as a proxy reward model. For learning the proxy reward model, we learn the reward function using sequences from the Atlas dataset [Vander Meersche et al., 2024] since it contains diverse samples. The reward model is then trained using a mean squared regression objective, by predicting the pTM scores for each sequence.\nTo ensure we learn a good reward model, we monitor the correlation scores between the proxy predicted scores and the oracle evaluations for each sequence. Additionally, once we have the pretrained proxy model, we can finetune the proxy within the training loop of the algorithms. Every 2000 environment steps, we compute the proxy pTM scores for the proposed sequences, filter the top K = 100 sequences, and finetune the proxy on the true pTM scores of these sequences from ESMFold for 50 epochs. We demonstrate the effectiveness of the proxy finetuning procedure in Figure 2; which shows an increasing trend in Pearson's correlation coefficient between the proxy and oracle model for sequences of length 50."}, {"title": "4.3 Evaluating Protein Sequences", "content": "Evaluating the designed protein sequences is inherently multi-objective. In our experiments, evaluations are primarily along two axes, scoring the sequences according to biological plausibility and diversity. Biological plausibility is the useful to assess the viability of the proposed sequences within biological contexts. To this end, we report the mean predicted Template Modeling (pTM) score and the mean predicted Local Distance Difference Test (pLDDT) score across the proposed sequences:\nTM score [Zhang and Skolnick, 2004]. A metric in (0, 1] that evaluates the structural similarity between two proteins, with higher TM-scores indicating better structural similarity. It is a variation of the Levitt-Gerstein (LG) score, which weights shorter distances between residues more strongly than longer distances. The TM-score between a target structure Stgt and template structure Stmp is defined as:\n\\[\nd_{TM}(S_{tgt}, S_{tmp}) = max \\frac{1}{L_{tgt}}\\sum_{i=1}^{L}\\frac{1}{1 + \\frac{d_i^2}{d_0(L_{tgt})^2}}\n\\]"}, {"title": "5 Experiments and Analysis", "content": "In this section, we outline our experiments for sequence design. In all our experiments, we use ESMFold either directly as the reward model or as a signal to finetune the proxy model. Our main goal is to test the efficacy of different sequence design algorithms. Through our experiments, we analyze the performance of all methods and ask the following questions.\nHow effectively do various sequence design algorithms optimize the reward from ESMFold?\nTable 1 and 4 (appendix) present the results for all methods evaluated in the infinite horizon setting for sequences of lengths 50 and 100, respectively. The reward used in these experiments is the pTM scores obtained from the oracle model (ESMFold). All results are averaged over three random seeds, and all experiments for this analysis were run on 4 A100 GPUs. The results in these tables highlight the effectiveness of RL algorithms and GFlowNets for sequence design.\nOur training results show MCMC outperforms PPO in terms of the pTM score. While this result might suggest that MCMC is superior for sequence design, it is important to consider the broader context and practical implications.\nIn contrast, learning a policy, such as with RL or GFlowNets, offers several compelling advantages:"}, {"title": "6 Limitations", "content": "Our results are constrained by sequence length due to the prohibitive computational requirements associated with longer sequences. The findings presented in this paper rely on either the proxy or the 3B ESMFold oracle model. However, the reward model may itself be uncertain or misspecified. Future research should consider extending these results to optimization on other PLMs, such as"}, {"title": "7 Conclusion", "content": "In this work, we propose to leverage existing pretrained PLMs to learn a mutation policy for generating protein sequences and demonstrate the efficacy of this approach. We benchmark the performance of the algorithms in a wide range of settings \u2013 training with the 3B ESMFold and the proxy, generating various sequence lengths, and studying the pTM-diversity tradeoff for different episode lengths. To this end, we demonstrate and compare performance, of several existing deep RL algorithms compared to well known baselines, and show that RL can be quite effective in this domain. We share a modular implementation in the appendix, with support for adding a PLM of the user's choice as the reward model, as well as adding other RL algorithms."}]}