{"title": "Reinforcement Learning for Sequence Design\nLeveraging Protein Language Models", "authors": ["Jithendaraa Subramanian", "Shivakanth Sujit", "Niloy Irtisam", "Umong Sain", "Derek Nowrouzezahrai", "Samira Ebrahimi Kahou", "Riashat Islam"], "abstract": "Protein sequence design, determined by amino acid sequences, are essential to\nprotein engineering problems in drug discovery. Prior approaches have resorted to\nevolutionary strategies or Monte-Carlo methods for protein design, but often fail\nto exploit the structure of the combinatorial search space, to generalize to unseen\nsequences. In the context of discrete black box optimization over large search\nspaces, learning a mutation policy to generate novel sequences with reinforcement\nlearning is appealing. Recent advances in protein language models (PLMs) trained\non large corpora of protein sequences offer a potential solution to this problem by\nscoring proteins according to their biological plausibility (such as the TM-score).\nIn this work, we propose to use PLMs as a reward function to generate new se-\nquences. Yet the PLM can be computationally expensive to query due to its large\nsize. To this end, we propose an alternative paradigm where optimization can be\nperformed on scores from a smaller proxy model that is periodically finetuned,\njointly while learning the mutation policy. We perform extensive experiments\non various sequence lengths to benchmark RL-based approaches, and provide\ncomprehensive evaluations along biological plausibility and diversity of the protein.\nOur experimental results include favorable evaluations of the proposed sequences,\nalong with high diversity scores, demonstrating that RL is a strong candidate for\nbiological sequence design. Finally, we provide a modular open source implemen-\ntation can be easily integrated in most RL training loops, with support for replacing\nthe reward model with other PLMs, to spur further research in this domain. The\ncode for all experiments is provided in the supplementary material.", "sections": [{"title": "Introduction", "content": "Proteins are indispensable for virtually every biological function, from providing structural integrity\nto managing immune responses and facilitating molecular transport within organisms. Despite\ntheir diverse roles, our current knowledge of proteins remains incomplete, representing merely a\nfraction of the vast protein landscape. Generating novel protein sequences holds promise to deepen\nour understanding of cellular mechanisms, physiological responses, disease pathways, and can\nsignificantly accelerate drug development [Stanton et al., 2022].\nThe goal of de novo protein design is to find sequences x that maximize an objective function\nf(x), scored by an oracle. This is a challenging discrete black box optimization problem over a\ncombinatorial search space, that grows according to |A|L where L is the sequence length of the protein,\nand A = 20 is the number of most commonly appearing \u03b1-amino acids to build proteins, although\nthere are upto 22 amino acids in the genetic code of life [Crick, 1968, Koonin and Novozhilov, 2009].\nDesigning proteins in this large search space has been attempted with various evolutionary methods,\nfrom directed evolution [Arnold, 1998] to adaptive greedy search like AdaLead [Sinai et al., 2020]. A\nmajor caveat, however, is that these methods are computationally inefficient, find sequences in the\nsame neighborhood, and do not scale well with increasing sequence length. In this regard, learning a\nsequence generation policy with reinforcement learning (RL) [Angermueller et al., 2020] offers a\npromising solution, with the potential to exploit the structure in the search space. Once such a policy\nhas been trained, actions can be sampled and sequences can be generated at a fraction of the training\ncost and can generalize to unseen sequences, even in the absence of the reward model.\nRecently, there has been massive efforts towards building large protein language models (PLMs)\n[Jumper et al., 2021, Lin et al., 2023, 2022, Meier et al., 2021, Abramson et al., 2024] to address\nthe protein folding problem, along with the prediction of scores (e.g., template modeling score)\nthat estimate biological plausibility of a protein sequence and its corresponding folded structure.\nMotivated by recent research on large language models (LLMs) which provides evidence that LLMs\ncan be repurposed as reward models [Rafailov et al., 2023], we propose to use PLMs as the reward\nfunction. In our work, we instantiate the 3B parameter version of ESMFold [Lin et al., 2023] as the\noracle reward model. We choose ESMFold over other competitive PLMs such as AlphaFold2 due to\nits favourable inference, offering upto 60\u00d7 faster inference time, which is crucial in an RL loop.\nOur work takes a sequence first approach, similar to EvoDiff [Alamdari et al., 2023] and LaMBO-\n2 [Gruver et al., 2023]. While protein structures are what ultimately determine their function,\nsequence determines structure. Moreover, a structure-first approach for protein design still needs\nto generate sequences, by solving the inverse folding problem in order to successfully synthesize\nthe protein in the laboratory. We address the problem of protein design by learning policies that\ndirectly generate protein sequences, by learning to mutate random sequences. Our contributions are\nas follows:\n\u2022 The primary purpose of this paper is to investigate and benchmark the performance of RL\nalgorithms where the reward is given by an oracle PLM, ESMFold [Lin et al., 2023] in our\ncase.\n\u2022 Since the oracle reward model is expensive to query, we demonstrate an alternate approach\nthat requires fewer evaluations under the oracle model, called ESM with Proxy-Finetuning\n(ESM-PF). ESM-PF jointly learns a proxy reward model and the sequence generation policy\nwith results comparable to that of directly optimizing on the oracle. The proxy model is\nfaster to query due to its smaller size, and is trained by periodically finetuning on pairs of\npreviously generated sequences and their oracle scores (xi, Yi).\n\u2022 We perform a multi-objective evaluation of the proposed protein sequences in both the oracle\nand ESM-PF settings, across various sequence lengths, with consideration to biological\nplausibility and diversity of the sequences, both in the sequence and structure space.\n\u2022 We provide a modular open source implementation, that provides easy support for both the\naddition of other exploration algorithms (evolutionary, RL-based, generative model based),\nas well as replacing the ESMFold reward model with a PLM of the user's choice (such as\nAlphaFold)."}, {"title": "Related Work", "content": "Several methods have been explored for designing biological sequences.\nEvolutionary Algorithms. Designing sequences by directed evolution [Arnold, 1998] was one of\nthe earliest attempts to solve the protein design problem, wherein random mutations are performed\non a set of sequences and the best performing ones are greedily chosen to be mutated further.\nAdaLead [Sinai et al., 2020] proposes selecting the sequences that are within a fraction k of the\nbest scoring sequence for further mutation. The fraction k controls the greediness of the algorithm.\nSetting the threshold based on the score rather than the number of candidate sequences allows the\nalgorithm to adapt to the optimization landscape; it can select many candidates in regions where the\nlandscape is flat, thereby promoting diversity, while being able to focus on the best sequences when\nthere are prominent peaks in the landscape. Covariance Matrix Adaptation Evolutionary Strategy\n(CMA-ES) [Hansen and Ostermeier, 2001] generates a population of candidate sequences from a"}, {"title": "Preliminaries", "content": "Reinforcement Learning In RL, we model a decision making task through an environment that\ndescribes a Markov Decision Process (MDP) < S, A, R, P >, where S, A, R and P represent the\nstate space, action space, rewards and transition dynamics respectively. At every timestep, the agent\nreceives a state st \u2208 S and predicts an action at \u2208 A, to transition to a new state according to the\ntransition probability p(st+1 | St, at), and receives a reward rt \u2208 R. The agent chooses an action\nusing a policy \u03c0\u03bf(at|st), parameterized by 0. The objective is to learn an optimal policy \u03c0\u03bf(at|st)\nthat maximises the expected sum of rewards from the environment. Concretely, optimization problem"}, {"title": "Reinforcement Learning for Generating Protein Sequences", "content": "In this paper, we provide a modular approach to leverage existing protein language models for\nsequence design, in an RL framework. Our work relies on using pretrained LMs and then being able\nto train a RL based agent for mutating the sequences over long horizon lengths, for generating amino\nacid sequences. To this end, we first provide an overview of the different RL algorithms being tests\nfor the protein design tasks. We note that even though prior works have explored this, we provide the\nfirst approach for RL based algorithms to leverage existing protein language models, such as the well\nknown ESMFold model Lin et al. [2023], for these tasks.\nBaselines: We provide an overview of the baselines that were tested on this benchmark. We\nexperiment with both value based and policy based deep RL algorithms for learning mutation policies\nfor protein sequences: (1) Deep Q Network (DQN) [Mnih et al., 2013] (2) Rainbow [Hessel et al.,\n2018] (3) Proximal Policy Optimization (PPO) [Schulman et al., 2017] (4) Soft Actor-Critic (SAC)\n[Haarnoja et al., 2018] for the base RL algorithms. We also try an additional exploration based\nbaseline, namley (5) PPO with Random Network Distillation (RND) [Burda et al., 2019]. We compare\nour approach with several works that have previously been proposed for similar tasks, namely (6)\nGFlowNets [Bengio et al., 2021, 2023] and (7) Metropolis Hastings MCMC with simulated annealing"}, {"title": "Learning the Reward Model", "content": "In this section, we outline our approach for learning a proxy to the reward function. As outlined\nin section 4.1, the RL based mutation process tries to maximize the reward function at each step.\nHowever, it is computationally expensive to evaluate the reward, especially for protein sequences of\nlonger lengths.\nInspired by knowledge distillation [Hinton et al., 2015,\nPolino et al., 2018], we distill the 3B ESM model\n(teacher) into a smaller transformer model [Vaswani\net al., 2017], with 15 \u00d7 103 parameters, to serve as\na proxy reward model. For learning the proxy reward\nmodel, we learn the reward function using sequences\nfrom the Atlas dataset [Vander Meersche et al., 2024]\nsince it contains diverse samples. The reward model is\nthen trained using a mean squared regression objective,\nby predicting the pTM scores for each sequence.\nTo ensure we learn a good reward model, we moni-\ntor the correlation scores between the proxy predicted\nscores and the oracle evaluations for each sequence.\nAdditionally, once we have the pretrained proxy model,\nwe can finetune the proxy within the training loop of\nthe algorithms. Every 2000 environment steps, we com-\npute the proxy pTM scores for the proposed sequences,\nfilter the top K = 100 sequences, and finetune the proxy on the true pTM scores of these sequences\nfrom ESMFold for 50 epochs. We demonstrate the effectiveness of the proxy finetuning procedure in\nFigure 2; which shows an increasing trend in Pearson's correlation coefficient between the proxy and\noracle model for sequences of length 50."}, {"title": "Evaluating Protein Sequences", "content": "Evaluating the designed protein sequences is inherently multi-objective. In our experiments, evalua-\ntions are primarily along two axes, scoring the sequences according to biological plausibility and\ndiversity. Biological plausibility is the useful to assess the viability of the proposed sequences within\nbiological contexts. To this end, we report the mean predicted Template Modeling (pTM) score and\nthe mean predicted Local Distance Difference Test (pLDDT) score across the proposed sequences:\nTM score [Zhang and Skolnick, 2004]. A metric in (0, 1] that evaluates the structural similarity\nbetween two proteins, with higher TM-scores indicating better structural similarity. It is a variation\nof the Levitt-Gerstein (LG) score, which weights shorter distances between residues more strongly\nthan longer distances. The TM-score between a target structure Stgt and template structure Stmp is\ndefined as:\n$d_{TM}(S_{tgt}, S_{tmp}) = max \\frac{1}{L_{tgt}} \\sum_{i=1}^{L} \\frac{1}{1+(\\frac{d_{i}}{d_{0}(L_{tgt})})^{2}}$\nwhere Ltgt is amino acid sequence length of the target protein, L is the number of residues that appear\nin both template and target structures, di is the distance between the ith residues in template and\ntarget structures, and do(Ltgt) is taken as 1.24 \u221a(Ltgt \u2013 15) \u2013 1.8 and normalizes sequence length\nIDDT score [Mariani et al., 2013]. A metric in (0, 100] which provides a per-residue measure\nof confidence in the local accuracy of a predicted protein structure. IDDT estimates how well the\nprediction would agree with an experimental structure and a score of above 70 usually corresponds\nto a correct backbone prediction with misplacement of some side chains. An IDDT score of 90 or\ngreater indicates high confidence in the predicted structure, while a score below 50 indicates poor\nconfidence.\nIn addition to obtaining high-scoring sequences, we are interested in obtaining diverse sequences.\nEvaluating diversity is multi-dimensional and can be viewed through different lenses such as diversity\nin sequence space, structural diversity of the protein, number of modes discovered, and hyper-volume.\nIn our experiments, we report multiple metrics for a comprehensive evaluation of sequence diversity\nthrough all these lenses:\nDiversity in Sequence Space. The Mean Pairwise Hamming Distance (MP-HD) over a dataset D\nof sequences is a standard metric to evaluate the diversity in the sequence space [Angermueller et al.,\n2020, Jain et al., 2022], and is measured as:\n$MP-HD = \\frac{\\sum_{X_{i} \\in D} \\sum_{X_{j} \\in D \\backslash X_{i}} HD(X_{i}, x_{j})}{|D| (|D| - 1)}$\nwhere dHD(., .) is the hamming distance between two sequences.\nStructural Diversity. We first obtain the structures S from D by performing a forward pass through\na protein structure prediction model (ESMFold in our case) and then compute the Mean Pairwise TM\nscore (MP-TM) and Mean Pairwise RMSD (MP-RMSD).\n$MP-TM = \\frac{\\sum_{S_{i} \\in S} \\sum_{S_{j} \\in S \\backslash S_{i}} d_{TM}(S_{i}, S_{j})}{|D| (|D| -1)}$\n$MP-RMSD = \\frac{\\sum_{S_{i} \\in S} \\sum_{S_{j} \\in S \\backslash S_{i}} d_{RMSD}(S_{i}, S_{j})}{|D| (D-1)}$\nSince TM-score is a closeness metric and RMSD is a distance metric, we typically want low MP-TM\n(\u2193) and high MP-RMSD (\u2191).\nHowever, the reward models could be imperfect or misspecified. To quantify this, we compare outputs\nof ESMFold with that of AlphaFold. Concretely, we compare the mean pTM and PLDDT scores\npredicted by each model across the proposed sequences for each algorithm. We also report the mean\nRMSD between the structure predicted by both models, over all proposed sequences."}, {"title": "Experiments and Analysis", "content": "In this section, we outline our experiments for sequence design. In all our experiments, we use\nESMFold either directly as the reward model or as a signal to finetune the proxy model. Our main\ngoal is to test the efficacy of different sequence design algorithms. Through our experiments, we\nanalyze the performance of all methods and ask the following questions.\nHow effectively do various sequence design algorithms optimize the reward from ESMFold?\nTable 1 and 4 (appendix) present the results for all methods evaluated in the infinite horizon setting\nfor sequences of lengths 50 and 100, respectively. The reward used in these experiments is the pTM\nscores obtained from the oracle model (ESMFold). All results are averaged over three random seeds,\nand all experiments for this analysis were run on 4 A100 GPUs. The results in these tables highlight\nthe effectiveness of RL algorithms and GFlowNets for sequence design.\nOur training results show MCMC outperforms PPO in terms of the pTM score. While this result\nmight suggest that MCMC is superior for sequence design, it is important to consider the broader\ncontext and practical implications.\nIn contrast, learning a policy, such as with RL or GFlowNets, offers several compelling advantages:"}, {"title": "Limitations", "content": "Our results are constrained by sequence length due to the prohibitive computational requirements\nassociated with longer sequences. The findings presented in this paper rely on either the proxy or\nthe 3B ESMFold oracle model. However, the reward model may itself be uncertain or misspecified.\nFuture research should consider extending these results to optimization on other PLMs, such as"}, {"title": "Conclusion", "content": "In this work, we propose to leverage existing pretrained PLMs to learn a mutation policy for generating\nprotein sequences and demonstrate the efficacy of this approach. We benchmark the performance of\nthe algorithms in a wide range of settings \u2013 training with the 3B ESMFold and the proxy, generating\nvarious sequence lengths, and studying the pTM-diversity tradeoff for different episode lengths. To\nthis end, we demonstrate and compare performance, of several existing deep RL algorithms compared\nto well known baselines, and show that RL can be quite effective in this domain. We share a modular\nimplementation in the appendix, with support for adding a PLM of the user's choice as the reward\nmodel, as well as adding other RL algorithms."}]}