{"title": "Towards Automated Data Sciences with Natural Language and SageCopilot: Practices and Lessons Learned", "authors": ["Yuan Liao", "Jiang Bian", "Yuhui Yun", "Shuo Wang", "Yubo Zhang", "Jiaming Chu", "Tao Wang", "Kewei Li", "Yuchen Li", "Xuhong Li", "Shilei Ji", "Haoyi Xiong"], "abstract": "While the field of NL2SQL has made significant advancements in translating natural language instructions into executable SQL scripts for data querying and processing, achieving full automation within the broader data science pipeline-encompassing data querying, analysis, visualization, and reporting-remains a complex challenge. This study introduces SageCopilot, an advanced, industry-grade system system that automates the data science pipeline by integrating Large Language Models (LLMs), Autonomous Agents (AutoAgents), and Language User Interfaces (LUIs). Specifically, SageCopilot incorporates a two-phase design: an online component refining users' inputs into executable scripts through In-Context Learning [4] (ICL) and running the scripts for results reporting & visualization, and an offline preparing demonstrations requested by ICL in the online phase. A list of trending strategies such as Chain-of-Thought and prompt-tuning have been used to augment Sage-Copilot for enhanced performance. Through rigorous testing and comparative analysis against prompt-based solutions, SageCopilot has been empirically validated to achieve superior end-to-end performance in generating/executing scripts and offering results with visualization, backed by real-world datasets. Our in-depth ablation studies highlight the individual contributions of various components and strategies used by SageCopilot to the end-to-end correctness for data sciences.", "sections": [{"title": "1 INTRODUCTION", "content": "The advancements in Text-to-SQL generation propose promising approaches to convert textual instructions into structured query language (SQL) for database operations [7, 15, 17, 20, 21]. However, achieving full automation in the data science pipeline is challenging [7] due to the complexity of understanding user intents [6, 9], integrating access control mechanisms [8], and providing user-intention-driven visualizations of query results [2]. Thus, a comprehensive automation system needs to encapsulate intention understanding, data retrieval and analysis, and the generation of visual outputs, which traditionally requires expert involvement.\nThe integration of LLMs and AutoAgents has paved the way for establishing comprehensive data science pipelines that connect language user interfaces (LUIs), databases, and data visualization tools with LLMs [19]. AutoAgents can clarify user intentions using conversational interfaces and then plan for data querying, analysis, and visualization by utilizing LLM capabilities. By coordinating tasks across databases, authenticators, and visualization tools, as well as employing SQL & scripts generation [1, 3, 13], and data format conversions [1], AutoAgents can fulfill complex data science requirements autonomously. Though techniques to improve every single component, such as NL2SQL, have been intensively studied in previous works [1, 3, 7, 13, 15, 17, 20, 21], integrating these components is still a challenging task for achieving the goal of automated data sciences in an industry settings. Several non-trivial technical issues should be addressed as follows.\n\u2022 Closed-Loop System with Multi-Tool: Existing systems, such as ChatGPT or Code Interpreter, either generate scripts under users' supervision or manipulate uploaded datasets for potential data analysis. Human intervention (e.g., checking the correctness of the generated scruots, copying and pasting the scripts to a local execution environment for further data manipulations) might be indispensable between every steps of interactions. However, for full automation of data science pipelines, there needs a system that processes user requests, manages databases, executes scripts, and reports analysis results in a seamless closed-loop, minimizing human intervention.\n\u2022 Domain Adaptation and Generalization: To enhance the accuracy of script generation, supervised fine-tuning (SFT) of LLMs subject to various tasks is desired. However, SFT for adapting every data domain (e.g., databases for groceries stores, wholesales, corporate finance, etc.) or collecting a sufficiently large dataset for cross-domain generalization is resource consuming. Inappropriate SFT could hurt emergent abilities of an LLM due to catastrophic forgetting [12]. Thus, in addition to domain-specific SFT, the system should be capable of adapting to every data domain in the context of interactions with LLMs.\n\u2022 End-to-End Correctness and Reflection: Though every single component could be optimized with better accuracy or capacity, the"}, {"title": "2 FRAMEWORK DESIGN", "content": "As depicted in Figure 1, the SageCopilot framework presents a so- sophisticated dual-phase approach to managing and processing natu- ral language queries against complex databases. In the offline phase, significant emphasis is placed on the preparation and enrichment of a robust data foundation that underpins the online operational stage. Through metadata semantic governance, the framework lays the groundwork for a deep understanding of the data structure, ensuring comprehensive query understanding and accurate SQL generation. The construction of a rich seed data repository, supplemented by effective data augmentation strategies, further strengthens the system's ability to produce precise query responses.\nTransitioning into the online phase, SageCopilot seamlessly shifts from data curation to real-time user engagement. The framework's intent understanding and decision-making module utilizes the nuanced capabilities of LLM to interpret diverse user queries, cater to non-standard question phrasings, and provide contextually relevant visualizations. The system's adeptness at memory recall and schema linking further streamlines the process, enabling efficient and relevant information retrieval. The integration of SQL generation with in-context prompting, coupled with the reflective oversight offered by the SQL reflection module, ensures the pro- duction of accurate and logically coherent database queries. These features, alongside the essential components of tool use and authentication, confirm that SageCopilot operates with precision, reliability, and security. Result presentation stands as the final touchpoint"}, {"title": "2.1 Offline Phase", "content": "The offline phase is devoted to the careful preparation of high- quality data that will be used in the online phase. This preparatory work involves four key activities: practicing metadata semantic governance, constructing a base of seed data, enhancing the dataset through data augmentation, and extracting information for schema linking. Once collated and refined, these datasets are embedded into a memory vector database which, in turn, equips the online phase with the necessary contextual data to meet specific user queries."}, {"title": "2.1.1 Schema Semantic Governance.", "content": "Schema information is essen- tial for data analysis tasks such as SQL writing, akin to the necessity for humans to understand table schematics. SageCopilot generates complex SQL statements aligned with human intentions by inter- preting schema details. A detailed exposition of table metadata is indispensable for accurately matching user intent. For fields with enumerated values, a comprehensive description of the values stored in the database and their interpretations is required. In cases involving complex data types like structures and maps, a detailed account of key attributes and their meanings is crucial. This detailed metadata aids in understanding the complex relations between at- tributes, allowing for accurate data retrieval in response to queries. Additionally, SQL dialects should be annotated to ensure generated SQL adheres to the specific syntax required by the target system."}, {"title": "2.1.2 Seed Data Construction.", "content": "The capacity of current LLMs in generating complex SQL autonomously is fairly restricted. We leverage the ICL ability of the LLMs to bolster the stability and accuracy of SQL generation by incorporating dynamic examples of <Query, SQL> pairs relevant to the user's current question. There are three strategies for constructing these seed data pairs. The first is pop- ulating <Query, SQL> pairs common in the business's everyday workflows, especially because certain queries routinely include bespoke business indicators. Without few-shot prompting or fine- tuning mechanisms, expecting large models to autonomously gen- erate user-specific SQL is impractical. The second method is the SQL2NL technique, which facilitates the cold start process without imposing the requirement for users to supply <Query, SQL> pairs, particularly given the abundance of SQL in the enterprise data ware- house. In this context, natural language queries are generated by LLMs based on SQL and schema information, thereby constituting <Query, SQL> pairs. The final approach involves deriving <Query, SQL> pairs from user feedback, targeting instances where LLMs have failed to produce precise results. Here, human intervention is key to correcting inaccuracies."}, {"title": "2.1.3 Data Augmentation Techniques.", "content": "SageCopilot incorporates two data augmentation strategies to enhance the performance and adaptability, whose ablation study detailed in Section 3.2.1.\n\u2022 Semantic-Preserving Augmentation - the aim is to bolster the initial dataset, utilizing the LLM to transform a given query into"}, {"title": "2.1.4 Schema Linking Information Extraction.", "content": "We utilize the analy- sis of SQL syntax trees for multiple dialects to convert our dataset from <Query, SQL> format into a <Query, Tables, Fields> format. This transformation creates a direct linkage between user-generated queries and their associated database fields."}, {"title": "2.1.5 Memory Vector Database Integration.", "content": "The memory vector database is pivotal in bridging the gap between offline preparation and online functionality. It encapsulates and consolidates data into a memory structure that unites both the original and augmented data pairs-including <Query, SQL, Tables, Fields> along with <Query', SQL, Tables, Fields>-to support robust and efficient data retrieval and management."}, {"title": "2.2 Online Phase", "content": "In the online phase, the system delivers a refined user experience by discerning user intent and generating precise SQL queries through a series of LLM-driven modules. The system begins with intent recog- nition, proceeding to link questions to database schemas, and then crafting SQL statements guided by user queries paired with similar examples. The SQL Reflection module improves accuracy by correct- ing errors in SQL queries. After authentication, the database runs the SQL and results are processed. The Result Generation module translates these results into textual analyses, avoiding visual com- plexity while still offering comprehensive insights. A Visualization component transforms data into charts. Additionally, a Forecast ca- pability allows for the prediction of time-based data trends through simple, intuitive LLM-user interactions. Overall, the online phase is a streamlined sequence ensuring precise, user-aligned outputs, from query interpretation to actionable predictions."}, {"title": "2.2.1 Intent Understanding and Decision-Making.", "content": "In authentic busi- ness environments, diverse user inquiries arise, including content beyond query parameters, topics outside the scope of the domain, incomplete queries, and follow-up questions that challenge conti- nuity. To navigate these intricacies, an LLM-powered intent under- standing and decision-making mechanism has been meticulously designed. This mechanism integrates four principal components: comprehensive intent assimilation, assessment of relevance, provi- sion for direct query result visualization, and chart type discern- ment, all aligning towards heightened service precision for users. This system harnesses user inquiries, analogous example queries, and predefined decision metrics to formulate elaborate prompts that optimize LLM responses. The comprehensive intent under- standing component is calibrated to capture the essence of user requirements, addressing lacunae within multi-turn dialogues. The relevance assessment ascertains the pertinence of queries to the currently engaged topic, thereby filtering out unrelated or off-topic"}, {"title": "2.2.2 Multiple Recall & Schema Linking.", "content": "In SageCopilot, thematic domain schemas involving 1 to N tables are depicted. The challenge in practical business situations is handling expansive tables within the confined contextual bandwidth of LLM prompts. This context constraint renders the integration of comprehensive schema details for all N tables into a solitary prompt impractical. Moreover, incor- porating superfluous schema information can detrimentally impact model effectiveness. As a remedy, we introduce a schema linking module preceding SQL formulation to circumvent the complications of excess schema inputs.\nContemporary schema linking interventions [10, 15, 16] often revolve around the development of fresh models, necessitating sub- stantial resources with potential limitations in domain-agnostic generalizability. Counteracting this, our method employs a prelimi- nary multi-recall tactic to pinpoint tables aligning with the user's query intention. The LLM is then utilized to forge links between the user's question and the relevant tables and fields.\nWe execute the multi-recall approach through two principal channels: (1) a similarity-based recall that aligns user queries with table schemas, and (2) a homologous recall that harnesses pre-stored schema linking data from a memory vector database initialized dur- ing an offline phase. These distinct channels function autonomously, guaranteeing that the targeted table is suitably represented within the pool of candidates. After the table recall, Schema Linking par- ticulars are deduced from the prescribed prompts as detailed in Figure 2. This dual-phase methodology fosters efficient SQL genera- tion for voluminous tables within specific domains and streamlines the schema linking process, thereby enhancing the model's efficacy and generalization potential."}, {"title": "2.2.3 SQL Generation & In-Context Prompting.", "content": "The objective of this module is to accurately generate SQL statements by fusing intent comprehension with schema linking, utilizing the in-context capabilities of LLMs. A significant technical challenge is crafting precise prompts with a restricted token input capacity. Typically, in-context examples in Figure 3 are comprised of <Query, SQL> pairs retrieved from a vector database via similarity search. Initially, an input query is converted into an embedding vector\u00b9. This vector is then compared to the closest <Query, SQL> pair in the vector database by calculating the Inner Product (IP) distance, a ubiquitous\n\u00b9https://github.com/shibing624/text2vec"}, {"title": "2.2.4 SQL Reflection.", "content": "The SQL Reflection module is crafted to har- ness the exceptional proficiency of LLMs in amending flawed SQL expressions, significantly elevating SQL accuracy. Through detailed evaluation and comprehensive review, we scrutinize the veracity of SQL commands, delving into error etiologies encompassing verifi- cations for table and column presence, as well as syntactic integrity. The SQL Reflection mechanism is activated solely upon the unequiv- ocal identification of an anomaly within the SQL command. The configuration of the prompt is meticulously engineered, delineating not just the error typologies but also encompassing the SQL queries, explicit table schemas, and in-depth diagnostic evaluations of the discrepancies. Such a framework primes the LLM to concentrate on redressing the underlying SQL inaccuracies, thus garnering an accurate and meticulously directed correction process. The efficacy and practicability of this module have been corroborated through empirical studies detailed in Section 3.2.5. The data derived from these experiments solidifies the premise that this modality is not merely efficacious but also eminently viable."}, {"title": "2.2.5 Tool Use & Authentication.", "content": "Our system, SageCopilot, accom- modates SQL command execution across a spectrum of dialects, spanning MySQL, PostgreSQL, Flink SQL, Hive SQL and Spark SQL. It is noteworthy that queries executed via MySQL and PostgreSQL"}, {"title": "2.2.6 Result Generation.", "content": "Upon the retrieval of execution outcomes from the database engine, the result generation module commences its role in constructing the conclusive output for web-based appli- cations. Demonstration of this process is evident in the lower right segment of Figure 1, wherein a diversely formatted report analo- gous to a portfolio is composed. Such a report encompasses textual analysis, visual representations through graphs and charts, and tabular data. Additionally, this phase may initiate further user inter- action through the integration of predictive services and knowledge-driven analytical processes.\nText Analysis. Predominantly, the result generation module act- ing online employs a LLM agent for the transformation of execution results, articulated in markdown or JSON-esque structures, into tex- tual discourse. The composition of the textual output is not limited to the portrayal of SQL execution outcomes but also encapsulates a synthesized overview and preliminary analysis of the data. The prompt utilized for guiding the LLM agent is formulated in Ap- pendix D.1. In such an approach, the aspect of visualization has been deliberately decoupled, as a distinct module with an exclusive focus on graphical representation has been specifically crafted to augment the inherent capabilities of chart formation to their fullest extent without complication. It warrants attention that under cer- tain circumstances, users may venture inquiries pertaining to key, yet uncomputed, metrics-take for instance the \"closure rate\", poten- tially non-existent within the current table schema. This presents a challenge as the LLM may find itself incapable of delivering the anticipated response. A remedy to this quandary is the activation of a knowledge enhancement mechanism, prompting the user for the precise formula necessitated for the computation of the \"closure rate\". Upon acquisition of the accurate or anticipated formula, the LLM is equipped to amalgamate it with the SQL execution findings, thereby generating an apt and tailored response.\nVisualization. In order to refine front-end processing and deliver a more comprehensible presentation of graphical data, we utilize the capabilities of an LLM agent to generate bar, line, and pie chart code in Echarts-compatible JSON format. However, due to the broad range of outputs possible from the LLM agent, the produced visu- alizations may not consistently meet the user's precise needs. To"}, {"title": "3 DEPLOYMENT AND EVALUATION", "content": "In the following section, we present the intricate details of deploy- ing our novel SageCopilot within an industrial environment and carry out extensive experimental evaluations to assess the system's overall performance and the impact of each optimization technique. Setups. Figure 5 depicts the deployment of SageCopilot using a microservice architecture within a Kubernetes (K8s) cluster, with service configurations detailed in Table 6 in Appendix B. To enhance the system's stability and performance, SageCopilot strategically avoids direct connections between Python web services, such as the query service, and the MySQL database. Instead, a Java-based web service, namely the manager service, acts as an intermediary, managing front-end user requests and SQL database interactions. This design choice aims to enhance system throughput and reduce stability risks associated with direct Python-MySQL connections. Additionally, a MySQL database is implemented prior to employing operations within the vector database to maintain index uniqueness. Based on this foundational deployment, we perform a series of experiments to ascertain the efficiency and efficacy of the deployed system and to conduct an ablation study for each optimization strategy in SageCopilot."}, {"title": "Datasets.", "content": "The DuSQL dataset, frequently used to evaluate the accuracy of Text-to-SQL parsing across various databases, chal- lenges models to adapt to new database schemas. The dataset con- tains 23,797 high-quality Chinese Text-SQL pairs over 200 distinct databases. In this investigation, we opted for 10 databases from"}, {"title": "Metrics.", "content": "To assess the performance of the proposed SageCopilot, we evaluate it from two perspectives: (1) the quality of SQLs gen- erated in the SageCopilot pipeline, and (2) the quality of answers as an automated data analysis tool. For the first aspect, we adopt the mainstream NL2SQL metrics such as exact match, execution accuracy. We also explore one human-aligned metric as one sup- plementary indicator. For the second one, we cooperate a team of experts (trained with the domain knowledge) to rate the answers based on a panel of human-designed criteria. The detailed definition for each metric is illustrated as follows:\n\u2022 Exact Match (EM), namely the percentage of questions whose predicted SQL query is equivalent to the gold SQL query, is widely used in text-to-SQL tasks. Suppose that $Y_i$ is the i-th ground truth SQL and $\\hat{Y_i}$ is the i-th predicted SQL, EM can be computed by $\\text{EM} = [\\sum \\mathbb{1}(Y_i, \\hat{Y_i})]/N$.\n\u2022 Execution Accuracy (EX), namely the percentage of questions whose predicted SQL obtains the correct result, assumes that each SQL has an result. Considering the result $V_i$ is executed by the $Y_i$ and $\\hat{V_i}$ is executed by the $\\hat{Y_i}$, EX can be computed by $\\text{EX} = [\\sum \\mathbb{1}(V_i, \\hat{V_i})]/N$, where $\\mathbb{1}(\\cdot)$ is an indicator function, which can be represented as $\\mathbb{1}(V, \\hat{V}) = \\begin{cases} 1, V = \\hat{V} \\\\ 0, V \\neq \\hat{V} \\end{cases}$\n\u2022 Human-aligned Accuracy (HA) is introduced to reconcile the discrepancies between traditional benchmark evaluation metrics"}, {"title": "3.1 Overall Performance", "content": "In Table 2, we present the experimental results of the system's over- all online serving performance when leveraging the framework designated as SageCopilot. The assessed components of the exper- iment revolve around SQL query accuracy and the multifaceted"}, {"title": "3.2 Ablation Test", "content": "3.2.1 Data augmentation. In the conducted ablation studies, we sought to evaluate the efficacy of our method, denominated as SageCopilot, through different data augmentation strategies. The experiments were categorized into four distinct sets: i) the zero- shot approach, devoid of any exemplar collection, relying solely on the language model's inherent capabilities; ii) the ER method, which leverages exemplar sets to invoke the model's in-context learning by recalling similar instances; iii) the ER+SA method-our proposed approach-where similar instances are retrieved from a semantically augmented exemplar set; iv) the ER+D2N approach, where the recall procedure is applied to an exemplar set enriched with domain-specific NL2SQL augmentations.\nThe outcomes, detailed in Table 3, indicate a marked enhance- ment in both EM and EX when the ER process is employed. Incre- mental improvements were further observed with our proposed ER+SA strategy, albeit to a lesser extent. However, the introduction of the domain to NL&SQL data augmentation (ER+D2N) did not cul- minate in any notable performance gains. The stagnation observed with the ER+D2N augmentation suggests that the similarity in dis- tribution between the augmented and test samples was too great to provide a significant benefit. In contrast, the implementation of both example recall and semantic-preserving augmentation-the cornerstone techniques of SageCopilot-demonstrate effective en- hancements to the model's performance."}, {"title": "3.2.2 SQL2NL.", "content": "The SQL2NL mechanism substantively augments the automation of data augmentation and the optimization of tasks"}, {"title": "3.2.3 Schema Linking.", "content": "In the context of multiple recall, each re- trieval channel should strive to maintain independence to ensure that the target table can appear in the candidate set. The evalua- tion mainly focuses on two aspects: (1) whether the target table appears in the candidate set; and (2) the desirability of the target table appearing towards the beginning of the candidate set. Figure 9 presents the results of four retrieval experiments conducted on 100 actual business tables, with most tables containing over 100 fields. The experimental results indicate that the retrieval strate- gies achieved recall of 50%, 83%, 83%, and 92% respectively. This suggests that the strategy based on direct table schema similarity performed notably better than others based on summarization of schema details, key words and the values of the key words. It's also important to consider the potential implications of the experi- mental results. The high recall of the direct table schema similarity strategy indicates its effectiveness in identifying relevant tables and columns for a given query. In contrast, the lower recall achieved by the summarization-based strategies may suggest limitations in capturing the nuanced details necessary for precise retrieval."}, {"title": "3.2.4 Slot Feature Extraction.", "content": "In response to the observed challenge where a user's query regarding \"close ratio\" is obscured by extrane- ous language (e.g., in Appendix D.5), resulting in potential retrieval of non-germane instances by schema matching techniques, we in- troduce the Slot Feature Extraction (SFE) approach. This method's efficacy is rigorously evaluated through a direct comparison, uti- lizing EM and EX metrics, on a targeted subsection of our com- prehensive real-traffic dataset. The selected subset comprises 81"}, {"title": "3.2.5 Reflection on SQL Generation.", "content": "In our study, we conducted an ablation experiment to validate our SQL Reflection methodology by addressing diverse SQL errors such as syntax inconsistencies, refer- ences to non-existent tables, and omissions of requisite columns. Our dataset, composed of 20 distinct samples, employed Evalua- tion Metrics (EM) and Execution Accuracy (EX) as benchmarks for assessing our strategy's efficacy. The SQL Reflection strategy, which operates without detailed SQL error annotations, was em- ployed as the baseline under review and is summarized in Table 5. This baseline registered an EM score of 65% and an EX rate of 15%. However, upon the integration of explicit SQL error descriptions into the learning model, our records indicated a modest decline in EX by 5%, whilst EM exhibited a notable improvement of 15%. These findings substantiate the effectiveness of descriptive error integration in SQL query generation methodologies."}, {"title": "3.2.6 Effect of Miscellaneous Designs.", "content": "The system enhances the performance of LLMs through two innovative forms of feedback: human-to-machine and machine-to-human. By correcting errors in SQL responses manually and feeding these corrections back into the system, human-to-machine feedback significantly improves system accuracy, as visually represented in Figure 21. Meanwhile, machine-to-human feedback recognizes the need for human intervention when the system's responses may not be accurate. In such cases, the dialogue system seeks additional information from a human to clarify ambiguous queries. This ensures responses are precise and tailored to the user's intended meaning, creating a dynamic, itera- tive loop that continuously refines the system's performance and user experience. We show the supplementary results in Appendix E."}, {"title": "3.3 Lessons Learned and Discussions", "content": "In this section, we outlined the deployment and experimental eval- uation of our advanced system, SageCopilot, in an industrial set- ting. Our exploration covered the system architecture, datasets, performance metrics, and the impact of design enhancements. Re- sults showed that while not every SQL query was an exact match (EM), correct results (EX) were frequently produced, indicating that the system can tolerate some structural variances. The overall assessment confirmed the system's consistency in generating text responses and charts, though it suggested that analysis depth and chart selection could be improved. Ablation studies revealed that methods like example recall and schema similarity significantly affected system performance, with SQL2NL and Slot Feature Extrac- tion improving EX scores by aiding in accuracy. Evaluating SQL generation strategies also demonstrated the benefits of incorporat- ing error descriptions for better EM rates. Overall, the evaluation highlighted effectiveness of SageCopilot in SQL generation and data analysis, as well as the significance of human feedback mechanisms in refining precision, thereby providing insights into enhancing LLM systems in complex industrial environments.\nIn response to the limitations in LLM capabilities and the com- plexity of SQL data within the industrial sector, we have developed an array of engineering solutions and learned lessons from realistic deployment. Lesson 1: Confronting the issue of restricted token input length for LLM (that make it impossible to put all schemes and attribute descriptions into one prompt), our research has adopts an innovative \"multiple recall and schema linking\" paradigm. This technique facilitates the association of user inquiries to expansive datasets, including numerous tables and columns. Lesson 2: In scenarios where SQL queries are characterized by complexity and include shared components, an advanced strategy was employed: the restructuring of original SQL queries through the creation of views. This strategic alteration is cataloged in Appendix D.7 and has resulted in a significant enhancement of end-to-end query ac- curacy, approximately by 50% in our experiments. Lesson 3: The end-to-end correctness requires not only self-reflection, but also the continuous collection of demonstrations for ICL, as feedback from the online phase, to improve the system accuracy."}, {"title": "4 CONCLUSION", "content": "In summary, this paper introduces SageCopilot, an advanced, industry- grade system that offers automated data science pipeline by effec- tively integrating LUIs, AutoAgents, databases, data visualizers and LLMs. It delivers an end-to-end solution capable of handling natu- ral language instructions on querying, analysis, and visualization tasks with minimal human intervention. SageCopilot incorporates a two-phase design an online component refining users' inputs into executable scripts through In-Context Learning (ICL) and run- ning the scripts for results reporting & visualization, and an offline preparing demonstrations requested by ICL in the online phase. Various prompt-tuning strategies have been used to augment Sage- Copilot for enhanced performance. Our rigorous evaluation across real-world scenarios demonstrates the advantages of SageCopilot in generating/executing scripts and offering results with visualization. Open issues and the lesson learned have been discussed as part of our contribution from industry perspectives."}, {"title": "A RELATED WORK", "content": "The quest to bridge the gap between natural language and data- base querying has led to the emergence of the NL2SQL field, which"}, {"title": "B SYSTEM CONFIGURATION", "content": "[TABLE 6]"}, {"title": "C MAIN FLOW", "content": "C.1 Intent Understanding and Decision-Making\nIn practical business settings, users often engage in multi-turn dia- logues, posing follow-up questions subsequent to an initial inquiry. These successive questions, however, are frequently characterized by missing context or complete departure from the original topic. Mishandling such queries as definitive intents can result in erro- neous outcomes and needless processing time. For instance, fol- lowing an initial question about authorization request counts in October '22, a user may ask, \"What about June?\" or prompt for visual representations like \"Could you draw a bar chart?\" or pose unrelated questions such as \"How's the weather today?\". To navi- gate these multifaceted interactions, a systematic approach to intent understanding and decision-making is crucial. This involves meth- ods for comprehensive interpretation to supplement incomplete follow-ups, the ability to directly generate visualizations from query results, assistance in chart type selection for optimal data portrayal, and relevance filtering to appropriately address or dismiss off-topic queries."}, {"title": "C.2 SQL Generation", "content": "Delving into the industry realm, consider a user question such as \"What is the trend in employee [Zhou Hui]'s monthly sales in [the second half of 2022]?\" Initially, we extract the pertinent schema information from the domain of the user's query, analogous to the procedures for a chain supermarket domain. We then compute the similarity between the user's question and data vectors in the vector database to select the most suitable examples containing <Query, SQL> pairs, which is shown in Figure 10. These examples serve as contextual aid in formulating a precise SQL statement."}, {"title": "C.3 Chart Generation", "content": "Albeit the generation of an accurate SQL statement, presenting the resulting data to users in an intuitive format remains a challenge. To bolster user experience, two chart generation methods were employed, leveraging the in-context capabilities of the LLM agent and external knowledge bases.\nThe first method involves generating complete eCharts JSON code. For instance, for the real user question concerning the sales trend of employee [Zhou Hui] in [2021], the LLM agent determines that a line chart would best illustrate the desired trend. The final eCharts code, which includes additional interactive features like a title, legend, and tooltips, is crafted through a prompt, with the result displayed in Figure 12.\nThe second method applies rule normalization to expedite the LLM agent's processing time and diminish variation in personal- ized scenarios. We introduced a simplified prompt structure for constructing basic two-dimensional charts. This is exemplified in the query about the highest total profits among products for March 2022, where we translate SQL results into a list before extracting col- umn names for prompt incorporation, as outlined in Appendix D.3. The immediate inference is that product names populate the x-axis and total profits the y-axis, with a bar chart effectively emphasiz- ing the ranking. This refined process enables prompt, adaptable insertion of query results into tailored chart templates, evidenced by the output in Figure 13."}, {"title": "C.4 Analysis and Summary Capability", "content": "While general LLMs are adept with vast textual information, their proficiency in custom and specialized vertical knowledge domains can be limited. To address specialized queries more empathetically and accurately, we have developed a pipeline incorporating prompt engineering and external knowledge bases. For example, in ana- lyzing and predicting time-series data, the SageCopilot begins by consulting an external knowledge base for relevant insights into the user's issue. It then assimilates this information, grasps the user's underlying intent, and proceeds with the generation and execution of the corresponding SQL statements to gather the relevant time- series data. It's necessary to call a time-series prediction model for further analysis, with the LLMs for accessing the prediction model, referencing pertinent API documentation from the knowledge base. It then delivers insightful analyses to the user, including trends, periodic behavior, and outlier detection."}, {"title": "D THE EXAMPLE OF PROMPTS AND CASES", "content": "In this section, we disclose the prompts and actual cases employed in the development of SageCopilot."}, {"title": "D.1 Prompt: Text Analysis.", "content": "TEXT_ANALYSIS_TEMPLATE =\nQuestion: {query}\nDatabase Query Result: {result}\nBased on the provided question and the results from the database query, please describe the results line by line using appropriate language. The\ndescription should cover the full results and provide concise conclusion,\nfocusing solely on the core issue. There is no need to display any charts,\nbut if the results are related to time, please infer the relevant time range\nretrospectively. If the query results are empty, return a suitable\ndescription without making any assumptions."}, {"title": "D.2 integretion.", "content": "SQL2NL_TEMPLATE = \"\"\"You are now required to generate user questions based on table information and SQL statements: {table_info}\nGenerate 3 instances of user questions based on table information and SQL statements:\n[SQL Statements]: {sql}\n[Generated Questions]:\"\"\""}, {"title": "D.3 Prompt: Axis Checker.", "content": "AXIS_CHECKER = \"\"\"Based on the problem and chart description analysis, select the horizontal and vertical coordinates from {column_name}\nKeep the original information in column_name\nChart types: line, bar, pie\nBased on the table description analysis, select only any one type from the chart types.\nOnly returns a```json```format code\nThe structure only contains\n{{\n\"xAxis\": \"\"\n\"yAxis\": \"\"\n\"type\": \nTable description: {column_desc}\"\"\""}, {"title": "D.4 Prompt: Chart Generation.", "content": "[Figure 17]."}, {"title": "D.5 Case: Key Word Missing in Generated SQL.", "content": "Question: What is the closure rate of online issues for the Intelligent Office Platform Department in August 2023?\npred:\nSELECT\nSUM(\nCASE\nWHEN closed_time IS NOT NULL\nAND closed_time > begin_time THEN 1\nELSE\nEND\n) AS closed_count,\ngold:\nSELECT\nCONCAT (\nROUND (\nCOUNT(IF (status IN ('closed', 'finished', 'published'), 1, NULL)) /\nCOUNT(*) * 100,\n2\n),\n'%'\n) AS close_ratio FROM..."}, {"title": "D.6 Case: Prediction with Prophet [Figure 19].", "content": "The Prophet\u00b2 forecasting model is known for its intuitive approach to time series data by focusing on components like trend, season- ality, and"}]}