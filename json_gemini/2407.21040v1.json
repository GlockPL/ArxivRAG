{"title": "Towards Automated Data Sciences with Natural Language and SageCopilot: Practices and Lessons Learned", "authors": ["Yuan Liao", "Jiang Bian", "Yuhui Yun", "Shuo Wang", "Yubo Zhang", "Jiaming Chu", "Tao Wang", "Kewei Li", "Yuchen Li", "Xuhong Li", "Shilei Ji", "Haoyi Xiong"], "abstract": "While the field of NL2SQL has made significant advancements in translating natural language instructions into executable SQL scripts for data querying and processing, achieving full automation within the broader data science pipeline-encompassing data querying, analysis, visualization, and reporting-remains a complex challenge. This study introduces SageCopilot, an advanced, industry-grade system system that automates the data science pipeline by integrating Large Language Models (LLMs), Autonomous Agents (AutoAgents), and Language User Interfaces (LUIs). Specifically, SageCopilot incorporates a two-phase design: an online component refining users' inputs into executable scripts through In-Context Learning [4] (ICL) and running the scripts for results reporting & visualization, and an offline preparing demonstrations requested by ICL in the online phase. A list of trending strategies such as Chain-of-Thought and prompt-tuning have been used to augment SageCopilot for enhanced performance. Through rigorous testing and comparative analysis against prompt-based solutions, SageCopilot has been empirically validated to achieve superior end-to-end performance in generating/executing scripts and offering results with visualization, backed by real-world datasets. Our in-depth ablation studies highlight the individual contributions of various components and strategies used by SageCopilot to the end-to-end correctness for data sciences.", "sections": [{"title": "1 INTRODUCTION", "content": "The advancements in Text-to-SQL generation propose promising approaches to convert textual instructions into structured query language (SQL) for database operations [7, 15, 17, 20, 21]. However, achieving full automation in the data science pipeline is challenging [7] due to the complexity of understanding user intents [6, 9], integrating access control mechanisms [8], and providing user-intention-driven visualizations of query results [2]. Thus, a comprehensive automation system needs to encapsulate intention understanding, data retrieval and analysis, and the generation of visual outputs, which traditionally requires expert involvement.\nThe integration of LLMs and AutoAgents has paved the way for establishing comprehensive data science pipelines that connect language user interfaces (LUIs), databases, and data visualization tools with LLMs [19]. AutoAgents can clarify user intentions using conversational interfaces and then plan for data querying, analysis, and visualization by utilizing LLM capabilities. By coordinating tasks across databases, authenticators, and visualization tools, as well as employing SQL & scripts generation [1, 3, 13], and data format conversions [1], AutoAgents can fulfill complex data science requirements autonomously. Though techniques to improve every single component, such as NL2SQL, have been intensively studied in previous works [1, 3, 7, 13, 15, 17, 20, 21], integrating these components is still a challenging task for achieving the goal of automated data sciences in an industry settings. Several non-trivial technical issues should be addressed as follows.\n\u2022 Closed-Loop System with Multi-Tool: Existing systems, such as ChatGPT or Code Interpreter, either generate scripts under users' supervision or manipulate uploaded datasets for potential data analysis. Human intervention (e.g., checking the correctness of the generated scruots, copying and pasting the scripts to a local execution environment for further data manipulations) might be indispensable between every steps of interactions. However, for full automation of data science pipelines, there needs a system that processes user requests, manages databases, executes scripts, and reports analysis results in a seamless closed-loop, minimizing human intervention.\n\u2022 Domain Adaptation and Generalization: To enhance the accuracy of script generation, supervised fine-tuning (SFT) of LLMs subject to various tasks is desired. However, SFT for adapting every data domain (e.g., databases for groceries stores, wholesales, corporate finance, etc.) or collecting a sufficiently large dataset for cross-domain generalization is resource consuming. Inappropriate SFT could hurt emergent abilities of an LLM due to catastrophic forgetting [12]. Thus, in addition to domain-specific SFT, the system should be capable of adapting to every data domain in the context of interactions with LLMs.\n\u2022 End-to-End Correctness and Reflection: Though every single component could be optimized with better accuracy or capacity, the"}, {"title": "2 FRAMEWORK DESIGN", "content": "As depicted in Figure 1, the SageCopilot framework presents a sophisticated dual-phase approach to managing and processing natural language queries against complex databases. In the offline phase, significant emphasis is placed on the preparation and enrichment of a robust data foundation that underpins the online operational stage. Through metadata semantic governance, the framework lays the groundwork for a deep understanding of the data structure, ensuring comprehensive query understanding and accurate SQL generation. The construction of a rich seed data repository, supplemented by effective data augmentation strategies, further strengthens the system's ability to produce precise query responses.\nTransitioning into the online phase, SageCopilot seamlessly shifts from data curation to real-time user engagement. The framework's intent understanding and decision-making module utilizes the nuanced capabilities of LLM to interpret diverse user queries, cater to non-standard question phrasings, and provide contextually relevant visualizations. The system's adeptness at memory recall and schema linking further streamlines the process, enabling efficient and relevant information retrieval. The integration of SQL generation with in-context prompting, coupled with the reflective oversight offered by the SQL reflection module, ensures the production of accurate and logically coherent database queries. These features, alongside the essential components of tool use and authentication, confirm that SageCopilot operates with precision, reliability, and security. Result presentation stands as the final touchpoint"}, {"title": "2.1 Offline Phase", "content": "The offline phase is devoted to the careful preparation of high-quality data that will be used in the online phase. This preparatory work involves four key activities: practicing metadata semantic governance, constructing a base of seed data, enhancing the dataset through data augmentation, and extracting information for schema linking. Once collated and refined, these datasets are embedded into a memory vector database which, in turn, equips the online phase with the necessary contextual data to meet specific user queries."}, {"title": "2.1.1 Schema Semantic Governance", "content": "Schema information is essential for data analysis tasks such as SQL writing, akin to the necessity for humans to understand table schematics. SageCopilot generates complex SQL statements aligned with human intentions by interpreting schema details. A detailed exposition of table metadata is indispensable for accurately matching user intent. For fields with enumerated values, a comprehensive description of the values stored in the database and their interpretations is required. In cases involving complex data types like structures and maps, a detailed account of key attributes and their meanings is crucial. This detailed metadata aids in understanding the complex relations between attributes, allowing for accurate data retrieval in response to queries. Additionally, SQL dialects should be annotated to ensure generated SQL adheres to the specific syntax required by the target system."}, {"title": "2.1.2 Seed Data Construction", "content": "The capacity of current LLMs in generating complex SQL autonomously is fairly restricted. We leverage the ICL ability of the LLMs to bolster the stability and accuracy of SQL generation by incorporating dynamic examples of pairs relevant to the user's current question. There are three strategies for constructing these seed data pairs. The first is populating pairs common in the business's everyday workflows, especially because certain queries routinely include bespoke business indicators. Without few-shot prompting or fine-tuning mechanisms, expecting large models to autonomously generate user-specific SQL is impractical. The second method is the SQL2NL technique, which facilitates the cold start process without imposing the requirement for users to supply pairs, particularly given the abundance of SQL in the enterprise data warehouse. In this context, natural language queries are generated by LLMs based on SQL and schema information, thereby constituting pairs. The final approach involves deriving pairs from user feedback, targeting instances where LLMs have failed to produce precise results. Here, human intervention is key to correcting inaccuracies."}, {"title": "2.1.3 Data Augmentation Techniques", "content": "SageCopilot incorporates two data augmentation strategies to enhance the performance and adaptability, whose ablation study detailed in Section 3.2.1.\n\u2022 Semantic-Preserving Augmentation - the aim is to bolster the initial dataset, utilizing the LLM to transform a given query into"}, {"title": "2.1.4 Schema Linking Information Extraction", "content": "We utilize the analysis of SQL syntax trees for multiple dialects to convert our dataset from format into a format. This transformation creates a direct linkage between user-generated queries and their associated database fields."}, {"title": "2.1.5 Memory Vector Database Integration", "content": "The memory vector database is pivotal in bridging the gap between offline preparation and online functionality. It encapsulates and consolidates data into a memory structure that unites both the original and augmented data pairs-including along with including to support robust and efficient data retrieval and management."}, {"title": "2.2 Online Phase", "content": "In the online phase, the system delivers a refined user experience by discerning user intent and generating precise SQL queries through a series of LLM-driven modules. The system begins with intent recognition, proceeding to link questions to database schemas, and then crafting SQL statements guided by user queries paired with similar examples. The SQL Reflection module improves accuracy by correcting errors in SQL queries. After authentication, the database runs the SQL and results are processed. The Result Generation module translates these results into textual analyses, avoiding visual complexity while still offering comprehensive insights. A Visualization component transforms data into charts. Additionally, a Forecast capability allows for the prediction of time-based data trends through simple, intuitive LLM-user interactions. Overall, the online phase is a streamlined sequence ensuring precise, user-aligned outputs, from query interpretation to actionable predictions."}, {"title": "2.2.1 Intent Understanding and Decision-Making", "content": "In authentic business environments, diverse user inquiries arise, including content beyond query parameters, topics outside the scope of the domain, incomplete queries, and follow-up questions that challenge continuity. To navigate these intricacies, an LLM-powered intent understanding and decision-making mechanism has been meticulously designed. This mechanism integrates four principal components: comprehensive intent assimilation, assessment of relevance, provision for direct query result visualization, and chart type discernment, all aligning towards heightened service precision for users.\nThis system harnesses user inquiries, analogous example queries, and predefined decision metrics to formulate elaborate prompts that optimize LLM responses. The comprehensive intent understanding component is calibrated to capture the essence of user requirements, addressing lacunae within multi-turn dialogues. The relevance assessment ascertains the pertinence of queries to the currently engaged topic, thereby filtering out unrelated or off-topic"}, {"title": "2.2.2 Multiple Recall & Schema Linking", "content": "In SageCopilot, thematic domain schemas involving 1 to N tables are depicted. The challenge in practical business situations is handling expansive tables within the confined contextual bandwidth of LLM prompts. This context constraint renders the integration of comprehensive schema details for all N tables into a solitary prompt impractical. Moreover, incorporating superfluous schema information can detrimentally impact model effectiveness. As a remedy, we introduce a schema linking module preceding SQL formulation to circumvent the complications of excess schema inputs.\nContemporary schema linking interventions [10, 15, 16] often revolve around the development of fresh models, necessitating substantial resources with potential limitations in domain-agnostic generalizability. Counteracting this, our method employs a preliminary multi-recall tactic to pinpoint tables aligning with the user's query intention. The LLM is then utilized to forge links between the user's question and the relevant tables and fields.\nWe execute the multi-recall approach through two principal channels: (1) a similarity-based recall that aligns user queries with table schemas, and (2) a homologous recall that harnesses pre-stored schema linking data from a memory vector database initialized during an offline phase. These distinct channels function autonomously, guaranteeing that the targeted table is suitably represented within the pool of candidates. After the table recall, Schema Linking particulars are deduced from the prescribed prompts as detailed in Figure 2. This dual-phase methodology fosters efficient SQL generation for voluminous tables within specific domains and streamlines the schema linking process, thereby enhancing the model's efficacy and generalization potential."}, {"title": "2.2.3 SQL Generation & In-Context Prompting", "content": "The objective of this module is to accurately generate SQL statements by fusing intent comprehension with schema linking, utilizing the in-context capabilities of LLMs. A significant technical challenge is crafting precise prompts with a restricted token input capacity. Typically, in-context examples in Figure 3 are comprised of pairs retrieved from a vector database via similarity search. Initially, an input query is converted into an embedding vector\u00b9. This vector is then compared to the closest pair in the vector database by calculating the Inner Product (IP) distance, a ubiquitous"}, {"title": "2.2.4 SQL Reflection", "content": "The SQL Reflection module is crafted to harness the exceptional proficiency of LLMs in amending flawed SQL expressions, significantly elevating SQL accuracy. Through detailed evaluation and comprehensive review, we scrutinize the veracity of SQL commands, delving into error etiologies encompassing verifications for table and column presence, as well as syntactic integrity. The SQL Reflection mechanism is activated solely upon the unequivocal identification of an anomaly within the SQL command. The configuration of the prompt is meticulously engineered, delineating not just the error typologies but also encompassing the SQL queries, explicit table schemas, and in-depth diagnostic evaluations of the discrepancies. Such a framework primes the LLM to concentrate on redressing the underlying SQL inaccuracies, thus garnering an accurate and meticulously directed correction process. The efficacy and practicability of this module have been corroborated through empirical studies detailed in Section 3.2.5. The data derived from these experiments solidifies the premise that this modality is not merely efficacious but also eminently viable."}, {"title": "2.2.5 Tool Use & Authentication", "content": "Our system, SageCopilot, accommodates SQL command execution across a spectrum of dialects, spanning MySQL, PostgreSQL, Flink SQL, Hive SQL and Spark SQL. It is noteworthy that queries executed via MySQL and PostgreSQL"}, {"title": "2.2.6 Result Generation", "content": "Upon the retrieval of execution outcomes from the database engine, the result generation module commences its role in constructing the conclusive output for web-based applications. Demonstration of this process is evident in the lower right segment of Figure 1, wherein a diversely formatted report analogous to a portfolio is composed. Such a report encompasses textual analysis, visual representations through graphs and charts, and tabular data. Additionally, this phase may initiate further user interaction through the integration of predictive services and knowledge-driven analytical processes.\nText Analysis. Predominantly, the result generation module acting online employs a LLM agent for the transformation of execution results, articulated in markdown or JSON-esque structures, into textual discourse. The composition of the textual output is not limited to the portrayal of SQL execution outcomes but also encapsulates a synthesized overview and preliminary analysis of the data. The prompt utilized for guiding the LLM agent is formulated in Appendix D.1. In such an approach, the aspect of visualization has been deliberately decoupled, as a distinct module with an exclusive focus on graphical representation has been specifically crafted to augment the inherent capabilities of chart formation to their fullest extent without complication. It warrants attention that under certain circumstances, users may venture inquiries pertaining to key, yet uncomputed, metrics-take for instance the \"closure rate\", potentially non-existent within the current table schema. This presents a challenge as the LLM may find itself incapable of delivering the anticipated response. A remedy to this quandary is the activation of a knowledge enhancement mechanism, prompting the user for the precise formula necessitated for the computation of the \"closure rate\". Upon acquisition of the accurate or anticipated formula, the LLM is equipped to amalgamate it with the SQL execution findings, thereby generating an apt and tailored response.\nVisualization. In order to refine front-end processing and deliver a more comprehensible presentation of graphical data, we utilize the capabilities of an LLM agent to generate bar, line, and pie chart code in Echarts-compatible JSON format. However, due to the broad range of outputs possible from the LLM agent, the produced visualiza"}]}