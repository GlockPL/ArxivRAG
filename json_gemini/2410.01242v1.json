{"title": "RGD: Multi-LLM Based Agent Debugger via Refinement and Generation Guidance", "authors": ["Haolin Jin", "Zechao Sun", "Yiheng Yang", "Huaming Chen"], "abstract": "Large Language Models (LLMs) have shown incredible potential in code generation tasks, and recent researches in prompt engineering have enhanced LLMs' understanding of textual information. However, ensuring the accuracy of generated code often requires extensive testing and validation by programmers. While LLMs can typically generate code based on task descriptions, their accuracy remains limited, especially for complex tasks that require a deeper understanding of both the problem statement and the code generation process. This limitation is primarily due to the LLMs' need to simultaneously comprehend text and generate syntactically and semantically correct code, without having the capability to automatically refine the code. In real-world software development, programmers rarely produce flawless code in a single attempt based on the task description alone, they rely on iterative feedback and debugging to refine their programs. Inspired by this process, we introduce a novel architecture of LLM-based agents for code generation and automatic debugging: Refinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based agent debugger that leverages three distinct LLM agents-Guide Agent, Debug Agent, and Feedback Agent. RGD decomposes the code generation task into multiple steps, ensuring a clearer workflow and enabling iterative code refinement based on self-reflection and feedback. Experimental results demonstrate that RGD exhibits remarkable code generation capabilities, achieving state-of-the-art performance with a 9.8% improvement on the HumanEval dataset and a 16.2% improvement on the MBPP dataset compared to the state-of-the-art approaches and traditional direct prompting approaches. We highlight the effectiveness of the RGD framework in enhancing LLMs' ability to generate and refine code autonomously.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have made significant advancements in the domain of automated code generation, showcasing their capability to translate natural language into functional code, generate code explanations [1], and even perform code-to-code translations across different programming languages [2] [3]. The most prevalent methods for employing LLMs in code generation rely heavily on prompt engineering, where well-crafted prompts are designed to guide the LLM in generating code snippets or interpreting existing code [4] [5]. This approach has proven effective in a range of scenarios, from generating code based on textual descriptions to converting code between different languages and frameworks.\nHowever, the approach of generating code from text in a single pass has its limitations. Code generation is inherently a complex task, and a one-time generation approach often fails to account for the numerous edge cases and detailed task requirements that arise in software development, especially given the complexity and precision needed in software development tasks [6]. Consequently, researchers have introduced multi-round code generation frameworks that involve iterative refinement. These frameworks iteratively generate programs through multiple interactions, significantly improving the quality of program synthesis and making the development process more efficient and accurate [7]. Researchers have explored ways for LLMs to autonomously learn from errors and perform debugging and repairs. These frameworks leverage reflection, including failed test cases and error messages, and learns from these outcomes to improve subsequent code generations [8]. Although these approaches have demonstrated significant improvements, they cannot guarantee that every reflection result will lead the LLM to make effective changes based on failed test cases. As a result, LLMs often end up generating the same code over multiple iterations.\nMoreover, the performance of LLMs in code generation is highly dependent on the clarity and completeness of the task description provided in the prompts, there is a high likelihood of the LLM overlooking critical edge cases or missing essential requirements. Previous studies have attempted to address this issue by introducing the concept where the LLM first reasoning the task and then proceeds with code generation [9] [6]. However, this strategy presents its own challenges; the LLM tends to rely heavily on the initial plan, leading to a lack of flexibility as it fails to incorporate further refinements or adjustments that may be necessary to address evolving requirements or unforeseen issues in the code.\nIn this paper, we introduce a novel framework called RGD (Refinement and Guidance Debugging) that leverages multiple LLMs in a collaborative manner to improve the quality of code generation. RGD incorporates multiple specialized LLMs, each with distinct roles, to simulate a comprehensive code repair process. The RGD framework utilizes three LLMs: the Guide LLM, responsible for generating a generation guide based on the task description, then passed the generation guide and the task description to the Debug LLM for initial code generation. The third LLM is tasked with consolidating the execution results and conducting failure analysis. The feedback LLM will use generated code and failed test cases to analysis the reason of failure and potential fix procedures."}, {"title": "II. RELATED WORK", "content": "Large Language Models for Code Generation Recently, large language models (LLMs) have demonstrated outstanding performance multiple tasks especially in the code generation, showing great potential across various benchmarks [4] [15] [16]. However, when faced with complex problems, these models are prone to generating hallucinated responses [17]. Prompt engineering [18] has reduced the need for extensive fine-tuning of LLMs on specific downstream tasks, which can better understand user requirements through contextual information. Chain-of-thought prompting [6], which guides LLMs to provide step-by-step responses, enhances their reasoning abilities, leading to significant improvements over baselines on multiple benchmarks [19]. Tree-of-thought (ToT) [20] framework enhances the reasoning capabilities of large language models by structuring the reasoning process as a tree search. Each node represents a possible reasoning state, and edges denote transitions between states. In addition, researchers have explored enabling LLMs to debug the code they generate on their own [21], allowing the models to autonomously fix errors by understanding the causes from generated outputs [22] [8]. Beyond self-repair based on execution results, these frameworks can also leverage external tools for dynamic and automatic code correction [23].\nInformation Retrieval is a popular strategy recently, which involves retrieving helpful information from local storage or the cloud to assist in generation tasks [24]. It has gained"}, {"title": "III. REFINMENT AND GUIDANCE DEBUGGING", "content": "Following recent works [14] [8], we divide the benchmarking dataset HumanEval and MBPP into three components: (Q, Tv, Th, E). Here, Q represents the task description, which includes code snippets and requirements in natural language. Tv stands for visible test cases, Th for hidden test cases, and E is the entry point. All test cases are executed starting from the entry point."}, {"title": "B. Guide LLM Agent", "content": "The Guide Agent is assigned a special role with the primary task of conducting initial reasoning and thinking. It dynamically adjusts the input information based on the task's complexity and the current stage of execution. The Guide LLM is also responsible for selecting and applying relevant guides extracted from the Memory Pool. However, to avoid unnecessary token usage and excessive context that may lead to LLM overhead, the memory pool matching and extraction are not performed during the initial generation. As shown in Figure 2, different prompts are used by the Guide LLM at the initial stage and the debug stage. In the initial stage, the Guide Agent generates a Generation Guide based on the task description Q and entry point E without adding any additional information. During the debug stage, samples are matched from the memory pool based on task similarity. The system's matching mechanism includes the following components:\n\u2022 Q: The task description.\nThe Guide LLM generates a final guide Gfinal by augmenting the initial guide Ginit generated from Qnew with the relevant guide G\u2081 retrieved from the memory pool based on the similarity score:\nGfinal = GuideLLM(Ginit, Gi, Q, A) (2)\nwhere A represents the failure analysis generated by the Feedback Agent which will be introduced in (Section. III-C). The use of a memory pool to augment the Guide LLM with"}, {"title": "C. Debug LLM Agent", "content": "For the Debug Agent, after receiving the generated Generation Guide, it works in conjunction with the task description Q and the entry point E to facilitate code generation. The prompts used for the initial code generation and for fixing code are different, and during the initial generation phase, there is no matching with the memory pool or any failure analysis involved. This choice is similar to the approach in previous work [14], where seed code is generated before debugging to prioritize simpler tasks. However, if require running an initial execution to generate seed code for 500 samples before proceeding with debugging like MBPP dataset-this seed process can be extremely time-consuming. Especially in the case of MBPP-ET, where the length of the test set is several times longer than the original, it can lead to a significant waste of time on execution and computational costs, followed by the manual running of debugging procedures. Conversely, for smaller dataset HumanEval, this approach can accelerate the overall debugging process. In RGD, simpler tasks are filtered out in the first round, thereby streamlining the debugging process."}, {"title": "D. Feedback LLM Agent", "content": "The Feedback Agent is quite similar to the previous reflection strategy [35], where the LLM analyzes the code errors based on execution results to achieve self-repair. In the RGD architecture, the Feedback Agent plays a similar role by first recording both the failed and successful test cases and then consolidating them based on the execution results. If an exception occurs during execution, the type of exception is also recorded as part of the Real Execution Output. Subsequently, both the passed visible test cases and the failed visible test cases are provided to the Feedback LLM for analysis. This analysis also includes the failed code and the task description, as shown in Figure 4. The final feedback generates a failure analysis, which serves as auxiliary information for both the Guide LLM Agent and the Debug LLM Agent.\nDirectly regenerating code based on execution results and failed code often results in output that is far inferior to handling a single task [36]. The Feedback Agent is responsible for independently processing the failure analysis and providing possible modification suggestions. By isolating the failure analysis and proposed corrections, the Feedback Agent enables the LLMs to perform better when dealing with complex tasks that require nuanced debugging and iteration."}, {"title": "IV. EXPERIMENT SETUP", "content": "During the execution process, we use the test sets from various benchmarks to verify if the current code contains any bugs. In RGD, We utilized the HumanEval [4] and MBPP [12] datasets, and followed the approach in [14] for allocating visible and hidden test cases. In HumanEval, the visible tests are extracted from the task description, while the hidden tests are taken from the dataset's own tests. In MBPP, the first test case is used as the visible test case.\nAdditionally, we use HumanEval-ET and MBPP-ET [11] to address the limitations of test cases from the original datasets. For these datasets, the ratio of visible to hidden test cases is set to 6:4. The last dataset we used is APPS [13], from which we selected 100 samples for testing with difficulty levels categorized as introductory, interview, and competition in the ratio of 5:3:2. Unlike the previous datasets, APPS relies on input-output results for validation, requiring a unique handling approach when processing the APPS dataset."}, {"title": "B. Method", "content": "We compared RGD with five different approaches. Direct is the baseline measurement, where code is generated directly based on the task description. Chain-of-Thought [5] enhances the model's self-planning ability by adding step-by-step prompts. Self-Planning [6] is based on the idea of planning first and then reasoning. Self-Debugging [8] involves self-reflection by incorporating feedback from execution results. Lastly, we also tested the LDB framework [14]."}, {"title": "V. EXPERIMENT RESULT AND ANALYSIS", "content": "The main experimental results are presented in Table I. Here, Acc represents the Pass@1 accuracy, and A denotes the percentage improvement over the direct code generation approach based on the task description. We can observe that RGD demonstrates state-of-the-art performance across all datasets, with further improvements compared to the LDB approach. For the HumanEval dataset with GPT-40, RGD shows an improvement of 9.8% over the baseline and 3.1% over LDB. Our experiments reveal that RGD achieves particularly significant improvements on the HumanEval-ET and MBPP-ET benchmarks, which contain more test cases, with a maximum improvement of 25.1% on MBPP-ET. This performance boost is mainly due to the limited number of visible test cases in the original datasets, leading to situations where code that passes the specific tests still contains vulnerabilities and fails the hidden test cases, this highlights the critical role of the Feedback Agent's analysis throughout the debugging process.\nFor the APPS dataset, we conducted experiments using a total of 100 samples. We did not test the LDB framework on the APPS benchmark because it would require significant modifications to its program. Overall, it is evident that RGD achieves enhancements over other methods, with a maximum improvement of 8.6% in the GPT-40-mini model."}, {"title": "B. Ablations Studies", "content": "We conducted ablation studies to validate the effectiveness of our approach. We selected GPT-40 for experiments on two benchmarks, HumanEval and MBPP, and compared the results with the original RGD framework's performance on these benchmarks, which were 97.6 pass@1 for HumanEval and 83.4 pass@1 for MBPP, respectively. We performed ablation tests under three different scenarios:\nMemory Pool Removal: In this scenario, we removed the memory pool, and the Guide Agent no longer retrieved cases from it to generate and refine guides. As shown in Table II, this resulted in a drop of 1.9 percentage points on the HumanEval dataset and a drop of 5 percentage points on the MBPP dataset.\nGuide Agent Removal: In this case, we removed the entire Guide Agent component, this led to a 4.4% drop on the HumanEval dataset and a 6.4% drop on the MBPP dataset. The results clearly demonstrate the significant impact of the Guide Agent on the overall performance of the RGD framework.\nFailure Feedback Removal: Here, we removed the Failure Feedback component, and the Feedback LLM did not generate Failure Analysis to assist with debugging and refinement. The results showed performance drops on both datasets, especially on the MBPP dataset, where the decrease was as high as 9.8%.\nThe ablation study results confirm that the inclusion of the Memory Pool, Guide Agent, and Failure Feedback significantly enhances the overall performance of the RGD framework. Furthermore, we observe a trend from the drop percentages: datasets with more samples (e.g., MBPP) tend to experience a more substantial impact from the ablations."}, {"title": "VI. CONCLUSION", "content": "In this paper, we present RGD, a novel framework designed to enhance code generation by leveraging a Memory Pool for refining Generation Guides and incorporating feedback through a dedicated Feedback Agent. Our approach allows LLMs to iteratively refine generated code by utilizing previously stored memory and dynamic failure analysis. By selectively extracting relevant guides from the Memory Pool and continuously refining them based on task similarity, RGD significantly boosts the accuracy of code generation across multiple benchmarks. Experiments demonstrate that RGD achieves state-of-the-art performance in code generation and debugging. We anticipate that our work further demonstrates and enhances the capability of LLMs to learn from past, and efficiently adapt to new challenges."}]}