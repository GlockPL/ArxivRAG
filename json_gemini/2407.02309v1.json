{"title": "SEMANTICALLY GUIDED REPRESENTATION LEARNING FOR ACTION ANTICIPATION", "authors": ["Anxhelo Diko", "Danilo Avola", "Federico Fontana", "Bardh Prenkaj", "Luigi Cinque"], "abstract": "Action anticipation is the task of forecasting future activity from a partially observed sequence of events. However, this task is exposed to intrinsic future uncertainty and the difficulty of reasoning upon interconnected actions. Unlike previous works that focus on extrapolating better visual and temporal information, we concentrate on learning action representations that are aware of their semantic interconnectivity based on prototypical action patterns and contextual co-occurrences. To this end, we propose the novel Semantically Guided Representation Learning (S-GEAR) framework. S-GEAR learns visual action prototypes and leverages language models to structure their relationship, inducing semanticity. To gather insights on S-GEAR's effectiveness, we test it on four action anticipation benchmarks, obtaining improved results compared to previous works: +3.5, +2.7, and +3.5 absolute points on Top-1 Accuracy on Epic-Kitchen 55, EGTEA Gaze+ and 50 Salads, respectively, and +0.8 on Top-5 Recall on Epic-Kitchens 100. We further observe that S-GEAR effectively transfers the geometric associations between actions from language to visual prototypes. Finally, S-GEAR opens new research frontiers in anticipation tasks by demonstrating the intricate impact of action semantic interconnectivity.", "sections": [{"title": "1 Introduction", "content": "Anticipating future actions is a key attribute of human intelligence for navigating the world. This remarkable skill translates directly to advanced computer vision applications such as self-driving cars [1, 2] or wearable assistants [3, 4], enabling safer navigation and better user experience [5].\nRecent developments in deep learning techniques have boosted the research on video understanding, reaching remarkable milestones on tasks like action recognition [6, 7, 8, 9, 10, 11]. Models related to action recognition can extrapolate essential spatiotemporal information from videos of isolated actions and correctly classify them. However, real-world applications operate in dynamic environments where actions are interconnected. For instance, imagine a self-driving car observing pedestrians. Predicting their intent to cross the street requires analyzing how observed dynamics relate to likely future events. This temporal misalignment between observation and future target introduces a challenge for recognition models to capture actionable insights, proving them insufficient and shifting the attention towards action anticipation [1, 3, 12, 13, 5, 4]. This emerging research area focuses on enabling vision systems to predict future activity by observing ongoing events. A temporal gap between the observed events and the target future further amplifies its difficulty."}, {"title": "2 Related Work", "content": "Action anticipation predicts future actions before they occur in video clips and is well explored both in third-person (exocentric) videos [28, 13, 29, 30, 31], and first-person (egocentric) videos [1, 32, 5, 26, 33, 4, 34, 12, 35, 36, 37], due to its applicability on autonomous agents and wearable assistants [38, 2, 12]. Funari et al. [1] introduce RU-LSTM, a model with two LSTMs and a modality attention component. Osman et al. [39] integrate RU-LSTM into SlowFast. Qi et al. [5] enhance LSTMs with Self-Regulated Learning (SRL). Dessalene et al. [33] use hand-object contact representations for action anticipation. Xu et al. [4] employ curriculum learning. Roy et al. [34] predict final goals for near-future anticipation. Liu et al. [36] store long-term action prototypes for richer short-term representations. Girdhar et al. [12] propose AVT, combining ViT and causal transformer, paving the way for [3, 35, 7]. Unlike previous works, S-GEAR considers the semantic relationship between action representations [15] by using vision and language prototypes to guide the model's training process semantically.\nVision-Language alignment relies on effectively aligning concepts between vision and language in a unified repre-sentation space. Typically achieved through contrastive training of modality encoders [40, 41, 42], these methods use vision-language pairs for encouraging proximity between corresponding visual and text embeddings. Zhai et al. [40] utilize contrastive learning to align text encoder representations with a frozen pre-trained vision model. Radford et al. [42] introduce CLIP, training separate encoders for text and images and aligning representations through contrastive loss. Ma et al. [43] extend CLIP to videos, employing multi-grained contrastive learning. Advancements include cross-modal fusion architectures using a cross-modality encoder for text and visual inputs [44, 45]. Unlike previous works, S-GEAR only translates the geometric association between action prototypes from language to vision without shifting spaces.\nPrototype Learning involves creating characteristic \"prototypes\" of labeled data samples. Initially dominant in few-shot learning for novel class prediction [46, 47], this strategy now successfully encodes spatial and temporal patterns in domains such as video semantic segmentation [48] and action recognition [49]."}, {"title": "3 Method", "content": "We propose S-GEAR for action anticipation. S-GEAR discerns essential spatiotemporal signals and understands the semantic relationships between actions. It contains a neural network architecture tailored for understanding spatiotemporal video sequences and a learning policy that guides the network semantically to map out the interconnections between actions.\nTask Formulation. Action anticipation involves predicting an action category for an event starting at time $T_s$, observing a video segment $V_o$ within the interval $[T_s - (T_o + T_a); T_s - T_a]$ [20]. Here, $\\tau_o$ and $\\tau_a$ denote the observation and anticipation periods set specifically to the dataset.\n3.1 Proposed Architecture\nS-GEAR processes a sequence of video frames and produces a set of features that can accurately describe the subsequent action. To achieve this, as shown in Fig. 2, S-GEAR employs an architecture composed of (1) a visual encoder for extracting feature vectors from the input frames; (2) the Temporal Context Aggregator (TCA) module designed to incorporate detailed temporal context from past to current observations; (3) the Prototype Attention (PA) block, which"}, {"title": "3.2 Semantic Guiding Policy", "content": "We exploit vision/language prototypes and a common communication space between them to facilitate a semantic-based guiding policy for action anticipation."}, {"title": "Prototypes", "content": "We aim to translate semantic relationships from language-based action concepts to the visual domain. Thus, we define two sets of prototypes. The first, defined as the language prototypes $p_e \\in \\mathbb{R}^{K \\times d}$ (where $K$ is the number of action classes), is extracted by encoding action labels composed of verb and noun combination using the \u201cSentence Transformer\u201d proposed in [22]. These prototypes serve as the reference space for learning actions \u201csemantic connectivity\u201d [21]. The second, defined as the visual prototypes $p_v \\in \\mathbb{R}^{K \\times d}$, ensures that S-GEAR remains in the visual domain and effectively preserves characteristic visual patterns. Such prototypes are learnable and initialized from typical action samples encoded from the proposed architecture trained for action recognition. We exploit $p_v$ to encode visual action representations and inherit the semanticity from $p_e$. Refer to Appendix A.3 for initialization details."}, {"title": "Common Communications Space", "content": "To translate action relationships from language to vision without shifting domains, we define a common space where vision and language representations co-exist and are compared via their relative associations w.r.t. the prototypes. In more detail, given an action visual encoding $z_t \\in \\mathcal{S}$, we compute its relative representation by comparing it against all elements in $p_v$ using a similarity function: i.e., $r_t^z = \\{r_1^t, ..., r_K^t\\}$ s.t. $r_k^t = \\cos(z_t, p_v[k])$ for each action class $k \\in \\{1, ..., K\\}$. Similarly, we compute the relative representation of a language encoding $enc(y)_t$ \u2013 i.e., the language encoding of the action label at time $t$ \u2013 against the prototypes in $p_e$ as $r^{enc(y)_t} = \\{r_1^{enc(y)_t}, ..., r_K^{enc(y)_t}\\}$ s.t. $r_k^{enc(y)_t} = \\cos(enc(y)_t, p_e[k])$ for all action classes $k \\in \\{1, ..., K\\}$. Now, we ensure that each k-th entry in $r_t^z$ and $r^{enc(y)_t}$ represents the geometric association with the k-th class prototype in the language/vision domain. Hence, we can directly compare these two representations based on their relative position in their original vector spaces."}, {"title": "3.3 Training", "content": "To train the model, for each labeled action segment, we sample a clip preceding it and ending exactly $\\tau_a$ seconds before the start of the action. We pass the clip through S-GEAR to obtain $z_t$ and then optimize it to learn semantically and visually meaningful prototypes for action anticipation.\nPrototype Learning. Learning prototypes aim to establish a visual latent space where predefined semantic connections describe actions by \"aligning\" the latent space topology defined by $p_e$ with $p_v$. To do so, we calculate the relative positions\u00b2 $r_t^z$ and $r^{enc(y)_t}$, which we use to define the semantic loss in Eq. 1.\n$\\mathcal{L}_{Sem} = ||r_t^z - r^{enc(y)_t}||.$  (1)\nDuring optimization, the prototypes in $p_v$ will be refined to represent relative relationships between actions akin to those inferred from the language space. Additionally, to guide S-GEAR push the action $z_t$ towards the prototype of the same class $k$ (i.e., $p_v[k]$) and avoid divergences, we add a lasso regularization\u00b3 to $\\mathcal{L}_{sem}$ as in Eq. 2.\n$\\mathcal{L}_{reg} = ||z_t - p_v[k]||^2$\n$\\mathcal{L}_{Sem} = \\mathcal{L}_{Sem} + \\mathcal{L}_{reg}.$ (2)\nThus, while shaping the visual latent space geometry defined by $p_v$ (Eq. 1), we enforce action representations to fall close to their visual prototype (Eq. 2).\nAnticipation Training. Besides prototype learning, we train S-GEAR for action anticipation by optimizing the cross-entropy loss between the predicted class label $\\hat{y}_T$ and the ground truth $y_T$. $\\hat{y}_T$ is obtained from the encoded action representation and its relative position w.r.t. the visual prototypes. More specifically, for the action representation $z_{T-1}$, we calculate $r^{z_{T-1}}$. Since $r^{z_{T-1}}$ contains values in [-1, +1], we transform them into probabilistic weights using softmax. Now, we aggregate all the prototype vectors into a single representation, $\\hat{z}_{T-1} \\in \\mathbb{R}^d$, according to the obtained weights (see Eq. 3).\n$\\hat{z}_{T-1} = \\text{softmax}(r^{z_{T-1}}) \\cdot p_v.$ (3)\nTo jointly learn the action representation and its exact collocation in the visual space w.r.t. the prototypes, we perform a weighted sum as in Eq. 4:\n$\\check{z}_{T-1} = \\sigma(\\alpha)\\hat{z}_{T-1} + (1 - \\sigma(\\alpha))z_{T-1},$ (4)\nwhere $\\sigma$ is a sigmoid function, and $\\alpha$ is a learnable scalar. Such operations are represented as Cosine Attention in Fig. 2. Lastly, we feed $\\check{z}_{T-1}$ through a linear layer and softmax its output to obtain $\\hat{y}_T$. We calculate the cross-entropy loss (Eq. 5) between the ground truth and the predicted action class.\n$\\mathcal{L}_{Cls} = -\\sum_{i=1}^K y_i \\log(\\hat{y}_i).$ (5)"}, {"title": "Additionally, inspired by [12, 13], we leverage the causality of the decoder \u03a9. Here, we use any true class label for the past frames and minimize the cross-entropy on past label predictions (Eq. 6). Notice that the predicted label \u0177t is produced following the same reasoning described above for \u0177T (see Eq. 3, 4).", "content": "$\\mathcal{L}_{Past} = - \\sum_{t=0}^{T-2} \\sum_{i=1}^K y_{i+1} \\log(\\hat{y}_{t+1}^i).$ (6)\nTo produce faithful future features, we minimize the distance between the predicted future frame features and the actual ones:\n$\\mathcal{L}_{Feat} = \\sum_{t=0}^{T-2} ||\\hat{I}_{t+1} - z_t||.$ (7)\nThe overall loss function used to train S-GEAR is a weighted sum of all the individual losses: $\\mathcal{L}_{tot} = \\lambda_1 \\mathcal{L}_{Sem} + \\lambda_2 \\mathcal{L}_{Cls} + \\lambda_3 \\mathcal{L}_{Past} + \\lambda_4 \\mathcal{L}_{Feat}$."}, {"title": "4 Experiments", "content": "4.1 Datasets and Metrics\nThe EPIC-Kitchens 55 (EK55) dataset [19] is a medium-scale first-person cooking dataset comprising 432 videos from 32 different individuals and approximately 40,000 segments. It encompasses 92 verbs and 272 object classes, resulting in 2,747 action classes. Additionally, we use the train and validation splits provided in [1]. Our model's performance on EK55 is evaluated using Top-1/5 Accuracy at $T_a$ = 1s, following prior works [1, 5, 4, 12].\nThe EPIC-Kitchens 100 (EK100) dataset [20] is a substantial extension of EK55, encompassing 700 videos from 37 individuals in 45 diverse kitchens. It comprises ~90,000 activity segments spanning 495 training, 138 validation, and 67 test videos. EK100 offers a richer representation of cooking activities through its broader range of verbs (97), objects/nouns (300), and action classes (4,053). To assess model performance on EK100, we employ the class aware mean Top-5 Recall [20, 12, 4] metric at $T_a$ = 1s.\nThe EGTEA Gaze+ dataset (EG) [26] includes 28 hours of first-person cooking videos from 32 subjects across 86 sessions, covering 7 tasks. The dataset contains 10,325 activity instances, categorized into 19 verbs, 51 objects, and 106 activity classes. To evaluate our model, we employed Top-1 Accuracy on split 1 for $T_a$ = 0.5s [50, 3, 12] and Top-5 Accuracy averaged across all three splits to evaluate overall performance for $T_a$ = 1s [1, 5, 36].\nThe 50 Salads dataset (50S) [27] comprises 50 exocentric videos featuring salad preparation activities performed by 25 different actors and categorized into 17 activity classes. We assess our model using mean Top-1 Accuracy across the 5 official splits following previous works [5, 1]. Unlike other benchmarks, the 50S offers a dense action anticipation challenge with variable observation and anticipation times. Specifically, for a given video segment in input, $T_a$ goes from 10% to 50% of the video's duration while $T_o$ is set to 20% or 30%.\n4.2 Implementation Settings\nVisual Encoder. S-GEAR employs the ViT Base (ViT-B) architecture as its visual encoder with a patch size of 16\u00d716. It comprises 12 transformer blocks, feature dimension 768, and operates with 12 attention heads. We set each frame size for input dimensions to 384\u00d7384 for the EK55/100 datasets and 224\u00d7224 for the EG and 50S datasets. Besides the default encoder, following prior works [4, 5, 12], we show that S-GEAR can also be used with other backbones like TSN and irCSN using pre-extracted features as in [1] and [12], respectively.\nIntermediate Stage. Our intermediate processing stage, crucial for linking the visual encoder's output to the causal transformer decoder, consists of 2 TCA blocks and 1 PA block. Note that when replacing ViT with other backbones, we omit TCA blocks. This is because, without ViT's detailed local patches, the architecture essentially becomes a standard causal transformer.\nCausal Transformer Decoder. For EK55/100 datasets, we employ a 6-layer causal transformer decoder with 4 heads and a dimensionality of 2048 to process the observed context and predict future events. For the EG dataset, we reduce the number of layers to 2. Meanwhile, for the 50S dataset, an 8-layer decoder with eight heads and the same dimensionality is used.\nObservation. For EK100, we set the observation time, $T_o$, to 15s, processing video segments at 1fps. For EK55 and EG, we maintain the same processing rate but reduce $T_o$ to 10s. In contrast, for the 50S, we align with [5, 51, 14] and adopt observation rates of 20% and 30% for each input sequence, with 0.25fps."}, {"title": "4.4 Unimodal Comparison", "content": "Table 1 (a), (b) provide unimodal results on EK55 and EK100 datasets, ensuring a fair comparison of S-GEAR against baselines. In EK55 (Table 1 (a)), in RGB, S-GEAR demonstrates a point improvement of 1.1 on Top-5 Acc. (vs. the second-best SRL) and 2.0 on Top-1 Acc. (vs. the second-best DCR) for the TSN features. Regarding the irCSN features, S-GEAR surpasses DCR by 1.8 points in Top-1 Acc. while trailing it on Top-5 Acc. by 0.9. Using the ViT-B backbone, S-GEAR surpasses AVT by 3.3 (Top-1) and 6.4 (Top-5) although S-GEAR uses bigger frame sizes. For the object modality, we use Faster R-CNN features (as in [1, 5, 4, 12]) for a fair comparison, obtaining 0.9 Top-1 Acc. improvement, yet failing behind on Top-5 by 0.1. Finally, S-GEAR yields 1.9 (Top-1) and 3.1 (Top-5) point gains for the flow modality over prior works.\nTable 1 (b) details our results on the more complex EK100 benchmark. Here, S-GEAR competes with MeMViT [7] and RAFTformer [35] (with the MViTv2-16 backbone) on the RGB modality. S-GEAR demonstrates improvements in Top-5 Recall for actions (3.2 over MeMViT, 0.7 over RAFTformer) and nouns (4.1 over MeMViT, 1.8 over RAFTformer). While trailing slightly on verbs, unlike its competitors (Kinetics-400), S-GEAR performs well without spatiotemporal initialization. Surprisingly, even when MeMViT and RAFTformer employ larger backbones with Kinetics-700 initialization, S-GEAR exceeds them on actions and nouns. Additionally, we formed S-GEAR-2B by"}, {"title": "4.5 Comparison with the SOTA", "content": "Epic-Kitchens. Previous approaches often utilize cross-modality ensembling [20, 3] or joint training [1, 3] for multimodal evaluation on these benchmarks. Ensembling S-GEAR across modalities, we observe significant gains. On EK55 (Table 1 (c)), late-fusing our models (RGB+Obj+Flow) yields a boost of 3.5 (Top-1 Acc.) and 2.0 (Top-5 Acc.) absolute points, outperforming prior work. Similarly, on EK100 (Table 1 (d)), late-fusing RGB modalities with object features leads to a 1.4 (0.8 against RAFTformer) point improvement in action Top-5 Recall. Finally, though we report EK100 test set results (Table 1 (d)) and obtain competitive performances, it is crucial to note that leaderboard rankings often rely on large-scale external data or fusion across diverse models (i.e., the de-emphasized models on Table 1 (d)). This makes the test set less effective for comparing the core strengths of models [4]. We point the reader to Appendix B.2 for details on our specific ensembling weights.\nEGTEA Gaze+. We evaluate S-GEAR on two task on EG (Table 2). The first includes Top-1 Acc. on split-1 for Ta = 0.5 where we achieve 2.5 point improvement compared to previous work. The second includes the average Top-5 Acc. across the three splits at Ta = 1s where we surprisingly improve on HRO with 0.4 points despite using only the RGB modality with our ViT-B backbone.\n50 Salads. Our dense anticipation experiments on the 50S (Table 3) show S-GEAR's potential for long-term and exocentric tasks. It outperforms competitors in 5/8 scenarios, with Top-1 Accuracy gains of up to 3.5, despite not being tailored for long-term anticipation like Time-Cond. [51]."}, {"title": "4.6 Ablation Study", "content": "We analyze the importance of S-GEAR's components to justify our design choices. Specifically, we investigate (a) the impact of architectural and training elements, (b) the significance of encoding semantic action relationships, (c) the number of prototypes for defining relative action positions, and (d) S-GEAR's performance for different anticipation time $T_a$.\n(a) We use EK100 (RGB) to evaluate the impact of architectural components and our prototype learning strategy (see Table 4). We use a baseline (1) comprising a ViT-B encoder, a casual decoder, and a linear classification head similar to AVT [12]. On top of this baseline, we switch on/off each component that comprises S-GEAR: i.e., (2) the prototype learning with semantic guidance, including the cosine attention block on the classification head, (3) the TCA block, and (4) the PA block. Note that the PA block needs prototypes; thus, in the table, we toggle the semantic column as well. While all strategies improve over (1), (2) has the most impact, adding up to 2.6 points on Top-5 Recall for action classes. Such improvements are caused by the ability of the prototype learning strategy to cluster actions that co-occur frequently. The network then uses this proximity to encode action representations aware of their exact collocation through the cosine attention block, taking hints that the next probable action can be found in its proximity in the latent space. Finally, to motivate our choice of learning visual prototypes $p_v$ rather than directly using language prototypes $p_e$, we rely on (5), which includes all the architecture components except the prototype learning strategy. Instead, action representations are directly aligned with fixed $p_e$. This resulted in decreased performance compared to S-GEAR. While $p_e$ captures semantic structure, we believe it lacks the scene information crucial for accurate anticipation, such as motion and visual context. S-GEAR overcomes this limitation by learning its visual prototypes, allowing them to adapt to the specific visual cues relevant to the task.\n(b) S-GEAR builds on the principle that semantically similar actions often co-occur, making semantic relationship encoding crucial. To ablate on the importance of such relationships, we leverage two Sentence Transformer variations from HuggingFace: \u201cbert-large-nli-max-token\u201d (BERT) and \u201cstsb-mpnet-base-v2\u201d (STSB). These models share a similar architecture but differ in training data size, with STSB being better at semantic relation extraction. Fig. 3 shows that S-GEAR performs better with STSB-generated prototypes, highlighting that modeling accurate semantic interconnections gives better results."}, {"title": "4.7 Qualitative Results", "content": "Fig. 6 demonstrates S-GEAR's ability to anticipate future actions on the EG dataset, using Ta = 1s and To = 32s. Alongside S-GEAR's Top-5 predictions, we include the Top-5 semantically similar language prototypes given the observed action sequence. These examples reveal the connection between anticipation and semantics, suggesting that an alignment between the two exists. On the other hand, the last row example also highlights divergences emphasizing S-GEAR's room for semantic improvement. To further investigate the semantic alignment between S-GEAR and language prototypes, in Fig. 7, we illustrate the geometric association learned by S-GEAR prototypes (middle) on EG, comparing it with its initial values (left) and the language prototypes (right) both in terms of absolute and relative positions. The latter is determined using cosine similarity to compare each prototype against all others. S-GEAR's prototypes demonstrate a latent space topology closer to the language prototypes than its counterpart w/o semantic alignment in terms of absolute and relative position. Such phenomenon indicates that S-GEAR can reason upon the semantic connectivity between actions, projecting contextually similar ones closer in latent space. However, S-GEAR's topology is slightly different since visual cues influence inter-prototype distances. We point the reader to Appendix B for more experimental details."}, {"title": "5 Conclusion", "content": "We presented S-GEAR, a novel framework for action anticipation that leverages semantic interconnectivity between actions. S-GEAR learns visual and language prototypes that encode typical action patterns and their relationships based on contextual co-occurrences. S-GEAR transfers the geometric associations between actions from language to vision without direct alignment, creating a common communication space. S-GEAR employs a transformer-based architecture incorporating temporal context aggregation and prototype attention to enhance the action representations and predict future events. We evaluate S-GEAR on four action anticipation benchmarks, showing improved results compared to previous works. We also demonstrate that we can effectively encode semantic relationships between actions, opening"}, {"title": "A.1 Temporal Context Aggregation", "content": "Sequential models like LSTMs and causal transformers excel at handling temporal frame sequences. However, relying on class tokens, they prioritize global information [4, 12] and neglect spatial cues. To illustrate the difference between the standard causal transformer and our proposed Temporal Context Aggregator (TCA), we provide the reader with Fig. 8. Here, the left-hand side shows the workflow of a standard causal transformer applied on a sequence of ViT frame features composed of local feature tokens and a global class token. In this scenario, the causal transformer omits the local information and only propagates the global information in time to create causal representations [12, 4]. Because local tokens encode specific scene details within different regions, not propagating their information hinders the model from understanding scene dynamics (e.g., how an object's location changes as a particular action progresses). Therefore,"}, {"title": "TCA builds on the attention mechanism and processes intermediate features It. Thus, It undergoes linear processing to generate the query (Qt), key (Kt), and value (Vt) vector representations. Afterward, as shown in Fig. 9 (left), the TCA uniquely aggregates keys and values from past frames to subsequent ones before computing the attention matrix.", "content": "This approach enables the queries of each frame to access a rich set of keys and values infused with comprehensive spatiotemporal information about past contexts, enabling better temporal dependency [7]. Specifically, the Kt and Vt vectors are augmented as in Eqns. 8 and 9, respectively:\n$K_t = \\begin{cases} \\widehat{K}_t & \\text{if } t = 0 \\\\ \\delta(\\widehat{K}_t, \\alpha_{t-1}. \\widehat{K}_{t-1}) & \\text{otherwise} \\end{cases}$ (8)\n$V_t = \\begin{cases} \\widehat{V}_t & \\text{if } t = 0 \\\\ \\delta(\\widehat{V}_t, \\alpha_{t-1}. \\widehat{V}_{t-1}) & \\text{otherwise} \\end{cases}$ (9)\nwhere Kt and Vt are the augmented keys and queries of frame ft, at\u22121 is a learnable weight parameter that balances the quantity of the information transmitted from past observations, and \u03b4 is a permutation invariant aggregation function. Note that we tried different functions for d (e.g., cumulative-max), but through empirical analyses, we chose summation."}, {"title": "Recall that S-GEAR encodes semantic relationships between actions. To help the encoding of such relationships, we integrate a Prototype Attention (PA) block y detailed in Fig. 9 (right). The PA module helps the network learn meaningful representations by incorporating semantic information from the visual prototypes. PA has two stages: (1) selecting the prototypes and (2) modeling the relationship between features and prototypes.", "content": "Similar to TCA, we build PA upon the attention mechanism, giving in input both the class tokens from the intermediate encodings generated by ViT \u2013 i.e., I\u00b0 = {I\u00ba, I\u00b0, . . ., I\u00ba_1} and the visual prototypes pr. We rely on the relative similarities between actions to address (1). Specifically, we begin by calculating the cosine similarity between each I and the visual prototypes, obtaining the relative representation vector r\u00b9 of frame t as in Eq. 11:\nr\u00b9 = cos(I, \u03c1\u03c5). (11)\nThen, we select the top k most similar prototypes for each feature vector for the remaining calculations. However, for simplicity, let us assume we select the most similar prototype for each feature vector. After acquiring the estimated prototypes, PA addresses (2) by modeling their relationship with the feature encodings using the attention mechanism. In this case, the set of prototypes represents both the key (K) and value (V) vectors. Conversely, the query (Q) vector is derived from I\u00ba. The first step of the relationship modeling is the computation of the attention scores Wa through a scaled matrix multiplication between Q and K as in Eq. 12:\n$W_a = \\frac{QK^T}{\\sqrt{d}}$ (12)\nFollowing the standard attention procedure, the next step in the attention process should be normalizing and applying Wa to V and having the output features. However, the selected prototypes do not have temporal continuity like the sequential frames and contradict the temporal causality built from TCA when the fusion occurs (see Sec. 3.1 in MP). Inspired by [56], we introduce a Temporal Order Encoding (TOE) weight vector shaped as a Toeplitz matrix to model the temporal order between elements of V. We provide the reader with an example to illustrate Toeplitz matrices and their unique structure. Here, we show a 5-element TOE as a 3 \u00d7 3 Toeplitz matrix A as in Eq. 13:\n$\\Delta = \\begin{bmatrix} w_0 & w_1 & w_2 \\\\ w_3 & w_0 & w_1 \\\\ w_4 & w_3 & w_0 \\end{bmatrix},$ (13)\nwhere wi represents the ith weight from the TOE for i \u2208 {0,1, ..., 4}. Notice that a single weight represents each diagonal. Additionally, with a 3 \u00d7 3 matrix, we can model the temporal relationships of a sequence of three elements. Hence, for a sequence of T elements like V, we need a T \u00d7 T Toeplitz matrix built from a (2T - 1)-element TOE. Generalizing, the T \u00d7 T A functionality allows PA to model the relative temporal position or order between elements of V when aggregating features. To apply A to V, we first sum A with the normalized Wa and then perform a matrix multiplication between the resulting matrix and V as in Eq. 14:\n$\\hat{I} = (\\beta(\\text{softmax}(W_a)) + (1 - \\beta)\\Delta)V,$ (14)\nwhere \u1e9e is a learnable scaling factor that balances the sum between Wa and \u2206. \u00ce \u2208 RT\u00d7d now represents the selected visual prototypes fused with the current context and with encoded relative temporal awareness."}, {"title": "A.3 Prototype Initialization", "content": "We initialize our visual prototypes pr using action samples generated by the proposed architecture (detailed in Sec. 3.1 in MP). First, we train the network (PA omitted) for action recognition on EK100 for the egocentric tasks and 50S for"}, {"title": "A.4 Training S-GEAR With Pre-extracted Features", "content": "S-GEAR's modular design allows it to work with different visual backbones, although it's primarily designed for end-to-end training with a ViT architecture [23]. To ensure a fair comparison with the SOTA [1, 4, 12], we also train our network using pre-extracted features from TSN, Faster R-CNN (FRCNN) and irCSN backbones provided by Furnari et al. [1] and Girdhar et al. [12], respectively.\nOur training process (see Fig. 10) is built upon aligning the pre-extracted feature distribution with pr learned from S-GEAR in its end-to-end training with ViT. These prototypes already capture the desired structure of the latent space. By aligning with them, we simplify training when we lack the variability introduced by typical video preprocessing techniques (i.e., random cropping or flipping). To this end, given pre-extracted visual features Xt \u2200t \u2208 [0, T \u2013 1], we first apply a linear transformation and then normalize them using the mean up and standard deviation \u03c3\u03c1 from the learned prototypes as in Eq. 15:\n$I_t = \\frac{\\text{lin}(x_t) - \\mu_{p_v}}{\\sigma_{p_v}}$ (15)\nHence, leveraging the pre-trained weights of PA, causal decoder, and classification head from the end-to-end training, we fine-tune S-GEAR to adapt to the new visual features It. During this process, pr remains unchanged."}, {"title": "B Experiments Extension", "content": "Here, we provide additional details regarding our experiments. In Sec. B.1, we provide details regarding the weights used for each loss function to train S-GEAR for each dataset. In Sec. B.2, we provide the results supporting the ablation"}, {"title": "B.1 Composed Loss Weights", "content": "Here, we provide the specific weights for each loss term introduced in Sec"}]}