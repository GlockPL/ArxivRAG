{"title": "Assessing and Prioritizing Ransomware Risk Based on Historical Victim Data", "authors": ["Spencer Massengale", "Philip Huff"], "abstract": "We present an approach to identifying which ransomware adversaries are most likely to target specific entities, thereby assisting these entities in formulating better protection strategies. Ransomware poses a formidable cybersecurity threat characterized by profit-driven motives, a complex underlying economy supporting criminal syndicates, and the overt nature of its attacks. This type of malware has consistently ranked among the most prevalent, with a rapid escalation in activity observed. Recent estimates indicate that approximately two-thirds of organizations experienced ransomware attacks in 2023 [1]. A central tactic in ransomware campaigns is publicizing attacks to coerce victims into paying ransoms. Our study utilizes public disclosures from ransomware victims to predict the likelihood of an entity being targeted by a specific ransomware variant. We employ a Large Language Model (LLM) architecture that uses a unique chain-of-thought, multi-shot prompt methodology to define adversary SKRAM (Skills, Knowledge, Resources, Authorities, and Motivation) profiles from ransomware bulletins, threat reports, and news items. This analysis is enriched with publicly available victim data and is further enhanced by a heuristic for generating synthetic data that reflects victim profiles. Our work culminates in the development of a machine learning model that assists organizations in prioritizing ransomware threats and formulating defenses based on the tactics, techniques, and procedures (TTP) of the most likely attackers.", "sections": [{"title": "Introduction", "content": "With a steady rise in the frequency and sophistication of ransomware attacks, organizations are compelled to intensify their efforts in fortifying digital defenses against evolving threats. Ransomware, a form of malware that restricts user access to systems until a ransom is paid, has not only become more prevalent but also increasingly damaging. Organizations of all sizes are affected, with smaller to medium-sized entities disproportionately targeted [2], [3]. The consequences of successful ransomware attacks are severe, leading to significant downtime, financial losses, and reputational damage. On average, affected organizations face ransom payouts nearing $812,000, reflecting the escalating financial toll of this cybersecurity threat [2], [4]. Alarmingly, compliance with ransom demands does not always guarantee data recovery, highlighting the complex and evolving challenges posed by ransomware [5].\nIn response to these growing threats, the strategic implementation of Cyber Threat Intelligence (CTI) becomes imperative. Using CTI to understand the adversary can equip entities to proactively defend themselves and improve their ability to make risk-informed decisions [6]. CTI is systematically categorized into tactical, operational, and strategic domains. The tactical facet of CTI focuses on specific Indicators of Compromise (IOCs), such as IP addresses and malware hashes, that facilitate immediate threat detection. Operational CTI provides insights into the Tactics, Techniques, and Procedures (TTPs) employed by adversaries, while strategic CTI delivers an extensive analysis of overarching threat trends, guiding organizational leaders in decision-making and strategic planning [7].\nHowever, CTI is predominantly consumed through unstructured, natural-language sources such as media reports, news, and bulletins [8], and the effectiveness of CTI is impeded by the overwhelming volume of information, especially for smaller to medium-sized entities [9]. These organizations work with limited resources, hindering their ability to effectively consume CTI [6]. Although these sources offer rich context and real-time insights, assimilating the information into meaningful defensive actions remains challenging. Manual processing becomes impractical with rising numbers of automated systems, sophistication of threats, and growing complexity of data, underlining the urgency for advancements in CTI automation\nIn this paper, we concentrate on enabling organizations to distill the extensive number of global ransomware threats into a concise list of potential ransomware groups through the application of machine learning techniques. We utilize high-quality data on ransomware variants that are amenable to machine learning analysis and elaborate on our method for extracting pertinent data features through LLMs. Furthermore, we elaborate on our strategy to augment the dataset, ensuring comprehensive coverage of the problem space while preserving attributes that accurately reflect real victim scenarios. Our model uses openly accessible real victim data for analysis, but we recognize the limitations posed by the current scarcity of such data for predicting attacks solely based on existing incidents. To address this gap, we propose a heuristic approach for generating synthetic data to serve as a bridge until the quantity and availability of open-source data are improved. One notable limitation of our method is the temporal evolution of ransomware variants and their target victim profiles, which we address by incorporating time-sensitive characteristics of attacks into our machine learning model. Nevertheless, continuous observation is essential to understand the shifting patterns of ransomware attacks and to update the model's training data accordingly. This methodology is designed to be broadly applicable across various organizational contexts, given that the data features we focus on are universally relevant. The following sections will detail the existing literature, our data management techniques, and provide an in-depth analysis of the machine learning model and its outcomes."}, {"title": "Related Works", "content": ""}, {"title": "Automating Cyber Threat Intelligence Extraction", "content": "Recent studies have showcased advancements in Cyber Threat Intelligence (CTI) extraction, particularly through the use of Large Language Models (LLMs). Various methods have been developed to refine the automation of extracting different types of CTI, leveraging LLM capabilities for efficient analysis.\nThe \"Time for aCTIon\" framework employs GPT-3.5 to streamline CTI extraction using prompt engineering, zero-shot learning, and in-context learning. This approach obviates the need for in-house model training by utilizing LLMs as a Service (LLMaaS), effectively handling diverse CTI scenarios. The framework has been rigorously evaluated against other tools, demonstrating its superiority in quickly and accurately extracting critical CTI elements while optimizing resource utilization [10].\nOther initiatives, such as TTPDrill and ThreatKG, utilize NLP and information retrieval models to transform adversarial TTPs into structured formats like the Structured Threat Information eXpression v2.1 (STIX) framework and develop comprehensive threat actor profiles through knowledge graphs [11], [12]. The Ladder framework further extends this by employing models like XLM-ROBERTa and Roberta to associate extracted attack patterns with Mitre Att&ck IDs, providing a systematic strategy to preempt threats [13], [14].\nFurther, models like those showcased in CTI View and research by Irshad and Siddiqui extract a combination of strategic CTI and TTPs to pinpoint specific threat actors behind cyber-attacks, aiding the analysis of APT threat trends and the development of proactive defense strategies [15], [16].\nBuilding on these advancements, our unique approach leverages LLMS for CTI extraction and employs techniques such as zero-shot learning, chain-of-thought, and polymorphic prompting. This strategy allows for a one-time querying process, significantly enhancing resource utilization and speeding up the extraction of critical CTI elements. Unlike traditional methods, which often rely heavily on manual efforts and provide incomplete threat landscapes, our method offers clear, rationale-driven outputs that improve extraction accuracy and provide deeper insights for analysts. By incorporating these innovations, we deliver a more sophisticated and effective solution for navigating and addressing the complexities of the threat landscape."}, {"title": "Ransomware Prevention and Mitigation", "content": "Numerous studies have focused on identifying and mitigating ransomware threats [17]\u2013[21]. Recognizing its effectiveness in safeguarding against this pervasive threat, the primary goal of these measures is to prevent and mitigate potential risks and security threats, effectively protecting systems, data, and networks from unauthorized access, breaches, or malicious activities [22].\nLevesque et al. [23] developed a user-risk prediction model to identify potential malware targets by analyzing social, demographic, and behavioral factors, identifying twelve key features as crucial predictors. Similarly, Yilmaz et al. [24] explored the human element by investigating the relationship between personality types and ransomware victimization, though they found no significant correlation.\nIn contrast to these individual-focused studies, our solution emphasizes the broader context of company profiles and the operational characteristics of ransomware groups using the SKRAM (Skills, Knowledge, Resources, Authorities, and Motivation) model. This shift enables a more strategic approach to cybersecurity, allowing organizations to proactively adjust their defenses by understanding both the potential targets and the attackers. Additionally, our model generates a risk score for each identified threat, providing a quantifiable measure that organizations can use to prioritize and filter threats based on their potential impact. This holistic view significantly improves organizational resilience to ransomware attacks by reducing the noise in threat detection and focusing on the most pertinent threats."}, {"title": "Methodology for Dataset Construction and Management", "content": ""}, {"title": "In-Depth Analysis of Ransomware Campaigns", "content": "Data collection for our study began with the aggregation of over 10,000 documented ransomware victim cases from Ransomware.live, a platform that compiles data from various open-source projects monitoring ransomware incidents since January 12, 2020 [25]. These records provide the ransomware group, the names of the victim company, and the dates of the attacks. Using this dataset, we initiated a filtering process to refine the selection of incidents for predictive modeling. Initially, we excluded all cases that occurred before 2021. We then omitted records related to threat actors not covered in our current dataset and discarded entries lacking detailed descriptions of the victim organizations. After applying these filters, we identified 409 distinct attacks for our predictive model.\nThe data features we extracted and summarized in Table 2 were obtained through a dual approach: directly from victim records and supplemented by open-source databases. This method furnishes a profile for each victim, highlighting key attributes such as employee numbers and annual revenue. These details are critical as adversaries often leverage this information to gauge a company's capacity to meet ransom demands. For simplicity and practicality in organizational data management, we initially focused on static organizational data features. Including dynamic data features, which could offer a more in-depth understanding of a target's vulnerability to ransomware attacks, represents a potential area for future enhancement of our model. Adversary attributes, by contrast, present a more complex challenge for data extraction, requiring the sophisticated techniques detailed in the subsequent section to accurately capture these characteristics."}, {"title": "Extraction of Ransomware Profiles Leveraging LLM Techniques", "content": "To address the challenge of structuring narrative-based open-source intelligence on ransomware adversaries, we developed a methodology using LLMs to transform diverse data sources, including news articles and comprehensive threat analyses, into a machine-learning-friendly STIX format. Figure 1 illustrates our approach involving a flexible prompt design strategy that interfaces with multiple LLMs. The responses from the LLMs are passed through the response processor schema for refining the raw output into formats ready for further analysis and eventual transformation into STIX-compliant records. This method enhances the adaptability and efficiency of our system, streamlining the development of new content processors and ensuring the relevance and precision of the data in security applications.\nFor our study, we analyzed approximately 229 threat reports to identify SKRAM attributes for 146 distinct adversaries. While the actual number of unique adversaries may be lower due to aliases and overlapping identifiers, feature extraction was performed using OpenAI's gpt-4-1106-preview model, which has a context window size of 128,000 tokens and an output token limit of 4,096 tokens. These specifications enabled the model to process extensive contextual information from multiple reports or large segments of text in a single input, while still generating detailed and comprehensive outputs. To ensure the accuracy of the SKRAM features extracted by the LLMs, we engaged cybersecurity research students, who were compensated for their efforts, to review and verify the accuracy of the LLM-generated data."}, {"title": "Designing the Prompt", "content": "The prompting architecture is developed manually as reusable specifications designed to enhance learning accuracy through various approaches. It includes techniques to boost the reliability and accuracy of the LLM when extracting data features. Each feature extraction prompt is structured to include the key name of the feature, its extraction intent, and precise guidance, which are developed through the observation of LLM outputs as shown in the sample 1.1. Additionally, the specification incorporates a set of examples following a many-shot prompt approach, improving output formatting. The process field outlines a sequence of logical steps that emulate a chain-of-thought reasoning pattern, aiding the LLM in feature extraction, as supported by Wei et al. [26]. Lastly, each feature extraction should align with a predefined standard, ensuring the extracted data is consistently formatted and immediately ready for integration into machine learning datasets.\nConsistency in the JSON output is paramount for its application in machine learning. We achieve this by either referencing well-known open standards that are familiar to the LLM or by specifying a list of possible responses. For instance, we use the standard ISO 3166-1 alpha-2 codes for country identification and the MITRE ATT&CK framework for defining threat attributes, mitigations, and potential impacts [14]. Although there is no universally accepted standard for industry sectors, the STIX vocabulary provides a well-defined set of industry sectors that can be enumerated [27].\nMoreover, our prompts are augmented to elicit a rationale for each feature extracted, mirroring the stop-and-think approach advocated in recent studies [28]."}, {"title": "Processing LLM Response", "content": "Due to the intricacies of our prompts, LLMs often generate multiple responses because of the limitations of output tokens. To address this, our LLM Response Processor, illustrated in Figure 1, serializes these responses to create a unified output, which improves coherence and consistency in the generated information. Furthermore, the LLM Processor ensures that the LLM response adheres to proper standards and validates enumerated standards, such as target sectors, as specified by our prompts. Any invalid or erroneous values are filtered out during processing. Additionally, vulnerabilities, adversary techniques, and mitigation identifiers are validated with the National Vulnerability Database (NVD) and Mitre Att&ck to ensure their existence, thereby minimizing the propagation of misinformation to the analyst [14], [29]."}, {"title": "Content Analysis", "content": "While fully automated feature extraction using LLMs is desirable, their non-deterministic nature can introduce potential inaccuracies. To address this, our analysts validate the SKRAM output to identify any inaccuracies, incompleteness, or outdated information [30]. They ensure that the output generated by the LLM accurately represents the content of the original report. This validation process significantly expedites the overall workflow, as analysts can review and confirm the extracted content directly. The Processing LLM Response module undertakes the bulk of this validation work, ensuring the removal of erroneous information and verifying that the extracted standards align with the expected criteria (see 3.4 for details)."}, {"title": "Content Synthesizer", "content": "Finally, the Content Synthesizer is specifically tailored for each type of prompt. This customization is critical for converting features in CTI reports into valid STIX objects. It provides a method to handle the diverse nature of CTI content effectively.\nCTI reports, such as threat reports, contain information about vulnerabilities, targeted entities, and capabilities. These elements are accurately transformed into corresponding STIX objects. Similarly, campaign reports, which include details on tools, malware, and attack patterns, are processed into STIX objects and relationships. The Content Synthesizer ensures that all CTI content is structured appropriately into STIX objects. These objects are then inserted into the STIX Database, facilitating their application and analysis."}, {"title": "Dataset Augmentation via Synthetic Data Generation", "content": "The initial dataset's scope, limited to 409 ransomware victims, proved insufficient for accurate machine learning predictions. We addressed this through synthetic data generation, enhancing the dataset's volume while preserving the integrity of real-world insights.\nFirst, to generate synthetic samples that closely resemble the characteristics of actual victims, each attack record replicates with the following process:\nPreserve Key Attributes: For each victim, retain their original country of origin, sectors, and company type as these features are critical identifiers that should remain constant to maintain the integrity and relevance of the synthetic data.\nPermute Numeric Attributes with Gaussian Noise: Introduce variability into the revenue, number of employees, and EWMA attributes by adding Gaussian noise."}, {"title": null, "content": "In our methodology, Gaussian noise was introduced to simulate the natural fluctuations in key numerical features, such as revenue and employee count, observed in real-world data. This noise was calculated based on the mean values derived from the actual data, ensuring that the distribution was centered around zero. The standard deviation was chosen to mirror the variability observed within the dataset, thereby enhancing the representativeness of the data while maintaining similarity to the characteristics of observed victims. By employing Gaussian noise in this manner, we successfully augmented our dataset to include ten times as many samples as the original, each enriched with diverse but plausible values, which broadened the scope of our analysis without deviating from the realism inherent to the victim data. The approach is presented in the following pseudo-code:\nTo ensure a balanced model, we incorporated 'safe' samples, signifying entities that, with a high degree of certainty, are unlikely to be targeted by specific ransomware groups. These safe samples might represent organizations located outside the geographical or sectoral focus of the ransomware or entities that are potentially immune to an inactive ransomware variant, indicated by an EWMA approaching zero. The method involves generating an equivalent number of safe samples for each identified ransomware variant. This is achieved by introducing probabilistic variations to critical attributes, thereby crafting simulated profiles that reflect characteristics of non-targeted entities as follows:\nIn this algorithm, we assign probability weights to data features as seen in Figure 2. These weights dictate the likelihood of each feature being altered to simulate a safe sample. With the probability weight of the feature, we will make the feature safe. Otherwise, we do nothing to the feature for the victim record. For example, the permutation for categorical features such as country of origin includes randomly sampling a country not present in the ransomware variant's victim dataset. Finally, we ensure at least one field has permuted in order to declare the record safe from any ransomware attacks."}, {"title": "Ransomware Risk Machine Learning", "content": "In this section, we detail the development and results of a machine-learning model designed to predict an entity's ransomware risk. To deploy the model, we opted for a Random Forest Classifier due to its performance in tackling classification tasks to discern between safe and unsafe entities, which is then translated into a risk score. Random Forests stand out for their resilience to overfitting, attributed to their ensemble-based methodology, which mitigates the risk of memorizing training data patterns and facilitates better generalization to unseen data. Moreover, Random Forests demonstrate proficiency in handling high-dimensional datasets containing numerous features. Their adeptness in managing complex data structures ensures our model's capability to capture subtle relationships between the various safe and unsafe entities."}, {"title": "Model Configuration and Training", "content": "The Random Forest model was configured with n_estimators set to 100 and a random_state of 42. We used 100 trees to balance accuracy with computational efficiency, considering the size of our dataset. The specific random_state ensures consistent results across multiple runs of the model. The dataset, consisting of 8,200 records evenly split between safe and unsafe instances, was divided using an 80/20 split for training and testing.\nCategorical columns with multiple labels were binarized, and single-label string categorical columns were encoded using one-hot encoding. Non-categorical features were label-encoded to ensure uniform data formatting conducive to effective model input. It is important to note that as new industry sectors and countries are introduced into our dataset, the model will undergo periodic retraining. This adaptation helps maintain its accuracy in assessing the likelihood of an entity being targeted by a ransomware group."}, {"title": "Results", "content": "The resulting model achieves high precision, recall, and F1-score, all measuring at 99%. In analyzing the confusion matrix, we observe the model correctly predicted 807 instances as negative and 820 instances as positive. It incorrectly classified 13 instances as positive but did not mislabel any instances as negative 3.\nThe feature importance depicted in Figure 6 highlights the entity attributes that significantly influence the risk score. The EWMA, although removed from the chart to underscore the importance of the company profile, was the most significant feature, as seen by the predictions in Figure 5 and Figure 4. The likelihood of a ransomware group targeting an entity decreases substantially if the group is inactive. This is followed in significance by the company's location, size, revenue, and industry sectors. Despite this, the chart shows that the company profile can override the EWMA feature if the company's size, location, and industry do not align with the ransomware group's profile. However, the prominence of US-based companies in the feature importance indicates a potential internal bias in our dataset, suggesting a possible imbalance in the distribution by country of origin."}, {"title": "Ransomware Prediction", "content": "This section explores two predictions generated by our Ransomware Prediction Model. The model produces a confidence score that assesses the likelihood of a company being a potential target based on how closely their profile aligns with those typically targeted by a specific ransomware group. This assessment also considers the current level of activity of the ransomware group. The confidence score is then converted into a 10-point risk scale, ranging from None (0) to Extremely High (9). The data presented in Figure 3 depict predictions for the same U.S.-based company, for two different ransomware groups. This company operates within the Automotive and Manufacturing sectors, employs 5,000 people, and generates an annual revenue of $2,100,000,000.\nThe 'Extremely High' prediction presented in Figure 3 provides a prediction concerning the ransomware group Phobos, which is identified as an intermediate-level \"crime syndicate.\" The group's EWMA score is 2.08 indicates active participation in malicious activities. Notably, this threat actor possesses a singular capability, primarily aimed at financial gain and information theft, which marks it as a well-resourced entity. As illustrated in Figure 4, the extremely high risk score assigned to this record stems from the threat actor's on-going activity and strategic focus on targeting for-profit entities in the United States, especially those comparable in size regarding employee numbers and annual revenue.\nHowever, the 'Low' prediction shown in Figure 3 details a prediction for the ransomware group Rhysida. This group is characterized as advanced, with motives including financial theft, disruption of service, and information theft, all driven by financial gain. The group is well-resourced, possessing eight known capabilities. While the group does show activity, the prediction received a \"Low\" risk rating primarily due to the profile of this entity. As seen in Figure 5, the entity in question does not fit the typical victim profile of the ransomware group, which usually does not target companies of this size within the United States, specifically those in the automotive sector."}, {"title": "Dataset Considerations", "content": "In reflecting upon our model's high accuracy and its implications for generalizability, it is imperative to consider the role of synthetic data in our study. While synthetic data has significantly expanded our training dataset, enabling a comprehensive analysis that would not be possible with the limited number of real cases alone, this approach inherently introduces questions about the representativeness of our findings. The synthetic generation process aims to mimic the diversity of real-world scenarios, yet the extent to which it captures the nuanced behaviors of ransomware attackers and victim entities remains an area for further scrutiny. To address this, future work will focus on validating model predictions with more extensive datasets of real incidents as they become available, enhancing our understanding of the model's applicability across different contexts.\nFurthermore, the analysis, as illustrated in 6, reveals a pronounced over-representation of US-based companies in our dataset. Importantly, this geographical bias is not a result of the original composition of data but stems from our data annotation process. Faced with incomplete or scarce information on any company, our protocol has been to exclude such entities to uphold the dataset's integrity and ensure analytical precision. While this decision is crucial for maintaining the quality of our analysis, it inadvertently introduces a geographical bias, potentially affecting our model's ability to accurately predict ransomware risks for organizations in less represented regions or countries."}, {"title": "Future Work", "content": "In considering future research directions, a critical focus lies in automating, expanding, and diversifying the dataset used for analysis. Currently, our dataset consists of 8200 records, evenly split between 4100 safe and 4100 unsafe entities, all derived from 409 manually annotated authentic ransomware victims. There is a significant opportunity to enhance the dataset's size and diversity.\nFuture efforts should focus on transitioning away from reliance on synthetic data generation, moving towards automating the incorporation of real-world ransomware victim data. This transition would not only introduce greater diversity"}]}