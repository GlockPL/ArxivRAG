{"title": "DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment", "authors": ["Wendi Chen", "Han Xue", "Fangyuan Zhou", "Yuan Fang", "Cewu Lu"], "abstract": "In recent years, imitation learning has made progress in the field of robotic manipulation. However, it still faces challenges when dealing with complex long-horizon deformable object tasks, such as high-dimensional state spaces, complex dynamics, and multimodal action distributions. Traditional imitation learning methods often require a large amount of data and encounter distributional shifts and accumulative errors in these tasks. To address these issues, we propose a data-efficient general learning framework (DeformPAM) based on preference learning and reward-guided action selection. DeformPAM decomposes long-horizon tasks into multiple action primitives, utilizes 3D point cloud inputs and diffusion models to model action distributions, and trains an implicit reward model using human preference data. During the inference phase, the reward model scores multiple candidate actions, selecting the optimal action for execution, thereby reducing the occurrence of anomalous actions and improving task completion quality. Experiments conducted on three challenging real-world long-horizon deformable object manipulation tasks demonstrate the effectiveness of this method. Results show that DeformPAM improves both task completion quality and efficiency compared to baseline methods even with limited data. Code and data will be available at deform-pam.robotflow.ai.", "sections": [{"title": "I. INTRODUCTION", "content": "Efficiently learning to perform general manipulation tasks has been a persistent focus in the field of robotics. In recent years, imitation learning has made significant progress in robotic manipulation [1-5]. However, these algorithms usually require a huge amount of data (e.g., thousands of demonstrations) for complex long-horizon deformable object manipulation tasks [5]. Such tasks present the following unique properties:\n\u2022 High-dimensional state space that often leads to complex initial and intermediate object states.\n\u2022 Complex dynamics that are difficult to accurately model in simulations.\n\u2022 Multi-modal distribution in action space.\nThese characteristics lead to significant distribution shifts and accumulation errors in traditional imitation learning algorithms for complex long-horizon tasks [6]. For an action policy modeled with probabilistic models (e.g., diffusion [1]), encountering unseen complex states makes the agent gradually drift away from the desired trajectory (see Fig. 1 left). To ensure that the data covers the high-dimensional state space as much as possible, the cost of collecting data in the real world will increase significantly. So, how can we learn to perform complex long-horizon deformable object manipulation tasks with a limited amount of data?\nOur core idea is simple: we try to make the policy model distinguish between good and bad actions and only select the best action (see Fig. 1 right) during inference. This will reduce abnormal actions and alleviate distribution shifts in long-horizon tasks. Reward functions are a common way to evaluate actions, but as in previous works [7, 8], designing a reward function for each task individually has much hidden cost. Therefore, we choose to use human preference data as a general representation across tasks for evaluating action quality.\nBased on this idea, we propose a general learning framework DeformPAM (see Fig. 2) for long-horizon Deformable object manipulation via Preference-based Action alignMent. Our approach has three stages: (1) In the first stage, we collect a small amount of human demonstration data and use supervised learning to train an initial probabilistic policy model based on diffusion [9] and action primitives. (2) In the second stage, we run rollouts on real robots with the initial probabilistic policy model and record the N predicted actions for each state, which are then annotated with preference data by humans. We use DPO (Direct Preference Optimization) [10] on diffusion models [11] to directly learn an implicit reward model from this preference data. (3) Finally, during inference, we use Reward-guided Action Selection (RAS) to boost the performance of the initial policy model from the first stage. Specifically, we use the initial policy model to generate N actions, score them using the implicit reward model, and select the action with the highest reward score to execute. We find that this approach effectively reduces the occurrence of anomalous actions, thereby improving the performance of complex long-horizon tasks for deformable objects.\nTo validate the effectiveness of our learning framework, we conducted extensive real-world experiments on three challenging long-horizon deformable object manipulation tasks involving granular (granular pile shaping), 1D (rope shaping), and 2D (T-shirt unfolding) deformable objects. All these tasks start with very complex initial object states. We use IoU, coverage, and Earth Mover's Distance (EMD) to quantitatively measure the task completion quality of the model. Real-world results indicate that our method improves task completion quality and time compared to baseline methods across multiple complex tasks. Our contributions are summarized as follows:\n\u2022 We design a general primitive learning framework (DeformPAM) for long-horizon deformable object manipulation, which uses an implicit reward model trained by preference data to select the action with higher quality.\n\u2022 We evaluate our method with real robots on several highly challenging long-horizon deformable object manipulation tasks."}, {"title": "II. RELATED WORKS", "content": "Deformable object manipulation is a field with a long research history and numerous applications. Most methods in this domain typically construct specific simulation environments tailored to particular object types [12-15], designing specialized rewards [7, 8] or learning pipelines [16, 17] to accomplish specific tasks. These hidden costs make it challenging for these learning frameworks to generalize across tasks. Recently, Differentiable Particles [18] attempted to use a differentiable simulator to plan optimal action trajectories applicable to various tasks. However, it requires additional object state estimators as input, whereas our approach learns actions directly from raw point clouds. AdaptiveGraph [19] is a model-based method for general-purpose deformable object manipulation, which learns the dynamics model of deformable objects using massive data in simulation and online interaction data in the real world, followed by using MPC to plan optimal execution trajectories. However, like Differentiable Particles [18], this method requires building simulation environments for each object type and each task, and it also suffers from the sim-to-real gap due to complex dynamics of deformable objects.\nIn recent years, there have been two main approaches to extend imitation learning to complex long horizon tasks: hierarchical imitation learning [20-24] and learning from play data [25-28]. Hierarchical imitation learning decomposes task learning into high-level planning and low-level controllers, while the latter approach collects interaction environment data through human teleoperation of robotic arms, without requiring specific task goals. Our method is more akin to hierarchical imitation learning, which improves sample efficiency by utilizing atomic action skills. However, these learning methods usually perform experiments on long-horizon tasks with rigid objects [24, 28], or assume simple initial object states [25] (e.g., flattened cloth). In comparison, our framework focuses on long-horizon tasks with deformable objects in complex initial states. Robo-Cook [29] is a framework for learning long horizon tasks involving deformable objects, but it is specifically designed for elasto-plastic objects (i.e., dough), making it difficult to adapt directly to 1D (e.g., ropes) and 2D (e.g., garments) deformable objects. In contrast, our method theoretically applies to deformable objects of various dimensions.\nLearning from human preference data [30-32] has garnered attention in the field of robotics. Recently, reinforcement learning from human feedback (RLHF) [33, 34] has become a popular way of leveraging preference data for aligning policy models (e.g., large language models). Subsequently, to eliminate the reliance on an explicit reward model in RLHF, DPO [10] and CPL [35] enable direct policy finetuning from preference data, based on contextual bandits and Markov decision processes respectively. Additionally, PFM [36] learns a conditional flow matching model from preference data to optimize the actions predicted by the policy model. Owing to their convenience, this methodology has also been applied in fields like image generation (Diffusion-DPO [11]). Instead of directly using the finetuned policy model [11] or learning an action transformation model [36], we leverage the underlying implicit reward model of DPO to guide action selection from multiple generated action samples."}, {"title": "III. PRELIMINARY", "content": "Diffusion models are a series of generative models that excel at generating samples 20 from arbitrary multimodal distributions by progressively denoising Gaussian noise $x_\u0442$. They can be conditional when given some condition c. A conditional diffusion model comprises two processes: the forward diffusion process and the reverse denoising process. They are considered as a Markow chain with fixed transitions q and learnable transitions $p_\u03b8$ respectively, which can be expressed as\n$q(x_t|x_{t-1}) : x_t = \\sqrt{\u03b1_t}x_{t-1} + \\sqrt{(1 - \u03b1_t)}\u03f5_{t-1}$, (1)\n$p_\u03b8(x_{t-1}|x_t, c) : x_{t-1} = \u03bc_\u03b8(x_t, c, t) + (\u03a3_\u03b8(x_t, c,t))^{1/2}\u03be_{t-1}$. (2)\nwhere {at \u2208 (0,1)} are the predefined variance schedule and \u03f5, \u03be are Gaussian noise. Moreover, an expression for directly calculating the diffusion result can be written as\n$q(x_t|x_0) : x_t = \\sqrt{\\prod_{i=1}^t \u03b1_i}x_0 + \\sqrt{(1 - \\prod_{i=1}^t \u03b1_i)}\u03f5$. (3)\nDuring training, reparameterize $\u03bc_\u03b8$ as $\u03bc_\u03b8(\u03f5_\u03b8, x_0)$ with Eq. 3 and a simplified ELBO objective in DDPM [9] is derived as\n$L_{simple} = E_{x_0,t,\u03f5}|| \u03f5 \u2013 \u03f5_\u03b8 (x_t, c, t)||^2$. (4)"}, {"title": "IV. METHODOLOGY", "content": "We will illustrate our learning pipeline DeformPAM (see Fig. 2) in three parts: (1) In Section IV-A, we demonstrate how to train a diffusion-based primitive policy model with supervised learning. (2) In Section IV-B, we describe how to use preference data to learn an implicit reward model with DPO finetuning. (3) In Section IV-C, we present Reward-guided Action Selection (RAS), which boosts the performance of the supervised model during inference by using an implicit reward model to guide action selection.\nWe will first introduce the basics of action primitive learning in Sec. IV-A.1. Then we will illustrate how to collect data to train an initial primitive policy model with supervised learning in Sec. IV-A.2 and Sec. IV-A.3.\nIn order to improve data efficiency, we decompose long-horizon tasks into multiple action primitives, and our model predicts only the action parameters for each primitive. This approach not only reduces the horizon length [24] but also allows us to perform highly dynamic actions (e.g. fling a garment [13]). Our primitive learning network takes an RGB-D image I as input in each manipulation step. After that, Grounded SAM [37] is used to segment the point cloud $P_t$ of the target object, then the policy model M will predict the predefined primitive action $\u00e2 = M(P_t)$. We use OMPL [38] to generate planning trajectories based on primitive parameters.\nIn order to acquire demonstration data, we design a graphic interface to let the user annotate one optimal action primitive parameter $a^o$ for each observation step and let the robot execute the action primitive. Since the optimal actions in deformable objects manipulation are often diverse (multi-modal), we offline annotate additional K ground truth actions {$a^k$}$^K$ called auxiliary actions (see Fig. 2 upper left) for previous seen observation states. Intuitively, using auxiliary actions allows the policy model to better understand the multi-modal nature of expert action distributions. These pairs of point cloud and action constitute the supervised learning dataset $D_{SL}$.\nOur network takes a masked 3D point cloud as input. We adopt a ResUNet3D [39] and a lightweight Transformer [40] as backbone. We use a diffusion head to predict final action primitive parameters.\nTo facilitate the efficiency of training and inference with auxiliary actions, we design a special technique for parallel training and inference. We reorganize the data in self-attention layers of the Transformer to prevent information leakage between distinct action tokens. This allows our network to simultaneously take multiple auxiliary actions and diverse noise in parallel for each state during training. Furthermore, during inference, for each state, our model can output multiple (N) potential actions in parallel with only one pass. We use the following DDPM [9] loss function for supervised learning:\n$L_{SL} = E_{(a_0,P)\u2208D_{SL},t,\u03f5}||\u03f5 \u2013 \u03f5_\u03b8 (\u03b1_t, P, t)||^2$. (5)\nTo alleviate distribution shifts in long-horizon tasks, we collect a new round of on-policy data by running rollouts with the supervised model trained in Sec. IV-A. We annotate the data with human preferences, then use DPO [11] to finetune the supervised model. Next, we will describe how we collect preference data and illustrate the learning algorithm."}, {"title": "C. Parallel Inference with Reward-guided Action Selection", "content": "Preference learning enables models to differentiate between good and bad actions. However, with limited data, DPO finetuning can cause significant forgetting and performance degradation, a phenomenon observed in [42]. To reduce abnormal actions and alleviate distribution shifts, we propose Reward-guided Action Selection (RAS) to choose from the multiple actions predicted by the supervised model trained in IV-A (see Fig. 2 right).\nA key byproduct of DPO finetuning is the implicit reward function. We exploit this to ensure robust action selection during inference. For the N potential actions predicted by the supervised model, we calculate the corresponding rewards and use a greedy strategy to select the action with the highest reward for execution. This inference process can be formulated as\n$a^* = arg \\underset{a\u2208M(P)}{max} r(a, P)$ (9)\nwhere r is the reward function. As in Diffusion-DPO [11], we can computer as\n$r(a_0, P) = -E_{t,\u03b2_T}(||e-E_{PL} (a_t, P, t)||^2 - ||e-E_{SL} (a_t, P, t)||^2)$. (10)\nIt can be intuitively interpreted as evaluating the finetuned model's tendency of denoising to $a_0$ while using the supervised model as a reference point.\nTo calculate rewards, we approximate the expectation through sampling. We observe that sampled values vary significantly across different diffusion timesteps t, with larger t producing smaller values. Thus, we use only the smallest 10% of timesteps for efficient reward calculation.\nIn the context of generative probabilistic policy models, such as diffusion models, the predicted actions typically form a multimodal distribution, concentrating around several centroids. In our experiments, we observe that for previously unseen states, the optimal action is often included among the multiple predictions generated by the policy model. However, the relative probability of this optimal action is generally low, resulting in its infrequent sampling. RAS can be understood as maintaining the original distribution of centroids while adjusting the assessment of their quality through reward-guided action selection. When online data are limited, the discriminative quality prediction can be generalized more effectively and efficiently to unseen states."}, {"title": "V. EXPERIMENTS", "content": "We conduct experiments on three challenging real-world long-horizon manipulation tasks. We first describe the experimental design and baseline methods, then focus on examining how does the model perform and what enables its capabilities through quantitative and qualitative evaluations.\nAs shown in Fig. 3a, We have designed three challenging long-horizon tasks: granular pile shaping, rope shaping and T-shirt unfolding. These tasks involve 1D, 2D, and granular deformable objects and all start with complex initial states. Next, we describe the definition of each task.\n\u2022 Granular Pile Shaping: In this task, the robot sweeps a disordered pile of granular objects (i.e., nuts) into the shape of the character T. We design a 3D-printed flat board as the robot tool and define the primitive parameters as a = ($p_s, p_e$), where $p_s$ and $p_e$ represent the start and end locations.\n\u2022 Rope Shaping: In this task, the robot shapes a looped rope from a random shape into a circle using the pick-and-place primitive action a = (p,q), where p and q stand for the pick and place locations.\n\u2022 T-shirt Unfolding: The goal of this task is to smooth out a short-sleeved T-shirt from a highly crumpled state. We use the fling action in Flingbot [13] as the primitive a = ($p_l, p_r$), where $p_l$ and $p_r$ denote the left and right pick locations.\nWe employ intersection over union (IoU), coverage, and Earth Mover's Distance (EMD) to evaluate the completion quality during the execution process.\nFor hardware setup, the dual-arm platform and tools illustrated in Fig. 3b are used to conduct all the experiments.\nWe design the following primitive-based methods for quantitative comparison.\n\u2022 SL: supervised model trained by offline data of stage 1.\n\u2022 SL + SL: supervised model trained with offline data of stage 1 and the on-policy data (only the optimal actions) of stage 2.\n\u2022 DPO [10] + Implicit RAS: DPO-finetuned model in stage 2 with implicit RAS during inference.\n\u2022 SL + Explicit RAS [41]: We implement an explicit reward model by adding a prediction head (similar to the action diffusion head) to the supervised pretrained network in stage 1. It is trained by directly optimizing Eq. 7 using preference data of stage 2. We use the pretrained supervised model in stage 1 for sampling actions and conduct Reward-guided Action Selection (RAS) by explicit reward prediction.\n\u2022 SL + Implicit RAS i.e., DeformPAM (Ours).\nWe trained for 2000 epochs for supervised learning and 200 epochs for preference learning. All methods predict (sample) N = 8 actions for each state during data collection and evaluation. We only capture object states before/after each action primitive for all primitive-based methods. We also implement Diffusion Policy (DP) [1] with teleoperation data (RGB inputs, 10 FPS) as a primitive-free method only for qualitative comparison due to very different hardware and task settings. We annotate K = 9 auxiliary actions for each state in the supervised dataset $D_{SL}$. The specific dataset sizes are shown in the Tab. I.\n$L_{SL} = E_{(a_0,P)\u2208D_{SL},t,\u03f5}||\u03f5 \u2013 \u03f5_\u03b8 (\u03b1_t, P, t)||^2$. (5)\nThe quantitative metrics from the real-world experiments are presented in Fig. 4. We then illustrate the impacts of key components in our proposed framework and what enables its capabilities by answering the following questions.\nQ1: Is using only supervised learning adequate for long-horizon tasks? As shown in Fig. 4, for the three tasks, with the help of reward-guided action selection, DeformPAM leads to an increase in the final completion quality. The variance in the quality metrics also tends to be smaller. Meanwhile, SL is more likely to generate abnormal action and get trapped in an intermediate state, preventing further improvement in the quality curve. The instability caused by these abnormal actions is mitigated through reward-guided action selection.\nQ2: How about training a supervised model with both off-policy and on-policy data? Training with on-policy data is another method to alleviate distribution shifts. Although such a method can reduce the long-tail phenomenon of completion steps in Fig. 4c, the results in Fig. 4a and Fig. 4b indicate that SL + SL achieves only marginal improvements in harder tasks compared to the one using off-policy data. Thus, employing reward-guided action selection is a more efficient method to enhance model performance.\nQ3: Does employing the finetuned model to predict action primitives result in better performance? As seen in Fig. 4a and Fig. 4b, DPO + Implicit RAS performs worse on the shaping tasks compared to the standard DeformPAM, and even underperforms the model using only supervised learning T-shirt Unfolding. It is probably due to the forgetting issues [42] in DPO finetuning, which leads to worse action prediction quality. This issue is more severe when data are limited, as is the case in this paper.\nQ4: Is it more effective to extract the implicit reward model from DPO or to directly predict the reward? Besides extracting an implicit reward model, another way to obtain rewards is to directly train an explicit reward model with preference data. From Fig. 4a and Fig. 4b, it can be found that for harder tasks like shaping, it is challenging for SL + Explicit RAS to achieve a high completion quality as the standard DeformPAM. This may be caused by reward overfitting when the size of the preference dataset is limited. In contrast, an implicit reward model from the DPO-finetuned model can fully leverage the action distribution learned during supervised learning. This phenomenon is inconsistent with conclusions in natural language processing (NLP) [43], primarily because both pre-training and preference fine-tuning data are relatively abundant in NLP tasks. Actually, as in Fig. 4c, an explicit reward model can also achieve a good performance in a simpler task (i.e., T-shirt unfolding) with relatively more data.\nQ5: How does reward-guided action selection contribute to performance? We analyzed the distribution of normalized implicit reward values during inference, as shown in Fig. 5a. This indicates that there is no positive correlation between the sampling probability of the action generation model and the predicted reward values, which suggests that employing reward-guided action selection can serve as a quality reassessment. From another perspective, we compare the performance between random sampling and reward-guided action selection by adjusting the number N of predicted actions during inference in the T-shirt unfolding task and computing the final coverage. As shown in Fig. 5b, as N increases, the model's performance gradually improves. This demonstrates that reward-guided action selection enables the model to select superior samples, thereby benefiting from a greater number of samples."}, {"title": "VI. CONCLUSION", "content": "In this paper, we introduced DeformPAM, a novel framework for long-horizon deformable object manipulation that leverages preference-based action alignment to mitigate distributional shifts and enhance task performance. By integrating supervised learning with a preference learning model, DeformPAM employs Reward-guided Action Selection (RAS) to improve decision-making. Our experiments on three challenging real-world tasks demonstrate that DeformPAM enhances both task completion quality and efficiency compared to baseline methods. Future works could explore extending this approach to incorporate multiple action primitives for more complex tasks."}]}