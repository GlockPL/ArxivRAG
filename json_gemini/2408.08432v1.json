{"title": "Predictive Uncertainty Estimation in Deep Learning for Lung Carcinoma Classification in Digital Pathology under Real Dataset Shifts", "authors": ["Abdur R. Fayjie", "Jutika Borah", "Florencia Carbone", "Jan Tack", "Patrick Vandewalle"], "abstract": "Deep learning has shown tremendous progress in a wide range of digital pathology and medical image classification tasks. Its integration into safe clinical decision-making support requires robust and reliable models. However, real-world data comes with diversities that often lie outside the intended source distribution. Moreover, when test samples are dramatically different, clinical decision-making is greatly affected. Quantifying predictive uncertainty in models is crucial for well-calibrated predictions and determining when (or not) to trust a model. Unfortunately, many works have overlooked the importance of predictive uncertainty estimation. This paper evaluates whether predictive uncertainty estimation adds robustness to deep learning-based diagnostic decision-making systems. We investigate the effect of various carcinoma distribution shift scenarios on predictive performance and calibration. We first systematically investigate three popular methods for improving predictive uncertainty: Monte Carlo dropout, deep ensemble, and few-shot learning on lung adenocarcinoma classification as a primary disease in whole slide images. Secondly, we compare the effectiveness of the methods in terms of performance and calibra-tion under clinically relevant distribution shifts such as in-distribution shifts comprising primary disease sub-types and other characterization analysis data; out-of-distribution shifts comprising well-differentiated cases, different organ origin, and imaging modality shifts. While studies on uncertainty estimation exist, to our best knowledge, no rigorous large-scale benchmark compares predictive uncertainty estimation including these dataset shifts for lung carcinoma classification.", "sections": [{"title": "Introduction", "content": "Recent advancement across a variety of domains (security, autonomous driving, healthcare, etc.) has led to extensive utilization of deep neural networks (DNNs) in real-world applications. Modern deep learning (DL) has achieved great success in predictive accuracy for supervised learning tasks but fails to provide information on the reliability of predictions. Moreover, most recent works neglect the need to quantify uncertainty prevalent in the networks while evaluating the networks.\nThe performance of models drops when encountered with distributional shifts from training data, one of the reasons being overconfidence in predictions. Distributional shifts refer to the problem where the model is evaluated on test data drawn from a different distribution"}, {"title": "Background", "content": "Estimation of uncertainty is an important topic in DL research that holds the potential to provide enhanced calibrated predictions and increased robustness of neural networks. Bayesian Neural Networks are dominant in the estimation of predictive uncertainty as they often give a good uncertainty estimation by computing the parameters over the posterior distribution, given a training distribution [6,15,17]. However, exact inference of Bayesian networks is hard and computationally expensive, which raises the need for solutions that can deliver quality uncertainty estimates with minor modifications to the standard training pipeline. Estimation of uncertainty with modifications to softmax confidence scores [27], and slight modification to neural network architectures [24] are new lines of research that may produce reasonable estimates of uncertainty.\nIn medical imaging, most DL applications have utilized MC-dropout and deep ensemble as"}, {"title": "Uncertainty estimation methods", "content": "All the methods are trained on data (images) drawn from a training distribution, $D_{in,train}$. During inference, methods are evaluated for uncertainty estimation on the $D_{ext,prot}, D_{ext,5ad}, D_{ood,cxr}, D_{ood,scc}$, and $D_{ood,cad}$ datasets. Additionally, the first two methods are also evaluated on the $D_{in,test}$ dataset. However, while evaluating FSL, we do not consider $D_{in,test}$ as we draw this distribution from $D_{in,train}$, which violates the unseen data generalization condition of FSL."}, {"title": "MC dropout", "content": "MC-dropout has been introduced as an alternative to the computationally expensive Bayesian probabilistic method for estimating uncertainty. A neural network, $f_{\\theta}(\u00b7)$, parameterized by $\\theta$ is trained on a training (data) set. Dropouts are generally enabled during training to regularize the learning and avoid overfitting of the neural networks. Most commonly, dropouts are disabled during test time to leverage all the connections of the network. However, following the work of [34], we enable the dropouts during test time. In our setting, we compute the posterior"}, {"title": "Deep ensemble", "content": "Deep ensemble is a non-Bayesian approach that involves training multiple neural networks [20,38] denoted as $f_{\\theta_{i}}; (.)$, where $\\theta$ is the weights and $i = \\{1,2,..., n\\}$ is the total number of networks in the ensemble. The trained weights, $\\theta_{i}$ are utilized to classify an image $x$ during inference. The final prediction is obtained by aggregating the predictions from $i$ independent neural networks in the ensemble based on prediction confidence. More specifically, we compute the probability of the input image $x$ to be in a class, $c$ as,\n$$p(y = c) = \\sum_{i=1}^{n}p(y | \\theta_{i}, x).$$"}, {"title": "Few-Shot Learning", "content": "In few-shot classification, C-way K-shot tasks are defined within the context of a test dataset. In our setting, we use two test datasets, denoted by $D_{test,ext}$ and $D_{ood,test}$. Each task samples a query image x and a support set comprised of K labeled examples, denoted by $S_c = \\{(x_i, y_i)\\}_{i=1}^{K}$ for each class c\u2208 C, drawn from the test dataset. Note that, K < Ntrain. Given this support set, the model, $f(\u00b7)$ is tasked with predicting a class label, $\u0177$ for the query image, $x$.\nWe employ a metric-based FSL method using Prototypical Networks [33]. Given an image, x, a backbone network $f_{\\theta}(\u00b7)$ extracts a corresponding $m$-dimensional feature vector, $z = f_{\\theta}(x) \u2208 \\mathbb{R}^m$. Class prototypes, indexed by c are then calculated by averaging the embedding vectors for K support samples as, $z_c = [\\frac{1}{|S_c|}\\sum_{(x,y)\u2208S_c} f_{\\theta}(x_i)$, where $|S_c|$ denotes the number of support samples belonging to the k-th class. The probability of classifying a query image x into class c\u2208 C is then obtained by applying a softmax over distances to the prototypes:\n$$p(y = c | x, \\theta) = \\frac{exp(d(f_{\\theta}(x), z_c))}{\\sum_{c=1}^{C} exp(d(f_{\\theta}(x), z_c))},$$\nThe similarity metric, $d(f_{\\theta}(x), z_c))$ utilizes Euclidean distance in our setting to measure similarities between query embedding and prototypes, $z_c \u2208 C$. Furthermore, we set the number of classes, $C$, and support samples, $K$ to 2 and 5, respectively. The predictive uncertainty of a query sample, $x$ is then calculated as, 1 - max(p). We train our FSL model using an episodic training strategy that mimics the test time C-way K-shot task definition to sample data from the training dataset, i.e. D(in,train) during training."}, {"title": "Datasets", "content": "Lung carcinoma is one of the most common causes of major cancer incidence and the second most common cause of cancer mortality worldwide [28]. This can be diagnosed pathologically either by a histologic or cytologic approach. Adenocarcinoma, Squamous Cell Carcinoma (SCC), and small and large cell carcinoma are four major histologic types of lung carcinoma. Adenocarcinoma is one of the most common carcinoma conditions which constitutes around 31% of carcinoma cases. SCC is the second most common carcinoma accounting for roughly 30% of the positive cases. Most cancers of the breast, pancreas, lung, prostate, and colon are adenocarcinomas. Table 1 presents the data distribution settings used in our experiments. Moreover, we present an example from each class of the datasets in Fig. 1.\nLC25000: LC25000 is a histopathology image dataset with 25,000 color images in 5 classes [2]. The 5 classes are divided into separate subfolders each containing 5,000 images of histologic entities, namely, lung adenocarcinoma, lung benign tissue, lung SCC, colon adeno-carcinoma, and benign colonic tissue. The images are publicly available and are de-identified,"}, {"title": "Uncertainty metrics", "content": "Entropy: Given an input image, $x$, modeled as a discrete random variable with $C$ possible class labels, Shannon's entropy quantifies the amount of uncertainty associated with its classification:\n$$H(x) = - \\sum_{c=1}^{C}p_c \\log p_c.$$"}, {"title": "Evaluation metrics", "content": "For evaluation of the results area-under-the-receiver-operating-characteristic-curve (AUROC) and area-under-precision-recall (AUPR) are considered. AUROC is the most common metric for performance evaluation in a binary classification problem. AUROC captures the trade-off between the true positive rate (TPR), also known as recall or sensitivity, and the false positive rate (FPR), also known as 1-specificity. AUPR is a performance measure for binary classification in a situation of class imbalance. High AUPR values indicate that the model is effective at identifying the positive classes without misclassifying many negatives as positives. Low AUPR indicates poor performance of the model as it struggles to maintain a good balance between precision and recall. Additionally, we consider accuracy for examining in detail how classification performance varies with different methods. The TPR and FPR are defined as: TPR=TP/(TP + FN) and FPR = FP/(TN + FP). The accuracy metric is given as:\n$$Accuracy = \\frac{TP+TN}{TP+FP+TN + FN},$$\nwhere TP, TN, FP, and FN denote the true positive, true negative, false positive, and false negative scores, respectively."}, {"title": "Experiments", "content": "To study a relevant application of DL in digital pathology, we define the primary target task as identifying adenocarcinoma in hematoxylin and eosin (H&E) stain tissue from lung sections. The main in-domain data in this study is lung adenocarcinoma. To enable the experiment, we obtained publicly available datasets and performed evaluations on three data distribution shifting scenarios with three uncertainty estimation methods. Fig. 2 illustrates our experimental data distribution setting. We considered adenocarcinoma and SCC as well-differentiated cases of carcinoma. Furthermore, adenocarcinoma is sub-classified into more specific sub-types such as acinar, lepidic, micropapillary, papillary, and solid. We evaluate the methods' ability to generalize to these carcinoma types, their sub-types, as well as carcinoma with a different organ origin other than lungs. These distribution settings fall under $D_{ext,test}$ and $D_{ood,test}$, and are not a part of our training phase. Furthermore, we investigate if higher predictive uncertainties with higher entropy values are exhibited by the network for those unknown new instances. In this work, we train a vanilla neural network classifier as a baseline for the evaluation of the uncertainty method."}, {"title": "Data Distribution Shifts", "content": "\u2022 In-distribution shift, $D_{ext,test}$. Here, we refer to in-distribution shifts as variations or changes that might affect the performance of the trained network. This distribution is not a part of the training distribution with subtle differences. The network has to adapt"}, {"title": "Out-of-Distribution (OOD) shift", "content": "\u2022 Out-of-Distribution (OOD) shift, $D_{ood,test}$. Here, we refer to OOD shifts as changes where ground truth labels are not one of the c classes from the training distribution. In Fig. 2, the OOD shift is illustrated in the rightmost column. The network encounters data at test time that corresponds to a completely different distribution (previously unseen) and that is outside the scope of the training distribution. This type of distribution scenario offers challenges for the network to make accurate estimations. Primarily, under these shifts, the networks are expected to exhibit higher entropy values as the distributions deviate largely from the training distribution. The classes over $D_{ext,test}$ and $D_{ood,test}$ are mutually exclusive. This category of shift includes:\n1. Single organ (lung) and multiple conditions (carcinoma sub-type), $D_{ood,scc}$: This category corresponds to lungs as a single organ with the possibility of occurrence of different carcinoma as sub-types (SCC) referred to as multiple conditions. This shift concerns the challenge in dealing with different carcinoma sub-types, and whether uncertainty estimation could be beneficial in mimicking the scenario of identifying the possibility of the presence of other sub-types such as SCC or rare conditions. Also, adenocarcinoma (training distribution) and SCC (test distribution) are two well-differentiated cases, morphologically different where the former has glandular characteristics with mucin production while the latter exhibits keratinization and intercellular bridges with solid nested growth patterns (refer Fig. 2, $D_{ood,scc}$).\n2. Same condition (adenocarcinoma) with organ shifts (colon), $D_{ood,cad}$: Carcinoma that begins in glandular (secretory) cells are found in the tissue lining of certain internal organs. Adenocarcinoma contributes to most carcinomas of the pancreas, breast, lung, colon, and prostate. Morphological assessment of differentiation of colon and rectum carcinoma applies only to adenocarcinomas. Epithelial tumors other than adenocarcinomas are rarely encountered in the colon or rectum (refer Fig. 2, $D_{ood,cad}$).\n3. Different modalities and different lung conditions (other than cancer), $D_{ood,cxr}$: Clinicians assess CXRs for lung conditions ranging from pneumonia, edema, COVID-19 to TB. In this category, we collected samples that contain lung infections such as pneumonia, and healthy lungs that the network has never been trained and tested previously (refer Fig. 2, $D_{ood,cxr}$)."}, {"title": "Implementation Details", "content": "For MC-dropout, we trained a Residual Networks [11], specifically, the ResNet50 model with pre-trained ImageNet [5] weights and two dropout layers before the logits. Here, dropout is enabled during test time as an approximation of the uncertainty of the model. This is obtained by 50 stochastic forward passes through the neural network. The dropout layers have drop rates of 0.25 and 0.5.\nFor deep ensemble, we trained 5 independent networks VGG19 [31], ResNet50, DenseNet121 [14], Xception [3], and EfficientNetB0 [36]. We employed Adam optimizer with an initial learning rate of 1 \u00d7 10-3. Standard cross-entropy loss is used as the objective function. The learning rate is decayed by a factor of 0.1 when validation accuracy stops improving for 5 epochs. We trained each network for 300 epochs with a batch size of 32 until convergence.\nFor FSL, we adapt ResNet10 pre-trained on ImageNet weights. We trained the network using cross-entropy loss, defined as $L = \\sum_{i=1}^{Cxnq} y_i.log(p(y_i|x_i))$, where $p(y_i|x_i)$ is computed according to Eq. 2. We sampled 1000 training episodes. The model is validated on 100 episodes every 200 training iterations. The Adam optimizer is instantiated with a learning rate of 10-4, and an L2 regularization term with a weight of 5 \u00d7 10-5 is applied. Finally, the trained model is evaluated on 20 test tasks sampled from the unseen test distribution, and report the average performance across these tasks."}, {"title": "Experimental Results and Analysis", "content": "In a practical clinical scenario, it is highly desirable for a decision making system to avoid being overconfident with incorrect predictions. To draw meaningful conclusions on the evaluation of uncertainty methods and metrics, we first trained a baseline model, a vanilla neural network that gives reasonable performance. We obtained interesting results. We summarize the results in Table 2, 3, 4, and 5."}, {"title": "Evaluation on predictive performance under dataset shift", "content": "Initially, we evaluate the predictive performance of the methods on the primary task of classifying adenocarcinoma from lung sections. Table 2 shows accuracy, AUROC and AUPR on six distributions including the internal test distribution. Overall the performance achieved on in-domain LC25000 adenocarcinoma data gives an AUROC of 0.9813 and AUPR of 0.9840 with baseline. This indicates that the baseline network was sufficient for this interpretation task. The drop in performance between the CPTAC-LUAD ($D_{ext,prot}$), BMRIDS ($D_{ext,5ad}$), CXR ($D_{ood,cxr}$) and LC25000 ($D_{ood,scc}$, and $D_{ood,cad}$) dataset is consistent with other observations that a well-trained model should suffer a relative decrease in performance when there is a shift in data distribution having different geographical origin (different medical centers) with associated diversity within data in the datasets.\nInvestigating the performance of the models having different organ origins (i.e. colon) with similar disease morphology as in-domain data from the LC25000 dataset, we found a drop in performance on colon adenocarcinoma: 0.5791 AUROC and 0.6068 AUPR compared to 0.9663 AUROC and 0.9425 AUPR on SCC carcinoma subtype (2). These results confirm that there is indeed a condition of data shift effect due to carcinoma types and different origins, which is in line with our assumptions. The same effect is also reflected in the accuracy score.\nThe methods can achieve high predictive performance on both internal (LC25000, $D_{in,test}$) and in-distribution datasets (CPTAC-LUAD, $D_{ext,prot}$) except for five adenocarcinoma sub-type (BMIRDS $D_{ext,5ad}$). All the methods performed significantly worse when there is a complete shift in distribution except for $D_{ood,scc}$. When evaluated on $D_{ood,cad}$, the baseline performed the worst while the ensemble and FSL performed better compared to MC-dropout. Interestingly, FSL has higher AUROC on $D_{ood,scc}$ compared to $D_{ood,cad}$ even though the cancer sub-type is morphologically different. On $D_{ood,c\u00e6r}$ all the methods have the worst performance compared to other distributions. In general, deep ensemble and FSL slightly outperform other methods on AUROC and AUPR."}, {"title": "Evaluation on predictive uncertainty under distribution shifts (in-distribution and OOD shifts)", "content": "We assume the predictions to exhibit higher uncertainty when the test data distribution shifts from the original source (training) distribution. This needs to be evaluated on data in a clinical context considering the relevant distributional shifts prevalent in real clinical applications.\nTable 3 and 4 summarizes the entropy, AUROC, and AUPR results under in-distribution shifts and OOD shifts with LC25000, CPTAC-LUAD, BMIRDS, and CXR data respectively, for different combinations of uncertainty methods. Here, we choose the entropy of the predictive distribution as an uncertainty metric for evaluating the quality of predictive uncertainty estimates. We assumed that the performance of a model would degrade as it predicts on increasingly shifted data, and ideally, this reduction in performance becomes sound and would coincide with increased entropy. For known classes, the entropy will be lower.\nFrom the results in Table 5, for in-distribution shifts in $D_{ext,prot}$, the ensemble showed the least entropy while the baseline achieved the highest entropy compared to MC-dropout and FSL. In the case of $D_{ext,5ad}$, MC-dropout again achieved the highest entropy compared to all other methods. Under OOD shift (4), baseline showed the highest entropy value while ensemble showed the least value in case of $D_{ood,cad}$. On $D_{ood,scc}$, FSL showed the least entropy value compared to ensemble and MC-dropout. For increasingly shifted data, in our case $D_{ood,c\u00e6r}$, all the methods showed high entropy values compared to $D_{ood,scc}$ and $D_{ood,cad}$."}, {"title": "Evaluation on Different Carcinoma sub-types", "content": "Table 5 summarizes the AUROC, AUPR, and FPR corresponding results for OOD detection. Here, a False Positive Rate or FPR is used in the evaluation pipeline for OOD detection as it indicates the model's ability to classify the positive instances correctly and avoid incorrectly"}, {"title": "Discussion", "content": "We evaluated current popular methods for predictive uncertainty on clinically relevant distribution shifts for lung carcinoma interpretation (classification) on H&E-stained tissues from lung section in digital pathology. In line with our assumption, all methods demonstrated the ability to generalize from internal to external test data while maintaining the quality of predictive uncertainty. However, when applied to interpret adenocarcinoma from a different origin (organ) other than lungs (colon adenocarcinoma), all the investigated methods exhibited a decrease in performance compared to in-domain lung adenocarcinoma data. This behavior was also observed when evaluating the methods on different adenocarcinoma sub-types. However, compared to organ shift, the performance of all the methods on adenocarcinoma sub-type inter"}, {"title": "Future Prospects and Challenges", "content": "Translation of DL to real clinical utility holds potential challenges. This includes addressing diverse needs and context of healthcare in the DL pipeline and establishing trust between clinicians and predictive model-generated outcomes. Furthermore, research volumes more often align with the necessity of academic incentives rather than patient and clinician needs. The clinicians are guided by their knowledge and intuitions to evaluate the value of patient health for translating it into actionable clinical information. They rely on identifying and understanding the features of the model that align with evidence-based medical practices. Incorporating predictive uncertainty estimation into these models can significantly enhance clinicians' understanding of the model's limitations. This, in turn, can serve as a valuable tool for the DL community in developing more trustworthy models for future clinical applications.\nDifferent factors lead to variations in the performance of DL models across different modalities like differences in specific medical imaging tasks and data availability, image resolution and its preprocessing needs, information content, complexity in model architecture, and clinical variability. Each modality in medicine has its unique challenges. Understanding the subtle differences is crucial for building effective deep-learning solutions in medicine and healthcare including digital pathology. Considering and leveraging approaches, that incorporate such spectrum within model design, training, and evaluation can impact and enhance the performance of deep learning and its transferability in clinical applications.\nTo optimize patient-centered decision-making, healthcare needs a shift towards personalized risk management tailored to individual patient characteristics, medical history, and current health status. For real clinical practice, it is anticipated that rich, multi-institutional data representing patient diversity and heterogeneity in diseases and their states including patient demographics will be required. Future work should investigate the implementation of how the uncertainty of deep ensemble and FSL extensions can be introduced for enhanced calibration. The main focus would be on investigating if a combination of various uncertainty estimation methods would result in improved calibrated performance."}, {"title": "Conclusion", "content": "In this work, we investigated the application of predictive uncertainty estimation for deep learning models in carcinoma classification for digital pathology, accounting for realistic data distribution shifts. Our evaluations demonstrate strong performance on in-domain data distribution (training and internal test distribution). However, performance degrades as the data distributions shift towards more diversified and differentiated cases that are underrepresented. This performance degradation is particularly evident in real-world settings where encountering data from new hospitals or diverse disease sub-types is common. Based on the results obtained with reliable uncertainty estimates and under clear indication and monitoring of the clinicians, DL-based methods can be utilized in clinical practices. However, it is noted that these methods are not ready for alarming novel abnormalities."}]}