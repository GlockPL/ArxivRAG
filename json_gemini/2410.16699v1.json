{"title": "Graph Transformers Dream of Electric Flow", "authors": ["Xiang Cheng", "Lawrence Carin", "Suvrit Sra"], "abstract": "We show theoretically and empirically that the linear Transformer, when applied to graph data, can implement algorithms that solve canonical problems such as electric flow and eigenvector decomposition. The input to the Transformer is simply the graph incidence matrix; no other explicit positional encoding information is provided. We present explicit weight configurations for implementing each such graph algorithm, and we bound the errors of the constructed Transformers by the errors of the underlying algorithms. Our theoretical findings are corroborated by experiments on synthetic data. Additionally, on a real-world molecular regression task, we observe that the linear Transformer is capable of learning a more effective positional encoding than the default one based on Laplacian eigenvectors. Our work is an initial step towards elucidating the inner-workings of the Transformer for graph data.", "sections": [{"title": "1 Introduction", "content": "The Transformer architecture (Vaswani et al., 2017) has seen great success in the design of graph neural networks (GNNs) (Dwivedi and Bresson, 2020; Ying et al., 2021; Kim et al., 2022; Muller et al., 2023), and it has demonstrated impressive performance in many applications ranging from prediction of chemical properties to analyzing social networks (Yun et al., 2019; Dwivedi et al., 2023; Rong et al., 2020). In contrast to this empirical success, the underlying reasons for why Transformers adapt well to graph problems are less understood. Several works study the expressivity of attention-based architectures (Kreuzer et al., 2021; Kim et al., 2021, 2022; Ma et al., 2023), but such analyses often rely on the ability of neural networks to approximate arbitrary functions, and may require a prohibitive number of parameters.\nThis paper is motivated by the need to understand, at a mechanistic level, how the Transformer processes graph-structured data. Specifically, we study the ability of a linear Transformer to solve certain classes of graph problems. The linear Transformer is similar to the standard Transformer, but softmax-based activation is replaced with linear attention, and the MLP layers are replaced by linear layers. Notably, the linear Transformer contains no a priori knowledge of the graph structure; all information about the graph is provided via an incidence matrix B. For unweighted graphs, the columns of B are just {\u22121,0,1}-valued indicator vectors that encode whether an edge touches a vertex; no other explicit positional or structural encodings are provided.\nEven in this minimal setup, we are able to design simple configurations of the weight matrices of a Transformer that enable it to solve fundamental problems such as electric flow and Laplacian eigenvector decomposition. Furthermore, we provide explicit error bounds that scale well with the number of Transformer layers. Several of our constructions rely crucially on the structure of the linear attention module, and may help shed light on the success of attention-based GNNs. We hope that our analysis paves the way to a better understanding of the learning landscape of graph Transformers, such as concrete bounds on their generalization and optimization errors."}, {"title": "1.1 Summary of Contributions", "content": "Below, we summarize the main content of our paper.\n1. We provide explicit weight configurations for which the Transformer implements efficient algorithms for several fundamental graph problems. These problems serve as important primitives in various graph learning algorithms, and have also been useful as PEs in state-of-the-art GNNs.\n(a) Lemma 1 constructs a Transformer that solves electric flows by implementing steps of gradient descent to minimize flow energy; consequently, it can also invert the graph Laplacian.\n(b) Lemmas 2 and 3 construct Transformers that compute low-dimensional resistive embeddings and heat kernels. Both constructions are based on implementing suitable power series.\n(c) By implementing a multiplicative polynomial expansion, Lemma 4 provides a construction for electric flow with exponentially higher accuracy than Lemma 1. Similarly, Lemma 5 provides a construction that computes the heat kernel using much fewer layers than Lemma 3.\n(d) In Lemma 6, we show that the Transformer can implement subspace iteration for finding the top-k (or bottom-k) eigenvectors of the graph Laplacian. Central to this analysis is the ability of self-attention to compute a QR decomposition of the feature vectors.\nWe derive explicit error bounds for the Transformer based on the convergence rates of the underlying algorithm implemented by the Transformer. We summarize these results in Table 1.\n2. In Section 5, we provide a more efficient, constrained version of the linear Transformer with much fewer parameters. Although the constrained Transformer is less expressive by definition, we show that it can nonetheless implement all the above-mentioned constructions. Further, we show in Lemma 10 that the parameter-efficient linear Transformer has desirable invariance and equivariance properties.\n3. We test the empirical performance of our theory on synthetic random graphs. In Section 3.5, we verify that Transformers with a few layers can achieve high accuracy in computing electric flows, resistive embeddings, as well as heat kernels. In Section 4.1, we verify that the Transformer can accurately compute top-k and bottom-k eigenevectors of the graph Laplacian.\n4. In Section 6, we demonstrate the advantage of using the linear Transformer as a replacement for Laplacian eigenvector positional encoding on a molecular regression benchmark using the QM9 and ZINC datasets (Ruddigkeit et al., 2012; Ramakrishnan et al., 2014; Irwin et al., 2012). After replacing the Laplacian eigenvector-based PE with the linear Transformer, and training on the regression loss, we verify that the linear Transformer automatically learns a good PE for the downstream regression task that can outperform the original Laplacian eigenvector-based PE by a wide margin."}, {"title": "1.2 Related Work", "content": "Numerous authors have proposed different ways of adapting Transformers to graphs (Dwivedi and Bresson, 2020; Ying et al., 2021; Rampasek et al., 2022; Muller et al., 2023). A particularly promising approach is to use a positional encodings to incorporate structural information in the input, examples include Laplacian eigenvectors (Dwivedi and Bresson, 2020; Kreuzer et al., 2021), heat kernel (Choromanski et al., 2022), resistance distance and commute time (Ma et al., 2023; Velingker et al., 2024; Zhang et al., 2023) and shortest path distance (Ying et al., 2021). Lim et al. (2022) design a neural network to transform eigenvectors into an encoding that has certain invariance properties. Black et al. (2024) compare the expressivity of different PE schemes."}, {"title": "2 Preliminaries and Notation", "content": "We use $\\mathcal{G} = (V, E)$ to denote a graph with vertex set V and edge set E; n denotes |V| and d represents |E|.\nWe often identify the vertex $v_i$ with its index i for i = 1...n, and the edge $e_j$ with j for j = 1...d. We will generally consider weighted graphs, where $r(\\cdot) : E \\rightarrow \\mathbb{R}^+$ gives the edge weights. We will use $r_j := r(e_j)$. A central object of interest is the incidence matrix $B \\in \\mathbb{R}^{n \\times d}$, defined as follows: to each edge $e_j \\in E$, assign an arbitrary orientation, i.e. $e = (u \\rightarrow v) \\in E$. The matrix B is given by\n$B_{ij} =\\begin{cases}\n  -1/\\sqrt{r_j}, & \\text{if exists } v \\in V \\text{ such that } e_j = (u_i \\rightarrow v)\\\\\n  +1/\\sqrt{r_j}, & \\text{if exists } v \\in V \\text{ such that } e_j = (v \\rightarrow u_i)\\\\\n  0, & \\text{otherwise}.\n\\end{cases}$ (2.1)\nNext, we define the graph Laplacian as $L := BB^T \\in \\mathbb{R}^{n \\times n}$. We will often need to refer to the maximum eigenvalue of L, which we denote as $\\lambda_{max}$. Note that L always has 0 as its smallest eigenvalue, with corresponding eigenvector $\\frac{1}{\\sqrt{n}}1$, the all-ones vector. This fact can be verified by noticing that each column of B sums to 0. For a connected graph (as we will assume is the case throughout the paper), the second-smallest eigenvalue is always non-zero, and we will denote it as $\\lambda_{min}$."}, {"title": "2.2 Linear Transformer", "content": "We will use $Z_0 \\in \\mathbb{R}^{h \\times n}$ to denote the input to the Transformer. $Z_0$ encodes a graph $\\mathcal{G}$, and each column of $Z_0$ encodes a single vertex in h dimensions. Let $W^Q, W^K, W^V \\in \\mathbb{R}^{h \\times h}$ denote the key, query and value parameter matrices. We define linear attention Attn as\n$Attn_{W^V, W^Q, W^K}(Z) := W^V Z Z^T W^{Q^T}(W^K Z)$. (2.2)\nUnlike standard attention, (2.2) does not contain softmax activation. We construct an L-layer Transformer by stacking L layers of the attention module (with linear feed-forward). To be precise, let $Z_l$ denote the output of the $(l-1)^{th}$ layer of the Transformer. Then\n$Z_{l+1} := Z_l + Attn_{W_l^V, W_l^Q, W_l^K}(Z_l) + W^RZ_l,$ (2.3)\nwhere $W_l^V, W_l^Q, W_l^K$ are the value, query and key weight matrices of the linear attention module at layer l, and $W_l^R \\in \\mathbb{R}^{h \\times h}$ is the weight matrix of the linear module. Henceforth, we let $W^V := \\{W_l^V\\}_{l=0...L}, W^Q := \\{W_l^Q\\}_{l=0...L}, W^K := \\{W_l^K\\}_{l=0...L}, W^R := \\{W_l^R\\}_{l=0...L}$ denote collections of the parameters across all layers of an L-layer Transformer.\nFinally, we will often need to refer to specific rows of $Z_l$. We will use $[Z_l]_{i...j}$ to denote rows i to j of $Z_l$."}, {"title": "3 Transformers as powerful solvers for Laplacian problems", "content": "In this section, we discuss the capacity of the linear Transformer 2.3 to solve certain classes of canonical graph problems. We begin with the problem of Electric Flow in Section 3.2, where the constructed Transformer can be interpreted as implementing steps of gradient descent with respect to the energy of the induced flow. Subsequently, in Section 3.3, we provide constructions for computing the resistive embedding, as well as the heat kernel, based on implementing a truncated power series. Finally, in Section 3.4, we provide faster alternative constructions for solving electric flow and computing heat kernels, based on implementing a multiplicative polynomial expansion. In each case, we bound the error of the Transformer by the convergence rate of the underlying algorithms."}, {"title": "3.1 Additional Setup", "content": "We introduce some additional setup that is common to many lemmas in this section. We will generally consider an L-layer Transformer, as defined in (2.3), for some arbitrary $L \\in \\mathbb{Z}^+$. As in (2.3), we use $Z_l$ to denote the input to layer l. The input $Z_0$ will encode information about a graph $\\mathcal{G}$, along with a number of demand vectors $\\psi_1 ... \\psi_k \\in \\mathbb{R}^n$. We use $\\Psi$ to denote the $n \\times k$ matrix whose $i^{th}$ column is $\\psi_i$. Unless otherwise stated, $Z_l \\in \\mathbb{R}^{(d+2k)\\times n}$, where n is the number of nodes, d is the number of edges, and k is the number of demands/queries. $Z_l := [B, \\Psi, 0_{n \\times k}]$.\nOn parameter size: In a straightforward implementation, the above Transformer has feature dimension $h = (d + 2k)$. The size of $W^Q, W^K, W^V, W^R$ is $O(h^2) = O(d^2 + k^2)$, which is prohibitively large as d can itself be $O(n^2)$. The size of parameter matrices can be significantly reduced to $O(k^2)$ by imposing certain constraints on the parameter matrices; we present this reduction in (5.1) in Section 5. For simplicity of exposition, lemmas in this section will use the naive implementation in (2.3). We verify later that all the constructions presented in this section can also be realized in (5.1)."}, {"title": "3.2 Solving Electric Flow with Gradient Descent", "content": "Assume we are given a graph $\\mathcal{G} = (V,E)$, along with a non-negative vector of resistances $r \\in \\mathbb{R}^d$. Let R be the d x d diagonal matrix with r on its diagonal. A flow is represented by $f \\in \\mathbb{R}^d$, where $f_j$ denotes the (directed) flow on edge $e_j$. The energy of an electric flow is given by $\\sum_{j=1}^d r_j f_j^2$. Let $\\psi \\in \\mathbb{R}^n$ denote a vector of demands. Throughout this paper, we will assume that the demand vectors satisfy flow conservation, i.e., $(\\psi,1) = 0$. The $\\psi$-electric flow is the unique minimizer of the following (primal) flow-optimization problem (by convex duality, this is equivalent to a dual potential-optimization problem):\n$\\begin{aligned}\n  & \\text{(primal)} &\\min_{f \\in \\mathbb{R}^d} & \\sum_{j=1}^d r_j f_j^2 & \\text{subject to the constraint } BR^{1/2} f = \\psi. & (3.1) \\\\\n  & \\text{(dual)} &\\min_{\\phi} & \\phi^T L \\phi - 2 \\psi. & (3.2)\n\\end{aligned}$\nThe argument is standard; for completeness, we provide a proof of equivalence between (3.1) and (3.2) in (8.1) in Appendix 8.1. It follows that the optimizer $\\phi^*$ of (3.2) has closed-form solution $\\phi^* = L^+ \\psi$. In Lemma 1 below, we show a simple construction that enables the Transformer in (2.3) to compute in parallel, the optimal potential assignments for a set of k demands $\\{\\psi_i\\}_{i=1..k}$, where $\\psi_i \\in \\mathbb{R}^n$.\nMotivation: Effective Resistance Metric\nAn important practical motivation for solving the electric flow (or equivalently computing $L^+$) is to obtain the Effective Resistance matrix $R \\in \\mathbb{R}^{n \\times n}$. GNNs that use positional encodings derived from $R$ have demonstrated state-of-the-art performance on numerous tasks, and can be shown to have good theoretical expressivity (Zhang et al., 2023; Velingker et al., 2024; Black et al., 2024). Formally, R is defined as $R_{ij} := (u_i - u_j)^T L^+ (u_i - u_j)$, where $u_i$ denotes the vector that has a 1 in the $i^{th}$ coordinate, and 0s everywhere else. Intuitively, $R_{ij}$ is the potential drop required to send 1-unit of electric flow from node i to node j. Let $l \\in \\mathbb{R}^n$ denote the vector of diagonals of $L^+$ (i.e. $l_i := L_{ii}$). Then $R = l1^T + 1l^T - 2L^+$; thus computing $L^+$ essentially also computes R.\nLemma 1 (Transformer solves Electric Flow by implementing Gradient Descent). Consider the setup in Section 3.1. Assume that $\\langle \\psi_i, 1 \\rangle = 0$ for each i = 1...k. For any $\\delta > 0$ and for any L-layer Transformer, there exists a choice of weights $W^V, W^Q, W^K, W^R$, such that each layer of the Transformer (2.3) implements a step of gradient descent with respect to the dual electric flow objective in (3.2), with stepsize $\\delta$. Consequently, the following holds for all i = 1...k and for any graph Laplacian with maximum eigenvalue $\\lambda_{max} < 1/\\delta$ and minimum nontrivial eigenvalue $\\lambda_{min}$:\n$||[Z_L]_{d+k+i} - L^+\\psi_i||_2 \\leq \\exp(\\frac{-\\delta L \\lambda_{min}}{2}) ||\\psi_i||_2$\nDiscussion. In the special case when k = n and $\\Psi := I_{n \\times n} - \\frac{1}{n}11^T$, $[Z_L]_{d+k+1...d+2k} \\approx L^+$. An $\\epsilon$-approximate solution requires $L = O(\\log(1/\\epsilon))$ layers; this is exactly the convergence rate of gradient descent on a Lipschitz smooth and strongly convex function. In Lemma 4 below, we show an alternate construction that reduces this to O(log log(1/\\epsilon)).\nWe defer details of the proof of Lemma 1 to Appendix 8.1, and we provide the explicit choice of weight matrices in (8.3). The constructed weight matrices are very sparse; each of $W^V, W^Q, W^K, W^R$ contains a single identity matrix in a sub-block. This sparse structure makes it possible to drastically reduce the number of parameters needed, which we exploit in Section 5 to design a more parameter efficient Transformer. We provide experimental validation of Lemma 1 in Section 3.5."}, {"title": "3.3 Implementing Truncated Power Series", "content": "In this section, we present two more constructions: one for computing $\\sqrt{L^+}$ (Lemma 2), and one for computing the heat kernel $e^{-sL}$ (Lemma 3). Both quantities have been successfully used for positional encoding in various GNNs. The constructions proposed in these two lemmas are also similar, and involve implementing the power series of the respective targets."}, {"title": "3.3.1 Computing The Principal Square Root $\\sqrt{L^+}$", "content": "Motivation: Resistive Embedding\nThe following fact relates the effective resistance matrix R to any \u201csquare root\u201d of $L^+$:\nFact 1. Let M denote any matrix that satisfies $MM^T = L^+$. Let $R \\in \\mathbb{R}^{n \\times n}$ denote the matrix of effective resistances (see Section 3.2). Then $R_{ij} = ||M_i - M_j||_2$, where $M_i$ is the $i^{th}$ row of M.\nOne can verify the above by noticing that $||M_i - M_j||_2^2 = M_i M_i + M_j M_j - 2 M_i M_j = [L^+]_{ii} + [L^+]_{jj} - 2L_{ij}$. In some settings, it is more natural to use the rows of M to embed node position, instead of directly using R: By assigning an embedding vector $w_i := M_i$ to vertex i, the Euclidean distance between $w_i$ and $w_j$ equals the resistance distance.\nThe matrix M is under-determined, and for any $m > n$, there are infinitely many choices of M that satisfy $MM^T = L^+$. Fact 1 applies to any such M. Velingker et al. (2024) uses the rows of $M = L^+B$ for resistive embedding. Under this choice, $M_i$ has dimension d, which is the number of edges and can be quite large. To deal with this, Velingker et al. (2024) additionally performs a dimension-reduction step using Johnson Lidenstrauss.\nAmong all valid choices of M, there is a unique choice that is symmetric and minimizes $||M||_F$, namely, $U S^{-1/2} U^T$, where $U S U^T = L$ is the eigenvector decomposition of L. We reserve $\\sqrt{L^+}$ to denote this matrix; $\\sqrt{L^+}$ is called the principal square root of $L^+$. In practice, $\\sqrt{L^+}$ might be preferable to, say, $L^+B$ because it has an embedding dimension of n, as opposed to the possibly much larger d. We present in Lemma 2 a Transformer construction for computing $\\sqrt{L^+}$.\nLemma 2 (Principal Square Root $\\sqrt{L^+}$). Consider the setup in Section 3.1. Assume that $\\psi_1...\\psi_k \\in \\mathbb{R}^n$ satisfy $\\langle \\psi_i, 1 \\rangle = 0$. For any L-layer Transformer (2.3), there exists a configuration of weights $W^V, W^Q, W^K, W^R$, such that the following holds: For any graph with maximum Laplacian eigenvalue less than $\\lambda_{max}$ and minimum non-trivial Laplacian eigenvalue greater than $\\lambda_{min}$, and for all i = 1...k:\n$||[Z_L]_{d+k+i} - \\sqrt{L^+}\\psi_i||_2 \\leq \\frac{e^{-L \\lambda_{min}/\\lambda_{max}}}{\\lambda_{min}} \\sqrt{\\frac{\\lambda_{max}}{L}} ||\\psi_i||_2$.\nDiscussion. We defer a proof of Lemma 2 to Appendix 8.2. The high-level idea is that each layer of the Transformer implements one additional term of the power series expansion of $\\sqrt{L^+}$. Under the choice k = n and $\\Psi = I_{n \\times n} - (1/n)11^T$, $[Z_L]_{d+k+1...d+2k} \\approx \\sqrt{L^+}$. An $\\epsilon$ approximation requires log(1/\\epsilon) layers. We consider the more general setup involving arbitrary $\\psi_i$s as they are useful for projecting the resistive embedding onto a lower-dimensional space; this is relevant when $\\psi_i$'s are trainable parameters (see e.g., the setup in Appendix 12.3). We empirically validate Lemma 2 in Section 3.5."}, {"title": "3.3.2 Computing the heat kernel: $\\exp (-sL)$", "content": "Finally, we present a result on learning heat kernels. The heat kernel has connections to random walks and diffusion maps Coifman and Lafon (2006). It plays a central role in semi-supervised learning on graphs (Xu et al., 2020); it is also used for positional encoding (Choromanski et al., 2022). We note that in some settings, the heat kernel excludes the nullspace of L and is instead defined as $e^{-sL} - 11^T/n$.\nLemma 3 (Heat Kernel $e^{-sL}$). Consider the setup in Section 3.1. Assume that $\\psi_1...\\psi_k \\in \\mathbb{R}^n$ satisfy $\\langle \\psi_i, 1 \\rangle = 0$. Let s > 0 be an arbitrary temperature parameter. There exists a configuration of weights for the L-layer Transformer (2.3) such that the following holds: for any input graph whose Laplacian L satisfies $8s \\lambda_{max} \\leq L$, and for all i = 1...k,\n$||[Z_L]_{i} - e^{-sL}\\psi_i||_2 \\leq 2^{-L+8s \\lambda_{max}+1}||\\psi_i||_2$\nDiscussion. We defer the proof of Lemma 3 to Appendix 8.3. To obtain $\\epsilon$ approximation error, we need number of layers $L > O(\\log(1/\\epsilon) + s \\lambda_{max})$. As with the preceding lemmas, the flexibility of choosing any number of $\\psi_i$'s enables the Transformer to learn a low-dimensional projection of the heat kernel. The dependence on $s \\lambda_{max}$ is fundamental, stemming from the fact that the truncation error of the power series of $e^{-sL}$ begins to shrink only after $O(s \\lambda_{max})$ the first terms. In Lemma 5 in the next section, we weaken this dependence from $s \\lambda_{max}$ to $\\log(s \\lambda_{max})$. We empirically validate Lemma 3 in Section 3.5."}, {"title": "3.4 Implementing Multiplicative Polynomial Expansion", "content": "We present two alternative Transformer constructions that can achieve vastly higher accuracy than the ones in preceding sections: Lemma 4 computes an $\\epsilon$-accurate electric flow in exponentially fewer layers than Lemma 1. Lemma 5 approximates the heat kernel with higher accuracy than Lemma 3 when the number of layers is small. The key idea in both Lemmas 4 and 5 is to use the Transformer to implement a multiplicative polynomial; this in turn makes key use of the self-attention module.\nThe setup for Lemmas 4 and 5 differs in two ways from that presented in Section 3.1. First, the input to layer 1, $Z_l$, are now in $\\mathbb{R}^{3n \\times n}$, instead of $\\mathbb{R}^{(d+2k) \\times n}$. When the graph $\\mathcal{G}$ is sparse and the number of demands/queries k is small, the constructions in Lemma 1 and 3 may use considerably less memory. This difference is fundamental, due to the storage required for raising matrix powers. Second, the input is also different; in particular, information about the graph is provided via L as part of the input $Z_0$, as opposed to via the incidence matrix $B^T$ as done in Section 3.1. This difference is not fundamental: one can compute $L = B^T B$ from B in a single attention layer; in the spirit of brevity, we omit this step. We begin with the faster construction for electric flow:\nLemma 4. Let $\\delta > 0$. Let $Z_1 := [(I_{n \\times n} - \\delta L), I_{n \\times n}, \\delta I_{n \\times n}]$. Then there exist a choice of $W^V, W^Q, W^K, W^R$ for a L-layer Transformer (2.3) such that for any graph with Laplacian with smallest non-trivial eigenvalue $\\lambda_{min}$ and largest eigenvalue $\\lambda_{max} < 1/\\delta$,\n$||[Z_L]_{2n+1...3n} - L^+||_2 \\leq \\frac{1}{2} \\frac{1}{\\lambda_{min}} \\exp(-\\delta^2 L \\lambda_{min})$.\nDiscussion. Lemma 4 shows that the Transformer can compute an $\\epsilon$-approximation to $L^+$ (which is sufficient but not necessary for solving arbitrary electric flow demands) using $\\log \\log(1/\\epsilon)$ layers. This is much fewer than the log(1/$\\epsilon$) layers required in Lemma 1. The key idea in the proof is to implement a multiplicative polynomial expansion for $L^+$. We defer the proof to Appendix 8.4.\nNext, we show the alternate construction for computing the heat kernel:\nLemma 5 (Fast Heat Kernel). Let s > 0. Let L be the number of Transformer layers. Let $Z_0 := (I_{n \\times n} - \\frac{1}{L}3 sL)$. Then there exist a choice of $W^V, W^Q, W^K, W^R$ such that for any graph whose Laplacian L satisfies $s \\lambda_{max} \\leq 3L$,\n$|| Z_L - \\exp(-sL)||_2 \\leq 3^{-L+1}s^2 \\lambda_{max}$.\nDiscussion. Lemma 5 shows that the Transformer can compute an $\\epsilon$-approximation to $e^{-sL}$ using $O(\\log(1/\\epsilon) + \\log(s \\lambda_{max}))$ layers. The $\\epsilon$ dependence is the same as Lemma 3, but the $\\log(s \\lambda_{max})$ is an improvement. When the number of layers is small, Lemma 5 gives a significantly more accurate approximation. The proof is based on the well-known approximation $e^x \\approx (1 + x/L)^L$. We defer proof details for Lemma 5 to Appendix 8.4."}, {"title": "3.5 Experiments", "content": "In Figure 1, we experimentally verify that the Transformer is capable of learning to solve the three objectives presented in Lemmas 1, 2 and 3. The setup is as described in Section 3.1, with the Transformer described in (2.3) with k = n, but with the following important difference: the output of each layer is additionally normalized as follows: $[Z_l]_{1...n} \\leftarrow [Z_l]_{1...n}/||[Z_l]_{1...n}||_F, [Z_l]_{n+1...n+2k} \\leftarrow [Z_l]_{n+1...n+2k}/||[Z_l]_{n+1...n+2k}||_F$, where $|| \\cdot ||_F$ is the Frobenius norm. Without this normalization, training the Transformer becomes very difficult beyond 5 layers.\nWe consider two kinds of random graphs: fully-connected graphs (n = 10,d = 45) and Circular Skip Links (CSL) graphs (n = 10,d = 20). Edge resistances are randomly sampled. We provide details on the sampling distributions in Appendix 11. For each input graph $\\mathcal{G}$, we sample n demands $\\Psi_1...\\Psi_n \\in \\mathbb{R}^n$ independently from the unit sphere. Let $\\Psi = [\\Psi_1...\\Psi_n]$. The input to the Transformer is $[B^T, \\Psi^T, 0_{n \\times n}]$, consistent with the setup described in Section 3.1. The training/test loss is given by\n$loss_{\\mathcal{U}} := \\mathbb{E} \\frac{1}{n} \\sum_{i=1}^n \\frac{||[Z_L]_{d+n+i} - U \\psi_i||^2}{||U \\psi_i||^2}$, where $U \\in \\{L^+, \\sqrt{L^+}, e^{-0.5L}\\}$. We learn the correct solutions {L^+, \\sqrt{L^+}, e^{-0.5L}}."}, {"title": "4 Transformers can Implement Subspace Iteration to Compute Eigenvectors", "content": "We present a Transformer construction for finding the eigenvectors of the graph Laplacian L. The eigenvector for the smallest non-trivial eigenvalue, for example, has been applied with great success in graph segmentation (Shi and Malik, 1997) and clustering (B\u00fchler and Hein, 2009). Additionally, the Laplacian eigenvectors are also commonly used for positional encoding (see Section 1.2). Our construction is based on the subspace iteration algorithm (Algorithm 1), aka block power method\u2014see e.g., (Bentbib and Kanber, 2015). The output $\\Phi$ of Algorithm 1 converges to the top-k eigenvectors of L. We show in Corollary 7 that a modified construction can also find the bottom-k eigenvectors of L.\nAlgorithm 1 \u2013 Subspace Iteration\n$\\Phi_0 \\in \\mathbb{R}^{n \\times k}$ has full column rank .\nwhile not converged do\n$\\Phi \\leftarrow L \\Phi$\n$\\Phi \\leftarrow QR(\\Phi)$\nend while \nFor the purposes of this section, we consider a variant of the Transformer defined in (2.3).\n$Z_{l+1} := Z_l + Attn_{W_l^V, W_l^Q, W_l^K}(Z_l) + W_l^R Z_l$\n$Z_{l+1} = normalize(\\tilde{Z}_{l+1}),$ (4.1)\nwhere $normalize(Z)$ applies row-wise normalization: $[normalize(Z)]_i \\leftarrow [Z]_i/||[Z]_i||_2$ for i = d + 1...d + k. We use $B_l \\in \\mathbb{R}^{n \\times d}$ and $\\Phi_l \\in \\mathbb{R}^{n \\times k}$ to denote refer to specific columns of $Z_l$, defined as $Z_l =: [B_l, \\Phi_l]$. The notation is chosen as $\\Phi_l$ corresponds to $\\Phi$ in Algorithm 1. We initialize $B_0 = B$ and let $\\Phi_0$ be any column-orthogonal matrix.\nLemma 6 (Subspace Iteration for Finding Top k Eigenvectors). Consider the Transformer defined in (4.1). There exists a choice of $W^V, W^Q, W^K, W^R$ such that k + 1 layers of the Transformer implements one iteration of Algorithm 1. Consequently, the output $\\Phi_L$ of a L-layer Transformer approximates the top-k eigenvectors of L to the same accuracy as L/ (k + 1) steps of Algorithm 1."}, {"title": "4.1 Experiments for Lemma 6", "content": "We verify Lemma 6 and Corollary 7 experimentally by evaluating the ability of the Transformer (2.3) to learn top-k and bottom-k eigenvectors. As in Section 3.5", "nodes": "fully connected (d = 45 edges) and CSL (d = 20 edges); each edge is has a randomly sampled resistance; see Appendix 11 for details. For a graph $\\mathcal{G}$ with Laplacian L, let $\\lambda_1 \\leq \\lambda_2 \\leq ... \\lambda_{10}$ denote its eigenvalues. Let $v_1, ..., v_{10}$ denote its eigenvectors. $\\lambda_1$ is always 0 and $v_1$ is always $1/\\sqrt{n}$.\nThe Transformer architecture is as defined in Section 4.1, with k = n. We increase the dimension of $\\Phi_l$ to (2n) \u00d7 n, and thus the dimension of $Z_l$ to (d + n + n) \u00d7 n. We read out the last n rows of $Z_l$ as output. The purpose of increasing the dimension is to make the architecture identical to the one used in the experiments in Section 3.5; the construction in Lemmas 6 and Corollary 7 extend to this setting by setting appropriate parameters to 0. In addition, we also normalize $[Z_l"}]}