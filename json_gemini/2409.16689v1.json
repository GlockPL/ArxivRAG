{"title": "Layout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete Diffusion Model", "authors": ["Shoma Iwai", "Atsuki Osanai", "Shunsuke Kitada", "Shinichiro Omachi"], "abstract": "Layout generation is a task to synthesize a harmonious layout with elements characterized by attributes such as category, position, and size. Human designers experiment with the placement and modification of elements to create aesthetic layouts, however, we observed that current discrete diffusion models (DDMs) struggle to correct inharmonious layouts after they have been generated. In this paper, we first provide novel insights into layout sticking phenomenon in DDMs and then propose a simple yet effective layout-assessment module Layout-Corrector, which works in conjunction with existing DDMs to address the layout sticking problem. We present a learning-based module capable of identifying inharmonious elements within layouts, considering overall layout harmony characterized by complex composition. During the generation process, Layout-Corrector evaluates the correctness of each token in the generated layout, reinitializing those with low scores to the un-generated state. The DDM then uses the high-scored tokens as clues to regenerate the harmonized tokens. Layout-Corrector, tested on common benchmarks, consistently boosts layout-generation performance when in conjunction with various state-of-the-art DDMs. Furthermore, our extensive analysis demonstrates that the Layout-Corrector (1) successfully identifies erroneous tokens, (2) facilitates control over the fidelity-diversity trade-off, and (3) significantly mitigates the performance drop associated with fast sampling.", "sections": [{"title": "Introduction", "content": "Creating a layout is one of the most crucial tasks involving human labor when designing [1], and there are a wide variety of applications, including academic papers [50], application user interfaces [8], and advertisements [45]. Layout generation has been formulated as a task that determines a set of elements that consist of categories, positions, and sizes [33,40]. In response, layout-generation methods on deep learning have shown remarkable performance, and in particular, discrete generative models such as masked language modeling [9]-based methods [5, 26, 43] and discrete diffusion models (DDMs) [13, 20, 21,47] are the current state-of-the-art (SoTA).\nTo create an aesthetically pleasing layout, human designers typically modify layouts through trial and error. However, we found that even SoTA DDMs can not update elements in layouts once they have been generated, i.e., layout sticking. An intuitive example of the sticking behavior is depicted in Fig. 1, where inharmonious elements that arose during generation persist until the final generated result. While former studies [6, 24,35,36] tried to refine these elements in the post-processing phase that minimizes the rule-based costs such as alignment, they could not capture higher-order senses that determine layout aesthetics.\nIn the image generation domain, non-autoregressive (Non-AR) decoding methods with an external critic have demonstrated remarkable performance [29, 30]. The module identifies deviated visual tokens from the real distribution and reset them to re-sample. Reviewing the success of the masked image modeling [16], a few visual clues can provide plenty of information to identify erroneous tokens. On the other hand, the layout domain has different characteristics; (i) unlike images with a fixed and enough number of tokens (e.g., 16 \u00d7 16 patches), the number of layout elements is small and varies across samples (e.g., 1 to 25 elements), and (ii) as the element composed of multiple attributes, then partially observed tokens do not provide enough clues. Thus, it is non-trivial whether the technique in the vision can apply to the layout generation.\nIn this paper, we propose a simple yet effective approach, named Layout-Corrector, to address the layout sticking problem. It works as an external module that evaluates each token's correctness score in a learning-based manner, aiming to identify erroneous tokens in a layout. As shown in Fig. 1, during the generation process, tokens with low correctness scores are reset to the ungenerated state (i.e., [MASK]). Then, a DDM regenerates them using the remaining high-scored tokens as clues. Additionally, to deal with the characteristics of the layout tokens mentioned above, we propose a new objective and application schedule that accommodates variable numbers of elements while providing reliable layout cues.\nWe conducted extensive experiments on Layout-Corrector using three benchmarks [8, 45, 50]. When in conjunction with strong baselines [5, 13, 21], Layout-Corrector significantly enhanced their performance in both unconditional and conditional generation tasks. Both quantitative and qualitative evaluations confirmed that our approach effectively corrects inharmonious layout elements, addressing the challenge present in SoTA DDMs. By adjusting the application schedule of the corrector, we also achieved enhanced control over the fidelity-diversity and speed-quality trade-offs, demonstrating Layout-Corrector's versatility across different application scenarios.\nOur contributions are summarized as follows: (1) We empirically demonstrate that current SoTA DDMs struggle to correct inharmonious elements in layouts; however, they can effectively correct them when erroneous elements are initialized to the ungenerated state, [MASK]. (2) We propose Layout-Corrector for evaluating the correctness score of each element and resetting the element with a lower score to [MASK], enabling DDMs to regenerate improved layouts. (3) We confirm consistent improvements by applying Layout-Corrector to various DDMs. We also analyze the behavior of Layout-Corrector and demonstrate that it enhances fidelity-diversity and speed-quality trade-offs."}, {"title": "Related Works", "content": "Layout Generation. Automatic layout generation [33,40] is a task involving the assignment of positions and categories to multiple elements, which has diverse applications in design areas like application user interfaces and academic papers [8, 14, 19, 45, 46, 48-51]. This task includes unconditional and conditional generation, considering user constraints, e.g., partially specified elements.\nEarly layout generation research explored classical optimization [36,37] and generative models such as GAN [12]-based models [31,49,51] and VAE [10]-based models [23,45]. Following the success in NLP, Transformer-based approaches [44] were proposed. Auto-regressive (AR) models [2, 15] iteratively generate layouts, however, struggle with conditional generation [26]. Non-AR models [5, 26, 43] overcome this difficulty by using a bidirectional architecture, where user-defined conditions serve as clues to complete blank tokens. Recently, diffusion model-based [18,41] layout generation methods in both continuous [4,28] and discrete spaces [20,21,47] have been developed. To enable unconditional and conditional generation within a single framework, it is essential for models to process both discrete and continuous data present in elements. DDMs can accommodate both data types by quantizing geometric attributes into a binned space.\nDiscrete Diffusion Models. D3PM [3] introduces the special token [MASK], where regular tokens are absorbed into [MASK] through a forward process. Based on this, Non-AR models, such as MaskGIT [5], can be understood as a subclass of DDMs. MaskGIT introduces a scheduled masking rate, akin to the diffusion process in D3PM. It also adopts the parallel decoding [11] based on the token confidence, serving as a deterministic denoising process. To address the issue of non-regrettable decoding strategy, DPC and Token-Critic [29, 30] introduce an external module to mitigate discrepancies between training and inference distributions. VQDiffusion [13] facilitates transitions between regular tokens in addition to [MASK], while LayoutDM [21] advances the diffusion process to allow transitions within the same modality. LayoutDiffusion [47] introduces a mild corruption process that considers the continuity of geometric attributes. For the layout generation, we explore the potential of the correction during the generation process to alleviate layout token sticking problem.\nLayout Correction. There are several studies aimed at layout modification. In optimization-based methods [24, 35, 36], layouts are refined to minimize hand-crafted costs, such as alignment score. RUITE [38] and LayoutFormer++ [22] learn to restore the original layout from noisy input. LayoutDM [21] proposes a logit adjustment under the constraints of noisy layouts. While previous research has focused on layout refinement, our method aims to correct layouts during the generation process. Compared to rule-based optimization, our approach achieves superior performance while preserving the distribution of the generated results. Please refer to the supplementary material for details."}, {"title": "Method", "content": "In Sec. 3.1, we first provide a brief overview of DDMs for layout-generation [21] and examine the potential for layout correction in Sec. 3.2. We then present Layout-Corrector in Sec. 3.3 and explain its application across diverse layout-generation tasks in Sec. 3.4."}, {"title": "Layout Generation Models", "content": "A layout is represented as a set of elements, where an element consists of category, position, and size. Following previous studies [2, 15, 26], we use the quantized expression for geometric attributes. Defining $l_i = (c_i, x_i, y_i, w_i, h_i)$, a layout $l$ with $N \\in \\mathbb{N}$ elements is expressed as $l_N = (l_1, \\dots, l_N)$, where $c_i \\in \\{1, \\dots, C\\}$ denotes the category (e.g., text, button), $(x_i, y_i, w_i, h_i) \\in \\{1, \\dots, B\\}^4$ represents the center position and size of $i$-th element, and $B \\in \\mathbb{N}$ denotes the number of bins. Under this representation, we review DDMs to gain insight into the behavior of the generation process, as discussed in Sec. 3.2.\nLet $T$ represent the total number of timesteps in the corruption process. We consider a scalar variable $z_t$ with $K \\in \\mathbb{N}$ classes at $t$, where $z_t \\in \\{1, ..., K\\}$. Here, $z_t$ substitutes an attribute of an element. Following LayoutDM [21], we include the special tokens [PAD] and [MASK], resulting in $(K + 2)$ classes. Here, [PAD] token is employed to fill the empty element, achieving variable length generation. [MASK] token denotes the absorbing state, to which tokens converge through the diffusion process. Using a transition matrix $Q_t \\in [0, 1]^{(K+2)\\times(K+2)}$, we can define a transition probability from $z_{t-1}$ to $z_t$ as follows:\n$q(z_t|z_{t-1}) = v(z_t) Q_t v(z_{t-1}),$ (1)\nwhere $v(z_t) \\in \\{0,1\\}^{K+2}$ is a one-hot vector of $z_t$. Due to the Markov property, a transition from $z_0$ to $z_t$ is similarly written as: $q(z_t|z_0) = v(z_t)Q_t v(z_0)$, where $Q_t = Q_tQ_{t-1} \\dots Q_1$. Applying the Markov property $q(z_{t-1}|z_t, z_0) = q(z_{t-1}|z_t)$, we can obtain the posterior distribution $q(z_{t-1}|z_t, z_0)$ (Eq. (5) in [13]).\nFor the reverse process, we compute a conditional distribution $p_\\theta(Z_{t-1}|Z_t) \\in [0,1]^{N\\times(K+2)}$. Categorical variable $z_{t-1}$ is sampled from this distribution. As proposed in a previous study [3], we use the re-parametrization trick and obtain a posterior distribution as $p_\\theta(\\approx_{t-1}|Z_t) \\propto \\sum_{\\tilde{z}_0} q(Z_{t-1}|Z_t, \\tilde{Z}_0) p_\\theta(\\tilde{z}_0|z_t)$, where $p_\\theta (\\tilde{z}_0 z_t)$ is a neural network that predicts the noiseless token distribution at $t = 0$. Following previous studies [3,13,21], we employ the hybrid loss of variational lower bound and auxiliary denoising loss.\nThe design of transition matrix $Q_t$ is pivotal in defining the corruption process. The token transition is categorized into three types: (1) keeping the current token, (2) replacing the token with other tokens, and (3) replacing the token with [MASK]. For each, we employ the probabilities $(\\alpha_t, \\beta_t, \\gamma_t)$. Hence, using $Q'_t = \\alpha_t I + \\beta_t\\mathbb{1}\\mathbb{1}^\\copyright \\in [0,1]^{(K+1)\\times(K+1)}$, $Q_t \\in [0,1]^{(K+2)\\times(K+2)}$ is defined as:\n$Q_t = \\begin{bmatrix} Q'_t & 0 \\\\ \\gamma_t \\mathbb{1} & 1 \\end{bmatrix}$ (2)\nThe cumulative transition matrix $Q_t$ can be computed in the closed form as $Q_t v(x_0) = \\bar{\\alpha}_t v(x_0) + (\\hat{\\beta}_t - \\beta_t)v(K + 2) + \\beta_t, \nwhere \\bar{\\alpha}_t = \\prod_{i=1}^{t} \\alpha_i, \\hat{\\gamma}_t = 1 - \\prod_{i=1}^{t}(1 - \\gamma_i), \\hat{\\beta}_{t,K} = (K + 1)\\beta_t = 1 - \\bar{\\alpha}_t - \\hat{\\gamma}_t, and v(K + 2) denotes the one-hot representation of [MASK]. A transition with $\\beta_t > 0$ introduces a layout inconsistency. Since the corresponding DDM is trained to correct such mismatches, we expect that it can update erroneous tokens caused in the generation process. However, in the following section, we will demonstrate that the scheduling of $\\beta_t$ is suboptimal for layout correction."}, {"title": "Preliminary: Potential of Token Correction in DDM", "content": "We explore token correction with DDMs, specifically focusing on LayoutDM [21], by assessing the impact of the $\\beta_t$ schedule. For $\\epsilon \\ll 1$ and $\\beta_{t,K} = \\epsilon$ for any $t$, $p_\\theta(Z_{t-1}|z_t)$ struggles to correct tokens, due to the diffusion process not facilitating token replacement, except for [MASK]. This limitation is analogous to those seen with parallel decoding methods used in MaskGIT [5]. A possible solution is to increase $\\beta_{t,K} > \\epsilon$ to promote transition between regular tokens. To verify the effect of $\\beta_{t,k}$, we compare the token-sticking-rate (TSR) in the reverse process, which measures the proportion of tokens at $z_0$ that remain unchanged from $z_t$. As depicted in Fig. 2a, $\\beta_{t,K} = \\epsilon$ leads to TSR ~ 100% at most t, indicating token sticking. In contrast, when $\\beta_{t.K} > \\epsilon$, the TSR is reduced below 100%, indicating that $p_\\theta(Z_{t-1}|z_t)$ can update tokens during generation process.\nWe next evaluate the DDM's error-correction capability by simulating replacements of three randomly selected tokens in a sequence with either [MASK] or other tokens, and then observing the model's ability to restore the original tokens. These methods are referred to as Mask-replace and Token-replace, respectively. In this setup, LayoutDM executes the reverse step from timestep t = 10 to 1. Our metric is the success rate of token recovery, which is deemed successful if recovery is complete, and we assess this across different B, schedules. Fig. 2b demonstrates that Token-replace with $\\beta_{t,K} > \\epsilon$ is moderately more effective than when $\\beta_{t,K} = \\epsilon$. However, Mask-replace exhibits significant improvements over Token-replace. This finding motivated us to develop Layout-Corrector, which resets inharmonious tokens to [MASK]."}, {"title": "Layout-Corrector", "content": "To enhance the replacement of the erroneous tokens with [MASK], we introduce Layout-Corrector. Functioning as a quality assessor, Layout-Corrector evaluates the correctness of each token in a layout during the generation process. The tokens with lower correctness scores are replaced with [MASK], and the updated tokens are fed back into the DDM. Therefore, Layout-Corrector can explicitly prompt the DDM to modify the erroneous tokens.\nArchitecture. Evaluating the correctness of each token requires Layout-Corrector to consider the relationships between elements in a layout. To this end, we use a Transformer [44] encoder to capture global contexts, as shown in Fig. 3. We first apply a multi-layer perceptron (MLP) to fuse five tokens $(c, x, y, w, h)$ of each element, obtaining $N$ element embeddings. These embeddings are processed by a transformer encoder, producing five-channel outputs. Each channel corresponds to the correctness score of each token in the element. Since the layout elements are order-agnostic, we eliminate positional encoding to avoid unintended biases.\nTraining. The objective of Layout-Corrector is to detect erroneous tokens during the generation process. To achieve this, we train Layout-Corrector as a binary classifier with a pre-trained DDM, which is frozen during training, as shown in Fig. 3. Given an original layout $z_0$ and $t$, a forward process is applied to obtain a distribution $q(z_t|z_0)$, and DDM estimates the distribution $p_\\theta(Z_{t-1}|z_t)$. Then, for a [MASK]-free token sequence $z_{t-1}$ sampled from $p_\\theta(Z_{t-1}|z_t)$, Layout-Corrector evaluates the correctness score $p_\\phi(z_{t-1},t) \\in [0,1]^{5N}$ for each token in $z_{t-1}$. Unlike existing assessors [29,30], which are trained to detect tokens in $z_{t-1}$ that are originally masked in $z_t$, we train Layout-Corrector to predict whether each token in $z_{t-1}$ aligns with the corresponding original token in $z_0$, as in [7]. It encourages the Layout-Corrector to evaluate the correctness of each token directly. Specifically, we use binary cross-entropy (BCE) loss:\n$\\mathcal{L}_{Corrector} = BCE(m, p_\\phi(z_{t-1},t)),$ (3)\nwhere $m^{(i)} = 1$ if $z^{(i)}_{t-1} = z^{(i)}_0$, otherwise $m^{(i)} = 0$. Through the training, Layout-Corrector learns to identify erroneous tokens that disturb the layout harmony."}, {"title": "Generating Layout with the Layout-Corrector", "content": "Unconditional Generation. As shown in Fig. 3, all tokens $z_N$ are initialized with [MASK], and the final output is obtained at $t = 0$. At timestep $t$, a DDM predicts the distribution $p_\\theta(z_{t-1}|z_t)$, from which we sample a [MASK]-free token sequence $z_{t-1}$. Layout-Corrector then assesses the correctness scores $p_\\phi(\\tilde{z}_{t-1},t)$. We add Gumbel noise to $p_\\theta(Z_{t-1},t)$ to introduce randomness into the token selection. Then, we mask tokens whose scores are lower than a threshold $\\Theta_{th}$. Another possible way is to choose tokens with the lowest $\\delta \\cdot N$ scores, similar to [29,30], where $\\delta_t$ is the mask ratio at $t$ (see Sec. 3.1). However, it may mask high-quality tokens when the majority have higher scores, leading to diminished cues for the DDM. The threshold mitigates this issue by selectively masking only those tokens with lower scores, thus preserving reliable cues for regeneration.\nConditional Generation. Layout-Corrector is versatile and can be seamlessly used for various conditional generation tasks without specialized training or fine-tuning. Given a set of partially known tokens, e.g., element categories or sizes, the goal of conditional generation is to estimate the remaining unknown tokens. Following LayoutDM [21], we utilize the known condition tokens as an initial state for the generation process and maintain these tokens at each $t$. When Layout-Corrector is applied, a correctness score of 1 is assigned to the conditional tokens. In this way, Layout-Corrector encourages the DDM to modify erroneous tokens while ensuring that the known tokens are preserved.\nCorrector Scheduling. Layout-Corrector can be applied at any $t$ during the generation process. Unlike existing methods [29,30], which apply the external assessor at every $t$, we selectively apply Layout-Corrector at specific timesteps. Remarkably, alongside LayoutDM [21], Layout-Corrector enhances generation quality with just three applications, effectively reducing additional forward operations during inference. Moreover, by adjusting the schedule of Layout-Corrector, we can modulate the fidelity-diversity trade-off of the generated layouts. Specifically, more frequent corrector applications enhance fidelity by removing a larger number of inharmonious tokens, while a more sparse schedule improves diversity. The experimental section provides a more detailed analysis on the schedules."}, {"title": "Experiments", "content": "Datasets. We evaluated Layout-Corrector on the following three challenging layout datasets over different domains: Rico [8] contains user interface designs for mobile applications. It contains 25 element categories such as text, button, and icon. Crello [45] consists of design templates for various formats, such as social media posts and banner ads. PubLayNet [50] comprises academic papers with 5 categories, such as table, image, and text. We follow the dataset splits presented in a previous study [21] for Rico and PubLayNet and use the official splits for Crello. We excluded layouts with more than 25 elements as in [21].\nEvaluation Metrics. We used the following evaluation metrics: Fr\u00e9chet In-ception Distance (FID) [17] evaluates the similarity between distributions of generated and real data in the feature space using the feature extractor [24]. Alignment (Align.) [32] measures the alignment of elements in generated layouts. This metric is normalized by the number of elements, as in [24]. Maximum IoU (Max-IoU) [24] evaluates the similarity of the elements in bounding boxes of the same category, comparing the generated layouts to the ground truth. For fidelity and diversity, we used Precision and Recall [27].\nLayout-generation Tasks. We evaluated our Layout-Corrector across three tasks: Unconditional generates a layout without any constraints. Category \u2192 size + position (C \u2192 S + P) generates a layout given only the category of each element. Category + size \u2192 position (C + S \u2192 P) generates a layout given the category and size of each element.\nImplementation Details. We used DDMs, i.e., LayoutDM [21] and VQDiffu-sion [13], as well as a non-AR model i.e., MaskGIT [5], as baseline models, and applied Layout-Corrector to them. Since non-ARs can be understood as a sub-class of DDMs as discussed in Sec. 2, we can seamlessly apply Layout-Corrector to them. We used the publicly available pre-trained LayoutDM on Rico and PubLayNet, while we trained other models using LayoutDM implementation. Unless otherwise specified, the total timesteps $T$ for LayoutDM and VQDif-fusion were set to 100, and Layout-Corrector was applied at $t = \\{10,20, 30\\}$, leading to a total of 103 forward operations. In MaskGIT [5], $T = 10$ and Layout-Corrector was applied at every $t$. For the threshold $\\Theta_{th}$, we set it to 0.7 for LayoutDM and VQDiffusion, and 0.3 for MaskGIT. To train Layout-Corrector, we used AdamW [25,34] with an initial learning rate of $5.0 \\times 10^{-4}$ and $(\\beta_1, \\beta_2) = (0.9, 0.98)$. Refer to supplementary material for more details."}, {"title": "Effectiveness of Layout-Corrector", "content": "To evaluate the applicability and effectiveness of Layout-Corrector with various discrete generative models, we applied our corrector and Token-Critic [29] to MaskGIT [5], VQDiffusion [13], and LayoutDM [21]. The results in Tab. 1 show that Layout-Corrector consistently improved FID across all tested models, confirming its effectiveness. In contrast, while Token-Critic [29] enhanced FID when applied to MaskGIT, its application to VQDiffusion and LayoutDM resulted in diminished performance. These results demonstrate that the direct application of Token-Critic can lead to suboptimal performance in layout generation, highlighting the importance of tailored approaches. Regarding fidelity and diversity, evaluated using Precision and Recall [27], we observed different trends between a non-AR model and DDMs. For MaskGIT, Layout-Corrector boosted both diversity and fidelity. The parallel decoding in MaskGIT keeps high-confidence tokens and rejects low-confidence ones. It often leads to stereotypical token patterns, resulting in high fidelity but low diversity. Layout-Corrector resets such patterns while considering the overall harmony, thereby improving diversity without sacrificing fidelity. In the case of VQDiffusion and LayoutDM, Layout-Corrector increased fidelity while maintaining diversity. While the stochastic nature of DDMs promotes diversity, it can produce low-quality tokens. Layout-Corrector mitigates this issue by resetting these tokens, thereby enhancing fidelity."}, {"title": "Analysis", "content": "Intrinsic Evaluation of Corrector. We assess Layout-Corrector's ability to detect erroneous tokens. To this end, we randomly replace three tokens in the ground truth with alternate ones using the test set of Rico dataset. The goal is for the corrector to identify these altered tokens. For Layout-Corrector and Token-Critic trained with the same LayoutDM, we evaluate the detection accuracy of these altered tokens, selecting the three tokens with the lowest corrector scores for comparison. Fig. 4 shows that Layout-Corrector outperforms Token-Critic due to the objective that directly estimates the correctness of tokens, underscoring its effectiveness in layout assessment.\nFurthermore, we analyze the correlation between the degree of layout corruption and the correctness scores. To modulate the extent of disruption, we limit the maximum transition step for the geometric attributes when replacing. Fig. 5 depicts the average correctness scores for the three replaced and the other clean tokens within the corrupted layouts. We observe that a greater deviation from the original token leads to a lower correctness score against clean tokens. These results suggest that the corrector can measure the degree of discrepancy between the ideal and the actual layouts.\nCorrector Scheduling: Impact on Fidelity and Diversity. We applied Layout-Corrector to LayoutDM with various schedules. Specifically, it is ap-plied at $t = \\{\\{10\\}, \\{10, 20\\}, . . ., \\{10, 20, . . ., 90\\}\\}$, yielding nine distinct schedules. Fig. 6 shows the FID-Precision trade-offs with and without Layout-Corrector. It can be observed that varying the schedule effectively adjusts the FID-Precision trade-off. More frequent corrector applications enhance fidelity (i.e., Precision), while decreasing diversity (i. e., FID). These results align with Layout-Corrector's role of resetting poor-quality tokens. Its frequent use leads to high-fidelity outputs; however, this also reduces the stochastic nature of DDMs, thus diminishing diversity. Moreover, it shows that our default schedule $t = \\{10,20,30\\}$ yields preferable FID, confirming the effectiveness of our schedule.\nTo further explore the impact of the corrector schedule, we analyzed the distribution of tokens' width attribute on Rico dataset. Fig. 7 compares the histograms of real and generated tokens under various corrector schedules. It reveals that more frequent correction amplifies the frequency trends of the original distribution. Concretely, values that are already common in the real data (e.g., $w = 1.0$ in Fig. 7) become more prevalent, while the occurrence of rarer values further diminishes. This observation aligns with the trend in Fig. 6, where frequent correction leads to higher fidelity but at the cost of reduced diversity.\nSpeed-Quality Trade-off. Since generation speed is crucial in practical ap-plications, we examined the speed-quality trade-off. To adjust the runtime of LayoutDM, we used the fast-sampling technique [3], which uses modulated dis-tribution $p_e(Z_{t-1}|z_t)$ instead of $p_\\theta(Z_{t-1} Z_t)$. $A$ is a step size, and the total steps are reduced to $T' = T/\\Delta$. Regardless of $\\Delta$, Layout-Corrector is applied at $t = \\{10,20,30\\}$. Fig. 8 presents the results on $T' = \\{20,30, 50, 75, 100\\}$. Compared with LayoutDM alone, Layout-Corrector improves the FID score with only a minimal increase in runtime, offering a superior trade-off. While the original LayoutDM's performance significantly degrades with a smaller $T'$, Layout-Corrector effectively mitigates this issue by rectifying the misgenerated tokens, demonstrating the robustness in smaller $T'$. Notably, Layout-Corrector attains a competitive FID to that of the original LayoutDM ($T' = 100$) with just $T' = 20$.\nTo further analyze the benefits of Layout-Corrector for smaller $T'$, we ex-amined the correctness scores $p_\\phi(\\hat{c}_{t-1},t)$ across different $T'$. Fig. 9 presents the average scores for intermediate tokens $z_t$ at each $t$ on $T' = \\{20,50,100\\}$. We include the scores of real layouts for reference. Note that $p_\\phi(z_{t-1},t)$ is affected by the input $t$ because of the training procedure. For example, at smaller $t$, the corrupted tokens $z_t$ become closer to $z_0$; therefore, most tokens in $z_{t-1}$ align"}, {"title": "Conclusion", "content": "We introduced Layout-Corrector, a novel module working with a DDM-based layout-generation model. Our preliminary experiments highlighted (1) the token-sticking problem with DDMs and (2) the importance of masking mis-generated tokens to correct them. Based on these insights, we design Layout-Corrector to assess the correctness score of each token and replace the tokens with low scores with [MASK], guiding the generative model to correct these tokens. Our experiments showed that Layout-Corrector enhances the generation quality of various generative models on various tasks. Additionally, we have shown that it effectively controls the fidelity-diversity trade-off through its application schedule and mitigates the performance decline associated with fast sampling.\nLimitations and Future Work. While Layout-Corrector adds marginal run-time, it increases memory usage and total parameter count. Our future work will aim to incorporate layout-specific mechanisms, e.g., element-relation em-beddings [20], to improve Layout-Corrector's layout understanding capabilities."}, {"title": "Details of Token Refinement Task", "content": "In Sec. 3.2, we conducted preliminary experiments to evaluate the token refinement capability of DDMs. We present the experimental setup and results in more detail."}, {"title": "Transition Probability Design", "content": "We use LayoutDM [21] as the representative of DDMs. The default setting of $\\beta_{t,k} = (K+1)\\beta_t$ in LayoutDM is not exactly zero but is sufficiently close. As a baseline, we set $\\beta_{t,K} = \\epsilon$ for any timestep $t$, where $\\epsilon$ equals to $10^{-6}$. In this setting, the diffusion process primarily induces transitions from regular tokens to [MASK], and transitions between regular tokens rarely occur. Therefore, we cannot expect corrections of errors in regular tokens during the generation process. Setting a large value for $\\beta_{t,k}$ is expected to facilitate transitions between regular tokens during the diffusion process, allowing the corresponding denoising model $p_\\theta(Z_{t-1}|z_t)$ to acquire the capability to correct regular tokens. To verify the effect of $\\beta_{t,k}$ schedule, we consider schedules for $\\beta_{t,k}$ based on two guidelines. The first involves assigning high $\\beta_{t,K}$ values later in the diffusion process, while the second involves high $\\beta_{t,K}$ values earlier. A detailed schedule, including $\\bar{\\alpha}_t$ and $\\hat{\\gamma}_t$, is shown in Fig. 12. Here, we adopt a linear scheduling for timesteps."}, {"title": "Impact of Transition Schedules on FID", "content": "In Tab. 5, we report the results of FID for each schedule depicted in Fig. 12. When $\\beta_{t,K}$ increases from $\\epsilon$ to 0.05 or 0.1 with the timestep $t$, the performance is comparable to the baseline for the unconditional generation task; however, we observe degradation in the conditional generation tasks. Conversely, when $\\beta_{t,K}$ decreases from 0.05 or 0.1 to $\\epsilon$, the performance is inferior to the baseline for both unconditional and conditional tasks.\nRegarding the degradation in conditional generation tasks, we hypothesize that it stems from the condition gap between training and inference time. Training is conducted in an unconditional manner, where, especially for $\\beta_{t,k} > \\epsilon$, the model learns to restore the original layout while correcting substitutions of regular tokens. On the other hand, in conditional settings, the model is expected to preserve the conditioned regular tokens, leading to the discrepancy between the training and inference phases. When $\\beta_{t, K} = \\epsilon$, substitutions between regular tokens rarely occur, which means that conditioning on regular tokens does not negatively impact the generation process.\nAdditionally, applying high $\\beta_{t,K}$ values in the earlier timesteps leads to poor FID in the unconditional setting. This schedule causes rapid replacements of regular tokens, indicated by $\\bar{\\alpha}_t < 1$, as observed in Fig. 12d and Fig. 12e. The results imply that it is necessary to design a schedule for $\\bar{\\alpha}_t$ that starts at 1.0 when $t = 0$ and gradually decreases as $t$ increases, reflecting the fundamental concept of the discrete diffusion process."}, {"title": "More Detailed Experimental Setup", "content": "In this section, we describe the experimental setup in detail in addition to the description in Sec. 4.1."}, {"title": "Datasets", "content": "We provide a more detailed explanation of the benchmark datasets used for evaluation", "8": "We follow the dataset split in [21", "50": "We use the dataset split in [21"}]}