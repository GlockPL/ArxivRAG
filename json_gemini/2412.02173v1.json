{"title": "Keeping Experts in the Loop: Expert-Guided Optimization for Clinical Data Classification using Large Language Models", "authors": ["Nader Karayanni", "Aya Awwad", "Chieh-Lien Hsiao", "Surish P Shanmugam"], "abstract": "Since the emergence of Large Language Models (LLMs), the challenge of effectively leveraging their potential in healthcare has taken center stage. A critical barrier to using LLMS for extracting insights from unstructured clinical notes lies in the prompt engineering process. Despite its pivotal role in determining task performance, a clear framework for prompt optimization remains absent. Current methods to address this gap take either a manual prompt refinement approach, where domain experts collaborate with prompt engineers to create an optimal prompt, which is time-intensive and difficult to scale, or through employing automatic prompt optimizing approaches, where the value of the input of domain experts is not fully realized. To address this, we propose StructEase, a novel framework that bridges the gap between automation and the input of human expertise in prompt engineering. A core innovation of the framework is SamplEase, an iterative sampling algorithm that identifies high-value cases where expert feedback drives significant performance improvements. This approach minimizes expert intervention, to effectively enhance classification outcomes.\nThis targeted approach reduces labeling redundancy, mitigates human error, and enhances classification outcomes. We evaluated the performance of StructEase using a dataset of de-identified clinical narratives from the US National Electronic Injury Surveillance System (NEISS), demonstrating significant gains in classification performance compared to current methods. Our findings underscore the value of expert integration in LLM workflows, achieving notable improvements in F1 score while maintaining minimal expert effort. By combining transparency, flexibility, and scalability, StructEase sets the foundation for a framework to integrate expert input into LLM workflows in healthcare and beyond.", "sections": [{"title": "Introduction", "content": "With the advent of large language models (LLMs), the landscape of healthcare delivery is evolving as these technologies find novel applications across the field (Systems 2024; Landi 2024). Prompt engineering has emerged as an important element to harness the full potential of LLMs, where minor changes in the natural language inputs can impact the output's quality and relevance (Mesk\u00f3 2023). Among the various applications, the classification of unstructured clinical narrative notes from electronic medical records (EMRS) is particularly relevant for researchers, healthcare administrators, and policymakers (Wang et al. 2018). Medical records are rich with clinical data that are not always fully captured by structured fields like International Classification of Diseases (ICD) codes, making the ability to navigate and classify unstructured free text essential.\nThe challenge lies in extracting this information effectively. Previous studies have shown that generating task-specific prompts typically requires the combined efforts of prompt engineers, who have a deep understanding of LLMs and best prompting practices, and domain experts, who are well-versed in the task-specific intricacies (Zhang et al. 2024; Huang et al. 2024; Guevara et al. 2024; Burford et al. 2024; Jethani et al. 2023). This ensures creating an ideal prompt where elements such as task description, domain-specific details, and benchmarks are incorporated to drive the prompt quality and performance. However, refining prompts often involves an iterative, manual exploration process that is tedious, costly and requires technical expertise (Pryzant et al. 2023). Moreover, the absence of a systematic framework to guide this process increases inefficiencies and limits the wider adoption of these methods (Ding et al. 2021).\nTo address these challenges, new methods are being developed to improve and automate prompt engineering. Beyond single-prompt techniques such as zero-shot and few-shot learning, meta-prompting approaches are gaining traction (Ruksha 2024; Cleary 2024). In essence, those approaches generate, refine, and optimize LLM prompts for a given task. Techniques such as evolutionary algorithm (e.g., Monte Carlo search, Gibbs sampling) and textual gradient-based optimization are some of the underlying methods to achieve this automation (Zhang et al. 2024; Yuksekgonul et al. 2024). While effective, these approaches limit user involvement in prompt optimization.\nOf interest is the work introduced by PromptAgent (Wang et al. 2023), which addresses the challenge of generating expert-level prompts. PromptAgent employs Monte Carlo Tree Search (MCTS) to navigate the prompt space strategically, mimicking human experts to achieve nuanced and high-quality prompts. While PromptAgent effectively highlights the limitations of prior methods and advances automated prompt engineering, it relies on an exhaustive exploration of the expert prompt space. It introduces practi-"}, {"title": "Methods", "content": "cal constraints, where the process can take hours to iterate through the search space\u2014inherently lacking domain-specific knowledge and feedback during runtime. Similarly, DSPy (Khattab et al. 2022, 2024), provides a programmatic framework for managing LLM interactions and refining prompts, abstracting the challenges of prompt engineering from programmers, and automatically generating the task-specific prompts with the best practices of prompt engineering.\nThe strive to automate the prompt engineering process is understandable, given the tedious nature of the task. However, the cost of excluding the expert from the loop can be significant. Recent work in material science highlights the critical role of human expertise in understanding AI-generated results, emphasizing that while AI systems excel at computation and automation, human expertise provides the vision and contextual understanding necessary to make the results meaningful (Toner-Rodgers 2024).\nBuilding on this understanding, we introduce StructEase, an innovative framework that bridges the gap between full automation and expert involvement in prompt engineering. Designed to classify large sets of unstructured clinical notes using LLMs, StructEase keeps domain experts at the center of the workflow, ensuring iterative refinement of classification prompts through expert-guided corrections. Unlike fully automated approaches, StructEase strategically integrates expert feedback into the prompt generation process, while addressing key challenges such as class imbalances and redundant sampling. Our novel approach ensures that expert feedback effectively distills domain knowledge into the system while requiring minimal effort from the expert. Our research shows that leveraging an expert in the loop outperforms the fully automated approaches and provides a novel approach to do so effectively."}, {"title": "Proposed Framework", "content": "We propose StructEase, a novel framework that integrates domain experts into the AI workflow to enable the classification of large sets of unstructured clinical notes using LLMs. The framework is designed to efficiently distill domain expertise into classification prompts, which are iteratively refined to improve the performance and relevance of LLM outputs. An overview of the framework is illustrated in Figure 1.\nClassification Prompt Generation At the core of StructEase is a classification prompt's generation and iterative refinement. This prompt is generated and refined to fulfill the clinical note classification request. The prompts are designed using Chain-of-Thought (CoT) reasoning to enhance the effectiveness of classification tasks. StructEase starts by creating an initial classification prompt Po and continuously improves it after each labeling iteration. The prompt is improved by refining the classification instructions and incorporating Few-Shot (FS) examples from expert-labeled data. We annotate the prompt generation methods as follows:\nPo = GenerateInitialPrompt(Classes, Request, Dataset)\nPi = UpdatePrompt(Pi\u22121, Few_Shots)\nFew_Shots are the notes that the prompt Pi-1 classified incorrectly compared to the expert labels. The above methods use an LLM call to generate the new prompt.\nChoosing Data for Expert Review A critical challenge in designing StructEase is selecting data samples that maximize the value of expert-labeled data. This is particularly difficult in scenarios with class imbalances, where it is essential to ensure that the reviewed data samples adequately represent all classes. Another challenge arises from the possibility of a large proportion of relatively simple samples in the dataset, which may not contribute meaningfully to improving the model's classification performance. Furthermore, it is crucial to avoid redundant expert reviews across labeling iterations to ensure that each iteration yields unique and valuable insights that enhance the classification capabilities of the LLM. To address these challenges, we propose SamplEase, A novel algorithm, detailed in Algorithm 1.\nCOMPUTECONFIDENCE calculates the confidence of the LLM for classifying each clinical note separately as follows:\nConfidence = exp(1-\\frac{1}{n}\\sum_{i=1}^{n}log P_i)\nWhere:\n\u2022n is the number of tokens in the completion.\n\u2022 log Pi is the log probability of the i-th token.\n\u2022 exp(x) is the exponential function, ex.\nThrough this iterative process, StructEase continuously incorporates expert insights to improve the classification"}, {"title": "Framework Implementation", "content": "prompt. At any stage, the expert can deploy the most recent prompt, applied across the entire dataset to classify the clinical notes. By abstracting the technical complexities, StructEase is designed to enhance classification accuracy and to ensure advanced AI capabilities are accessible to domain specialists with minimal technical training.\nWe developed StructEase in Python and it is available as an open-source project on GitHub\u00b9. The implementation includes a web-based application that provides a user-friendly interface, enabling users to upload datasets, define classification requests, and engage in the iterative labeling process. To meet real-world needs, the implementation incorporates two key steps. First, the classification process using P\u00bf is parallelized across clinical notes to minimize latency during labeling iterations. This parallelization significantly reduces the time required for both iterative refinements and the final dataset classification. Furthermore, the end-to-end framework is containerized using Docker (Merkel 2014), to encourage adoption and future research and collaboration."}, {"title": "Data and Task Definition", "content": "The data for this study was sourced from the 2023 US Consumer Product Safety Commission's National Electronic Injury Surveillance System (NEISS). This system operates on a nationally representative, stratified probability sample from 96 hospitals across the US and its territories, each equipped with at least six beds and an emergency department (ER). NEISS provides publicly accessible, de-identified data that do not require institutional review board approval or informed consent for use, in compliance with the HIPAA Privacy Rule. We filtered for micromobility-related visits based on NEISS product codes. The final dataset included 17,888 different clinical narratives (U.S. Consumer Product Safety Commission 2024; Burford et al. 2024).\nIn Our study, we use the \"gpt-4o-mini-2024-07-18\" model accessible via the OpenAI API (OpenAI 2024). To ensure reproducibility and consistency, we configured the language model with a temperature setting of 0 and a top-p value of 1. This setup ensures that the model consistently selects the most probable token during text generation. As a use case, the specific task was to extract helmet usage status from unstructured clinical narratives of ER visits. The goal was to classify each note into one of three classes: helmet, no helmet, and cannot determine helmet status. To evaluate our framework, 2,000 clinical narratives were randomly selected and independently reviewed by two expert physicians. Any disagreements between the reviewers were resolved through discussion until a 100% inter-rater agreement was reached. It is important to note that the manually labeled dataset created for this study is proprietary and has not been published online, addressing any potential concerns regarding data leakage to the LLM before this study."}, {"title": "Evaluation Experiments", "content": "We conducted a series of experiments to evaluate our framework under various conditions:\n1. Iterative Prompt Refinement: We evaluated the performance of StructEase by running the framework six times independently, generating the prompts P0, P1, and P2 in each run. Starting with the baseline prompt Po, the framework iteratively refined the prompts through expert feedback to produce P\u2081 and P2. The experiment aimed to assess how iterative prompt refinement improves classification performance across runs.\n2. Comparing SamplEase vs. Random Sampling: To evaluate the sampling approach used in StructEase, we compared SamplEase with a baseline random sampling method. Six independent runs were conducted for each sampling strategy using a single prompt iteration (P1). This experiment aimed to explicitly assess the novel proposed sampling algorithm in improving the performance of the framework compared to random sampling.\n3. Comparison with Baselines: We compared the performance of StructEase against four baseline methods designed for similar classification tasks: (1) Human prompt: human-provided instructions for direct classification task. (2) DSPy Chain of Thought (CoT) classifier: A DSPy classifier that incorporates step-by-step reasoning to improve interpretability and accuracy. (3) DSPy optimized: A version of the DSPy CoT classifier that requires labeled data. (Khattab et al. 2022, 2024). This method utilizes the provided labeled data to create a CoT classification prompt and then fine-tunes the LLM to minimize the loss for the classification task. Although this approach involves fine-tuning\u2014unlike the other methods\u2014we included it to assess how effective"}, {"title": "Results", "content": "our approach can be when focusing solely on prompt optimization. We ran this version with 30 randomly labeled samples to benchmark its performance.\n4. Bias evaluation: To examine potential biases in model performance, we evaluated the framework across key demographic groups. Specifically, we compared the model's performance across genders (males vs. females) and across self-reported racial categories: White, Black-/African American, Asian, Other, or Not Specified.\nUnless otherwise mentioned, all our experiments are conducted with the SamplEase default parameter, specified in Algorithm 1.\nPerformance Metrics and Statistical Analysis\nOverall and Per-Class Evaluation The framework was evaluated using 2,000 expert-labeled notes (ground truth), excluding the training data used to refine the prompt. Performance was assessed using macro-level classification metrics (macro-precision, macro-recall, and macro-F1 score) and per-class metrics (precision, recall, and F1 score). These metrics were defined as follows:\n\u2022 Precision = TP / (TP + FP)\n\u2022 Recall = TP / (TP + FN)\n\u2022 F1 = (2 Precision Recall) / (Precision + Recall)\n(Abbreviations: TP = true positives, FP = false positives, FN = false negatives)\nThe framework was evaluated across six independent runs to account for variability in the model outputs. Macro-level metrics were computed by averaging the class-specific metrics, while per-class metrics were calculated separately for each class.\nTo quantify uncertainty, 95% confidence intervals (CI) were derived using bootstrap resampling (n=1000). For macro-level metrics, resampling was performed within each run, and the resulting bootstrap distributions were aggregated across the six runs to compute final CIs.\nFor per-class metrics, stratified bootstrap resampling was applied to preserve class proportions within each resampled dataset. Metrics for each class were recalculated for every bootstrap iteration, and results were aggregated across runs. Final point estimates and 95% confidence intervals were reported for both overall and per-class metrics.\nStatistical Analysis To compare Macro F1-Score between prompt iterations (Po vs. P2), we calculated the mean difference and its 95% confidence interval using the bootstrap distributions. Statistical significance was determined by whether the confidence interval excluded zero. A permutation test further validated the difference by generating a null distribution of shuffled samples. For the sampling experiments, the medians of the performance metrics across the runs were calculated. Differences in these medians between the sampling methods were compared using the Mann-Whitney U test, a non-parametric test for comparing medians between two independent groups. A two-sided significance threshold of P < 0.05 was used. All statistical analyses and visualization were conducted using R software (version 4.4.1)."}, {"title": "Iterative Prompt Refinement", "content": "Figure 2 presents the aggregated results of the iterative prompt refinement process. The iterative refinement of prompts (Po\u2192 P\u2081 \u2192 P2) resulted in consistent improvements in overall (Figure 2a) and per-class performance metrics (Figure 2b). F1 score increased from 0.935 (95% CI:"}, {"title": "Comparing SamplEase vs. Random Sampling", "content": "0.919-0.951) with the baseline prompt (Po) to 0.973 (95% CI: 0.951-0.992) after the first refinement (P\u2081) and reached 0.986 (95% CI: 0.972\u20130.997) after the second refinement (P2). The increase in F1 score and precision between (Po \u2192 P2) was statistically significant with P < 0.001. Similar trends were observed for precision (0.901 (95% CI: 0.879-0.923) at Po to 0.981 (95% CI:0.963-0.996) at P2) and recall (0.976 (95% CI: 0.967-0.984) at Po to 0.990 (95% CI: 0.979-0.999) at P2).\nPer-class analysis of F1 scores (Figure 2b) shows significant improvements across all categories, particularly for the \"No Helmet\" class, where initial performance (Po) was the lowest at 0.88 (95% CI: 0.85, 0.91), but increased markedly to 0.98 (95% CI: 0.95-1) at P2), this \u25b310% improvement was statistically significant (P < 0.001).\nFigure 3 presents the performance comparison between smart and random sampling strategies across six independent runs. Smart sampling demonstrated consistent improvements in performance metrics compared to random sampling. For the F1 score, smart sampling achieved a higher median value (0.974) compared to random sampling (0.959), with a statistically significant difference (P = 0.044). Similarly, recall showed a significant improvement, with smart sampling yielding a median value of 0.988 compared to 0.984 for random sampling (P = 0.023). The improvement in precision (0.962 for smart sampling vs. 0.939 for random sampling) did not reach statistical significance (P = 0.077)."}, {"title": "Comparison with Baselines", "content": "The results in Table 1 demonstrate that StructEase mostly outperformed other baseline methods across the evaluated metrics. The second prompt iteration P2 achieved the highest performance, with a F1-Score of 0.986, precision of 0.981, recall of 0.990, and accuracy of 0.995.\nAmong the baselines, DSPy Optimized showed strong performance, with F1-Score of 0.980 and an accuracy of 0.991. However, StructEase surpassed these results, particularly in recall and F1-Score, highlighting the advantage of iterative prompt refinement. By contrast, the human prompt achieved the lowest overall performance, with a F1-Score of 0.735 and accuracy of 0.730."}, {"title": "Bias Evaluation", "content": "The bias evaluation experiment demonstrated that the framework performed consistently across different demographic groups, with minimal expected variation in classification metrics between genders and among self-reported racial categories. When comparing performance across genders (male vs. female), the results indicated no substantial disparities in macro-level metrics or per-class evaluations. Similarly, when examining performance across racial categories (White, Black/African American, Asian, Other, Not Specified), the framework maintained equitable classification outcomes across all groups."}, {"title": "Conclusion and Future Work", "content": "Effective prompt engineering for leveraging LLMs in healthcare-related tasks is pivotal, yet current research falls short in bridging automation with domain expertise. This gap arises from the absence of established frameworks and the challenge of integrating human expertise strategically rather than mimicking it entirely.\nTo address this gap, we present StructEase, a novel framework that empowers domain experts to perform a classification of clinical notes without needing prior expertise in LLMs. By leveraging an iterative process of minimal yet impactful expert input, StructEase demonstrates how strategic integration of domain expertise can significantly enhance system performance.\nStructEase achieves substantial improvements in using LLMs for both overall and per-class classification performance. A critical challenge our framework addresses is identifying data samples where expert input yields the greatest performance gains. To this end, we introduce SamplEase, a novel algorithm for iterative sampling that consistently outperforms random sampling. Our results demonstrate significant gains in F1 score and recall respectively, underscoring SamplEase's efficiency in leveraging expert input.\nBy focusing the expert attention to a select sample of the data, StructEase reduces labeling errors stemming from fatigue or ambiguity in simpler, repetitive tasks. Even with well-defined rules and training, labeling variability is common. Our framework mitigates these challenges by selecting high-value data for expert review, ensuring consistent improvement with each iteration (Wong, Paritosh, and Bollacker 2022).\nIn our work, we optimized the classification performance solely by refining the prompt Pi. Other methods to further improve classification quality exist, such as dynamically fine-tuning the LLM-as demonstrated by the DSPy Optimized method in our baseline comparisons. Exploring such additional techniques to enhance StructEase remains an exciting avenue for future research. Our evaluation shows that integrating expert input through SamplEase outperforms the"}]}