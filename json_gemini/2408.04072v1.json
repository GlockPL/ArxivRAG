{"title": "AEye: A Visualization Tool for Image Datasets", "authors": ["Florian Gr\u00f6tschla", "Luca A. Lanzend\u00f6rfer", "Marco Calzavara", "Roger Wattenhofer"], "abstract": "Image datasets serve as the foundation for machine learning models in computer vision, significantly influencing model capabilities, performance, and biases alongside architectural considerations. Therefore, understanding the composition and distribution of these datasets has become increasingly crucial. To address the need for intuitive exploration of these datasets, we propose AEye, an extensible and scalable visualization tool tailored to image datasets. AEye utilizes a contrastively trained model to embed images into semantically meaningful high-dimensional representations, facilitating data clustering and organization. To visualize the high-dimensional representations, we project them onto a two-dimensional plane and arrange images in layers so users can seamlessly navigate and explore them interactively. AEye facilitates semantic search functionalities for both text and image queries, enabling users to search for content. We open-source the codebase for AEye, and provide a simple configuration to add datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "In today's data-driven landscape, the role of data in shaping the performance of artificial intelligence (AI) applications cannot be overstated. The quality, quantity, and complexity of the data significantly affect the performance and reliability of machine learning models across various domains. As datasets continue to grow in size, researchers and practitioners face challenges in understanding and extracting meaningful insights, such as identifying patterns or outliers in the datasets. Traditional methods of data analysis often fall short when analyzing large-scale image datasets, highlighting the need for novel approaches to data exploration and visualization.\nEffectively visualizing large-scale image datasets requires approaches that can distill visual information into semantically meaningful representations, enabling users to uncover patterns, trends, and anomalies within the data. In response to these challenges, we introduce AEye a novel approach to visualizing image datasets.\nAEye leverages recent advancements in AI, specifically contrastive learning techniques, to embed semantic information into high-dimensional image representations. By projecting these representations onto a two-dimensional plane, AEye facilitates the visualization of image datasets in a manner that aligns with human perception and intuition.\nWe present the design and implementation of AEye and demonstrate its effectiveness in visualizing large image datasets. Through a series of demonstrative use cases, we illustrate how AEye enables researchers and practitioners to gain deeper insights into image data, uncover hidden patterns, and facilitate informed decision-making. By providing AI-guided visualization, AEye offers a practical solution for visualizing large-scale image datasets and lets researchers and laymen extract insights from their data.\nAEye is available at aeye.ethz.ch."}, {"title": "2 RELATED WORK", "content": "Large-scale data visualization is often facilitated by clustered and hierarchical representations. Dimensionality reduction techniques such as Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE) or Uniform Manifold Approximation and Projection (UMAP) have previously been applied for data visualization. These techniques map high-dimensional data points to lower dimensions while clustering the data and making inherent patterns in the data more apparent. In our work, we choose UMAP for dimensionality reduction as it is more scalable than PCA and t-SNE while preserving local and global structures. Our work also touches upon the intuitive search for biases and imbalances in image datasets, which have previously been observed, and techniques were proposed to uncover and combat them.\nTo obtain high-dimensional image embeddings, we use Contrastive Language-Image Pretraining (CLIP), which learns to understand images and text simultaneously by embedding them in a shared latent space. CLIP is trained on a diverse range of image-text pairs from the internet, enabling it to learn robust and generalized representations that capture the semantic content of images across a wide spectrum of concepts and categories. CLIP embeddings capture rich semantic information for images and text, enabling a wide range of tasks such as image classification, image retrieval, and text-to-image generation without task-specific supervision. We use the pretrained OpenAI CLIP model to embed all images. CLIP and other contrastive learning-based approaches have been used for image visualization before, mostly as point cloud visualizations.\nThe visualization technique most closely aligned to ours is the Embedding Projector, which also visualizes embeddings generated by ML models with projected positions and offers a similar navigation technique consisting of zooming and panning. While it can also display images at the projected positions, it does not offer the layered visualization approach we provide.\nAEye builds on these previous works and adds novel methods to show only a representative selection of images with a layered visualization style. Using contrastive learning methods, we can ensure that embeddings maintain semantic information and facilitate additional search features. Lastly, our approach scales to larger datasets in terms of the visualization itself, which always remains comprehensible with not too many images on screen, and computational resources, which directly benefit from the former."}, {"title": "3 AEYE APPLICATION", "content": "AEye is a web-based application designed to facilitate the exploration and comprehension of large-scale image datasets. At its core, AEye leverages the CLIP (Contrastive Language-Image Pretraining) embedding space to organize and visualize images in a two-dimensional plane. The positions of images within this embedding space are determined by their semantic similarity, allowing for intuitive navigation and exploration.\nAn overview of the AEye processing steps can be seen in Figure 2. In a data preprocessing stage, we compute CLIP embeddings for all images in the dataset, which are then stored in a vector database for fast nearest-neighbor lookups, which the semantic search relies on. The high-dimensional CLIP embeddings are then projected to two dimensions using the UMAP algorithm to find spatial positions for all images. To accommodate the limited screen space and the large number of images in the dataset, AEye employs a layered visualization approach. Multiple layers are created that the user can navigate through. The last layer contains all images of the dataset at their projected positions, while the other layers only contain a selection of representatives. As the user zooms in, the view transitions from layer to layer while zooming in on a continuously smaller area of the layers, which lets us populate them more and more densely while limiting the number of images on screen at any time. The representatives are chosen as the centers of a clustering we achieve with a modified k-means clustering algorithm. This ensures that each layer provides a condensed yet informative dataset view. The process is outlined in more detail in Section 3.2. Lastly, we can compute AI-generated captions with LLaVA for all generated images in the preprocessing stage.\nThe visualization starts with a view of the first layer, which displays representative images from each cluster in the embedding space. As the user interacts with the visualization, they have the ability to zoom in on specific regions of interest, progressively revealing more detailed subsets of images from deeper layers. This interactive exploration enables users to uncover hidden patterns, clusters, and relationships within the dataset, empowering them to gain insights into the underlying structure of the data."}, {"title": "3.1 CLIP Embeddings and UMap Projection", "content": "While CLIP embeddings effectively preserve the semantic meaning of images by encoding rich semantic information learned during pretraining, their high dimensionality, typically 512 dimensions, poses challenges for direct visualization. The sheer number of dimensions hinders intuitive interpretation and exploration of the"}, {"title": "3.2 Choosing Representative Images", "content": "Large-scale image datasets contain an overwhelming number of images, necessitating a strategic approach to presenting a subset of images to users. We adopt a hierarchical strategy comprising multiple layers to address this challenge. In the initial layer, users encounter a limited set of representative images. By zooming in, users can traverse through these layers, progressively revealing additional images until the final layer displays all images. This hierarchical approach effectively manages the number of images on screen, resulting in a comprehensible visualization. The selection of representative images is guided by several criteria: they should provide a coarse-grained overview of the embedding space, maintain sufficient spacing between each other to avoid too many overlapping images, and reflect the characteristics of images within their respective area. Moreover, continuity in representation across layers is ensured by maintaining representatives from previous layers for all following ones. By doing so, images presented to the user in a previous layer do not disappear when zooming in but instead stay in place. We achieve these objectives through a clustering-based approach and tiling of the projected embedding space.\nFigure 3 shows an overview of the proposed approach. Each layer consists of a regular grid of tiles, with the side length halving from one layer to the next. Within each tile, a fixed predefined number $k$ of images serves as representatives, approximately corresponding to the number of images visible on screen at any time. Selecting representatives is done by traversing layers from top to bottom and applying a k-means clustering algorithm with $k$ centers to each tile. Representatives are then chosen as the images closest to the cluster centers to ensure that they reflect the underlying structure of the embedding space. To maintain consistency in representatives across layers, we modify the k-mean algorithm by retaining the positions of representatives from previous layers as fixed centroids throughout the algorithm's execution. A detailed algorithm description in pseudocode is presented in Algorithm 1.\nWhen transitioning from layer to layer, the viewport's size is scaled proportionally with the size of the tiles, meaning that the viewport approximately covers the same number of tiles in every layer. As we limit the number of representative images per tile by $k$, the number of images on screen at the same time does not get too large.\nThis approach yields meaningful representatives, maintains scalability, and improves performance in the web application. While the computation of the k-mean algorithm poses a significant computational burden, especially in the first layer, subsequent layers benefit from its application to smaller subsets, albeit more numerous. These computations can be efficiently parallelized, and the number of required layers grows insignificantly with larger dataset sizes. Our demonstration webpage accommodates image datasets exceeding 100k images, where the preprocessing for the tiling and clustering took about as long as the generation of CLIP embeddings. This was on commodity hardware and took only few hours, even on the biggest datasets. As the number of layers is expected to scale logarithmically with the number of images, there will never be a need to transition through too many zoom layers. Additionally, the generated tiling facilitates efficient data loading for the front end. As users navigate through layers, each tile consistently occupies a proportional screen space when in focus. These tiles serve as subdivisions, simplifying the selective loading of necessary data for visualization. The front end can request data for specific tiles, optimizing resource utilization and enhancing user experience."}, {"title": "3.3 Semantic Search and Al Captions", "content": "Beyond exploring visualized data through spatial navigation, AEye features semantic search functionality for text queries and images. Semantic search leverages the rich semantic representations encoded by the CLIP model for text and images, enabling users to retrieve relevant content easily, for example, with natural language."}, {"title": "3.4 Interface Design", "content": "The user is initially presented with a few of the top layer of the embedding space and a search bar for the semantic text and image search in the center. If the user decides to submit a search query, the view zooms in to the closest match in the embedding space and shows the view depicted in Figure 5, with an example for the MNIST dataset in Figure 4. For MNIST, we can clearly observe a semantic clustering of numbers in the projected embedding space, which lets us infer emerging patterns from the overview.\nUsers can further navigate the layered embedding space by zooming and moving the viewport around. Blurred previews are shown in the background to give a sense of the images in the next layer. Clicking on an image results in the same view as provided by the search, with more information provided by the dataset and an AI-generated caption of the image, as well as the closest neighbors in the CLIP embedding space. To stay oriented, a minimap with an overview of the whole embedding space is provided in the bottom right, as visible in Figure 1. Datasets can be selected on the top left, and an information button on the top right shows a small explanation for the application."}, {"title": "3.5 Case Study", "content": "To demonstrate a possible use case for AEye, we consider a hypothetical machine-learning practitioner working with the Common Objects in Context (COCO) 2017 dataset. The practitioner aims to improve their object detection model by first understanding the data. The COCO 2017 dataset contains 163,000 images, making it impractical to visualize all images at once or manually sift through them. This highlights the need for a comprehensive tool. The practitioner uses AEye to facilitate this process.\nAfter the preprocessing, AEye produces its interactive visualization of the dataset on a 2D plane, clustering similar images together. This allows the practitioner to observe distinct clusters corresponding to different object categories, such as \u201cperson,\u201d \u201cvehicle,\u201d and \u201canimal,\u201d revealing the distribution of categories and identifying under- or overrepresented ones. Several outliers were also detected, which, upon further examination, revealed labeling errors and unusual object combinations that could impact model performance. The insights gained from AEye's visualization enable the practitioner to make informed decisions about the dataset. They identify underrepresented categories needing augmentation and corrected labeling anomalies, leading to a more balanced and accurate dataset for training their object detection model."}, {"title": "4 CONCLUSION", "content": "AEye offers a comprehensive solution for visualizing large-scale image datasets, leveraging contrastively trained embedding models for semantically rich representations. By incorporating hierarchical tiling, clustered subspaces, and semantic search features alongside AI-generated captions, AEye facilitates intuitive navigation and exploration of diverse image collections. The scalability and extensibility of AEye enable researchers and other users to explore various datasets, from machine learning to general art collections. With AEye, unlocking insights and uncovering patterns within large-scale image datasets becomes both accessible and insightful. A demonstration website with a selection of datasets, as well as the source code, can be found online."}]}