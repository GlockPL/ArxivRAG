{"title": "VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis", "authors": ["Donggoo Kang", "Dasol Jeong", "Hyunmin Lee", "Sangwoo Park", "Hasil Park", "Sunkyu Kwon", "Yeongjoon Kim", "Joonki Paik"], "abstract": "The Large Vision Language Model (VLM) has recently addressed remarkable progress in bridging two fundamental modalities. VLM, trained by a sufficiently large dataset, exhibits a comprehensive understanding of both visual and linguistic to perform diverse tasks. To distill this knowledge accurately, in this paper, we introduce a novel approach that explicitly utilizes VLM as an objective function form for the Human-Object Interaction (HOI) detection task (VLM-HOI). Specifically, we propose a method that quantifies the similarity of the predicted HOI triplet using the Image-Text matching technique. We represent HOI triplets linguistically to fully utilize the language comprehension of VLMs, which are more suitable than CLIP models due to their localization and object-centric nature. This matching score is used as an objective for contrastive optimization. To our knowledge, this is the first utilization of VLM language abilities for HOI detection. Experiments demonstrate the effectiveness of our method, achieving state-of-the-art HOI detection accuracy on benchmarks. We believe integrating VLMs into HOI detection represents important progress towards more advanced and interpretable analysis of human-object interactions.", "sections": [{"title": "1 Introduction", "content": "Human-object interaction (HOI) detection plays a pivotal role in understanding the interactions between humans and objects within visual scenes. This task involves identifying and locating both the objects in an image or video and the specific interactions or actions that humans perform with those objects. The ability to comprehend these intricate relationships is crucial for a wide range of applications, including image captioning [45], robotics [42], action recognition [44], scene understanding [41].\nHOI Detection involves two subtasks: 1) localizing the subject (human) and the target (object) of the interaction, and 2) classifying the interaction label"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Human-Object Interaction Detection", "content": "Human-object interaction (HOI) detection is an active area of research in computer vision. As noted in the introduction, HOI detection is a high-level task built on top of object detection. Existing HOI detection methods can be categorized into two main approaches: one-stage models and two-stage models.\nOne-stage models [31,37] aim to detect human-object interactions in a single feedforward pass through a neural network. These models take an image as input and directly output bounding boxes for humans, objects, and their interactions. While computationally efficient, one-stage models struggle to model complex interactions and scale to large numbers of interaction categories. In contrast, two-stage models [6, 12, 25, 49\u201351] separate the tasks of detecting humans and objects from modeling their interactions. In the first stage, off-the-shelf object detectors are used to localize candidate humans and objects in the image. The second stage then classifies their relationship based on appearance and spatial cues. By decomposing the problem, two-stage models are able to achieve higher accuracy but at the cost of reduced speed.\nAnother content [16, 18, 26, 34, 47, 52] recent work has focused on improving both branches of HOI detection. For one-stage models, newer architectures incorporate attention mechanisms and graphical networks to better model interactions [16,18,34]. For two-stage models, progress has been made in embedding space designs and developing robust spatial models [52].\nFrom another perspective, GEN-VLKT [26] proposes a Visual-Linguistic Knowledge Transfer (VLKT) strategy that leverages CLIP to enhance interaction understanding. It uses CLIP text embeddings to initialize the HOI classifiers and mimics CLIP image features. RLIP [47,48] proposes a pre-training strategy to train a robust backbone network by aligning the image representations of entities and relations with their corresponding text descriptions. Both works are impressive approaches in that leverage VLM and pre-training strategy."}, {"title": "2.2 Vision-Lauguage Model", "content": "Integrating vision and language has been a long-standing goal in artificial intelligence. Early work focused on image captioning [39], generating textual descriptions of image contents. Recent years have seen rapid progress in visual question answering [1, 2], enabled by large-scale datasets [7] and deep neural encoder-decoder models [43].\nMore advanced vision-language tasks require a tighter integration between the visual and linguistic modalities. This has led to a surge of interest in unified multimodal representation models that can process both images and text within a single framework [3].\nOne line of work explores joint embedding models to learn aligned vector representations for image regions and language fragments [15,19]. However, these approaches do not explicitly model interactions between modalities."}, {"title": "3 Proposed Method", "content": "In this section, we introduce our proposed VLM-HOI method, which is illustrated in Figure 3. As shown, we adopt a detection transformer (DETR) as the feature encoder and object detector, following recent works [16, 18, 37, 49, 51, 53, 57]. A query-based transformer decoder is also utilized to predict HOI triplets, as done in recent studies [16, 18]. The goal of this work is to effectively transfer the comprehensive language understanding capabilities of vision-language models (VLMs) to HOI detection.\nIn the following section, we first briefly define the baseline VLM model. We then introduce a brief problem definition and present HOI triplet association technique that converts predicted HOI triplets into positive and negative text forms as inputs to the VLM. These text sets are used to compute image-text matching scores with the VLM. Subsequently, we introduce a contrastive learning loss to learn representations of VLM using these matching scores."}, {"title": "3.1 Baseline VLM Model", "content": "While large models like CLIP [35] excel at overall image-text similarity, their limitations in pinpointing specific objects become apparent. This paper takes a step forward, focusing on BLIP [20, 21], a VLM specifically designed for object-level understanding, and its potential as a powerful teacher model for knowledge"}, {"title": "3.2 Problem Definition", "content": "Human-object interaction (HOI) detection aims to locate human and object instance in an image and classify their interaction relationship. Formally, given an image $I$, the objective is to detect a set of $N$ interacted human-object pairs $\\{(h_i, O_i, V_i)\\}$ where $h_i$ and $o_i$ represent detected human and object. $v_i$ denotes the interaction between $h_i$ and $o_i$."}, {"title": "3.3 HOI Triplet Association", "content": "The objective of the HOI triplet association is to convert these triplets into grounded natural language representations to leverage the language comprehension capabilities of vision-language models (VLMs). Our model first predicts a set of N predicted HOI triplets $T = \\{(\\hat{h}_1, \\hat{o}_1, \\hat{v}_1), ..., (\\hat{h}_N, \\hat{o}_N, \\hat{v}_N)\\}$ from the input image I, where $\\hat{h}, \\hat{o}, \\hat{v}$ are the detected human, object and predicted interaction verb, respectively.\nGiven these predictions, we extract the detected object and interaction class names $o_i$ and $v_i$ from the classifier outputs for each triplet. We exclude any triplets predicted as \"no interaction\" or \"no object\", denoted by $\\varnothing$, as determined by the Hungarian matching. For simplicity, we set all predicted human classes h as \"A person\". We then construct a positive sentence $S^{pos}$ for each triplet using the template:\n$s_i^{+} = [\\text{``A person''} + v_i + \\text{`` a ''} + o_i]$, if $T \\in H$\n$s_i^{+} = [\\text{``A person''} + v_i + \\text{`` a ''} + o_i]$, if $T \\notin H$\nThe examples of these sentences are described in Figure 1. The generated sentences may not be grammatically correct, since the class names in most HOI datasets are not designed to produce fluent phrases. Simply concatenating the human, verb, and object classes can result in unnatural or awkward language. However, these grounded sentences still provide useful context and supervision for knowledge distillation from the pre-trained VLM. The key elements of human, action, and object capture the core semantics of the visual HOI detections in a textual representation. While not linguistically flawless, this allows transferring relevant knowledge about humans, objects, and their interactions from the VLM to the HOI model.\nThe paired positive and negative sentences provide contrasting language context for the VLM to comprehend the visual HOI concept deeply. This gives us a set of positive sentences $S^{pos} = \\{s_1^+, ..., s_n^+\\}$ and a set of negative sentences $S^{neg} = \\{s_1^-, ..., s_m^-\\}$ that are semantically grounded in the visual HOI predictions. By optimizing the VLM representations on these contrasting text sets, we enable the model to learn fine-grained HOI concepts based on its language priors. This facilitates transferring the VLM's comprehensive language understanding to improve HOI prediction."}, {"title": "3.4 Image-Text Matching based Knowledge Distillation", "content": "To harness the image-text matching capabilities of VLMs, we compute similarity scores between a given image I and paired sentence sets: $S^{pos}$ for positive (correct) associations and $S^{neg}$ for negative (incorrect) ones. The calculation is formalized as:\n$sim(I, S) = VLM_{itm}(I, S)$,\nwhere sim(I, S) denotes the image-text similarity score obtained from the VLM for the image I and sentence set S.\nIdeally, the similarity scores of positive sets should be as close to zero as possible, while the similarity scores of negative sets should be as high as possible. However, Figure 1 illustrates that similarity scores cannot be zero for positive triplets. This ensures that the loss is never zero, even when all predictions are correct. We will discuss later how this problem inhibits the optimization of other losses.\nTo explicitly distill the rich knowledge encoded in the VLM's parameters, we employ these scores in a contrastive learning loss:\n$L_{ITM} = \\sum_{i=0}^{n}(max(0, \\alpha - sim(I, s_i^+))) + \\sum_{j=0}^{m}(sim(I, s_j^-))$\nHere, $\\alpha$ serves as positive margins that anchor the contrastive loss. This loss function is structured to optimize the pairing of images with their corresponding positive sentences while disassociating them from negative ones.\nThe key insight is that the pre-trained vision-language model has learned a robust joint distribution over both visual and textual modalities through extensive pre-training. By optimizing our HOI network to align with the VLM's image-text similarities via contrastive loss, we essentially distill this strong prior knowledge into our model. This guides the HOI network to find an optimal parameter configuration that encodes visual-linguistic concepts effectively. This distillation leads to more efficient optimization and substantial gains for rare HOI categories that lack sufficient training examples. In essence, the VLM's contextual knowledge helps regularize the HOI model, reducing overfitting and improving generalization."}, {"title": "3.5 Training and Inference", "content": "Following prior query-based HOI detection methods [16, 18, 49], we utilize the Hungarian matching algorithm to associate predicted triplets with ground truth triplets. As illustrated in Figure 3, our network consists of a DETR-based encoder and three parallel decoder branches with task-specific queries $Q^P$, $Q^A$, and $Q^O$ for human, object, and interaction prediction respectively. We select BLIP [21] as the VLM to compute image-text matching scores because it has"}, {"title": "4 Experimental Results", "content": ""}, {"title": "4.1 Implementation Details", "content": "Our model uses a ResNet-50 CNN backbone followed by a 6-layer transformer encoder, with L = 6 parallel prediction branches. We set $N$ = 64 queries for HICO-DET and $N$ = 100 for V-COCO following prior work [18,49]. The loss weights $A_i$ are set to 2.5, 1, 1, 1 for $\\mathcal{L}_{L1}$, $\\mathcal{L}_{GIOU}$, $\\mathcal{L}_{Loc}$, and $\\mathcal{L}_{Lic}$ respectively. We initialize from a DETR model [5] pretrained on COCO [27] and optimize using AdamW [29] with weight decay 1e-4. The CNN backbone has a learning rate of 1e-5 while all other components use 1e-4, trained for 100 epochs. For V-COCO, the CNN weights are frozen to prevent overfitting and the learning rate is reduced to 4e-5. All experiments use a batch size of 4 on 4 RTX 3090ti GPUs. These optimized hyperparameters enable efficient end-to-end training of our method."}, {"title": "4.2 Dataset", "content": "We conduct experiments on two standard HOI detection benchmarks: HICO-DET [14] and V-COCO [13].\nHICO-DET consists of 47,776 images, with 38,118 for training and 9,658 for testing. It contains 600 HOI categories composed of 80 object classes and 117 action verbs.\nV-COCO is a subset of 10,396 COCO images, split into 5,400 train and 4,964 test images. It has 29 action classes including 4 body motions without object interactions, and the same 80 objects as HICO-DET. In total, V-COCO has 263 unique HOI triplets."}, {"title": "4.3 Evaluation Metric", "content": "Following the evaluation protocol from [18,49], we use mean Average Precision (mAP) as the metric for measuring HOI detection performance. A predicted triplet is considered a true positive if: 1) the detected human and object boxes have over 50% IOU with ground truth, and 2) the predicted interaction categories match the labels.\nOn HICO-DET, we report mAP on the full 600 classes, 138 rare classes, and 462 non-rare classes. For V-COCO, we evaluate on S1 with 29 actions including body motions, and S2 with 25 actions excluding no-object categories. By benchmarking on these diverse splits, we comprehensively analyze our method's HOI detection capabilities."}, {"title": "4.4 Comparison with State-of-the-Art", "content": "We extensively evaluate our VLM-HOI method against state-of-the-art approaches on two standard HOI detection benchmarks - HICO-DET [14] and V-COCO [13].\nOn HICO-DET (Table 1), our proposed knowledge distillation method improves upon GEN-VLKT [26], which also utilizes external knowledge but only"}, {"title": "4.5 Ablation Study", "content": "Effects of the Margin: A critical hyperparameter in our image-text contrastive loss is the positive sample margin $\\alpha$. This margin controls the lower bound on the similarity scores between positive text prompts and the image. Intuitively, it determines how close we want to pull the positive grounded sentence representations to the image representation."}, {"title": "4.6 Computational Resource", "content": "A core component of our approach is the large pre-trained VLM which increases the computational requirements. As shown in Table 5, we use BLIP with 361M parameters due to hardware constraints, compared to 69M parameters for the baseline HOI network.\nThe additional VLM computations result in longer training times compared to the baseline - around 2.4\u00d7 in our experiments. However, we found that our method requires fewer training epochs to converge, likely due to the beneficial regularization and optimized initialization from the VLM distillation.\nAt inference time, our model has the same efficiency as the baseline since the VLM is only used during training for knowledge transfer. We only need to perform a single forward pass through the learned HOI network."}, {"title": "5 Conclusion", "content": "In this paper, we present a novel approach that leverages the capabilities of the Large Vision Language Model (VLM) to enhance Human-Object Interaction (HOI) detection. By utilizing VLM as an objective function in the context of HOI detection, we have successfully quantified the similarity of predicted HOI triplets through Image-Text matching, harnessing VLM's comprehensive understanding of both visual and linguistic modalities. Our experiments have yielded state-of-the-art results in HOI detection accuracy on benchmark datasets, marking a significant advancement in the field. This novel integration of VLM into HOI detection not only showcases the potential of language comprehension capabilities in bridging modalities but also takes a promising stride towards more advanced and interpretable human-object interaction analysis. We hope the findings presented in this paper offer valuable insights and open new avenues for research at the intersection of vision and language."}]}