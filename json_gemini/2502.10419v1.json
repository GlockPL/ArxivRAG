{"title": "A HYBRID SWARM INTELLIGENCE APPROACH FOR OPTIMIZING\nMULTIMODAL LARGE LANGUAGE MODELS DEPLOYMENT IN\nEDGE-CLOUD-BASED FEDERATED LEARNING ENVIRONMENTS", "authors": ["Gaith Rjoub", "Hanae Elmekki", "Saidul Islam", "Jamal Bentahar", "Rachida Dssouli"], "abstract": "The combination of Federated Learning (FL), Multimodal Large Language Models (MLLMs), and\nedge-cloud computing enables distributed and real-time data processing while preserving privacy\nacross edge devices and cloud infrastructure. However, the deployment of MLLMs in FL environ-\nments with resource-constrained edge devices presents significant challenges, including resource\nmanagement, communication overhead, and non-IID data. To address these challenges, we propose\na novel hybrid framework wherein MLLMs are deployed on edge devices equipped with sufficient\nresources and battery life, while the majority of training occurs in the cloud. To identify suitable\nedge devices for deployment, we employ Particle Swarm Optimization (PSO), and Ant Colony Op-\ntimization (ACO) is utilized to optimize the transmission of model updates between edge and cloud\nnodes. This proposed swarm intelligence-based framework aims to enhance the efficiency of MLLM\ntraining by conducting extensive training in the cloud and fine-tuning at the edge, thereby reducing\nenergy consumption and communication costs. Our experimental results show that the proposed\nmethod significantly improves system performance, achieving an accuracy of 92%, reducing com-\nmunication cost by 30%, and enhancing client participation compared to traditional FL methods.\nThese results make the proposed approach highly suitable for large-scale edge-cloud computing\nsystems.", "sections": [{"title": "1 INTRODUCTION", "content": "Internet of Things (IoT) devices have become increasingly prevalent, generating vast amounts of data that neces-\nsitate real-time processing and intelligent decision-making. To effectively analyze this diverse and extensive data,\ndeep learning approaches have emerged, particularly Multimodal Large Language Models (MLLMs), which excel in\nprocessing and understanding various data types, including text, images, audio, and sensor readings. These models\nhave contributed significantly to advancements in several emerging domains Huang et al. (2024), such as autonomous\nsystems Aldeen et al. (2024), smart healthcare Wang et al. (2023), and industrial IoT Fan et al. (2024), Rjoub et al.\n(2024a). In this context, the integration of edge-cloud computing systems with Federated Learning (FL) has garnered\nsubstantial attention for its ability to facilitate decentralized training of the deep learning model while ensuring data\nprivacy and security Akhtarshenas et al. (2024), Bao & Guo (2022), Wu et al. (2024b), Trindade et al. (2022), Sagar"}, {"title": "1.1 CONTEXT AND MOTIVATION", "content": "Internet of Things (IoT) devices have become increasingly prevalent, generating vast amounts of data that neces-\nsitate real-time processing and intelligent decision-making. To effectively analyze this diverse and extensive data,\ndeep learning approaches have emerged, particularly Multimodal Large Language Models (MLLMs), which excel in\nprocessing and understanding various data types, including text, images, audio, and sensor readings. These models\nhave contributed significantly to advancements in several emerging domains Huang et al. (2024), such as autonomous\nsystems Aldeen et al. (2024), smart healthcare Wang et al. (2023), and industrial IoT Fan et al. (2024), Rjoub et al.\n(2024a). In this context, the integration of edge-cloud computing systems with Federated Learning (FL) has garnered\nsubstantial attention for its ability to facilitate decentralized training of the deep learning model while ensuring data\nprivacy and security Akhtarshenas et al. (2024), Bao & Guo (2022), Wu et al. (2024b), Trindade et al. (2022), Sagar"}, {"title": "1.2\nCONTRIBUTIONS", "content": "In this paper, we present a novel hybrid swarm intelligence approach to optimize the deployment of MLLMs in FL\nwithin smart edge-cloud computing systems. Our key contributions are summarized as follows:\n1. Hybrid PSO-ACO Optimization Framework for Efficient Deployment of MLLMs: We propose a hybrid\noptimization framework that integrates PSO and ACO. In this framework, PSO is utilized for the efficient\nselection of edge devices for deploying pretrained MLLMs, based on criteria such as resource availability, data\nrelevance, and network stability. ACO is employed to optimize communication pathways and facilitate the\nsharing of MLLM updates between the selected edge devices and cloud servers, minimizing communication\noverhead and latency.\n2. Addressing Non-IID Data: The proposed PSO-based method tackles the challenges associated with non-IID\ndata by intelligently selecting a subset of edge devices to ensure diverse contributions to the global model.\nThis approach enhances the generalization and accuracy of the MLLM despite the heterogeneity present in\nthe local data distributions.\n3. Energy Efficiency and Model Performance Trade-off Optimization: Our approach balances energy con-\nsumption with model accuracy, achieving significant reductions in total energy usage while maintaining or\nimproving the performance of the MLLM. This is particularly important for prolonging the operational life of\nedge devices in resource-constrained environments.\n4. Scalability and Adaptability in Dynamic Environments: The framework is designed to scale effectively\nand adapt to real-time variations in edge-cloud environments, managing large numbers of devices and varying\nnetwork conditions while ensuring optimal system performance.\n5. Extensive Experimental Validation: We conducted comprehensive simulations to evaluate the performance\nof our hybrid PSO-ACO framework. The results demonstrate notable improvements in energy efficiency,\ncommunication overhead reduction, and model accuracy compared to relevant benchmarks, including deep\nand reinforcement learning-based approaches.\nThese contributions collectively provide a robust solution to the challenges of deploying MLLMs in FL systems within\nsmart edge-cloud computing environments. Our hybrid PSO-ACO framework enhances the efficiency, scalability, and\nperformance of FL, making it more viable for real-world applications involving large-scale, resource-constrained edge\nnetworks.\nThe paper is structured as follows: Section 2 presents the problem formulation, describing the objectives and con-\nstraints of the federated learning scenario with the integration of swarm intelligence over the edge-cloud computing\nenvironment. Section 3 provides a brief background on MLLMs, including their training process and fine-tuning meth-\nods, which are crucial for understanding the implementation of our proposed solution. Section 4 gives an overview of\nthe related work in FL, swarm intelligence, and edge-cloud computing, establishing the foundation for the proposed\napproach. Section 5 details the proposed hybrid PSO-ACO framework, explaining the integration of particle swarm\noptimization and ant colony optimization to enhance model performance. The experimental setup and simulation\nresults are presented in Section 6, where we compare the performance of the proposed framework against relevant\nbenchmarks, including traditional and learning-based models. Finally, Section 7 concludes the paper by summarizing\nthe findings and discussing potential future research directions."}, {"title": "2 PROBLEM FORMULATION", "content": "Deploying MLLMs in FL systems within smart edge-cloud computing environments poses several critical challenges.\nThese challenges arise due to the decentralized nature of FL, the limited resources of edge devices, and the inherent\ncomplexities in managing communication and model training across a large and diverse network of devices. In this\nsection, we outline the key challenges that must be addressed to enable the efficient deployment of MLLMs in FL\nsystems."}, {"title": "2.1\nRESOURCE CONSTRAINTS ON EDGE DEVICES", "content": "One of the primary challenges in deploying MLLMs in FL is the limited computational and energy resources available\non edge devices, such as IoT sensors, cameras, and mobile devices. These devices often have restricted battery life\nand processing power, making it impractical for all devices to participate in every round of FL training. Given the high\ncomputational demands of training MLLMs, selecting a subset of edge devices that can contribute effectively to the\nglobal model without exhausting their resources is essential.\nThe total energy consumption $E_i$ for each device i includes the energy required for both model training $E_{\\text{train}}$, com-\nmunication $E_{\\text{comm}}$ and time $E_{\\text{time}}$:\n$$E_i = E_{\\text{train}} + E_{\\text{comm}} + E_{\\text{time}}$$\nwhere $E_{\\text{train}}$ is the energy consumed during training, $E_{\\text{comm}}$ and $E_{\\text{train}}$ represents the energy and time required to\ntransmit model updates to a central server or cloud coordinator, respectively. This central server aggregates local\nmodel updates from multiple edge devices. This aggregation step is crucial in FL for updating the global model,\nwhich is then shared back with the participating devices.\nIn FL, training takes place over multiple rounds, where each round consists of three main steps: (1) Global Model\nDistribution: The central server sends the current global model to a selected set of edge devices. (2) Local Training:\nEach selected device trains the model on its local data for a specified number of iterations, using its own computational\nresources. (3) Model Update Communication: The locally updated models are then transmitted back to the central\nserver for aggregation. The process is repeated for several rounds, with the goal of gradually improving the global\nmodel's accuracy. The challenge is to minimize the total energy consumption across the selected devices during these\nrounds, while still ensuring meaningful contributions to the global model updates:\n$$\\min_{S \\subset D} \\sum_{i \\in S} E_i$$\nwhere S represents the subset of devices selected from the entire device set D."}, {"title": "2.2 COMMUNICATION OVERHEAD AND LATENCY", "content": "Another significant challenge is the communication overhead associated with transmitting large model updates be-\ntween edge devices and the cloud or edge servers. In FL, frequent communication is required to synchronize local\nmodel updates with the global model, which can result in substantial bandwidth usage, network congestion, and in-\ncreased latency. This is particularly problematic when deploying MLLMs, which typically have large model sizes.\nThe communication cost $C_{\\text{comm}}$ for each device i can be modeled as follows:\n$$C_{\\text{comm}} = \\sum_{i \\in S} \\frac{M_i}{B_i} d_i$$\nwhere $M_i$ is the size of the model update, $B_i$ is the available bandwidth, and $d_i$ represents the distance between device\ni and the nearest server. Reducing communication overhead and minimizing latency is crucial to maintaining the\nefficiency of the FL process, particularly in large-scale deployments."}, {"title": "2.3\nNON-IID DATA DISTRIBUTION ACROSS DEVICES", "content": "In FL systems, data is distributed across edge devices in a non-IID (non-independent and identically distributed)\nmanner, meaning that each device collects data that reflects its specific environment or context. This leads to significant\nvariability in the type and distribution of data across the network, which can negatively affect the performance and\ngeneralization of the global model, especially when training MLLMs that rely on diverse and balanced data.\nThe local loss function $L_i(w)$ for each device i is computed based on its local data, where w represents the model\nparameters:\n$$L_i(w) = \\sum_{j=1}^{n_i} \\frac{1}{N_i} l(f(x_{j}; w), y_{j})$$\nwhere ni is the number of local data points, $l(f(x_{j}; w), y_{j})$ is the loss function between the model prediction $f (x_{j}; w)$ and the true label $y_j$. The global objective in FL is to minimize the weighted sum of the local losses across all selected\ndevices:\n$$L(w) = \\sum_{i \\in S} \\frac{N_i}{n} L_i(W)$$"}, {"title": "2.4 ENERGY AND TIME EFFICIENCY VS. MODEL PERFORMANCE TRADE-OFF", "content": "A critical challenge in FL systems is balancing the energy and time consumption of edge devices with the overall\nperformance of the MLLM. Edge devices with limited energy and time resources may still hold valuable data that\ncould significantly improve the global model, however, their continuous participation could lead to battery depletion,\nincreased latency, and potential system failures. Conversely, prioritizing solely on energy and time efficiency may\nresult in the exclusion of devices with high-quality data essential for effective model training. Balancing these factors\nis crucial to ensuring both sustainable resource usage and optimal model performance across the network.\nThe total energy consumed by a device can be formulated as shown in Eq.1. The optimization problem involves\nminimizing a weighted combination of energy consumption and model performance as follows:\n$$\\min_{S \\subset D} \\alpha \\sum_{i \\in S} E_i + \\gamma \\sum_{i \\in S} T_i + \\beta L(w)$$\nwhere \u03b1, \u03b2, and y are weighting factors that balance the trade-off between energy efficiency and model accuracy."}, {"title": "2.5\nSCALABILITY AND ADAPTABILITY IN DYNAMIC EDGE-CLOUD ENVIRONMENTS", "content": "As edge-cloud systems scale to include thousands or even millions of devices, the need for scalability and dynamic\nadaptability becomes increasingly important. Edge devices are often mobile and subject to changing environmental\nconditions, leading to variability in resource availability and connectivity. This dynamic nature requires FL systems\nto continuously adapt to changes in the network, ensuring efficient use of resources and maintaining high model\nperformance.\nScalability challenges arise in ensuring that the system can manage the increasing number of devices without over-\nwhelming network resources or introducing significant delays. Dynamic adaptability involves efficiently handling\ndevices joining or leaving the network, adjusting communication paths, and reallocating computational tasks in real\ntime."}, {"title": "3 MULTIMODAL LARGE LANGUAGE MODEL (MLLM) BACKGROUND", "content": "MLLM are advanced machine learning models capable of processing and integrating data from multiple modalities,\nsuch as text, images, audio, and structured sensor data Salin et al. (2022). The structure and architecture of MLLM\ncenter on integrating a Large Language Model (LLM) with additional components to handle other data modalities, like\nimages, audio, or video. MLLMs primarily have a language-centric architecture where other modalities are treated as\nextensions, allowing the model to understand and generate responses based on complex, multimodal input Wu et al.\n(2023). Fig. 2 depicts the architectural overview of MLLMs illustrating core components and workflow."}, {"title": "3.1 BACKBONE OF MLLM", "content": "LLM Foundation: The core of an MLLM is an LLM such as GPT, PaLM, or BERT, trained on extensive text datasets\nCui et al. (2024). The language model is typically a transformer-based model with self-attention mechanisms that\nenable it to understand and generate text Islam et al. (2024)."}, {"title": "3.2\nMODALITY-SPECIFIC ENCODERS", "content": "To handle non-textual data like images or audio, modality-specific encoders are used. The encoders transform each\nmodality into embeddings that can be processed by the language model Tibon et al. (2019).\n\u2022 Image Encoder: Given an image I, it is divided into patches, and each patch is flattened into a vector\nNguyen & Fernando (2022). These patch vectors are linearly embedded into a vector space, similar to text\ntoken embeddings:\n$$f_i = \\text{Embed}(\\text{patch}_i) \\text{ for each patch}$$\nThis results in image embeddings $F = \\{f_1, f_2, ..., f_m\\}$, where each $f_i \\in \\mathbb{R}^d$.\n\u2022 Audio Encoder: Audio data A is processed into a sequence of feature vectors $a_j$ using an encoder like\nwav2vec Ge et al. (2024b).\n$$a_j = \\text{wav2vec}(A_j) \\text{ for each audio segment j}$$\nThis results in audio embeddings $A = \\{a_1, a_2, ..., a_k\\}$, with each $a_j \\in \\mathbb{R}^d$.\n\u2022 Sensor Data Encoder: Sensor data S is processed into a sequence of feature vectors $S_z$ using an encoder\nmodule of Transformer model Islam et al. (2024).\n$$S_z = \\text{Transformer-Encoder}(S_z) \\text{ for each sequential data segment z}$$\nThis results in audio embeddings $S = \\{s_1, s_2, ..., s_k\\}$, with each $S_z \\in \\mathbb{R}^d$."}, {"title": "3.3\nMULTIMODAL EMBEDDING LAYER", "content": "The outputs of the modality-specific encoders (e.g., image and audio embeddings) are mapped to a unified embedding\nspace compatible with the language model. These embeddings are typically augmented with special tokens to identify\ntheir modality Kim et al. (2020).\nLet $E_{\\text{text}}$ represent text embeddings, $E_{\\text{image}}$ represent image embeddings, $E_{\\text{audio}}$ represent audio embeddings. Each\nmodality is projected into the shared space by learned projections:\n$$E_{\\text{modality}} = W_{\\text{modality}} \\times E_{\\text{modality}}$$\nwhere $W_{\\text{modality}}$ is a learnable weight matrix that aligns each modality to the language model's embedding space."}, {"title": "3.4 FUSION MECHANISM", "content": "\u2022 Attention-Based Fusion: MLLMs typically leverage the transformer's self-attention mechanism to fuse mul-\ntimodal information. During processing", "Layers": "In some MLLMs, additional cross-attention layers are introduced to explicitly in-\nteg"}]}