{"title": "EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models", "authors": ["He Hu", "Yucheng Zhou", "Lianzhong You", "Hongbo Xu", "Qianning Wang", "Zheng Lian", "Fei Richard Yu", "Fei Ma", "Laizhong Cui"], "abstract": "With the integration of Multimodal large language models (MLLMs) into robotic systems and various Al applications, embedding emotional intelligence (EI) capabilities into these models is essential for enabling robots to effectively address human emotional needs and interact seamlessly in real-world scenarios. Existing static, text-based, or text-image benchmarks overlook the multimodal complexities of real-world interactions and fail to capture the dynamic, multimodal nature of emotional expressions, making them inadequate for evaluating MLLMs' EI. Based on established psychological theories of EI, we build EmoBench-M, a novel benchmark designed to evaluate the EI capability of MLLMs across 13 valuation scenarios from three key dimensions: foundational emotion recognition, conversational emotion understanding, and socially complex emotion analysis. Evaluations of both open-source and closed-source MLLMs on EmoBench-M reveal a significant performance gap between them and humans, highlighting the need to further advance their EI capabilities. All benchmark resources, including code and datasets, are publicly available at https://emo-gml.github.io/.", "sections": [{"title": "Introduction", "content": "Emotional Intelligence (EI), initially conceptualized by Salovey and Mayer (1990), emphasizes the ability to perceive, understand, regulate, and apply emotions in oneself and others. Recent advancements in multimodal large language models (MLLMs) have significantly improved human-computer interaction and natural language understanding, and integrating MLLMs into robotic control systems has become increasingly prevalent (Sartor and Thompson, 2024). Incorporating EI capabilities within MLLMs is essential for improving robotic performance in real-world environments. It will enable robots to address human emotional needs better and ensure more effective interactions. However, there is currently no universal bench-"}, {"title": "Related Work", "content": null}, {"title": "Multimodal Large Language Models", "content": "With the success of LLMs in various natural language processing (NLP) tasks, such as reasoning (Zhou et al., 2023) and euphemism detection (Li"}, {"title": "Evaluation of Emotional Intelligence", "content": "Given that EI is essential for understanding and responding to human emotions, many studies have focused on evaluating the EI capabilities of LLMs. MERBench (Lian et al., 2024b) standardizes evaluation for multimodal emotion recognition by addressing inconsistencies in feature extractors and offering a unified framework. It introduces MER2023 (Lian et al., 2023a), a dataset focused on the Chinese language, emphasizing multi-label learning and robustness analysis. Moreover, MC-EIU (Liu et al., 2024) offers a joint evaluation of emotion and intent in multimodal conversations. MOSABench (Song et al., 2024b) introduces a novel method for multi-object sentiment analysis, emphasizing the challenges MLLMs face in handling spatial complexities. Lian et al. (2023b) eval-uates GPT-4's visual capabilities in emotion recog-nition tasks but reveals limitations in recognizing micro-expressions and leveraging temporal data effectively. EmotionBench (Huang et al., 2023b) employs emotional appraisal theory to evaluate LLMs, exposing misalignments between LLM responses and human emotional behaviors. To deep dive"}, {"title": "Multimodal Benchmarks for LLMs", "content": "The rapid development of Multimodal Large Language Models (MLLMs) has necessitated the creation of diverse benchmarks to systematically evaluate their capabilities across perception, reasoning, and application domains. MME (Fu et al., 2023) and MMT-Bench (Ying et al., 2024) serve as comprehensive benchmarks for foundational multimodal tasks and general-purpose intelligence across varied domains. MultiTrust (Zhang et al.) evaluates trustworthiness, focusing on truthfulness, safety, robustness, fairness, and privacy risks. Moreover, HumanVBench (Zhou et al., 2024b) and MVBench (Li et al., 2024c) center on humancentric and temporal understanding in video content, exposing gaps in MLLMs' abilities to align cross-modal and temporal dynamics effectively. Specialized benchmarks have emerged to tackle domain-specific challenges. For instance, MathScape (Zhou et al., 2024a) targets multimodal mathematical reasoning, while M3SciQA (Li et al., 2024b) focuses on scientific question answering. BenchLMM (Cai et al., 2024) evaluates models under diverse style shifts, and BLINK (Fu et al., 2024) targets core visual perception tasks that remain challenging for multimodal models. Zhou et al. (2025) evaluate the medical diagnostic capabilities and generalization abilities of LVLMs. Furthermore, SEED-Bench-2-Plus (Li et al., 2024a) assesses MLLMs' abilities in text-rich visual scenarios, such as interpreting charts and maps."}, {"title": "EmoBench-M", "content": null}, {"title": "Evaluation Taxonomy", "content": "To systematically evaluate MLLM EI capabilities, the evaluation focuses on three dimensions based on established psychological theories of EI (Salovey and Mayer, 1990; Goleman, 1996; Mayer et al., 1999): \"Foundational Emotion Recognition\", \"Conversational Emotion Understanding\", and \"So-"}, {"title": "Foundational Emotion Recognition", "content": "Foundational emotion recognition, a core aspect of Emotional Intelligence (EI), focuses on identifying basic emotions such as anger, happiness, and sadness (Ekman, 1992; Scherer, 2005). This dimension evaluates a Multimodal Large Language Model's (MLLMs') ability to extract and integrate emotional information from multimodal signals (video, audio, and text) to recognize these fundamental emotions, a crucial capability for higherlevel EI. The MLLMs' proficiency in discerning emotions conveyed through speech, music, and video is assessed. Song and Speech Emotion Recognition uses data sourced from (Livingstone and Russo, 2018), which provides video clips with audio-visual emotional cues. Opinion Sentiment Analysis utilizes data sourced from (Zadeh et al., 2016), focusing on speech and facial expressions in opinion videos. Emotion Intensity Analysis goes beyond simple polarity; data sourced from the CMU-MOSEI dataset (Zadeh et al., 2018) is used to assess both the emotional state and its intensity from audio and video. This requires the model to identify specific emotion categories (e.g., happiness, sadness, anger) and quantify their intensity levels across diverse video content. Stock Comment Emotion Analysis employs data sourced from (Song et al., 2024a), analyzing emotions expressed in stock-related video comments."}, {"title": "Conversational Emotion Understanding", "content": "Conversational emotion understanding requires MLLMs to track emotional dynamics and interpret their contextual significance (Poria et al., 2019b; Hazarika et al., 2018). This involves identifying emotional shifts in multi-party conversations, leveraging semantic and tonal cues, and adapting to dynamic contexts, including inter-participant emotional interplay. Several scenarios in this dimension: Fine-grained Dialog Emotion Analysis (data source from (Lian et al., 2023a)) captures subtle emotional shifts. Face-centric Dialog Emotion Analysis (data source from CH-SIMS (Yu et al., 2020)) focuses on facial expressions and verbal/visual cues in interactive, conversational settings. Presentation Emotion Analysis (data source from CH-SIMSv2 (Liu et al., 2022)) examines emotions in formal presentations. Crucially, CH-SIMSv2 extends CH-SIMS by encompassing broader presentation styles, multi-speaker scenarios, and more diverse non-verbal cues. Conversational Emotion and Intent Analysis (data source from (Liu et al., 2024)) detects emotions and infers intentions. Multi-party Dialog Emotion Recognition (data source from (Poria et al., 2019a)) analyzes multi-party conversations, classifying seven emotions based on speech and facial cues."}, {"title": "Socially Complex Emotion Analysis", "content": "Emotional expression is influenced by internal drives and external social/cultural contexts (Frith"}, {"title": "Data Collection and Processing", "content": "The EmoBench-M benchmark was meticulously curated to evaluate the EI capabilities of MLLMs across a diverse range of tasks. As shown in Figure 2, the data collection and processing pipeline involved rigorous filtering and class balancing to ensure high quality and fairness."}, {"title": "Filtering and Quality Assurance", "content": "After compiling datasets for each emotion recognition task, the data underwent a multi-stage filtering process. In this process, we remove ambiguous, mislabeled, or controversial samples. Three graduate students manually reviewed video samples and their corresponding labels to identify and remove problematic instances. Reviewers independently assessed the emotion displayed in each video clip. These assessments were then compared against the original dataset labels. A voting system was implemented: each reviewer cast a vote for what they believed to be the correct emotion. If the original dataset"}, {"title": "Class Imbalance Correction", "content": "To ensure a fair and unbiased evaluation, we addressed potential class imbalances within the dataset. Some classes, particularly in tasks with numerous emotional categories, were underrepresented. To mitigate this, we employed oversampling of the minority classes, creating a more balanced distribution. This prevents the benchmark from being biased towards specific emotional classes and promotes better generalization across the full spectrum of emotions."}, {"title": "Dataset Statistics", "content": "EmoBench-M encompasses tasks with varying sample sizes, ranging from 80 to 500 samples per task, as detailed in Table 2. Each task is designed to evaluate different facets of emotional intelligence, spanning from basic emotion recognition to understanding more complex social emotions. Performance metrics such as accuracy (ACC), Weighted Average F-score (WAF), and, for generation tasks, LLM-based evaluation are employed."}, {"title": "Experiments", "content": null}, {"title": "Experimental Settings", "content": "We evaluate all MLLMs in a zero-shot setting on EmoBench-M to assess their innate capabilities. For classification tasks, the models are prompted to predict emotion categories directly from multimodal inputs, such as audio, video, and text. For generative tasks, the models are required to provide detailed explanations or inferences regarding emotional contexts. The prompts used for these tasks are carefully designed to ensure clarity and consistency across different models and are detailed in the Appendix F. We employ metrics such as accuracy (ACC), Weighted Average F-score (WAF) for classification tasks,"}, {"title": "Results and Findings", "content": "We show the performance of various models on the EmoBench-M benchmark across three dimensions: Foundational Emotion Recognition (FER), Conversational Emotion Understanding (CEU), and Socially Complex Emotion Analysis (SCEA). As shown in Table 3, Gemini-2.0-Flash achieves the best results (ACC: 61.4%, WAF: 60.6%), outperforming all open-source models. Among the latter, Qwen2-Audio-7B-Instruct stands out with an ACC of 59.9%. Table 4 show the superior performance of closed-source models, with Gemini-1.5-Flash achieving the highest ACC (55.6%). Open-source models like InternVL2.5-38B also perform well (ACC: 48.9%). As shown in Table 5, Gemini2.0-Flash excels again, achieving the highest average ACC (72.0%). Among open-source models, InternVL2.5-78B leads with an ACC of 70.6%. Table 6 shows Gemini-2.0-Flash as the overall top performer (Avg.: 62.3%), followed by Gemini-1.5-Flash (Avg.: 61.3%). Open-source models like InternVL2.5-38B show promise with an average score of 54.4%. Closed-source models outperform open-source models, especially on complex tasks."}, {"title": "Analysis on Generation Metric", "content": "Table 7 shows cosine similarity and Pearson correlation results for traditional metrics (BLEU-4, ROUGE-L, BERTScore) and the open-source LLM Qwen2.5-72B-Instruct with human judgments. Among traditional metrics, BERTScore showed the highest consistency with human evaluation (cosine similarity: 0.8762, Pearson correlation: 0.3199), outperforming BLEU-4 and ROUGE-L. Notably, Qwen2.5-72B-Instruct achieved superior results (cosine similarity: 0.9353, Pearson correlation: 0.4042), highlighting its ability to better align with human assessments in laughter reasoning tasks. It reveal the limitations of traditional"}, {"title": "Analysis on Class-wise Performance", "content": "Figure 3 shows the class-wise performance of Gemini-2.0-Flash across three scenarios: Foundational Emotion Recognition (FER), Conversational Emotion Understanding (CEU), and Socially Complex Emotion Analysis (SCEA). In FER (SOER, SPER, SCEA, OSA, EIA), the model performs well on primary emotions like \u201cangry\u201d and \u201cneutral\u201d, as well as sentiments like \u201cpositive\u201d and \u201cnegative\u201d. However, it struggles with subtle or overlapping emotions, such as \u201cfearful\u201d and \u201ccalm\u201d, and distinctions between \u201cneutral\u201d and"}, {"title": "Comparison with Human Performance", "content": "Table 8 compares the performance of MLLMs and human evaluators across three dimensions of EmoBench-M: FER, CEU, and SCEA, along with the average score (Avg.). Among the mod-"}, {"title": "Stability Analysis of MLLM", "content": "We analyze the stability of Gemini-2.0-Flash across three dimensions: FER, CEU, and SCEA. Stability is evaluated by running predictions 1, 3, and 5 times, using majority voting for final results (shown in Table 9). In FER, scores are 61.4, 61.2, and 61.0, with the highest at 61.4. For CEU, scores improve from 53.4 to 54.1 across iterations. In SCEA, results are 72.0, 72.5, and 72.8, considering only HU and SD scenarios due to the generative nature of this task. These results demonstrate the model's robust stability, with minor variations and improvements in complex emotional contexts. More details can be found in Appendix D."}, {"title": "Conclusion and Future Work", "content": "We introduced EmoBench-M to evaluate the EI of MLLMs across three key areas: Foundational Emotion Recognition, Conversational Emotion Understanding, and Socially Complex Emotion Understanding. Our results show that while MLLMS perform reasonably well on basic emotion recognition, they struggle significantly with conversational and socially complex emotional understanding. It reveals a crucial gap between MLLM capabilities and human-level EI in dynamic, multimodal settings. Future research can prioritize work that can focus on enhancing models' contextual understanding and social awareness in MLLMs. Key areas include: leveraging advanced dialogue modeling, incorporating social reasoning, leveraging mechanisms, and diverse datasets with encompassing varied cultural contexts, i. Improving multimodal fusion, integrating techniques, and embedding psychological principles directly into model design architecture can further advance EI capabilities."}, {"title": "Limitations", "content": "The limitation of our study is the exclusion of MLLMs specifically fine-tuned on emotion-centric datasets. We focus on evaluating general-purpose MLLMs to understand their EI capabilities in realistic applications, where models are expected to generalize across diverse tasks and domains. This aligns with our objective of assessing the broader applicability of MLLMs in real-world scenarios rather than focusing on specialized models designed for emotion computation."}, {"title": "Ethical Considerations", "content": "In this study, we exclusively utilized publicly available datasets with open-source links. Furthermore, we ensured compliance with the licensing agreements associated with these datasets by formally obtaining usage permissions where required. As a result, our research does not raise any ethical concerns regarding data usage or handling."}, {"title": "Details and Case of Datasets", "content": "This section provides an overview of the datasets used in our experiments, highlighting the number of test samples and the corresponding labels associated with each dataset. Table 11 summarizes this information, covering a wide range of emotional and intent-based annotations across various modalities. Visual examples for each dataset are provided in Figure 4-16.\n\u2022 RAVDESS (song & speech): The RAVDESS dataset includes both speech and song audio files annotated with emotions such as neutral, calm, happy, sad, angry, and fearful. The speech subset contains additional labels, including surprised and disgust.\n\u2022 CMU-MOSI and CMU-MOSEI: These datasets are designed for multimodal sentiment analysis and include three sentiment labels: neutral, positive, and negative.\n\u2022 FMSA-SC: This dataset captures finegrained sentiment annotations with labels such as weak negative, strong negative, neutral, weak positive, and strong positive.\n\u2022 MER2023: Designed for emotion recognition, this dataset provides six emotion categories: happiness, sadness, anger, surprise, neutral, and calm.\n\u2022 CH-SIMSv2 and CH-SIMS: These datasets, used for sentiment analysis in Chinese, are annotated with three labels: neutral, negative, and positive.\n\u2022 MC-EIU: This dataset offers both emotion annotations (happy, surprise, sad, disgust, anger, fear, neutral) and intent annotations (questioning, agreeing, acknowledging, encouraging, consoling, suggesting, wishing, neutral).\n\u2022 MELD: This multimodal dataset includes seven emotion labels: neutral, surprise, fear, sadness, joy, disgust, and anger.\n\u2022 UR-FUNNY and MUSTARD: Both datasets are binary-labeled for humor detection, with annotations of true or false.\n\u2022 SMILE: A small-scale dataset focusing on humor comprehension, annotated with explanations of why the audience laughed."}, {"title": "Model Configuration", "content": "The configuration details of the models evaluated on EmoBench-M are summarized in Table 10. The table provides a detailed comparison of key hyperparameters, including top-p and top-k sampling values, temperature settings, and VRAM requirements. For models accessed via APIs, the VRAM is denoted as \u201cAPI\u201d, reflecting their closed-source nature and cloud-based deployment. The evaluated models encompass a range of architectures, from smaller-scale models, such as InternVL2.5-4B (Chen et al., 2024), to largescale variants like InternVL2.5-78B (Chen et al., 2024). Noteworthy configurations include VideoLLaMA2.1-AV-7B (Cheng et al., 2024), which incorporates audiovisual processing, and Gemini-2.0Flash-Thinking (Team et al., 2024), which features enhanced reasoning capabilities. Most models exhibit consistent sampling parameters (e.g., top-p and temperature), ensuring a standardized basis for comparison. The VRAM requirements reflect the computational demands of these models, ranging from 14 GB for smaller models to 168 GB for the largest configurations. In contrast, API-based models provide an accessible option for users, particularly when local resources are constrained, albeit at the cost of reduced transparency due to their closed-source nature."}, {"title": "Details on Comparison with Human Performance", "content": "Table 12 provides a detailed comparison of performance metrics between MLLMs and human participants across various evaluation scenarios."}, {"title": "Details on Stability Analysis of MLLM", "content": "Table 13 shows the results of stability experiments conducted on Gemini-2.0-Flash. The analysis evaluates the model's performance across different numbers of prediction iterations (1, 3, and 5), with the final output determined through a majority vot-"}, {"title": "More Analysis on Class-wise Performance", "content": "For class-wise performance, confusion matrices for more models can be found in Figure 17-32."}, {"title": "Prompt", "content": null}, {"title": "Prompt From RAVDESS(song)", "content": "Please watch the provided video and determine the emotion it conveys. Do not provide any additional explanations or extra content. Choose one of the following labels as your final answer: neutral, calm, happy, sad, angry, fearful. Respond in the format: {'emotion': 'label' }."}, {"title": "Prompt From RAVDESS(speech)", "content": "Please watch the provided video and determine the emotion it conveys. Do not provide any additional explanations or extra content. Choose one of the following labels as your final answer: neutral, calm, happy, sad, angry, fearful, surprised, disgust. Respond in the format: {'emotion': 'label' }."}, {"title": "Prompt From CH-SIMSv2", "content": "The person in video says: {Subtitle}. Determine the emotion conveyed. Do not provide any additional explanations or extra content. Choose one of the following labels as your final answer: neutral, negative, positive. Respond in the format: {'emotion': 'label' }."}, {"title": "Prompt From FMSA-SC", "content": "The person in video says: {Subtitle}. Determine the emotion conveyed. Do not provide any additional explanations or extra content. Choose one of the following labels as your final answer: weak negative, strong negative, neutral, strong positive, weak positive. Respond in the format: {'emotion': 'label' }."}, {"title": "Prompt From UR-FUNNY", "content": "The context sentences in the video is: {context sentences}. The punchline sentence in the video is: {punchline sentence}. Choose one of the following labels as your final answer: true, false."}, {"title": "Prompt From MC-EIU", "content": "The person in video says: {Subtitle}. Analyze the emotion and intent. Choose one emotion: happy, surprise, sad, disgust, anger, fear, and neutral. Choose one intent: questioning, agreeing, acknowledging, encouraging, consoling, suggesting, wishing, and neutral. Respond in the format: {'emotion label': 'label', 'intent label': 'label' }."}, {"title": "Prompt From MELD", "content": "The person in video says: {subtitle}. Do not provide any additional explanations or extra content. Choose one of the following labels as your final answer: neutral, surprise, fear, sadness, joy, disgust, anger. Respond in the format: {'emotion': 'label' }"}, {"title": "Prompt From MER2023", "content": "The person in video says: {Subtitle}. Do not provide any additional explanations or extra content. Choose one of the following labels as your final answer: happy, sad, neutral, angry, worried, surprise. Respond in the format: {'emotion': 'label' }."}, {"title": "Prompt From CMU-MOSI", "content": "The person in video says: {Subtitle}. Do not provide any additional explanations or extra content. Choose one of the following labels as your final answer: neutral, negative, positive. Respond in the format: {'emotion': 'label' }."}, {"title": "Prompt From CMU-MOSEI", "content": "The person in video says: {Subtitle}. Do not provide any additional explanations or extra content. Choose one of the following labels as your final answer: neutral, negative, positive. Respond in the format: 'emotion': 'label'."}, {"title": "Prompt From MUSTARD", "content": "The person in the video says: {Subtitle}. Does this statement express sarcasm? Do not provide any additional explanations or extra content. Choose one of the following labels as your final answer: true, false."}, {"title": "Prompt From SMILE", "content": "The person in video says: {Subtitle}. Choose one of the following labels as your final answer: neutral, negative, positive. Respond in the format: 'emotion': 'label'."}, {"title": "Prompt From CH-SIMS", "content": "Reasoning task: you are to answer why the audience laughed given the video clip. The video clip from the Sitcom, titled {video title}, with multimodal information (Utterance, Facial Action Units, Video caption, Acoustic features(6 dimension; 1.mean of F0 contour, 2.var of F0 contour, 3. mean of energy contour, 4. var of energy contour, 5. jitter, 6. shimmer)) is given. The audience laughing moment is marked as (audience laughing) in certain utterance Explain why the audience laughed given the video clip, at most 40 words, starting with 'The audience laughed because '. Given video clip: {query}."}, {"title": "Evaluation Prompt for SMILE Dataset", "content": "You need to evaluate the quality of a model-generated reasoning for why a video audience laughed. You will be provided with two reasons for laughter reasoning: 1. The reason for laughter generated by the model. 2. The reference reason for laughter annotated manually (as a benchmark). Please score based on the following dimension, with a maximum of 5 points: Logical Judgment Dimension: Based on the reference reason, evaluate the model-generated reason in terms of logical clarity, the rationality of the causal chain, and coherence with the context.\nScoring Criteria: \u2022 1 Point: The reasoning lacks logic, with unclear or missing causal relationships, and is incoherent with the context. \u2022 2 Points: The reasoning has some logical flaws and partial causal connections but is largely incoherent with the context. \u2022 3 Points: The reasoning is moderately logical, with clear causal links, though some minor inconsistencies with the context exist. \u2022 4 Points: The reasoning is mostly logical, with well-defined causal relationships and strong coherence with the context. \u2022 5 Points: The reasoning is fully logical, with clear and rational causal chains and excellent coherence with the context.\nInput: \u2022 Reference Reason:  \u2022 Generated Reason:  \nOutput Format: Please strictly follow the format below to output the scoring result, and only output the scoring result without adding any additional explanations or text: Logical Judgment Dimension:"}, {"title": "Prompt from Multimodal Content Association Dimension Evaluation Criteria for Model-Generated Reasoning", "content": "You need to evaluate the quality of a model-generated reasoning for why a video audience laughed. You will be provided with two reasons for laughter reasoning: 1. The reason for laughter generated by the model. 2. The reference reason for laughter annotated manually (as a benchmark). Please score based on the following dimension, with a maximum of 5 points: Multimodal Content Association Dimension: Based on the reference reason, evaluate whether the generated text accurately reflects the interactions between language, visual, audio, and other modal contents, especially whether these contents are consistent with the triggers for laughter.\nScoring Criteria: \u2022 1 Point: The reasoning fails to associate with multimodal content, showing no consistency with language, visual, audio, or other modalities. \u2022 2 Points: The reasoning shows minimal association with multimodal content, with limited consistency and several mismatches. \u2022 3 Points: The reasoning moderately reflects multimodal interactions, maintaining some consistency but with noticeable gaps. \u2022 4 Points: The reasoning strongly associates with multimodal content, showing clear consistency with most language, visual, audio, and other modalities. \u2022 5 Points: The reasoning perfectly captures and reflects the interactions between all relevant multimodal contents, with complete consistency with the triggers for laughter.\nInput: \u2022 Reference Reason:  \u2022 Generated Reason:  \nOutput Format: Please strictly follow the format below to output the scoring result, and only output the scoring result without adding any additional explanations or text: Multimodal Content Association Dimension:"}]}