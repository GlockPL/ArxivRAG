{"title": "On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M Microcontrollers", "authors": ["Mark Deutel", "Frank Hannig", "Christopher Mutschler", "J\u00fcrgen Teich"], "abstract": "On-device training of DNNs allows models to adapt and fine-tune to newly collected data or changing domains while deployed on microcontroller units (MCUs). However, DNN training is a resource-intensive task, making the implementation and execution of DNN training algorithms on MCUs challenging due to low processor speeds, constrained throughput, limited floating-point support, and memory constraints. In this work, we explore on-device training of DNNs for Cortex-M MCUs. We present a method that enables efficient training of DNNS completely in place on the MCU using fully quantized training (FQT) and dynamic partial gradient updates. We demonstrate the feasibility of our approach on multiple vision and time-series datasets and provide insights into the tradeoff between training accuracy, memory overhead, energy, and latency on real hardware.", "sections": [{"title": "I. INTRODUCTION", "content": "ADAPTING, fine-tuning, or retraining deep neural net- works (DNNs) on microcontroller units (MCUs) to im- prove alongside a changing input domain or application is not a straightforward task [1]. Typically, once the DNN is deployed on the MCU, its trainable weights are stored read-only in Flash, and adapting the DNN without reprogramming the MCU is often not feasible. In addition, the limited memory and computational resources on the MCU require that the weights and feature maps of a DNN are often stored in quan- tized form, which further complicates regular DNN training normally performed in floating-point. Moreover, DNN training itself adds significant memory and computational overhead, making on-device training on MCUs for DNNs of meaningfull size and complexity even more challenging.\nHowever, training DNNs on MCUs has a number of highly desirable properties, such as (a) a reduced communication of the MCU with remote training servers, since no data recorded by the MCU needs to be sent away for offline training, (b) increased privacy, since training with data from the input domain, which may contain personally identifiable information such as speech or likeness, can be performed directly on the MCU without any persistent storage of data on a remote server, (c) energy efficiency, since MCUs have an energy footprint several orders of magnitude smaller than any GPU- based server, and (d) in place training with zero downtime, since DNN training can be performed on the fly alongside the regular inference of recorded samples on the MCU."}, {"title": "A. Challenges", "content": "Supervised training of DNNs relies on two algorithms: (a) backpropagation (BP), which is used to compute local gradi- ents for each intermediate layer of the DNN based on the chain rule, and (b) stochastic gradient descent (SGD), which is used to update the trainable weights of each layer based on the local gradients calculated by BP. Usually, the gradients used by SGD are stochastically approximated by accumulating them over a small subset of samples (minibatches) of all available training samples. The combination of BP and SGD is well-known, versatile, and the backbone of all major AI advances of recent years. However, it is also extremely memory- and compute- intense, resulting in DNN training usually being performed on larger server systems and with computing accelerator cards like GPUs. This results in major challenges that need to be addressed to enable DNN training on regular MCUs.\nThe high computational cost of DNN training lies in the \"backward pass\", i.e., the BP and SGD algorithms, that must be executed for each training sample in addition to the \"forward pass\", i.e., the inference. The backward pass is responsible for propagating the error computed for the last layer of the DNN by a loss function to calculate local errors for all intermediate layers. As a result, for each layer, at least one partial derivative with respect to its input during the forward pass must be computed, while for all trainable layers another partial derivative with respect to their weights must also be computed to facilitate the parameter updates performed by SGD. For example, for linear layers and when using minibatches, this results in two additional transposed matrix multiplications, while for convolutional layers, two additional \"transposed convolutions\" have to be performed in addition to the forward pass. In conclusion, for trainable layers in a DNN, the added computational cost of performing their backward pass is about twice as high as the computational cost of performing their forward pass.\nThe high memory overhead of DNN training is the result of four aspects: First, the partial derivatives of most layers commonly used in DNNs depend on intermediate tensors used in the forward pass of the network, see Fig. 1 for a schematic overview. If only inference is performed, these tensors are usually short-lived because they only temporarily store data between successive layers. Therefore, to save RAM, many inference libraries for MCUs try to optimize memory requirements by reusing areas on the heap for multiple tensors during an inference pass [2], [3]. However, when performing backpropagation, many of these intermediate results need to be kept in memory longer, since they are required by operations"}, {"title": "B. Contribution", "content": "To address the challenges outlined in Section I-A and to make on-device training on tiny devices such as Cortex-M MCUs feasible, we propose the following contributions in this paper:\nWe introduce (i) an 8-bit fully quantized training (FQT) algorithm that quantizes both weights and intermediate feature tensors during the forward pass and errors and gradients tensors during the backward pass by using the same linear quantization scheme also used for regular DNN inference. As a result, our approach enables on-device DNN training on MCUs online, in place, and without any transformations, changes, or conversions to the DNN's representation in memory or code between inference and training.\nFurthermore, to allow FQT while still being able to achieve stable training results similar to regular floating-point training on GPUs, we propose (ii) a memory-efficient version of minibatching with standardized gradients as well as (iii) a method to optimize the distribution of values in the 8-bit range of quantized tensors by adapting the quantization parameters of trained weight tensors dynamically during training, similar to quantization-aware training (QAT) [4].\nTo further reduce the computational overhead of our ap- proach, we employ (iv) a dynamic gradient update strategy that is fully compatible with our FQT method, which prunes the computational tree of BP per training sample based on an online computed magnitude of error based heuristic.\nFinally, we demonstrate the flexibility of our approach for both transfer learning and full DNN training for multiple datasets (11 in total), and provide insights into the trade-off between memory, latency, energy, and DNN accuracy for three Cortex-M-based MCUs, see Tab. II.\nThe rest of this paper is structured as follows. Section II outlines work on on-device training of DNNs on MCUs related to ours. In Section III we discuss our approach and its implementation in more detail. In Section IV we present our evaluation and then conclude in Section V."}, {"title": "II. RELATED WORKS", "content": "On-device training on MCUs is gaining popularity as it promises a much simpler and more energy-efficient solution for DNN fine-tuning and adaptation than training on a remote server and then redeploying the model. As a result, much research on on-device training so far has focused on achiev- ing a better trade-off between memory overhead, computing resource requirements, and accuracy.\nA popular approach to reduce memory utilization is to recompute intermediate results from the forward pass, which are required by the backward pass, instead of keeping them in memory [5], [6]. While this helps to minimize the additional memory overhead introduced by training, it does so at the cost of a significant additional computational overhead due to multiple computations of results. Another proposed approach is to reduce the memory footprint of intermediate activation tensors by constructing a dynamic and sparse computational graph [7]. However, the approach as presented by the authors is only designed to minimize activations of the forward pass and still uses a matrix-vector product, albeit reduced in size, to decide which part of the computational graph to eliminate. Other approaches focus on a hierarchical approach with dif- ferent levels of granularity at which to drop computations, such as the data, model, and algorithm levels [8]. Finally, reducing memory utilization during both inference and training by processing both activations and weights quantized has also been proposed [9], [10].\nDue to the limited memory and computational resources on embedded devices, the most common DNN training tasks addressed by researchers so far are transfer learning and fine- tuning tasks, where an already well-trained DNN is adapted to a changed input domain or application. The reason for the popularity of these tasks is that they usually do not require training of a DNN from scratch, but only fine-tuning of its last layers. For example, Tiny-transfer-learning [11] freezes the weights of the DNN and trains only the biases, for"}, {"title": "III. CONCEPTS", "content": "To make supervised training feasible on MCUs, we propose a training framework centered around two techniques that we will discuss below: Fully quantized training (FQT), see section III-A, and dynamic sparse gradient updates, see section III-\u0412."}, {"title": "A. Fully Quantized Training", "content": "Typically, DNNs deployed on a MCU for inference are quantized using a linear quantization. This is done to save memory and because integer arithmetic can be performed more efficiently on most MCUs than floating-point arithmetic. Usually, quantization is implemented per tensor in the form of vq = [ s \u00b7 v ] + z , where s and z are two parameters scale and zero point derived from the distribution of floating-point values in the original unquantized tensor. Therefore, in order to train a DNN in place without any conversion, training must be performed using the same quantization scheme already estab- lished for inference. In addition, this has the advantage that all optimizations already established for quantized inference can be reused for training.\nA well-established method for training quantized DNNs on GPU-based systems is called quantization-aware training (QAT) [4]. However, while the method helps to train DNNs robust to additional errors introduced by quantization, the DNN is not actually trained quantized, but still processed in floating-point arithmetic, with quantization only simulated, see Fig. 2a for a schematic overview. As a result, in QAT, the DNN is not fully transferred to quantized space until training is complete. Therefore, QAT does not provide any immediate memory or computational savings for the training process itself. In addition, using QAT would require a costly conversion process at the beginning and end of training.\nTo overcome the problems of QAT, and to achieve our goal of in place, online training on fully quantized DNNs, we propose a training method based on FQT, facilitating training directly with the quantized weights of a DNN and using the already established quantization scheme for inference, see Fig. 2b. As a result, our method is flexible, since there is no difference in how the DNN is represented in memory and code between inference and training. Therefore, the same forward pass of a DNN executed on-device using our method can be used for both regular inference and training at the same time. Furthermore, since both the forward and backward pass of our training method are performed on quantized weights, feature maps, errors and gradients, the method is significantly more memory-efficient than traditional floating-point based training.\nBP for DNNs is based on the chain rule, and calculation of local errors and gradients for both linear and convolutional layers can be described as\n\nE_{n-1} = W_{n}^{T} E_{n} (1)\n\u2207W_{n} = E_{n} X_{n}^{T} (2)\n\nwith the operator either being a matrix multiplication or a 2D-convolution when using minibatches. Therefore, Eq. (1) calculates the error E_{n-1} of the n \u2212 1th layer of the DNN, given the error E_{n} and weight W_{n} of the nth layer, and Eq. (2) calculates the gradient \u2207W_{n} used to update the weights of the nth layer given the error E_{n} and the input of the nth layer X_{n}, see also Fig. 1. Note that both Eq. (1) and (2) are based on the same mathematical operation performed during the forward pass of the layer\n\nY_{n} = W_{n}X_{n} (3)"}, {"title": "B. Dynamic Sparse Gradient Updates", "content": "While quantized BP can help overcome the high memory overhead of BP, it does not directly contribute to reducing the algorithm's computational complexity\u00b9. Therefore, we propose Dynamic Sparse Gradient Updates as a technique that can be\nNote that from a practical standpoint, just using FQT often already reduces the latency per training sample, since many MCUs can compute more efficiently with integers than with floats"}, {"title": "IV. EVALUATION", "content": "We evaluate our approach on two training tasks: Transfer learning, see Secs. IV-A, IV-B, and IV-C, and full DNN training, see Sec. IV-D. We consider three DNN configurations for all experiments: uint8 where the complete DNN is fully quantized, mixed where the DNN is quantized except its classification head that remains in floating-point, and float32 where the DNN remains completely in its original floating- point representation as a reference. Since we observed in our experiments that especially the quantization of tensors in the last layers of the DNN can significantly affect on-device training, we chose the mixed configurations as another trade- off between accuracy and resource utilization besides FQT. For all three DNN configurations, we used the micromod\u00b2 platform and considered three MCUs for testing: IMXRT2062, nrf52840, and RP2040, see Tab. II for detailed information."}, {"title": "A. On-Device Transfer Learning", "content": "To validate the effectiveness of our on-device FQT ap- proach, we performed transfer learning on seven different datasets, see Tab. I for a detailed overview. We chose problems of different complexity, input size and domains to show the versatility of our approach in enabling stable FQT."}, {"title": "B. Transfer Learning Across MCUs", "content": "As an additional experiment to the transfer learning task, and to get an understanding of the performance and constraints of FQT across different MCU platforms, we deployed the daliac and cwru datasets on the nrf52840 and RP2040 MCU and compared our findings with the results we observed on the IMXRT2062, see Fig. 5. We only deployed the two datasets and not all seven because they were the only two that met the memory constraints on all three MCUs. As expected, processing on the IMXRT2062 is significantly faster than on the other Cortex-M4 and Cortex-M0+ based systems due to the much higher clock speed of the IMXRT2062, see Fig. 5a. However, when comparing the RP2040 to the nrf52840 alone, even though the RP2040 has a higher clock speed, the nrf52840 was able to process training samples faster. This is because the nrf52840 has a dedicated floating- point unit (FPU) and implements the DSP instruction set"}, {"title": "C. Dynamic Sparse Gradient Updates", "content": "We validate the ability of our partial gradient updating approach to reduce the computational complexity of back- propagation by comparing the training accuracy achieved with different dynamic gradient update rate configurations, see Fig. 6, with the results of our experiments presented in Section IV-A, where we used full gradient updates. We show the test accuracy for three different \\lambda_{min} (blue: \\lambda_{min} = 1.0, orange: \\lambda_{min} = 0.5, and green: \\lambda_{min} = 0.1) for all DNN configurations and for all seven datasets considered in our transfer learning experiments, see Figs. 6a, 6b, and 6c. For all experiments, we performed on-device training in combination with dynamic gradient updates for 20 epochs, and we repeated each training 5 times and provide averaged results.\nFor both floating-point and mixed transfer learning (Figs. 6a and 6b), both \\lambda_{min} = 0.5 and \\lambda_{min} = 0.1 achieved the same level of accuracy after 20 epochs as \\lambda_{min} = 1.0, showing that, if chosen appropriately, only a small fraction of gradients need"}, {"title": "D. Complete On-Device Training", "content": "To evaluate that our FQT approach can also be used to train DNNs on MCUs completely from scratch, we used a smaller DNN pre-trained on the MNIST dataset [30], which we then fully retrained, i.e., all layers, on the device for four"}, {"title": "E. Comparison with MCUNet-5FPS", "content": "We compare the performance of our FQT optimizer with the results reported by the authors of the optimizer SGD+M+QAS [10] when training the last two blocks of MCUNet-5FPS [2], see Tab. IV. The first row in Tab. IV shows the baseline accuracy reported by [10] for MCUNet trained on eight datasets using a floating-point SGD optimizer with momentum (SGD-M). We could reproduce these results using the publicly available sources of MCUNet provided by the authors. The next two rows show the results obtained by [10] first for naively using quantized SGD-M without any changes besides the usage of quantized tensors and second for their SGD+M+QAS approach, which addresses shortcomings of the SGD-M approach. The last row shows the results we obtained for retraining MCUNet using our approach, i.e., fully quantized training with standardized gradients and dynamic adaptation of the zero-point and scale parameters as described in Sec. III. All our reported results are on unseen test splits of the datasets. Like [10] we retrained for 50 epochs. We used a learning rate of 0.001 and a batch size of 48."}, {"title": "V. CONCLUSION", "content": "In this work, we discussed the challenges of on-device training on Cortex-M microcontrollers, such as computational overhead and memory constraints, and proposed a method to overcome them. To this end, we proposed a combination of fully quantized on-device training (FQT) combined with dynamic partial gradient updates to reduce compute overhead by dynamically adapting to the distribution and magnitude of gradients values during backpropagation. In an extensive evaluation, focusing on three different Cortex-M-based MCUs, we show that on-device transfer learning and full on-device training are both feasible from an accuracy and resource utilization standpoint. Therefore, our results indicate that on- device MCU training is possible for a wide range of datasets. Furthermore, our method is flexible and allows for training configurations, where some layers of a DNN are trained in floating-point, while others are trained quantized. This solidifies the capability of our method in adapting to problems of varying complexity and to different DNN architectures."}]}