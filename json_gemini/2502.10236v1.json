{"title": "SHAPING INDUCTIVE BIAS IN DIFFUSION MODELS\nTHROUGH FREQUENCY-BASED NOISE CONTROL", "authors": ["Thomas Jiralerspong", "Berton Earnshaw", "Jason Hartford", "Yoshua Bengio", "Luca Scimeca"], "abstract": "Diffusion Probabilistic Models (DPMs) are powerful generative models that have\nachieved unparalleled success in a number of generative tasks. In this work, we\naim to build inductive biases into the training and sampling of diffusion models to\nbetter accommodate the target distribution of the data to model. For topologically\nstructured data, we devise a frequency-based noising operator to purposefully\nmanipulate, and set, these inductive biases. We first show that appropriate ma-\nnipulations of the noising forward process can lead DPMs to focus on particular\naspects of the distribution to learn. We show that different datasets necessitate\ndifferent inductive biases, and that appropriate frequency-based noise control in-\nduces increased generative performance compared to standard diffusion. Finally,\nwe demonstrate the possibility of ignoring information at particular frequencies\nwhile learning. We show this in an image corruption and recovery task, where we\ntrain a DPM to recover the original target distribution after severe noise corruption.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion Probabilistic Models (DPMs) have recently emerged as powerful tools for approximating\ncomplex data distributions, finding applications across a variety of domains, from image synthesis to\nprobabilistic modeling (Yang et al., 2024; Ho et al., 2020b; Sohl-Dickstein et al., 2015; Venkatraman\net al., 2024; Sendera et al., 2024). These models operate by gradually transforming data into noise\nthrough a defined diffusion process and training a denoising model (Vincent et al., 2008; Alain\n& Bengio, 2014) to learn to reverse this process, enabling the generation of samples from the\ndesired distribution via appropriate scheduling. Despite their success, the inductive biases inherent\nin diffusion models remain largely unexplored, particularly in how these biases influence model\nperformance and the types of distributions that can be effectively modeled.\nInductive biases are known to play a crucial role in deep learning models, guiding the learning process\nby favoring certain types of data representations over others (Geirhos et al., 2019; Bietti & Mairal,\n2019; Tishby & Zaslavsky, 2015). A well-studied example is the Frequency Principle (F-principle)\nor spectral bias, which suggests that neural networks tend to learn low-frequency components of data\nbefore high-frequency ones (Xu et al., 2019; Rahaman et al., 2019). Another related phenomenon is\nwhat is also known as the simplicity bias, or shortcut learning (Geirhos et al., 2020; Scimeca et al.,\n2021; 2023b), in which models are observed to preferentially pick up on simple, easy-to-learn, and\noften spuriously correlated features in the data for prediction. If left implicit, it is often unclear\nwhether these biases will improve or hurt the performance of generative model on downstream"}, {"title": "2 METHODS", "content": "task, and they could lead to flawed approximations(Scimeca et al., 2023a). In this work, we aim to\nexplicitly tailor the inductive biases of DPMs to better learn the target distribution of interest.\nRecent studies have begun to explore the inductive biases inherent in diffusion models. For instance,\nKadkhodaie et al. (2023) analyze how the inductive biases of deep neural networks trained for image\ndenoising contribute to the generalization capabilities of diffusion models. They demonstrate that\nthese biases lead to geometry-adaptive harmonic representations, which play a crucial role in the\nmodels' ability to generalize beyond the training data (Kadkhodaie et al., 2023). Similarly, Zhang et\nal. (2024) investigate the role of inductive and primacy biases in diffusion models, particularly in\nthe context of reward optimization. They propose methods to mitigate overoptimization by aligning\nthe models' inductive biases with desired outcomes (Zhang et al., 2024). Other methods, such as\nnoise schedule adaptations (Sahoo et al., 2024) and the introduction of non-Gaussian noise (Bansal\net al., 2022) have shown promise in improving the performance of diffusion models on various tasks.\nHowever, the exploration of frequency domain techniques within diffusion models is a relatively\nnew area of interest. One of the pioneering studies in this domain investigates the application of\ndiffusion models to time series data, where frequency domain methods have shown potential for\ncapturing temporal dependencies more effectively (Crabb\u00e9 et al., 2024). Similarly, the integration\nof spatial frequency components into the denoising process has been explored for enhancing image\ngeneration tasks (Qian et al., 2024; Yuan et al., 2023), showcasing the importance of considering\nfrequency-based techniques as a means of refining the inductive biases of diffusion models.\nIn this work, we explore a new avenue, to build inductive biases in DPMs by frequency-based noise\ncontrol. The main hypothesis in this paper is that the noising operator in a diffusion model has a direct\ninfluence on the model's representation of the data. Intuitively, the information erased by the noising\nprocess is the very information that the denoising model has pressure to learn, so that reconstruction\nis possible. Accordingly, we propose that by strategically manipulating the noising operation, we\ncan effectively steer the model to learn particular aspects of the data distribution. We focus our\nattention to the generative learning of topologically structured data, and propose an approach that\ninvolves designing a frequency-based noise schedule that selectively emphasizes or de-emphasizes\ncertain frequency components during the noising process. In this paper, we refer to our approach\nas frequency diffusion. Because the Fourier transform of a Gaussian is just another Gaussian in the\nfrequency domain, this approach allows us to maintain the Gaussian assumptions of the diffusion\nprocess while reorienting the noising operator within the frequency domain, enabling the generation\nof Gaussian noise at different frequencies and thereby influencing the model's learning trajectory.\nWe report several findings. First, we show that when the information content in the data lies more\nheavily in particular frequencies, frequency diffusion yields better samplers. Furthermore, we test\nthis in several natural datasets, and show that depending on the dataset characteristic, different\nsettings of our frequency diffusion approach yield optimal results, often with comparable or superior\nperformance to standard diffusion. Finally, we show that through frequency-denoising we can recover\ncomplex distributions after severe noise corruption at particular frequencies, opening interesting\nvenues for applications within the generative landscape.\nWe summarize our contributions as follows:\n1. We introduce a frequency-informed noising operator that can shape the inductive biases of\ndiffusion models.\n2. We empirically show that frequency diffusion can steer models to better approximate infor-\nmation at particular frequencies of the underlying data distribution.\n3. We provide empirical evidence that models trained with frequency-based noise schedules\ncan outperform traditional diffusion schedules across multiple datasets.\n4. We show that through frequency-denoising we can recover complex distributions after severe\nnoise corruption at particular frequencies."}, {"title": "2.1 DENOISING PROBABILISTIC MODELS (DPMS)", "content": "Denoising Probabilistic Models, are a class of generative models that learn to reconstruct complex\ndata distributions by reversing a gradual noising process. DPMs are characterized by a forward and\nbackward process. The forward process defines how data is corrupted, typically by Gaussian noise,\nover time. Given a data point xo sampled from the data distribution q(x0), the noisy versions of the\ndata X1, X2, ..., xy are generated according to:\n$q(x_t | x_{t-1}) = N(x_t; \\sqrt{\\alpha_t}x_{t-1}, (1 - \\alpha_t)I)$\nwith a variance schedule. The reverse process models the denoising operation, attempting to recover\nXt-1 from xt:\n$p_\\theta(x_{t-1} | x_t) = N(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2I)$,\nwhere $\u03bc\u0473(xt, t)$ is predicted by a neural network $f_\u03b8$, and the variance $\u03c3^2_t$ can be fixed, learned, or\nprecomputed based on a schedule. We often train the denoising model by minimizing a variational\nbound on the negative log-likelihood:\n$L = E_{t,x_0, \\epsilon} [||\\epsilon - \\epsilon_\\theta(x_t,t)||^2]$\nwhere e is the Gaussian noise added to xo, and ee is the model's prediction of this noise. To generate\nnew samples, we sample from a Gaussian distribution and apply the learned reverse process iteratively,\noften starting from a sample drawn from a simple Gaussian noise distribution."}, {"title": "2.2 FREQUENCY DIFFUSION", "content": "The objective of this section is to generate spatial Gaussian noise whose frequency content can\nbe systematically manipulated according to an arbitrary weighting function. In subsection 2.1, we\ndescribe how x, is obtained from X1-1 by adding Gaussian noise sampled from a normal distribution\nto the sample at time step t 1. Specifically, we can sample et ~ N(0, I) and obtain xt as:\n$x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1 - \\alpha_t} \\epsilon$.\nLet us denote by x \u2208 RH\u00d7W an image (or noise field) in the spatial domain, and by F the two-\ndimensional Fourier transform operator. We let Nfreq \u2208 CH\u00d7W be a complex-valued random field\nwhose real and imaginary parts are i.i.d. Gaussian:\n$N_{freq} = N_{real} + i N_{imag}, N_{real}, N_{imag} \\sim N(0, I)$."}, {"title": "2.3 FREQUENCY NOISE OPERATORS", "content": "In this work, the design of w (f) is especially important. In this section, we propose several alternatives,\nwhile showing empirical results on a particular choice of w(f).\nPOWER-LAW WEIGHTING.\nA natural alternative choice is the power-law weighting, expressed as:\n$w(f) = ||f||^\\alpha$,\nwhere f = (fx, fy) denotes a frequency coordinate, and the exponent a determines which frequencies\nare amplified or suppressed. Power-law weighting is popular in the modeling if natural phenomena\n(e.g., fractal landscapes, turbulence) where the energy distribution often follows an approximate\npower spectrum (Van der Schaaf & van Hateren, 1996).\n\u0395\u03a7\u03a1\u039f\u039d\u0395\u039d\u03a4\u0399AL DECAY WEIGHTING\nAnother alternative is an exponential decay function, defined as as:\n$w(f) = exp(-\\beta ||f||^2)$,\nwhere \u1e9e > 0, and frequencies with larger norms ||f|| are exponentially suppressed. This weighting\neffectively imposes spatial correlations, e.g. for \u1e9e close to 0 the function induces the retention of\nmore high-frequency components, while for large \u1e9e, the function quickly damps out high frequencies,\nresulting in a smoothing of the spatial domain.\nBAND-PASS MASKING AND TWO-BAND MIXTURE\nFinally, a band-pass mask can be viewed as a special case of a more general weighting function:\n$w(f) \\in {0,1}$.\nIn this case, the frequency domain is split into a set of permitted and excluded regions, or radial\nthresholds. We this, we can construct several types of filters, including a low-pass filter retaining only\nfrequencies below a cutoff (e.g., ||f|| \u2264 wc) a high-pass filter keeping only frequencies above a cutoff,\nor more generally a filter restricting ||f|| to lie between two thresholds [amin, bmax]. We thus define a\nsimple band pass filter as:\n$w(f) = M_{[a,b]}(f_x, f_y) =\\begin{cases}\n1, & \\text{if } a \\leq d(f_x, f_y) \\leq b, \\\\\n0, & \\text{otherwise}.\n\\end{cases}$"}, {"title": "2.4 DATASETS", "content": "Here, $d(f_x, f_y) = \\sqrt{(\\frac{f_x}{\\frac{H}{2}})^2+(\\frac{f_y}{\\frac{W}{2}})^2}$ measures the radial distance in frequency space. In this\nspecial case, w(f) is simply a binary mask, selecting only those frequencies within [a, b].\nFor the experiments in this paper we formulate a simple two-band mixture, where, we limit ourselves\nto constructing noise as a simple linear combination of two band-pass filtered noise components.\nSpecifically, as in the original band-based approach, we generate frequency-filtered noise ef via:\n$\\epsilon_f = \\gamma_l \\epsilon_{[a_l,b_l]} + \\gamma_h \\epsilon_{[a_h,b_h]}$,\nwhere y\u0131 and yn denote the relative contributions of a low- and a high-frequency noise components,\neach filtering noise respectively in the ranges [ai, b\u2081] (low-frequency range) and [an, bh] (high-\nfrequency range). We uniquely refer to \u20ac[a,b] as the noise filtered in the [a, b] frequency range\nfollowing Equation 6 and Equation 7. Standard Gaussian noise emerges as a particular instance (with\n\u03b3\u03b9 = 0.5, \u03b3\u03b7 = 0.5, a\u2081 = 0, b\u2081 = 0.5, an = 0.5, and bh = 1) of this formulation.\nFor the experiments, we consider five datasets, namely: MNIST, CIFAR-10, Domainnet-Quickdraw,\nWiki-Art and CelebA; providing examples of widely different visual distributions, scales, and domain-\nspecific statistics.\nMNIST:\nMNIST consists of 70, 000 grayscale images of handwritten digits (0-9) (Matthey et al.,\n2017). MNIST provides a simple test-bed to for the hypothesis in this work, as a well understood\ndataset with well structured, and visually coherent samples.\nCIFAR-10: CIFAR-10 contains 60,000 color images distributed across 10 object categories\n(Krizhevsky et al., 2009). The dataset is highly diverse in terms of object appearance, backgrounds,\nand colors, with the wide-ranging visual variations across classes like animals, vehicles, and other\ncommon objects.\nDomainNet-Quickdraw: DomainNet-Quickdraw features 120, 750 sketch-style images, These\nimages, drawn in a minimalistic, abstract style, present a distribution that is drastically different from\nnatural images, with sparse details and heavy visual simplifications.\nWikiArt: WikiArt consists of over 81, 000 images of artwork spanning a wide array of artistic styles,\ngenres, and historical periods (Saleh & Elgammal, 2015). The dataset encompasses a rich and varied\ndistribution of textures, color palettes, and compositions, making it a challenging benchmark for\ngenerative models, which must capture both the global structure and fine-grained stylistic variations\nthat exist across different forms of visual art.\nCelebA: CelebA contains 202, 599 images of celebrity faces, each 178 \u00d7 218 pixels in resolution\n(Liu et al., 2015). The dataset presents a diverse distribution of human faces with variations in pose,\nlighting, and facial expressions."}, {"title": "3 RESULTS", "content": "All experiments involve separately training and testing DPMs with various frequency diffusion\nschedules, as well as baseline standard denoising diffusion training. We use DDPM fast sampling\n(Ho et al., 2020a) to efficiently generate samples for all reported metrics. Across the experiments, we\nreport FID and KID scores as similarity score estimate metrics of the generated samples with respect\nto a held-out set of data samples. In all relevant experiments, we compute the metrics on embeddings\nfrom block 768 of a pre-trained Inception v3 model."}, {"title": "3.1 IMPROVED DIFFUSION SAMPLING VIA FREQUENCY-BASED NOISE CONTROL", "content": "In the first set of experiments, we wish to test our main hypothesis, i.e. that appropriate manipulation\nof the frequency components of the noise can better support the learning of the distribution of interest."}, {"title": "3.1.1 QUALITATIVE OVERVIEW", "content": "First, we show a qualitative example of a stan-\ndard linear noising schedule forward operation\nin Figure 2, as compared to two particular set-\ntings of our constant high and low-frequency lin-\near schedules of the band-pass filter. With stan-\ndard noise, information is uniformly removed\nfrom the image, with sample quality degrading\nevenly over time. In the high-frequency nois-\ning schedule, sharpness and texture are removed\nmore prominently, while in the low-frequency\nnoising schedule, general shapes and homoge-\nneous pixel clusters are affected most, yielding\nqualitatively different information destruction\noperations. As discussed previously, we hypoth-\nesize that this will in turn purposely affect the\nstatistics of the information learned by the de-\nnoiser model, effectively focusing the diffusion sampling process on different parts of the distribution."}, {"title": "3.1.2 LEARNING TARGET DISTRIBUTIONS FROM FREQUENCY-BOUNDED INFORMATION", "content": "We conduct experiments to learn the distribution\nof data where, by construction, the information\ncontent lies in the low frequencies. We use the\nCIFAR-10 dataset, and corrupt the original data\nwith high-frequency noise \u20ac[.3,1.], thus erasing\nthe high-frequency content while predominantly\npreserving the low-frequency details in the range\n\u20ac[0.,.3]. We train 9 diffusion models, including a\nstandard diffusion (baseline) model, and 8 mod-\nels trained with frequency-based noise control\nspanning 8 combinations of \u03b3\u03b9 (\u03b3\u03b7 = 1 \u2212 \u03b3\u03b9).\nWe repeat the experiment over three seeds and\nreport the average FID and error in Figure 3. In\nthe figure, we observe the DPMs trained with\nhigher amounts of low-frequency noise (higher\n\u03b3\u03b9) to perform significantly better than both the\nbaseline (\u03b3\u03b9 = 0.5), and higher frequency de-\nnoising models (lower y\u0131). Furthermore, we see a mostly monotonically descending trend in FID for\nincreasing values of lower frequency noise in the diffusion forward schedule, supporting the original\nintuition of how the frequency manipulation of the noising operator can directly steer the denoiser's\nlearning trends, and therefor how progressively higher amount of low-frequency forward noise aid in\nthe learning of samplers for data containing mostly low-frequency information."}, {"title": "3.1.3 FREQUENCY-BASED NOISE CONTROL IN NATURAL DATASETS", "content": "We further test our hypothesis by training 9 models for each of the datasets considered, inclusive\nof all y-variations of our two-band mixture frequency-based noise schedule. We train these models"}, {"title": "3.2 SELECTIVE LEARNING: FREQUENCY-BASED NOISE CONTROL TO OMIT TARGETED\nINFORMATION", "content": "Following our original intuition, a denoising model has pressure to learn the very information that is\nerased by the forward noising operator to achieve successful reconstruction. Conversely, when the\nnoising operator is crafted to leave parts of the original distribution intact, no such pressure exists,\nand the denoising model can effectively discard the left-out statistics during generation.\nIn this section, we perform experiments whereby the original data is corrupted with noise at different\nfrequency ranges. The objective is to manipulate the inductive biases of diffusion denoisers to avoid\nlearning the corruption noise, while correctly approximating the relevant information in the data. We\nformulate our corruption process as x' = Ac(x), where:\n$A_c(x) = x + \\gamma_c \\epsilon_{f[a_c,b_c]}$\nHere, $\u03f5_{f[a_c,b_c]}$ denotes noise in the $[a_c, b_c]$ frequency range. We default $\u03b3_c = 1.$ and show samples\nof the original and corrupted distributions in Figure 4. For any standard DPM training procedure,\nthe denoiser would make no distinction of which information to learn, and thus would approximate\nthe corrupted distribution presented at training time. As such, the recovery of the original, noiseless,\ndistribution would normally be impossible. Assuming knowledge of the corruption process, we\nframe the frequency diffusion learning procedures as a noiseless distribution recovery process, and\nset $a\u2081 = 0, b_h = 1, b\u2081 = a_c$, and $a_h = b_c$. This formulation effectively allows for the forward\nfrequency noising operator to omit the range of frequencies in which the noise lies. In line with our\nprevious rationale, this would effectively put no pressure on the denoiser to learn the noise part of the\ndistribution at hand, and focus instead on the frequency ranges where the true information lies.\nWe compare original and corrupted samples from MNIST, as well as samples from standard and\nfrequency diffusion-trained models in Figure 4. In line with our hypothesis, we observe frequency\ndiffusion DPMs trained with an appropriate frequency noise operator to be able to discard the\ncorrupting information and recover the original distribution after severe noisy corruption. We further\nmeasure the FID and KID of the samples generated by the baseline and frequency DPMs against the\noriginal (uncorrupted) data samples in Table 2. We perform 8 ablation studies, considering noises\nat 0.1 non-overlapping intervals in the [0.1,.9] frequency range. We observe frequency diffusion\nto outperform standard diffusion training across all tested ranges. Interestingly, we observe better\nperformance (lower FID) for data corruption in the high-frequency ranges, and reduced performance\nfor data corruptions in low-frequency ranges, suggesting a marginally higher information content in\nthe low frequencies for the MNIST dataset."}, {"title": "4 DISCUSSION AND CONCLUSION", "content": "In this work, we studied the potential to build inductive biases in the training and sampling of\nDiffusion Probabilistic Models by purposeful manipulation of the forward, noising, process. We\nintroduced frequency diffusion, an approach that enables us to guide DPMs toward learning specific\nstatistics of the data distribution. We compare frequency diffusion to DPS trained with standard\ngaussian noise on generative visual tasks set by several datasets, with significant varying structure and\nscales. We show several key findings. First, we show that appropriate manipulation of the forward\nnoising process can serve as a stong inductive bias for diffusion models to better learn the information\nof the distribution at particular frequencies. Second, we show that this important characteristic can\nbe readily used when training diffusion models on natural dataset, some of which may be better\nsupported by appropriate frequency diffusion schedules, yielding higher sampling quality. Third, we\nshow how this processes can be used to discard unwanted information at particular frequency ranges,\nyielding DPMs capable of extract noiseless signals from the remaining ranges.\nIn our approach, we have limited the results to a simple two-band pass frequency filter. We propose\nin subsection 2.3 several other alternatives, which may serve as more flexible tools to inject useful\ninductive biases for similar tasks. Moreover, the approach can be extended beyond constant schedules.\nFor instance, it may prove useful to introduce dynamic frequency noise strategies that shift the focus\nfrom low-frequency (general shapes) to high-frequency (sharp edges and textures) components over\nthe time discretization of the sampling process. Such methods could more closely align with human\nvisual processing, which progressively sharpens details over time, offering a more natural sampling\nprocess. Additionally, other domains of noise manipulation\u2014outside of the frequency domain may\nalso present new opportunities for further improving DPMs across various tasks.\nFinally, a current limitation of this approach lies in the complexity of understanding the relationship\nbetween visual data in spatial and frequency domains. The perception of information in the frequency\ndomain does not always translate straightforwardly to visual content, complicating the process of"}]}