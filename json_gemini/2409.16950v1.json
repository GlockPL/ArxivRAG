[{"title": "Dynamic Obstacle Avoidance through Uncertainty-Based Adaptive Planning with Diffusion", "authors": ["Vineet Punyamoorty", "Pascal Jutras-Dub\u00e9", "Ruqi Zhang", "Vaneet Aggarwal", "Damon Conover", "Aniket Bera"], "abstract": "By framing reinforcement learning as a sequence modeling problem, recent work has enabled the use of generative models, such as diffusion models, for planning. While these models are effective in predicting long-horizon state trajectories in deterministic environments, they face challenges in dynamic settings with moving obstacles. Effective collision avoidance demands continuous monitoring and adaptive decision-making. While replanning at every timestep could ensure safety, it introduces substantial computational overhead due to the repetitive prediction of overlapping state sequences-a process that is particularly costly with diffusion models, known for their intensive iterative sampling procedure. We propose an adaptive generative planning approach that dynamically adjusts replanning frequency based on the uncertainty of action predictions. Our method minimizes the need for frequent, computationally expensive, and redundant replanning while maintaining robust collision avoidance performance. In experiments, we obtain a 13.5% increase in the mean trajectory length and 12.7% increase in mean reward over long-horizon planning, indicating a reduction in collision rates, and improved ability to navigate the environment safely.", "sections": [{"title": "I. INTRODUCTION", "content": "Diffusion models have recently emerged as a promising approach to planning, demonstrating superior performance across a wide range of domains [1], [2], [3], [4]. Given a dataset of reward-labeled sub-optimal trajectories, diffusion models are capable of stitching them together to generate reward-maximizing optimal trajectories. Unlike single-step autoregressive models, diffusion models enable planning over long horizons without suffering from compounding errors. While such long-horizon planning is beneficial in static environments, it is not well-suited for dynamically changing environments often encountered in reality.\nDynamically varying, realistic environments with a large number of moving obstacles pose significant challenges. Long-horizon planning becomes inadequate in such settings, as rapidly moving obstacles can quickly render the long-term plan obsolete, increasing the likelihood of collisions. Conversely, re-planning at each time step incurs prohibitively high computational costs. In this work, we address this problem through uncertainty-based adaptive diffusion planning for collision avoidance in dynamic environments.\nCollision avoidance is a central challenge in planning, control, and robotics, critical for autonomous systems operating in environments with dynamic obstacles. Efficient and reliable mechanisms are essential for self-driving cars navigating unpredictable traffic with vehicles and pedestrians, and industrial robots are adjusting swiftly to changes to ensure safety and delivery drones, avoiding obstacles like birds or other drones. In military applications, autonomous ground vehicles and unmanned aerial systems must navigate complex environments with moving threats, while autonomous underwater vehicles for mine detection and surveillance need to avoid dynamic obstacles to ensure mission success and asset safety.\nTraditionally, methods like obstacle potential fields and rule-based approaches, such as the Dynamic Window Approach (DWA) [5] and Timed Elastic Band (TEB) [6], have been employed for collision avoidance. However, these methods are primarily designed for static obstacles and struggle in dynamic environments. Advancements in Reinforcement Learning (RL) have shown promise in handling dynamic environments by enabling agents to learn from interactions. In this context, deep RL-based algorithms, such as CADRL [7], [8] and MRCA [9], have demonstrated progress in collision avoidance. More recently, offline RL has emerged as a superior alternative to conventional RL, especially in scenarios where direct interaction with the environment could be dangerous. Offline RL allows agents to learn optimal policies from pre-collected datasets without further exploration. In this context, deep generative models have been effectively applied to sequential decision-making, treating it as a long-sequence modeling task [10], [11], [12]. In real-world, dynamically changing environments with moving obstacles, such long-horizon planning can severely increase the risk of collisions. However, collision avoidance within deep generative model-based offline RL has yet to be explored.\nIn this work, we introduce a novel solution to collision avoidance by combining diffusion models with adaptive planning strategies, offering a way to both predict and avoid collisions with minimal computational cost. By leveraging uncertainty estimates obtained from a deep ensemble inverse dynamics action model, we demonstrate the ability to dynamically adjust the planning horizon, utilizing computational resources only when necessary. This approach reduces unnecessary re-planning and enhances safety in environments with moving obstacles.\nOur work has the following key contributions:\n\u2022 We propose a novel approach to collision avoidance in dynamically changing environments through adaptive re-planning based on uncertainty estimates obtained from a deep ensemble action dynamics model.\n\u2022 Our approach provides a tunable and flexible trade-off between long-horizon planning (with high collision risk in a dynamic environment) and re-planning at every step (which is computationally expensive and redundant), through a single tuning parameter.\n\u2022 We demonstrate the effectiveness of our approach through experiments on a dynamic environment involving fast-moving obstacles, highway-env [13].\nIn our experiments, we obtain a 13.5% increase in the mean trajectory length, which indicates a reduction in collision rates, as longer trajectories suggest that the agent successfully avoids collisions.\nThe rest of the paper is structured as follows. In Section II we review related work in this field, emphasizing collision avoidance, planning using generative models and the estimation of uncertainty in neural networks. In Section III we formulate the problem of collision avoidance using offline RL and uncertainty estimation through deep ensembles. In Section IV we demonstrate the effectiveness of our collision avoidance algorithm and present our experimental results on the highway-env environment while discussing our key findings. Finally, in Section V we summarize our results, discuss the limitations of our model and outline future research directions."}, {"title": "II. RELATED WORK", "content": "A. Collision Avoidance in Motion Planning\nCollision avoidance is of fundamental importance in robotics, particularly in environments with dynamically moving obstacles. Factors such as lack of communication between multiple users in an environment and lack of a central planner make avoidance of collision with dynamic obstacles challenging. Traditional, rule-based approaches include methods such as dynamic window approach (DWA) [5] and timed elastic band (TEB) [6], which are, however, limited to static obstacles. The Velocity Obstacle (VO) series method [14], [15], [16], [17] was proposed to tackle the problem of dynamic obstacle avoidance. However, these methods involve significant computational overhead due to their reliance on conventional rules defined by complex conditions and equations.\nMore recent methods involve learning to avoid collisions by utilizing deep neural networks in reinforcement learning to simplify complex rules and conditions. Principal collision avoidance methods in this category include CADRL (collision avoidance with deep reinforcement learning) [7], [8] and MRCA (Multi-robot collision avoidance) [9]. While both CADRL and MRCA rely on the prediction of future states of obstacles to avoid a collision, MRCA requires continuous communication between the multiple agents. On the other hand, CADRL utilizes reinforcement learning to enable an agent to learn collision avoidance behaviors from simulated experiences without any communication between the agents. Similar other approaches to collision avoidance based on deep reinforcement learning include [18], [19], [20], [21], [22], [23].\nB. Planning using Generative Models\nRecently, planning using generative models such as transformers and diffusion models has gained significant attention in the context of offline RL, where the goal is to learn optimal policies from pre-collected datasets without further interaction with the environment. Generative models are used to generate trajectories of future states conditioned on the current state, enabling robust planning in the absence of real-time feedback. This approach is especially effective in tackling problems where exploration is costly or risky, such as in robotics and autonomous driving, by improving the agent's ability to generalize from limited offline data [24].\nOne of the representative works in this area is Decision Transformer [10] (DT), which casts the sequential decision-making problem into a sequence-modeling task, and solves it using a Transformer [25]. DT utilizes a decoder-only GPT-style transformer [26], leveraging its self-attention mechanism to model trajectories as sequences of states, actions, and returns. Given an offline dataset of trajectories {\u03c4= (so,ao, R0,...ST,ar,R\u2081)}, where $R_t = \\sum_{t'=t}^T r_{t'}$ denotes the cumulative future returns, DT is trained to predict the next action based on the previous k+1 transitions [24]:\n$\\min_{\\pi} \\mathbb{E}_{\\tau} \\sum_{t=0}^T - \\log \\pi (a_t | \\tau_{t-k:T})$\nDT simplifies offline RL by eliminating the need to fit Q-value networks through dynamic programming or computing policy gradients, and instead utilizes supervised sequence modeling. This has eventually evolved into a large class of transformer-based algorithms, and has been collectively referred to as Transformer-based RL (TRL) [27].\nMore recently, diffusion models, which have achieved significant success in image and video generation, have been effectively applied to trajectory modeling within offline RL. Diffusion models have been notably used as planners, where they are trained to generate a trajectory clip \u03c4 = (\u04351,2,...\u0435\u043d), where H is the planning horizon. Possible choices for et include et = (St,at) [11], et = st [12], and other combinations. A prominent work in this domain is the Decision Diffuser (DD) [12], which leverages diffusion processes to model the distribution of future trajectories based on past observations. DD operates by progressively refining noisy samples of potential future state sequences, conditioned on the desired outcomes, allowing it to generate trajectories that align with target rewards or goals. Unlike standard generative models, diffusion models excel at modeling complex distributions, making them particularly suited for environments with high variability and uncertainty. Other principal works in this area include Diffuser [11] and SafeDiffuser [28].\nC. Uncertainty Estimation in Neural Networks\nUncertainty estimation is an essential component in a wide range of tasks, particularly in those involving decision-making in safety-critical applications such as autonomous driving and robotics. The predictive uncertainty of a neural network consists of two components: epistemic uncertainty, which is the uncertainty associated with a model's knowledge, and aleatoric uncertainty, which is associated with the noise in the data [29].\nTraditionally, uncertainty quantification has been approached through Bayesian inference, where a prior distribution is placed on a network's parameters, and the posterior distribution is computed over the training data. The goal is to compute the posterior distribution p(\u03b8|D) where D is the dataset, which can then used to compute the uncertainty. However, since exact Bayesian inference is intractable, various approximation techniques have been proposed.\nOne such method is Monte Carlo Dropout [30], which approximates Bayesian inference by applying dropout both during training and inference. By performing multiple stochastic forward passes, we obtain a sample of network outputs {\u0177\u2071}\u2071=1, where each pass provides an approximation of the posterior, and the mean and variance of these outputs are used to estimate predictive uncertainty\nDeep ensembles [31] offer a more straightforward yet highly effective alternative for uncertainty estimation. In this approach, multiple independent models {f\u03b8\u2071}\u2071=1 are trained from different random initializations, each representing a different mode of the posterior distribution. The ensemble prediction is formed by averaging the predictions from each model, while the variance across the model outputs serves as an estimate of the model's uncertainty. Deep ensembles are highly scalable, as they don't need complex posterior approximations while providing strong uncertainty estimates. Deep ensembles capture both the epistemic and aleatoric uncertainties and do so without requiring any changes to the prediction network architecture."}, {"title": "III. METHODS", "content": "A. Problem Description\nWe consider a Markov Decision Process (MDP) with state space 8 and action space A. The dynamics of the MDP are governed by a stochastic transition function $T : \\mathcal{S"}, "times \\mathcal{A} \\leftrightarrow \\mathcal{S}$. The reward function is denoted by $R: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$. A trajectory \u03c4 consists of a sequence of states, actions and rewards:\n$\\tau:= \\{(s,a,r)\\}_{t=t_1}^{t_n}  \\text{ for } t \\in \\{t_1,t_2...t_n\\}$         (1)\nwhere $(s, a, r)_t \\in \\mathcal{S} \\times \\mathcal{A} \\times \\mathbb{R}$. The return R(\u03c4) is defined as the sum of rewards over all time steps within a trajectory: R(\u03c4) = \u03a3\u03c4rt. The goal is to learn an optimal policy \u03c0\u2217 which maps the environment's state to an agent's action \u03c0: 8 \u2192 A such that it maximizes the expected return over all trajectories, i.e. \u03c0\u2217 = argmax \u03c4~\u03c0\u0395 [R(\u03c4)].\nConventional approaches to obtaining an optimal policy include Q-learning, Deep Q-Networks (DQN) and Policy Gradient Methods under the online reinforcement learning paradigm, where the agent continuously interacts with the environment to learn a policy.\nRecent advancements in reinforcement learning have enabled offline RL, where an agent learns an optimal policy \u03c0\u2217 from a static dataset D consisting of pre-collected trajectories \u03c4 = {(St,at,rt)}1, rather than through continuous interaction with the environment. This dataset typically contains sub-optimal trajectories generated by various policies, with each trajectory labeled with corresponding rewards. The goal is to infer a return-maximizing policy \u03c0*(a|s) using the data in D. Offline RL offers significant advantages, particularly in environments where exploration is risky or costly (e.g., autonomous driving), as the agent learns exclusively from pre-existing data without interacting with the real world. Additionally, offline RL allows the integration of large, diverse datasets from multiple sources, increasing the robustness of the learned policy, while mitigating the dangers of unsafe exploration.\nB. Diffusion Models for Offline Reinforcement Learning\nDiffusion models have emerged as a powerful tool for modeling complex distributions in modalities such as images and videos. In the context of offline RL, the task of learning an optimal policy from a static dataset can be framed as a sequence modeling task, where trajectories \u03c4 = {e}=1 are treated as sequences. Here e, could be state-action pairs (St, at) [11], just st [12] or other possible options. Following [12], we choose to use et = st and exclude the actions from the sequences, in consideration of the fact that actions sequences are usually high-frequency, making them harder to model. We use a separate network, called the inverse dynamics action model (described in Section III-C) to map the state sequence back to actions: fe(St, St+1) = at.\nWe use a Denoising Diffusion Probabilistic Model (DDPM) [3] to model the state sequences. DDPM consists of a forward process in which noise is progressively added to an input sample until it becomes pure noise, and a reverse process in which the model learns to reverse the noising process through a kernel to progressively generate trajectories from pure noise. Given an input sample xo from a distribution Pdata(x0), the forward noising process produces a sequence of noisy vectors X0, X1 XK with the transition kernel:\nq(xk+1|Xk) = N(xk+1; \u221a\u221a\u03b1\u03baxk; (1 \u2212 \u03b1\u03bc)I)\nwhere ak follows a noise schedule. As the number of diffusion steps k \u2192 \u221e, the final state distribution qk converges in distribution to a standard normal distribution N(0,I). The reverse process is accomplished using successive applications of a learnable kernel:\nPo (Xk1|Xk) = N (Xk\u22121|\u03bc\u03b8 (xk,k), \u03a3k)\nThe kernel is trained to match the intermediate noisy vectors in the forward process and the loss function takes the form:\n$\\mathbb{E}_{k \\sim \\mathcal{U}[1,K], x_0 \\sim P_{data}(x_0), \\epsilon \\sim \\mathcal{N}(0,1)} [|| \\epsilon - \\epsilon_{\\theta} (x_k, k) ||^2]$\nwhere U[1,K] is a discrete uniform distribution over {1,2,...K} and \u025be is a deep neural network which predicts the noise & from xk and k.\nC. Inverse Dynamics Action Model\nThe diffusion model generates a state sequence $1:T ~ pe ($1:T|so) given an initial state so. In order to learn the policy, we must learn the action sequences that enable the transitions in this state sequence. We accomplish this by using an inverse dynamics model, which predicts the action that effects the transition between a pair of consecutive states: fo (St, St+1) = at. The action model is trained using the offline dataset to learn the transition kernel of the environment with cross entropy loss as the criteria for optimization.\nD. Uncertainty-based Obstacle Detection using Deep Ensemble\nIn a dynamically changing environment with moving obstacles, the risk of collision between the agent and obstacles is significantly higher than in a static or slowly varying environment. Therefore, long-horizon planning described in III-B is not well suited for such dynamically changing environments often encountered in real life. On the other extreme, generating a trajectory at each time step, conditioned on the current state, would enable dynamic planning and lower the risk of collision, but it is computationally expensive and highly redundant during the times when obstacles do not pose a real risk. Therefore, we propose an adaptive strategy that triggers re-planning only when we detect that the uncertainty of our planning process is above a set threshold. This method is based on the idea that increased uncertainty indicates a stronger need to reassess the plan using the generative model, ensuring that future decisions are informed by the latest environmental observations, aiding in collision avoidance.\nFollowing [32], we quantify this uncertainty using a Deep Ensemble of action models. The predictive uncertainty of deep ensembles captures both aleatoric (arising from the noise in the environment's transitions) and epistemic uncertainties (arising from lack of model's knowledge). In the case of discrete actions, an action model outputs the probability of each action:\n$f_{\\theta}(s_t, s_{t+1}) = [p(a^0), p(a^1), ..., p(a^{K-1})]$\n    },\n    {", "title\": \"IV. RESULTS AND DISCUSSION", "content", "We perform experiments on the highway-env environment [13], where an agent vehicle is tasked with navigating through a multi-lane road, which is shared by other vehicles, which act as moving obstacles for the agent. We choose a 4-lane highway environment, with total vehicle count set to 200 and maximum episode length of 100. The offline dataset for this environment is generated by training a Proximal Policy Optimization (PPO) algorithm [33] implemented by Stable Baselines3 [34]. The trained model is then used to sample trajectories with a cumulative length of N \u2248 10\u00ba steps, which is then used to train the diffusion model and the action model ensemble, as described in Section III.\nA. Mean Trajectory Length\nWe first evaluate the performance of our adaptive planning strategy using the metric of mean trajectory length, which reflects the agent's ability to avoid collisions and navigate the environment safely, over a maximum of 100 simulation steps. We choose the uncertainty threshold value to be \u025b = 0.1."]}, {"title": "V. CONCLUSION", "content": "This work presents a novel approach for enhancing collision avoidance in dynamically changing environments by leveraging uncertainty estimates from a deep ensemble of inverse dynamics action models alongside a diffusion model for trajectory planning. Our method focuses on improving the safety and robustness of trajectory generation in scenarios where obstacles are constantly moving and the environment is highly unpredictable. By using a diffusion model to generate long-horizon trajectories and selectively re-planning based on uncertainty estimates, we strike a balance between collision safety and computational cost.\nThe results show that the proposed approach leads to longer mean trajectory lengths, indicating successful collision avoidance without sacrificing computational efficiency. Importantly, the tunability of the re-planning threshold allows for a fine balance between minimizing collisions and managing computational load, making it adaptable to a variety of real-world scenarios where responsiveness and safety are important.\nOur approach offers a promising solution for applications that require real-time decision-making in dynamic environments, such as autonomous driving, robotics, and drones. By focusing on improving collision avoidance through uncertainty-aware planning, our work contributes to the broader effort of making autonomous systems safer and more reliable in the presence of environmental uncertainty."}]