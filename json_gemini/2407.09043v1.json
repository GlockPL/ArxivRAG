{"title": "Molecule Language Model with Augmented Pairs and Expertise Transfer", "authors": ["Namkyeong Lee", "Siddhartha Laghuvarapu", "Chanyoung Park", "Jimeng Sun"], "abstract": "Understanding the molecules and their textual descriptions via molecule language models (MoLM) recently got a surge of interest among researchers. However, unique challenges exist in the field of MoLM due to 1) a limited amount of molecule-text paired data and 2) missing expertise that occurred due to the specialized areas of focus among the experts. To this end, we propose AMOLE, which 1) augments molecule-text pairs with structural similarity preserving loss, and 2) transfers the expertise between the molecules. Extensive experiments on various downstream tasks demonstrate the superiority of AMOLE in comprehending molecules and their descriptions, highlighting its potential for application in real-world drug discovery.", "sections": [{"title": "1 Introduction", "content": "Recently, Language Models (LMs) have gained traction in molecular science, fueled by the abundance of literature in this domain. Notably, the conventional method of representing molecules as strings, such as SMILES strings (Weininger, 1988), facilitates the integration of two different modalities, i.e., text and molecules, into a single LM. Then, following the masked language modeling (Devlin et al., 2018), Zeng et al. (2022) train the model on masked SMILES and text from the scientific literature. Moreover, inspired by T5 models (Raffel et al., 2020), training the Molecule Language Model (MoLM) with multiple tasks and fine-tuning the downstream task has been proposed (Edwards et al., 2022; Pei et al., 2023; Christofidellis et al., 2023). However, all these models require molecules to be represented in a string format, like SMILES, to be understood by language models.\nAs an alternative approach, one can treat molecules and language as multiple modalities, following the recent success of the vision language model (VLM). For instance, several recent works (Edwards et al., 2021; Liu et al., 2023a; Seidl et al., 2023) have proposed using separate encoders for each modality and training the paired modalities to be similar in the representation space using contrastive learning, inspired by the CLIP model (Radford et al., 2021). These models are effective in capturing complementary information between different modalities by learning a joint representation space that can be utilized for various downstream tasks such as cross-modal retrieval and molecular property prediction.\nDespite promising early strides, the progress of MoLM lags far behind its VLM counterparts due to the scarcity of molecule-text paired data, both in quantity and expertise. First, in terms of quantity, the VLM community largely follows the viewpoint that scale is everything, as image-text pairs are widely available on the web (Wang et al., 2023). Pre-training VLM on the massive amounts of crawled image-text pairs, ranging from tens of thousands to billions, consistently leads to significant performance gains in various downstream tasks (Jia et al., 2021; Yu et al., 2022). In contrast, MOLM faces a bottleneck due to the limited amount of molecule-text paired data available, which demands costly domain knowledge and significant time investment for wet lab experiments.\nMoreover, each molecule has various descriptions provided by different experts, each focusing on its unique areas of expertise. For example, in the case of a molecule named Rivastigmine (Figure 1 (a)), FDA explains its function as a cholinesterase inhibitor, whereas LiverTox (Hoofnagle, 2013) focuses on its effectiveness on Alzheimer's disease. However, due to the specialized areas of the experts, they typically restrict their knowledge to a selective group of molecules, resulting in numerous molecules having missing expertise across various experts. Specifically, our examination of the largest molecule database, PubChem (Kim et al., 2021), as illustrated in Figure 1 (b), reveals that only 27K out of 299K molecules have descriptions from multiple experts. Consequently, the remaining 272K molecules are each documented with a single expert description, which may lack comprehensive expertise about the molecule.\nIn this paper, we propose AMOLE which addresses the unique challenges faced in MOLM by Augmenting MOLecule-text pair and transferring Expertise between the molecules. To overcome the lack of molecule-text paired data, we utilize the idea that molecules with similar structures have similar properties, which is grounded in the well-established biochemical principle (Martin et al., 2002). Specifically, we propose to augment molecule-text pairs by sharing descriptions among structurally similar molecules. However, this approach can lead to false positives, as the descriptions may not be specifically written for structurally similar molecules. To address this issue, we introduce a novel loss function that preserves structural similarity, guiding the model to align the augmented molecule-text pairs more closely based on the structural similarity of the molecules.\nFurthermore, to address the missing expertise issue, we utilize the fact that different areas of expertise are interrelated, allowing us to infer additional expertise based on one known area. As an example, in Figure 1 (a), since the Cholinesterase inhibitor is widely known to improve communication between nerve cells by increasing levels of Acetylcholine in the nervous system (Grossberg, 2003), one could infer Livertox's expertise about Alzheimer's disease from FDA descriptions. To this end, we propose to transfer the expertise acquired from molecules with extensive descriptions to those with less description. Specifically, given a molecule that possesses descriptions from multiple experts, we train the model to reconstruct one description from another, thereby enhancing the ability to deduce expertise from one expert using information from another. With our proposed training strategy, the model behaves as if it has access to abundant expertise, even if the model faces the molecule with missing expertise.\nWe make the following contributions:\n\u2022 To increase the limited amount of molecule-text paired data, we propose to selectively share descriptions among molecules with a novel loss function based on their structural similarity.\n\u2022 To address the issue of missing expertise, we propose to transfer the expertise between molecules by enhancing the model's ability to reconstruct one description from another.\n\u2022 Extensive experiments including two novel and practical tasks; zero-shot question and answering, and zero-shot virtual screening, demonstrate the superiority and potential applicability of AMOLE in real-world drug discovery process."}, {"title": "2 Related Works", "content": "MoLM with a Single Language Model. Thanks to the wealth of literature and the traditional string-based representation of molecules, such as SMILES, LMs have been applied to the domain of molecular science. Drawing inspiration from the masked language model approach used in BERT training (Devlin et al., 2018), KV-PLM (Zeng et al., 2022) proposes to train LMs by reconstructing masked SMILES and textual data. Moreover, MolT5 (Edwards et al., 2022) proposes to pre-train the LMs with the \"replace corrupted spans\" objective (Raffel et al., 2020) on both SMILES string and textual data, followed by fine-tuning for tasks such as molecule captioning and generation. Pei et al. (2023) and Christofidellis et al. (2023) extend MolT5 with various pre-training tasks, such as protein FASTA reconstruction and chemical reaction prediction. However, these models rely on string-based representations of molecules, e.g., SMILES, which are broadly recognized for their lack of topology awareness (Rong et al., 2020). Furthermore, merging two modalities into a single model prevents the adoption of existing pre-trained models tailored for each modality (Liu et al., 2023a).\nMoLM with Multi-Modal Contrastive Learning. Inspired by the recent success of VLM, researchers have started to conceptualize molecules and text as separate modalities, particularly by adopting separate encoders for each modality. As a pioneering work, Text2Mol (Edwards et al., 2021) proposes training separate encoders for molecular graph and textual description with cross-modal contrastive learning. Following this, CLAMP (Seidl et al., 2023) suggests employing contrastive learning for predicting activities based on the textual description of the task. Furthermore, MoleculeSTM (Liu et al., 2023a) develops the largest multi-modal dataset sourced from the PubChem database for cross-modal contrastive learning applications. It is essential to recognize that the distinction between these models is based on their architectural design and the data used for training rather than the training loss itself. On the other hand, MoMu (Su et al., 2022) introduces an intermolecular contrastive loss along with random molecular augmentations like node dropping, which may lead to chemically invalid structures (Lee et al., 2022a). Unlike previous works, AMOLE concentrates on overcoming the specific hurdles encountered in MoLM: the lack of abundance and expertise in molecule-text pairs, through innovative training loss strategies.\nMoLM with Other Multi-Modal Learning. On the other hand, there have been other approaches for integrating molecule text through a novel model architecture. GIMLET (Zhao et al., 2023) suggests encoding graph structure and instructional text directly, without separate graph encoding modules, by utilizing generalized position embeddings. Furthermore, inspired by BLIP-2 (Li et al., 2023) in VLM, MolCA (Liu et al., 2023b) introduces the alignment of two modalities through the Q-Former, allowing the language model to understand 2D molecular graphs. Note that these works are not directly related to our research, as we focus on introducing novel training loss designed for molecule-text pair augmentation and expertise transfer."}, {"title": "3 Preliminaries", "content": "3.1 Problem Statement\nNotations. Let G = (V, E) represent a molecular graph with atoms V as nodes and the edges E given by covalent bonds. Moreover, we have a set of textual descriptions T = {t1,...,tN} regarding the molecule G from N different experts, such as FDA and LiverTox, in Figure 1, each of which details various attributes of the molecule. Note that the number of descriptions for each molecule varies, i.e., Ni depends on molecule Gi.\nTask Description. Given a molecule graph G with its textual description t \u2208 T, we aim to train encoders $f_{mol}$ and $f_{text}$ that produce a molecule representation $z_G \\in R^D$ and a textual representation $z_t \\in R^D$, respectively. We aim to obtain the encoders that produce a generalized representation of molecules and text, that can be utilized for a wide range of downstream tasks.\n3.2 Tanimoto Similarity\nOne traditional way of representing a molecule is through fingerprints, a series of binary bits indicating the presence or absence of specific substructures (Rogers and Hahn, 2010). The Tanimoto similarity is a widely accepted criterion for calculating the similarity between two molecules (Bajusz et al., 2015) based on the fingerprints. Specifically, for the molecules Gi and Gj that are represented with the fingerprints $fp_i$ and $fp_j$, respectively, the Tanimoto similarity is calculated as follows:\n$S_{ij} = \\frac{|fp_i \\cap fp_j|}{|fp_i| + |fp_j| - |fp_i \\cap fp_j|}$        (1)\nIntuitively, the Tanimoto similarity takes both common and distinct substructures between two molecules into account, thereby offering an assessment of their structural similarity.\n3.3 Molecule-Text Contrastive Learning\nPrevious works (Liu et al., 2023a; Su et al., 2022) have introduced multi-modal contrastive learning to obtain encoders that establish qualified joint space between a molecule G and its corresponding text t. This approach ensures that paired molecules and texts are closely aligned in the representation space, while unpaired ones remain distant. Specifically, given a molecule G and its corresponding text t, we first obtain the molecule and text representations as follows: $z_G = f_{mol}(G)$ and $z_t = f_{text}(t)$. Then, the model is trained with the following Noise-Contrastive Estimation (InfoNCE) (Oord et al., 2018) loss:\n$L_{InfoNCE} = - \\frac{1}{2} \\{log\\frac{exp(sim(z_G, z_t) / \\tau)}{\\sum_{G' \\in G}exp(sim(z_G, z_{G'})/\\tau)} + log\\frac{exp(sim(z_t, z_G) / \\tau)}{\\sum_{t' \\in t}exp(sim(z_t, z_{t'})/\\tau)}\\},$             (2)\nwhere t' and G' represent all the texts and molecules within the batch, respectively, $sim(\\cdot,\\cdot)$ indicates the cosine similarity between the representations, and \u03c4 denotes the temperature hyperparameter."}, {"title": "4 Methodology", "content": "In this section, we introduce our proposed method, called AMOLE, a novel molecule-text contrastive learning approach that addresses the two unique challenges faced in MoLM, i.e., scarcity of molecule-text paired data in quantity and expertise.\n4.1 Augmenting Molecule-Text pairs\nWhile image-text pairs can be easily generated by the general public, molecule-text pairs are typically produced by specialized experts, making it more challenging to augment the data with external sources such as Web. To this end, we propose to augment the molecule-text pair by sharing the textual description among molecules within the data. Building on the well-established biochemical principle that structurally similar molecules often display analogous biological activities (Martin et al., 2002), we propose to share textual descriptions among molecules with structural resemblances. Specifically, we begin by computing all the pairwise structural similarity, i.e., Tanimoto similarity (Eq. 1), between the molecules in the training set. Then, given the similarity information of a molecule Gi to other molecules, we identify the top k molecules that exhibit the highest similarity to Gi. We represent these molecules as the set Si. During training, a molecule Gi\u2032 is randomly selected from the set Si and substituted for the original molecule Gi according to a predetermined probability p; otherwise, it is kept identical to the original molecule Gi. By doing so, the textual description ti of molecule Gi is shared among k molecules within the set Si, generating a new molecule-text pair (Gi\u2032,ti). As a result, this approach effectively expands the initial n molecule-text pairs to nk pairs.\n4.2 Structural Similarity Preserving Loss\nWhile we increase the number of pairs by sharing the descriptions, training the model with augmented pairs is challenging since the shared description ti is not specifically written for the substituted molecule Gi\u2032. Consequently, employing traditional InfoNCE loss in Eq. 2 for model training could make the model prone to false positives. To address this issue, we propose structural similarity preserving (S2P) loss, designed to preserve molecules' structural similarity in molecule-text joint space. That is, given a text ti and a molecule Gi, instead of directly defining the pair as positive, we propose to utilize the Tanimoto similarity $s_{ii'}$ between Gi and $G_{i'}$ as a pseudo label for contrastive learning. In particular, for a given molecule Gi, we compile a set of $s_{ij'}$ values, where $j' = 1,..., N_{batch}$ represents the range of molecules within a batch. Then, the pseudo label is calculated as follows:\n$\\hat{y}_{i \\rightarrow m}^{t \\rightarrow m} = \\frac{exp(s_{ij'} / \\tau_1)}{\\sum_{k'=1}^{N_{batch}}exp(s_{ik'} / \\tau_1)},$         (3)\nwhere \u03c41 is a temperature hyperparameter. Then, we make a prediction on the pseudo label based on the similarity between text ti and molecule $G_{j'}$, in representation space as follows:\n$\\hat{y}_{i \\rightarrow m}^{t \\rightarrow m} = \\frac{exp(sim(z_{t_i}, z_{G_{j'}}) / \\tau_2)}{\\sum_{k'=1}^{N_{batch}}exp(sim(z_{t_i}, z_{G_{k'}}) / \\tau_2)},$           (4)\nwhere \u03c42 is a temperature hyperparameter. It's worth noting that in order to approximate the representation similarity $sim(z_{t_i}, z_{G_{j'}})$ between a molecule and text to structural similarity $s_{ij'}$, we generate pseudo labels by applying softmax normalization to structural similarity. Then AMOLE is trained with the following cross-entropy loss:\n$L_{S^2P} = - \\frac{1}{N_{Batch}} \\sum_{i=1}^{N_{Batch}} \\sum_{j'=1}^{N_{batch}} \\hat{y}_{i \\rightarrow m}^{t \\rightarrow m}log \\hat{y}_{i \\rightarrow m}^{t \\rightarrow m}.$(5)\nBy doing so, the structural similarity between molecules Gi and Gi\u2032 instructs the model on how to align the text ti and molecule Gi\u2032 closely in the representation space, enabling the selective sharing of descriptions based on structural similarities. For the symmetricity, we compute Lmt\u2192s2p and derive our final training loss: $L_{S^2P} = \\frac{L_{t \\rightarrow m}^{S^2P} + L_{m \\rightarrow t}^{S^2P}}{2}$.\n4.3 Expertise Transfer Module\nExperts in the field often have a specific area of focus, which results in some molecules lacking comprehensive expertise coverage from different specialists. To address this issue of missing expertise, we suggest transferring the expertise gained from molecules with extensive descriptions to those with less description. Specifically, we use a molecule Gi with a set of descriptions Ti obtained from various experts. We train the language model $f_{text}$ to reconstruct the description $t_{i'}$ \u2208 Ti from a given description ti. This approach allows the language model $f_{text}$ to become skilled at inferring expertise from one institution based on another. It behaves as if it has access to abundant expertise even when dealing with molecules with missing expertise.\nHowever, reconstructing the description $t_{i'}$ in text space poses a challenge, as our model is designed to learn a qualified representation space without a decoder structure. To this end, we propose a novel expertise reconstruction (ER) loss, which guides the model to reconstruct the description within the representation space rather than directly in the text space. More formally, when we have a textual description ti and another associated description $t_{i'}$ for a molecule Gi, we formulate the reconstruction target ti by concatenating the two descriptions with [SEP] token, i.e., $t_i = t_i [SEP] t_{i'}$. Then, the model is trained to minimize the L2 distance as follows:\n$L_{ER} = \\frac{1}{N_{Batch}}\\sum_{i=1}^{N_{Batch}} ||f_{text}(t_i) - SG(f_{text}(t_i))||_2,$        (6)\nwhere SG denotes the stop-gradient operation, which halts the propagation of gradients when inputs are structured as ti, thereby preventing the model from acquiring degenerate solutions by disregarding the text following the [SEP] token. With the novel ER training loss, the model simulates having extensive expertise available, even when specific expertise is lacking.\n4.4 Model Training\nFinally, AMOLE is trained by jointly optimizing two losses, i.e., structural similarity preserving loss and expertise reconstruction loss, as follows:\n$L = L_{S^2P} + \\alpha \\cdot L_{ER},$    (7)\nwhere \u03b1 denotes the hyperparameter for controlling the weight of the expertise reconstruction loss."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nPretraining Dataset. We pre-train AMOLE with PubChem database (Kim et al., 2021), which is one of the most extensive public molecular databases available. Our pre-training dataset is compiled using the preprocessing script provided in the repository of the previous work (Liu et al., 2023a). However, due to regular updates to the PubChem database, our dataset varies slightly from the previous work, comprising a total of 299K unique molecules and 336K molecule-text pairs. We provide further details on PubChem database and diverse datasets utilized across four distinct downstream tasks in Appendix C.\nImplementation Details. One key benefit of treating molecules and text as distinct modalities is the opportunity to leverage powerful pre-trained models specifically designed for each modality. Following a previous work (Liu et al., 2023a), we employ a GraphMVP (Liu et al., 2021) pre-trained checkpoint for graph isomorphism network (GIN) model (Xu et al., 2018) for our molecule encoder $f_{mol}$. Moreover, for our text encoder $f_{text}$, we utilize a pre-trained SciBERT (Beltagy et al., 2019), which has been trained on a vast corpus of textual data from the biochemistry domains. We provide further implementation details in Appendix A.\nBaseline Methods. We evaluate AMOLE against three single encoder models: MolT5 (Edwards et al., 2022), BioT5 (Pei et al., 2023), and KV-PLM (Zeng et al., 2022), all of which represent molecules as 1D SMILES strings. While T5-based models are not designed for learning representations of molecules and text, we assess their capabilities by examining the hidden representations produced by the encoder model following a previous work (Seidl et al., 2023). Additionally, we compare AMOLE with two separate encoder models: MoMu (Su et al., 2022) and MoleculeSTM (Liu et al., 2023a). While MoMu depicts molecules as 2D graph structures, MoleculeSTM introduces two models that utilize both SMILES and 2D graph representations for molecules. For the single encoder models, we utilize the checkpoints provided by the authors of the original papers. However, for models with separate encoders, we independently pre-train models with the same pre-training data and model architecture for fair comparison. We provide further details on baseline methods in Appendix B.\n5.2 Zero-Shot Cross-Modal Retrieval\nTask Description. This task requires choosing an appropriate description from several alternatives for a specific molecule (Given Molecule) or re-"}, {"title": "6 Conclusion", "content": "In this paper, we propose AMOLE, addressing the two unique challenges in MoLM, i.e., scarcity of molecule-text paired data in both quantity and expertise. Our extensive testing on four downstream tasks, notably including two innovative and practical tasks such as zero-shot question and answering and zero-shot virtual screening, demonstrate the efficacy of AMOLE in grasping the nuances of molecules and their textual descriptions."}]}