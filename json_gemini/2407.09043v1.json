{"title": "Molecule Language Model with Augmented Pairs and Expertise Transfer", "authors": ["Namkyeong Lee", "Siddhartha Laghuvarapu", "Chanyoung Park", "Jimeng Sun"], "abstract": "Understanding the molecules and their textual descriptions via molecule language models (MoLM) recently got a surge of interest among researchers. However, unique challenges exist in the field of MoLM due to 1) a limited amount of molecule-text paired data and 2) missing expertise that occurred due to the specialized areas of focus among the experts. To this end, we propose AMOLE, which 1) augments molecule-text pairs with structural similarity preserving loss, and 2) transfers the expertise between the molecules. Extensive experiments on various downstream tasks demonstrate the superiority of AMOLE in comprehending molecules and their descriptions, highlighting its potential for application in real-world drug discovery.", "sections": [{"title": "1 Introduction", "content": "Recently, Language Models (LMs) have gained traction in molecular science, fueled by the abundance of literature in this domain. Notably, the conventional method of representing molecules as strings, such as SMILES strings (Weininger, 1988), facilitates the integration of two different modalities, i.e., text and molecules, into a single LM. Then, following the masked language modeling (Devlin et al., 2018), Zeng et al. (2022) train the model on masked SMILES and text from the scientific literature. Moreover, inspired by T5 models (Raffel et al., 2020), training the Molecule Language Model (MoLM) with multiple tasks and finetuning the downstream task has been proposed (Edwards et al., 2022; Pei et al., 2023; Christofidellis et al., 2023). However, all these models require molecules to be represented in a string format, like SMILES, to be understood by language models.\nAs an alternative approach, one can treat molecules and language as multiple modalities, following the recent success of the vision language model (VLM). For instance, several recent works (Edwards et al., 2021; Liu et al., 2023a; Seidl et al., 2023) have proposed using separate encoders for each modality and training the paired modalities to be similar in the representation space using contrastive learning, inspired by the CLIP model (Radford et al., 2021). These models are effective in capturing complementary information between different modalities by learning a joint representation space that can be utilized for various downstream tasks such as cross-modal retrieval and molecular property prediction.\nDespite promising early strides, the progress of MoLM lags far behind its VLM counterparts due to the scarcity of molecule-text paired data, both in quantity and expertise. First, in terms of quantity, the VLM community largely follows the viewpoint that scale is everything, as image-text pairs are widely available on the web (Wang et al., 2023). Pre-training VLM on the massive amounts of crawled image-text pairs, ranging from tens of thousands to billions, consistently leads to significant performance gains in various downstream tasks (Jia et al., 2021; Yu et al., 2022). In contrast, MOLM faces a bottleneck due to the limited amount of molecule-text paired data available, which demands costly domain knowledge and significant time investment for wet lab experiments.\nMoreover, each molecule has various descriptions provided by different experts, each focusing on its unique areas of expertise. For example, in the case of a molecule named Rivastigmine (Figure 1 (a)), FDA explains its function as a cholinesterase inhibitor, whereas LiverTox (Hoofnagle, 2013) focuses on its effectiveness on Alzheimer's disease. However, due to the specialized areas of the experts, they typically restrict their knowledge to a selective group of molecules, resulting in numerous molecules having missing expertise across various experts. Specifically, our examination of the largest molecule database, PubChem (Kim et al., 2021), as illustrated in Figure 1 (b), reveals that only 27K out of 299K molecules have descriptions from multiple experts. Consequently, the remaining 272K molecules are each documented with a single expert description, which may lack comprehensive expertise about the molecule.\nIn this paper, we propose AMOLE which addresses the unique challenges faced in MOLM by Augmenting MOLecule-text pair and transferring Expertise between the molecules. To overcome the lack of molecule-text paired data, we utilize the idea that molecules with similar structures have similar properties, which is grounded in the well-established biochemical principle (Martin et al., 2002). Specifically, we propose to augment molecule-text pairs by sharing descriptions among structurally similar molecules. However, this approach can lead to false positives, as the descriptions may not be specifically written for structurally similar molecules. To address this issue, we introduce a novel loss function that preserves structural similarity, guiding the model to align the augmented molecule-text pairs more closely based on the structural similarity of the molecules.\nFurthermore, to address the missing expertise issue, we utilize the fact that different areas of expertise are interrelated, allowing us to infer additional expertise based on one known area. As an example, in Figure 1 (a), since the Cholinesterase inhibitor is widely known to improve communication between nerve cells by increasing levels of Acetylcholine in the nervous system (Grossberg, 2003), one could infer Livertox's expertise about Alzheimer's disease from FDA descriptions. To this end, we propose to transfer the expertise acquired from molecules with extensive descriptions to those with less description. Specifically, given a molecule that possesses descriptions from multiple experts, we train the model to reconstruct one description from another, thereby enhancing the ability to deduce expertise from one expert using information from another. With our proposed training strategy, the model behaves as if it has access to abundant expertise, even if the model faces the molecule with missing expertise.\nWe make the following contributions:\n\u2022 To increase the limited amount of molecule-text paired data, we propose to selectively share descriptions among molecules with a novel loss function based on their structural similarity.\n\u2022 To address the issue of missing expertise, we propose to transfer the expertise between molecules by enhancing the model's ability to reconstruct one description from another.\n\u2022 Extensive experiments including two novel and practical tasks; zero-shot question and answering, and zero-shot virtual screening, demonstrate the superiority and potential applicability of AMOLE in real-world drug discovery process."}, {"title": "2 Related Works", "content": "MoLM with a Single Language Model. Thanks to the wealth of literature and the traditional string-based representation of molecules, such as SMILES, LMs have been applied to the domain of molecular science. Drawing inspiration from the masked language model approach used in BERT training (Devlin et al., 2018), KV-PLM (Zeng et al., 2022) proposes to train LMs by reconstructing masked SMILES and textual data. Moreover, MolT5 (Edwards et al., 2022) proposes to pre-train the LMs with the \"replace corrupted spans\" objective (Raffel et al., 2020) on both SMILES string and textual data, followed by fine-tuning for tasks such as molecule captioning and generation. Pei et al. (2023) and Christofidellis et al. (2023) extend MolT5 with various pre-training tasks, such as protein FASTA reconstruction and chemical reaction prediction. However, these models rely on stringbased representations of molecules, e.g., SMILES, which are broadly recognized for their lack of topology awareness (Rong et al., 2020). Furthermore, merging two modalities into a single model prevents the adoption of existing pre-trained models tailored for each modality (Liu et al., 2023a).\nMoLM with Multi-Modal Contrastive Learning. Inspired by the recent success of VLM, researchers have started to conceptualize molecules and text as separate modalities, particularly by adopting separate encoders for each modality. As a pioneering work, Text2Mol (Edwards et al., 2021) proposes training separate encoders for molecular graph and textual description with cross-modal contrastive learning. Following this, CLAMP (Seidl et al., 2023) suggests employing contrastive learning for predicting activities based on the textual description of the task. Furthermore, MoleculeSTM (Liu et al., 2023a) develops the largest multi-modal dataset sourced from the PubChem database for cross-modal contrastive learning applications. It is essential to recognize that the distinction between these models is based on their architectural design and the data used for training rather than the training loss itself. On the other hand, MoMu (Su et al., 2022) introduces an intermolecular contrastive loss along with random molecular augmentations like node dropping, which may lead to chemically invalid structures (Lee et al., 2022a). Unlike previous works, AMOLE concentrates on overcoming the specific hurdles encountered in MoLM: the lack of abundance and expertise in molecule-text pairs, through innovative training loss strategies.\nMoLM with Other Multi-Modal Learning. On the other hand, there have been other approaches for integrating molecule text through a novel model architecture. GIMLET (Zhao et al., 2023) suggests encoding graph structure and instructional text directly, without separate graph encoding modules, by utilizing generalized position embeddings. Furthermore, inspired by BLIP-2 (Li et al., 2023) in VLM, MolCA (Liu et al., 2023b) introduces the alignment of two modalities through the Q-Former, allowing the language model to understand 2D molecular graphs. Note that these works are not directly related to our research, as we focus on introducing novel training loss designed for moleculetext pair augmentation and expertise transfer."}, {"title": "3 Preliminaries", "content": "3.1 Problem Statement\nNotations. Let G = (V, E) represent a molecular graph with atoms V as nodes and the edges E given by covalent bonds. Moreover, we have a set of textual descriptions T = {t1,...,tN} regarding the molecule G from N different experts, such as FDA and LiverTox, in Figure 1, each of which details various attributes of the molecule. Note that the number of descriptions for each molecule varies, i.e., Ni depends on molecule Gi.\nTask Description. Given a molecule graph G with its textual description t \u2208 T, we aim to train encoders $f_{mol}$ and $f_{text}$ that produce a molecule representation $z_G \\in \\mathbb{R}^D$ and a textual representation $z_t \\in \\mathbb{R}^D$, respectively. We aim to obtain the encoders that produce a generalized representation of molecules and text, that can be utilized for a wide range of downstream tasks.\n3.2 Tanimoto Similarity\nOne traditional way of representing a molecule is through fingerprints, a series of binary bits indicating the presence or absence of specific substructures (Rogers and Hahn, 2010). The Tanimoto similarity is a widely accepted criterion for calculating the similarity between two molecules (Bajusz et al., 2015) based on the fingerprints. Specifically, for the molecules Gi and Gj that are represented with the fingerprints $fp_i$ and $fp_j$, respectively, the Tanimoto similarity is calculated as follows:\n$S_{ij} = \\frac{ |fp_i \\cap fp_j| }{ |fp_i| + |fp_j| - |fp_i \\cap fp_j| }$.   (1)\nIntuitively, the Tanimoto similarity takes both common and distinct substructures between two molecules into account, thereby offering an assessment of their structural similarity.\n3.3 Molecule-Text Contrastive Learning\nPrevious works (Liu et al., 2023a; Su et al., 2022) have introduced multi-modal contrastive learning to obtain encoders that establish qualified joint space between a molecule G and its corresponding text t. This approach ensures that paired molecules and texts are closely aligned in the representation space, while unpaired ones remain distant. Specifically, given a molecule G and its corresponding text t, we first obtain the molecule and text representations as follows: $z_G = f_{mol}(G)$ and $z_t = f_{text}(t)$. Then, the model is trained with the following NoiseContrastive Estimation (InfoNCE) (Oord et al., 2018) loss:\n$L_{InfoNCE} = \\frac{1}{2N} { \\sum_{i=1}^{N} {log \\frac{exp(sim(z_G, z_t)/\\tau)}{\\sum_{G' \\in \\mathbb{G}} exp(sim(z_G, z_{G'})/\\tau)} + log \\frac{exp(sim(z_t, z_G)/\\tau)}{\\sum_{t' \\in \\mathbb{T}} exp(sim(z_t, z_{t'})/\\tau)} } }$,   (2)\nwhere t' and G' represent all the texts and molecules within the batch, respectively, sim(\u00b7, \u00b7) indicates the cosine similarity between the representations, and \u03c4 denotes the temperature hyperparameter."}, {"title": "4 Methodology", "content": "In this section, we introduce our proposed method, called AMOLE, a novel molecule-text contrastive learning approach that addresses the two unique challenges faced in MoLM, i.e., scarcity of molecule-text paired data in quantity and expertise. Figure 2 presents the overall model architecture.\n4.1 Augmenting Molecule-Text pairs\nWhile image-text pairs can be easily generated by the general public, molecule-text pairs are typically produced by specialized experts, making it more challenging to augment the data with external sources such as Web. To this end, we propose to augment the molecule-text pair by sharing the textual description among molecules within the data. Building on the well-established biochemical principle that structurally similar molecules often display analogous biological activities (Martin et al., 2002), we propose to share textual descriptions among molecules with structural resemblances. Specifically, we begin by computing all the pairwise structural similarity, i.e., Tanimoto similarity (Eq. 1), between the molecules in the training set. Then, given the similarity information of a molecule Gi to other molecules, we identify the top k molecules that exhibit the highest similarity to Gi. We represent these molecules as the set Si. During training, a molecule $G_{i'}$ is randomly selected from the set Si and substituted for the original molecule Gi according to a predetermined probability p; otherwise, it is kept identical to the original molecule Gi. By doing so, the textual description ti of molecule Gi is shared among k molecules within the set Si, generating a new molecule-text pair ($G_i'$, ti). As a result, this approach effectively expands the initial n moleculetext pairs to nk pairs.\n4.2 Structural Similarity Preserving Loss\nWhile we increase the number of pairs by sharing the descriptions, training the model with augmented pairs is challenging since the shared description ti is not specifically written for the substituted molecule $G_i'$. Consequently, employing traditional InfoNCE loss in Eq. 2 for model training could make the model prone to false positives. To address this issue, we propose structural similarity preserving (S2P) loss, designed to preserve molecules' structural similarity in moleculetext joint space. That is, given a text ti and a molecule Gi, instead of directly defining the pair as positive, we propose to utilize the Tanimoto similarity $s_{ii'}$ between Gi and $G_i'$ as a pseudo label for contrastive learning. In particular, for a given molecule Gi, we compile a set of $s_{ij'}$ values, where $j' = 1,..., N_{batch}$ represents the range of molecules within a batch. Then, the pseudo label is calculated as follows:\n$\\gamma^{t \\rightarrow m}_{i j'} = \\frac{exp(s_{ij'} / \\tau_1)}{\\sum_{k'=1}^{N_{batch}} exp(s_{ik'} / \\tau_1)}$,   (3)\nwhere \u03c41 is a temperature hyperparameter. Then, we make a prediction on the pseudo label based on the similarity between text ti and molecule $G_i'$ in representation space as follows:\n$\\gamma^{t \\rightarrow m}_{i j'} = \\frac{exp(sim(z_{ti}, z_{G_{j'}}) / \\tau_2)}{\\sum_{k'=1}^{N_{batch}} exp(sim(z_{ti}, z_{G_{k'}}) / \\tau_2)}$,   (4)\nwhere \u03c42 is a temperature hyperparameter. It's worth noting that in order to approximate the representation similarity $sim(z_{ti}, z_{G_{j'}})$ between a molecule and text to structural similarity $s_{ij'}$, we generate pseudo labels by applying softmax normalization to structural similarity. Then AMOLE is trained with the following cross-entropy loss:\n$L_{S2P} = -\\frac{1}{N_{Batch}} \\sum_{i=1}^{N_{batch}} \\sum_{j'=1}^{N_{batch}} \\gamma^{t \\rightarrow m}_{ij'} log \\gamma^{t \\rightarrow m}_{ij'}$.   (5)\nBy doing so, the structural similarity between molecules Gi and $G_i'$ instructs the model on how to align the text ti and molecule $G_i'$ closely in the representation space, enabling the selective sharing of descriptions based on structural similarities. For the symmetricity, we compute $L_{m \\rightarrow t}^{S2P}$ and derive our final training loss: $L_{S2P} = L_{t \\rightarrow m}^{S2P} + L_{m \\rightarrow t}^{S2P}$.\n4.3 Expertise Transfer Module\nExperts in the field often have a specific area of focus, which results in some molecules lacking comprehensive expertise coverage from different specialists. To address this issue of missing expertise, we suggest transferring the expertise gained from molecules with extensive descriptions to those with less description. Specifically, we use a molecule Gi with a set of descriptions Ti obtained from various experts. We train the language model $f_{text}$ to reconstruct the description $t_{i'}$ \u2208 Ti from a given description ti. This approach allows the language model $f_{text}$ to become skilled at inferring expertise from one institution based on another. It behaves as if it has access to abundant expertise even when dealing with molecules with missing expertise.\nHowever, reconstructing the description $t_{i'}$ in text space poses a challenge, as our model is designed to learn a qualified representation space without a decoder structure. To this end, we propose a novel expertise reconstruction (ER) loss, which guides the model to reconstruct the description within the representation space rather than directly in the text space. More formally, when we have a textual description ti and another associated description $t_{i'}$ for a molecule Gi, we formulate the reconstruction target $t_i$ by concatenating the two descriptions with [SEP] token, i.e., $t_i = t_i [SEP] t_{i'}$. Then, the model is trained to minimize the L2 distance as follows:\n$L_{ER} = \\frac{1}{N_{Batch}} \\sum_{i=1}^{N_{batch}} || f_{text}(t_i) - SG(f_{text}(t_i)) ||_2$,   (6)\nwhere SG denotes the stop-gradient operation, which halts the propagation of gradients when inputs are structured as $t_i$, thereby preventing the model from acquiring degenerate solutions by disregarding the text following the [SEP] token. With the novel ER training loss, the model simulates having extensive expertise available, even when specific expertise is lacking.\n4.4 Model Training\nFinally, AMOLE is trained by jointly optimizing two losses, i.e., structural similarity preserving loss and expertise reconstruction loss, as follows:\n$L = L_{S2P} + \\alpha \\cdot L_{ER}$,   (7)\nwhere \u03b1 denotes the hyperparameter for controlling the weight of the expertise reconstruction loss."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nPretraining Dataset. We pre-train AMOLE with PubChem database (Kim et al., 2021), which is one of the most extensive public molecular databases available. Our pre-training dataset is compiled using the preprocessing script provided in the repository of the previous work (Liu et al., 2023a). However, due to regular updates to the PubChem database, our dataset varies slightly from the previous work, comprising a total of 299K unique molecules and 336K molecule-text pairs. We provide further details on PubChem database and diverse datasets utilized across four distinct downstream tasks in Appendix C.\nImplementation Details. One key benefit of treating molecules and text as distinct modalities is the opportunity to leverage powerful pre-trained models specifically designed for each modality. Following a previous work (Liu et al., 2023a), we employ a GraphMVP (Liu et al., 2021) pre-trained checkpoint for graph isomorphism network (GIN) model (Xu et al., 2018) for our molecule encoder $f_{mol}$. Moreover, for our text encoder $f_{text}$, we utilize a pre-trained SciBERT (Beltagy et al., 2019), which has been trained on a vast corpus of textual data from the biochemistry domains. We provide further implementation details in Appendix A.\nBaseline Methods. We evaluate AMOLE against three single encoder models: MolT5 (Edwards et al., 2022), BioT5 (Pei et al., 2023), and KVPLM (Zeng et al., 2022), all of which represent molecules as 1D SMILES strings. While T5-based models are not designed for learning representations of molecules and text, we assess their capabilities by examining the hidden representations produced by the encoder model following a previous work (Seidl et al., 2023). Additionally, we compare AMOLE with two separate encoder models: MoMu (Su et al., 2022) and MoleculeSTM (Liu et al., 2023a). While MoMu depicts molecules as 2D graph structures, MoleculeSTM introduces two models that utilize both SMILES and 2D graph representations for molecules. For the single encoder models, we utilize the checkpoints provided by the authors of the original papers. However, for models with separate encoders, we independently pre-train models with the same pre-training data and model architecture for fair comparison. We provide further details on baseline methods in Appendix B.\n5.2 Zero-Shot Cross-Modal Retrieval\nTask Description. This task requires choosing an appropriate description from several alternatives for a specific molecule (Given Molecule) or re-"}, {"title": "5.3 Zero-Shot Question and Answering", "content": "Task Description. While earlier studies have assessed the retrieval capabilities of MoLM by contrasting the correct description with randomly chosen descriptions from other molecules (Liu et al., 2023a; Su et al., 2022), we propose a novel task, termed the \"Zero-shot Question and Answering\" task. Specifically, given a textual description of a molecule, we instruct GPT-4 (Achiam et al., 2023) to generate a multiple-choice question comprising five options, all derived from the given textual description. Then, with a generated question and its five corresponding options, we merge the question with each option to form a single input, i.e., input\u2081 = Concat(question, option), where i = 1,...,5. Given a molecule and these combined inputs, we then select the one that includes the correct answer from the options. In Appendix C.3, we offer details for generating and validating questions and answer datasets, along with an evaluation scheme for the task in Appendix D.2.\nEmpirical Results. In Table 3, we have the following observations: 1) Comparing to Table 1, we observe that despite having far fewer options for retrieval, most models learning representation space (i.e., KV-PLM, MoMu, MoleculeSTM, and AMOLE) perform much worse in the task. This is because the model must discern based solely on the subtle differences between the options provided, requiring the model to have a more fine-grained understanding of molecules than cross-modal re-trieval. 2) On the other hand, AMOLE consistently outperforms baseline models in this task, showcasing its ability to extract more intricate information from the slight variations in options through the inference of related expertise. In conclusion, we posit that AMOLE offers benefits in tasks that demand a more intricate understanding of molecules, achieved by integrating additional expertise. In Appendix E.2, we further showcase the effectiveness of the expertise transfer module by integrating modified inputs as AMOLE into the baseline methods.\n5.4 Molecular Property Prediction\nTask Description. In this task, we assess the potential benefits of incorporating external knowledge, i.e., textual descriptions, into the molecule encoder $f_{mol}$ as done in AMOLE, in enhancing the prediction of molecular properties. We mainly compare to recent graph self-supervised learning (GraphSSL) approaches, i.e., AttrMask (Hu et al., 2019), ContextPred (Hu et al., 2019), GPT-GNN (Hu et al., 2020), InfoGraph (Sun et al., 2019), MolCLR (Wang et al., 2022), GraphMVP (Liu et al., 2021), and Mole-BERT (Xia et al., 2022), and MoLMs which represent molecules with graph structure (MoLM w/ Graph). Following previous works (Liu et al., 2021, 2023a), we pre-train the molecule encoder $f_{mol}$ using each of the proposed strategies, and fine-tuning on MoleculeNet benchmark (Wu et al., 2018). We provide further details on the dataset and evaluation scheme for the task in Appendix C.4 and D.3, respectively.\nEmpirical Results. In Table 4, we have following observations: 1) We observe that incorporating external textual descriptions into the pre-training phase uniformly enhances the prediction of molecular properties, as evidenced by improved overall"}, {"title": "5.5 Zero-Shot Virtual Screening", "content": "Task Description. Virtual screening is a computational technique to search large libraries of compounds quickly to identify those structures most likely to have desired properties, emerging as a principal technique in the drug discovery process (Mehta et al., 2021). Therefore, in this paper, we propose a novel task named \u201cZero-shot Virtual Screening,\" where we assess the model's proficiency in virtual screening drugs by supplying a textual description of a desired property. Specifically, our evaluation of the model's capability in virtual screening is conducted by providing two distinct prompts for each property as shown in Table 5: one brief and abstract, and the other longer and more detailed, offering comprehensive information about the property. For each description, we identify the top 100 molecules nearest to the prompt in the representation space and compute the hit rate to evaluate the model's performance."}, {"title": "6 Conclusion", "content": "In this paper, we propose AMOLE, addressing the two unique challenges in MoLM, i.e., scarcity of molecule-text paired data in both quantity and expertise. Our extensive testing on four downstream tasks, notably including two innovative and practical tasks such as zero-shot question and answering and zero-shot virtual screening, demonstrate the efficacy of AMOLE in grasping the nuances of molecules and their textual descriptions."}, {"title": "A Implementation Details", "content": "In this section, we provide implementation details of AMOLE.\nText Encoder $f_{text}$. Following previous work (Liu et al., 2023a), we use BERT architecture (Devlin et al., 2018) as the text encoder $f_{text}$, and adapt the SciBERT (Beltagy et al., 2019) checkpoint to initialize the model parameters. Text encoder contains a total 109,918,464 number of parameters.\nMolecule Encoder $f_{mol}$. We use GIN (Xu et al., 2018) architecture as a molecular encoder $f_{mol}$, which has been widely used as the backbone model in recent graph self-supervised learning works (Hu et al., 2019; Liu et al., 2021). Additionally, we initialize the encoder parameters using the GraphMVP (Liu et al., 2021) checkpoints provided by the original authors. Molecule encoder contains a total 1,885,206 number of parameters.\nTraining Details. Our method is implemented on Python 3.7.16, PyTorch 1.10.1, and Torchgeometric 2.0.3. All experiments are conducted using an 80GB NVIDIA A100 GPU. It takes 90 minutes per epoch for training, a total of 2700 minutes for training.\nHyperparameters. We list the key hyperparameters used during training in Table 6."}, {"title": "B Baseline Methods", "content": "In this section, we elaborate on baseline methods compared during the experiments.\nSingle Encoder Models. For single encoder models, we mainly compare AMOLE with MolT5, BioT5, and KV-PLM. While T5-based models were"}, {"title": "C Datasets", "content": "C.1 Pre-training\nWe pre-train AMOLE with the PubChem database, which is one of the most extensive public molecular databases available. PubChem database consists of multiple data sources including DrugBank, CTD, PharmGKB, and more. Please refer to the following URL for more details: https://pubchem.ncbi.nlm.nih.gov/sources/.\nThe PubChem database we used during training comprises a total of 299K unique molecules and 336K molecule-text pairs. During preprocessing, we consolidate each expertise into a unified description. Thus, each description for an individual molecule originates from a distinct database (expertise). On average, molecules are associated with 1.115 descriptions, with a maximum of 17 descriptions and a minimum of one. We provide a histogram and boxplot on the number of descriptions per molecule in Figure 4. As shown in Figure 4, it is evident that the majority of molecules have a singular description, indicating a lack of comprehensive expertise across various experts for numerous molecules. Moreover, each description in training data consists of 17.62 words on average, with a maximum of 874 words and a minimum of one. We also provide a histogram on the number of words per description in Figure 5.\nC.2 Zero-Shot Cross-Modal Retrieval\nFollowing previous work (Liu et al., 2023a), we use the datasets extracted from the Description field, the Pharmacodynamics field, and the anatomical therapeutic chemical (ATC) field in the DrugBank database (Wishart et al., 2018). Each field contains the following information:\n\u2022 Description (Descr.) field provides an overview of 1,154 drugs, including their chemical characteristics, development history, and standing in terms of regulatory approval."}, {"title": "C.3 Zero-Shot Question and Answering", "content": "We generate questions based on the textual descriptions used for the cross-modal retrieval task in Section 5.2. Please note that we restrict our question generation to descriptions and pharmacodynamics datasets since the ATC dataset consists of brief labels for molecules.\nQuestion and Answer Generation. In Section 5.3, we assess the MoLM's capacity to discern the correct answer from options with minor differences, aiming to evaluate a more nuanced understanding of molecules and their textual descriptions. To achieve this, we employ GPT-4 to craft a multiplechoice question with five options, each based on the textual descriptions of molecules, by providing specific prompts to guide its generation. We"}, {"title": "C.4 Molecular Property Prediction", "content": "For the molecular property prediction task, we use the MoleculeNet benchmark (Wu et al., 2018) for evaluation, which is commonly used for evaluating the machine learning methods for molecular property prediction (Liu et al., 2021). The MoleculeNet benchmark encompasses a diverse array of datasets, each characterized by distinct properties"}, {"title": "C.5 Zero-Shot Virtual Screening", "content": "For the zero-shot virtual screening task, we utilize datasets from the Therapeutics Data Commons"}, {"title": "D Experimental Setups", "content": "D.1 Zero-Shot Cross-Modal Retrieval\nIn this section, we provide further details on experimental setups for the zero-shot cross-modal retrieval task. Following previous work (Liu et al., 2023a), we evaluate the task performance in two distinctive settings 1) given molecule to retrieve the textual description, and 2) given texture description to retrieve the molecule. In each scenario, we conducted experiments with a range of options, specifically 4, 10, and 20 choices. Within these options, one is the matching counterpart, while the others are randomly selected from the dataset. Following this, the model performance is determined by its capacity to pinpoint the correct counterpart from the options provided, such as correctly matching the description to the given molecule or vice versa. For the evaluation, we conducted five separate experiments, each featuring a distinct random selection of options, and we present both the mean and standard deviation of these experiments.\nD.2 Zero-Shot Question and Answering\nIn this section, we provide further details on the zero-shot questions and answering task. We provide an overall pipeline for generating a dataset for the question and answering tasks in Figure 6. After the creation of a set of validated questions and answers, we design the question and answering task as a retrieval task. Specifically, for a given question and its five options, we concatenate the question with each option to generate a singular input, i.e., input\u2081 = Concat(question, option), for i = 1,..., 5. The MoLM then determines the correct answer by selecting from these inputs the one that correctly matches the question as shown in Figure 9. The only difference between the options comes from option, makes the task much harder compared to the cross-modal retrieval task in Section 5.2.\nD.3 Molecular Property Prediction\nIn this section, we provide details on experimental setups for the molecular property prediction task. In this task, we evaluate how the pre-training methods affect the prediction of various molecular properties. To achieve this, we initially divide the dataset according to scaffold information using an 8:1:1 ratio for the training, validation, and test sets, respectively. That is, the molecules in the training, validation, and test sets possess distinct scaffolds. Subsequently, we fine-tune the molecular encoder $f_{mol}$ using the training data across 100 epochs. Following previous work (Liu et al., 2023a), the model's performance is then evaluated on the test set where the hyperparameters achieve optimal performance on the validation set. We ran five individual experiments and report the average and standard deviation of the results.\nD.4 Zero-Shot Virtual Screening\nIn this section, we provide further details on the zero-shot virtual screening task. In this task, we assess the model's capability to conduct virtual screening for drugs with specific properties described in a language model prompt. Given a"}, {"title": "E Additional Experimental Results", "content": "E.1 Zero-Shot Cross-Modal Retrieval\nComplete Set of Experimental Results. In this section, we provide a complete set of experimental results for the zero-shot cross-modal retrieval task. In Table 8, we provide empirical results on the (a) description, (b) pharmacodynamics, and (c) ATC dataset. We observe that as tasks get harder, i.e., increase in the number of options from 4 to 20, the performance disparity between baseline methods and AMOLE broadens.\nStatistical Significance Test. To demonstrate the statistical significance of the improvement in Table 1, we performed a paired t-test in the representation learning context for each dataset. Since we evaluate models on five independent trials, we compared the mean performances of MoleculeSTM, ablations, and AMOLE across each trial. The pvalues presented in the table are the outcomes of these paired t-tests, based on the following hypothesis: \u0397\u03bf: \u03bcAMOLE = \u00b5Baseline and H1 : \u03bcAMOLE \u2265"}, {"title": "E.2 Zero-Shot Question and Answering", "content": "In this section, we explore how changes in the input format impact the performance of graph-based baseline models on Zero-Shot question-and-answer tasks. Specifically, during training, we randomly replace the input textual description ti (Original Input) with t\u2081 = t\u2081 [SEP"}]}