{"title": "STRIVE: Structured Reasoning for Self-Improvement in Claim Verification", "authors": ["Haisong Gong", "Jing Li", "Junfei Wu", "Qiang Liu", "Shu Wu", "Liang Wang"], "abstract": "Claim verification is the task of determining whether a claim is supported or refuted by evidence. Self-improvement methods, where reasoning chains are generated and those leading to correct results are selected for training, have succeeded in tasks like mathematical problem solving. However, in claim verification, this approach struggles. Low-quality reasoning chains may falsely match binary truth labels, introducing faulty reasoning into the self-improvement process and ultimately degrading performance. To address this, we propose STRIVE: Structured Reasoning for Self-Improved Verification. Our method introduces a structured reasoning design with Claim Decomposition, Entity Analysis, and Evidence Grounding Verification. These components improve reasoning quality, reduce errors, and provide additional supervision signals for self-improvement. STRIVE begins with a warm-up phase, where the base model is fine-tuned on a small number of annotated examples to learn the structured reasoning design. It is then applied to generate reasoning chains for all training examples, selecting only those that are correct and structurally sound for subsequent self-improvement training. We demonstrate that STRIVE achieves significant improvements over baseline models, with a 31.4% performance gain over the base model and 20.7% over Chain of Thought on the HOVER datasets, highlighting its effectiveness.", "sections": [{"title": "1 Introduction", "content": "The proliferation of misinformation is a major challenge in today's society, eroding trust in digital information and affecting domains such as public health (Naeem and Bhatti, 2020) and politics (Mishra et al., 2022). As a result, claim verification-determining whether a claim is supported or refuted by evidence-has become crucial for ensuring reliable information.\nClaim verification is a natural language inference task. While large language models (LLMs)"}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Claim Verification", "content": "Early approaches to claim verification focused on fine-tuning pre-trained models, either by concatenating evidence and claims into a single input (Aly et al., 2021; Thorne et al., 2018; Hu et al., 2022) or processing evidence separately and aggregating the results (Soleimani et al., 2020; Jiang et al., 2021; Gi et al., 2021). Graph Neural Networks have also been applied to capture relationships between evidence pieces (Gong et al., 2024b; Zhao et al., 2020; Chen et al., 2022b). With the impressive generative capabilities demonstrated by large language models (LLMs), many studies have turned to LLMs for claim verification (Ma et al., 2024). FACT-GPT (Choi and Ferrara, 2024) and FactLlama (Cheung and Lam, 2023) fine-tune LLMs to directly predict the truthfulness of claims. Factscore (Min et al., 2023) employs systematic decomposition to assess the factuality of individual claim segments, while ProgramFC (Pan et al., 2023) frames claim verification as step-wise program execution. Other works, such as Li et al. (2024), Chen et al. (2022a), and Rani et al. (2023), transform the verification task into a series of sub-questions to be answered."}, {"title": "2.2 Chain of Thought Reasoning (CoT)", "content": "Chain of Thought (CoT) reasoning (Wei et al., 2022) was proposed to help LLMs solve complex problems by breaking them down into intermediate step-by-step reasoning. Kojima et al. (2022) demonstrated that adding a prompt such as \u201cLet's think step by step\" significantly boosts LLM performance. CoT reasoning has been applied to a variety of tasks, including claim verification. Studies like Hu et al. (2024) and Dougrez-Lewis et al."}, {"title": "2.3 Self-Improvement Methods", "content": "Self-improvement methods for LLMs have garnered attention in recent years, where models are fine-tuned on their self-generated solutions, optionally iterating this process (Hosseini et al., 2024). ReSTEM (Singh et al., 2023) generates reasoning chains for solving math problems and selects those leading to correct answers for retraining. RFT (Yuan et al., 2023) enhances reasoning chain diversity by sampling multiple chains before selection. STaR (Zelikman et al., 2022) introduces hints during reasoning generation for chains that lead to incorrect results. V-STaR (Hosseini et al., 2024) incorporates Direct Preference Optimization (Rafailov et al., 2023) into the self-improvement process. Our method shares similarities with STaR. We are the first to apply self-improvement to claim verification. We also highlight the unique challenges of claim verification, distinguishing it from tasks like math problem-solving, and address these challenges through the integration of structured reasoning design."}, {"title": "3 Method", "content": "In this section, we introduce our approach to claim verification. We first define the task itself, followed by our Structured Reasoning Design, which is specifically tailored for claim verification. Finally, we describe how this structure is applied within the self-improvement process to enhance the verification model's performance."}, {"title": "3.1 Task Formulation", "content": "The task of claim verification is to determine the truthfulness of a given claim based on a set of evidence. Our approach aims to generate reasoning chains connecting the claim and evidence to a final prediction, rather than outputting the answer directly. However, it's important to note that reasoning chains are not provided in the verification datasets, and only the final truth label is available.\nFormally, given a claim $c$ and an evidence set $E = \\{e_1, e_2,..., e_n\\}$, where each $e_i$ is a descriptive piece of evidence (such as a sentence or paragraph), the goal is to obtain a model that can generate a reasoning chain $r$ that forms the intermediate reasoning steps. The final prediction"}, {"title": "3.2 Structured Reasoning Design", "content": "Freely generated reasoning chains often struggle with verifying complex claims due to issues like evidence confusion, entity misidentification, and omission of key information. These problems can lead to low-quality chains being selected during self-improvement, affecting performance.\nTo address this, we introduce a Structured Reasoning Design that guides the verification process systematically. Our Structured Reasoning Design consists of three key components: Claim Decomposition, Entity Analysis, and Evidence Grounding Verification. An example is illustrated in Figure 2."}, {"title": "3.2.1 Claim Decomposition", "content": "Complex claims typically describe multiple facets of a situation, such as in our example (Figure 2), where the claim covers the champion's nationality, victory, and origin. When a claim involves multiple elements, decomposing it into subclaims allows for independent verification of each component, a method known to improve performance in claim verification (Gong et al., 2024a; Min et al., 2023). In our design, we structure the reasoning chain into distinct blocks, each dedicated to a specific subclaim, labeled as C1:, C2:, and so on. This block-based structure helps mitigate the risk of overlooking key aspects during verification, which is common in free reasoning. Each subclaim block concludes with a verification result, labeled as Status:, to make the reasoning easy to follow and parse."}, {"title": "3.2.2 Entity Analysis", "content": "Complex claims often involve ambiguous or un-specified entities or pronouns that may appear after Claim Decomposition. To resolve these ambiguities, we designed a two-step entity analysis process: Entity Resolution and Resolution Verification.\n\u2022 Entity Resolution aims to identify the specific entity corresponding to an ambiguous term by leveraging evidence, labeled as Entity Resolution: in the reasoning chain. For instance, the term \u201cdefending champion\" in the example claim (Figure 2) is resolved to the specific person \u201cLin Dan\u201d using information from the evidence.\n\u2022 Resolution Verification ensures the correctness of the Entity Resolution, labeled as Resolution Verification: in the reasoning chain. While Entity Resolution may correctly match an ambiguous term to a known entity in many cases, errors can occur. For example, if the claim stated a \"Doubles tournament\" instead of a \"Singles tournament\" in the example, the Entity Resolution step might still match the term \u201cdefending champion at the Men's Doubles tournament\u201d to \u201cLin Dan\u201d, which is incorrect for the revised claim. Resolution Verification cross-checks the entity to ensure that the resolution is accurate, preventing errors in the reasoning process.\nThis two-step design ensures more precise entity analysis, minimizing the risk of errors that could"}, {"title": "3.2.3 Evidence Grounding Verification", "content": "Our structured reasoning framework incorporates explicit verification steps at multiple stages. In addition to the Resolution Verification process mentioned earlier, each subclaim undergoes a final verification step, labeled as Verification:. This step evaluates the subclaim as a whole, considering the resolved entities and assessing its validity against the provided evidence. It also serves as an explanation for the truthfulness decision of the subclaim.\nOur structured reasoning format enforces grounding at every stage of the reasoning chain. As illustrated in Figure 2, both the resolution and verification processes explicitly cite the corresponding evidence or subclaim identifiers (e.g., C1, E2). This ensures that conclusions are drawn from verifiable sources, making the reasoning more transparent and easier for humans to interpret and verify."}, {"title": "3.3 Self-Improvement with Structure", "content": "In this section, we describe how STRIVE leverages the Structured Reasoning Design to improve model's performance in claim verification, with the general flow of this process shown in the bottom part of Figure 2. Given a training set\n$D = \\{(c_1, E_1, p_1),\u2026\u2026,(c_N,E_N,p_N)\\}$,\nwhere each $c_i$ is a claim, $E_i$ is the corresponding evidence set, and $p_i$ is the final label, STRIVE follows three main steps to complete the self-improvement process: Structured Warm-up, Reasoning Chain Generation and Selection, and Self-Improvement Training."}, {"title": "3.3.1 Structured Warm-up", "content": "Given a base model $M$, the purpose of the warm-up phase is to fine-tune $M$ into $M^*$, enabling it to generate reasoning chains conforming to our predefined structure. We use a preset prompt template $T(c, E)$ as follows:\nBased on the evidence, determine if the claim is supported by the evidence or refuted by it. Output the reasoning chain.\nClaim: [claim text c]\nEvidence: (1)[evidence text e\u2081](2)..."}, {"title": "3.3.2 Reasoning Chain Generation and Selection", "content": "With the fine-tuned model $M^*$, our next objective is to leverage $M^*$ and training set $D$ to generate and select high-quality structured reasoning chains for further self-improvement. Inspired by the chain generation strategy in STaR (Zelikman et al., 2022), our method involves three stages: (i) Initial Generation and Selection and (ii) Refinement with Hint and (iii) Format Checking:\nInitial Generation and Selection: For each sample $(c_i, E_i, p_i) \\in D$, we first generate a reasoning chain $r_i$ using the prompt template $T(c_i, E_i)$ with the model $M^*$: \n$r_i = M^*(T(c_i, E_i)) \\forall i \\in [1, N]$.\nA predicted label $\\hat{p_i}$ is then derived from $r_i$ by a"}, {"title": "3.3.3 Self-Improvement Training", "content": "Finally, we fine-tune the base model $M$ using both the previously selected dataset $D_{st}$ and the human-annotated dataset $D_h$. Note that $M^*$ is discarded after generating the reasoning chains. This allows us to obtain the final model $M_{st}$ with the enhanced reasoning ability defined by our structured approach. The entire process is summarized in Algorithm 1."}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Datasets", "content": "We evaluate STRIVE using two publicly available datasets, following prior work in claim verification (Gong et al., 2024a). All evaluations are performed on the validation sets, since the test sets are not publicly released. Detailed information can be found in Appendix A.3. Results are reported using the Macro-F1 score.\n\u2022 HOVER (Jiang et al., 2020) This dataset comprises claims necessitating multi-step reasoning across multiple pieces of evidence and is categorized into three subsets: HOVER-2 for two-hop reasoning, HOVER-3 for three-hop reasoning, and HOVER-4 for four-hop reasoning.\n\u2022 FEVEROUS-S (Aly et al., 2021) Derived from the FEVEROUS dataset, this subset contains claims that rely solely on unstructured textual evidence. Compared to HOVER, claims in FEVEROUS-S generally exhibit lower complexity in reasoning."}, {"title": "4.2 Baselines", "content": "To assess the effectiveness of STRIVE, we compare it against a variety of baselines, including pretrained/fine-tuned models, prompt-based approaches using the base model, and base model fine-tuned on the training data.\nFor pretrained/fine-tuned models, we include the following: (i) BERT-FC (Soleimani et al., 2020): Pretrained BERT model (Devlin et al., 2019) tailored for fact-checking tasks. (ii) LisT5 (Jiang et al., 2021): Pretrained T5 model (Raffel et al., 2020) specialized for fact-checking tasks. (iii) ROBERTa-NLI (Nie et al., 2020): Pretrained ROBERTa-large model (Liu et al., 2019) fine-tuned on four natural language inference datasets. (iv) MULTIVERS (Wadden et al., 2022): A Long-Former model (Beltagy et al., 2020) fine-tuned on the FEVER (Thorne et al., 2018) dataset.\nFor prompt-based approaches, we evaluate the following: (i) Zero-shot: The model predicts the final label directly, without requiring reasoning steps. (ii) Zero-shot + CoT: The model outputs reasoning chains before predicting the final label. (iii) Few-shot: Similar to zero-shot, but with the inclusion of labeled examples. (iv) Few-shot + Structured CoT: Similar to zero-shot + CoT, but with examples of structured reasoning chains from Dh. All baseline prompts are similar to T(c, E) for fairness, detailed in Appendix A.1.\nFinally, for base models that undergo fine-tuning, we compare with: (i) Lora Fine-tuning: Fine-tuning the base model on the training set D using only binary labels. (ii) STaR* (Zelikman et al., 2022): We re-implemented this self-improvement method for claim verification. While similar to our approach (Algorithm 1), it differs in that reasoning chains are generated freely by the model, without structural constraints or a warm-up phase."}, {"title": "4.3 Implementation Details", "content": "We use Llama-3-8B-Instruct as the base model, as it is a widely used open-source language model. For all fine-tuning tasks, including those in baseline models and STRIVE, we employ the GPU memory-efficient LoRA fine-tuning method (Hu et al., 2021), allowing our experiments to fit on a single consumer-grade GPU (e.g., NVIDIA 4090).\nIn the Structured Warm-up process, we use a small human-annotated dataset Dh containing only $H = 10$ examples, with 8 labeled as Refuted and 2 as Supported. We prioritize teaching the model to identify errors rather than admitting correct claims. The intermediate model $M^*$ is obtained by fine-tuning on Dh for 10 epochs using LORA. Despite the large number of epochs, only 0.1% of the model's parameters are updated, ensuring the model's overall performance is maintained while enforcing the prescribed structure. The training set D consists of $N = 600$ examples, with reasoning chains generated at a temperature setting of 0.01. The final model $M_{st}$ is obtained by fine-tuning for 2 epochs on the union of the collected set and the human-annotated set, $D_{st} \\cup D_h$."}, {"title": "5 Results and Discussion", "content": null}, {"title": "5.1 Overall Performance", "content": "We present the overall results of STRIVE and the baseline models in Table 1. Among all methods, STRIVE achieves the best performance, underscor-"}, {"title": "5.2 Effectiveness of Structured Reasoning", "content": "In this section, we explain how Structured Reasoning contributes to performance improvement in claim verification.\nStructured Design Improves Reasoning Quality. We compare three approaches: Zero-shot + CoT, Few-shot + CoT, and Few-shot + Structured CoT."}, {"title": "5.3 Analysis of Structure Design", "content": "In this section, we analyze the structural reasoning design of STRIVE. As outlined in Section 3.2, the core components\u2014Claim Decomposition, Entity Analysis, and Evidence Grounding Verification\u2014are introduced to address the issue of low-quality reasoning chains generated by the model. To assess the contribution of each component, we conduct an ablation study by removing each design element individually, resulting in three variants: STRIVEw/o CD, STRIVEw/o EA, and STRIVEw/o EG.\nFrom the table, it is evident that Claim Decomposition is the most impactful design; its removal causes performance to drop across all HOVER datasets. Entity Analysis also plays a significant role, particularly in more complex scenarios (HOVER-4), as complex claims often involve multiple ambiguous entities. In contrast, the Evidence Grounding Verification design has a smaller impact on performance but still contributes valuable benefits, primarily by improving human readability of the reasoning chains."}, {"title": "5.4 Analysis of Self-Improvement Training", "content": "To analyze the self-improvement training process, we propose three variants: STRIVEw/o FC, STRIVEw/o hint and STRIVE2 rounds, with performance results shown in Table 2."}, {"title": "7 Limitations", "content": "Our approach relies on a structured warm-up phase that requires a small amount of annotated data. In our experiments, we selected 10 moderately difficult claims for reasoning chain labeling without further extensive sample filtering. While this approach has yielded positive results, we recognize that the choice of these samples may influence the subsequent model training. We believe that more carefully selected or diverse samples could further enhance the model's performance and provide additional insights into how sample selection impacts self-improvement. Additionally, our approach improves the claim verification capabilities of LLM in a resource-efficient manner. Both the quantity of annotations and the training strategies were designed for resource efficiency. This low-cost approach has proven effective for performance enhancement, but it also presents opportunities for future research, particularly in terms of scalability. Expanding to larger datasets and more complex models could offer valuable insights, though it remains to be explored in future works."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Prompts for Baseline Models", "content": "We incorporate various prompt-based methods in our experiments to ensure a fair comparison. To maintain consistency, we keep most of the prompt content similar to that used for STRIVE in Section 3.3.1. Below, we list the prompts used for the baseline models.\nFor the zero-shot and LoRA fine-tuning experiments, we use the following prompt:\nBased on the evidence, determine if the claim is supported by the evidence or refuted by it. Claim: [claim text c]\nEvidence: (1)[evidence text e\u2081](2)...\nPlease respond with only whether the claim is \"Supported\" or \"Refuted.\u201d\nFor the zero-shot + CoT and STaR* experiments, the prompt is as follows:"}, {"title": "A.2 Prompts with Hint", "content": "In STRIVE, we add hint and regenerate reasoning chains for the ones that falsely predict the label of the claim. If the truth label is p = Supported, we"}, {"title": "A.3 Dataset Statistics", "content": "The following table presents the information of the dataset (validation set) that we have tested on. \"HV\" represents HOVER, while \u201cFS\" stands for FEVEROUS."}, {"title": "A.4 STRIVE-Generated Reasoning Chains", "content": "In this section, we present several reasoning chains generated by STRIVE. The evidence is omitted due to its length, which makes it unsuitable for display. Red coloring is used to highlight where the reasoning chains become incorrect."}]}