{"title": "Recall: Empowering Multimodal Embedding for Edge Devices", "authors": ["Dongqi Cai", "Shangguang Wang", "Chen Peng", "Zeling Zhang", "Mengwei Xu"], "abstract": "Human memory is inherently prone to forgetting. To address\nthis, multimodal embedding models have been introduced,\nwhich transform diverse real-world data into a unified em-\nembedding space. These embeddings can be retrieved efficiently,\naiding mobile users in recalling past information. However,\nas model complexity grows, so do its resource demands,\nleading to reduced throughput and heavy computational re-\nquirements that limit mobile device implementation. In this\npaper, we introduce Recall, a novel on-device multimodal\nembedding system optimized for resource-limited mobile\nenvironments. Recall achieves high-throughput, accurate\nretrieval by generating coarse-grained embeddings and lever-\naging query-based filtering for refined retrieval. Experimen-\ntal results demonstrate that Recall delivers high-quality\nembeddings with superior throughput, all while operating\nunobtrusively with minimal memory and energy consump-\ntion.", "sections": [{"title": "1 Introduction", "content": "Mobile devices are ubiquitous nowadays. They capture lots\nof data in users' daily usage, which are invaluable to making\ndevices intelligent assistants [1-4]. For example, this data\ncan be used for memory recall, helping users retrieve spe-\ncific information or moments from the past. For instance,\nMicrosoft launches a project called Recall that makes a note\nof everything ever displayed on personal computer for AI-\npowered retrospective search [5].\nHowever, such data has not been fully utilized, attributed\nnot to how to store them, but how to accurately retrieve\nthem [6]. Specifically, smartphones have abundant storage\n(up to 1TB for iPhone 15 Pro) to host the information cap-\ntured at 24x7, or local network-attached storage (NAS) can\nhelp accommodate those data as well; yet there has been a\nlack of method to efficiently locate the data intended at query\ntime [7, 8]. The fundamental challenge is that data generated\non devices is multimodal by nature (e.g., text, image, audio,\nIMU, etc), which are hard to be accurately retrieved in a\nuser-friendly manner, e.g., through natural language [9].\nFortunately, the recent development of multimodal em-\nbedding models (MEM) has shed light on multimodal data\nretrieval. For example, CLIP unifies text and image modalities\ninto one embedding space [10]. ImageBind further extends\nthe functionality to 6 modalities through contrastive learn-\ning [11]. At architecture level, those models primarily consist\nof multi-layer transformer encoders [12].\nIn general, MEMs will catelyze two novel, exciting types of\nmobile applications as shown in Figure 1: (1) cross-modality\nsearching, which allows users to retrieve data in any modal-\nity with user-friendly interface, e.g., language; (2) retrieval-\naugmented LLM generation, which first identifies the relevant\nmultimodal data (e.g., a picture) in a historical database with\nuser prompt, and uses it to enhance the LLM generation\nquality, e.g., \"in the picture I took for my kid yesterday, is\nshe wearing a blue skirt or yellow?\u201d.\nThis work addresses the emerging scenario of on-device\nmultimodal embedding, where MEMs operate on local de-\nvices to embed continuous data streams [13-16]. The local\ngeneration of embeddings is motivated by user privacy con-\ncerns, since MEMs can greatly expand the usage of device\ndata, including screen Uls, recorded voices, etc. Offloading\nsuch information to the cloud may expose it to unauthorized\naccess. For instance, it was revealed that Apple Siri had been\neavesdropping on uploaded user conversations to enhance\ntheir public voice assistant model [17]. With cloud-based\nMEMs, users risk comprehensive life surveillance, with no\nway to verify.\nCost of on-device MEMs. Despite MEM is generalizable to\nvarious downstream tasks [11, 18-20], it comes at a cost of\nresource intensity. Specifically, our pilot experiments in \u00a72.3\nidentify two key obstacles towards on-device multimodal\nembedding: (1) Low embedding throughput. It takes dozens\nof seconds for billion-sized MEMs to embed a single image,\nwhich is significantly slower than the rate at which mobile\ndevices generate data. As a result, even if the CPU runs\ncontinuously, only 20% of daily information can be embed-\nded. (2) High energy consumption. The slow inference speed,"}, {"title": "2 Background and Motivations", "content": "Unified multimodal embedding Embedding was initially\nproposed to vectorize text data for understanding similari-\nties between different texts [32]. Large language models use\nembedding layers to generate text embeddings [12, 33]. Sim-\nlarly, vision, audio, and sensor data can also be transformed\ninto vectorized embeddings [34-36]. However, embedding\nmethods focused on a single modality cannot access infor-\nmation across different modalities due to the gap between\ntheir embedding spaces.\nTo bridge this gap, multimodal embedding models (MEMS)\nhave been developed to unify different modalities into a\nsingle embedding space, enhancing the model's ability to\nunderstand and bind multimodal inputs. CLIP [10] aligns\ntext and vision by jointly training on image-text pairs, using\ncontrastive learning to map both modalities into a shared\nspace while maintaining their distinction through a dual-\ntower architecture. ImageBind [11] extends this to align six\nmodalities, including text, vision, audio, depth, thermal, and\nIMU readings. Each modality is processed by a separate en-\ncoder, and the embeddings are fused in a multimodal head\nto generate a unified embedding. ImageBind demonstrates\nstrong zero-shot classification and retrieval performance\nacross these modalities, matching or outperforming single-\nmodality models. This is achieved through training on large-\nscale multimodal data.\nMultimodal mobile applications MEMs optimize align-\nment between high-quality representations across modalities.\nAs such, multimodal information can be composed to enable\na rich variety of mobile context-aware applications. For ex-\nample, MEMs could embed visual, audio, text and sensor data\nexperienced on a mobile device into a personalized mem-\nory palace [37, 38]. Whenever users want to recall a specific\nmoment or items, they can query the memory palace with\na multimodal query, and the system will retrieve the most\nrelevant items. MEMs can also facilitate mobile agents to\niteract with users in a more human-like manner [1, 39, 40].\nOn-device Multimodal Embedding Data for embedding is\ncontinuously sourced from end users and is often private and\nsensitive. Evidence suggests that cloud service providers may\nbe curious about uploaded data to improve their services [17],\nand database leaks and breaches pose significant threats [41].\nConducting embedding locally prevents the need to upload\ndaily viewed, sensed, or heard data to the cloud, offering\nstrong privacy protection. From the cloud perspective, a\nsingle user views over 6,000 images per day, according to our\nuser study (\u00a72.3), requiring approximately 1065.6KJ of energy\nand 0.8 GPU hours. For 1 billion daily active users, cloud\nproviders would need 1.1 TWh of energy and 0.8M GPU\nhours daily, costing over $100 million per day. On-device\nmultimodal embedding shifts this cost to end users, making\nthe service more practical to deploy."}, {"title": "2.2 MEM-empowered Search Service", "content": "As shown in Figure 2, we prototype an on-device MEM-\npowered search service to embed multimodal streaming data\nfor future retrieval, functioning like a memory palace [37].\nWe specifically target mobile devices, including smartphones\nand IoT devices with similar computing capabilities. These\ndevices have usable but weaker processing units compared\nto cloud servers, with limited battery and memory available\nfor long-term background processes [42].\nFrom the device perspective, the service consists of two\nruntimes:"}, {"title": "2.3 Preliminary Measurements", "content": "We conducted a user study to collect viewed images from\ndaily mobile applications used by 8 volunteers, aged 20 to 52,\nover the course of a week. To achieve this, we developed an\nAndroid application with accessibility services [45] to detect\nand store newly appeared visual content. One collected\ntrace is shown as an example in Figure 3.\nObservation: MEMs are contextually expressive."}, {"title": "3 Design", "content": ""}, {"title": "3.1 Recall Overview", "content": "In this work, we develop Recall, an efficient on-device multi-\nmodal embedding system to address the challenges outlined\nabove. Recall is designed to minimize embedding energy\ncosts and query latency while maximizing throughput and\nachieving near state-of-the-art retrieval accuracy. Addition-\nally, Recall shall integrate easily into off-the-shelf mobile\napplications to enhance user experience without requiring\ncomplex hardware modifications. Lastly, Recall aims to be\nboth versatile and transferable across a wide range of tasks.\nTo achieve these goals, we leverage early exit, a widely stud-\nied optimization technique, as the backbone of our system.\nKey building block: early exit terminates the computa-\ntion of a deep neural network at an intermediate layer based\non prediction confidence. Typically, a prediction head is in-\ntroduced at the end of each layer to serve as a separate exit\nbranch, allowing samples to be correctly classified at the\nearliest possible layer.\nWe choose early exit as the backbone of Recall because\nit aligns with our design principles: (1) Early exit is mo-\nbile hardware-friendly: it requires no sparsification kernel\ncompilation and integrates easily into existing multimodal\nembedding applications. Most mobile devices do not fully\nsupport advanced sparsification or quantization optimiza-\ntions, providing little to no benefit during inference [49-53].\n(2) Early exit preserves the raw structure of MEMs, main-\ntaining their generalization capacity while bypassing only\ndownstream alignment. Additionally, early exit is caching-\nfriendly, as the top layers share the same bottom weights\nwith the exited layers, allowing intermediate activations to\nbe reused and reducing duplicated computations. Other tech-\nniques like pruning and quantization cannot fully leverage\nthe intermediate computation of coarse-grained embeddings.\nThis reduction is crucial for Recall, as it eliminates redun-\ndant forward passes, accelerating both embedding and query\nphases, which we discuss in detail in \u00a73.4.\nSimplified workflow: As shown in Figure 6, Recall pro-\nvides a memory encoder for clients to build coarse-grained\nembeddings offline, while the rest of the model functions as a\nlive encoder for precise online retrieval. (1) System developer\npreparation: Developers first refine widely-used pretrained\nmultimodal models to reduce the number of layers needed for\ntoken prediction (\u00a73.3). The refined model is then deployed\nto mobile devices for offline embedding. (2) Client offline em-\nbedding: Users employ part of the memory encoder to build\nsuperficial embeddings for pre-exit prediction (\u00a73.2). After\npre-exit, samples with the same exits are batched and pro-\ncessed layer by layer through pipeline scheduling to generate\ncoarse-grained embeddings. (3) Client online query: During\nthe query phase, the query is embedded for matching. Likely\ncandidates are filtered and refined from the coarse-grained\nembeddings, which are then matched with the query embed-\nding to finalize retrieval (\u00a73.4).\nIn short, we offload the full-sized embedding cost to the\nquery phase, which is infrequent and carries precise retrieval\ninformation [7]. This mirrors the human brain, which retains\nkey information in long-term memory and recalls details\nonly when necessary [54]. Retrieval accuracy and latency\nare sacrificed within acceptable limits to significantly reduce\nembedding costs, as demonstrated in \u00a75.\nUnique challenges introduced by early exit: While early\nexit reduces computational load, its application in mobile\nMEMs introduces several unique challenges: (1) Low paral-\nlism: Early exit is incompatible with batching, as all samples\nin a batch must exit before processing the next [21]. This sig-\nnificantly reduces throughput on mobile devices with limited\ncomputational resources. Without batching, it is also harder\nto amortize loading costs, further slowing layer-wise infer-\nence. (2) Limited benefits: MEMs are not naturally designed\nfor early prediction and tend to distribute computation across\nall layers. For instance, ImageBind's 32-layer vision module\nrequires an average of 21.4 layers to process data, limiting\ncomputation savings to 33.1%. MEMs need to reduce the\nlayers required for token prediction and minimize computa-\ntional resources spent on hesitant or fluctuating predictions.\n(3) Performance degradation: Despite thorough training of\nexit branches and predictors, some samples may exit too\nearly, leading to degraded search performance. This is es-\npecially problematic in MEMs, where incorrect embeddings\ncan disrupt the unified embedding space, causing unbalanced\ndistributions and inaccurate retrieval."}, {"title": "3.2 Data-aware Pre-exit Predictor", "content": "Traditionally, most early-exit methods decide whether to\nexit at the end of each branch computation [21, 27, 55]. This\napproach limits hardware acceleration and batching, as exit\npoints vary by data, leading to inconsistent workloads within\nbatches and memory fragmentation [21, 22, 56]. Although\nsome predictive models for CNNs [22] predict exit values in\nadvance, they cannot scale to MEMs due to their convolution-\nspecific design. In this work, we propose a unified, light-\nweight early-exit predictor model for all modalities, derived\nfrom intermediate data embeddings. The data-aware pre-exit\npredictor preemptively decides the exit point for MEMs, en-\nabling batch scheduling for better parallelism and helping to\namortize and hide loading time.\nData-aware coarse-grained embedding granularity Dif-\nferent data contains varying amounts of information content.\nUnlike previous work that defines predictive models manu-\nally, we propose using intermediate embeddings to predict\nthe exit value without supervision. First, we build the fine-\ngrained embedding Fx for each data point x \u2208 X as a proxy\nquery label. Next, we feed the input into the pre-trained\nMEM layer by layer, obtaining a set of coarse-grained em-\nbeddings Ci at different granularities i \u2208 range(layers). We\nthen measure the similarity between the fine-grained and\ncoarse-grained embeddings. When the similarity between Fx\nand Ci becomes the largest among Fx and C, query retrieves\nC from C successfully. We mark it as a valid embedding\nexit. The intermediate embeddings are fed into the predictor\nmodel, and an MLP model is trained to predict its exit value."}, {"title": "3.3 Progressive LoRA Healing", "content": "Original MEMs are not designed for early exit, as they tend\nto distribute computation across all layers. As a result, most\ndata requires many layers before exiting. We propose a pro-\ngressive LoRA approach to heal the model, reducing the\nnumber of layers needed for each token.\nParameter-efficient LoRA Healing Previous early-exit\nhealing approaches [26] use the parameter-efficient fine-\ntuning method, LoRA [28], to distill knowledge into lower\nlayers, reducing the number of layers required for each token.\nNaive LoRA tuning fine-tunes a separate LoRA suite for each\nearly-exit layer. For instance, with 32 exits, 32 LoRA suites\nare required. While this ensures good performance, it has a\nsignificant drawback: the embedding from layer n cannot be\nreused to compute the embedding for layer n + 1. As illus-\ntrated in Figure 9, this occurs because LoRA1\u2026n for layer n\nis not the same as the first n layers of LoRA 11\u2026n+1. Unlike\nstandard embeddings, which complete all layers sequentially,\nearly-exit methods must check whether each layer is the\nfinal one. If layer n's embedding is incompatible with layer\nn + 1, the early-exit method must recompute the embedding\nfor layer n + 1 from scratch, negating many of the benefits\nof early exit.\nProgressive LoRA healing (P-LoRA) Recall proposes a\nprogressive LoRA healing method to address this issue, aim-\ning to use a single LoRA suite for all exits. To achieve this, we\ntune the LoRA layer by layer. For each exit, we tune only the\nLORA for the current exit while keeping the previous exits'\nLORA fixed. Since the tunable parameters are fewer than the\nfixed ones, the healing capacity is weaker compared to using\nseparate LoRA suites, which negatively impacts convergence\n(i.e., fine-grained embedding) performance, as shown in Fig-\nure 10. To mitigate this, instead of tuning one LoRA layer\nat a time, we progressively tune more LoRA layers at later\nexits. Similar to the window size in convolutional layers, we\ndefine the number of tuned LoRA layers as the LoRA step.\nP-LORA step decision As shown in Figure 10, the optimal\nhealing step varies across exit layers. In general, the larger\nthe n, the greater the per-step healing capacity, due to the\nincreased number of tunable parameters. However, if step 4\nis applied to all exits, exits 2 and 3 will miss opportunities for\nhealing. This is acceptable for the top layers, as they already\nhave a strong feature representation from earlier healing.\nLarger steps benefit later layers by improving convergence\nperformance. For smaller exits, earlier features are still weak\nand require healing at each exit.\nTo determine the optimal step during training, we use\ninformation from the predicted exit statistics. We set the\ntraining step at the pivot of the predicted exit statistics, en-\nsuring that most exits are healed with an appropriate step\nsize. This approach prioritizes smaller exits, aligning with the\nheuristic that most data exits occur at earlier layers, which\nrequire more focused healing. At later stages, larger steps\nenhance fine-grained performance during queries without\nsignificantly affecting exit flexibility.\nTraining Details The healing P-LoRA is designed to be\nparameter-efficient and highly transferable. Application de-\nvelopers can customize the personalized healing adapter"}, {"title": "3.4 Speculative Fine-grained Retrieval", "content": "With coarse-grained embeddings, we can filter out poten-\ntial candidates. Further fine-grained embeddings are then\nprocessed on these filtered candidates to complete the final\nretrieval. However, using the default query embedding with\na full-capacity encoder does not achieve precise top-1 re-\ntrieval (R@1), as shown in Figure 11. This poor performance\nstems from two unique challenges.\n# Challenge 1: Reduced embedding capacity. Even if we\nmodify the model to predict early and align it with the full\nembedding, exiting early during inference inevitably reduces\naccuracy compared to full-capacity embedding. Fortunately,\nwhile coarse-grained embeddings may not achieve precise\ntop-1 retrieval, they can filter out the most likely candidates\nwhen expanding the retrieval range to top-10 as shown in\nFigure 11a. Thus, this challenge can be alleviated by refining\nthe coarse-grained embeddings filtered with query informa-\ntion.\n# Challenge 2: Unbalanced embedding distribution. As de-\nscribed in \u00a73.2, different data exits at different layers, leading\nto unbalanced embeddings in storage. Although each em-\nbedding is fine-tuned to approximate the full embedding,\nembeddings from different exit layers retain unique charac-\nteristics. For example, samples from similar exit layers tend\nto have similar embedding distributions. As a result, a query\nembedding from a full-capacity encoder cannot retrieve these\nembeddings precisely. This phenomenon is shown in Fig-\nure 11. For single-modality retrieval on the HARSMART\ndataset, using the full-capacity MEM to retrieve filtered em-\nbeddings results in a top-1 accuracy of only 24.9%, 56.6%\nlower than using a 2-layer query embedding, since over 99%\nof samples exit before 3 layers during local embedding. The\nsame phenomenon occurs in the cross-modal TWITTER\ndataset.\nSpeculative retrieval Inspired by speculative decoding [57],\na popular acceleration technique for language models, we\npropose feeding the query embedding at different granu-\nlarities to achieve balanced filtering, as shown in Figure 12.\n(1) Speculative filtering: The top k candidates at each query\ngranularity are preserved for the second round of filtering.\n(2) Global verifying: The second round selects the final top k\ncandidates from all granularities. If a sample ID is duplicated,\nthe candidate with the next highest score is preserved. (3)\nFine-grained correcting: Finally, the coarse-grained embed-\ndings are refined using the rest of the model to generate\nfine-grained embeddings, which are then matched with the\nquery for more precise retrieval.\nIntermediate results reuse As shown in Figure 9, the\ncoarse-grained embedding can be reused for fine-grained\nembedding. However, due to the down-sampling structure of\nthe output head, the coarse-grained embedding cannot be di-\nrectly used for fine-grained embedding. To simplify this, we\nstore the intermediate activations before each down-sample\nlayer. This approach allows reusing the superficial embed-\nding to reduce the cost of data-aware coarse-grained embed-\nding, improving embedding throughput by up to 30%. It also\nextends the coarse-grained embedding to fine-grained em-\nbedding without encoding from scratch, accelerating query\nlatency by up to 70%.\nCache analysis The drawback of this approach is the need\nto cache intermediate activations. Fortunately, we can quan-\ntize them to INT4 and de-quantize them during reuse, which\ntakes significantly less time than re-computation (around\n10 ms per embedding). During prediction, the activations\ncan remain in RAM. Once coarse-grained embedding begins,\nthese cached activations replace the intermediate variables\ntypically stored in RAM during embedding, so no additional\npeak memory footprint is required. After the process ends,"}, {"title": "4 Implementation and Setup", "content": "Recall is built on ImageBind [11", "58": "are orthogonal to Recall. LoRA\ntuning and embedding accuracy evaluations are emulated\non a GPU server to enable faster iterations and energy sav-\nings. Embedding inference latency", "43": "."}, {"43": "currently does\nnot support the IMU modality and lacks GPU optimizations", "alternatives": "n(1) Multimodal Embedding Model (MEM) without any opti-\nmization. (2) BranchyNet [21", "59": "a novel early-exit-\naware batching algorithm that allows sample preemption at\nruntim"}]}