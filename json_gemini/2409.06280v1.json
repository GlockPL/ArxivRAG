{"title": "Catch Me if You Can: Detecting Unauthorized Data Use in Deep Learning Models", "authors": ["Zitao Chen", "Karthik Pattabiraman"], "abstract": "The rise of deep learning (DL) has led to a surging demand for training data, which incentivizes the creators of DL models to trawl through the Internet for training materials. Meanwhile, users often have limited control over whether their data (e.g., facial images) are used to train DL models without their consent, which has engendered pressing concerns.\nThis work proposes MembershipTracker, a practical data provenance tool that can empower ordinary users to take agency in detecting the unauthorized use of their data in training DL models. We view tracing data provenance through the lens of membership inference (MI). MembershipTracker consists of a lightweight data marking component to mark the target data with small and targeted changes, which can be strongly memorized by the model trained on them; and a specialized MI-based verification process to audit whether the model exhibits strong memorization on the target samples.\nOverall, MembershipTracker only requires the users to mark a small fraction of data (0.005%~0.1% in proportion to the training set), and it enables the users to reliably detect the unauthorized use of their data (average 0% FPR@100% TPR). We show that MembershipTracker is highly effective across various settings, including industry-scale training on the full-size ImageNet-1k dataset. We finally evaluate MembershipTracker under multiple classes of countermeasures.", "sections": [{"title": "1 Introduction", "content": "Modern deep learning (DL) models have attained remarkable performance in various tasks such as image classification and language generation. Their success is largely driven by the availability of massive training data [50, 52]. However, it is not always clear whether the data used to train these models has been used with the permission of the data owners. For instance, Clearview.ai, in developing their facial recognition system, scraped billions of user photos from social media platform without the users' consent [26]. The unregulated use of personal data for building DL models has engendered mounting concerns [20, 26, 66], and is in violation of privacy laws such as the European Union's General Data Protection Regulation (GDPR) [2]. In spite of this, data holders often have limited agency in detecting the unauthorized use of their data due to the lack of practical data provenance tools.\nThis leads to two main lines of work for data provenance. The first line of work seek to inject designated features into the user data, which can induce the model to exhibit a certain detectable behavior for provenance purposes [27, 39,53,68]. The injected features can take the form of a backdoor trigger pattern [27,39]; or a spurious feature pattern that can cause output probability shift by the model [68]. However, these techniques either impose strong assumptions on the users (e.g., the ability to mark a large fraction of data) [27,39,53] and/or suffer from limited auditing performance [39, 53, 68].\nThe second line of work is based on membership inference (MI). There are instance- and user-level MI - the former infers whether a specific sample is used to train a model [8,56]; while the latter detects the usage of any of the user's data [12, 32,58]. Our work focuses on auditing whether specific user samples are used for training the target model. Unfortunately, existing MI methods often suffer from limited effectiveness, as measured by their true positive rate (TPR) and false positive rate (FPR) [8]. However, a high-power MI method is needed to facilitate data provenance with low FPR (to avoid false accusation) and high TPR (for correct data provenance).\nTo improve, recent studies propose several techniques based on data poisoning [10, 13, 65]. Their common idea is to inject mislabeled samples into the model's training set, which can amplify the model's behavioral changes on some target samples, thereby making them easier to be de-identified. To apply these techniques for data provenance, the users have to be able to mislabel some training data, which, unfortunately, excludes the real-world scenarios where the data labels are assigned by the model owner or an external service [19,21]. This renders these techniques challenging for real-world use.\nThis work. We present MembershipTracker, a MI-based data provenance tool that can overcome the aforementioned challenges, with a two-step process as illustrated in Fig. 1."}, {"title": "2 Related Work", "content": "Our work focuses on detecting the unauthorized use of personal data in training DL models, which is complementary to existing work that aim to protect the data from unwanted use by rendering them un-learnable [30, 55]. Hence we focus on existing literature for data provenance, and we classify them into two categories.\nNon membership-inference based solutions. We divide them into dataset- and user-level solutions.\nDataset level provenance aims to detect whether a DL model is trained on a specific dataset [39, 46, 53]. Maini et al. propose dataset inference, which detects whether two models trained on the same dataset exhibit similarities in their decision boundaries [46]. Other work modify portions of training set to leave a certain artifact on the models, such as radioactive data [53] and backdoor watermark [39]. However, recent work [31] finds that these techniques [39,53] still have poor auditing performance.\nMoreover, dataset level solutions require several assumptions that are outside the scope of our work. First, many of them require control over a nontrivial portion of training set (e.g., 10%~20% [39,53]). Some solutions assume access to some known feature extractor [53], access to the training set or the ability to train a proxy model [46, 53]. These are well beyond the user capability we consider (in Section 3.1.1).\nUser level provenance seeks to detect whether the users' data are used to train a DL model [27,68], and they require only modifying the users' own data. Their idea is to mark the users' data with a designated feature to induce a detectable behavior into the model. Hu et al. propose to inject backdoor trigger feature into the user data [27], but they assume the user capability to manipulate the data labels. Other studies on clean-label backdoor attacks may be repurposed to overcome this, but they still require training a proxy model to compute the poisoned data [59, 73].\nWenger et al. propose to inject a target spurious feature into the user data to be learned by the suspected model, and then detect whether the model has associated the injected feature with the target class label [68]. To detect, the users can separately overlay the target and non-target features to some auxiliary data (outside the target class), and then determine whether the target feature causes a slightly higher prediction probability on the target class, compared with the non-target features (e.g., 10% vs. 1%). Both their work and ours involve adding spurious features to mark users' data, but there are several major differences between the two. Notably, our analysis reveals that they [68] significantly underestimate their detection false positives (from 0% to >30%). This is because they employ a discrepant detection procedure for the testing features that are used for training (true positives), and those that are not (false positives); however, a fair evaluation should follow a consistent procedure (details in Appendix D). Moreover, we also realign their technique for MI-based data provenance in our work, and find that even when their approach has incurred higher distortion to the data, it still has poorer MI performance than MembershipTracker (Appendix D).\nMembership-inference (MI) based solutions. MI also represents a natural fit for tracing data provenance, and there are instance- and user-level MI solutions.\nInstance-level MI detects whether a specific instance is used to train a model. It was first proposed by Shokri et al. [56] and there are many follow-up studies [6, 8, 16, 70, 71]. However, even state-of-the-art MI methods [6, 8, 70] still struggle with achieving high TPR when controlled at the low FPR regime (more in Section 4.1).\nThis has led to several attempts at improving the effectiveness of existing MI methods via data poisoning. Tramer et al. propose to inject mislabeled samples into the model's training set, which can transform the target samples into outliers and amplify their influence on the model's decision, thereby making the target samples easier to be de-identified [65]. While injecting mislabeled samples represents an effective solution for improving the MI success [10, 13, 65], it may be challenging for the users to apply to their data, because in many scenarios the users may not have control over the data labels [19, 21, 68]. Even if this is feasible, we find that such a method can still only increase the MI success to a limited extent (validated in Appendix E). By contrast, MembershipTracker does not assume control of the data labels, and is still able to facilitate high-power MI for tracing data provenance.\nUser-level MI aims to detect whether any of a user's data is used to train a model. Unlike instance-level MI, the auditor possesses a set of samples from a target user that are not necessarily used to train the target model. Song et al. [58] propose the first user-level MI for natural language models, which first trains multiple shadow models, and then uses the outputs from the shadow models as the features to train an audit model. Subsequent work develop solutions under various settings, including speech recognition models [11,48], metric learning models [12,38] and large language models [32, 47].\nA common thread of existing user-level MI methods is to aggregate the information across multiple samples (e.g., different voices of the same speaker) to perform MI [11, 12, 32, 58]. The MI process in MembershipTracker follows a similar philosophy. However, prior work often requires access to additional reference models [32] or shadow models [11, 12,58], and they still have limited MI effectiveness (e.g., limited TPR under the low FPR regime [32,47]). MembershipTracker overcomes these limitations by means of a novel data-marking and specialized MI auditing process, which can simultaneously achieve high TPR and low FPR for data provenance.\nConcurrent work by Huang et al. [31] proposes a general framework for auditing unauthorized data use in training DL models. Their idea is to generate two perturbed copies of the target data, randomly publish one of them, and then compare the model's membership score on the published vs. the unpublished one. While their framework can work for different domains such as foundation models, they still require the data holders to mark a substantial portion (1%~10%) of dataset. Instead, MembershipTracker considers the more challenging (and realistic) setting where the marked data amounts to \u2264 0.1% of the dataset. We also \"stress test\" their technique under a similar setting as ours and find that their performance degrades considerably then (see Appendix F)."}, {"title": "3 Problem Formulation", "content": "Membership Inference (MI) Game. We denote \\(F_\\theta : \\mathcal{X} \\rightarrow [0,1]^n\\) as a model that maps an input sample \\(x \\in \\mathcal{X}\\) to a probability vector over \\(n\\) classes. \\(\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^m\\) is a training set sampled from some distribution \\(\\mathcal{D}\\). \\(F_\\theta \\leftarrow \\mathcal{T}(\\mathcal{D})\\) denotes a model \\(F_\\theta\\) produced from running the training algorithm \\(\\mathcal{T}\\) on \\(\\mathcal{D}\\). Next, we define a MI game, which proceeds between a challenger \\(\\mathcal{C}\\) and an adversary \\(\\mathcal{A}\\).\n1. The challenger samples a dataset \\(\\mathcal{D} \\leftarrow \\mathbb{D}\\), and a target point \\(z \\leftarrow \\pi\\) (such that \\(\\mathcal{D} \\cap \\pi = \\emptyset\\)).\n2. The challenger trains a model \\(F_\\theta \\leftarrow \\mathcal{T}(\\mathcal{D} \\cup \\{z\\})\\) on the dataset \\(\\mathcal{D}\\) and target point \\(z\\).\n3. The challenger gives the adversary query access to \\(F_\\theta\\).\n4. The adversary emits a guess \\(\\hat{z} \\in \\pi\\).\n5. The adversary wins the game if \\(\\hat{z} = z\\).\nFor a specific target point \\(x\\), \\(\\pi = \\{x, \\cancel{x}\\} \\), where \\(\\cancel{x}\\) indicates the absence of an example. The adversary's goal is to guess whether the model \\(F\\) is trained on \\(\\mathcal{D}\\) or \\(\\mathcal{D} \\cup \\{x\\}\\).\n3.1 MI for Data Provenance\nWe now describe the MI game for data provenance, which proceeds between a challenger (model creator) \\(\\mathcal{C}\\) and a target user \\(\\mathcal{U}\\). We consider image classification in this work, and \\(\\mathcal{U}\\) seeks to audit whether his/her images (denoted as \\(\\mathcal{X}\\)) are used by \\(\\mathcal{C}\\) to train a model \\(F\\) without permission. We add the ability for \\(\\mathcal{U}\\) to modify \\(\\mathcal{X}\\) for provenance purpose.\n1. The user is allowed to modify \\(\\mathcal{X} \\in \\pi\\) (\\(\\pi = \\{\\mathcal{X}, \\cancel{\\mathcal{X}}\\} \\)).\n2. The challenger samples a dataset \\(\\mathcal{D} \\leftarrow \\mathbb{D}\\), and a target set \\(\\mathcal{Z} \\leftarrow \\pi\\) (such that \\(\\mathcal{D} \\cap \\pi = \\emptyset\\)).\n3. The challenger trains a model \\(F_\\theta \\leftarrow \\mathcal{T}(\\mathcal{D} \\cup \\{\\mathcal{Z}\\})\\) on the dataset \\(\\mathcal{D}\\) and target set \\(\\mathcal{Z}\\).\n4. The challenger gives the user query access to \\(F_\\theta\\).\n5. The user emits a guess \\(\\hat{Z} \\in \\pi\\).\n6. The user wins the game if \\(\\hat{Z} = \\mathcal{Z}\\).\nFor a given user \\(\\mathcal{U}\\), \\(\\pi = \\{\\mathcal{X}, \\cancel{\\mathcal{X}}\\} \\), where \\(\\cancel{\\mathcal{X}}\\) indicates the absence of the target samples by \\(\\mathcal{U}\\), and the goal is to determine whether \\(F\\) is trained on \\(\\mathcal{D}\\) or \\(\\mathcal{D} \\cup \\{\\mathcal{X}\\} \\).\n3.1.1 User capability\nWe consider ordinary personal users \\(\\mathcal{U}\\) with limited ML expertise and resources, and they are willing to add visual distortion to their data for provenance purposes. We next specify their capabilities and constraints.\nFirst, we assume \\(\\mathcal{U}\\) can only mark a small number of samples, and these are to be included as part of a larger dataset. This simulates the realistic scenario where the dataset is curated from multiple data sources (e.g., [19]), and thus data contributed by each user constitutes only a small portion of the dataset (e.g., 0.1%).\nNext, in the provenance verification process, we assume \\(\\mathcal{U}\\) has access to an auxiliary set of non-member samples, which is a common assumption to control the MI process at a low FPR regime [6, 8, 70]. \\(\\mathcal{U}\\) also has black-box access to query the target model. In a real world setting, users often have limited control and knowledge beyond their own data, and thus we specify the constraints by \\(\\mathcal{U}\\) as follows.\n\u2022 \\(\\mathcal{U}\\) cannot modify their data's labels, which are assigned by the model owner or an external service [19,21,68].\n\u2022 \\(\\mathcal{U}\\) has no access to \\(\\mathcal{D}\\) (i.e., the training data by other users).\n\u2022 \\(\\mathcal{U}\\) has no knowledge of the black-box model \\(F\\).\n\u2022 \\(\\mathcal{U}\\) has no expertise/resources to train any shadow/proxy models, as in many settings, training even a single shadow model can impose prohibitively high data and compute requirements (especially for the large models) [65].\n3.1.2 Evaluation metric\nWe follow the best practice to measure the MI success by its true positive rate (TPR) and false positive rate (FPR) [8]. There are two ways to define the success metric: \u2460 TPR under a fixed FPR; or \u2461 FPR under a fixed TPR. We use metric \u2461, and we first explain why \u2460 is unsuitable for evaluating the data-provenance based MI method in our work.\nMotivation. Metric \u2460 is commonly adopted by existing work where MI is posed as an attack to expose privacy leakage in DL models [6, 8, 70]. Their goal is to evaluate whether the adversary can reliably de-identify (even just a few of) the training samples. This places emphasis on the requirement of low FPR, for which \u2460 represents a suitable metric [6, 8, 70].\nIn comparison, in our work of tracing data provenance, both low FPR (to avoid false accusation) and high TPR (for correct data provenance) are crucial. However, manually choosing a low FPR threshold and evaluating the corresponding TPR (as in \u2460) can be undesirable, as the pre-defined FPR threshold may be too conservative or aggressive."}, {"title": "4 Methodology", "content": "We first describe our design goals, and then conduct an empirical study to understand why directly leveraging existing MI methods cannot meet our goals. We then present MembershipTracker, a two-step technique with a data marking and provenance verification process: first by marking the users' protected data with targeted changes (Section 4.3); then performing a specialized MI process to audit whether the marked data are used to train the model (Section 4.4).\nDesign Goals. There are three design goals in our work.\nGoal (1): High provenance effectiveness. Our main goal is to enable those users whose data are misused for model training to perform successful data provenance (100% TPR), and we also want to ensure low FPR to avoid false accusation.\nGoal (2): Preserving high visual quality. The modification created to mark the users' data should not severely distort their visual information, e.g., the identity in a facial image should still be easily recognizable.\nGoal (3): Minimal technical requirements. The provenance tool should be usable by ordinary users even with limited expertise and resources (specified in Section 3.1.1).\n4.1 Motivation\nOur work addresses data provenance through MI, and there are today a number of available MI methods [6, 8, 65, 70]. However, these methods still suffer from limited MI effectiveness and/or impose strong assumptions on the users - we elaborate both limitations next.\n\u2460 Existing MI methods have limited provenance effectiveness, and incur very high FPR under 100% TPR.\nTo validate this, we use two methods in the state-of-the-art Likelihood-ratio attack, one is with and the other without shadow model calibration (128 shadow models) [8]. We evaluate both methods on a WideResNet-28-4 model trained on half of the training set in CIFAR100 dataset, and they both incur a high FPR of 75.32% and 64.62%, under 100% TPR.\nWith that said, the above results should not be interpreted as the failure of these approaches, because they were originally built from an adversarial perspective to expose the worst-case privacy leakage in DL models, where even just reliably de-identifying a few member samples is considered meaningful [8], e.g., the shadow-model based method in Fig. 3 can achieve 27.21% TPR@0.1% FPR.\nOn the other hand, our work considers MI for data provenance purposes, which represents a more challenging task, and requires both low FPR and high TPR. The high TPR is crucial for the users to be able to detect the unauthorized use of their data, while low FPR is important to minimize the chance of false accusation.\nAs in Fig. 3, existing MI methods still struggle with this task. The reason is that the training samples are not always strongly memorized by the model (except some outliers) [8, 65,71], which is directly related to the ability of performing accurate MI. This problem motivates related work [10, 13,65] to amplify the model's memorization on the training samples, and we discuss their limitation next.\n\u2461 State-of-the-art MI methods either require training shadow models or assume control to manipulate the data labels, both are beyond ordinary users' capabilities.\nFor example, leading MI methods [6, 8, 70] commonly require training shadow models to calibrate the inference process, which assumes additional data and compute access, and is challenging for the non-expert users.\nThere are other methods that do not require shadow-model calibration, but they either suffer from poor effectiveness [8, 71] or impose unrealistic assumption on the users [10,65]. For instance, the approach by Tramer et al. [65] can improve the MI success without shadow-model calibration, but it instead assumes the users have the ability to mislabel some training data, which can be challenging in the cases where the users have no control over the data labels [19,21,68]. Even if this is feasible, we find that the increase of MI success is still largely limited (see Appendix E).\n4.2 Overview\nWe next present MembershipTracker, a technique that requires neither shadow-model training nor manipulating data labels, and can still achieve high-power MI (with low FPR and high TPR) for tracing data provenance. It consists of a data marking and a MI-based verification process - we explain them next.\n4.3 Data Marking\nThe goal of data marking is to ensure that the membership of the marked samples can be reliably de-identified. To this end, we first observe that the ability to perform accurate MI is directly related to the model's propensity in memorizing individual data points, and data that are strongly memorized by the model (e.g., outlier samples, mislabeled samples) are known to be easier to be de-identified [8, 14, 65, 71]. Based on this, our goal can be formulated as: how can the users modify their data to amplify the model's memorization on the target data while preserving high visual quality?\nIn Section 4.3.1, we first present an initial approach that is highly effective in inducing the target samples to be memorized by the model, but it also completely destroys the visual information in the data. We use this effective yet unrealistic method as a strawman approach, and then introduce our proposal of a two-step solution, which can greatly reduce the visual distortion to the data while still fulfilling our goal.\n4.3.1 An initial strawman approach\nInspired by the observation that atypical samples presented in the dataset are easier to be de-identified (e.g., mislabeled samples [65], or samples that are out-of-distribution - OOD [8]), a straightforward approach is to directly replace the original target samples as OOD samples.\nThere are different ways to generate OOD samples, and we develop a simple method that creates samples consisting of random color stripes, such as the one shown below (the method's details are deferred to Section 5). Meanwhile, other alternative methods such as using samples from an OOD dataset may also be considered (Appendix C.1.3).\nWe first conduct an experiment to validate the effectiveness of the above approach. We use the CIFAR100 dataset with half of the training set for model training. We consider 100 target users from different classes replacing their data as some random OOD samples, and each contributes a small number of samples to the training set (0.1% in proportion). In terms of the signal function for MI, we follow prior work [54, 70] to use the prediction loss values.\nTo perform MI, we compare the prediction loss on the target OOD samples (from the member users) and some non-target OOD samples (from the non-member users). We find that the model indeed strongly memorizes those OOD samples present in the training set, for which we observe a 0% FPR@100% TPR. In comparison, if the target data are unmodified, the model only exhibits weak memorization on them, which yields 56.32% FPR@100% TPR.\nLimitation. Despite its effectiveness, this approach completely removes the visual information in the data (e.g., the facial identity is no longer recognizable), and they are also easy to notice. Next, we present two methods that can greatly reduce the visual distortion to the target data, while still enabling them to be strongly memorized by the model (Fig. 4).\n4.3.2 Step 1: Image blending\nAs discussed earlier, the marked samples should contain the original feature (to preserve visual information), as well as the OOD feature (for provenance purposes). Inspired by the common image blending technique [68, 74], we propose to blend the target sample with the OOD feature:\n\\(x(x_{\\text{ood}}, m) = m \\cdot x + (1 - m) \\cdot x_{\\text{ood}},\\)\n(1)\nwhere \\(m\\) moderates the contribution of different features. By using a large \\(m\\), we can largely preserve the high image quality (e.g., the center image in Fig. 4 is marked with \\(m = 0.7\\)) while keeping the OOD feature to amplify the model's memorization on the resulting samples. We find that merely blending the OOD feature into the target samples (\\(m = 0.7\\)) can reduce the FPR@100% TPR from 56.32% to 23.58%.\nLimitation. Although the FPR is largely reduced, it is still too high. We find that this is due to the existence of the original feature, which hinders the model's memorization on the OOD feature. We next explain how to mitigate this.\n4.3.3 Step 2: Noise injection\nOur idea is to inject a small amount of noise to suppress the influence of the original feature, and have the model memorize the OOD feature.\nFor this, we draw inspiration from the concept of adversarial samples, which works by adding imperceptible noise to the inputs and cause them to be misclassified by the model [17,40,43]. The perturbation can thus be viewed as the noise that can suppress the influence of the original feature and cause the model to predict the wrong label.\nIn our context, we can inject adversarial perturbation into the users' target samples and prompt the model to memorize the OOD feature. There are different ways to generate the perturbation. Optimization-based approach requires access to some proxy model to generate the perturbation [40,43], which can still be challenging for the ordinary users we consider. We therefore resort to another optimization-free strategy that does not assume any additional access. In particular, Co et al. [17] propose that procedural noise functions can be used to generate input-agnostic adversarial perturbation. These functions are commonly used in computer graphics to generate different textures and patterns and enrich the image details [17]. Co et al. find that the procedural noise patterns share visual resemblance with existing adversarial perturbation patterns, and demonstrate that they can be similarly used to construct adversarial samples."}, {"title": "4.4 Data Provenance via Membership Inference", "content": "To start with, we note that so far we have been considering MI on a common instance basis [6, 8, 56, 70], which uses the per-instance loss as the signal function value for MI. We first explain why this approach would incur high FPR, and then present a specialized MI process to overcome it.\n4.4.1 Why instance-based MI suffers from high FPR\nIt uses the per-sample loss for MI, which can incur high FPR even if the prediction loss on the target marked samples are very low. This is because there may exist many non-member samples with low loss values as well, which would be mis-identified as member samples, and result in high FPR.\nFor instance, the left figure in Fig. 5 shows the individual loss values on different samples, where many non-member samples also yield low prediction loss (the low loss values are shown towards the right side because they are plotted in the logit-scaled form [8] for visualization purposes). This consequently leads to a 7.97% FPR @100% TPR.\n4.4.2 Set-based membership inference for reliable data provenance\nTo overcome the above, our key insight is that, even though each target user only contributes a small number of samples to the model's training set (e.g., 25 out of 25,000 instances), they can still leverage the collective information across their own samples to construct a more reliable signal function for MI, thereby reducing the FPR.\nTo realize this, we start by identifying a pattern that can distinguish the model's behavior on the member and non-member samples, and it is as follows. For the non-member samples, although a few of them may have low loss, the majority of them would incur much higher prediction loss, because these samples are not used for training the model (e.g., see the left of Fig. 5). In contrast, the member samples predominantly have very low prediction loss.\nTherefore, we make the observation that the average loss for the samples by the non-member users are higher than that by the member users. A visualization for this is on the right of Fig. 5. This leads to our proposal of the set-based MI process, which leverages the average loss of the small set of target samples by each user as the signal function for MI. We explain the verification process next.\n4.4.3 Provenance verification process\nAlgorithm 1 outlines the process. Let each user possess k samples, where k is a small number compared with the dataset size (e.g., 25 out of 25,000 samples). \\(\\mathbb{P}_{out}\\) denotes a set of non-member data to control the MI process at the low FPR regime (Section 3.1.1), which are also marked with some random outlier features and perlin noise (data-marking details in Section 5). By comparing the model's behavior on the target data and the data in \\(\\mathbb{P}_{out}\\), the auditing user \\(\\mathcal{U}\\) is to detect whether his/her target data are used to train \\(F\\).\nThe first step is to derive an inference threshold c based on a pre-defined FPR (\\(\\alpha\\)). \\(\\alpha\\) is usually a small number such as 0.1% or 0% to ensure low false positive (e.g., Section 5 shows that MembershipTracker can support the users to achieve 100% TPR with 0% FPR in many cases). To derive the threshold c, \\(\\mathcal{U}\\) first computes the empirical loss histogram for data in \\(\\mathbb{P}_{out}\\)."}, {"title": "5 Evaluation", "content": "Datasets and model training. We consider six common benchmark datasets, including CIFAR100 [35], CIFAR10 [35], ArtBench [41], CelebA [45], TinyImageNet [36] and ImageNet-1k [19]. This section considers the first five datasets and the evaluation on the ImageNet-1k dataset is in Section 6. We follow standard model training practice such as using data augmentations and validation set - the model training details are in Appendix A.\nMembershipTracker configuration. We explain the default configurations for the main experiments, and we study different configurations in the ablation study (Appendix C).\nAs explained earlier, we create outlier features as random color stripes, each with 16 stripes from 11 different common colors. Such a large feature set (1116 choices) can support multiple users to generate their own features for data marking, and we use a image blending ratio m = 0.7. Next, we follow Co et al. [17] to generate random perlin noise for each sample with \u03b4 = 8/255. We provide additional visualizations of the original and marked samples in Appendix A.\nWe assume each user possesses a small number of samples from a given class, which constitute 0.1% of the training set (the detailed dataset sizes are in Appendix A).\nAs explained in Section 3.1.2, we adopt FPR@100% TPR as the evaluation metric, and we use 5,000 non-member users for computing FPR. We use all the samples that are not used for training as the non-member set. Due to the limited size of the non-member set, the non-member users' samples are drawn randomly from the non-member set (with replacement), and these are similarly marked with random outlier features and perlin noise.\nTo perform MI, we directly use the cross-entropy loss as the signal function value, which is easy to compute by the users. We consider the global-threshold method by Carlini et al. [8] as the baseline method; and we do not choose the shadow-model-based approaches [6, 8, 70]. The reason is that, even though these methods can achieve better results, they still yield limited increase of MI success (Section 4.1); and more importantly, training shadow models is prohibitively challenging for ordinary personal users to carry out (Section 3.1.1).\nThe closest work to ours in improving the MI success without shadow-model calibration is Tramer et al. [65]. However, their technique assumes the users can manipulate the data labels, which excludes the real-world scenarios where the data labels are assigned by the model creator or an external service [19,21,68]. Though MembershipTracker does not impose such an assumption, for completeness, we still quantitatively compare it with the method by Tramer et al. (Appendix E).\n5.1 Results\nWe first evaluate the single-target-user setting, and then the multi-target setting.\nSingle-target evaluation. For each experiment, we randomly choose 5 target users from different classes and report the average results. Table 1 presents the results.\nAs shown, with MembershipTracker, the users can reliably trace the provenance of their marked data with 0% FPR@100% TPR. In comparison, without MembershipTracker, performing standard instance-based MI on the unmodified target samples incurs much higher FPR, with an average of 47.52% FPR@100% TPR. This is because: 1) the unmarked samples cannot be strongly memorized by the model; and 2) the instance-based MI process fails to leverage the collective information across the target samples by each user. These two limitations are overcome by MembershipTracker's data marking and set-based MI process; and the detailed ablation study on these two components is presented in Appendix \u0421."}, {"title": "5.2 Robustness to Countermeasures", "content": "While Section 5.1 demonstrates MembershipTracker's capability in enabling the users to perform reliable data provenance, a deliberate model creator can also employ countermeasures to sabotage MembershipTracker.\nTo understand this, we evaluate a total of 6 types of countermeasures in the model, input and output level and we use the CIFAR100 dataset. For the model-level defenses that require model training, we first consider the single-target setting (similar to Section 5.1), and then the multi-target setting. For the input- and output-level defenses, we consider the more challenging multi-target setting (100 targets in total).\n5.2.1 MI defense\nWe first evaluate several representative privacy defenses for mitigating MI (five in total).\nDP-based defense. Differentially private (DP) training [3", "4": "."}]}