{"title": "OAH-Net: A Deep Neural Network for Hologram Reconstruction of Off-axis Digital Holographic Microscope", "authors": ["Wei Liu", "Kerem Delikoyun", "Qianyu Chen", "Alperen Yildiz", "Si Ko Myo", "Win Sen Kuan", "John Tshon Yit Soong", "Matthew Edward Cove", "Oliver Hayden", "Hweekuan Lee"], "abstract": "Off-axis digital holographic microscopy is a high-throughput, label-free imaging technology that provides three-dimensional, high-resolution information about samples, particularly useful in large-scale cellular imaging. However, the hologram reconstruction process poses a significant bottleneck for timely data analysis. To address this challenge, we propose a novel reconstruction approach that integrates deep learning with the physical principles of off-axis holography. We initialized part of the network weights based on the physical principle and then fine-tuned them via weakly supersized learning. Our off-axis hologram network (OAH-Net) retrieves phase and amplitude images with errors that fall within the measurement error range attributable to hardware, and its reconstruction speed significantly surpasses the microscope's acquisition rate. Crucially, OAH-Net demonstrates remarkable external generalization capabilities on unseen samples with distinct patterns and can be seamlessly integrated with other models for downstream tasks to achieve end-to-end real-time hologram analysis. This capability further expands off-axis holography's applications in both biological and medical studies.", "sections": [{"title": "1 Introduction", "content": "Digital holographic microscopy (DHM) is emerging as an innovative imaging modality in computational microscopy. It provides high-resolution, quantitative, and three-dimensional information about samples without labelling. These unique features make DHM a promising technique for imaging living cells, as it captures intracellular structures while preserving cells in their natural state, which could be used for more precise analysis [1-4]. DHM records the interference pattern between the object and the reference beams, which is then reconstructed using algorithms to retrieve the wave of the object beam in terms of phase and amplitude [5-7]. Holography can be classified into two main types based on beam alignment: inline holography and off-axis holography [8, 9]. For inline holography, the reference beam is parallel to the object beam. Although the setup is relatively simple, the hologram reconstruction requires multiple exposures of the same sample at varying sample-to-sensor distances. For off-axis holography, the reference beam is slightly titled to form a small angle with the object beam, resulting"}, {"title": "2 Result", "content": "The architecture of OAH-Net is shown in Fig.1c, comprising two main modules: Fourier Imager Heads (FIHs) and Complex Valued Network (CVN). FIHs are designed to filter out the undesired component of the hologram in the Fourier frequency domain and to correct the frequency shift caused by the titled reference beam. The CVN module converts the object beam wave in complex-valued form to amplitude and unwrapped phase. In some DHM devices, including the one used in our experiments, there is more than one reference beam, and the object beam wave can be derived from any of them individually. In such cases, the CVN module is also responsible for merging the multiple waves of the object beam into one. Further details are elucidated in the Methods Section. The entire model is trained end-to-end in a supervised manner using blood cell samples. The input consists of holograms recorded in the presence of samples and a background hologram recorded without any sample. The target images, or ground truth, are the phase and amplitude images reconstructed from the sampled hologram via other methods. This study generated target images using Ovizio, an API software provided for our customized DHM with undisclosed technical details [27]."}, {"title": "2.1 Model architecture and training", "content": null}, {"title": "2.2 Model performance", "content": "We first assess the performance of the well-trained model in test data that recorded blood cell samples. Overall, OAH-Net's predictions are highly accurate. Practically, it is more important to accurately reconstruct regions with blood cells, defined as pixels with target phase values greater than 0.2. We denote the MAE specifically for cell regions as MAE\u00b2. MAE\u00b2 is slightly higher than MAE\u00b9, possibly due to the imbalanced data distribution shown in Fig. 3d. The pixels in the cell region constitute only 1.04% of all pixels in the blood cell dataset. Although the loss function is customized to encourage the model to focus more on the cell regions, the data imbalance issue cannot be solved entirely. Nevertheless, further analysis suggests that the error has negligible effects on downstream task performance. Fig.3e shows that predicting pixels with negative target phase values is more error-prone. These pixels are usually part of the background, and their deviation between the target and prediction values plays a positive effect in making the background more homogeneous. As shown in Eq. 2, each FIH involves three trainable matrices: Mi, Mr, and Mmask, which are multiplied with the Fourier-transformed hologram F(IH). Mmask functions as a filter in the frequency domain, with the corresponding initial and fine-tuned weights shown in Fig. 3a-c. The fine-tuned Mmask of both FIHs are distinct from the initial circular shape and exhibit interesting patterns that have not been reported before [24]. The initial and fine-tuned weights of Mi and Mr are shown in Fig. C3 and Fig. C4. Their initial weights consist only of 0 or 1, and are solely responsible for cropping and shifting the Fourier spectrum. After being fine-tuned as trainable parameters, Mr and Mr partially contributed to frequency filtration and improved model performance. During training, the weights of Mi, Mr, and Mmask are not strictly restricted within the range of [0, 1], but most of the fine-tuned values (approximately 99%) fall within this range. More detailed statistics are provided in the Supplementary."}, {"title": "2.3 Benchmarks and model variations", "content": "In Table 1, we compared the performance of other selected methods, the OAH-Net and OAH-Net variants, in the blood cell sample test dataset. Methods requiring manual selection or iterative processing were excluded, as they are unsuitable for high-throughput analysis."}, {"title": "2.4 Model generalization ability", "content": "To demonstrate the generalizability of OAH-Net, we tested the model, trained exclu-sively on blood cell samples, on unseen samples with distinct patterns. As shown in Fig.4a, the model performs consistently well in various patterns, suggesting that it fol-lows physical principles and without overfitting the training data. On the other hand, Fig. 4b shows a significant drop in phase SSIM for benchmark methods, particularly in parallel stripes and Siemens star sample. The overall good performance on Wedding cake sample is likely due to the large background area, with minimal phase or amplitude variance."}, {"title": "2.5 Performance in downstream task", "content": "In most cases, hologram reconstruction serves as a precursor to subsequent tasks such as classification and object detection for further sample analysis [30]. To assess the broader utility of our model, we performed additional validation by integrating it with two object detection models, YOLOv5 and YOLOv8 [12, 13]. Our approach involved training and testing the YOLO models initially on target images, followed by repeat-ing the process using images reconstructed by OAH-Net. Both YOLO models were trained using default settings without hyperparameter tuning. Performance compari-son, detailed in Table 2, reveals no significant differences in object detection metrics between the two image sources. This outcome suggests that the accelerated hologram reconstruction achieved by our model does not entail any discernible performance trade-offs in subsequent object detection tasks. Furthermore, typical cells of various types reconstructed via OAH-Net are shown in Fig.2c. The intracellular structures are well preserved and can be used for a more detailed analysis. Due to limited annota-tions, object detection models treat all white blood cells as the same class, disregarding their subclasses in this study."}, {"title": "3 Discussion", "content": "We demonstrated an end-to-end network for off-axis hologram reconstruction to address the critical bottleneck in high-throughput DHM applications. By integrating deep learning models with the physical principles of off-axis holography, OAH-Net achieves high speed and accuracy in hologram reconstruction, outperforming state-of-the-art methods. Our model processes the original high-resolution hologram (1536x2048 pixels) without downsizing or dividing it into smaller fields of view, thus preserving all detailed sample information for precise downstream analysis. Our tests showed that the reconstructed images using OAH-Net performed equally well in the object detection task of different types of blood cells using YOLOv5 and YOLOv8 com-pared to ground truth data. Meanwhile, streamlined computation allows OAH-Net to achieve an inference speed of less than 3 ms/frame, significantly faster than the acqui-sition rate of the microscope camera (9.5 ms/frame). This enables not only real-time hologram reconstruction but also potentially real-time end-to-end digital holographic microscopy-based diagnostic applications, including reconstruction and data analysis. Compared to frameworks[10, 11], OAH-Net reduces holographic reconstruction time by up to 97% (excluding downstream data analysis), achieving a TAT of less than 5 minutes (Fig.1b). In addition, OAH-Net addresses the challenge of high-throughput imaging storage by eliminating the need to store raw data. With real-time recon-struction, only the smaller reconstructed images (1/16 of the original size) are saved, significantly reducing storage requirements and costs. When integrated with down-stream models to achieve real-time analysis, storage can be further optimized to save only frames with regions of interest or stored temporarily on memory, allowing the use of more affordable hardware and enhancing the framework's practicality and scal-ability in clinical settings. Hence, OAH-Net not only enables real-time processing with"}, {"title": "4 Materials and methods", "content": "As shown in Fig. Ala, The customized DHM used in this study is provided by Ovizio Imaging Systems, Belgium, patented as \"differential digital holographic microscopy\" [32, 33]. The setup employs a 528 nm superluminescent light-emitting diode (SLED) from Osram for partially coherent Koehler illumination in transmission mode. It is"}, {"title": "4.1 Digital holographic microscopy", "content": null}, {"title": "4.2 Spatial separation of off-axis holograms in frequency domain", "content": "In our DHM device setup, there is one object beam and two reference beams tilted along the perpendicular x and y axes (shown in Fig. Alb). The intensity distribution IH recorded by the CCD sensor is the interference pattern between the reference waves and the object wave:\n          \n          $I_{H} = (R_{x} + R_{y} + O)(R_{x} + R_{y} + O)^{*}$\\\n          $= R_{x}R_{x}^{*} + R_{y}R_{y}^{*} + OO^{*} + OR_{x}^{*} + R_{x}O^{*} + OR_{y}^{*} + R_{y}O^{*} + R_{x}R_{y}^{*} + R_{y}R_{x}^{*}$\n          \n          where * represents the complex conjugate. Fig. Alc shows the terms in Eq. 1 can be effectively separated in the frequency domain, facilitating the exacting of $OR_{x}^{*}$ or $OR_{y}^{*}$ from IH. The object wave O can be reconstructed from either $OR_{x}^{*}$ or $OR_{y}^{*}$, with more details provided in the Supplementary."}, {"title": "4.3 Deep learning framework for hologram reconstruction", "content": "Our network architecture is shown in Fig. 1c. In our network, $OR^{*}$ separation and shifting in the Fourier frequency domain are performed by two matrix multiplications (denoted as \u00d7) and one element-wise matrix multiplication (denoted as ) in the FIHs module:\n\n          $F = M_{1} \\times F(I_{H}) \\times M_{r} M_{mask}$\n          \nwhere F is the Fourier transformation. IH is kept at high resolution with an original size of 1536 x 2048 pixels. The matrices Mi, Mr, and Mmask are two-dimensional trainable matrices, with shapes 384 \u00d7 1536, 2048 \u00d7 512, and 384 \u00d7 512, respectively. F"}, {"title": "4.4 Network training and testing", "content": "We trained OAH-Net in a supervised manner using target amplitude (A) and phase (\u03c6) images generated by Ovizio.api. As shown in Fig. 3d, there is a data imbalance between the background pixels and the sample pixels. Hence, we use a weighted L1 loss function to guide the model's focus on areas of higher importance:\n\n          $L_{loss} = \\sum_{i=1}^{n} \\sum_{j=1}^{w} \\sum_{k=1}^{h}W_{jk} |\\phi_{jk} - \\hat{\\phi}_{jk}| + W_{A} |A_{jk} - \\hat{A}_{jk}|$\n          \nwhere A and are the predicted amplitude and phase, respectively, j and k are the pixel coordinates, while w and h are the width and height of the image, respectively. i indicate the ith sample and n is the batch size. wa is the weight for amplitude errors, with a default value of 0.1. W is the pixel weight function:\n\n          $W_{jk}(\\hat{\\phi}_{jk}, \\phi_{jk}) = 40 \\times clamp[max(|\\hat{\\phi}_{jk}|, |\\phi_{jk}|), 0, 0.05] +20 \\times clamp[grad(\\phi_{jk}),0,0.1]+1$\n\nwhere\n\n          $clamp(x, a, b) = max[a, min(x, b)]$\n\ngrad(\u03c6jk) is the gradient of the target phase image derived by the Sobel filter [34] followed by a Gaussian blur. All networks mentioned in this paper were trained using the Adam optimizer [35] with a constant learning rate optimized by grid search. Training was halted when the validation loss showed no improvement for 200 successive epochs, and the model with the lowest validation loss was selected for further testing. The inference speed was measured with a batch size of 1 on a NVIDIA GeForce RTX 4090 GPU, after compiling the networks with the Pytorch JIT compiler [36]. The structural similarity index (SSIM) was measured using the corresponding function in scikit-image [37],"}, {"title": "Appendix A Spatial Separation of Off-Axis Holograms in Frequency Domain", "content": "Based on the Fourier spectrum, we deduce that there are two reference beams, as illustrated in Figure. A1. These two reference beams were tilted along the x and y axes, corresponding waves denoted as Rx (x,y) and Ry(x,y), respectively. The CCD sensor records the intensity distribution I\u043d (x, y), representing the interference pattern of the object beam wave O(x, y) and two reference beam waves:\n          $I_{H} = (R_{x} + R_{y} + O)(R_{x} + R_{y} + O)^{*}$\\\n          $= R_{x}R_{x}^{*} + R_{y}R_{y}^{*} + OO^{*} + OR_{x}^{*} + R_{x}O^{*} + OR_{y}^{*} + R_{y}O^{*} + R_{x}R_{y}^{*} + R_{y}R_{x}^{*}$\n          \nwhere * denotes the complex conjugation. When IH is recorded with sample, denoted as IH,s, the corresponding object wave is:\n\n          $O_{s}(x,y) = A_{o,s}(x, y)e^{i\\phi_{o}}e^{i\\Delta\\phi(x,y)}$\n          \nwhere Ao,s is the amplitude of the object wave, \u03c6o is the object wave phase, and \u2206\u03c6(x, y) is the phase shift caused by the refractive index variations of the sample. When IH is recorded in the absence of sample, denoted as IH,\u044c, the corresponding object wave is:\n\n          $O_{b}(x,y) = A_{o,b}(x, y)e^{i\\phi_{o}}$\n          \nNo sample-induced phase shift is present in Or(x, y). The sample signal is filtered from the reference waves [38]. Therefore, they remains the same in both IH,s and I\u043d\u044c:\n\n          $R_{x,s}(x,y) = R_{x,b}(x,y) = A_{Rx}e^{i\\phi_{Rx}}e^{-ikx sin \\theta}$\n          $R_{y,s}(x,y) = R_{y,b}(x, y) = A_{Ry}e^{i\\phi_{Ry}}e^{-iky sin \\eta}$\n          $k = 2\\pi/\\lambda$\n          \nwhere \u03bb is the laser wavelength; ARx and ARy are the amplitudes of the reference waves that are uniform over the CCD. The phase shift - kx sin \u03b8 is caused by the tilted angle \u03b8 between the phase plane of Rx and the CCD, as similarly for -kx sinn and \u03b7. Substituting Eq. A2-A5 into Eq. A1, we have:\n\n          $I_{H,s}(x,y) =A_{Rx}(x, y) + A_{Ry} (x, y) + A^{3}_{0,s}(x, y)$\n          $+ A_{o,s}(x, y) A_{Rx}e^{i[\\phi_{0}-\\phi_{Rx}+\\Delta\\phi(x,y)+kx sin \\theta]}$\n          $+ A_{o,s}(x,y) A_{Rx}e^{i[\\phi_{Rx}-\\phi_{0}-\\Delta\\phi(x,y)-kx sin \\theta]}$\n          $+ A_{o,s}(x, y) A_{Ry}e^{i[\\phi_{\u03bf}-\\phi_{Ry}+i\\Delta\\phi(x,y)+ky sin \\eta]}$\n          $+ A_{o,s}(x, y) A_{Ry}e^{i[\\phi_{Ry}-\\phi_{0}-\\Delta\\phi(x,y)-ky sin \\eta]}$\n          $+ A_{Ry}A_{Rx}e^{i (\\phi_{Rx}-\\phi_{Ry}-kx sin \\theta+ky sin \\eta)}$\n          $+ A_{Rx}A_{Ry}e^{i(\\phi_{Ry}-\\phi_{Rz}+ky sin\\eta-kx sin \\theta)}$"}, {"title": "Appendix B Spatial filtering and demodulation", "content": "Using the spatial separation in the Fourier spectrum, the real image (either $OR_{x}^{*}$ or $OR_{y}^{*}$) can be extracted directly from the hologram. Their frequency shift caused by the tilt angle (sin \u03b8/\u03bb or sin\u03b7/\u03bb) could be corrected by shifting them back to the center of the spectrum. The detailed procedure is shown in Fig. C2. After the operations in the Fourier domain and the inverse Fourier transformation, the real images become:\n\n          $\\Psi_{s}(x,y) = A_{o,s}(x, y) A_{Re^{i(\\phi_{O}-\\phi_{R})}} e^{i\\Delta\\phi(x,y)}$\n          $\\Psi_{b}(x,y) = A_{o,b}(x, y)I_{Re^{i(\\phi_{O}-\\phi_{R})}}$\n          \nwhere \u03a8s(x, y) and \u03a8\u266d(x, y) are derived from IH,s and IH,6, respectively. \u03a8\u300f(x, y) could be further demodulated by dividing it by (x, y):\n\n          $\\Psi^{'}(x, y) = \\frac{\\Psi_{s}(x,y)}{\\Psi_{b}(x,y)} = \\frac{A_{o,s}(x,y)}{A_{o,b}(x, y)} e^{i\\Delta\\phi(x,y)}$\n          \nwhere \u2206\u03c6(x, y) provides the 3D information of the sample, and $A_{o,s}(x, y)/A_{o,b}(x,y)$ denotes the relative intensity changes induced by the sample."}, {"title": "Appendix C Matrix multiplications with Fourier Imager Heads", "content": "As demonstrated in Table C1, each step of the spatial filtering process in Fig. C2 can be converted into a matrix multiplication, which could be further simplified as:\n\n          $M_{1} = M_{1}^{1}M_{1}^{2}M_{1}^{3}$  $M_{r} = M_{r}^{1}M_{r}^{2}M_{r}^{3}$  $M_{mask} = M_{mask}^{1} M_{mask} M_{mask}^{2}$\n          \nClearly, Mi, Mr and Mmask can be determined manually, then loaded into the neural network as initial weights."}]}