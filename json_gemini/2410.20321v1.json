{"title": "Effective Instruction Parsing Plugin for Complex Logical Query Answering on Knowledge Graphs", "authors": ["Xingrui Zhuo", "Jiapu Wang", "Gongqing Wu", "Shirui Pan", "Xindong Wu"], "abstract": "Knowledge Graph Query Embedding (KGQE) aims to embed First-Order Logic (FOL) queries in a low-dimensional KG space for complex reasoning over incomplete KGs. To enhance the generalization of KGQE models, recent studies integrate various external information (such as entity types and relation context) to better capture the logical semantics of FOL queries. The whole process is commonly referred to as Query Pattern Learning (QPL). However, current QPL methods typically suffer from the pattern-entity alignment bias problem, leading to the learned defective query patterns limiting KGQE models' performance. To address this problem, we propose an effective Query Instruction Parsing Plugin (QIPP) that leverages the context awareness of Pre-trained Language Models (PLMs) to capture latent query patterns from code-like query instructions. Unlike the external information introduced by previous QPL methods, we first propose code-like instructions to express FOL queries in an alternative format. This format utilizes textual variables and nested tuples to convey the logical semantics within FOL queries, serving as raw materials for a PLM-based instruction encoder to obtain complete query patterns. Building on this, we design a query-guided instruction decoder to adapt query patterns to KGQE models. To further enhance QIPP's effectiveness across various KGQE models, we propose a query pattern injection mechanism based on compressed optimization boundaries and an adaptive normalization component, allowing KGQE models to utilize query patterns more efficiently. Extensive experiments demonstrate that our plug-and-play method improves the performance of eight basic KGQE models and outperforms two state-of-the-art QPL methods.", "sections": [{"title": "1 Introduction", "content": "Knowledge Graph (KG) question answering [14, 29, 43] aims to reason on KGs for answering a given question. However, the inherent ambiguity in natural language (NL) questions often obstructs their ability to accurately convey reasoning logic. Thus, previous methods [1] usually transform NL questions into First-Order Logic (FOL) queries for explicit reasoning on KGs. As shown in Figure 1(b), the FOL query can intuitively express the reasoning logic of the corresponding NL question (Figure 1(a)) through various logical operations, such as existence quantification (\ud06c), conjunction (\u06f8), disjunction (V), and negation (\u00ac).\nTo better capture the logical semantics in FOL queries, Knowledge Graph Query Embedding (KGQE) [20, 28, 30] is proposed. Specifically, KGQE models extract a computation graph (the red dashed box in Figure 1(c)) of an FOL query from a KG and embed this query into a low-dimensional KG space according to the entity-relation order in the computation graph. Through the computation graph, KGQE models can identify entities close to a given query as answers.\nIn general, computation graphs can convey complex logical patterns of FOL queries, such as the projection and intersection patterns in Figure 2. These patterns are beneficial for a KGQE model to gain a deeper understanding of the logical semantics of FOL queries. However, due to the incompleteness of KGs in reality, an extracted computation graph is difficult to fully display the logical pattern of a query. Therefore, recent studies propose Query Pattern"}, {"title": "2 Preliminaries", "content": "Before elaborating on our proposed method, let's first introduce the task background of KGQE models and our main objective."}, {"title": "2.1 Knowledge Graph and FOL Query", "content": "Let G = (&,R) be a KG consisting of an entity set & = {e_n}_{n=1}^N and a directed relation set R = {r_m}_{m=1}^M. In G, a relation assertion r_m(&_h, &_t) represents that any entity in a head entity set &_h \u2286 & can be mapped to any entity in a tail entity set &_t \u2286 & through a relation r_m \u2208 R.\nAccording to the definition of [32], an FOL query can be defined by the following disjunctive paradigm (i.e., the disjunction of conjunction atomic queries):\n\n q[&?] = \\bigcup_{i=1}^? \\exists &_1, &_2, ..., &k : \\bigwedge_{j=1}^? q_{ij},\\\\ \n q_{ij} = \\begin{cases} r(&_a, &*) \\text{ or } \\neg r(&_a, &*) \\\\ r(&', &*) \\text{ or } \\neg r(&', &*) \\end{cases}\n \nwhere &* \u2208 {&?, &1, ...,&k}, &' \u2208 {&1, ...,&k}, r \u2208 R, E* \u2260 E', &1, ..., &k C & and &' \u2286 & are the sets of existentially quantified bound variables (i.e., intermediate entities) and the set of target entities, respectively. An indivisible atomic query q_ij is represented by a relation assertion itself or a negation operation of a relation assertion. &_a \u2286 & is a set of anchor entities that appear in q[&?]."}, {"title": "2.2 Knowledge Graph Query Embedding", "content": "KGQE aims to embed an FOL query in a low-dimensional KG space according to the query's computation graph. Let q, g, and f_{en} be an FOL query q, the computation graph g of q, and the embedding of en \u2208 &, respectively, we first use a KGQE model QE(.) to obtain the query's embedding f_q based on g. Next, a score function S(.) is used to calculate the embedding similarity between q and each e_n \u2208 &. Then, a KGQE model defines a specific reasoning operation opt(.) to predict the answer e_i \u2208 & of q. The overall framework of a KGQE model can be summarized as:\n\n \\begin{aligned}\n l_i& \\leftarrow \\text{opt} \\left(e_i \\in \\mathcal{E} \\mid\\left\\{S\\left(f_q, f_{e_n}\\right) \\mid e_n \\in \\mathcal{E}\\right\\}_{n=1}^N\\right). \\\\\nf_q&=QE(q|g)\n\\end{aligned}\n \nAccording to the reasoning mechanism of opt(.), existing KGQE models can be divided into two categories: end-to-end KGQE models and iterative KGQE models.\nEnd-to-end KGQE models directly obtain the embedding of an FOL query q. At this point, opt(\u00b7) is represented by max (\u00b7), which means the entity e_i most similar to q is used as the predicted answer. Iterative KGQE models first pretrain a KG Embedding (KGE) model (such as ComplEx [34]) as QE(.) for atomic query embedding, and then split an FOL query into multiple atomic queries that are sequentially input into QE(.). At this moment, opt(\u00b7) relies on a specific T-norm inference framework [17, 23, 24] based on beam search [5] or tree optimization [7]. In this case, opt(\u00b7) iteratively finds the optimal result e_i of the given atomic query q and regards e_i as the anchor entity for the next atomic query. Once the iteration is complete, the final e_i is used as the predicted answer for the complex query."}, {"title": "2.3 QPL Plugin", "content": "When e_i is the ground truth of q, the purpose of a KGQE method is to optimize QE(q|g) such that S(f_q, f_{e_i}) approaches max P(e_i|q). However, relying solely on the corresponding g, existing KGQE models are difficult to predict the ground truth of q on an incomplete KG. Therefore, QPL plugins [19, 22] usually introduce external information to enrich g, so that f_q can represent more general logical semantics of q (i.e., query pattern information), thereby enhancing the generalization of KGQE models. Let X be the external information, the optimization of max P(e_i|q) can be transformed into:\n\n max P(e_i|q, X) \\propto opt \\left(e_i \\in \\mathcal{E} \\mid\\left\\{S\\left(f_q, f_{e_n}\\right) \\mid e_n \\in \\mathcal{E}\\right\\}_{n=1}^N\\right),  \\tag{1}\n f_q=QE(q|g,X)\n\nHere, the current QPL plugins are mainly used for end-to-end KGQE models."}, {"title": "2.4 Our Objective", "content": "As analyzed in Section 1, existing QPL plugins are affected by pattern-entity alignment bias primarily due to their insufficient methods for representing and learning external information X. Our objective is to build a more effective QPL plugin to overcome this issue and ensure the proposed plugin can adapt to both the end-to-end and iterative KGQE models outlined in Section 2.2.\nTo achieve this objective, we can process P(e_i|q, X) in Eq. (1) into P(X/q,e_i)P(e_i|q). Because \\sum_{e_j \\in \\mathcal{E}} P(X|q) = P(X|q, e_j)P(e_j|q) is a normalization term, we pay more attention to the modeling of P(X|q, e_i)P(e_i|q). Therefore, our main objective is:\n\n max P(e_i|q, X) \\propto max P(X|q, e_i)P(e_i|q). \\tag{2}\n \nSince P(e_i|q) can be directly obtained through a specific QE(\u00b7), we focus on analyzing the modeling method of P(X|q, e_i), i.e., the instruction parsing plugin described in Section 3."}, {"title": "3 Query Instruction Parsing Plugin", "content": "As outlined in Section 2.4, the instruction parsing plugin is used to inject the pattern information of the query into its embedding to achieve more accurate target entity prediction. Figure 3 provides the framework of our proposed QIPP."}, {"title": "3.1 Code-like Instruction", "content": "As described in Section 1, compared to the FOL query format that contains unintelligible logical symbols, the code-like instructions use textual codes to more clearly express the information of a query's elements and the hierarchical structure of a complex query. Table A1 in Appendix A1.1 presents the code-like instruction format for each query type we use.\nIn addition, following the operations of previous studies on disjunctive FOL queries (\"2u\u201d and \u201cup\u201d in Table A1) [18, 31, 32], we construct the code-like instructions only for all non-extensible conjunctive sub-queries in the disjunctive query for a KGQE model. The final prediction for the disjunctive query is obtained by taking the union of the candidate target entity sets generated for each conjunctive sub-query."}, {"title": "3.2 Instruction Encoder and Decoder", "content": "Inspired by the effectiveness of PLMs in understanding the structure of programming languages [3], we construct a BERT-based instruction encoder [13] shown in Eq. (3), which learns the context of instructions to build a pattern learning space for a query:\n\n X = INST (X), \\tag{3}\n\nwhere the code-like instruction X = {x_z}_{z=1}^Z is a sequence that consists of Z tokens, INST(\u00b7) is a BERT-based instruction encoder with pre-trained parameters \u0398, and X = {x_z \u2208 R^{1\u00d7F_1}}_{z=1}^Z is the result of INST (X) that consists of Z F_1-dimensional token embeddings.\nConsidering the differences in context learning scales across different layers of BERT [21], we aim for INST(\u00b7) to focus more on the hierarchical structure within X rather than the abstract representation of a query. Therefore, we define \u0398 to include only BERT's token embedding layer and shallow encoding layers. The exact number of layers will be analyzed in detail in Section 4.4.4."}, {"title": "3.2.2 Instruction decoder", "content": "The primary purpose of the instruction decoder is to allow q to retrieve for more representative pattern information in the corresponding instruction X, thereby optimizing the representation of q. Therefore, we adopt a pattern information retrieval method based on the multi-head attention mechanism to construct an instruction decoder. Let f_{x|q} be the decoded output retrieved by q in X, W_Q \u2208 R, W_K, W_V \u2208 R, and W_O \u2208 R^{F_2\u00d7F_2} are four trainable parameter matrices. When the embedding f_q \u2208 R^{1\u00d7F_2} of q is obtained from a specific KGQE model QE(\u00b7), f_{x|q} can be represented as:\n\n \\begin{aligned}\n f_{x|q} = (\\sigma( \\| \\|_{h=1}^H (f_q^{(h)} W_Q^{(h)}) (X^{(h)} W_K^{(h)})^T ) X^{(h)} W_V^{(h)} ) W_O, \\tag{4}\n \\sqrt{F_2/H}\n\\end{aligned}\n \nwhere H is the number of attention heads, \\| is a column-wise vector concatenation operation, \u03c3(\u00b7) is a softmax function, and f_q^{(h)} \u2208 R^{1\u00d7?_2/??} and X^{(h)} \u2208 R^{Z\u00d7?_2/??} are the embedding chunks of f_q and X in the h-th attention head, respectively."}, {"title": "3.3 Query Pattern Injection", "content": "In this section, we discuss how to solve P(X|q, e_i)P(e_i|q) in Eq. (2) by injecting query pattern information. According to Eq. (4), f_{x|q} is the query pattern obtained from X based on q, while P(X|q, e_i) theoretically refers to the query pattern obtained from X under the conditions of both q and e_i. To measure the correlation between f_{x|q} and the condition e_i, we approximate P(X|q, e_i) to a similarity between f_{x|q} and f_{e_i}:\n\n P(X|q, e_i) = S(f_{x|q},f_{e_i}). \\tag{5}\n \nSimilarly, according to the description in Section 2.4, P(e_i|q) can be expressed as Eq. (6):\n\n P(e_i|q) = S(f_q, f_{e_i}). \\tag{6}\n \nLet S(a, b) = exp (a \\circ b), where \\circ is a vector distance function in a specific QE(\u00b7). Combining Eqs. (5) and (6), we can convert Eq. (2) into Eq. (7):\n\n min S(f_{x|q}, f_{e_i}) S (f_q, f_{e_i}) \\propto min (f_{x|q} \\circ f_{e_i} + f_q \\circ f_{e_i}) \\tag{7}\n \nTo enhance the generalization of query pattern injection in the KGQE model, we construct a compressed optimization boundary for Eq. (7) based on the law of parsimony (Occam's Razor) [8, 16]. Instead of supervising f_q and f_{x|q} separately, we use f_{x|q} + f_q as a unified supervised signal for entity prediction. This approach can be proven through the following theorem.\nTHEOREM 3.1. When f_{x|q} and f_{x|q} + f_q satisfy a specific constraint of QE(\u00b7) on the values of f_q and f_{e_i}, the optimized parameter boundary of (f_{x|q} + f_q) \\circ f_{e_i} is more compact than that of f_{x|q} \\circ f_{e_i} + f_q \\circ f_{e_i}. Minimizing (f_{x|q} + f_q) \\circ f_{e_i} can accelerate model convergence, thereby reducing overfitting, especially when the model has far more parameters than training data. This ultimately improves model generalization [16]."}, {"title": "Adaptive Normalization Component", "content": "In Theorem 3.1, we assume that f_q + f_{x|q} satisfies a specific constraint on the value of f_q (such as a cone boundary [44] or clamp constraint [10, 32]). To meet this assumption, we apply an adaptive normalization component A(\u00b7) for f_q + f_{x|q} in Theorem 3.1.\nFor a clamp constraint, f_q + f_{x\\q} = {x^{(i)}}_{i=1}^{F_2} can be converted by Eq. (11):\n\n A(f_q+f_{x\\q}) = {max (min (x^{(i)}, \\mu_{max}), \\mu_{min})}_{i=1}^{F_2}, \\tag{11}\n \nwhere, [\u00b5_min, \u00b5_max] are the boundary of a specific clamp constraint. When f_q+f_{x|q} are defined in the real number, Beta distribution, and fuzzy reasoning space, [\u00b5_min, \u00b5_max] are represented as (-\u221e, +\u221e), [0.05, +\u221e), and [0, 1], respectively.\nWhen f_q + f_{x|q} satisfies the cone boundary, A(\u00b7) is set to Eq. (12):\n\n A(f_q+f_{x\\q}) = r tanh (\\alpha_q + \\alpha_{x|q}) \\| [\\pi + \\pi tanh 2(\\beta_q + \\beta_{x\\q})], \\tag{12}\n \nwhere \u03b1_q, \u03b1_{x|q}, \u03b2_q, \u03b2_{x|q} \u2208 R^{1\u00d7F_2} satisfy f_q + f_{x|q} = (\u03b1_q+ \u03b1_{x|q})\\|(\u03b2_q + \u03b2_{x|q})."}, {"title": "3.5 Training and Reasoning", "content": "After selecting a suitable normalization function from Eqs. (11) and (12), the query embedding of q can be defined as f^* = A(f_q+f_{x|q}). Then, the loss function can be represented by Eq. (13):\n\n L(q, X, e^+, {e_i}^k_{i=1}) = - log \\sigma(\\gamma - f^* \\circ f_{e^+}) - \\sum_{i=1}^k log \\sigma(f^* \\circ f_{e_i} - \\gamma), \\tag{13}\n \nwhere \u03c3(\u00b7) is a sigmoid function, \u03b3 > 0 is a fixed margin, e^+, e_i \u2208 & are a positive entity and the i-th random sampled negative target entity of q, respectively. f_{e^+} and f_{e_i} are the embeddings of e^+ and e_i, respectively. For a specific QE(\u00b7), f^*, f_{e^+}, and f_{e_i} correspond to f_q + f_x, f_{e_1}, and f_{e_1} in Eq. (8) (or Eq. (9) or Eq. (10)), respectively. The detailed training process is provided in Appendix A1.3.\nWhen reasoning the target entity of q, we calculate the scores of q and each entity e \u2208 & using log \u03c3(\u03b3 \u2013 f^* \\circ f_e), and select the highest-scoring entity as the inference result for q. It is important to note that we need to select a specific KGQE model according to Section 2.2 to implement this reasoning process."}, {"title": "4 Experiments", "content": "In this section, we assess the effectiveness of our proposed QIPP by discussing the following five research questions: RQ1. Does QIPP provide performance gains in both end-to-end and iterative KGQE frameworks, and does it outperform existing QPL plugins? RQ2. Does the construction of code-like instructions and the pre-trained BERT's awareness of instruction context contribute to QIPP's query pattern learning? RQ3. Can the pattern injection mechanism with a compressed optimization boundary improve QIPP's performance? RQ4. Is the adaptive normalization component beneficial for QIPP? RQ5. Does the scale of the instruction encoder influence QIPP's performance?"}, {"title": "4.1 Dataset, Baseline, and Evaluation Metric", "content": "We conduct experiments using three widely used KG datasets: FB15k-237 [9], FB15k [33], and NELL995 [40], pre-processed"}, {"title": "5 Related Work", "content": "This section reviews the research of Knowledge Graph Reasoning (KGR), with a focus on the study of Knowledge Graph Complex Query Answering (KGCQA) tasks.\nEarly KGR methods focus on processing logical queries over visible KGs, using first-order logic with conjunction and existential quantization operators to retrieve KG subsets that match the query [1]. However, when a KG contains incomplete facts, this method is difficult to infer correct answers that are not visible. To address this issue, various studies have applied Knowledge Graph Embedding (KGE) [9, 34] to map entities and relations in incomplete KGs to a joint low-dimensional space, leveraging semantic similarity to predict unseen correct answers. In addition, some studies [12, 35, 40] have integrated reinforcement learning with search strategies to enhance the performance of the model on multi-hop queries.\nTo further enable models to handle queries containing complex logical operation combinations, researchers propose Knowledge Graph Query Embedding (KGQE) methods [11, 27, 39, 42] based on the KGE framework. These methods iteratively encode the entity-relation projection of each atomic query within complex queries. The GQE model proposed by William et al. [18] successfully im-plements complex query embeddings on a KGE framework for the first time. Following this, various GQE-based studies introduce additional query modeling techniques to obtain richer query logic semantics. For example, the Q2B model proposed by Ren et al. [31] uses box embeddings to represent a query as a hyper-rectangle and defines the predicted results as entities inside the rectangle. BetaE [32] models entities and queries as beta distributions, which enable flexible implementation of complex logical operations such as negation. ConE [44] models a query as a sector-cone space and regard the entities covered within the space as the prediction results. In addition, other studies have applied multi-layer perceptrons [4], fuzzy reasoning [10], eventuality KGs [6], hyper-relational KGs [2], and the Transformer framework [25, 26, 38, 41] to enhance KGQE models.\nIn contrast to the above methods that complicate the KGE framework, some studies aim to use traditional symbolic reasoning logic to guide the KGE framework in completing KGCQA tasks [5, 45]. Most of these methods employ a T-norm-based inference framework [17, 23, 24] to gradually infer the target entity step by step. For example, Arakelyan et al. propose the CQD model [5], which incorporates a Beam search strategy into the Complex model [34]. Wang et al. [36] better constrain the inference process of the T-norm framework by generating variable embeddings for intermediate answers. The QTO model proposed by Bai et al. [7] effectively reduces the time complexity of the T-norm inference framework by using a forward-backward propagation method on tree-like query graphs.\nGiven the challenge of exhaustively extracting all types of com-plex queries from KGs, recent studies have focused on constructing more generalizable Query Pattern Learning (QPL) frameworks to improve the model predictions for unknown queries. Methods like TEMP [19] and CaQR [22] typically use entity types or relation"}, {"title": "6 Conclusion", "content": "This paper proposes a novel a plug-and-play Query Instruction Parsing Plugin (QIPP) for complex logic query answering tasks, which aims to address the limitations in the inference ability of KGQE models caused by pattern-entity alignment bias. QIPP consists of an encoder-decoder for code-like query instructions, a query pattern injection mechanism based on compressed optimization boundaries, and an adaptive normalization component. Theoretical proofs and extensive experimental analyses demonstrate QIPP's enhancement of eight basic KGQE models, its superiority over existing two query pattern learning methods, and the effectiveness of each component in QIPP."}]}