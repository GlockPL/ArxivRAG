{"title": "Subject Data Auditing via Source Inference Attack in Cross-Silo Federated Learning", "authors": ["Jiaxin Li", "Marco Arazzi", "Antonino Nocera", "Mauro Conti"], "abstract": "Source Inference Attack (SIA) in Federated Learning (FL) aims to identify which client used a target data point for local model training. It allows the central server to audit clients' data usage. In cross-silo FL, a client (silo) collects data from multiple subjects (e.g., individuals, writers, or devices), posing a risk of subject information leakage. Subject Membership Inference Attack (SMIA) targets this scenario and attempts to infer whether any client utilizes data points from a target subject in cross-silo FL. However, existing results on SMIA are limited and based on strong assumptions on the attack scenario. Therefore, we propose a Subject-Level Source Inference Attack (SLSIA) by removing critical constraints that only one client can use a target data point in SIA and imprecise detection of clients utilizing target subject data in SMIA. The attacker, positioned on the server side, controls a target data source and aims to detect all clients using data points from the target subject. Our strategy leverages a binary attack classifier to predict whether the embeddings returned by a local model on test data from the target subject include unique patterns that indicate a client trains the model with data from that subject. To achieve this, the attacker locally pre-trains models using data derived from the target subject and then leverages them to build a training set for the binary attack classifier. Our SLSIA significantly outperforms previous methods on three datasets. Specifically, SLSIA achieves a maximum average accuracy of 0.88 over 50 target subjects. Analyzing embedding distribution and input feature distance shows that datasets with sparse subjects are more susceptible to our attack. Finally, we propose to defend our SLSIA using item-level and subject-level differential privacy mechanisms. The attack accuracy decreases by 36% with a utility loss of 20%, using a subject-level differential privacy budget of 22.", "sections": [{"title": "1. Introduction", "content": "Even though Federated Learning (FL) [27, 17] avoids the transition of private data across clients (federation users) to collaboratively train a global model, it is still under the threat of security and privacy attacks. For example, the gradient leakage attack [31, 53, 51, 12] recovers training data information or labels of the local models exploiting the shared gradients, the reconstruction attack [15, 45] usually trains a GAN to reconstruct the data belonging to a specific label or local client, and, finally, the property inference attack [50, 44] aims to expose the property of the whole training data. The large number of attacks identified by the recent research community indicates that the basic strategy of training local models separately in FL is not enough to protect private data.\nWith the introduction of regulations like GDPR [42] and CCPA [4], people and governments are paying more attention to auditing privacy data usage. In FL, the Source Inference Attack (SIA) [16] and Subject Membership In-ference Attack (SMIA) [39, 22] are two recently proposed attack methods, which can also be considered suitable tools to audit data usage. SIA detects which client trains its local model with a target data point. SMIA checks whether any client (or a specific set of clients) utilizes the data from a target subject for training. However, an honest but curious central server may care more about detecting all the local clients (none, one, or more than one) that train their models with data from the target subject without having access to the specific target dataset used. However, to our best knowledge, there is no such practical and efficient method. Therefore, we propose a novel Subject-Level Source Inference Attack (SLSIA) to achieve this goal.\nOur SLSIA distinguishes itself from existing inference attacks, although it shares some similarities with the prop-erty inference attack. However, a property inference attack tries to infer the existence or distribution of sensitive prop-erty (e.g., voice accent, skin color, income, doctor specialty, product type, and age) [3, 11, 50, 25, 38] in the training data of the target model. By contrast, our SLSIA focuses on detecting whether a target local model uses data generated by a subject. The data points belonging to a specific subject are usually not equal to the data points sharing the same property, which distinguishes the property inference and our attack. Besides, our final purpose is to find all the clients using the data from a target subject in a cross-silo FL while holding another set of data points from the subject. In addition, the previous SMIA either can not precisely find which clients train their local models with the data from the target subject [39] or have strong assumptions (such as, they require that considered clients have a local dataset with 10% data of all subjects, or they interrupt the training of FL, and so forth) [22]. The DMIA [14] aims to distinguish two datasets that differ only in the contribution of a specific subject, and the remaining data are from the same subjects. However, the local clients holding data from the target subject can still have data from other non-overlapping subjects in cross-silo FL. Hence, different from the DMIA scenario, in the attack scenario we consider, the clients to identify are those having data from the target subject in their local dataset. Still, we cannot assume that the remaining part of the data somehow overlaps. As a final remark, we outline that in the scenario considered by the standard instance-level MIAs [35, 13, 49], the member data point is in the training data of one target model. Therefore, the data point is known, and the attack aims to verify whether the target model uses it. Instead, we consider a situation in which some FL clients may use data points (an arbitrary set) from a target source. The attacker, located on the server side, does not have access to the specific set of data possibly used by such clients but, instead, has access to other data samples from the same target source. This complex attack scenario makes our solution novel and more advanced concerning existing related works in this context.\nTo develop our solution, we analyze recent findings proposed by Song et al. [36]. The authors suggest that the model's intermediate outputs expose the private information of its training data. In particular, the representation of the last layer in the feature extractor overlearns and reveals sensitive attributes of the input data. Following this reasoning, in our solution, we utilize the intermediate embeddings of the local model on a set of data points from the target subject to predict whether the tested model (a local model from one of the clients) has been trained with data from the target subject. The ablation study we carried out and reported in Section 5.4 shows that the early output of layers is a key factor in developing our attack.\nOur SLSIA pipeline consists of three steps. First, the attacker uses data from the target source to pre-train two types of support models: the former exploiting data from the source and the latter exploiting only data from other sources. Once trained, the embeddings produced by these two model types for input data produced by the target source are used to build a binary attack classifier. After that, the binary attack classifier is used to evaluate local models during the first round of FL. The main reason for selecting the first round is that local models trained without the data from the target subject will also learn about the subject as the FL aggregation is executed after each training step, ultimately disturbing the source inference. Therefore, during the first round of FL, the central server obtains the embeddings produced by each local model for the same evaluation set of data points from the target subject. Such embedding is then used as input to our binary attack classifier to identify all the clients trained with the data from the target subject.\nOur contributions are listed as follows.\n\u2022 We propose the SLSIA, a novel source inference at-tack that can also be used as an efficient and prac-tical method to audit the subject data usage from the perspective of the central server in cross-silo FL. Our SLSIA achieves higher accuracy than previous existing works [16, 39] in three datasets.\n\u2022 We explore the characteristics of the input data that maximize the performance of our attack. Moreover, through a thorough ablation study, we analyze the main factors allowing our SLSIA attack to be effec-tive. In particular, we demonstrate datasets in which subjects' data have highly different distributions are extremely vulnerable to our SLSIA.\n\u2022 We explore item-level and subject-level privacy pro-tection mechanisms against our SLSIA. In general, differential privacy could reduce our attack but also considerably impact the model's utility. The analyzed defenses cannot prevent our SLSIA for most configu-rations explored in this work.\nThe remainder of this paper is organized as follows. Section 2 describes background concepts to introduce our proposal better. Our proposal, instead, is detailed in Section 3. In Section 4, we report all the experiments carried out to validate our proposal. The obtained results and the analysis of possible defenses against our solution are described in Section 5. Section 6 analyzes the related literature and iden-tifies the novel aspects of our proposal. Finally, in Section 7, we draw our final conclusion."}, {"title": "2. Background", "content": "In this section, we introduce the background of FL in Section 2.1. Then, we recall the SIA in Section 2.2. Finally, we review two previous strategies of the SMIA in Section 2.3."}, {"title": "2.1. Federated Learning", "content": "In cross-silo FL, a few hundred reliable data silos (or-ganizations or geographically distributed data centers) with powerful computing resources and high-speed connections collectively train a global model without sharing local pri-vate data [17]. A central server S averages the gradients or weights from local clients (silos) to obtain the global model. Among the averaging strategies, the federated averaging (FedAvg) algorithm [27] is the first and most frequently used algorithm [34]. Therefore, we utilize FedAvg to average weights. For each selected local client $C_i$ among $n$ clients in round $j$, its updated weight $W^j$ is sent to S after its local training with stochastic gradient descent (SGD) on local dataset $D_i$, which is sampled from the whole dataset D. The global weight $W^j = \\sum_{i=1}^{n} I_j(C_i)\\frac{|D_i|}{|D|}W_i^j$, where $I_j(C_i) = 1$ means $C_i$ is selected for updating the global model at round $j$; otherwise, 0. The local client updates the previous global model weight $W^{j-1}$ via multiple local epochs to get the local model weight $W_i^j$. For the local epoch, $W_{epoch_k} = W_{epoch_{k-1}} - \\eta(\\frac{1}{|D_i|}\\sum_{(x,y)\\in D_i} \\nabla L(f_{W_{epoch_{k-1}}}(x),y))$, where $\\eta$ is the learning rate, $(x, y)$ is one data point in local dataset $D_i$, L is the loss function (e.g., Cross-Entropy loss for classification task), and $f_{W_{epoch_{k-1}}}(x)$ is the output of model f with parameters $W_{epoch_{k-1}}$ on the input x. We set $W_{epoch_0}$ as the previous global model $W^{j-1}$ in the first local epoch. After m local epochs, we set the new local model $W_i^j$ from $C_i$ at round $j$ as $W_{epoch_{m-1}}$. In our experiments, we follow the work of Hu et al. [16] to select all the clients (i.e., $I_j(C_i) = 1$ for all $C_i$) for updating to simplify the scenario. We leave the scenario that does not select all clients for updating as the future work."}, {"title": "2.2. Source Inference Attack", "content": "For a target data point $z_t = (x_t, y_t)$, the adversary assumes that only local client $C_i$ trains its local model with this target data point. The source inference attack aims to find out this local client. From the strategy of Hu et al. [16], they obtain the loss values of $z_t$ on all local models and predict the one with the smallest loss value as the one trained with $z_t$. In particular, they formulate the probability of predicting $z_t$ is in the training data of local client $C_i$ as $\\mathbb{P}(s_{ti} = 1) = \\lambda$. Where $s_{ti}$ is the i-th element, referred to as the i-th client $C_i$, of the one-hot encoding vector $s_t$ (only one element equals 1), which presents the source status of $z_t$. Given the weights of the i-th client $W_i$ and $z_t$, the server rephrases Eq. 1 to the posterior probability as follows: $S(W_i, z_t) = \\mathbb{P}(s_{ti} = 1|W_i, z_t)$. The authors derive $S(W_i, z_t)$ from the Bayesian per-spective, providing insights on how the prediction loss of the local clients can be exploited to implement their source inference attack. The formula of $S(W_i, z_t)$ can be defined as follows: $S(W_i, z_t) = \\mathbb{E}_{\\tau} \\Big[\\sigma \\big[log(\\frac{\\mathbb{P}(W_i | s_{ti} = 1, z_t, \\tau)}{\\mathbb{P}(W_i | s_{ti} = 0, z_t, \\tau)}) + \\mu_\\lambda\\big]\\Big]$. $\\tau = \\{z_1, ..., z_{t-1}, z_{t+1}, ..., z_{|D|}, s_1, ..., s_{t-1}, s_{t+1}, ..., s_{|D|}\\}$. Where $\\tau$ is the set of the remaining target data points from the whole distribution D and their one-hot encoding vectors containing the source status, $\\mu_\\lambda = log(\\frac{1-\\lambda}{\\lambda})$, and $\\sigma(x) = (1 - e^{-x})^{-1}$ (the sigmoid function). Then, they replace $\\mathbb{P}(W_i | s_{ti} = 1, z_t, \\tau)$ with the posterior probability of obtaining the model in the energy-based model [19] to relate $S(W_i, z_t)$ with the loss. By analyzing the optimal source in-ference from the derived version of $S(W_i, z_t)$, they conclude that the smaller the loss of client $C_i$'s local model on the target data point ($z_t$), the higher the posterior probability that it belongs to client $C_i$."}, {"title": "2.3. Subject Membership Inference Attack", "content": "In the conventional membership inference attack, the adversary aims to infer whether a specific data point is in the training data of the target model [35]. Considering local datasets are generated by multiple subjects (e.g., various devices or individuals) in cross-silo FL, the adversary wants to know whether any local dataset $D_i$ contains data from a target subject $s_t$, which is formulated as the SMIA in the work of Suri et al. [39]. For $n$ clients in FL to update the global model weight $W^j$ for r rounds, where j denotes the j-th round, the authors propose Loss-Threshold and Loss-Across-Rounds attacks. In the Loss-Threshold attack, they record the loss values (L) for each data point (x, y) sampled from the data of the target subject ($D_{s_t}$) and check if the loss values are lower than a given threshold a as follows: $c = \\sum_{(x,y)\\in D_{s_t}} \\mathbb{I}[L(f_W(x), y) \\leq a]$. Where $\\mathbb{I}$ is the identity function. With the obtained c, the attacker predicts the data of the target subject ($s_t$) is used in FL if c is non-zero or the attacker defines an additional threshold for c based on the metric he wants to maximize. In the Loss-Across-Rounds attack, the attacker sums the loss values of data points from the target subject as $C_j = \\sum_{(x,y)\\in D_{s_t}} L(f_{W_i^j}(x), y)$ in round j. Then, the attacker records the number of times the summed loss decreases concerning the previous round. $c = \\sum_{j=1}^{r} \\mathbb{I}[C_j < C_{j-1}]$. The attacker can now derive a threshold for c in Eq. 6 to determine whether the data from the target subject ($s_t$) is used in FL. In the work of Liu et al. [22], instead, they care about whether a set of concerned clients rather than any clients contain the data from the target subject in FL with strong assumptions (10% data of all subjects in concerned clients, interrupt the training of FL, and datasets without a concept of the subject). From previous definitions [39, 22, 14], the SMIA differentiates between models trained on datasets sampled from two distributions, the only difference of which is that one has the data from the target subject while the other does not."}, {"title": "3. Our Subject-Level Source Inference Attack", "content": "Our SLSIA targets detecting all the local clients that train their local models with the data from the target subject from the perspective of an honest but curious central server. We explain the attack objective of our SLSIA in Section 3.1. Then, we formulate the threat model in Section 3.2. Finally, we detail the methodology of applying our SLSIA in Sec-tion 3.3."}, {"title": "3.1. Attack Objective", "content": "In cross-silo FL, an honest but curious central server S updates global model weight $W^j$ at each round $j$ with local model weights $W_i^j$ ($i = 1,...,n$) from local clients ($n = 10$ in default). Each local dataset $D_i$ is sampled from the whole distribution D, which consists of data from k subjects. In FL, the data from a subject might only be available to a few local clients as a requirement for pri-vacy projection (e.g., the patient's health records are only available in the post-diagnostic hospitals and are usually not allowed to be shared). In the worst-case scenario, two local datasets typically do not sample from the same subjects and obtain data from entirely different subjects. To regulate and audit the data usage of local clients, an honest but curious central server S wants to figure out all the local clients that utilize the data from a target subject ($s_t$) to train their local models. In other words, we distinguish two local models, one containing the data from the target subject for training while the other does not. Besides, the remaining data from those two local datasets might be sampled from different subjects, which means the data from the target subject is not the only difference. It is one aspect that distinguishes our attack from the subject membership and property inference attacks."}, {"title": "3.2. Threat Model", "content": "To correctly detect all the local clients trained with the data from the target subject as an honest but curious server, we assume that the server knows (1) the structure of the local model, (2) hyper-parameters of local training, (3) reserved data points from the target subject (not used in local training), (4) data points from other subjects which are not used in local training, and (5) weights of local models (white-box). As the central server, obtaining the structures and weights of local models is reasonable as the server needs to update the global model. The server can determine the hyper-parameters for local clients before FL or apply the stealing attack to get hyper-parameters [43]. The data points from the target subject are necessary for the attack to be applied. The central server holds a set of data points ($D_{s_t}^v$) from the target subject set ($D_{s_t}$) that does not overlap with the training data ($D_{s_t}$) used by m target local clients ($C = \\{C_{t_1},..., C_{t_m}\\}$) it wants to detect. Besides, the server collects a set of datasets ($\\cup_{r=1}^{\\iota}\\{D_{r_1},...,D_{r_{\\iota}}\\}$) from other random subjects ($\\cup_{r=1}^{\\iota}\\{r_1,...,r_{\\iota}\\}$). The server exploits the mentioned knowledge to design a strategy for detecting whether a client uses the data from the subject target ($s_t$). Our approach to im-plementing SLMIA is presented in the following Section 3.3."}, {"title": "3.3. Methodology", "content": "As we present in Section 2, SIA predicts the only tar-get client as the one with the smallest loss on the target data point. SMIA compares the losses of data points from the target subject with a threshold or accumulates those losses among the training epochs to observe the number of decreases of the summed loss. We can not directly apply previous approaches to our scenario, in which there can be more than one or no target client, the total number of them is unknown to the server, and the final purpose is to detect all target clients. We explain two transferred methods from previous works in Section 5.2 for comparison. The target model means the local model trained with one target data point (SIA), the global model trained with the data from a target subject (SMIA), or the local model trained with the data from a target subject (our SLSIA).\nOur SLSIA is based on the data from the target sub-ject ($s_t$). As we mentioned in Section 3.2, we divide the data point set ($D_{s_t}$) from the target subject into two non-overlapping sets, $D_{s_t}^e$ (for pre-training and evaluation) and $D_{s_t}$ (for FL). Then, we further split $D_{s_t}^e$ into two sets, $D_{s_t}^{ep}$ (for pre-training) and $D_{s_t}^{ev}$ (for evaluation). We explain the default proportions of $D_{s_t}^{ep}$, $D_{s_t}^{ev}$, and $D_{s_t}$ over $D_{s_t}$ in Section 4.2.\nIn our proposed attack, we employ a binary classifier as Attack Model AM to detect whether the embeddings of the data of target subject $s_t$ are obtained from a local model (with the weight $W_i^j$ from the local client $C_i$) has been trained with the data of $s_t$ in the first round. To train our Attack Model AM offline before the FL process starts, the server pre-trains $N_{pre}$ pre-trained models $M^{pre}$, with the same architecture as the global model, to generate the embeddings $H^{pre}$ as the input features of AM. The $H^{pre}$ comprises two sets of embeddings, one generated by $\\frac{N_{pre}}{2}$ pre-trained models partially trained with the data from $s_t$ ($D_{s_t}^{ep}$) and one obtained from other $\\frac{N_{pre}}{2}$ pre-trained models trained on data points only from random subjects.\nIn detail, the pre-training of models, $M^{pre}$, is performed by simulating $N_{pre}$ different datasets $D^{pre}$ using the data from the target subject ($D_{s_t}^{ep}$) and the data from random subjects ($\\{\\cup_{r=1}^{\\iota}\\{D_{r_1},...,D_{r_{\\iota}}\\}\\}$). Among them, $D^{pre}_{t_i}$ contains the data from the target subject ($D_{s_t}^{ep}$) and a similar number of data points from a random subject. $D^{pre}_{r_i}$ includes a similarly equal number of data points from two random subjects separately. It means the number of data points in $D^{pre}_{t_i}$ and $D^{pre}_{r_i}$ is similar.\n$D^{pre} = \\{D^{pre}_{t_1},..., D^{pre}_{t_{\\frac{N_{pre}}{2}}}, D^{pre}_{r_1},..., D^{pre}_{r_{\\frac{N_{pre}}{2}}}\\}$.\nAfter training with $D^{pre}$, the server trains $N_{pre}$ pre-trained models $M^{pre}$ to extract the data of training Attack Model $A_M$.\n$M^{pre} = \\{M^{pre}_{t_1},..., M^{pre}_{t_{\\frac{N_{pre}}{2}}}, M^{pre}_{r_1},..., M^{pre}_{r_{\\frac{N_{pre}}{2}}}\\}$.\n$M^{pre}_{t_i}$ is the model trained on $D^{pre}_{t_i}$, and $M^{pre}_{r_i}$ is trained on $D^{pre}_{r_i}$. We call $M^{pre}_{t_i}$ the target pre-trained model and $M^{pre}_{r_i}$ the random pre-trained model separately. With $N_{pre}$ pre-trained models $M^{pre}$, the server generates the embeddings $H^{pre}$ via querying the pre-trained models with an evaluation set of data points from the target subject ($D_{s_t}^{ev}$). If the embed-ding is obtained from the target pre-trained model ($M^{pre}_{t_i}$), the server assigns the label $y^{pre}=1$ (\"in\") to this embedding. If the embedding is generated by the random pre-trained model $M^{pre}_{r_i}$, the label is $y^{pre}=0$ (\"out\").\n$H^{pre}_{t_i} = M^{pre}_{t_i}(D_{s_t}^{ev}); H^{pre}_{r_i} = M^{pre}_{r_i}(D_{s_t}^{ev})$."}, {"title": "4. Experiments", "content": "To verify the effectiveness of our proposed SLSIA, we experiment on three datasets and three types of models. We describe the datasets used in experiments in Section 4.1. For models, settings to train models, and partial default settings of our SLSIA, we expose them in Section 4.2. Finally, we detail the evaluation metrics for measuring the attack performance in Section 4.3."}, {"title": "4.1. Datasets", "content": "Following the work of Suri et al. [39], we select FEM-NIST and Shakespeare datasets with a clear concept of the subject for the attack. The FEMNIST is a federated extended MNIST [9] that not only contains classes characterized by handwritten digits and letters but also includes information on writers who contributed to the collection of data points. This additional writer information makes the dataset per-fectly fit our scenario, in which we consider each writer a subject. Here, we only use the digits following the MNIST, which only contains the digits. Besides, we can reduce the impact of the internal difference of the dataset caused by combining the digits and letters to the classification and the attack.\nAs the second dataset, we selected the Shakespeare dataset from the LEAF [5] family, a repository that pro-vides different federated benchmark datasets. The Shake-speare dataset contains dialogues of characters in the clas-sic William Shakespeare plays, and the characters can be considered different subjects. In this case, the main task of FL is the next-word prediction. We opted for this task due to the limitations of alternatives applicable to this dataset in our subject-centric scenario, like the next-character task, as highlighted by other works [39]. Besides, we excluded characters with less than 400 words to guarantee the data points for each character and eliminate the impact of data inadequacy while being selected as the target subject of the attack. Each data point in the Shakespeare dataset is a continuous sequence of 32 words with a label of the next word. Finally, we obtain data on 404 characters in the Shakespeare dataset.\nApart from those two already existing datasets, we also construct a Synthetic dataset following the guide of [39]. We model each subject as a random (and valid) mean and covariance matrix in a multivariate Gaussian distribution to obtain well-separated and characterized subjects. In particu-lar, we enforce a minimum pair-wise ($L2 > 0.35$) separation between all subject distributions' means to avoid overlap. The label of a generated data point is the outcome of an XOR operation between a list of indicator values, each represent-ing whether the specific feature is larger than 0. For the input feature $x = [x_1,..., x_p]$, the label $y = \\bigoplus_{i=1}^{p} \\mathbb{I}(x_i > 0)$, where $\\bigoplus$ is the XOR operation and $\\mathbb{I}$ is the identity function. We construct a Synthetic dataset with 200 subjects, each with 400 data points. The length of the feature is 60, and the number of categories is two due to the range of XOR values. We summarize the statistics of each dataset in Table 1."}, {"title": "4.2. Models and Settings", "content": "This section is devoted to presenting the architectures of the models and the settings used to conduct our experimental campaign. For each dataset, we defined a specific architec-ture shared by the global, local, and pre-trained models used in our attack, specific for the domain that characterizes their features.\nIn particular, for FEMNIST, we use a convolutional neural network (CNN), as the one in [16], characterized by two layers with kernel size equal to 5 and, respectively, 32 and 64 filters. A max-pooling layer of kernel 2 follows each of the two layers. The classifier on top is a multilayer perceptron (MLP) network composed of three layers, of which the first two are characterized by 512 and 128 neuron units and ReLu activation function, and the final one exits with a softmax function. For Shakespeare, we considered an embedding network characterized by a one-layer long short-term memory (LSTM [8]) followed by a classification layer of 64 neuron units as our next word prediction model. The size of the embedding generated in output by the LSTM equals 64, which is also the size of the internal hidden states. We take the last hidden state of LSTM as the input of the classification layer. For Synthetic, following the work of [16], we employ an MLP network with one hidden layer with 200 units and a ReLu activation function.\nWe call the above three models CNN, LSTM, and MLP for convenience, except for additional illustration. We utilize SGD as an optimizer with a learning rate of 0.01 and a momentum of 0.9 for all experiments. For the training of the local model on the client side, we set the batch size equal to 12, and at each round, the client performs five local epochs before sending the updates to the server. In the same way, we pre-train $N_{pre}$ models used by the server to generate the embeddings useful for the training of the attack model.\nWe select two types of models for the binary attack model: 1-dimensional CNN and support vector machine (SVM). The CNN-based attack model comprises two 1-dimensional convolution layers with 4 and 8 filters and a classification layer with Softmax function. Each 1-dimensional convolution layer is followed by a 1-dimensional max pool with a kernel size of three and a Batch Normalization layer. The SVM classifier utilizes the default settings in the scikit-learn [10] library. We optimize the CNN-based attack model via Adam with a learning rate of 0.0001 and a weight decay of 0.1. The batch size is 16, and the training epoch is 100. As the loss function, we consider the coss-entropy loss.\nFinally, we explain the partial default settings of apply-ing our attack strategy here. The number of local clients (n) is 10, the same as the work of Hu et al. [16]. The number of pre-trained models ($N_{pre}$) is 20. The reason for pre-training 20 models is to guarantee the embedding dataset size for training the attack model. The number of target local clients (m) trained with the data from the target subject is 5, half of the total local clients, as we explain in Section 4.3. To divide the data points from the target subject, we split 25% of $D_{s_t}$, as $D_{s_t}^e$ used by m target local clients, which means $D_{s_t}= 75\\% D_{s_t}$. As the central server knows the target subject, he can collect data points from the target subject by himself. Hence, it is reasonable that the server holds most (75%) data points of the target subject. $D_{s_t}^e$ is split into $D_{s_t}^{ep} = 50\\% D_{s_t}$, (for pre-training) and $D_{s_t}^{ev} = 25\\%D_{s_t}$, (for extracting the embeddings). As the number of pre-trained models is twice that of the local clients of FL in our setting, we utilize more data from the target subject for pre-training. Besides, we illustrate the default settings and have an ablation study for the percentage of the data from the target subject in the target local client, the embedding layer, and the local training epoch in Section 5.4."}, {"title": "4.3. Evaluation metrics", "content": "In this section, we discuss the evaluation metrics we de-fined to assess the performance of our attack. In the work of Hu et al. [16], they utilize the Attack Success Rate (ASR) to measure the performance of the SIA on an evaluation group of target data points. They define the ASR as the fraction of the target data points whose source status is correctly identified by the server in the evaluation group. Their ASR is similar to the recall, which is the percentage of correctly predicted positive instances among all the actual positive instances. To measure the performance of our SLSIA, we define metrics from the perspective of each target subject. Assuming the data from the target subject $s_t$ is in the training data of partial of n clients, we have a ground-truth source list, [0, ..., 1], comprised of n indicator values; each value is 0 or 1. If the i-th indicator value is 1, the local client $C_i$ trains its model with the data from the target subject $s_t$; otherwise, it does not.\nAfter applying our SLSIA, we obtain the potential clients that train their local model using the data from the target subject, which is also formulated as a potential source list, [0,..., 1], having the same length as the ground-truth source list and representing the same meaning with the value at the same position. Comparing the ground-truth source list with the potential source list predicted by our attack, we calculate each target subject's accuracy, precision, recall, and F1. Then, distinct metrics of target subjects are averaged to produce the performance of our attack, and we compare it with previous methods. When we compare the ground-truth source list and the potential source list by calculating the accuracy in the classification task, the number of \"1\" in the ground-truth source list determines whether the calculated accuracy is balanced. To obtain a balanced accuracy value and guarantee the fairness of evaluation, we set the number of local clients trained with the data from the target subject as half of the total number of clients as default without additional illustration, which means that half of the ground-truth source list is \"1\". Because we implement our SLSIA at the first round of FL, setting half of the total number of clients to be trained with the data of the target subject will not impact the attack performance as the FL aggregation is applied after the first FL round."}, {"title": "5. Results and Discussions", "content": "This section presents and analyzes the results of our SLS"}]}