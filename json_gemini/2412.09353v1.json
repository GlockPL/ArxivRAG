{"title": "CAUSAL GRAPHICAL MODELS FOR VISION-LANGUAGE COMPOSITIONAL UNDERSTANDING", "authors": ["Fiorenzo Parascandolo", "Nicholas Moratelli", "Enver Sangineto", "Lorenzo Baraldi", "Rita Cucchiara"], "abstract": "Recent work has empirically shown that Vision-Language Models (VLMs) struggle to fully understand the compositional properties of the human language, usually modeling an image caption as a \"bag of words\". As a result, they perform poorly on compositional tasks, which require a deeper understanding of the different entities of a sentence (subject, verb, etc.) jointly with their mutual relationships in order to be solved. In this paper, we model the dependency relations among textual and visual tokens using a Causal Graphical Model (CGM), built using a dependency parser, and we train a decoder conditioned by the VLM visual encoder. Differently from standard autoregressive or parallel predictions, our decoder's generative process is partially-ordered following the CGM structure. This structure encourages the decoder to learn only the main causal dependencies in a sentence discarding spurious correlations. Using extensive experiments on five compositional benchmarks, we show that our method significantly outperforms all the state-of-the-art compositional approaches by a large margin, and it also improves over methods trained using much larger datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Vision-Language Models (VLMs) have shown impressive results in different tasks such as, for instance, zero-shot classification, image-text retrieval, vision-question answering, image-captioning, and many others (Radford et al., 2021; Li et al., 2023b; Singh et al., 2021; Liu et al., 2023). However, despite this success, most VLMs still struggle in understanding the compositional nature of the human language. For instance, Yuksekgonul et al. (2023) empirically showed that common VLMs usually do not consider the order and the syntactic/semantic relations of words in a sentence, which is treated as a bag of words, where \u201cthe horse is eating the grass\" and \"the grass is eating the horse\" can easily be confused. Jointly with Yuksekgonul et al. (2023), many other authors have recently proposed different compositional benchmarks which confirm the poor performance of common VLMs when tested against compositional tasks (Hsieh et al., 2024; Zhao et al., 2022; Burapacheep et al., 2024). One of the probable reasons of this bag-of-words behavior is the contrastive loss used in CLIP (Radford et al., 2021) (and in other VLMs), which compares a single vector representing the textual encoder's output with a single vector representing the visual encoder's output, sacrificing textual and visual details (Yuksekgonul et al., 2023; Kamath et al., 2023; Basu et al., 2024). Another reason is the low quality of the captions used for VLM pre-training, which are usually noisy or do not describe the details of the image and the interactions among its objects (Doveh et al., 2023a).\nMost of the compositional methods that have been recently proposed to alleviate this problem focus on creating annotations with a richer compositional structure, used to fine-tune a VLM (Yuksekgonul et al., 2023; Doveh et al., 2023a; Cascante-Bonilla et al., 2023). For instance, NegCLIP (Yuksekgonul et al., 2023) creates hard negatives, in which the original caption is modified swapping the positions of some words, and these hard negatives are used jointly with common negatives to fine-tune CLIP using the standard contrastive loss. However, the automatic creation of hard negatives is itself noisy, leading to captions which often do not have a correct syntactic/semantic meaning (this problem is inherited by some compositional benchmarks, see Sec. 4). In (Tschannen et al., 2023),"}, {"title": "2 RELATED WORK", "content": "Compositional Methods. Most of the compositional methods are based on creating annotated training samples which force the VLM to acquire compositional knowledge. For instance, (Yuksekgonul et al., 2023; Zhang et al., 2024; Huang et al., 2024; Buettner & Kovashka, 2024; Momeni et al., 2023; Doveh et al., 2023b; Singh et al., 2023; Oh et al., 2024; Yellinek et al., 2023; Herzig et al., 2023) use either a rule-based method or a Large Language Model (LLM) to create hard negatives (Sec. 1), which typically consist in replacing or swapping the position of some words in the ground-truth caption associated with a training image. In (Cascante-Bonilla et al., 2023), dense captions are constructed using synthetic videos created with a 3D physics-based simulator, while (Singh et al., 2024) use real videos. In DAC (Doveh et al., 2023a), dense captions are created by combining the results of either an LLM (GPT-NEO-2.7B) or a segmentation network (SAM (Kirillov et al., 2023)) with a captioner (BLIP-2 (Li et al., 2023b)). SAM is also used in (Sahin et al., 2024) jointly with Stable Diffusion (Rombach et al., 2022) to generate hard negative images. Moreover, Stable Diffusion is used in (Li et al., 2023a; Clark & Jaini, 2023; Krojer et al., 2023) as an alternative VLM. The main idea is that the noise prediction error of the Diffusion Model (DM) (Ho et al., 2020), obtained by feeding Stable Diffusion with a corrupted version of the test image and a given caption, can be\na VLM is pre-trained from scratch using a captioning strategy and a huge private dataset. Specifically, the authors propose both Cap, where the pre-training strategy is a standard AutoRegressive (AR) next-token prediction, and CapPa, where the AR training is mixed with a parallel training, in which all the textual tokens are simultaneously predicted. Tschannen et al. (2023) show that both Cap and CapPa achieve excellent results on compositional tasks, and argue that a generative training encourages the VLM to focus on fine-grained descriptions of the visual content.\nIn this paper, inspired by Cap and CapPa, we propose a VLM adaptation approach for compositional reasoning which is based on a decoder trained with a captioning strategy. However, differently from the standard fully-sequential AR and the parallel predictions used in (Tschannen et al., 2023), we propose a partially ordered, semi-parallel AR prediction strategy which is guided by the dependency relations of a Causal Graphical Model (CGM) (Sch\u00f6lkopf et al., 2021). In more detail, we use an off-the-shelf dependency parser (Dozat & Manning, 2016), which creates a syntactic tree from a given textual sentence. Specifically, given a caption, a dependency parser automatically builds a Dependency Tree (DT), in which each node is associated with a caption word and each edge represents a syntactic dependency relation between two words (Fig. 1). The DT, jointly with the visual features extracted from the image using a frozen visual encoder, are used to build a CGM, which describes the dependency relations among image patches and textual tokens. Our token prediction strategy is based on the dependency relations contained in this CGM. The rationale behind this approach is illustrated in Fig. 1 using the caption \"A brown bird has a small yellow head\". For instance, in the resulting DT, the adjective \"brown\" depends on the noun \"bird\". However, using a standard AR approach, where the token prediction order follows the English grammar, the captioning model should predict \"brown\u201d before knowing that this adjective refers to \u201cbird\u201d, which is a quite ambiguous task, since many objects may be brown in the image. Conversely, when our model predicts the adjective (\"brown\"), it knows the noun (\u201cbird\") it refers to, thus the word generation can be specific to the entities, the attributes and the relations contained in the input image. Generally speaking, we factorize the joint distribution of all the caption words following the disentangled factorization of a CGM (Sch\u00f6lkopf et al., 2021), and our semi-parallel AR model predicts a token conditioned only on the tokens on which it depends. For instance, in the example of Fig. 1, \"small\" and \"yellow\" are predicted in parallel and they are conditionally independent given \u201chead\u201d, thus no statistical dependence is learned between these two words. The advantage of this strategy is that the decoder can focus on learning only the main causal dependency relations, ignoring possible spurious associations (Pearl & Verma, 1995) induced by the sequential order of the words in a natural language sentence. Moreover, we use the same prediction strategy also at inference time, when we compute the likelihood of a candidate caption. In this case too, the use of the CGM makes the likelihood estimation independent of spurious associations due to the sequential order of the words.\nWe validate our method using different VLMs (CLIP, XVLM (Zeng et al., 2022) and InstructBLIP (Dai et al., 2023)). Using extensive experiments with five compositional datasets, we show that our approach largely outperforms all previous works, setting a new state of the art in all the evaluated benchmarks, and that it also improves on Cap and CapPa, despite being trained on much less data."}, {"title": "3 METHOD", "content": "Given an image-caption pair (X, C), our goal is to define a set of conditional distributions over the random variables associated with the image features and the caption words. For this purpose, as anticipated in Sec. 1 and 2, we use an off-the-shelf dependency parser (Dozat & Manning, 2016) which, for a specific C = [W1,..., wn], returns a DT T\u00b2 (Fig. 1), where each node corresponds to a word and each edge (i, j) connects the \u201cdependent\u201d word wj with its \"head\" wi (Sec. 2). \u0422 contains the syntactic and semantic dependencies between the words in C (Nivre, 2005), and we make this dependency explicit by connecting each word to all the words it transitively depends on in the tree. Specifically, we define a CGM G by associating each word wj with a random variable Wj, corresponding to a node of G. Moreover, we connect the node corresponding to Wj with all the variables corresponding to the ancestors of wj in T (Fig. 1).\nFormally, if Wi\u2081, ..., Wik are the ancestors of wj in T, then we assume a causal dependence between the corresponding variables: Wi\u2081 \u2192 Wj, ..., Wik \u2192 Wj. Furthermore, the parser labels each word in T with a syntactic type using a prefixed vocabulary V (Silveira et al., 2014; Zhang et al., 2020). For instance, if type(wj) = nsubj \u2208 V, it means that wj is a noun and it plays the role of the subject in the sentence. Intuitively, we can think of these syntactic types as categorical syntactic features extracted from C, which we formally describe using n random variables S1, ..., Sn, where each S; ranges over V. In G, we assume that each W; depends on its corresponding syntactic variable Sj: S; \u2192 Wj.\nFinally, we extract a set of features from X using the VLM visual encoder E: Z = E(X) = {Z1,..., Zm} (details in Sec. 3.1), and, similarly to the textual case, we associate a random variable Zk to each feature zk \u2208 Z. In G, we assume that W; depends on all the visual variables: Z1 \u2192 Wj, ..., Zm \u2192 Wj. Using the above assumptions, we define the parents (Sch\u00f6lkopf et al., 2021) of"}, {"title": "4 EXPERIMENTS", "content": "In our evaluation we use four common compositional benchmarks: ARO (Yuksekgonul et al., 2023), SugarCrepe (Hsieh et al., 2024), VL-CheckList (Zhao et al., 2022) and ColorSwap (Burapacheep et al., 2024), and an additional benchmark FG-OVD (Bianchi et al., 2024) which we propose in this paper. Most of them are composed of different tasks and datasets, and we report both the task-specific and the average accuracy across all tasks. Following (Zhang et al., 2024), we do not use COCO Order and Flickr Order (two of the ARO tasks) because it has been previously showed that a \"blind\" LM, with no access to the image, can achieve about 99% accuracy on these tasks (Tschannen et al., 2023). The reason of this is due to the grammatical and semantic errors introduced in the negatives when swapping or replacing caption words (see Sec. 1). For instance, the sentence \u201cwith man is wearing ears the an glasses pierced orange hat and\" (Flickr Order) can be easily detected as false by an LM without any visual knowledge. In App. C.2 we show additional results with Winoground (Thrush et al., 2022), which, however, is often not used by other methods based on CLIP (Zhang et al., 2024) because requires the VLM to be able to detect out-of-focus objects in low-resolution images (Diwan et al., 2022). In contrast, we propose to use FG-OVD (Bianchi et al., 2024), a benchmark originally proposed to evaluate the ability of open-vocabulary object detectors to discern fine-grained object properties. In FG-OVD, negative captions are created starting from the object-specific captions by replacing attributes referring to the object's color, material, texture, etc. We crop the objects' bounding boxes which we use jointly with positive and negative captions and an image-to-text retrieval task (more details in App. B)."}, {"title": "4.1 ABLATIONS", "content": "In the experiments of this section we follow a widely adopted protocol, first proposed in (Yuksekgonul et al., 2023), in which the VLM backbone is CLIP and the only training dataset is COCO (Lin"}, {"title": "4.2 MAIN EXPERIMENTS", "content": "Setting. In this section we compare COGT with state-of-the-art compositional methods. Since different works are based on different VLMs and use different training data, to make the comparison as fair as possible, we split our evaluation based on both the VLM backbone and the used training set. Specifically, in Tab. 3 we group the approaches based on CLIP (Radford et al., 2021) and in\n3There are no publicly available network weights for (Tschannen et al., 2023)."}, {"title": "4.3 DOWNSTREAM TASKS", "content": "Doveh et al. (2023b) show that most compositional methods usually deteriorate the VLM skills on non-compositional, standard tasks. We analyze this aspect using the protocol adopted by (Yuksekgonul et al., 2023; Zhang et al., 2024), which is based on linear probing the fine-tuned CLIP visual encoder on CIFAR10, CIFAR100 and ImageNet. Since in COGT E is frozen, we use \ud835\udc38 jointly with our mapping network M and, specifically, the feature \ud835\udc67 [CLS]. The results shown in Tab. 6 show that COGT not only does it not deteriorate CLIP's features but it can even improve them."}, {"title": "5 CONCLUSION", "content": "In this paper we presented COGT, a compositional method based on a semi-parallel generative training. Specifically, we exploit the a priori knowledge of an off-the-shelf dependency parser to define a set of causal relations between the words of a sentence. These relations, collectively represented using a CGM, are used to make sparser the joint probability distribution of the textual variables by removing possible spurious inter-variable associations. As a result, COGT can better exploit the training data and reduce the risk of overfitting. Using extensive experiments, we showed that COGT is much more effective than standard AR or fully-parallel generative predictions and it largely outperforms all previous compositional works, including methods trained with much larger datasets."}, {"title": "A CAUSAL GRAFICAL MODELS", "content": "A Causal Graphical Model (CGM) over n random variables X = {X1, ..., Xn} is defined (Perry et al., 2022) as M(G, Px), where: (1) G is a directed acyclic graph with vertices X and edges X\u1d62 \u2192 X\u2c7c iff X\u1d62 is a direct cause of X\u2c7c; (2) Px is the joint distribution of X which follows the disentangled (or causal) factorization (Sch\u00f6lkopf et al., 2021; Perry et al., 2022):\n$P(X\u2081, ..., Xn) = \\prod_{j=1}^{n} P(X_j | PA(X_j)),$ \nwhere PA(X\u2c7c) is the set of parents (direct causes) of X\u2c7c in G. The difference between a CGM and a Directed Graphical Model is that the former assumes that PA(X\u2c7c) are direct causes of X\u2c7c. Although a formal proof that a statistical dependence is also a causal relation using only observational data is notoriously difficult (Pearl, 2009), in this paper we assume that the linguistic dependencies (Nivre, 2005) extracted by a dependency parser have a causal nature because they represent a strict linguistic association between the \"head\" and its \"dependent\". Specifically, Dependency Grammars (Sec. 2) can be considered as (probabilistic) generative grammars (Nivre, 2005; Chen & Manning, 2014; Obr\u00eabski & Gralinski, 2004; Diaconescu, 2002), in which a dependence between a \"head\" word and its \"dependent\" word can be extracted using context-free generative rules. We interpret these rules as causal mechanisms (Sch\u00f6lkopf et al., 2021), which describe the causal influence of generating a specific \"dependent\" word given the value of the 'head' word. We leave as future work the possibility of replacing the dependency grammars used in this paper with other grammars such as, for instance, the causal grammars proposed in (Tenenbaum et al., 2007), as well as the possible introduction of counterfactual reasoning (Pearl, 2009) in our framework.\nIn Sec. 3, Eq. (1) is obtained using Eq. (5), the definition of conditional distribution and the assumption that S1, ..., Sn and Z1,..., Zm are independent of each other. Finally, the cardinality of {W1, ..., Wj-1} in Eq. (2) is, on average, \u00bd, while, assuming a balanced DT T, the cardinality of {Wi1,..., Win}C PA(W;) is, on average, O(log(n)). The consequence of this is that the conditional distributions learned at training time (Eq. (1)) and used at inference time (Eq. (4)) are sparser than those learned by a standard AR model (Sec. 3)."}, {"title": "B THE FG-OVD DATASET", "content": "In this section, we describe the compositional benchmark based on the FG-OVD dataset which we propose in this paper and which was used in Sec. 4. The FG-OVD dataset was originally proposed to evaluate the fine-grained discriminative capabilities of open-vocabulary detectors in object detection tasks. Each image usually contains multiple objects, where each object is associated with both a bounding box and a corresponding caption. We use the bounding box to crop the image and the corresponding caption as the true caption. The cropped images are resized to a resolution of 224 x 224. Then, each object image is associated with several false captions (on average, ten), selected based on the original FG-OVD Trivial, Easy, Medium, and Hard tasks (Bianchi et al., 2024). Specifically, in the Trivial task, negative captions are randomly sampled from unrelated objects (of different images), offering a basic challenge for retrieval. The Easy, Medium, and Hard tasks progressively increase the difficulty by generating negative captions starting from the true caption. For instance, in the Easy task, three attributes of the true caption are replaced with three unrelated attributes. In the Medium and the Hard task, two and one attributes are replaced, respectively. The rationale is that the less the true sentence is modified, the harder is to distinguish the false from the true captions (Bianchi et al., 2024). Specifically, as fewer attributes are replaced, the distinction between correct and incorrect captions becomes more subtle, requiring the model to capture increasingly fine-grained, compositional details in the image-text pairs. Tab. 11 reports the number of testing images for each task, while different examples are shown in App. E.1."}, {"title": "C ADDITIONAL EXPERIMENTS", "content": "C.1 ADDITIONAL ABLATIONS\nIn Tab. 7 we show an extension of Tab. 2 containing all the possible combinations of the components analyzed in Tab. 2. For instance, this table shows the importance of using two visual feature layers in some of the datasets. Indeed, when both layers are used, the model can leverage not only high-level, abstract visual features but also more detailed, lower-level information. This is particularly important in tasks which require attention to object details, where the additional layer helps to capture a more nuanced representation of the input. For instance, in datasets like ColorSwap, this deeper feature extraction leads to drastic improvements (observed across all parsers), which is probably due to the better representation of the color/texture appearance in the lower level features. On the other hand, the use of mask-specific tokens also plays a significant role. Although the improvement magnitude varies depending on the dataset and the task, the overall trend indicates that using mask-specific tokens contributes positively to the accuracy.\nComparing the results of COGT-CLIP with COGT-CLIP+ and COGT-XVLM with COGT-XVLM+ (Tab. 3 and Tab. 4), the mean improvement of the larger-training versions with respect to the COCO-only trained models is about 9 points averaged across all the datasets. This shows that COGT can benefit from larger training and, indirectly, that the noisier captions in CC3M can be effectively parsed by our parser. Finally, a recent trend in VLM fine-tuning and/or pre-training adopts LLMs to create or re-write the textual annotations (Li et al., 2024c; Doveh et al., 2023a), which can in principle help the dependency parser with very noisy captions. We leave this as future work."}, {"title": "C.2 WINOGROUND AND THE MULTIMODAL VISUAL PATTERN (MMVP) BENCHMARKS", "content": "In Tab. 8 we show additional results obtained using the Winoground (Thrush et al., 2022) and the MMVP (Tong et al., 2024) benchmarks. MMVP is a relatively small dataset, with 9 tasks but only 15 samples per task (135 total samples), thus performance measured on this benchmark has a limited statistical significance. Nevertheless, also in this dataset COGT-InstructBLIP+ largely outperforms the second best tested model (DAC-LLM) by 4.45 points. On the other hand, on Winoground, COGT-InstructBLIP+, despite an improvement of +7.5 points with respect to InstructBLIP zero-shot, it is largely outperformed by InstructBLIP-VQA Score (Lin et al., 2025). InstructBLIP-VQA could eventually be merged with COGT, but we leave this as a future work."}, {"title": "C.3 FG-OVD", "content": "Tab. 10 reports the FG-OVD task-specific accuracy of the methods compared in Tab. 3. In the Zero-shot category, CLIP (Radford et al., 2021) shows a solid performance on all but the Hard task (21.35 points accuracy). The same applies to those methods based on CLIP, such as NegCLIP (Yuksekgonul et al., 2023), GNM (Sahin et al., 2024), Plausible Adj. Neg (Buettner & Kovashka, 2024), and CE-CLIP (Zhang et al., 2024), which show a noticeable drop in performance as the difficulty increases. Among these models, Plausible Adj. Neg (Buettner & Kovashka, 2024) stands out with a relatively strong performance, especially on the Medium (43.13) and Easy (45.88) tasks. On the other hand, COGT-CLIP and COGT-XVLM demonstrate a significant improvement in performance across all difficulty levels, particularly on the Hard task, where COGT-CLIP achieves a"}, {"title": "C.4 COMPUTATIONAL EFFICIENCY", "content": "Tab. 9 shows the training and inference times for COGT and CLIP-based models. COGT-CLIP and COGT-CLIP+ require respectively 8 and 72 hours to train on a single RTX A5000 GPU with a batch size of 128 using the datasets of Sec. 4.2. For comparison, we use DAC-SAM and DAC-LLM (Doveh et al., 2023a), which are the only models we know with publicly available training times that can be directly compared to COGT-CLIP+, as they are trained on a similar dataset of ~3.3M samples. In particular, both DAC-SAM and DAC-LLM complete their training in 12 hours on six V100 GPUs with a batch size of 32. However, this training time does not include the computationally intensive dense annotation generation pipeline (Sec. 2), which involves BLIP2 and SAM or GPT-Neo-2.7B (for DAC-SAM and DAC-LLM, rispectively). Similarly, the training times for COGT-CLIP and COGT-CLIP+ do not include the DT generations, which however involves a relatively quick preprocessing step (one DT per caption), taking approximately 3 minutes for COCO and 1.5 hours for the combined CC3M, COCO, and Visual Genome datasets. Moreover, we evaluate the computational costs of COGT and CLIP in terms of memory usage and inference time. COGT-CLIP, COGT-CLIP+, and CLIP require 0.73 GB, 0.88 GB, and 1.16 GB of memory, respectively, where the difference with respect to CLIP is mainly due to the fact that COGT does not use the CLIP textual encoder. Finally, for both COGT-CLIP and COGT-CLIP+, the inference times reported in Tab. 9 include an additional 0.01 seconds required for generating the DT of the testing caption using the Deep Biaffine + ROBERTa parser, and COGT-CLIP+ is slower than COGT-CLIP because of its larger (four blocks) decoder. All times are computed using an RTX A5000 GPU with a batch size of 32."}, {"title": "D IMPLEMENTATION DETAILS", "content": "D.1 ARCHITECTURES\nIn COGT-CLIP and in COGT-XVLM we use ViT-B/32 CLIP (Ilharco et al., 2021) and the Swin-Transformer of XVLM (Zeng et al., 2022) as the visual encoder, respectively. In both cases, we use both the last and the penultimate layer features of the encoder (Sec. 3.1). In COGT-InstructBLIP, we use the output of the InstructBLIP Q-Former (Dai et al., 2023) as the visual encoder. Since InstructBLIP needs a textual description of the task (called \u201cinstruction\" (Dai et al., 2023)), COGT-InstructBLIP is trained using the prompts suggested in (Dai et al., 2023) for captioning tasks. At inference time, both the zero-shot results of InstructBLIP and those of COGT-InstructBLIP are obtained using the prompt \"Write a description for the photo.\". The above considerations apply also to the COGT-X+ models.\nIndependently of the VLM encoder, the features Z are obtained using a mapping network M on top of the corresponding frozen visual encoder (Sec. 3.1). Our decoder consists of 3 blocks (respectively, 4 blocks in case of COGT-X+), each composed of a multi-head Dependency Guided Attention (Sec. 3.1) and a cross-attention layer. Each attention layer is composed of 8 attention heads, with embedding size equal to 512, while we use 12 heads and embedding size equal to 768 in the COGT-X+ models. We apply a dropout rate of 0.1 to the residual connections, the attention weights, and the embeddings.\nThe differences in the number of trainable parameters among the different baselines in Tab. 1 are only due to the size of the embedding dictionary. Fully-Parallel has an embedding dictionary consisting of only one MSK token, resulting in a total of 13 million trainable parameters, of which only 512 are dedicated to represent the MSK token. The total number of trainable parameters for Mixed, Sequential-AR, and COGT is approximately 64 million (M included). Among these, Sequential-AR uses an embedding dictionary that matches the size of the CLIP ViT-B/32 textual encoder, while Mixed introduces an additional MSK token for parallel processing. In contrast, COGT employs 45 extra MSK tokens, each representing a specific dependency relation extracted by the parser, resulting in a negligible increase in the total parameter count of only 0.04%. All the decoders in Tab. 1 are composed of three blocks which differ only in their attention masks, and they all alternate a textual-token embedding attention layer with a cross-attention layer with the features Z (extracted from the last and the penultimate layer of the CLIP visual encoder, see Sec. 3.1)."}, {"title": "D.2 TOKENIZATION", "content": "The output of the dependency parser is a tree in which each node corresponds to a caption word. In contrast, the COGT decoder uses a standard sub-word tokenization, splitting words into smaller tokens which brings to a smaller embedding dictionary. This discrepancy leads to cases where a word of the dependency tree is split into multiple tokens by the COGT decoder's tokenizer. We"}, {"title": "E DATASETS AND TASKS", "content": "We provide details about the FG-OVD dataset and in App. B and we briefly summarize here the main characteristics of the other benchmarks. Tab. 11 shows the main statistics of each dataset and in App. E.1 we show a few images illustrating the benchmark typically tasks.\nARO (Yuksekgonul et al., 2023) is a VLM benchmark for compositional reasoning and word-order sensitivity. It is composed of two main tasks: Visual Genome Relation and Visual Genome Attribution. In the Visual Genome Relation task, the goal is to evaluate the models' ability to correctly interpret the relationships between objects. On the other hand, Visual Genome Attribution focuses on evaluating the ability to associate the correct attribute with the correct object. As mentioned in Sec. 4, we do not use COCO Order and Flickr Order because different authors recently found that grammatical errors in the generated captions of these datasets lead to tasks which can be solved purely relying on an LLM language prior (Zhang et al., 2024; Tschannen et al., 2023; Lin et al., 2024).\nSugarCrepe (Hsieh et al., 2024) is a dataset developed to evaluate how well VLMs can understand and process complex compositional tasks by presenting them with carefully designed hard negative examples. Drawing inspiration from datasets like CREPE (Ma et al., 2023), VL-CheckList (Zhao et al., 2022), and ARO (Yuksekgonul et al., 2023), SugarCrepe focuses on atomic concepts and their compositions, such as objects, attributes, and relations. The dataset is split into three tasks: \"Replace\", \"Swap\" and \"Add\". In \"Replace\", an atomic concept in the original text is replaced with a new, mismatched concept. A replacement can involve an object, an attribute or a relation. In"}, {"title": "F SYNTACTIC CATEGORIES", "content": "We report in Tab. 12 the 45 syntactic categories defined in (Silveira et al., 2014) and which form our set V (Sec. 3)."}]}