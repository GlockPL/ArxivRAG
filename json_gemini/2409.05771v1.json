{"title": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models", "authors": ["Emily Cheng", "Richard J. Antonello"], "abstract": "Research has repeatedly demonstrated that intermediate hidden states extracted from large language models are able to predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use manifold learning methods to show that this abstraction process naturally arises over the course of training a language model and that the first \"composition\" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.", "sections": [{"title": "Introduction", "content": "How do brains and machines take low-level information, such as a collection of sounds or words, and compose it into the rich tapestry of ideas and concepts that can be expressed in natural language? This question of composition, or abstraction, is at the heart of most studies of human language comprehension. Recent work has shown that representations from large language models (LLMs) are able to successfully model human brain activity at varying spatial and temporal resolutions with only a linear transformation [1\u20137]. This has led to questions about the reason for this brain-model similarity. Do LLMs and brains possess similar representations because they have similar learning properties or objectives? [8, 9, 1] Or is the similarity merely a consequence of shared abstraction, the ability to represent features not derivable from the lexical properties of language alone? [10]\nIn this work, we present new evidence that it is the abstractive, compositional properties of LLMs that drive predictivity between LLMs and brains. We do this by examining an underexplored and unexplained phenomenon of the similarity - the tendency for intermediate hidden layers of LLMs to be optimal for this linear transfer task. We show that an LLM layer's performance at predicting brain activity is strongly related to intrinsic dimensionality of that layer relative to other layers in the same network. Furthermore, we demonstrate that this relationship is itself an indicator that pretrained LLMs naturally split into an early abstraction, or composition, phase, and a later prediction, or extraction, phase, a result independently suggested in the LM interpretability literature [11, 12]. We suggest that it is the first abstraction phase, rather than the latter prediction phase, that primarily drives the observed correspondence between brains and LLMs."}, {"title": "Methods", "content": "We test the hypothesis that feature abstraction, not next-token prediction per se, drives brain-model similarity. To do so requires three observables. First, we measure the dependent variable, (1) brain-model representational similarity, by scoring the prediction performance of a learned linear mapping from LLM representations to brain activity. Then, we compute the (2) dimensionality of representations to measure abstract feature complexity over the LM's layers. Finally, to test the alternate hypothesis that next-token prediction drives brain-LM similarity, as has been suggested by others [13, 8, 1], we compute the (3) surprisal, or next-token prediction error, from each layer. In contrast to prior surprisal measurement approaches [10], we compute this layerwise surprisal using the TunedLens approach devised by Belrose et al. [14] to reduce measurement noise."}, {"title": "Brain-model similarity", "content": "fMRI data We used publicly available functional magnetic resonance imaging (fMRI) data col-lected from 3 human subjects as they listened to 20 hours of English language podcast stories over Sensimetrics S14 headphones. Stories came from podcasts such as The Moth Radio Hour, Modern Love, and The Anthropocene Reviewed. Each 10-15 minute story was played during a separate scan. Subjects were not asked to make any responses, but simply to listen attentively to the stories. For encoding model training, each subject listened to roughly 95 different stories, giving 20 hours of data across 20 scanning sessions, or a total of ~33,000 datapoints for each voxel across the whole brain. Additional details of the MRI methods are summarized in Appendix D.\nNeural encoding model training To train encoding models, we use the method described in [4]. For each word in the stimulus set, activations were extracted by feeding that word and its immediate preceding context into the LLM. A sliding window was used to ensure each word received a minimum of 256 tokens of context. Activations were then downsampled using a Lanczos filter and FIR delays of 1,2,3 and 4 TRs were added to account for the hemodynamic lag in the BOLD signal. A linear projection from the downsampled, time-delayed features was trained using ridge regression. Encoding models were built using the OPT language model [15] (three sizes - 125M, 1.3B, 13B) and the 6.9B parameter deduped Pythia language model [16]. To study model training, 9 different Pythia model checkpoints were used (at 1K, 2K, 3K, 4K, 8K, 16K, 32K, 64K, and 143K training steps)."}, {"title": "Dimensionality of neural manifolds", "content": "To measure the relationship between encoding performance and representational complexity, we compute the intrinsic dimensionality $I_d$ as well as the linear effective dimensionality d of activations at each layer. $I_d$ and d describe different geometric properties of the representations: while the former describes the dimension of the representations' underlying (nonlinear) manifold, the latter describes the number of linear directions that explain their variance up to a threshold. When unambiguous, we will use the word dimensionality to refer to both $I_d$ and d, specifying when necessary.\nWe are interested in an LM's behavior on a representative sample of natural language, so that the computed dimensionality is informative about the LM's linguistic processing in general. For all LMs mentioned in the previous section, we compute the ID on N = 10000 20-word contexts randomly sampled from The Pile [17], which constitute Pythia's training data,\\u00b9 for 5 random data samples. Each sequence is first transformed into a sequence of tokens, which are the atomic (subword) units that constitute the LM's vocabulary. Then, as the tokenization scheme may result in sequences of variable length, we aggregate representations at each layer by taking that of the last token in the model's residual stream [18]; this yields one N \u00d7 D matrix of representations per layer, D being the model's hidden dimension, or extrinsic dimension.\nNonlinear ID estimation To compute $I_d$, we apply the Generalized Ratios Intrinsic Dimension Estimator (GRIDE) [19], an extension of the popular TwoNN estimator [20] to general scales. GRIDE operates on ratios $\u00b5_{i,2k,k} := r_{i,2k}/r_{i,k}$, where $r_{i,j}$ is the Euclidean distance between point i and its jth neighbor. Assuming local uniform density up to the 2kth neighbor, the ratios $\u00b5_{i,2k,k}$ follow a"}, {"title": "Measuring layerwise surprisal", "content": "Causal LLMs are trained to predict the next token in context. The LLM produces a conditional probability distribution $P_{LM} (\u00b7|x_{<i>})$ over the next token $x_i$ given a linguistic context $x_{<i>}$. The loss is given by the negative log-likelihood of the ground-truth token under $P_{LM}$, equivalent to minimizing the information-theoretic coding length, or surprisal, of the next token given context."}, {"title": "Results", "content": "Layerwise encoding performance and layerwise representational dimensionality across layers are highly correlated, consistently across brain areas involved in linguistic processing. Table 1 shows the correlation between encoding performance and dimensionality, averaged over all voxels. The relationship is largely consistent across a variety of metrics for measuring dimensionality.\nFigure 1a shows the correlation between average encoding performance and normalized $I_d$ for various model sizes in from the OPT model family. The positive relationship, p = 0.85, between normalized $I_d$ and encoding performance suggests that in trained language models, the $I_d$ of layer activations captures abstract linguistic feature complexity needed to support language comprehension.\nFigure 1b overlays, for OPT-1.3b, the encoding performance, $I_d$, and next-token prediction loss computed from each layer. Observe that encoding performance peaks at layer 17, which exactly marks the sharp downwards turn in prediction loss. While Cheng et al. [12] show that layers leading up to the $I_d$ peak extract high-level features related to syntax and semantics, our results additionally indicate a shift in post-$I_d$-peak to next-token prediction. This sharp phase transition from abstraction to prediction is observed across model sizes, but it is more gradual for Pythia (see Appendix E.1).\nTo further verify the existence of a phase transition, we report the inter-layer representational similarity via linear Centered Kernel Alignment [23]. Figure 1d depicts at least two phases of inter-layer similarity (lighter is more similar): the $I_d$ peak approximately marks a junction at which preceding layers are no longer similar to following layers. Results generally hold across models, see Appendix E.3.\nFigure 1c shows the correlation of $I_d$ with encoding performance across layers at the voxelwise level (red is better), in a single subject. With the exception of the primary auditory cortex, which processes low-level auditory information, encoding performance in brain areas thought to handle higher-level linguistic processing is well-predicted by $I_d$ across layers. Results generally hold across subjects, model families, and model sizes, see Appendix E.2.\nThe relationship between encoding performance and $I_d$ arises nontrivially from learning, in a way that does not simply reflect the layer position. Figure 2 plots the encoding performance and $I_d$ across layers over the course of training for Pythia-6.9B (each curve is a different checkpoint). We confirm an existing result from the literature that the characteristic $I_d$ peak emerges, and moreover, that $I_d$ generally grows for all layers over training (Figure 2 right) [12]. Furthermore, the encoding performance (left) and $I_d$ (right) increase at similar rates over training, seen by similar positions of the checkpoint curves in the two plots. The two plots are globally correlated with p = 0.94.\nThe location of the $I_d$ peak (red dots, right), changes over the course of training, eventually settling at the same layers for peak encoding performance (red dots, left). This rules out the possibility that the $I_d$ peak is a trivial function of the Transformer architecture, e.g., layer index."}, {"title": "Discussion", "content": "Recent studies of the properties of language encoding models have observed that the intermediate layers of LLMs, rather than the output layers, have the highest linear similarity to measured brain activity. This is true regardless of the scanning modality (be it fMRI [4], ECoG [24], or MEG [8]), and regardless of the chosen LLM. Despite this very frequently observed trend, little research has been dedicated to explaining this phenomenon. Yet, an understanding of this trend would greatly benefit our understanding of both brains and LLMs, not least because layerwise differences in LLMs have highly useful epistemic properties. LLM layers are invariant to many confounding variables - each layer has seen the same data in the same order, has an identical architecture, was trained on the same loss term, and built using the same hyperparameters. Therefore, differences between layers can only arise either as a result of the compositional nature of the transition from earlier layers to later ones, or due to the \"time pressure\" exerted by the loss term on the final output layers.\nThese competing pressures, to first build up the most comprehensive representation of the input text possible, and to then ultimately use this representation to resolve towards a distribution over predicted next word outputs, have opposite effects, as we demonstrate here. The composition effect leads to a increase in encoding performance and dimensionality, whereas the prediction effect narrows the dimensionality to the detriment of encoding. Furthermore, we observe that as models get larger and more thoroughly trained, the best layer for encoding slowly drifts to earlier in the model, perhaps suggesting a saturation effect for this initial compositional phase.\nWhat conclusions should we draw from this? Firstly, that it is not likely to be the autoregressive nature of language models that drives brain-model similarity [9, 1, 10]. As models get more potent at prediction, their most predictive and most descriptive layers drift apart.\u00b2 Secondly, we can draw that the multi-phase abstraction process in LLMs that has been proposed independently by other authors [12, 11] is supported by evidence from the only other system known to effectively reason with complex language, the human brain. As the present work only tests two model families, it will be necessary to test more models for conclusions to hold in the general case.\nFrom a practical perspective, conclusions point to a potential new avenue for improving the perfor-mance of encoding models. If the spectral properties of different LLM layers can be measured and efficiently combined to produce a representation with higher $I_d$ than any individual layer, then we might expect that new representation to outperform any single layerwise encoding model coming from the same LLM. As linear layerwise encoding models reach their limit, such methods may be necessary to see further benefits."}, {"title": "Computing resources", "content": "Dimensionality and surprisal computation were run on a cluster with 12 nodes with 5 NVIDIA A30 GPUs and 48 CPUs each. Extracting and computing dimensionality on LM representations took a few wall-clock hours per model. Training TunedLens took around 15 minutes per layer, so overall 30 wall-clock hours. We parallelized all computation, and estimate the overall parallelized runtime, including preliminary experiments and failed runs to be around 6 days.\nRidge regression was performed using compute nodes with 128 cores (2 AMD EPYC 7763 64-core processors) and 256GB of RAM. In total, roughly 1,000 node-hours of compute was expended for these models. Feature extraction for language models was performed on specialized GPU nodes similar to the AMD compute nodes but with 3 NVIDIA A100 40GB cards. Feature extraction required roughly 300 node-hours of compute on these GPU nodes."}, {"title": "ID Estimation", "content": "For ID estimation using GRIDE, we reproduce the setup in Cheng et al. [12]. For each model, checkpoint, and layer, we perform a scale analysis. The intrinsic dimension of the manifold is sensitive to the scale, or neighborhood size, for which it is estimated [20, 19]. Figure B.1 shows an example, where the GRIDE scale k varies from 20 to 212. As recommended in Denti et al. [19], we choose a scale k corresponding in a range where the intrinsic dimension is stable, or plateaus, by visual inspection. For simplicity, we choose one scale k per model. In the particular example in Figure B.1, we choose k = 24, where the derivative of the curve is closest to 0 for as many layers as possible. Scales chosen for all models are in Table B.1."}, {"title": "Surprisal Estimation", "content": "We used the TunedLens implementation by Ghandeharioun et al. [26]. TunedLens ascertains the amount of information (linearly) encoded in hidden layer t about the next token. To do so, an affine mapping is learned from the last-token hidden representation ht at layer t as follows:\n$\\min_{A_t,b_t} DKL(f_{>t}(h_t) || LayerNorm(A_th_t + b_t)W_U).$ (C.1)\nHere, $A_t \u2208 R^{D\u00d7D}, b_t \u2208 R^D$ are the learnable parameters of the affine mapping. $W_U$ is the LM's unembedding matrix that maps the final layer to the vocabulary. Finally, $f_{>t}(h_t)$ is the layers of the LM f after layer t, producing the model's original distribution over the vocabulary. In the provided code [26], TunedLens is implemented using a direct solver numpy.linalg.lstsq on N = 8000 randomly sampled sequences from The Pile dataset [17], returning the least squares solution that minimizes the $l_2$-norm between ht and last layer representation hr. Finally, we compute the next-token surprisals on a validation set of The Pile (N = 2000) from the TunedLens-modified hidden layers."}, {"title": "fMRI Methods", "content": "MRI data were collected on a 3T Siemens Skyra scanner at The University of Texas at Austin Biomedical Imaging Center using a 64-channel Siemens volume coil. Functional scans were collected using a gradient echo EPI sequence with repetition time (TR) = 2.00 s, echo time (TE) = 30.8 ms, flip angle = 71\u00b0, multi-band factor (simultaneous multi-slice) = 2, voxel size = 2.6mm x 2.6mm x 2.6mm (slice thickness = 2.6mm), matrix size = 84x84, and field of view = 220 mm. Anatomical data were collected using a T1-weighted multi-echo MP-RAGE sequence with voxel size = 1mm x 1mm x 1mm.\nIn addition to motion correction and coregistration [27], low frequency voxel response drift was identified using a 2nd order Savitzky-Golay filter with a 120 second window and then subtracted from the signal. The mean response for each voxel was subtracted and the remaining response was scaled to have unit variance."}]}