{"title": "Exploring Reasoning Biases in Large Language Models Through Syllogism:\nInsights from the NeuBAROCO Dataset", "authors": ["Kentaro Ozeki", "Risako Ando", "Takanobu Morishita", "Hirohiko Abe", "Koji Mineshima", "Mitsuhiro Okada"], "abstract": "This paper explores the question of how accurately current large language models can\nperform logical reasoning in natural language, with an emphasis on whether these models\nexhibit reasoning biases similar to humans. Specifically, our study focuses on syllogistic\nreasoning, a form of deductive reasoning extensively studied in cognitive science as a nat-\nural form of human reasoning. We present a syllogism dataset called NeuBAROCO, which\nconsists of syllogistic reasoning problems in English and Japanese. This dataset was origi-\nnally designed for psychological experiments to assess human reasoning capabilities using\nvarious forms of syllogisms. Our experiments with leading large language models indicate\nthat these models exhibit reasoning biases similar to humans, along with other error tenden-\ncies. Notably, there is significant room for improvement in reasoning problems where the\nrelationship between premises and hypotheses is neither entailment nor contradiction. We also\npresent experimental results and in-depth analysis using a new Chain-of-Thought prompting\nmethod, which asks LLMs to translate syllogisms into abstract logical expressions and then\nexplain their reasoning process. Our analysis using this method suggests that the primary lim-\nitations of LLMs lie in the reasoning process itself rather than the interpretation of syllogisms.", "sections": [{"title": "1 Introduction", "content": "The ability to perform accurate deductive reasoning in natural language, once pursued by classi-\ncal symbolic AI, has recently become a vital issue\nin the development and evaluation of Large Lan-\nguage Models (LLMs) (Qiao et al., 2022; Huang\nand Chang, 2022). Regarding humans, empirical\nresearch in cognitive science has demonstrated that\nhumans often exhibit various errors and tendencies\nin reasoning, known as reasoning biases (Evans,\n1989; Pohl, 2022). Among various forms of reason-\ning, syllogistic reasoning is one of the basic forms\nof deductive reasoning and has been studied exten-\nsively (Johnson-Laird and Steedman, 1978; Evans\net al., 1993; Geurts, 2003). However, the evaluation\nof LLMs and the construction of datasets incorpo-\nrating these experimental methodologies has not\nbeen systematically pursued.\nIn this paper, we present the evaluation of LLMS\nwith NeuBAROCO, a manually constructed syllo-\ngism dataset with diverse properties and annota-\ntions designed to evaluate the reasoning abilities\nand biases of LLMs in syllogistic reasoning tasks.\nNeuBAROCO is a bilingual (Japanese and English)\ndataset and includes detailed annotations for the\ntypes of reasoning biases associated with each prob-\nlem. The dataset is based on a problem set used for\na series of psychological experiments assessing hu-\nman reasoning ability with syllogisms (Shikishima\net al., 2009, 2015). A subset of the problems in the\ndataset is aligned with human performance metrics\nfrom these experiments. Building on the work of\nAndo et al. (2023), we have adapted this problem\nset to evaluate whether language models exhibit\nreasoning biases similar to those of humans.\nThe main contributions in this paper are summa-\nrized as follows.\u00b9 First, we constructed a dataset\ncontaining 95 and 790 syllogistic reasoning prob-\nlems in the format of Multiple-Choice and Natu-\nral Language Inference (NLI) tasks, respectively.\nThis dataset design facilitates comparison with re-\nsults and insights from human psychological exper-\niments and preliminary studies on machine learning\nmodels.\nSecond, we systematically investigated various\nreasoning biases observed in LLMs in relation to\nthe form and content of syllogisms, including belief\nbiases, conversion errors, and atmosphere effects,\nalong with other tendencies, across multiple tasks\nand in both English and Japanese.\nFinally, to more precisely identify the reasoning"}, {"title": "2 Background: Syllogistic reasoning", "content": "In this study, we primarily focus on the logical\ninference task that classifies inferences into three\nlabels: entailment, contradiction, and neutral (nei-\nther entailment nor contradiction).\nA syllogism is an inference that consists of two\npremises and one conclusion, where the premises\nand the conclusion are composed of four basic\ntypes of quantified sentences: all, no, some, and\nsome-not, as shown in Table 1. For example, (1)\nand (2) are syllogisms composed of sentences with\nthe quantifiers all and no.\nThe syllogism in (1) is an instance of entailment,\nwhere if the premises (P1 and P2) are true, then the\nconclusion (C) is also true. The syllogism in (2) is\nan instance of neutral, where the relationship be-\ntween the premises (P1 and P2) and the conclusion\n(C) is neither entailment nor contradiction.\nSyllogisms are relatively simple logical infer-\nences that can be represented in monadic predicate\nlogic (\u0141ukasiewicz, 1951), a fragment of first-order\nlogic where each predicate can take only one argu-\nment. Table 1 shows how to translate each type of\ncategorical sentence into logical notations, that is,\ninto predicate logic and set theory.\nDespite being logically simple, syllogisms are\nknown to be challenging inferences for humans.\nWhich types of syllogisms are prone to causing\nerrors in human reasoning, or in other words, in-\nvolve reasoning biases, is a topic widely studied\nin the field of cognitive science of human reason-\ning (Evans et al., 1993; Manktelow, 1999; Geurts,\n2003; Stenning and van Lambalgen, 2012; Khem-\nlani and Johnson-Laird, 2012). Our choice to focus\non syllogistic reasoning is deliberate and aims to\nfacilitate comparisons with insights from the exten-\nsive research on biases and reasoning in cognitive\nscience. Some typical biases of syllogism will be\nintroduced in Section 3.2. Focusing on the pos-\nsibility of a detailed classification of such human\nreasoning biases, this study uses syllogistic reason-\ning in natural language as a testbed to evaluate the\nlogical reasoning capabilities of LLMs."}, {"title": "3 The NeuBAROCO dataset", "content": "The NeuBAROCO dataset is based on a syllo-\ngism problem set called the BAROCO test (Shik-\nishima et al., 2005, 2009), originally designed for\nlarge-scale research on human reasoning abilities.\nBAROCO includes the so-called belief-bias tasks,\nwhich are typical examples involving human rea-\nsoning biases (see Section 3.2.1). In addition to\nlinguistic tasks, it also includes Euler diagram tasks\nto test spatial cognition. These formats of reason-\ning were used to investigate the correlation and\nthe contributions of genetic and environmental fac-\ntors through twin studies (Shikishima et al., 2005).\nFurthermore, studies combining these tasks with\nexperimental tasks in behavioral economics have\nalso been conducted (Shikishima et al., 2015).\nAndo et al. (2023) provides a preliminary\nstudy preceding this research, aiming to apply the\nBAROCO problem set for evaluating LLMs by re-"}, {"title": "3.1 Overview of the dataset", "content": "The original BAROCO problem set consists of two\npremises and multiple choices that could serve as\nconclusions. Experiment participants are asked\nto select a logically valid conclusion from the\ngiven choices. The NeuBAROCO dataset was\nconstructed by converting each problem from the\nBAROCO problem set into a format commonly\nused for the NLI task. The NeuBAROCO dataset\nwe use in this paper includes 790 problems for the\nNLI task, namely tasks that classify inferences into\nentailment, contradiction, and neutral. Of these,\n254 problems are classified as entailment, 188 as"}, {"title": "3.2 Annotation", "content": "We focus on three types of biases in syllogistic\nreasoning. The three types of biases addressed here\ncan be categorized into two kinds: biases related\nto content (belief bias) and biases related to form\n(conversion errors and atmosphere effects). By\ninvestigating these three types of biases, we can\nsystematically evaluate whether LLMs are sensitive\nto the roles of content words and function words in\ndeductive reasoning."}, {"title": "3.2.1 Labels for bias related to content", "content": "To investigate biases caused by content words such\nas nouns and verbs, we categorized each inference\ninto three types based on whether it is congruent\nwith commonsense beliefs. Table 2 shows exam-\nples and the number of instances for each type.\nSymbolic When all terms are composed of sen-\ntences from abstract symbols, the inference is la-\nbeled as symbolic. These types of problems are\nneutral with respect to the beliefs held by humans;\nthat is, the question of whether they agree or dis-\nagree with those beliefs does not arise.\nCongruent If there is no inconsistency with com-\nmonsense beliefs in all premises and conclusions,\nthe inference is labeled as congruent.\nIncongruent If at least one of the premises or\nthe conclusion does not align with commonsense\nbeliefs, the inference is labeled as incongruent. In\nthe example in the bottom row of Table 2, All ani-\nmals are tomatoes and Some humans are tomatoes\ncontradict commonsense beliefs.\nIf it is unclear whether the sentence is consistent\nwith commonsense beliefs, or if it requires spe-\ncialized knowledge (e.g., All agnostics are Stoics.\nSome agnostics are skeptical. Therefore, all Stoics\nare skeptical), the inference is classified as others.\nThere are 50 instances of this type."}, {"title": "3.2.2 Labels for bias related to form", "content": "In addition", "terms": "All A are B and Some A are not\nB are misinterpreted as All B are A and Some B are\nnot A, respectively. For instance, interpreting the\nsentence All students who score above 90 points re-"}]}