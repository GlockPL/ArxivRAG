{"title": "Exploring Reasoning Biases in Large Language Models Through Syllogism:\nInsights from the NeuBAROCO Dataset", "authors": ["Kentaro Ozeki", "Risako Ando", "Takanobu Morishita", "Hirohiko Abe", "Koji Mineshima", "Mitsuhiro Okada"], "abstract": "This paper explores the question of how ac-\ncurately current large language models can\nperform logical reasoning in natural language,\nwith an emphasis on whether these models\nexhibit reasoning biases similar to humans.\nSpecifically, our study focuses on syllogistic\nreasoning, a form of deductive reasoning ex-\ntensively studied in cognitive science as a nat-\nural form of human reasoning. We present a\nsyllogism dataset called NeuBAROCO, which\nconsists of syllogistic reasoning problems in\nEnglish and Japanese. This dataset was origi-\nnally designed for psychological experiments\nto assess human reasoning capabilities using\nvarious forms of syllogisms. Our experiments\nwith leading large language models indicate\nthat these models exhibit reasoning biases sim-\nilar to humans, along with other error tenden-\ncies. Notably, there is significant room for im-\nprovement in reasoning problems where the\nrelationship between premises and hypotheses\nis neither entailment nor contradiction. We also\npresent experimental results and in-depth anal-\nsis using a new Chain-of-Thought prompting\nmethod, which asks LLMs to translate syllo-\ngisms into abstract logical expressions and then\nexplain their reasoning process. Our analysis\nusing this method suggests that the primary lim-\nitations of LLMs lie in the reasoning process it-\nself rather than the interpretation of syllogisms.", "sections": [{"title": "1 Introduction", "content": "The ability to perform accurate deductive reason-\ning in natural language, once pursued by classi-\ncal symbolic AI, has recently become a vital issue\nin the development and evaluation of Large Lan-\nguage Models (LLMs) (Qiao et al., 2022; Huang\nand Chang, 2022). Regarding humans, empirical\nresearch in cognitive science has demonstrated that\nhumans often exhibit various errors and tendencies\nin reasoning, known as reasoning biases (Evans,\n1989; Pohl, 2022). Among various forms of reason-\ning, syllogistic reasoning is one of the basic forms\nof deductive reasoning and has been studied exten-\nsively (Johnson-Laird and Steedman, 1978; Evans\net al., 1993; Geurts, 2003). However, the evaluation\nof LLMs and the construction of datasets incorpo-\nrating these experimental methodologies has not\nbeen systematically pursued.\nIn this paper, we present the evaluation of LLMS\nwith NeuBAROCO, a manually constructed syllo-\ngism dataset with diverse properties and annota-\ntions designed to evaluate the reasoning abilities\nand biases of LLMs in syllogistic reasoning tasks.\nNeuBAROCO is a bilingual (Japanese and English)\ndataset and includes detailed annotations for the\ntypes of reasoning biases associated with each prob-\nlem. The dataset is based on a problem set used for\na series of psychological experiments assessing hu-\nman reasoning ability with syllogisms (Shikishima\net al., 2009, 2015). A subset of the problems in the\ndataset is aligned with human performance metrics\nfrom these experiments. Building on the work of\nAndo et al. (2023), we have adapted this problem\nset to evaluate whether language models exhibit\nreasoning biases similar to those of humans.\nThe main contributions in this paper are summa-\nrized as follows. First, we constructed a dataset\ncontaining 95 and 790 syllogistic reasoning prob-\nlems in the format of Multiple-Choice and Natu-\nral Language Inference (NLI) tasks, respectively.\nThis dataset design facilitates comparison with re-\nsults and insights from human psychological exper-\niments and preliminary studies on machine learning\nmodels.\nSecond, we systematically investigated various\nreasoning biases observed in LLMs in relation to\nthe form and content of syllogisms, including belief\nbiases, conversion errors, and atmosphere effects,\nalong with other tendencies, across multiple tasks\nand in both English and Japanese.\nFinally, to more precisely identify the reasoning"}, {"title": "2 Background: Syllogistic reasoning", "content": "In this study, we primarily focus on the logical\ninference task that classifies inferences into three\nlabels: entailment, contradiction, and neutral (nei-\nther entailment nor contradiction).\nA syllogism is an inference that consists of two\npremises and one conclusion, where the premises\nand the conclusion are composed of four basic\ntypes of quantified sentences: all, no, some, and\nsome-not, as shown in Table 1. For example, (1)\nand (2) are syllogisms composed of sentences with\nthe quantifiers all and no.\nThe syllogism in (1) is an instance of entailment,\nwhere if the premises (P1 and P2) are true, then the\nconclusion (C) is also true. The syllogism in (2) is\nan instance of neutral, where the relationship be-\ntween the premises (P1 and P2) and the conclusion\n(C) is neither entailment nor contradiction.\nSyllogisms are relatively simple logical infer-\nences that can be represented in monadic predicate\nlogic (\u0141ukasiewicz, 1951), a fragment of first-order\nlogic where each predicate can take only one argu-\nment. Table 1 shows how to translate each type of\ncategorical sentence into logical notations, that is,\ninto predicate logic and set theory.\nDespite being logically simple, syllogisms are\nknown to be challenging inferences for humans.\nWhich types of syllogisms are prone to causing\nerrors in human reasoning, or in other words, in-\nvolve reasoning biases, is a topic widely studied\nin the field of cognitive science of human reason-\ning (Evans et al., 1993; Manktelow, 1999; Geurts,\n2003; Stenning and van Lambalgen, 2012; Khem-\nlani and Johnson-Laird, 2012). Our choice to focus\non syllogistic reasoning is deliberate and aims to\nfacilitate comparisons with insights from the exten-\nsive research on biases and reasoning in cognitive\nscience. Some typical biases of syllogism will be\nintroduced in Section 3.2. Focusing on the pos-\nsibility of a detailed classification of such human\nreasoning biases, this study uses syllogistic reason-\ning in natural language as a testbed to evaluate the\nlogical reasoning capabilities of LLMs."}, {"title": "3 The NeuBAROCO dataset", "content": "The NeuBAROCO dataset is based on a syllo-\ngism problem set called the BAROCO test (Shik-\nishima et al., 2005, 2009), originally designed for\nlarge-scale research on human reasoning abilities.\nBAROCO includes the so-called belief-bias tasks,\nwhich are typical examples involving human rea-\nsoning biases (see Section 3.2.1). In addition to\nlinguistic tasks, it also includes Euler diagram tasks\nto test spatial cognition. These formats of reason-\ning were used to investigate the correlation and\nthe contributions of genetic and environmental fac-\ntors through twin studies (Shikishima et al., 2005).\nFurthermore, studies combining these tasks with\nexperimental tasks in behavioral economics have\nalso been conducted (Shikishima et al., 2015).\nAndo et al. (2023) provides a preliminary\nstudy preceding this research, aiming to apply the\nBAROCO problem set for evaluating LLMs by re-"}, {"title": "3.1 Overview of the dataset", "content": "The original BAROCO problem set consists of two\npremises and multiple choices that could serve as\nconclusions. Experiment participants are asked\nto select a logically valid conclusion from the\ngiven choices. The NeuBAROCO dataset was\nconstructed by converting each problem from the\nBAROCO problem set into a format commonly\nused for the NLI task. The NeuBAROCO dataset\nwe use in this paper includes 790 problems for the\nNLI task, namely tasks that classify inferences into\nentailment, contradiction, and neutral. Of these,\n254 problems are classified as entailment, 188 as"}, {"title": "3.2 Annotation", "content": "We focus on three types of biases in syllogistic\nreasoning. The three types of biases addressed here\ncan be categorized into two kinds: biases related\nto content (belief bias) and biases related to form\n(conversion errors and atmosphere effects). By\ninvestigating these three types of biases, we can\nsystematically evaluate whether LLMs are sensitive\nto the roles of content words and function words in\ndeductive reasoning."}, {"title": "3.2.1 Labels for bias related to content", "content": "To investigate biases caused by content words such\nas nouns and verbs, we categorized each inference\ninto three types based on whether it is congruent\nwith commonsense beliefs. Table 2 shows exam-\nples and the number of instances for each type.\nSymbolic When all terms are composed of sen-\ntences from abstract symbols, the inference is la-\nbeled as symbolic. These types of problems are\nneutral with respect to the beliefs held by humans;\nthat is, the question of whether they agree or dis-\nagree with those beliefs does not arise.\nCongruent If there is no inconsistency with com-\nmonsense beliefs in all premises and conclusions,\nthe inference is labeled as congruent.\nIncongruent If at least one of the premises or\nthe conclusion does not align with commonsense\nbeliefs, the inference is labeled as incongruent. In\nthe example in the bottom row of Table 2, All ani-\nmals are tomatoes and Some humans are tomatoes\ncontradict commonsense beliefs.\nIf it is unclear whether the sentence is consistent\nwith commonsense beliefs, or if it requires spe-\ncialized knowledge (e.g., All agnostics are Stoics.\nSome agnostics are skeptical. Therefore, all Stoics\nare skeptical), the inference is classified as others.\nThere are 50 instances of this type."}, {"title": "3.2.2 Labels for bias related to form", "content": "In addition, we assigned the tags conversion and\natmosphere to types of inferences that are prone\nto the two major types of reasoning biases. These\nbiases are induced by function words such as all\nand not, as well as by grammatical factors such as\nword order in the premises and conclusions.\nConversion Conversion error is known as a typi-\ncal reasoning bias in syllogisms (Evans et al., 1993;\nGeurts, 2003). This error occurs when quantified\nsentences are misinterpreted by converting the or-\nder of two terms: All A are B and Some A are not\nB are misinterpreted as All B are A and Some B are\nnot A, respectively. For instance, interpreting the\nsentence All students who score above 90 points re-\nceive an A grade as equivalent to All students with\nan A grade score above 90 points exemplifies this\nerror. Although these two sentences may appear\nsimilar, they do differ in logical meaning. Table 3\npresents examples of syllogisms where such illicit\nconversion results in inference being erroneously\nclassified as valid (entailment) rather than invalid\n(neutral). We assign the conversion label to those\ninferences where a sentence containing all or some-\nnot appears in the premises, and the label changes\nfrom neutral to entailment when the order of terms\nin the sentence is reversed.\nAtmosphere The atmosphere effect indicates the\ntendency to select conclusions that mirror the form\nof the premises (Woodworth and Sells, 1935). This\ninvolves selecting conclusions that superficially re-\nsemble the premises in terms of their logical struc-\nture (Chater and Oaksford, 1999). For example, a\nconclusion containing some might be preferentially\nselected if a premise also contains some. Similarly,\nif a premise containing a negation (no or some-not)\ntends to promote a negative conclusion. We assign\nthe atmosphere label to those inferences with neu-\ntral labels where either (1) a premise contains some\nand the conclusion is particular (some or some-not),\nor (2) a premise contains some-not and the conclu-\nsion is particular or negative (i.e., no, some, or\nsome-not). Table 3 shows an example that satisfies\ncondition (1)."}, {"title": "4 Evaluation Tasks", "content": "We introduce three types of tasks for evaluating\nLLMs using the NeuBAROCO dataset: Multiple-\nChoice, NLI, and Translate-and-Explain.\nMultiple-Choice The Multiple-Choice task is a\nformat widely used in cognitive psychology. In\nthis task, models are presented with two premises\nand asked to choose the correct conclusion from\nfive options. These options include the all, some,\nno, and some-not sentences, as well as the \"none\nof them\" choice. Table 4 presents examples of the\nprompts used in this task.\nNLI The NLI task is a common problem setting\nin NLP, enabling evaluations on specific instances\nof reasoning and aligning well with other NLP"}, {"title": "Translate-and-Explain", "content": "To provide a finer-\ngrained analysis of the reasoning ability of the\nmodels, we design the Translate-and-Explain task,\na variant of the NLI task with a dedicated Chain-\nof-Thought (CoT) prompt. In this task, we emulate\nthe translation between natural language sentences\nand formal expressions of reasoning before the ac-\ntual reasoning step, identifying whether errors and\nbiases stem from the process of interpreting sen-\ntences or from the process of reasoning.\nCoT prompting is a technique of having LLMs\nperform intermediate reasoning steps by using few-\nshot examples or other means, and has been re-\nported to improve the reasoning ability of LLMs\n(Wei et al., 2022). This enables us to apply a\nmethod analogous to the protocol analysis in psy-\nchological experiments (Evans et al., 1983) to the\nevaluation of LLMs.\nThe Translate-and-Explain CoT prompt is a 1-\nshot structural prompt that instructs LLMs to per-\nform (i) a translation step, (ii) an explanation step,\nand (iii) an answer step for each problem. For the\ntranslation step, LLMs are instructed to translate\nthe given syllogism into abstract expressions. In\nthis study, we compare translations into predicate\nlogic in formal language and into set-theory in nat-\nural language (see Table 1 in Section 2). We con-\nducted experiments in three setups: (a) explanation\nwithout translation, (b) predicate logic translation +\nexplanation, (c) set-theoretic translation + explana-\ntion. We manually checked the correctness of the\ntranslation outputs of LLMs."}, {"title": "5 Experiments", "content": "We conducted experiments on the three types of\ntasks both in English and in Japanese. In the\nMultiple-Choice task, we evaluated the following\nmodels:"}, {"title": "5.1 Experimental Setup", "content": "- GPT-3.5 (Ouyang et al., 2022) and GPT-4 (Ope-\nnAI, 2023). The GPT models used were gpt-3.5-\nturbo-1106 and gpt-4-0613 available via Ope-\nnAI's API. The number of parameters for the\nGPT models has not been disclosed.\n- Llama-2 (Touvron et al., 2023) with 13 billion\n(13B) and 70 billion (70B) parameters. For\nLlama-2, detailed model information, including\nthe number of parameters, is publicly available.\n- Swallow (Fujii et al., 2024) with 13B and 70B\nparameters. Swallow is a model family based\non Llama-2 with state-of-the-art Japanese lan-\nguage capability, enhanced through continual pre-\ntraining using a dedicated corpus.\nIn the NLI and Translate-and-Explain tasks, the\nLlama-2 and Swallow models failed to produce\noutput with a valid answer in most cases. There-\nfore, we focus on the evaluation results of the GPT\nmodels, which consistently produced valid outputs.\nThe default values were used for the hyperparam-\neters, except for the maximum output token length."}, {"title": "5.2 Results and Analysis", "content": "This was set to 10 for the Multiple-Choice and NLI\ntasks, and 2,048 for the Translate-and-Explain task,\nwhich are sufficiently long given the design of the\ntasks."}, {"title": "5.2.1 Multiple-Choice Task", "content": "Table 6 shows the experimental results for the\nMultiple-Choice task. The row labeled Human\npresents the average scores of 440 participants\nbased on data from the psychological experiment\nconducted by Shikishima et al. (2009). Note that\nthere are some terminological differences between\nShikishima et al. (2009) and our study.\nIn terms of the biases related to content, incon-\ngruent cases are generally harder for the LLMs than\ncongruent cases in English. In contrast, no similar\ntrend is clearly observed in Japanese problems.\nIn terms of model scale, while the overall ac-\ncuracy of the smaller (13B-parameter) models\nranged from 20% (chance level) to 30%, the 70B-\nparameter models and GPTs achieved an overall\naccuracy of 42% to 95%, with some surpassing\nhuman overall accuracy in Japanese (53%). The\nstrong performance of large-parameter LLMs can\nbe partly attributed to the nature of the Multiple-\nChoice task. As we will see in Section 5.2.2, LLMs\n(especially those with larger parameter) mark high\naccuracy particularly in identifying entailment and\ncontradiction over neutral cases. In this task, the\ncorrect choice is always an entailment of the given\npremises, unless the answer is \u201cnone of them.\u201d Fol-\nlowing the original BAROCO problem set, the\nNeuBAROCO dataset does not include any prob-\nlems where the correct answer is \"none of them.\""}, {"title": "5.2.2 NLI Task", "content": "Table 7 shows the results of the NLI task. The\nfew-shot setting improves the overall accuracy of\nGPT-4 both in English and Japanese.\nIn terms of gold labels, The models achieve\nhigher scores on problems labeled as entailment,\nwhile those labeled as neutral are typically the most\nchallenging. Even GPT-4, which performed the\nbest, scored approximately 30 points lower on neu-\ntral problems compared to other labels. As noted\nabove, when comparing the Multiple-Choice and\nNLI tasks, the Multiple-Choice task includes only\nproblems that correspond to the entailment prob-\nlems in the NLI task. Consequently, the scores for\nthe Multiple-Choice task are similar to those for\nthe entailment problems in the NLI task.\nIn terms of bias-related labels, the results sug-\ngest that LLMs are influenced by the content and\nform of syllogisms. Among the symbolic, congru-\nent, and incongruent cases, the accuracies in the\nincongruent cases are generally lower than those in\nthe other cases and the overall score for each model.\nRegarding conversion errors, the accuracies for the\nproblems labeled conversion, which may cause con-\nversion errors, are significantly lower than the over-\nall accuracies. A similar trend is observed for the\nproblems labeled atmosphere in the context of at-\nmosphere effects."}, {"title": "5.2.3 Translate-and-Explain Task", "content": "Table 8 shows the experimental results for the\nTranslate-and-Explain task. Table 9 provides an\nexample output from GPT-4.\nEnglish With GPT-4, the translations from En-\nglish to predicate logic are highly accurate, cor-\nrectly translating 87/90 problems. However, trans-\nlating the same problems into set theory is more\nchallenging, with 82/90 problems correctly trans-\nlated. It is observed that GPT-4 often responds\nwith contradiction when the correct answer to an\ninference is neither. As a typical error, GPT-4 in-\nterpreted All animals are tomatoes as The set of\nanimals is identical to the set of tomatoes rather\nthan The set of animals is a subset of the set of\ntomatoes.\nWith GPT-3.5, the translations into predicate\nlogic are almost accurate, correctly translating\n82/90 problems. As a typical error, GPT-3.5 inter-\nprets A certain police officer is not a public servant\nas $\\neg \\exists x(Px \\land Sx)$ instead of $\\exists x(Px \\land \\neg Sx)$, failing\nto capture the correct scope of negation. The num-\nber of problems translated correctly to set theory\nis 79/90. GPT-3.5 mistakenly interprets A certain\npolice officer is not a public servant as The set of\npolice officers does not intersect with the set of\npublic servants, which is logically equivalent to the\nwrong predicate logic formula, $\\neg \\exists x(PxSx)$.\nJapanese With GPT-4, 86/90 problems are cor-\nrectly translated from Japanese to predicate logic.\nTranslations into set theory are more error-prone\nthan those into predicate logic, with only 69/90\nproblems correctly translated. GPT-3.5 has trouble\nin translation into predicate logic, with 64/90 prob-\nlems correctly translated. Similar to the mistakes\noften made by beginners in logic, the model incor-\nrectly interprets a Japanese sentence corresponding\nto Some A are B as $\\exists x(A(x) \\rightarrow B(x))$, while the\ncorrect interpretation is $\\exists x(A(x) \\land B(x))$.\nWith GPT-3.5, 28/90 problems are correctly\ntranslated from Japanese to set theory. The trans-\nlation of A-type (all-type) sentences in Japanese\nis generally accurate, but the translations of the"}, {"title": "6 Related Work", "content": "Recent years have seen active research on machine\nlearning and deep learning models that focus on\nlogical reasoning in natural language, including\nsyllogistic reasoning.\nRichardson et al. (2020) investigated the abil-\nities of NLI models in handling various logical\ninferences involving boolean operators, quantifiers,\nconditionals, and negation using synthetically gen-\nerated data. Yanaka et al. (2019) studied mono-\ntonicity inferences. Monotonicity inferences are\nsimpler than syllogistic inferences in that they only\nhave single premises, whereas syllogisms involve\nmultiple premises with challenging combinations\nof quantifiers and negation. Schlegel et al. (2022)\nconducted an empirical study to explore the de-\ntection of formally valid inferences within con-\ntrolled fragments of natural language, designed to\nincrease satisfiability problem complexity. These\nstudies combine pre-training and fine-tuning with\nrelatively large datasets for logical reasoning and\nare not aimed at evaluating current LLMs based on\nin-context learning.\nAs datasets for learning and evaluating syllogis-\ntic reasoning, Dong et al. (2020) and Gubelmann\net al. (2022) have constructed datasets using linguis-\ntic resources such as WordNet and their own lists of\nwords, employing template-based automatic gener-\nation methods based on types of syllogisms. The\ndataset of Gubelmann et al. (2022) includes three-\nclass labels (entailment, contradiction, and neutral)\nbut does not include additional information related\nto reasoning biases. AVICENNA (Aghahadi and\nTalebpour, 2022) is a crowdsourced dataset con-\ntaining binary labels that indicate whether the con-\nclusion of a syllogism follows from two given sen-\ntences (and if so, the conclusion sentence is also\nprovided), but it does not contain information about\nreasoning biases. SYLLOBASE (Wu et al., 2023)\ncontains five types of syllogisms. It consists 50,000\nsyllogism samples automatically generated from\nexisting knowledge bases, with 1,000 of these sam-\nples manually annotated as a test set. Experiments\nwere conducted in zero-shot and few-shot settings,\ncovering both generation and selection tasks. All\nof the datasets above are in English.\nAs a framework for generating deduction\ndatasets, FLD (Morishita et al., 2023) and its\nJapanese version, JFLD (Morishita et al., 2024),\nhave been proposed. Morishita et al. (2023) em-\npirically verifies that language models trained with\nFLD demonstrate enhanced generalizable deduc-\ntive reasoning abilities.\nDasgupta et al. (2022) and Eisape et al. (2023)\nare the works closely related to ours. Dasgupta\net al. (2022) focused on belief biases in syllogistic\nreasoning and showed that the alignment of conclu-\nsions with human beliefs affects the performance\nof LLMs. In their studies, scenarios were classified"}, {"title": "7 Summary and Conclusion", "content": "In this paper, we evaluated the logical reasoning\nability of LLMs by developing the NeuBAROCO\ndataset, which consists of syllogisms in both\nJapanese and English, annotated with information\non reasoning biases and results from large-scale\nhuman evaluation data. The results of experiments\nusing the NLI task demonstrated that some LLMs,\nparticularly GPT-4, achieved high accuracy in both\nEnglish and Japanese, especially in the few-shot\nsetting. However, significant room for improve-\nment remains in problems where the entailment\nrelation is neutral (neither entailment nor contra-\ndiction). The results suggest that, particularly in\nproblems labeled as Conversion and Atmosphere,\nthere is a tendency to exhibit the same reasoning\nbiases as humans.\nMoreover, in the experiments using the Multiple-\nChoice task, a method commonly used in psy-\nchological studies of syllogisms, some models\nachieved accuracies surpassing those of human\nparticipants in large-scale experiments. It is im-\nportant to note that the methodology for comparing\nhuman accuracy with LLMs is not yet fully es-\ntablished, and a detailed comparison with human\nperformance remains a future challenge.\nFinally, in the Translate-and-Explain task, which\nrequires providing explanations of reasoning along\nwith translations of syllogisms into logical expres-\nsions, many models showed improved accuracy,\nwith translation accuracy nearly reaching 100%.\nHowever, reasoning errors remain, suggesting that\nthe source of these errors is not the misinterpreta-\ntion of the premise sentences but rather the reason-\ning process itself.\nWhile syllogisms represent one of the basic\nforms of logical reasoning used in psychology, ex-\npanding our evaluation to include more diverse\nand complex natural language inferences that in-\nduce reasoning biases remains an essential chal-\nlenge. This includes boolean propositional infer-\nences (Evans, 1989), if-then conditionals (Johnson-\nLaird and Byrne, 2002), and spatial inferences\ninvolving polyadic relations (Byrne and Johnson-\nLaird, 1989). The biases we addressed are not spe-\ncific to syllogistic reasoning and, therefore, may\npotentially be generalized to tasks beyond syllo-\ngistic reasoning. Addressing these issues in future\nresearch is crucial for advancing our understanding\nof the reasoning capabilities of LLMs."}, {"title": "Limitations", "content": "In this study, we used closed-source LLMs, partic-\nularly noting that the GPT models we used lack\nprecise information about the models, such as the\nnumber of parameters, and scale and distribution\nof the training data. Although the NeuBAROCO\ndataset has not been publicly released so far, it\ncannot be denied that some parts of syllogistic rea-\nsoning, especially symbolic syllogisms themselves,\nmay be included in the training data. Also, there is\na risk associated with using closed-source models\nin scientific research due to the lack of reproducibil-\nity.\nComparing the accuracy of LLMs to human ac-"}]}