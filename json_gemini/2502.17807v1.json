{"title": "DocPuzzle: A Process-Aware Benchmark for Evaluating Realistic Long-Context Reasoning Capabilities", "authors": ["Tianyi Zhuang", "Chuqiao Kuang", "Xiaoguang Li", "Yihua Teng", "Jihao Wu", "Yasheng Wang", "Lifeng Shang"], "abstract": "We present DocPuzzle, a rigorously constructed benchmark for evaluating long-context reasoning capabilities in large language models (LLMs). This benchmark comprises 100 expert-level QA problems requiring multi-step reasoning over long real-world documents. To ensure the task quality and complexity, we implement a human-AI collaborative annotation-validation pipeline. DocPuzzle introduces an innovative evaluation framework that mitigates guessing bias through checklist-guided process analysis, establishing new standards for assessing reasoning capacities in LLMs. Our evaluation results show that: 1) Advanced slow-thinking reasoning models like o1-preview (69.7%) and DeepSeek-R1 (66.3%) significantly outperform best general instruct models like Claude 3.5 Sonnet (57.7%); 2) Distilled reasoning models like DeepSeek-R1-Distill-Qwen-32B (41.3%) falls far behind the teacher model, suggesting challenges to maintain the generalization of reasoning capabilities relying solely on distillation.", "sections": [{"title": "1 Introduction", "content": "The rapid evolution of large language models (LLMs) (OpenAI, 2024b; Anthropic, 2024; Guo et al., 2025) has demonstrated unprecedented capabilities in long-context processing and complex reasoning. These advancements extend the boundaries of machine intelligence and reignite discussions about achieving Artificial General Intelligence (AGI). However, current technical reports of these LLMs predominantly focus on structured mathematical problem-solving (Hendrycks et al., 2021; Lightman et al., 2023; MAA, 2024) and coding tasks (Jain et al., 2024b), creating a significant disconnect between benchmark performance and real-world reasoning requirements. A critical gap in assessing models' capacity for extended logical chaining with implicit connections remains under-studied."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Context-free Reasoning Benchmarks", "content": "Recently, with the release of OpenAI's o1 (OpenAI, 2024d), there has been growing attention on slow thinking and long-form Chain of Thought (CoT) as a means of improving reasoning capabilities (Guo et al., 2025; Team, 2024; Qin et al., 2024). These efforts primarily focus on domains that demand rigorous logical reasoning: (1) Competitive math problems: AIME\u00b9, CNMO2 and MATH(Hendrycks et al., 2021); (2) Coding challenges: SWE-Bench (OpenAI, 2024c), LiveCodeBench (Jain et al., 2024b) and Codeforces\u00b3; (3) Scientific academic problems: GPQA (Rein et al., 2023) and Humanity's Last Exam (Phan et al., 2025).\nWhile these tasks address reasoning difficulties at the forefront of human intelligence, we do not regard these domain-specific reasoning benchmarks as adequate measures of the generalization ability of LLMs. With the intuition that diverse domains can naturally emerge from a wide range of documents, we pay attention to long-context documents to present problems that span an extensive range of fields."}, {"title": "2.2 Long-context Reasoning Benchmarks", "content": ""}, {"title": "Elementary Long-context Benchmarks", "content": "In the early stages of the development of long-context LLMs, synthetic tasks (Kamradt, 2024; Hsieh et al., 2024) dominated the evaluation landscape as a basic measure of long-context understanding. Recognizing the potential divergence between the results of synthetic tasks and real-world applications, LongBench (Bai et al., 2023) and InfiniteBench (Zhang et al., 2024) incorporate realistic tasks, such as document-based question answering and summarization. To achieve better discrimination, HELMET (Yen et al., 2024) aggregates contemporary long-context benchmarks. The aforementioned studies focus primarily on evaluating the fundamental capacity for long-context comprehension. Reasoning questions are scarcely incorporated, and when present, they tend to be rudimentary, requiring only one or two steps.\nAlthough several studies explicitly emphasize evaluating long-context reasoning, their ability to discriminate is often limited. BABILong (Kuratov et al., 2024) assesses reasoning abilities through synthetically generated extended contexts. DocBench (Zou et al., 2024) and Loong (Wang et al., 2024) incorporate reasoning tasks based on single or multiple documents. These benchmarks face two key limitations: (1) compromised validity due to semantically inconsistent synthetic contexts, or (2) excessively simplified questions that fail to pose genuine challenges to LLMs proficient in slow and deliberate reasoning."}, {"title": "Difficult Long-context Reasoning Benchmarks", "content": "Recent works have introduced more challenging tasks that increase the complexity of evidence retrieval or require longer chains of logical reasoning. Nocha (Karpinska et al., 2024), RuleArena (Zhou et al., 2024), and DetectiveQA (Xu et al., 2024) include difficult reasoning tasks that focus on specific domains such as fantasy novels, policies, and orthodox detective stories, respectively. LongBench v2 (Bai et al., 2024) introduces difficult human-curated long-context reasoning tasks across various domains. However, their discriminative validity is undermined by three key factors: (1) limited domain coverage (Karpinska et al., 2024; Zhou et al., 2024; Xu et al., 2024); (2) reduced reasoning difficulty due to the exclusion of deliberately designed challenging puzzles and the reliance on domain-specific expertise (Bai et al., 2024); (3) the use of a unified multiple-choice question format, which allows the model to guess the correct answer randomly (Bai et al., 2024). We compare our DocPuzzle benchmark and others in Table 2."}, {"title": "3 DocPuzzle", "content": "DocPuzzle is a benchmark crafted to evaluate the ability of LLMs to handle highly intricate reasoning tasks based on long contexts. Figure 1 illustrates the carefully designed data annotation process, which emphasizes ensuring the difficulty of the puzzles and incorporates a meticulous verification of answer controversy. Section 3.1 introduces three stages in detail."}, {"title": "3.1 Data Construction Pipeline", "content": "Data Collection We curate contexts from five domains: literature, news articles, policy documents, financial reports, and scientific papers. The domain distribution of collected contexts is visualized in Figure 2.\nWe recruit domain experts with at least a master's degree as annotators. To ensure sufficient reasoning depth, these annotators upload reasoning-intensive documents that they are highly familiar with. For policy documents and news reports requiring extended context, annotators perform article aggregation by retrieving related documents and concatenating them into coherent narratives. Contexts undergo semantic integrity verification through manual review, with necessary modifications to ensure answer determinism. The final dataset contains contexts with median and average token counts of 10,641 and 10,215 in respective 4.\nData Annotation Annotators follow rigorous guidelines that adhere to the following principles. 1)Context-Dependent Reasoning: In all cases, the"}, {"title": "3.2 Evaluation Method", "content": "Existing reasoning benchmarks predominantly focus on structured domains like mathematics and coding (Lightman et al., 2023; Jain et al., 2024a) or utilize constrained formats (Bai et al., 2024), limiting their applicability to free-form textual reasoning. As we focus on the capacity of reasoning, we allow imperfect textual organizing such as language mixing and redundant content. Under this circumstance, employing a parser for the response may introduce bias. On the other hand, even an erroneous reasoning chain may lead to a correct answer. Thus, a model good at guessing will get an inflated score, especially for multiple-choice and true-or-false questions. Our evaluation framework addresses these limitations. To mitigate these biases, we take not only the final answer but also the reasoning chain into account by utilizing a checklist for each sample. We allow a calculation error within an acceptable range in the checklist. This accommodates acceptable calculation deviations while penalizing fundamental logical errors. We prompt a judge model to judge whether the response is correct. To mitigate randomness, we design three similar evaluation prompts, and the evaluation result is decided by majority voting. The prompt templates we use in the evaluation are shown in Appendix A."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Baselines", "content": "We conduct comprehensive evaluations across two model categories:\nSlow-Thinking Reasoning Models We evaluate the following four slow-thinking reasoning models: o1-preview (OpenAI, 2024b), DeepSeek-R1(Guo et al., 2025), DeepSeek-R1-Distill-Qwen-32B(Guo et al., 2025), QwQ-32B-Preview(Team, 2024). For DeepSeek-R1, the response consists of reasoning content and the response content, so we take both into accounts. For DeepSeek-R1-Distill-Qwen-32B, we set temperature to 0.3, top_p to 0.2, and top_k to 0.\nInstruct Models We evaluate the following instruct models: GPT-40(OpenAI, 2024a), Claude 3.5 Sonnet (Anthropic, 2024), DeepSeek-V3(Liu et al., 2024), Qwen2.5-72B-Instruct(Yang et al., 2024) and moonshot-v1-128k7. We do not evaluate the performance of the LLaMA series, as questions in our benchmark are mostly Chinese. For instruct models, we also evaluate the performance when employing the powerful zero-shot COT prompt, \"Let's think step-by-step\" (Kojima et al., 2023).\nSetups We use GPT-406 as the judge model. As demonstrated in Section 3.2, we utilize majority voting across three prompt variants. Furthermore,"}, {"title": "4.2 Main Results", "content": "Due to factors like risk control, not all requests may get valid responses. We present the main results in Table 3.\nReasoning Capability Ranking Slow-thinking reasoning models like o1-preview and DeepSeek-R1 significantly outperform other models on the benchmark. DocPuzzle shows consistent rankings for the advancing models with other reasoning benchmarks, demonstrating the reasoning nature of DocPuzzle. While most instruction models under-perform specialized slow-thinking reasoning models, Claude 3.5 Sonnet and DeepSeek-V3 demonstrate exceptional reasoning capability, surpassing the Qwen-based models.\nScaling Law Manifestation Model capability exhibits a strong correlation with reasoning ability and domain knowledge. Qwen2.5 series shows consistent accuracy gains from 7B (20.3%) to 72B (39.7%) variants.\nContrastive Prompt Effects While zero-shot CoT prompting (Kojima et al., 2023) generally improves performance, effectiveness diminishes for models with limited reasoning performance. CoT prompting yields maximum gains for Claude 3.5 Sonnet and Qwen2.5-72B-Instruct. Performance degradation is observed in Qwen2.5-7B-Instruct and moonshot-v1. CoT effectiveness emerges when instruct models achieve a score over 32.7%, indicating minimum model capacity requirements for reasoning path utilization."}, {"title": "5 Analysis", "content": ""}, {"title": "5.1 Response Length", "content": "We conduct an analysis of response length and its correlation with accuracy metrics as illustrated in Figure 3. The o1-preview model is excluded from this analysis due to the unavailability of its reasoning chain. A significant disparity emerges between the average lengths of reasoning models and instruct models. Notably, while distilled models exhibit substantially increased response lengths that do incorporate reasoning chains, a persistent gap remains between distilled models and their teacher counterparts in terms of length-accuracy optimization."}, {"title": "5.2 Is the reasoning ability of distilled models generalizable?", "content": "Though DeepSeek-R1-Distill-Qwen-32B achieves comparable scores on math and code datasets(Guo et al., 2025), its efficacy substantially degrades on the DocPuzzle benchmark(see Figure 4). Furthermore, as evidenced in Table 3, the gap between DeepSeek-R1-Distill-Qwen-32B and Qwen2.5-32B-Instruct with a zero-shot COT prompt remains statistically insignificant. These findings suggest that reasoning capabilities acquired through supervised finetuning on distilled datasets produced by heterogeneous models exhibit limited generalization to complex, long-context reasoning scenarios. This conclusion aligns with Chu et al. (2025), who demonstrate that models trained by supervised finetuning tend to memorize surface patterns in training data while struggling with out-of-distribution generalization. The empirical evidence challenges the universal applicability of knowledge distillation techniques and calls for other techniques, such as reinforcement learning and self-improvement, to improve reasoning generalization."}, {"title": "5.3 The Potential of Exploration", "content": "Given the growing prominence of reinforcement learning (RL) in language model training, we quantify exploration capacity through the pass@3 score. Figure 5 illustrates that DeepSeek-R1 achieves superior exploration capability. Our analysis reveals no significant correlation between LLM exploration potential (the difference between pass@3 and baseline scores) and pairwise accuracy scores. We extend this investigation to DeepSeek-R1-Distill-Qwen-32B and Qwen2.5-32B-Instruct, derived from the Qwen2.5-32B base model. Under controlled experimental conditions (temperature = 0.3, top_p = 0.2, top_k = 0), both models exhibit constrained exploration capacities as shown in Figure 5."}, {"title": "5.4 Typical Error Analysis", "content": "To investigate the limitations of the reasoning capacity of state-of-the-art LLMs, we conduct an error analysis for o1-preview and DeepSeek-R1 in Table 4.\nArithmetic Reasoning Deficits The answer can be inferred by comparing two tables of futures prices and spot prices: (1) Spot Price = Basis + Futures Price; (2) As the equation does not hold where Zhangjiagang spot price = 6650, basis = 406, futures price = 6344, and Hubei rapeseed oil spot price = 9350, basis = 661, futures price = 8449. (3) According to the question, there is an error in one of the soy-bean oil prices in the spot table, and there is an error in one of the rapeseed oil prices in the basis table. Hence, the error in the spot table should be corrected as Zhangjiagang spot price =406 + 6344 = 6750. The error in the basis table should be corrected as Hubei rapeseed oil basis = 93508449 = 901. Both models fail to extract the price relationship formula from tabular data, instead generating plausible but incorrect responses through heuristic estimation.\nCommon Sense Reasoning Limitations It is common knowledge that the gross floor area is larger than the usable area. However, o1-preview and DeepSeek-R1 fail to recognize this difference, leading to incorrect judgments. The lack of certain common sense knowledge prevents LLMs from engaging in further implicit reasoning."}, {"title": "6 Conclusions", "content": "We introduce DocPuzzle, a challenging yet realistic long-context reasoning benchmark. By a carefully designed construction pipeline, DocPuzzle ensures both reliability and difficulty and shows consistent rankings for advanced LLM models. Instead of only considering the final answer, a process-aware evaluation method is also proposed to better evaluate the reasoning process of LLMs. Our study through DocPuzzle reveals the strong long-context reasoning ability of slow-thinking models. Another insight is that Knowledge distillation may be insufficient for transferring complex reasoning patterns."}, {"title": "Ethics Statement", "content": "DocPuzzle seeks to establish an instructive benchmark for advancing research in complex reasoning with long context. Certain data incorporated within DocPuzzle are sourced from open-source datasets and publicly available data on the Internet. The content presented does NOT reflect the viewpoints of the authors. As the definition of commonsense knowledge may vary among individuals, the current evaluation criteria are derived from the consensus of the annotators."}, {"title": "A Evaluation Prompt", "content": "Evaluation Prompt 1 You are an experienced Q&A expert tasked with evaluating the quality of a response based on four components: the user's question, the standard answer, the answer checklist, and the provided response.\nUser Question: {query}\nStandard Answer: {answer}\nAnswer Checklist: {checklist}\nResponse: {model_response}\nEvaluation Criteria:\nAccuracy Check: Compare the response with the standard answer. If the meaning aligns with the standard answer, mark it as correct. If inconsistent, mark it as incorrect. Note: For numerical answers, responses within the allowable margin of error specified in the checklist are considered correct.\nChecklist Compliance: Assess whether the response meets the mandatory requirements and flexible criteria outlined in the answer checklist. Compliance results in \"correct\"; non-compliance results in \"incorrect\".\nFinal Judgment: The response is deemed correct only if both criteria 1 and 2 are satisfied. Otherwise, it is marked as incorrect.\nOutput Requirements: Step 1: Analysis Begin with \"Step 1: Analysis:\" and conduct thorough, logical reasoning until you reach a conclusive evaluation. Stop once your reasoning is sufficient to determine the final judgment.\nStep 2: Final Judgment Begin with \"Step 2: Final Judgment:\" and output the result strictly in the following dictionary format. Do not include additional text like \"json\" or unrelated content. Format: {{\"Response Correctness\": \"Correct\"/\"Incorrect\"}}\nEvaluation Prompt 2 As an experienced Q&A expert, you need to evaluate the accuracy of responses based on the given user question, standard answer, answer checklist, and model response, while considering two core dimensions.\nUser Question: {query}\nStandard Answer: {answer}\nAnswer Checklist: {checklist}\nResponse: {model_response}\nEvaluation Criteria:\nAccuracy Alignment: Assess whether the response matches the standard answer in meaning. If consistent, mark as correct; otherwise, mark as incorrect. Note: For numerical answers, responses within the allowable error range specified in the checklist are deemed correct.\nChecklist Adherence: Evaluate whether the response fulfills all mandatory and flexible requirements outlined in the answer checklist. Compliance results in \"correct\"; non-compliance results in \"incorrect\".\nFinal Verdict: The response is considered correct only if both criteria 1 and 2 are satisfied. Otherwise, it is marked as incorrect.\nOutput Requirements: Step 1: Analysis Begin with \"Step 1: Analysis:\" and perform detailed, rigorous reasoning and comparisons. Continue until your logical analysis is sufficient to reach a definitive conclusion.\nStep 2: Evaluation Result Begin with \"Step 2:"}, {"title": "", "content": "Evaluation Result:\" and strictly output the final assessment in the specified dictionary format. Do not include terms like \"json\" or unrelated content. Final output format: {{\\\u201dResponse Correctness\": \\\u201dCorrect\\\u201d or \\\u201dIncorrect\u201d}}\nEvaluation Prompt 3 As a seasoned expert in Q&A systems, you will comprehensively evaluate responses to questions based on the provided user query, standard answer, answer checklist, model response, and the following two evaluation dimensions.\nUser Question: {query}\nStandard Answer: {answer}\nAnswer Checklist: {checklist}\nResponse: {model_response}\nEvaluation Criteria:\nContent Accuracy: Compare the response with the standard answer. If the response accurately reflects the content and intent of the standard answer, mark it as correct. Otherwise, mark it as incorrect. Note: For numerical calculations, minor deviations in results are deemed acceptable and marked as correct.\nChecklist Compliance: Verify whether the response satisfies all requirements specified in the answer checklist. Full compliance results in \"correct\"; any violation leads to \"incorrect\".\nFinal Judgment: The response is considered correct only if both criteria 1 and 2 are satisfied. If either fails, the final result is \"incorrect\".\nOutput Requirements: Step 1: Reasoning Begin with \"Step 1: Reasoning:\" and conduct meticulous, logical analysis and comparisons. Continue until your reasoning process is thorough and conclusive enough to determine the final judgment.\nStep 2: Evaluation Outcome Begin with \"Step 2: Evaluation Outcome:\" and output the result strictly in the specified dictionary format. Avoid including terms like \"json\" or unrelated content. Final output format: {{\\\u201dResponse Correctness\": \\\u201dCorrect\\\u201d or \\\u201dIncorrect\u201d}}"}]}