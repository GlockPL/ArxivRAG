{"title": "Towards Scalable and Deep Graph Neural Networks via Noise Masking", "authors": ["Yuxuan Liang", "Wentao Zhang", "Zeang Sheng", "Ling Yang", "Quanqing Xu", "Jiawei Jiang", "Yunhai Tong", "Bin Cui"], "abstract": "In recent years, Graph Neural Networks (GNNs) have achieved remarkable success in many graph mining tasks. However, scaling them to large graphs is challenging due to the high computational and storage costs of repeated feature propagation and non-linear transformation during training. One commonly employed approach to address this challenge is model-simplification, which only executes the Propagation (P) once in the pre-processing, and Combine (C) these receptive fields in different ways and then feed them into a simple model for better performance. Despite their high predictive performance and scalability, these methods still face two limitations. First, existing approaches mainly focus on exploring different C methods from the model perspective, neglecting the crucial problem of performance degradation with increasing P depth from the data-centric perspective, known as the over-smoothing problem. Second, pre-processing overhead takes up most of the end-to-end processing time, especially for large-scale graphs. To address these limitations, we present random walk with noise masking (RMask), a plug-and-play module compatible with the existing model-simplification works. This module enables the exploration of deeper GNNs while preserving their scalability. Unlike the previous model-simplification works, we focus on continuous P and found that the noise existing inside each P is the cause of the over-smoothing issue, and use the efficient masking mechanism to eliminate them. Experimental results on six real-world datasets demonstrate that model-simplification works equipped with RMask yield superior performance compared to their original version and can make a good trade-off between accuracy and efficiency.", "sections": [{"title": "Introduction", "content": "Graph Neural Networks (GNNs) have achieved great success in graph representation learning. In recent years, GNNS have been widely used in many graph-based applications, such as databases (Piao et al. 2023; Helali et al. 2022; Cui et al. 2021), data management (Li et al. 2023; Chen et al. 2023; Zhong et al. 2023), and data mining (He, Chen, and Chen 2024; Wu, Sun, and Yang 2024; Hao, Liu, and Bai 2024; Zhang, Ni, and Fu 2023). Despite the success of GNNs, scaling them to large graphs is challenging due to the high computational and storage costs of repeated feature propagation during training. Consequently, these limitations impede the scalability of GNNs on large graphs, limiting their applicability and advancement in real-world scenarios.\nTo address the scalability issues, model-simplification GNNs, as a promising direction for scalable performance has aroused great interest recently. The most representative work is SGC (Wu et al. 2019), which puts feature propagation in the pre-processing step by removing nonlinearities and collapsing weight matrices between consecutive layers. Following the design principle of SGC, piles of works have been proposed to improve the performance of SGC further while maintaining high scalability, such as SIGN, S2GC, GBP, GAMLP (Rossi et al. 2020; Zhu and Koniusz 2021; Chen et al. 2020; Zhang et al. 2021). Generally, these model-simplification GNNs can be disentangled into two independent operations: Propagation (P) and Combination (C). The P operation can be viewed as aggregating the information of first-order neighbors for each node. The C operation can be viewed as a combination of consecutive P operations. Continuous P operations are used to capture deeper receptive fields and C operations are used to combine these receptive fields for better performance. Existing model-simplification GNNs mainly focus on designing different C. Despite their high scalability and predictive performance, existing model-simplification GNNs still face the following two limitations:\nPropagation with Noise Information. Despite the success of model-simplification GNNs, the exploration of deep propagation steps remains limited because simply stacking P leads node representations to become indistinguishable and results in performance degradation, i.e., over-smoothing problem. Existing model-simplification GNNs, as shown in Figure 1(a) (left), pay more attention to designing different combination methods of propagation steps without considering the noise problem within each hop. From Figure 1(a) (left), we can see that each hop contains overlapping graph structure information of the previous hop, such noise information will be propagated along the edges and hinder the extraction of truly useful high-hop information. As shown in Figure 1(a) (right), we conducted an experiment using four model-simplification GNNs on the ogbn-arxiv dataset. Experimental results show that the performance continues to decline as the propagation depth increases.\nPropagation with High Pre-processing Overhead. Even though existing model-simplification GNNs are significantly faster than traditional GNNs (Kipf and Welling 2017; Velickovic et al. 2018) by putting the expensive feature propagation step into the pre-processing step and performing it only once, pre-processing overhead still accounts for a large proportion of the entire training time. As shown in Figure 1(b), we performed end-to-end training time cost breakdown using the SGC model on Cora, Citeseer, Pubmed, ogbn-arxiv, ogbn-products and ogbn-papers100M datasets (Kipf and Welling 2017; Hu et al. 2020). We can see that the pre-processing time occupies the vast majority of the overall training time. Taking ogbn-papers100M as an example, the end-to-end training time takes 4.4 hours, and pre-processing overhead accounts for 96%.\nIn this paper, we introduce random walk with noise masking (RMask), a plug-and-play module that seamlessly integrates with existing model-simplification GNNs. By tracking the information of each hop in the pre-processing step, we found that the continuous P operations generate a significant amount of noise information, hindering the ability towards deeper GNNs. Based on this observation, our key insight is to employ a noise masking mechanism to extract valuable high-hop information and mitigate the over-smoothing issue. Furthermore, to tackle the issue of high pre-processing cost in model-simplification GNNs, we utilize the de-noise-based random walk method to extract pure graph structure information. This approach enables us to strike a balance between accuracy and efficiency effectively.\nThis paper does not intend to diminish the contribution of current methods for eliminating over-smoothing, like DropEdge (Rong et al. 2020), GPR (Chien et al. 2021), DAGNN (Liu, Gao, and Ji 2020). Instead, we aim to provide new insights into the over-smoothing problem from the P operation itself in scalable GNNs, which is orthogonal to other methods to eliminate over-smoothing.\nContirbutions. The main contributions of this work are as follows: New findings. In contrast to previous model-simplification GNNs that primarily focused on studying the combination methods (C) between different hops, our research delves into exploring the noise information within each hop. We discover that the P operations introduce a significant amount of noise, exacerbating the over-smoothing problem. New method. We introduce RMask, a plug-and-play module compatible with existing model-simplification GNNs. To address the noise issue caused by P operations, we propose a noise masking mechanism that extracts pure information within each hop. Additionally, we leverage random walks to strike a balance between accuracy and efficiency. State-of-the-art performance. We evaluate the effectiveness of RMask on three widely used datasets (Cora, Citeseer, Pubmed) (Kipf and Welling 2017) and three large-scale datasets (ogbn-arxiv, ogbn-products, ogbn-papers100M) (Hu et al. 2020). Experimental results show that model-simplification GNN equipped with RMask has superior performance compared to itself."}, {"title": "Preliminaries", "content": "Notations and Problem Formulation. We consider an undirected graph G = (V, E) comprising a set of nodes V =\nV1,..., Vn and a set of edges E = {(Vi, Vj)|Vi, Vj \u2208 V}.\nThe cardinality of the node and edge sets is |V| = N and\n|E| = M, respectively. A \u2208 RN\u00d7N represents the adjacency matrix of G. Besides, we define the degree matrix\nof A as D, which is a diagonal matrix with entries cor-\nresponding to the degrees of the nodes. Specifically, D =\ndiag(d1,\u2026,dn) \u2208 RN\u00d7N, where d\u2081 = \u2211jev Aij rep-\nresents the degree of node i. Each node has a feature vector\nxi \u2208 Rd, resulting in an N \u00d7 d matrix X when stacked together.\nHere, Xi refers to the feature vector of node i.\nSmoothness Level. To measure the over-smoothing lev-\nels, we introduce the concept of Smoothness Level in\nDAGNN (Liu, Gao, and Ji 2020) and employ cosine sim-\nilarity to measure the similarity between two nodes. We\nformally define the \"Node Smoothness Level (NSL)\" and\n\"Graph Smoothness Level (GSL)\" as follows: $NSL_i = \\frac{1}{N-1} \\sum_{j \\in V, j \\neq i} \\frac{x_i^T x_j}{\\|x_i\\| \\|x_j\\|}$ , GSL = \\frac{\\sum_{(i,j)\\in E} NSL_i}{\\|E\\|}$. NSLi\ncomputes the average similarity between node i and all other\nnodes in the graph, while GSL calculates the average simi-\nlarity between pairs of nodes in the graph. A higher smooth-\nness level indicates a higher probability of similarity be-\ntween two randomly selected nodes from a given set. In this\npaper, we use GSL to measure the smoothness level."}, {"title": "Related Work", "content": "Sampling Graph Neural Networks. Over the past few years, the graph convolution operation introduced by GCN has increasingly become the standard form in most GNN architectures (Kipf and Welling 2017). GCN adopts a layer-wise propagation rule and a multi-layer non-linear feature transformation network to form the new representation. However, this approach has expensive message propagation overhead during training, resulting in computationally expensive and low scalability. A commonly used method to tackle the scalability issue is sampling, which focuses on a smaller portion of the graph while still preserving its structural properties. Existing methods typically employ sampling techniques at various levels: node-level samplings, such as GraphSAGE (Hamilton, Ying, and Leskovec 2017) and VR-GCN (Chen, Zhu, and Song 2018); layer-level samplings, such as Fast-GCN (Chen, Ma, and Xiao 2018) and ASGCN (Huang et al. 2018); and graph-level samplings, such as ClusterGCN (Chiang et al. 2019a) and Graph-SAINT (Zeng et al. 2020).\nModel-simplification Graph Neural Networks. The other direction is to build model-simplification GNNs. The main idea is to decouple the feature propagation and non-linear transformation in the GNN layer and finish the time-consuming feature propagation process without model parameter training. SGC (Wu et al. 2019) removes nonlinearities between consecutive graph convolutional layers, leading to higher scalability and efficiency. Specifically, SGC performs P operations as follows:\nX(k) = (\u010e\u2212\u00b9\u00c2\u010e\u2212\u00bb)*X(0)  (1)\nWhere X(0) represents the raw feature, and \u00c2 = \u010e\u00ab\u2212\u00b9\u00c3\u010e\u2212", "\u010e\u00bb\u2212\u00b9\u00c3\u010e\u00af": "\u00c2\u221e follows \u0100 =\n$\\frac{(d_i+1)*(d_j+1)}{2M+N}$ X\u221e = \u00c2\u221eX\u00b0 which shows that as the depth of P approaches \u221e, the influence from node j to node i is only determined by their node degrees.\nPrevious research usually addresses the over-smoothing problem from three aspects: manipulating graph topology (Rong et al. 2020), refining model structure (Chien et al. 2021), and dynamic learning (Liu, Gao, and Ji 2020). However, there is no method to consider over-smoothing from the perspective of noise information during feature propagation."}, {"title": "Motivatoin", "content": "In this section, we make a deep analysis of the two limitations that exist in model-simplification GNNs and then provide our insight to help us design the architecture of RMask.\nStudy on Noise Information. Model-simplification GNNs implement P operations by continuously performing matrix product operations on the initial normalized matrix \u00c2 using Eq. (1) to increase the propagation depth (i.e., the number of hops). However, this approach will weaken the importance of high-hop information. We randomly select 10 nodes on the Cora dataset and observe the average weight of each hop through P operations with L2 normalization. As shown in Figure 2(a), nodes with higher weights are frequently captured within lower hops, while nodes holding valuable information in higher hops exhibit considerably lower weights. This phenomenon hinders the capture of higher-hop information. To explain further, we conduct a 2-hop propagation starting from the target node. As shown in Figure 2(c), the information captured by 2-hop encompasses not only the current hop but also 2-hop redundant information, since this information can already be captured within 1-hop, we refer to it as Noise Information.\nAs the propagation depth increases, the nodes captured by high hop contain a large amount of low hop noise information, making it difficult to distinguish between high hop and low hop information, exacerbating the over-smoothing problem. To further examine the impact of noise information on over-smoothing, we increase the hop numbers and measure the proportion of noise information and GSL using the SIGN model. As shown in Figure 2(b), GSL grows explosively with the increase of hop number, and the noise information also grows continuously. The information captured after 7 hops is completely redundant.\nInsight 1: Noise information will hinder the utilization of high-hop information and aggravate over-smoothing. We re-implemented SIGN with noise masking. As shown in Figure 2(d), the node can not only capture the effective information of higher hop but also eliminate the over-smoothing problem as shown in Figure 2((e). As the number of hops increases, the accuracy and smoothness level tends to be flat.\nStudy on High Pre-processing Overhead. Moreover, this propagation method results in significant pre-processing overhead. The upper part of Figure 2(f) illustrates the unified pre-processing process employed by current model-simplification GNNs. First, the time complexity of preprocessing is linearly related to the number of edges (Chen et al. 2020). Each hop captures a significant amount of graph structural information from all previous hops, which leads to computational intensity due to the dense matrix involved. Second, this approach relies on the interdependence of information across different hops, which can only be obtained serially. Compared with expensive pre-processing overhead, model-simplification CNNs usually use simple models for fast training. For the above reasons, the pre-processing overhead constitutes the majority of the end-to-end training time, as demonstrated in Figure 1(b).\nInsight 2: As shown in the lower part of Figure 2 (f), to reduce the high overhead of pre-processing, we need a sparse and parallel method to efficiently capture the important information of each hop."}, {"title": "Proposal of RMask", "content": "Motivated by the two insights, on one hand, we can eliminate noise information by identifying redundant information of each hop and using the masking mechanism. On the other hand, we can use random walks to capture truly useful information for different hops in a highly parallel manner. Additionally, the masking mechanism produces a sparse graph, further reducing the computational overhead of aggregation."}, {"title": "Method", "content": "In this section, we introduce RMask, a plug-and-play module designed for model-simplification GNNs as illustrated in Figure 3. The noise masking mechanism consists of two main components: noise information identification and neighbor nodes importance assignment. In this section, we will explain the pipeline of RMask and its methods in detail. In addition, we also analyze the time complexity of RMask.\nRMask Pipeline\nExisting model-simplification GNNs follow a common pipeline consisting of two main steps (upper part of Figure 3). They employ the same P operation and emphasize the combination (C) of information from different hops. In contrast, RMask utilizes a random walk noise masking mechanism (W operations) to replace the P operations, aiming to address the over-smoothing issue and extract the truly useful information from high-hop (lower part of Figure 3). Given a specified number of hops and a graph structure, we first perform random walks with the masking mechanism for each node based on the graph structure. Then the captured graph structure information and features are aggregated to obtain results for different hops. Furthermore, the feature propagation results obtained in this manner can directly replace P operations in other model-simplification GNNs (such as S2GC, GBP, SIGN, GAMLP, etc.). Simultaneously, we retain the advantages of feature combination and model selection from existing model-simplification GNNs.\nNoise Masking Mechanism\nThe key insight of the noise masking mechanism is that if the information captured at a higher hop is already encompassed by the information captured at lower hops, then this noise information needs to be masked in the higher hop. Based on this insight, our noise masking mechanism consists of two components: noise information identification and neighbor nodes importance assignment. The first component identifies the noise information in each hop and uses random walks to efficiently capture non-redundant graph structure information. The second component assigns importance weights to each neighbor nodes to help random walks capture more important information.\nNoise Information Identification. Considering the influence of noise, high hops often contain redundant information from low hops. We need to traverse the entire graph to identify the noise information of each hop. Here, we use the de-noise matrix to record the noise information. Given the hop number h, the de-noise matrix of target node $v_i$ is defined as $M_i^h$:\n$M_i^h = \\bigcup_{j=1}^{N_h} m_{ij}, m_{ij} = \\begin{cases}\n1 & \\text{if } distance(v_i, v_j) = h \\\\\n0 & \\text{elif } distance(v_i, v_j) < h\n\\end{cases} $(2)\nwhere Nh is the number of neighbor nodes within a distance h from the target node $v_i$. The distance between $v_i$ and $v_j$ is the shortest path length. If $distance(v_i, v_j) = h$, mij is set to 1, otherwise it is identified as noise information and set to 0. The de-noise matrix of the entire graph can be expressed as $M^h = \\bigcup_{i \\in V} M_i^h$, where V is all the nodes in the entire graph. The de-noise matrix enables us to extract the pure information at each hop while ensuring low smoothness level. Afterward, for each hop, we use the random walk function (i.e., RW) to capture the graph structure information of the current hop, and the de-noise matrix is combined to extract useful information from each hop:\nWh = RW(G, Mh,T)  (3)\nwhere T is the number of random walks. By controlling the number of random walks, we achieve a favorable balance between accuracy and efficiency, making it well-suited for large-scale graphs.\nNeighbor Nodes Importance Assignment. Compared to existing model-simplification-based methods, the masking mechanism provides deeper and high-efficiency advantages, as it avoids the noise information by inter-hop parallel extraction of pure information from the graph structure. However, due to the uniform sampling-based random walk method, this approach assigns equal importance to all neighbor vertices, which may not be expressive enough to capture the most important nodes from the graph. To overcome this problem, we adopt a biased random walk based on neighbor node importance. Specifically, we use Personalized PageRank (Bojchevski et al. 2020) to get neighbor node importance. Because it can calculate the correlation of all neighbor nodes concerning the target nodes and can be efficiently computed using the approximation techniques to facilitate the scalability for large-scale graphs:\nS = \u03b1(I \u2013 (1 \u2212 \u03b1)\u00c2)-\u00b9.  (4)\nwhere I is the identity matrix, \u03b1 \u2208 (0, 1] is the random walk restart probability, \u00c2 is the normalized matrix of A. S is the importance score matrix for each node. We then use S in Eq. (3) to guide the random walks:\nWh = RW(G, Mh,T, S)  (5)\nby assigning the importance weight to each edge in the graph, the direction of the random walks can be guided so that it can capture more important de-noise information. Note that neighbor nodes importance assignment is optional"}, {"title": "Time Analysis", "content": "Table 1 compares the time complexity of RMask with several representative sampling GNNs and model-simplification GNNs. For model-simplification GNNs, in the pre-processing step, the time complexity of most linear models is $O(Lmf)$, GBP conducts this process approximately with a bound of $O(Lmf + L\\sqrt{mlgn})$, where \u03b5 is an error threshold. The time complexity of the serial version for RMask is $O(L(nR + rf) + m)$. By running RMask in parallel using c threads, the time complexity of the parallel version for RMask is O(L (nR+rf) + m). Compared with model-simplification GNNs, the time complexity of our method is significantly lower. As a plug-and-play module for model-simplification GNNs, RMask inherits the training model, ensuring consistent time and memory complexity during the training phase."}, {"title": "EXPERIMENTS", "content": "In this section, we execute comprehensive experiments to evaluate the proposed RMask for the node classification task on six widely-used graphs including Cora, Citeseer, Pubmed (Kipf and Welling 2017), ogbn-arxiv (arxiv), ogbn-products (products), and ogbn-papers100M (papers100M) (Hu et al. 2020). The details about all experiment settings and the network configurations are reported in the Appendix. We showcase the benefits of RMask through five distinct perspectives: (1) a comparison from end-to-end with state-of-the-art model-simplification methods, (2) analyzing the ability towards deeper architecture, (3) analyzing the trade-off between efficiency and accuracy, (4) analyzing efficiency, (5) ablation study."}, {"title": "End-to-end Comparison", "content": "In Table 2, we present the results of node classification prediction using SIGN, S2GC, GBP, and GAMLP on six datasets, both with and without our method. The experimental results demonstrate that when equipped with RMask, SIGN, S2GC, GBP, and GAMLP all achieve superior performance compared to their respective original versions across all six datasets. Notably, since each baseline method represents a different combination approach for propagation steps, our proposed RMask can seamlessly integrate with all model-simplification GNNs, delivering enhanced performance without sacrificing scalability."}, {"title": "Towards Deeper Architecture", "content": "Existing model-simplification GNNs often suffer from over-smoothing issues when the propagation depth is large, leading to indistinguishable node representations and poor predictive performance. In this subsection, we perform experiments on the ogbn-arxiv dataset to demonstrate that model-simplification GNNs, when equipped with RMask, can effectively handle large P operations. Figure 4 depicts the results as we increase the number of P operations from 1 to 30, with the green line indicating the original version and the red line representing the version equipped with RMask. While GBP and GAMLP experience a significant decline in performance as the number of P operations increases, the predictive accuracy of GBP + RMask, and GAMLP + RMask either remain stable or only exhibit slight decreases. This stark contrast highlights the effectiveness of RMask in mitigating over-smoothing problem. Thus, utilizing RMask, model-simplification methods can leverage deep information more effectively, resulting in higher accuracy."}, {"title": "The Trade-off between Efficiency and Accuracy", "content": "Considering the high computational overhead incurred when processing large-scale graphs. We make a trade-off between efficiency and accuracy by controlling the number of random walks. As shown in Figure 5(a), we inserted RMask into four model-simplification GNNs on the ogbn-products dataset, the number of random walks are 10, 15, and 20, and compared with the original version (Original). The barplots represent the runtime and the lines represent the accuracy. For all methods integrated with RMask, the accuracy is higher than the original version when random walk numbers are larger than 10, and the accuracy can be further improved as the number of random walks increases. In addition, high efficiency is guaranteed by controlling the number of random walks. The comparative outcomes highlight that employing RMask as a plugin module yields satisfactory results with a minimal number of random walks."}, {"title": "Efficiency Analysis", "content": "To verify the efficiency of RMask, we first conducted a time cost breakdown of the S2GC model on six datasets with five random walks. In Figure 5(b), the bottom bar of each dataset represents the original version, while the top bar represents the version equipped with RMask. Our method successfully reduces the proportion of pre-processing overhead in end-to-end training, particularly for large-scale graph datasets. In Figure 5(c), we further compare the end-to-end training cost. In all the datasets, S2GC + RMask exhibits more than 2.9 times faster compared to S2GC. And the speedup is even greater on large-scale data sets, up to 4.9 times. This outcome shows the efficiency of our approach."}, {"title": "Ablation Study", "content": "To conduct a comprehensive study of the proposed RMask, we performed ablation studies into two RMask variants. The original is the RMask with the noise masking mechanism, we use S2GC+ RMask as the original version. Variant#1 does not use noise information identification. Variant#2 does not use the neighbor nodes importance assignment. Variant#3 utilizes another over-smoothing solution, DAGNN (Liu, Gao, and Ji 2020), as an orthogonal method for model training.\nWe conduct experiments with different numbers of random walks r on the above three variants on the Cora and Pubmed datasets, respectively. r is set to 10, 15, 20. Two observations can be made from Table 3.\nFirstly, Variant#1 and Variant#2 have different contributions to the performance. The noise information identification component can help RMask overcome the over-smoothing problem and make model-simplification GNNs go deeper. As the number of hops increases, the accuracy will not decrease. The neighbor nodes importance assignment component can help RMask further improve performance. Secondly, from variant#3, we can see that RMask can be orthogonal to other over-smoothing methods, thus further improving the accuracy."}, {"title": "Conclusion", "content": "This paper introduces RMask, a plug-and-play module designed to enhance existing model-simplification GNNs in exploring deeper graph structures at higher speeds. Unlike existing model-simplification GNNs focus on improving the combination method of propagated features, RMask offers a novel perspective by enhancing the utilization of valuable information at each hop. In the pre-processing step, RMask employs a mask method to eliminate noise information at each hop. To reduce the high overhead of pre-processing, RMask employs random walks to achieve a good trade-off between efficiency and accuracy. As a plugin method,re RMask seamlessly integrates with most model-simplification GNNs. Experimental results on six real-world datasets demonstrate that RMask effectively enhances the accuracy and efficiency of model-simplification GNNs."}, {"title": "More Related Works", "content": "Appendix\nSampling GNNs. An intuitive scalable approach is to use sampling techniques. Existing sampling work is usually divided into three categories: node level, layer level, and graph level. As a node-level sampling method, Graph-SAGE (Hamilton, Ying, and Leskovec 2017) samples the target nodes as a mini-batch and samples a fixed-size set of neighbors for computing. VR-GCN (Chen, Zhu, and Song 2018) analyzes the variance reduction on node-wise sampling, and it can reduce the size of samples with an additional memory cost. In the layer level, Fast-GCN (Chen, Ma, and Xiao 2018) samples a fixed number of nodes at each layer, and ASGCN (Huang et al. 2018) proposes adaptive layer-wise sampling with better variance control. For the graph level sampling, Cluster-GCN (Chiang et al. 2019b) clusters the nodes and only samples the nodes in the clusters, and GraphSAINT (Zeng et al. 2020) directly samples a subgraph for mini-batch training.\nModel-simplification GNNs. In addition to sampling GNNs, another scalable approach is model-simplification GNNs. Based on Eq. (1), several model-simplification works have been proposed (Wu et al. 2019; Zhu and Koniusz 2021; Chen et al. 2020) for scalable GNNS, which combine features at a finer granularity, i.e., hop-wise. For example, SIGN (Rossi et al. 2020) concatenates neighbor-aggregated features from different propagation layers: [$X(0)W_0,\u2026\u2026\u2026, X(k)W_k$], while S2GC (Zhu and Koniusz 2021) proposes a simple spectral graph convolution to average the propagated features in different propagation layers: $\\bar{X}(k) = \\sum_{l=0}^{k} \\hat{A}^{l}X(0)$. GBP (Chen et al. 2020) further improves the combination process by weighted averaging as $\\bar{X}(k) = \\sum_{l=0}^{k} w_l \\hat{A}^{l}X(0)$ with the layer weight w\u2081 = \u03b2(1\u2212\u03b2)'. GAMLP (Zhang et al. 2021) further considers feature propagation from a node-wise perspective, with each node having a personalized combination of the different layers of propagated features.\nComparison. Unfortunately, despite the focus of these models on combining different hops and improving the model design, they still face over-smoothing problem when the P operation deepens. RMask addresses this problem by eliminating the noise problem generated within each hop, which hinders the effective"}]}