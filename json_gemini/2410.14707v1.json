{"title": "FACMIC: Federated Adaptative CLIP Model for Medical Image Classification", "authors": ["Yihang Wu", "Christian Desrosiers", "Ahmad Chaddad"], "abstract": "Federated learning (FL) has emerged as a promising approach to medical image analysis that allows deep model training using decentralized data while ensuring data privacy. However, in the field of FL, communication cost plays a critical role in evaluating the performance of the model. Thus, transferring vision foundation models can be particularly challenging due to the significant resource costs involved. In this paper, we introduce a federated adaptive Contrastive Language Image Pretraining (CLIP) model designed for classification tasks. We employ a light-weight and efficient feature attention module for CLIP that selects suitable features for each client's data. Additionally, we propose a domain adaptation technique to reduce differences in data distribution between clients. Experimental results on four publicly available datasets demonstrate the superior performance of FACMIC in dealing with real-world and multisource medical imaging data. Our codes are available at https://github.com/AIPMLab/FACMIC.", "sections": [{"title": "Introduction", "content": "The success of deep learning (DL) highly depends on the availability of large amounts of data for training. However, there has been a growing focus on data privacy and security in recent years, with some organizations implementing regulations and laws such as the EU General Data Protection Regulation (GDPR) [1]. Collecting raw data is often impractical in this case, which poses a challenge to the feasibility of centralized DL approaches. Federated learning (FL) has emerged as a new distributed learning method to deal with this challenge and has been widely adopted in various applications [2]. FL enables model aggregation without directly accessing the raw user data from different clients. As pioneering work in FL, FedAVG efficiently combines distributed information through a simple but effective averaging algorithm [3]. This method ensures that raw data remain on the local client, preserving data privacy and security.\nThe performance of FL models is typically influenced by two main factors: data distribution shifts and communication costs [4]. Shifts in data distribution can negatively impact model prediction accuracy, particularly in the medical"}, {"title": "Material and methods", "content": null}, {"title": "Problem formulation", "content": "In our federated learning setting, we have a set of N clients {C1, C2,\u2026,CN}, each client Ci having a private data set Di = {(xi,j, Yi,j)}=1. As in similar studies [9], we assume that the data of separate clients have the same input and output space, but follow different distributions, i.e., P(Div) \u2260 P(Di), di' \u2260 i.\nEach dataset Di consists of three non-overlapping parts, namely a training set Dtrain, a validation set Dual and a test set Diest. Our goal is to train a robust global model fe (.) while preserving data privacy and security. This model should provide good testing performance on the test data of every client, i.e.,\n$\\min_{f_N} \\frac{1}{N}\\sum_{n=1}^{N} \\frac{1}{n_{test}^n}\\sum_{j=1}^{n_{test}^n} l(f_{\\theta}(x_{n,j}^{test}), y_{n,j}^{test}),$ (1)"}, {"title": "Our federated adaptive CLIP framework", "content": "Our federated learning framework for CLIP-based medical image classification is illustrated in Figure 1. This framework comprises three key components: a feature attention module for efficient FL, a feature adaptation strategy that addresses the problem of data distribution shifts across different clients, and a global aggregation strategy to combine the learned information from multiple clients. We present these components in what follows.\nTraining the attention module. We use a pretrained CLIP model comprising an image encoder e\u2081(\u00b7) and a text encoder er(\u00b7), to extract features from the data for each client C\u2081. For a training example x; \u2208 Dtrain, we denote as Ij = ex(xj) \u2208 RD the D-dimensional vector of image features. For text features, we use the standard prompt \"a picture of a {class}\" as input to the text encoder to obtain features Tj = er(xj) \u2208 RD.\nPretrained foundation models hold the ability to extract a rich set of fea- tures, however, not all of those are suitable for learning a specific task. This is particularly true for detecting and classifying abnormal regions such as lesions in medical images, as these regions are absent in normal images and typically"}, {"title": null, "content": "represent a small part of the image. To identify the regions of focus for locally- trained models, we introduce a client feature attention module, denoted as a\u00bf(\u00b7). This attention module takes as input image features I and returns an attention mask a\u00bf(I) \u2208 [0, 1]D. This mask is then used to generate masked images features I = a; (I) I, where is the Hadamard (element-wise) product.\nWe measure the probability that an example x; belongs to a class c using the cosine similarity between the image features of x; and the text features Tc corresponding to the prompt of c:\n$p(Y=c|x_j) = \\frac{exp(s_{j,c}/\\tau)}{\\sum_{c'=1}^{K} exp(s_{j,c'}/\\tau)},$ with $s_{j,c} = \\frac{(I_j, T_c)}{||I_j||||T_c||}$  (3)\nwhere \u03c4 is the softmax temperature parameter.\nKeeping the image and text encoders frozen, we train the local adapter mod- ules by minimizing a contrastive loss Lcontr that pushes together the image and text features from the same training example and pulls apart non-matching ones. Following [10], we compute the contrastive loss over batches of size B.\nLet S be the B\u00d7B matrix where sj,j' is the cosine similarity between im- age features I; and Tj, as measured in Eq (3). We compute an image prob- ability matrix P = softmax(S/\u03c4) \u2208 [0,1]B\u00d7B B\u00d7B and a text probability matrix Q = softmax(ST/\u03c4) \u2208 [0,1]B\u00d7B. The contrastive loss is then formulated as follows:\n$L_{contr} = - \\frac{1}{B} \\sum_{j=1}^{B} (log p_{j,j} + log q_{j,j})$  (4)\nLocal feature adaptation. Discrepencies in the local data distribution of clients may affect the training of the global model via federated learning. To address this problem, we propose a client-specific feature adaptation strategy based on the Local Maximum Mean Discrepancy (LMMD) method [11]. This domain adaptation method aligns the class-wise distribution statistics of the data from a source domain D, and a target domain Dt, by minimizing the fol- lowing loss:\n$L_{DA} = \\frac{1}{K}\\sum_{c=1}^{K}| \\frac{1}{n_s} \\sum_{x_i \\in D_s} \\omega_i \\phi(x_i) - \\frac{1}{n_t} \\sum_{x_j \\in D_t} \\omega_j \\phi(x_j) |^2_H , \\omega_{i,c}^{s,t} = \\frac{1(Y_i = c)}{\\sum_{y_j \\in D_{s\\t}} 1(y_j = c)}$ (5)\nHere, \u03c6(\u00b7) is a mapping function to a Hilbert space H, while wi,c, we are weights that measure the membership of example x and x in class c.\nIn our setting, we suppose that the shift occurs only on the image features, thus x=I in Eq. (5). Furthermore, since we cannot compute \u03c6(\u00b7) directly (as it maps features to a high-dimensional space), we use the kernel trick and refor-"}, {"title": null, "content": "mulate the loss as\n$L_{DA} = \\frac{1}{K}\\sum_{c=1}^{K}[\\frac{1}{n_s^2} \\sum_{i=1}^{n_s} \\sum_{j=1}^{n_s} \\omega_i^c \\omega_j^c k(I_i, I_j) + \\frac{1}{n_t^2} \\sum_{i=1}^{n_t} \\sum_{j=1}^{n_t} \\omega_i^c \\omega_j^c k(I_i, I_j) - \\frac{2}{n_s n_t}\\sum_{i=1}^{n_s} \\sum_{j=1}^{n_t} \\omega_i^c \\omega_j^c k(I_i, I_j)]$ (6)\nwhere ns, nt are the number of samples in the source domain and target domain, respectively, and k(\u00b7, \u00b7) = ($(\u00b7), (\u00b7)) is the kernel function.\nAlthough we defined our domain adaptation loss, we still need to specify the source and target domains used in this formulation. When performing a local adaption for client Ci, we set the source domain to be the data of this client, i.e., Ds = Di. However, to preserve privacy, we cannot exchange data directly between clients, therefore we cannot use the data of other clients for the target domain. Instead, we use a global set of unlabeled images as the target domain data. This constraint can be easily satisfied since there are many publicly available datasets in medical imaging and no labels are needed for this reference data.\nTo compute the weights wie in Eq. (5), we employ a pseudo-label strategy to estimate the class labels. Specifically, we obtain the class probabilities of a target image I using Eq. (3), and assign this image to the class with the highest probability. Our final loss to update the feature attention parameters of each client is the combination of the contrastive loss and domain adaptation loss,\n$L = L_{contr} + \\lambda L_{DA},$  (7)\nwhere X is a hyper-parameter controlling the trade-off between these two loss terms.\nGlobal aggregation. The last part of our proposed FACMIC framework is the aggregation strategy to combine the parameters of different clients into a single global model. This strategy works as follows. In each round, each client Ci uploads its attention module parameters to the server. Thereafter, the server combines these parameters into a single vector \u03b8\u03b1 global using a weighted average\n$\\theta_{global} = \\sum_{i=1}^N w_i \\theta_i , w_i = \\frac{n_{train}}{\\sum_{i'=1}^N n_{train}}$  (8)\nNext, the server broadcasts the global attention module parameters back to each client. Since the attention module has only a small amount of parameters compared to the CLIP encoders, this strategy has very low communication and computational costs."}, {"title": "Experiments", "content": null}, {"title": "Datasets", "content": "Brain Tumor. We conduct experiments on a public MRI brain tumor classification dataset, denoted as BT [12]. BT has four different classes, namely glioma"}, {"title": "Implementation details", "content": "The ViT-B/16 pre-trained model is adopted as the CLIP backbone in our frame- work. Our light-weight attention module is composed of five layers: a first linear layer, a batch normalization layer, a LeakyReLU layer, a second linear layer, and a softmax activation function. During training, we keep the CLIP encoders frozen and optimize only the attention module's parameters using Adam with"}, {"title": "Comparison with state-of-the-art methods", "content": "To have a comprehensive comparison, we include several related approaches in our experiments: FedAVG [3], MOON [18], FedProx [19], FedFocal [20], Fed- CLIP [9] and a centralized method. FedAVG fine-tunes and aggregates all the parameters of the CLIP image and text encoders. MOON extends FedAVG with a contrastive loss between the previous model and the current model. The Fed- Prox approach adds a proximal term to FedAVG that allows having slight dif- ferences between clients and the server. FedFocal replaces the standard CE loss with a focal loss for FedAVG. FedCLIP adds an adapter to the CLIP model and only considers the parameters of this adapter during the fine-tuning and aggre- gation steps. Finally, the centralized method is designed with only one client holding all the training data. For a fair comparison, the same experimental set- ting described above is used for all tested methods. We evaluate performance using classification accuracy (ACC) as the primary metric. Additionally, due to the class imbalance in the Real dataset, we also consider balanced accuracy (BACC) for a more comprehensive assessment [21]. Additional implementation details and visualization results can be found in the Supplemental materials."}, {"title": "Ablation study", "content": "To validate the effectiveness and generalizability of our model, a series of ablation studies were conducted following the same experimental setting. As reported in Table 3, adding the domain adaptation loss LDA leads to a better classification accuracy in the BT dataset, in all situations. These results demonstrate the need to address the problem of shifting data distribution among clients.\nWe also studied the performance of our method for different batch sizes (from 4 to 32), as using large batches is not practical for less capable devices. For sizes of 4 and 8, the adaptation loss is rescaled by a factor of 1/10 since it is too large compared with the contrastive loss in those cases. Table 4 shows the global testing ACC using BT (iid) and Real dataset. As one can see, our model is robust to batch size on the BT data, however, using a too small batch size for the large-scale SC data can result in negative adaptation."}, {"title": null, "content": "To assess our method's generalization ability, we use the fine-tuned global model trained on the BT dataset under iid condition to perform classification directly on another brain tumor dataset (BT2) without any fine-tuning. The BT2 dataset comprises 7023 samples obtained from figshare, SARTAJ, and Br35H [22-25], and has the same classes as the BT dataset. We tested these methods using the whole dataset. As reported in Table 5, our method achieves the highest generalization accuracy of 94.42% on BT2."}, {"title": "Conclusion", "content": "We explored the usefulness of VLMs for medical imaging in FL and presented a novel FACMIC framework that combines a feature attention module to re- duce communication costs and a domain adaptation strategy to minimize data distribution differences between each client. Experimental results in brain tu- mor and skin cancer classification tasks demonstrate the superior performance of FACMIC compared to state-of-the-art FL approaches."}]}