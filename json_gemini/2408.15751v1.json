{"title": "Adaptive Traffic Signal Control Using Reinforcement Learning", "authors": ["Muhammad Tahir Rafique", "Ahmed Mustafa", "Hasan Sajid"], "abstract": "Traffic demand is continuously increasing, leading to significant congestion issues in major urban areas. Constructing new infrastructure is a potential solution but presents a substantial financial burden on national economies. An alternative approach involves optimizing existing traffic networks through the dynamic control of traffic signals at intersections. Recent advancements in Reinforcement Learning (RL) techniques have demonstrated their capability to address the complexities associated with traffic congestion. In this paper, we propose a solution to traffic congestion using reinforcement learning. We define the state as a scalar representing the queue length, demonstrating that the algorithm can effectively learn from this simplified state representation. This approach can potentially reduce deployment costs by minimizing the number of sensors required at intersections. We have developed two RL algorithms: a turn-based agent, which prioritizes traffic signals for the intersection side with higher traffic, and a time-based agent, which adheres to a fixed phase cycle, adjusting the phase duration based on traffic conditions. To assess the performance of these algorithms, we designed four distinct traffic scenarios and computed seven evaluation metrics for each. Simulation results indicate that both algorithms outperform conventional traffic signal control systems.", "sections": [{"title": "I. INTRODUCTION", "content": "With regard to the fast-growing population around the world, the urban population increases rapidly which leads to more and more traffic on roads. More vehicles on the road cause traffic congestion that emerged as a very serious issue in populated cities. Traffic congestion leads to problems like fuel consumption, CO2 emission, and wastage of time. It is impossible to stop people from buying cars and to start building new traffic infrastructure that might cause a huge burden on the country's economy. So, the easy way to solve this problem is to optimize the traffic signals at the intersection.\n\nThe existing traffic light control opens the signal for each side of the intersection in a fixed rotation. This control sets fixed time duration for all directions of intersection and opens them one after another without considering the real-time traffic. In low traffic scenario, this strategy opens the signal even for that side of the intersection that has no traffic and result in wastage of time of commuters at the other side of the intersection. In other cases, where the traffic flow increases such as a cricket match event, during school and office timings, this control system becomes paralyzed. We often witness that when a traffic signal is off due to any technical reason, a policeman is seen controlling the flow of traffic at the intersection. This human operator sees the real-time traffic condition and according to his experience, he selects the time duration for each side of the intersection. The limitation with the human operator is that his view is local to one side of the intersection and cannot plan at a global level. So overall performance of the human operator is sub-optimal. We need a system that not just have learning capability like human operator but also have the ability to see all sides of the intersection at the same time to plan a global level strategy.\n\nIn developing countries, road infrastructure is in a progressive state and changes every day. This causes a tremendous change in traffic flow on different routes. So, a system is needed that not just controls the traffic flow at the intersection but also adopts a new control policy according to traffic behavior. In recent years, machine learning has shown major progress in many learning tasks (like classification and object detection). To solve traffic control problem, a machine learning technique called Reinforcement Learning [1] can be used. The reinforcement learning has successfully completed the complex tasks in multiple fields such as games [2], robotics [3], healthcare [4], finance [5] and traffic signal control. In reinforcement learning, the agent learns based on the hit and trial method and does not need to have perfect knowledge of the environment.\n\nMost of the recent studies [6] [7] on traffic signal control have shown improvement in simulation but only few of them have been implemented in a real-world application. The main reason is that it is difficult to get an environment state for a reinforcement learning agent. In order to get state, sensors are needed that cost heavily and many developing countries cannot afford it. These developing countries can afford camera surveillance systems for the security purpose. These camera-based surveillance systems at intersections can be used for traffic flow optimization. The camera output is fed to the object detection algorithm to detect and count the stationary vehicles. This vehicle count can be used as a state of a reinforcement learning agent.\n\nIn this paper, we aim to use the reinforcement learning approach to build a traffic light control system to improve the traffic flow at the intersection. In our research, state representation is different from previous studies [6] [8] [9]. We have proposed a scalar state (queue length) and shown that the reinforcement learning agent learns from it. Instead of using fewer evaluation metrics, we have proposed seven evaluation metrics to evaluate the performance of the reinforcement learning agent. The reason for choosing multiple evaluation metrics is that fewer evaluation metrics are not enough to define the overall performance of the reinforcement learning agent. The two different reinforcement learning agents, turn-"}, {"title": "II. REINFORCEMENT LEARNING BACKGROUND", "content": "Reinforcement Learning is a type of Machine Learning in which an algorithm learns by experience. The Fig. 1 shows the reinforcement learning standard cycle, in which agent corresponds to reinforcement learning algorithm and environment corresponds to an intersection. The problem that reinforcement learning tries to solve is treated as Markov Decision Process (MDP). MDP defines five tuples < S, A, T, R, \u03b3 > with following meanings:\n\n\u2022 S: set of possible states. s is single state ( $s \\in S$)\n\n\u2022 A: set of possible actions. a is single action ( $a \\in A$)\n\n\u2022 T: transition probability. The probability of moving from current state st to next state st+1 by taking an action at.\n\n\u2022 R: reward function. Give immediate reward of transition to state st+1 from state st by taking an action at.\n\n\u2022 \u03b3: discount factor \u03b3\u2208 [0, 1]. Importance given to future reward.\n\nIn a reinforcement learning problem, the agent interacts with the environment and gets the current condition of the environment. This current condition is called the current state st of the environment at time interval t. Agent performs an action at on environment, which leads the environment to new state st+1. After the transition of environment state, the agent receives a reward rt that tells the agent how good the action at was. The goal of a reinforcement learning agent is to find a policy \u03c0* that maximizes the discounted future reward.\n\nReinforcement learning agent learns to maximize the cumulative future reward. It is denoted by Rt, where t represents the timestep at which we get a reward. In mathematical notation it is written as:\n\n$R_t = \\sum_{k=0}^{\\infty}r_{t+k}$ (1)\n\nIf we let this reward series to go on to infinity, then we end up with infinite reward and it is not a good way to define a problem. Reward series must be terminated to get a finite reward. So, a discount factor y is introduced to limit the reward series. As a result of this, we get a cumulative discounted future reward.\n\n$R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}$ (2)\n\nDiscount factor y is to trade-off the worth of immediate and future reward. Smaller the value of y, short-sighted will be the agent. Such an agent gives more importance to immediate reward and less importance to future reward. A large value of y makes the agent far-sighted and it will give importance to future rewards.\n\nThe policy is a function that describes the behavior of an agent in a given state. It takes the state and action and returns the probability of taking an action in that state. The goal of reinforcement learning is to learn the optimal policy \u03c0* to maximize the cumulative discounted future reward. To learn the optimal policy, we use action-value function (Q-value function [10]) denoted by $Q^\\pi(s_t, a_t)$. The action-value function describes the value of taking an action at in state St while following a certain policy \u03c0. It is the expected discounted accumulative reward given the state and action under policy \u03c0.\n\n$Q^\\pi (s, a) = E_\\pi [R_t | S_t = S, a_t = a]$ (3)\n\nThe Bellman equation is used to find the optimal action-value function. Bellman equation decomposes the action-value function into immediate reward and discounted Q-value of successor states. Mathematically it can be written as:\n\n$Q^\\pi(s_t, a_t) = E_\\pi [R_t | R_t + \\gamma max_{a} Q(s_{t+1}, a)]$ (4)\n\nIf the action-value function is correctly estimated, then the greedy policy becomes the optimal policy and we choose the action with a larger Q-value.\n\n$\\pi^* = arg max_{a} Q^\\pi (s, a)$ (5)"}, {"title": "III. LITERATURE REVIEW", "content": "Traffic flow optimization is the one of the most active research areas in a transportation system. Researchers around the world are trying to solve this problem for more than three decades. For the first time, Webster [11] try to solve the problem by using a mathematical approach. It gave the equation to find the optimal phase time. Webster's theory faded as the traffic at the intersection increased and it is modified by Robertson [12].\n\nWith the advancement in sensor and computer technology, better solutions like fuzzy control [13] [14] and genetic algorithm [15] [16] were introduced in traffic control system. These solutions derive from historical data and their performance decreases as the traffic behavior changes with time. In recent years, reinforcement learning [17] [18] has gained researchers importance because of its learning capability. Reinforcement learning approaches used from 1997 to 2010 are summarized by El-Tantawy [19]. These approaches used Q-table [10] and linear function to find Q-values.\n\nLi et al. [6] has used deep stacked autoencoder (SAE) network to find out the action in current state. Their algorithm takes queue length as input and queue difference between north-south traffic and east-west traffic as a reward. This algorithm has only two actions with no left and right turns. They compared the performance of their algorithm with conventional traffic light control and shown that their algorithm"}, {"title": "IV. SYSTEM DESCRIPTION", "content": "A. State Representation\n\nThe state of agent represents the condition of the environment at a specific interval of sampling time ts and it is denoted by Sts. The design of state is very important in reinforcement learning because it has a huge impact on reinforcement learning agent performance. In this research, the state is represented by queue lengths at an intersection. Queue length is a scalar number and it is obtained by counting the total number of stationary vehicles at the intersection. In the case of a turn-based agent, a state is formed by obtaining queue length from all sides of the intersection. The number of queue lengths obtained from the intersection depends upon the sides of the intersection. For example, in the case of a four-way intersection, four queue lengths are obtained to form a state of intersection. In a time-based agent, the state is the queue length of one side of the intersection.\n\nIn simulator, the state is obtained by counting all the stationary vehicles at the intersection. Vehicles that have a speed less than 1 m/s are considered stationary. The obtained state is a scalar number and cannot be used for the training of reinforcement learning agent. In order to learn the relationship between state and action, a deep neural network is required. It is not possible to train the deep neural network using scalar inputs because the input dimension is very low and the neural network does not learn from it. The solution to this problem is to use human-crafted features that increase the input dimension and help in training. In this research, binary encoding is used to increase the dimension of the state. The encoding has two key elements, encoding size, and encoding weights. Encoding size represents the number of cells in the encoding matrix and encoding weights represent the minimum number of vehicles required to fill 1's in the encoding matrix cell. The way to fill the encoding matrix is given in Algorithm 1. It is the pseudo-code that explains the steps to fill the encoding matrix.\n\nB. Action Space\n\nIn order to guide the vehicle through the intersection, reinforcement learning agent has to choose appropriate action according to the current state. In the case of a turn-based agent, action is the possible phase that is available, but in the case of a time-based agent, action is the time duration of the green phase.\n\nThe possible actions for turn-based agent are given in (6). NG (North-Green) allows vehicles to pass from North to other directions (E, W, and S) and indicates the vehicles on East, West, and South routes should stop and not proceed through the intersection. In the same way, WG (West-Green), EG (East-Green), and SG (South-Green) allows vehicles from West, East and South respectively to pass to other directions and stopping the proceeding of vehicles from these other directions respectively through the intersection (as E, N and S routes for WG; N, W and S for EG; and E, W, and N for SG.).\n\n$A_{turn} = \\{NG, WG, EG, SG\\}$ (6)\n\nThe time-based agent selects phase duration instead of phase. Its phase cycle is fixed but the time duration of the phase change according to the state of the environment. The possible actions of time-based agent are given in (7). Where 0 means no increment in green light base time and 10 means ten seconds are added to green light base time. Where green light base time is 15 seconds, so possible minimum time duration for a phase is 15 seconds and the maximum time duration for a phase is 34 seconds.\n\n$A_{time} = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 11, 12, 13, 14, 15, 16, 17, 18, 19\\}$ (7)\n\nReinforcement learning agent chooses action on sampling timestep ts. Once the agent chooses the action, then next time it will be able to choose action on sampling time ts + 1. In a conventional traffic light control system, the transition phase will occur after every green phase. The transition phase activates the yellow light on the intersection for 4 seconds. In a turn-based agent, the transition phase will occur if the phase chosen by an agent is different from the previous phase. For example, the action taken by the agent at sampling timestep ts is ao and if it is the same as it was taken at the previous timestep ts - 1 then there is no transition phase. After the completion of action ao, the agent selects a new action at sampling timestep ts + 1. Let us consider that this time agent selects action a3, and it is different from action taken at timestep ts. In this case, the transition phase of 4 seconds will occur. The timeline of turn-based agent action is shown in Fig. 3. In the case of a time-based agent, the transition phase will always occur just like conventional traffic light control.\n\nC. Reward\n\nThe reward is the feedback that the reinforcement learning agent obtains from the environment after taking an action. Reward tells the agent; how good or bad action is? The agent uses this information to update its model for future action choices. The reward is either positive or negative. Positive reward indicates that the action chosen by the agent helps in reducing traffic congestion. A negative reward indicates that the chosen action has increased the traffic conjunction."}, {"title": "V. EXPERIMENT RESULT", "content": "A. Experiment setup\n\nWe use Simulation of Urban Mobility (SUMO) [22] traffic simulator in our experiment. SUMO is an open-source microscopic simulator and provides every needed information about vehicles and networks. The detail of simulation setting is described in following sections.\n\nOur main objective is to improve the efficiency of intersection by reducing the wait time of vehicles. Thus, we define the reward as the difference between awt (accumulative wait time) of all vehicles obtained at two different sampling timestep, i.e.\n\n$r(ts) = awt(ts-1) - awt(ts)$ (8)\n\nwhere awt is accumulative wait time. The awt is obtained by adding the wait time of all vehicles that are stationary at all sides of the intersection. The reward rts is obtained by taking the difference between accumulative wait time at previous timestep awtts-1 and at current timestep awt(ts). The equation (8) is designed in such a way that if an agent takes good action it will return a positive reward and if bad action is taken by then it returns a negative reward.\n\nD. Policy\n\nThe policy is a function that maps the given state of the environment to the corresponding action. The agent's objective is to reduce the wait time of vehicles in the long run. So, the agent observes the current state of the environment and takes an action according to policy and gets a reward. In order to reduce the wait time, the agent chooses an action that maximizes immediate reward r(ts). The agent must find optimal policy \u03c0* that maximizes the following accumulative reward.\n\n$Q^\\pi (s, a) = E[r_{ts} + \\gamma r_{ts+1} + \\gamma^2 r_{ts+2}+...] = E\\sum_{k=0}^{\\infty} \\gamma^k r_{ts+k}$ (9)\n\nWhere y is a discount factor having range 0 \u2264 y \u2264 1. It reflects that how much importance the agent gives to future reward. If y = 0 then it gives no importance to future reward and if y = 1 then it gives more importance to future reward.\n\nTo find the policy, we use a deep neural network as a function approximator and its parameters o refers to policy parameters. The policy distribution \u03c0(ats|sts;0) is learned by experience replay. We store the tuple (sts, ats,rts, Sts+1) in memory at every sampling timestep. After few episodes of simulation, we have enough data to train a deep neural network. We fetch a batch of 64 examples and train our deep neural network. After few hours of training, the parameters of the deep neural network reach the optimal location. The policy with optimal parameters becomes the optimal policy \u03c0(\u03b1 | s; 0*).\n\n$\\pi (\\alpha | s; \\theta^*) = arg max_{\\pi} Q (s, a) \\forall s \\in S, a \\in A$ (10)\n\nB. Deep neural network design\n\nReinforcement learning agents require a deep neural net-work as a function approximator to learn the relationship between state-action pairs. The deep neural network has 5 hidden layers, denoted by hl. All the hidden layers are fully connected and used rectified linear units as activation functions. The number of nodes in the hidden layer can vary.\n\nC. Performance measure\n\nThe performance measure is a criterion through which an agent's performance is evaluated. It defines how good or bad agent perform in a simulated environment. Instead of using a single evaluation metric, seven evaluation metrics are introduced to compute the reinforcement learning agent's performance. The evaluation metrics are described in detail in the following sections.\n\n1) Total Negative Reward: As it is mentioned earlier that we can get either negative or positive reward from reward function, we accumulate negative reward to see the agent's performance. The Total Negative Reward (Tnr) is the sum of all the negative rewards obtained from the reward function during one episode of simulation. It is obtained at the end of each episode and the reinforcement learning agent tries to maximize it.\n\n$Tnr(e) = \\sum_{ts=1}^m min(0, 'r'(ts))$ (11)\n\nwhere ts is the sampling timestep, m is the total number of sampling timesteps in one episode, e is the episode number, r(ts) is the reward at sampling timestep ts and Tnr(e) is the total negative reward for episode e.\n\n2) Total Accumulative Wait Time: Wait Time (wt) is the sum of wait for individual vehicle c that is on the incoming road rd at a specific interval of sampling timestep ts. If we add up wt for all vehicles in the network at ts then we get an accumulative wait time awt(ts).\n\n$awt (ts) = \\sum_{c=1}^n [rd(c) wt(s)]$ (12)\n\nwhere n is the total number of vehicles in a network and rd(c) is explained below:\n\n$rd(c) = \\begin{cases} 1 \\text{ if vehicle on incoming road} \\\\ 0 \\text{ else} \\end{cases}$\n\nTotal Accumulative Wait Time (Tawt) is the sum all aut(ts) for each episode e. It is calculated by using (13).\n\n$Tawt(e) = \\sum_{ts=1}^m awt (ts)$ (13)\n\n3) Expected Wait Time per Vehicle: Expected Wait Time per Vehicle (ewpv) is the average wait time wtavg that individual vehicle c can face when it tries to pass through the intersection. It is calculated at the end of each episode e.\n\n$ewpv (e) = \\sum_{t_s=1}^{t_s=m}  \\frac {\\sum_{c=1}^{n} rd(c) \\ w_{t(c)}} {(\\sum_{t_s=1}^{t_s=m} rd(c) ) } $ (14)\n\nIt is the most important metric to check the performance of the reinforcement learning agent. The lower value of ewpv (e) leads to a good performance of agent while higher value leads to poor performance.\n\n4) Average Queue Length: Queue Length (ql) is the number of vehicles that are stationary at the intersection. The vehicles with speeds less than 1 m/s are considered stationary and counted in queue length. The queue length is calculated at every sampling timestep by using the following equation:\n\n$ql(ts) =  \\sum_{c=1}^n rd(c) [1-floor \\frac { sgn (v_{p(ts)}^{(c)}) + 1} {2} ] $ (15)\n\n$p_{ps}^{(c)} = vehicle \\ c \\ speed \\ at \\ sampling \\ time \\ step \\ t_s$\n\nE. Training\n\nBoth turn-based and time-based agents are trained for 300 episodes. Training more than these episodes do not affect the performance of agents. Each episode corresponds to 5400 seconds of simulation. To prevent overfitting, we shift the traffic scenario after each episode. In the training phase, the agent's main objective is to find the best action on the given state. In the early stage of training, the agent does not know which action is the best. So, the agent does not care about the performance and takes random action to explore the state-action space. As the training goes on, the agent gains knowledge about state action pair and reduces its exploration. Now, the agent increases the frequency of exploitative action to increase the performance. The probability to choose random action is defined by epsilon & also known as exploration rate. At the start of training exploration rate is high that is why the agent takes random action and at the end of the training, the exploration rate is low and forces the agent to take exploitative action. We used non-linear function given in (16) to model the exploration rate.\n\n$ \\epsilon = \\begin{cases} 1 - \\frac {e}{90} \\ \\  e < 90 \\\\ 1 - (\\frac {e -90}{120}) \\ \\  90 <e < 210 \\\\ 0.2 -(\\frac {e-210}{90}) \\ \\  210 <e < 300 \\end{cases}  $ (16)\n\nE. Result\n\nTo evaluate the performance of proposed agents, we com-pare it against a conventional traffic light control system. We run five episodes of SUMO simulation with randomly generated traffic by fixing traffic scenarios (low, high, EW and NS) and compute seven evaluation metrics for each episode. After that, we take the mean of five episodes values to get the average value for each evaluation metrics. This evaluation procedure is followed for both reinforcement learning agents and conventional traffic light control system. After that, we compare the reinforcement learning agents (turn-based and time-based) evaluation metrics with conventional traffic light control system to find out the performance improvement.\n\n1) Turn-based Agent: Table IV shows the evaluation result of turn-based agent on four different traffic scenarios. Columns with traffic scenarios header represent the result of each evaluation metrics and with % header represent the percentage improvement that turn-based agent has made from conven-tional traffic light control system. The percentage improvement columns are highlighted with green and red colors. Green color means the agent performs well and there is improvement in the performance while red color means that the agent performs badly and has reduced the performance. In summary, the turn-based agent performs well in low traffic scenario, bad in high traffic scenario, and normal in EW and NS traffic scenarios. In the case of the EW traffic scenario, most of the traffic coming from the east and west direction, so the agent gives importance to these sides. The same phenomenon occurs in the case of the NS traffic scenario where it gives more importance to the north and south direction and less importance to the east and west direction. Such type of behavior indicates that the turn-based agent becomes greedy and gives importance to that side of the intersection that has a large queue length. The poor performance of the agent in a high traffic scenario is due to this greedy behavior. In the case of a high traffic scenario, a large number of vehicles are coming from all directions towards the intersection. At each sampling timestep, the agent finds high queue length in all direction and selects the green phase for the direction that have maximum queue length. At the next sampling time instant, it finds a high queue length in the other direction and chooses the green phase of that side. So, the agent continuously switches between green phases of different sides which results in additional 4 seconds of transition phase (yellow light). As a result of this wait time of vehicles increases and the agent's performance decreases.\n\n2) Time-based Agent: The motivation behind the time-based agent is the greedy approach of the turn-based agent. Turn-based agent always gives importance to the side that has high queue length. Let consider the example of an EW traffic scenario in which most of the traffic is coming from east and west directions. The turn-based agent selects the green phase for the east and west side and neglects the north and south side of the intersection. It does not give importance to the north and south side until the queue length on these sides increases to that of the east and west sides or queue length of the east and west side decreases to the queue length of the north and south sides. The turn-based agent forces the traffic of the north and south sides to wait more at the intersection. Such type of behavior is not acceptable in a real-world application. To overcome this problem, the simplest solution is to use the time-based agent as it cycles through all phases, and traffic on the short queue length side gets a chance to pass through the intersection.\n\n3) Comparison: To find out which agent performs well, we compare the performance of a turn-based agent against"}, {"title": "VI. CONCLUSION", "content": "In this paper, we proposed turn-based and time-based re-inforcement learning agents to solve the traffic congestion problem at the intersection. Both agents encode the scalar queue length input to a useful features matrix and find optimal traffic signal control policy. Instead of using reward function as a performance measure, we have used seven evaluation metrics to compute performance improvement. The simulation result shows that both agents perform better than the conventional traffic light control system. We have also compared the performance of both agents on four different traffic scenarios and find out that the turn-based agent is good for the low traffic scenario and the time-based agent is good for the high traffic scenario. In the end, we have computed the overall performance of both agents on all traffic scenarios and find out that on average, the time-based agent performs better than the turn-based agent."}]}