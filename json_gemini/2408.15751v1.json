{"title": "Adaptive Traffic Signal Control Using Reinforcement Learning", "authors": ["Muhammad Tahir Rafique", "Ahmed Mustafa", "Hasan Sajid"], "abstract": "Traffic demand is continuously increasing, leading to significant congestion issues in major urban areas. Constructing new infrastructure is a potential solution but presents a substantial financial burden on national economies. An alternative approach involves optimizing existing traffic networks through the dynamic control of traffic signals at intersections. Recent advancements in Reinforcement Learning (RL) techniques have demonstrated their capability to address the complexities associated with traffic congestion. In this paper, we propose a solution to traffic congestion using reinforcement learning. We define the state as a scalar representing the queue length, demonstrating that the algorithm can effectively learn from this simplified state representation. This approach can potentially reduce deployment costs by minimizing the number of sensors required at intersections. We have developed two RL algorithms: a turn-based agent, which prioritizes traffic signals for the intersection side with higher traffic, and a time-based agent, which adheres to a fixed phase cycle, adjusting the phase duration based on traffic conditions. To assess the performance of these algorithms, we designed four distinct traffic scenarios and computed seven evaluation metrics for each. Simulation results indicate that both algorithms outperform conventional traffic signal control systems.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH regard to the fast-growing population around the world, the urban population increases rapidly which leads to more and more traffic on roads. More vehicles on the road cause traffic congestion that emerged as a very serious issue in populated cities. Traffic congestion leads to problems like fuel consumption, CO2 emission, and wastage of time. It is impossible to stop people from buying cars and to start building new traffic infrastructure that might cause a huge burden on the country's economy. So, the easy way to solve this problem is to optimize the traffic signals at the intersection.\nThe existing traffic light control opens the signal for each side of the intersection in a fixed rotation. This control sets fixed time duration for all directions of intersection and opens them one after another without considering the real-time traffic. In low traffic scenario, this strategy opens the signal even for that side of the intersection that has no traffic and result in wastage of time of commuters at the other side of the intersection. In other cases, where the traffic flow increases such as a cricket match event, during school and office timings, this control system becomes paralyzed. We often witness that when a traffic signal is off due to any technical reason, a policeman is seen controlling the flow of traffic at the intersection. This human operator sees the real-time traffic condition and according to his experience, he selects the time duration for each side of the intersection. The limitation with the human operator is that his view is local to one side of the intersection and cannot plan at a global level. So overall performance of the human operator is sub-optimal. We need a system that not just have learning capability like human operator but also have the ability to see all sides of the intersection at the same time to plan a global level strategy.\nIn developing countries, road infrastructure is in a progressive state and changes every day. This causes a tremendous change in traffic flow on different routes. So, a system is needed that not just controls the traffic flow at the intersection but also adopts a new control policy according to traffic behavior. In recent years, machine learning has shown major progress in many learning tasks (like classification and object detection). To solve traffic control problem, a machine learning technique called Reinforcement Learning [1] can be used. The reinforcement learning has successfully completed the complex tasks in multiple fields such as games [2], robotics [3], healthcare [4], finance [5] and traffic signal control. In reinforcement learning, the agent learns based on the hit and trial method and does not need to have perfect knowledge of the environment.\nMost of the recent studies [6] [7] on traffic signal control have shown improvement in simulation but only few of them have been implemented in a real-world application. The main reason is that it is difficult to get an environment state for a reinforcement learning agent. In order to get state, sensors are needed that cost heavily and many developing countries cannot afford it. These developing countries can afford camera surveillance systems for the security purpose. These camera-based surveillance systems at intersections can be used for traffic flow optimization. The camera output is fed to the object detection algorithm to detect and count the stationary vehicles. This vehicle count can be used as a state of a reinforcement learning agent.\nIn this paper, we aim to use the reinforcement learning approach to build a traffic light control system to improve the traffic flow at the intersection. In our research, state representation is different from previous studies [6] [8] [9]. We have proposed a scalar state (queue length) and shown that the reinforcement learning agent learns from it. Instead of using fewer evaluation metrics, we have proposed seven evaluation metrics to evaluate the performance of the reinforcement learning agent. The reason for choosing multiple evaluation metrics is that fewer evaluation metrics are not enough to define the overall performance of the reinforcement learning agent. The two different reinforcement learning agents, turn-"}, {"title": "II. REINFORCEMENT LEARNING BACKGROUND", "content": "Reinforcement Learning is a type of Machine Learning in which an algorithm learns by experience. The Fig. 1 shows the reinforcement learning standard cycle, in which agent corresponds to reinforcement learning algorithm and environment corresponds to an intersection. The problem that reinforcement learning tries to solve is treated as Markov Decision Process (MDP). MDP defines five tuples < S, A, T, R, \u03b3 > with following meanings:\n\u2022 S: set of possible states. s is single state ($s \\in S$)\n\u2022 A: set of possible actions. a is single action ($a \\in A$)\n\u2022 T: transition probability. The probability of moving from current state st to next state st+1 by taking an action at.\n\u2022 R: reward function. Give immediate reward of transition to state st+1 from state st by taking an action at.\n\u2022 \u03b3: discount factor \u03b3\u2208 [0, 1]. Importance given to future reward.\nIn a reinforcement learning problem, the agent interacts with the environment and gets the current condition of the environment. This current condition is called the current state st of the environment at time interval t. Agent performs an action at on environment, which leads the environment to new state st+1. After the transition of environment state, the agent receives a reward rt that tells the agent how good the action at was. The goal of a reinforcement learning agent is to find a policy \u03c0* that maximizes the discounted future reward.\nReinforcement learning agent learns to maximize the cumulative future reward. It is denoted by Rt, where t represents the timestep at which we get a reward. In mathematical notation it is written as:\n$R_t = \\sum_{k=0}^{\\infty} r_{t+k}$ (1)\nIf we let this reward series to go on to infinity, then we end up with infinite reward and it is not a good way to define a problem. Reward series must be terminated to get a finite reward. So, a discount factor y is introduced to limit the reward series. As a result of this, we get a cumulative discounted future reward.\n$R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}$ (2)\nDiscount factor y is to trade-off the worth of immediate and future reward. Smaller the value of y, short-sighted will be the agent. Such an agent gives more importance to immediate reward and less importance to future reward. A large value of y makes the agent far-sighted and it will give importance to future rewards.\nThe policy is a function that describes the behavior of an agent in a given state. It takes the state and action and returns the probability of taking an action in that state. The goal of reinforcement learning is to learn the optimal policy \u03c0* to maximize the cumulative discounted future reward. To learn the optimal policy, we use action-value function (Q-value function [10]) denoted by $Q^{\\pi}(s_t, a_t)$. The action-value function describes the value of taking an action at in state St while following a certain policy \u03c0. It is the expected discounted accumulative reward given the state and action under policy \u03c0.\n$Q^{\\pi}(s, a) = E_{\\pi}[R_t | S_t = s, a_t = a]$ (3)\nThe Bellman equation is used to find the optimal action-value function. Bellman equation decomposes the action-value function into immediate reward and discounted Q-value of successor states. Mathematically it can be written as:\n$Q^{\\pi}(S_t, a_t) = E_{\\pi}[R_t + \\gamma max_a Q(S_{t+1}, a)]$ (4)\nIf the action-value function is correctly estimated, then the greedy policy becomes the optimal policy and we choose the action with a larger Q-value.\n$\\pi^* = argmax_\\pi Q^{\\pi}(s, a)$ (5)"}, {"title": "III. LITERATURE REVIEW", "content": "Traffic flow optimization is the one of the most active research areas in a transportation system. Researchers around the world are trying to solve this problem for more than three decades. For the first time, Webster [11] try to solve the problem by using a mathematical approach. It gave the equation to find the optimal phase time. Webster's theory faded as the traffic at the intersection increased and it is modified by Robertson [12].\nWith the advancement in sensor and computer technology, better solutions like fuzzy control [13] [14] and genetic algorithm [15] [16] were introduced in traffic control system. These solutions derive from historical data and their performance decreases as the traffic behavior changes with time. In recent years, reinforcement learning [17] [18] has gained researchers importance because of its learning capability. Reinforcement learning approaches used from 1997 to 2010 are summarized by El-Tantawy [19]. These approaches used Q-table [10] and linear function to find Q-values.\nLi et al. [6] has used deep stacked autoencoder (SAE) network to find out the action in current state. Their algorithm takes queue length as input and queue difference between north-south traffic and east-west traffic as a reward. This algorithm has only two actions with no left and right turns. They compared the performance of their algorithm with conventional traffic light control and shown that their algorithm reduces 14% average traffic delay time. However, they did not explain the way of updating network parameters which is important for the stability of the algorithm.\nGenders et al. [8] proposed a deep reinforcement learning algorithm in which a convolutional neural network is used to find out the optimal Q-values. They define the state by combining the vehicle position matrix, vehicle velocity matrix, and most recent state of the intersection. The vehicle position matrix is obtained by dividing the road into small segments, representing the presence of a vehicle in it. For the vehicle velocity matrix, vehicle speed was normalized with a speed limit. Such type of state representation is hard to find out in the real-world application. They used four different evaluation metrics and showed that their algorithm performs better in three evaluation metrics. They found queue length as a scalar number by taking an average of queue length for all four sides of the intersection. This number is not a true representation of algorithm performance. For example, in the case, where high traffic is found at the north-south side of the intersection as compare to the east-west side of the intersection, the algorithm performs well on the east-west side but bad on the north-south side. If we take the average queue length for all sides of the intersection, then overall performance might be good, but it does not show that at which side of the intersection algorithm performs badly.\nGao et al. [20] proposed a reinforcement learning algorithm that performs better than the previous ones [6] [8]. Their algorithm automatically extracts the useful features from raw traffic data and learns the optimal policy. They define two actions for reinforcement learning agent. One action opens the signal for east-west traffic and other action opens the signal for north-south traffic. They did not define specific action for left and right turns. Such type of small action space work well for some specific region but lacks global application. In a simulation, they generated more traffic at the east-west side of the intersection than at the north-south. This type of traffic behavior is one of the traffic scenarios that mostly occur in real-world traffic scenarios. They evaluate the performance of the algorithm by calculating the delay time of vehicles that are stationary at all sides of the intersection. They have shown that their algorithm performs better than conventional traffic light control system.\nIn recent research, most of the work is done on finding the optimal phase for the intersection but less work is done on finding the phase duration of intersection. Liang et al. [21] control the signal duration in every cycle based on the information gathered from the vehicular network. They implement this idea by using dueling network, double Q-learning network, and prioritized experience replay. They obtained the state by dividing the intersection into square-shaped grids. Every vehicle is registered in a state when it is found in these grids. In real-world applications, vehicles have different lengths. There is a possibility that one long vehicle will be resisted in two grids. They define the action as an increment or decrement of time duration in all four phases of intersection. This time duration is selected based on the current state of intersection and remains fixed for the next complete phase cycle. Instead of using this selection technique, there is also the possibility exist that the phase time duration of one side of the intersection is selected by using the current state of that side of the intersection.\nSome researcher [7] [9] have used raw features instead of using human crafted features for state. These features were obtained in the form of images which were the snapshots of the current state of the intersection. To observe the motion, four images were stacked together to represent the state. Practically, such state can be obtained by an overhead camera using a drone. Such type of arrangements is difficult to make in a real-world application.\nIn contrast to all the previous work done, our algorithm takes a state that is possibly obtained in a real-world application. We evaluate the performance of the algorithm by using seven different evaluation metrics. The recent researches focused on either phase selection or phase duration algorithms, with no comparison between these two but in our study, we have used both of them and found the best for practical application."}, {"title": "IV. SYSTEM DESCRIPTION", "content": "The state of agent represents the condition of the envi- ronment at a specific interval of sampling time ts and it is denoted by Sts. The design of state is very important in reinforcement learning because it has a huge impact on reinforcement learning agent performance. In this research, the state is represented by queue lengths at an intersection. Queue length is a scalar number and it is obtained by counting the total number of stationary vehicles at the intersection. In the case of a turn-based agent, a state is formed by obtaining queue length from all sides of the intersection. The number of queue lengths obtained from the intersection depends upon the sides of the intersection. For example, in the case of a four-way intersection, four queue lengths are obtained to form a state of intersection. In a time-based agent, the state is the queue length of one side of the intersection.\nIn simulator, the state is obtained by counting all the stationary vehicles at the intersection. Vehicles that have a speed less than 1 m/s are considered stationary. The obtained state is a scalar number and cannot be used for the training of reinforcement learning agent. In order to learn the relationship between state and action, a deep neural network is required. It is not possible to train the deep neural network using scalar inputs because the input dimension is very low and the neural network does not learn from it. The solution to this problem is to use human-crafted features that increase the input dimension and help in training. In this research, binary encoding is used to increase the dimension of the state. The encoding has two key elements, encoding size, and encoding weights. Encoding size represents the number of cells in the encoding matrix and encoding weights represent the minimum number of vehicles required to fill 1's in the encoding matrix cell. The way to fill the encoding matrix is given in Algorithm 1. It is the pseudo-code that explains the steps to fill the encoding matrix. The Fig. 2 shows the encoding matrix with encoding weights per cell. This encoding matrix can handle up to 304 vehicles at"}, {"title": "A. State Representation", "content": "one side of the intersection. It means that if the queue length of one side of the intersection reaches up to 304 vehicles then every cell of the encoding matrix corresponds to that side, will be filled with 1's or if there is no vehicle then every cell of the encoding matrix will be filled with O's.\nLet's take an example in which the queue length at one side of the intersection is 5 then the first column and first cell of the second column will be filled with 1's and the remaining all cells will be filled with 0's. From this example, it is observed that the first's columns of the encoding matrix are filled first. The reason for this filling is that in real-world traffic scenarios every driver gives priority to fill empty spaces at the intersection first. In the case of a turn-based agent, encoding matrices for all sides of the intersection are obtained and concatenate to form a single vector that represents the state of the intersection and can be used to train the agent."}, {"title": "B. Action Space", "content": "In order to guide the vehicle through the intersection, reinforcement learning agent has to choose appropriate action according to the current state. In the case of a turn-based agent, action is the possible phase that is available, but in the case of a time-based agent, action is the time duration of the green phase.\nThe possible actions for turn-based agent are given in (6). NG (North-Green) allows vehicles to pass from North to other directions (E, W, and S) and indicates the vehicles on East, West, and South routes should stop and not proceed through the intersection. In the same way, WG (West-Green), EG (East-Green), and SG (South-Green) allows vehicles from West, East and South respectively to pass to other directions and stopping the proceeding of vehicles from these other\n$A_{turn} = \\{NG, WG, EG, SG\\}$ (6)\nThe time-based agent selects phase duration instead of phase. Its phase cycle is fixed but the time duration of the phase change according to the state of the environment. The possible actions of time-based agent are given in (7). Where 0 means no increment in green light base time and 10 means ten seconds are added to green light base time. Where green light base time is 15 seconds, so possible minimum time duration for a phase is 15 seconds and the maximum time duration for a phase is 34 seconds.\n$A_{time} = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 \\newline 11, 12, 13, 14, 15, 16, 17, 18, 19\\}$ (7)\nReinforcement learning agent chooses action on sampling timestep ts. Once the agent chooses the action, then next time it will be able to choose action on sampling time ts + 1. In a conventional traffic light control system, the transition phase will occur after every green phase. The transition phase activates the yellow light on the intersection for 4 seconds. In a turn-based agent, the transition phase will occur if the phase chosen by an agent is different from the previous phase. For example, the action taken by the agent at sampling timestep ts is ao and if it is the same as it was taken at the previous timestep ts - 1 then there is no transition phase. After the completion of action ao, the agent selects a new action at sampling timestep ts + 1. Let us consider that this time agent selects action a3, and it is different from action taken at timestep ts. In this case, the transition phase of 4 seconds will occur. The timeline of turn-based agent action is shown in Fig. 3. In the case of a time-based agent, the transition phase will always occur just like conventional traffic light control. The timeline of the time-based agent's action is shown in Fig. 4."}, {"title": "C. Reward", "content": "The reward is the feedback that the reinforcement learning agent obtains from the environment after taking an action. Reward tells the agent; how good or bad action is? The agent uses this information to update its model for future action choices. The reward is either positive or negative. Positive reward indicates that the action chosen by the agent helps in reducing traffic congestion. A negative reward indicates that the chosen action has increased the traffic conjunction.\nOur main objective is to improve the efficiency of intersection by reducing the wait time of vehicles. Thus, we define the reward as the difference between awt (accumulative wait time) of all vehicles obtained at two different sampling timestep, i.e.\n$r(ts) = awt(t_{s-1}) - awt(t_s)$ (8)\nwhere awt is accumulative wait time. The awt is obtained by adding the wait time of all vehicles that are stationary at all sides of the intersection. The reward rts is obtained by taking the difference between accumulative wait time at previous timestep awtts-1 and at current timestep awt(ts). The equation (8) is designed in such a way that if an agent takes good action it will return a positive reward and if bad action is taken by then it returns a negative reward."}, {"title": "D. Policy", "content": "The policy is a function that maps the given state of the environment to the corresponding action. The agent's objective is to reduce the wait time of vehicles in the long run. So, the agent observes the current state of the environment and takes an action according to policy \u03c0 and gets a reward. In order to reduce the wait time, the agent chooses an action that maximizes immediate reward r(ts). The agent must find optimal policy \u03c0* that maximizes the following accumulative reward.\n$Q^{\\pi}(s, a) = E[r_{t_s} + \\gamma r_{t_s+1} + \\gamma^2 r_{t_s+2}+...]$ (9)\n$= E[\\sum_{k=0}^{\\infty}\\gamma^k r_{t_s+k}]$\nWhere \u03b3 is a discount factor having range 0 \u2264 \u03b3 \u2264 1. It reflects that how much importance the agent gives to future reward. If \u03b3 = 0 then it gives no importance to future reward and if \u03b3 = 1 then it gives more importance to future reward.\nTo find the policy, we use a deep neural network as a function approximator and its parameters o refers to policy parameters. The policy distribution $\u03c0(a_{ts}|s_{ts};\\theta)$ is learned by experience replay. We store the tuple ($s_{ts}, a_{ts}, r_{t_s}, S_{t_s+1}$) in memory at every sampling timestep. After few episodes of simulation, we have enough data to train a deep neural network. We fetch a batch of 64 examples and train our deep neural network. After few hours of training, the parameters of the deep neural network reach the optimal location. The policy with optimal parameters becomes the optimal policy $\u03c0(\u03b1 | s; \u03b8^*)$.\n$\\pi (\\alpha | s; \\theta^*) = argmax_{\\pi} Q (s, a) \\qquad \\forall s \\in S, a \\in A$ (10)"}, {"title": "V. EXPERIMENT RESULT", "content": "We use Simulation of Urban Mobility (SUMO) [22] traffic simulator in our experiment. SUMO is an open-source microscopic simulator and provides every needed information about vehicles and networks. The detail of simulation setting is described in following sections.\n1) Intersection: The intersection geometry used in our experiment is shown in Fig. 5. It is a four-way (four incoming roads and four outgoing roads) intersection with four lanes on each road. We have set the left-hand drive as a driving rule. So the left-most lane allows left-turn, the middle two lanes are the through-only lanes and the right-most lane allows only right-turn. The length of roads towards and away from the intersection is set to 750 meters. The attributes of the vehicle used in the simulation are listed in Table I.\n2) Phase: A traffic light that is part of this experiment has eight phases listed in Table II. Both conventional traffic light control and time-based agent pass through all these phases one by one. After 8th phase, they loop back to 1st phase. At that point, one phase cycle is completed. The turn-based agent does not follow the phase cycle. It selects the phase according to traffic conditions at the intersection. The turn-based agent can only control 1, 3, 5, and 7 phases. The phase duration is a time period for which the phase remains active on the intersection. In the case of conventional traffic light control and turn-based agent, the time duration remains fixed but in the case of a time-based agent, it can be changed. The time-based agent can only change the phase duration of 1, 3, 5, and 7 phases.\n3) Traffic Generation: Traffic in the network is generated randomly by using Weibull distribution. The reason for using this distribution is that in the real world, traffic flow increases rapidly and decreases slowly. We have introduced four different traffic scenarios: low, high, east-west (EW), and north-south (NS). In the case of low and high traffic scenarios, an equal number of vehicles are generated in all directions. In the"}, {"title": "A. Experiment setup", "content": "EW scenario, a higher number of vehicles are generated on the East and West sides and fewer vehicles are generated on the North and South sides and vice versa in the case of the NS traffic scenario. The number of vehicles that are generated in each traffic scenario is listed in Table III. 60 % of the generated vehicles go straight and the remaining 40 % of vehicles go either left or right. Once we know the origin and destination of vehicles, a path planning algorithm A* is used to find the optimal path."}, {"title": "B. Deep neural network design", "content": "Reinforcement learning agents require a deep neural network as a function approximator to learn the relationship between state-action pairs. The deep neural network has 5 hidden layers, denoted by hl. All the hidden layers are fully connected and used rectified linear units as activation functions. The number of nodes in the hidden layer can vary. Fig. 6 shows the proposed architecture design of deep neural network. The hl1, hl2, hl3, hl4 and hl5 have 512, 512, 512, 256 and 128 nodes, respectively. The deep neural network has one input and one output layer. The input layer takes a state of the environment and the output layer returns the Q-value of actions. The number of nodes in input and output layers can vary according to the type of agent that is used. In the case of turn-based, the number of nodes in the input layer is equal to the product of the size of encoding matrix and sides of intersection and the number of nodes in the output layer is equal to the size of action space Aturn. In the case of a time-based agent, the input layer nodes are equal to the size of the encoding matrix and output layer nodes are equal to the size of action space Atime. The output layer of the neural network is fully connected and has a linear activation unit."}, {"title": "C. Performance measure", "content": "The performance measure is a criterion through which an agent's performance is evaluated. It defines how good or bad agent perform in a simulated environment. Instead of using a single evaluation metric, seven evaluation metrics are introduced to compute the reinforcement learning agent's performance. The evaluation metrics are described in detail in the following sections.\n1) Total Negative Reward: As it is mentioned earlier that we can get either negative or positive reward from reward function, we accumulate negative reward to see the agent's performance. The Total Negative Reward (Tnr) is the sum of all the negative rewards obtained from the reward function during one episode of simulation. It is obtained at the end of each episode and the reinforcement learning agent tries to maximize it.\n$Tnr(e) = \\sum_{t_s=1}^m min(0, r(t_s))$ (11)\nwhere ts is the sampling timestep, m is the total number of sampling timesteps in one episode, e is the episode number, r(ts) is the reward at sampling timestep ts and Tnr(e) is the total negative reward for episode e.\n2) Total Accumulative Wait Time: Wait Time (wt) is the sum of wait for individual vehicle c that is on the incoming road rd at a specific interval of sampling timestep ts. If we add up wt for all vehicles in the network at ts then we get an accumulative wait time awt(ts).\n$awt (t_s) = \\sum_{c=1}^n [rd(c) wt(s)]$ (12)\nwhere n is the total number of vehicles in a network and rd(c) is explained below:\n$rd(c) = \\begin{cases} 1 & \\text{if vehicle on incoming road} \\\\ 0 & \\text{else} \\end{cases}$\nTotal Accumulative Wait Time (Tawt) is the sum all aut(ts) for each episode e. It is calculated by using (13).\n$Tawt(e) = \\sum_{t_s=1}^m awt (t_s)$ (13)\n3) Expected Wait Time per Vehicle: Expected Wait Time per Vehicle (ewpv) is the average wait time wtavg that individual vehicle c can face when it tries to pass through the intersection. It is calculated at the end of each episode e.\n$ewpv (e) = \\sum_{c=1}^n [\\sum_{t_s=1}^m  rd(c) wt(c)]$ (14)\nIt is the most important metric to check the performance of the reinforcement learning agent. The lower value of ewpv (e) leads to a good performance of agent while higher value leads to poor performance.\n4) Average Queue Length: Queue Length (ql) is the number of vehicles that are stationary at the intersection. The vehicles with speeds less than 1 m/s are considered stationary and counted in queue length. The queue length is calculated at every sampling timestep by using the following equation:\n$ql(t_s) = \\sum_{c=1}^n rd(c) [1-floor \\frac{sgn (\u03c5_{P(t_s)}^c(t_s)) + 1}{2}]$ (15)\n$P(t_s)^c = \\begin{cases} 1 & \\text{if } v > 0 \\\\ 0 & \\text{if } v = 0 \\\\ -1 & \\text{if } v <0 \\end{cases}$\nwhere floor is a math function that is used to round down the values and sgn is the signum function. The Average Queue Length (aql) is the mean of ql at the end of each episode e. For each compass direction of intersection, it is calculated by using (15). The lower value of aql(e) leads to a good performance of agent while higher value leads to poor performance.\n$aql(e) = \\frac{\\sum_{t_s=1}^m ql(t_s)}{m}$"}, {"title": "D. Training", "content": "Both turn-based and time-based agents are trained for 300 episodes. Training more than these episodes do not affect the performance of agents. Each episode corresponds to 5400 seconds of simulation. To prevent overfitting, we shift the traffic scenario after each episode. In the training phase, the agent's main objective is to find the best action on the given state. In the early stage of training, the agent does not know which action is the best. So, the agent does not care about the performance and takes random action to explore the state-action space. As the training goes on, the agent gains knowledge about state action pair and reduces its exploration. Now, the agent increases the frequency of exploitative action to increase the performance. The probability to choose random action is defined by epsilon & also known as exploration rate. At the start of training exploration rate is high that is why the agent takes random action and at the end of the training, the exploration rate is low and forces the agent to take exploitative action. We used non-linear function given in (16) to model the exploration rate.\n$\\epsilon = \\begin{cases} 1 & e < 90 \\\\ 1-\\frac{(e-90)}{90} & 90 < e < 210 \\\\ 0.2-\\frac{(e-210)}{90} & 210 < e < 300 \\end{cases}$ (16)\nExploration of state-action space is very important for the stability of training that is why (16) allows the agent to explore the state-action space for the first 90 episodes. After that epsilon decreases rapidly till 210 episodes, forcing the agent to take more valuable actions. From 210 to 300 episodes epsilon decreases slowly to allow the agent to take some random actions even at the end of training.\nTo train the deep neural network, training data is required. Training data is quadruple that contains state, action, reward, and next state. This training data is stored in memory called experience replay memory. The maximum storage capacity of memory is 50,000 training examples. When memory becomes full then old training examples are removed, and new training examples are added. At the start of traffic simulation, there is no training example. As the simulation goes on, new training examples are added into memory at every sampling timestep ts. The training examples in memory are in raw form and need to be processed for deep neural network training. The training examples are processed according to the input and output requirements of a deep neural network. The input of"}, {"title": "Algorithm 2 Reinforcement learning algorithm with experience replay.", "content": "neural network is a state which is easily obtained from training example and output is Q-values that are obtained after using Bellman equation. The batch of a random sample of training examples is obtained from memory and processed using the Bellman equation. The Q-value obtained from the Bellman equation is called target Q-values. These target Q-values are compared with Q-values predicted by the deep neural network to compute the difference. This difference is considered as an error and can be used to update the parameters @ of a deep neural network using Adam optimizer. The hyper-parameters used for training are learning rate and batch size and they are set to 0.001 and 64 training examples, respectively. The Algorithm 2 is the complete pseudo-code of reinforcement learning agent training.\nTurn-based agent training result in term of reward and average queue length are shown in Fig. 7a and Fig. 7b respectively and time-based agent training results are shown in Fig. 7c and Fig. 7d. The training plots show spikes at the start of training. These spikes are considered normal in the"}, {"title": "E. Result", "content": "to evaluate the performance of proposed agents", "Agent": "Table IV shows the evaluation result of turn-based agent on four different traffic scenarios. Columns with traffic scenarios header represent the result of each evaluation metrics and with % header represent the percentage improvement that turn-based agent has made from conventional traffic light control system. The percentage improvement columns are highlighted with green and red colors. Green color means the agent performs well and there is improvement in the performance while red color means that the agent performs badly and has reduced the performance. In summary, the turn-based agent performs well in low traffic scenario, bad in high traffic scenario, and normal in EW and NS traffic scenarios. In the case of the EW traffic scenario, most of the traffic coming from the east and west direction, so the agent gives importance to these sides. The same phenomenon occurs in the case of the NS traffic scenario where it gives more importance to the north and south direction and less importance to the east and west direction. Such type of behavior indicates that the turn-based agent becomes greedy and gives importance to that side of the intersection that has a large"}]}