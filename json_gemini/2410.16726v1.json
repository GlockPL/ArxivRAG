{"title": "Enhancing Low-Resource ASR through Versatile TTS: Bridging the Data Gap", "authors": ["Guanrou Yang", "Fan Yu", "Ziyang Ma", "Zhihao Du", "Zhifu Gao", "Shiliang Zhang", "Xie Chen"], "abstract": "While automatic speech recognition (ASR) systems have achieved remarkable performance with large-scale datasets, their efficacy remains inadequate in low-resource settings, encompassing dialects, accents, minority languages, and long-tail hotwords, domains with significant practical relevance. With the advent of versatile and powerful text-to-speech (TTS) models, capable of generating speech with human-level naturalness, expressiveness, and diverse speaker profiles, leveraging TTS for ASR data augmentation provides a cost-effective and practical approach to enhancing ASR performance. Comprehensive experiments on an unprecedentedly rich variety of low-resource datasets demonstrate consistent and substantial performance improvements, proving that the proposed method of enhancing low-resource ASR through a versatile TTS model is highly effective and has broad application prospects. Furthermore, we delve deeper into key characteristics of synthesized speech data that contribute to ASR improvement, examining factors such as text diversity, speaker diversity, and the volume of synthesized data, with text diversity being studied for the first time in this work. We hope our findings provide helpful guidance and reference for the practical application of TTS-based data augmentation and push the advancement of low-resource ASR one step further.", "sections": [{"title": "I. INTRODUCTION", "content": "Although ASR technology has seen remarkable advancements, achieving near-perfect word error rates (WER) on high-quality, large-scale datasets, ASR still faces significant challenges when applied to low-resource datasets [1]. These datasets encompass accented speech, regional languages or dialects, minority languages, and long-tail hotwords, which are of critical practical value.\nImproving ASR models on low-resource datasets has been extensively researched. One widely used approach is self-training, where an ASR model, trained with limited human-transcribed data, is utilized to generate transcriptions, which are then used to train the ASR system further together with original annotated data [2], [3]. While this method can enhance ASR performance, it has several limitations. For certain low-resource languages, the scarcity of textual data, let alone speech resources, renders the use of unlabeled speech for pseudo-labeling more of an idealized assumption than a practical solution. Besides, the model employed for labeling has to exhibit ideal performance to produce high-quality labels, which necessitates training with substantial data, making this approach somewhat circular. Furthermore, self-training typically requires multiple iterations to refine the ASR model and label quality continuously, leading to increased costs and extended experimental cycles. Moreover, the iterative self-training approach may prove less efficient, with minor performance gains compared with alternative methods, such as augmenting training data with TTS systems [4].\nAnother promising approach is utilizing TTS systems to generate synthetic speech for data augmentation [5]. This method stands out for its cost-effectiveness and practicality, requiring only textual data, which is generally more accessible than speech data. Even in cases"}, {"title": "II. RELATED WORKS", "content": "Previous research has explored TTS data augmentation in various domains, such as ASR for children's speech [16], conversational speech [17], minority language speech [18], dysarthic speech [19] and other tasks such as speech emotion recognition [20], [21] and keyword spotting [22].\nBeyond demonstrating the effectiveness of this approach across different speech tasks and diverse domains, a series of works delve deeply into more efficient ways of leveraging the imperfect TTS data, addressing the distribution gaps and domain mismatches between synthetic and real data. For instance, Ma et al. [20] introduce four distinct methods for integrating TTS data with real data including curriculum learning and transfer learning. Rolland et al. [16] validate the effectiveness of adapter-based architectures in using imperfect TTS data. Hu et al. [23] propose a rejection sampling algorithm and utilize separate batch normalization statistics for real and TTS data. Liu et al. [24] select synthetic speech dissimilar to real speech via a neural network, effectively boosting recognition performance.\nSince our approach of fine-tuning CosyVoice and a straightforward combination of real and TTS data achieves superior results without employing complex strategies, we focus on a different question: which characteristics of TTS data are most beneficial for augmenting ASR training? Several existing studies have provided preliminary answers and insights. [25] discover that the NISQA MOS [26] and synthetic WER have no clear relations to the ASR performance, aligning with observations in [12] that lower WER typically indicates more standard speech easier for recognition but lose distinctive qualities like strong accent or high expressiveness and may not be helpful for ultimate ASR training. [27] proposes a speaker augmentation approach for TTS data augmentation to improve speaker diversity of synthetic speech. Experiments in [11] reveal that text diversity is more critical than the duration of speech for the ASR task. Inspired by these related works, we posit the diversity of synthesized speech is a pivotal factor influencing the ultimate ASR performance and endeavor to investigate the impacts of text diversity and speaker diversity on the efficacy of TTS data augmentation, alongside the influence of the quantity of synthetic data employed."}, {"title": "III. METHODS", "content": "Cosy Voice is a scalable multilingual zero-shot TTS system that builds upon supervised semantic tokens derived from a multilingual ASR model. Unlike conventional unsupervised methods, which lack explicit alignment between text and speech, CosyVoice employs supervised tokens to enhance content consistency and speaker similarity.\nIn addition to the enhanced content consistency and speaker similarity through the use of supervised semantic tokens, Cosy Voice leverages x-vectors [28] to disentangle the multifaceted aspects of speech, including semantic content, speaker identity, and prosody. This enables the model to capture subtle details like speaker timbre and environmental factors, producing more natural and diverse speech. Consequently, Cosy Voice is capable of synthesizing speech with a wide range of voices and speaking styles, a feature critical for developing robust ASR systems."}, {"title": "B. Proposed Pipeline", "content": "We propose a succinct three-stage pipeline for leveraging TTS-generated data to augment ASR training datasets, as illustrated in Figure 1. In the first stage, we fine-tune the Cosy Voice model with limited ASR training data, specifically adjusting its language model and conditional flow matching model separately. The fine-tuning step tailors the pre-trained CosyVoice model to better capture the acoustic and linguistic characteristics of the low-resource target data of real-world scenarios. In the second stage, for each text input, we generate synthetic speech by applying a different speaker vector, effectively creating diverse utterances with various speakers. In the third stage, this enriched synthetic data is integrated with the original training data to create a more comprehensive and diverse dataset for follow-up ASR training. We employ the standard Conformer [29] ASR model to evaluate the quality and effectiveness of the generated speech."}, {"title": "C. Model Configuration and Training", "content": "We utilize the open-source CosyVoice-base-300M model as our pre-trained model, which is trained on a large-scale multi-lingual dataset comprising 130k hours of Mandarin, 30k hours of English, 5k hours of Cantonese, 4.6k hours of Japanese, and 2.2k hours of Korean content. The two integral components of CosyVoice, the LLM and the conditional Flow Matching Model, can be fine-tuned separately. The text encoder is composed of 8 Transformer layers. The LLM has a similar structure but a deeper architecture, consisting of 16 Transformer layers and an additional linear output decoder layer. The text encoder and the LLM have a total of 343.20 million parameters. The speech encoder includes 6 Transformer layers, while the conditional flow matching model primarily comprises ResNet blocks and Transformer blocks. The decoder also includes Conv1D layers as downsampling and upsampling layers to adjust the resolution of the input features. The combined speech encoder and conditional flow matching model consist of 104.87 million parameters. The speech tokenizer is built on the proprietary SenseVoice-Large ASR model [30], [31], which places a vector quantizer after the first six encoder layers, using a single 4,096-entry codebook.\nFor fine-tuning these two modules, we use AdamW as the optimizer with betas set to (0.9, 0.999) and a weight decay of 0.01. The"}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Experimental Setup", "content": "1) Dataset: For the accented English data, we employ the Common Voice English dataset [33]. We mainly utilize the eight high-resource accents in this dataset, including Australia, Canada, England, India, Ireland, New Zealand, Scotland, and the United States. The training set split follows the same procedure as [34].\nThe remaining datasets are in-house annotated data within the industry. For Chinese dialects, the Wu dataset is collected mainly from Suzhou, Hangzhou, and Shanghai. About 600 speakers participate in the recording process, consisting of equal numbers of male and female speakers. The recordings are conducted in both quiet and noisy environments to ensure robustness across different acoustic conditions. The recorded texts encompass a variety of everyday conversational topics, readings from news, and social media texts such as tweets, providing diverse linguistic contexts. Similarly, the Min Nan dialect dataset is compiled with a comparable methodology. The hotwords dataset is collected from real-world conversational data related to the automotive industry. It features a wealth of hotwords, specifically automotive terminology encompassing vehicle models and configurations, alongside sales and financial terms. The Korean dataset is collected from multiple domains, including literature, education, news, sports etc."}, {"title": "B. Experimental Results", "content": "To evaluate the impact of TTS-generated data augmentation on ASR training, we conduct experiments across a variety of low-resource datasets, including different languages and dialects. The results are summarized in Table I. For accented English, we use a dataset containing 300 hours of real English speech. Without TTS augmentation, the ASR system achieves a WER of 12.61. When 300 additional hours of TTS-generated data are incorporated, the WER is reduced to 10.69. For minority languages, we perform experiments on Korean. Similarly, for Korean, the CER decreases from 8.48 to 6.31 when 4,000 hours of TTS-generated speech is added to the original 3,700 hours of real speech. In the case of Chinese dialects, we experiment with Min Nan and Wu. For Min Nan, using 200 hours of real speech results in a high CER of 55.42. Augmenting the training set with an additional 200 hours of TTS-generated data leads to a substantial WER reduction to 30.00. For Wu, a similar pattern is observed, with the CER dropping from 27.47 to 19.93 when TTS data is included. Lastly, for a Chinese hotword dataset with 300 hours of real speech, the ASR system initially achieves a CER of 21.32. After augmenting the data with an additional 300 hours of TTS-generated speech, the CER is reduced to 13.15.\nThe results demonstrate that TTS-generated data can effectively"}, {"title": "C. Ablation Study", "content": "To delve deeper into which characteristics of synthesized speech play a role in enhancing ASR training, we specifically examine the influence of the quantity, speaker diversity, and text diversity of generated speech, on the ultimate ASR performance. The experimental results are respectively presented in Tables II, III, IV.\nTo explore the impact of augmenting ASR training with different amounts of TTS-generated data, we synthesize varying durations of Korean speech. The experimental results from table II demonstrate that increasing the amount of TTS-augmented data consistently improves ASR performance, as indicated by the decreasing CER. The greatest improvement occurs when comparing the baseline with no TTS augmentation (8.48 CER) to the model trained with 5000 hours of TTS data (6.18 CER). However, the performance gains begin to converge as the amount of TTS data increases, indicating that the benefits may taper off after a certain point. Consequently, generating an amount of TTS data equivalent to real data may suffice, as additional production incurs higher costs with marginal improvement."}, {"title": "V. CONCLUSION", "content": "In this work, we emphasize that the rapid advancements in TTS technology, especially the rise of large-scale, versatile models, have opened up unprecedented opportunities for enhancing ASR systems in low-resource settings. The improved synthesis quality, diversity, and controllability of modern TTS models have transformed the landscape, offering new avenues for addressing the persistent chal-lenges of lacking training data in ASR research. Through extensive experiments, we observe substantial performance gains across various datasets by generating diverse and high-quality synthetic speech with CosyVoice. Key factors such as text diversity, speaker diversity, and the volume of synthesized data are systematically analyzed, revealing that text diversity plays a more critical role than speaker diversity in enhancing ASR performance, and generating a moderate amount of TTS data, comparable to the original data provides the highest cost-efficiency. These findings underscore the potential of advanced TTS models as a practical and efficient solution to the persistent challenges of low-resource ASR development and provide helpful insights for future research and real-world applications."}]}