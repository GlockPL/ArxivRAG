{"title": "3D Priors-Guided Diffusion for Blind Face Restoration", "authors": ["Xiaobin Lu", "Xiaobin Hu", "Jun Luo", "Ben Zhu", "Yaping Ruan", "Wenqi Ren"], "abstract": "Blind face restoration endeavors to restore a clear face image from a degraded counterpart. Recent approaches employing Generative Adversarial Networks (GANs) as priors have demonstrated remarkable success in this field. However, these methods encounter challenges in achieving a balance between realism and fidelity, particularly in complex degradation scenarios. To inherit the exceptional realism generative ability of the diffusion model and also constrained by the identity-aware fidelity, we propose a novel diffusion-based framework by embedding the 3D facial priors as structure and identity constraints into a denoising diffusion process. Specifically, in order to obtain more accurate 3D prior representations, the 3D facial image is reconstructed by a 3D Morphable Model (3DMM) using an initial restored face image that has been processed by a pretrained restoration network. A customized multi-level feature extraction method is employed to exploit both structural and identity information of 3D facial images, which are then mapped into the noise estimation process. In order to enhance the fusion of identity information into the noise estimation, we propose a Time-Aware Fusion Block (TAFB). This module offers a more efficient and adaptive fusion of weights for denoising, considering the dynamic nature of the denoising process in the diffusion model, which involves initial structure refinement followed by texture detail enhancement. Extensive experiments demonstrate that our network performs favorably against state-of-the-art algorithms on synthetic and real-world datasets for blind face restoration.", "sections": [{"title": "1 INTRODUCTION", "content": "Blind face restoration is a long-standing vision task that involves recovering a high-quality face image from a low-quality observation. It plays an important role in old photo recovery and face recognition. As an ill-posed problem, this task is highly challenging since low-quality face images can suffer from multiple degradations, such as downsampling, blurring, noise, and compression.\nMultifarious traditional end-to-end methods have been proposed based on convolutional neural networks (CNNs) to learn mapping relationships between low-quality and high-quality image pairs [6-10, 36], but they fail to restore fine details on the face [5, 32]. Recently, due to the powerful ability of Generative Adversarial Networks (GANs) [13] to generate realistic face images, some methods [14, 50, 55] have utilized GANs as a prior for generating high-quality faces. These methods extract low-quality face features using CNNs and encode them into the latent space of the GAN. However, methods employing GANs as priors may encounter training collapse [53], a consequence of challenges in optimizing the objective function during training. To improve the identity consistency of the restored images, some methods incorporate prior information during the face restoration process, such as facial geometric priors [4, 21] and reference priors [28, 29]. For example, Gu et al. use vector quantization face Restoration (VQFR) [16] to guide the model for generating more realistic facial details by storing the features extracted from high-resolution images in a dictionary. Some methods [47, 54] leverage the powerful feature extraction capability of Vision Transformer (ViT) [43] to achieve better restoration results. However, as shown in Fig. 2, the restoration results of these models cannot achieve a balance between fidelity and authenticity when dealing with complex scenes or under specific conditions. These approaches continue to face challenges in accurately restoring intricate facial features while simultaneously capturing realistic textural qualities, as exemplified in Fig. 1.\nMore recently, denoising diffusion models have been introduced into image restoration. For example, [39] feeds the combination of low-quality and noisy images into the denoising diffusion model for noise prediction. [37] encodes the low-quality image once and passes it through a cross-attention mechanism, feeding the features into a diffusion model. [31] utilizes a parallel encoding module to encode the condition information and inputs it into the decoder of the denoising diffusion model at each step of the denoising process. Latent diffusion [37] learns the data distribution in the latent space by gradually diffusing the noise signal and completes the guidance by applying cross attention to the conditions in the denoising network. However, throughout the denoising diffusion process, the low-frequency components exhibit gradual changes as the time step progresses, whereas the high-frequency components undergo more pronounced alterations [41]. Besides, in the initial phase of the denoising process, the primary emphasis is on refining the structure of the restored image, while in the later stages, the main focus shifts to refining the intricate textures of the restored image [42]. To improve restoration quality, it is not sufficient to extract features on the guide image only once, nor is it adequate to rely solely on simple addition or cross-attention to fuse the features.\nDifferent from aforementioned methods, to ensure the fidelity, authenticity, and identity consistency of restored images in complex degradation scenarios, we propose a 3D prior-guided diffusion model by incorporating 3D facial prior information as constraints. Besides, we propose a multi-level feature extraction module to extract structural and identity information from 3D prior information at each time step and then weight-adaptively fuse this conditional guidance information with noisy image feature information through a Time-Aware Fusion Block.\nOverall, the main contributions of this paper can be summarized as follows:"}, {"title": "2 RELATED WORK", "content": "Blind face restoration\nAs a fundamental task in computer vision, blind face restoration (BFR) aims to recover a high-quality face image from its degraded counterpart in the presence of unknown degraded types and parameters. CNN-based approaches can learn mapping relationships between low-/high-quality image pairs from large-scale collected datasets, but they often fail to restore high-frequency texture details on the face. In recent years, significant progress has been made for BFR by using facial geometry priors and generative priors. Harnessing the specific structure and details of faces, techniques grounded in facial geometry priors utilize prior information such as parsing maps, landmarks, and reference images to enhance the restoration of facial images [3, 4, 12]. On the other hand, generative priors-based methods employ powerful generative models like StyleGAN [26] and incorporate adversarial training to enhance the visual quality of the restored images. Despite the fact that generative prior methods have the ability to restore facial details more effectively when compared to CNN-based restoration approaches, they may encounter significant challenges pertaining to training difficulties and model collapse problems [53].\nDiffusion model\nRecently, a majority of generative tasks in computer vision have been dominated by GAN-based methods [23, 40], which generate decent images through adversarial training. However, these methods may encounter challenges in training difficulties and model collapse [53]. With the application of diffusion models [17, 20, 34, 44] in the generative task domain, these models have demonstrated unprecedented generative capabilities in terms of image quality and diversity. Diffusion models have also been widely applied to various computer vision tasks, including image super-resolution [27, 37, 39], image inpainting [33], image segmentation [1, 2], image-to-image translation [38], text-to-image translation [15], and more. In the context of BFR, DiffBIR [31] Utilizes the pre-trained text-to-image diffusion model and adopts a multi-stage pipeline approach for image restoration. However, it is characterized by many parameters and relies solely on addition for feature fusion. To address the challenge of incorporating diverse degradations for real-world scenarios in training data, the Diffusion-based Robust Degradation Remover (DR2) [48] introduces a method that transforms degraded images into degradation-agnostic predictions before utilizing an enhancement module for high-fidelity image restoration.\nDenoising diffusion probabilistic models (DDPM) consist of forward and backward Markov processes. The forward process gradually adds random noise to the image, and we denote these latent variables as $x_1, ..., x_T$, where $x_T$ becomes a completely noisy image. The backward process of DDPM is a denoising process, where it learns a Markov chain that gradually transforms a simple noise distribution (such as isotropic Gaussian distribution) into the target data distribution. Throughout the entire forward and backward processes of DDPM, the dimensions of the image remain consistent. Each step of the forward process can be represented by the following equation:\n$q (x_t | x_{t-1}) := N (x_t; \\sqrt{1 \u2013 \u03b2_t}x_{t-1}, \u03b2_tI)$,\nwhere $\u03b2_1, ..., \u03b2_T$ are fixed variance values. At each step, Gaussian noise with variance $\u03b2_t$ is added, resulting in the final $x_T$ being mapped to pure Gaussian noise. Let $x_0$ be the original image, and it is possible to obtain the noisy image at any step $t$ based on $x_0$:\n$q (x_t | x_0) := N (x_t; \\sqrt{\\bar{\u03b1}_t}x_0, (1 \u2013 \\bar{\u03b1}_t) 1)$,\nwhere $\u03b1_t := 1 - \u03b2_t$ and $\\bar{\u03b1}_t := \u03a0_{n=1}^t \u03b1_n$. These parameters are predefined prior to training. DDPM achieves the denoising process by predicting the mean of the noise added from step $x_t$ to $x_{t\u22121}$:\n$p_\u03b8 (x_{t-1} | x_t) = N (x_{t-1}; \u03bc_\u03b8 (x_t, t), \u03c3_tI)$,\nwhere $p_\u03b8 (x_{t\u22121} | x_t)$ represents the backward process from $x_t$ to $x_{t\u22121}$, while $\u03bc_\u03b8 (x_t, t)$ denotes the diffusion model with parameter $\u03b8$. At step $t$, $x_{t\u22121}$ can be expressed by the predicted mean and $x_t$:\n$x_{t-1} = \\frac{1}{\\sqrt{\u03b1_t}}(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\u03b5_\u03b8(x_t, t))=\u03bc_\u03b8 (x_t, t) + \u03c3_t z,$\nwhere $z \u223c N(0, I)$ is a standard Gaussian noise and has the same dimensionality as noisy image $x_1, ..., x_T$. By performing denoising for $T$ steps, the pure noisy image can be transformed into the target data distribution."}, {"title": "3 PROPOSED METHOD", "content": "The overall framework for face restoration by incorporating 3D priors into a diffusion model is illustrated in Fig. 4. Our overall framework mainly consists of two branches, including the 3D reconstruction branch and the diffusion branch. The 3D reconstruction branch includes the SwinIR model and the 3DMM model, while the denoising diffusion branch mainly includes a U-net, a multi-level feature extraction module, and a Time-Aware Fusion Block (TAFB). The low-quality image is initially restored using the pre-trained restoration module, SwinIR, resulting in an initial face restoration result denoted as $x_{init}$. The 3D facial image is reconstructed using the D3DFR method [11]. The multi-level feature extraction module extracts identity and structural information features across different scales of the 3D facial image. These features are then input into the TAFB, in conjunction with features extracted from the noisy image and the current timestep t. Subsequently, the time-aware block fuses the features across various time steps and passes them to the subsequent feature extraction block. We will introduce the 3D reconstruction branch and the diffusion branch in detail.\nMotivation and novelty\nAlthough current diffusion-based blind face restoration methods have shown promising results in terms of image quality restoration, they often fail to ensure the identity consistency of the restored faces, as illustrated in Fig. 2. It is mainly due to the conflict and balance of realism and fidelity, particularly in complex degradation scenarios. To mitigate this problem, 3D facial priors as structure and identity constraints are embedded into a denoising diffusion process to keep the high fidelity while generating high-realism images. However, considering the dynamic nature of the denoising process in the diffusion model, which involves initial structure refinement followed by texture detail enhancement, we design ingenious modules to incorporate the 3D priors into the diffusion model. 1.) A customized multi-level feature extraction method is designed to fully exploit both structural and identity information of 3D facial images, which are then mapped into the noise estimation process. 2.) A Time-Aware Fusion Block (TAFB) is proposed to enhance the fusion of identity information into the noise estimation and offer a more efficient and adaptive fusion of weights for the denoising process. 3.) The features of 3D facial priors are multi-iteratively embedded into the denoising process at each step and avoid inadequate excavation of 3D facial priors.\n3D reconstruction block\n3D face priors encompass a wealth of hierarchical features, including low-level details such as sharp edges and lighting, as well as perceptual-level information related to identity. However, Low-quality images for blind face restoration often suffer from multiple complex types of degradation (e.g., blur, noise, JPEG compression artifacts, low resolution, etc.). So the input low-quality image $x_{Lq}$ is first processed by the pre-trained restoration module, SwinIR [30], to obtain the initial face restoration result $X_{init}$.\n$X_{init} = F_{SwinIR}(x_{Lq}),$\nThe outcomes obtained will be utilized as inputs for the diffusion model and the 3D Morphable Model (3DMM) [11] module. The low-quality image is processed using ResNet-50 [18] to obtain the 3DMM coefficients $z_{3d}$.\n$Z_{3d} = G_{Res50} (X_{init}),$\nwhere $z_{3d}$ is a 257-dimensional vector, denoted as $z_{3d} = (\u03b1, \u03b2, \u03b4, \u03b3, \u03c1)$, where $\u03b1, \u03b2, \u03b4, \u03b3$ and $\u03c1$ respectively represent the identity information, facial expression, texture, illumination [35], and facial pose.We define the 3D shape $S$ and texture $T$ of a face as follows:\n$S(\u03b1, \u03b2) = S + B_{id}\u03b1 + B_{exp}\u03b2$\nand\n$\u03a4(\u03b4) = \u03a4 + B_\u03b4,$\n$S$ and $T$ respectively represent the average facial shape and facial texture. The variables $B_{id}$, $B_{exp}$ and $B_t$ represent the principal component analysis (PCA) bases for identity, expression, and texture, respectively. The color information of 3D face, denoted as $C$, can be represented as:\n$C(i) = c_i (n_i, t_i, y) = t_i\\sum_{b=1}^{27} Y_b\u03b3_b (n_i),$\nThe 3D image is reconstructed using the D3DFR method, with coefficients $S$, $T$, and $C$. The preliminary restoration module and the 3D reconstruction module are not involved in the training process. As illustrated in Fig. 3, 3D faces reconstructed directly from low-quality images exhibit significant deviations in facial expressions, structure, facial features, mouth shape, and illumination. In contrast, the 3D faces we reconstructed appear more realistic and faithful to the original subject.\nDenoising diffusion branch\nIn the denoising diffusion branch, we initially subject $x_{hq}$ to the forward process to derive the noise image as per Eq. 2, (where $t \u2208 [1, T]$, and $T$ represents the total number of denoising diffusion steps). Subsequently, the reconstructed facial 3D image is fed into the multi-level feature extraction module to extract features ranging from coarse to fine, thereby capturing both structural and identity information within the facial 3D image.\n$F_{3d}^1 = ResBlock (x_{3d}),$\nResBlock uses the same structure as SR3 [39], and the obtained features will be sent to the next level for feature extraction.\n$F_{3d}^i = ResBlock (Conv_{3\u00d73} (F_{3d}^{i-1})),$\nDownsampling is performed through a convolution layer, where $F_{3d}^i$ represents the facial 3D image features extracted from the i-th layer. We also use ResBlock for feature extraction on the noisy image $x_t$. $F_{in}$, $F_{i-1}^i$, and timestep $t$ is input into the Time Aware Fusion Block, as depicted in Fig. 4. Since the guidance information required by the denoising diffusion model is different at different time steps, we first input t into the Multilayer Perceptron (MLP) to learn the weight parameters.\n$\u03b1_1, \u03b2_1, \u03b3_1, \u03b3_2, \u03b3_3 = MLP(t),$\nWe apply Spatial Feature Transform (SFT) to modulate the facial 3D image features processed by the LayerNorm layer.\n$F^i_{3d} = SFT (LayerNorm (F^i_{3d}), \u03b1_1, \u03b2_1)$\n$= a_1 \\odot (1 + LayerNorm (F^i_{3d})) + \u03b2_1,$\nwhere $\\odot$ represents Element-wise Multiplication, passing $F^i_{3d}$ through different Channel Squeezes (CS) can obtain different weights in the channel dimension. For the sake of simplicity, we only draw four channels in Fig. 4, two sets of different weights. Multiply with $F^i_{3d}$ and Fin passing through the LayerNorm layer, respectively. Through channel attention, the model can focus more on important structural information in facial images.\n$F_3 = CS_1(F^i_{3d}) \\odot F^i_{3d},$\n$F_4 = CS_2 (F^i_{3d}) \\odot LayerNorm (F_{in}),$\nAfter splicing F3 and F4 through concat, we use 1X1 convolution to transform the number of channels, and then pass through the scale add module. This module uses the weights learned at time step t to combine the output features, denoising image features, and 3D Features.\n$F_5 = Conv 1 \u00d7 1(Concat(F_3, F_4)) + \u03b3_1F_{in} + \u03b3_1F_{3d},$\nThen F5 is input to the Feedforward Neural Network (FFN) and then passes through the Gate layer. The gate layer is mainly used as a gating mechanism in the feedforward network, and the weights are learned by t.\n$F_{out} = FFN (F_5) + F_5,$\nWe optimize the conditional denoising diffusion model through the following equation:\n$L_{DM} = E_{x, \u03b5\u223cN(0,1),t} [||\u03b5 \u2013 \u03bc_\u03b8 (x_t, x_{3d}, t)||_2^2].$\nIn the inference stage, we use the same truncated sampling method as [51] for inference. We set the denoising steps to 100, and the specific network architecture layers will be presented in the supplementary materials."}, {"title": "4 EXPERIMENTS", "content": "Experimental settings\nDatasets. Following the methods [28, 45, 50], we also selected the FFHQ [25] dataset as our training dataset, which consists of 70,000 high-resolution face images with a resolution of 1024\u00d71024. Initially, we used simple downsampling to resize the images in the dataset from 1024 \u00d7 1024 to 512 \u00d7 512, which served as the high-quality (HQ) images in our training dataset. Following the methods [28, 45, 50], we also employed the same random degradation approach to synthesize the LQ images:\n$x = [(y \\otimes k_\u03c3) \u2193_r +n_s]JPEG_q,$\nThe corresponding LQ images were synthesized via Eq. (18), where $y$ represents the HQ image, $k_\u03c3$ denotes the Gaussian blur kernel, $r$ signifies the downsampling factor, and $q$ represents JPEG compressed images with a quality factor of q. We randomly sampled the parameters \u03c3, r, \u03b4, q from {0.1: 10}, {4:20}, {1:20}, {30: 70}.\nWe utilized four datasets for evaluation: CelebA-Test [24], LFW-Test [22], WIDER-Test [49], and WebPhoto [45]. The CelebA-Test dataset consists of 3000 synthetic images randomly sampled from CelebA-HQ [24], a high-resolution image dataset. The LQ images represent degraded images with an unknown degradation model and parameters via Eq. (18). The LFW-Test dataset comprises 1711 face images collected from the internet, representing real-world data with a certain level of complexity and diversity. The WebPhoto-Test dataset consists of 407 face images gathered from various online sources. The WIDER-Test dataset comprises 970 severely degraded facial images sourced from the WIDER Face dataset [49].\nMetrics. We evaluate our method using the distinct perceptual metrics: LPIPS [52], and FID (Fr\u00e9chet Inception Distance) [19]. For completeness, we also include two distortion-based metrics: PSNR and SSIM [46]. In particular, when calculating the FID metric, both ground truth images and the FFHQ [25] dataset are used as reference images. We label them as FID-G and FID-F, respectively.\nImplementation details. In the experiment, we first validated the effectiveness of the proposed method on the blind restoration task. Then, we further demonstrated its superiority by testing on synthetic and real datasets. Our method employed the Adam optimizer. The default initial learning rate is set to 0.0001, and the learning rate does not decay during training. The experiment is conducted on the A100 GPUs with a batch size of 16. In the tables, the best and second-best results are highlighted in red and blue, respectively.\nQuantitative comparisons of CelebA-test dataset. The results obtained from our experiments, as presented in Tab. 1, demonstrate that our approach outperforms other methods in terms of quantitative measures when evaluated on the CelebA-Test dataset. Our approach achieves the highest scores in FID-F, and FID-G, indicating that our restoration results closely resemble both the distribution of real face images and natural images, while also maintaining the structural consistency and identity consistency of the restored face. Besides, the pixel-level metrics SSIM and PSNR are metrics used to evaluate structural similarity and image quality. In these two indicators, the performance of our method also reaches the top scores among all methods, indicating that the restored face image is the best in terms of structural similarity and image quality.\nQuantitative comparisons of the real-world dataset. Furthermore, our study encompasses a comprehensive quantitative analysis of the real-world datasets LFW-Test, WebPhoto, and WIDER-Test as delineated in Tab. 2. Notably, our method emerged as the top performer on the WebPhoto dataset, showcasing its exceptional efficacy in facial restoration. Conversely, our method secured the second-highest performance on the LFW-Test dataset, as evidenced by the FID evaluation metric. However, real-world images typically exhibit lesser degradation compared to synthetically altered images. Consequently, the full extent of our method's optimal facial repair capabilities may not be fully realized in such scenarios.\nQualitative comparisons of CelebA-test dataset. We conduct a qualitative analysis of six blind face restoration methods. As shown in Fig. 5, the highlighted regions in red boxes indicate significant differences among the methods in terms of facial detail restoration. Our method, by incorporating 3D facial prior information into the diffusion model, better ensures the preservation of facial identity. Our approach demonstrates excellent fidelity in facial contours, nose, eyes, and mouth, approaching the ground truth.\nQualitative comparisons of real-world dataset. The qualitative outcomes of the real-world dataset are depicted in Fig. 6. Existing methodologies struggle to compensate adequately for information when the input image experiences extensive degradation. In contrast, our methodology introduces 3D facial prior information, resulting in visually more appealing outputs, particularly in cases of severe degradation in the input image.\nAnalysis of the weight in Scale Add Block. As shown in Fig. 7, the weight of 3D image features in Scale Add Block begins to increase as denoising proceeds. The first stage is mainly the recovery of structural information, and the subsequent stages are the recovery of detailed texture information [42]. The process will destroy certain structural information, and the weight of 3D prior information will be reduced. The second stage mainly relies on denoising image features, and the weight continues to increase."}, {"title": "4.2 Ablation study", "content": "Ablation settings. To demonstrate the benefits of incorporating 3D embeddings into the diffusion model for capturing more facial feature information while ensuring identity consistency in the restored images, we conduct ablation experiments on our proposed method, considering five experimental groups: (a) removing facial 3D images as guiding conditions and eliminating the multi-level feature extraction module and the TAFB module; (b) excluding TAFB as a fusion block and using concatenation to combine facial 3D images and noisy images, only altering the input channel numbers of the diffusion model; (c) employing a TAFB without time embedding; (d) removing the 3DMM reconstruction module and directly inputting the output of SwinIR [30] into the multi-level feature extraction module; and (e) our proposed model.\nQuantitative and qualitative analyses of ablation settings. We conduct both quantitative and qualitative analyses on the CelebA-Test dataset [24]. As depicted in Fig. 8, Our method shows better recovery effects on individual glasses, eyes, mouth, eyebrows, and facial structures. Since the 3D prior information can reconstruct the area under the glasses, when the 3D prior information is missing, the eyes will fail to reconstruct. If the TAFB module fusion function or time embedding is not integrated, the generated image is prone to over-smoothing because conditional guidance cannot provide different guidance information at different time steps."}, {"title": "5 CONCLUSION", "content": "We propose a blind degraded face image restoration model based on a 3D facial prior diffusion model, which is inspired by the fact that the 3D prior information not only contains facial details but also includes identity information. To ensure realism and fidelity, 3D priors can be regarded as identity and structure constraints in the denoising diffusion process to ensure identity consistency while generating high-quality images. Specifically, the structural features and identity features in the 3D prior information are extracted through the multi-level feature extraction module. Given that the denoising process of the diffusion model primarily involves initial structure refinement followed by texture detail enhancement, we propose a Time-Aware Fusion Block (TFAB). TAFB is used to effectively and weight-adaptively fuse the features with the features extracted from the noise image to more accurately predict the noise and restore the identity and structure consistent with face images. Extensive experiments demonstrate that the proposed model performs favorably against state-of-the-art algorithms."}]}