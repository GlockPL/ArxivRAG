{"title": "Detecting Cognitive Impairment and Psychological Well-being among Older Adults Using Facial, Acoustic, Linguistic, and Cardiovascular Patterns Derived from Remote Conversations", "authors": ["Xiaofan Mu", "Salman Seyedi", "Iris Zheng", "Zifan Jiang", "Liu Chen", "Bolaji Omofojoye", "Rachel Hershenberg", "Allan I. Levey", "Gari D. Clifford", "Hiroko H. Dodge", "Hyeokhyen Kwon"], "abstract": "INTRODUCTION: The aging society urgently requires scalable methods to monitor cognitive decline and identify social and psychological factors indicative of dementia risk in older adults.\nMETHODS: Our machine learning models captured facial, acous-\ntic, linguistic, and cardiovascular features from 39 individuals\nwith normal cognition or Mild Cognitive Impairment derived\nfrom remote video conversations and classified cognitive status,\nsocial isolation, neuroticism, and psychological well-being.\nRESULTS: Our model could distinguish Clinical Dementia Rat-\ning Scale of 0.5 (vs. 0) with 0.78 area under the receiver operating\ncharacteristic curve (AUC), social isolation with 0.75 AUC, neu-\nroticism with 0.71 AUC, and negative affect scales with 0.79\nAUC.\nDISCUSSION: Our findings demonstrate the feasibility of re-", "sections": [{"title": "1 | INTRODUCTION", "content": "The number of older adults living with Alzheimer's disease and related dementias (ADRD) is expected to rise by nearly 13 million in the U.S. by the year 20601;2. Mild Cognitive Impairment (MCI) often precedes ADRD. It is characterized by cognitive decline that is greater than expected for an individual's age and education level, while the person remains capable of performing daily activities independently. 3;4. Along with cognition, emotional well-being such as anxiety, loneliness, or depressive symptoms potentially have bidirectional links with cognitive impairments, due to shared neurobiological links and behavioral mechanisms that impair brain functions 3;4;5;6;7;8;9;10;11;12;13. Additionally, mood disorders, social isolation, and negative emotions often co-occur with or even precede MCI and can further accelerate cognitive decline. 14;12;15. Cognitive impairment and psychological well-being significantly impact the lives of elderly individuals, highlighting the importance of early detection, monitoring, and intervention of these symptoms to maintain their quality of life.\nClinical tools, such as the Montreal Cognitive Assessment (MOCA) and Clinical Dementia Rating (CDR), have become widely accepted as standardized methods for evaluating and monitoring cognitive impairment, which assess a range of cognitive functions 16;17. However, traditional cognitive assessments are not sensitive enough to identify MCI or monitor the progression during the early MCI stage 18;19;20. In standard geriatric care, subtle behavioral changes related to psychological well-being, such as reduced social engagement and declining mental health, are often overlooked. These factors, however, can signal early dementia risk and offer opportunities for timely intervention21;5, highlighting the urgent need for innovative approaches to monitor them 22;23;24;25;26. The expected shortage of care services further exacerbates this situation: it is expected that over 1 million additional direct care workers will be needed by 2031, and the U.S. must nearly triple its geriatricians by 2050\u00b9. The current global shortage of mental health professionals and geriatricians further complicates disparities in care, especially in underserved regions 27;28. Even developed countries are facing such challenges, with twenty states in the U.S. identified as \"dementia neurology deserts\" 29, let alone developing countries.\nThe recent widespread adoption of video conferencing platforms for telehealth 30 presents an opportunity to utilize these tools to prescreen cognitive impairment and identify other associated factors, including social isolation and psychological well-being of older adults remotely 31;32;33;34. Recent advancements in artificial intelligence (AI) have spurred research into using telehealth platforms to quantify psychological-relevant behaviors in individuals with MCI through facial, audio, and text analysis 35;36;37. Despite the active research in computerized interview analysis 32;33;34, there are still relatively few publications that focus on quantifying cognitive abilities, social isolation, and psychological well-being in older adults automatically through remote interviews.\nFurthermore, with the recent emergence of generative AI models, so-called foundation AI models trained on vast amounts of"}, {"title": "2 METHODS", "content": "The data source for this project was the Internet-Based Conversational Engagement Clinical Trial (I-CONECT) (NCT02871921) study 46;47. This behavioral intervention aimed to enhance cognitive functions by providing social interactions (conversational interactions) to older subjects living in social isolation. The study was based on the accumulating evidence that social isolation is"}, {"title": "2.2 Outcomes and Clinical Assessment", "content": "In this study, we aimed to classify participants with cognitive assessments derived from various scales. The MoCA scores were dichotomized into 'high' and 'low' categories using a cutoff of 24, which is based on the median score of our participants. A high score indicates better cognitive function. The Normal Cognition (vs. MCI) assessment is the binary encoding of clinician evaluations (NACC UDS V3, Form D1: Clinical Diagnosis Section 1). This encoding assigns a value of 1 to indicate normal cognition and 0 to indicate MCI. Regarding the CDR, participants had scores of either 0, indicating no cognitive impairment, or 0.5, indicating questionable or very mild dementia. These scores were dichotomized accordingly.\nSocial network and Psychological well-being assessments included LSNS-652 for the amount of social interaction, neuroticism from the NEO Five-Factor Inventory (Neuro)55, and NIH Toolbox Emotional Battery (NIHTB-EB)56. The latter has three composite scores: negative affect, social satisfaction, and psychological well-being 57. For the LSNS-6 items version, which is used here, the cutoff score of 12 is the suggested threshold to define social isolation 58. For neuroticism, our participants' median score of 16 is used to dichotomize our participants into groups that have higher or lower negative emotional reactivity to stressful stimuli. From NIHTB-EB, negative affect, social satisfaction, and psychological well-being composite scores 57 were dichotomized with medians of 44.10, 48.66, and 53.70, respectively, to group our participants into high and low-score groups."}, {"title": "2.3 | Predictors and Multimodal Analysis System for Remote Interview", "content": "Overall Pipeline Our proposed multimodal analysis framework uses facial, acoustic, linguistic, and cardiovascular patterns to quantify the cognitive function and psychological well-being of the participants during remote interviews (i.e., semi-structured conversations). The participant segment of the interview recordings for the facial, vocal, linguistic, and cardiovascular patterns are extracted. The extracted time-series multimodal features are aggregated over the video using temporal pooling or Hidden Markov Model (HMM). The video-level features are processed with binary classifiers, logistic regression, and/or gradient-boosting"}, {"title": "Preprocessing", "content": "Our video data records both the participant's and moderator's activities during the remote interview, which requires segmenting the participant portion of the recording for analysis. Our video recording shows the indicator of the speaker on the screen, either as a moderator or participant ID (starting with \"C\" followed by 4 digits). We used optical character recognition (OCR), namely EasyOCR 60;61;62, and only the frames indicating participant ID are kept for further analysis. From participant video segments, we used RetinaFace 63, a facial detection model, for detecting, tracking, and segmenting participants' faces. For the language analysis, the participants' speeches were transcribed with the transcription models 64 specifically developed for older adults."}, {"title": "Facial Biomarkers", "content": "We extracted generic facial expression features using DINOv2 38, a foundation model for facial representation with 1024-dimensional visual embeddings. We also extracted facial emotion, landmark, and action units with facial analysis pipelines used for previous mental health studies 65;66;67. Facial emotion included 7 categories included being neutral, happy, sad, surprised, fearful, disgusted, and angry 66. Overall, our facial features were extracted at a 1 Hz sampling rate."}, {"title": "Cardiovascular Biomarkers", "content": "rPPG signals indicate physiological information by capturing subtle variations in skin color that result from blood volume changes in peripheral blood vessels. These signals are extracted from video recordings of a person's face. We extracted rPPG signals using the pyVHR package 68, a rPPG extraction model. To estimate heart rate from rPPG signals, we analyzed the power spectral density of rPPG signals every six-second intervals, advancing 1 second at a time. The final HRV features were derived by taking the 5th, 25th, 50th, 75th, and 95th quantile of the estimated beats per minute, representing statistical properties of HRV during the interview, using the pyVHR package."}, {"title": "Acoustic Biomarkers", "content": "We first downsampled the audio to 16k Hz for the acoustic feature extraction. Then, we extracted generic acoustic features from vocal tones using the WavLM39 model, a foundation model for human speech analysis, every 20ms. We also extracted hand-crafted statistical acoustic features every 100ms using PyAudioAnalysis package 69, such as spectral energy or entropy, which was effective for depression and anxiety analysis 65."}, {"title": "Linguistic Biomarkers", "content": "We encoded the entire interview transcript for participants using LLaMA-65B 40 to capture the high-level context of text representations in an 8196-dimensional vector. We also extracted 7 emotions (neutral, happiness, sadness, surprise, fear, disgust, and anger) and positive and negative sentiments in utterance level using RoBERTa models 59;70;71. Both models are large language foundation models (LLMs)."}, {"title": "Participant-level Feature Aggregation", "content": "Once all modality features are extracted, they are aggregated over time to represent the entire interview sequence for the participant outcomes. Specifically, we extracted statistical features, such as average or standard deviation, over the entire video sequence. We also trained a two-state Hidden Markov Model (HMM) with Gaussian observation models to capture the temporal dynamics of each biomarker time series using the SSM package 72. The dimensionality of the observations was determined by the length of the feature set corresponding to the patient with the longest sequence. Figure 1 shows the details of statistical features used in our study."}, {"title": "Multi-modal Fusion and Classification", "content": "We applied a late-fusion approach for classification, which was more effective than the early-fusion approaches in previous studies 65. For each modality, we first used a logistic regression classifier with L2 regularization or a gradient-boosting classifier to classify the dichotomized ratings (high and low) for cognitive impairment and other outcomes. Then, we aggregated the classification scores from all modalities using majority voting or average scores approaches. The majority voting outputs the final prediction by voting classification results from all modalities. The average score first averages the probability of the predicted class for all modalities and makes the final decision with the class with the higher probability."}, {"title": "2.4 Experiment", "content": "For evaluating our multimodal analysis system, we used participant-independent 5-fold cross-validation with 20 repetitions. For each fold, we used 64%, 16%, and 20%, as training, validation, and testing splits, respectively. For the evaluation metric, we used an area under the receiver operating characteristic curve (AUROC or AUC) and accuracy, following previous work 65. We also evaluated the standard deviation of the performance from all cross-validated models to measure the statistical significance of the model performances.\nWe evaluated unimodal and multimodal fusion models in various combinations to understand the relevance of each modality and the interplay between the modalities for assessing cognitive function and psychological well-being in older adults. For multimodal fusion, we explored 1) acoustic and language fusion, 2) face and cardiovascular fusion, and 3) all modalities combined. In addition to majority voting and average score, we also studied the selected score voting approach, which was effective in previous work 65 for the late-fusion approach. Differently from majority voting, which considers all modalities, the selected voting only includes the classification results for the modality that achieved AUC > 0.5 for the validation set. This was to exclude noise from classifiers performing poorly due to irrelevant features for multimodal fusion."}, {"title": "3 RESULTS", "content": "Our experiment results for quantifying cognitive functions are shown in Table 2 for each modality and Table 3 for multimodal fusion analysis. The results for quantifying social network, neuroticism, and psychological well-being are shown in Table 4 for each modality and Table 5 for multimodal fusion."}, {"title": "3.1 | Quantifying Cognitive Assessment", "content": "MOCA Our results show that differentiating those with high vs. low MoCA scores (4th column in Table 2 & Table 3) is best when using linguistic modality, LLaMA-65B alone (0.64 AUC and 0.63 accuracy). This was followed by acoustic features (0.63 AUC and 0.58 accuracy) and demographic variables (0.62 AUC and 0.59 accuracy). Facial features and HRV features derived from rPPG were not useful indicators of quantifying MoCA. All multimodal fusion approaches in Table 3 underperformed the best unimodal approach using the linguistic feature, LLaMA-65B.\nMCI diagnosis Quantifying MCI diagnosis (5th column in Table 2 & Table 3) was also most effective when using language-based sentiment and emotion features (0.66 AUC and 0.69 accuracy), followed by acoustic and language fusion model (0.66 AUC and 0.63 accuracy).\nCDR For CDR (6th column in Table 2 & Table 3), the acoustic and language fusion model performed the best (0.78 AUC and 0.74 accuracy)."}, {"title": "3.2 | Quantifying Social Network and Psychological Well-being Assessment", "content": "LSNS Our results showed that language-based emotion and sentiment features were most effective for quantifying social network score, LSNS (4th column in Table 4 & Table 5) with 0.75 AUC and 0.73 accuracy. Facial emotion, landmark, and action unit features showed the second-best performance with 0.6 AUC and 0.59 accuracy. When fusing modalities, only the all-modality fusion with the majority vote matched the facial model with 0.6 AUC and 0.56 accuracy.\nNeuroticism For quantifying neuroticism (5th column in Table 4 & Table 5), multimodal fusion from all modalities performed effectively with 0.71 AUC and 0.65 accuracy. This was followed by features from facial emotion, landmark, and action units (0.69 AUC and 0.66 accuracy), which contributed the most when fusing multiple modalities.\nNegative Affect Facial and cardiovascular fusion performed effectively when quantifying negative affect (6th column in Table 4 & Table 5) with 0.79 AUC and 0.75 accuracy. Cardiovascular features alone showed 0.76 AUC and 0.67 accuracy, contributing most when fused with other facial features.\nSocial Satisfaction Facial emotion, landmark, and action unit features were most useful when quantifying social satisfaction (7th column in Table 4 & Table 5) during remote interviews (0.68 AUC and 0.63 accuracy).\nPsychological Well-being When quantifying overall psychological well-being (8th column in Table 4 & Table 5), facial emotion, landmark, and action unit features were most effective with 0.66 AUC and 0.61 accuracy. This was followed by cardiovascular features (0.62 AUC and 0.60 accuracy), but fusing facial and cardiovascular features showed no improvement"}, {"title": "4 DISCUSSION", "content": "4.1 | Identifying Cognitive States.\nOverall, identifying low MoCA scores and clinical diagnosis of MCI was challenging with our multimodal analysis system, showing 0.64 and 0.66 AUCs, respectively. This is possibly due to the narrow range of MoCA in this group: The majority (68%) of our subjects had MoCA scores between 21 and 28, with a median score of 24, having limited variability of scores to detect distinguishable features associated with MoCA \u2264 24 vs. > 24. Conversely, quantifying CDR (0 vs. 0.5) proved more effective, achieving an AUC of 0.78. The CDR assesses functional outcomes in daily life (e.g., forgetfulness of events, functionality in shopping, and participation in volunteer or social groups), reflecting cognitive performance but not necessarily linked to cognitive testing scores. These functional aspects are likely better captured by acoustic and linguistic features from remote interviews compared to measures like MoCA and clinical MCI diagnoses.\" It is also worth noting that, while age has been a significant factor for predicting MoCA scores 74;75 in the general population, it does not show any predictive capacity in our analysis. This"}, {"title": "4.2 | Quantifying Social Network and Psychological Well-being Assessment.", "content": "Overall, quantifying social isolation (LSNS) was most effective using language features. This is consistent with prior research indicating the utility of text sentiment analysis in understanding the social health of individuals 87;88;81. Other scales for psychological well-being required facial videos to quantify them, and cardiovascular measures were most effective at quantifying negative affect, similar to those reported in mental health studies for various populations using wearable-based cardiovascular health monitoring 89;90;91. Negative affects, such as mood disturbances, anxiety, and depression, can be an early sign of dementia, and these emotional changes often precede noticeable cognitive decline and may be linked to neurodegenerative processes in the brain 92. Our study demonstrates that contactless cardiovascular measures have the potential to quantify behavioral symptoms often accompanied by MCI, which calls for further exploration.\nOur findings show that facial, acoustic, and linguistic features from foundation models (DINOv2, WavLM, and LLaMA-65B) significantly underperformed, with an average absolute decrease of 22% AUC compared to the best models using other features from facial expression, acoustic, and linguistic-based emotion signals. Those foundation models are trained to capture generic facial appearance, acoustic waveforms, and language embeddings by training the models with a large-scale database available"}, {"title": "4.3 Limitations & Future work", "content": "The proposed work studies the association of facial, acoustic, linguistic, and cardiovascular patterns with cognitive impairment and the associated social and psychological well-being in older adults with normal cognition or MCI. Our study demonstrated that features extracted from remotely conducted conversations can detect a broad range of symptoms linked to cognitive decline, including social engagement and emotional well-being. This remote assessment approach holds promise for the early identification of individuals at risk for cognitive decline, ultimately creating opportunities for timely interventions to slow or prevent further deterioration. Our study has several limitations.\nFirst, this study includes subjects with normal cognition and MCI, which helps to quantify and contrast the behavior"}]}