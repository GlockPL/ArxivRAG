{"title": "Scaling Image Tokenizers with Grouped Spherical Quantization", "authors": ["Jiangtao Wang", "Zhen Qin", "Yifan Zhang", "Vincent Tao Hu", "Bj\u00f6rn Ommer", "Rania Briq", "Stefan Kesselheim"], "abstract": "Vision tokenizers have gained a lot of attraction due to their scalability and compactness; previous works depend on old school GAN-based hyperparameters, biased comparisons, and a lack of comprehensive analysis of the scaling behaviours. To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to a spherical surface. Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing a solid foundation for scaling studies. Building on this, we systematically examine the scaling behaviours of GSQ-specifically in latent dimensionality, codebook size, and compression ratios and their impact on model performance. Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces. We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality. As a result, GSQ-GAN achieves a 16\u00d7 down-sampling with a reconstruction FID (rFID) of 0.50.", "sections": [{"title": "Introduction", "content": "Recent advancements in generative models for images and videos have seen substantial success, with approaches like autoregressive models Sun et al. (2024); Kondratyuk et al. (2024); Wang et al. (2024b), masked language models Yu et al. (2024b; 2023); Chang et al. (2022); Weber et al. (2024), and diffusion-based methods (including score-matching and flow-matching) Rombach et al. (2022); Yang et al. (2024); Hu et al. (2024); Gao et al. (2024) and surpass GAN-based Kang et al. (2023); Sauer et al. (2023) models. A common factor in many of these models is the reliance on latent discrete representations of images, especially within language-model-based, where continuous feature maps are quantized into discrete tokens. This quantization has become critical for high-fidelity generation, as tokenized images facilitate model efficiency and enhance generative quality, avoiding the need to work on high-resolution images directly. Recent studies Yu et al. (2024b); Wang et al. (2024b) confirm image tokenizer directly translates to generative quality, and the effectiveness of generative models is closely tied to the performance of image tokenizers.\nThe fundamental challenge in training image tokenizers is balancing compression efficiency with reconstruction accuracy. While recent methods show progress, several critical issues remain unresolved: (1) Many current tokenizers still depend on outdated GAN-based hyperparameters, often resulting in suboptimal, even negative, performance due to inconsistencies between generation and reconstruction objectives. (2) Benchmarking efforts frequently rely on legacy VQ-GAN implementations with outdated configurations, leading to biased comparisons and limited assessment accuracy. (3) Although various quantization models"}, {"title": "Related Work", "content": "The Variational AutoEncoder Kingma (2013) is the foundational approach for image tokenization, initially developed to compress images into a continuous latent space, while later one more work focuses on refining continuous representations Higgins et al. (2017); Vahdat & Kautz (2020); Kim et al. (2019); Luhman & Luhman (2022); Bhalodia et al. (2020); Egorov et al. (2021); Su & Wu (2018); Qin & Huang (2024). Despite their strengths, however, these image encodings, often constrained by strong KL regularization, are rarely applied as image tokenizers within generative models. Instead, the VAE with vector quantization (VQ-VAE) Van Den Oord et al. (2017); Razavi et al. (2019) have become the preferred choice due to their effective use of a codebook for latent distribution regularization. Alternative variance is Residual Vector Quantizer (RVQ) Zeghidour et al. (2021) that can achieve image compression and discrete quantization simultaneously.\nBuilding on the success of VQ-VAE, the VQ-GAN model Esser et al. (2021) further advanced image tokenizer training by incorporating a perceptual loss Zhang et al. (2018) and an adversarial loss, enhancing the quality of generated images. Subsequent research has extended VQ-GAN through (1) architectural improvements, such as transformer-based structures Yu et al. (2022) and Layer Normalization Chang et al. (2022); (2) novel vector quantizers like Finite Scalar Quantization Mentzer et al. (2024), Lookup-Free Quantizer Yu et al. (2024b) and so on Zhao et al. (2024); Zheng et al. (2022a); Zhu & Soricut (2024); Sadat et al. (2024); Adiban et al. (2023); Yu et al. (2024a); Cao et al. (2023); You et al. (2022); Lee et al. (2022); Adiban et al. (2022); Kumar et al. (2024); Zheng et al. (2022b); Kumar et al. (2024); Li et al. (2024); Luo et al. (2024); Tian et al. (2024); Fifty et al. (2024); and (3) refined loss functions with perceptual enhancements, for example, using ResNet-based perceptual loss Weber et al. (2024); Yu et al. (2023) and incorporating StyleGAN discriminators Yu et al. (2022; 2024b). Our work primarily focuses on this stream of compression-oriented image tokenizer training, examining scaling behaviours and their influence on reconstruction quality.\nAn alternative line of research in image tokenization focuses on embedding semantic visual representations in the latent space, rather than maximising compression rates. This approach typically leverages pre-trained visual foundation models, such as DINO Oquab et al. (2024), CLIP Radford et al. (2021), and MAE He et al. (2022), by transferring their learned representations into the latent space of image tokenizers or quantizing their latent representations. Early studies Peng et al. (2022); Hu et al. (2023); Park et al. (2023) demonstrated the feasibility of this strategy, though these models traditionally underperform in reconstruction quality compared to compression-driven tokenizers. Recent advancements have narrowed this gap by optimizing codebook initialization, refining network architectures, and employing advanced knowledge distillation methods, resulting in models that achieve competitive reconstruction fidelity while preserving strong semantic representation capabilities Yu et al. (2024c); Zhu et al. (2024a;b); Li et al. (2024)."}, {"title": "Methodology", "content": null}, {"title": "Preliminary: VQ Image Tokenizer", "content": "The image tokenizer consists of an encoder Enc and a decoder Dec. The encoder compresses the high-resolution input image $I\\in R^{H\\times W\\times 3}$ into continuous latent maps:\n$Z = Enc(I) = \\{z_i \\in R^{D}\\}_{i=1}^{S} = \\{z_i \\in R^{D}\\}^{h\\times w}$.\nand the decoder reconstructs the image from the latent representation, $I = Dec(Z)$. The down-sampling factor $f = \\frac{H}{h} = \\frac{W}{w}$ denotes spatial reduction, and the compression ratio is given by $R = \\frac{3HW}{Dh w}$, where H, W is the height and width of input image I and h, w, D is the height, width and dimension of latent.\nWith a vector quantizer, the latent space is discretised by mapping $Z$ to indices in the codebook $C = \\{C_j \\in R^{D}\\}_{j=1}^{V}$, where V is the vocabulary size. Each latent vector $z_i$ from $Z$ is quantized to the nearest codebook entry using a look-up operation, often based on Euclidean distance:\n$VQ(z_i) = lookup(z_i, C) = \\mathop{\\arg\\min}_j || z_i - C_j ||_2^2.$"}, {"title": "Simple Scaling with GSQ", "content": "Pursuing higher spatial reduction f requires increasing the latent dimensionality D to maintain R, thus preserving reconstruction fidelity. However, increasing D introduces high-dimensionality challenges, making distance computations less effective and limiting achievable compression ratios. One of the solutions is using a product quantizer Vahdat & Kautz (2020); Zheng et al. (2022a;b); Jegou et al. (2010), hence we decompose each latent vector zi into G groups:\n$GSQ(z_i) = \\{lookup^*(z_i^{(g)}, C^{(g)})\\}_{G=1}^G,$\nHere, each $z_i^{(g)}$ represents a sub-group of zi with d channels, where $G \\times d = D$ enables efficient compression without compromising reconstruction fidelity. To improve stability and performance, we propose to initialize codebook entries from a spherical uniform distribution and same as Yu et al. (2022); Zhao et al. (2024), apply 12 normalization during lookup:\n$c^{(g)} \\sim l_2(\\mathcal{N}(0,1)),$\n$lookup^*(z_i, C) = \\mathop{\\arg\\min}_j ||l_2(z_i) - l_2(c_j)||_2.$\nWe employ a shared codebook among all groups and omit 12 when $G/D \\in \\{1, 2\\}$, in which case GSQ reduces to LFQ Yu et al. (2024b), and the spherical space significantly collapsed, which requires additional entropy loss during training Yu et al. (2024b); Zhao et al. (2024). Further discussion is provided in Appendix C."}, {"title": "Experiments", "content": null}, {"title": "Optimized Training for GSQ-VAE", "content": "We first investigate the efficacy of our proposed improvements to GSQ on VAE-based tokenizers, including impacts of training configurations, auxiliary losses, model architecture, and hyperparameter settings. We set G = 1 for all modes, they were trained on 1282 resolution ImageNet Deng et al. (2009) with a down-sampling factor f = 8, vocabulary size V = 8,192, latent dimensionality D = 8, with batch size 256, and learning rate of le 4 for 100k steps (20 epochs). Specific hyperparameters are reported in Appendix D. All tokenizers adopted an exponential moving average with a decay rate of 0.999. We utilized the LPIPS perceptual loss Zhang et al. (2018) as proposed in Esser et al. (2021) with a weight of 1.0 in training."}, {"title": "Effectiveness of Spherical Quantization", "content": "Baseline and codebook initialization. table 1 demonstrates that our spherical uniform distribution codebook initialization significantly improved codebook usage to nearly 100% during training. Using 12 normalization, mentioned with previous studies Yu et al. (2022); Zhao et al. (2024), is crucial for stabilizing codebook usage (especially in larger codebooks) and ensuring all codes are usually equal. As illustrated in fig. 7, our approach maintained approximately 100% codebook utilization throughout training, which enabled the reduction of the rFID from 11.37 to 5.375, and with 12 the perplexity of codebook usage is close to the vocabulary size.\nQuantizer Comparisons. Taking the proposed spherical codebook initialization method and 12 normalized lookup, GSQ (similar to VQ, when G is 1) can outperform FSQ Mentzer et al. (2024), and by scaling G to 8, GSQ can beat RVQ Zeghidour et al. (2021), as we reported in fig. 2, all model here has same latent dimension eight and vocabulary size 8,192.\nCodebook auxiliary loss. We investigated the effectiveness of codebook auxiliary losses, e.g. entropy loss Yu et al. (2024b); Luo et al. (2024) and TCR loss Zhang et al. (2023). table 2 reveals that these losses negatively impacted the tokenizer performance and impeded codebook usage. Entropy loss only provided a marginal improvement with a minimal weight (0.01). Given their limited utility and computational cost on large vocab size during training, we opted not to use them. Also, the later results show that our method maintained 100% codebook usage for vocabulary sizes up to 512k without these losses."}, {"title": "Ablation of Network Backbone", "content": "We explored variations in baseline architectures, including the effect of Adaptive Group Normalization (as known as AdaLN) Huang & Belongie (2017) and Depth2Scale Yu et al. (2024b). As detailed in table 3, surprisingly, these modules degraded the reconstruction's perceptual quality, increasing the rFID but decreasing the pixel-wise error. We use Adaptive Group Normalization as the default and further invested Depth2Scale in GAN's training in section 4.2.4."}, {"title": "Ablation of Perceptual Loss Selection", "content": "We explored various perceptual loss configurations, including LPIPS Zhang et al. (2018) and logit-based perceptual loss with different backbone architectures: ResNet He et al. (2016), VGG Simonyan & Zisserman (2015), and Dino Oquab et al. (2024). As presented in table 4, our findings indicate that ResNet-based logit loss is ineffective as a perceptual loss, which contradicts earlier findings Weber et al. (2024). In contrast, Dino and VGG-based logit losses yielded lower rFID scores, demonstrating their potential. However, we opted for LPIPS due to its ability to effectively balance rFID and pixel-wise error. We anticipate that further optimisation through detailed hyperparameter tuning could enhance the performance of stronger perceptual losses."}, {"title": "Hyper-parameters optimization for GSQ-VAE", "content": "Optimizers. The choice of hyper-parameters specifically \u03b2 in Adam, significantly affects training dynamics. We evaluated combinations of \u03b2 values, ranging from 0 to 0.9, and reported results in table 5. Our experiments reveal higher \u03b2 always brings better reconstruction performance by promoting stable training."}, {"title": "Optimized Training for GSQ-GAN", "content": "Next, we incorporated a discriminator and adversarial loss to ablate training configurations for GSQ-GAN training on ImageNet Deng et al. (2009) at 1282 resolution for up to 80k steps; the VAE and discriminator have a learning rate of 1e-4. Detailed hyperparameters are reported in Appendix E."}, {"title": "Ablations of Discriminator and Combinations of Adversarial Loss", "content": "We evaluated three types of discriminator: N-Layer Discriminator (NLD) Isola et al. (2017), StyleGAN Discriminator (SGD) Karras et al. (2019), and Dino Discriminator (DD) Sauer et al. (2023). We also compared three adversarial loss types: vanilla non-saturating (V), hinge (H), and improved non-saturating (N), resulting in six combinations of adversarial-discriminator loss setups.\nChoosing an improper GAN loss led to negative performance for N-Layer and Dino Discriminators. As shown in table 7. All GAN models trained with Dino Discriminators consistently outperformed GAN with"}, {"title": "Hyper-parameters Optimization for GSQ-GAN", "content": "Discriminator optimizer and adversarial loss weights. We performed ablation studies on optimizer hyper-parameters (\u03b2) for N-Layer and Dino Discriminator. The results, presented in table 9, indicate that higher \u03b2 values (\u03b2 = [0.9, 0.99]) led to more stable training dynamics for both discriminator types. We used this configuration for the remainder of the experiments. Additionally, varying the weight of adversarial loss did not show significant benefits, leading us to set the adversarial loss weight to 0.1."}, {"title": "GAN Regularization Ablations", "content": "We explored several regularization techniques for stabilizing discriminator training: gradient penalty Gul-rajani et al. (2017), LeCAM regularisation Yu et al. (2023), and autoencoder warm-up, as well as adaptive discriminator loss weights Yu et al. (2022), weight decay, and gradient clipping. table 11 summaries our findings."}, {"title": "Analysis of Attention Integration", "content": "We conducted ablation studies on the attention module and Depth2Scale layers. Recent works such as Luo et al. (2024); Yu et al. (2024b) omit attention layers, but as seen in table 12, incorporating attention into mid-blocks improved model performance. We also re-evaluated Depth2Scale, observing that it enhanced GAN's performance under adversarial training. The model's performance across different resolutions is also reported in table 12, showing the model's cross-resolution inference capabilities. We take Depth2Scale, as it generally benefits the GAN training; the model trained with Depth2Scale has rFID 1.53 with 80k training steps. Including attention modules can further boost reconstruction, though it may introduce instability during training."}, {"title": "Scaling Behaviors of GSQ-GAN", "content": "This section investigates how variations influence reconstruction quality in latent dimensions and codebook vocabulary size. All models in this study were trained at a 2562 resolution with a batch size of 512 over 50k steps (20 epochs). Detailed hyper-parameters are provided in Appendix F."}, {"title": "Network Capacity.", "content": "We examine the effects of network capacity on reconstruction fidelity, specifically looking at the width and depth. Width scaling was implemented by increasing the number of channels in convolution layers, while depth scaling involved adding additional convolution blocksYu et al. (2024b). The results, summarized in fig. 3, demonstrate consistent improvements in reconstruction as network width and depth increase. Integrating attention modules within wider networks yielded further gains as used in Esser et al. (2021)."}, {"title": "Scaling of Latent Space and Vocabulary.", "content": "Next, we investigate the impact of scaling latent dimensionality and codebook vocabulary size. Models were trained with latent dimensions of 23, 24, 25, and 26, each paired with vocabulary sizes of 8k, 16k, 64k, 256k, and 512k. Results in fig. 4a and fig. 4b indicate that larger vocabulary sizes, combined with lower latent dimensions, consistently yielded superior reconstruction performance. Remarkably, a model with a latent"}, {"title": "Latent Space and Downsample Factor, and Better Scaling with GSQ", "content": "To address the limitations regarding the difficulty of scaling attend dimension. We use GSQ to decompose large latent dimensions into low dimensions, thus maximizing reconstruction fidelity more effectively. As demonstrated in table 13, by decomposing latent vectors into multiple groups, GSQ significantly enhances reconstruction performance without changing the overall latent dimensionality or vocabulary size. This result confirms GSQ's ability to harness the representational power of high-dimensional latent spaces, leading to substantial gains in model fidelity.\nNotably, the model achieves near-lossless reconstruction with D = 64 and G = 16, approaching theoretical maximum performance. Although the compression ratio is very low and lacks practical value, it highlights GSQ's remarkable scalability and representational power.\nScaling Down-sample Factor. With GSQ optimizing latent space utilization, we further investigate the impact of varying down-sampling factors on reconstruction quality. We conducted experiments across"}, {"title": "Conclusion", "content": "We introduce a novel quantization method, Grouped Spherical Quantization(GSQ), incorporating spherical codebook initialization, lookup normalization, and latent decomposition. We systematically investigate training strategies and optimizations for the proposed GSQ-GAN, identifying key configurations that enhance reconstruction quality with significantly fewer training iterations. We highlight critical scaling behaviours related to the model, latent space, and codebook vocabulary size, emphasizing the role of compact latent spaces in achieving high-fidelity reconstruction. Our results demonstrate that GSQ efficiently scales in high-dimensional latent spaces, leverating latent decomposition and spherical normalization for improved compression and reconstruction."}]}