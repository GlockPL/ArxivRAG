{"title": "Language model developers should report\ntrain-test overlap", "authors": ["Andy K Zhang", "Kevin Klyman", "Yifan Mai", "Yoav Levine", "Yian Zhang", "Rishi Bommasani", "Percy Liang"], "abstract": "Language models are extensively evaluated, but correctly interpreting evaluation\nresults requires knowledge of train-test overlap, which refers to the extent to which\nthe language model is trained on the very data it is being tested on. The public\ncurrently lacks adequate information about train-test overlap: most models have no\npublic train-test overlap statistics, and third parties cannot directly measure train-\ntest overlap since they do not have access to the training data. To make this clear,\nwe document the practices of 30 models, finding that just 9 models report train-\ntest overlap: 4 models release training data under open-source licenses, enabling\nthe community to directly measure train-test overlap, and 5 models publish their\ntrain-test overlap methodology and statistics. By engaging with language model\ndevelopers, we provide novel information about train-test overlap for three addi-\ntional models. Overall, we take the position that language model developers should\npublish train-test overlap statistics and/or training data whenever they report eval-\nuation results on public test sets. We hope our work increases transparency into\ntrain-test overlap to increase the community-wide trust in model evaluations.", "sections": [{"title": "Introduction", "content": "The artificial intelligence (AI) community has built hundreds of evaluations to better un-\nderstand language models (Srivastava et al., 2023; Hendrycks et al., 2021a; Gao et al., 2024;\nMyrzakhan et al., 2024; Chiang et al., 2024; Rein et al., 2023; Liang et al., 2023). These evalu-\nations cannot be correctly interpreted without knowledge of train-test overlap, which we define as\nthe extent to which the evaluation test data appears in the training data.\nPrior to the rise of language models trained on web-scale data, the AI community used standard\ntrain/test set splits, where a model would be trained on the training set and tested on the test set\nto ensure validity of results (Jurafsky & Martin, 2009; Russell & Norvig, 2009). In that regime, the\ndesigner of an evaluation generally would specify both the training and test sets. In contrast, today\nfoundation model developers decide on their own training sets, which they often do not release, and\nevaluation designers decide on test sets, which they often release. Overall, the shift to web-scale\ntraining data with poor documentation of data provenance, along with the two-party specification"}, {"title": "Strategies to Estimate and Address Train-Test Overlap", "content": "The prevalence of train-test overlap as an issue in the AI community\u00b2 has led to the development of\nvarious strategies to estimate and address train-test overlap, including black-box methods, private\ntest sets, novel test sets, and canary strings. We cover each of these in turn then discuss our\napproach.\nBlack-box methods involve researchers working to estimate train-test overlap through model\napi access and the test set rather than directly through access to the training set. Notably, there\nhave been efforts to estimate train-test overlap via prompting, word probabilities, and test exam-\nple orderings (Golchin & Surdeanu, 2023; Shi et al., 2023; Oren et al., 2023). Golchin & Surdeanu\n(2023) prompt the model with the dataset name, partition type, and an initial segment of the ref-\nerence string, and mark train-test overlap if the model responds with a exact or similar copy in the\noutput. Shi et al. (2023) estimate train-test overlap via the probability outputs of outlier words,\nwith the hypothesis that unseen examples is likely to contain few outliers with low probabilities.\nOren et al. (2023) estimate train-test overlap by considering the ordering of test instances, noting\nthat language models with train-test overlap are likely to memorize such ordering. These methods\ncan be helpful for estimation and as a sanity check to white-box approaches, but have currently\nhave limitations as they are not robust to adversarial settings such as if a developer fine-tuned its\nmodel to avoid revealing training data and even in the benign setting, require certain assumptions\nsuch as requiring a certain threshold of frequencies for detection or certain methods of training\n(Casper et al., 2024; Golchin & Surdeanu, 2023; Shi et al., 2023; Oren et al., 2023). Estimating\nand interpreting train-test overlap is difficult even in the white-box setting with direct access to\nthe training data as current approaches have significant limitations; with further constraints in the\nblack-box setting, the challenges only increase.\nPrivate test sets such as SQuAD (Rajpurkar et al., 2018) and SEAL (Scale, 2024) allow re-\nsearchers to keep a portion or all of the test set hidden, meaning that the test set is not publicly\naccessible on the internet and developers are therefore much less likely to train models on it. While\nprivate test sets can be valuable, they raise potential concerns regarding data transparency. For\ninstance, unless the private test set is shared with a trustworthy third party, the community must\nrely upon a single organization's assessment of the test set's validity. In any event, public test sets\nare the industry standard and will continue to exist, though private and public test sets can coexist\nin a healthy testing ecosystem.\nNovel test sets that include data that was produced after the knowledge cutoff date of a model\nalso help mitigate train-test overlap. Including recent data is a best practice for new test sets,\nthough this may be difficult if, for example, a new test set is derived from existing data (e.g.\nbased on old Wikipedia data or AI-generated data). Even when this approach is implemented\nsuccessfully, new models are released regularly that are trained on more recent data, necessitating\nsome analysis of train-test overlap with the previously novel test set. One modification of this\napproach is to add novel data to the test set at regular intervals, as with Livebench (White et al.,\n2024) or Image2Struct.3 In addition to the financial cost of continually adding novel data, which"}, {"title": "Discussion", "content": "Overall, while train-test overlap is a fundamental to interpreting evaluation results, there is still sig-\nnificant limitations in the measurement methodology, beyond data access challenges and developer\nresponsibility. As described above, direct string comparison is the most common way to quantify\ntrain-test overlap. This method has slight variations, but typically involves detecting substring\nmatches between training and test data. N-gram matching is commonly employed (Yang et al.,"}, {"title": "Conclusion", "content": "In this work, we highlight the need to improve transparency of train-test overlap. Our position\nis that any language model developer that publishes results on public test sets should release its\ntraining data and/or publish accompanying train-test overlap so that the community can interpret\nthe results. We discuss various strategies to address train-test overlap, and how our position com-\nplements these efforts. We document the train-test overlap practices of 30 models with published\nresults on public test sets. Of these, 9 models have published sufficient data for the community to\ncontextualize train-test overlap. Finally, we discuss limitations with current approaches to quan-\ntifying train-test overlap, while emphasizing that current methods still have value. Instead, we\nsuggest that as the AI community increasingly becomes aware of train-test overlap we can continue\nto improve upon and align on methodology for measuring and reducing train-test overlap."}]}