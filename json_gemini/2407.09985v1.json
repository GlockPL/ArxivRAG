{"title": "A Training Data Recipe to Accelerate A* Search with Language Models", "authors": ["Devaansh Gupta", "Boyang Li"], "abstract": "Recent works in AI planning have proposed to combine LLMs with iterative tree-search algorithms like A* and MCTS, where LLMs are typically used to calculate the heuristic, guiding the planner towards the goal. However, combining these techniques is not trivial : LM-based heuristics are quite weak, incurring a high computational cost without a significant performance improvement. Existing methods to learn these heuristics do not consider the requirements of the planner, and typically need a lot of compute. Thus, in this work, we propose a distribution to downsample training data by identifying relevant data points to learn a performant heuristic, while constraining computational costs. To arrive at this model, we disentangle the requirements of the planner, in our case A* search, from that of the language model to generalise on this task. Surprisingly, we find an overlap between their requirements; A* requires more accurate predictions on nodes near the goal, and LMs need the same set of nodes for effective generalisation. With these insights, we can quantify the contribution of each node towards accelerating A* search, and subsequently derive a training distribution for learning LM-based heuristics. Following a recent work, we conduct our experiments on two classical planning domains, maze navigation and sokoban, with two test splits per domain, and two conventional loss functions. We reduce the number of iterations required to find the solutions by upto 13\u00d7, with a wall-clock speed-up of upto 5x.", "sections": [{"title": "1 Introduction", "content": "Cognitive science reveals that human cognition can be understood as two closely collaborating systems (Kahneman, 2011). The fast system has high bandwidth, produces instantaneous responses, but is slow to adapt to novel problems. The slow system can solve novel problems, excels at logical reasoning, but works at a much slower speed. Recent analyses suggest that due to their statistical learning nature, LLMs are analogous to fast systems; they perform poorly at problems that are novel (Wu et al., 2024) or require planning (Valmeekam et al., 2023). On the other hand, tree-search methods like A* (Hart et al., 1968) and variants (e.g., Korf 1985; Kocsis and Szepesv\u00e1ri 2006), are classic solutions to logical reasoning and planning, but are unable to learn from past experiences and limited in speed due to sequential dependencies. Combining these techniques remains an open problem.\nWe study the problem of using LLMs to learn A* heuristics, which are functions that estimate the distance from a search node to the goal state. However, ground-truth labels for training can only be obtained from successfully solved problems, rendering the training inefficient. Training LLMs is also computational demanding. Previous works typically train on all the search nodes, further exacerbating this issue. With this paper, we aim to improve the computational and data efficiency of heuristic learning by downsampling the training data from solved problems.\nA common strategy in coreset selection is to select training data based on some measures of difficulty (Paul et al., 2021). In A*, a natural definition for difficulty is the distance to goal, as it is an estimate of how many decisions the search algorithm must make before reaching the goal, assuming a constant branching factor. Nonetheless, the A* + LLM setup contains two systems and each system may impose different requirements on training data, complicating optimization.\nHence, we first attempt to quantify the requirements of the A* system and the LLM respectively. We divide the search trajectory into three equally sized portions, the beginning, the middle, and the end, and test the effect of using each portion as training data. For the LLM, training on the last portion, where the data are easiest to fit, leads to better generalization than the other two portions. Perhaps surprisingly, A* has a similar behavior; erroneous predictions on the end portion are the most detrimental.\nInspired by the empirical results, we devise a sampling strategy for LM training data that includes a higher proportion of search nodes near the end, than the beginning. This strategy outperforms baselines by finding solutions in an average of 8% lesser iterations, and, in some cases, even outperforms models trained with double the data. Our contributions can be summarised as follows,"}, {"title": "2 Related Works", "content": "2.1 Learning Heuristics for Planning\nMachine Learning Perspective In order to reduce the search length, learning for planning problems can be drawn back to (Yoon et al., 2006; Fern et al., 2011), which focused on learning domain-specific representations for planners through classical machine learning methods. Posing the task as a regression problem, learnt with neural networks was studied in (Arfaee et al., 2011; \u00fas Virseda et al., 2013). Post their success, more recent works explored various neural architectures and objective functions for this problem (Chrestien et al., 2021; Groshev et al., 2018; Kirilenko et al., 2023). These works serve to prove the effectiveness of neural networks for learning strong heuristics in A* search, however, their methods are not designed by studying the problem from the planner's perspective, i.e., by analysing the requirements of the planner to reduce the search length.\nPlanner Perspective Some works do look at the problem from this angle; (Yonetani et al., 2021; Vlastelica et al., 2019) reformulate each step of the planner as a differentiable function, and sequentially execute them, with the loss taken after the final step. Backpropagating through time enables them to create a search-dynamics aware heuristic. Similarly, (Speck et al., 2021; Orseau et al., 2023; Orseau and Lelis, 2021) learn heuristics by performing reinforcement learning on the actions taken by the planner. Such methods, while showing strong performance, incur a high training cost. In this work, we pose the problem as a regression task and take an alternate data-centric approach, where we analyse the planning algorithm, specifically A*, and find an optimal subset of nodes to train the neural network (in our case, language models). With this, we can constrain the training cost, while still learning a heuristic suitable for A*.\n2.2 Heuristics with Language Models\nTree-search in LLMs While in this work, we only study LMs being used as a heuristic in environments where possible actions are generated by rule-based methods, previous works have also explored using them as a world model, where the LLM generates an action given the state of the environment. (Yao et al., 2024) uses such a framework to build a tree and parse it with DFS/BFS, while (Hao et al., 2023) extends it to MCTS. The node to be expanded is selected with a heuristic computed by the LLM self-evaluating its generated actions. Such strategies are heavily dependent on the performance of this heuristic (Chen et al., 2024), which have been shown to be quite weak. In this work, we aim to improve such a heuristic by training it. Constraining our environments allows us to isolate the heuristic from the world model.\nLLMs with external planners Besides a heuristic, LMs have been combined with external planners in various capacities. For instance, (Valmeekam et al., 2023) uses an LLM with the LPG planner (Gerevini et al., 2002), which iteratively corrects errors in a plan. Seeding LPG with an LLM plan has been shown to work better than a random plan. LLMs have also been used to translate tasks to formal languages for symbolic solvers like PDDL(Liu et al., 2023) and ASP(Yang et al., 2023). Combining such planners with LLMs has also been explored in dynamic settings to incorporate environment feedback (Guan et al., 2023; Dagan et al., 2023). While these works primarily use off-the-shelf LLMs to improve symbolic planners, our work aims to train an LM.\nImproving LM-based Heuristics (Shinn et al., 2024) proposed to improve LLM heuristics by incorporating failure states into the input prompt, thereby providing a semantic gradient signal away from such states. This has further been incorporated into tree-based frameworks (Zhou et al., 2023a). Such failure states are discovered during the course of solving a problem, and thus the improvements brought about by them are restricted to that particular instance. On the other hand, we consider the setting where the weights are updated, thus bringing improving performance across problems. An alternate line of work addresses the weak performance of LLMs for planning with CoT prompting. These works (Lehnert et al., 2024; Gandhi et al., 2024) propose to train on the traces of tree-search algorithms, and with bootstrapping methods, aim to implicitly learn an improved heuristic. In contrast, we explicitly learn the heuristic by supervision.\n2.3 Optimising Training Data\nCoreset Selection involves pruning the training dataset to only contain important datapoints, without a significant drop in performance. While various works exist for LM pre-training (Paul et al., 2021; Marion et al., 2023; Abbas et al., 2023), to the best of our knowledge, we are the first work to study this in the context of heuristic learning. Our findings correlate with those of (Zhou et al., 2023b; Sorscher et al., 2022); easier data is required for learning in the low-data regime.\nCurriculum Learning In identifying nodes required by the LM for generalisation, we categorise them as \"easy\", \"medium\" and \"hard\". Thus, our work also bears some similarities with curriculum learning methods, which propose a temporal ordering of data points of varying difficulty (Bengio et al., 2009). Some works demonstrate that learning from easier points first, helps learn complex concepts later (Xu et al., 2020), others find the reverse to be more beneficial (Maharana and Bansal, 2022). The aim of curriculum learning is to improve the performance on hard data points, whereas our aim is to reduce the search length of A*, regardless of the difficulty of the training data."}, {"title": "3 Preliminaries", "content": "3.1 A* Search\nA* Search is a tree-based search algorithm that aims to find a path in an environment, given a start node and a goal node by building a tree T. The set of all nodes in the tree is defined as N. For each node n, A* search keeps track of two values, (i) historical cost g(n), which is the distance between the start node and n and (ii) heuristic h(n) which is an estimate of the true distance h*(n) between n and the goal node. Each node also stores the domain-dependent environment state s(n), as a result of an action. For the search, A* maintains two lists, the frontier list PQ frontier and the closed list PQclosed, which are used to iteratively build the search tree. At the beginning, the closed list list is empty and the frontier list is initialised with the start node(alias root node). The search is terminated when either the goal state is encountered, or PQ frontier is empty. Each iteration performs two steps, described below.\nSelection This step picks the \"most-promising\" leaf node in the search tree. All leaf nodes are stored in the frontier list, implemented as a minimum priority queue on the node's fitness-value f(n) = g(n) + h(n). If the state of the selected node is equal to the goal state, the search is terminated. Else, the next step is performed.\nExpansion This step adds new children nodes to the selected node, thereby expanding the search tree. A child node is only added to the search tree if it is more promising than a future node(in the frontier list) or a previously expanded node(in the closed list). More formally, it is added to the search tree iff there does not exist a node with the same state in either the frontier, or the closed list, with a lower f(\u00b7) value. Finally, the selected node is moved from the frontier to the closed list.\nWe define the search length S of A* as the length of the closed list\u00b9 after termination of the search. The use of h(n) makes A* an informed search algorithm, significantly reducing the size of the search tree, when compared with uninformed search. The path from start to goal, defined as \u03c0 = (\u03c0\u2080, \u03b7\u2081...\u03b7\u2081), is the sequence of l nodes from the root to the goal node. The start-to-goal path with minimum length is called the optimal path, denoted by \u03c0*. A* guarantees that the resulting path will be optimal if the heuristic is admissible\u00b2,\ni.e., h(n) \u2264 h*(n),\u2200n \u2208 N. It can be shown that\nwith h(\u00b7) = h*(\u00b7) and non-trivial tie-breaking, A*\nwill act as an optimal policy with S = |\u03c0*|. An\ninadmissible heuristic, however, does not guarantee\nsub-optimal solutions.\nA* search is summarised in Algorithm 1.\n3.2 Problem Definition\nOur goal is train a language model 0, that, given a node n, can predict the residual between the perfect heuristic and its estimated value, d*(n) = h*(n) \u2013 h(n). From a puzzle instance \u0393, training instances can be derived for this task by considering each node in the search tree. For supervision, the perfect heuristic needs to be calculated, however, doing that for each node will require running A* again for all of them\u00b3. This becomes very expensive, particularly since the S for a single puzzle can be > 10k. Therefore, we only consider nodes on the optimal path, since calculating their h*(\u00b7) is trivial : for any node nj \u2208 \u03c0*, h*(nj) = |\u03c0*| \u2212 j. This is also the conventional setting used in previous works (Chrestien et al., 2021; \u00fas Virseda et al., 2013). Therefore, the training sequences X are given by X = U\u03c0~\u0393\u039d {(nj, d* (nj)), n; \u0395\u03c0}.\n3.3 Loss functions\n2=0\nWe consider two losses to train 9, the L2 loss,\nLL2 = (f(0,n) \u2013 d*(n))2\nand the language modelling objective, given by,\nLLM = -log p(d*(n)|0)\nWe use encoder-decoder transformers for our experiments, and add a regression head \u00d8L2 on the decoder that predicts d*(n) given the < bos > token while training with LL2. With LLM, the (pre-trained) language model head \u00d3LM is used. The encoder input consists of a prompt, along with the details of a node.\n3.4 Inference\nInference involves leveraging the trained LM in A* search. This is done by adding an LM inference with the previous heuristic calculation. During the expansion step, the child nodes are converted into a prompt and collated in a batch, which is used to predict d(n). This is added to h(n), and search continues. Notably, only a single forward pass is performed per expansion. Additionally, we cache these prompts, such that if a state is revisited in a another node m, d(m) can simply be retrieved, rather than predicted. For 0 trained with LLM, we perform top-k decoding, with k = 5, along with self-consistency(Wang et al., 2022), predicting 3 sequences, as this works slightly better in practice.\n3.5 Domains\nWe conduct our experiments on two domains, denoted in Figure 2 and Figure 3. All experiments are conducted on two test splits, the IID and OOD.\nMaze Navigation is a standard maze puzzle that involves finding an unobstructed path from the start to the goal state. The state of a node s(\u00b7) is characterised by the position of the player on the board. The admissible heuristic function used in the training data (and reference solutions) is the manhattan distance between the player and the goal positions. Training and validation is performed on sequences derived from mazes of size 20 x 20. The IID test split thus consists of mazes of the same size, while the OOD split consists of mazes of size 30 \u00d7 30.\nSokoban is a japanese puzzle game, that involves a player pushing one or more boxes to fixed docks. This puzzle is considerably harder than maze, since a few wrong moves can lead to deadlocked situations, rendering the puzzle unsolvable. The state of a node s(\u00b7) is characterised by the position of the player on the board, and the position of the boxes. Note that all boxes and docks are identical. The admissible heuristic function used is the sum of the minimum manhattan distance between the player position and a box, and the sum of manhattan distances between the boxes and their assigned docks. Boxes are assigned to docks by solving the minimum cost assignment problem with the Hungarian algorithm. Training and validation is performed on 2 box problems, while testing is done on a mixture of harder 2, 3 and 4 box problems.\nThe exact data generation process is described in subsection A.1.\n3.6 Metrics\nWe modify the metrics defined in (Lehnert et al., 2024), (i) inverse-length-ratio(ILR) to measure the differences in the search length, (ii) success weighted by cost(SWC) to measure the differences d*(n) = h*(n) \u2013 h(n) (Kirilenko et al., 2023), where the final heuristic used during the search is h(n) + d*(n). However, as with any neural network, there is some error in the prediction, resulting in a value close to d*(n), and in turn close to h*(n). We aim to study two aspects, (i) how this error affects S, and (ii) how it affects optimality of the solution. To achieve this, we perform experiments with an oracle heuristic, that always uses h*(n) during the search. We artificially introduce error into this heuristic to study these effects. The error is introduced in different sections of the tree to identify correlations between the sets of nodes with low error, S and optimality.\nExperiment Here, we use to use the perfect heuristic h*(\u00b7) in certain sections of the tree, and h(\u00b7) ~ N(h*(\u00b7), \u03c3) in other sections. h*(\u00b7) is calculated offline, by running Dijkstra's algorithm on the maze, starting from the goal. The tree is divided into three sets - initial, middle and end. A node n is placed in the initial set if g(n) < |\u03c0*|/3, in the middle set if |\u03c0*|/3 \u2264 g(n) < 2|\u03c0*|/3, and in the end set if g(n) > 2|\u03c0*|/3. all refers to all the nodes in the tree. We perform an ablation, where we use the perfect heuristic in each of these sections, and introduce noise in the others. The upper limit of each metric is given by using h*(\u00b7) in all sections, albeit with trivial tie-breaking. Results are shown in Table 1.\nResults As evidenced by the scores of all, low error on all the sets leads to the best performance. However, achieving that with a learned heuristic is difficult (discussed later). Hence, we try to identify smaller sets with the best scores. Amongst these, with the same o, using h*(\u00b7) on nodes in the end set performs the best on both, ILR-on-solved and ILR-on-optimal, than using it on any other set. Moreover, using h*(\u00b7) in the middle and initial sets has diminishing returns, particularly with higher \u03c3. While there does not seem to be a trend between SWC and Optimal % amongst the three sets, both these metrics go down with increasing \u03c3. This is not surprising, since the heuristic will be less admissible, thereby increasing the possibility of finding suboptimal solutions.\nInference 1. With these experiments, we make the following conclusions, (i) ILR can be improved by having low error in the heuristic for nodes near the goal, (ii) there are diminishing returns to ILR when the heuristic has low error for nodes further away from the goal, and (iii) optimality is directly related to the error in the heuristics.\n4.2 Requirements for Heuristic Learning\nWe have shown that in order to reduce the search length of A*, we need to predict the heuristic more accurately for nodes closer to the goal. Here, we explore how training on these nodes affects generalisation to other nodes, and in turn model performance on ILR.\nExperiment We create four training splits from the puzzles by uniformly sampling nodes (on the optimal path) from the initial, middle, end and all sets. Additionally, we also train on exclusion sets, ~initial, ~middle and ~end, which excludes the corresponding set from the nodes to be sampled. For instance, ~initial is trained on data sampled from the middle and end sets. All training splits have the same size. We train the model on both domains with the following evaluations, (i) mean average error on validation splits containing nodes from each of the aforementioned splits and (ii) and ILR performance achieved by models trained on them. While (i) demonstrates the generalisation capability of the model, (ii) serves to extend the observations of the oracle heuristic to the LM.\nResults Figure 1 shows that each split generalises the best to itself, but shows poor generalisation to the others. The all set achieves the best generalisation to each split. Comparing ILR performance, the LM heuristic performs the best when trained on nodes from the end set, when compared to the performance of middle and initial. However, this is still lesser than the performance of all. This confirms that the trends observed with the oracle heuristic corroborate with those seen with the LM."}, {"title": "5 Proposed Solution", "content": "Quantifying a Node's Contribution to S Based on Inference 1, we propose to quantify the contribution of a node to reducing the search length of a puzzle as,\nC(n) = log(\n\u03c0*\n\u03c0* -g(n))\nC(\u00b7) assigns the highest value to nodes closer to the goal, while accounting for the diminishing returns to reducing search length of nodes further from it. Note that while there can be nodes with g(n) \u2265 |\u03c0*|, they are never added to the tree, thus C(\u00b7) is not defined for them.\nSampling Distribution In order to best generalise to nodes that will maximally reduce the search length, we propose to sample from a distribution D(\u00b7) that prioritises these nodes, based on Equation 6 (as opposed to a uniform distribution). At the same time, in line with Inference 2, we want to enhance the generalisation capabilities of the learned LM. Therefore, this distribution can be given by,\nD(n,\u03c4) = SoftMax(C(n)/\u03c4),\u2200n \u2208 \u03c0*\nwhere \u03c4 denotes temperature. Increasing \u03c4 increases the hardness of the training dataset, thereby increasing the number of nodes sampled from the initial and middle sets.\n6 Experiments\nExperimental Settings Sampling from D(\u03c0, \u03c4) is compared with sampling from a uniform distribution U(n). We also add a full-data baseline, which does not subsample and trains on all nodes on the optimal path. Both LL2 and LLM are compared in Table 3. The full-data baseline is trained on 26.3k nodes for sokoban and 22.3k for maze, while the models with sampling are trained on 8k and 12k nodes, respectively. Subsampling involves sampling equal number of nodes from each puzzle. All models are initialised with code-t5-small with 60M parameters, unless otherwise mentioned. Hyperparameter details are mentioned in subsection A.4\nResults Sampling from D(\u03b7, \u03c4) consistently outperforms uniform sampling on ILR by an average of 4.4% on maze, with a larger improvement of 12.5% on sokoban. On maze, LL2 also outperforms the full-data baseline, which is trained on 46.5% more data points. The results on SWC and Optimal % are mixed; while both models find shorter solutions when trained with nodes sampled from D(n, t) on sokoban, the same cannot be said for maze. Regardless, it can be concluded that both subsampling methods give similar-cost, near-optimal solutions. Interestingly, training on all the data gives higher performance on optimality metrics, which could be a result of lower validation error, due to more training data.\nTraining Target Between LL2 and LLM models, the former consistently outperforms the latter on the IID test split, while on OOD, the results are mixed, with LLM being slightly better, atleast with uniform sampling on sokoban. Since LLM is more aligned with the pre-training of the base model, its effect could be stronger, thus improving generalisation beyond the training data.\nAnother interesting observation is that the hyperparameter 7 used with D(n, \u03c4) is consistently higher for LLM, suggesting that it has a higher preference for data points in the initial set, which can be considered harder than other nodes.\nModel Scale To test the effectiveness of our method while scaling up the LM, we demonstrate similar trends of D(n,r) outperforming U(n) in Table 7. We experiment with two larger models, codet5-base(220M) and codet5-large(770M). Notably, the performance of larger models is not always better than that of smaller models. This could be attributed to the fact that our experiments have been performed in the low-data regime. Studying the effects of scaling up data with parameters is left for future works. The learned heuristics with larger models are more optimal, suggesting lesser error in the predictions.\nComputational Cost It is well accepted in the planning domain that a more informative heuristic is more expensive to compute (Bylander, 1994). While this holds true even for language models, we demonstrate that the informativeness of this learned heuristic is high enough to amortize the additional computational time. To demonstrate this, we demonstrate results on ITR, in Table 4. Recall that ITR is an averaged factor that demonstrates speed-ups in wall-clock time from the reference solution. A value > 1 implies that the solution found by the LM heuristic is faster than the base heuristic. Experiments are performed on the best models, trained with D(n, r) sampling. The ITR is \u226a 1 for maze, which we attribute to the simplicity of the domain. The number of additional nodes searched is already quite low, therefore a reduction does not necessarily find solutions quicker. Since sokoban has a higher number of explored nodes, the ITR on the IID test splits are much closer to 1. On the OOD split, which contains even harder puzzles, that need more iterations to solve, the LM finds solutions much faster than the inexpensive base heuristic, with an ITR > 1. Additionally, LLM is almost 2.5\u00d7 slower on average than LL2, despite the ILR being only 1.1\u00d7 worse. This suggests that while OLM is capable of learning an informative heuristic, the forward pass through the larger linear layer significantly affects efficiency."}, {"title": "7 Conclusion", "content": "In this work, we studied the training data requirements to learn a strong heuristic for A* search to reduce the search length. This was done by disentangling the search process from language model training. In doing so, we found that the search algorithm, requires accurate prediction of heuristics for nodes nearer the goal. For enhanced generalisation of the LM heuristic, we need a greater proportion of the same set of nodes in the training data. Based on these insights, we proposed a mathematical model to quantify the contribution of nodes towards reducing the search length, and derived a distribution from which nodes can be sampled for training. This resulted in significantly better performance than uniform sampling, and in some cases outperforming heuristics trained on double the data. Moreover, we studied the computational requirements of using LM-based heuristics in A* search and find significant wall-clock speedups on harder problems. We hope such a treatment of tree-based frameworks leads to learned heuristics become more commonplace, particularly in LLM reasoning frameworks.\n8 Limitations\nOur study is restricted to classical puzzle domains, maze and sokoban. While we expect our domain-independent analysis to generalise to other problems, the same will need to be thoroughly evaluated. Moreover, since our work focuses on language models used as heuristics, it inherits the bias and fairness concerns associated with language models, which should be taken into consideration when deploying such models. To the best of our knowledge, there are no other negative impacts of our work."}, {"title": "A Appendix", "content": "Algorithm 1 A* Search\nPfrontier \u2190 {nstart}\nPclosed \u2190 {}\nwhile |Pfrontier| > 0 do\nn \u2190 argminn \u2208 Pfrontier f(n) \nif s(n) = sgoal then\nreturn n\nend if\nfor c \u2208 children(n) do\ng(c) \u2190 g(n) + 1\nf(c) \u2190 g(c) + h(c)\nfor m \u2208 Pfrontier \u222a Pclosed do\nif s(c) = s(m)&f(c) < f(m) then\nT\u2190TU{c}\nPfrontier \u2190 Pfrontier \u222a {c}\nend if\nend for\nend for\nPfrontier \u2190 Pfrontier - {n}\nPclosed \u2190 Pclosed \u222a {n}\nend while\nA.1 Data Generation\nMaze We generate mazes with a modified Prim's algorithm\u2075. The start and goal states are randomly chosen until the following criteria are met, (i) length of the optimal plan > O\u1f31, (ii) ratio between length of closed set after search and length of optimal plan is > a = 3.5. If either of these are not met within 10 tries, a new maze is generated. Criterion (i) ensures that the start and goal positions are not too close and (ii) ensures that there are sufficient number of additional expanded nodes. It serves as a surrogate for the measure of hardness h*(ns)/h(ns) where ns is the start node, proposed in (Takahashi et al., 2019). The surrogate is used since it is more aligned with the chosen metrics (ILR) in this work. However, this method only creates a maze with a single path to the goal. To get multiple paths, each node is designated to either be closer to the start, or to the goal, and walls are randomly broken at the boundary of these groups.\nSokoban This dataset is adapted from the open-source boxoban dataset proposed in (Guez et al., 2019). For the training puzzles, we randomly shuffle the provided training set from the \"unfiltered\" split, followed by subsampling B boxes per puzzle. We use the same filters as maze, but with different hyperparameters. The IID test split uses the same criteria, but samples puzzles from the testing set of boxoban. To reduce the data creation time, we constrain the number of iterations required by A* to solve a puzzle between \u1e9emin and \u1e9emax. The OOD split is curated to contain a mix of harder puzzles with varying number of boxes, length of optimal plans and higher number of iterations. All puzzles have size 10 \u00d7 10, \u039f\u03b9 = 20 and a = 6.\nA.2 A* Search Algorithm\nA* search is summarized in Algorithm 1.\nA.3 Prompts\nThe language models have been trained on a regression task with context prompts, which are provided below. Since the experiments are performed with code models, we tailor the prompt accordingly. The same prompt is used for both domains, shown in Figure 4, with the puzzle representations and legend in Figure 2 for Sokoban and Figure 3."}]}