{"title": "Adaptive Few-shot Prompting for Machine Translation with Pre-trained Language Models", "authors": ["Lei Tang", "Jinghui Qin", "Wenxuan Ye", "Hao Tan", "Zhijing Yang"], "abstract": "Recently, Large language models (LLMs) with in-context learning have demonstrated remarkable potential in handling neural machine translation. However, existing evidence shows that LLMs are prompt-sensitive and it is sub-optimal to apply the fixed prompt to any input for downstream machine translation tasks. To address this issue, we propose an adaptive few-shot prompting (AFSP) framework to automatically select suitable translation demonstrations for various source input sentences to further elicit the translation capability of an LLM for better machine translation. First, we build a translation demonstration retrieval module based on LLM's embedding to retrieve top-k semantic-similar translation demonstrations from aligned parallel translation corpus. Rather than using other embedding models for semantic demonstration retrieval, we build a hybrid demonstration retrieval module based on the embedding layer of the deployed LLM to build better input representation for retrieving more semantic-related translation demonstrations. Then, to ensure better semantic consistency between source inputs and target outputs, we force the deployed LLM itself to generate multiple output candidates in the target language with the help of translation demonstrations and rerank these candidates. Besides, to better evaluate the effectiveness of our AFSP framework on the latest language and extend the research boundary of neural machine translation, we construct a high-quality diplomatic Chinese-English parallel dataset that consists of 5,528 parallel Chinese-English sentences. Finally, extensive experiments on the proposed diplomatic Chinese-English parallel dataset and the United Nations Parallel Corpus (Chinese-English part) show the effectiveness and superiority of our proposed AFSP.", "sections": [{"title": "Introduction", "content": "Neural Machine Translation (NMT) (Bahdanau, Cho, and Bengio 2015), the core of which lies in the encoder-decoder architecture, aims to translate texts in the source language into the target language automatically. NMT is a challenging task since it involves translating text among different languages and requires semantic alignment between languages (Fan et al. 2021; Costa-juss\u00e0 et al. 2022; Yuan et al. 2023). Even so, it has made remarkable progress in recent years, especially with the emergence of large language models (LLMs) like ChatGPT & GPT-4 (Ouyang et al. 2022), GLM (Du et al. 2022), Llama (Touvron et al. 2023; Dubey et al. 2024), etc. Benefiting from the increasing scale of parameters and training corpus, these LLMs have gained a universal ability to handle various NLP tasks via in-context learning (ICL) (Brown et al. 2020) or prompt engineering (Chen et al. 2023), which is the process of structuring input text with exemplars and human-written instructions for LLMs, rather than conducting costly task-specific fine-tuning. Unsurprisingly, LLMs with ICL or prompting techniques have shown outstanding potential in machine translation (Zhang, Haddow, and Birch 2023a; Zhu et al. 2024; Zhang et al. 2023) by constructing elaborate instruction or prompts with different prompting strategies.\n\nPioneering work (Zhang, Haddow, and Birch 2023a) conducted a systematic study on prompting strategies for machine translation with the testbed GLM-130B (Zeng et al. 2022), including zero-shot prompting and few-shot prompting. Coincidentally, another work (Zhang et al. 2023) evaluated 15 publicly available language models on machine translation tasks with zero-shot prompting and few-shot learning. (Zhu et al. 2024) explored the multilingual translation capabilities of eight popular LLMs, including ChatGPT and GPT-4. Although all these existing works have shown promising translation performance under the settings of both zero-shot prompting and few-shot ICL, they found that the prompt examples matter the translation performance, which means that LLMs are prompt-sensitive. Using suboptimal examples or instructions can degenerate translation. For example, as shown in Figure 1, the LLM with prompt 1 which is more related to the input text can generate a better translation result than the LLM with prompt 2 according to the BLEU and Comet. In terms of semantic consistency, we can also observe that the translation quality of the LLM with prompt 1 is higher than the LLM with prompt 2. Therefore, selecting suitable adaptive translation demonstrations to elicit the translation capability of an LLM is crucial for high-quality machine translation under in-context learning.\n\nChoosing suitable translation demonstrations for different input text is challenging and nontrivial. To address this issue, we propose an Adaptive Few-Shot Prompting (AFSP) framework to automatically select suitable translation demonstrations for various source input sentences to further elicit the translation capability of an LLM for better machine translation. First, we build a translation demonstration retrieval module based on LLM's embedding to retrieve top-k semantic-similar translation demonstrations from aligned parallel translation corpus. The retrieval top-k translation demonstrations will be filled into the hand-crafted instruction prompt template which is used for various source sentences uniformly. These translation demonstrations are crucial in eliciting the translation capability of an LLM to generate more semantic-consistent target sentences with current input source sentences. M3-Embedding (Chen et al. 2024) shows that conducting semantic retrieval with a combination of different retrieval functionalities can achieve better retrieval performance by improving the discrimination of embeddings. Inspired by this, we construct a demonstration retrieval module based on dense embedding, sparse embedding, and multi-vector embedding to build better input representation for retrieving more semantic-related translation demonstrations. The dense embedding, sparse embedding, and multi-vector embedding of a sentence are generated from deployed LLM which is also used for machine translation. Then, we use a constructed adaptive few-show prompt to obtain the translation result in the target language. There is output diversity in an LLM (Kirk et al. 2023) due to the probabilistic sampling. Different outputs can lead to different translation quality. To mitigate semantic bias caused by LLMs' probabilistic sampling and ensure semantic-consistent translation, we force the deployed LLM to generate multiple output candidates in the target language and rerank these candidates by a rerank model based on a small language model (SLM). Since there is no available large-scale annotated corpus about the translation quality of different translation outputs and annotating such a corpus is costly, we train the rerank model at a lower cost with a self-supervision way by negative sampling with different text perturbation. With the rerank model, we can choose better translation results, ensuring better semantic consistency between source inputs and target outputs.\n\nBesides, Language evolves throughout time. To better evaluate the effectiveness of our AFSP framework on the latest language and extend the research boundary of neural machine translation, we construct a high-quality diplomatic Chinese-English parallel dataset that consists of 5,528 parallel Chinese-English sentences about the question answers with Chinese foreign-ministry spokesman and foreign journalists. These parallel sentences have very high semantic consistency since they are diplomatically oriented and have been rigorously vetted and proofread. Extensive experiments on our proposed diplomatic Chinese-English parallel dataset and United Nations Parallel Corpus (Chinese-English part) show that the effectiveness and superiority of our AFSP.\n\nThe main contributions of this work are concluded as follows: First, we propose an adaptive few-shot prompting (AFSP) framework to automatically select suitable translation demonstrations for various source input sentences to further elicit the translation capability of an LLM for better machine translation. Second, in our AFSP, rather than using other embedding models for semantic retrieval, we build a hybrid demonstration retrieval module based on the embedding layer of the deployed LLM itself to build better input representation for retrieving more semantic-related translation demonstrations. Third, to recognize better translation results, we build a rerank model trained in a self-supervision way with negative sampling, ensuring better semantic consistency between source inputs and target outputs and mitigating semantic bias caused by LLMs' probabilistic sampling. Finally, we construct a high-quality diplomatic Chinese-English parallel dataset and extensive experiments on it and the United Nations Parallel Corpus (Chinese-English part) show the effectiveness and superiority of our proposed AFSP quantitatively and qualitatively."}, {"title": "Related work", "content": "The emergence of LLMs has shown outstanding potential in the field of machine translation. Unlike traditional neural machine translation methods (Bahdanau, Cho, and Bengio 2014; Sennrich, Haddow, and Birch 2015; Wang et al. 2022) which need to be trained with a large-scale machine translation dataset, LLMs were trained on general large-scale corpus and could effectively finish downstream machine translation tasks via prompt engineering or in-context learning without extra model tuning. Current research evaluating and improving the machine translation capabilities of LLMs can be included in two lines. The first line focuses on comprehensive evaluations of LLMs under various translation scenarios, including multilingual translation (Jiao et al. 2023; Hendy et al. 2023), document-level translation (Wang et al. 2023; Hendy et al. 2023), low-resource translation (Jiao et al. 2023; Bawden and Yvon 2023), etc. Another line focuses on designing novel mechanisms to improve the machine translation capabilities of LLMs, including the design of prompt templates (Zhang, Haddow, and Birch 2023b; Jiao et al. 2023), demonstration selection for in-context learning (Zhang, Haddow, and Birch 2023b; Vilar et al. 2022; Garc\u00eda et al. 2023; Yao et al. 2023; Merx et al. 2024; Jiang and Zhang 2024a), self-refinement (Feng et al. 2024b,a), agentic workflow (Wu et al. 2024; Guo et al. 2024), etc."}, {"title": "Adaptive Few-shot Prompting (AFSP)", "content": "We introduce our Adaptive Few-Shot Prompting framework, which first adaptively retrieves suitable demonstrations to fill into the placeholder in the prompt template from the demonstration corpus and then sorts the multiple candidate sampled outputs generated by the deployed LLM to obtain final translation result. In this work, except for the demonstration placeholder, we use fixed prompt words in the prompt template for general task description. The overview of the inference phase in our AFSP framework is shown in Figure 2. AFSP relies on three key components: a translation demonstration corpus, a hybrid demonstration retrieval module based on the deployed LLM-driven embedding, and a re-ranker module. The translation demonstration corpus aims to provide high-quality parallel translation pairs. The retrieval module takes charge of selecting suitable demonstrations to fill into the prompt template for each input source text. The hybrid demonstration retrieval module is train-free and produces a relevance score based on multiple types of embedding ways for each input source and the text in the demonstration corpus by the deployed LLM for machine translation, rather than using third-party embedding models. With the retrieval demonstrations, we fill the demonstrations into a predefined few-shot prompt and enter it into an LLM for multiple candidate output generation. Finally, we deploy a re-ranker module, which is a small language model (SLM) trained in a self-supervised manner, to sort the generated candidate outputs and obtain the final translation result."}, {"title": "Prompt template and Demonstration Corpus", "content": "In AFSP, as shown in Table 1, we only use a fixed prompt template with variable placeholders inspired from prior works (Jiang and Zhang 2024b; Agarwal et al. 2024). We do not focus on the diversified design of prompt templates in this work and achieve adaptive prompts for different source text by filling suitable demonstrations according to the retrieved results from the demonstration corpus. Demonstration corpus can be any high-quality parallel translation corpus. In practice, it can be expanded with extra parallel translation corpus. For the sake of simplicity, we simply use the training set from specific translation tasks as the demonstration sources to build the demonstration corpus. For example, we use the training part in the UN Open Corpus v1.0 (Chinese and English versions) as the demonstration corpus when conducting Chinese-English bilingual translation. Similarly, we also use the training subset of our newly constructed Diplomatic corpus as the demonstration corpus when conducting machine translation on its test set."}, {"title": "Hybrid Demonstration Retrieval", "content": "As claimed in the pioneering works (Zhang, Haddow, and Birch 2023a; Zhang et al. 2023; Zhu et al. 2024), the number, quality, and semantic similarity of prompt examples matter the translation performance. Therefore, it is crucial to adaptively retrieve high-quality and highly semantic similar demonstrations for different input texts to achieve better LLM prompting for better eliciting the translation capability of an LLM. To achieve this goal, superior embedding-based semantic representation is essential. (Chen et al. 2024) shows that a combination of different embedding-based retrieval functionalities can improve the discrimination of embedding-based semantic representation. Inspired by them, we build an embedding-based hybrid demonstration retrieval module for demonstration retrieval in a training-free way by utilizing the embedding matrix of the deployed LLM that conducts machine translation. The retrieval results are sorted by a weighted combination of relevance scores based on dense embedding, sparse embedding, and multi-vector embedding. The reason we use the deployed LLM as the embedding generator rather than other embedding models is that LLM is pre-trained with large-scale general corpus and can represent text accurately.\n\nFormally, given a query text q in the source language, the demonstration retrieval module can retrieve translation demonstration $(d_{src}, d_{tgt})$ from the corpus D based on the hybrid relevance score $s_{rank}$ of q and $d_{src}$: $(d_{src}, d_{tgt}) = f_h(q, D)$. Here, $f_h(\\cdot)$ denotes the retrieval function based on the hybrid relevance score. For the text q, dense embedding, sparse embedding, and multi-vector embedding can be formalized separately as follows: 1) dense embedding $e_{dense}$: the text q is first transformed into the embedding vectors $E_q$ based on the embedding layer in the LLM. Then, we obtain $e_{dense}$ by conducting max pooling on $E_q$ and normalization: $e_{dense} = norm(MaxPooling(E_q))$. 2) sparse embedding $e_{sparse}$: the embedding vector $E_q$ is also used to estimate the importance of each token to facilitate lexical representation. For each token t within the text q, the token weight is calculated as $w_{q_t} = ReLU (W_{sparse}E_q[t])$, where $ReLU$ is rectified linear unit and $W_{sparse} \\in R^{H \\times 1}$. H is the dimension size of the embedding and $W_{sparse}$ is a projection matrix mapping token embedding into a float number as its importance. It is only initialized by Gaussian initialization since we found Gaussian initialization is enough to make the model work fine without any model training. 3) multi-vector embedding $e_{multi}$: it is an extension of dense embedding by utilizing the entire output embeddings for text representation: $e_{multi} = norm(W_{multi}E_q)$, where $W_{multi} \\in R^{H \\times H}$ is a projection matrix initialized by Gaussian initialization. With the above three embeddings with different granularities, we can calculate three relevance scores for multi-granularity retrieval. For dense retrieval, given a text q and source demonstration p, we can compute the relevance score $S_{dense}$ by the inner product between the two embeddings $e_{dense}^q$ and $e_{dense}^p$ as follows: $S_{dense} = e_{dense}^q \\cdot e_{dense}^p$. For sparse retrieval, we can compute $S_{sparse}$ by the joint importance of the co-existed tokens (denoted as q$\\cap$p) as follows: $S_{sparse} = \\sum_{t \\in q \\cap p} (W_{q_t} \\times W_{q_t})$. For multi-vector retrieval, we can compute $S_{multi}$ by late interaction as follows:\n\n$S_{multi} = \\frac{1}{l_q} \\frac{1}{l_p} max_{i=1}^{l_a} \\sum_{j=1}^{l_b} e_{multi}^q[i] \\cdot e_{multi}^p[j]$, where $l_q$ and $l_p$ are the lengths of text q and source demonstration p. Based on the above three relevance scores, we conduct the demonstration retrieval in a hybrid process according to $S_{rank}$ which can be defined as follows:\n\n$S_{rank} = \\lambda_1 \\times S_{dense} + \\lambda_2 \\times S_{sparse} + \\lambda_3 \\times S_{multi} \\eqno{(1)}$\n\nwhere $\\lambda_1, \\lambda_2$, and $\\lambda_3$ are three hyper-parameters to adjust the weights of three retrieval functionality."}, {"title": "Result Re-ranking", "content": "There is output diversity in an LLM (Kirk et al. 2023) due to the probabilistic sampling. The different outputs may have different semantic biases, which will influence the final translation performance. To mitigate this issue, we force the deployed LLM to generate multiple output candidates in the target language and rerank these candidates by a re-ranker model based on a small language model (SLM). The re-ranker takes charge of scoring the output candidates. However, training this re-ranker is challenging since there is available large-scale annotated corpus about the translation quality of different translation outputs and annotating such a corpus is costly. Therefore, we design a self-supervised training method to train such a re-ranker at a low cost by conducting negative sampling with different text perturbations.\n\nFormally, given the parallel translation corpus $D = {<d_{src}, d_{tgt}>, <d_{src}, d_{tgt}>,...,<d_{src}, d_{tgt}>}$ where $d_{src}$ and $d_{tgt}$ are the texts in the source language and the target language respectively. To construct a dataset $D' = {<d'_{tgt}, s_1>, <d'_{tgt}, s_2>, ..., <d'_{tgt}, s_N>}$ to train the re-ranker, we can disturb the $d_{tgt}$ by multiple degeneration operation set A including converting to the parallel text $(Parallel)$, back translation $(Back)$, inserting source text $(Insert)$, spelling mistake $(Se)$, repeated translation $(Ret)$, synonym replacement $(Replace)$. $d'_{tgt}$ and $s_i$ are the degenerated text and corresponding quality score, respectively. We define the quality score of the original target text $d_{tgt}$ as 1. Assuming that B contains a null operation that means we just copy the original text into D' and all possible combinations of the degeneration operation in A, for each possible combination $b_i \\in B$, we can obtain the degenerated text $d'_{tgt}$ and calculate its score s' as follows:\n\n$d'_{tgt} = f_{b_i} (d_{tgt}), b_i \\in B$ \n\n$s'= 1 - 0.2|b_i| \\eqno{(2)}$\n\nwhere $f_{b_i}()$ represents the degeneration function with the degenration operation combination $b_i$ and $|b_i|$ is the number of degeration operations in $b_i$. In this way, we can generate a large-scale dataset D' from the parallel translation corpus D to train the re-ranker in a self-supervised manner.\n\nWe deploy BERT (Devlin et al. 2019) as the backbone of the SLM in the Re-ranker. For Chinese-English translation, we use Bert-large-cased while we use Bert-based-Chinese as the SLM for English-Chinese translation. Given a degenerated text $d'_{tgt}$ and its quality scores, the re-ranker takes the degenerated text $d'_{tgt}$ as input and predicts a quality assessment score $s_{rerank}$ as close to the annotated score s as possible. The re-ranker calculates the quality assessment score by applying a Linear layer to map the output encoding of [CLS] token into 1-D float number followed by the Sigmoid function to normalize the output to between 0 and 1. To optimize the re-ranker, we adopted Mean Squared Error as its objective function to enable the re-ranker to predict quality assessment scores. Therefore, the re-ranker and its learning objective can be modeled as follows:\n\n$E = BERT(d'_{tgt})$\n\n$s_{rerank} = Sigmoid(Linear(E[0]))$\n\n$\\mathcal{L} = || s_{rerank} - s'||_2 \\eqno{(3)}$"}, {"title": "Experiments", "content": "To validate the effectiveness of the proposed AFSP, we first crawled a high-quality parallel Chinese-"}, {"title": "Experiment Result", "content": "Main Results Table 3 and Table 4 show the performance of our AFSP and baselines on different translation datasets and different LLMs. Compared to baselines, our AFSP demonstrates superior performance by always generating higher-quality translation according to various metrics. For instance, when Llama-3-8B translates from Chinese to English on the UN, our ASFP achieves significant improvements over KNN Few-shot with 9.31 improvement in BLEU-4, 7.3 improvement in METEOR, 7.35 improvement in ROUGE-L, and 1.96 improvement in COMET-Kiwi. Other models also show significant metric improvements in translation across different datasets and LLMs, highlighting the effectiveness of our AFSP method.\n\nTo further validate the effectiveness of the AFSP, we also conducted a human evaluation of both two datasets and two translation directions to compare AFSP with baselines. For each translation direction, we randomly selected 5 examples from each dataset. Participants judged the options based on fluency, accuracy, and style retention by selecting the sentence they deemed best. We tested the translation results generated by Llama3-8B and invited 14 teachers or students fluent in English or Chinese to participate in the evaluation for each translation direction. To avoid bias, the output order was randomized. The evaluation results in Table 5 demonstrate that our AFSP outperforms all baselines in fluency, semantic accuracy, and style consistency.\n\nIn our hybrid demonstration retrieval, we use the embedding model in the deployed LLM to compute relevance scores. To show the effectiveness of using the embedding model of the deployed LLM model, we conduct an ablation study on various embedding models including BGE-M3, E5-Large, BGE-large, and BCE. The results in Table 6 show using the embedding model of the deployed LLM model is a better choice than using third-party embedding models.\n\nThe hybrid demonstration retrieval uses multiple retrieval functions to compute relevance scores. To investigate the influence of different weights a1, a2, and a3, we conduct experiments with ChatGLM3-6B on the Diplomatic Corpus by setting different a1, a2, and 03. The results in Table 7 show that a1, a2, and a3 are set to 0.4, 0.4, and 0.2 can achieve the best performance on most of the metrics.\n\nWe verify the effects of different numbers of demonstrations for few-shot prompting by using the Diplomatic Corpus dataset on ChatGLM3-6B. The results in Table 8 show the best translation performance can be achieved when the number of demonstrations is 3."}, {"title": "Conclusion", "content": "In this work, we propose an adaptive few-shot prompting (AFSP) framework to automatically select suitable translation demonstrations for various source input sentences to further elicit the translation capability of an LLM for better machine translation. First, we retrieve top-k semantic-similar translation demonstrations from aligned parallel translation corpus based on hybrid demonstration retrieval. Then, to ensure better semantic consistency between source inputs and target outputs, we force the deployed LLM itself to generate multiple output candidates in the target language with the help of translation demonstrations and rerank these candidates. Besides, to better evaluate the effectiveness of our AFSP framework on the latest language and extend the research boundary of neural machine translation, we construct a high-quality diplomatic Chinese-English parallel dataset that consists of 5,528 parallel sentences. Extensive experiments on the proposed Diplomatic dataset and UN show the effectiveness and superiority of our AFSP."}]}