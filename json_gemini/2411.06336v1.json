{"title": "Balancing Power and Ethics: A Framework for Addressing Human Rights Concerns in Military AI", "authors": ["Mst Rafia Islam", "Azmine Toushik Wasi"], "abstract": "AI has advanced significantly in recent years, resulting in a wide range of applications in both the civilian and military sectors. The military, which is continually seeking innovations for more effective, faster, and stronger technology or weapons, sees AI as a perfect answer to address these needs [8, 11].\nAI technologies offer many advantages, such as increased operational efficiency, precision targeting, and reduced human casualties [18, 14]. However, these advancements come with profound ethical and legal concerns, particularly regarding potential human rights violations [3, 14, 6].\nUse of AI in military applications raises serious concerns [12, 1, 11]. Autonomous weapons systems, capable of making life-or-death decisions without human intervention, threaten the right to life and violate international humanitarian law. While AI-enhanced surveillance systems improve data collection, they can lead to widespread privacy violations and unnecessary monitoring of individuals [11]. Additionally, inherent biases in AI systems may result in discriminatory practices, exacerbating existing disparities and violating principles of equality, non-discrimination, and human rights [3, 14].\nMotivated by these concerns, in this work, we propose a novel three stage framework to address human rights issues in the design, deployment, and use of military AI, with each aspect consisting of multiple components. Our work aims to propose a comprehensive framework for evaluating human rights violations by military AI, addressing both technical and statutory aspects. By examining the ethical implications and regulatory challenges, this framework seeks to provide a balanced approach to harnessing the benefits of AI in military operations while safeguarding human rights.", "sections": [{"title": "1 Introduction", "content": "AI has advanced significantly in recent years, resulting in a wide range of applications in both the\ncivilian and military sectors. The military, which is continually seeking innovations for more effective,\nfaster, and stronger technology or weapons, sees AI as a perfect answer to address these needs [8, 11].\nAI technologies offer many advantages, such as increased operational efficiency, precision targeting,\nand reduced human casualties [18, 14]. However, these advancements come with profound ethical\nand legal concerns, particularly regarding potential human rights violations [3, 14, 6].\nUse of AI in military applications raises serious concerns [12, 1, 11]. Autonomous weapons systems,\ncapable of making life-or-death decisions without human intervention, threaten the right to life\nand violate international humanitarian law. While AI-enhanced surveillance systems improve data\ncollection, they can lead to widespread privacy violations and unnecessary monitoring of individuals\n[11]. Additionally, inherent biases in AI systems may result in discriminatory practices, exacerbating\nexisting disparities and violating principles of equality, non-discrimination, and human rights [3, 14].\nMotivated by these concerns, in this work, we propose a novel three stage framework to address\nhuman rights issues in the design, deployment, and use of military AI, with each aspect consisting of\nmultiple components. Our work aims to propose a comprehensive framework for evaluating human\nrights violations by military AI, addressing both technical and statutory aspects. By examining the\nethical implications and regulatory challenges, this framework seeks to provide a balanced approach\nto harnessing the benefits of AI in military operations while safeguarding human rights."}, {"title": "2 Framework", "content": "As shown in Figure 1, this framework addresses human rights concerns in the Military AI lifecycle\nacross three key phases: Design, Deployment, and Use, with each phase having several components.\nBelow, we discuss each phase and its components in detail."}, {"title": "2.1 Design Phase", "content": "Design phase involves creating military AI systems, focusing on algorithms and functionalities. It\nsets the foundation for AI behavior, ensuring it operates correctly, without bias and discrimination."}, {"title": "Targeting Errors of Autonomous Weapons.", "content": "Al-powered autonomous weapons systems can\nmake life-and-death decisions without human intervention [4]. AI systems can make errors in\nidentifying and targeting individuals, leading to unintended casualties and injuries among civilians\n[2]. Implementing ethical guidelines in the design and development of military AI systems can\nensure compliance with human rights standards. Moreover, incorporating human oversight in AI\ndecision-making processes ensures accountability and ethical compliance. Human-in-the-loop (HITL)\n[13] systems allow human operators to intervene and override AI decisions when necessary."}, {"title": "Bias and Discrimination.", "content": "Al systems can perpetuate or even exacerbate existing biases, leading to\ndiscriminatory practices in targeting or profiling based on race, ethnicity, or other characteristics [10].\nBy developing algorithms that detect and mitigate biases in AI systems can prevent discriminatory\noutcomes. This not only enhances the fairness of AI applications but also helps in building public\ntrust and ensuring compliance with ethical standards."}, {"title": "2.2 In Deployment Phase", "content": "In Deployment phase involves integrating military AI systems into specific environments, focusing\non their usage while emphasizing ethical considerations and legal compliance."}, {"title": "Surveillance and Privacy.", "content": "AI-driven surveillance systems can infringe on individuals' privacy\nrights, leading to unauthorized monitoring and data collection [7]. Implementing strong data privacy\nand security measures to protect sensitive information collected and processed by military AI systems.\nThis includes encryption, access controls, and regular security assessments."}, {"title": "Deployment of the weapon.", "content": "To ensure maximum efficacy while minimizing collateral damage\n[19], the automated weapon system must be deliberately positioned in a location that maximizes\noperational range and precision. In order to prevent unintended harm, this entails a detailed investiga-\ntion of the terrain, potential targets, and civilian areas [5]. Continuous monitoring and changes are\nrequired to ensure its efficacy and safety. Additionally, real-time data analysis and feedback loops\nshould be integrated to adapt to dynamic battlefield conditions, ensuring that the system remains\nresponsive and minimizes risks to non-combatants [2]."}, {"title": "2.3 During/After Use Phase", "content": "During/After Use phase encompasses the actual operation of military AI systems during conflicts or\nmissions. It assesses the AI's performance in real-time, monitoring for accountability and compliance\nwith human rights and legal standards."}, {"title": "Concerns with Accountability.", "content": "Transparency of AI decision-making processes complicates\naccountability, making it difficult to attribute responsibility for human rights violations [15]. When\nAl systems make decisions, it creates a gap in justice and responsibility, as it becomes challenging\nto hold any individual or entity accountable for wrongful actions [16]. By ensuring Al systems\nare transparent and their decision-making processes are explainable, accountability issues can be\nresolved. Ethical guidelines, such as the Asilomar AI Principles [9], emphasize the importance of\ntransparency, accountability, and human oversight in the development and deployment of military AI.\nAdhering to these principles can help mitigate the violations of human rights."}, {"title": "Bias and Discrimination.", "content": "Al systems can perpetuate or worsen existing biases, leading to\ndiscriminatory practices in targeting or profiling based on race, ethnicity, or other characteristics [7].\nTo mitigate these biases post-deployment, continuous monitoring and auditing of AI systems are\nessential. Implementing feedback loops to identify and correct biased outcomes can ensure fairness\n[17]. This approach addresses biases and helps build public trust in AI technologies by demonstrating\na commitment to ethical standards and accountability."}, {"title": "2.4 Violation of International Humanitarian Law", "content": "Deployment of AI in military operations poses serious risks to International Humanitarian Law\n(IHL), which aims to protect non-combatants. Autonomous weapons can make decisions without\nhuman oversight, leading to indiscriminate attacks that violate the principle of distinction [2]. AI\ntargeting decisions may also compromise proportionality, as these systems might not effectively\nbalance military advantage with civilian harm [5]. The lack of transparency and accountability in AI\ndecision-making complicates compliance with IHL, hindering the investigation and attribution of\nunlawful actions. Adherence to IHL requires strong ethical guidelines, oversight mechanisms, and\ninternational cooperation to prevent violations and uphold the laws of war."}, {"title": "3 Challenges and Future Work", "content": "Several challenges may hinder the proper implementation of our framework. There is no global\nagreement on the ethical use of military AI. Groups like the Campaign to Stop Killer Robots push for\na ban, but leading AI nations resist binding commitments. Accountability is also complex, as it's hard\nto determine who is responsible when AI systems cause harm\u2500creators, commanders, or the AI itself.\nAdditionally, AI evolves quickly, outpacing legal systems and creating oversight gaps that could\nbe exploited. Future research should focus on establishing international agreements and fostering\ncollaboration between engineers, ethicists, and legal experts to keep pace with AI advancements."}]}