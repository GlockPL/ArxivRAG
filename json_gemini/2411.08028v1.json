{"title": "Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data", "authors": ["Juanhui Li", "Sreyashi Nag", "Hui Liu", "Xianfeng Tang", "Sheikh Sarwar", "Limeng Cui", "Hansu Gu", "Suhang Wang", "Qi He", "Jiliang Tang"], "abstract": "In real-world NLP applications, Large Language Models (LLMs) offer promising solutions due to their extensive training on vast datasets. However, the large size and high computation demands of LLMs limit their practicality in many applications, especially when further fine-tuning is required. To address these limitations, smaller models are typically preferred for deployment. However, their training is hindered by the scarcity of labeled data. In contrast, unlabeled data is often readily which can be leveraged by using LLMs to generate pseudo-labels for training smaller models. This enables the smaller models (student) to acquire knowledge from LLMs (teacher) while reducing computational costs. This process introduces challenges, such as potential noisy pseudo-labels. Selecting high-quality and informative data is therefore critical to enhance model performance while improving the efficiency of data utilization. To address this, we propose LLKD that enables Learning with Less computational resources and less data for Knowledge Distillation from LLMs. LLKD is an adaptive sample selection method that incorporates signals from both the teacher and student. Specifically, it prioritizes samples where the teacher demonstrates high confidence in its labeling, indicating reliable labels, and where the student exhibits a high information need, identifying challenging samples that require further learning. Our comprehensive experiments show that LLKD achieves superior performance across various datasets with higher data efficiency.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) such as LLAMA (Touvron et al., 2023) and GPT-4 (Achiam et al., 2023) have demonstrated superior language understanding abilities in many real-world NLP applications (Schopf et al., 2022; Thirunavukarasu et al., 2023; Zhao et al., 2023) due to the vast knowledge acquired from pre-training on extensive corpora. However, deploying LLMs is resource-intensive with high memory requirements, computational costs, and increased latency during inference, especially when additional fine-tuning is needed for specific tasks (Shoeybi et al., 2019). To tackle these limitations, smaller models (Liu, 2019; Devlin, 2018) are often preferred due to their lower resource demands. Nonetheless, smaller models are not as powerful as LLMs (Kaplan et al., 2020) and typically require further training for specific tasks using labeled data, as they usually do not have the capacity to capture broad knowledge. Without the guidance of labeled data, self-supervised training on smaller models may lead to suboptimal performance, as these models struggle to generalize across diverse tasks and often fail to learn task-specific features effectively (Goyal et al., 2019). This challenge is further hindered by the high cost of obtaining task-related labeled data. While unlabeled data is generally more abundant, it cannot be directly utilized without proper labeling, posing a significant challenge for model training.\nOne promising approach is to use LLMs to generate pseudo-labels for unlabeled data, which can then be used to train smaller models. This strategy allows smaller models to benefit from the extensive knowledge embedded in the LLM while reducing computational costs. This process can be seen as a form of knowledge distillation (Mishra and Sundaram, 2021; Zhou et al., 2023). However, this approach presents challenges. Pseudo-labels generated by LLMs may be noisy or unreliable, potentially degrading the performance of the student model. In some cases, the availability of unlabeled data may also be limited, particularly for domain-specific tasks where data collection is challenging (Adams et al., 2017; Xiao et al., 2018). Thus, achieving data efficiency is crucial-not only to reduce the impact of noisy pseudo-labels but also to ensure that representative data samples are selected for optimal training.\nA potential solution is to select data that not only has high pseudo-label quality but is also informative for the student model. However, as the student model continuously updates during training, identifying informative knowledge throughout this process remains a challenge. Several existing works have proposed methods for data selection in the knowledge distillation process (Mishra and Sundaram, 2021; Zhou et al., 2023; Li et al., 2021). However, most of these approaches (Zhou et al., 2023; Li et al., 2021) rely on datasets with true labels and do not consider the challenge of noisy pseudo-labeled samples, which can lead to suboptimal performance. While some methods (Kontonis et al., 2024; Iliopoulos et al., 2022) address unlabeled data, they often overlook the student model's learning progress or fail to consider data efficiency. Therefore, it is beneficial to develop a method that enables the student model to learn from the most valuable data while improving the data efficiency by reducing the amount of training data required.\nTo address these challenges, we propose LLKD that enables Learning with Less computational resources and less data for Knowledge Distillation from LLMs. It is an adaptive sample selection method for each training step that considers the student's dynamic learning status. We prioritize samples where the teacher model exhibits high confidence in its labeling, indicating reliable pseudo-labels (Mishra and Sundaram, 2021), and the student model shows high uncertainty, pointing to challenging examples that require further learning (Zhou et al., 2023). Specifically, we design two types of thresholds at each training step based on teacher confidence and student uncertainty, selecting overlapping samples that meet these criteria from both models' perspectives. This data selection strategy promotes efficient knowledge transfer from the LLM to the smaller model, ensuring that the most informative samples are used for training while reducing the amount of data needed, thereby improving data efficiency. We apply LLKD to a fundamental NLP task, text classification, and present comprehensive evaluation across various datasets. The results demonstrate that LLKD significantly enhances model performance, achieving superior results with higher data efficiency."}, {"title": "2 Related Work", "content": "Knowledge Distillation. Knowledge distillation (Mishra and Sundaram, 2021; Zhou et al., 2023; Xu et al., 2023; Li et al., 2021; Kontonis et al., 2024) has been used to transfer knowledge from a cumbersome teacher model to a lightweight student model. Most traditional methods focus on datasets with true labels, and some have explored data selection during this process. For example, Mishra and Sundaram (2021) proposes a threshold based on learning epochs to select hard samples for student. Similarly, Li et al. (2021) and Xu et al. (2023) select samples with high student uncertainty by setting a fixed sampling ratio. Zhou et al. (2023) introduces a reinforcement learning-based selector to measure student uncertainty in different ways. However, most of these methods rely on true labels and do not address the issue of noisy pseudo-labels from the teacher model. While some approaches focus on unlabeled data (Lang et al., 2022; Dehghani et al., 2018), they often overlook data efficiency or fail to account for the student's evolving nature to identify informative samples for student. For instance, Kontonis et al. (2024) generates new soft labels by combining the student's soft labels with denoised teacher labels, and Iliopoulos et al. (2022) reweights the student's loss function to simulate loss with noise-free pseudo-labels.\nThresholding Methods. In classification tasks with large amounts of unlabeled and noisy data, various confidence-based thresholding methods (Zhang et al., 2021; Sohn et al., 2020; Wang et al., 2023; Chen et al., 2023) have been proposed to prioritize samples with high confidence. For example, FlexMatch (Zhang et al., 2021) employs a curriculum learning approach to flexibly adjust the threshold for each class based on the number of learned samples. FreeMatch (Wang et al., 2023) and SoftMatch (Chen et al., 2023) use confidence-based thresholds, with FreeMatch considering both global and class-wise learning status, while SoftMatch weights the loss function using a Gaussian function. However, these methods often rely on limited labeled data, which can result in suboptimal performance with the self-training manner.\nUnsupervised Text Classification. It aims to categorize text without labeled data. A common approach is similarity-based methods (Abdullahi et al., 2024; Schopf et al., 2022; Yin et al., 2019), which generate embeddings for both input texts and labels, and then match texts to labels based"}, {"title": "3 Method", "content": null}, {"title": "3.1 Notations", "content": "Given the unlabeled dataset $X = \\{X_1, X_2, ..., X_N \\}$ where $x_i \\in X$ is the input text, we do not have labels for the samples in X. However, we are provided with a label set $Y = \\{Y_1, Y_2, \u2026, Y_K\\}$ consisting of K possible labels, where each $y_i \\in Y$ is the i-th label. For model training, we use X as the training set. To evaluate the performance, we have the validation and test set $X_{dev} = \\{(x_j, y_j)\\}_{1}^{M_1}$, $X_{test} = \\{(x_j, y_j)\\}_{1}^{M_2}$, where both sets contain the ground truth labels. The validation set is used to select the model.\nWe leverage the unlabeled data to train a smaller model using pseudo-labels generated by the teacher LLM. Let $G_t$ denote the teacher LLM and $G_s$ represent the student language model. The framework of LLKD is illustrated in Figure 1. It consists of three key components: 1) the teacher model, which generates pseudo-labels along with corresponding confidence scores. It remains fixed and is only used for inference; 2) the student model, which is trained on these pseudo-labels and generates uncertainty estimates for each sample; and 3) a data selection process, which selects samples based on both pseudo-label quality and informativeness for the student."}, {"title": "3.2 Teacher Model", "content": "Given the strong performance of LLMs in handling NLP tasks, we use LLAMA (Touvron et al., 2023) as the teacher model to generate pseudo-labels. Typically, utilizing LLMs involves applying prompts with target inputs to guide the model in generating customized, task-specific outputs. These prompts are carefully designed templates that frame the input text for the LLM. They often include a few examples to illustrate the task, enabling the model to generalize from a limited amount of data by leveraging its extensive pre-training knowledge. This approach eliminates the need for additional fine-tuning, thereby enhancing the LLMs' flexibility and efficiency in real-world applications. We select a few examples from the validation set to construct the prompt. More details are given in the section A.1 in the Appendix. Formally, we use P(xi) to denote the prompt for input text xi, then the pseudo-label is defined as follows:\n$y(x_i)^{pl} = arg max_y p_{G^+}(y | P(x_i))$ (1)\nwhere $p_{G^+}(y | P(x_i))$ is the probability vector of the teacher over the labels given the input prompt P(xi). To measure the teacher's confidence in"}, {"title": "3.3 Student Model", "content": "For the student model, we utilize a smaller and more efficient pretrained language model, such as RoBERTa (Liu, 2019) denoted as $G_s$. It is observed that framing the classification task as a masked language modeling task within the prompt-learning framework yields superior performance compared to standard fine-tuning (Yu et al., 2023; Yin et al., 2019). This approach facilitates a clearer understanding of the task for the pretrained language model and enhances generalization (Yin et al., 2019). For example, a possible prompt template could be defined as $P_s$ = [xi, it was [MASK]], where the [MASK] token is used to predict the label. To measure the student model's uncertainty, we adopt a commonly used entropy-based approach, which relies on the probability distribution over the label set. Using this template, we can derive the probability distribution over the label set Y:\n$p(y | x_i) = p([MASK] = V(y) | P_s)$\n$= \\frac{exp(h(y)^T h_{[MASK]})}{\\Sigma_{y' \\in Y} exp(h(y')^T h_{[MASK]} )}$ (3)\nHere, $V(\u00b7)$ is the verbalizer that maps each label y to a word or words in the vocabulary of the student model, and h represents the token embedding generated by G. Specifically, $h_{[MASK]}$ denotes the embedding of the [MASK] token, while $h_{V(y)}$ corresponds to the embedding of the label word $V(y)$. As the student model is updated at each training step t, the uncertainty may vary at each step. We formally define the uncertainty at each step as:\n$U_t(x_i) = - \\sum_{j=1}^K P(y_j | x_i) log p(y_j | x_i)$ (4)\nwhere $U_t(x_i)$ is the uncertainty of the student model on the sample $x_i$ at the t-the training step."}, {"title": "3.4 Data Selection", "content": "We propose leveraging teacher confidence and student uncertainty to identify samples with high-quality pseudo-labels and those that present challenging knowledge. This is motivated by some empirical observations showing that teacher confidence can assess pseudo-label quality, while student uncertainty indicates sample informativeness, as illustrated in Figure 2. Additional results on more datasets are presented in Section A.2 in the Appendix which show similar observations. Based on the validation set of the PubMed-RCT-20k dataset (detailed in section 4.1), we generate pseudo-labels and compute teacher confidence with the teacher model. The confidence scores are grouped into bins, and the average accuracy is calculated for each bin, as shown in Figure 2(a). Similarly, the student model provides predictions and student uncertainty, which are also binned to calculate average accuracy, as illustrated in Figure 2(b). The student uncertainty varies across training steps, and the figure shows results from one step (we observe similar trends in other steps). We observe that higher teacher confidence typically correlates with higher accuracy, signifying high-quality pseudo-labels, whereas higher student uncertainty generally corresponds to lower accuracy, reflecting incorrect predictions and harder samples.\nTo effectively leverage the two signals, we introduce two thresholds: one based on teacher confidence and the other on student uncertainty. As the student model continuously updates during training and its learning status may differ across classes, necessitating an adaptive approach to define the thresholds as training progresses. Inspired from the FreeMatch framework (Wang et al., 2023), which has shown strong performance in the image domain, we design the thresholds to incorporate both the global training status and the local class-specific status within each batch. Although the teacher's confidence is not updated, we empirically found that determining the teacher confidence threshold on a batch-wise basis yields superior performance. Consequently, we adopt a similar batch-wise approach when setting the teacher confidence threshold. Formally, the global component of the student uncertainty threshold, which reflects the overall training status, is defined as follows:\n$\\tau_S^t = \\lambda_S \\tau_{t-1}^S + (1 - \\lambda_S) \\frac{1}{B} \\sum_{i=1}^B U_t(x_i)$ (5)\nwhere t is the training step, $\\tau_0^S$ = 0 and B is the batch size. $\\lambda_S \\in (0,1)$ is the momentum in Exponential Moving Average (EMA) over previous batches for a more stable estimation. And the local part is computed based on each class y:\n$p_t^f(y) = \\lambda_S p_{t-1}^f(y) + \\frac{\\sum_{i=1}^B 1(y(x_i)^{pl} = y) U_t(x_i)}{\\sum_{i=1}^B 1(y(x_i)^{pl} = y)}$ (6)\nwhere $p_0^f(y)$ = 0, 1(\u00b7) is the indicator function. It is 1 when $y(x_i)^{pl} = y$, and 0 otherwise. Then we obtain the final threshold based on the student uncertainty by integrating the global and local parts:\n$\\tau_t^f(y) = MaxNorm(p_t^f(y))^{\\beta_{S1}} * (\\tau_S^t)^{\\beta_{S2}}$\n$= (\\frac{p_t^f(y)}{max \\{ p_t^f(y) : y \\in Y \\} })^{\\beta_{S1}} * (\\tau_S^t)^{\\beta_{S2}}$ (7)\nwhere MaxNorm is the Maximum Normalization, $\\beta_{S1}$ and $\\beta_{S2}$ are two hyper-parameters used to control the contribution of the local and global threshold. Similarly, by replacing the $U_t(x_i)$ as the teacher confidence $C_i$ in Eq.(5) and Eq.(6), we can obtain the final threshold $\\tau_t^+(y)$ with its own momentum $\\lambda_T$ and hyper-parameters $\\beta_{T1}$ and $\\beta_{T2}$. Then we combine $\\tau_t^f(y)$ and $\\tau_t^+(y)$ to select samples with high teacher confidence and high student uncertainty. The loss function for one batch at step t is defined as follows:\n$L = \\frac{1}{B} \\sum_{i=1}^B 1(U_t(x_i) \\geq \\tau_t^f(y(x_i)^{pl})) * 1(C_i \\geq \\tau_t^+(y(x_i)^{pl})) * H(y(x_i)^{pl}, p(y|x_i))$ (8)"}, {"title": null, "content": "where H is the cross entropy loss. Furthermore, we apply a weighting scheme to the selected samples based on teacher confidence and student uncertainty. Specifically, we use a simple approach by adding the teacher confidence and student uncertainty to calculate the weight. The modified loss function is then defined as follows:\n$L_w = \\frac{1}{B} \\sum_{i=1}^B 1(C_i) * (f(C_i)+f(U_t(x_i))) * L_i$ (9)\nwhere f is a sum normalization function across samples in the batch, $L_i$ is the loss of each sample in the batch in Eq (8), i.e., $L = \\frac{1}{B} \\sum L_i$"}, {"title": "4 Experiment", "content": "In this section, we conduct comprehensive experiments to validate the performance of LLKD. We will introduce the experimental settings, followed by results and their analysis."}, {"title": "4.1 Experimental Settings", "content": "Datasets. We use five datasets from various domains: PubMed-RCT-20k (Dernoncourt and Lee, 2017), extracted from medical papers; Yahoo! Answers (Zhang et al., 2015), a collection of question-answer pairs from the Yahoo! Answers platform; Emotions (Saravia et al., 2018), which contains Twitter messages categorized into six basic emotions; Arxiv-10 (Farhangi et al., 2022), built from ArXiv papers; and BiosBias (De-Arteaga et al., 2019), a dataset of textual biographies aimed at predicting professional occupations. More details are given in Section A.3 in the appendix.\nBaselines. We compare our method against four groups of baseline approaches: 1) Thresholding methods, such as FreeMatch (Wang et al., 2023), which uses an adaptive threshold to select samples with high student confidence, and SoftMatch (Chen et al., 2023), which assigns higher weights to samples with greater student confidence. 2) Knowledge distillation methods, which focus on filtering noisy pseudo-labels based on the teacher or selecting informative samples based on the student. For the first type, we evaluate CCKD_L (Mishra and Sundaram, 2021), which weights samples according to the teacher's probability, and Entropy Score (Lang et al., 2022), which selects samples with the lowest teacher entropy, indicating high teacher confidence. For the second type, we include CCKD_T+Reg (Mishra and Sundaram, 2021), which uses a threshold to select challenging samples for the student, and UNIXKD(Xu et al., 2023), which selects samples with the highest student uncertainty. 3) Unsupervised text classification method: Lbl2TransformerVec(Schopf et al., 2022), which predicts labels based on the similarity between the embeddings of input text and label words. 4) Basic baselines: Random, where a subset of samples is randomly selected from each batch, and Teacher, where predictions are made directly using the teacher model.\nImplementation details. For the teacher model, we use LLaMA (Touvron et al., 2023), an open-source LLM that has demonstrated strong performance in various applications. For the student model, we adopt RoBERTa (Liu, 2019). To ensure a fair comparison, all baseline models use the same pseudo-labels as our model, and the baseline models use RoBERTa as the backbone model except for the Lbl2TransformerVec. Model performance on the text classification task is evaluated using Accuracy (ACC) and Macro-F1 scores, the data efficiency is evaluated based on the total used training number. We provide a parameter analysis in Section 4.7. Additional details are given in Section A.4 in the Appendix."}, {"title": "4.2 Classification Performance Comparison", "content": "We present the classification performance in Table 1, where \"LLKD_w\" denotes the version with weighting in the loss function, as defined in Eq (9). We also report the average rank for each method over all datasets and metrics. The relative improvement of our best method over the best baseline is indicated as \"Improv.\" The Lbl2TransformerVec is the similarity-based method without any training progress, thus it does not have standard deviation.\nOur key observations are as follows: 1) As shown in Table 1, our model consistently outperforms all baseline methods, achieving a significant relative improvement of 6.25% on the Pubmed-rct-20k dataset in terms of F1 score. The weighted version generally performs better, indicating that leveraging teacher confidence and student uncertainty to prioritize selected samples further enhances overall model performance. 2) Generally, we observe that directly using the teacher model performs worse than our method, as well as all other baselines (except for Lbl2TransformerVec), which fine-tune the student model using pseudo-labels. This demonstrates that the student model not only effectively learns from the teacher but also achieves superior results. These findings suggest that, with proper tuning, the student model can deliver better performance while maintaining much lower computational costs. 3) Additionally, the similarity-based method, Lbl2TransformerVec, exhibit the weakest performance, highlighting that relying solely on text-label similarity is insufficient for effective classification."}, {"title": "4.3 Ablation Study", "content": "In this subsection, we conduct an ablation study to evaluate the effectiveness of each component in our method, including the pseudo-label, teacher confidence, and student uncertainty. The results are shown in Figure 3. We use \"w/o TC\u201d to denote the model without using the teacher confidence to select samples, relying solely on the student uncertainty threshold. Similarly, \"w/o SU\" denotes the model without using the student uncertainty threshold to select, while \"w/o TC+SU\" represents the model without any data selection. The figure clearly shows that our model consistently outperforms the ablated versions across all datasets, highlighting the importance of each component. Notably, when no data selection is performed, the model exhibits the worst performance, further validating the critical role of our data selection strategy in enhancing the model performance."}, {"title": "4.4 Data Efficiency", "content": "In this subsection, we evaluate data efficiency by presenting the total number of samples seen and the percentage of original total samples seen in each run. For example, the original training size of Arxiv-10 is 79,790 and we set the epoch to be 6, so the student model sees a total of 79,790 x 6 = 478,740 samples without any data selection. All methods have the same original total seen samples. The results of various data selection method are shown in Table 2. We also include results from our ablated versions to provide deeper insights into our approach. For UNIXKD and Entropy Score which have fixed selection ratio, we experimented with ratios of {10%, 30%, 50%, 70%, 90%} and used the ratio that achieved the best validation performance. Since SoftMatch and CCKD_L weight samples instead of selecting, they use the full set of original samples. Generally, the results indicate that our method consistently outperforms others in both effectiveness and data efficiency. In most cases, our approach requires selecting only about 20% of the training samples. Notably, on the PubMed-RCT-20k dataset, we use as little as 3.7% of the training samples while achieving a significant relative improvement of 6.25%, as shown in Table 1. Although the UNIXKD is trained with less data on the Arxiv-10 dataset, our method has large performance improvement, indicating UNIXKD does not select sufficient informative data."}, {"title": "4.5 Choice of Teacher Model", "content": "To evaluate if LLKD is agnostic of the choice of teacher LLM, we try Gemma\u00b9 (Team et al., 2024) as the teacher model which has recently demonstrated strong performance in various applications. The results on the Arxiv-10 dataset are presented in Table 3. We observe that Gemma outperforms LLaMA, achieving 59.5% accuracy compared to LLaMA's 56.02%. The baseline models also show slightly improved performance when using Gemma instead of LLaMA. LLKD continues to demonstrate the best performance across all scenarios. Since Lbl2TransformerVec relies on text-label similarity, its performance remains unchanged regardless of the teacher model. Notably, these results indicate that LLKD is agnostic to the choice of"}, {"title": "4.6 Thresholds Evaluation", "content": "In this subsection, we evaluate whether the thresholds $\\tau_t^f(y)$ (based on teacher confidence) and $\\tau_t^+(y)$ (based on student uncertainty) effectively select high-quality pseudo-labeled samples and hard samples from the training set, respectively. Specifically, we compute teacher accuracy by comparing the teacher's pseudo-label with the true label in the training set (note that the true label is only used for evaluation here and is never involved in our method). Student accuracy is calculated by comparing the student's predictions with the pseudo-labels. Teacher accuracy reflects the quality of pseudo-labels, with higher accuracy indicating a greater likelihood that the pseudo-label is correct. Student accuracy, on the other hand, identifies difficult samples, where lower accuracy suggests a higher likelihood of incorrect predictions, marking these as hard samples. The results, shown in Figure 4, are presented every 100 training steps for the ArXiv-10 datasets. Additional results on other datasets are given in Section A.5 in the appendix, which show similar observations. Figure 4 reveals that using $\\tau_t^+(y)$ tends to yield higher teacher accuracy, while selecting samples with $\\tau_t^f(y)$ leads to lower student accuracy, indicating the selection of harder samples. It demonstrates that the two thresholds work as expected."}, {"title": "4.7 Hyper-parameter Analysis", "content": "We analyze the sensitivity of key hyper-parameters used to define thresholds for teacher confidence and student uncertainty: $\\lambda_S$ and $\\lambda_T$, which control the momentum between previous and current learning status, and $\\beta_{S1}, \\beta_{S2}, \\beta_{T1}$, and $\\beta_{T2}$, which balance the contributions of global and local components in $\\tau_t^f(y)$ and $\\tau_t^+(y)$. The results on the Arxiv-10 dataset are shown in Figure 5. We vary $\\lambda_S$ and $\\lambda_T$ within the range {0.1, 0.3, 0.5, 0.7, 0.9} and observe that our model remained robust, consistently outperforming the best baseline in almost all cases. For $\\beta_{S1}, \\beta_{S2}, \\beta_{T1}$, and $\\beta_{T2}$, we test values in {0, 1} to assess the effects of using global, local, or both components. Specifically, when $\\beta_{S1}$ = 1 and $\\beta_{S2}$ = 0, the threshold involves only the local part; when $\\beta_{S1}$ = 0 and $\\beta_{S2}$ = 1, it involves only the global part; and when both $\\beta_{S1}$ = 1 and $\\beta_{S2}$ = 1, it considers both. The same applies to $\\beta_{T1}$ and $\\beta_{T2}$. Our observations indicate that the best performance is achieved when both global and local components are considered, demonstrating their combined effectiveness in enhancing model performance."}, {"title": "5 Conclusion", "content": "In this work, we propose to learn with less computational resources and less data for knowledge distillation from LLMs via unlabeled data. The student model is fine-tuned with pseudo-labels generated by the teacher LLM. We empirically demonstrate that teacher confidence can effectively indicate the quality of pseudo-labels, while student uncertainty signals whether a sample contains challenging knowledge for the student. Specifically, we propose an adaptive data selection method to select the most informative samples, improving data efficiency by reducing the required training data. We introduce two thresholds based on teacher confidence and student uncertainty. By selecting samples that meet both thresholds, we ensure the chosen data has high pseudo-label quality and contains challenging knowledge. The effectiveness of the proposed method is validated through extensive experiments on various datasets, showing superior model performance and higher data efficiency."}, {"title": "6 Limitations", "content": "The experiments in this paper are limited to the text classification setting and do not explore other tasks and architectures commonly used in knowledge distillation, such as generation tasks. Additionally, due to resource constraints, we were unable to perform a thorough study on different LLM sizes, which could be important to further demonstrate the generalizability of our approach."}, {"title": "A Appendices", "content": null}, {"title": "A.1 Prompts", "content": "We provide more details about the prompts used for the teacher and student models. The teacher prompt template is shown in Table 4 and consists of several components: a system prompt with task instructions, examples from the validation set demonstrating the task, the input text, and the output label. The system prompt for each dataset is listed in Table 7. For the student model, we use the template \"<Input text>. It is [MASK]\"."}, {"title": "A.2 Additional Results of Empirical Observations", "content": "In this subsection, we explore the relationship between teacher model accuracy and teacher confidence, as well as the relationship between student model accuracy and student uncertainty across additional datasets. The results are shown in Figure 6. They reveal similar trends to those observed with the PubMed-RCT-20k dataset: higher teacher confidence generally correlates with higher teacher accuracy, and greater student uncertainty typically corresponds to incorrect predictions, i.e., hard samples. These findings further validate that teacher confidence can be used to assess pseudo-label quality, while student uncertainty can be used to access the informativeness of samples."}, {"title": "A.3 Datasets", "content": "In this subsection, we provide more details about the datasets used in our experiments, which are summarized in Table 6. All datasets are in English, and their respective licenses are listed in Table 5.\n\u2022 PubMed-RCT-20k\u00b2(Dernoncourt and Lee, 2017) is a sequential sentence classification dataset derived from PubMed abstracts, where each sentence is labeled with roles such as background, objective, method, result, or conclusion. The dataset contains 20k abstracts and has fixed train, validation, and test splits.\n\u2022 Yahoo! Answers\u00b3 (Zhang et al., 2015) is a topic classification dataset collected from the Yahoo Answers platform consisting of 10 classes. Due to computational limitations, we randomly select a subset: 10k samples from each class for training, and 6k samples from each class for validation.\n\u2022 Emotions (Saravia et al., 2018) is a dataset of Twitter messages categorized into six basic emotions: anger, fear, joy, love, sadness, and surprise. Since the dataset does not have fixed splits, we randomly split it into train, validation, and test sets using an 80%, 10%, 10% ratio.\n\u2022 Arxiv-105 (Farhangi et al., 2022) consists of abstracts and titles from 100,000 ArXiv papers, balanced across 10 classes in fields such as computer science, physics, and mathematics. This dataset also lacks fixed splits, so we randomly split it into train, validation, and test sets using an 80%, 10%, 10% ratio.\n\u2022 BiosBias (De-Arteaga et al., 2019) contains textual biographies aimed at predicting professional occupations with 28 classes. This dataset includes predefined splits for training, validation, and testing."}, {"title": "A.4 Baselines and Implementaion Details", "content": "All baseline models, except for the teacher and Lbl2TransformerVec, use RoBERTa as the backbone model and share the same pseudo-labels as"}, {"title": "A.5 Additional Results of Thresholds Evaluation", "content": "In this subsection, we present additional results from the thresholds evaluation on other datasets, as shown in Figure 7. We observe similar trends to those seen in the Arxiv-10 dataset, confirming that selecting samples with values larger than $\\tau_t^+(y)$ yields high-quality pseudo-labels, while larger $\\tau_t^f(y)$ selects more challenging samples."}]}