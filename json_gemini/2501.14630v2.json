{"title": "Extracting Problem Structure with LLMs for Optimized SAT Local Search", "authors": ["Andr\u00e9 Schidler", "Stefan Szeider"], "abstract": "Local search preprocessing makes Conflict-Driven Clause Learning (CDCL) solvers faster by providing high-quality starting points and modern SAT solvers have incorporated this technique into their preprocessing steps. However, these tools rely on basic strategies that miss the structural patterns in problems. We present a method that applies Large Language Models (LLMs) to analyze Python-based encoding code. This reveals hidden structural patterns in how problems convert into SAT. Our method automatically generates specialized local search algorithms that find these patterns and use them to create strong initial assignments. This works for any problem instance from the same encoding type. Our tests show encouraging results, achieving faster solving times compared to baseline preprocessing systems.", "sections": [{"title": "1 Introduction", "content": "Local search preprocessing guides CDCL solvers to faster solutions through better starting points. Balint and Manthey [[2013]] showed that preprocessing improves CDCL-based SAT-solving performance. Current SAT solvers like CaDiCaL [Biere, 2019; Biere et al., 2020; Biere et al., 2024] and CryptoMiniSat [Soos, 2020] have adopted this approach in their preprocessing phase. These tools apply basic strategies that work well for random problems but miss critical patterns in structured instances. SAT encodings of real problems contain inherited patterns from graph layouts, data connections, and domain-specific rules. The transformation to Conjunctive Normal Form (CNF) obscures these patterns. Current local search methods skip these structures in favor of general approaches. This paper addresses these limitations by introducing a framework that leverages LLMs to generate local search strategies tailored to encoding structures, enabling solvers to take advantage of these patterns for improved performance. Our research addresses three questions:\n\nHow can LLMs analyze PySAT [Ignatiev et al., 2024] code to interpret how problem structure translates to SAT clauses?\n\nHow can we create local search strategies that recognize and exploit these encoding patterns?\n\nWhat performance gains do structure-aware preprocessing methods achieve versus standard approaches?\n\nOur method applies LLMs to read and interpret the PySAT code that converts structured problems to SAT. The model spots high-level constructs like graph connections, path constraints, or counting"}, {"title": "2 Background and Related Work", "content": ""}, {"title": "2.1 SAT solving", "content": "The propositional satisfiability problem (SAT) takes a propositional formula and asks if there exists a variable assignment that makes the formula true, i.e., that satisfies the formula. Modern complete SAT solvers use Conflict-Driven Clause Learning (CDCL) [Silva and Sakallah, 1996; Marques-Silva et al., 2021; Fichte et al., 2023] and search for a satisfying assignment or proves that no such assignment exists.\n\nA common way of using SAT solvers is by encoding other problems into SAT, i.e., representing a problem instance in propositional logic such that the satisfiability/unsatisfiability of the formula corresponds to a Yes/No answer for the original problem; and a satisfying variable assignment corresponds to a solution of the original problem. An example would be encoding whether a given graph admits a proper k-coloring, where a satisfying assignment can be translated into a k-coloring. An encoding requires an encoding scheme that translates instances from the original problem into propositional logic and thereby creates the actual encoding of an instance. In our example, it would be an algorithm that produces for a given graph and integer k an encoding that is satisfiable if and only if the graph admits a k-coloring."}, {"title": "2.2 Local Search Algorithms for SAT", "content": "The GSAT algorithm [Selman et al., 1992] introduced the basic local search approach for SAT. GSAT picks the variable flip that leads to the most satisfied clauses. WalkSAT [Selman et al., 1994] operates by randomly selecting an unsatisfied clause and then choosing a variable within that"}, {"title": "2.3 Local Search in CDCL Solvers", "content": "Modern CDCL solvers use local search during preprocessing to find promising initial assignments. CaDiCaL [Biere, 2019; Cai et al., 2022; Biere et al., 2024] implements a variant of ProbSAT in its rephasing procedure. The solver stores good assignments found during search and uses them as starting points after restarts. This hybrid approach performs particularly well on random and hard combinatorial instances by combining the systematic nature of CDCL with local search's ability to find solutions in satisfiable regions quickly. The integration of local search into CDCL brings two main benefits: better initial assignments can guide the solver toward solutions faster, and local search patterns can inform restart strategies. However, current implementations use generic local search methods that don't account for problem structure."}, {"title": "2.4 Machine Learning and SAT solving", "content": "Machine learning has enhanced SAT solving in multiple ways. Deep neural networks can learn variable selection policies that speed up SAT solving [Selsam and Bj\u00f8rner, 2019]. A recent line of work leverages graph neural networks to guide SAT local search [Yolcu and P\u00f3czos, 2019]. LLMs have recently shown success in algorithm generation. AlphaCode [Li et al., 2022] and CodeGen [Nijkamp et al., 2023] can generate correct implementations of algorithms. For constraint satisfaction, system StreamLLM generates streamlining constraints using LLM-calls [Voboril et al., 2025; Voboril et al., 2024], and MCP-Solver, on the other hand, allows interactive calls to a constraint solver within an LLM chat via the Model Context Protocol [Szeider, 2024]. However, using LLMs to analyze and improve algorithms is still a new direction. Related to our approach is work that uses LLMs to optimize compiler passes [Cummins et al., 2023]. For our target problems, SAT encodings remain competitive with specialized algorithms.\n\nThe work in this paper differs from this related work in three fundamental ways: (i) we use LLMs to generate specialized local search algorithms, not general SAT solvers, (ii) we focus on problem structure, not instance-specific features, and (iii) we provide runtime and correctness guarantees."}, {"title": "3 Problem Statement", "content": "We consider the problem of automatically finding problem specific local search approaches that perform well in conjunction with CDCL SAT solvers. Existing local search approaches have two drawbacks. First, these algorithms are usually general purpose algorithms that do not consider, or know of, the encoding scheme that has been used to create the formula. We expect that special considerations for the encoding scheme boosts the performance of local search. Second, it has been shown empirically that local search often struggles to find satisfying assignments on its own [Li and Li, 2012; Cai and Zhang, 2021; Cai et al., 2022]. As discussed in the previous section, hybrid approaches that combine local search and CDCL solvers often achieve better results on hard combinatorial instances than either paradigm on its own [Cai et al., 2022]. Hence, local search methods specifically designed for hybrid approaches that are specific to the encoding scheme promise an improved performance on these hard instances.\n\nCreating specialized algorithms is a time-consuming effort and is often focused on well-known approaches. This focus is necessary, as manually creating specialized algorithms with a variety of"}, {"title": "4 Methodology", "content": "In this section, we describe our approach that is sketched in Figure 1. The process consists of two phases, a Gathering Phase and a Refinement Phase. We will describe each part of the process, starting with the input, the encoding scheme."}, {"title": "4.1 Standardizing Encoding Schemes", "content": "There are many ways to implement encoding schemes. Since we are defining a general approach for generating local search methods, we need a standardized way of representing encoding schemes. We express our encoding schemes in Python using the popular framework PySAT [Ignatiev et al.,"}, {"title": "4.2 Gathering Phase", "content": "We provide the LLM with the PySAT encoding scheme, without stating the problem it encodes or any information on the instances we will test the local searches on. The LLM is then requested to return a local search function that meets the following specifications:1\n\nPerforms local search for the encoding.\n\nHas a specific function name and takes as input the original instance, the encoded instance as PySAT objects, and a timeout.\n\nReturns a (partial) variable assignment where a Boolean value can be accessed using the variable identifier.\n\nMust return within the specified timeout.\n\nIs encouraged to use the input instance and the structure of the encoding,\n\nas well as novel approaches.\n\nWhenever we receive a local search function, we verify its correctness (\"Error?\" in Figure 1). We verify that the local search function can be executed, finishes within the timeout, and returns a (partial) assignment by running the local search function on an easy instance for half a minute. Whenever an error is detected, the LLM is provided the error and, when applicable, the line the"}, {"title": "4.3 Scoring", "content": "Since we aim to generate a wide variety of local search functions, we require a method to automatically compare their performance. Given a local search function, we run it on a set of training instances (\"score on training instances\" in Figure 1). For each instance we let the local search find an assignment and pass it to a SAT solver that tries to solve the instance based on the provided assignment. This allows us to rank a set of local search functions. All local search functions that caused a runtime error are ranked last and we use the following criteria for the remaining functions, lower values are better.\n\nThe number of instances where the local search function did not return an assignment within the timeout.\n\nThe number of instances with a SAT solver timeout.\n\nThe average runtime over successful SAT solver calls."}, {"title": "4.4 Refinement Phase", "content": "In this phase, we focus on improving the existing local search functions found in the previous phase. We pick the top searches from the gathering phase and process them one by one. Therefore, refinement runs are independent of each other. We start from the original request and search function as context and ask the LLM to vary the function, while not changing the overall idea. Each version is scored as described above and the LLM receives feedback about the function's relative performance to the previous version, where a SAT runtime change is only considered significant if it the average runtime changed by more than 10%. Depending on the function's performance being better or worse, the LLM is requested to either continue with similar refinements, or revert and try a different approach. In case there is no significant difference, the LLM is prompted to perform a bigger change.\n\nAfter completing the refinement phase, we pick the top local search functions for our final Test Evaluation."}, {"title": "5 Experimental Evaluation", "content": "In this section, we evaluate the quality the generated local search functions. Further, we also explore how well the different parts of our approach contribute to this quality.2"}, {"title": "5.1 Setup", "content": "We use the OpenAI models 01-mini-2024-09-12 and gpt-40-2024-11-20, as well as Anthrophic claude-3-5-sonnet-20241022. We run gathering and refinement on a MacBook M1 and the Test Evaluation on servers with two AMD EPYC 7402 CPUs having 24 cores running at 2.80GHz. Each run has a memory limit of 128 GB. We use Cadical 1.9.5 as the SAT solver and PySAT 1.8.dev13."}, {"title": "5.2 Comprehension", "content": "The LLMs do not receive any information on the purpose of the PySAT encoding schemes. Hence, it is an interesting question if they understand the encodings. We evaluate this by asking the three LLM models to explain the PySAT encoding schemes. All three models perform similarly on this task. They correctly identify the Graph Coloring and BDDT encoding scheme in detail, including the concept of the encoding scheme and the semantics of the schemes's variables. All three models can explain the parts of the DFVS encoding scheme. However, they fail to identify its overall purpose."}, {"title": "5.3 Gathering Phase", "content": "The LLM models generate the local search functions with varying speed during the Gathering Phase. The difference in performance is also observable in the Refinement Phase, where it matters less as the evaluation on the training instances takes much longer than the LLM queries."}, {"title": "5.4 Refinement Phase", "content": "We pick the top five local search functions from each LLM model for the Refinement Phase. This results in 15 Base local search versions for each problem. We refine each of these local searches 19 times, to obtain a total of 20 versions per local search, which results in 100 local searches per LLM model. After the first ten refinements (versions 2\u201311), we re-encourage the LLM to use the structure of the encoding (versions 12-20). We refer to versions 2\u201311 as Refined versions and version 12-20 as Structure versions."}, {"title": "5.5 Test Instance Performance", "content": "In this evaluation, the base local search functions and their refinements are run on the test instances. Since it is infeasible to evaluate all refinements of all local search functions, we focus on the Base version, the best Refined version, and the best Structure version. The best version is determined by the performance on the training instances. We run the SAT solver without any local search on all instances for one hour as a reference. We evaluate a local search function on a test instance by running it with a timeout of 15 minutes and then run the SAT solver using the local search's assignment for up to one hour. We use a timeout of 15 minutes and do not decrease the SAT solver timelimit, as we expect a good native implementation, e.g., in C++, would run orders of magnitudes faster than the Python prototypes. The results of the best local search function from each LLM model are in Table 1.6\n\nWe are interested in two metrics: (i) how many instances are solved by the SAT solver within the timeout when using a local search function, and (ii) how many instances can be solved within the timeout using the local search function that the SAT solver alone could not solve. The first metric establishes a general usefulness of the local search function, while the second metric establishes whether the local search function is useful for hard instances."}, {"title": "5.6 Discussion", "content": "Our results show that it is indeed possible to automatically generate and evaluate local search prototypes using LLMs. In the last part of our evaluation we want to discuss aspects of our approach that are of interest for adapting our approach to other problems."}, {"title": "5.6.1 Test and Training Correlation", "content": "The automatic evaluation relies on some automated way of ranking the different local search functions. Since large scale tests for each of the many generated functions are infeasible, we rely on"}, {"title": "5.6.2 Code Diversity", "content": "We manually reviewed the code generated by the LLMs in an effort to judge how much the code varies, as well as the overall quality of the code. Due to the large number of generated local search functions-over 900-we cannot review all the code in detail. Hence, we focus on trends within the code.\n\nThe LLMs manage to create prototypes and these prototypes are rarely performance-optimized. An example is the scoring function present in all of the local search functions we reviewed. The scoring function determines how good an assignment is and is often implemented by iterating over all clauses and checking if they are satisfied. The scoring function is then called whenever the assignment changes, or even for each considered change to the assignment. In case of methods like WalkSAT that flip one variable per iteration, this is a major performance bottleneck. We address this specific issue via prompt and the LLMs can often fix it, but performance bottlenecks that are specific to single local search functions cannot be addressed via generic prompt. Hence, we expect that the local search functions can be improved even more with a performance-oriented implementation. Further, potential issues should be assessed before the Gathering Phase by generating a small set of initial local search functions and manually reviewing them. These issues can then be addressed directly, as with the scoring function bottleneck.\n\nThe LLMs usually provide their implementation goal in the comments. According to these comments, the local search functions implement over 50 different meta-heuristics, like\n\ngenetic algorithms, tabu search, simulated annealing, harmony search, ant colony opti-mization, agent-based optimization, cellular automaton based optimization, swarm op-timization, great deluge, firefly, bee colony optimization, multi-agent-based optimiza-tion, market-based optimization, quantum-based binary optimization.\n\nFurther, every LLM model also tries WalkSAT in some iteration for every problem. Interestingly, Claude Sonnet often tries to combine different meta-heuristics, while the GPT models try to implement a single approach. This single approach closely resembles pseudocode in local search functions generated by GPT 4o, while those generated by GPT 01-mini have clear adaptations due to our prompts. These adaptations are also observable in the Claude Sonnet generated functions. This great variety is encouraging for sampling promising prototypes. Unfortunately, the comments often don't match the semantics of the code, making a manual review necessary to verify what the code actually does."}, {"title": "5.6.3 Analysis of Generated Search Strategies", "content": "In this last part, we analyze the code of the best local search functions and report the most interesting ideas. For a variable flip, we define its conflict score as $S \u2013 U$, where S is the number of unsatisfied clauses that become satisfied and U is the number of satisfied clauses that become unsatisfied. Higher scores indicate better flips.\n\nGraph Coloring The graph coloring encoding is comparatively simple. Consequently, the LLMs often generate local search functions for graph coloring that run on the graph, and only when returning, convert the best coloring into a variable assignment. This works well and the best function we found works with this principle. The best local search function that uses the SAT encoding implements tabu search and in each iteration performs the best-according to the conflict score-variable flip among 20 random variables. Whenever the improvements stagnate, the search uses the 20 random variables whose flips lead to an improvement most often. The initial assignment is created by assigning colors to nodes in order if decreasing node degree, choosing the color that causes the fewest monochromatic edges.\n\nDFVS The DFVS PySAT encoding scheme is an interesting case. As described in Section 5.2, this scheme is the only one that the LLMs do not fully comprehend. This leads to strange effects, where several local search functions are able to extract the upper bound d from the encoding, but misunderstand the polarity of the variables inside the cardinality constraint. This leads to the local search function seemingly excluding d many nodes from the graph, while actually excluding all but d many nodes.\n\nThe best local search function is a straightforward WalkSAT implementation. Other good local search functions use greedy heuristics based on node degree or node centrality to pick the best variable flips. While there are many well-performing local search functions, there is a lack of problem specific functionality, and where it is present, it is wrong, as with the polarity issue stated above. Overall, the lesson-learned from this PySAT case, is the importance of the LLM model's understanding of the encoding scheme.\n\nBDDT The best local search functions for BDDT are highly adapted to the PySAT encoding scheme. The best function for BDDT is generated by Claude Sonnet. It correctly identifies the variables that must be assigned, and ignores those variables that are implied by unit propagation. The remaining variables are separated in levels, corresponding to the depth of the respective node in the tree. During initialization, the function assigns each node a random feature and picks a threshold from the middle of the dataset. This is reflected in the assignment such that all corresponding constraints are satisfied. The search is performed level by level. Whenever the search on one level stagnates, it moves to the next level, cycling back to the root level if necessary."}, {"title": "6 Conclusion", "content": "We demonstrated that it is possible to automatically generate effective local search algorithms by having LLMs analyze SAT encoding schemes. Our key innovation lies in targeting the encoding methodology rather than specific problem instances, allowing the generated strategies to work across all problems sharing the same encoding pattern. This scheme-centric approach produced diverse search strategies whose performance correlated with the LLMs' comprehension of the underlying encoding structures.\n\nWhile we focused on LLM's, exploring distilled variants could make the generation process more computationally feasible. Our diversity measures in the exploration phase could also be enhanced by leveraging model embeddings to quantify the distinctness of generated strategies better. Finally, while computationally more demanding, the next generation of reasoning models could enable deeper encoding comprehension and more sophisticated search strategies, further expanding the possibilities of automated algorithm generation."}]}