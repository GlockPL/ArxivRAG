{"title": "Real-Time Anomaly Detection and Reactive Planning with Large Language Models", "authors": ["Rohan Sinha", "Amine Elhafsi", "Christopher Agia", "Matthew Foutter", "Edward Schmerling", "Marco Pavone"], "abstract": "Abstract-Foundation models, e.g., large language models (LLMs), trained on internet-scale data possess zero-shot generalization capabilities that make them a promising technology towards detecting and mitigating out-of-distribution failure modes of robotic systems. Fully realizing this promise, however, poses two challenges: (i) mitigating the considerable computational expense of these models such that they may be applied online, and (ii) incorporating their judgement regarding potential anomalies into a safe control framework. In this work, we present a two-stage reasoning framework: First is a fast binary anomaly classifier that analyzes observations in an LLM embedding space, which may trigger a slower fallback selection stage that utilizes the reasoning capabilities of generative LLMs. These stages correspond to branch points in a model predictive control strategy that maintains the joint feasibility of continuing along various fallback plans to account for the slow reasoner's latency as soon as an anomaly is detected, thus ensuring safety. We show that our fast anomaly classifier outperforms autoregressive reasoning with state-of-the-art GPT models, even when instantiated with relatively small language models. This enables our runtime monitor to improve the trustworthiness of dynamic robotic systems, such as quadrotors or autonomous vehicles, under resource and time constraints. Videos illustrating our approach in both simulation and real-world experiments are available on our project page: https://sites.google.com/view/aesop-llm.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous robotic systems are rapidly advancing in capabilities, seemingly on the cusp of widespread deployment in the real world. However, a persistent challenge is that the finite datasets used to develop these systems are unlikely to capture the limitless variety of the real world, leading to unexpected failure modes when conditions deviate from training data, or when the robot encounters rare situations that were not well-represented at design time. To mitigate the resulting safety implications, we require methods that can 1) assess the reliability of a machine learning (ML) enabled system at runtime and 2) judiciously enact safety-preserving interventions if necessary.\nIn this work, we investigate the utility of foundation models (FMs), specifically, large language models (LLMs), towards these two objectives by employing LLMs as runtime monitors tasked with 1) detecting anomalous conditions and 2) reasoning about the appropriate safety-preserving course of action. We do so because recent work has shown that the internet-scale pretraining data provides FMs with strong zero-shot reasoning capabilities, which has enabled robots to perform complex tasks [5], identify and correct failures [18], and reason about potential safety hazards in their surroundings [12] without explicit training to do so.\nHowever, the adoption of FMs in-the-loop of safety-critical robotic systems is immediately met with two challenges. First, the ever growing scale of FMs poses a major obstacle towards enabling real-time, reactive reasoning about unexpected safety-critical events, especially on agile robotic systems with limited compute. Hence, existing work that applies FMs to robotics has focused on quasi-static (e.g., manipulation) or offline settings that afford large times delays while the LLM completes its reasoning. Second, the application of FMs as runtime monitors requires that they are grounded with respect to the task and capabilities of the system. However, the community has not converged on rigorous methods for grounding FMs without compromising on their generalist zero-shot reasoning abilities (e.g., fine-tuning [24] or linear probing [54] often underperform OOD); prompt design remains a standard practice."}, {"title": "II. RELATED WORK", "content": "Out-of-Distribution Robustness: The fact that learning-based systems often behave unreliably on data that is dissimilar from their training data has been extensively documented in both the machine learning and robotics literature [14, 37, 33, 45]. Approaches to address the subsequent challenges broadly fall into two categories [45]: First are methods that strengthen a model's performance in the face of distributional shift. For example, through robust training (e.g., [41]) or by adapting the model to changing conditions (e.g., [16, 8]). Second are so called out-of-distribution detection algorithms [42, 40], that aim to detect when a given model is unreliable, e.g., by computing the variance of an ensemble [25] or computing energy scores [29]. Recent work has shown the merits of generalist FMs like LLMs in both domains: Studies have shown that zero-shot application of a FM (e.g., in [54], the authors apply CLIP zero-shot on ImageNet), vastly improves OOD generalization over previous approaches, like distributionally robust training [31, 54, 6]. In addition, existing OOD detection methodologies and their application within robot autonomy stacks are tailored to detect conditions that compromise the reliability of individual components of an autonomy stack, like whether a perception system's detections are correct [39, 13, 36, 46]. Instead, recent work showed that LLMs may provide a more general mechanism to detect context dependent safety hazards, especially those that are hard to measure with predefined performance metrics [12]. For example, an autonomous EVTOL may monitor the quality of the vision system's landing pad location estimate, but even if the EVTOL has high confidence that it can land successfully, the outcome of landing on a building that is on fire can have profound negative consequences. However, despite the attractive properties of LLMs, these works do not propose practical strategies to integrate them in closed-loop. Therefore, we propose a closed-loop control framework that can both use the LLM to identify unseen anomalies and strengthen performance in the presence of rare failure modes.\nFoundation Models in Robotics: The integration of large lanugage models (LLMs) and, more broadly, foundation models (FMs) into robotics has sparked considerable interest due to their proficiency in managing complex, unstructured tasks that demand sophisticated reasoning skills. These models"}, {"title": "III. PROBLEM FORMULATION", "content": "In this work, we consider a robot with discrete time dynamics\n$$x_{t+1}= f(x_t,u_t),$$\nwhere $$x_t \\in \\mathbb{R}^n$$ represents the robot's state, and $$u_t \\in \\mathbb{R}^m$$ is the control input. Nominally, we aim to minimize some control objective $$C$$ that depends on the states and inputs, subject to safety constraints on the state $$x_t \\in \\mathcal{X} \\subseteq \\mathbb{R}^n$$ and input $$u_t \\in \\mathcal{U} \\subseteq \\mathbb{R}^m$$. For example, a quadrotor's state consists of its pose and velocity (estimated from e.g., GPS, visual-SLAM, and IMUs), and its objective may be to minimize distance to a landing zone subject to collision avoidance constraints.\nIn addition to the state variables tracked by the nominal control loop, the robot receives an observation $$o_t \\in \\mathcal{O}$$ at each timestep, which provides further contextual information about the robot's environment. Our goal is to design a runtime monitor that interferes with the nominal system to avoid system-level safety hazards, which may depend on environmental factors not represented in the robot's state $$x_t$$. For example, a quadrotor cannot safely land on a landing zone covered in burning debris even if the nominal control stack has the ability to do so. In the spirit of [12], we refer to such events as semantic failure modes, as they do not necessarily constitute violation of precise state constraints $$\\mathcal{X}$$, but instead depend on the qualitative context of the robot's task.\nFurther, we assume that we have access to a dataset $$\\mathcal{D}_{nom} = \\{o_i\\}_{i=1}^N$$ of nominal observations wherein the robot was safe and reliable. Conceptually, $$\\mathcal{D}_{nom}$$ corresponds to the operational data of a notionally mature system, and may consist of data used to train the system or of previously collected deployment data. This data will overwhelmingly contain mundane scenarios where the robot performs well; our monitoring framework is targeted instead at the challenging, extremely rare corner cases that are unlikely to have been recorded before and threaten the robot's reliability.\nIn the event that a failure mode of the nominal autonomy stack is imminent, we must select and engage a safety-preserving intervention. For example, we may choose to land the quadrotor in another open landing zone like a grassy field. To this end, we follow [46] and we assume that we are given a number of recovery regions $$\\mathcal{X}_1,\\mathcal{X}_2,...,\\mathcal{X}_h \\subseteq \\mathcal{X}$$, control invariant subsets of the state space that correspond to high-level safety interventions. For example, $$\\mathcal{X}_h$$ may represent the alternate landing zone. By planning a trajectory to the appropriate recovery set, potential safety hazards can be avoided. As shown in [46], such sets can both be hand defined up-front and identified using reachability analysis."}, {"title": "IV. PROPOSED APPROACH", "content": "It is virtually impossible to account for all the corner-cases and semantic failure modes that a system may experience through a standard engineering pipeline. Even if we train e.g., classifiers to detect obstructions on landing zones, there may always remain a class of semantic failure modes that we have not accounted for. Instead, we propose to leverage generalist foundation models to detect and reason holistically about a robot's environment. We first present our FM-based monitoring approach, after which we construct a planning algorithm that accounts for the latency that FM-based reasoning may induce.\nA. Runtime Monitor: Fast and Slow Reasoning\nTo detect and avoid semantic failure modes, we propose a two-stage pipeline. The first is the detection of anomalies, simply defined as conditions that deviate from the mundane, nominal experiences where we know that our notionally mature system is reliable. The second is slower reasoning about the downstream consequence of an anomaly, if detected, towards a high-level decision on whether a safety-preserving intervention should be executed. We refer to Appendix B for a brief introduction to anomaly detection used hereafter.\nFast Anomaly Detection: To detect anomalies, we need to inform a FM of the context within which the autonomous system is known to be trustworthy. The prior, nominal experiences of the robot serve as such grounding. We construct an anomaly score function $$s(o_t,\\mathcal{D}_{nom}) \\in \\mathbb{R}$$ to query whether a current observation $$o_t$$ differs from the previous experiences in $$\\mathcal{D}_{nom}$$. We do not require any particular methodology to generate the score, we just require that scoring an observation is computationally feasible in real-time; that is, within a single time step.\nThis work emphasizes the value of computing anomaly scores using language-based representations, which we show capture the semantics of the observation within the context of the robot's task in \u00a7V. To do so, we first create a cache of embedding vectors $$\\mathcal{D}_e = \\{e_i\\}_{i=1}^N$$ where $$e_i = \\Phi(o_i) \\in \\mathbb{R}^D$$ for each $$o_i \\in \\mathcal{D}_{nom}$$ by embedding the robot's prior experiences offline using an embedding FM $$\\Phi$$. Then, at runtime, we observe $$o_t$$, compute its corresponding embedding $$e_t$$, and compute an anomaly score $$s(e_t;\\mathcal{D}_e)$$ using the vector cache. We investigate several simple score functions (see Appendix D3 for a full list), each of which roughly measures a heuristic notion of difference with respect to $$\\mathcal{D}_{nom}$$. For example, the simplest metric uses the maximum cosine similarity with respect to samples in the prior experience cache,\n$$s(e_t;\\mathcal{D}_e):= - \\max_{e_i \\in \\mathcal{D}_e} \\frac{e^\\top_te_i}{||e_i|| ||e_t||},$$\nwhich, in effect, retrieves the most similar prior experience from $$\\mathcal{D}_e$$ to construct the score. Intuitively, this approach measures whether anything similar to the current observation has been seen before.\nFinally, to classify whether an observation should be treated as nominal or anomalous, we can calibrate a threshold $$T \\in \\mathbb{R}$$ as the $$\\alpha \\in (0,1)$$ quantile of the nominal prior experiences,\n$$T=\\inf \\{q \\in \\mathbb{R}: \\frac{|e_i \\in \\mathcal{D}_e: s(e_i;\\mathcal{D}_e \\setminus \\{e_i\\}) \\leq q|}{N} \\geq \\alpha\\},$$\ni\ni\ni$$(2)\ni\ni\ni.e., the smallest value of q that upper bounds at least $$\\alpha N$$ nominal samples. Note that for nominal embeddings, we must compute the anomaly score $$s$$ in a leave-one-out fashion, since $$s(e_i;\\mathcal{D}_e) = -1$$ for $$e_i \\in \\mathcal{D}_e$$. Determining the threshold $$T$$ using empirical quantiles as in (2) is a standard approach [39], but could be extended in future work to make precise guarantees on false positive or negative rates using recent results in conformal prediction [3, 30].\nSlow Generative Reasoning: Once we detect an anomaly, we trigger the autoregressive generation of an LLM to generate a zero-shot assessment of whether we need to engage any of the interventions associated with the recovery sets $$\\mathcal{X}_1$$,\u2026\u2026\u2026,$$\\mathcal{X}_d$$ (\u00a7III) to maintain the safety of the system. The value of this approach is that the LLM's internet-scale pretraining data allows it to generate outputs that resemble the generalist common sense reasoning that a human operator is likely to suggest, as a result, making superior decisions on OOD examples, on which existing task-specific learning algorithms are notoriously unreliable.\nTo do so, we follow [12] in using a VLM to convert the robot's current visual observation into a text description of the environment. We simply encode this scene description into a prompt that provides context on the monitoring task, as illustrated in Fig. 3. We then parse the resulting output string to yield a classification $$y \\in \\{0,1,...d\\}$$ on whether the anomaly does not present a hazard and the system can continue it's nominal operation ($$y=0$$), or whether we should engage intervention $$y \\in \\{1,...,d\\}$$ and steer the state into recovery set $$X_y$$. As we illustrate in our experiments, the recovery sets naturally correspond to high-level behaviors (e.g., landing in a field), which facilitates prompt design. We use the shorthand $$w(o_t,\\Upsilon)$$ to denote the output of the slow reasoner when given observation $$o_t$$ and a (sub)set of intervention strategies $$\\Upsilon \\subseteq \\{1,...,d\\}$$.\nWhether inference is run onboard or the model is queried remotely over unreliable networks in the cloud, we must account for the latency that autoregressive reasoning introduces. For example, a fast moving vehicle may collide with an anomalous obstacle if it's reaction time is too slow. Therefore, we account for the LLM's compute latency by assuming that it takes at most $$K \\in \\mathbb{N}_{>0}$$ timesteps to receive the output string from the slow reasoner. It is usually straightforward to identify the value of $$K$$ in practice, since we prompt the model to adhere to a strict output template that tends to stabilize the length of the output generations. Alternatively, as we describe in \u00a7V-C and Appendix I, a simple field-test can be sufficient to identify an upper bound on typical network latency.\nB. Planning a Tree of Recovery Trajectories\nWe control the robot's dynamics (1) in state-feedback using a receding horizon control strategy that 1) minimizes the nominal control objective along a horizon of $$T>K$$ timesteps, while 2) maintaining a set of $$d$$ recovery trajectories that each reach one of the respective recovery sets $$\\mathcal{X}_h$$ within the horizon $$T$$. The goal of this approach is to ensure that the high-level safety interventions provided to the slow reasoner can be executed. Additionally, it is essential that these options remain feasible throughout the $$K$$ time steps it takes the monitor to decide on the most appropriate choice. Otherwise, a fast moving robot may, for example, no longer be able to stop in time to avoid a collision. To this end, we solve the following finite-time optimal control problem online, which maintains a consensus between the recovery trajectories for $$K$$ timesteps:\n$$J_t(\\Upsilon,K,T)=\\min_{\\{\\mathbf{x}^i_{t:t+T+1|t},\\mathbf{u}^i_{t:t+T|t}\\}_{i \\in \\Upsilon \\cup \\{0\\}}} \\mathbb{C}(\\mathbf{x}^0_{t:t+T+1|t},\\mathbf{u}^0_{t:t+T|t})$$\ns.t. $$\\mathbf{x}^i_{t+k+1|t}=f(\\mathbf{x}^i_{t+k|t},\\mathbf{u}^i_{t+k|t})$$\n$$\\mathbf{u}^i_{t+k|t} \\in \\mathcal{U} \\qquad \\mathbf{x}^i_{t+k|t} \\in \\mathcal{X}$$\n$$\\mathbf{x}^i_{t|t}=x_t$$\n$$\\mathbf{x}^i_{t+T+1|t} \\in \\mathcal{X}^i \\qquad \\forall i \\in \\Upsilon$$\n$$\\mathbf{u}^i_{t:t+K|t}=\\mathbf{u}^j_{t:t+K|t} \\qquad \\forall i,j \\in \\Upsilon$$\n$$(3)$$\nHere, the notation $$\\mathbf{x}^i_{t+k|t}$$ indicates the predicted value of variable x at time $$t+k$$ computed at time $$t$$ for each trajectory $$i \\in \\Upsilon \\cup \\{0\\}$$. The MPC in (3) optimizes a set of $$|\\Upsilon|+1$$ trajectories. The first corresponds to a nominal trajectory $$\\mathbf{x}^{0:T+1|t}$$ plan that minimizes the control objective and a set of $$|\\Upsilon|$$ recovery trajectories that each reach their respective recovery set $$\\mathcal{X}_h$$ within $$T$$ timesteps. In addition, the MPC problem (3) includes two consensus constraints, one associated with the fast anomaly detector and the other with the slow reasoner. First, by fixing"}, {"title": "V. EXPERIMENTS", "content": "Having outlined our approach, we conduct a series of experiments to test the following five hypothesis:\nH1 By quantifying semantic differences of observations with respect to the prior experience of a system, our fast embedding-based anomaly detector performs favorably to generative reasoning-based approaches.\nH2 Embedding-based anomaly detection does not necessitate the use of high-capacity generative models; small models incurring marginal costs can be used.\nH3 Once an anomaly is detected, generative reasoning approaches can effectively deduce whether the anomaly warrants enacting safety-preserving interventions.\nH4 Our full approach, which unifies embedding-based anomaly detection and generative reasoning-based anomaly assessment, can be integrated in a broader robotics stack for real-time control of an agile system.\nH5 Additional forms of embeddings, including those from vision and multi-modal models, offer a promising future avenue for end-to-end anomaly detection.\nExperiment Rationale: We run four main experiments. The first experiment (\u00a7V-A) tests the performance of our fast anomaly detector in three synthetic (i.e., text-based) robotic environments. We then evaluate the slow generative reasoner for the assessment of detected anomalies on two of these environments. The second experiment (\u00a7V-B) evaluates our full approach (integrating the runtime monitor with the MPC fallback planner) in a simulation of real-time control of an agile drone system. The third is a full-stack experiment on real quadrotor hardware, including a timing breakdown for each component in our approach running on a Jetson AGX Orin module, thereby demonstrating viability for hardware deployment. The fourth experiment evaluates whether our runtime monitor transfers to a realistic, semantically rich self-driving environment, where we investigate the use of both language and multi-modal embeddings for anomaly detection.\nAll code used in our experiments, including scripts to generate the synthetic datasets and prompt templates, can be found through our project page at https:"}, {"title": "VI. CONCLUSION AND OUTLOOK", "content": "In this paper, we presented a runtime monitoring framework utilizing generalist foundation models to facilitate safe and real-time control of agile robotic systems faced with real-world anomalies. This is enabled through a reasoning hierarchy: a fast anomaly classifier querying similarity with the robot's prior experiences in an LLM embedding space, and a slow generative reasoner assessing the safety implications of detected anomalies and selecting the appropriate mitigation strategy. These reason-ers are interfaced with a new model predictive control strategy that maintains the feasibility of multiple safe recovery plans.\nIn extensive experiments, we demonstrate that a) embedding-based anomaly detection performs favorably to zero-shot generative reasoning with high-capacity LLMs, thanks in part to the grounding afforded by the prior embedding experience of the robot; b) embedding-based anomaly detection attains strong performance even when instantiated with small language mod-els, allowing our method to run onboard computationally con-strained robotic systems; c) dual-stage reasoning enables LLMS to operate in the real-time reactive control loop of an agile robot; d) alternative forms of embeddings, such as those obtained from vision-based foundation models, can be used to efficiently detect anomalies in high-dimensional observation spaces.\nAs such, our work highlights the potential of LLMs and, more broadly, foundation models toward significant increases in the robustness of autonomous robots with respect to unpredictable and unusual out-of-distribution scenarios or tail events. Improving the performance and generality of our framework presents several promising avenues for future research. For example, the impact of LLM inference latencies could be reduced by devising methods to constrain generative reasoning to a fixed word budget, or by using intermediate generations to inform decision-making during the generation process. Further analysis is required on the correctness of fallback plans selected by the LLM, and whether fallbacks can be programmatically determined upon latency timeout. Finally, continual learning based on the delayed anomaly assessment of the generative reasoner could be used to avoid triggering the slow reasoner on non-safety-critical anomalies a second time."}, {"title": "APPENDIX", "content": "These appendices contain further details on the experiments in the main body of the paper, additional results and ablations supporting the main hypotheses of the paper, analysis of these supplementary results, and proofs of theoretical results. Besides the results within this document, we also refer the reader to videos of the quadrotor experiments in the quad_videos/ directory of the supplemental material. Furthermore, we include videos describing our approach, prompt templates, and further results on our project page: https://sites.google.com/view/\naesop-llm. These appendices are organized as follows:\nA. Notation and Glossary\nWe include a glossary of all the notation and symbols used in this paper in Table V.\nB. Background: Anomaly Detection\nIn essence, an anomaly detector is a classifier $$h: \\mathcal{O} \\rightarrow \\{nominal, anomaly\\}$$ that maps observations to a detection at runtime. However, limited access to anomalous examples (after all, it is their dissimilarity from prior experiences that makes them anomalous) typically precludes us from training a classifier with an obvious decision boundary using supervised learning [3, 40]. Instead, anomaly detection algorithms require two steps: First, we must construct a scalar score function $$s(0) \\in \\mathbb{R}$$ from an observation, where a higher score indicates that the sample is \u201cmore\u201d anomalous. Second, we need to cali-brate a decision threshold $$T \\in \\mathbb{R}$$ on the score function such that:\n$$h(o) = \\begin{cases}anomaly & if s(0)>T\\\\nominal & if s(0) 39].\nSlow Generative Reasoning: Once we detect an anomaly, we trigger the autoregressive generation of an LLM to generate a zero-shot assessment of whether we need to engage any of the interventions associated with the recovery sets $$\\mathcal{X}_1$$,\u2026\u2026\u2026,$$\\mathcal{X}_d$$ (\u00a7III) to maintain the safety of the system. The value of this approach is that the LLM's internet-scale pretraining data allows it to generate outputs that resemble the generalist common sense reasoning that a human operator is likely to suggest, as a result, making superior decisions on OOD examples, on which existing task-specific learning algorithms are notoriously unreliable.\nTo do so, we follow [12] in using a VLM to convert the robot's current visual observation into a text description of the environment. We simply encode this scene description into a prompt that provides context on the monitoring task, as illustrated in Fig. 3. We then parse the resulting output string to yield a classification $$y \\in \\{0,1,...d\\}$$ on whether the anomaly does not present a hazard and the system can continue it's nominal operation ($$y=0$$), or whether we should engage intervention\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023."}]}