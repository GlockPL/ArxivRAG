{"title": "A Hybrid Graph Neural Network for Enhanced\nEEG-Based Depression Detection", "authors": ["Yiye Wang", "Wenming Zheng", "Yang Li", "Hao Yang"], "abstract": "Graph neural networks (GNNs) are becoming increasingly\npopular for EEG-based depression detection. However, previous GNN-\nbased methods fail to sufficiently consider the characteristics of depres-\nsion, thus limiting their performance. Firstly, studies in neuroscience\nindicate that depression patients exhibit both common and individual-\nized brain abnormal patterns. Previous GNN-based approaches typically\nfocus either on fixed graph connections to capture common abnormal\nbrain patterns or on adaptive connections to capture individualized pat-\nterns, which is inadequate for depression detection. Secondly, brain net-\nwork exhibits a hierarchical structure, which includes the arrangement\nfrom channel-level graph to region-level graph. This hierarchical struc-\nture varies among individuals and contains significant information rel-\nevant to detecting depression. Nonetheless, previous GNN-based meth-\nods overlook these individualized hierarchical information. To address\nthese issues, we propose a Hybrid GNN (HGNN) that merges a Com-\nmon Graph Neural Network (CGNN) branch utilizing fixed connection\nand an Individualized Graph Neural Network (IGNN) branch employ-\ning adaptive connections. The two branches capture common and indi-\nvidualized depression patterns respectively, complementing each other.\nFurthermore, we enhance the IGNN branch with a Graph Pooling and\nUnpooling Module (GPUM) to extract individualized hierarchical infor-\nmation. Extensive experiments on two public datasets show that our\nmodel achieves state-of-the-art performance. The code and models are\navailable at https://github.com/wyy0925/HGNN_I\nKeywords: EEG-based depression detection Graph neural network.\nGraph connection Graph pooling", "sections": [{"title": "1 Introduction", "content": "Depression is a medical condition that includes abnormalities of mood, cognition\nand neurovegetative functions [1]. Depression has become a significant public\nhealth issue, affecting more than 300 million people worldwide [2] and causing\nabout 850,000 suicides each year [3]. Clinically, doctors often diagnose depression"}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Overview", "content": "The framework of the proposed HybGNN is shown in Fig.1. The input of our\nmodel is the raw data segments of the EEG signal and a 1-D CNN is designed to\nextract the temporal features. Two key parts in the proposed HybGNN model\nare summarized as follows: 1) To simultaneously identify common depression-\nrelated patterns and address individual differences, we propose two branches of\nGNNs: the Common Graph Neural Network (CGNN, shown in the upper branch\nof the figure) and the Individualized Graph Neural Network (IGNN, shown in\nthe lower branch of the figure). 2) To further extract hierarchical information,\na Graph Pooling and Unpooling Module (GPUM) is applied in the IGNN. The\ndetails of the model are introduced in the following sections."}, {"title": "2.2 EEG Temporal Feature Extraction", "content": "1-D CNN has been widely used in multivariate time series, such as biomedical\ndata, due to its competitive performance, as well as its real-time and low-cost\nhardware implementation compared to RNN-based methods [25]. We employ a\n1-D CNN module to extract temporal dependencies within each electrode, which\nis operated on time dimension and ensures that the spatial information remains\nas it is, to construct a graph in the next step.\nA raw EEG sample is denoted by $S \\in \\mathbb{R}^{N\\times T_s}$, where the sample originates\nfrom $N$ EEG electrodes and each electrode has $T_s$ time stamps. We use the 1-D\nCNN to extract temperal features from the raw EEG sample $S$ and the output\nof 1-D CNN can be denoted as $X \\in \\mathbb{R}^{N\\times F_d}$, where $N$ is the number of EEG\nelectrodes and $F_d$ is the number of features on each electrode."}, {"title": "2.3 CGNN and IGNN", "content": "Definition of Brain Network A brain network can be defined as a graph\nin the form of $G = \\{V,E, A\\}$, where $V$ represents the set of nodes with the\nnumber of $|V| = N$, and each node in the network represents an EEG electrode.\n$E$ represents the set of edges, which indicates the relationships between the\nnodes. $A \\in \\mathbb{R}^{N\\times N}$ denotes the adjacency matrix, whose element $a_{ij}$ measures\nthe importance of the connection between the $i$th node and the $j$th one. In our\nmodel, $X \\in \\mathbb{R}^{N\\times F_d}$, which denotes the EEG features matrix extracted by 1-D\nCNN above, is transformed into two graphs: $G_c$ and $G_I$. $G_C$ denotes the graph\nin the CGNN and $G_I$ denotes the graph in the IGNN.\nConstruction of CGNN Previous researchers have identified several common\nabnormal patterns that indicate depression, which are shared by most patients"}, {"title": "Graph Convolution in CGNN and IGNN", "content": "After the construction of the\ntwo graphs, we perform graph convolution operations within the two GNNs re-\nspectively to extract richer spatial features. Generally, GNNs perform a feature\ntransformation on $X \\in \\mathbb{R}^{N\\times F_d}$ and produce an output $Y \\in \\mathbb{R}^{N\\times d}$, where $d$ rep-\nresents the output feature dimension. We adopt the popular graph convolutional\nnetworks (GCNs) [30] and the transformation of the feature between adjacent\nlayers of GCNs can be written as follows:\n$H^{(l)} = \\sigma \\left( \\widetilde{D}^{-\\frac{1}{2}} \\widetilde{A} \\widetilde{D}^{-\\frac{1}{2}} H^{(l-1)} W^{(l-1)} \\right),$\\nwhere $l = 1,2,..., L$. $L$ denotes number of layers. $\\widetilde{A} = A + I$, $\\widetilde{D} = \\sum_j \\widetilde{A}_{ij}$,\n$H^{(0)} = X$, $H^{(L)} = Y$. $H^{(l)}$ denotes the output of $l$ layers of GCNs. $W^{(l-1)} \\in\n\\mathbb{R}^{F_d \\times d}$ denotes a trainable weight matrix at layer $l - 1$. To capture different"}, {"title": "2.4 Graph Pooling and Unpooling Module", "content": "Hierarchical structure refers to the arrangement from channel-level graph to\nregion-level graph. In order to extract individualized hierarchical information, we\nembed a graph pooling and unpooling module (GPUM) into the IGNN, which\nis illustrated in Fig.1. Since how to partition EEG channels into brain regions\nis not well-defined, we propose an adaptive pooling operation to automatically\ndetermine these brain regions instead of manually defining them.\nIn graph pooling, an assignment matrix is learned that takes into account\nboth the adjacency matrix and the node features. Suppose that the number of\nnodes is $N$, the number of regions is $N_r$, and the assignment matrix is denoted as\n$R\\in \\mathbb{R}^{N\\times N_r}$. Each row of the assignment matrix represents an original node and\neach column represents a region. R performs a assignment of each original node\nto a region. This assignment varies adaptively according to the EEG instances,\ndefined as\n$R= softmax(\\widehat{A}_{I}XQ),$\\nwhere $X \\in \\mathbb{R}^{N\\times F_d}$ is the input features, $\\widehat{A}_{I}$ is the normalized adjacency matrix\ndefined above in the IGNN and $Q\\in \\mathbb{R}^{F_d\\times N_r}$ denotes the parameter matrix to\nfuse the features. The assignment matrix $R$ generates a new coarsened adjacency\nmatrix representing the relationships between the regions and a new matrix of\nembeddings for the each region:\n$A_r = R^T A_I R \\in \\mathbb{R}^{N_r\\times N_r},$\n$X_r = R^T X \\in \\mathbb{R}^{N_r\\times F_d},$"}, {"title": "Loss Function", "content": "Note that the membership for each brain region should be clearly defined, i.e.,\none node comes to one region in assignment matrix R. So an entropy minimiza-\ntion regularization is composed to each row of R, encouraging the rows to be\napproximately one-hot vectors [23]. Consequently, our model aims to minimize\nboth the cross-entropy loss and the entropy minimization loss of the assignment\nmatrix R. The loss can be formulated as follows:\n$\\mathcal{L} = - \\sum_{c=1}^C y_c log \\hat{y}_c - \\lambda \\sum_{i=1}^N \\sum_{j=1}^{N_r} R_{ij} log R_{ij},$\\nwhere $C$ denotes number of classes, $y_c \\in \\{0,1\\}$, $\\hat{y}_c \\in [0, 1]$. If the sample belongs\nto the c-th class, then $y_c = 1$, otherwise $y_c = 0$. $\\hat{y}_c$ represents the predicted\nprobability that the sample is classified into the c-th class. The second term is\nthe entropy minimization loss, where $\\lambda$ is the regularization coefficient, $N$ is the\nnumber of nodes and $N_r$ is the number of regions."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Datasets and preprocessing", "content": "Two public datasets are used to validate the effectiveness of the proposed model.\nWe mainly focus on the resting-state EEG data in the two datasets.\n1)MODMA: Multi-modal Open Dataset for Mental Disorder Analysis\n(MODMA) [32] consists of EEG data from 24 depression patients and 29 healthy\ncontrols(HCs). Each subject has 5 minutes of resting-state data acquired by an\nEEG cap with 128 channels.\n2)HUSM: The Hospital University Sains Malaysia (HUSM) [34] dataset\nconprises EEG data from 34 depression patients and 30 HCs. Each participant\nwas recorded for two conditions with a 19-channels EEG cap: 5 minutes with\neyes closed and 5 minutes with eyes open.\nWe first perform pre-processing following the previous studies [14]. For the\nMODMA dataset, we only use the 19 channels in the 10-20 system to evaluate\nthe generalizability of our method and to be consistent with HUSM dataset. We\nfollow the previous study [14] to divide the EEG signals into multiple fragments\nand each fragment has data of 4 s with 75% overlap, based on which we obtain\n6864 MDD segments and 8294 HC segments. For the HUSM dataset, we divided\nthe EEG recordings into non-overlapping 4-second segments and combined the\neyes-closed and eyes-open data. After removing subjects with missing data, we\nobtained 3726 MDD segments and 3588 HC segments."}, {"title": "Implementation Details", "content": "We follow previous studies [10, 13, 20] and perform a ten-fold cross-validation on\neach dataset to obtain reliable evaluation results. Furthermore, to prevent infor-\nmation leakage, the data from a subject is used exclusively in either the training\nset or the test set. To thoroughly assess depression detection performance, ac-\ncuracy (ACC), recall (REC), precision (PRE), and F1 score (F1) are used as\nevaluation metrics in experiments.\nWe implement HybGNN using PyTorch libraries [49] on a NVIDIA GeForce\nRTX 4060. For the MODMA dataset, the learning rate is set to 0.09, the max-\nimum number of epochs is set to 100 and the number of regions $N_r$ is set to 5.\nFor the HUSM dataset, the learning rate is set to 0.001, the maximum number\nof epochs is set to 60 and the $N_r$ is set to 4. For both datasets, the number of\nGCN layers $L$ in two branches is set to 2, and the number of GCN layers $L'$ in\nGPUM is set to 1. The batch size is set to 128, and the regularization coefficient\n$\\lambda$ is set to $10^{-5}$."}, {"title": "3.3 Compared method", "content": "To evaluate the proposed HybGNN model, we perform a quantitative compar-\nison with several other approaches applied in the depression detection as well\nas those popular in other EEG tasks: EEGNet [26], DeprNet [14], GCN [30],"}, {"title": "4 Results and Discussion", "content": ""}, {"title": "4.1 Comparison of Depression Detection Performance", "content": "The results of the detection performance comparison on the MODMA and HUSM\ndatasets are summarized in Table 1. Our proposed model outperforms other\nmethods across most metrics on both datasets, demonstrating its effectiveness\nin depression detection. Specifically, our HybGNN achieves superior detection\nresults. On the MODMA dataset, it gains a 1.69% higher accuracy and a 0.017\nhigher F1 score compared to SDGCN, which achieves the second-best result on\nthis dataset. Similarly, on the HUSM dataset, HybGNN achieves a 1.45% higher\naccuracy and a 0.025 higher F1 score compared to IGCNN. Although they are\nall graph-based approaches, our model stands out in its ability to represent the\nbrain connection better and integrate hierarchical information.\nOur model improves EEG-based detection on MODMA and HUSM dataset,\nachieving the highest accuracy (95.42% on MODMA, 93.50% on HUSM), preci-\nsion (94.44% on MODMA, 93.37% on HUSM), and F1 score (0.955 on MODMA,\n0.941 on HUSM). While DGCNN achieves the highest recall on MODMA\n(98.19%), our model still performs competitively with a recall of 97.18%."}, {"title": "4.2 Ablation Studies", "content": "To further verify the validity of the key components in our model, we conduct\nan ablation study. In the experiment, we initially consider the CGNN as the\nbaseline framework. Subsequently, we progressively integrate additional compo-\nnents, including CGNN, IGNN (without GPUM) and GPUM to construct the\nproposed model. Additionally, we explore the optimal integration of the GPUM\nmodule.\nSpecifically, Variant a, b, c are generated to verify the effectiveness of CGNN,\nIGNN (without GPUM) and GPUM respectively, with Variant a serving as\nthe baseline. Variant d, e are generated to explore the optimal integration of\nthe GPUM and compare with our HGNN. The specific process is described as\nfollows:\nVariant a: CGNN without GPUM.\nVariant b: IGNN without GPUM.\nVariant c: CGNN without GPUM + IGNN without GPUM.\nVariant d: CGNN with GPUM + IGNN without GPUM.\nVariant e: CGNN with GPUM + IGNN with GPUM.\nOurs: CGNN without GPUM + IGNN with GPUM.\nThe results are shown in Table 2. Firstly, by comparing the Variant a, b\nwith Variant c, we notice a decrease in depression detection accuracy on both\ndatasets without CGNN or IGNN. This leads us to conclude that CGNN and\nIGNN together improve the accuracy of depression detection. Secondly, Com-\nparing the Variant c with our model reveals that GPUM module effectively aids\nin extracting hierarchical information, thereby improving depression detection\naccuracy. Additionally, by comparing the Variant d, e with our model, we infer\nthat integrating GPUM with the IGNN yields the best performance. This proves\nthat hierarchical information based on individualized graph connections is more\ndiscriminative. In summary, the ablation study demonstrates the effectiveness\nof each key component in our model."}, {"title": "4.3 Hyperparameter Optimization", "content": "The outcomes of hyperparameter optimization are depicted in Figure 2. Re-\ngarding the number of regions $N_r$, the proposed model maintains optimal per-\nformance for HUSM at $N_r = 4$, and for MODMA at $N_r = 5$. Regarding the\nregularization coefficient $\\lambda$, the proposed model obtains the best performance at\n$\\lambda = 10^{-5}$ for the both datasets."}, {"title": "5 Conclusion", "content": "In this paper, we introduce a hybrid graph neural network (HybGNN) for de-\npression detection using EEG data. Our HybGNN framework incorporates both\na common graph neural network (CGNN) branch and an individualized graph\nneural network (IGNN) branch to identify depression-related patterns while ac-\ncommodating individual variations. Additionally, the IGNN includes a graph\npooling and unpooling module (GPUM) to extract individualized hierarchical\ninformation. Extensive experiments on two public datasets demonstrate that\nHybGNN achieves state-of-the-art performance."}]}