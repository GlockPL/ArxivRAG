{"title": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation", "authors": ["Shahriar Rifat", "Jonathan Ashdown", "Francesco Restuccia"], "abstract": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate the performance degradation of Deep Neural Networks (DNNs) in the presence of corruption/ noise affecting inputs. Existing approaches in TTA continuously adapt the DNN, leading to excessive resource consumption and performance degradation due to accumulation of error stemming from lack of supervision. In this work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to address such issues. Our key approach is to proactively learn latent representations of some corruption types, each one associated with a sub-network state tailored to correctly classify inputs affected by that corruption. After deployment, DARDA adapts the DNN to previously unseen corruptions in an unsupervised fashion by (i) estimating the latent representation of the ongoing corruption; (ii) selecting the sub-network whose associated corruption is the closest in the latent space to the ongoing corruption; and (iii) adapting DNN state, so that its representation matches the ongoing corruption. This way, DARDA is more resource-efficient and can swiftly adapt to new distributions caused by different corruptions without requiring a large variety of input data. Through experiments with two popular mobile edge devices \u2013 Raspberry Pi and NVIDIA Jetson Nano we show that DARDA reduces energy consumption and average cache memory footprint respectively by 1.74\u00d7 and 2.64\u00d7 with respect to the state of the art, while increasing the performance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.", "sections": [{"title": "1. Introduction", "content": "Traditional mobile edge computing scenarios assume that the inputs of DNNs are received uncorrupted. However, in many real-life scenarios, sudden and unexpected corruptions (e.g., snowy or foggy conditions) can cause a drastic change in data distribution, consequently causing performance loss [1, 20]. For example, a semantic segmentation DNN trained with data collected in normal weather conditions has been shown to exhibit a performance loss of more than 30% when tested in snowy conditions [19], while an image classification DNN can experience a similar decrease in the case of reduced lighting conditions [7].\nTest-time Adaptation (TTA) tackles this issue by adapting the DNN with unlabeled test data in an online manner, thus handling distributional shifts in real time. Existing TTA methods lose performance when encountering continuously changing distributions with highly correlated input samples [23, 25]. This assumption is true in many real-world scenarios. For example, an unmanned autonomous vehicle (UAV) monitoring an outdoor environment will likely encounter similar classes as video feeds are very likely to be highly correlated when considering limited time spans. Continuous adaptation of an edge deployed DNN to such a challenging yet practical scenario causes many adaptation methods to fail. Moreover, existing methods lack awareness of when the domain shift happens, thus they continuously fine-tune the DNN even if there is no shift in data distribution. However, in a real-life deployment scenario, certain data distribution might persist for a certain period of time (e.g., a bright sunny day). This imposes unnecessary burden on energy consumption and cache memory \u2013 without yielding performance improvements.\nTo address the critical issues defined above, we propose a new framework named Domain-Aware Real-Time Dynamic"}, {"title": "2. Related Work and Existing Issues", "content": "Some existing works [3, 21] on TTA have provided empirical evidence of performance improvement by only re-estimating the normalization statistics of Batch Normalization (BN) layers from test data. The absence of supervision is typically covered by two unsupervised forms of losses. Firstly, a line of work [4, 17, 24] minimizes the entropy of the predictions over a batch of data to prevent the collapse of a trivial solution. Invariance regularization-based TTA algorithms perform some data augmentation (e.g. rotation [25], adversarial perturbation [16]) on test data during inference. The inconsistency of the prediction of DNN on different augmented test data is leveraged as an unsupervised loss function to update the learnable parameters during inference. The proposed DARDA framework uses cross-modal learning to acquire a shared representation space between the corruption space and the DNN space [18, 30]. However, to our knowledge, none of the existing research addresses cross-modal learning between the corruption process and the state of the DNN model. Next, we discuss into some practical limitations of TTA in edge vision application.\nExcessive Resource Consumption. To improve performance, existing TTA approaches typically involve continuous adaptation even with uncorrupted input samples, thus imposing a heavy burden on edge resources. Ideally, adaptation should be only performed upon changes in the corruption process, thereby conserving constrained resources such as energy and processing power at the edge. Despite the potential benefits, current methods have yet to explore this direction."}, {"title": "3. Problem Statement", "content": "We define ds as the number of available learning domains, each characterizing a different imperfection and/or corruption type. We further define the related set of ds datasets where $x_i^d$ and $y_i^d$ indicates ith data sample and label from domain d respectively, as\n$D^{d}=\\{(x_{i}^{d}, y_{i}^{d})\\}_{i=1}^{n_{d}}$, with 0 \u2264 d < ds. (1)\nEach dataset Dd is composed of na independent and identically distributed (IID) samples characterized by some probability distribution $P^{d}(X, Y)$ where X and Y are random variables of input and output respectively. We assume a DNN has been trained on an uncorrupted dataset and ds sub-networks f(.; 0d) are created so that (i) their architecture includes the batch normalization layer and the dense layers of the DNN; (ii) their weights \u03b8\u03b1 are obtained by fine-tuning each sub-network to each specific domain. We assume continuous and correlated (thus, non-IID) data flow to the DNN in real time, coming from du unknown domain datasets Du, with 0 < u < du. By unknown we mean $P^{u}(X,Y) \u2260 P^{d}(X, Y)$, for all (0 \u2264 d < ds , 0 \u2264 u < du).\nWe define a domain latent space $O \u2286 R^o$, where o is the dimension of the latent space. Our goal is to (i) sense when the data flow has changed domain from the current domain d to the unknown domain u; (ii) infer domain t that is closest to u in the latent space; (ii) select the related fine-tuned sub-network f(.; 0t) to quickly recover performance, and (iii) adapt f(.; 0t) so as to find optimal f*(.; 0*) such that:\n$\\theta^* = \\arg \\min_{\\Theta_{\\tau}} \\sum_{i=1}^{N_u} \\mathcal{L} \\{ f(x_i^u; \\Theta_{\\tau}); y_i^u \\}$ (2)\nwhere nu is a given number of samples in the unknown domain. Such samples are assumed to be available sequentially and the distribution of labels is different from the current domain's, i.e., Pu(Y) \u2260 Pd(Y). Notice that ground-truth labels yu are usually not available in real-world settings and are only used for performance evaluation."}, {"title": "4. Description of DARDA Framework", "content": "The main components of DARDA are a corruption extractor (Section Sec. 4.1), a corruption encoder (Section Sec. 4.2) and a model encoder (Section Sec. 4.3), a new corruption-aware memory bank (Section Sec. 4.4), new batch normalization scheme (Section Sec. 4.5) and a new real-time adaptation module (Section Sec. 4.6)."}, {"title": "4.1. DARDA Corruption Extractor", "content": "Fig. 3 shows our proposed corruption extraction approach. Our key intuition is that features related to corruption and semantic features for inference are tightly intertwined. Since decoupling these features is difficult without corresponding clean samples, we design a process to decouple corruption features without corresponding clean sample. Specifically, we learn the corruption features by mapping corrupted data to a different corrupted version of the same data [8, 13]. For a given corrupted data x we down-sample it through two convolution kernels with static filters G1(.) = [[0,0.5], [0.5,0]] and G2(.) = [[0.5,0], [0, 0.5]] to generate two downsampled version of the corrupted data. From the first downsampled corrupted data G1(x), we try to create an exact copy of the other downsampled data G2(x) by subtracting some residual information learned by passing G1(x) through the corruption extractor g(.). We denote this predicted copy as $G_2(x)$. Similarly, we compute G1(x) from G2(x). The mapping functions are as follows:\n$G_{2}(x)=G_{1}(x)-g(G_{1}(x))$ (3)\n$G_{1}(x)=G_{2}(x)-g(G_{2}(x))$ (4)\nIn Fig. 3, the extracted residual is denoted by dotted lines. The parameters of g(\u00b7) can be optimized by minimizing the following loss function, which is the loss mean squared error (MSE) indicated in Fig. 3.\n$\\mathcal{L}_{N}=\\sum_{c=1}^{d_{s}} \\sum_{i=1}^{n_{d}} (||G_{2}(c_{i})-G_{2}(P_{c,i})||^{2} + ||G_{1}(P_{c,i})-G_{1}(c_{i})||^{2})$ (5)"}, {"title": "4.2. DARDA Corruption Encoder", "content": "We use the corruption-related features to detect a corruption shift in real time. Specifically, we use a corruption encoder h(.) to encode corruption information of the input data from the known corruption types into a projection in the corruption latent space. While generating the latent space, we ensure that samples from the same corruption distribution are grouped together in that latent space and samples from different corruption distribution are located distant from each other. For the N samples {Ci, Di}$^{2.N}_{i=1}$ in a training data batch, we define C as the set of each corruption projection $C_i^2$ in the latent space. We also define as $D_i^2$ as the corruption label for projection $C_i^2$, and as D the corresponding set. We define the following supervised contrastive loss function for a batch of training data:\n$\\mathcal{L}_{D}(C, D)=\\sum_{i=1}^{2.N}L_{i}(C, D)$ (6)\nwhere $L_i(C, D)$ is defined as\n$L_{i}(C, D) = - \\frac{1}{2.N_d} \\sum_{j=1}^{2.N} \\frac{1}{ \\sum_{k=1}^{2.N} \\chi_{i \\ne k} \\exp(C_i^2 C_k^2/\\tau) }$ (7)\nwhere N represents the total number of samples in the batch and \u03c4 is a scaling parameter. While training the corruption encoder, we generate a soft augmentation (random rotations and flips) from each data sample to have more samples from each noise classes. This way, our training batch size becomes 2 \u00b7 N. For each sample i in our training batch, we calculate its contrastive loss using Eq. Eq. (7). Here, the numerator enforces cosine similarity between similar corruption types and the denominator penalizes high similarity between projections which are from different corruption class.\nThus, similar corruption projections are positioned closer and dissimilar ones are positioned far apart. We jointly train the corruption extractor g(.) and corruption encoder h(.) by minimizing the loss function:\n$\\mathcal{L} = \\mathcal{L}_D + \\lambda_e \\mathcal{L}_N$ (8)\nwhere \u03bbe is a constant which does not impact performance yet makes the convergence of g(.) and h(.) faster. The training process is described in Algorithm 2."}, {"title": "4.3. DARDA Sub-Network Encoder", "content": "To guide the adaptation of the sub-network, we need to obtain a \"fingerprint\" of the current sub-network, whose state space is by definition continuous and infinite. We address this issue by creating a set of unique fingerprints $F_1 ... F_{ds}$ of each sub-network by feeding a fixed Gaussian noise into the DNN and consider its output response vector as the fingerprint of the sub-network. Our intuition is that since a DNN works as a non-linear function approximator, it will produce a different output for the same input with different parameters. We generate a signature Sd of each sub-network from each fingerprint Fd as Sd = S(Fd;\u03c8) where S: F\u2192 R\u00ba which is a shallow neural network parameterized by & that maps the fingerprint to the corruption latent space. The S encoder is trained so as to minimize the following loss function, which maximizes the cosine similarity between the latent space projections:\n$\\mathcal{L}_{CM} = \\sum_{i=1}^{d_s} \\sum_{j=1}^{d_s} \\mathbf{1}_{i=j} \\{ \\exp(- S_i^{2} \u00b7 C^j) \\}$ (9)\nHere, 1(.) is the indicator function. We use a regularization term in addition to LCM to distribute encoder S's projection in regions from where sub-network's projections would produce well-performing sub networks. The measured cosine similarity between a sub-network signature $S_i$ and a corruption signature $C_j$ is converted into probability distribution \u03c0ij using:\n$\\Pi_{ij} = \\sigma (\\frac{S_i \u00b7 C^j}{\\sum_{k=1}^{d_s} S_k \u00b7 C^k})$ , i, j\u2208 [0, ds] (10)\nwhere \u03c3 is the softmax function. If aij indicates accuracy of sub-network i in corruption domain j we can generate a probability distribution aij such as\n$\\alpha_{ij} = \\sigma (\\frac{log \\frac{1}{1 - a_{ij}}}{\\sum_{k=1}^{d_s} log \\frac{1}{1 - a_{ik}}})$ ,i,j\u2208 [0,ds] (11)\nWe can calibrate the sub-network encoder to to generate projections that have affinity with other corruption domains where it can perform well by minimizing the KL divergence"}, {"title": "4.4. Corruption-Aware Memory Bank", "content": "In practical scenarios, the distribution of labels differs from the actual label distribution. Importantly, while during training, the DNN is given input data with IID labels, in real-world scenarios sequential data is highly correlated while other classes are very scarce at a particular time. Adaptation to this unreliable label distribution leads to substantial performance loss in traditional approaches, as shown in Section Sec. 5. To address this problem, we need to have a stable snapshot of the ongoing corruption at inference time. Thus, we create and maintain a memory bank M with N slots to store samples. We construct the memory bank in a label-balanced manner. Recalling that Y is the set of labels, for each class y \u2208 Y, we we store \u2611 number of incoming test samples. As we do not have labeled data, the labels are inferred from the prediction \u0177 of the model. However, sampling based on prediction of continuously adapted model leads to error accumulation [28].\n$L_r = \\sum_{i=1}^{d_s} \\sum_{j=1}^{d_s} |\\pi_{ij}- a_{ij}|$ (12)\n$L_m = L_{CM} + \\lambda_r L_r$, (13)\nwhere 0 < \u03bb, < 1 is a regularization parameter.\nTo solve error accumulation, existing methods [25, 28] resort to inference with multiple DNNs by feeding different augmented views of test samples to them. However, this requires additional computation and memory for multiple inference. Although sensing the corruption and bootstrapping with proper sub-network signature leads to reliable memory bank construction, we store the samples that are only representative of the ongoing corruption. For each incoming test samples we predict its class label, and store it in the memory bank if we have room for that particular class and if it is highly representative of the ongoing corruption. The process of memory bank construction is described in Algorithm 1."}, {"title": "4.5. Corruption-Aware Batch Normalization", "content": "Due to sudden corruption, the normalization statistics (\u03bc, \u03c3\u00b2) estimated on uncorrupted training data become unreliable. Although using a specific sub-network with normalization statistics (\u03bc\u03c2, \u03c3\u00b2) would be feasible, we can use the samples stored in the memory bank to further refine our estimation of the ongoing statistics (\u03bc\u03c4, \u03c3\u03c4). To this end, we use BN [9]. Let A' \u2208RB\u00d7Ch\u00b9\u00d7N\u00b9 be a batch of activation tensors of the lth convolutional layer, where B corresponds to the batch size, Ch\u00b9 denotes the number of channels in [th layer and Ni is the dimension of activations in each channel.\nA BN layer first calculates $\\mu_{ch}=\\frac{1}{|B||N|} \\sum_{b\\in B, n \\in N}{A_{b,n}^{(l)}}$ and $\\sigma_{ch}=\\frac{1}{|B|} \\sum_{b\\in B}(A_{b,n}^{(l)} - \\mu_{ch})^2$ and subtracts \u03bcch from all input activations in the channel. Subsequently, BN divides the centered activation by the standard deviation \u03c3\u03c4\u03b7. Normalization is applied:\n$B N\\left(A_{b, c h, N}\\right)=\\gamma \\frac{A_{b, c h, N}-\\mu_{c h}}{\\sqrt{\\sigma_{c h}^2+\\epsilon}}+\\beta \\forall b, c h, N,$ (14)\nHere, \u03b3 and \u03b2 are the affine scaling and shifting parameters followed by normalization, while (\u20ac > 0) is a small constant added for numerical stability. The normalized and affine transformed outputs are passed to the next (l + 1)th layer, while the normalized output is kept to the [th layer. BN also keeps track of the estimate of running mean and variance to use during the inference phase as a global estimate of normalization statistics, and y and \u1e9e are optimized with the other DNN parameters through back propagation.\nWhenever the corruption changes, the projection from the corruption encoder matches with the closest corruption centroid in the latent space. At the same time, samples affected by the new corruption are being stored in the memory bank. As we constrict the memory bank to have certain amount of samples from a particular class, due to our design of non IID real world data stream, initially there will be samples from previous corruption distribution also on the memory bank. Fig. Fig. S4 shows that the projections for even the unknown corruption get clustered nearby in the latent space. When the samples of the memory bank become representative of the current corruption, they should have low variance among their cosine similarity with current closest corruption centroid Ccur. Therefore, when the change in corruption is detected and the variance of cosine similarity from the centroid becomes lower than thresh, current DNN normalization statistics are updated as:\n$\\mu_t=(1-m) \u00b7 \\mu_s + m \u00b7 \\hat{\\mu}$\n$\\sigma_t=(1-m) \u00b7 \\sigma_s + m \u00b7 \\hat{\\sigma}$ (15)\nwhere m is the momentum and (\u00fbt, \u00f47) are the current normalization statistics of different layers of the DNN, which we obtain by making one forward pass using the samples in the memory bank."}, {"title": "4.6. Corruption-Aware Real-Time DNN Adaptation", "content": "Adapting the parameters of the current DNN in an unsupervised manner usually needs careful selection of hyperparameters. To avoid this issue, only the sub-network is adapted. As explained earlier, we make one forward pass with the samples in the memory bank and the fixed Gaussian noise to calculate the normalization statistics of the current ongoing corruption and current sub-network fingerprint. The mean of the corruption embedding C="}, {"title": "5. Experimental Results", "content": "Datasets. We use the tool described in [14] to synthetically generate realistic corruptions for CIFAR-10 and CIFAR-100 datasets. Both CIFAR-10 and CIFAR-100 have 50,000 training images and 10,000 testing images. In line with prior work, we use the following 15 different corruptions of different categories which are \u201cNoise\u201d (Gaussian, shot, impulse), \"Blur\" (defocus, glass, motion, zoom), \u201cWeather\u201d (snow, frost, fog, bright) and \u201cDigital\u201d (contrast, elastic, pixelate, JPEG) to train our corruption encoder. For fair comparison, we evaluate the performance on 4 corruptions (Gaussian blur, saturate, spatter, speckle noise) which are unseen during training phase. In line with [3, 23, 25, 29], we consider the corruptions in their highest severity."}, {"title": "5.1. Comparison with State-of-the-Art Benchmarks", "content": "Fig. 4a and Fig. 4b show the performance DARDA as compared to other state of the art approaches. We show results with ideal IID assumption and a more realistic non-IID assumption (i.e., samples are correlated).\nAs we can observe, DARDA performs consistently better in both datasets and across both setups, with DARDA improving the performance by 10.4% on CIFAR-10 test corruptions and 5.7% on CIFAR-100 compared to the 2nd best performing baseline ROTTA. Among the other considered baselines, TENT, COTTA and BN achieves poor performance for non-IID samples. Since DARDA starts from a bootstrapped sub-network in changed corruption domain and update only with samples from our memory bank and corruption latent space, DARDA performs consistently well in both cases. Since NOTE is equipped to handle correlation among online data batches, there is no significant performance drop for non-IID assumption. However, NOTE resets the DNN after evaluation on each corruption type which is unrealistic as it does not have the capability to know when current corruption domain is changing. Thus, due to error accumulation for continuous adaptation, the performance is fairly poor. DARDA does not accumulate errors from the previous corruption domain, as in every new corruption domain we start from a new DNN state."}, {"title": "5.2. Sensitivity to Correlated Samples", "content": "The performance of DARDA does not depend on the label space to bootstrap from an appropriate sub-network. To prove this point, we plot the average performance in the first five batches of incoming data by varying the value of the correlation parameter 8 for the CIFAR-100 dataset using sequences similar to those used for Fig. 4b. From Fig. 5 it emerges that the correlation does not have a sensible effect on DARDA. Furthermore, as the value of d decreases, the performance of BN and CoTTA drastically decreases, and for the best method RoTTA, performance starts to plummet when encountering severe correlation. Indeed, with high correlation among samples, there is not enough sample diversity to have a stable update of the DNN. However, DARDA can reliably sense corruption drift even with a single sample and bootstrap with the most similar sub-network stored in the buffer. Thus, even for extremely correlated samples, we have a well-performing sub-network from the early instances of data batches after incurring in corruption. This indicates the efficacy and reliability of DARDA in critical mobile edge computing scenarios."}, {"title": "5.3. Effect of Batch Size and Dirichlet Parameter", "content": "We can observe from Fig. 6 that the batch size and Dirichlet parameter do not have a significant effect on performance of DARDA. This is because the corruption signature can be extracted even with a single sample. Moreover,"}, {"title": "5.4. Evaluation of Catastrophic Forgetting", "content": "The adaptation of DNN should not result in performance degradation on uncorrupted inputs, since in most real-life scenarios uncorrupted data is most common. However, during TTA, the DNN might get specialized in certain corruptions and fail to deliver performance. Fig. 8 shows the performance showing 5 uncorrupted data batches from CIFAR-10 after two sequences, which are (on the left subfigure) saturate \u2192 Gaussian blur \u2192 spatter \u2192 speckle and (on"}, {"title": "5.5. On-Device Dynamic Adaptation Efficiency", "content": "In this section, we evaluate the on-device performance of DARDA in diverse edge platforms. We select commonly available Raspberry-Pi-5 and Nvidia Jetson-Nano, since they are representative of resource-constrained devices widely applied for mobile vision applications.\nDARDA adapts using one backward pass to update the sub-network only when a corruption is perceived by the corruption extractor. The corruption extractor and corruption encoder are also active for each data sample. Tab. 1 reports the average number of multiply and accumulate (MAC) operations for the forward pass of each method. For the backward pass, the average number of samples for different corruptions with which the backward pass was called is reported. To calculate DARDA forward pass MAC, all the operations involved in the corruption extractor, corruption encoder and sub-network encoder are summed with the operation performed by different sub-networks. To support continuous adaptation without error accumulation, COTTA and ROTTA continuously perform two forward passes with the original data sample and another augmented sample respectively. Thus, their number of operations is two times more than BN, TENT and CoTTA. Although BN, TENT and NOTE have less forward computation than DARDA, the unrealistic assumption of IID data stream and episodic adaptation - notice that the DNN state is continuously reset after adaptation \u2013 which makes them not applicable in real-time mobile edge applications. From Fig. 9 it is also evident that DARDA incurs lower CPU and GPU latency compared to closest performing benchmarks.\nFrom Tab. 1, we can be observe that there is an excessive amount of cache usage during adaptation, except for BN, which can slow down or even block some other tasks"}, {"title": "5.6. Impact of Submodules of DARDA", "content": "To investigate the contribution of different components toward performance gain, we replace different parts of DARDA using different alternative options. We report the related performance in Tab. 2. We consider (i) DARDA without Corruption Extractor, where corrupted data is directly projected into latent space; (ii) DARDA without context-aware BN & Adaptation, directly using the DNN constructed from current sub-network signature for prediction; (iii) DARDA with context-aware BN & without fine tuning, we update the normalization values of the BN layers using samples from the memory bank but do not update the tunable parameters; (iv) DARDA with context-aware BN and Entropy Minimization, where we update the tunable parameters by minimizing entropy of predictions.\nFrom Tab. 2 it can be observed that the corruption extractor is crucial for the performance as erroneous corruption projection would select the wrong sub-network. Creating corruption projections using only corrupted data leads to performance drop of 20.4% and 16.6% on CIFAR-10 and"}, {"title": "6. Conclusion", "content": "In this work, we have proposed Domain-Aware Real-TimeDynamic Neural Network Adaptation (DARDA). DARDA adapts the DNN to previously unseen corruptions in an unsupervised fashion by (i) estimating the latent representation of the ongoing corruption; (ii) selecting the sub-network whose associated corruption is the closest in the latent space to the ongoing corruption; and (iii) adapting DNN state, so that its representation matches the ongoing corruption. This way, DARDA is more resource-efficient and can swiftly adapt to new distributions without requiring a large variety of input data. Through experiments with two popular mobile edge devices \u2013 Raspberry Pi and NVIDIA Jetson Nano we show that DARDA reduces energy consumption and average cache memory footprint respectively by 1.74x and 2.64\u00d7 with respect to the state of the art, while increasing the performance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet."}, {"title": "S1. Different Domain Generalization Setup", "content": "The problem of adapting a DNN to tackle real life data corruption at the edge can be formulated by different kind of Domain Generalization (DG) settings based on the nature of data stream and learning paradigm. Fig. S1 summarizes the four main DG approaches in literature, namely Fine Tuning (FT), Unsupervised Domain Adaptation (UDA), Source-Free Domain Adaptation (SFDA) and TTA.\nFine Tuning (FT) adapts a DNN by making it match labeled test data [2, 12]. FT approaches includes Few-Shots Learning (FSL), among others [22]. The downside of FT is that it requires labeled test data and is performed offline, thus they are hardly applicable in a mobile edge computing context. Moreover, it does not take into account samples from the previous domain, so it incurs in catastrophic forgetting [6].\nUnsupervised Domain Adaptation (UDA). This approach addresses the issues of FT considering samples from the previous domain and thereby eliminating the need of labels from the new domain [27]. However, as FT, UDA assumes that we can simultaneously access (unlabeled) samples from the current domain and from the prior domain, which is not always the case. In stark opposition, our goal is achieve real-time adaptation of a DNN in dynamic and uncertain scenarios.\nSource-Free Domain Adaptation (SFDA). Conversely from UDA, in SFDA the DNN adaptation is performed using unlabeled data from the target domain only [10, 11]. While SFDA approaches take into account numerous losses"}, {"title": "S2. Distribution and Label Shift", "content": "Different corruptions lead mainly to a distribution shift in the input data, which is also widely known as a covariate shift. Distribution shift happens when the distribution of input data changes while the distribution of true labels remain unchanged. In a real-life adaptation of DNNs at inference time, we usually have a batch of samples to work with that have both distribution and label shift (due to correlation in labels in certain scenarios) at the same time. Fig. S2 illustrates this scenario with an example. This setting is challenging for most existing TTA algorithms, but is more practical."}, {"title": "S3. Hyperparameters and Implementation Details.", "content": "We implement DARDA with the PyTorch framework. For generating non IID real time data flow, we adopted Dirichlet distribution (with parameter 8) to create a non-IID data flow. Furthermore, to simulate a domain change due to corruption, we feed samples from different test corruption types sequentially one after another following Dirichlet distribution with control parameter d, when all samples from the current test corruption are exhausted. With lower values of the Dirichlet parameter 8, there is less diversity among online data batches, thus less correlation. The noise extractor is designed in a lightweight manner with three convolution layer with Leaky-ReLU non-linearity stacked sequentially. For the noise encoder, we use a sequential model consisting of two convolution units each consisting of a single convolution layer with ReLU non-linearity and a MaxPooling layer. The sequential model is followed by two dense layers. The sub-network encoder consists of two dense layers with ReLU non-linearity. For the reported results, we choose the output dimension of the sub-network encoder and the corruption encoder, i.e. the dimension of latent space to be 128 and the batch size is kept at 64, while the size of the memory bank is kept the same as batch size.\nFor CIFAR-100 corrupted dataset, we cannot insert samples from all classes in the memory bank. The parameter value \u03b4 = 0.1 is considered across all test scenarios unless otherwise specified. However, we have found empirically that presence of representative samples from the majority of the classes is sufficient. We use Adam optimizer with learning rate 1 \u00d7 103 to perform the adaptation. To generate the sub-network signature, we first train a ResNet-56 backbone on the uncorrupted training dataset (CIFAR-10 and CIFAR-100) and fine-tune the sub-networks for 20 epochs using data from 15 train corruption domain to create the 15 sub-networks and their related signatures. For the hyper parameters, we assume a fixed set of values throughout the experiments, which are r = 0.2, \u03bb\u03b5 = 10, thresh = 0.005 and momentum value m = 0.5. As we make one step upgrade of the current BN layer statistics by making sure that the samples of the memory banks are reliable, we chose a rather aggressive momentum value to weigh the normalization of current test samples highly."}, {"title": "S4. Power Measurement Setup", "content": "As both Raspberry Pi and Jetson Nano do not have a system integrated in them to measure power at a certain instance, we use the setup of Fig. S3 to calculate the energy consumption of different adaptation methods. The ina219 IC can provide accurate power consumption for a device at a certain time. We initially take some samples of power measurement to estimate the idle power usage of the de-"}, {"title": "S5. Performance of Corruption Extractor", "content": "As discussed previously, in a corrupted data sample the corruption related features are intertwined with label information which impedes extraction of contextual information from data.\nTo verify whether the corruption extractor is effective in extracting corruption information from data, we trained our corruption extractor and encoder using data available from 15 different corruptions and evaluate how it performs for"}, {"title": "S7. More Details of Corruption Encoder", "content": "The processes involved both in the training and inference phase of the proposed Corruption Encoder is illustrated through Algorithm 2."}, {"title": "S7.1. Theoretical Analysis of the Corruption Encoder", "content": "We offer insights into the information learned by the corruption encoding process through a theoretical analysis of the corruption encoder for additive noise.\n$g(y_{1}, x; \\phi)=\\arg \\min _{\\phi} E \\left[\\left\\|y_{1}-g_{\\phi}\\left(y_{1}\\right)-x\\right\\|^{2}\\right]$\n$g\\left(y_{1}, y_{2}; \\phi\\right)=\\arg \\min _{\\phi} E \\left[\\left\\|y_{1}-g_{\\phi}\\left(y_{1}\\right)-y_{2}\\right\\|^{2}\\right]$\n$g(Y1, Y2; \u03c6) = argmin E||Y1 \u2212 g\u03c6(Y1) \u2212 x \u2212 2||2\n$g(Y1, x; \u03c6) = argmin E[||g\u03c6(Y1)||2 \u2212 2YTg\u03c6(Y1) + 2xTg\u03c6(Y1)]\nargmin E[||g\u03c6(Y1)||2 \u2212 2YTg\u03c6(Y1) + 2xTg\u03c6(Y1) + 2eTg\u03c6(Y1)]"}]}