{"title": "Aligning Sentence Simplification with ESL Learner's Proficiency for Language Acquisition", "authors": ["Guanlin Li", "Yuki Arase", "No\u00ebl Crespi"], "abstract": "Text simplification is crucial for improving accessibility and comprehension for English as a Second Language (ESL) learners. This study goes a step further and aims to facilitate ESL learners' language acquisition by simplification. Specifically, we propose simplifying complex sentences to appropriate levels for learners while also increasing vocabulary coverage of the target level in the simplifications. We achieve this without a parallel corpus by conducting reinforcement learning on a large language model. Our method employs token-level and sentence-level rewards, and iteratively trains the model on its self-generated outputs to guide the model to search for simplification hypotheses that satisfy the target attributes. Experiment results on CEFR-SP and TurkCorpus datasets show that the proposed method can effectively increase the frequency and diversity of vocabulary of the target level by more than 20% compared to baseline models, while maintaining high simplification quality.", "sections": [{"title": "Introduction", "content": "Controlled text simplification considers audience-targeted attributes when generating simplified texts, so that the generated texts do not only meet the criteria of simplicity, but also preserve desired attributes for the targeted audiences. Recent studies on controlled text simplification aimed to help reading comprehension for language learners and employed school grade levels annotated in the training corpus as the simplification target (Scarton and Specia, 2018; Sheang and Saggion, 2021; Agrawal and Carpuat, 2023) or text features (number of words, character-level Levenshtein similarity etc.) between source and target sentences (Nishihara et al., 2019; Martin et al., 2020).\nDifferent from these studies, we aim to aid language learning and education for English as a Second Language (ESL) learners by simplifying sentences while preserving desirable attributes for language acquisition. We use the Common European Framework of Reference for Languages (CEFR), the world standard definition of language proficiency. Our method is motivated by two classic L2 learning theories: the input hypothesis (Krashen, 1981) and frequency effect (Ellis, 2002). The input hypothesis stated that in order for the language acquisition to happen, the textual input which is either too simple or too complex for learner comprehension will not be useful for acquisition. If a learner's current competence is i, then comprehensible input should contain both i and (i+1) content (Mitchell et al., 2019). Frequency theory holds that the frequency of the words and phrases in the input is a key determinant of acquisition, and words with higher frequency in the usage tend to be easier to acquire (Ellis and Ferreira-Junior, 2009). The key challenge here is the lack of a parallel corpus for training that should provide complex-simple sentence pairs labelled their levels. Parallel sentences of this kind are naturally scarce, and worse, annotation of difficulty levels, in particular, CEFR, is non-trivial and requires language education experts (Arase et al., 2022).\nTo achieve sentence simplification for aiding language learning without a parallel corpus, we propose reinforcement learning on a pre-trained large language model (LLM). Based on the aforementioned L2 learning theories, the proposed method simplifies the complex sentences to the one corresponding to the learner's proficiency level or one level higher (i and i + 1 levels) and increases the coverage (frequency and diversity) of the corresponding level's vocabulary in the generated simplifications. Specifically, we reformulate the controlled simplification task as a lookahead search problem: in the decoding step t, the model searches for the token that satisfies the target vocabulary constraint while also ensuring that future tokens"}, {"title": "Problem Definition", "content": "We aim to facilitate language learning by simplification targeted at ESL learners. In this study, we use CEFR levels as a representative measure for the learners' proficiency and model the target level based on the vocabulary\u00b2 (words, phrases, idioms) and sentence CEFR levels\u00b3.\nOur problem is thus defined as follows. We assume that learners know their own CEFR level i. Given a sentence above the learner's level i, we generate its simplified version that (a) contains as much vocabulary of the level i and i + 1 as possible, and (b) corresponds to the target (learner's) level i at the sentence level."}, {"title": "Constraint Formalization", "content": "Generating simplified texts subject to vocabulary constraints can be approached as a lexical-constrained text generation task (Zetsu et al., 2022). Traditionally, lexical constraints in text generation involve a short list of required words, which Lu et al. (2021) expressed as a Conjunctive Normal Form (CNF), such as $(D_1 \\lor D_2 \\lor \\cdots) \\land \\cdots \\land \\atop{C_1}{(D_{m-1} \\lor D_m)}$ in which $D_m$ stands for a single constraint, and all clauses must be satisfied, imposing hard constraints on the generation process.\nIn our setting, however, this formulation is no longer applicable because the vocabulary constraint is as large as the size of the vocabulary of a specific level. In addition, we aim to satisfy as many clauses as possible. Therefore, we formalize constraints as Disjunctive Normal Form (DNF), indicating words and phrases suitable for the target proficiency level: $D = (D_1) \\lor (D_2 \\land D_3 \\land \\cdots) \\lor \\cdots \\lor (D_m)$, where the form stands for the word list of the target"}, {"title": "Optimization Function", "content": "Based on the DNF constraints, our task imposes soft constraints that aim to include as many clauses as possible. Given the simplification hypotheses {seq1, seq2,..., seqn}, the goal is to maximize:\n$\\sum_{j=1}^{m} \\sum_{k=1}^{n} count(C_j, seq_k)$    (1)\nwhere count(Cj, seqk) indicates the number of clauses Cj satisfied by seqk. Consequently, the target during the generation process is to search for the next token that:\n\u2022 simplifies the original text;\n\u2022 is contained in \u2203C \u2208 D;\n\u2022 leads to future tokens that satisfy Ci;\n\u2022 leads to complete phrases or phrases with slots (discontinuity) that satisfy Ci."}, {"title": "Proposed Method", "content": "To search for a hypothesis that better satisfies predetermined constraints, some previous methods use rollout in decoding that generates partial future sequences (Chaffin et al., 2022; Lu et al., 2022). These methods become infeasible for large models due to the inefficiency of sampling in decoding time and handling the large vocabulary constraints in our task. To effectively and efficiently search for the tokens that satisfy our constraints, we instead consider sampling in the training time and formulate the lookahead search problem using RL search (Fickinger et al., 2021) (see Fig. 1)."}, {"title": "RL Search", "content": "Consider the text generation process as a Markov Decision Process (MDP), at each timestep t of the generation, the language model observes a state st, which is the generated partial sequence seqt\u22121, and takes an action at to choose the token from its vocabulary. When the EOS token is reached, a reward R for the generated sequence is calculated and used to update the model. In this setting, the language model is the policy function that searches for a token vi \u2208 V where V is the vocabulary, and we can use any policy gradient algorithm to guide the language model to search for the generations that"}, {"title": "Policy Model", "content": "The policy model generates a simplified sentence seq given a complex counterpart as a prompt pmt. The policy model is initialized from an instruction tuned language model, which unsupervisedly provides robust text simplifications (Kew et al., 2023).\nBy design, the rewards for the policy model across different proficiency levels are varied. For instance, given the same model response, a positive reward for C level could correspond to a negative reward for A level. Therefore, using the original language model as the backbone, we train separate copies of the policy model for A, B and C levels by adding and updating distinct LoRA parameters to the backbone parameters (Hu et al., 2022), while keeping the backbone frozen."}, {"title": "Reward Models", "content": "Inspired by the L2 learning theories, we design two types of rewards at lexical and sentence levels."}, {"title": "Lexical Constraint Reward", "content": "We use a simple heuristic to guide the search for generations that satisfy the lexical constraints:\n$H(seq) = \\sum_{C_j \\in D} \\sum r(count(C_j, seq))$,  (2)\nwhere C is a clause from D, r denotes the reward according to the number of satisfactions of C in seq, and H denotes the reward score for the generated sequence seq in the current decoding step. To calculate the match counts, we remove basic stop words from the sentence after lemmatization.\nAs a simple baseline, we define r as a constant value 1 for word and 1.5 for phrase to encourage the model to generate more phrases and idioms. However, we found that this simple baseline is easily hacked by the model after a few steps of training,"}, {"title": "Sentence Level Reward", "content": "To go beyond words and guide the simplification model's search for a sentence of the target level, we incorporate a sentence-level reward model by simulating human experts' judgment for the sentence's CEFR level. We use pairwise ranking loss to train the reward model, since the class distribution for the CEFR-SP data is imbalanced (Arase et al., 2022). The ranking loss has been shown to be able to encourage the model to only pay attention to the focused class (Henning et al., 2023), thus may mitigate the class imbalance problem.\nConsequently, we construct sentence pairs prioritizing the level we focus on generating: for a collection of sentences $S = {s_1, s_2, . . ., s_n}$, each sentence si is evaluated by human experts and annotated with a language level l. Given the level we want to generate, we select the sentences with the target level $S_{tgt} = {s_i \\in S | l_i = level_{tgt}}$, and randomly sample sentences from other levels to construct a negative set $S_{non-tgt} = {s_j \\in S | l_j \\neq level_{tgt}}$. Then, we construct sentence pairs $P = {(s_i, s_j) | s_i \\in S_{tgt}, s_j \\in S_{non-tgt}}$ by randomly selecting from Stgt and Snon-tgt.\nNotably, we do not require the pair to be parallel;"}, {"title": "Stabilized RL Training", "content": "The original instruct-tuned model is used as a frozen reference model, providing an entropy regularization for the updated policy model to ensure training stability during the search process. Specifically, the simplification seq' produced by the frozen backbone model f' is added as an entropy regularization to the overall reward:\n$R' = R - log(p(f(seq|pmt)/P(f'(seq'\\pmt)))$. (7)\nBy doing so, we may keep the LLM's strong paraphrasing ability while letting it acquire controllability in CEFR levels.\nThe policy model f, namely the simplification model is then updated to search for the generations that maximize the reward. In this study, we adopt Proximal Policy Optimization (Schulman et al., 2017) to update the policy model, which achieves stable training and faster convergence."}, {"title": "Experiment Settings", "content": "We aim to evaluate the effectiveness of the proposed model in generating high-quality simplifications that align with the target vocabulary and sentence-based CEFR level. This section provides details of the experiment settings."}, {"title": "Resource and Implemetation", "content": "Sentence CEFR Level To train the sentence-level reward model, we used CEFR-SP (Arase et al., 2022), which provides labels of six CEFR levels"}, {"title": "Automatic Evaluation Results", "content": "Target Attributes Tables 2 and 3 show the evaluation results for the target vocabulary coverage. These results demonstrate that compared to the baseline models, the proposed model significantly increases the frequency of target vocabulary in simplified sentences while also improving vocabulary diversity. Notably, the proposed method successfully increases the frequency and diversity of A and C-level vocabulary, which should be harder than B-level due to the scarcity of level A and C samples (Arase et al., 2022).\nSimplification Quality Tables 5 and 6 show the evaluation results for the simplification quality. Overall, these results indicate that our models can produce high-quality simplifications, greatly out-performing the baseline models. Remind that our model does not have reward to encourage the model to follow the adequacy requirement. We attribute these improvements to the benefits of using entropy regularization imposed by the reference model that allows the preservation of the high paraphrasing capability of LLMs. Table 4 shows a randomly"}, {"title": "Human Evaluation Results", "content": "We perform a human evaluation to assess the simplification quality from human perspectives. We recruited three graduate-level students majoring in linguistics to perform the evaluation. The evaluators were first trained with the background knowledge and then given a guideline to evaluate the following aspects of the samples: fluency, simplicity, adequacy, and CEFR sentence level.\nWe asked annotators to make binary judgements for fluency, simplicity, and adequacy. For sentence level, because CEFR-level judgements require expertise in language education, we simplified the task to collect reliable decisions. We asked the evaluators to judge if a simplified sentence matches the desired sentence level (denoted as \u201cLevel\u201d). We showed a reference with its CEFR level and requested the evaluators to judge if the model output matches the reference's simplicity. In addition, we asked them if a simplification output is preferable in terms of its CEFR level compared to the one generated by a model targeting a different level (denoted as \"Prefer\"). For example, an evaluator judges if an output of the A-level model is preferable to that of the C-level compared to the"}, {"title": "Ablation Study", "content": "In this section, we show how each part of the proposed rewards contributes to the final performance. We compare the following models: vanilla phi3 model, reward using only target vocabulary counts,"}, {"title": "Conclusion", "content": "In this paper, we target ESL learners as audiences for text simplification aiming to facilitate the learning process of the foreign language. Referring to the input hypothesis and frequency effect theory in L2 learning, we propose a reinforcement learning method on LLM to control the simplification model to generate outputs that satisfy the vocabulary and sentence level constraints. Experiment results show that the proposed method can increase the target vocabulary coverage in the generated simplifications, and human evaluation results confirmed that the simplified texts generally preserve the targeted CEFR levels.\nIn practice, different individuals have varied lev-"}, {"title": "Limitations", "content": "This work assumes the target vocabulary for the learner is accessible, which in reality may not be the case as the target vocabulary varies with learner individuals and has to be first estimated. Although it is out of the scope of this paper, this direction constitutes our future work. Besides, currently, we do not control the frequency to a specific number, such as 95% i level and 5% i + 1 level, which is an important aspect to consider according to the L2 learning theory.\nThe control for target vocabulary and sentence is implemented individually for different levels rather than using one model altogether, causing heavier computational loads. In future, we seek to improve the design of reward model to integrate rewards for different proficiency levels into one model, and explore for a finer control over the frequency of the generated vocabulary."}]}