{"title": "Hyperband-based Bayesian Optimization for Black-box Prompt Selection", "authors": ["Lennart Schneider", "Martin Wistuba", "Aaron Klein", "Jacek Golebiowski", "Giovanni Zappella", "Felice Antonio Merra"], "abstract": "Optimal prompt selection is crucial for maximizing large language model (LLM) performance on downstream tasks. As the most powerful models are proprietary and can only be invoked via an API, users often manually refine prompts in a black-box setting by adjusting instructions and few-shot examples until they achieve good performance as measured on a validation set. Recent methods addressing static black-box prompt selection face significant limitations: They often fail to leverage the inherent structure of prompts, treating instructions and few-shot exemplars as a single block of text. Moreover, they often lack query-efficiency by evaluating prompts on all validation instances, or risk sub-optimal selection of a prompt by using random subsets of validation instances. We introduce HbBoPs, a novel Hyperband-based Bayesian optimization method for black-box prompt selection addressing these key limitations. Our approach combines a structural-aware deep kernel Gaussian Process to model prompt performance with Hyperband as a multi-fidelity scheduler to select the number of validation instances for prompt evaluations. The structural-aware modeling approach utilizes separate embeddings for instructions and few-shot exemplars, enhancing the surrogate model's ability to capture prompt performance and predict which prompt to evaluate next in a sample-efficient manner. Together with Hyperband as a multi-fidelity scheduler we further enable query-efficiency by adaptively allocating resources across different fidelity levels, keeping the total number of validation instances prompts are evaluated on low. Extensive evaluation across ten benchmarks and three LLMs demonstrate that HbBoPs outperforms state-of-the-art methods.", "sections": [{"title": "1. Introduction", "content": "In recent years, pre-trained auto-regressive large language models (LLMs) have demonstrated remarkable capabilities in addressing a wide range of machine learning tasks involving natural language (Brown et al., 2020; Liu et al., 2023), such as Q&A (Joshi et al., 2017; Clark et al., 2018), text summarization (Koupaee & Wang, 2018), text generation (Hendrycks et al., 2020), and mathematical problem solving (Hendrycks et al., 2020; Cobbe et al., 2021). The performance on these tasks relies heavily on prompt engineering, where the input is formatted within a carefully designed prompt that may include instructions, few-shot examples, and additional information, because LLMs are highly sensitive to their input (Zhou et al., 2022; Honovich et al., 2022; Lu et al., 2021; Wu et al., 2024). As such, selecting the optimal instruction and the examples to include in a few-shot exemplar becomes an imperative task (Liu et al., 2023; Ye et al., 2023).\nStatic black-box prompt optimization and selection have recently gained much attention (Sun et al., 2022b; Chen et al., 2023; Lin et al., 2023; Wu et al., 2024; Shi et al., 2024). The goal is to construct or identify a single prompt for a black-box LLM that performs well across all instances of a downstream task. This process involves evaluating different prompts on a validation set and using derivative-free techniques to guide optimization or selection. The static black-box setting allows for offline optimization, with the resulting prompt being used for the downstream task.\nWhile much research has focused on automatically generating new prompts (Sun et al., 2022b; Xu et al., 2022; Zhou et al., 2022; Chen et al., 2023; Fernando et al., 2023; Lin et al., 2023), there is growing interest in efficiently selecting prompts from a predefined candidate pool (Shi et al., 2024). This is partly because many optimization techniques involve generating a large candidate pool before identifying the best prompt (Xu et al., 2022; Zhou et al., 2022; Fernando et al., 2023; Prasad et al., 2022). Methods that operate in this selection setting are given by MIPROv2 (Opsahl-Ong et al., 2024), EASE (Wu et al., 2024), TRIPLE-SH and TRIPLE-GSE (Shi et al., 2024).\nFor these methods we identify the following limitations: (1) With the exception of MIPROv2 they are not explicitly"}, {"title": "2. Problem Statement", "content": "Let $I = \\{i_1, ..., i_i\\}$ denote a finite set of instructions and $E = \\{e_1, ..., e_m\\}$ a finite set of few-shot exemplars. Note that by exemplar we refer to an ordered tuple of a given number of input-output examples of a task. Let $P = I \\times E$ be the set of prompts which are generated by combining each $i \\in I$ with each $e \\in E$.\nInstructions can be generated either manually by experts or automatically by LLM-based methods, e.g., Automatic Prompt Engineering (APE) by (Zhou et al., 2022). Few-shot exemplars can be generated by selecting different input-output instances from the training set.\nA prompt $p\\in P$ is instantiated by combining it with a given input $x \\in X$ for which the LLM, $h : (P \\times X) \\rightarrow Y, h([p, x]) \\rightarrow \\hat{y}$, produces an output $\\hat{y} \\in Y$. We will use $h_p$ to denote $h([p, x])$ as a shorthand.\nWe make no assumptions regarding the nature of the LLM and treat it as a black-box and the LLM returns output given input without any additional information, i.e., no access to model parameters, gradients, or token probabilities.\nBased on a validation set $\\{(x_i, y_i)\\}_{i=1}^{n_{\\text{valid}}}$, evaluating a prompt is performed by comparing the ground truth output $y_i$ to the output $\\hat{y}_i = h_p(x_i)$ generated by the LLM based on a point-wise loss function $l : Y \\times Y \\rightarrow \\mathbb{R}, (y, \\hat{y}) \\rightarrow l(y, \\hat{y})$. This loss function quantifies how close the output generated by the LLM is to the ground truth. For example, a loss function based on the widely used exact match (Chang et al., 2024) scoring function is given by:\n$$l(y, \\hat{y}) = \\begin{cases}\n0 & \\text{if } y = \\hat{y} \\\\\n1 & \\text{otherwise}.\n\\end{cases}$$\nOur task is to identify a single prompt $p \\in P$ which performs optimal with respect to the loss in expectation:\n$$ \\arg \\min_{p \\in P} \\mathbb{E}_{(x,y) \\sim P_{xy}} [l(y, h_p(x))]. $$\nHere the expectation is taken over all input-output instances $(x, y)$. In practice Equation (2) is approximated based on the validation instances available:\n$$ f(p) := \\frac{1}{N_{\\text{valid}}} \\sum_{i=1}^{n_{\\text{valid}}} l(y_i, h_p(x_i)) $$\nWe refer to this setting as the static setting as we are searching for a single optimal prompt on the target downstream task. Note that due to the non-deterministic nature of LLMs, $f$ itself can in general only be observed with noise. We will denote by\n$$ v = f(p) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2) $$\nthe observed validation error of the LLM configured to use prompt $p$. $f$ is a black-box function as the LLM is a black-box and no analytic description or gradient information is available.\nGiven the finite search space $P$, our goal is to identify the best prompt using as few LLM calls as possible."}, {"title": "3. Method", "content": "We want to learn a surrogate model of the black-box function $f$ described in Equation (3) that models the LLM validation error of each prompt on the downstream task. This surrogate is used to predict the validation error of the not-evaluated prompts during the optimization process (the selection of the next candidate prompt). As discussed in Section 2, prompts are not merely blocks of text but possess a specific structure composed of various building blocks. While it may seem intuitive to embed prompts and train a Gaussian Process (GP) on high-dimensional embeddings, we argue that this simplistic approach fails to adequately capture the intricate structure of prompts. To address this limitation, we propose a structure-aware deep kernel (Section 3.2)."}, {"title": "3.1. Gaussian Process as a Surrogate Model", "content": "To learn a surrogate model, we collect, for a given optimization step $t$, design data $D_t := \\{(p_j, v_j)\\}_{j=1}^t$, where each tuple is given by the prompt $p_j \\in P$ and its validation error $v_j$ described in Equation (4) at the $j$-th previous optimization step. This design data is the collection of prompts and their performance during the sequential optimization process. To learn a model that maps prompts to their validation errors, we embed each prompt into a $d$-dimensional numeric space making use of pre-trained language encoders. Let $\\text{enc} : P \\rightarrow \\mathbb{R}^d, p \\rightarrow z$ be the encoding function, then we augment the design data $(D_t = \\{(p_j, z_j, v_j)\\}_{j=1}^t)$ with $z_j$, the embedding of the prompt $p_j$.\nWe want to use a GP as our surrogate model since it allows for flexible probabilistic modeling of black-box functions by returning a point estimate and well-calibrated uncertainty estimations in the form of a Gaussian posterior predictive distribution (Williams & Rasmussen, 2006). In the following, we assume a GP prior over $f$ in the $d$-dimensional space of the embedded prompts, $f(z) \\sim GP(m, k); f \\sim \\mathcal{N}(m(Z), k(Z, Z|\\theta))$, where $m$ is the prior mean function usually set to 0 (zero mean), $k$ is the covariance function depending on kernel parameters $\\theta$, and $Z$ is a matrix of prompt embeddings.\nThen, given the design data $D_t$, and new prompts $p_*$ with their embeddings $Z_*$, the function $f$ is modelled as a random variable that is jointly Gaussian distributed with all previously observed validation errors $v = (v_1, ..., v_t)$. In short\n$$\\begin{bmatrix}\nv \\ f_*\n\\end{bmatrix} \\sim \\mathcal{N} \\left( m\\left(\\begin{bmatrix}\nZ \\\\\nZ_*\n\\end{bmatrix}\\right), \\begin{bmatrix}\nK_t & K_+ \\\\\nK_+^{\\top} & K_{**}\n\\end{bmatrix} \\right),$$\nwhere $K_t = k(Z, Z|\\theta) + \\sigma^2 I_t, K_+ = k(Z, Z_*|\\theta)$, and $K_{**} = k(Z_*, Z_*|\\theta)$ are the kernel matrices.\nThe posterior predictive distribution under the (zero mean) GP is obtained as\n$$ \\begin{aligned}\n\\mathbb{E}[f_* | Z, v, Z_*] &= K_+^{\\top} (K_t)^{-1} v, \\\\\n\\text{cov} [f_* | Z, Z_*] &= K_{**} - K_+^{\\top} (K_t)^{-1} K_+, \\\\\n\\end{aligned} $$ \nwhere common choices for kernel functions $k$ are given by the squared exponential kernel or variations of the Mat\u00e9rn kernel (see, e.g., Williams & Rasmussen, 2006, Chapter 4).\nAt this point, we could proceed and train a \"vanilla\" GP as outlined above on the $d$-dimensional space of embedded prompts. However, as stated in many previous works on BO (Kandasamy et al., 2015; Wang et al., 2016; Gardner et al., 2017; Eriksson et al., 2019; Eriksson & Jankowiak, 2021), GPs struggle with high-dimensional input as the one that we have in our design data $D_t$, e.g., the dimension of the BERT [CLS] token embedding is 768. Moreover, dimensionality reduction techniques such as principle component"}, {"title": "3.2. Structural-aware Deep Kernel", "content": "To learn a lower-dimensional representation of the embedded prompts aligned to the downstream task, we propose to use a deep kernel (Wilson et al., 2016). We design a feature extractor, $\\Phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}^p, p << d$ to learn a flexible kernel transformation function: $k(\\Phi(z, w), \\Phi(z', w)|\\theta)$, where $\\theta$ and $w$ are the parameters of the kernel and the extractor.\nGiven that prompts are composed of two distinct building blocks, we hypothesize that embedding these components separately would enhance the deep kernel GP (DK-GP)'s ability to exploit both structural and semantic differences between them. For instance, instructions tend to be relatively similar across prompts, while few-shot exemplars can vary significantly due to their diverse input-output pairs and varying orders.\nTo address this, we propose learning a structure-aware latent representation of prompts. This approach involves embedding the instructions $i \\in I$ and few-shot exemplars $e \\in E$ of prompts separately. We proceed to use two distinct feed forward neural networks consisting of two layers with ReLU activation functions for the embedded instructions and exemplars: $\\Phi_{\\text{enc}}(i) : \\text{Lin}(d, 64) \\rightarrow \\text{ReLU}() \\rightarrow \\text{Lin}(64, 32) \\rightarrow \\text{ReLU}()$. After having processed the instructions and exemplars separately, we concatenate the outputs and proceed with another feed forward neural network to learn a joint latent representation: $\\Phi (\\text{enc}(i), \\text{enc}(e)) : \\text{Lin}(32 \\cdot 2, 32) \\rightarrow \\text{ReLU}() \\rightarrow \\text{Lin}(32, 10)$.\nDuring training the GP we obtain both the optimal kernel parameter values, as well as the parameters of the neural network feature extractors by optimizing the log marginal likelihood criterion:\n$$ \\theta, w = \\arg \\max_{\\theta, w} \\frac{1}{2} \\left(-v^{\\top} K(\\theta, w)^{-1}v - \\log |K(\\theta, w)| \\right) $$\nIn Section 3.4 we illustrate how to make use of our structural-aware DK-GP during optimization. First, we describe how we achieve query-efficiency."}, {"title": "3.3. Hyperband for Multi-Fidelity Scheduling", "content": "To boost query-efficiency identified as one of the limitation of the existing works (EASE and MIPROv2), we want to terminate the evaluation of poor performing prompts early saving time (and cost) during the evaluation process. Similarly to Shi et al. (2024) we model the number of validation instances prompts are evaluated on as a fidelity parameter. Full-fidelity methods evaluate prompts on all validation instances, while multi-fidelity methods adaptively schedule evaluations on varying numbers of instances. TRIPLE implements Successive Halving (SH; Karnin et al. 2013) to schedule the number of validation instances. In contrast, we use the Hyperband (HB; Li et al. 2018) algorithm because it will not evaluate all prompts and hedges against a poorly configured SH as we explain below.\nGiven a total budget of B LLM calls to evaluate prompts, SH allocates a budget of $b = B/(|P| \\log_2 (|P|)$ to each prompt (see further details in Appendix C). After having evaluated the prompts on $b$ validation instances, the lower half of bad performing prompts is discarded and the process repeats, doubling the number of calls for the remaining prompts in the next stage, until a single prompt remains.\nThis strategy is affected by the \"budget vs. number of configurations\" dilemma (Li et al., 2018), since, at the beginning of the algorithm it is not clear if it should evaluate many (by default all) prompts on few instances (good exploration but noisy performance estimates) or few prompts on many instances (less exploration but accurate performance estimates). However, when many prompts need to be evaluated within a limited total budget, SH's initial budget will be low which risks discarding a prompt based on noisy performance estimation. In contrast, we propose to use HB since it hedges against a poor choice of the number of starting prompts and budget by repeatedly running SH in different brackets using different numbers of starting prompts and starting budgets. This allows HB to be robust under various scenarios without knowing the optimal resource allocation beforehand making it ideal for prompt selection.\nTo adapt HB to prompt selection we make the following design choices: (1) We extend previous evaluations when advancing stages within a bracket, ensuring validation instances of higher stages subsume those of lower stages and (2) return the prompt with the lowest validation error among those evaluated on the entire validation set. (3) In addition, we combine HB with our structural-aware DK-GP by employing a sequential proposal mechanism for candidate prompts in each bracket, which we outline next."}, {"title": "3.4. HbBoPs", "content": "HbBoPs combines HB with our structural-aware DK-GP. While the vanilla HB algorithm for prompt selection samples prompts uniformly at random, HbBoPs replaces the random proposal mechanism of HB with a sequential BO proposal (highlighted in gray) similar to the approach proposed by Falkner et al. (2018) for Hyperparameter Optimization. During the execution of HB, HbBoPs trains the GP on a subset of the design data $D_{t|b}$ for a given fidelity-level $b$. We use the highest fidelity $b$ for which \"enough\" observations have been collected. This design choice stems from the observation that validation errors are estimated more accurately with more instances (see Appendices C and B).\nAfter having trained the GP on $D_{t|b}$, we obtain the next candidate prompt $(p_{t+1})$ for evaluation based on the Expected Improvement (Jones et al., 1998) as acquisition function:\n$$ \\alpha_{EI}(p|D_{t|b}) := \\mathbb{E}[\\max{\\{v_{\\min,b} - f(z_p), 0\\}}] $$\n$$ p_{t+1} = \\arg \\max_{p \\in P} \\alpha_{EI} (p|D_{t|b}), $$\nwhere, given the incumbent (the best performing prompt on the highest fidelity evaluated yet) and its validation error $v_{\\min,b}$ at the given fidelity level $b$, the Expected Improvement tells us how much we can expect the given candidate prompt to improve over this incumbent given the posterior predictive distribution as described in Equation (5) of the GP. The full algorithm is described in Algorithm 1."}, {"title": "4. Experimental Setup", "content": "4.1. Benchmark Tasks\nTo benchmark HbBoPs we select ten tasks commonly used for LLM evaluation (Zhou et al., 2022; Lin et al., 2023; Chen et al., 2023; Wu et al., 2024; Shi et al., 2024). AI2's Reasoning Challenge (ARC) (Clark et al., 2018): multiple choice question answering problems; Grade School Math 8K (Cobbe et al., 2021): math problems that take between two and eight steps to solve; 8 Tasks from the BBII subset of the BIG-bench and instruction induction benchmarks (Srivastava et al., 2022; Honovich et al., 2022) used in Zhou et al. (2022); Wu et al. (2024); Shi et al. (2024): antonyms, larger animal, negation, second word letter, sentiment, object counting, orthography starts with, and word unscrambling. Task statistics are reported in Table 5 in Appendix D.\n4.2. Methods\nWe compare HbBoPs against full-fidelity and multi-fidelity methods described in Table 1. Additional details of the methods are reported in Section 6.2 and Appendix D. All methods that rely on embeddings of prompts use BERT's [CLS] token embedding. All full-fidelity BO methods (vanilla BO, HDBO, BOPCA) use an ARD Mat\u00e9rn kernel and Expected Improvement as acquisition function and normalize inputs to the unit cube and standardize outputs. HDBO is a simply but well performing high-dimensional BO algorithm and uses adjusted priors on kernel and likelihood parameters as described in Hvarfner et al. (2024). BOPCA uses a ten component PCA inspired by Zhang et al. (2024). We run MIPROv2, EASE'S NUCB ($\\upsilon = 0.1)$, TRIPLE-SH and TRIPLE-GSE as implemented in their publicly available code bases. All full-fidelity methods use the same initial design of ten prompts sampled uniformly at random. HbBoPs uses an ARD Mat\u00e9rn kernel, normalizes inputs and standardizes outputs. We train the DK-GP on the highest fidelity level for which at least 4 observations are available. To optimize the log marginal likelihood described in Equation (6), we use AdamW (Loshchilov, 2017) with learning rate = 0.01, maximum number of epochs = 3,000 epochs,"}, {"title": "4.3. Experimental Protocol", "content": "For each task, we generate a search space P of candidate prompts by combining five task-specific instructions with 50 few-shot exemplars. Instructions are created using APE's forward mode (Zhou et al., 2022), where Claude 3 Sonnet (Anthropic, 2024) generates instructions based on ten input-output samples from each task's training set. For exemplars, we sample 25 sets of five input-output instances from the training set of each task, then permute each set twice to create 50 exemplar tuples, allowing assessment of example ordering effects. The final 250 prompts are formed by the Cartesian product of instructions and exemplars.\nFor LLMs, we use Claude 3 Haiku (Anthropic, 2024), LLAMA3 8B Instruct (Dubey et al., 2024), and Mistral 7B Instruct (Jiang et al., 2023).\nFor each benchmark scenario (a given task and LLM), we run each method for a total budget of 25 full-fidelity evaluations (i.e., being allowed as many LLM calls as 25 prompts evaluated on all validation instances would require for a given task) to mimic a budget constrained scenario. We repeat each method run 30 times. We evaluate prompts using the loss function described in Equation (1) that is based on the exact match scoring function."}, {"title": "5. Results", "content": "In this section, we discuss the experimental results. We report the validation and test errors computed on the best prompt identified by each method given a specific budget. For instance, given GSM8k with a validation set of 1319 instances, a budget of 0.25 means that we report the results of the methods after performing int(0.25*25*1319) = 8244 LLM calls. Therefore, full-fidelity methods always start after having executed a fraction of 1/25 total LLM calls."}, {"title": "5.1. Analysis of overall performance", "content": "We start by analyzing the overall performance of the methods averaged across all the benchmark tasks and LLMs. To allow for averaging results over different benchmarks and models, we normalize validation and test errors for each benchmark scenario by the performance of the worst and best prompt. Analysis of the figures reveals that HbBoPs outperforms all full-fidelity and multi-fidelity methods, particularly in terms of anytime performance on both the validation and test sets.\nBeginning with an analysis of test performance at full budget (i.e., a fraction of LLM calls equal to 1.0), we can see that our HbBoPs on average outperforms all full-fidelity and multi-fidelity approaches with an average normalized test error of 0.150. In detail, we observe that all full-fidelity methods surpass the RS baseline (0.214) with the following errors: Vanilla BO (0.211), MIPROv2 (0.198), EASE (0.195), BOPCA (0.192), and HDBO (0.185). However, they all have higher error values than HbBoPs (0.1500). Additionally, HbBoPs also outperforms all the multi-fidelity methods. Although both TRIPLE-GSE (0.158) and TRIPLE-SH (0.159) exhibit superior performance compared to their best-in-class full-fidelity counterpart, i.e., HDBO (0.185), they on average have identified prompts that yield error values higher than the ones obtained for HbBoPs's prompts.\nLooking at the anytime performance with a more limited budget, e.g., a fraction of 0.25 LLM calls, we can confirm HbBoPs's improvements over the baselines. Indeed, HbBoPs on average outperforms HDBO, the best full-fidelity method, by approximately 35% and TRIPLE-SH, the best multi-fidelity method, by 24%."}, {"title": "5.2. Analysis of the performance for each LLMs", "content": "As demonstrated in Section 5.1, TRIPLE-SH emerges as the strongest competitor. To assess whether HbBoPs's improvements are consistent across different LLMs, we present in Table 2 the median relative improvement over the ten"}, {"title": "5.3. Ablation study", "content": "To better understand the contributions of the individual components in HbBoPs, we conduct a comprehensive ablation study. Figure 2 reports the average anytime normalized validation and test errors of the best prompt found by systematically removing specific components of our HbBoPs such that we can quantify their importance.\nOur analysis focuses on four key aspects: the use of a GP with a deep kernel, the incorporation of a structural-aware deep kernel, the integration of Hyperband for multi-fidelity scheduling, and the final proposed method."}, {"title": "5.4. Analysis of varying the encoder", "content": "As HbBoPs relies on embeddings of prompts, we conduct a sensitivity analysis to evaluate the impact of different encoder models on the performance of HbBoPs. While our primary results were obtained using BERT's (Devlin et al., 2019) [CLS] token embedding, we extend our analysis to include two more encoder models that are MPNet (Song et al., 2020) and DistillRoBERTa (Liu et al., 2019). For each encoder model, we rerun all benchmarks with HbBoPs.\nResults demonstrate that the average normalized validation and test performance of HbBoPs remains within a similar range across all encoder models, which suggests that HbBoPs is robust to the choice of encoder. This robustness can be expected, as none of the encoder models were specifically trained or fine-tuned for our task of predicting prompt performance. The effectiveness of HbBoPs stems from its ability to learn a mapping from prompts to performance through the DK-GP, provided that the raw embedding features can semantically differentiate between prompts and carry relevant information."}, {"title": "6. Related Work", "content": "6.1. Automatizing Prompt Engineering\nAs the construction and evaluation of a large number of prompts on the validation set can be a huge bottleneck, recent work has been concerned with the general topic of automatizing prompt engineering."}, {"title": "6.2. Static Black-box Prompt Selection", "content": "MIPROv2is DSPy's (Khattab et al., 2023) state-of-the-art teleprompter for joint instruction and few-shot exemplar selection. MIPROv2 searches over a finite set of candidate prompts by combining instructions with few-shot exemplars (which DSPy first constructs automatically). While MIPROv2 is a variant of BO, it learns a surrogate Tree-Structured Parzen Estimator (TPE; Bergstra et al. 2011) based on the categorical indices of instructions and exemplars (ID and IDe) that compose a prompt. A downside is that learning a surrogate model based on indices does not use any semantic information of the prompts which may result in a sub-optimal mapping from prompts to their performance. Moreover, MIPROv2 does not directly address query efficiency. Indeed, while DSPy can be configured to use a smaller random subset of the validation data to evaluate prompts, this risks sub-optimal selection due to noisy performance estimates (see also Appendix B.\nEASE proposed by Wu et al. (2024) mainly focuses on few-shot exemplar selection. It uses NeuralUCB (NUCB; Zhou et al. 2020) with prompt embeddings as features, allowing for sequential evaluation of promising prompts based on the UCB criteria. EASE's main contribution, however, is to make the combinatorial problem of selecting examples to build the few-shot exemplar from a larger training set computationally feasible. It prunes the candidate space using an optimal transport inspired heuristic before applying UCB. However, EASE is affected by query inefficiency since it evaluates prompts on all validation instances (or a random subset again risking sub-optimal selection).\nTRIPLE is a class of algorithms for black-box prompt selection recently proposed by Shi et al. (2024) to address query-efficiency. TRIPLE solves the problem of black-box prompt selection via a multi-armed bandit approach. In details, the authors rely on existing bandit algorithms such as Successive Halving (SH; Karnin et al. 2013 or Generalized Successive Elimination (GSE; Azizi et al. 2021). These algorithms accelerate prompt evaluation by discarding poorly performing prompts early, thereby avoiding the evaluation of all prompts on all validation instances during the selection process. TRIPLE-SH and TRIPLE-GSE have the following limitations: TRIPLE-SH is sensitive to the initial evaluation budget, potentially discarding promising prompts based on noisy performance estimates. This is especially problematic when evaluating many prompts with a limited budget which often is the case when jointly searching over instructions and few-shot exemplars. TRIPLE-GSE attempts to mitigate this by modeling the expected prompt performance based on embeddings projected to a lower di-mensional space via a random projection and point-wise performance observations of the prompts. However, GSE has not been analyzed for structured bandit problems beyond the generalized linear setting (Azizi et al., 2021)."}, {"title": "7. Conclusion", "content": "Existing methods applicable to static joint instruction and few-shot exemplar selection for black-box LLMs suffer from either not being sample-efficient, i.e., they evaluate all prompts on at least some validation instances, or not being query-efficient, i.e., they use all validation instances during the evaluation of each prompt. In this work we have proposed HbBoPs, a both sample- and query-efficient method tailored to joint instruction and few-shot exemplar selection. HbBoPs relies on a structural-aware deep kernel Gaussian Process that models the downstream performance of prompts which is used to propose promising, yet not evaluated prompts during the selection process. In addition, HbBoPs employs a Hyperband scheduler that governs the number of validation instances prompts are evaluated on. Our extensive experiments have demonstrated that HbBoPs improves upon existing baselines and state-of-the-art competitors in the limited budget regime while showing strong anytime performance. Future work might expand HbBoPs to a multi-objective setting. As for example the inclusion of many examples in a prompt induces longer response latency, the trade-off between the number but also concrete selection of examples for a prompt give rise to a multi-objective optimization problem of performance and response latency. Ultimately, we believe that our HbBoPs presents a significant step forward in making prompt selection for black-box LLMs more accessible and efficient, accelerating the usage of these models across a broader range of applications."}, {"title": "A. Illustrations of the structural-aware Deep Kernel GP", "content": "As mentioned in Section 3.1, unsupervised dimensionality reduction techniques such as PCA or random projections will not result in a lower latent dimension that is aligned with the downstream performance of prompts. To illustrate this, we perform the following experiment. We collect the validation performance of 250 prompts on the GSM8K task using LLAMA3 8B Instruct as LLM. We embed each prompt using the [CLS] token embedding of BERT (768d). We then split the prompts $p_1, ..., p_{250}$ and their corresponding validation errors $v_1, ..., v_{250}$ in a train (80%) and test set (20%). Using the train set we perform a PCA retaining ten principle components as features. Moreover, we train our structural-aware deep kernel GP introduced in Section 3.2 on the training set and extract the 10d latent features from the output of the feature extractor ($\\Phi_{\\text{enc}}(i), \\text{enc}(e)$). We visualize the raw 768d embedding features of prompts, the 10d PCA features and the 10d deep kernel features for the training set using a two component t-SNE (Van der Maaten & Hinton, 2008). The latent representation the feature extractor of the deep kernel has obtained during training does generalize and it has effectively learned its own low-dimensional embedding that is in relevance to the downstream task."}, {"title": "B. On the Generalization from Validation to Test", "content": "When performing black-box prompt selection, we evaluate prompts on a validation set and iteratively improve over the current best prompt by trying to identify a better one. While progress on the validation set is expected (i.e., if we perform full-fidelity evaluations using the same validation instances for each prompt, the validation error of the incumbent will be monotonically decreasing as optimization progresses), it must not necessarily be the case that we also improve performance on a separate test set of instances, i.e., the prompt identified as being validation optimal might not be optimal on the test set.\nIn this section we provide some additional results regarding generalization from validation to test. Recall that many methods for black-box prompt selection (e.g., EASE and MIPROv2) are by design not query-efficient but evaluate all prompts on all validation instances or a random subset We will now empirically demonstrate that using small random subsets during optimization is not a sensible choice, because this will result in increased variance of the estimate of the validation error which prevents us from making correct decisions on the validation set and can result in generalization issues when moving from the validation set to the test set.\nTo illustrate this, we perform the following experiment. We collect the validation and test error (according to the splits described in Table 5) of 250 prompts on the GSM8K task using LLAMA3 8B Instruct as the LLM. We now vary the number of validation instances used to evaluated the performance of a prompt via bootstrapping, using $k$ = 10, 50, 100, 500 instead of the original $n_{\\text{valid}}$ = 1319 validation instances. Note that for the test performance we always use the full test set.\\"}]}