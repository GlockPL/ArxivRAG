{"title": "Foundation Models for Music: A Survey", "authors": ["Yinghao Ma", "Anders \u00d8land", "Anton Ragni", "Bleiz MacSen Del Sette", "Charalampos Saitis", "Chris Donahue", "Chenghua Lin", "Christos Plachouras", "Emmanouil Benetos", "Elio Quinton", "Elona Shatri", "Fabio Morreale", "Ge Zhang", "Gy\u00f6rgy Fazekas", "Gus Xia", "Huan Zhang", "Ilaria Manco", "Jiawen Huang", "Julien Guinot", "Liwei Lin", "Luca Marinelli", "Max W. Y. Lam", "Megha Sharma", "Qiuqiang Kong", "Roger B. Dannenberg", "Ruibin Yuan", "Shangda Wu", "Shih-Lun Wu", "Shuqi Dai", "Shun Lei", "Shiyin Kang", "Simon Dixon", "Wenhu Chen", "Wehhao Huang", "Xingjian Du", "Xingwei Qu", "Xu Tan", "Yizhi Li", "Zeyue Tian", "Zhiyong Wu", "Zhizheng Wu", "Ziyang Ma", "Ziyu Wang"], "abstract": "In recent years, foundation models (FMs) such as large language models (LLMs) and latent diffusion models (LDMs) have profoundly impacted diverse sectors, including music. This comprehensive review examines state-of-the-art (SOTA) pre-trained models and foundation models in music, spanning from representation learning, generative learning and multimodal learning. We first contextualise the significance of music in various industries and trace the evolution of AI in music. By delineating the modalities targeted by foundation models, we discover many of the music representations are underexplored in FM development. Then, emphasis is placed on the lack of versatility of previous methods on diverse music applications, along with the potential of FMs in music understanding, generation and medical application. By comprehensively exploring the details of the model pre-training paradigm, architectural choices, tokenisation, finetuning methodologies and controllability, we emphasise the important topics that should have been well explored, like instruction tuning and in-context learning, scaling law and emergent ability, as well as long-sequence modelling etc. A dedicated section presents insights into music agents, accompanied by a thorough analysis of datasets and evaluations essential for pre-training and downstream tasks. Finally, by underscoring the vital importance of ethical considerations, we advocate that following research on FM for music should focus more on such issues as interpretability, transparency, human responsibility, and copyright issues. The paper offers insights into future challenges and trends on FMs for music, aiming to shape the trajectory of human-AI collaboration in the music realm.", "sections": [{"title": "I. INTRODUCTION", "content": "Music is an important part of human culture, universal in its cross-cultural presence, yet taking many different forms across cultures. Its functions include emotion regulation, communication, and promoting social cohesion; it appears in art, entertainment, worship, and advertising; and represents a large industry contributing to the global economy. It presents opportunities to benefit both human society culturally and music industries economically, as well as unique technical challenges when combined with AI.\nThe field of computer music is at the intersection of music, computer science, electrical engineering, and artificial intelligence, drawing upon fields such as philosophy (aesthetics), psychology (perception, cognition, and production), and physics (acoustics). Computational approaches to music often employ signal processing and other techniques to extract features from audio signals, and then apply machine learning algorithms for music information retrieval (MIR) tasks or music composition.\nAlthough natural language processing, computer vision, and speech processing have widely used foundation models (FMs), we are still only scratching the surface of AI for art, of which music is an essential component. One challenge specific to music is polyphonic signal modelling. Unlike speech and language signals, music usually has several simultaneous \"speakers\", and the \u201cmeaning\" of what they \"say\" is not grounded in real-world objects or events. The occurrences of different note events are not independent, making it a challenging modelling task to capture the \"language(s)\" of music. Moreover, music typically has a much longer duration with a much higher sample rate compared to speech or general audio, making it harder to model the whole musical piece.\nRecent advances in pre-trained language models (PLMs) significantly outperform traditional algorithms on a range of music-related computational tasks, demonstrating the potential of modern machine learning techniques to understand and process music on an unprecedented scale [LYZ+24]. However, a critical bottleneck has emerged in terms of dataset size and quality. For algorithms to be reliable, especially those deployed in complex, realistic scenarios, they need to be trained on diverse and representative datasets. The performance of these algorithms is deeply dependent on the size of the annotated dataset and the quality of its annotations, which justifies the need for large quantities of high-quality data. Unfortunately, music datasets are often size-constrained due to the limited availability of copyright-free public domain data and the high costs associated with labelling and annotation.\nFMs address this problem by employing self-supervised learning (SSL) approaches for pre-training on a large amount of unlabelled music data. SSL enables the model to learn meaningful representations without the need for explicit labelling, by exploiting intrinsic structures within the data. This approach is similar to the natural human learning process. For example, when children hear different instruments played, they learn the characteristics of each unknown instrument and are able to identify the instruments in new pieces of music without necessarily knowing their names. Similarly, SSL enables machine learning models to derive general knowledge from large unlabelled datasets, thereby improving their performance on downstream tasks that lack large amounts of labelled data. As has proven successful across other domains, models trained through such approaches show promising results for music understanding and generation."}, {"title": "A. What is a Foundation Model?", "content": "The term foundation model was coined to describe a multi-purpose machine learning model that, rather than being trained for a single specific task, serves as the basis of multiple derived models that are able to perform a wide range of tasks [BHA+21]. This terminology reflects the shift from the traditional emphasis on the specifics of architectures or tasks to a focus on broadly applicable models whose emergent abilities and generalisation are unlocked by significantly increasing the number of model parameters [WBZ+21], [CND+22]. Contrary to terms such as large language models or self-supervised learning, which emphasise narrow aspects of AI development, foundation model captures the essence of the generality of these models.\nThe rise of foundation models has come about due to advances in computational hardware, architectural innovations in neural networks (e.g., the Transformer architecture), and an enhanced focus on minimally supervised training paradigms. Foundation models employ deep neural network architectures that are typically trained on large-scale unlabelled datasets with SSL. Following this pre-training phase, foundation models can be adapted for various downstream tasks via a relatively lightweight finetuning or in-context learning stage, for example, using a labelled dataset that is orders of magnitude smaller than the pre-training data.\nBeginning with language models such as Google's BERT (Bidirectional Encoder Representations from Transformers [DCLT18]) and OpenAI's GPT (Generative Pre-trained Transformer [BMR+20]) series, foundation models have demonstrated the power of SSL for training on extensive web-sourced datasets, freeing researchers from reliance on labelled data, which does not scale economically to web-scale data sizes. In addition to text analysis and text generation, such PLMs have also demonstrated their utility across various modalities, including image processing with CLIP [RKH+21a], DALL-E [RPG+21], and Flamingo [ADL+22]; speech and audio generation with Audiobox [VSL+23]; music generation with Jukebox [DJP+20a], MusicLM [ADB+23] and MusicGen [CKG+24]; and robotic control with RT-2 [BBC+23].\nThe release of Stable Diffusion\u00b9 and ChatGPT2 in 2022 marked a significant turning point for foundation models, in terms of public impact, as well as industrial and academic interest in the creation of AI-generated content (AIGC). This significant progress is primarily due to the ability to follow language instruction, emergent ability in algorithmic advances when scaling up to large language models (LLMs), and the authentic quality of Latent Diffusion Models (LDMs) [RBL+21]. These methods suggest a paradigm shift in AI, as generalised frameworks can support multiple applications across disparate domains. Although developing AI with universal capability for multiple and unseen tasks has been the goal of AI researchers since its earliest days [NSS59], most AI research in the ensuing decades has focused on a single or a limited number of pre-defined task(s). In addition, access to advanced problem-solving capabilities through natural language interaction facilitates uptake by non-specialists. Although the development of foundation models demands substantial financial and computational investments plus significant human effort, the adaptation of pre-existing models for tailored needs is more cost-effective, and the release of open-source foundation models like Stable Diffusion, Llama [TLI+23a], Mistral [JSM+23], and MAP-NEO [ZQL+24] gives access to users, developers and researchers alike to explore the possibilities of the models.\nIn this paper, we will discuss two types of self-supervisedly pre-trained foundation models which can perform multiple downstream tasks. The first is the single-modality pre-trained model in the waveform or symbolic domain that requires finetuning on downstream tasks. This can be some variant of PLM for music understanding such as MERT[LYZ+24] or music generation such as Jukebox[DJP+20a]. The second is multimodal pre-trained models that can take both natural language and music as input and have the potential to solve downstream tasks with in-context learning. This includes Latent Diffusion Models (LDMs) with multiple text inputs such as MusicLDM[CWL+23a], a music encoder prepended to an LLM such as Mu-llama[LHSS24] or an LLM with multimodal tokenisers such as AnyGPT[ZDY+24], Gemini 1.5[RST+24] and GPT-40."}, {"title": "B. Why Foundation Models for Music?", "content": "FMs for music not only address data scarcity and reduce annotation costs, but also enhance generalisation in music"}, {"title": "1) Impact on Industry:", "content": "Foundation models have, or will potentially have, more robust and commercially viable applications for music than previous methods, including in creative processes, music understanding, and approaches within the entertainment industry.\nIn the area of Creative Applications, AIGC is perhaps the most obvious application of foundation models, including music, such as personalised music generation and collaborative composition with musicians. Foundation models enable the generation of music based on user-specified preferences as input such as genre, mood, tempo, and instruments. Following recent progress in LLMs and LDMs in music, many music generation startups with proven commercial impact such as SunoAI\u00b3, TiangongAI4 and Udio, have emerged. Musicians and producers can manipulate aforementioned parameters to steer the composition processes, assisting in the ideation process. Such music generation applications can enable new forms of interaction with users and musicians. Music can change based on the listener's feedback or input prompts, potentially creating more immersive and personalised listening experiences. Additionally, FMs show potential in collaborating with musicians or music editors by following their instructions more professionally and robustly.\nFoundation models address several aspects of Music Understanding. By analysing listening habits and understanding musical preferences, FMs can offer listeners more personalised recommendations, improving the user experience on streaming platforms. FMs may also be used to better detect cover songs and identify copyright infringement, helping artists and companies protect their intellectual property more efficiently. They may also provide analyses of musical pieces to aid musicologists in understanding music structure, characteristics, and innovation."}, {"title": "For Entertainment and Media,", "content": "foundation models can create adaptive soundtracks that correspond to the narratives of visual media for musicians and music editors, enhancing the impact and immersion of movies and video games."}, {"title": "2) Social Impact:", "content": "Foundation models for music, with their capacity to understand, generate, and process music, can provide profound implications for culture and society. As FM has the potential to better resolve all kinds of music-related tasks, most main applications of MIR can be regarded FM territory, and therefore FMs have the potential to change the way we interact with, preserve, and understand music, raising significant ethical and heritage considerations.\nConcerning Cultural Preservation and Diversity, foundation models can play a role in preserving world cultural and musical traditions that are at risk of being lost. By analysing diverse musical datasets, these models can identify unique characteristics of styles, compositions, and performances from cultures around the world, much similar to current LLMs' capability of understanding minor languages. Moreover, FMs can promote cultural awareness by facilitating exploration of music from different parts of the world.\nFor the field of Music Anthropology, foundation models may serve as a tool for studying the evolution of music across different nations and eras. By analysing vast amounts of music data, FMs can uncover music patterns and cultural influences. By relating this analysis to social and historical data, FMs could potentially provide insights into music's role in human societies.\nFoundation models can improve access to Music Education by creating personalised learning experiences adapted to the learner's pace and style; For example as a virtual tutor that provides theoretical and practical knowledge, feedback, virtual accompaniment and simulated ensemble playing. This could make music education more accessible, regardless of access to traditional music education resources, encouraging a more inclusive culture of music learning, and removing barriers that have historically limited participation in music-making.\nIn Music Therapy, FMs could be tailored to produce music for therapeutic purposes, aligning with individual therapeutic goals or emotional needs, potentially offering mental health support. Likewise, in non-clinical settings, by generating music that reflects or counteracts listeners' emotional states, foundation models can play a role in mood regulation and wellness practices."}, {"title": "3", "content": "https://suno.com/\n4https://music.tiangong.cn/\n5https://www.udio.com/"}, {"title": "5", "content": "The ability of foundation models to generate music that mimics human compositions raises important Ethical Considerations. The fact that the models benefit from the intellectual property of the millions of musicians and artists who created the training data leads to legal challenges and debates about the lawful use of data. Ethical discussions focus on issues of copyright, originality, and the role of AI in creative processes, ideally with interpretability and transparency. As these models become more prevalent, society must navigate between leveraging technology for innovation in music creation and respecting the rights and contributions of human artists.\nThe impact of foundation models for music is likely to be profound, offering new tools for the generation, analysis and interaction of music, as well as for music education and therapy. As these models are developed, it is essential to consider their ethical implications thoughtfully, ensuring that they serve to enrich human culture and promote a more fair and inclusive global society. For more information about the ethical issues of FMs in music, please refer to Section VI."}, {"title": "C. Aims of the Survey", "content": "The purpose of this survey is to provide a thorough and comprehensive overview of foundation models as they relate to the domain of music, including LLMs and LDMs. While some previous overview papers have addressed FMs [BHA+21] or LLMs [ZZL+23], [HLC+24] in general and as they apply to specific areas such as vision [ZHJL24], speech [ZLL+23], [MMB+23], [LCP+23] and audio [WCL+24], [MLPW22], [LSS+23], [TTG+24], they do not comprehensively cover music-related applications of FMs. Besides, previous music surveys also exhibit limitations in providing a comprehensive overview of FMs. For example, [JLY20] fails to incorporate new advancements post-2021, particularly in LLMs and audio LDMs. Similarly, [HSF+24] focuses predominantly on digital signal processing methods, neglecting the integration of FMs into music synthesis and understanding. [HOHOB22] briefly mentions LLMs and LDMs but lacks an in-depth exploration of their applications in music understanding as well as multimodality. [ZBRR23] provides a limited discussion on music generation models, primarily focusing on commercial scenarios and overlooking critical technical details and ethical considerations.\nOur survey aims to bridge this gap by reviewing the wide array of FM applications, from music understanding to generation, therapy, and the ethical implications associated with these technologies. By doing so, we seek to highlight the unique challenges and opportunities that musical data presents for FMs, including aspects such as modelling long-term temporal dependencies, and the evaluation of artistic outputs. Additionally, this survey endeavours to update the literature with recent advances in LLMs and audio LDMs that are not covered in existing surveys.\nThe survey provides a detailed exploration of FMs in music. Section 2 examines music modalities and representations, including psychoacoustics, audio representations, symbolic music representations, and their integration with other modalities."}, {"title": "II. REPRESENTATIONS OF MUSIC", "content": "Music representations can be used to describe, encode, and communicate musical information mentally and computationally. These representations can capture different aspects of music, such as pitch, rhythm, harmony, dynamics, expression, timbre or structure. Humans perceive acoustic music signals in both the temporal and frequency domains, at multiple time and frequency scales, and abstract this rich sensory information to perceptual representations (experiences) of musical concepts such as pitch, rhythm, dynamics, and timbre, among others. Such a process inspires people to design a broad range of music representations with often very different focuses and characteristics. Many representations have been adopted to or exclusively devised as computer representations of music, and yet more are developed for or used as input representations to neural networks. Studying music representations in the context of foundational music models is essential because the choice of representation affects the model's effectiveness, efficiency and performance. Appropriate representations ensure that the data is relevant and informative, enhancing accuracy and reducing computational load. They may also help extract meaningful features and aid in the generalisation of new data. A suitable representation may also incorporate domain-specific knowledge, better align with the task requirements and facilitate interpretability, ultimately leading to more robust, transparent and explainable models.\nIn this section, we discuss manually designed computer representations of musical notation and audio content at various levels of abstraction, starting from a very brief overview of how sound and musical concepts are experienced by humans. Additionally, the representations of foundation modelsF(Ms) can also be fully learnable audio tokens other than manually designed representations; please refer to Section IV-C for more information. Finally, In the last subsection of this part, we discuss multimodal representations for music.\nWe can tell from the following paragraphs that the current FMs for music are developed on limited modalities, including waveform, MIDI or ABC notations. And FMs with more comprehensive input modalities remain underexplored."}, {"title": "A. Music Perception & Notation", "content": "Humans process musical information through a complex interplay of perceptual and cognitive processes. A sound wave transmits a pattern of oscillations, created by an excitation force (e.g., bowing) that stimulates a vibrating object (e.g., a viola's string), through a physical medium such as air [CSS19]. On the way from the source (e.g., bowed viola string) to our ears, the sound waves carry information about the vibrating object, the driving force, and possibly other interactions through the physical medium (e.g., the walls of a room). When the waves reach the ears, these oscillations are translated into a pattern of neural pulses that the brain processes to make sense of musical elements such as melody, harmony, rhythm, and timbre. The ear acts like a set of overlapping bandpass frequency filters [Lyo17]. Sound energy within a frequency range is integrated across frequency and time (the temporal pattern of neural pulses can also encode frequency). The notion of the critical band (the range of frequencies that are integrated) is important psychoacoustically because it provides a basis for explanations of the degree of masking and loudness summation, but also of frequency discrimination.\nPitch may be defined as the human perceptual correlate of acoustic frequency. Specifically, the perceptual representation of pitch involves at least two dimensions: a circular dimension of pitch chroma (the pitch class, sometimes called tonality) and a vertical dimension of pitch height [She82]. A pitch class is a set of all heights of pitches that are perceived as repeating once per octave (e.g., all C notes). Both pitch chroma and pitch height dimensions generally allow ordering pitches on a scale from low to high, but not always unambiguously (e.g., Shepard tones [She64]). Furthermore, the relationship between pitch and frequency is not linear: we can better detect differences in lower frequencies than higher frequencies [SVN37]. For example, we can tell the difference between 500 and 1500 Hz, but probably struggle to discriminate between 9 and 10 kHz, although the frequency distance remains the same. This is because the width of the critical band increases at higher frequencies (frequencies are smeared) and decreases at lower frequencies (frequencies can be resolved).\nMusically, chroma is the most important part of the pitch and forms the basis of melody (distinct pitches played in sequence) and harmony (distinct pitches coinciding with one another), but also of rhythm. Rhythm is what orders the movement of melodic and harmonic patterns in time, both in the sense of how listeners perceive and how musicians perform music, consisting of beats (perceived pulses that mark equally spaced points in time), tempo (their frequency), and meter (perceived cyclical groupings of beats into units marked by accents; the time signature). Staff notation is a symbolic visual representation of music that allows musicians to read the pitch and rhythm of notes they are supposed to play, and forms the basis of digital systems to encode musical documents in a machine-readable format. The pitch (class) and octave (height) of a note are indicated by the vertical position of the note within, below, or above the staff. Notes have different durations or note lengths, represented by whole notes, half notes, quarter notes, eighth notes etc. Each note length has a specific duration relative to the beat defined by the meter. Roughly speaking, rhythms and melodies are made up of notes with different durations.\nTimbre, or tone colour, is a \"catch-all\" term, as it has been called, that refers to everything on what a sound or musical piece \u201csounds like,\" except for pitch information, loudness, and rhythm-related features, although changes in pitch, dynamics, and timing also produce changes in timbre [SSM+19]. Timbre is primarily determined by the relative amplitudes of frequency components. Phase can also play a role, especially when certain changes in the relative phases of harmonics within a critical band can change the resulting envelope modulation [Lyo17]. Temporal characteristics of the signal (e.g., attack time, decay) and vibrato, are key contributors to timbre too [APS05]. However, changes in harmonic amplitudes affect timbre in a much more direct way, such that timbral (i.e., perceptual) distances between sounds correlate well with spectral differences, such as measured in terms of the log power outputs of a one-third-octave filterbank [Plo76]. Such an approach to timbre is not accurate or complete [Lyo17], but captures a reasonable part of the way a sound or musical piece \u201csounds\u201d and, together with non-linear frequency scales, is the basis for computer representations of musical spectra."}, {"title": "B. Computer Representations of Music", "content": "Inspired by the process through which humans perceive music, as mentioned in the previous subsection, we will introduce the various computational methods for music representation. Echoing how the ear transforms sound waves into neural signals, we begin by detailing the conversion of the waveform in the time domain into spectral representations using band-pass frequency filters. Subsequently, we delve into symbolic music representations at both performance-level and note-level that abstract neural signals in terms of pitch, timbre and time through our brain. Though MIDI (Musical Instrument Digital Interface) in performance-level and ABC notations in note-level are the most widely-used presentations for symbolic music LLMs, we introduce other music representations which have been used with traditional deep learning methods in this subsection as they have potential to be trained for foundation models. We then explore more abstract note-level representations which prioritise relational and structural aspects of music notation. An example is the ABC notation system, which organises music by relative beats in staff notation rather than specific timestamps, offering a more human-understandable representation suited to computational models. At last, we will discuss combined representations like note-tuples and the potential of learnable note-level tokenisers. These advanced topics pave the way for integrating these representations into foundation models, enhancing our ability to process and generate music through AI."}, {"title": "1) Acoustic-level Representations of Music:", "content": "The log Mel spectrogram is a key audio representation technique that synthesises signal processing with psychoacoustic principles to closely mirror human auditory perception. It starts with converting audio signals from the time domain to the frequency domain using the Fast Fourier Transform (FFT). The Short-Time Fourier Transform (STFT) further analyses these signals over time, segmenting the audio into overlapping frames and applying FFT to capture temporal dynamics. Transitioning to the Mel scale involves applying Mel band-pass filters to the STFT output. These filters, aligned with the non-linear nature of human pitch perception, group frequencies into bins that are linearly spaced at lower frequencies and logarithmically at higher ones. The logarithmic transformation of this Mel-scaled output adjusts for the human ear's non-linear response to loudness and pitch, compressing the spectrogram's dynamic range and emphasising perceptually significant changes in sound energy. The log Mel spectrogram has been directly applied in tasks of music generation [HSR+22], [FM22a] due to its perceptual relevance, enabling accurate modelling of musical characteristics. In the foundation model paradigm, however, log Mel spectrograms usually serve as input into audio representation models, which learns an efficient latent space [GLCG22a], [FFH24], [HXL+22b] in which generation operates.\nThe Mel-Frequency Cepstral Coefficients (MFCCs) are a crucial audio feature extraction technique that encapsulates the characteristics of human speech perception. This process begins with converting audio signals from the time domain to the frequency domain via FFT. It then applies the STFT to segment the audio into overlapping frames and analyse temporal changes. The Mel scale is employed through Mel filters that mimic the human ear's logarithmic frequency perception, focusing on discriminative lower frequencies. The computation of MFCCs involves taking the logarithm of the energies in each Mel filter, followed by a Discrete Cosine Transform (DCT). Due to their effectiveness in capturing vocal tract configurations, MFCCs are widely used in speech recognition and speaker identification applications. AV-HuBERT [SHLM22], Hubert [HBT+21a], and MusicHubert [MYL+23] use MFCC features as re-prediction targets.\nThe Constant-Q Transform (CQT) enhances audio processing in music analysis by providing a log-frequency spectrogram aligned with the musical scale. Unlike the linear Fourier Transform, the CQT uses a logarithmic scale that reflects the exponential nature of musical pitch, simplifying the extraction of note frequencies. Its key feature is that the centre frequency to bandwidth ratio remains constant (denoted by Q), allowing variable filter lengths that optimise performance. Despite less popularity compared to the log-Mel spectrogram, CQT has proven useful for tasks like musical timbre transfer [HLA+19] and music representation learning [LYZ+24].\nChroma features focus on the twelve pitch classes of Western music, capturing the harmonic core of a piece regardless of timbre or instrumentation. These features, displayed in chromagrams, condense music into pitch class profiles that highlight the harmonic and melodic structure. Chroma features are especially effective in identifying chords, key signatures, and modulations due to their alignment with the equal-tempered scale. The most recent application of chromagram is melody-guided music generation in MusicGen [CKG+24]."}, {"title": "2) Symbolic Music Format & Their Content:", "content": "In this subsection, we introduce the low-level symbolic music formats and their informational content at both performance-level and note-level. Symbolic music formats are essential for representing musical notation and facilitating digital music processing. This subsection will cover key formats, highlighting their structures, capabilities, and limitations, especially in the context of training computer music foundation models.\na) Performance-level symbolic music repsentation: Musical Instrument Digital Interface (MIDI) protocol is a standard way of saving and transferring MIDI data for playback on different systems. Unlike MusicXML or MEI, MIDI encodes musical performance data, capturing information about note pitches, durations, velocities, and other performance aspects like dynamics, articulations, and control changes. This makes MIDI particularly suited for applications requiring precise control over musical performance and real-time interaction between devices. In ML, the detailed performance data in MIDI is invaluable for training models focused on tasks such as music generation, transcription [BDDE19], Optical Music Recognition (OMR) [SF20] and style transfer [BKWW18]. Models can learn from the nuances of human performances captured in MIDI data, enabling the generation of expressive and realistic musical outputs. MIDI's widespread use and standardisation mean there is a vast amount of data available for training purposes. However, the format's focus on performance rather than notation means it lacks explicit information about musical structure, such as key signatures, time signatures, and notation details.\nNote-level representations focus on the structural elements of music, such as pitch and rhythm, offering a simplified, human-readable format ideal for sharing musical scores. In contrast, performance-level formats like MIDI encompass additional expressive details of a piece's execution, including dynamics, musical techniques and duration/tempo variations. While note-level symbols capture the composition's essence, MIDI provides a detailed account of its performance, capturing both the notation and its expressive execution.\nb) Note-level symbolic music representation: The following paragraphs introduce the diverse note representation of symbolic music which are shown in Figure 2.\nMusicXML is a significant format in the digital music domain, encapsulating the complexities of musical notation in a universally transferable manner [Goo12]. MusicXML is rooted in the Extensible Markup Language (XML), and offers a textual format for encoding music scores (see Figure 2f), ensuring human and machine readability. This format enables a detailed representation of musical elements, from the pitch and duration of notes to their graphical representation in sheet music. By design, MusicXML does not encode all document information in a fully-featured music notation system, which limits capabilities for other non-CWMN scores.\nDespite MusicXML's strength in accurately representing musical notation and the fact that many music scores are shared and published in MusicXML format, it is not commonly used to train FMs for music in previous works. For one thing, this is due to its symbolic nature, like ABC notation, which, while rich in notational detail, lacks the audio information necessary for models to learn the acoustic properties of music. Foundation models, especially those geared towards music generation and understanding, require data that encompass the timbral, dynamic, and expressive aspects of music as it is heard, not just as it is written. For another, the process of encoding or decoding MusicXML can be cumbersome for AI models, which thrive on larger datasets and longer context lengths for development. The complexity and verbosity of XML-based formats may introduce additional challenges in processing and interpreting data efficiently, but may also provide more benefits on music details compared to other music notations. Therefore, while MusicXML excels in notation accuracy and interoperability between software, its application in training FMs is limited by the challenges and still underexplored.\nMusic Encoding Initiative (MEI) aims at creating a digital format to represent music notation [Rol02]. MEI utilises an XML-based schema, see Figure 2g, providing rules for documenting the intellectual and physical properties of music notation, enabling consistent search, retrieval, display, and exchange of information across platforms. MEI's modular and extensible structure supports encoding various music notation systems, including common Western notation, mensural (Re-naissance), and neume (Medieval) notations. This flexibility ensures an accurate representation by preserving the unique structure and semantics of each notation system rather than merely imitating them visually. One of the primary goals of MEI is to create a semantically rich model for music notation. It enables the encoding of traditional facsimile, critical, and performance editions. Its foundation on open standards and platform independence promotes the development of extensive and international archives of notated music, which serve as essential resources for editions, performances, analyses and research. In comparison, while MEI and MusicXML encode musical elements such as notes, staves, rests, and clefs using XML, they differ in focus. MusicXML primarily facilitates interchange between notation editors. In contrast, MEI provides a more detailed and systematic encoding of notation information, supporting various notation systems beyond standard common Western notation. Moreover, MEI can document relationships among digital page images, notational features and audio recordings, offering a more comprehensive framework for music representation. Similarly to MusicXML, MEI faces challenges in training computer music FMs. Its complexity, lack of standardisation, and focus on detailed musical notation make it less suitable for ML, which requires more streamlined and uniform data formats. MEI's emphasis on visual representation rather than audio features, large data files, and limited integration with machine learning tools further hinder its effectiveness. Foundational models benefit more from compact, standardised data representations [BHLP16].\nLilyPond is a syntax similar to LATEX, allowing users to write musical notation in plain text files, which are then compiled into engraved scores [NN03]. The syntax is comprehensive (see Figure 2b) covering a wide range of musical symbols and formatting options, making it suitable for complex and professional-quality scores. It has been previously used for"}, {"title": "9", "content": "automatic transcription of organ tablatures [SKM+21", "CS17": ".", "Hur02": "kern aims to provide a flexible and comprehensive way to represent musical scores in plain text. The format encodes musical information such as pitch, rhythm, meter, and articulation using a straightforward syntax, see Figure 2c, facilitating both human readability and computational processing. **kern is highly extensible, allowing users to include additional musical parameters and metadata as needed. Its standardised text-based structure makes it suitable for training models in symbolic music analysis, pattern recognition, and music generation [RSIM21", "z": "arking rests (see Figure 2d). Additional notation specifies musical nuances such as sharps, flats, octave shifts, note lengths, keys, and ornamentation. This notation system, which integrates elements from Helmholtz pitch notation, was designed to simplify the sharing of music online and provide a straightforward language for software development, paralleling the simplicity of tablature and solf\u00e8ge. The structure of ABC notation is divided into a header and a body. The header contains metadata such as reference numbers for organising multiple pieces, titles, time signatures, default note lengths, and keys. Conversely, the body focuses on the musical content, encoding each note and rest into tokens that reflect pitch, duration, and rhythmic placement, delineated by bar lines. Like other symbolic music notations, ABC notation is compatible with natural language notations, making it an ideal candidate for developing text-to-symbolic music models, such as chatMusician [YLW+24a", "HHRK98": ["HHRK01"]}]}