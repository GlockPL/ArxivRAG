{"title": "Foundation Models for Music: A Survey", "authors": ["Yinghao Ma", "Anders \u00d8land", "Anton Ragni", "Bleiz MacSen Del Sette", "Charalampos Saitis", "Chris Donahue", "Chenghua Lin", "Christos Plachouras", "Emmanouil Benetos", "Elio Quinton", "Elona Shatri", "Fabio Morreale", "Ge Zhang", "Gy\u00f6rgy Fazekas", "Gus Xia", "Huan Zhang", "Ilaria Manco", "Jiawen Huang", "Julien Guinot", "Liwei Lin", "Luca Marinelli", "Max W. Y. Lam", "Megha Sharma", "Qiuqiang Kong", "Roger B. Dannenberg", "Ruibin Yuan", "Shangda Wu", "Shih-Lun Wu", "Shuqi Dai", "Shun Lei", "Shiyin Kang", "Simon Dixon", "Wenhu Chen", "Wehhao Huang", "Xingjian Du", "Xingwei Qu", "Xu Tan", "Yizhi Li", "Zeyue Tian", "Zhiyong Wu", "Zhizheng Wu", "Ziyang Ma", "and Ziyu Wang"], "abstract": "In recent years, foundation models (FMs) such as large language models (LLMs) and latent diffusion models (LDMs) have profoundly impacted diverse sectors, including music. This comprehensive review examines state-of-the-art (SOTA) pre-trained models and foundation models in music, spanning from representation learning, generative learning and multimodal learning. We first contextualise the significance of music in various industries and trace the evolution of AI in music. By delineating the modalities targeted by foundation models, we discover many of the music representations are underexplored in FM development. Then, emphasis is placed on the lack of versatility of previous methods on diverse music applications, along with the potential of FMs in music understanding, generation and medical application. By comprehensively exploring the details of the model pre-training paradigm, architectural choices, tokenisation, finetuning methodologies and controllability, we emphasise the important topics that should have been well explored, like instruction tuning and in-context learning, scaling law and emergent ability, as well as long-sequence modelling etc. A dedicated section presents insights into music agents, accompanied by a thorough analysis of datasets and evaluations essential for pre-training and downstream tasks. Finally, by underscoring the vital importance of ethical considerations, we advocate that following research on FM for music should focus more on such issues as interpretability, transparency, human responsibility, and copyright issues. The paper offers insights into future challenges and trends on FMs for music, aiming to shape the trajectory of human-AI collaboration in the music realm.", "sections": [{"title": "I. INTRODUCTION", "content": "Music is an important part of human culture, universal in its cross-cultural presence, yet taking many different forms across cultures. Its functions include emotion regulation, communication, and promoting social cohesion; it appears in art, entertainment, worship, and advertising; and represents a large industry contributing to the global economy. It presents opportunities to benefit both human society culturally and music industries economically, as well as unique technical challenges when combined with AI.\nThe field of computer music is at the intersection of music, computer science, electrical engineering, and artificial intelligence, drawing upon fields such as philosophy (aesthetics), psychology (perception, cognition, and production), and physics (acoustics). Computational approaches to music often employ signal processing and other techniques to extract features from audio signals, and then apply machine learning algorithms for music information retrieval (MIR) tasks or music composition.\nAlthough natural language processing, computer vision, and speech processing have widely used foundation models (FMs), we are still only scratching the surface of AI for art, of which music is an essential component. One challenge specific to music is polyphonic signal modelling. Unlike speech and language signals, music usually has several simultaneous \"speakers\", and the \u201cmeaning\" of what they \"say\" is not grounded in real-world objects or events. The occurrences of different note events are not independent, making it a challenging modelling task to capture the \"language(s)\" of music. Moreover, music typically has a much longer duration with a much higher sample rate compared to speech or general audio, making it harder to model the whole musical piece.\nRecent advances in pre-trained language models (PLMs) significantly outperform traditional algorithms on a range of"}, {"title": "A. What is a Foundation Model?", "content": "The term foundation model was coined to describe a multi-purpose machine learning model that, rather than being trained for a single specific task, serves as the basis of multiple derived models that are able to perform a wide range of tasks [BHA+21]. This terminology reflects the shift from the traditional emphasis on the specifics of architectures or tasks to a focus on broadly applicable models whose emergent abilities and generalisation are unlocked by significantly increasing the number of model parameters [WBZ+21], [CND+22]. Contrary to terms such as large language models or self-supervised learning, which emphasise narrow aspects of AI development, foundation model captures the essence of the generality of these models.\nThe rise of foundation models has come about due to advances in computational hardware, architectural innovations in neural networks (e.g., the Transformer architecture), and an enhanced focus on minimally supervised training paradigms. Foundation models employ deep neural network architectures that are typically trained on large-scale unlabelled datasets with SSL. Following this pre-training phase, foundation models can be adapted for various downstream tasks via a relatively lightweight finetuning or in-context learning stage, for example, using a labelled dataset that is orders of magnitude smaller than the pre-training data.\nBeginning with language models such as Google's BERT (Bidirectional Encoder Representations from Transformers [DCLT18]) and OpenAI's GPT (Generative Pre-trained Transformer [BMR+20]) series, foundation models have demonstrated the power of SSL for training on extensive web-sourced datasets, freeing researchers from reliance on labelled data, which does not scale economically to web-scale data sizes. In addition to text analysis and text generation, such PLMs have also demonstrated their utility across various modalities, including image processing with CLIP [RKH+21a], DALL-E [RPG+21], and Flamingo [ADL+22]; speech and audio generation with Audiobox [VSL+23]; music generation with Jukebox [DJP+20a], MusicLM [ADB+23] and MusicGen [CKG+24]; and robotic control with RT-2 [BBC+23].\nThe release of Stable Diffusion\u00b9 and ChatGPT2 in 2022 marked a significant turning point for foundation models, in terms of public impact, as well as industrial and academic interest in the creation of AI-generated content (AIGC). This significant progress is primarily due to the ability to follow language instruction, emergent ability in algorithmic advances when scaling up to large language models (LLMs), and the authentic quality of Latent Diffusion Models (LDMs) [RBL+21]. These methods suggest a paradigm shift in AI, as generalised frameworks can support multiple applications across disparate domains. Although developing AI with universal capability for multiple and unseen tasks has been the goal of AI researchers since its earliest days [NSS59], most AI research in the ensuing decades has focused on a single or a limited number of pre-defined task(s). In addition, access to advanced problem-solving capabilities through natural language interaction facilitates uptake by non-specialists. Although the development of foundation models demands substantial financial and computational investments plus significant human effort, the adaptation of pre-existing models for tailored needs is more cost-effective, and the release of open-source foundation models like Stable Diffusion, Llama [TLI+23a], Mistral [JSM+23], and MAP-NEO [ZQL+24] gives access to users, developers and researchers alike to explore the possibilities of the models.\nIn this paper, we will discuss two types of self-supervisedly pre-trained foundation models which can perform multiple downstream tasks. The first is the single-modality pre-trained model in the waveform or symbolic domain that requires finetuning on downstream tasks. This can be some variant of PLM for music understanding such as MERT[LYZ+24] or music generation such as Jukebox[DJP+20a]. The second is multimodal pre-trained models that can take both natural language and music as input and have the potential to solve downstream tasks with in-context learning. This includes Latent Diffusion Models (LDMs) with multiple text inputs such as MusicLDM[CWL+23a], a music encoder prepended to an LLM such as Mu-llama[LHSS24] or an LLM with multimodal tokenisers such as AnyGPT[ZDY+24], Gemini 1.5[RST+24] and GPT-40."}, {"title": "B. Why Foundation Models for Music?", "content": "FMs for music not only address data scarcity and reduce annotation costs, but also enhance generalisation in music"}, {"title": "C. Aims of the Survey", "content": "The purpose of this survey is to provide a thorough and comprehensive overview of foundation models as they relate to the domain of music, including LLMs and LDMs. While some previous overview papers have addressed FMs [BHA+21] or LLMs [ZZL+23], [HLC+24] in general and as they apply to specific areas such as vision [ZHJL24], speech [ZLL+23], [MMB+23], [LCP+23] and audio [WCL+24], [MLPW22], [LSS+23], [TTG+24], they do not comprehensively cover music-related applications of FMs. Besides, previous music surveys also exhibit limitations in providing a comprehensive overview of FMs. For example, [JLY20] fails to incorporate new advancements post-2021, particularly in LLMs and audio LDMs. Similarly, [HSF+24] focuses predominantly on digital signal processing methods, neglecting the integration of FMs into music synthesis and understanding. [HOHOB22] briefly mentions LLMs and LDMs but lacks an in-depth exploration of their applications in music understanding as well as multimodality. [ZBRR23] provides a limited discussion on music generation models, primarily focusing on commercial scenarios and overlooking critical technical details and ethical considerations.\nOur survey aims to bridge this gap by reviewing the wide array of FM applications, from music understanding to generation, therapy, and the ethical implications associated with these technologies. By doing so, we seek to highlight the unique challenges and opportunities that musical data presents for FMs, including aspects such as modelling long-term temporal dependencies, and the evaluation of artistic outputs. Additionally, this survey endeavours to update the literature with recent advances in LLMs and audio LDMs that are not covered in existing surveys.\nThe survey provides a detailed exploration of FMs in music. Section 2 examines music modalities and representations, including psychoacoustics, audio representations, symbolic music representations, and their integration with other modalities. We then turn to the diverse applications of FMs in music in"}, {"title": "II. REPRESENTATIONS OF MUSIC", "content": "Music representations can be used to describe, encode, and communicate musical information mentally and computationally. These representations can capture different aspects of music, such as pitch, rhythm, harmony, dynamics, expression, timbre or structure. Humans perceive acoustic music signals in both the temporal and frequency domains, at multiple time and frequency scales, and abstract this rich sensory information to perceptual representations (experiences) of musical concepts such as pitch, rhythm, dynamics, and timbre, among others. Such a process inspires people to design a broad range of music representations with often very different focuses and characteristics. Many representations have been adopted to or exclusively devised as computer representations of music, and yet more are developed for or used as input representations to neural networks. Studying music representations in the context of foundational music models is essential because the choice of representation affects the model's effectiveness, efficiency and performance. Appropriate representations ensure that the data is relevant and informative, enhancing accuracy and reducing computational load. They may also help extract meaningful features and aid in the generalisation of new data. A suitable representation may also incorporate domain-specific knowledge, better align with the task requirements and facilitate interpretability, ultimately leading to more robust, transparent and explainable models.\nIn this section, we discuss manually designed computer representations of musical notation and audio content at various levels of abstraction, starting from a very brief overview of how sound and musical concepts are experienced by humans. Additionally, the representations of foundation modelsF(Ms) can also be fully learnable audio tokens other than manually designed representations; please refer to Section IV-C for more information. Finally, In the last subsection of this part, we discuss multimodal representations for music.\nWe can tell from the following paragraphs that the current FMs for music are developed on limited modalities, including"}, {"title": "A. Music Perception & Notation", "content": "Humans process musical information through a complex interplay of perceptual and cognitive processes. A sound wave transmits a pattern of oscillations, created by an excitation force (e.g., bowing) that stimulates a vibrating object (e.g., a viola's string), through a physical medium such as air [CSS19]. On the way from the source (e.g., bowed viola string) to our ears, the sound waves carry information about the vibrating object, the driving force, and possibly other interactions through the physical medium (e.g., the walls of a room). When the waves reach the ears, these oscillations are translated into a pattern of neural pulses that the brain processes to make sense of musical elements such as melody, harmony, rhythm, and timbre. The ear acts like a set of overlapping bandpass frequency filters [Lyo17]. Sound energy within a frequency range is integrated across frequency and time (the temporal pattern of neural pulses can also encode frequency). The notion of the critical band (the range of frequencies that are integrated) is important psychoacoustically because it provides a basis for explanations of the degree of masking and loudness summation, but also of frequency discrimination.\nPitch may be defined as the human perceptual correlate of acoustic frequency. Specifically, the perceptual representation of pitch involves at least two dimensions: a circular dimension of pitch chroma (the pitch class, sometimes called tonality) and a vertical dimension of pitch height [She82]. A pitch class is a set of all heights of pitches that are perceived as repeating once per octave (e.g., all C notes). Both pitch chroma and pitch height dimensions generally allow ordering pitches on a scale from low to high, but not always unambiguously (e.g., Shepard tones [She64]). Furthermore, the relationship between pitch and frequency is not linear: we can better detect differences in lower frequencies than higher frequencies [SVN37]. For example, we can tell the difference between 500 and 1500 Hz, but probably struggle to discriminate between 9 and 10 kHz, although the frequency distance remains the same. This is because the width of the critical band increases at higher frequencies (frequencies are smeared) and decreases at lower frequencies (frequencies can be resolved).\nMusically, chroma is the most important part of the pitch and forms the basis of melody (distinct pitches played in sequence) and harmony (distinct pitches coinciding with one another), but also of rhythm. Rhythm is what orders the movement of melodic and harmonic patterns in time, both in the sense of how listeners perceive and how musicians perform music, consisting of beats (perceived pulses that mark equally spaced points in time), tempo (their frequency), and meter (perceived cyclical groupings of beats into units marked by accents; the time signature). Staff notation is a symbolic visual representation of music that allows musicians to read the pitch and rhythm of notes they are supposed to play, and forms the basis of digital systems to encode musical documents in a machine-readable format. The pitch (class) and octave"}, {"title": "B. Computer Representations of Music", "content": "Inspired by the process through which humans perceive music, as mentioned in the previous subsection, we will introduce the various computational methods for music representation. Echoing how the ear transforms sound waves into neural signals, we begin by detailing the conversion of the waveform in the time domain into spectral representations using band-pass frequency filters. Subsequently, we delve into symbolic music representations at both performance-level and note-level that abstract neural signals in terms of pitch, timbre and time through our brain. Though MIDI (Musical Instrument Digital Interface) in performance-level and ABC notations in note-level are the most widely-used presentations for symbolic music LLMs, we introduce other music representations which have been used with traditional deep learning methods in this subsection as they have potential to be trained for foundation models. We then explore more abstract note-level representations which prioritise relational and structural aspects of music notation. An example is the ABC notation system, which organises music by relative beats in staff notation rather than specific timestamps, offering a more human-understandable representation suited to computational models. At last, we will discuss combined representations like note-tuples and the potential of learnable note-level tokenisers. These advanced topics pave the way for integrating these representations into foundation models, enhancing our ability to process and generate music through AI."}, {"title": "1) Acoustic-level Representations of Music:", "content": "1) Acoustic-level Representations of Music: The log Mel spectrogram is a key audio representation technique that synthesises signal processing with psychoacoustic principles to closely mirror human auditory perception. It starts with converting audio signals from the time domain to the frequency domain using the Fast Fourier Transform (FFT). The Short-Time Fourier Transform (STFT) further analyses these signals over time, segmenting the audio into overlapping frames and applying FFT to capture temporal dynamics. Transitioning to the Mel scale involves applying Mel band-pass filters to the STFT output. These filters, aligned with the non-linear nature of human pitch perception, group frequencies into bins that are linearly spaced at lower frequencies and logarithmically at higher ones. The logarithmic transformation of this Mel-scaled output adjusts for the human ear's non-linear response to loudness and pitch, compressing the spectrogram's dynamic range and emphasising perceptually significant changes in sound energy. The log Mel spectrogram has been directly applied in tasks of music generation [HSR+22], [FM22a] due to its perceptual relevance, enabling accurate modelling of musical characteristics. In the foundation model paradigm, however, log Mel spectrograms usually serve as input into audio representation models, which learns an efficient latent space [GLCG22a], [FFH24], [HXL+22b] in which generation operates.\nThe Mel-Frequency Cepstral Coefficients (MFCCs) are a crucial audio feature extraction technique that encapsulates the characteristics of human speech perception. This process begins with converting audio signals from the time domain to the frequency domain via FFT. It then applies the STFT to segment the audio into overlapping frames and analyse temporal changes. The Mel scale is employed through Mel filters that mimic the human ear's logarithmic frequency perception, focusing on discriminative lower frequencies. The computation of MFCCs involves taking the logarithm of the energies in each Mel filter, followed by a Discrete Cosine Transform (DCT). Due to their effectiveness in capturing vocal tract configurations, MFCCs are widely used in speech recognition and speaker identification applications. AV-HuBERT [SHLM22], Hubert [HBT+21a], and MusicHubert [MYL+23] use MFCC features as re-prediction targets.\nThe Constant-Q Transform (CQT) enhances audio processing in music analysis by providing a log-frequency spectrogram aligned with the musical scale. Unlike the linear Fourier Transform, the CQT uses a logarithmic scale that reflects the exponential nature of musical pitch, simplifying the extraction of note frequencies. Its key feature is that the centre frequency to bandwidth ratio remains constant (denoted by Q), allowing variable filter lengths that optimise performance. Despite less popularity compared to the log-Mel spectrogram, CQT has proven useful for tasks like musical timbre transfer [HLA+19] and music representation learning [LYZ+24].\nChroma features focus on the twelve pitch classes of Western music, capturing the harmonic core of a piece regardless of timbre or instrumentation. These features, displayed in chromagrams, condense music into pitch class profiles that highlight the harmonic and melodic structure. Chroma features are especially effective in identifying chords, key signatures, and modulations due to their alignment with the equal-tempered scale. The most recent application of chromagram is melody-guided music generation in MusicGen [CKG+24]."}, {"title": "2) Symbolic Music Format & Their Content:", "content": "2) Symbolic Music Format & Their Content: In this subsection, we introduce the low-level symbolic music formats and their informational content at both performance-level and note-level. Symbolic music formats are essential for representing musical notation and facilitating digital music processing. This subsection will cover key formats, highlighting their structures, capabilities, and limitations, especially in the context of training computer music foundation models.\na) Performance-level symbolic music repsentation: Musical Instrument Digital Interface (MIDI) protocol is a standard way of saving and transferring MIDI data for playback on different systems. Unlike MusicXML or MEI, MIDI encodes musical performance data, capturing information about note pitches, durations, velocities, and other performance aspects like dynamics, articulations, and control changes. This makes MIDI particularly suited for applications requiring precise control over musical performance and real-time interaction between devices. In ML, the detailed performance data in MIDI is invaluable for training models focused on tasks such as music generation, transcription [BDDE19], Optical Music Recognition (OMR) [SF20] and style transfer [BKWW18]. Models can learn from the nuances of human performances captured in MIDI data, enabling the generation of expressive and realistic musical outputs. MIDI's widespread use and standardisation mean there is a vast amount of data available for training purposes. However, the format's focus on performance rather than notation means it lacks explicit information about musical structure, such as key signatures, time signatures, and notation details.\nNote-level representations focus on the structural elements of music, such as pitch and rhythm, offering a simplified, human-readable format ideal for sharing musical scores. In contrast, performance-level formats like MIDI encompass additional expressive details of a piece's execution, including dynamics, musical techniques and duration/tempo variations. While note-level symbols capture the composition's essence, MIDI provides a detailed account of its performance, capturing both the notation and its expressive execution.\nb) Note-level symbolic music representation: The following paragraphs introduce the diverse note representation of symbolic music which are shown in Figure 2."}, {"title": "3) Transformed Symbolic Music Representations for Foundational Models:", "content": "3) Transformed Symbolic Music Representations for Foundational Models: Unlike audio, symbolic music files in the previous section often contain varied informational content and do not provide a straightforward way of processing through deep-learning-based models. In this section, we introduce different mid-level representations that have been applied in literature (a visualisation can be found in Figure 3), with an emphasis on foundation models.\nTokenised sequence representation: Tokenised sequence is currently the most popular way of inputting symbolic music. It can be extracted from MIDI or MusicXML, designed with flexible encodings and emphasise specific musical information. Early designs like [DHYY18] encode music as sequences comprising pitch, duration, chord, and bar position, with"}, {"title": "C. Multimodal Music Representations", "content": "C. Multimodal Music Representations\nMusic is a multidimensional artistic medium with various facets, each of which gives musical compositions a unique viewpoint. Apart from the auditory and symbolic aspects that are immediately associated with music, two important modalities in the field of multimodal learning for music have been extensively investigated:\nText: Words are essential to creating and comprehending music, including lyrics, metadata, and user comments. They provide a deeper understanding of the themes and storylines found in music by bridging the gap between musical expression and language context.\nVisual: Visual components such as album art, frames from music videos, and associated visuals not only enhance the whole listening experience but also have an aesthetic impact.\nThe subsequent sections will delve into multimodal representations of music, specifically focusing on the interaction between music tokens and their representations in both textual and visual domains.\n1) Interactions with Textual Modality: In this section, we will examine music-text representation learning. Traditional NLP methods, such as TF-IDF, will not be covered. Instead, we will briefly introduce contemporary deep learning approaches, including using pre-trained embedding from encoder models like the encoder of T5 [RSR+20a], using pre-trained language models as decoders like Llama2 [TMS+23], and training from scratch.\na) Pre-trained Embeddings from Natural Language Encoders: There are three types of pre-trained text encoders used as text conditioning in previous work: pre-trained encoders such as the encoder of T5, instructed-based encoders like FLAN-T5 [CHL+24], and pre-trained text-audio encoders like CLAP [WCZ+23a].\nAudiogen [KSP+22] is an LLM conditioned on T5 embeddings to generate learned quantised audio tokens autoregressively. Noise2Music [HPW+23b], and Mo\u00fbsai [SJS23a] generate high-quality music clips with a Latent Diffusion Model (LDM) conditioned on pre-trained T5 embeddings of text prompts. ERNIE-Music utilises ERNIE-M [OWP+21] to encode multi-lingual inputs to produce music waveforms directly from free-form text [ZPW+23] with a LDM architecture.\nMusTango [MGG+23] leverages FLAN-T5, showcasing advancements in enhancing music generation controllability and employing a high-fidelity Latent Diffusion Model (LDM) for text-guided universal music generation [LCY+23a]. Similarly, MusicGen [CKG+23a] integrates FLAN-T5 with T5 and CLAP to generate conditioning text embeddings for an audio LLM.\nMusicLDM [CWL+23b] is a diffusion model that has been proven powerful in text-conditioned music generation. It uses the pre-trained model CLAP during training to condition on audio embeddings. At inference time, CLAP is used to extract text embeddings to condition generation. Effective text-to-music generation is made possible by this method, which"}, {"title": "1) Interactions with Textual Modality:", "content": "1) Interactions with Textual Modality: In this section, we will examine music-text representation learning. Traditional NLP methods, such as TF-IDF, will not be covered. Instead, we will briefly introduce contemporary deep learning approaches, including using pre-trained embedding from encoder models like the encoder of T5 [RSR+20a], using pre-trained language models as decoders like Llama2 [TMS+23], and training from scratch.\na) Pre-trained Embeddings from Natural Language Encoders: There are three types of pre-trained text encoders used as text conditioning in previous work: pre-trained encoders such as the encoder of T5, instructed-based encoders like FLAN-T5 [CHL+24], and pre-trained text-audio encoders like CLAP [WCZ+23a].\nAudiogen [KSP+22] is an LLM conditioned on T5 embeddings to generate learned quantised audio tokens autoregressively. Noise2Music [HPW+23b], and Mo\u00fbsai [SJS23a] generate high-quality music clips with a Latent Diffusion Model (LDM) conditioned on pre-trained T5 embeddings of text prompts. ERNIE-Music utilises ERNIE-M [OWP+21] to encode multi-lingual inputs to produce music waveforms directly from free-form text [ZPW+23] with a LDM architecture.\nMusTango [MGG+23] leverages FLAN-T5, showcasing advancements in enhancing music generation controllability and employing a high-fidelity Latent Diffusion Model (LDM) for text-guided universal music generation [LCY+23a]. Similarly, MusicGen [CKG+23a] integrates FLAN-T5 with T5 and CLAP to generate conditioning text embeddings for an audio LLM.\nMusicLDM [CWL+23b] is a diffusion model that has been proven powerful in text-conditioned music generation. It uses the pre-trained model CLAP during training to condition on audio embeddings. At inference time, CLAP is used to extract text embeddings to condition generation. Effective text-to-music generation is made possible by this method, which"}, {"title": "2) Interactions with Visual Modality:", "content": "2) Interactions with Visual Modality:\na) Pre-trained Embedding from Visual Encoder: Pre-trained visual models like the Vision Transformer (ViT) [DBK+20] and Video Vision Transformer (ViViT) [ADH+21] have revolutionised tasks that integrate visual and musical elements. Utilised by systems such as M2UGen [HLSS23a], these models adapt NLP-focused transformer architectures to process image patches and spatio-temporal visual tokens.\nBoth V2Meow [SLH+23] and VidMuse [TLY+24] leverage the multimodal CLIP model [RKH+21b] as their visual encoders to effectively bridge the gap between visual and audio modalities. In V2Meow, the visual encoder extracts high-level features from video frames, which are then used to condition the auto-regressive music generation model. Similarly, VidMuse employs CLIP's visual encodings to capture both local and global visual cues. By integrating these cues through long- and short-term modelling, VidMuse produces music tracks that are not only acoustically rich but also semantically synchronised with the video content.\nb) Trained jointly from Scratch: AnyGPT [ZDY+24] uses a unified approach to process multiple modalities, including speech, text, images, and music with specialised tokenisers. For example, music uses the EnCodec model [DCSA22] to convert audio into discrete tokens, while images use the SEED tokeniser [GGZ+23] for visual data. These tokens are processed by the language model and reconstructed using de-tokenisers, ensuring the perceptual integrity of the original modalities and enabling effective content generation across diverse inputs and outputs.\nMM-Diffusion [RMY+23] introduces a joint audio-video generation framework that employs a sequential multimodal U-Net to generate aligned audio-video pairs from Gaussian noise. The integration of audio and video generation in a single framework ensures that the audio and visual elements are harmoniously aligned, resulting in more immersive and coherent multimedia content."}, {"title": "3) Vocal Music Understanding:", "content": "3) Vocal Music Understanding: Vocal music understanding is similar to speech understanding and/or speech processing, and many techniques for vocal music or singing voice are inspired by the algorithms for speech processing, such as speaker identification for singer identification and speech recognition for lyrics transcription. The algorithms for speech are typically optimised for more uniform pitch, rhythm, and dynamic articulation. Different from speech signals, the singing voice has a wider pitch range, greater variability in rhythm and pronunciation, and involves complex vocal techniques. Therefore, keeping the differences in mind is essential when applying the algorithms to singing voice.\nIn the past few years, SSL models for speech processing have proven effective in various vocal understanding tasks. There has also been an exploration into training SSL features specifically for the singing voice. This section introduces representative work in singer identification, lyrics transcription, singing transcription, vocal source separation, and lyrics interpretation.\na) Singer Identification (Singer ID): This task involves recognising the identity of a singer\u00b9\u00b9. It is often formulated as a classification problem within a dataset containing a fixed number of singers. Many approaches extract Mel-Frequency Cepstral Coefficients (MFCCs) to capture voice characteristics, paired with Gaussian Mixture Models (GMMs) [Zha03], [CLG11]. Embeddings designed for speaker recognition and verification, such as i-vector [PART13] and x-vector [SGS+18], have also been used in singer ID [Kru14], [ZWCX22]. With the advancement of neural networks, it has become possible to extract latent features from general audio representations, such as spectrograms [LN19], [ZQY+21], using CNNs and CRNNs. Recent methods propose using singing source separation to suppress background music [SDL19] or data augmentation [HCF+20]. These approaches have"}, {"title": "B. Music Generation", "content": "B. Music Generation\n1) Symbolic Music Generation: In the area of symbolic music generation", "categories": "generating from scratch or based on given conditions like chords", "scratch.": "Some early works explored monophonic/melody music generation from scratch as well as polyphonic music. In early research stage", "SSBTK16": "trained LSTM networks on 23k music pieces represented in ABC notation in two approaches: one predicts characters based on the previous 50 characters", "HN17": "generates melodies through a RNN-based generative model while allowing user-defined positional constraints. Besides RNNs", "YZWY17": "and VAE-based models", "RER+18a": "to generate music melodies.\nFor polyphonic music generation", "DHYY18": "was the first to propose models to generate multi-track symbolic music using the jamming model", "GYY19": "employs a self-attention mechanism to extract both spatial and temporal features", "DCD+23": "introduced a Transformer-based multitrack music representation that accommodates a diverse array of instruments while keeping the sequence length short", "al.[MEHS21": "proposed a method to train diffusion models using symbolic music data", "YLW+22": "introduced a fine-coarse-grained attention mechanism to handle the challenge of long music sequences. In this approach", "condition.": "Generating music from scratch can often result in music that follows a certain pattern, limiting the creativity of music generation. By incorporating various types of input conditions, such as chords, melody tracks, lyrics, and text descriptions, not only can users interact with the music generation process more dynamically, but they also gain higher and more fine-grained control over the output. Consequently, there is a growing group of research focused on generating symbolic music under different input conditions.\nConditioned on chord sequences, MIDINet [YCY17", "CPH+21": "introduced a Transformer model conditioned on chords for generating K-POP melodies. This model generates rhythm and pitch components while adhering to a specified chord progression. MelodyDiffusion [LS23a", "WLS23": "can generate a four-part chorale based on given melody and chord progression.\nMelody harmonisation refers to crafting a chord progression that complements a given melody. This progression must harmonise with the melody while also aligning with its rhythmic pattern. Lim et al. [LRL17", "WYW+24": "is a system designed for melody harmonisation with controllable harmonic density and rhythm. It features an extensive vocabulary of 1,462 chord types, enabling it to generate chord progressions with varying harmonic density for a given melody. Not limited to melody, Multi-Track Music Machine (MMM) [EP20"}]}