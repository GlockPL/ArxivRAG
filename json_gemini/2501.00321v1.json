{"title": "OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning", "authors": ["Ling Fu", "Biao Yang", "Zhebin Kuang", "Jiajun Song", "Yuzhe Li", "Linghao Zhu", "Qidi Luo", "Xinyu Wang", "Hao Lu", "Mingxin Huang", "Zhang Li", "Guozhi Tang", "Bin Shan", "Chunhui Lin", "Qi Liu", "Binghong Wu", "Hao Feng", "Hao Liu", "Can Huang", "Jingqun Tang", "Wei Chen", "Lianwen Jin", "Yuliang Liu", "Xiang Bai"], "abstract": "Scoring the Optical Character Recognition (OCR) capabilities of Large Multimodal Models (LMMs) has witnessed growing interest recently. Existing benchmarks have highlighted the impressive performance of LMMs in text recognition; however, their abilities in certain challenging tasks, such as text localization, handwritten content extraction, and logical reasoning, remain underexplored. To bridge this gap, we introduce OCRBench v2, a large-scale bilingual text-centric benchmark with currently the most comprehensive set of tasks (4\u00d7 more tasks than the previous multi-scene benchmark OCRBench), the widest coverage of scenarios (31 diverse scenarios including street scene, receipt, formula, diagram, and so on), and thorough evaluation metrics, with a total of 10,000 human-verified question-answering pairs and a high proportion of difficult samples. After carefully benchmarking state-of-the-art LMMs on OCRBench v2, we find that 36 out of 38 LMMs score below 50 (100 in total) and suffer from five-type limitations, including less frequently encountered text recognition, fine-grained perception, layout perception, complex element parsing, and logical reasoning. The benchmark and evaluation scripts are available at https://github.com/Yuliang-Liu/MultimodalOCR.", "sections": [{"title": "1. Introduction", "content": "The emergence of Large Language Models (LLMs) [1, 8, 101] has greatly improved the understanding and generation of structured text. However, in reality, much of the textual content is unstructured; it appears within images, videos, and other non-textual media in varied positions, orientations, and shapes. The need for processing such unstructured content leads to the study of Large Multimodal Models (LMMs) [5, 53, 139] that extend the text-only LLMs to additional modalities. By pretraining on multimodal data, LMMs acquire the zero-shot ability to interpret across diverse media such as recognizing and understanding complex visual scene text [59]. Such capability represents a significant advancement over standard Optical Character Recognition (OCR), because LMMs not only spot text but also interpret its semantic relevance to a scene."}, {"title": "2. Related Work", "content": "We review work related to OCR-enhanced LMMs and benchmarks for text-centric LMMs."}, {"title": "OCR-Enhanced LMMs.", "content": "Inspired by the success of LLMs, visual encoders are integrated into LLMs to create LMMs capable of processing both visual and textual information. Early LMMs exhibit strong zero-shot OCR capabilities, which motivates further exploration into text-centric LMMs. One pioneering approach, LLaVAR [131], enhances OCR abilities by training LLaVA [53] on text-centric datasets. mPLUG-DocOwl [118] constructs an instruction-tuning dataset with visual-text understanding to improve the model's performance. However, these early methods are restricted to low-res inputs, limiting their ability to recognize dense and small text. To address this limitation, attention has shifted to increasing the input resolution of LMMs. DocPedia [25] processes high-res images in the frequency domain without increasing the input sequence length. UReader [119] crops an image into multiple sub-images, thereby achieving fine-grained perception of high-res text. On the contrary, LayoutLLM [67] adopts the LayoutLMv3 [33] as the vision encoder to process the high-res images. As the resolution of inputs increases, so do the challenges related to computational efficiency. To tackle this issue, TextMonkey [59] proposes a Token Resampler to compress redundant visual feature tokens, while mPLUG-DocOwl 1.5 [31] introduces an H-Reducer to reduce the number of image tokens. As these models advance, they have achieved remarkable results on established benchmarks. Nevertheless, challenges remain unsolved in certain key areas such as text localization, entity extraction, and logical reasoning."}, {"title": "Benchmarks for Text-Centric LMMs.", "content": "To evaluate LMMs, developing a comprehensive benchmark is essential. Previous effort has focused on creating scenario-specific benchmarks to assess LMMs in particular contexts. For example, DocVQA [71] is designed to evaluate the document comprehension abilities of LMMs, while ChartQA [69] is tailored to chart interpretation skills. Similarly, Infographics VQA [73] is dedicated to assessing the understanding of infographic images. Additionally, TextVQA [93] aims to evaluate text comprehension in real-world scenes. To further investigate the robustness of the model, some methods expand the scope of evaluation scenarios. OCRBench [58] introduces a holistic evaluation framework that covers five core OCR tasks. CONTEXTUAL [103] is developed with context-sensitive instructions. SEED-Bench-2-Plus [46] encompasses a wide spectrum of text-rich images from various sources, including web content, maps, and charts. To provide a more thorough assessment, some benchmarks design multiple evaluation tasks within a specific scenario. TableVQA-Bench [42] first focuses on VQA tasks in the table domain. MMTab [134] and ComTQA [132] then extend the task scope, including table detection, structure recognition, and table querying. Moreover, ChartY [9], ChartX [111], and MMC [51] evaluate LMMs in chart understanding through tasks such as chart information extraction and reasoning. In this work, we focus on establishing a new benchmark called OCRBench v2, which contains more tasks than previous benchmarks and provides a systematic evaluation framework to reveal the limitations of LMMs in diverse text-rich environments."}, {"title": "3. OCRBench v2 Benchmark", "content": "In this section, we introduce OCRBench v2 from four aspects, including task, annotation, statistics, and evaluation."}, {"title": "3.1. Task Description", "content": "To provide a comprehensive evaluation framework for text-reading tasks, we categorize OCR capabilities into eight core areas, each encompassing specific sub-tasks that address various aspects of text comprehension and interpretation. Figure 3 demonstrates representative examples for each task, which showcases both visual inputs and corresponding instructions. Detailed descriptions of these core capabilities are as follows."}, {"title": "Text Recognition.", "content": "This fundamental capability focuses on perceiving textual content. The related tasks include (fine-grained) text recognition and full-page OCR."}, {"title": "Text Referring.", "content": "Determining the location of texts is necessary for real-world OCR applications. OCRBench v2 evaluates this ability with text grounding and VQA with position tasks, requiring LMMs to localize text positions accurately."}, {"title": "Text Spotting.", "content": "Text spotting is a widely studied OCR task that requires models to output both the location and content of text. We consider it a distinct capability due to this unique output format."}, {"title": "Relation Extraction.", "content": "Given that texts are often densely arranged in images, the ability to extract and map visual components is essential. This capability is assessed through key information extraction, key information mapping, and handwritten content extraction."}, {"title": "Element Parsing.", "content": "Apart from text recognition, LMMs face the need of parsing complex elements for downstream applications. This ability is evaluated via table parsing, chart parsing, document parsing, and formula recognition."}, {"title": "Mathematical Calculation.", "content": "Mathematical calculation abilities are essential for LMMs to address tasks that require numerical reasoning. Hence, text counting is introduced to assess the textual perception ability by counting the number of text instances. Additionally, we enhance the original math QA data by rendering textual questions into images, accompanied by geometric figures."}, {"title": "Visual Text Understanding.", "content": "To tackle sophisticated tasks involving human interaction, LMMs need to comprehend semantic information of texts, a capability we term visual text understanding. This ability is evaluated by document classification and diagram QA. Additionally, we include basic VQA instructions where answers are located directly within the image, which refers to cognition VQA."}, {"title": "Knowledge Reasoning.", "content": "Certain tasks require reasoning based on world knowledge and involve complex inference procedures, including science QA, APP agent interactions, ASCII art classification, and text translation. Further, we collect some complicated VQA instructions, where answers are not directly visible in images, termed reasoning VQA."}, {"title": "3.2. Annotation Curation", "content": "In this section, we introduce annotation curation in three parts, including dataset collection, instruction formulation, and manual verification."}, {"title": "Dataset Collection.", "content": "To curate diverse data for OCRBench v2, we manually harvest and screen 81 text-rich academic datasets. More details about the data sources can be found in Appendix B. To ensure diverse scenarios coverage, we also supplement them with additional private data. In all, our dataset comprises 31 typical scenarios, and the number of images in each scene is displayed in Table 9."}, {"title": "Instruction Formatting.", "content": "To convert existing annotations into the LMM-compatible instruction format, we design specific prompts for each task. For complex tasks such as document parsing that require structured output, we include a format example to minimize the impact of instruction-following ability and focus the evaluation on OCR capabilities. Additionally, considering the distinct training strategies for localization tasks in LMMs, we standardize the coordinates by normalizing them with image sizes and by scaling to the range of [0, 1000]. Such a standardization is explicitly specified in the related prompts. For unlabeled data, we manually annotate them according to task requirements."}, {"title": "Manual Verification.", "content": "To ensure data quality, we manually review all instructions and correct approximately 1% annotation errors."}, {"title": "3.3. Statistics of OCRBench v2", "content": "Here we present the statistics of OCRBench v2, including the proportion of eight task categories, the OCR-related statistics, and the measurement of prompt quality. More details on statistics can be found in Appendix D."}, {"title": "Capabilities Distribution.", "content": "The proportion of eight capabilities in OCRBench v2 is shown in Figure 4. The proportions across different categories are relatively balanced."}, {"title": "OCR-Related Statistics.", "content": "We further count the distribution of line-level OCR results of 7, 400 English and 2, 600 Chinese images in Figure 5(a) and Figure 5(b), respectively. The average number of line-level OCR results per category is shown in Figure 5(c). These statistics demonstrate that the text information is sufficiently rich in OCRBench v2."}, {"title": "Prompt Quality.", "content": "We also compare the metrics of Average Entropy, Type-Token Ratio, and Average Variability Index of the questions between OCRBench v2 and OCRBench in Figure 6. Compared with OCRBench, OCRBench v2 shows greater Average Entropy, indicating higher unpredictability and diversity in its questions, which challenges the LMMs to tackle more diverse scenarios. It also exhibits a higher Type-Token Ratio, reflecting greater lexical variety and less redundancy in its question formulation. Additionally, the Average Variability Index of OCRBench v2 is significantly higher, suggesting a wider range of question types and structures, offering a more thorough performance evaluation of LMMs."}, {"title": "4. Results and Discussions", "content": "Here we first benchmark state-of-the-art LMMs on OCRBench v2, presenting the quantitative analysis of their OCR capabilities. We then investigate several critical factors influencing OCR performance, including the resolution setting of visual encoder, pre-provided OCR information, and the direct integration between OCR information and language models. Further, we summarize key findings of current limitations and challenges for LMMs. All results are presented as percentages."}, {"title": "4.1. Baselines", "content": "We evaluate a total of 31 open-source LMMs including InternVL series [12, 13], Qwen-VL series [5, 105], GLM-4v-9B [28], Monkey [48], TextMonkey [59], LLaVANext-8B [52], XComposer2-4KHD [21], EMU2-chat [95], mPLUG-Owl3 [120], CogVLM-chat [106], Deepseek-VL-7B [62], Molmo-7B [17], MiniCPM-V-2.6 [117], TextHarmony [133], VILA1.5-8B [60], LLaVAR [131], DocOwl2 [32], UReader [119], Yi-VL-6B [122], Janus-1.3B [110], Cambrian-1-8B [100], LLaVA-OV-7B [47], Eagle-X5-7B [92], Idefics3-8B [44], Ovis1.6-3B [66], and Pixtral-12B [2] with the publicly available checkpoints. Further, some cutting-edge commercial models are also tested, including GPT4V [78], GPT40 [79], GPT40-mini [77], Gemini-Pro [99], Claude3.5-sonnet [3], GLM-4V-Plus [28], and Step-1V [94]."}, {"title": "4.2. Main Results", "content": "The main evaluation results are shown in Table 2, Table 3, and Figure 7. We observe that, while current LMMs perform relatively well on some basic capabilities such as text recognition and visual text understanding, most LMMs achieve low scores in other capabilities, such as text spotting and element parsing, mostly below 50. In particular, some LMMs show significant limitations in text spotting capabilities, failing to precisely locate and recognize the texts. Additionally, LMMs demonstrate inadequate abilities in element parsing and mathematical calculation, which are crucial for complicated tasks like document analysis and mathematical reasoning. Besides, after comparing the performance of LMMs on visual text understanding and knowledge reasoning capabilities, we find that they perform poorly in knowledge reasoning, despite being provided with multiple-choice options. This suggests the deficiency of LMMs in logical reasoning."}, {"title": "4.3. Potential Factors Affecting OCR Capabilities", "content": "To provide additional insights, we also conduct experiments to explore the potential factors that may affect the performance of LMMs on OCRBench v2."}, {"title": "High-Res Visual Encoders.", "content": "Since text often appears small in images, the resolution setting of the visual encoder could be a key factor affecting the text perception ability of LMMs [48]. Here we change the input resolution of the LMMs and observe the performance changes. In particular, InternVL2-8B is chosen, and the resolution setting includes 448, 896, and dynamic. Indeed, when the input resolution increases from 448 to 896, the performance increases by 4.1%."}, {"title": "Pre-provided OCR Information.", "content": "To study the impact of OCR information, we use PaddleOCR2 to pre-extract OCR results and incorporate them with prompts. We observe that adding OCR information does not help much. This suggests that OCRBench v2 evaluates LMMs capabilities across multiple dimensions, rather than solely focusing on text recognition abilities."}, {"title": "Connection Between OCR and LLMs.", "content": "We further explore a direct pipeline by first extracting OCR information and then by feeding it directly into Qwen2.5. Unlike LMMs, this pipeline separates OCR and language modeling into distinct stages. The results suggests that Qwen2VL-8B outperforms Qwen2.5 with OCR information, demonstrating LMMs' remarkable ability to incorporate both textual and visual features efficiently."}, {"title": "4.4. Main Findings", "content": "Here we further highlight some interesting findings from the experimental results:"}, {"title": "Finding 1.", "content": "Limited recognition on less frequently encountered texts. Despite LMMs achieving high performance on text recognition, they can perform poorly when facing with less frequently encountered texts, such as dot matrix texts and mathematical formulas. This performance gap highlights the continuing challenges LMMs face in real-world text recognition."}, {"title": "Finding 2.", "content": "Limited fine-grained spatial perception. Given that both closed- and open-source LMMs perform poorly in tasks such as text referring and text spotting in OCRBench v2, the current LMMs still lack the capability for accurate text localization."}, {"title": "Finding 3.", "content": "Insufficient layout perception. While LMMs achieve good performance on basic text recognition, they struggle with complex layouts such as overlapping or rotated texts. For example, GPT-40 fails to detect all characters in overlapping handwritten text and misrecognizes numbers in 90\u00b0 rotated images, revealing LMMs' limitations in handling texts with complex layouts."}, {"title": "Finding 4.", "content": "Weak analytics for complex elements. When facing complex visual elements like charts and formulas, LMMs struggle to parse them into structured data formats, hindering their effectiveness in downstream applications such as document digitalization."}, {"title": "Finding 5.", "content": "Limited logical reasoning. While LMMs demonstrate basic multimodal reasoning abilities, they struggle with complex scenarios involving mathematical problems or complicated textual reasoning, highlighting the room for improvement in their reasoning capabilities."}, {"title": "5. Conclusion", "content": "In this work, we introduce OCRBench v2, an improved benchmark for assessing the OCR capabilities of LMMs. By studying 38 representative LMMs across 23 OCR tasks, we not only reveal some defects of the current LMMs but also explore factors affecting the OCR performance of LMMs. We hope OCRBench v2 could facilitate future research on improving the OCR capabilities of LMMs."}, {"title": "Limitations.", "content": "One challenge we encountered is that LMMs sometimes produce responses that deviate from the given instructions, making it difficult to extract the desired answers. In future work, we plan to develop a more objective assessment framework to address this issue."}, {"title": "Appendix", "content": "This appendix mainly contains the following contents:\n\u2022 Section A: Comparison experiments between LMMs and some text-centric expert models.\n\u2022 Section B: Data source of OCRBench v2.\n\u2022 Section C: Task definition of OCRBench v2.\n\u2022 Section D: More statistics of OCRBench v2.\n\u2022 Section E: Evaluation metrics of OCRBench v2.\n\u2022 Section F: Visualization samples of task examples.\n\u2022 Section G: Visualization samples of failure cases."}, {"title": "A. Comparison with LMMs and Expert Models", "content": "Comparison with text recognizers. We compare LMMs with several representative scene text recognizers, including CRNN [87], ABINet [23], ASTER [89], MASTER [63], and SVTR [22], on the text recognition task. The weights of these models are loaded from mmocr\u00b3. The results are shown in Table 6, where we selected the top three LMMs with the best performance, including Qwen2VL-8B [105], GPT4V [78] and Step-1V [94]. The results demonstrate that LMMs exhibit remarkable text recognition capabilities, validating our motivation to evaluate LMMs on more challenging OCR-related tasks."}, {"title": "Comparison with text spotters.", "content": "We also compare LMMs with ABCNet series [56, 57] and TESTR [130] on the text spotting task. The weights of the ABCNet series are loaded from mmocr and TESTR is loaded from the official checkpoint fine-tuned with TotalText [15]. The results are shown in Table 7. We observe that although LMMs demonstrate promising capabilities in text recognition, there remains significant potential for improvement in the text spotting task."}, {"title": "Comparison with GOT.", "content": "We noticed a recent work, GOT [109], that can parse the textual elements within images. We conduct comparison experiments between GOT and some representative LMMs, and the results are shown in Table 8. We observe that LMM shows advantages in general text recognition, while GOT demonstrates better performance in document parsing task."}, {"title": "B. Data Collection", "content": "Text Recognition. The data for text recognition task are sampled from ICDAR2013 [38], SVT [91], IIIT5K [75], ICDAR2015 [39], SCUT-CTW1500 [55], COCO-Text [102], CUTE80 [84], TotalText, SVTP [83], WordArt [112], NonSemanticText [58], IAM [68], ORAND-CAR-2014 [19], HOST [108] and WOST [108]. Meanwhile, CAPTCHA(Completely Automated Public Turing Test to Tell Humans Apart) images used in text recognition are sourced from a CAPTCHA dataset and a number CAPTCHA dataset. And dot matrix images in the text recognition task are obtained from the web page.\nFine-grained Text Recognition. In the fine-grained text recognition task, the images are sampled from the testset of Fox [50], Totaltext, COCO-Text, CTW1500 [127] and ICDAR2015. For the Fox dataset, we used the original annotations provided. For the other four datasets, we performed manual annotation.\nFull-page OCR. The data sources for full-page OCR task"}, {"title": "C. Task Definition", "content": "In this section, we introduce the definition of each task, and the visualizations for each task can be found in Section F.\nText Recognition. Text recognition refers to the fundamental OCR ability on text image patches, which asks LMMs to read the text content. To comprehensively evaluate LMMs' text recognition ability across diverse scenarios, our collection incorporates various text types, including regular text, irregular text, artistic text, handwriting text, digit string text, non-semantic text, occluded text, doc matrix text, and CAPTCHA text.\nFine-grained Text Recognition. This task requires LLMs to read and comprehend textual content within the given region. It evaluates LLMs' fine-grained perception capabilities in understanding text in natural scenes and documents.\nFull-page OCR. Full-page OCR [50] task requires LMMs to extract and recognize all text content from the given images. Converting text into digital format facilitates subsequent processing and analysis of text images.\nText grounding. In this task, users would provide a text string and require LMMs to locate its specific location, evaluating LMMs' fine-grained perception capabilities.\nVQA with Position. For VQA with position task, LMMs need to not only respond to the question but also provide the exact position coordinates that directly correspond to the answer. We ask LMMs to output both information in JSON format for convenient evaluation, and the coordinates are required to be normalized with image sizes and scaled to the range of [0, 1000].\nText Spotting. Text spotting task needs LMMs to output the localization and content of all appeared text simultaneously. Due to the interference of background elements and the large number of text instances, this task demands high fine-grained perception capabilities from the model. Besides, the coordinates are required to be normalized with image sizes and scaled to the range of [0, 1000].\nKey Information Extraction. The key information extraction task is to extract the necessary information from densely arranged text. In this task, we provide some desired entities as keys and demand LMMs to output the corresponding values to form the output JSON string.\nKey Information Mapping. In this task, we provide a set of entity keys and their corresponding values in the prompt. The LMMs are then asked to match and pair these keys with their respective values into groups.\nHandwritten Content Extraction. To investigate the information extraction capabilities of LMMs in educational scenarios, we collect some Chinese examination papers, containing both printed question text and handwritten student responses. There are four types of questions in these examination papers, including single-choice, multiple-choice, true or false, and brief response questions. The prompts require LMMs to extract the handwritten content for specific questions.\nTable Parsing. Table parsing task requires LMMs to parse the given table into structured text, including Markdown and HTML format.\nChart Parsing. Apart from tables, charts can also be converted to structured information. In this task, LLMs are required to transform visual charts into JSON format.\nDocument Parsing. In the document parsing task, both text and the complex elements, including chart, table, and formula, are required to parse.\nFormula Recognition. This task asks LMMs to recognize the given formula in the latex format. The collection includes mathematical and chemical formulas.\nMath QA. Math QA task evaluates the LMMs' mathematical calculation ability. In particular, we render the mathematical problem description and related figures into images and ask LMMs to answer the questions within the images.\nText Counting. Text counting task is built to evaluate the quantity property perceiving ability of LMMs, including the character frequency in words and the word counting in the given image.\nCognition VQA. In OCRBench v2, we split text-centric VQA instructions into cognition VQA and Reasoning VQA based on whether the answers can be directly found in the images. Cognition VQA task refers to the instructions where answers are explicitly present in the given image. This task evaluates the fundamental text-centric question-answering ability based on visual content.\nDiagram QA. In the diagram QA task, LMMs need to respond to the question about the given diagrams, reflecting LMMs' ability to understand the relationship between the visual elements.\nDocument Classification. Document classification task asks LMMs to classify the category of the given document image. The included categories are letters, forms, emails, handwritten documents, advertisements, scientific reports, scientific publications, specifications, file folders, news articles, budgets, invoices, presentations, questionnaires, resumes, and memos.\nReasoning VQA. In reasoning VQA tasks, the answers often do not directly appear in the image. This forces LMMs to perform logical reasoning to respond to questions based on visual information.\nScience QA. In the Science QA task, LMMs are required to respond to the scientific problem. We use PaddleOCR 15 to extract text from the collected images and filter out those with fewer than four OCR results. Additionally, when extra subject-related knowledge is provided by the source, we incorporate them by rendering them into the images.\nAPP Agent. For the APP agent task, LMMs need to understand the relationship between textual content, icons, and world knowledge to respond to the question from the user, simulating the real-world application scene.\nASCII Art Classification. We incorporate a recent image classification task that uses images composed purely of ASCII characters [36]. This task is included in OCR-Bench v2 to evaluate LMMs' ability to assess LMMs' pattern recognition and visual abstraction abilities.\nText Translation. In the text translation task, LMMs need to execute translation between Chinese and English texts, evaluating LMMs' semantic understanding abilities."}, {"title": "", "content": "if $\\frac{N_{3}}{N_{2}}$"}, {"title": "", "content": "Recall = $\\frac{N_{3}}{N_{1}}$"}, {"title": "", "content": "$\\frac{2 * Precision * Recall}{Precision + Recall}$"}, {"title": "", "content": "BLEU = BP $\\cdot$ exp($\\sum_{n=1}^{N} w_{n}$log $p_{n}$),"}, {"title": "", "content": "BP = $\\begin{cases} 1 &\text{if } L_{p} \\geq L_{g}\\ e^{(1-\\frac{L_{g}}{L_{p}})} & L_{p} < L_{g} \\end{cases}$"}, {"title": "", "content": "TEDS($T_{1}$, $T_{2}$) = 1 \u2013 $\\frac{TED(T_{1},T_{2})}{max(|T_{1}|, |T_{2}|)},$"}, {"title": "", "content": "IOU ($B_{1}$, $B_{2}$) = $\\frac{Intersect(B_{1},B_{2})}{Union(B_{1},B_{2})},$"}, {"title": "", "content": "score = $\\begin{cases} 0&\\text{Cpred $\\leq$ 0}\\\\ 1 - | \\frac{C_{pred}-C_{gt}}{C_{gt}}| & \\text{0 < $C_{pred}$ < 2 *$C_{gt}$} \\\\ 0 & \\text{Cpred $\\geq$ 2 *$C_{gt}$} \\end{cases}$"}]}