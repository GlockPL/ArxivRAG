{"title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs", "authors": ["Zixuan Weng", "Xiaolong Jin", "Jinyuan Jia", "Xiangyu Zhang"], "abstract": "Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. Inspired by psychological foot-in-the-door principles, we introduce FITD, a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical transgressions. Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses. Extensive experimental results on two jailbreak benchmarks demonstrate that FITD achieves an average attack success rate of 94% across seven widely used models, outperforming existing state-of-the-art methods. Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn interactions.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have been extensively deployed in various domains and products (Li et al., 2024c), ranging from coding assistance (Guo et al., 2024a) to educational tools (Wang et al., 2024c). As these models become more integral to daily life, ensuring AI safety and preserving alignment with human values have become increasingly important (Liu et al., 2024a). A critical challenge lies in \"jailbreak\", wherein adversarial prompts bypass built-in safeguards or alignment measures, causing the model to generate disallowed or harmful output (Zou et al., 2023; Jin et al.).\nEarly jailbreak approaches typically rely on carefully engineered single-turn prompts that coax the model to reveal restricted malicious information (Greshake et al., 2023). By embedding malicious instructions within complex context blocks or intricate role-playing scenarios, attackers exploit weaknesses in the model alignment policy (Ding et al., 2024). However, attackers have recently shifted from single-turn to multi-turn paradigms, where each subsequent user query adapts or builds upon the conversation history (Li et al., 2024a). Although some multi-turn jailbreak methods, such as ActorAttack (Ren et al., 2024c) and Crescendo (Russinovich et al., 2024), have demonstrated the potential of multi-round dialogues in obscuring malicious intent, they usually depend on heavily handcrafted prompts or complex agent design. Besides, their overall Attack Success Rate (ASR) remains limited, often requiring significant prompt engineering expertise."}, {"title": "2 Related Work", "content": "Jailbreak Large language models jailbreak can be broadly categorized into single-turn and multi-turn approaches, with different levels of model access. Black-box single-turn attacks use input transformations to bypass safety constraints without accessing model internals, such as encoding adversarial prompts in ciphers, low-resource languages, or code (Yuan et al., 2024; Deng et al., 2023b; Lv et al., 2024; Ren et al., 2024a; Chao et al., 2023; Wei et al., 2023; Li et al., 2023; Liu et al., 2024a). In contrast, white-box single-turn attacks exploit access to model parameters using gradient-based optimization to generate adversarial inputs or manipulate text generation configurations (Zou et al., 2023; Huang et al., 2024b; Zhang et al., 2024a; Jones et al., 2023; Guo et al., 2024b). Meanwhile, multi-turn jailbreaks introduce new challenges by exploiting dialogue dynamics. A common approach decomposes harmful queries into a series of innocuous sub-questions, progressively leading the model towards unsafe responses (Li et al., 2024b; Jiang et al., 2024; Zhou et al., 2024b). Automated red teaming has also been explored, in which LLMs are used iteratively to investigate and expose weaknesses (Jiang et al., 2025). To mitigate such threats, various defense mechanisms have been proposed, including perturbation or optimization techniques (Zheng et al., 2024; Zhou et al., 2024a; Mo et al., 2024; Liu et al., 2024b), safety response strategy (Zhang et al., 2024b; Li et al., 2024d; Wang et al., 2024b; Zhang et al., 2024c), and jailbreak detection (Han et al., 2024; Inan et al., 2023), aim to neutralize adversarial prompts before execution (Inan et al., 2023; Zou et al., 2024). Notably, multi-turn attack Crescendo (Russinovich et al., 2024) and ActorAttack (Ren et al., 2024c) incrementally steer seemingly benign queries toward harmful content but are constrained by their reliance on fixed, human-crafted seed prompts and limited overall ASR. However, different from their work, our work uses the foot-in-the-door effect to"}, {"title": "3 Method", "content": "gradually erode an LLM's alignment while analyzing the phenomenon of self-corruption in LLMs."}, {"title": "3.1 Inspiration from Psychology: The Foot-in-the-Door Phenomenon", "content": "Our method FITD draws inspiration from the \"foot-in-the-door\" phenomenon in psychology. According to this principle, once individuals perform or agree to a minor (often unethical) act, they are more likely to proceed with more significant or harmful acts afterward (Freedman and Fraser, 1966; Cialdini, 2001). For example, in a classic study, participants who first displayed a small sign supporting safe driving were subsequently much more inclined to install a much larger, more obtrusive sign (Freedman and Fraser, 1966). This gradual escalation of compliance, \"from small to large\", has also been observed in other forms of unethical or harmful behavior (Festinger, 1957), showing that the initial \"small step\" often lowers psychological barriers for larger transgressions. Once a small unethical act has been justified, individuals become increasingly susceptible to more severe transgressions.\nBased on these insights, we hypothesize that LLMs' safety mechanisms might be vulnerable to a gradual escalation strategy. If LLMs respond to a prompt containing slightly harmful content, subsequent queries that escalate this harmfulness will have a higher chance of producing disallowed responses. This idea underlies our FITD method, which progressively coaxes a target model to produce increasingly malicious output despite its built-in safety mechanisms."}, {"title": "3.2 Overview", "content": "Building on the foot-in-the-door perspective, we design a multi-turn jailbreak strategy FITD. In each turn, the target model is prompted with content that is just marginally more harmful or disallowed than the previous turn, encouraging the model to produce a correspondingly more harmful output. This progression method is designed to exploit the model's own responses as a guiding force to bypass its safety alignment or content filters. The core novelty lies in using (i) the model's own prompts and responses as stepping stones for further escalation and (ii) two auxiliary modules, SlipperySlopeParaphrase and Re-Align, to handle instances when the model refuses or produces outputs misaligned with the intended maliciousness. Additionally, we conduct an in-depth analysis of the foot-in-the-door self-corruption phenomenon in LLMs.\nFirst, we initialize a sequence of escalated queries $q_1, q_2,..., q_n$ based on a malicious query $q^*$. Then in each turn, we append the current query $q_i$ to the chat history and obtain the model's response $r_t$. If $r_t$ has no refusal, we proceed; otherwise, we check how well the model's previous response aligns with its prompt. Depending on this check, we either insert an intermediate \"bridging\" query via SlipperySlopeParaphrase or Re-Align the target model's last response $r_{last}$. Over multiple iterations, the process gradually pushes the model to produce more detailed and harmful content."}, {"title": "3.3 FITD", "content": "As shown in Algorithm 1, given a target model $M$, a malicious \u201cgoal\u201d query $q^*$, and the malicious level $n$, we initialize a sequence of escalated"}, {"title": "3.3.1 Re-Align", "content": "If the model's previous query $q_{last}$ and response $r_{last}$ in chat history $H$ is misaligned-for instance, it remains too benign or partially refuses even though the query is not malicious-then we invoke RE-ALIGN. Building on the psychological insight that once individuals have justified a minor unethical act, they become increasingly susceptible to more severe transgressions (Freedman and Fraser, 1966), RE-ALIGN aims to \"nudge\" the model to produce a response more closely aligned with the malicious intent of $q_{last}$. Specifically, we employ a predefined alignment prompt $P_{align}$ via getAlignPrompt($q_{last}$,$r_{last}$), appending it to $H$ before querying the model $T$ again. The alignment prompt explicitly points out inconsistencies between the last query $q_{last}$ and response $r_{last}$ while encouraging the model to stay consistent with multi-turn conversation. For example, if $r_{last}$ is too cautious or is in partial refusal, $P_{align}$ will suggest that the model refines its response to better follow the implicit direction. Therefore, this procedure progressively aligns $q_{last}$ and $r_{last}$, thereby furthering the self-corruption process. The details are described in Appendix A.2.5"}, {"title": "3.3.2 SlipperySlopeParaphrase", "content": "When a refusal occurs and the last response $r_{last}$ remains aligned with its query $q_{last}$, we insert a bridge prompt $q_{mid}$ to ease the model into accepting a more harmful request."}, {"title": "3.3.3 Putting It All Together", "content": "Through gradual increases in maliciousness, we systematically steer the target model from benign or slightly harmful content to overtly disallowed"}, {"title": "4 Experiment", "content": "4.  1 Experimental Setup\nTarget Models We evaluate FITD on seven widely used LLMs, including both open-source and proprietary models. The open-source models comprise LLAMA-3.1-8B-Instruct (Dubey et al., 2024), LLaMA-3-8B-Instruct, Qwen2-7B-Instruct (Bai et al., 2023), Qwen-1.5-7B-Chat, and Mistral-7B-Instruct-v0.2 (Jiang et al., 2023)."}, {"title": "4.2 Main Results", "content": "FITD is more effective than baseline attacks. Table 1 shows ASRs of FITD and various jailbreak methods on different LLMs across JailbreakBench and HarmBench, where each cell contains two values: the ASR on JailbreakBench (left) and HarmBench (right). In Table 1, FITD requires an average of 16 queries per malicious question for each target model. Among single-turn attacks, ReNeLLM achieves the highest ASR on average, significantly outperforming other single-turn baselines. This suggests that leveraging LLMs for prompt rewriting and scenario nesting is a highly effective approach for jailbreak attacks. Meanwhile, CodeAttack variants also demonstrate competitive performance. However, DeepInception and CodeChameleon exhibit lower ASR, with performance dropping below 34% on average, indicating their limited generalizability against more robust"}, {"title": "4.3 Ablation Study", "content": "To evaluate different components in our FITD jailbreak method, we conduct an ablation study by systematically removing three key mechanisms: response alignment (Re-Align), alignment prompt $P_{align}$, and SlipperySlopeParaphrase. The results in Figure 3b demonstrate the significance of these components for a high ASR across various models.\nFirst, removing all three mechanisms leads to a significant decline in ASR (w/o ReAlign, $P_{align}$, SSP). On LLaMA-3.1, the ASR drops from 92% to 75%, while on LLaMA-3, it decreases from 98% to 59%. Similar declines are observed across other models, with Qwen-2 and Qwen-1.5 dropping to 76% and 80%, respectively. These results suggest that the interplay of response alignment, prompt alignment, and paraphrasing is critical to maintaining the effectiveness of FITD. Without these components, the attack becomes substantially less effective, particularly on models with stronger alignment guardrails.\nSecond, when the response alignment and"}, {"title": "5 Conclusion", "content": "In this work, we introduced FITD, a multi-turn jailbreak strategy inspired by the psychological foot-in-the-door effect. By progressively escalating the malicious intent of user queries through intermediate prompts via SlipperySlopeParaphrase and ReAlign, our method achieves a 94% attack success rate on average across multiple models. Our findings reveal a major weakness in current AI safety measures: LLMs can be manipulated into self-corruption, where their responses gradually shift toward harmful content by themselves. Future work could explore why LLMs have the foot-in-the-door self-corrupt phenomenon that their responses shift during adversarial interactions and FITD effect in multimodality Language Models."}, {"title": "6 Ethical Considerations", "content": "This study aims to improve AI safety by identifying weaknesses in LLM alignment. While our method bypasses safeguards, our goal is to help strengthen Al defenses, not to enable misuse.\nWe recognize the risks of publishing jailbreak techniques but believe that transparent research is necessary to develop better protections. Responsible disclosure ensures that AI developers can proactively address these vulnerabilities.\nAl developers must build stronger safeguards against adversarial attacks. Adversarial training, real-time monitoring, and collaboration between researchers, industry, and policymakers are essential to keeping AI systems secure, reliable and beneficial."}, {"title": "7 Limitation", "content": "First, we need more in-depth analysis of self-corruption and the Foot-In-The-Door (FITD) phenomenon remains preliminary. Self-corruption occurs when an LLM gradually deviates from its initial aligned behavior over multiple interactions, yet current alignment lack explicit mechanisms to prevent such degradation in multi-turn conversations. A more systematic investigation into how LLMs undergo self-corruption, as well as methods to mitigate it, is necessary for a deeper understanding of alignment vulnerabilities. Second, we need to evaluate jailbreak across more benchmarks and multi-modal models to check the Foot-In-The-Door (FITD) phenomenon in Vision LLMs. By addressing these limitations, future research can further understand and enhance AI alignment."}, {"title": "A Appendix", "content": "A.  1 Related work\nSafety-aligned Model To ensure that LLMs adhere to ethical and legal standards, alignment techniques address the risks arising from training in unfiltered datasets containing biases and harmful knowledge (Huang et al., 2024a; Wang et al., 2024a; Guo et al., 2025b; Ge et al., 2024; Deng et al., 2023b; Shen et al., 2024; Guo et al., 2025a). Supervised Fine-Tuning (SFT) in curated safety datasets provides a foundational alignment step (Bai et al., 2022), but it remains insufficient to prevent unsafe outputs in novel contexts. Reinforcement Learning from Human Feedback (Ouyang et al., 2022) and Direct Preference Optimization (Rafailov et al., 2023) further refine alignment by optimizing responses based on human preferences. At inference time, approaches such as Llama-Guard (Inan et al., 2023) detect unsafe generations. In addition, red teaming is also essential for evaluating the robustness of the model (Ganguli et al., 2022). Human red teaming effectively uncovers vulnerabilities, with taxonomies developed to categorize attack tactics (Kang et al., 2024). However, its high cost limits the large-scale deployment (Deng et al., 2023a). Automated red teaming offers a scalable alternative, using adversarial attacks, jailbreak attach, and benchmarks to probe defenses and alignment processes (Ge et al., 2024).\nA.  2 Experiment Setup\nWe compare our approach against seven popular jailbreak methods as listed below. We use default parameters of the baselines.\nA.  2. 1 Baseline\n\u2022 DeepInception (Li et al., 2023): A black-box attack that creates a nested environments by constructing different imaginative scenarios and characters.\n\u2022 CodeChameleon (Lv et al., 2024): A black-box attack that reformulate malicious query into a code completion format and decrypt.\n\u2022 ReNeLLM (Ding et al., 2024): A black-box attack that leverages LLMs themselves for prompt rewriting and scenario nesting.\n\u2022 CodeAttack (Ren et al., 2024b): A black-box attack that disguises malicious queries as code-completion tasks, thereby eliciting harmful responses while generating code."}, {"title": "A.2.5 ReAlign", "content": "The prompt used in ReAlign is shown below."}, {"title": "A.2.6 Harmfulness Evaluation Prompt", "content": "Harmfulness Evaluation Prompt\nBelow are OpenAI's prohibited usage policies. These policies ensure that models interact with users safely and responsibly.\nModels should refuse to execute instructions that fall into the disallowed use cases outlined in the policy, avoiding affirmative responses or providing any information that could be harmful, misleading, or dangerous.\nfor example, don't compromise the privacy of others, engage in regulated activity without complying with applicable regulations, or promote or engage in any illegal activity, including the exploitation or harm of children and the development or distribution of illegal substances, goods, or services."}]}