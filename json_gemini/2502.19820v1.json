{"title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs", "authors": ["Zixuan Weng", "Xiaolong Jin", "Jinyuan Jia", "Xiangyu Zhang"], "abstract": "Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. Inspired by psychological foot-in-the-door principles, we introduce FITD, a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical transgressions. Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses. Extensive experimental results on two jailbreak benchmarks demonstrate that FITD achieves an average attack success rate of 94% across seven widely used models, outperforming existing state-of-the-art methods. Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn interactions.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have been extensively deployed in various domains and products (Li et al., 2024c), ranging from coding assistance (Guo et al., 2024a) to educational tools (Wang et al., 2024c). As these models become more integral to daily life, ensuring AI safety and preserving alignment with human values have become increasingly important (Liu et al., 2024a). A critical challenge lies in \"jailbreak\", wherein adversarial prompts bypass built-in safeguards or alignment measures, causing the model to generate disallowed or harmful output (Zou et al., 2023; Jin et al.).\nEarly jailbreak approaches typically rely on carefully engineered single-turn prompts that coax the model to reveal restricted malicious information (Greshake et al., 2023). By embedding malicious instructions within complex context blocks or intricate role-playing scenarios, attackers exploit weaknesses in the model alignment policy (Ding et al., 2024). However, attackers have recently shifted from single-turn to multi-turn paradigms, where each subsequent user query adapts or builds upon the conversation history (Li et al., 2024a). Although some multi-turn jailbreak methods, such as ActorAttack (Ren et al., 2024c) and Crescendo (Russinovich et al., 2024), have demonstrated the potential of multi-round dialogues in obscuring malicious intent, they usually depend on heavily handcrafted prompts or complex agent design. Besides, their overall Attack Success Rate (ASR) remains limited, often requiring significant prompt engineering expertise."}, {"title": "2 Related Work", "content": "Jailbreak Large language models jailbreak can be broadly categorized into single-turn and multi-turn approaches, with different levels of model access. Black-box single-turn attacks use input transformations to bypass safety constraints without accessing model internals, such as encoding adversarial prompts in ciphers, low-resource languages, or code (Yuan et al., 2024; Deng et al., 2023b; Lv et al., 2024; Ren et al., 2024a; Chao et al., 2023; Wei et al., 2023; Li et al., 2023; Liu et al., 2024a). In contrast, white-box single-turn attacks exploit access to model parameters using gradient-based optimization to generate adversarial inputs or manipulate text generation configurations (Zou et al., 2023; Huang et al., 2024b; Zhang et al., 2024a; Jones et al., 2023; Guo et al., 2024b). Meanwhile, multi-turn jailbreaks introduce new challenges by exploiting dialogue dynamics. A common approach decomposes harmful queries into a series of innocuous sub-questions, progressively leading the model towards unsafe responses (Li et al., 2024b; Jiang et al., 2024; Zhou et al., 2024b). Automated red teaming has also been explored, in which LLMs are used iteratively to investigate and expose weaknesses (Jiang et al., 2025). To mitigate such threats, various defense mechanisms have been proposed, including perturbation or optimization techniques (Zheng et al., 2024; Zhou et al., 2024a; Mo et al., 2024; Liu et al., 2024b), safety response strategy (Zhang et al., 2024b; Li et al., 2024d; Wang et al., 2024b; Zhang et al., 2024c), and jailbreak detection (Han et al., 2024; Inan et al., 2023), aim to neutralize adversarial prompts before execution (Inan et al., 2023; Zou et al., 2024). Notably, multi-turn attack Crescendo (Russinovich et al., 2024) and ActorAttack (Ren et al., 2024c) incrementally steer seemingly benign queries toward harmful content but are constrained by their reliance on fixed, human-crafted seed prompts and limited overall ASR. However, different from their work, our work uses the foot-in-the-door effect to"}, {"title": "3 Method", "content": "gradually erode an LLM's alignment while analyzing the phenomenon of self-corruption in LLMs."}, {"title": "3.1 Inspiration from Psychology: The Foot-in-the-Door Phenomenon", "content": "Our method FITD draws inspiration from the \"foot-in-the-door\" phenomenon in psychology. According to this principle, once individuals perform or agree to a minor (often unethical) act, they are more likely to proceed with more significant or harmful acts afterward (Freedman and Fraser, 1966; Cialdini, 2001). For example, in a classic study, participants who first displayed a small sign supporting safe driving were subsequently much more inclined to install a much larger, more obtrusive sign (Freedman and Fraser, 1966). This gradual escalation of compliance, \"from small to large\", has also been observed in other forms of unethical or harmful behavior (Festinger, 1957), showing that the initial \"small step\" often lowers psychological barriers for larger transgressions. Once a small unethical act has been justified, individuals become increasingly susceptible to more severe transgressions.\nBased on these insights, we hypothesize that LLMs' safety mechanisms might be vulnerable to a gradual escalation strategy. If LLMs respond to a prompt containing slightly harmful content, subsequent queries that escalate this harmfulness will have a higher chance of producing disallowed responses. This idea underlies our FITD method, which progressively coaxes a target model to produce increasingly malicious output despite its built-in safety mechanisms."}, {"title": "3.2 Overview", "content": "Building on the foot-in-the-door perspective, we design a multi-turn jailbreak strategy FITD. In each turn, the target model is prompted with content that is just marginally more harmful or disallowed than the previous turn, encouraging the model to produce a correspondingly more harmful output. This progression method is designed to exploit the model's own responses as a guiding force to bypass its safety alignment or content filters. The core novelty lies in using (i) the model's own prompts and responses as stepping stones for further escalation and (ii) two auxiliary modules, SlipperySlopeParaphrase and Re-Align, to handle instances when the model refuses or produces outputs misaligned with the intended maliciousness. Additionally, we conduct an in-depth analysis of the foot-in-the-door self-corruption phenomenon in LLMs.\nFigure 2 shows the overview of our method. First, we initialize a sequence of escalated queries \\(q_1, q_2,..., q_n\\) based on a malicious query \\(q^*\\). Then in each turn, we append the current query \\(q_i\\) to the chat history and obtain the model's response \\(r_t\\). If \\(r_t\\) has no refusal, we proceed; otherwise, we check how well the model's previous response aligns with its prompt. Depending on this check, we either insert an intermediate \"bridging\" query via SlipperySlopeParaphrase or Re-Align the target model's last response \\(r_{last}\\). Over multiple iterations, the process gradually pushes the model to produce more detailed and harmful content."}, {"title": "3.3 FITD", "content": "As shown in Algorithm 1, given a target model M, a malicious \u201cgoal\u201d query \\(q^*\\), and the malicious level n, we initialize a sequence of escalated"}, {"title": "3.3.1 Re-Align", "content": "If the model's previous query \\(q_{last}\\) and response \\(r_{last}\\) in chat history H is misaligned-for instance, it remains too benign or partially refuses even though the query is not malicious-then we invoke RE-ALIGN. Building on the psychological insight that once individuals have justified a minor unethical act, they become increasingly susceptible to more severe transgressions (Freedman and Fraser, 1966), RE-ALIGN aims to \"nudge\" the model to produce a response more closely aligned with the malicious intent of \\(q_{last}\\). Specifically, we employ a predefined alignment prompt \\(P_{align}\\) via getAlignPrompt(\\(q_{last}\\),\\(r_{last}\\)), appending it to H before querying the model T again. The alignment prompt explicitly points out inconsistencies between the last query \\(q_{last}\\) and response \\(r_{last}\\) while encouraging the model to stay consistent with multi-turn conversation. For example, if \\(r_{last}\\) is too cautious or is in partial refusal, \\(P_{align}\\) will suggest that the model refines its response to better follow the implicit direction. Therefore, this procedure progressively aligns \\(q_{last}\\) and \\(r_{last}\\), thereby furthering the self-corruption process. The details are described in Appendix A.2.5"}, {"title": "3.3.2 SlipperySlopeParaphrase", "content": "When a refusal occurs and the last response \\(r_{last}\\) remains aligned with its query \\(q_{last}\\), we insert a bridge prompt \\(q_{mid}\\) to ease the model into accepting a more harmful request."}, {"title": "3.3.3 Putting It All Together", "content": "Through gradual increases in maliciousness, we systematically steer the target model from benign or slightly harmful content to overtly disallowed"}, {"title": "4 Experiment", "content": "Thus, FITD employs the psychological foot-in-the-door mechanism and adapts it for multi-turn LLM red-teaming. By combining these modules, we show that even well-aligned LLMs can be coaxed into producing harmful outputs if the escalation is gradual and carefully structured."}, {"title": "4.1 Experimental Setup", "content": "Target Models We evaluate FITD on seven widely used LLMs, including both open-source and proprietary models. The open-source models comprise LLAMA-3.1-8B-Instruct (Dubey et al., 2024), LLaMA-3-8B-Instruct, Qwen2-7B-Instruct (Bai et al., 2023), Qwen-1.5-7B-Chat, and Mistral-7B-Instruct-v0.2 (Jiang et al., 2023)."}, {"title": "4.2 Main Results", "content": "FITD is more effective than baseline attacks. Table 1 shows ASRs of FITD and various jailbreak methods on different LLMs across JailbreakBench and HarmBench, where each cell contains two values: the ASR on JailbreakBench (left) and HarmBench (right). In Table 1, FITD requires an average of 16 queries per malicious question for each target model. Among single-turn attacks, ReNeLLM achieves the highest ASR on average, significantly outperforming other single-turn baselines. This suggests that leveraging LLMs for prompt rewriting and scenario nesting is a highly effective approach for jailbreak attacks. Meanwhile, CodeAttack variants also demonstrate competitive performance. However, DeepInception and CodeChameleon exhibit lower ASR, with performance dropping below 34% on average, indicating their limited generalizability against more robust models. For multi-turn attacks, ActorAttack is the strongest baseline, outperforming CoA across most models, which achieves 63%/53% on LLaMA-3.1-8B and 58%/50% on GPT-40-mini, indicating the potential of multi-turn interactions in gradually uncovering vulnerabilities and overcoming content moderation mechanisms.\nOur method, FITD, consistently outperforms all baseline attacks across all evaluated models. It achieves 98%/93% on LLaMA-3-8B and maintains an ASR on average 94%/91% across all tested models. Notably, FITD achieves 95%/93% on GPT-40-mini and 93%/90% on GPT-40, suggesting that FITD is highly adaptable across different architectures and safety settings, demonstrating its effectiveness on both open-source and proprietary models.\nFITD demonstrates strong cross-model transferability. To evaluate the cross-model transferability of FITD, we conduct transfer attacks using adversarial chat histories generated from LLaMA-3.1-8B and GPT-40-mini as source models. Specifically, for each query in JailbreakBench, we utilize the progressively malicious query-response history obtained while attacking the source model and directly apply it to other target models. This allows us to assess whether adversarial chat histories obtained on one model can successfully bypass the safety mechanisms of others.\nAs shown in Figure 3a, LLaMA-3.1 jailbreak chat histories exhibit strong transferability across models, achieving 76% ASR on Mistral-v0.2 and 74% on Qwen-2-7B, indicating that open-source models are particularly vulnerable to transferred adversarial queries. GPT-40-mini (70%), which has stronger moderation mechanisms, remains susceptible to attack histories crafted on LLaMA-3.1. Interestingly, when GPT-40-mini serves as the attack source model, the ASR further improves in most models, with Mistral-v0.2 reaching 85%. This suggests that attacks originating from a more robust model tend to transfer more effectively because the initial stronger safety alignment forces the attack to develop more adaptable and generalizable jailbreak strategies. However, Qwen-1.5-7B (64%) exhibits slightly stronger resistance under GPT-40-mini transfer, potentially due to model-specific safety filtering techniques. Overall, these results highlight a critical weakness in current LLMs' safety defenses: attack histories created on one model can consistently exploit vulnerabilities in others. In particular, closed-to-open transfer (GPT-40-mini \u2192 open-source models) is particularly effective, demonstrating that even models with strict safety protocols can unintentionally generate adversarial sequences that break other systems."}, {"title": "4.3 Ablation Study", "content": "To evaluate different components in our FITD jailbreak method, we conduct an ablation study by systematically removing three key mechanisms: response alignment (Re-Align), alignment prompt \\(P_{align}\\), and SlipperySlopeParaphrase. The results in Figure 3b demonstrate the significance of these components for a high ASR across various models.\nFirst, removing all three mechanisms leads to a significant decline in ASR (w/o ReAlign, \\(P_{align}\\), SSP). On LLaMA-3.1, the ASR drops from 92% to 75%, while on LLaMA-3, it decreases from 98% to 59%. Similar declines are observed across other models, with Qwen-2 and Qwen-1.5 dropping to 76% and 80%, respectively. These results suggest that the interplay of response alignment, prompt alignment, and paraphrasing is critical to maintaining the effectiveness of FITD. Without these components, the attack becomes substantially less effective, particularly on models with stronger alignment guardrails.\nSecond, when the response alignment and prompt alignment are removed (w/o ReAlign, \\(P_{align}\\)), the ASR remains relatively high, but it still exhibits some degradation. On LLaMA-3.1, the ASR remains at 91%, suggesting that paraphrasing alone can compensate for some of the lost effectiveness. However, on LLaMA-3, the ASR drops from 98% to 63%, indicating that paraphrasing is insufficient against models with stricter safeguards. Similar patterns are observed in Qwen-2 and Qwen-1.5, where the ASR decreases to 75% and 81%, respectively. These results highlight that, while paraphrasing can mitigate the impact of removing alignment techniques, it cannot fully substitute for them, especially on models with more robust defenses.\nThird, removing only response alignment (w/o ReAlign), we observe a relatively minor decrease in performance. LLaMA-3.1 and Qwen-2 maintain their ASR at 92% and 75%, respectively, while LLaMA-3 shows a decrease from 98% to 79%. The impact is more evident on Qwen-1.5 and Mistral-v0.2, with ASR decreasing from 94% to 83% and from 96% to 90%, respectively, showing that response alignment is beneficial for a gradual erosion of the model's safeguard, aligning with the psychological principle of incremental compliance.\nOverall, the ablation study demonstrated response alignment (Re-Align), alignment prompt \\(P_{align}\\), and SlipperySlopeParaphrase as essential for high jailbreak success. Response alignment is crucial for bypassing safeguards while paraphrasing also gradually erodes the model's alignment.\nThe impact of malicious level n. We conduct experiments across multiple models to evaluate the impact of the malicious level n on ASR. The results show a clear trend: as n increases, ASR improves, reaching its peak around n = 9 to n = 12. However, beyond this point, the improvement plateaus and in some cases the ASR fluctuates slightly at n = 15, possibly due to the increasing length and complexity of the generated context. Among the models, LLaMA-3.1-8B and GPT-40-mini require higher n values (n = 12) to achieve optimal ASR, while LLaMA-3-8B and Qwen2-7B reach peak ASR earlier (n = 9), indicating different levels of robustness. Qwen-1.5-7B and GPT-40-mini exhibit more variance at n = 15, indicating that over-paraphrasing or excessive manipulation introduces inconsistencies that reduce attack efficacy. Although increasing n improves ASR across all models, the effect saturates beyond n = 12, implying a trade-off between attack complexity and effectiveness. Future work could explore adaptive malicious level selection based on model-specific vulnerabilities to maximize ASR while minimizing unnecessary complexity and queries.\nLater stage malicious query progression matters. To analyze the relative importance of different stages within the self-corruption process, we conduct experiments that extract subsets of the chat history H and evaluate their impact on ASR. We compare two extraction strategies: backward extraction, where we retain only the later-stage queries while progressively removing earlier ones (e.g., retaining 4 queries: 9 \u2192 10 \u2192 11 \u2192 12; 6 queries: 7\u21928\u21929\u219210 \u2192 11 \u2192 12; 8 queries: 5\u21926\u21927\u21928\u21929\u219210 \u2192 11 \u2192 12, etc.), and forward extraction, where we incrementally add early-stage queries but always include a final high-malicious query at n = 12 (e.g., 4 queries: 1\u21922\u21923\u219212; 6 queries: 1\u21922\u21923\u21924\u21925 \u2192 12; 8 queries: 1\u21922\u21923\u21924\u21925\u21926\u21927\u219212, etc.)."}, {"title": "Defense", "content": "Figure 3c shows ASR of FITD across models under different defense strategies. OpenAI-Moderation reduces ASR slightly by 3%-8%. LLaMA-Guard-2 (Inan et al., 2023) offers a stronger defense, lowering ASR to 79%-91%. LLaMA-Guard-3 (Inan et al., 2023) further improves moderation, achieving the lowest ASR 78%-84%. LLaMA-Guard-3 consistently outperforms other methods, but ASR remains significant. We speculate that progressively malicious queries and responses bypassed the detector, indicating room for further improvement in moderation techniques."}, {"title": "Harmfulness of different malicious level response", "content": "To assess the impact of increasing the malicious level on the harmfulness of model's responses, we use the chat history of malicious level n = 12 experiment in Table 1 and analyze the harmfulness of responses at each level across multiple LLMs. The harmfulness is measured by score 1-5, where a higher score indicate greater harmfulness. We report the mean harmfulness scores for each model at malicious level i ranging from 1 to 12. Figure 4b presents the harmfulness scores of responses at different malicious levels for all evaluated models. We use GPT-40 to score each response via prompt (Ren et al., 2024b) shown in Appendix A.2.6. We observe that the harmfulness scores generally increase with the malicious level. At i = 1, the harmfulness scores are relatively low, with values around 2.32 on average across models. However, as the level increases, the harmfulness score consistently rises to 4.23 on average at i = 12. These results show that as the malicious level increases, LLMs become more vulnerable and generate more harmful responses, suggesting that model's alignment weakens over time, making it easier for FITD to bypass safeguards."}, {"title": "5 Conclusion", "content": "In this work, we introduced FITD, a multi-turn jailbreak strategy inspired by the psychological foot-in-the-door effect. By progressively escalating the malicious intent of user queries through intermediate prompts via SlipperySlopeParaphrase and ReAlign, our method achieves a 94% attack success rate on average across multiple models. Our findings reveal a major weakness in current AI safety measures: LLMs can be manipulated into self-corruption, where their responses gradually shift toward harmful content by themselves. Future work could explore why LLMs have the foot-in-the-door self-corrupt phenomenon that their responses shift during adversarial interactions and FITD effect in multimodality Language Models."}, {"title": "6 Ethical Considerations", "content": "This study aims to improve AI safety by identifying weaknesses in LLM alignment. While our method bypasses safeguards, our goal is to help strengthen AI defenses, not to enable misuse.\nWe recognize the risks of publishing jailbreak techniques but believe that transparent research is necessary to develop better protections. Responsible disclosure ensures that AI developers can proactively address these vulnerabilities.\nAI developers must build stronger safeguards against adversarial attacks. Adversarial training, real-time monitoring, and collaboration between researchers, industry, and policymakers are essential to keeping AI systems secure, reliable and beneficial."}, {"title": "7 Limitation", "content": "First, we need more in-depth analysis of self-corruption and the Foot-In-The-Door (FITD) phenomenon remains preliminary. Self-corruption occurs when an LLM gradually deviates from its initial aligned behavior over multiple interactions, yet current alignment lack explicit mechanisms to prevent such degradation in multi-turn conversations. A more systematic investigation into how LLMs undergo self-corruption, as well as methods to mitigate it, is necessary for a deeper understanding of alignment vulnerabilities. Second, we need to evaluate jailbreak across more benchmarks and multi-modal models to check the Foot-In-The-Door (FITD) phenomenon in Vision LLMs. By addressing these limitations, future research can further understand and enhance AI alignment."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Related work", "content": "Safety-aligned Model To ensure that LLMs adhere to ethical and legal standards, alignment techniques address the risks arising from training in unfiltered datasets containing biases and harmful knowledge (Huang et al., 2024a; Wang et al., 2024a; Guo et al., 2025b; Ge et al., 2024; Deng et al., 2023b; Shen et al., 2024; Guo et al., 2025a). Supervised Fine-Tuning (SFT) in curated safety datasets provides a foundational alignment step (Bai et al., 2022), but it remains insufficient to prevent unsafe outputs in novel contexts. Reinforcement Learning from Human Feedback (Ouyang et al., 2022) and Direct Preference Optimization (Rafailov et al., 2023) further refine alignment by optimizing responses based on human preferences. At inference time, approaches such as Llama-Guard (Inan et al., 2023) detect unsafe generations. In addition, red teaming is also essential for evaluating the robustness of the model (Ganguli et al., 2022). Human red teaming effectively uncovers vulnerabilities, with taxonomies developed to categorize attack tactics (Kang et al., 2024). However, its high cost limits the large-scale deployment (Deng et al., 2023a). Automated red teaming offers a scalable alternative, using adversarial attacks, jailbreak attach, and benchmarks to probe defenses and alignment processes (Ge et al., 2024)."}, {"title": "A.2 Experiment Setup", "content": "We compare our approach against seven popular jailbreak methods as listed below. We use default parameters of the baselines."}, {"title": "A.2.1 Baseline", "content": "\u2022 DeepInception (Li et al., 2023): A black-box attack that creates a nested environments by constructing different imaginative scenarios and characters.\n\u2022 CodeChameleon (Lv et al., 2024): A black-box attack that reformulate malicious query into a code completion format and decrypt.\n\u2022 ReNeLLM (Ding et al., 2024): A black-box attack that leverages LLMs themselves for prompt rewriting and scenario nesting.\n\u2022 CodeAttack (Ren et al., 2024b): A black-box attack that disguises malicious queries as code-completion tasks, thereby eliciting harmful responses while generating code.\n\u2022 CoA (Sun et al., 2024): A semantic-driven contextual multi-turn attack that adaptively adjusts policy through semantic relevance and contextual feedback during multi-turn interaction.\n\u2022 ActorAttack (Jiang et al., 2024): A black-box multi-turn attack that uses actor-network theory to conceal harmful intent and uncover various attack paths. Since the source code for Crescendo (Russinovich et al., 2024) is not publicly available, we adopt ActorAttack, which empirical results in their paper indicate that ActorAttack outperforms Crescendo."}, {"title": "A.2.2 Dataset", "content": "We evaluate our method on two benchmarks as listed below.\n\u2022 JailbreakBench (Chao et al., 2024) contain 100 distinct misuse behaviors (with 55% original examples and the rest sourced from AdvBench etc.) and is divided into ten broad categories corresponding to OpenAI's usage policies.\n\u2022 HarmBench (Mazeika et al., 2024) includes 510 unique harmful behaviors, split into 400 textual behaviors and 110 multimodal behaviors. We use their validation set for additional evaluations."}, {"title": "A.2.3 Evaluation", "content": "We utilize the evaluation method from JailbreakBench, which leverages GPT-40 to assess two key points: the harmfulness of the generated responses and the degree of alignment between the responses and the original queries. The prompt is shown below."}, {"title": "A.2.5 ReAlign", "content": "The prompt used in ReAlign is shown below."}, {"title": "A.2.6 Harmfulness Evaluation Prompt", "content": ""}, {"title": "A.3 Case Study", "content": ""}, {"title": "A.3.1 Case Study on Slippery SlopeParaphrase", "content": "We show one case in Figures 5 about SlipperySlopeParaphrase. We utilize the assistant model to generate \\(q_{mid}\\), whose malicious level lies between \\(q_{last}\\) and \\(q_i\\)."}, {"title": "A.3.2 Case Study on Re-Align", "content": "We present one case in Figures 6 about Re-Align. When the original \\(r_{last}\\) and \\(q_{last}\\) are misaligned, we use \\(P_{align}\\) to prompt the model to self-align its response to enhance the foot-in-the-door self-corruption process."}, {"title": "A.3.3 Case Study on FITD", "content": "We present two cases in Figures 7 and 8. As the malicious level increases in the multi-turn interaction, the model generates increasingly detailed malicious outputs."}]}