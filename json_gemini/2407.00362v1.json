{"title": "JSCDS: A Core Data Selection Method with Jason-Shannon Divergence for Caries RGB Images-Efficient Learning", "authors": ["Peiliang Zhang", "Yujia Tong", "Chenghu Du", "Chao Che", "Chao Che"], "abstract": "Deep learning-based RGB caries detection improves the efficiency of caries identification and is crucial for preventing oral diseases. The performance of deep learning models depends on high-quality data and requires substantial training resources, making efficient deployment challenging. Core data selection, by eliminating low-quality and confusing data, aims to enhance training efficiency without significantly compromising model performance. However, distance-based data selection methods struggle to distinguish dependencies among high-dimensional caries data. To address this issue, we propose a Core Data Selection Method with Jensen-Shannon Divergence (JSCDS) for efficient caries image learning and caries classification. We describe the core data selection criterion as the distribution of samples in different classes. JSCDS calculates the cluster centers by sample embedding representation in the caries classification network and utilizes Jensen-Shannon Divergence to compute the mutual information between data samples and cluster centers, capturing nonlinear dependencies among high-dimensional data. The average mutual information is calculated to fit the above distribution, serving as the criterion for constructing the core set for model training. Extensive experiments on RGB caries datasets show that JSCDS outperforms other data selection methods in prediction performance and time consumption. Notably, JSCDS exceeds the performance of the full dataset model with only 50% of the core data, with its performance advantage becoming more pronounced in the 70% of core data.", "sections": [{"title": "1 INTRODUCTION", "content": "Dental caries is one of the chronic diseases affecting the health of children and adolescents. If not treated promptly, it can lead to complications such as pulpitis and periapical periodontitis, adversely affecting the development and quality of life of adolescents [9]. According to incomplete statistics, approximately 50% of children aged 6-11 have caries in their primary teeth, and over 34% of adolescents aged 12-19 have caries in their permanent teeth globally [2]. This underscores the importance of early detection and treatment of dental caries in maintaining the oral health of children and adolescents. Currently, the most widely used methods for caries detection are visual inspection, probing, and X-ray examination. However, these methods depend heavily on the professional expertise of dentists and increase the economic burden on patients. Therefore, efficient, accurate, and affordable caries detection methods are urgently needed.\nPrevious Work: According to different detection methods, caries detection can be categorized into physical examination, auxiliary examination, and automated detection [4, 5, 11]. Physical examination refers to dentists visually inspecting the tooth surface for changes"}, {"title": "Limitation and Motivation", "content": "Deep learning methods based on RGB images have significantly improved the efficiency and accuracy of caries detection while reducing its costs [15]. However, deep learning methods require high-quality and large training datasets. Low-quality, confusing data in the training dataset can severely affect the model's performance. In caries detection, patients primarily use mobile devices to capture RGB images of their teeth. During this process, due to the oral cavity's unique structure and the photographers' varying skill levels, the captured images often contain a certain amount of unclear, mixed-boundary confusing data. These data can severely impact the performance of the detection model [4, 5]. Data core set selection is one effective way to address this issue. By quantifying the impact of different data on the model, core data selection identifies key data and reduces the influence of confusing data, thereby enhancing the performance and efficiency of model training [7, 10, 16]. As shown in Figure 1(A), the Moderate method achieved superior performance and faster computational efficiency using 70% of the core dataset, with similar results observed in Figure 1(B). Therefore, how to measure the impact of different caries data on the detection model and selecting the core dataset accordingly is crucial for improving caries detection performance and computational efficiency.\nTo address the aforementioned problems, we propose a core data selection method with Jensen-Shannon Divergence (JSCDS) for efficient caries image learning and caries classification. Specifically,"}, {"title": "2 METHODOLOGY", "content": "2.1 Definition\nWe define the core data selection problem in caries classification. For a given full dataset of caries training data $M = \\{m_1, m_2,\\dots, m_x\\}$, which contains $x$ caries images. Any caries image $m_i \\in (F_i, T_j)$, where $T_j \\in \\{T_{Mil} | T_{Mod} | T_{Sev}\\}$ represents the image's label. We aim to design a data selection strategy $\\Psi (*)$ to construct a subset $M^* = \\{m^*_1, m^*_2, \\dots, m^*_y | y \\leq x\\}$ to remove confusing data, such as ambiguous images from the original dataset. Formally, $\\Psi (M) \\to M^*$. When the model's performance on $M^*$ is comparable to $M$, and its training efficiency significantly improves, we refer to $M^*$ as the core set and use it for model training.\n2.2 Model Structure\nTo construct the core set for caries, we propose JSCDS with Jensen-Shannon Divergence. The workflow is shown in Figure 2. JSCDS primarily consists of two parts: data representation and data selection."}, {"title": "2.2.1 Data Representation and Cluster Center Calculation", "content": "For the neural network model for caries classification, it can be viewed as a black-box feature extractor. For the caries image $F_i$, the probability distribution $P(Z, F_i)$ of the image sample can be obtained through the mapping $Z(*) = H(h(*))$. Here, $h(*)$ represents the final embedding representation of the input caries image in the hidden layer, and $H(*)$ is the feature mapping function represented by Softmax(*). The hidden layer embedding representation of the training data $F_i$ can be obtained through the above calculation, and from this, the hidden embedding representation of all training data $M$ can be derived, as shown in Equation (1). We calculate the cluster center $C_j$ for each data class based on the hidden layer representations by Equation (2).\n$M = \\{m_1, \\dots, m_x\\} \\longrightarrow \\varepsilon = \\{e_1 = h(F_1), \\dots, e_x = h(F_x)\\}$  (1)\n$C_j = \\frac{\\sum_{i=1}^x \\mathbb{I} [T_i = j] \\cdot e_i}{\\sum_{i=1}^x \\mathbb{I} [T_i = j]}, j = 1,2,...,J$ (2)\nHere, the numerator represents the sum of the sample embeddings in class $T_j$, and the denominator represents the number of samples in class $T_j$. The cluster center $C_j$ is the reference for subsequent data selection processes."}, {"title": "2.2.2 Data Selection with JSD", "content": "Relevant research primarily measures the relevance of samples by calculating the distance between them [10, 16]. While this method is computationally efficient, it heavily depends on the data size [7, 16]. Identifying high-dimensional nonlinear dependencies in the training data is challenging, making it difficult to distinguish between samples. In contrast, information theory-based methods can capture nonlinear relationships between samples, enabling effective high-dimensional data processing. Therefore, JSCDS designs a JSD-based data sample importance selection strategy from the information theory perspective.\nBased on the hidden layer representations $\\{e_1, e_2,\\dots, e_x\\}$ of the training data and the cluster centers $\\{C_1, C_2, \\cdots, C_j\\}$ of the classes, the mutual information $jsd(e, C)$ between each image embedding representation and its corresponding cluster center is calculated. $jsd(e, C)$ is obtained by computing the JSD:\n$jsd(e, C) = JSD(e || C)$  (JS divergence)\n$= \\frac{1}{2} KL(e || G) + \\frac{1}{2} KL(C || G)$ (KL divergence)\n$= \\frac{1}{2} \\sum_{w} e(w) \\log \\frac{2 * e(w)}{e(w) + C(w)} + \\frac{1}{2} \\sum_{w} C(w) \\log \\frac{2 * C(w)}{e(w) + C(w)}$   $(G = \\frac{e + C}{2})$  (3)\nHere, $y$ represents the selection ratio of data in the core set (To ensure clarity in our descriptions, we describe $y$ as \"Fraction\" in the experimental section.), and $\\{m_y,\\cdots, m_{x-y}\\}$ is the core set after selection, which is used for model training.\nIn information theory, the more considerable mutual information between a data sample and the cluster center indicates the higher posterior probability given by the neural network for the sample. However, obtaining embedding representations in neural networks is easier than computing posterior probabilities [1]. Therefore, our method is more practical in real-world applications and requires fewer resources, thereby improving the computational efficiency of deep learning."}, {"title": "3 EXPERIMENTAL", "content": "3.1 Datasets\nWe used the RGB dental caries dataset labeled and processed by professional dentists. This dataset has been widely used in caries object detection and classification [4, 5]. It contains 5619 caries images, each in 24-bit JPG format. Professional dentists classified the caries images into three categories: mild, moderate, and severe caries, based on the extent and severity of the lesions. Following the settings in previous related studies [4, 5, 17], we divided the dataset into training, validation, and test set in the ratio of 8:1:1."}, {"title": "3.2 Experimental Setup", "content": "3.2.1 Evaluation Metrics. We constructed caries fine-grained classification to verify the data selection capability of JSCDS. In the caries classification task, Accuracy (ACC), Precision (Pre), Recall (Rec), F1-score (F1), Specificity (SPE), AUPR, and AUROC are commonly used to evaluate model performance [17-19]. AUPR and AUROC are closely related to evaluation metrics such as Pre and Rec. Due to page limitations, we use ACC, Pre, Rec, F1, and SPE to evaluate model performance in this work.\n3.2.2 Training Details. We designed and implemented JSCDS based on Python and PyTorch, and conducted training and testing on a server equipped with two NVIDIA GeForce RTX 4090 GPUs (each with 24GB RAM), Ubuntu operating system and an Intel(R) Core(TM) i9-12900KF CPU. The backbone networks for the experiments were MobileNetV2 [12] and ResNet18 [3]. We initialize the model parameters using the model that has been pre-trained on the ImageNet-1K dataset. The training parameters for JSCDS are as follows: we train the network for 50 epochs using a learning rate of 0.001, no weight decay, batch size of 64, and Adam optimizer. Every 10 epochs, the core set is reselected.\n3.3 Overall Performance\nWe compared the performance of JSCDS with four core set selection methods. The primary comparison methods include: random data selection, Moderate [16], kCenterGreedy [13], and Forgetting [14]."}, {"title": "3.3.1 Comprehensive Performance Analysis", "content": "As shown in Table 1, our proposed JSCDS achieves the best fine-grained caries prediction results in different fractions. In predictive performance, JSCDS's performance on 50% of the core set is already close to or even exceeds the predictive results using the full dataset. At the 70% fraction, its predictive performance significantly surpasses that of the full dataset, as highlighted by the red data in Table 1. Figure 3 further visually confirms these findings. Regarding time overhead, JSCDS effectively reduces the training time, substantially enhancing training efficiency. In summary, JSCDS effectively selects high-quality data from the original dataset, improving the model's classification performance. The main reason is that JSCDS calculates the AvgMI with JSD and uses it to select the core set, capturing high-dimensional nonlinear dependencies among samples."}, {"title": "3.3.2 The Performance Analysis of Different Core Set Selection Methods", "content": "Compared to Moderate, kCenterGreedy, and Forgetting, JSCDS achieves the best comprehensive predictive performance in different backbone networks. Although the training time for JSCDS is marginally longer than that for Moderate, the substantial improvement in performance makes this additional time investment acceptable. Moderate's shorter training time can be attributed to its use of Euclidean distance for data selection, which is computationally simpler. However, as the volume of selected data increases, Moderate's computational speed significantly decreases. This experimentation result aligns with the general knowledge that Euclidean distance incurs higher computational overhead in large-scale datasets. In contrast, JSCDS maintains its advantages in predictive performance and time overhead as the data scale grows. kCenterGreedy and Forgetting fail to achieve satisfactory predictive performance and training time, possibly due to the limited number of caries data classes. Moderate and JSCDS yield commendable results regarding training time overhead, significantly reducing the training time."}, {"title": "3.3.3 The Performance Analysis in Different Fractions", "content": "As shown in Table 1, all models achieve optimal predictive performance when the fraction is 50% or 70%, with minimal performance differences from the full dataset. JSCDS and Moderate even surpass the full dataset's performance at 70% fraction, indicating their effectiveness in core data selection. As the fraction increases from 30% to 70%, the training time for JSCDS only increases by 21% and 19.5% in MobilenetV2 and Resnet18, respectively. This demonstrates that JSCDS is efficient in data selection, with its time consumption not proportionally increasing with larger fractions. In contrast, kCenterGreedy and Forgetting have consistently high training times, and Moderate's performance in this aspect is slightly inferior to JSCDS. This result further underscores JSCDS's efficiency. The results in Figure 3 indicate that JSCDS achieves the best performance at a 70% fraction, with its predictive performance in both backbone networks showing a near-normal distribution trend. This suggests that the core set size significantly impacts model performance. The core set that is too large may include some noisy data, while a core set that is too small lacks sufficient high-quality data, both of which can degrade the model's predictive performance."}, {"title": "4 CONCLUSION", "content": "In this paper, we designed a core set selection method with Jensen-Shannon Divergence. By capturing high-dimensional dependencies in caries images, we construct a high-quality core set for model training to improve the model's predictive performance. Extensive experiments on RGB caries datasets show that JSCDS outperforms other data selection methods in prediction performance and time consumption. Notably, JSCDS exceeds the performance of the full dataset model with only 50% of the core data, with its performance advantage becoming more pronounced in the 70% of core data."}]}