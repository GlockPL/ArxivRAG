{"title": "POMONAG: PARETO-OPTIMAL MANY-OBJECTIVE NEURAL ARCHITECTURE GENERATOR", "authors": ["Eugenio Lomurno", "Samuele Mariani", "Matteo Monti", "Matteo Matteucci"], "abstract": "Neural Architecture Search (NAS) automates the design of neural network architectures, minimising dependence on human expertise and iterative experimentation. While NAS methods are often computationally intensive and dataset-specific, employing auxiliary predictors to estimate architecture properties has proven extremely beneficial. These predictors substantially reduce the number of models requiring training, thereby decreasing overall search time. This strategy is frequently utilised to generate architectures satisfying multiple computational constraints. Recently, Transferable Neural Architecture Search (Transferable NAS) has emerged, generalising the search process from being dataset-dependent to task-dependent. In this domain, DiffusionNAG stands as a state-of-the-art method. This diffusion-based method streamlines computation, generating architectures optimised for accuracy on unseen datasets without the need for further adaptation. However, by concentrating exclusively on accuracy, DiffusionNAG neglects other crucial objectives like model complexity, computational efficiency, and inference latency \u2013 factors essential for deploying models in resource-constrained, real-world environments. This paper introduces the Pareto-Optimal Many-Objective Neural Architecture Generator (POMONAG), extending DiffusionNAG through a many-objective diffusion process. POMONAG simultaneously considers accuracy, the number of parameters, multiply-accumulate operations (MACs), and inference latency. It integrates Performance Predictor models to estimate these secondary metrics and guide the diffusion gradients. POMONAG'S optimisation is enhanced by expanding its training Meta-Dataset, applying Pareto Front Filtering to generated architectures, and refining embeddings for conditional generation. These enhancements enable POMONAG to generate Pareto-optimal architectures that outperform the previous state-of-the-art in both performance and efficiency. Results were validated on two distinct search spaces \u2013 NASBench201 and MobileNetV3 \u2013 and evaluated across 15 image classification datasets.", "sections": [{"title": "1 Introduction", "content": "Deep learning has become indispensable in various domains by enabling models to learn intricate patterns from large datasets. The architecture of neural networks plays a pivotal role in their performance, traditionally requiring expert knowledge and extensive experimentation. Neural Architecture Search (NAS) automates this design process, aiming to discover optimal architectures without human intervention. However, conventional NAS methods often involve significant computational costs and are typically tailored to specific datasets, limiting their scalability and general applicability. Transferable Neural Architecture Search (Transferable NAS) addresses these limitations by generalising the search process across different tasks. DiffusionNAG stands out in this domain, utilising diffusion processes to generate neural architectures optimised for accuracy on unseen datasets. While effective in reducing computational overhead, DiffusionNAG focuses solely on maximising accuracy, neglecting other crucial performance metrics such as model size, computational cost, and inference latency. In practical applications, especially within resource-constrained environments like mobile devices and embedded systems, it is essential to consider multiple objectives simultaneously. Existing Multi- and Many-objective NAS methods often face scalability challenges or require extensive computational resources, making them less practical for widespread adoption.\nThis work introduces the Pareto-Optimal Many-Objective Neural Architecture Generator (POMONAG), which extends the capabilities of DiffusionNAG by performing a many-objective optimisation. POMONAG simultaneously considers accuracy, the number of parameters, multiply-accumulate operations (MACs), and inference latency during the architecture generation process. Auxiliary Performance Predictor models, trained to estimate these metrics, are integrated into the diffusion process, guiding exploration towards regions of the search space offering optimal trade-offs among these objectives. This approach facilitates the generation of architectures that are both highly accurate and efficient in computational resources and speed. To enhance POMONAG's performance and adaptability, several key improvements have been implemented. The training Meta-Dataset has been significantly expanded, incorporating a diverse set of architectures and tasks to enhance the Performance Predictors' capabilities across different domains. Pareto Front Filtering and Stretching techniques are applied to balance the multiple objectives effectively, ensuring that the generated architectures are Pareto-optimal. Additionally, the embeddings used for conditional generation have been refined to facilitate more accurate and dataset-aware architecture synthesis. Extensive experiments validate POMONAG'S effectiveness. Evaluations conducted on two prominent search spaces-NASBench201 and MobileNetV3\u2014and tested across 15 diverse image classification datasets demonstrate that POMONAG outperforms existing state-of-the-art methods, including DiffusionNAG. The generated architectures achieve superior accuracy while satisfying various computational constraints and require significantly fewer trained models, highlighting the efficiency of the proposed technique.\nThe key contributions of this work include:\n\u2022 Diffusion-based Many-Objective Optimisation. POMONAG introduces a many-objective diffusion approach within Transferable NAS, optimising neural architectures for accuracy, number of parameters, MACs, and inference latency.\n\u2022 Pareto-Optimal Architecture Generation. By computing Pareto Fronts, POMONAG effectively navigates trade-offs among multiple objectives, generating architectures that offer balanced compromises suitable for diverse deployment scenarios.\n\u2022 Enhanced Meta-Datasets. New Meta-Datasets have been developed to improve the Performance Predictors' ability to predict architecture performance across various tasks and datasets.\n\u2022 Refined Performance Predictor Models. The Performance Predictor models have been enhanced to increase prediction accuracy and effectiveness in guiding the architecture generation process.\n\u2022 State-of-the-Art Transferable NAS Model. POMONAG establishes a new benchmark by generating architectures that are high-performing and adaptable to various computational constraints.\nThe POMONAG model's code and the accompanying Meta-Datasets will be made publicly available upon publication of this paper at the following link."}, {"title": "2 Related Works", "content": "Neural Architecture Search. Neural Architecture Search (NAS) seeks to automate neural architecture design, removing the need for manual trial-and-error processes. Early methods, such as reinforcement learning [1, 2], evolutionary algorithms [3, 4], and gradient-based techniques [5, 6], are computationally expensive as they require full training of numerous architectures.\nOne-shot NAS. To mitigate computational costs, one-shot NAS methods employ weight sharing among candidate architectures. ENAS [7] uses an RNN controller to generate sub-networks; DARTS [8] relaxes the search space into a continuous one; OFA [9] trains a single large network with many architectural choices, and OFAv2 [10] extends this approach with an enriched search space. While these methods reduce costs, they may face challenges in optimisation bias, training stability, or performance across diverse sub-networks.\nMulti- and Many-Objective NAS. As application complexity grows, NAS methods increasingly consider multiple objectives. Approaches like MONAS [11] and DPP-Net [12] balance criteria such as accuracy, latency, and energy consumption. NSGANetV2 [13] handles up to 12 objectives. Methods such as POPNASv3 [14], NAT [15], and NATv2 [16] offer Pareto-optimal solutions, addressing scalability and computational challenges inherent in multi-objective optimisation."}, {"title": "3 Method", "content": "This section outlines the techniques developed and implemented in POMONAG, a Transferable NAS model derived from the DiffusionNAG framework proposed by An et al. [26]. It presents the Many-Objective Reverse Diffusion Guidance process, the creation of new Meta-Datasets, and enhancements to the Performance Predictors. Additionally, it explains the Pareto Front Stretching and Pareto Front Filtering techniques used to optimise and refine the architecture generation process."}, {"title": "3.1 Many-Objective Reverse Diffusion Guidance", "content": "The diffusion model in DiffusionNAG employs the Reverse Diffusion Process for architecture generation, described as:\n$dAt = [ft(At) - gt\u2207At log pt(At)] dt + gtdw$.\nThe score function $\u2207_{A_t} \\log p_t(A_t)$ is approximated by introducing the Score Network, which iteratively applies the transformation $s_\u03b8(A_t, t)$ to the noisy architecture $A_t$. The process is guided using the dataset-aware Performance Predictor $f_\u03b8(y|D, A_t)$, resulting in:\n$dAt = {ft(At) - gt s\u03b8(At,t) + k\u2207A_t log f\u03b8(y|D, At)]} dt + gtdw$.\nThe Score Network is responsible for denoising the noisy architecture sampled from the generative distribution at each denoising step. The output of the Performance Predictor is used to compute the guidance term that steers the generation process towards regions of the search space with a higher density of accurate architectures for the given dataset.\nIn POMONAG's Reverse Diffusion Guidance process, the generation gradient is directed in a many-objective fashion by simultaneously considering three additional terms:"}, {"title": "3.2 Meta-Dataset", "content": "The training of the Score Network, as well as the Performance Predictors, requires a Meta-Dataset containing the types of architecture structures to be generated, along with their characteristics and performance metrics. These architectures must be sampled from a sufficiently large search space and subsequently trained to solve specific tasks using a broad pool of datasets. This approach generalises the models and enables the Score Network to perform dataset-aware generation.\nIn line with practices used in other models within the Transferable NAS family [21, 22, 26], the datasets used for training the architectures - whose metadata will compose the Meta-Dataset - are extracted from the ImageNet32 dataset [27]. For each iteration, extraction is performed randomly by selecting 20 classes from ImageNet32. All corresponding samples of these classes form the dataset D. In parallel, an architecture A is selected from the designated search space to be associated with this classification task. The structure of this architecture is stored in encoded form. For more information regarding the encoding of architectures, refer to Appendix A. Subsequently and in contrast to other approaches-the number of parameters, MACs, and the inference latency on a single 32\u00d732 sample are calculated for the architecture. Since latency is highly susceptible to noise, each measurement is repeated 100 times; measurements outside the 90% confidence interval are discarded, and the mean of the remaining values is computed."}, {"title": "3.3 Score Network and Performance Predictors", "content": "Score Network. The POMONAG framework adapts the core architecture of the Score Network from DiffusionNAG [26], incorporating modifications to accommodate the novel encodings detailed in Appendix A. The Score Network is responsible for iteratively denoising the encodings of the architectures to be generated. The fundamental structure remains consistent with DiffusionNAG. The input embeddings combine operation information (Embops), node positions (Embpos), and time step (Embtime):\n$Embi = Embops(vi) + Embpos(vi) + Embtime(t)$\nwhere vi represents the i-th row of the operator type matrix V. The training strategy for the Score Network remains consistent with that defined by An et al. [26].\nPerformance Predictors. The Performance Predictors are models designed to predict the characteristics of architectures during and after the generation phase. In the first instance, the objective is to calculate the regression error used as Reverse Diffusion Guidance; in the second, the aim is to estimate the performance of the architecture once denoised, allowing for ranking without the need for any training and thereby identifying the most promising architectures.\nIn DiffusionNAG, there are two Performance Predictors: one for estimating the accuracy of noisy architectures during generation, and one for estimating the accuracy of denoised architectures. In POMONAG, there are five Performance Predictors. Four are dedicated to the respective estimation of accuracy, parameters, MACs, and inference latency of noisy architectures during the diffusion phase. The fifth estimates the accuracy of denoised architectures, given that the other metrics can be extracted with negligible overhead.\nThe Performance Predictors in POMONAG retain a similar architecture to that in DiffusionNAG. The substantial modifications pertain to the training procedure \u2013 which has been revised to maximise the Spearman correlation between the predicted values and the actual metrics \u2013 the size and content of the Meta-Dataset used during training, and the model employed as the Dataset Encoder. Specifically, while DiffusionNAG employs a ResNet18 architecture, POMONAG utilises a Vision Transformer (ViT-B-16) model for dataset embedding. From the conducted ablation studies, where the two Performance Predictors of DiffusionNAG are gradually adapted to the version used in POMONAG, a gradual improvement in terms of Spearman correlation is observable. Concretely, this amounts to improvements of +0.168 and +0.117 for the versions applied to noisy and denoised architectures, respectively. More details on the study are presented in Appendix C."}, {"title": "3.4 Pareto Front Filtering and Stretching", "content": "In DiffusionNAG, architectures are generated in batches of size 256. The scaling factor k used to weight the contribution of the Performance Predictor during the generation process is set to 10000 and remains constant across all experiments. The generated architectures are then validated by filtering out those with inadequate or incorrect structures. Subsequently, the Performance Predictor for denoised architectures estimates their accuracy, obviating the need to train them. The architectures are then ranked based on this estimate, and the top five are returned as the output of the generation.\nPareto Front Filtering. In POMONAG, an additional filtering step is introduced through the construction of a Pareto Front, aimed at leveraging the secondary metrics. Specifically, after the many-objective generation of architectures, the elimination of invalid configurations, and the estimation of accuracies via the Performance Predictor, a Pareto Front is constructed for each of the three secondary metrics, and only the dominant architectures are retained. This approach enables the selection of architectures that are Pareto-optimal, allowing for choices based on trade-offs between accuracy and secondary metrics. To this end, three configurations, extractable from each Pareto Front are identified. The configuration POMONAGAcc represents the architecture for which the highest accuracy is predicted; the configuration POMONAGBal represents the architecture for which the ratio between predicted accuracy and the considered secondary metric is highest; finally, the configuration POMONAGEff represents the architecture with the lowest value of the secondary metric and therefore the most efficient.\nPareto Front Stratching. The Many-Objective Reverse Diffusion Guidance process introduced in POMONAG utilises four different guides, one for each Performance Predictor, to model the distribution of the architecture space. To optimise the corresponding scaling factors, POMONAG employs a dynamic approach rather than a fixed value as in DiffusionNAG. This optimisation process aims to maximise the mean of the estimated accuracies of the architectures in the Pareto Front by adjusting the scaling factors corresponding to each guide.\nThis optimisation leverages Optuna [30], incorporating a Tree-structured Parzen Estimator sampling method [31] alongside a Hyperband pruning mechanism [32]. The process involves 100 evaluations on four datasets (CIFAR10, CIFAR100, Aircraft, Oxford III Pets). This procedure is replicated for each search space.\nTo fully exploit the secondary metrics, POMONAG introduces a novel technique termed Pareto Front Stretching. This entails generating architectures in two phases, each with batches of 128 elements. The first batch favours efficient architectures with respect to the secondary metrics, assigning greater weight to the predictors of parameters, MACs, and inference latency. The second batch aims to obtain highly accurate architectures while still maintaining constraints on their complexity.\nTo implement these two generation processes, the optimisation of the scaling factors is executed twice with different constraints. For efficient architectures, the scaling factor for the accuracy guide is searched within the interval [1000, 5000], while for the other guides it is within [100, 500]. For highly accurate architectures, the scaling factor for the accuracy guide is optimised within [10, 000, 50,000], while for the other metrics within [10, 50].\nThe optimisation results yielded specific values for each search space. For NASBench201, the optimal scaling factors for efficient architectures were 4732, 482, 421, and 368, respectively for accuracy, parameters, MACs, and latency. For highly accurate architectures, the values are 24,943, 12, 26, and 13. In the case of MobileNetV3, for efficient architectures, the values obtained are 4987, 494, 478, and 481, while for highly accurate ones, 48,321, 21, 16, and 39.\nThis approach allows for a more thorough exploration of the solution space, generating an overall set of architectures more widely spread with respect to the secondary metrics. The effect is therefore a diversified generation of architectures and a Pareto Front with a more elongated shape. For more information, the reader is referred to Appendix B."}, {"title": "4 Experiments and Results", "content": "This section presents the experiments conducted and the results obtained to evaluate the performance of POMONAG compared to other NAS models, particularly DiffusionNAG. All experiments were conducted using an NVIDIA Quadro RTX 6000 graphics card.\nTransferable NAS Evaluation on MobileNetV3. In Table 1, the comparison between POMONAG and other Transferable NAS methods on the MobileNetV3 search space is presented. POMONAG achieves the best performance in terms of minimum, mean, and maximum accuracy on almost all the datasets considered. Notably, on the Aircraft dataset, POMONAG reaches an accuracy of 87.62%, significantly surpassing other methods. On CIFAR10, CIFAR100, and Oxford III Pets, POMONAG also demonstrates"}, {"title": "5 Conclusion and Future Directions", "content": "This paper has presented POMONAG, an innovative framework for generating neural architectures optimised in a many-objective context. By integrating advanced techniques like Many-Objective Reverse Diffusion Guidance, the creation of extended Meta-Datasets, and the adoption of improved Performance Predictors, POMONAG addresses and overcomes limitations of existing approaches, effectively combining many-objective and Transferable NAS paradigms. The conducted experiments demonstrate that POMONAG is capable of generating architectures offering an effective balance between accuracy and computational efficiency. Specifically, it achieves superior performance across various datasets and search spaces requiring training only the identified optimal architecture. As a future direction, extending POMONAG's approach to other computer vision tasks \u2013 such as segmentation and object detection \u2013 is envisaged. This trajectory aims to develop a foundational NAS model for computer vision tasks that is both task-aware and dataset-aware, capable of effectively adapting to a variety of applications, domains, and computational constraints. Such an advancement would further enhance the adaptability and applicability of NAS methodologies in diverse real-world scenarios."}, {"title": "A Meta-Datasets", "content": "This appendix describes the search spaces utilised in this work, the encoding and sampling strategies employed for the creation of the Meta-Datasets, and introduces the Meta-Datasets themselves."}, {"title": "A.1 NASBench201 Search Space", "content": "NASBench201, introduced by Dong et al. [28], is a standardised search space for Neural Architecture Search (NAS) tasks within the domain of image classification. This search space is confined to the composition of an optimal cell, which is used in series to construct the final architecture. A cell comprises four fixed nodes, representing the summation operation of the feature maps received as input, and six possible connections between the nodes. The connections are associated with operations for transforming and adapting the feature maps. The available operations are Zeroise, Skip connection, 1x1 2D Convolution, 3x3 2D Convolution, and 3x3 Average Pooling. Each convolution operation is followed by a Batch Normalisation layer and a ReLU activation. This leads to a total of $5^6 = 15, 625$ unique architectures within the NASBench201 search space. The evaluation of models in NASBench201 is conducted through full training on three datasets: CIFAR10, CIFAR100, and ImageNet16. For each architecture, metrics such as accuracy, cross-entropy loss, training time, number of parameters, MACs, and inference latency are provided.\nArchitecture Encoding and Sampling. The encoding of NASBench201 architectures used in this work involves two complementary matrices: an operations matrix and an adjacency matrix, as proposed by Dong et al. [28].\nThe operations matrix describes the transformations applied to the feature maps between the nodes of the graph. This 8\u00d77 matrix is structured such that each cell represents a possible edge of the directed acyclic graph. There are eight rows one for the input, six for the possible intermediate connections, and one for the output-and seven columns, one for each possible operation, including placeholders for the input and output. The matrix is binary: a '1' indicates that the connection is assigned the corresponding operation, and a '0' otherwise. In this sense, each row has exactly one '1', since each connection must be assigned an operation. Parallelly, the adjacency matrix defines the connection structure of the graph. Also of size 8\u00d78, this binary matrix is fixed for all architectures in NASBench201. The elements of the matrix are '1' to indicate the presence of a connection between two nodes summing the feature maps, and '0' to indicate the absence of a connection. Again, the first row and column represent the input, while the last row and column represent the output. The sampling of architectures is optimised by exploiting the knowledge of the evaluations present in NASBench201. Specifically, the top-250 most performant architectures are identified by calculating the average relative to the three datasets used by the authors for their evaluation. Sampling is conducted such that there is a 95% probability of sampling from this subset of high-performing architectures, and a 5% probability from the rest of the search space."}, {"title": "A.2 MobileNetV3 Search Space", "content": "The MobileNetV3 search space [29], implemented via Once-for-All (OFA) introduced by Cai et al. [9], is a flexible search space for NAS tasks in the domain of image classification on mobile devices. This search space is defined by a pre-trained super-network that supports various architectural configurations, allowing exploration of a wide range of sub-networks. The super-network comprises a sequence of inverted bottleneck blocks, similar to those in MobileNetV2 [40]. The main dimensions of the search space are:\n\u2022 Depth. Each stage of the network can have a variable number of blocks, typically chosen from 2, 3, 4.\n\u2022 Expansion ratio. The number of channels in each layer can be adjusted using two width multipliers.\n\u2022 Kernel size. For depthwise convolutions, the kernel size can be 3\u00d73, 5\u00d75, or 7\u00d77.\n\u2022 Input resolution. The input image can have variable dimensions, typically from 128\u00d7128 to 224x224.\nThe input resolution is fixed at 224x224. Each of the five stages in the super-network, as provided by the original implementation, can include a Squeeze-and-Excitation module and use different activation functions (ReLU or h-swish) based on its position within the architecture. This approach leads to a search space comprising approximately $10^{19}$ unique architectures. The evaluation of models in this space is conducted through the selection of subgraphs of the pre-trained super-network, eliminating the need to train each architecture from scratch.\nArchitecture Encoding and Sampling. As with NASBench201, the encoding of MobileNetV3 architectures used in this work involves representations related to the operators of the architecture and the internal connections within the network. The operators are represented by a 21\u00d79 matrix, with the first row dedicated to the placeholder of the width multiplier, followed by one row for each of the 20 blocks within the architecture. If the width multiplier is set to 1.0, then the first row is populated with zeros; if it is set to 1.2, the first row is populated exclusively with ones. From the second row onwards, the columns are divided into three groups of three, each representing a combination of expansion ratio and kernel size. The first group corresponds to an expansion ratio of 3, the second to 4, and the third to 6. Within each group, the three columns represent kernel sizes of 3\u00d73, 5\u00d75, and 7\u00d77, respectively. Each row of the matrix contains a '1' in the position corresponding to the chosen combination of expansion ratio and kernel size for that block, and zeros elsewhere. If a block is inactive (as indicated by the depth mask), its corresponding row is filled with zeros. The adjacency matrix, on the other hand, represents the connections between the blocks of the architecture. It is a square matrix of size 20\u00d720, where each element (i, j) is '1' if there is a direct connection from block i to block j, and '0' otherwise. The structure of this matrix reflects the topology of the network, taking into account the variable depth of each stage. For this search space, the sampling of architectures is conducted uniformly with respect to the values of the width multiplier, depth, expansion ratio, and kernel size, drawing from a possible number of architectures equal to $2 \u00d7 10^{19}$ (the factor of 2 arises from the addition of the width multiplier)."}, {"title": "A.3 Meta-Datasets Distributions", "content": "Figure 2 illustrates the distributions of key metrics for the Meta-Datasets created in this study. For the NASBench201 search space, 10,000 architectures were sampled, while 20,000 were selected for the MobileNetV3 space. These architectures were sampled from their respective search spaces and then trained using the proposed pipelines on a dataset of 20 classes with 1,000 samples per class, randomly extracted from ImageNet32. The reported accuracy performance is based on a test set of 10 samples per class for the same 20 classes. The distributions shown are consistent with the characteristics of their respective search spaces. Notably, the graphs clearly demonstrate that MobileNetV3 architectures are, as expected, more complex yet relatively lightweight, and also more performant. This is due to their incorporation of more advanced architectural designs compared to NASBench201 architectures, as well as the benefit of fine-tuning during training."}, {"title": "B Extensive Results", "content": "Figures 3 and 4 present a comprehensive comparison of neural architectures generated by DiffusionNAG and POMONAG across 15 diverse datasets: CIFAR10, CIFAR100, Aircraft, Oxford III Pets, BloodMNIST, DermaMNIST, EuroSAT, FashionMNIST, OCTMNIST, OrganAMNIST, OrganCMNIST, PathMNIST, STL10, TinyImageNet, and TissueMNIST. The comparisons are made using the NASBench201 and MobileNetV3 search spaces, respectively. The plots illustrate the trade-offs between accuracy and secondary metrics (parameters, MACs, and latency) for each dataset. POMONAG's results are represented by Pareto Fronts, showcasing the range of optimal solutions. Three key variants are highlighted: Efficient, emphasising minimal secondary metric; Accurate, prioritising highest predicted accuracy; and Balanced, optimising the ratio of predicted accuracy to secondary metric. DiffusionNAG's results, both reproduced (orange points) and as reported by An et al. [26] (dashed lines), provide a benchmark for comparison. The 95% confidence intervals, averaged over three runs, underscore the robustness of the results. These visualisations demonstrate POMONAG's capability to generate a diverse set of architectures, effectively balancing performance and efficiency across various datasets and search spaces. The consistent outperformance of POMONAG, particularly in terms of Pareto-optimal solutions, highlights its effectiveness in navigating complex Neural Architecture Search spaces. It is important to note that the performances of POMONAG's architectures, as well as those replicated from DiffusionNAG and represented in both figures, were explicitly calculated and are not estimates from the Performance Predictors."}, {"title": "C Performance Predictors Ablation Study", "content": "An ablation study was conducted to evaluate the improvements introduced in the Performance Predictors on the NASBench201 search space. The study involved a series of modifications to the predictors and their training strategies, aiming to maximise the Spearman correlation between the predicted values and the actual metrics.\nInitially, a new predictor was implemented using the same training pipeline as An et al. [26], which yielded slight improvements in correlation scores over the baseline DiffusionNAG predictor. Specifically, the Spearman correlation for the accuracy of noisy architectures increased from 0.687 to 0.705, and for denoised architectures from 0.767 to 0.772. Subsequently, the introduction of a novel training strategy significantly enhanced performance for both noisy and denoised architecture accuracy estimates. This new training strategy involved switching from the Adam optimiser to AdamW, incorporating a cosine annealing learning rate scheduler, and introducing a weight decay of $5 \u00d7 10^{-3}$. These modifications improved convergence and generalisation of the Performance Predictors, leading to Spearman correlations of 0.822 for noisy architectures and 0.854 for denoised architectures.\nExpanding the Meta-Dataset size from 4,630 to 10,000 architectures further improved the results, with correlations reaching 0.842 for noisy architectures and 0.884 for denoised ones. This demonstrates the benefit of a larger and more diverse training set, providing the Performance Predictors with a richer array of examples to learn from. Finally, replacing the ResNet18 Dataset Encoder with a Vision Transformer (ViT-B-16) feature extractor led to the best performance, achieving Spearman correlations of 0.855 and 0.884 for noisy and denoised architectures, respectively. These results indicate that both the architectural choices and the training strategies for the Performance Predictors have a substantial impact on their effectiveness, directly influencing the quality of guidance provided during the diffusion process."}, {"title": "D Hyperparameter Tuning and Optimisation", "content": "The training of the architectures included in the Meta-Datasets required meticulous hyperparameter tuning to maximise performance. For each search space \u2013 NASBench201 and MobileNetV3 a comprehensive hyperparameter search was conducted to identify optimal training configurations. The hyperparameter search involved exploring a range of values for key training parameters, such as the number of epochs, warm-up epochs, optimisers, learning rates, learning rate schedulers, weight decay, label smoothing, and various data augmentation techniques. The search space for these hyperparameters is summarised in Table 7, providing an overview of the parameters considered during the optimisation process. The optimal training pipelines identified for each search space are detailed in Table 8."}]}