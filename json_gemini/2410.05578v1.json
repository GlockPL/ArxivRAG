{"title": "Swift Sampler: Efficient Learning of Sampler by 10 Parameters", "authors": ["Jiawei Yao", "Chuming Li", "Canran Xiao"], "abstract": "Data selection is essential for training deep learning models. An effective data sampler assigns proper sampling probability for training data and helps the model converge to a good local minimum with high performance. Previous studies in data sampling are mainly based on heuristic rules or learning through a huge amount of time-consuming trials. In this paper, we propose an automatic swift sampler search algorithm, SS, to explore automatically learning effective samplers efficiently. In particular, SS utilizes a novel formulation to map a sampler to a low dimension of hyper-parameters and uses an approximated local minimum to quickly examine the quality of a sampler. Benefiting from its low computational expense, SS can be applied on large-scale data sets with high efficiency. Comprehensive experiments on various tasks demonstrate that SS powered sampling can achieve obvious improvements (e.g., 1.5% on ImageNet) and transfer among different neural networks. Project page: https://github.com/Alexander-Yao/Swift-Sampler.", "sections": [{"title": "1 Introduction", "content": "Training data plays a pivotal role in deep learning tasks. The sampling probability of data in the training process can significantly influence the performance of the learned model. A set of works [Jiang et al., 2019, Katharopoulos and Fleuret, 2018, Needell et al., 2014, Johnson and Guestrin, 2018, Han et al., 2018, Hu et al., 2024] have demonstrated improvements in model training by sampling data according to different features and under different rules. These studies reveal that how to sample examples during training is nontrivial and sampling data in a totally uniform way is not always the optimal choice.\nPrevious works on data sampling strategy include two main categories: human-defined rules and learning-based methods. Some works [Han et al., 2018, Jiang et al., 2019, Katharopoulos and Fleuret, 2018, Hacohen and Weinshall, 2019, Kumar et al., 2010] proposed manually designed rules, such as setting the sampling probability of training examples with loss values larger than a threshold as zero, or making the sampling probability proportional to the gradient norms. Such human-defined rules are designed for specific tasks and hardly adapts to different scenarios, because optimal sampling strategy varies among different tasks and data sets. Learning-based methods [Fan et al., 2017, Jiang et al., 2017, Ren et al., 2018] explore an automatic way to assign sampling probability to a given learning task, including sample-based and differential based methods. Sample-based methods take advantage of deep reinforcement learning (DRL) [Fan et al., 2017] to model the training process of the target model as an environment and use a DRL model to learn the optimal sampling strategy during training. These methods need hundreds of replays of the training process and require too much searching cost to be applied on large-scale data sets, e.g., ImageNet [Russakovsky et al., 2015]. Differential based methods [Ren et al., 2018, Shu et al., 2019] assume a completely clean meta dataset and use the inner"}, {"title": "2 Related Work", "content": "Hard-wired Methods Hard-wired methods have fixed sampling rules and focus on a few particular problems, e.g., imbalance, noise and training speed up. Each problem needs respective hand-crafted rules and the designs are based on specific understandings of data. Thus, these methods hardly generalizes over a wide range of data sets. In [Han et al., 2018], the sampling method focuses on denoising problem, and neglects instances with loss values larger than a gradually increasing"}, {"title": "3 Method", "content": "We introduce SS in this section. First, we describe the problem of sampler search in a bilevel optimization framework, consisting of the outer loop and the inner loop. The outer loop uses an agent to learn to sample the sampling probabilities of training instances and optimizes the performance of the target model when trained under the sampled sampling probabilities. And the inner loop minimizes the loss of model parameters on the training set, with sampling probabilities given by the outer loop. Further, we formulate the sampler, define its search space, and design a transform function to smooth the curvature of the objective function of the outer loop. Then, we introduce our agent for the learning of sampling in the outer loop. Finally, we propose a highly efficient method to approximate the local minima of given sampling probabilities rather than training from scratch."}, {"title": "3.1 Problem Formulation", "content": "A common practice of training deep neural networks (DNN) is using mini-batch SGD to update the network parameters. The batches are formed by uniformly sampling from the training set. Sampling methods usually take different settings where the sampling probability of training instances are optimized [Hacohen and Weinshall, 2019, Kumar et al., 2010, Jiang et al., 2015, Ma et al., 2017, Jiang et al., 2019, Katharopoulos and Fleuret, 2018, Needell et al., 2014, Johnson and Guestrin, 2018]. In this work, we focus on finding a static sampling probability which guides the parameters of target DNN to the local optimum with the best performance on the validation set. We formulate this problem as follows."}, {"title": "3.2 Sampler Formulation", "content": "The complexity of the optimization problem increases exponentially with the dimension of t, which is the number of training data. However, modern hyper-parameter optimization methods like BO can hardly handle more than 30 hyper-parameters [Brochu et al., 2010]. Hence we reduce the dimension of 7 by a simple formulation. The high dimension comes from independence among training samples. However, we assume that the optimal sampler lies in a more compact subdomain. Instead, we restrict that the difference in the sampling probabilities of two training instances is bounded by their distance in the feature space, e.g., the (loss, entropy) space.\nExample As an intuitive example, we consider cropped images in ImageNet [Russakovsky et al., 2015] dataset as samples and define a one-dimension feature space where the feature is the loss value. Cropped images with the highest loss values have small distance between each other in this feature space. Further, as we show in Sec. 4, most of them are noisy instances, which should all be assigned sampling probabilities near 0. It means they also have small sampling probabilities difference.\nMathematically, the intuition above can be formulated as constraining samplers to satisfy Lipschitz condition:\n$|\\tau (x_i) - \\tau (x_j)| \\leq C \\cdot || f(x_i) - f(x_j)||_2,$\nwhere $|| f(x_i) - f(x_j) ||_2$ is the $L_2$ distance in the space of feature vector f and C is a real positive number.\nDimension Reduction With the above consideration, we define t (x) as a multivariate continuous function of the features of instance x, described by\n$\\tau (x) = F (f_1 (x), f_2 (x), ... f_n (x)),$\nwhere F is a multivariate continuous function and $f_i (x)$ is i-th feature of example x (i = 1, ..., N). The choices of features have been explored in a wide range, e.g., loss in [He and Garcia, 2009, Maciejewski and Stefanowski, 2011, Jiang et al., 2019, Katharopoulos and Fleuret, 2018], and density in [Fan et al., 2017]. Our choice of features is discussed in the latter sections.\nWe consider a family of F which have flexible expression and low dimension. F are formed by: (1) a univariate piecewise linear function H defined on the period [0, 1], (2) a univariate transform function T which balances the density of the gradient of instances along the period [0, 1] to smooth the objective function of the agent and (3) an multivariate aggregation function G (x) which aggregates"}, {"title": "3.3 Optimization", "content": "Bayesian Optimization For simplicity, we define z = [eo, ..., es, vo, ..., VS, C1, ..., CN], and replace P (Dv; w* (\u03c4)) with P (z) in the latter discussion because 7 and z are one-to-one mapped. In the outer loop, to find maxima z* of P with as few trials as possible, we explore the advantage of Bayesian Optimization [Brochu et al., 2010]. Bayesian Optimization is an approach to optimize objective functions that take a long time (e.g., hours) to evaluate. It is best-suited for optimization over continuous domains with the dimension less than 30, and tolerates stochastic noise in function evaluations, which are exactly characteristics of our problem. Recent works proved the potential of Bayesian Optimization (BO) on hyper-parameter tuning of DNN [Klein et al., 2016, Snoek et al., 2012]. Given the black-box performance function P : Z \u2192 R, BO aims to find an input z* = arg maxzez P (z) that globally maximizes P (z).\nBO requires a prior p (P) over the performance function, and an acquisition function ap : Z \u2192 R quantifying the utility of an evaluation at any z, depending on the prior p. With these ingredients, the following three steps are iterated [Brochu et al., 2010]: (1) find the most promising Zt+1 \u2208 arg max ap (z) by numerical optimization; (2) evaluate the expensive and often noisy function Qt+1 ~ P(t+1)+N (0, 0\u00b2) and add the resulting data point (2t+1, Qt+1) to the set of observations Ot = (zj, Qj), j = 1, ..., t; (3) update the prior p (P|Ot+1) and acquisition function ap(P|Ot+1) with new observation set Ot+1. Typically, evaluations of the acquisition function a are cheap compared to evaluations of P such that the optimization effort is negligible.\nGaussian Processes Gaussian processes (GP) are a prominent choice for p (P), thanks to their descriptive power and analytic tractability [Brochu et al., 2010]. Formally, a GP is a collection of random variables, such that every finite subset of them follows a multivariate normal distribution.\nTo detail the distribution, a GP is identified by a mean function m (often set to m (z) = 0), and a positive definite covariance function (kernel) k (often set to RBF kernel [Brochu et al., 2010]). Given history observation Ot, the posterior p (P|Ot) follows normal distribution according to the above definition, with mean and covariance functions of tractable, analytic form. It means we can estimate the mean and variance of P on a new point zt+1 by marginalize over p (P|Ot). The mean and the variance denote the expected performance of and the potential of Zt+1.\nAcquisition Function The role of the acquisition function is to trade off expected performance and potential by choosing next tried point 2t+1. Popular choices include Expected Improvement (EI), Upper Confidence Bound (UCB), and Entropy Search (ES) [Brochu et al., 2010].\nIn our method, following the popular settings in [Brochu et al., 2010], we choose GP with RBF kernel [Brochu et al., 2010] and a constant m function whose value is the mean of performance P(zt) of all tried samples. For the choice of acquisition function, we use UCB in all our experiments."}, {"title": "3.4 Local Minima Approximation", "content": "The critical problem of the inner loop is how to get w* (7) at an acceptable computational cost. Training from scratch is too computationally expensive to be used on a large data set. Hence, we design a method to obtain an approximation of w* (7) at a limited cost."}, {"title": "4 Experiment", "content": "In this section, we choose three classification and face recognition tasks as benchmarks to illustrate the effectiveness of SS: CIFAR10, CIFAR100, ImageNet, and MS1M, including data sets with both small and large sizes.\nIn all experiments, the optimization step E, is fixed as 40, and the fine-tune epochs Ef are set to 5. We set the number of segments S as 4 in all cases and utilize 8 NVIDIA A100 GPUs to ensure efficient processing. We tuned these hyper-parameters by separately increasing them until negligible improvements are obtained on ImageNet. The features and shared start points wshare are from the pre-trained models with the same architectures as the target models. We norm the sum 7 to 1 when use it to sample. For the choice of features, we consider the following two features: (1) Loss: Training loss is frequently utilized in curriculum learning, hard example mining and self-paced method. It is a practicable discriptor and is usually viewed as signals of hardness or noise. For our classification benchmarks, we use the Cross Entropy (CE) loss. (2) Renormed Entropy: The entropy of predicted probabilities is also widely used in current methods. To decouple it from CE loss, we delete the probability of target label from the probability vector, and renorm the rest ones in the vector to 1. Then we use the resulted renormed vector to calculate the entropy:\n$E_r (x_i) = \\frac{\\sum_{j \\neq y_i} -p_j \\log (\\frac{p_j}{\\sum_{j \\neq y_i} p_j})}{\\sum_{j \\neq y_i} p_j};$\nwhere pj denotes the predicted probability of the j-th class. A small Er results from a peak in the distribution over the rest vector, which often implies a misclassification, while a large Er implies a hard instance."}, {"title": "4.1 CIFAR Experiment", "content": "CIFAR10 and CIFAR100 are two well-known classification tasks. We explore the performance of our method with the target model ResNet18 on both the original data sets and the data sets with four ratios of noisy labels, 10%, 20%, 30%, and 40%. When generating noisy labels, we random sample instances in each class with the given ratios and uniformly change their labels into the rest incorrect"}, {"title": "4.2 ImageNet Experiment", "content": "Benefiting from the high efficiency of SS, we implement it on a large classification dataset, ImageNet ILSVRC12. We conduct experiments on it with models of different sizes, including MobileNet-v2 [Sandler et al., 2018], ResNet-18, ResNet-50, ResNet-101 [He et al., 2016], SE-ResNext-50, SE- ResNext-101 [Hu et al., 2018], Swin-T and Swin-B [Liu et al., 2021]. Because the pipeline of the"}, {"title": "4.3 Face Recognition Experiment", "content": "We further apply SS on a face recognition task where the training set is MS1M [Guo et al., 2016] and test set is YTF [Sengupta et al., 2016]. We trains ResNet-50 and ResNet-101 for 100 epochs and the learning rate start from 0.1 and drop to 0 with the cosine scheduler. We set momentum to 0.9 and weight decay to 5e - 4. Results in Tab. 3 implies SS's generality in improvements among tasks."}, {"title": "4.4 Ablation Study", "content": "In this section, we verify the effectiveness of our designs of the optimization in both the outer and the inner loops.\nInner Loop Verification A critical question is, to what extent the performance of the approximated local minima is consistent with the real minima of training from scratch. One of the appropriate"}, {"title": "5 Conclusion", "content": "In this paper, an automatic sampler search method called SS is proposed. We describe the sampler search problem in a bilevel way and construct a sampler search space with a low dimension and a flexible expression. We design objective function smoothness and local minima approximation methods separately for the outer and inner loop, achieving a low computational cost of the search. Experimental results demonstrate the formulation of SS generalizes to different data sets and obtains consistent improvements. The low computation cost facilitates the SS to boost the target model on a large dataset ImageNet. Further, resulted sampler shows a good ability to transfer between models."}, {"title": "A Appendix", "content": "We provide a theoretical analysis section focusing on the bounds of the representation error of SS algorithm.\nThe true sampling function (x) is defined as:\n$\\tau(x) = F(f_1(x), f_2(x), ..., f_n(x))$\nwhere f(x) are the features of instance x.\nThe approximation \u2191(x) is defined as:\n$\\uparrow(x) = H(T(G(x)))$\nwhere G(x) = $\\sum_{i=1}^N C_if_i(x)$.\nAssuming F is Lipschitz continuous with constant L:\n$|F(f(x)) - F(f(y))| \\leq L \\cdot || f(x) - f(y)||_2$\nThe representation error \u20ac(x) is bounded by:\n$\u20ac(x) = |F(f(x)) - H(T(G(x)))|$\n$\u20ac(x) \\leq |F(f(x)) - F(f(y))| + |F(f(y)) - H(T(G(x)))|$\n$\u20ac(x) \\leq L \\cdot || f(x) - f(y)||_2 + |F(f(y)) - H(T(G(x)))|$\nAssuming f(y) = f(x):\n$\u20ac(x) \\leq L \\cdot || f(x) - f(x)||_2+ |F(f(x)) - H(T(G(x)))|$\nSince H(T(G(x))) approximates F(f(x)):\n$\u20ac(x) \\leq L \\cdot || f(x) - f(x)||_2 + \u20ac'$\nThis bound indicates that the error introduced by our low-dimensional representation is controlled by the Lipschitz constant of the sampling function F and the error in the feature space representation, plus a small approximation error \u20ac'.\nA.2 Practical and Challenging Scenarios\nThese additional experiments demonstrate the practical benefits of SS in both large-scale and limited data scenarios.\nFoundation Model Training on Large-Scale Datasets. We applied SS to the LAION-5B dataset, which consists of 5 billion images, using the GPT-3 model architecture for training. We focused on a subset of 500 million images to test the feasibility and effectiveness of SS. The training was conducted on a cluster with 32 NVIDIA A100 GPUs. Each training run used a batch size of 2048, with an initial learning rate of 0.1, decayed by 0.1 at the 30th, 60th, and 90th epochs, over a total of 100 epochs. As shown in Tab. 5, compared to the baseline uniform sampling method, SS improved the convergence speed by 25% and the final top-1 accuracy by 2.3% (from 72.4% to 74.7%)."}, {"title": "A.3 Training Time Variation", "content": "We conducted additional experiments to measure the relative training time for each sampling method used in Tab. 7."}, {"title": "A.4 Parameter Analysis", "content": "We evaluated the impact of varying the number of segments S on the performance of our method. The experiments were conducted on the CIFAR10 dataset with a noise rate of 20%. We tested S = 2, 4, 6, 8."}, {"title": "A.5 Further Analysis", "content": "How sensitive is the performance to the quality of the initial model? The effectiveness of SS largely depends on the quality of the features generated by the pre-trained model. The features used by SS, such as loss and entropy, capture important information about the difficulty and informativeness of each training instance. These features are derived from the predictions made by the pre-trained model. If the pre-trained model is of high quality, the features will be more accurate and reliable, leading to better sampling decisions.\nTo empirically evaluate the sensitivity of SS to the quality of the pre-trained model, we conducted additional experiments using pre-trained models of varying quality on the CIFAR10 dataset with a noise rate of 20%. Specifically, we used three different backbone models available from widely-used model repositories::\n1. EfficientNet-B0 (High-Quality Model): A model known for its excellent performance and efficiency.\n2. ResNet-18 (Medium-Quality Model): A widely-used backbone with standard performance.\n3. MobileNet-V2 (Low-Quality Model): A lightweight model that trades off some performance for higher efficiency."}]}