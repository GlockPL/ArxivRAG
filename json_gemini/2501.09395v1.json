{"title": "ELM-DEEPONETS: BACKPROPAGATION-FREE TRAINING OF DEEP OPERATOR NETWORKS VIA EXTREME LEARNING MACHINES", "authors": ["Hwijae Son"], "abstract": "Deep Operator Networks (DeepONets) are among the most prominent frameworks for operator\nlearning, grounded in the universal approximation theorem for operators. However, training Deep-\nONets typically requires significant computational resources. To address this limitation, we propose\nELM-DeepONets, an Extreme Learning Machine (ELM) framework for DeepONets that leverages\nthe backpropagation-free nature of ELM. By reformulating DeepONet training as a least-squares prob-\nlem for newly introduced parameters, the ELM-DeepONet approach significantly reduces training\ncomplexity. Validation on benchmark problems, including nonlinear ODEs and PDEs, demonstrates\nthat the proposed method not only achieves superior accuracy but also drastically reduces computa-\ntional costs. This work offers a scalable and efficient alternative for operator learning in scientific\ncomputing.", "sections": [{"title": "Introduction", "content": "The emergence of Physics-Informed Machine Learning (PIML) has revolutionized the intersection of computational\nscience and artificial intelligence [1]. PIML frameworks integrate physical laws, often expressed as partial differential\nequations (PDEs), into machine learning models to ensure that the predictions remain consistent with the underlying\nphysics. This paradigm shift has enabled efficient and accurate solutions to complex, high-dimensional problems that\nwere traditionally intractable with conventional numerical methods [2, 3, 4].\nPhysics-Informed Neural Networks (PINNs) have been at the forefront of PIML applications [5]. By incorporating\ngoverning equations as soft constraints within the loss function, PINNs solve forward and inverse problems for PDEs\nwithout requiring large labeled datasets [6, 7, 8, 9]. Despite their versatility, PINNs face significant challenges,\nparticularly in scenarios requiring frequent retraining. The need to repeatedly optimize network parameters from the\ninitialization for varying problem instances renders PINNs computationally expensive and time-consuming.\nIn response to these limitations, Deep Operator Networks (DeepONets) [10], and Neural Operators (NOs) [11], have\nemerged as powerful alternatives. Unlike PINNs, which solve individual instances of PDEs, DeepONets and NOs\nlearn mappings between function spaces, enabling real-time inference across a wide range of inputs. This operator\nlearning approach eliminates the need for retraining, making it particularly suitable for applications requiring repeated\nevaluations, such as parameter studies and uncertainty quantification. However, the training of NOs and DeepONets\ninvolves substantial computational overhead due to the large neural network architectures and the need for expensive\ntraining data.\nA brief comparison reveals distinct differences between DeepONets and NOs. While NOs, such as the Fourier Neural\nOperator (FNO) [12], utilize global representations of functions via Fourier transforms [12], DeepONets rely on\nbranch and trunk networks to approximate operators as a linear combination of basis functions [10]. More specifically,\nDeepONets parameterize the target functions as functions of the input variable, so that the physics-informed training\nparadigm can directly be incorporated [13, 14]. Each method has unique strengths, but both share the challenge of\nexpensive training processes, motivating the need for more efficient alternatives.\nDeep Operator Networks (DeepONets) have proven to be highly effective as function approximators, particularly in the\ncontext of operator learning. Inspired by the universal approximation theorem for operators, DeepONets approximate\nmappings between infinite-dimensional function spaces by decomposing the problem into two networks: a branch\nnetwork that encodes the input function's coefficients and a trunk network that evaluates the operator at target points.\nThis structure allows DeepONets to generalize across a variety of input-output relationships, making them suitable for\nsolving both forward and inverse problems in scientific computing.\nVariants of DeepONets have been developed to enhance their approximation capabilities. [15] expands DeepONets by\nallowing flexible input function representations, enabling robust operator learning across diverse scenarios. Fixed basis\nfunction approaches, such as the Finite Element Operator Network and Legendre Galerkin Operator Network [16, 17],\nincorporate predefined basis functions to improve efficiency and accuracy. These modifications leverage domain\nknowledge to reduce the computational burden while maintaining flexibility in approximating complex operators.\nDespite these advances, the training process remains a bottleneck, necessitating the exploration of alternative training\nmethodologies.\nExtreme Learning Machine (ELM) is a feed-forward neural network architecture designed for fast and efficient learning.\nIntroduced to overcome the limitations of traditional learning algorithms, ELM operates with a Single-Layer Fully\nconnected Networks (SLFNs) where the weights between the input and hidden layers are randomly initialized and\nfixed. This unique feature eliminates the need for iterative tuning of these weights, significantly reducing computational\ncost. The output weights are determined analytically by minimizing the training error, e.g., least squares, making ELM\ntraining extremely fast compared to conventional methods like backpropagation. Its simplicity, efficiency, and capacity\nfor handling large-scale datasets make ELM a powerful tool in various domains, including regression, classification,\nand feature extraction.\nThe Universal Approximation Theorem for ELMs establishes that SLFNs with randomly generated hidden parameters\ncan approximate any continuous function on a compact domain, given a sufficient number of neurons [18]. This\ntheoretical foundation has spurred interest in applying ELMs to a variety of tasks, including classification, and\nregression [19, 20, 21]. More recently, physics-informed learning. ELMs have demonstrated promising results when\napplied to PINN frameworks, reducing training complexity while maintaining accuracy [22, 23]. This success motivates\nthe application of ELMs to operator learning, particularly for DeepONets.\nIn this work, we present a novel methodology ELM-DeepONets that combines the strengths of ELM and DeepONets to\naddress the computational challenges in operator learning. Our approach leverages ELM's efficiency to train DeepONets\nby solving a least squares problem, bypassing the need for expensive gradient-based optimization. The proposed\nELM-DeepONets framework is validated on a diverse set of problems, including nonlinear ODEs and forward-inverse\nproblems for benchmark PDEs. Our experiments demonstrate that the method achieves comparable accuracy to\nconventional DeepONet training while significantly reducing computational costs. Additionally, we highlight the\npotential of ELM as a lightweight and scalable alternative for operator learning in scientific computing.\nThe remainder of this paper is organized as follows. Section 2 briefly introduces important preliminaries to our method.\nSection 3 details the mathematical framework and implementation of the ELM-based DeepONet. Section 4 presents the\nresults of numerical experiments, showcasing the effectiveness of the proposed method. Finally, Section 5 concludes\nwith a discussion of potential extensions and future research directions."}, {"title": "Preliminaries", "content": ""}, {"title": "Extreme Learning Machines", "content": "In this subsection, we briefly outline the ELM with SLFN for the regression problem. Let $X = [x_1, x_2,...,x_N]^T\\in\\mathbb{R}^{N\\times d}$ and $Y = [Y_1, Y_2,\\ldots, Y_N]^T \\in \\mathbb{R}^{N\\times 1}$ be the input data and label, respectively, where $N$ is the number of samples\nand $d$ is the input dimension. The network maps $x_i$ to an output using $p$ hidden neurons with randomly initialized and\nfixed weights $W_1 \\in \\mathbb{R}^{d\\times p}$. The hidden layer output is computed as $H = \\sigma(XW_1)$, where $\\sigma(\\cdot)$ is an activation function\napplied in an elementwise manner. The output $\\hat{Y}$ is computed by $\\hat{Y} = HW_2 = \\sigma(XW_1)W_2$ where $W_2 \\in \\mathbb{R}^{p\\times 1}$. For\nfixed $W_1$ and $b$, we obtain $W_2$ by least square fit to the label $Y$, i.e., $W_2 = H^{\\dagger}Y$ where $H^{\\dagger}$ is the Moore-Penrose\npseudoinverse of $H$."}, {"title": "A brief introduction to DeepONets", "content": "DeepONet is a recently proposed operator learning framework that is usually trained in a supervised manner. The\nsupervised dataset consists of labeled pairs of functions $\\mathcal{D} = \\{(u_i, F(u_i)(y_j))\\}_{i,j=1}^{N,M}$, where $N$ input functions $u_i$ are\ncharacterized by $m$-dimensional vector $u_i = (u_i(x_1), u_i(x_2), ..., u_i(x_m))$ at $m$ discrete sensor points $x_1, x_2, ..., x_m$\nand the target function $F(u)$ is evaluated at $M$ collocation points $y_1, y_2, \\cdot \\cdot \\cdot, y_M$.\nDeepONet comprises two subnetworks, the branch network and the trunk network. The branch network takes the\n$m$-dimensional vector $u_i = (u_i(x_1), u_i(x_2),..., u_i(x_m))$ and generates $p$-dimensional vector $b = (b_1,b_2, ..., b_p)$.\nOn the other hand, the trunk network takes the collocation point $y$ as input to generate another $p$-dimensional vector\n$t = (t_1, t_2, . . ., t_p)$. Finally, we take the inner product of $b$ and $t$ to generate the output $F(u)(y) = \\sum_{k=1}^{p}b_k(u)t_k(y)$.\nThen, two subnetworks are trained to minimize the loss function defined by:\n$\\mathcal{L}(\\mathcal{D}) = \\frac{1}{2N}\\sum_{i=1}^{N}\\sum_{j=1}^{M} ||F(u_i)(y_j) - \\hat{F}(u_i)(y_j)||^2$\n$\\mathcal{L}(\\mathcal{D}) = \\frac{1}{2N}\\sum_{i=1}^{N}\\sum_{j=1}^{M} ||\\sum_{k=1}^{p}b_k(u_i)t_k(y_j) - \\hat{F}(u_i)(y_j)||^2$."}, {"title": "ELM-DeepONets", "content": "ELM employs fixed basis functions generated by random parameters and determines the coefficients for their linear\ncombination. Similarly, DeepONet constructs a linear combination of basis functions produced by the trunk network,\nwith the coefficients generated by the branch network. Building on the structural similarity between ELM and DeepONet,\nwe propose a novel framework called ELM-DeepONet.\nRecall that the output of DeepONet is expressed as:\n$\\hat{F}(u)(y) = \\sum_{k=1}^{p}b_k(u)t_k(y)$,\nwhere $b_k(u), k = 1, 2, . . ., p$, are the coefficients generated from the branch network and $t_k(y), k = 1, 2, . . ., p$, are the\nbasis functions generated from the trunk network. To incorporate the fixed basis functions of ELM into DeepONet, we\nmodel $t_k(y)$ using a fully connected neural network with fixed parameters. This modification aligns the trunk network\nwith the ELM philosophy by removing the need to train its parameters. In the standard ELM framework, the coefficients\nare determined by solving a simple least squares problem. However, in DeepONet, the coefficients $b_k(u)$ are functions\nof the input $u$, introducing additional complexity that makes it challenging to directly apply the ELM methodology. We\naddress this issue by incorporating an ELM to model the coefficients.\nLet\n$\\hat{G}(u) = \\sum_{k=1}^{p_2}c_k(u)t_k(y)$,\nbe the ELM-DeepONet output, where $t_k(y)$, for $k = 1, 2, . . ., p_2$ are the basis functions modeled by the trunk network\nwith fixed parameters, and $p_2$ is a hyperparameter. To compute $c_k(u)$, we use another fully connected neural network\nwith fixed parameters to generate $b_k(u)$ for $k = 1, 2, . . ., p_1$, and define $c_k(u)$ as:\n$\\begin{pmatrix}\nc_1(u) \\\\\nc_2(u) \\\\\n\\vdots \\\\\nc_{p_2}(u)\n\\end{pmatrix} = \\begin{pmatrix}\nW_{11} & W_{12} & \\ldots & W_{1p_1} \\\\\nW_{21} & W_{22} & \\ldots & W_{2p_1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nW_{p_21} & W_{p_22} & \\ldots & W_{p_2p_1}\n\\end{pmatrix} \\begin{pmatrix}\nb_1(u) \\\\\nb_2(u) \\\\\n\\vdots \\\\\nb_{p_1}(u)\n\\end{pmatrix}$,\nwhere $W \\in \\mathbb{R}^{p_2\\times p_1}$ is a learnable parameter. Consequently, the output of ELM-DeepONet can be expressed as:\n$\\hat{G}(u)(y) = \\sum_{k=1}^{p_2}c_k(u)t_k(y)$,\n$= \\sum_{k=1}^{p_2}(\\sum_{l=1}^{p_1} W_{kl}b_l(u))t_k(y)$,\nwhere $p_1$ and $p_2$ are hyperparameters that will be discussed in detail later. In this novel framework, the only learnable\nparameter is $W$, while all the parameters in $b_k(u)$ and $t_k(y)$ remain fixed. This design drastically reduces the number\nof trainable parameters in ELM-DeepONet to $p_1p_2$ which is significantly smaller than that of the vanilla DeepONet.\nThe overall architecture is illustrated in 3.\nLet $\\mathcal{D} = \\{(u_i, \\hat{G}(u_i)(y_j))\\}_{i,j=1}^{N,M}$ be a supervised dataset for training ELM-DeepONet. Then the objective\n$\\mathcal{L} = \\sum_{i,j=1}^{N,M} L_i = \\sum_{i=1}^{N}\\sum_{j=1}^{M} (\\hat{G}(u_i) (y_j) - G(u_i) (y_j))^2$\ncan be expressed as:\n$\\mathcal{L} = ||TWB - G||_F$,\nwhere $|| \\cdot ||_F$ denotes the Frobenius norm, and\n$T=\\begin{pmatrix}\nt_1(y_1) & t_2(y_1) & \\ldots & t_{p_2} (y_1) \\\\\nt_1(y_2) & t_2(y_2) & \\ldots & t_{p_2} (y_2) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nt_1(y_M) & t_2(y_M) & \\ldots & t_{p_2}(y_M)\n\\end{pmatrix} \\in \\mathbb{R}^{M\\times p_2},$\n$W = \\begin{pmatrix}\nW_{11} & W_{12} & \\ldots & W_{1p_1} \\\\\nW_{21} & W_{22} & \\ldots & W_{2p_1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nW_{p_21} & W_{p_22} & \\ldots & W_{p_2p_1}\n\\end{pmatrix} \\in \\mathbb{R}^{p_2\\times p_1},$\n$B = \\begin{pmatrix}\nb_1(u_1) & b_1(u_2) & \\ldots & b_1(u_N) \\\\\nb_2(u_1) & b_2(u_2) & \\ldots & b_2(u_N) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{p_1}(u_1) & b_{p_1}(u_2) & \\ldots & b_{p_1}(u_N)\n\\end{pmatrix} \\in \\mathbb{R}^{p_1\\times N},$\n$G = \\begin{pmatrix}\nG(u_1)(y_1) & G(u_2)(y_1) & \\ldots & G(u_N)(y_1) \\\\\nG(u_1)(y_2) & G(u_2)(y_2) & \\ldots & G(u_N)(y_2) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nG(u_1)(y_M) & G(u_2)(y_M) & \\ldots & G(u_N)(y_M)\n\\end{pmatrix} \\in \\mathbb{R}^{M\\times N}.$\nHere, $T$ represents the output of the trunk network, $W$ is the learnable parameter, $B$ represents the output of the branch\nnetwork, and $G$ is the label."}, {"title": "Numerical Results", "content": "We present the numerical results demonstrating the superior performance of the proposed method. All experiments\nwere conducted using a single NVIDIA GeForce RTX 3090 GPU."}, {"title": "Ordinary Differential Equations", "content": ""}, {"title": "Antiderivative", "content": "We consider learning the antiderivative operator:\n$G: u(x) \\rightarrow s(x) = \\int_{0}^{x}u(t)dt$,\nas studied in [10]. To generate the input functions, we sampled 2000 instances of $u_i$ from a Gaussian Random Field\n(GRF):\n$u \\sim \\mathcal{G}(0, K_l(x_1,x_2))$,\nwhere the covariance kernel is defined by $K_l(x_1, x_2) = exp(-\\frac{||x_1-x_2||^2}{l^2})$ with $l = 0.1$. Using numerical integration,\nwe created the dataset $\\{(u_i, G(u_i)(y_j))\\}_{i,j=1}^{N,M}$ with $N = 2000, M = 100$ where $y_j \\in [0,1]$ represents the uniform\ncollocation points. The dataset was split into 1000 training samples and 1000 testing samples.\nAs baseline models, we employed two DeepONets, M1 (Model 1) and M2 (Model 2), each utilizing fully connected\nneural networks with ReLU activation for both the branch and trunk networks. M1 features branch and trunk networks\nwith architectures 1-64-64-64-1, while M2 employs larger networks with architectures 1-256-256-256-1. We trained the\nDeepONet by using Adam optimizer with a learning rate 1e-3 for epochs [24]. We fixed the branch network of the\nELM-DeepONet with SLFNs with ReLU activation and the trunk network with 3-layer MLP consisting of $p_2$ hidden\nnodes in each layer. We also tested the sinusoidal basis function (Sinusoidal ELM-DeepONet) rather than the trunk net,\nsuch as $\\{\\sin(\\frac{32k\\pi}{L}y)\\}_{k=1}^{p_2} \\cup \\{\\cos(\\frac{32k\\pi}{L}y)\\}_{k=1}^{p_2}$. In this example, we set $p_1 = N = 1000$ and $p_2 = M = 100$.\nThe results are summarized in Table 1. As shown, training the ELM-DeepONet requires negligible time compared to\nthe vanilla DeepONets, while the proposed methods achieve significantly lower relative test errors. Figure 4 illustrates\nthe results on a representative test sample."}, {"title": "Nonlinear ODE", "content": "Next, we focus on learning the solution operator $G : u(x) \\rightarrow s(x)$ of a nonlinear ODE defined as:\n$s'(x) = -0.1s^2(x) + u(x)$.\nthe source function u is sampled from the same GRF as described in Section 4.1.1. Using numerical integration, we\nconstruct the dataset $\\{(u_i, G(u_i)(y_j))\\}_{i,j=1}^{N,M}$ with $N = 2000, M = 100$, and $y \\in [0, 1]$. We then compare the relative\ntest errors of the proposed ELM-DeepONet and the Sinusoidal ELM-DeepONet to those of the baseline models M1 and\nM2. The results are summarized in Table 2."}, {"title": "Sensitivity analysis", "content": "The hyperparameters $p_1$ and $p_2$ play a critical role in the performance of ELM-DeepONet. As previously discussed, it\nis important to set $p_1$ and $p_2$ to satisfy the condition\n$p_1 \\leq N,p_2 \\leq M,$\nto ensure that the pseudoinverses $T^{\\dagger}$ and $B^{\\dagger}$ function as left and right inverses, respectively. However, we have observed\nthat, in practice, significantly larger values of $p_1$ and $p_2$ which violate the condition in Equation (2), can still yield\nsatisfactory results. We provide sensitivity analysis for the choice of parameters $p_1$ and $p_2$ through the antiderivative\nexample.\nTables 3 and 4 present the relative errors corresponding to various choices of $p_1$ and $p_2$. These relative errors were\ncalculated over 10 trials and averaged to ensure statistical reliability. The results reveal several intriguing trends. First,\nas $p_2$ increases, the relative error consistently grows, reinforcing the empirical validity of the condition $p_2 < M$. This\nobservation aligns well with theoretical expectations and highlights the importance of carefully constraining $p_2$ for\nstable performance.\nOn the other hand, a contrasting pattern emerges for $p_1$; the relative error decreases as $p_1$ increases, even when $p_1 > N$.\nThis behavior deviates from the theoretical condition $p_1 \\leq N$ suggesting that relaxing this constraint might improve\nperformance in practical settings. These findings underline the relationship between the hyperparameters $p_1, p_2$ and the\nmodel's performance, offering valuable insights for their optimal selection in ELM-DeepONet."}, {"title": "Darcy Flow", "content": "Next, we investigate the Darcy Flow, a 2D Partial Differential Equation (PDE) defined as:\n$\\nabla \\cdot (\\kappa \\nabla u) = f, \\text{ for } (x, y) \\in \\Omega := [0, 1]^2,$\n$u = 0, \\text{ for } (x, y) \\in \\partial \\Omega,$\nwhere $\\kappa$ represents the permeability and the source function is given by $f(x, y) = 1$. The objective is to train the\nmodels to learn the solution operator, which maps the permeability field to the corresponding solution\n$G: \\kappa \\rightarrow u$.\nHere $\\kappa$ is sampled from the GRF to generate diverse input scenarios. We use a 50 $\\times$ 50 uniform mesh in $\\Omega$ as collocation\npoints. We create the dataset $\\{(\\kappa_i, G(\\kappa_i)(x_j, y_j))\\}_{i,j=1}^{N,M}$ with $N = 2000$ and $M = 2500$.\nAs a baseline algorithm, we employed two DeepONets with different branch networks. One with a 3-layer MLP with\neach layer consisting of 128 nodes (DeepONet-MLP), and the other with a Convolutional Neural Network (CNN) with\nthree convolutional layers followed by two fully connected layers (DeepONet-CNN). For the trunk network, we utilized\na 3-layer Multilayer Perceptron (MLP), with each layer consisting of 128 nodes. We use ReLU as an activation function\nfor all networks. We trained the network by using Adam optimizer for 10000 epochs with a learning rate 1e-3.\nWe employed two ELM-DeepONets with MLP branch and CNN branch network, as the DeepONets, with all parameters\nkept fixed. In the Sinusoidal ELM-DeepONet, the trunk network was replaced with sinusoidal basis functions,\nspecifically: $\\{\\sin(n\\pi x) \\sin(n\\pi y), \\sin(n\\pi x) \\cos(n\\pi y + \\pi/2), \\cos(n\\pi x + \\pi/2) \\sin(n\\pi y), \\cos(n\\pi x + \\pi/2) \\cos(n\\pi y +\\pi/2)\\}$, designed to enforce boundary conditions and leverage the advantages of harmonic representations, see Figure 5.\nWe choose the hyperparameters $p_1$ and $p_2$ by using a simple grid search."}, {"title": "Reaction Diffusion equation: An inverse source problem", "content": "We consider an inverse source problem for a parabolic equation, as discussed in both the PINN and operator learning\nliterature [25, 14]. The equation is defined as:\n$u_t = Du_{xx} + ku^2 + s(x), \\qquad (t, x) \\in \\Omega_{\\tau} := [0, 1] \\times \\Omega,$\n$u(0, x) = u_0(x), \\qquad x \\in \\Omega,$\n$u(t, x) = 0, \\qquad (t, x) \\in [0, 1] \\times \\partial \\Omega,$\nwhere $\\Omega \\subset [0, 1]$. The objective is to learn the inverse operator\n$\\mathcal{G}: u| _{\\Omega_{\\tau}} \\mapsto s$.\nwhich maps the boundary observations of $u$ to the source term $s(x)$. [25] established that this problem is well-posed."}, {"title": "Conclusion", "content": "We propose a novel training algorithm, ELM-DeepONet, to enable the efficient training of DeepONet by leveraging the\nleast square formulation and computational efficiency of ELM. Through extensive validation, we have demonstrated\nthat ELM-DeepONet outperforms vanilla DeepONet in both test error and training time. We believe that ELM-\nDeepONet represents a significant advancement for the operator learning community, addressing the challenge of high\ncomputational costs.\nWe leave several points for future work. First, the choice of $p_1$ and $p_2$ significantly impacts the performance of\nELM-DeepONet. Empirically, we observed that larger values of $p_1$ and smaller values of $p_2$ generally lead to better\nperformance. Notably, extremely large $p_1$, such as $p_1 = 10000$, performed well in practice, despite violating our\ntheoretical insight that the pseudoinverse should act as a proper right inverse. A more thorough analysis of this\nphenomenon would be valuable for further advancing the framework. Second, recent studies have incorporated ELM\ninto Physics-Informed Neural Networks (PINNs). Building on this trend, it is natural to consider extending ELM to\nPhysics-Informed DeepONets as a promising direction for future work."}]}