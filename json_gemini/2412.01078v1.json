{"title": "Advancing Speech Language Models by Scaling Supervised Fine-Tuning with Over 60,000 Hours of Synthetic Speech Dialogue Data", "authors": ["Shuaijiang Zhao", "Tingwei Guo", "Bajian Xiang", "Tongtang Wan", "Qiang Niu", "Wei Zou", "Xiangang Li"], "abstract": "The GPT-40 represents a significant milestone in enabling real-time interaction with large language models (LLMs) through speech, its remarkable low latency and high fluency not only capture attention but also stimulate research interest in the field. This real-time speech interaction is particularly valuable in scenarios requiring rapid feedback and immediate responses, dramatically enhancing user experience. However, there is a notable lack of research focused on real-time large speech language models, particularly for Chinese. In this work, we present KE-Omni, a seamless large speech language model built upon Ke-SpeechChat, a large-scale high-quality synthetic speech interaction dataset consisting of 7 million Chinese and English conversations, featuring 42,002 speakers, and totaling over 60,000 hours, This contributes significantly to the advancement of research and development in this field. The model, dataset, code and demo can be accessed at https://huggingface.co/spaces/KE-Team/KE-Omni.", "sections": [{"title": "Introduction", "content": "Large-language models (LLMs) hold significant promise for enhancing human-computer interaction, offering advanced conversational skills and versatility in managing diverse, open-ended user requests in various tasks and domains. Integrating speech with LLMs, namely large speech language models, enables a more natural form of interaction, allowing models to listen, process, and respond like humans. Notably, GPT-40(OpenAI, 2024) with its real-time speech interaction capabilities has made significant strides in this direction, taking a crucial step toward realizing human-like natural speech interaction.\nHowever, the exploration of seamless speech interaction with large speech language models remains largely absent. Achieving effective speech interaction with LLMs presents several challenges: (1) Difficulty of speech-text modality alignment. Aligning continuous, diversity speech signals with discrete text symbols poses a challenge. (2) Challenge of seamless speech interaction. Speech responses must be of high quality and low latency to ensure a fluid user experience. (3) Serious lack of speech interaction data. Acquiring large-scale speech datasets, particularly for interactive scenarios, is costly and resource-intensive, creating significant barriers to advancement.\nTo effectively align speech and text modalities and achieve seamless speech interaction with large language models, we introduce KE-Omni, inspired by large speech language models, like LLama-Omni(Fang et al., 2024) and SpeechGPT(Zhang et al., 2023). Unlike LLama-Omni and SpeechGPT, which are limited to English, KE-Omni is proficient in both Chinese and English. Experimental results show that KE-Omni can simultaneously generate high-quality text and low-latency speech responses.\nTo address the scarcity and cost of speech interaction data while protecting individuals' voices from misuse, we explore efficient synthetic data methods and have constructed the large-scale, high-quality speech interaction dataset in Chinese and English, promoting the development of this field.\nIn summary, this paper makes two main contributions:\n\u2022 We present a novel approach to constructing Ke-SpeechChat, a large scale high-quality speech interaction dataset comprising 7 million Chinese and English conversations, featuring 42,002 speakers and totaling over 60,000 hours of audio.\n\u2022 We introduce KE-Omni, a seamless large speech language model designed for real-time speech interaction in both Chinese and English, built upon the Ke-SpeechChat dataset."}, {"title": "Related work", "content": "Large speech-language models for interaction. An easy-to-implement solution is to integrate speech recognition and synthesis with a large language model (LLM), as demonstrated in (Huang et al., 2024). However, this integration presents several challenges that significantly degrade user experience, including high latency in the cascading process, non-spoken style responses, and a lack of paralinguistic communication capabilities.\nPrior work such as Qwen2-Audio(Chu et al., 2024) and SALMONN(Tang et al., 2023) enhances LLMs with speech perception capabilities while relying on external text-to-speech (TTS) toolkits for speech generation. This approach has the potential to leverage paralinguistic information but hardly support duplex speech interaction.\nThe end-to-end method integrates both speech perception and generation within large speech-language models. SpeechGPT(Zhang et al., 2023) is a speech-text cross-modal conversational model, but it is not real-time due to Chain-of-Modality. AnyGPT(Zhan et al., 2024) is a token-based any-to-any multimodal language model, which can understand and generate speech autoregressively, but the high frame rate of speech tokenizer limits the real-time interaction. VITA(Fu et al., 2024) is a multimodal large language model that processes audio modalities and supports duplex speech interaction by requiring two models used as a monitor or a generator with role switching when the user interrupts. LLama-Omni(Fang et al., 2024) achieves low latency benefits from a streaming vocoder, however it does not support full-duplex interaction. Kyutai introduced Moshi(D\u00e9fossez et al., 2024), a speech-to-speech conversational model that supports full-duplex spoken dialogue, enabling fluid and seamless interactions.\nSpeech Interaction Datasets. SpeechInstruct\u00b9 (Zhang et al., 2023) contains 37,969 spoken dialogues based on the chain-of-modality mechanism. However, all speech clips are encoded into discrete units by HuBERT, limiting the exploration of speech representation. AnyInstruct\u00b2 (Zhan et al., 2024) consists of 108,000 spoken dialogues generated by the Azure Text-to-Speech API, featuring 39 different timbres. the datasets mentioned above are entirely in English, making them hardly suitable for Chinese speech interaction research and applications. Additionally, both the scale of the datasets and the diversity of speakers are inadequate for large speech-language models.\nSpeech Interaction Benchmarks. Speech interaction benchmarks are scarce up to now. AIR-Bench(Yang et al., 2024) includes both foundation and chat benchmarks, featuring a variety of audio types, such as human speech, natural sounds, and music. However, the amount of speech interaction data, particularly for Chinese, is very limited."}, {"title": "Ke-SpeechChat Dataset", "content": "The success of large language models (LLMs) significantly relies on the availability of large-scale models and datasets. However, to our knowledge, open-source large-scale speech interaction datasets remain unseen,\ngreatly hindering the advancement of speech conversation research. This scarcity can be attributed to two main factors: the high cost associated with constructing speech data and the inherent privacy risks involved.\nTo effectively construct large-scale and high-quality speech interaction datasets, we explore efficient synthetic data methods by leveraging advanced LLMs and TTS toolkits. To avoid privacy risks, we build a virtual voice library for speech generation, in which voices do not exist in the real world. Additionally, we inject watermarks to indicate that the data is generated by AI and prevent data from misuse.\nIn constructing the dialogue data, we first focus on creating text dialogue data that accurately reflects the characteristics of spoken language. We then synthesize speech from these textual dialogues. Subsequently, we perform quality assurance and filtering on the synthetic speech.\nIn this section, we provide a comprehensive overview of the construction process. The collection, rewriting, and post-processing of textual dialogues will be discussed in subsection 3.1, while the steps for converting textual dialogues to speech dialogues will be covered in subsection 3.2."}, {"title": "Textual Dialogue Data", "content": "To synthesize textual dialogue data, we leveraged various entries from open-source datasets, such as IndustryInstruction(Shi et al., 2024), LaMini-instruction (Wu et al., 2023), BELLE (BELLEGroup, 2023; Ji et al., 2023; Wen et al., 2023) (belle1M\u00b3, belle2M4, and belle3.5M5), among others.\nWhile such open-source instruction datasets have been invaluable for various applications, they present challenges that make them inappropriate for direct application in the task of speech interaction. Firstly, many instructions in these datasets involve tasks that are not conducive to speech interaction, such as generating images, writing long articles, or creating structured text. Secondly, the format of these instructions is often too formal and detailed compared to everyday spoken language. For example, a dataset might include an instruction such as \"how to install and set up a piece of software or device (e.g., a printer),\" whereas in everyday conversation, one would simply ask, \"How do I use the printer?\" Lastly, both the instructions and responses in these datasets tend to be overly lengthy and contain special characters that cannot be pronounced, such as markdown symbols, underscores, and line breaks. These factors collectively make text-based datasets inadequate for the nuanced and dynamic nature of speech interaction."}, {"title": "Rewriting Instructions", "content": "The primary goal of rewriting is to transform instruction tasks to be more appropriate for verbal interactions. Our approach involves designing a specific prompt for LLM to convert the original instruction data into questions that a human might verbally ask.\nWe discovered that when the LLM is given the complete original instructions, they often preserve the task's original format with only minor rephrasing. This results in outputs that remain ill-suited for conversational purposes.\nTo tackle this, we implemented a strategy for specific types of tasks, such as classification, summarization, and other directive instructions. We removed the directive sentences from these tasks, leaving only key pieces of information. The LLM was then instructed that these fragments were incomplete and should be used as inspiration to generate new questions creatively.\nThis approach minimizes the LLM's tendency to adhere too closely to the original directives and encourages the creation of more natural, conversational questions. Consequently, we can effectively convert formal and structured instruction texts into queries better suited for the speech scenario."}, {"title": "Filtering of Rewritten Instructions", "content": "Following the pre-rewriting process, we filter the rewritten instructions to ensure they are suitable for spoken interactions, involving three key considerations. First, we assess whether the rewritten instructions are appropriate for verbal communication, excluding tasks that require generating long-form or structured content such as essays, lyrics, or emails. Second, we evaluate the clarity and completeness of each instruction, ensuring they include sufficient context. Instructions that are too vague or lack necessary background information, such as \"What is the main content of this article?\" are filtered out. Third, we assess the safety of the instructions using our internal system and Qwen2-72B-instruct.\nThe filtering stage ensures that the dataset mainly consists of instructions that are clear, contextually complete and safe, enhancing their suitability for conversational interactions."}, {"title": "Spoken Style Post-Processing", "content": "In the final stage, we use LLM to further modify the selected instructions for enhanced conversational quality and generate corresponding responses in a similarly natural spoken style. The LLM is instructed to adhere to a conversational tone, avoid unpronounceable content, and convert numbers and formula symbols into their verbal equivalents. Additionally, responses are kept under 100 words to ensure that no excessive information is generated in a single response. By following these guidelines, the dataset is refined to better support the training of models for natural and effective speech interactions.\nIn terms of LLMs, this section uses Qwen2.5-72B-Instruct, while sections 3.1.1 and 3.1.2 use Qwen2.5-14B-Instruct7. It is worth mentioning that, compared to smaller variants of Qwen2.5, such as Qwen2.5-32B-Instruct or Qwen2.5-14B-Instruct, Qwen2.5-72B-Instruct produced similar instructions but quality improved response, examples can be found in Appendix B."}, {"title": "Speech Dialogue Data", "content": "This section describes the strategy for constructing and ensuring the quality of speech dialogues derived from textual dialogues. We utilized the Cosy Voice (Du et al., 2024) model, which supports custom voice profiles, to convert the textual dialogues into speech dialogues. To ensure speaker diversity, we built a large voice library that includes numerous virtual speakers sourced from open-source speech data. To maintain the quality of the synthetic speech dialogues, we transcribed the synthetic audio and calculated the Character Error Rate (CER), filtering the data based on CER to ensure the high quality of dataset."}, {"title": "Voice Library", "content": "In this section, we describe the process of constructing a virtual voice library using the premium part of WenetSpeech4TTS dataset. The workflow is illustrated in Figure 2 (A) and (B).\nData Sources. The WenetSpeech4TTS dataset is derived from WenetSpeech (Zhang et al., 2022), which consists of long audio recordings ranging from several minutes to hours, collected from the internet. WenetSpeech4TTS processes these long recordings by applying Voice Activity Detection (VAD) to segment them into shorter clips, while simultaneously measuring the DNSMOS (Reddy et al., 2022) for each segment. These short clips are then merged based on speaker cosine similarity, ensuring that each short clip is spoken by the same individual. However, WenetSpeech4TTS does not perform similarity detection between different short clips within the same long recording. This limitation is critical for our work, as we need to identify multiple segments spoken by the same person to create stable embeddings for individual voice profiles.\nReal Speakers. Our first task was to categorize the \"Premium\" short audio clips a (i.e., those with DNS-MOS \u2265 4.0) from WenetSpeech4TTS based on their original long audio recordings A. We filtered out long recordings that contained at least ten \"Premium\" short audio clips, denoted as \\(A^{i} = \\{a_{1}, a_{2}, ..., a_{n}\\}\\), where i is the index of the long recording and n \u2265 10 is the number of short clips.\nNext, we extracted X-vectors for each short clip in these long recordings using WavLM (Chen et al., 2022). We then calculated the speaker similarity between every pair of short clips within \\(A^{premium}\\). If a long recording contained at least 5 [10] pairs of short clips with a similarity score over 0.97, we considered these short clips to be spoken by the same person. Using this method, we identified over 5000 speakers, which are gender balanced.\nVirtual Speakers. For each identified voice profile, we calculated the speaking rate, defined as the average time per character, rounded to the nearest 10 ms interval. We then categorized these profiles based on their speaking rates. We randomly selected one voice and it paired with another same gender voice having the same speaking rate from the remaining profiles. The pair is weighted average to create composite voice profiles, with the specific aim of protecting privacy by generating non-existent, synthetic, virtual voice. The process can be applied to create an unlimited number of composite virtual voices.\nThese steps ensured that our voice library consisted of high-quality, diverse synthetic voices that are gender-balanced and suitable for various applications in speech synthesis, without corresponding to real individuals."}, {"title": "Speech Synthesis", "content": "Based on the aforementioned voice library, we utilized Cosy Voice for speech synthesis. CosyVoice is a state-of-the-art text-to-speech (TTS) model known for its high-quality, natural-sounding voice synthesis and flexibility in customizing vocal characteristics. For each dialogue, we randomly selected one user voice and one agent voice for synthesis. To prevent data abuse, all synthetic speech is watermarked using AudioSeal (San Roman et al., 2024). Procedures are shown in Figure 2 (C)."}, {"title": "Quality Assurance", "content": "To ensure the quality of synthetic dialogues, we transcribe the Chinese parts using Belle-whisper-large-v3-turbo-zh and the English parts using Whisper-large-v3-turbo. The Character Error Rate (CER) is computed for Chinese, while the Word Error Rate (WER) is computed for English. Dialogues with a CER exceeding 5% for Chinese and a WER exceeding 10% for English are dropped to maintain high quality. Processes are shown in Figure 2 (D)."}, {"title": "Details of Ke-SpeechChat", "content": "Metadata\nAll the metadata information is saved to a single JSON file. The id, speakers, genders, texts, audio paths are provided for each dialogue. An example is presented in Appendix C."}, {"title": "Statistics", "content": "Detailed statistics are presented in Table 1. Statistics for Chinese and English dialogues are provided separately. The number of Chinese dialogues exceeds 5.1 million, totaling 40,884 hours. While the number of English dialogues exceeds 1.7 million, totaling 19,484 hours. The dataset features gender-balanced speakers, including 40,000 users and 2 agents, for both Chinese and English, The large scale of dialogues and speakers ensures the diversity of the dataset."}, {"title": "Partitions", "content": "We randomly split the training data into five subsets of varying sizes: XS, S, M, L, and XL. Each larger subset includes all the data from the smaller subsets, and all subsets contain the complete set of 42,002 speakers. The details are presented in Table 2."}, {"title": "Quality", "content": "To evaluate the quality of Ke-SpeechChat, we compared objective metrics including DNSMOS (Reddy et al., 2022) and UTMOS (Saeki et al., 2022a), with those from other datasets. Additionally, we conducted ASR and TTS tasks for further evaluation.\nQuality Metrics. We calculated the DNSMOS P.835 OVRL(Reddy et al., 2022) and UTMOS (Saeki et al., 2022b) scores for the XS training subset to assess the audio quality and speech naturalness of Ke-SpeechChat. These scores were then systematically compared with those obtained from various established large scale speech datasets. It is worth noting that the UTMOS scores for the other datasets were derived from a sample of 100 hours from the corresponding datasets. This comparative analysis provides a valuable perspective on the performance of Ke-SpeechChat in relation to existing datasets, allowing us to highlight its strengths and identify areas for potential improvement.\nThe table 4 presents a comparative evaluation of various datasets based on two key quality metrics: DNSMOS (P.835 OVRL) and UTMOS, which is indicated by the mean scores along with standard deviations. Ke-SpeechChat achieved the highest DNSMOS score of 3.41\u00b10.14, demonstrating its superior speech quality compared to the studio recordings like MLS. Additionally, Ke-SpeechChat scored 3.47\u00b10.35 in UTMOS, indicating that its naturalness is comparable to that of MLS. These results position Ke-SpeechChat as a leading dataset in both audio quality and naturalness.\nOverall, the results illustrate that Ke-SpeechChat outperforms several established datasets in terms of perceived audio quality, as reflected in the DNSMOS scores, while also maintaining competitive naturalness performance in UTMOS evaluations.\nASR Task. We establish speech recognition task based on Whisper-large-v3-turbo to further evaluation the quality of Ke-SpeechChat.\nThe training set comprises all utterances spoken by users in the S subset, totaling 3,291 hours. Since the utterances are recorded in a clean acoustic environment, noise from MUSAN (Snyder et al., 2015) is added with a probability of 0.2 and an SNR ranging from 10 to 50. Additionally, speed perturbation and SpecAugment are applied with a probability of 0.5 to enhance robustness. The training configuration includes a learning rate of 1e-7, utilizing AdamW optimization. All parameters are fine-tuned, and the training epoch is set to 1. The code and configure are at https://github.com/shuaijiang/Whisper-Finetune.\nWe construct ASR test set, which synthesized by Cosy Voice, with details shown in Table 3. All speakers in test set are unseen in the training set. Each audio segment has been reviewed by professional annotators to ensure high transcription quality. Additionally, we also adopt AISHELL-1 and LibriSpeech as our test sets.\nThe results of speech recognition are presented in Table 6. We compare the performance of Whisper-large-v3-turbo, Belle-whisper-large-v3-turbo-zh, and our KeASR using character error rate (CER) and word error rate (WER) for Chinese and English, respectively. Despite having less training data, KeASR demonstrates highly competitive performance, particularly on the KeASR test-zh and Librispeech test sets. The results validate the high quality of the Ke-SpeechChat dataset.\nTTS Task. We evaluated the performance of Ke-SpeechChat on the TTS task based on Cosy Voice.\nThe training set included all speakers from the Ke-SpeechChat dataset, with 10 Chinese and 10 English utterances randomly selected from each speaker to ensure balanced timbre representation. This resulted in a total of 40,040 utterances, with durations of 40.28 hours for Chinese and 34.56 hours for English.\nFor the test set, we utilized 100 virtual speakers not included in the training set, with each generating one prompt in both Chinese and English using CosyVoice. These speakers are required to leverage zero-shot capabilities to synthesize 6 utterances in Chinese and 5 in English. Additionally, SeedTTS test-zh and test-en come from the DiDiSpeech(Guo et al., 2021) and the Common Voice(Ardila et al., 2019) respectively, are adopted as test sets. Details of training and test sets are shown in Table 3.\nThe training configuration includes a learning rate of 1e-5, utilizing AdamW optimization. All parameters are fine-tuned, and the training epoch is set to 1.\nCharacter error rate (CER), word error rate (WER), UTMOS and speaker similarity (SIM) are adopted for evaluate the performance of TTS, with results presented in Table 7. Compared to Cosy Voice, our KeTTS outperforms in both CER and UTMOS, while achieving comparable results in SIM. This indicates that Ke-SpeechChat has enhanced both the generation accuracy and sound quality of the base model.\nOverall, the high quality of the Ke-SpeechChat dataset is verified based on the quality metrics, including DNSMOS and UTMOS, as well as the ASR and TTS tasks."}, {"title": "KE-Omni", "content": "This section presents the details of our large speech language model, KE-Omni. The model architecture of KE-Omni illustrated in Figure 3, it comprises three main components: a speech encoder, a large language model (LLM), and a speech decoder. Given the user's speech instruction, KE-Omni is designed to generate high quality text and speech response seamlessly."}, {"title": "Speech Encoder", "content": "We adopt the encoder of Whisper-large-v310(Radford et al., 2023), a widely used multilingual speech recognition model, as our speech encoder. Whisper is known for its robust performance across diverse languages, making it suitable for our application. A lightweight speech adapter facilitates speech-text modal alignment, connecting the speech encoder to the LLM.\nThe speech encoder processes each second of audio into 50 frames of features. The speech adapter is then employed to further compress the length of the speech feature sequence, aligning the speech modality with the LLM. We utilize a compression ratio of 5 in our speech adapter, meaning that each second of speech is ultimately converted into 10 frames of features. This enhances processing speed and reduces latency of LLM without compromising quality.\nThroughout the entire training process, the parameters of the speech encoder are frozen, except for the speech adapter. This approach preserves the encoder's robust speech representation capabilities while allowing the adapter to learn the necessary transformations for effective speech-text modal alignment with the LLM."}, {"title": "Large Language Model", "content": "We utilize the state-of-the-art open-source LLaMA(Dubey et al., 2024) model as our large language model (LLM), which exhibits strong reasoning capabilities across multiple languages, including both Chinese and English. In KE-Omni, the LLM takes the concatenation of prompt text embeddings and the speech representations generated by the speech encoder as input. This integration allows the LLM to leverage contextual information from both text and speech modality. It then autoregressively generates a text response based on the user's speech instructions. To balance the performance and efficiency, we prefer LLaMA-3.1-8B-Instruct variant as our LLM."}, {"title": "Speech Decoder", "content": "The speech decoder maps text response from LLM into corresponding speech signals, playing a crucial role to speech interaction. It consists of three key components: a duration predictor, a speech unit generator and a unit-based vocoder.\nSimilar to (Zhang et al., 2023) and (Fang et al., 2024), we adopt the pretrained HuBERT(Hsu et al., 2021) model to extract continuous representations of the speech, and convert the representations into discrete cluster indices using a K-means model.\nBefore generating the speech response, the duration of each text token is first predicted by the duration predictor. The duration predictor is a transformer-based model trained on the word-level timestamps extracted by Whisper. According to the duration information, the text token sequence is then upsampled to match the length of the target audio frame sequence. The duration predictor is trained in advance and kept frozen during the training process of KE-Omni.\nA transformer-based speech unit generator is then performed to obtain the discrete speech units sequence in an autoregressive manner. To improve prediction speed, we utilized a chunk-based autoregressive approach, predicting speech units chunk by chunk. Given chunk size C and the length of target speech unit sequence T, the embeddings of ith text token and the jth speech unit are concatenated as input, where \\(j = i-C \\) and \\( i \\in [C,T]\\). Zero-embeddings of speech units are used at the first chunk. To ensure the quality of speech unit generation, we introduce a delay of N steps between the extended text token sequence and the speech unit sequence. Finally, the unit-based vocoder, specifically HiFi-GAN, is performed to synthesis the waveform from these units. The HiFi-GAN vocoder is trained for the agent speakers in advance and kept frozen during the training process of KE-Omni."}, {"title": "Experiments", "content": "Setups\nKE-Omni use LLaMA-3.1-8B-Instruct(Fang et al., 2024) as the LLM backbone. The duraion predictor is optimized with mean square error (MSE) loss, and the token duration obtained by Whisper is used as training target. We distribute the world-level duration evenly to individual characters, and subsequently merge them to obtain the timestep of each token used in LLM. In addition, the duration predictor take the hidden states of the last transformer layer in the LLM as the input. In speech decoder, the chunk size of the autoregressive process C is set to 5. The hyper-parameter of delay step N is set to the sum of the timesteps of the first three text tokens in each sample. For discrete speech units, we employ a K-means model to convert speech representations extracted by HuBERT into 4000 clusters. On this basis, separate unit-based HiFi-GAN vocoders are trained for two agent speakers respectively.\nTo explore the impact of data size on model performance, we trained KE-Omni models on various subsets of the Ke-SpeechChat dataset separately. Each model underwent a two-stage training process: LLM fine-tuning and speech decoder training. In the first stage, all dialogues in each subset were used to train the speech adaptor and enhance the reasoning capabilities of the LLM for audio input. In the second stage, the LLM is frozen, and the dialogues were separated by agent speaker and used to train speech decoders for each agent speaker individually. Both of two stages are trained for 2 epochs, utilizing AdamW optimizer. The peak learning rate is set to le-4 in the first stage, while 2e-4 is used in the second stage. In order to make the number of training steps as consistent as possible across all datasets, we adopted different batch sizes for different datasets in the second stage."}, {"title": "Development and Test Sets", "content": "To evaluate the speech interaction ability, we construct the development and test sets, which are illustrated in Table 10. The chat-dev set consists of 2,754 spoken dialogues, totaling 14.6 hours in Chinese and 11.2 hours in English. The chat-test set comprises 2,945 spoken dialogues, totaling 12.4 hours in Chinese and 16 hours in English. All speakers, except the two agent speakers in these sets are unseen in the training data. All data have been meticulously reviewed by professional annotators to guarantee the quality.\nAdditionally, we also evaluate our models using VoiceBench(Chen et al., 2024), a benchmark that assesses voice dialogue systems on their general knowledge, instruction-following ability, and safety compliance. VoiceBench incorporates both synthetic and real spoken instructions to simulate diverse speaker styles, environmental conditions, and content variations."}, {"title": "Evaluation", "content": "In this section, we detail the methodologies and processes employed to evaluate the speech language models, as illustrated in Figure 4. We evaluate three key capabilities: Speech-to-Text Instruction-Following (S2TIF) similar to (Fang et al., 2024), modality alignment based on Character Error Rate (CER) and Word Error Rate(WER), and speech quality using UTMOS (Saeki et al., 2022b).\nSpeech-to-Text Instruction-Following (S2TIF). The S2TIF metric uses GPT-4 to score the response text based on transcribed instructions. It evaluates two dimensions: content and style, each rated from 1 to 5, respectively assessing whether the response covers the instructions and whether the style suits voice interaction. We used the same prompt as (Fang et al., 2024), shown in Appendix A.2. Additionally, the average length of the responses is computed to show the models' length preferences and to illustrate the difficulty of modality alignment in speech output.\nModality Alignment. We adopt the Word Error Rate (WER) and Character Error Rate (CER) metrics to evaluate the intelligibility of the speech language model's audio output and its alignment with the text output. Specifically, we used the Whisper-large-v3 model for transcription and applied text normalization to standardize the representation of numbers and symbols, removing punctuation marks before calculating the metrics.\nSpeech Quality. We adopt the UTokyo-SaruLab Mean Opinion Score (UTMOS) prediction system developed by (Saeki et al., 2022b) to assess the quality of the generated speech. This system generates a UTMOS score for the audio based on the naturalness and overall quality of the speech, with higher scores indicating better sound quality.\nVoiceBench We also evaluated the model on Voicebench(Chen et al., 2024), following the methodology provided by\u00b9\u00b9 . Voicebench is a benchmark designed to offer a multifaceted evaluation of LLM-based voice assistants, incorporating both real and synthetic spoken instructions. We evaluated on five subsets from this benchmark: AlpacaEval, CommonEval, OpenBookQA, IFEval, and AdvBench."}, {"title": "Baseline Systems", "content": "We include the following speech-language models as baseline systems: LLama-Omni, Qwen2-Audio and SpeechGPT. LLama-Omni is a large speech-language model that supports both speech input and output. For LLama-Omni and SpeechGPT are limited to English, only the English portion of Ke-SpeechChat chat-test is used to evaluate and compare the performance. In contrast, Qwen2-Audio, as a general audio understanding model, supports both Chinese and English. Since it only support the Speech-to-Text Instruction-Following (S2TIF) task, the S2TIF portion of Ke-SpeechChat chat-test is used for performance evaluation."}, {"title": "Results and Analysis", "content": "Based on the evaluation methodology mentioned in Section 5.3, performances of our KE-Omni model and baseline systmes are assessed and shown in Table 8. Since LLaMA-Omni utilize female agents for their speech responses, we selected the female agent of KE-Omni for comparison.\nIn the Speech-to-Text Instruction-Following (S2TIF) task, KE-Omni achieves significantly better performance than baseline systems when trained on datasets of comparable size. This highlights that the quality of speech dialogues is as critical as the quantity for optimizing model performance.\nIn the modal alignment task, KE-Omni outperforms to other baseline systems significantly when the training data scaling up to subset S and larger subsets, suggesting that reliable modal alignment capabilities require a substantial amount of data. The increase in training data volume from XS to S correlates with a sharp decline in character error rate (CER) or word error rate(WER). Interestingly, the lowest CER/WER is observed in subset L, not the largest XL subset, possibly because the larger batch size in XL negatively affect performance. It is worth mentioning that the length of responses from LLaMA-Omni and SpeechGPT tend to be shorter than those from KE-Omni, which results in less challenging speech-text modal alignment.\nIn the speech quality dimension, KE-Omni's performance progressively enhances with larger training subset, culminating in the highest quality outcomes when trained on the largest XL subset.\nThe results of VoiceBench are shown in Table 9, where we adopt speech-form instructions. As reflected in the table, KE-Omni achieves competitive performance compared to other models like Qwen2-Audio, LLaMa-Omni, Mini-Omni2, and Moshi across most evaluation dimensions.\nSpecifically, for the AdvBench safety bench, which evaluates the model's ability to refuse inappropriate requests, we observe that VoiceBench uses a detection method where the model's output is matched against a list of predefined keywords indicating refusal, such as \"Sorry I\" and \"It's wrong.\" If any of these keywords are found, the model is deemed to have correctly refused the request. However, we found that while our model correctly refuses inappropriate requests, it often does so without using these specific keywords. This may result in artificially lower scores.\nSimilarly, in the IFEval subset, which tests the model's ability to follow instructions, we noticed that some instructions require the model to write an article with a specified number of paragraphs, using two line breaks to separate paragraphs. The test then checks if the model's output correctly uses these line breaks. However, under KE-Omni's logic, which is designed for spoken interaction, our model tends to generate text that can be \"spoken\". While it correctly outputs the specified number of paragraphs, it prefers to indicate paragraph divisions with phrases like \"the first paragraph is:\" rather than using non-verbal line breaks. This may also result in lower scores."}, {"title": "Limitations", "content": "The textual dialogues in Ke-SpeechChat originate from BELLE and other open-source datasets, and then have been rewritten by Qwen2.5-72B-Instruct. Consequently, they may contain factual inaccuracies or outdated information due to the hallucination tendencies of large language models (LLMs).\nThe speech dialogues are generated by Cosy Voice and may still exhibit pronunciation flaws despite quality filtering. The synthetic speech data in our dataset is in clean acoustic environment, free from noise and reverberation, some complex scenarios may require the addition of noise and reverberation for better robust.\nThe Ke-SpeechChat dataset currently contains only single-turn conversations, and work on constructing multi-turn speech dialogues is underway."}, {"title": "Conclusions", "content": "In this work, we introduced KE-Omni, a large speech language model designed for seamless speech interaction. We also presented Ke-SpeechChat, a large-scale speech interaction dataset comprising 7 million English and"}]}