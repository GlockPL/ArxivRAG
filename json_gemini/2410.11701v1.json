{"title": "Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple Instructions", "authors": ["Yuhan Fu", "Ruobing Xie", "Jiazhen Liu", "Bangxiang Lan", "Xingwu Sun", "Zhanhui Kang", "Xirong Li"], "abstract": "Hallucinations in multimodal large language models (MLLMs) hinder their practical ap-plications. To address this, we propose a Magnifier Prompt (MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs via extremely simple instructions. MagPrompt is based on the following two key principles, which guide the design of various effective prompts, demonstrating robustness: (1) MLLMs should focus more on the image. (2) When there are conflicts between the im-age and the model's inner knowledge, MLLMS should prioritize the image. MagPrompt is training-free and can be applied to open-source and closed-source models, such as GPT-40 and Gemini-pro. It performs well across many datasets and its effectiveness is comparable or even better than more complex methods like VCD. Furthermore, our prompt design princi-ples and experimental analyses provide valu-able insights into multimodal hallucination.", "sections": [{"title": "1 Introduction", "content": "With the development of Large Language Mod-els (LLMs), Multimodal Large Language Mod-els (MLLMs) have emerged. MLLMs integrate a larger LLM with a smaller visual model, such as ViT (Dosovitskiy, 2020). While they inherit the strong capabilities of LLMs, they also carry over their tendency for hallucinations.\nPhD (Liu et al., 2024b) proposes that current ar-chitecture of MLLMs inherently causes models to prioritize textual knowledge, hallucinating with at-tributes or objects seen based on inner knowledge. The vision model is significant in MLLMs, but current MLLMs exhibit a deficiency in visual capa-bilities (Tong et al., 2024). VCD (Leng et al., 2024) mitigates the model's knowledge bias through con-trastive decoding. OPERA (Huang et al., 2024) observed common patterns of the decoding atten-tion scores when the model hallucinates and mit-igates hallucination by implementing special de-coding strategies. However, despite such great works, MLLMs still struggle with hallucinations. Besides, these methods are somewhat complex and unable to be applied to closed-source models like GPT4V (OpenAI, 2023). Some other approaches (Wu et al., 2024) attempt to reduce hallucinations through complex prompt engineering but overlook the model's intrinsic ability to follow instructions. This raises the question: is there a simpler yet ef-fective way to mitigate hallucinations in both open-source and closed-source models? In this paper, we explore whether an MLLM can be simply instructed on the fly to tackle its hallucination issue.\nWe argue that MLLMs hallucinate mainly be-cause of the lack of vision ability and conflicts between image and inner knowledge with details in Section 2. We propose a Magnifier Prompt (MagPrompt) to concentrate more on the visual information of MLLM tasks especially under possi-ble conflicts like counterfactual situations, fighting against hallucinations. Our contributions are as follows:\n(1) We make use of instruction-following abil-ity of MLLMs to propose MagPrompt to mitigate hallucinations. It's easier, training-free and can be applied to open-source and closed-source models, showing comparable or even better effectiveness than more complex methods like VCD.\n(2) We conduct a deeper analysis of the evalu-ation metrics and experimental results, which en-hances our understanding of current MLLMs and provides valuable insights for future research."}, {"title": "2 Analysis and Method", "content": "In this section, we analyze the causes of hallucina-tions in MLLMs and present our method. As out-lined in VCD, MLLMs need to eliminate language priors. OPERA observed that MLLMs often prior-itize summary tokens in longer responses, which can lead to hallucinations due to diminished at-tention to visual content. Additionally, Eyes Wide Shut (Tong et al., 2024) underscores the inadequacy of current visual capabilities in MLLMs, emphasiz-ing the need to enhance visual ability and maintain focus on the image. Clearly, current MLLMs do not pay sufficient attention to images.\nFrom a structural perspective, current MLLMS employ a smaller vision model and a larger LLM, creating an imbalance that favors language over visual factors. This design encodes images into tokens, with the LLM handling subsequent gener-ation autoregressively. As generation progresses, the influence of text on determining the next token increases, making the model more prone to textual biases. As noted in PhD, conflicts between image and model's inner knowledge can lead to incorrect outputs, so that counterintuitive images are partic-ularly likely to induce hallucinations. Given the prevalence of conflicting multimodal information in AIGC technology online, it is imperative to ad-dress this issue to reduce hallucinations in MLLMs. Thus, resolving conflicts within image and inner knowledge is crucial.\nWe summarized that MLLMs make hallucina-tions mainly stem from two aspects. First, MLLMS pay insufficient attention to image content with a deficiency in visual capabilities. Second, there ex-ist some conflicts such as the actual content of the image is contradictory to its inner knowledge (i.e., common sense about the world). In light of the two main causes of MLLMs' hallucinations, we adhere to two design principles: (1) The model needs to focus more on the image, as in MLLMs' usage scenarios, the models should primarily generate responses based on the information from the image itself. (2) When there exists conflicts between the image content and the model's internal knowledge, the model may tend to trust its knowledge. In such cases, priority should be given to the image.\nBased on the principles, we designed two rules. The MagPrompt we propose is illustrated in Fig. 1. With our method, the user's query is processed through our MagPrompt template before being in-put into the model. By reformatting the user's query, the model's hallucination will be mitigated to some extent. In fact, numerous effective prompts can be derived from our principles. And the princi-ples are robust, as demonstrated by our experiments"}, {"title": "3 Experiments", "content": "3.1 Experimental Settings\nMLLMs for Evalutation. To ensure a comprehen-sive evaluation, we select widely recognized and popular models: LLaVA-1.5 (Liu et al., 2024a), Qwen-VL (Bai et al., 2023), mPLUG-Owl2 (Ye et al., 2024), GPT-40 (OpenAI, 2023) and Gemini pro (Team et al., 2023).\nDatasets for Evaluation. We select POPE (Li et al., 2023), PhD (Liu et al., 2024b), AMBER (Wang et al., 2023) and HallusionBench (Guan et al., 2024). Different datasets provide a deeper and more comprehensive evaluation.\nMetrics for Evaluation. We calculate the positive F1 score (F1p), negative F1 score (F1N), macro F1 score, and PhD score to evaluate model performance. F1p measures F1 of 'yes', while F1N measures F1 of 'no'. The macro F1 is the average of these two scores. For details on the PhD score calculation, see Appendix A.\n3.2 Results on POPE\nWe compared MagPrompt with VCD using the POPE adversarial split, as shown in Table 1. MagPrompt outperforms VCD on LLaVA-1.5-7B and LLaVA-1.5-13B and performs comparably on Qwen-VL. Our improvement in F1N is notably larger, enhancing the macro F1 score. For results on random and popular splits, see Appendix B. Additional details on the relative improvements of VCD reported results are available in Appendix C.\n3.3 Experiments on GPT-40 and GeminiPro\nTo demonstrate scalability on closed-source SOTA models, we experimented with GPT-40 and Gem-ini 1.5 Pro using 100 random samples from POPE adversarial splits, ensuring a balanced distribution"}, {"title": "3.4 Robustness of Prompts Design", "content": "We proposed two insightful principles in Section 2. Here we experimented with various prompt formu-lations, as long as the prompts were written based on these two principles, most of them yielded com-parable results. The specific designed prompt1, prompt2, prompt3 and prompt4 are in Fig. 1,Fig. 2, Fig. 3 Fig. 4. Experimental results can be seen in Table 12, demonstrating robustness to prompt varia-"}, {"title": "3.5 Results on other datasets", "content": "Results on PhD. We evaluated several models on PhD dataset including GPT-4V across five sub-tasks with two questioning modes. Average results for five tasks are presented with separate results for each mode: results of neutral mode in Table 3 and misleading mode in Table 4. Detailed results for each sub-task can be found in Table 8 and Table 9. Results indicate improvements for most models in both questioning modes. VCD performs better in the misleading mode, while MagPrompt shows superior performance in the neutral mode. Addi-tionally, MagPrompt applied to GPT-4V yields en-hancements in both modes, demonstrating its appli-cability to closed-source models. Besides, Qwen-VL declines in performance in misleading mode. We guess that it has done some defending work on the hallucination dataset which may decrease the model's instruction-following ability.\nResults on AMBER. AMBER is developed us-ing COCO images and facilitates LLM-free hallu-cination evaluation. We evaluate MagPrompt on discriminative tasks and the results are in Table 6.\nResults on HallusionBench. HallusionBench dif-fers from POPE and AMBER in terms of data sources, as it consists of a wide range of manually collected images and presents relatively complex questions. We evaluate our MagPrompt on Hallu-sionBench and the results can be seen in Table 7. We determined the responses as 'yes' or 'no' using predefined rules rather than GPT for convenience."}, {"title": "3.6 In-Depth Analyses and Insights", "content": "MagPrompt is Effective and Universal. Experi-mental results demonstrate that MagPrompt effec-tively mitigates MLLMs' hallucinations. In most cases, its performance is comparable to VCD, and even surpasses VCD in certain metrics and models. Furthermore, MagPrompt can be applied to closed-source models: we assessed GPT-40 and Gemi-nipro and observed performance enhancements, in-dicating that it is both effective and universal.\nConsidering F1N & macro F1 as Metrics. Evalu-ation in previous works mainly refers to F1 score which is F1p. However, it tends to favor model's ability to answer \"yes\". If the model is biased to-wards predicting \"yes\" or is particularly good at predicting \"yes\" than \"no\", F1p will be relatively high, which is not comprehensive enough to eval-uate model's ability. In terms of hallucinations in MLLM, it would be scenarios where something is created out of nothing, i.e., \"asking about objects not present in the image.\" From this perspective, F1N is important. Therefore, we should also con-sider F1N and macro F1 to achieve a more balanced and comprehensive assessment.\nWhy MagPrompt Works. The results demonstrate that MagPrompt shows a more significant improve-ment in F1N compared to F1p. This tendency is reflected in POPE on many models. This also ex-plains why our method's improvement in macro F1 is larger. MagPrompt enhances models' perfor-mance by guiding MLLMs to pay more attention to the image through prompts, comprehensively analyzing the information, and reducing the occur-rence of hallucinations.\nPrompt Impacts MLLMs' Evaluation. Mag-Prompt enhances models' capabilities with just simple instructions, highlighting the importance of using stricter and more consistent prompts dur-ing evaluations to ensure fair comparisons. This also suggests that when using MLLMs, tailoring the prompt to the specific context can lead to better results.\nInstruction-following Matters. Instruction-following ability is critical for MagPrompt, and without it, performance may suffer. As such, we se-lected models for their relatively strong instruction-following capabilities. Besides, we have also con-ducted some attempts on InstructBLIP, and it did not perform well on POPE's adversarial split. Our method requires good ability to follow user's com-plex instructions, a capability that InstructBLIP lacks, indicated by (Qian et al., 2024). The instruc-tions data in InstructBLIP's fine-tuning process are short, which limits its ability to handle more com-plex prompts. The performance of InstructBLIP on POPE's adversarial split can be found in Table 13.\nSignificance and Contribution of MagPrompt. We believe that our method is important because it is simple, effective, and has practical value. Besides, it highlights some of the issues in current multimodal hallucination research and evaluation tasks, providing a strong baseline. We hope that this straightforward study can contribute to both the academic and industrial communities."}, {"title": "3.7 Ablation Study", "content": "We conducted comparative experiments in POPE shown in Table 2, verifying the effectiveness of each rule in our prompt design. For more details, refer to the Appendix D."}, {"title": "4 Conclusion", "content": "We make use of instruction-following ability of MLLMs to propose the MagPrompt to mitigate hallucinations in MLLMs. To validate its effective-ness, we conducted experiments on several datasets across several models. MagPrompt is training-free, effective, and can be easily applied to both open-source and closed-source models. We also conduct a deeper analysis of evaluation metrics and exper-imental results, providing valuable insights into multimodal hallucination for future work."}, {"title": "Limitation", "content": "This work introduces a straightforward and effec-tive prompt designed to mitigate hallucinations in MLLMs. However, it has several limitations. First, we simply added instructions to the prompt. If techniques like chain-of-thought (COT) reason-ing were applied on top of these rules, the results might be even better, which we leave for future re-search. Second, although we evaluated MagPrompt on several hallucination datasets, they still repre-sent only a limited range of tasks and settings com-pared to the numerous scenarios encountered in the real world. Further deployment of MagPrompt across different scenarios requires additional learn-ing. Third, while MagPrompt proves to be effective, MLLMs still suffer from hallucinations. Therefore, fully resolving hallucinations remains a significant challenge."}, {"title": "Ethics Statements", "content": "This work mitigates hallucinations of LVLMs to enhance their reliability. We have carefully con-sidered the ethical implications of our work. The models and datasets we used are publicly available and commonly used, and our findings may inherit the biases and limitations carried out in these re-sources."}, {"title": "A Calculation of PhD score", "content": "PhD score is calculated by first calculating the har-monic mean of the recall for 'yes' and 'no', which we refer to as H_recall here. Next, we compute the harmonic mean of H_recall and accuracy (ACC). The calculation can be expressed as below.\nH_recall = $\\frac{2 \\cdot yes\\_recall \\cdot no\\_recall}{yes\\_recall + no\\_recall}$\nPhD_score = $\\frac{2 \\cdot H\\_recall \\cdot accuracy}{H\\_recall + accuracy}$"}, {"title": "B Results on all splits on POPE", "content": "We also evaluate LLaVA-1.5-13b on random split and popular split on POPE. The experimental re-sults are shown in Table 10. MagPrompt is effective across three different settings: random, popular, and adversarial, demonstrating its stability."}, {"title": "C Reported Results of VCD on POPE", "content": "We also compared our results with the reported results of VCD in the original paper. In the re-sults, our MagPrompt makes comparable relative improvements with VCD, indicating the effective-ness of our MagPrompt. The results can be found in Table 11.\nIn macro F1 score and PhD score, our Mag-Prompt is comparable to VCD. VCD exhibits a relatively higher increase in F1 scores. However, in terms of macro F1, our MagPrompt demonstrates a more significant improvement, particularly on the LLaVA-1.5 and mPLUG-Owl2 models.\nBesides, you can see in the reported results of VCD, that the improvements are unbalanced, it im-proves more on the F1p. From another perspective, we need to consider the comprehensive of answer-ing 'yes' and 'no', which also indicates the macro F1 or the PhD score needed to be referred."}, {"title": "D Details of Ablation", "content": "Sometimes, using only rule 1 can be more effective than using both rules. This may be because rule 1 encompasses the meaning of rule 2, and it is also related to the model's instruction-following capability. But concerned for global performance, we finally developed MagPrompt."}, {"title": "E Robustness Experiments", "content": "The written prompts are shown below as prompt1, prompt2, prompt3 and prompt4. The experimen-tal results can be seen in Table 12, demonstrating robustness to prompt variation."}]}