{"title": "Quality over Quantity: Boosting Data Efficiency Through Ensembled Multimodal Data Curation", "authors": ["Jinda Xu", "Yuhao Song", "Daming Wang", "Weiwei Zhao", "Minghua Chen", "Kangliang Chen", "Qinya Li"], "abstract": "In an era overwhelmed by vast amounts of data, the effective curation of web-crawl datasets is essential for optimizing model performance. This paper tackles the challenges associated with the unstructured and heterogeneous nature of such datasets. Traditional heuristic curation methods often inadequately capture complex features, resulting in biases and the exclusion of relevant data. We introduce an advanced, learning-driven approach, Ensemble Curation Of DAta Through Multimodal Operators (EcoDatum), incorporating a novel quality-guided deduplication method to ensure balanced feature distributions. EcoDatum strategically integrates various unimodal and multimodal data curation operators within a weak supervision ensemble framework, utilizing automated optimization to score each data point effectively. EcoDatum, which significantly improves the data curation quality and efficiency, outperforms existing state-of-the-art (SOTA) techniques, ranked 1st on the DataComp leaderboard, with an average performance score of 0.182 across 38 diverse evaluation datasets. This represents a 28% improvement over the DataComp baseline method, demonstrating its effectiveness in improving dataset curation and model training efficiency.", "sections": [{"title": "Introduction", "content": "The vast amount of data presents opportunities for training advanced deep learning models, but it also introduces significant noise and irrelevant information, which can hinder model effectiveness. In both academia and industry, the need for robust data curation techniques to extract meaningful signals from extensive digital information has become a pressing concern. In web-crawled datasets, data curation is a multi-faceted task involving various stages and methodologies. The core objective is to identify and retain high-quality samples while discarding noise or mitigating the impact of irrelevant data. This process is crucial for optimizing model performance in the deep learning framework.\nWeb-crawled data is inherently unstructured, diverse, and constantly evolving, making it essential to develop adaptive curation methods capable of handling such complexity. Traditionally, data curation methods have relied heavily on heuristic filtering approaches based on manually defined content attributes, such as image resolution, graphic geometry, textual length, and linguistic complexity. While these heuristic methods provide a basic means of recognizing low-quality samples, they fail to adequately capture the subtle features of web-crawled data and may introduce biases or overlook relevant information. To address these limitations, researchers are increasingly adopting automated curation methods that leverage deep learning techniques, including natural language processing, computer vision and cross-modal representation learning, to achieve a balance of quality and quantity. This research proposes a data curation framework called EcoDatum to address the aforementioned issues. Specifically,"}, {"title": "Method", "content": "cally, we implement a range of efficient data curation strategies as operators, to enhance the data curation process and to achieve cross-modal data alignment at various levels of granularity. However, a simple combination of these operators may introduce bias and lead to insufficient utilization of their individual strengths. To fully capture their synergies, we develop a weak supervision ensemble framework that integrates these operators, achieving a synergistic effect. Furthermore, to enhance the integration efficiency of unimodal and multimodal operators, EcoDatum introduces an automated optimization approach. This is achieved by tuning the weak supervision integration module using a composite metric and a tiny-labeled dataset.\nAs a novel weak supervision-based framework for multimodal data curation, EcoDatum achieves state-of-the-art performance on the DataComp data filtering track. The visual language model trained on curated data demonstrates outstanding results over 38 downstream tasks, highlighting its strong generalizability. Extended experiments demonstrate the effectiveness of this research in understanding various operators in cross-modal data management, offering insights for future work.\nOur main contributions are as follows:\n1. We propose an auto-optimized ensemble framework, EcoDatum, which integrates techniques to enhance data quality and curate multimodal data, ensuring aligned, high-quality inputs for visual language pretraining.\n2. We introduce a search-based optimization algorithm for weak supervision labeling function tuning that enhances the curation process and boosts the system's robustness.\n3. EcoDatum surpasses existing state-of-the-art techniques in the DataComp benchmark over 38 downstream tasks and ranks 1st on the leaderboard\u00b9."}, {"title": "Related Work", "content": "Recent research underscores the critical role of data curation in enhancing model performance with large-scale image-text datasets. Various studies focus on improving dataset quality through curation methods, such as enhancing the descriptiveness and cross-modal feature alignment of image-text pairs, and reducing redundancy.\nIn a broader context, DataComp is a benchmark designed to evaluate the performance of multimodal models on large-scale, real-world datasets. Recent advancements in the DataComp benchmark highlight notable progress in data curation techniques. Yokoo et al. advanced data filtering using image-text similarity and caption modification, achieving notable progress in the Filtering and Bring Your Own Data (BYOD) Tracks. Yu et al. evaluated data selection criteria's impact on model performance, while Chen et al. introduced DataJuicer for managing large-scale datasets. Nguyen et al. enhanced image captioning for better modality alignment, and Maini et al. presented T-MARS for improved visual representation by bypassing text feature learning. Additionally, significant contributions have been made in the areas of synthetic data, contrastive learning, image-text alignment, and few-shot learning. Despite the significant advancements in data curation achieved by Radenovic et al. and Nguyen et al. through enhancing data relevance, existing automated filtering methods may still exclude valuable but less conventional data points or introduce biases by focusing too narrowly on specific aspects, potentially overlooking broader contextual information."}, {"title": "Ensemble Learning", "content": "Ensemble learning, which combines multiple models to improve performance and generalization, has long been a foundational approach in machine learning. Classic methods such as Bagging and Boosting initially demonstrated how model aggregation could reduce variance and improve accuracy.\nAfterwards, ensemble learning has been increasingly applied to specialized tasks. highlighted how ensemble methods enhance outlier detection by aggregating results from multiple models. demonstrated that ensemble-based uncertainty sampling can significantly improve efficiency in active learning. demonstrated that ensemble techniques enhance semi-supervised learning by effectively utilizing both labeled and unlabeled data. used ensemble methods to improve data quality by detecting and filtering noisy or mislabeled data.\nThese advancements illustrate how ensemble techniques refine data preprocessing and improve model inputs."}, {"title": "Overview", "content": "As illustrated in Figure 2, EcoDatum enhances the pretraining effectiveness of multimodal models like CLIP by strategically selecting high-quality subset S from the original dataset S. This targeted data curation improves the model's zero-shot performance on diverse tasks. The framework utilizes an ensemble of specialized data operators for comprehensive quality assessment, which addresses various dimensions including image filtering, text analysis, and cross-modal alignment at multiple granular levels. Automated optimization enables the weak supervision system to generate quality scores for data samples, thus minimizing manual input and enhancing the precision of threshold settings. Consequently, EcoDatum streamlines the data curation process and significantly elevates the quality, ensuring the dataset meets the rigorous requirements for model training."}, {"title": "Quality Guided Deduplication", "content": "To improve our dataset's diversity and distribution, we employ a quality-guided deduplication process that removes redundant text-image pairs. This approach uses perceptual hashing to generate hash codes, identifying duplicates based on visual and textual content. Subsequently, the CLIP model assesses the semantic coherence of each duplicate group, allowing us to retain text-image pairs with the highest CLIP scores, as shown in Figure 3. This selective retention enhances the dataset by preserving the most relevant and semantically rich examples, minimizing redundancy while maintaining quality and diversity."}, {"title": "Unimodal and Multimodal Curation Operators", "content": "EcoDatum enhances the quality of multimodal datasets by implementing rigorous unimodal and multimodal curation operators. The unimodal curation operators systematically filter out low-quality visuals and evaluate textual data for concreteness and relevance using both language identification and Image Caption Concreteness (ICC) metric. Multimodal curation integrates these approaches with advanced alignment techniques, employing models like GroundingDINO, an advanced open-set object detector for precise local feature alignment and the CLIP model, for global semantic coherence. Together, these strategies ensure the curated dataset is of high quality, with well-aligned multimodal content."}, {"title": "Unimodal Curation Operators", "content": "For images, the specific heuristic operators filter out blurred and low-quality visuals. For texts, the FastText model identifies the language and the ICC metric evaluates the relevance and clarity of textual data using a pre-trained autoencoder.\nImage-based quality filtering. Low-quality images can severely impact the learning of visual semantics. Our unimodal operators, based on heuristic rules, enhance dataset quality by filtering out images with detrimental attributes. The Geometric Operator targets images with non-standard aspect ratios that distort geometric relationships and compromise visual integrity when resized. Additionally, the DataComp dataset contains many intentionally blurred images to meet privacy standards, which reduces the visual detail crucial for effective model training. The Blurry Operator identifies and removes these excessively blurred images, ensuring that the curated dataset retains high visual quality.\nText-based caption assessment. We leverage the FastText model to identify and remove captions in rare languages, enhancing the linguistic consistency of our dataset. Additionally, we use the ICC metric, developed by a pre-trained autoencoder, to independently assess and filter captions. EcoDatum ensures the dataset retains only concrete and relevant captions, directly corresponding to their images."}, {"title": "Multimodal Curation Operators", "content": "EcoDatum enhances multimodal data curation by integrating both global and local image-text features, as shown in Figure 4. We employ GroundingDINO for precise local feature alignment, ensuring detailed correspondence between text and images at the object level. Additionally, we utilize the CLIP model, augmented with innovative adaptations, to maintain global semantic coherence throughout the dataset.\nLocal Cross-Modal Feature Alignment. We utilize GroundingDINO for the precise alignment of text descriptions with corresponding visual content. It integrates and analyzes text and visual data, effectively identifying relevant phrases in captions and accurately localizing associated visual elements within images, ensuring precise text-to-image alignment without prompt modification.\nTo quantitatively assess the alignment between text and images, we develop a metric based on the count of bounding boxes with confidence scores exceeding a predefined threshold,", "equations": ["Count_{GroundingDINO} = \\sum_{i=1}^n I{x_i > t} \\qquad(1)"]}, {"title": "", "content": "old, as shown in Eq (1). This metric serves to highlight the degree of correspondence between textual descriptions and visual representations. A higher count of accurate detections indicates richer, more detailed scenes, signifying that these data points are of higher value for training and subsequent applications. Data points that do not meet this threshold can be effectively filtered out, including those where the described objects do not visually correspond to the images or where the textual descriptions are insufficiently specific. This ensures our dataset excludes mismatches and generalities, retaining only high-quality, relevant multimodal content.\nThis operator enhances the ability to curate multimodal data effectively, ensuring that the dataset maintains the most relevant and accurately aligned text-image pairs locally.\nGlobal Cross-Modal Feature Alignment. In this module, EcoDatum utilizes the CLIP model, celebrated for its ability to assess the global semantic similarity between text descriptions and their visual counterparts. However, the effectiveness of the CLIP-Score can be compromised when images contain textual content that overlaps with captions. This issue is observed in 40% of the LAION dataset and 20% of the Datacomp dataset . To mitigate this, we implement an innovative adaptation known as Flip-CLIP, which includes Horizontal-CLIP (H-CLIP) and Vertical-CLIP (V-CLIP) techniques, inspired by. Before computing the CLIP scores, images are flipped horizontally or vertically, reducing the model's bias towards text-based features and enabling more equitable evaluations of purely visual elements. The development of Flip-CLIP is motivated by the observation that OCR tasks often disproportionately influence the standard CLIP score, especially when the image-text is overlapped.\nBy integrating both CLIP-Score and Flip-CLIP-Score, we foster the model's ability to learn from visual content independently of textual influences, thereby enhancing EcoDatum's capability to process and understand global visual features without excessive bias towards textual elements."}, {"title": "Modality Operators Ensemble", "content": "Given the vast volume of data and the high cost associated with obtaining high-quality labeled data, the availability of reliable labels is often limited. EcoDatum introduces a weak supervision labeling system that allows the efficient generation of quality-indicated labels at scale, mitigating the challenges of data scarcity and ensuring a more robust data quality assessment. In this study, data curation is abstracted as a data quality discrimination task, aiming to identify \"high-quality\" data. This ensemble-based system further enhances the capabilities of the data operators described above.\nSpecifically, EcoDatum employs a weak supervision ensemble model called LabelModel into the scope of data curation research, which integrates signal sources abstracted from unimodal and multimodal operators for data quality evaluation. This integration balances the limitations of individual operators and significantly reduces their erroneous impacts.\nEach operator serves as an independent weak supervision signal source, assessing data quality from its unique dimension. The integration approach in this work uses LabelModel to combine multiple operators, automatically inferring a data quality score for each data sample by modeling the accuracy and relationships of these operators.\nThis process begins by matching each operator with corresponding labeling functions (LFs), which converts the operator's inferred score s of the data sample x_i into weak supervision label L, as shown in Eq (2). The LFs compute operators' inference results with the mean value b and the standard deviation \u03b2 of the decision boundary to transform continuous scores into discrete labels. These labels are then aggregated to form a comprehensive weak supervision label matrix. In this context, weak supervision labeling with \"Abstain\u201d addresses situations where LFs face unclear features or inapplicable rules. Allowing the LabelModel to abstain from assigning labels in these cases prevents the generation of incorrect labels. This approach enhances the LabelModel's ability to integrate diverse LFs by learning transformed matrix, particularly when they exhibit different biases and error patterns, thereby increasing the model's robustness when handling heterogeneous data.", "equations": ["L_{xij} = \\begin{cases} 1, & \\text{if } s_{xij} \\geq b_j + \\beta_j \\quad \\text{(Selected)} \\\\ -1, & \\text{if } s_{xij} \\leq b_j - \\beta_j \\quad \\text{(Filtered)} \\\\ 0, & \\text{if } b_j - \\beta_j < s_{xij} < b_j + \\beta_j \\quad \\text{(Abstain)} \\end{cases} \\qquad(2)"]}, {"title": "", "content": "The LabelModel learns the transformed weak supervision label matrix LM, estimating the weight w_j for each LF. These weights are used to combine the outputs of all LFs, ultimately generating a score for each data sample, which determines whether it is retained or filtered out. This approach enhances the comprehensiveness and robustness"}, {"title": "Search-based Optimization", "content": "A novel search-based optimization method is introduced to enhance the design of LFs, improving the generation of a more accurate weak supervision label matrix for LabelModel modeling, as shown in Figure 5. This method addresses the challenge of converting operator-derived scores into labels by automatically optimizing LFs, reducing the need for manual experimentation. To further optimize the performance of the ensemble, EcoDatum proposes a composite metric that integrates the LabelModel's data quality assessment capability with the attributes of LFs combination from the transformation steps, enabling a refined weak supervision label matrix. This approach enhances the LabelModel's ability to analyze operator interrelations and importance, producing quality scores for data samples that closely approximate the ideal.\nThe evaluation stage automatically constructs a small labeled dataset containing \u201cclean\u201d and \u201cnoisy\u201d samples. Clean data, labeled \"1\", are sourced from the COCO dataset, while \u201cnoisy\u201d samples, labeled \u201c0\u201d, are randomly sampled from the DataComp dataset to introduce both unimodal and multimodal noise and include added cross-modal noise through image-text pair exchanges. This setup tests the LabelModel's ability to differentiate data quality via the F1-tiny scores in Eq (3). Importantly, this dataset is only used for assessing the LabelModel's performance and does not contribute to training the model or optimizing Eq (3) coefficients, ensuring unbiased validation of the LF effectiveness.\nTo evaluate the data quality discrimination capacity of the LabelModel after learning generated weak supervision label matrics with different combinations of LFs, this research develops a specialized composite metric, shown in Eq (3), which combines classification metrics against ground truth and further incorporates the attributes of each opera-", "equations": ["M = a_1 \\cdot F1_{tiny} + a_2 \\cdot f_{Overlap} - a_3 \\cdot f_{Conflict} + a_4 \\cdot f_{Coverage} \\qquad(3)", "f_{overlap} = \\frac{1}{n} \\sum_{i=1}^{n} I ( \\sum_{j=1}^{m} I (LF_j(x_i) \\neq 0) > 1 ) \\qquad(4)", "f_{Conflict} = \\frac{1}{n} \\sum_{i=1}^{n} I ( \\sum_{j_1 \\neq j_2, LF_{j_1}(x_i) \\neq LF_{j_2}(x_i) \\neq 0) ) \\qquad(5)", "f_{Coverage} = \\frac{1}{n} \\sum_{i=1}^{n} I( \\sum_{j=1}^{m} I (LF_j(x_i) \\neq 0) \\geq 1 ) \\qquad(6)"]}, {"title": "", "content": "tors' LFs, specifically measuring the foverlap, fConflict, and fCoverage. Here, they respectively indicate the frequency of agreement among LFs, the extent of disagreements, and the proportion of data labeled by at least one function."}, {"title": "Experiments", "content": "The DataComp benchmark uniquely emphasizes data curation over model development. Unlike typical machine learning competitions that seek the best model with a fixed dataset, DataComp challenges participants to curate optimal datasets using fixed training code. This highlights the crucial role of high-quality, well-curated data in enhancing model performance. We choose the small-scale filtering track to validate the proposed framework EcoDatum, we curate a subset from a vast pool of 12.8 million image-text pairs from Common Crawl, adhering to the competition's constraints of fixed training parameters and computational budgets. Our objective is to efficiently filter this dataset, ensuring consistency in training iterations regardless of dataset size.\nThe effectiveness of our curated dataset is evaluated across 38 diverse datasets, including ImageNet, 6 ImageNet distribution shift datasets, 12 datasets from the Visual Task Adaptation Benchmark, three retrieval datasets, and several others. This extensive range of evaluation datasets tested the generalizability and robustness of EcoDatum, providing a comprehensive assessment of their impact on model training across various real-world scenarios."}, {"title": "Implementation Details", "content": "For the local cross-modal curation operator, we employ the GroundingDINO-based model with Swin-Large as the image backbone and BERT-Base for encoding text, setting confidence thresholds at 0.1 to retain more potentially feature-aligned data. In global cross-modal curation, we use the CLIP-ViT-Large-14 architecture. In determining final data volume, we conducted extensive experiments and reviewed related works, concluding that approximately the top 40% samples by the EcoDatum quality score after deduplication (around 3.5M) provide the best balance between quality and quantity. Experiments utilized 8 NVIDIA A100 GPUs, The training and evaluation process required 2.5 hours. Data curation for the 12.8 million dataset involved approximately 10 hours."}, {"title": "Result Analysis", "content": "Existing Baselines. Several SOTA methods have previously set benchmarks in data filtering. LAION and CLIP Score utilize the CLIP model to refine datasets, while Datacomp Filtering employs heuristic unimodal operators for targeted data refinement. The HYPerbolic Entailment (HYPE) Filtering technique enhances data quality by integrating unimodal specificity with cross-modal alignment. LINE's strategy leverages large models for web data curation. The Text-Masking and Re-Scoring (T-MARS) method corrects imbalances where textual features overpower visual ones, and the University of Wisconsin-Madison's (WS) approach utilizes an ensemble of object detection methods to optimize data filtering.\nPerformance Comparison. Building upon these foundations, EcoDatum enhances both efficiency and model training outcomes. As outlined in Table 1, using only 3.5 million data pairs from the original 12.8 million, EcoDatum achieved the highest average score of 0.182. This surpasses the performance of established methods like T-MARS and WS, both of which scored 0.180 across 38 diverse evaluation datasets. This curation strategy not only reduces computational overhead by 72% but also significantly improves data quality. EcoDatum exceeds the \"No Filtering\" baseline score of 0.132 and the Datacomp Basic filtering score of 0.142 by 28%. The integration of advanced methodologies like our optimized LabelModel for labeling functions tuning further refines the data curation process, setting new benchmarks in multimodal applications. The empirical results robustly validate our hypothesis that smaller, well-curated datasets can outperform larger, unfiltered datasets, underscoring the effectiveness of EcoDatum. Moreover, additional experiments show that EcoDatum consistently improves performance and scales effectively with increasing dataset size.\nIn this study, we introduce a composite metric designed to automatically optimize the generation of labeling functions (LFs), thereby facilitating the creation of a more accurate weak supervision label matrix. This optimization directly enhances the learning efficiency of the LabelModel, significantly improving its ability to assess data quality. To validate the effectiveness of this composite metric, we conducted a rigorous experimental case study. The process involved documenting a systematic search to identify the most effective LF combinations and repeatedly evaluating their impact on the average performance across a diverse set of 38 bench-"}, {"title": "Ablation Study", "content": "This experiment conducts a systematic evaluation of data filtering techniques to assess impacts on the performance of the deep learning model, as detailed in Table 2. The \"No Filtering\" condition acts as the control group. \u201cRandom Deduplication\" utilizes a stochastic method to eliminate duplicates, indicating that even indiscriminate reductions can improve model performance by balancing feature distribution.\nThe introduction of QGD achieves a 1.4% improvement over the random method with the same dataset size. Incorporating a unimodal operators' ensemble within the QGD framework results in a 4.8% improvement, while a multimodal operators' ensemble leads to a more substantial 9.5% enhancement. These results highlight the efficacy of both unimodal and multimodal operator ensembles in data curation. By integrating QGD with both unimodal and multimodal ensembles, the combined approach outperforms all others, showing a 45.4% improvement in performance compared to the \"No Filtering\u201d baseline. These experiments illustrate that EcoDatum strategically integrates advanced deduplication techniques and sophisticated ensemble frameworks to markedly elevate data quality, optimizing the pretraining process for multimodal models.\nWe conduct another ablation study to assess the individual contributions of data processing operators in data curation. By applying each operator independently and incrementally adding them, we explored their impact on downstream tasks. This approach allowed us to identify the most effective combinations of operators, significantly streamlining the optimization process. Through meticulous integration and refinement of labeling function (LF) constructions, we deter-"}, {"title": "Conclusion and Future Work", "content": "The volume of web-crawled datasets is rapidly expanding, and training multimodal models with such data are increasingly prevalent. This paper addresses the challenge of variable sample quality in web-crawled datasets by introducing a novel data curation framework, EcoDatum, designed to select high-quality data. EcoDatum begins with quality-guided deduplication to preprocess the data, followed by the integration of unimodal and multimodal operators into a weak supervision ensemble model, LabelModel, and have employed a search-based optimization method to refine the labeling matrix within LabelModel. Our experiments demonstrate robust performance across all evaluated tasks, securing a 1st place ranking in the small-scale track of the DataComp benchmark. While this study validated EcoDatum on a small dataset, future work will extend the evaluation to larger datasets. This expansion will further test the scalability of EcoDatum, aiming to solidify its effectiveness and efficiency in enhancing the training of multimodal models with diverse, large-scale web-crawled data."}]}