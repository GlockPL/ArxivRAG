{"title": "DREAMRUNNER: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation", "authors": ["Zun Wang", "Jialu Li", "Han Lin", "Jaehong Yoon", "Mohit Bansal"], "abstract": "Storytelling video generation (SVG) has recently emerged as a task to create long, multi-motion, multi-scene videos that consistently represent the story described in the input text script. SVG holds great potential for diverse content creation in media and entertainment; however, it also presents significant challenges: (1) objects must exhibit a range of fine-grained, complex motions, (2) multiple objects need to appear consistently across scenes, and (3) subjects may require multiple motions with seamless transitions within a single scene. To address these challenges, we propose DREAMRUNNER, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout and motion planning. Next, DREAMRUNNER presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control. We compare DREAMRUNNER with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DREAMRUNNER exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly out-performing baselines on T2V-ComBench. Finally, we validate DREAMRUNNER's robust ability to generate multi-object interactions with qualitative examples.", "sections": [{"title": "1. Introduction", "content": "Storytelling video generation (SVG) models [16, 17, 68, 74] capable of creating multi-scene, multi-object, motion-based long videos are essential for advancing real-world applications of video generation, enabling the creation of rich, immersive narratives with realistic and engaging interactions. Unlike existing short-form video generation approaches [8, 11, 12, 14, 19, 25, 41, 46, 50, 56, 65, 67], these models allow characters and objects to evolve across scenes, enhancing the coherence of generated content to align more closely with human storytelling. Such capabilities hold vast potential in media, gaming, and interactive storytelling.\nSVG presents unique challenges in seamlessly translating text scripts into long-form videos, such as: (1) Ensuring fine-grained, complex object motions: subjects in storytelling videos often need to display fine-grained motions that align with the narrative's demands rather than relying on basic or repetitive movement patterns; (2) Maintaining multiple objects consistency across multiple scenes: characters introduced in one scene need to retain recognizable features (e.g., appearance and position) throughout the story, despite dynamic movements and changing environments; and (3) Managing smooth transitions for multiple motions within a single scene: a robust SVG model needs to represent seamless transitions between different actions or states of a subject within the same scene, ensuring continuity and coherence to enhance the flow of the story, such as a character running across a beach before transitioning to a calm walk. However, recent SVG approaches [16, 17, 36, 68, 71, 74], while effective at preserving the main character across multiple scenes, are limited in generating diverse and natural object motions, resulting in restricted movement throughout storytelling sequences. They also struggle to incorporate multiple objects and multiple concurrent motions, preventing dynamic interactions and transitions that are essential for richer narratives.\nTo address these challenges, we propose DREAMRUNNER, a novel retrieval-augmented story-to-video generative approach that produces long-form, multi-character, multi-motion videos across multiple scenes by incorporating a new motion and subject prior learning strategy along with region-based diffusion. DREAMRUNNER facilitates fine-grained video generation capabilities through test-time fine-tuning with retrieved motion-oriented videos and reference images, enabling consistent character appearance across multiple scenes while enhancing motion quality and transitions. DREAMRUNNER presents three essential processes in the framework (see Fig. 1): 1) Dual-Level Video Plan Generation, 2) Motion Retrieval and Subject/Motion Prior Learning, and 3) Spatial-Temporal Region-Based 3D Attention and Prior Injection (SR3AI). In (1) plan generation stage, given a generic story narration provided by the user, we use a large language model (LLM) for hierarchical planning by first generating a high-level sequence of character-driven, motion-rich events across scenes (see \"High-Level Plan\" in Fig. 1), and then building detailed, entity-specific plans for each frame within a single scene (see \"Fine-Grained Plan\" in Fig. 1). This two-step approach allows us to define precise entity motion and layout for each frame, ensuring that each scene is narratively coherent. In (2) motion retrieval and motion/subject priors learning stage, to enable the video diffusion model to accurately follow these scene-level scripts, especially for localized and complex motions involving multiple objects, we enhance the test-time motion adaptation approach in [69] with a novel automatic retrieval pipeline which retrieves videos relevant to the desired motions from a video database [52] for learning the motion prior. The prior is learned by only updating parameter-efficient modules [21] within the specific layers of DiT [40], while a similar design is employed for learning the character priors [45]. In (3) video generation with region-based diffusion stage, we introduce a novel spatial-temporal region-based 3D attention and prior injection module, SR3AI, designed for video generation with fine-grained control. Our SR3AI enables control over several aspects of video generation: (a) Detailed frame-by-frame semantics, supporting the smooth transition of multiple actions and across frames. (b) Regional motion and character control within the 3D attention layers, improving object-motion binding and reducing interference between the generation of multiple objects and motions. To achieve this, we first encode multiple conditions involved in the fine-grained plan. Next, in our SR3AI, for each condition, we compute the corresponding latents within the provided spatial-temporal layouts in the latent space, then for each condition, we mask its attention to unrelated regional latents, while unrelated conditions will be masked for regional latents as well. Such spatial-temporal-region-based attention allows fine-grained object-and-motion-level control of the generation process. Additionally, we inject the learned character and motion priors exclusively into their corresponding regions within the diffusion model, allowing multiple priors to be integrated seamlessly, avoiding conflicts and ensuring coherent results.\nWe validate the effectiveness of DREAMRUNNER on two tasks: story-to-video generation and compositional text-to-video generation. For story-to-video generation, we collect a story dataset, DreamStorySet, and compare DREAMRUNNER against previous SoTA approaches (i.e., VideoDirectorGPT [33] and VLogger [74]). DREAMRUNNER achieves a 13.1% relative improvement in character consistency (CLIP score) and an 8.56% relative gain in text-following capability (ViCLIP score) over these approaches. Additionally, DREAMRUNNER enhances smooth event transitions within a single scene, with a 27.2% relative improvement in DINO score, underscoring its effectiveness. In compositional text-to-video generation, DREAMRUNNER, built on CogVideoX-2B [62], outperforms baseline methods on T2V-CompBench [48] across all metrics, including attribute binding and motion, highlighting its strength in compositional generation. Notably, although based on an open-source model, DREAMRUNNER achieves the highest dynamic attribute binding and comparable results in spatial relationships and character interactions compared to closed-source models. This demonstrates the potential of our approach to elevate open-source models toward closed-source performance levels. Lastly, we present a qualitative example to demonstrate the effectiveness of DREAMRUNNER in multi-character generation and motion binding."}, {"title": "2. Related Work", "content": "Storytelling Video Generation. Storytelling video generation [16, 17, 35, 36, 68, 71, 74] aims to create multi-scene videos based on given scripts. VideoDirectorGPT [33] and Vlogger [74] use LLMs for high-level planning, decomposing scripts into multi-scene conditions, and generating videos scene by scene. Animate-A-Story [17] further improves motion control by using retrieval-augmented videos as depth conditions. Recently, DreamStory [16] and MovieDreamer [68] generate keyframes using text-to-image models, which are then animated with image-to-video models [9] to create a coherent flow. Customization methods [13, 26, 29, 39, 45, 47, 54, 63, 70] are also involved to keep the consistency of reference images. Unlike these approaches, our work is designed to address the video-centric challenge of generating multi-characters, motion-rich videos with smooth, natural transitions between different actions using dual-level LLM planning.\nMotion Customization. Motion customization is a fundamental challenge in video generation. One line of research focuses on pixel-level motion learning from a video editing perspective [23, 34, 43, 57, 66], where the goal is to generate videos that replicate low-level motions present in the original images, ensuring consistency in movement across frames. Another approach emphasizes learning generic motion priors, such as human or camera movements, from a manually-curated collection of related videos [53, 58, 69, 73], capturing high-level semantic actions essential for realistic motion depiction. Most of these methods leverage test-time fine-tuning using specific motion LoRAs or adapters to adapt to particular motions. Our method follows a similar pipeline but enhances it by learning motion priors from retrieved videos sourced from large-scale video databases, enabling more contextually relevant and diverse motion customization for generating realistic and dynamic scenes.\nCompositional Diffusion. Recent advances in diffusion models have opened new possibilities for compositional text-to-video generation by improving video coherence, semantic alignment, and user control. Several methods have explored utilizing large language models (LLMs) for fine-grained scene planning [12, 32, 33]. With the fine-grained planning for video scenes, some approaches employ regional masks to control multi-object generation [22, 49, 55, 60, 64], improving both visual and semantic continuity in video generation. Other works explore frame-level semantic controls by applying different text conditions for different frames [7, 59]. Additionally, compositional techniques that integrate multiple LoRA modules have been developed to introduce diverse concepts seamlessly within the generation process [15, 30, 61, 72]. However, these methods do not specifically address the binding between objects and their corresponding actions spatial-temporally. Our approach focuses on fine-grained control over both objects and motions, with an emphasis on maintaining a cohesive link between objects and their actions throughout the video."}, {"title": "3. Methodology", "content": "Task Setup. Storytelling video generation focuses on creating multi-scene, character-driven videos based on a given topic. The characters are defined by reference images (e.g., images of a witch), and the topic is presented as an instructional prompt (e.g., \"witch's one day\"). The generated videos should align with the given topic and accurately reflect the characteristics and behavior of the characters.\nMethod Overview. Our approach employs a hierarchical system where an LLM generates event-based scripts across multiple scenes, followed by detailed plans specifying the layout and motion transitions of key objects per scene (Sec. 3.1). A video diffusion model then synthesizes each scene step by step. We train motion priors from retrieval videos aligned with the LLM-generated plans, sourced from a large-scale video-language database, and character priors using the reference images (Sec. 3.2). Finally, we inject these priors and detailed plans into the video generation process in a zero-shot manner using our spatial-temporal regional diffusion module SR3AI (Sec. 3.3).\nBase Generation Model. We leverages CogVideoX-2B [62] as the foundational text-to-video backbone. CogVideoX employs a DiT-based architecture that integrates full 3D attention, rather than seperated spatial and temporal attentions. The model generates 6-second videos at 8 fps conditioned on input text. In our method, we extend CogVideoX by training character and motion priors in distinct layers (see Sec. 3.2) and by modifying its 3D attention (see Sec. 3.3) for better motion and character binding."}, {"title": "3.1. Generating Dual-Level Plans with LLMs", "content": "Story-Level Croase-Grained Planning. We use an LLM to generate a sequence of narrations that span multiple scenes. Specifically, given the task requirements, a sin-"}, {"title": "3.2. Motion Retrieval and Prior Learning", "content": "Retrieving Motion-Related Videos from Database. We employ a retrieval-augmented approach to fine-tune motion priors at test time, enhancing the model's capacity to generate complex and diverse motions. Based on motion descriptions generated from the LLM planning, we retrieve relevant videos from a large-scale video database [52]. For instance, for the query motion \u201csitting\u201d, our retrieval pro-"}, {"title": "Motion Prior Training.", "content": "Following MotionDirector [69], we apply test-time fine-tuning to learn specific motions. In MotionDirector, one or more retrieved videos are used to parameter-efficiently fine-tune a video diffusion model [1, 50] with LoRA. Temporal LoRA are injected into layers where temporal attention is applied to capture motion patterns (e.g., jumping), while per-video-specific LoRAs are injected into the spatial layers with spatial attention to capture unique characteristics of each video, while only the temporal LoRA are injected during inference. In our case, since we use CogVideoX [62] with 3D full attention instead of separate spatial and temporal attention, we manually designate the even layers as 'spatial' layers and the odd layers as 'temporal' layers to separate learning of spatial and temporal LoRAs. We train the LoRAs on our filtered top-ranked videos with all other backbone parameters frozen, using two diffusion losses: a standard diffusion loss $\\mathcal{L}_{org}$, which is a reconstruction loss of all the video frames, and an appearance-debiased temporal loss $\\mathcal{L}_{ad}$, which decouples the motion space from appearance space in the latent space, focusing on only reconstructing the motions in the videos. Formally,\n$\\mathcal{L}_{org} = \\mathbb{E}_{z_0, y, \\epsilon \\sim \\mathcal{N}(0,1), t \\sim \\mathcal{U}(0,T)} [||\\epsilon - \\epsilon_\\theta(z_t, t, y)||^2]$                                                             (1)\nwhere $z_0$ is the latent encoding of the training videos, y is the text prompt condition, $\\epsilon$ is the Gaussian noise added to the latent space, $\\epsilon_\\theta$ is the predicted noise, and t is the denoising time step. The appearance-debiased temporal loss optimizes the normalized latent space:\n$\\phi(\\epsilon) = \\frac{\\epsilon}{\\sqrt{\\beta \\mathbb{E}_{anchor} + 1e{-8}}}$                                                               (2)\nwhere $\\mathbb{E}_{anchor}$ is the anchor among the frames from the same training data, and $\\beta$ is the strength factor that controls the strength of the debiasing. $\\mathcal{L}_{ad}$ is defined as:\n$\\mathcal{L}_{ad} = \\mathbb{E}_{z_0, y, \\epsilon \\sim \\mathcal{N}(0,1), t \\sim \\mathcal{U}(0,T)} [||\\phi(\\epsilon) - \\phi(\\epsilon_\\theta(z_t, t,y))||^2]$                                                (3)\nIn the end, we update the model using a combined motion loss function defined as $\\mathcal{L}_{motion} = \\mathcal{L}_{org} + \\mathcal{L}_{ad}$. Notably, we do not apply scaling to each loss term throughout experiments in the paper, highlighting the robustness and simplicity of hyperparameter selection in DREAMRUNNER."}, {"title": "Subject Prior Learning.", "content": "We learn the subject's appearance by injecting LoRA modules into the spatial transformer layers. To train these LoRAs, we create videos by repeating reference images multiple times (48 time, similar to the output frame number of CogVideoX) and focus on reconstructing the first frame of the video during training, preventing overfitting to the static, repeated video. Notably, the subject priors are learned within spatial LoRAs, while the motion priors are learned within temporal LoRAs. Since their injections target different layers, there is no overlap, effectively avoiding conflicts between multiple LoRAs."}, {"title": "3.3. Sapatial-Temporal-Region-Based Diffusion", "content": "Region-Based 3D Attention. We build our model on CogVideoX-2B [62], a 2 billion text-to-video generation model designed on top of a Diffusion Transformer (DiT). Unlike methods that use separate spatial and temporal attention for efficient video modeling, CogVideoX employs a 3D full attention module, integrating self-attention across concatenated embeddings of all visual latents and the text condition embeddings. We extend this module to enable region-specific conditioning via masking, aligning different regions with their respective text descriptions. Specifically, given a fine-grained plan with N region-specific text descriptions $C_1, C_2, ..., C_N$ and corresponding layouts $L_1, L_2, ..., L_N$ across frames, we encode each text condition $C_i$ to produce embeddings $T_1, T_2, ..., T_N$ (Fig. 2 top right). At each attention layer, we identify the visual tokens corresponding to each layout $L_i$ in the latent space. We then perform masked self-attention on the concatenation of $T_1, T_2,..., T_N$ and $L_1, L_2,..., L_N$. The self-attention"}, {"title": "4. Experiments", "content": "In this section, we first introduce the evaluation datasets and evaluation metrics details in Sec. 4.1, then compare our DREAMRUNNER with prior methods on story-to-video generation in Sec. 4.2. Next, we present detailed ablation studies on the necessity of RAG and effectiveness of SR3AI in Sec. 4.3, and demonstrate the generalizability of our DREAMRUNNER to improve compositional text-to-video generation on T2V-CompBench [48] in Sec. 4.4. Moreover, we show the effectiveness of RAG for learning the motion prior on a more comprehensive motion dataset we collect in Sec. 4.5. Lastly, we present qualitative comparison between our DREAMRUNNER and previous approaches in Sec. 4.6."}, {"title": "4.1. Datasets and Evaluation Metrics", "content": "Evaluation Datasets. We evaluate DREAMRUNNER on two tasks: (1) story-to-video generation, and (2) compositional text-to-video generation. The first task focuses on the model's ability to follow the text closely while maintaining character and scene consistency throughout the story. The second task assesses various aspects of compositional-ity in video generation. For (1) story-to-video generation, we collect and introduce a new benchmark dataset, DreamStorySet. Specifically, we collect 10 characters, including 6 from existing customization datasets (CustomConcept101 [27] and Dreambooth [45]), and 4 with generation models (FLUX [4]). (featuring two motions per scene) and three multi-character stories (featuring two or three motions per scene). Each story comprises 5 to 8 scenes, incorporating a total of 64 diverse motions throughout. We focus on single-character stories for quantitative evaluation of SVG models and reserve multi-character stories for qualitative evaluation. For (2) compositional text-to-video generation, we use the T2V-CompBench [48] to benchmark the performance of DREAMRUNNER, where we select six dimensions except numeracy.\nEvaluation Metrics. We evaluate our generated storytelling videos across multiple dimensions, including Character Consistency (Frame-Reference-Image CLIP/DINO scores), Full-Narration-Text Alignment per Scene (Image/Video-Text CLIP/ViCLIP scores), Fine-Grained-Text Alignment per Scene (Image/Video-Text CLIP/ViCLIP scores), and Transition Smoothness (Frame-Frame DINO score). Detailed descriptions of the metrics and computation methods can be found in the appendix. We follow similar metrics to T2V-ComBench [48] for evaluating compositional text-to-video generation."}, {"title": "4.2. Story-To-Video Generation Evaluation", "content": "We compare our approach with previous SoTAs (VideoDirectorGPT [33] and VLogger [74]) for story-to-video generation on our DreamStorySet dataset. For VideoDirectorGPT and Vlogger, to promote better alignment between"}, {"title": "4.3. Ablation Studies", "content": "In this section, we demonstrate the effectiveness of RAG for automatic video retrieval in motion prior learning and SR3AI for achieving fine-grained control over objects and their motion. As shown in Table 2, leveraging SR3AI for enhanced object and motion binding (2nd row) significantly improves the smooth transition between events within a single scene (87.1\u219292.5). Besides, incorporating retrieval-augmented motion prior learning (3rd row) boosts the video-text similarity (fine-grained text following Vi-CLIP score: 22.5 23.5, and full prompt following ViCLIP score: 22.1\u219224.0). Lastly, we show that RAG and SR3AI can be combined effectively (last row), with the combined model achieving the best performance in both text alignment and smooth event transitions."}, {"title": "4.4. Compositional T2V Generalization", "content": "In this section, we demonstrate that DREAMRUNNER can be adapted to help the general compositional text-to-video generation task, as evaluated on T2V-CompBench [48]. Specifically, we employ an LLM (i.e., GPT-40) to generate fine-grained, hierarchical plans based on the prompt, while SR3AI enables regional control over objects and their motions. Due to computational constraints, we did not include motion LoRA or spatial LoRA for learning motion or character priors for each action in the benchmark. As shown in Table 3, DREAMRUNNER significantly outperforms the baseline approach, CogVideoX-2B [62], across all categories. Notably, DREAMRUNNER enhances consistent attribute binding by 0.0575 and dynamic attribute binding by 0.0554, indicating that the detailed planning by LLM improves attribute binding during video generation. Additionally, DREAMRUNNER improves spatial relationship accuracy by 0.1192 and motion binding by 0.0229, showing that SR3AI aids in maintaining spatial relationships between objects and binding their motions. Furthermore, DREAMRUNNER enhances multi-object interactions by 0.0975, underscoring its capability to manage interactions among multiple objects effectively. Beyond baseline comparisons, DREAMRUNNER built on CogVideoX-2B achieves superior or comparable performance compared to other open-sourced models (e.g., Open-Sora 1.2 [20], VideoTetris [49]), and achieves performance on par with some closed-sourced models (e.g., Gen-3 [5], PixVerse [3]), especially in dynamic attribute binding and maintaining good spatial relationships. This demonstrates our ap-"}, {"title": "4.5. Effect of RAG for Learning Motion Prior", "content": "We investigate the effectiveness of retrieval-augmented test-time fine-tuning for learning an enhanced motion prior. Specifically, for each motion, we use an LLM [37] to provide 6 prompts for each motion in the whole 64-motion set, then evaluate the average CLIP/ViCLIP scores using these prompts. As shown in Table 4, applying our approach to CogVideoX-2B results in both CLIP and ViCLIP score improvements, with an increase of 1.28 in CLIP and 2.20 in ViCLIP. The substantial ViCLIP gain indicates stronger alignment between the story and the generated video, highlighting significant enhancements in motion accuracy. Additionally, the improvement in CLIP score shows that our approach better preserves semantic alignment within individual frames. These results validate that RAG effectively retrieves videos with relevant motions, aiding in learning a more accurate motion prior for the model."}, {"title": "4.6. Qualitative Comparison", "content": "Fig. 3 presents one qualitative comparison with other approaches. Specifically, we compare with VideoDirectorGPT [33], Vlogger [74], and CogVideoX [62] with character LoRA. We observe that other approaches fail to maintain consistent character appearances (e.g., the bear and robot) across generated videos. VideoDirectorGPT fails to preserve character appearance, Vlogger exhibits character appearance interference, and CogVideoX merges the two characters into a single character with mixed features. In contrast, DREAMRUNNER successfully maintains the distinct appearances of both characters and generates realistic character interactions, which demonstrates that SR3AI significantly enhances fine-grained control over object-motion binding and reduces interference between different character LoRAs. Additionally, DREAMRUNNER achieves smoother event transitions within a single scene, showing the effectiveness of LLM planning for detailed scene descriptions and frame-level semantic injection with SR3AI."}, {"title": "5. Conclusion", "content": "In this work, we present DREAMRUNNER, a novel framework for story-to-video generation. Specifically, DREAMRUNNER first utilizes a LLM to structure a hierarchical video plan, then introduces retrieval-augmented test-time adaptation to capture target motion priors, and finally generates videos using a novel spatial-temporal region-based 3D attention and prior injection module for fine-grained object motion binding and frame-level semantic control. Experiments on both story-to-video and compositional T2V"}, {"title": "A. Ablations of RAG pipeline", "content": "We provide additional experiments in Section 3.2 to demonstrate the effectiveness of our data processing approach within the retrieval pipeline. Table 5 primarily evaluates the impact of the maximal number of retrieved videos and the use of CLIP and ViCLIP for filtering. For efficiency, we evaluate a subset of eight motions selected from the full pool of 64 motions. The results indicate that, compared to the CogVideoX zero-shot baseline (Row 1), retrieving videos without any filtering (Row 2) improves performance, even though BM-25 [44] retrieval introduces some noise. This highlights the importance of retrieval itself. Furthermore, adding CLIP and ViCLIP as filters (Row 4) further enhances performance, showcasing the benefit of using semantically aligned videos for improved motion learning. Additionally, retrieving a sufficient number of videos is critical, as evidenced by Row 3, where limiting retrieval to a maximum of three videos results in poorer performance compared to retrieving 20 videos (Row 4)."}, {"title": "B. Evaluation Metrics", "content": "We provide detailed evaluation metrics for evaluating generated storytelling videos in multiple dimensions in Section 4.1.\n\u2022 Similarity to Reference Images: We assess the alignment between the generated videos and reference images using Image-Image CLIP [18] and DINO [38] scores. The evaluation is conducted by averaging the CLIP/DINO similarity scores with each frame of the generated video and the reference images, with CLIP-L14 (336px) [18] and DINOv2-B16 [38] as image encoders.\n\u2022 Similarity to Full Narration per scene: We measure the alignment between the full narration and the generated videos for each scene using Image-Text CLIP [18] and Video-Text ViCLIP [52] scores. For the CLIP score, we uniformly sample eight frames from the single-scene video and compute the average score between each frame and the full narration. For the ViCLIP score, we directly use the alignment score between the video and the narration. Per-scene scores are averaged to obtain the overall score for the multi-scene storytelling video.\n\u2022 Fine-Grained Text Alignment per Scene: We also assess fine-grained alignment to textual descriptions using Image-Text CLIP [18] and Video-Text ViCLIP [52] scores. In our story, each narration contains two motions, which we decouple into two consecutive single-motion descriptions using an LLM [37]. For each generated single-scene video, we divide it into two segments at the temporal midpoint. We then compute the CLIP/ViCLIP scores between the first segment and the first description and between the second segment and the second description. The average of these two scores provides the per-scene score, and the per-scene scores are further averaged to compute the overall score for the storytelling video.\n\u2022 Transition: We assess whether the single-scene video achieves smooth transitions between two motions. To evaluate this, we uniformly sample four frames from the video per scene and calculate the average DINO similarity between adjacent frames. A higher transition score indicates smoother transitions, as it reflects minimal changes in the background across frames."}, {"title": "C. Compositional T2V Examples", "content": "In this section, we present qualitative examples demonstrating the capabilities of DREAMRUNNER in compositional text-to-video generation. DREAMRUNNER effectively generates videos with accurate action binding to different characters, consistent attributes across objects, dynamic attribute changes, motion control, object interactions, and appropriate spatial relationships between objects.\nFor action binding, as shown in Figure 4, DREAMRUNNER generates distinct motions for two objects (e.g., a wolf howling into a microphone and a fox playing the drums), ensuring the actions are correctly bound to their respective objects without interference.\nFor consistent attribute binding, as illustrated in Figure 5, our approach maintains separate attributes for different objects (e.g., a spherical globe and a cube-shaped clock) without any overlap or inconsistency.\nFor dynamic attribute changes, as shown in Figure 6, DREAMRUNNER naturally transitions object attributes over time (e.g., metal gradually rusting throughout the video).\nFor motion control, as depicted in Figure 7, DREAMRUNNER successfully directs object movements in different trajectories (e.g., a kite moving from left to right and a cyclist traveling from right to left).\nFor object interactions, as illustrated in Figure 8, interactions are accurately modeled, adhering to physical rules (e.g., pottery shattering upon hitting the floor).\nFinally, for spatial relationships, as shown in Figure 9, DREAMRUNNER generates rare or imaginative configurations (e.g., a duck positioned under a spacecraft) while maintaining spatial coherence.\nThese examples highlight the strong performance of DREAMRUNNER in generating high-quality compositional text-to-video outputs."}, {"title": "D. Characters.", "content": "We provide character examples in Figure 10, where the first four are generated using FLUX [4], and the others are collected from existing customization datasets (CustomConcept101 [27] and Dreambooth [45])."}, {"title": "E. Single-Character Examples", "content": "In this section, we present qualitative examples of video generation featuring a single main character. As shown in Figure 12 and Figure 11, DREAMRUNNER generates consistent characters throughout the entire story. Additionally, in each scene, DREAMRUNNER effectively captures multiple events, such as the mermaid first wandering through the plants and then examining unique shells."}, {"title": "F. Multi-Character Examples", "content": "In this section, we present qualitative examples of video generation featuring multiple characters. As illustrated in Figure 13 and Figure 14, DREAMRUNNER generates multi-scene, multi-character videos where each character retains its own motion and interacts seamlessly with others, without any interference. For instance, in Figure 13, the witch is shown pouring ingredients while the cat wanders around the room. Even as the cat approaches the witch, their motions remain independent, and the appearance of both characters is consistently preserved throughout the scene."}, {"title": "G. Region-Based 3D Attention Masks", "content": "Figure 15 shows and example of the attention Mask of our Spatial-Temporal Region-based 3D Attention introduced in Section 3.3. Different text colors represent different conditions, while the white region indicates the masked areas. Note that for simplicity, we reduce each condition to two words, each frame to three segments, and display only three conditions and two frames in the figure. In practice, conditions can be longer and more numerous, frames can have more segments, and there are 12 latent frames in total."}, {"title": "H. LLM Prompts", "content": "We provide detailed LLM prompts for both high-level plans and fine-grained plans in Listing 1 and Listing 3. respectively. For high-level plans, we use a simple in-context example with instructions, while fine-grained plans require reasoning before generating the output. Example outputs are shown in Listings 2 and 4. For multi-character scenarios, we modify character-related words and examples and limit required motions to a maximum of four."}]}