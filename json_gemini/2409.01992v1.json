{"title": "QueryCheetah: Fast Automated Discovery of Attribute Inference Attacks Against Query-Based Systems", "authors": ["Bozhidar Stevanoski", "Ana-Maria Cretu", "Yves-Alexandre de Montjoye"], "abstract": "Query-based systems (QBSs) are one of the key approaches for sharing data. QBSs allow analysts to request aggregate information from a private protected dataset. Attacks are a crucial part of ensuring QBSs are truly privacy-preserving. The development and testing of attacks is however very labor-intensive and unable to cope with the increasing complexity of systems. Automated approaches have been shown to be promising but are currently extremely computationally intensive, limiting their applicability in practice. We here propose QueryCheetah, a fast and effective method for automated discovery of privacy attacks against QBSs. We instantiate QueryCheetah on attribute inference attacks and show it to discover stronger attacks than previous methods while being 18 times faster than the state-of-the-art automated approach. We then show how QueryCheetah allows system developers to thoroughly evaluate the privacy risk, including for various attacker strengths and target individuals. We finally show how QueryCheetah can be used out-of-the-box to find attacks in larger syntaxes and workarounds around ad-hoc defenses.", "sections": [{"title": "1 INTRODUCTION", "content": "In the era of digital connectivity, we are generating data on an unprecedented level [55]. These data are collected on a large scale, which opens the door to new applications such as real-time traffic congestion update systems [5], systems for analyzing cycling and running routes [7], and large language models [59].\n\nThe data being collected is often personal and sensitive. It contains information about us and our interactions with technologies and people. For example, location data contains information about people's movement across space and time while census data contains information on households, including income.\n\nQuery-based systems (QBSs) are one of the key approaches to safely sharing data. A QBS is an interactive interface that a) allows the data provider to maintain control over a dataset and b) allows an analyst to retrieve answers to queries about a dataset without directly accessing the individual records. For example, a QBS protecting census data can allow an analyst to query the number of people with a salary higher than $50,000 residing in a given county and return an answer, of say 900. QBS implementations range from web application programming interfaces (APIs) to privacy-preserving SQL engines. They are widely used to share data by industry, academia, and government entities. Examples include QBSs for traffic congestion on the roads by Google Maps [5] and Uber Movement [8], cycling and running routes by Strava [7] and audience segment attributes by Meta [1]. Academia [37, 45] has proposed sharing data via QBSs with projects such as Airavat [54] for large-scale parallel computations on sensitive data. Government entities, for example, the Australian Bureau of Statistics (Table-Builder) [46] and UK's National Health Service (openSAFELY) [6] have used QBSs for sharing census and health data, respectively.\n\nQBSs answer queries by releasing aggregates about the protected dataset. Releasing aggregates has long been known not to be inherently privacy-preserving [29, 32, 42]. The non-privacy-preserving property is exacerbated by the flexibility given to analysts in QBSs to choose the aggregates themselves. An attacker can send queries highly specific to a target individual, for example, so-called difference queries [27, 34, 40], which are unlikely to be chosen by the data curator in case of a non-interactive one-time release of aggregates.\n\nTo protect privacy, QBSs implement defenses that provide formal privacy guarantees and ad-hoc defenses that do not. Differential privacy (DP) [30] stands as the gold standard for formal privacy guarantees. It aims to protect individual privacy by limiting the impact, measured by a parameter e, called privacy budget, of the inclusion or exclusion of any user's data. The implementation of DP defenses can be challenging in practice. For example, Google Maps [43] has used weaker, event-level instead of user-level, guarantees, while Amazon's data clean room [4] and LinkedIn's Audience Engagements [53] regularly (i.e., monthly) reset their budget to accommodate regular data releases, which can invalidate the guarantees in the long run. Even defenses that provide formal privacy guarantees might be at risk, from incorrect implementations [58, 60] to side-channel attacks [19]. Defenses that do not provide formal privacy guarantees instead rely on adversarial attacks to demonstrate their effectiveness [37]. This highlights the need to test the privacy guarantees of both types of defenses by using attacks.\n\nThere is a trend towards automating the privacy attacks [20, 26, 50, 57] as a response to the increasing flexibility given to the analysts to choose the aggregates from a wider range of options. A privacy attack consists of (1) queries and (2) a rule that combines their answers to infer private information. The level of automation varies from semi- to fully-automated methods. Fully-automated methods automate both the search for queries and the rule [26], while semi-automated methods require manual efforts in either or both components. All semi-automated methods in the literature manually reduce the possible queries through manual analysis and automate the search for the concrete queries [40] or the rule that combines their answers [50]. The search for queries, without manually reducing the possible queries, is a difficult task, in particular for QBSs that support a wide range of queries.\n\nQuerySnout [26] is, to the best of our knowledge, the only fully-automated method for discovering attacks against a QBS. QuerySnout's automatically discovered attacks consistently outperform, or perform on par with the best-performing known attacks. However, to explore the search space of (multisets of) queries, QuerySnout relies on a computationally expensive evolutionary search technique. Since it maintains a population of multisets of queries in every iteration, its time complexity is proportional to the product of (1) the number of iterations, (2) the number of multisets of queries in the population, and (3) the number of target users. Its ability to find vulnerabilities in QBSs is thus currently limited to a fraction of the attack surface, i.e., query syntax, offered by QBSs and to a highly limited number of target users. These limitations can lead to missed vulnerabilities, such as attacks that rely on expressive query syntax or attacks that only materialize for specific vulnerable users.\n\nContributions. In this paper, we present QueryCheetah, a method for fast automated discovery of attribute inference attacks against QBSs. At a high level, QueryCheetah moves away from a population of query multisets [26] to a single query multiset. It uses fast locally-informed iterations to search the space of (multisets of) queries. This makes each iteration 450 times faster than previous work while only requiring 25 times more steps, resulting in a speed-up of 18 times.\n\nIn line with previous work, we instantiate QueryCheetah on discovering attribute inference attacks (AIAs) against a real-world QBS, Diffix [37], that provides an SQL interface to analysts. The goal of an AIA is to infer a value for a sensitive attribute of the given user of interest. We formalize the AIAs as a distinguishability privacy game.\n\nWe first show that QueryCheetah outperforms both semi- and fully-automated methods [26, 40], while being an order of magnitude faster than the state-of-the-art methods. We second show how a) QueryCheetah can discover attacks specific to vulnerable users by attacking many users in a reasonable time, b) how it automatically discovers vulnerabilities in previously unexploited query syntax, and c) how it finds workarounds around defenses [36] developed and deployed to thwart discovered attacks [25, 40, 47, 49]."}, {"title": "2 BACKGROUND", "content": "2.1 Query-based system\n\nLet $U$ denote a universe of users, e.g., users of a service or a country's population. Given a set of users $U \\subset U$, a dataset $D \\sim \\mathcal{D}$, from a distribution $\\mathcal{D}$, consists of the records of these users over a set of attributes $A = {a_1, ..., a_n}$, where each attribute $a_i$ can take values in a set $V_i$. We denote by $s_D = |U| = |D|$ the size of the dataset $D$. For a given user $u \\in U$, we denote its record in D by $r_u = (r_u^1, r_u^2,.., r_u^n)$, with $r_u^i$ the user's value for attribute $a_i$. We denote the other possible values for an attribute $a_i$ by $V_u^i = V_i \\setminus \\{r_u^i\\}$. Given a subset of attributes $A' = \\{a_{i_1}, ..., a_{i_k} \\} \\subset A$ (with $\\{i_1, . . ., i_k\\} \\subset \\{1, ..., n\\}$ a subset of attribute indexes), we define the projection of a record $r_u$ over attributes $A'$ as $\\pi_{A'} : U \\rightarrow V_{i_1} \\times ... \\times V_{i_k}, \\pi_{A'}(u) = (r_u^{i_1},...,r_u^{i_k})$, which we write more concisely as $r_u^{A'} := \\pi_{A'}(u)$.\n\nConsider a data curator who wants to enable useful data analyses on a dataset D without releasing individual records. The data curator provides access to the dataset D via a query-based system (QBS), which implements an SQL interface. Analysts submit queries to this interface and retrieve their answers.\n\nWe denote by Q the query syntax supported by the QBS. To answer a query $q \\in Q$, the QBS performs the following steps: the QBS computes first its true answer on the dataset $T(D, q)$, then (optionally) perturbs it to obtain $R(D, q)$, and finally returns R(D, q) as a final answer. Figure 1 illustrates these steps. We denote by $Y(D, q) \\subseteq U$ the userset of query $q$, i.e., the set of users whose records satisfy all conditions in the query q. If a QBS receives an unsupported query $q \\notin Q$, in line with the literature [26], we assume that it returns 0 as an answer without evaluating it. Returning a 0 indeed reveals less information than a dedicated response as it introduces uncertainty as to whether the query answer is 0 or the query is unsupported.\n\nQueries that QBSs with an SQL interface typically support can be denoted as:\n\nSELECT agg FROM D\nWHERE $a_1 \\; c_1 \\; v_1 \\; O_1 \\; ... \\; O_{n-2} \\; a_{n-1} \\; c_{n-1} \\; v_{n-1} \\; O_{n-1} \\; a_n \\; c_n \\; v_n$,\n(1)\n\nwhere:\n\nagg is an aggregation function $agg \\in AGG$, such as $AGG = \\{count(), min(a_j), max(a_j), sum(a_j)\\}$ for some $j \\in \\{1, ..., n\\}$, respectively calculating the count of the records, the minimum, maximum or the sum of the values of an attribute $a_j$ of the records satisfying all conditions in the WHERE clause;\n\n$c_i$ is a comparison operator $c_i \\in C$, $i \\in \\{1, ... n\\}$, such as $C = \\{=, \\neq, \\bot\\}$ where $\\bot$ denotes that an attribute is skipped, i.e., the attribute does not appear in the WHERE clause of the query;\n\n$v_i$ is a value, $v_i \\in \\mathbb{R}$, $i \\in \\{1, ... n\\}$, and\n\n$O_i$ is a logical operator, $O_i \\in O$, such as $O = \\{AND, OR\\}$, $i \\in \\{1, ... n - 1\\}$."}, {"title": "2.2 Diffix", "content": "In this paper, we focus on a real-world QBS, Diffix [37], which implements a complex combination of privacy-preserving mechanisms, whose privacy loss can be difficult to manually test in practice. It is the most heavily studied and developed QBS that does not provide formal privacy guarantees.\n\nDiffix has used discovered attacks to patch and improve the system over time. The authors have organized two bounty programs [3, 15], where experts were invited to adversarially test the privacy guarantees of the system and get monetary prizes in return. Four manual or semi-automated vulnerabilities were discovered [2]: a membership inference attack (MIA) [49] based on earlier work [50], two reconstruction attacks [25, 47] based on earlier work as well [29], and an attribute inference attack (AIA) [40]. A fully-automated method has discovered AIAs against Diffix with stronger inference capabilities [26]. When patching the system against the discovered vulnerabilities, additional defenses were introduced, against which new vulnerabilities were discovered, leading to multiple versions of the system over time [35, 36, 38]. In this paper, we focus on up to the last version of Diffix against which, to the best of our knowledge, no known attacks exist, Diffix-Dogwood.\n\nWe refer to the defenses that were introduced in Diffix's first version as main defenses, while the defenses that were introduced in later versions to thwart discovered attacks as mitigations.\n\n2.2.1 Diffix's main defenses. Diffix's main defenses are: query-set size restriction, unbiased unbounded noise, and answer rounding.\n\n(1) Query-set size restriction: Diffix restricts releasing the answer of a query q if its true answer $T(D, q)$ is lower than 2 or a noisy threshold $T$, $T(D, q) < max(2, T)$, distributed as $T \\sim \\mathcal{N}(4, 0.5)$, where $\\mathcal{N}(\\mu, \\sigma)$ denotes the Gaussian distribution with mean $\\mu$ and standard deviation $\\sigma$.\n\n(2) Unbiased unbounded noise: Non-restricted queries are perturbed by adding two layers of noise, static and dynamic, to the true answer $T(D, q)$ for each filtering condition $a_i \\; c_i \\; v_i$, $i \\in \\{1, ..., n\\}$ in the WHERE clause of the query. The static $N_i^s$ and the dynamic $N_i^d$ noise terms are distributed as $N_i^s, N_i^d \\sim \\mathcal{N}(0, 1)$, $i \\in \\{1, ..., n\\}$.\n\n(3) Answer rounding: Finally, the noisy answer of non-restricted queries is rounded to the nearest integer, if the type of the true answer is an integer. Otherwise, the answer is not rounded.\n\nDiffix uses a seeded pseudo-random number generation (PRNG) for sampling the threshold $T$ and the static and dynamic noise terms $N_i^s, N_i^d$, $i \\in \\{1, ..., n\\}$. The seeding ensures that if a query q is received more than once, Diffix will respond with the same answer every time. For seeding the PRNG, Diffix is initialized with a secret salt $\\rho$ and an attribute $a_0$ that uniquely identifies all user records in D, $\\forall u, v \\in U, r_u^{a_0} \\neq r_v^{a_0}, u \\neq v$. Diffix seeds the PRNG for generating T with the secret salt $\\rho$ and the userset $Y(D, q)$. The sampling of the static noise, $N_i^s$, corresponding to the condition on the attribute $a_i$, is seeded with the secret salt $\\rho$ and the syntax of the condition itself, i.e., with the attribute $a_i$, the comparison operator $c_i$, and the value used $v_i$. The seed for the dynamic noise $N_i^d$ additionally includes the userset $Y(D, q)$ by using a bit-wise XOR over the attribute $a_0$ of all users in the userset $\\text{XOR}(r_{u_1}^{a_0}, ..., r_{u_{|Y(D,q)|}}^{a_0}), Y(D, q) = \\{u_1, ..., u_{|Y(D,q)|}\\}$.\n\nOverall, the perturbed answer R(D, q) has the following form:\n\n$R(D, q) = \\begin{cases}\n    0 & \\text{if } T(D, q) \\leq max(2, T) \\\\\n    round(T(D, q) + \\sum_{i=1}^{n} N_i^s + \\sum_{i=1}^{n} N_i^d) & \\text{otherwise}\n\\end{cases}$\n\n2.2.2 Query syntax. The privacy guarantees of the three mechanisms have only been tested within a limited query syntax, $Q_{lim}$, by previous work developing attribute inference attacks (AIAs). The limited syntax $Q_{lim}$ allows queries where:\n\nAGG = {count()};\n\n$C = \\{=, \\neq, \\bot\\}$;\n\n$v_i = r_u^i, i \\in \\{1, ... n - 1\\}$ and $v_n \\in \\{0, 1\\}$;\n\n$O = \\{AND\\}$.\n\nDiffix, however, supports a query syntax richer than $Q_{lim}$ [35, 36]. This leaves a large part of the syntax currently unexplored whether or not it leads to privacy vulnerabilities.\n\nThe limited syntax $Q_{lim}$ can be extended to a richer syntax of supported counting queries, $Q_{ext}$. The syntax $Q_{ext}$ extends $Q_{lim}$ along 4 axes, $D1, D2, D3, and D4$, $Q_{ext} = Q_{lim} \\cup \\{D_1, ..., D_4\\}$:\n\n(D1) Allowing any value: Allow conditions to compare to any value $v_i$ that is not necessarily the target user's value $r_u^i$. When axis $D1$ is used with any other axes $D2, D3$ or $D4$, $v_i$ can be a pair of values. Formally, $D_1$ extends the syntax to allow comparisons to any real value or pair of real values, $D_1 := v_i \\in \\mathbb{R} \\cup (\\mathbb{R} \\times \\mathbb{R})$, $i \\in \\{1, ..., n\\}$.\n\n(D2) Allowing BETWEEN: Allow the comparison operator BETWEEN, $BETWEEN \\in C$. This allows conditions $a_i \\text{ BETWEEN } (v_{i,1}, v_{i,2})$, $v_{i,1} < v_{i,2}$ that compare whether the value of the attribute $a_i$ is in the interval $(v_{i,1}, v_{i,2})$. Diffix allows intervals $(v_{i,1}, v_{i,2})$, $v_{i,1} < v_{i,2}$, with a width, $w = v_{i,2} - v_{i,1}$, that falls in the infinite set $w \\in \\{..., 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50,...\\}$ and offset $v_{i, 1}$ that falls on an even multiple of the width, $v_{i,1} = 2kw$, or an even multiple plus $1/2$ of the width, $v_{i,1} = (2k+\\frac{1}{2})w$, for some integer $k \\in \\mathbb{Z}$.\n\n(D3) Allowing IN: Allow the comparison operator IN, $IN \\in C$. This allows conditions $a_i \\text{ IN } \\{v_{i,1}, v_{i,2}\\}$ that compare whether the value of the attribute $a_i$ is in the set $\\{v_{i,1}, v_{i,2}\\}$. While Diffix allows for sets with more elements, here we focus only on two-element sets, in particular, $v_{i,1} = r_u^i$ and $v_{i,2} \\in V_u^i$.\n\n(D4) Allowing NOT IN: Allow the comparison operator NOT IN operator, $NOT IN \\in C$. This allows conditions $a_i \\text{ NOT IN } \\{v_{i,1}, v_{i,2}\\}$ that compare whether the value of the attribute $a_i$ is in not the set $\\{v_{i,1}, v_{i,2}\\}$. We use the same domains for $v_{i,1}$ and $v_{i,2}$ as in D3.\n\nThe extended syntax $Q_{ext}$ is a much broader space than $Q_{lim}$"}, {"title": "2.3 Threat model for attribute inference attacks", "content": "In this paper, we evaluate the privacy guarantees of Diffix in the context of attribute inference attacks (AIAs). We follow the threat model that has been used by existing AIAs against Diffix [26, 40]. AIAs aim to infer the value of a sensitive attribute for a target user u. Without loss of generality, we assume that the last attribute, $a_n$, is sensitive, and following the literature, we assume for simplicity that it is binary. The AIA thus aims to infer the binary value $r_u^n$.\n\nAlthough we focus on AIAs, note that our method can be extended to membership inference attacks (MIAs). MIAs aim to infer the presence of a given user record $r_u$ in the dataset, i.e., whether $r_u \\in D$, for some $u \\in U$.\n\nAccess to the query-based system. The threat model considers an attacker who has access a) to an instantiation of Diffix protecting a dataset D and b) to Diffix's software.\n\nThe attacker can access the protected dataset D only through sending queries to the Diffix instantiation. We refer to this Diffix instantiation as the target QBS. Although Diffix allows an unlimited number of queries, we assume that the attacker can send at most m queries to the target QBS. Since Diffix logs all received queries [19], using a low number of queries, typically tens or low hundreds, can help to avoid detection.\n\nWe assume that the attacker can use Diffix's software as a black-box executable, in line with the literature of fully-automated attacks [26]. Semi-automated attacks assume a white-box access to the software, which allows to manually study the system in detail, such as the noise addition mechanism [40]. Note that the white-box access here refers to access only to the software, and not to any values specific to the target QBS, such as the secret salt $\\rho$.\n\nAccess to auxiliary knowledge. The attacker has auxiliary knowledge a) about the target user u and b) about the dataset D. The attacker has access to a projection of the target record $r_u$ on a subset of attributes $A' \\subseteq A \\setminus \\{a_n\\}$, i.e., the attacker knows $r_u^{A'}$, and they also know that the target user is uniquely identifiable on $A'$, $\\forall v \\in U, v \\neq u, r_v^{A'} \\neq r_u^{A'}$.\n\nWe denote by $\\mathcal{K}$ the attacker's auxiliary knowledge about the dataset D. We follow the literature of fully-automated attacks [26] and instantiate $\\mathcal{K}$ as knowledge of an auxiliary dataset, $\\mathcal{K} = D_{aux}$, $D_{aux}$ of a distribution $\\mathcal{D}_{aux}$, $D_{aux} \\sim \\mathcal{D}_{aux}$, similar to the distribution $\\mathcal{D}$ of the protected dataset D. Semi-automated attacks instantiate $\\mathcal{K}$ as knowledge of the subsets of attributes for which the target record is unique [40].\n\nAttacker's goal The attacker's goal is to discover a multiset S of m queries, $S = \\{q_1,..., q_m\\}$ and a rule V to combine their answers by the target QBS, $V(R(D, q_1), . . ., R(D, q_m))$, to a prediction of the sensitive value $r_u^n$."}, {"title": "2.4 Semi-automated AIA against Diffix:Differential noise-exploitation attack", "content": "Gadotti et al. [40] introduced a semi-automated AIA against Diffix, called a differential noise-exploitation attack. It exploits Diffix's layered noise addition mechanism. The attack uses manually-identified pairs of queries, $(q_1, q_2)$, $q_1, q_2 \\in Q_{lim}$, whose usersets are either identical or differ in one record, depending on the value of the target user's sensitive attribute. In particular, it uses:\n\n$q_1 := \\text{SELECT count()} \\; \\text{FROM} \\; D$\n$\\text{WHERE} \\; a_{i_1} \\neq r_u^{i_1} \\; \\text{AND} \\; a_{i_2} = r_u^{i_2} \\; ... \\; \\text{AND} \\; a_{i_l} = r_u^{i_l}$\n\n$q_2 := \\text{SELECT count()} \\; \\text{FROM} \\; D$\n$\\text{WHERE} \\; a_{i_2} = r_u^{i_2} \\; ... \\; \\text{AND} \\; a_{i_l} = r_u^{i_l} \\; \\text{AND} \\; a_n = v_n$,\n(2)\n\nfor $v_n \\in \\{0, 1\\}$ and a subset of attributes $A'' = \\{a_{i_1}, ..., a_{i_l}\\}$ of the attributes known to the attacker A', $A'' \\subset A'$, on which (1) the target user u is uniquely identifiable, $\\forall v \\in U, v \\neq u, r_v^{A''} \\neq r_u^{A''}$, and (2) both queries $q_1$ and $q_2$ are not bucket suppressed. This attack uses on the fact that the target user u is the only user in the userset of $q_2, u \\in Y(D, q_2)$ who is not in the userset of $q_1, u \\notin Y(D, q_1)$, $\\forall v \\in U, v \\neq u, v \\notin Y(D, q_2) \\; \\forall v \\in Y(D, q_1)$. By calculating the difference in the query answers $\\Delta = R(D, q_2) - R(D, q_1)$ most of the static noise terms cancel out and the dynamic noise terms depend on the usersets of the queries that condition on the sensitive value. The difference $\\Delta$ is distributed as a Gaussian $\\Delta \\sim \\mathcal{N}(0, 2\\sigma^2)$ if $r_u^n = 1 - v_n$ and as a $\\Delta \\sim \\mathcal{N}(1, 2l + 2\\sigma^2)$ if $r_u^n = v_n$. To distinguish between the distributions of these two cases, the attack employs a likelihood ratio test.\n\nThe authors perform an automated search over the space of attribute subsets A'' to find the subsets that fulfill conditions (1) and (2). Note that, we here refer to the overall attack pipeline as only semi-automated because there are two parts discovered through manual analysis: the core vulnerability (queries) is identified manually by the authors, allowing them to restrict the search space, and the rule to combine the queries is manually crafted (likelihood ratio attack).\n\nNote that Gadotti et al. [40] extended the differential attack by appending filtering conditions a cv, a, c, v \u2208 A \u00d7 O \u00d7 R to queries $q_1$ and $q_2$, that do not change their usersets (e.g., appending the condition years_at_company \u2260 10 to a query containing employment_year = 2024). Following the literature on automated attacks [26], we do not include the extension in our comparison as crafting the conditions relies on domain knowledge."}, {"title": "2.5 Fully-automated AIA against Diffix:QuerySnout", "content": "Cretu et al. [26] proposed the only fully-automated method for discovering AIAs against Diffix, called QuerySnout. QuerySnout automates both parts that were manual in previous work: (1) the search for a rule to combine query answers in predicting the sensitive value and (2) the search for candidate multisets of queries. By devising the rule that combines the answers, QuerySnout estimates the likelihood that a given multiset of queries constitutes an attack, called the fitness of the multiset.\n\nFitness: combining the query answers. QuerySnout extends to AIAs an existing technique for estimating the vulnerability to MIAs by Pyrgelis et al. [50].\n\nThe attacker uses the auxiliary knowledge $\\mathcal{K} = D_{aux}$ and performs the following 9 steps to estimate the fitness of a query multiset $S = \\{q_1, ..., q_m\\}$:\n\n(1) Split the auxiliary dataset $D_{aux}$ in two equal partitions $D_{train}^{aux}$ and $D_{val}^{aux}$.\n\n(2) Uniformly at random sample without replacement z records from $D_{train}^{aux}$, $\\{r_{v_1}, . . . r_{v_z}\\}$, project them on A', $\\{r_{v_1}^{A'}, ... r_{v_z}^{A'}\\}$ and add the target user's record $r_u^{A'}$ to create a collection of z + 1 records $T = \\{r_{v_1}^{A'}...r_{v_z}^{A'}, r_u^{A'}\\}$. The goal of this step is to simulate the QBS behavior on similar datasets having different values of the sensitive attribute.\n\n(3) For each of the z + 1 records in T, independently sample a Bernoulli distribution, Bernoulli (0.5), and create a dataset $D_{train} = \\{r_{v_1}^{A'} \\cup \\{b_1\\},...r_{v_z}^{A'} \\cup \\{b_z\\}, r_u^{A'} \\cup \\{b_u\\}\\}$ over attributes A' $\\cup$ $\\{a_n\\}$, where $b_1, ..., b_z, b_u \\sim$ Bernoulli(0.5). Denote $b_u$ by $y_{train} = b_u$.\n\n(4) Repeat steps (2) and (3) to create f datasets $D_1^{train}, . . ., D_f^{train}$ and a vector of f values for the target user ($y_1^{train},..., y_f^{train}$).\n\n(5) Repeat steps (2) and (3) g times for $D_{val}^{aux}$ to create $D_1^{val}, ..., D_g^{val}$ and ($y_1^{val},..., y_g^{val}$).\n\n(6) Protect each of f + g datasets with the QBS by using the executable software they have access to. Each of the f + g instantiations of the QBS uses different values for the secret salt $\\rho$.\n\n(7) Evaluate the m queries in S to each of the f + g QBSs and obtain an (f + g) vectors of m query responses: $[R(D_1^{train}, q_1), ..., R(D_1^{train}, q_m)],..., [R(D_g^{val}, q_1), ..., R(D_g^{val}, q_m)]$.\n\n(8) Finally, train a logistic regression model to predict the sensitive value $y_i^{train}$ given the vector with m query answers, $[R(D_i^{train}, q_1), . . ., R(D_i^{train}, q_m)]$, from the QBS protecting $D_i^{train}$, as shown in Figure 2. The answers on the g QBSs protecting the datasets of step 5. are used for validation purposes.\n\n(9) Compute the multiset fitness $F = min(acc_{train}, acc_{val})$, where $acc_{train}$ and $acc_{val}$ denote the accuracy of the ML model predicting the sensitive value on QBSs from $D_{train}^{aux}$ and QBSs from $D_{val}^{aux}$, respectively.\n\nFinding candidate multisets of queries. QuerySnout proposes an evolutionary search technique with custom mutations to search for a multiset of queries with a high fitness value. It maintains a population $P_i$ of P multisets of m queries, $P_i = \\{S_{1,i},..., S_{P,i}\\}$ in each iteration i, where $S_{j,i} = \\{q_{j,i,1},..., q_{j,i,m}\\}$, $\\forall j \\in \\{1, ..., P\\}$, for $q_{j,i,1},..., q_{j,i,m} \\in Q_{lim}$ where $v_n = 1$.\n\nFirst, it initializes the population $P_0$ by constructing multisets of randomly sampled queries. Then, it performs an evolutionary search over I iterations. Finally, the attacker selects the multiset in $P_I$ with the highest fitness value and attacks the target QBS by sending the queries in it."}, {"title": "3 FORMALIZING AIA AGAINST A QBS AS A PRIVACY GAME", "content": "In this section", "by": "a query multiset S", "steps": "n\n(1) She samples a dataset of $s_D-1$ records from $\\mathcal{D}$, $\\{r_{u_1},..., r_{u_{s_D-1}}\\}$, projects them on A"}]}