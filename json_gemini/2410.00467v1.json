{"title": "Dynamic Planning for LLM-based Graphical User Interface Automation", "authors": ["Shaoqing Zhang", "Zhuosheng Zhang", "Kehai Chen", "Xinbe Ma", "Muyun Yang", "Tiejun Zhao", "Min Zhang"], "abstract": "The advent of large language models (LLMs) has spurred considerable interest in advancing autonomous LLMs-based agents, particularly in intriguing applications within smartphone graphical user interfaces (GUIs). When presented with a task goal, these agents typically emulate human actions within a GUI environment until the task is completed. However, a key challenge lies in devising effective plans to guide action prediction in GUI tasks, though planning have been widely recognized as effective for decomposing complex tasks into a series of steps. Specifically, given the dynamic nature of environmental GUIs following action execution, it is crucial to dynamically adapt plans based on environmental feedback and action history. We show that the widely-used ReAct approach fails due to the excessively long historical dialogues. To address this challenge, we propose a novel approach called Dynamic Planning of Thoughts (D-PoT) for LLM-based GUI agents. D-PoT involves the dynamic adjustment of planning based on the environmental feedback and execution history. Experimental results reveal that the proposed D-PoT significantly surpassed the strong GPT-4V baseline by +12.7% (34.66% \u2192 47.36%) in accuracy. The analysis highlights the generality of dynamic planning in different backbone LLMs, as well as the benefits in mitigating hallucinations and adapting to unseen tasks.", "sections": [{"title": "1 Introduction", "content": "Building autonomous agents capable of assisting humans in addressing real-world challenges has long been a central pursuit of artificial intelligence research. Recently, there has been a surge in exploration within the realm of autonomous agents, fueled largely by the emergence of large language models (LLMs). One prevalent application scenario involves automating graphical user interfaces (GUIs) on smartphones, where LLMs are tasked with perceiving smartphone GUIs and sequentially predicting action commands until the task is completed.While previous studies have made significant strides by enhancing environment perception through fine-grained GUI grounding, there has been limited focus on the planning capabilities of GUI agents. Evidence suggests that decomposing a complex task into a series of plans is effective in eliciting the ability of LLMs within agent systems. Additionally, given that the environment evolves based on action predictions, it is crucial to dynamically adapt plans based on environmental feedback and execution history.\nExisting LLMs-based GUI agents typically takes actions directly prior planning or adjustment of plans based on environmental feedback (e.g., new GUI screenshot) and execution history (e.g., previous steps described in natural language). For instance, as depicted in Figure 1, the static\u00b9 methods (black data stream) directly predicts actions based on the screenshot and goal. Those approaches struggles to handle complex real-world scenarios, where users often adjust subsequent actions based on past steps. We will show that the widely-used ReAct approach fails due to excessively long historical dialogues, revealing its inadequacy in handling complex real-world scenarios (Section 3).\nTo address the challenge above, we propose a novel method called Dynamic Planning of Thoughts (D-PoT) method to enable the LLM-based agent to formulate effective plans based on environment feedback and execution history during task execution (with dashed lines in Figure 1). Concretely, D-PoT dynamically adjusts its plans by incorporating new screenshots and execution history throughout the goal attainment process. Moreover, our proposed method allows for continuous refinement of the current plan, ensuring persistent optimization until the desired goal is achieved. Experimental results demonstrate that our planning mechanism substantially improves the task performance. Additionally, analysis highlights the efficacy of dynamic planning in mitigating hallucinations and adapting to unseen tasks.\nOur key contributions are as follows:\n(i) D-PoT dynamically formulates plans and selects steps for action prediction based on the new screenshots and execution history, thereby advancing the LLMs-based agent.\n(ii) D-PoT achieves a notable improvement in accuracy scores of +12.7% (34.66% \u2192 47.36%) compared with the strong GPT-4V baseline.\n(iii) Analysis highlights the efficacy of dynamic planning in not only enhancing action prediction accuracy but also in in mitigating hallucinations and adapting to previously unfamiliar tasks."}, {"title": "2 Related Work", "content": "Our work is related to LLMs-based GUI agents. This section will first review the recent progress of the work on building the GUI agents and then discuss the planning mechanism of the agents."}, {"title": "2.1 LLMs-based GUI Agent", "content": "LLMs have spurred considerable interest in the realm of language agents. Notable examples include AutoGPT, Hugging-GPT, and MetaGPT , all of which explored the integration of LLMs as the core of agents.This work focuses on ultilizing LLMs as intelligent assistants for smartphones. These assistants are crafted to assist people in accomplishing their daily tasks and meeting life's requirements, especially enhancing accessibility for individuals with disabilities. Notably, the advent of multi-modal LLMs such as GPT-4V, showcasing robust image understanding capabilities , has prompted previous research to predominantly concentrate on comprehending GUI interactions. For instance, MM-Navigator delved into leveraging optical character recognition (OCR) parsing to enhance GPT-4V's GUI comprehension, while AppAgent reinforced the understanding of Application GUI elements by introducing the roles of distinct GUI. In addition to these, CogAgent, Auto-GUI and CoCo-Agent fine-tuned the agent's understanding of GUI to enhance performance. With it comes risk, and these agents have also suffered many attacks.In contrast to the prior research that concentrates on multimodal perception, our work focuses on the planning mechanism to enhance the LLMs proficiency in planning and effectively tackle multi-step tasks on smartphones."}, {"title": "2.2 Planning Mechanisms for LLMs", "content": "LLMs have shown considerable potential in constructing agents with strong capabilities in following instructions and maintaining coherent chains of thought (CoT) via solving complex problems. Notably, the CoT prompting technique has enabled LLMs to engage in effective step-by-step problem-solving process."}, {"title": "3 Investigating the Necessity of Dynamic Planning in GUI Agent", "content": ""}, {"title": "3.1 Challenge of GUI Automation", "content": "GUI automation is a long-episode task where the LLM first receives a goal and an initial screen. To achieve this goal, it must navigate through multiple screens continuously until the task is complete. This presents a challenge for the LLM, requiring it to understand the current progress of the task and the execution history to avoid performing redundant actions in similar environments."}, {"title": "3.2 ReAct Fails Due to the Excessively Long Historical Dialogues", "content": "ReAct is a widely used method in the LLM-based agent. It encourages the LLM to think before taking action when encountering a new environment. Each round of input includes all previous thoughts and actions. We experiment with using ReAct for the GUI Automation task. We sampled 20 tasks from a general dataset and conducted experiments using GPT-4V. The experimental setup is detailed in Section 5.3.\nBased on the Table 1, we observe that accuracy does not always improve with the increased length of input historical dialogues. The best performance is achieved when the history length is 2. This is likely because, in the GUI Automation task, the input length is substantial, with each round containing at least 2000 tokens. Consequently, the performance does not significantly improve with an increase in the length of historical dialogues.\nAdditionally, ReAct can be misled by incorrect decisions made in the last turn. We speculate that the reason is because ReAct focuses more on the most recent interaction rather than the overall task completion progress. For instance, as depicted in Figure 2. In the third turn, a \"type\" action is performed, but it is incorrect. This error persist until the fifth turn when it is finally resolved.\nWhen facing frequent interactions or complex task inputs, reducing inference cost and speeding up inference become critical challenges. The key issue lies in how to effectively input execution history to make dynamic plans and guide LLM-based agents in understanding task progress."}, {"title": "4 Method", "content": "In light of the above experimental results and analysis, we propose Dynamic Planning of Thoughts (D-PoT) to mitigate the challenge. On a high level, D-PoT consists of three stages:\n(1) planning initialization: the LLMs initiate the planning process by generating an overall plan, considering the ultimate goal, current visual input, and prior execution history. The plan helps the LLM grasp the progress of the current task. Once the plan is formulated, the LLMs will select the most plausible step for execution. (2) dynamic planning adjustment: the executed step is appended to the execution history. This updated history list then carefully shapes subsequent planning cycles. In this way, the agent can access all historical information rather than just focusing on the most recent turns. Moreover, these historical details occupy only a small number of tokens, this reduces inference cost, speeds up inference, and improves decision efficiency in subsequent turns. The framework of D-PoT is shown in Figure 3."}, {"title": "4.1 Planning Initialization", "content": "In pursuit of the task goal g, the LLMs engage in k turns of interactions until task completion. Specifically, at each turn i (i = 1,...,k), the LLMs f processes the visual input $x^{(i)}$ (i.e., the current screenshot) and the textual input $x^{(i)}_t$. It then generates the plan $p_i$ and identifies the optimal step $s^{(i)} \\in p^{(i)}$ to execute:\n$(p^{(i)}, s^{(i)}) = f(x^{(i)}_v, x^{(i)}_t)$,                                                                                                                  (1)\nwhere the textual input $x^{(i)}_t$ consists of the task goal g, screen caption $x^{(i)}_c$, and execution history $x^{(i)}_h$. The textual input is further wrapped with prompts (Appendix A.1) before feeding the LLMs along with the visual input. Concretely, we articulate our task goal at the text's outset by prompting \u201cYour ultimate goal is: <g>", "The current on-screen input is: <$x^{(i)}_c$>\". Then, we include execution history, structured as \\\"Here are previous actions: <$x^{(i)}_h$>\".\nAfter feeding the inputs, we request the LLMs to generate a plan $p^{(i)} = [p^{(i)}_1,...,p^{(i)}_n]$, which consists of a sequence of steps to achieve the ultimate goal. Within those steps, the LLMs is also required to identify the optimal step $s^{(i)} \\in p^{(i)}$.\"\n    },\n    {\n      \"title\": \"4.2 Dynamic Planning Adjustment\",\n      \"content\": \"After the execution of $s^{(i)}$, the LLMs becomes anchored in the subsequent interaction turn with an updated visual input $x^{(i+1)}$ (e.g., a new screenshot). Simultaneously, we refine the execution history $x^{(i+1)}_h$ by concatenating $x^{(i)}_h$ and $s^{(i)}$:\n$x^{(i+1)}_h = CONCAT(x^{(i)}_h, s^{(i)})$,                                                                                                    (2)\nwhere CONCAT denotes the concatenation operation between strings.\nConsequently, the execution history is organized with consecutive elements in the format of \\\"step <turn id>: <action>\\\". This updated execution history $x^{(i+1)}_h$ is subsequently employed according to the planning initialization process outlined in Section 4.1 for turn (i + 1) until the task reaches completion. The task is considered complete when i = k or the LLMs predicts the \\\"Status\\\" action type with the \\\"Complete\\\" action description.\"\n    },\n    {\n      \"title\": \"5 Experiments\",\n      \"content\": \"\"\n    },\n    {\n      \"title\": \"5.1 Dataset and Setup\",\n      \"content\": \"We utilize the popular AITW dataset for evaluating our D-PoT. More details about the AITW dataset are in Appendix A.2. We sampled 60 episodes from each subset for analysis to get more convincing results, and incorporated screen caption results into textual input, detecting GUI icons using OCR and IconNet. Each GUI icon is associated with a bounding box and OCR-detected text. In line with prior research , our primary evaluation metric is the screen-wise action matching score, computed as the ratio of correct actions to the episode length. More details are shown in Appendix A.3.\"\n    },\n    {\n      \"title\": \"5.2 Baseline\",\n      \"content\": \"To verify the proposed D-PoT, we used several recent agent methods as our comparison systems:\n\u2022 PaLM-2 ZS : This setting evaluates the zero-shot performance of PaLM-2 by providing a textual description of the screen and prompting it to predict an action from the supported actions in AITW.\n\u2022 ChatGPT 5-shot : ChatGPT's performance is assessed with a 5-shot prompt format similar to PaLM-2. The experiments are conducted using the ChatGPT API.\n\u2022 GPT-4V ZS: Zero-shot prompting with GPT-4V. The model is presented with a screenshot image and a textual description of the screen, tasked with predicting an action from the available actions.\n\u2022 GPT-4V 4FS: Few-shot prompting with 4 examples. The model is presented with a screenshot image and a textual description of the screen, tasked with predicting an action from the available actions.\n\u2022 GPT-4V ReAct: It represents that the interaction method of LLM is ReAct, which includes a history input of 4 turns. The inputs of every turns are screenshots, goals, and screen captions.\n\u2022 GPT-4V Reflexion : It represents that the interaction method of LLM is Reflexion. The model is presented with a screenshot image and a textual description of the screen, tasked with predicting an action from the available actions. When the executed action is incorrect, the action will be re-predicted.\n\u2022 SeeAct : It represents that the interaction method of LLM is SeeAct. We choose the Text Choice method for SeeAct. The model is presented with a screenshot image and a textual description of the screen, tasked with predicting an action from the available actions.\"\n    },\n    {\n      \"title\": \"5.3 Implementation Details\",\n      \"content\": \"We use the GPT-4V interface provided by OpenAI as the backbone of our agent. The GPT-4V model we use is \\\"gpt-4-vision-preview\\\". We set the \u201cmax_tokens\\\" as 300 and the \\\"temperature\\\" as 0. We also fine-tune public large models, i.e., Llama2-7B and LLaVa-7B, to verify the general effectiveness of our approach. For the finetuning experimental setup, training epochs are set as 3, without eval set between epochs. The maximum length of the input sequence is 2560 tokens. Text input includes the goal, screen descriptions in HTML syntax, and execution history. For inputs with a \\\"Plan\\\" experimental group, the step is spliced at the end of the input. The fine-tuning results of these open source LLMs we put in Section 5.8.\"\n    },\n    {\n      \"title\": \"5.4 Main Results\",\n      \"content\": \"Table 3 presents the main results of the test sets for AITW. Based on the results, we have the following findings:\n(i) The proposed D-PoT achieves substantial performance gains on all comparison methods in terms of Overall scores. Particularly, D-PoT exhibits +12.7% (34.66% \u2192 47.36%) improvement on the strong baseline GPT-4V ZS. This presents the effectiveness of our D-PoT, that is, both environmental feedback and action history are beneficial for the GUI task.\n(ii) We observe that our D-PoT gains improvement on the comparison methods (PaLM-2 ZS, ChatGPT 5-shot, Fine-tuned Llama-2, GPT-4V ZS, GPT-4V 4FS and GPT-4V ReAct) in almost all five categories (General, GoogleApps, Install, Single, and WebShopping). This indicates that our D-PoT is generalized to different GUI tasks.\n(iii) We observed that improvement of D-PoT on certain tasks, such as the Install and WebShopping datasets, is not significant. We think that this slight improvement may be attributed to the generated low-quality plans. To verify it, we select 20 episodes from the Install dataset and label them with corresponding plans (e.g., Click, Scroll, Typ, Navigate Home, Navigate Back, Press, and Complete, see Table 2). These human-annotated plans are input into LLMs instead of plans generated by GPT-4V and are prompted to select steps and predict actions.\nTable 4 shows a significant improvement for these 20 episodes, with the Total accuracy scores increasing from_23.57%\u219252.23%. The high-quality plans are beneficial for the GUI task, which means that one of the slight decrease reasons is attributable to low-quality planning generated by GPT-4V, likely failing to stimulate this ability to generate high-quality plans during supervision fine-tuning.\"\n    },\n    {\n      \"title\": \"5.5 Alleviating Planning Hallucinations and Errors\",\n      \"content\": \"To mitigate planning hallucinations and errors, we additionally seek similar task goals during the planning initialization stage as a reference. Initially, we encode the goal of each episode using sentence-transformer and identify the goals of the two most similar episodes from the remaining testsets. We then combine the predicted actions of these two episodes as a reference for the plan. Additionally, we utilize InstructBlip to extract captions from the initial screen of each episode task, indicating starting point of the task. These inputs are incorporated into the prompt for planning initialization, as outlined in the Appendix A.1.\nThe experimental results are shown in Table 3. We observe that when all predicted actions from similar tasks are as a reference, the proposed D-PoT with reference gains the improvement of 0.89% accuracies on the D-PoT in terms of Overall scores. Specifically, on the General and Install datasets, incorporating references result in accuracy improvements of 2.09% and 2.43%, respectively. This indicates that D-PoT is effective at alleviating planning hallucinations and errors.\"\n    },\n    {\n      \"title\": \"5.6 Ablation Study of Varied Planning\",\n      \"content\": \"To study the impact of dynamic planning, we randomly sampled 20 episodes from each data subset, with a total of 100 episodes as the dataset for the ablation experiment, and built several baselines.\n\u2022 No Planning (NP): Its inputs are screenshots, goals, and screen captions. We ask the LLMs to predict actions directly based on these inputs without specifying a plan.\n\u2022 ReAct: It represents that the interaction method of LLM is ReAct, which includes a history input of 4 turns. The inputs of every turns are screenshots, goals, and screen captions.\n\u2022 Static Planning (SP): It represents the utilization of planning statically. We will ask LLMs to generate a plan at the beginning of the episode and add the plan to the prompt during the whole episode.\n\u2022 Dynamic Planning (DP): It represents the utilization of planning, excluding selecting steps and updating execution history. The inputs of DP are screenshots, goals, and screen captions. When receiving a new screenshot, we ask LLMs to generate a plan and then take action.\nTable 5 presents the detailed results of the test set for the AITW dataset. First, the accuracy scores of DP and D-PoT are higher than those of NP, SP and ReAct. This means that dynamic planning is significantly superior to static planning in the graphical user interface automation task. We think that this superiority contributes to two potential or possible factors: 1) This planning greatly stimulates the understanding ability of the LLMs-based agent for the graphical user interface automation task; 2) Throughout task execution, the historical information extracted by steps helps the LLMs-based agent flexibly update its plan for the environment changes and unseen scenarios, especially compared to ReAct, reduces inference cost and greatly improves the performance.\nSecond, the comparison among NP, ReAct, DP, and D-PoT reveals that integrating planning leads to substantial enhancements preceding the predicted action. We think that this effect arises as the generative planning may prompt LLMs to engage in GUI automation, thereby enhancing their comprehension of the intended goal. This demonstrates that the proposed D-PoT obtains notable enhancement via plan integration before action prediction.\nThird, we observe that D-PoT outperformed DP in terms of Overall scores. This indicates that incorporating execution history into LLMs enhances GUI automation through dynamic planning. In other words, historical information is beneficial for LLMs in GUI automation, especially dynamic planning based on historical steps. Moreover, the accuracy scores of D-PoT are inferior to those of DP on the Single datasets. In addition to the generated low-quality plans in Table 4, part of the reasons may be that the short episode length reduces the reliance on historical information for the Single dataset.\"\n    },\n    {\n      \"title\": \"5.7 Exploring the Proportion and Correct Rate of Predicted Actions\",\n      \"content\": \"To conduct a detailed analysis of the impact of dynamic planning, we dive into the correct rate and the proportion of predicted actions. Specifically, we combine five categories for an overarching analysis, more details of the correct rate of predicted actions are in Appendix A.4. Table 6 presents the overall predicted ratio and the predicted accuracy ratio for different actions. Due to the potential occurrence of unpredictable actions in LLMs, it's possible that the sum of predicted probabilities may not equal 1.\nOur observations based on these statistics reveal the following two findings:\n(i) Dynamic planning empowers LLMs to enhance their task management capabilities. Within the DP and D-PoT experimental groups, we observed a noteworthy increase in both the prediction proportion and accuracy rate of \\\"Complete\\\" actions. This suggests that dynamic planning enhances the comprehension of LLMs-based agent in the current task.\n(ii) Dynamic planning reduces the invalid predicted click action. We observed a significant decrease in the prediction ratio for \u201cClick": "ith the introduction of dynamic planning, but the prediction accuracy rate is not affected. Existing work indicates that GPT-4V is more likely to predict the \u201cClick\" action . However, the proposed D-PoT minimizes invalid and erroneous click actions, showcasing a better comprehension of the implementation progress of the current plan."}, {"title": "5.8 Adaptation to Unfamiliar Tasks", "content": "As new applications continually emerge, their interfaces often pose unfamiliarity to agents. Despite the diversity of GUI tasks, there exists a semblance of similarity in screen navigation logic. Even when the interface is unknown, certain screen transition patterns remain consistent. Consequently, the proposed D-PoT utilizes dynamic planning to capture environmental changes and historical steps, which will be beneficial for adaptation to unfamiliar tasks. To validate this, we select two base models, Llama2-7B and LLaVa-7B, for fine-tuning. Llama2-7B serves to verify the effectiveness of the D-PoT method on plain text, while LLaVa-7B serves to verify its effectiveness on multimodal data. We randomly choose the GoogleApps dataset as the training set and the remaining datasets as the test set. The five datasets contain various task categories. We utilize both the D-PoT instruction from our method and the action instruction from AITW for fine-tuning.\nThe results in Table 7 indicate that LLMs fine-tuned with D-PoT data exhibit significant improvements in other tasks and demonstrate robust adaptability to unknown tasks compared to direct fine-tuning with action instructions. Even on the Llama2-7B model, the experimental results of fine-tuning using only a small amount of D-PoT data are comparable to those of fine-tuning using the full AITW dataset. This verifies the effectiveness of D-PoT for out-of-domain tasks.\nAdditionally, in the experiment with LLaVa-7B, we observed that allowing LLaVa-7B to learn dynamic planning rather than following the planned prediction actions formulated by GPT-4V, yielded higher accuracy scores. This indicates that our fine-tuned LLaVa-7B model learned the plan from the GoogleApp dataset and is capable of planning effectively for tasks in other domains. This further supports the notion that D-PoT can adapt LLMs to unfamiliar tasks."}, {"title": "5.9 Error Analysis", "content": "To dive into the mistakes of GPT-4V in dynamic planning and facilitate future studies, we categorize three common errors that lead to discrepancies between the predictions of GPT-4V and human-annotated predictions. More details are presented in Appendix B."}, {"title": "6 Conclusion", "content": "This study introduces a prompting approach called D-PoT, designed to facilitate interactions in a multimodal environment. D-PoT encourages LLMs to dynamically update planning based on feedback from the environment and execution history. Through the application of D-PoT, we demonstrate that the D-PoT surpasses the widely adopted GPT-4V baseline on the AITW benchmark dataset. Meanwhile, our findings indicate that the D-PoT excels in adapting to unfamiliar tasks, and can predict different actions more correctly."}, {"title": "7 Limitation", "content": "This study utilizes the powerful zero-shot capability of LLMs to forecast smartphone actions by incorporating prompt constraints. Our focus lies predominantly on exploring the efficacy of dynamic planning in enhancing action prediction within a given scenario during an episode. In terms of social impact, employing LLM-based agents on mobile phones holds promise for assisting individuals with disabilities. It's worth noting that applying LLMs-based agents on smartphones presents certain constraints. While we find promise in the observed improvement in predicted action accuracy over longer episodes through dynamic planning, practical implementation remains a distant goal. Many challenges stem from the limited knowledge of the mobile phone domain within LLMs, highlighting inherent imperfections. These issues warrant further investigation in future research endeavors."}, {"title": "A Example Appendix", "content": ""}, {"title": "A.1 Dynamic planning prompting", "content": "We use the following prompt for Planning Initialization.\nImagine that you are a robot operating a mobile. Like how humans operate the mobile, you can click on the screen, type some text, go home, go back to the last screen, scroll up, down, left and right, or mark the status as complete. Given a goal and a mobiel screen, you need to make a plan to achieve your goals based on the current screen, and choose the steps that should be achieved on the current screen from the plan you have made. Since achieving this goal is a **continuous process**, you will be given the **previous steps and actions** that have been performed, so please pay attention to this information. There may be multiple ways to achieve your goals, but what you need to do is create the plan that best suits your current situation based on the current screen input.\n**Your ultimate goal is: check out phone information.**\nThe current on-screen input is:\nScreen:\n<p id=0 class='text'' alt=' 'vvaiipaper,'''>vvaiipaper,</p>\n<p id=1 class='text'' alt=''sieep,'''>sieep, </p>\n<p id=2 class='text'' alt=''ioll'''>iolL</p>\n<p id=3 class='text'' alt=''SIZE'''>SIZE</p>\n<p id=4 class='text'' alt=''Sound'''>Sound</p>\n<img id=5 class=ICON\\_VOLUME\\_STATE alt=\u2018\u2018\u2018\u2018>Volume,Volume,</p>\n<p id=7 class='text'' alt=''vibration,'''>vibration,</p>\n<p id=8 class='text'' alt=' 'Do'''>Do</p>\n<p id=9 class='text'' alt=' 'Not'''>Not</p>\n<p id=10 class='text'' alt=''Disturb''>Disturb</p>\n <p id=11 class='text'' alt=''Storage'''>Storage</p>\n<p id=12 class='text'' alt=''used'''>used</p>\n<p id=13 class='text'' alt=''GB free''>GB free</p>\n<p id=14 class='text'' alt=''49\\%'''>49\\%</p>\n<p id=15 class='text'' alt=''-32.63'''>-32.63</p>\n<p id=16 class='text'' alt=''Privacy''>Privacy</p>\n<p id=17 class='text'' alt=''Permissions, ''>Permissions,</p>\n<p id=18 class='text'' alt=''account''>account</p>\n<p id=19 class='text'' alt=''personal' '>personal</p>\n<p id=20 class='text'' alt=''data''>data</p>\n<p id=21 class='text'' alt=''activity,'''>activity,</p>\n<p id=22 class='text'' alt=''Location''>Location</p>\n<img id=23 class=ICON\\_LOCATION alt=''''></p>\n<p id=24 class='text'' alt=''On'''>On</p>\n<p id=25 class='text'' alt='have access''>have access</p>\n<p id=26 class='text'' alt=''- 4 apps'''>- 4 apps</p>\n<p id=27 class='text'' alt=''location''>location</p>\n<p id=28 class='text'' alt=''to'''>to</p>\n<p id=29 class='text'' alt=''Security''>Security</p>\n<p id=30 class='text'' alt=''lock, fingerprint''>lock, fingerprint</p>\n<p id=31 class='text'' alt=''Screen''>Screen</p>\nHere are previous actions: (format: action \\u2192 action description)\nPrevious Actions:\n{'step\\_idx': 0, 'action\\_description': 'scroll up'}\n{'step\\_idx': 1, 'action\\_description': 'click []'}\n{'step\\_idx': 2, 'action\\_description': 'scroll up'}\nAnd the previous steps:\nPrevious Steps:\nStep 1. Swipe up from the bottom of the screen to access the app drawer.\nStep 2. Tap on the 'Settings' icon to open the settings menu.\nStep 3. Scroll up to reveal more settings options.\nPlease formulate an operational guide for future operations for solving the goal. The guide includes:\n1. Plan: A **multi-step future** plan **(start from current screen, DON'T include previous steps)**; steps indexed by numbers.\n2. Step: Based on the current screen and Previous Steps, provide the **immediate** step that needs to be taken from the Plan.\n\"**Output Format:** A JSON dictionary strictly following the format: \"{'plan': '... ', 'step': '... '} \"If the goal has already been implemented, no more planning is required, Provide {'plan': '1. Mark the task as complete', 'step': 'Mark the task as complet'}. **Please do not output any content other than the JSON format.**"}, {"title": "A.2 The statistics for AITW dataset", "content": "AITW is a comprehensive benchmark tailored for GUI control, comprising natural language instructions, screenshots, and associated actions. Agent predicts execution actions based on screenshots and task goals across five categories shown in Table 8. The dataset spans over 350 applications and websites, totaling 715,000 episodes with 30,000 unique instructions. Subsequently, each filtered subset is partitioned episode-wise into training, validation, and test sets following 80/10/10 splits."}, {"title": "A.3 Evaluation metrics", "content": "Specifically, for click actions, correctness is determined if the selected element is within a 14% screen distance from the gold gestures or falls within the same detected bounding box as the user's gestures. Given the error in OCR identification, we select the top left, top right, bottom left, bottom right, and center of the box as sample points for calculating coordinate distances. Regarding scroll actions, correctness is assessed if the selected direction aligns with the scroll direction of the user's gestures. For other actions, correctness is established only if the types of actions match."}, {"title": "A.4 The correct rate of predicted actions in ablation studies", "content": "We compute the proportion of actions within the dataset in Table 9."}, {"title": "B Errors Examples", "content": "The three errors are shown here.\nThe first common error we identify is a bias of GPT-4V on mobile tasks. GPT-4V often exhibits \"preferences\" in its planning. As illustrated in Figure 5(a), when tasked with searching for specific information, GPT-4V tends to click on Google, while the human-annotated prediction suggests clicking on Chrome. Similarly, in Figure 5(b), when required to input text in the search bar, GPT-4V may plan to clear the search bar first, whereas the human prediction is to directly input the text.\nThe second common error we recognize is instruction overlap in the AITW dataset. The same operation on one mobile screen can correspond to two different actions. For instance, in Figure 6(a), when searching for an item, GPT-4V may click on 'search' or the search entry, whereas the human prediction is to press. In Figure 6(b), when returning to the home page, GPT-4V often clicks on the \"home\" button below, while the human instruction is to \u201cnavigate home\u201d. The third common error we classify as confusion in gesture operations. For example, in Figure 7, when swiping down to view more apps, the corresponding gesture should be from bottom to top, indicating \u201cscroll up\". However, GPT-4V also suggests swiping down, but its predicted instruction is \u201cscroll down\u201d."}]}