{"title": "A Survey on Recent Advances in Self-Organizing Maps", "authors": ["Axel Gu\u00e9rin", "Pierre Chauvet", "Fr\u00e9d\u00e9ric Saubion"], "abstract": "Self-organising maps are a powerful tool for cluster analysis in a wide range of data contexts. From the pioneer work of Kohonen, many variants and improvements have been proposed. This review focuses on the last decade, in order to provide an overview of the main evolution of the seminal SOM algorithm as well as of the methodological developments that have been achieved in order to better fit to various application contexts and users' requirements. We also highlight a specific and important application field that is related to commercial use of SOM, which involves specific data management.", "sections": [{"title": "Introduction", "content": "The Self-Organising Map algorithm is a well-known approach for unsupervised learning, designed to distill a high-dimensional dataset into a more manageable, typically two-dimensional, representation. Imagine a dataset full of p measured variables across n observations. A Self-Organising Map elegantly organises similar observations into groups and visually displays them on a map.\nThis model, also known as Kohonen maps or Kohonen networks, has been introduced by Teuvo Kohonen [Koh82, Koh97]. Unlike conventional neural networks, which rely on error correction, SOM training relies on competitive principles. Kohonen drew inspiration from biological paradigms, in particular the neural models [MP69] and Alan Turing's pioneering theories of morphogenesis [Tur52]. Basically, self-organising maps serve as powerful tools for dissecting and visualising complex data landscapes, facilitating a deeper understanding of the intricate structures and relationships that permeate multidimensional datasets.\nSelf-organising maps, like most artificial neural network architectures, operate in two distinct modes: training and mapping. In the training phase, a set of input data (representing the 'input space') is used to generate a reduced dimensional representation of the original data (defining the 'map space'). Subsequently, in the mapping phase, additional input data is classified using the generated map.\nIn most scenarios, the training objective is to project a p-dimensional input space onto a two-dimensional map space. The map space is structured into units called nodes or neurons, which are arranged in a two-dimensional hexagonal or rectangular grid. The number and arrangement of nodes is determined in advance, depending on the overall goals of data analysis and exploration.\nEach node within the map space is associated with a weight vector, which reflects the node's position in the input space. Although the nodes within the map space remain fixed, the training phase involves moving the weight vectors towards the input data (minimizing a distance measure such as Euclidean distance), while preserving the topology that derives from the structure of the map space. Once training is complete, the map can classify additional observations in the input space by identifying the node whose weight vector is closest (by the smallest distance measure) to the vector associated with the observation in question.\nWhile being rather simple, SOMs are used in a wide variety of domains. This broad scope includes visualisation, feature map generation, pattern recognition and classification. In his 1997's book[Koh97], Kohonen identified different applications, including machine vision, image analysis, optical character recognition, script reading, speech analysis, acoustic and musical studies, signal processing, radar measurements, telecommunications, industrial measurements, process control, robotics, chemistry, physics, electronic circuit design, non-imaging medical applications, linguistic and AI problems, mathematical puzzles, and neurophysiological research.\nLet us mention recent applications in economy to identify the determinants of the debt crisis [AC23], in software engineering for classifying Model-View-Controllers [GDP21], in linguistic for sentiment classification [TGI21], in combinatorial optimization for planning patrols of indoor unmanned aerial vehicule by means of TSP solving [Fre20], in medicine for epidemiology study [dSSCC18], in image processing including efficient dedicated implementation [dAdSPD20]. This list highlights the current relevance of SOM methodology across an array of diverse fields of application.\nThe purpose of this survey is to provide an overview of the recent advancements in SOM-based algorithms over the past decade. As previously mentioned, SOMs remain widely used for data analysis across various domains. However, like many traditional machine learning techniques, the SOM algorithm is computationally intensive and needs adaptation for different use cases. Recent research has focused on enhancing the efficiency and applicability of SOM algorithms. These improvements span various stages of the general method, from preprocessing input datasets to fine-tuning the algorithm's components and parameters, and ultimately providing optimal data visualizations for users.\nOur survey aims to highlight representative works that showcase the innovations in this field. We follow a methodological guideline to organize these works"}, {"title": "The SOM Algorithm in Brief", "content": "Let us first recall the basic principle of the SOM algorithm that involves the following key steps:\n1. Initialization: Initialize the weight vectors of the neurons, typically with small random values or by sampling from the input data distribution.\n2. Training: For each input vector, repeat the following steps for a fixed number of iterations or until convergence:\n(a) Best Matching Unit (BMU) Search: Find the neuron with the weight vector that is closest to the input vector (using a distance measure such as Euclidean distance).\n(b) Weight Update: Adjust the weights of the BMU and its neighboring neurons to move closer to the input vector. The degree of adjustment decreases with time.\n3. Convergence: After sufficient iterations, the weight vectors of the neurons stabilize, forming a topologically ordered map that reflects the input data structure.\nThe result of this algorithm is the weight map E containing weights of dimensions p corresponding to the p input variables. Each input element can be associated with the nearest weight. These p variables can be represented on a grid of squares, hexagons or triangles (depending on the topology defined). Each element of the grid is assigned a colour according to its value.\nThe map of weights constitutes a topological space (generally a discrete topology, i.e. any subset of the map E is an open). Different geometric shapes can be used to represent the neural network. The 3 most common shapes produce a pavement with regular shapes, i.e. triangles, squares or hexagons.\nThis topology space has a distance corresponding to the minimum number of polygons to be covered between two points. This distance is denoted d* in Algorithm 1."}, {"title": "Evolution of the SOM Algorithms During the Last Decade", "content": "In this section dedicated to the later evolution of Self-Organizing Maps, we will limit ourselves to examining the publications and research carried out over the last ten years. This period, from 2014 to now, represents a phase that remains dynamic and innovative in the development of SOMs as highlighted by Figure 2.\nComputing\nRessources\nDatasets\nSemi-supervized Datasets\nDisturbed Datasets\nCategorical Data\nParticular Data Distributions\nLarge Datasets\n3.1 Data Management Time and Patterns\nTopology\n3.2 Topology and Metrics Distances\nNeighborhood Relationships\nGeneral Learning Approach\nFeatures Extraction\nTraining Phase\nClustering\nTopology/Metrics\nLearning\n3.3 Learning Techniques\n3.5 Performances\nHyper\nparameters\n3.6 Hyperparametrization Convergence\nParameters\nSOM Algorithm\nVisualization\n3.4 Visualization"}, {"title": "", "content": "\u2022 Data management : SOM can be used to visualize very different types of data (numerical, categorical...). Moreover, the input data set may present difficulties in various real cases (missing values, heterogeneity..).\n\u2022 Topology and metrics SOM algorithms require the selection of a topology to represent the map as well as distances and neighborhood relations for building this map.\n\u2022 Learning techniques : since SOM algorithms belong to machine learning techniques they involve choices concerning the setting of the learning process as well as for the different learning phase, including training and preparation of the data.\n\u2022 Visualization : data visualization is obviously the main goal of the SOM algorithm. Therefore, different techniques and options can be selected to improve the resulting maps\n\u2022 Performances : as many machine learning approaches, the SOM algorithm can be very computationally costly. Proposals have been made to enhance the classification process, including dedicated hardware solutions\n\u2022 Hyperparameters: Hyperparameterisation refers to the process of selecting and adjusting hyperparameters, which are configuration parameters that influence the behaviour and performance of the SOM algorithm to enhance its results."}, {"title": "Data Management", "content": "The input datasets may present difficulties due to missing or incomplete information (e.g., missing labels). Moreover, specific kinds of data may require dedicated solutions (e.g., data described by statistical distributions).\nSemi-supervized Datasets \"A Semi-Supervised Self-Organizing Map for Clustering and Classification\" [BdFB18] presents SS-SOM, an innovative method for managing semi-supervised datasets. Faced with the prevalence of data with few labelled samples, SS-SOM offers a flexible solution, alternating between supervised and unsupervised learning depending on the availability of labels. This approach is distinguished by its ability to excel in conditions where labelled data is limited, while remaining effective for fully labelled sets. This makes SS-SOM particularly useful for clustering and classification scenarios in semi-supervised data contexts.\nIn \"Constrained Semi-Supervised Growing Self-Organizing Map\" (CS2GS) [AYH15], the CS2GS algorithm is introduced as a solution for online semi-supervised clustering. This model focuses on the use of constraints in unlabelled samples, a useful strategy when labelled data is sparse or difficult to obtain. CS2GS modifies the online learning of semi-supervised Self-Organizing Maps and adapts it into a constrained metric learning problem solved by iterative Bregman projections.\nThis approach addresses the challenges posed by massive or streaming datasets, where offline and batch methods are limited. Tests with synthetic and real-world data, including UCI datasets and a bilingual corpus for machine translation, demonstrate the effectiveness of CS2GS in online semi-supervised clustering. The results highlight the benefit of incorporating unlabelled samples to improve the accuracy of the system in realistic data scenarios.\nRemark: Note that CS2GS and SS-SOM focus on improving semi-supervised clustering, but with distinct approaches. CS2GS targets the optimisation of clustering in a context of data flow and constraints on unlabelled data, while SS-SOM focuses on the flexibility of learning in various label availability scenarios, offering versatility for clustering and classification in semi-supervised data contexts.\nDisturbed Datasets \"Semi-automated data classification with feature weighted self organizing map\" (FWSOM) [SA17] introduces the Feature Weighted Self-Organizing Map (FWSOM), a breakthrough in data classification. FWSOM uses topology information from a standard SOM to automatically guide the selection of important entries during training, improving the classification of data with irrelevant entries. This method has demonstrated improved classification accuracy over the standard SOM and other existing classifiers, on both synthetic and real datasets. A notable strength of FWSOM is its ability to identify relevant features, thereby improving the classification performance of other methods. This development represents an important step towards more accurate and automated data classification, particularly in scenarios where filtering out irrelevant features is crucial.\n[dGdSGMdCC23] presents IntraSOM, a new Python library dedicated to the implementation of Self-Organizing Maps. The library is distinguished by its ability to support hexagonal grids and toroidal topology, offering greater flexibility in the modelling and analysis of complex data. A major strength of IntraSOM is its effective handling of missing data during the training process, a frequent problem in real datasets. In addition, IntraSOM offers advanced visualisation tools and efficient clustering algorithms, making it a valuable tool for researchers and practitioners wishing to explore and analyse complex datasets intuitively. The library is designed to be accessible and complete, with an extensible framework that makes it easy to integrate with other Python algorithms and libraries.\nCategorical Data In [HKJC19], the authors address an important issue in the processing of categorical data with Self-Organizing Maps. Initially, SOMs were designed to process numerical data, but this study extends their applicability to categorical data by integrating distance hierarchies that reflect the semantic structure of this data.\nThe integration of learned distance hierarchies with the extended SOM enables better management and visualisation of mixed datasets, including both numerical and categorical elements. Experiments carried out in the study to verify the feasibility and compare the performance of different unsupervised learning methods demonstrate the effectiveness of the approach.\nA significant contribution for processing categorical data, a common challenge in pattern recognition and data mining has been achieved in [dCFD+15]. Traditionally, SOMs are designed to process numerical data, and integrating categorical data often involves converting it into binary codes. However, this transformation can lead to distortion during network training and subsequent data analysis. To overcome this limitation, this paper proposes an innovative SOM architecture that can process categorical values directly without the need for prior conversion into binary codes.\nA key aspect of this new architecture is its ability to effectively mix numeric and categorical data, assigning equal weight to all features. This approach ensures a more faithful and balanced representation of the different types of data within the network. In addition, the proposed architecture is described as scalable, suggesting its ability to handle large and diverse datasets.\n\"Hierarchical SOM (hSOM): Visualizing Self-Organizing Maps for Categorical Data\" [KTCN20] tackles the challenge of analyzing and visualizing multidimensional data, particularly when categorical variables are involved. The authors introduce an innovative method, hSOM, which integrates a histogram into the traditional SOM visualization to better represent categorical data. This integration enhances the understanding of how categorical data is distributed within the SOM, offering a more intuitive and informative analysis. The proposed approach has wide-ranging applications, making SOMs more accessible and useful in fields like biomedical research, marketing, and social behavior analysis where categorical data analysis is vital.\nParticular Data Distributions \"Batch Self-Organizing Maps for Distributional Data with an Automatic Weighting of Variables and Components\" (DBSOM) [DATCIVB22] introduces the DBSOM algorithm adapted to variables described by probability or frequency distributions. DBSOM uses the Wasserstein distance $L_2$, commonly applied in distributional data analysis, as the loss function. A key innovation lies in the introduction of automatically learned relevance weights for each variable with a distributional value, as well as for the components of the Wasserstein distance, which is broken down into means and the size/shape of the distributions.\nThis approach makes it possible to emphasise the importance of the different characteristics of the distributions in the value of the distance. The proposed algorithms have been validated on real datasets with distributional values, demonstrating the effectiveness of the DBSOM method for managing and interpreting complex data in a multi-dimensional context.\nSmoothed SOM (S-SOM) [DGM20] is designed to offer increased robustness to the presence of outliers or atypical data. This development aims to overcome a common limitation of traditional SOMs, which can be sensitive to these outliers, affecting the quality of input density mapping, vector quantization and clustering.\nThe key innovation of the S-SOM lies in the modification of the learning rule. This modification smoothes the representation of outlier input vectors on the map, using a complementary exponential distance between the input vector and its closest codebook. This approach enables the S-SOM to better handle atypical data and produce a map that is more representative of the real structure of the data.\nLarge Datasets [IA18] introduces an innovative method for the rapid visualization of large amounts of data. The Self-Organizing Map is known for its ability to efficiently cluster and visualize data in a lower dimensional space, but its computational complexity of o(n\u00b2) makes it less suitable for large datasets. To overcome this challenge, the authors propose a force-directed visualisation method that mimics the capabilities of SOMs, while significantly reducing complexity to o(n). The approach is based on force-driven fine-tuning of the 2D data representation. To demonstrate the effectiveness and potential of this method as a rapid visualisation tool, it is applied to the 2D projection of the MNIST handwritten digits dataset. This work suggests a promising advance for applications requiring rapid and accurate data synthesis and visualisation, such as trade trends, disaster response and disease outbreaks.\n[AA16] addresses a crucial problem in complex data processing: the management of high-dimensional data, often characteristic of various fields such as bioinformatics, medical imaging, marketing research or social network analysis. The authors highlight the fact that traditional clustering algorithms often see their performance decline as data dimensions increase. To overcome this challenge, they propose a novel multi-view clustering approach based on a self-organising map that is adaptive and scalable over time. This method is distinguished by its ability to efficiently process real data characterised by high sparsity and dimensionality, as well as by diverse representations. Their innovative solution uses a subspace clustering approach, dynamically adapting to different 'views' or perspectives of the data. Experimental results show that their model outperforms other state-of-the-art models specifically designed for multi-view clustering.\nTime and Patterns Innovative schemes for automated and weighted self-organizing time maps (SOTMs) are proposed in [Sar15]. These maps provide a visual method for evolutionary clustering, which consists of producing a sequence of clustering solutions over time, a process called visual dynamic clustering.\nAutomating SOTMs involves data-driven parameterisation, as well as adapting the training to the characteristics of the data at each time step. The objective of weighted SOTMs is to improve learning by giving more weight to data deemed reliable or important. This approach offers a variable weight for each data instance.\nTo illustrate the effectiveness of these schemes, the authors apply automated and weighted SOTMs to two real-world datasets: country risk indicators to measure the evolution of global imbalances, and credit applicant data to assess the evolution of credit risks at the firm level.\nA new approach that incorporates limit cycles as an encoding mechanism to represent input patterns or sequences is described in [HGR15]. This method departs from the traditional static coding representations used in SOMs, where each input pattern is represented by a fixed point activation pattern on the map, a concept that is not consistent with the rhythmic oscillatory activity observed in the brain.\nIn this study, the authors develop and examine an alternative coding scheme that uses sparsely coded boundary cycles to represent external input patterns or sequences. They establish conditions under which limit cycle representations reliably learn and dominate the dynamics in a SOM. These limit cycles tend to be relatively unique for different inputs, robust to perturbations, and fairly insensitive to timing."}, {"title": "Topology and Metrics", "content": "Topology As already mentioned, SOMs are traditionally organized in square or hexagonal grids. Nevertheless, the selected topology has a great impact on the resulting SOM and has to be carefully considered.\nIn [LR14], the authors explore extensions of SOMs considering alternatives derived from the geometric theory of tessellations. The goal is to assess the effectiveness of these new topologies across various application domains such as unsupervised clustering, color image segmentation, and classification. Experimental findings indicate statistically significant differences between the topologies, suggesting that the optimal choice varies depending on the use case. This highlights the importance of customizing grid topology selection for each use case to maximize performance. Furthermore, the authors offer a theoretical interpretation of these results, providing insights into the underlying mechanisms that render certain topologies more efficient in particular contexts. This research sets the stage for further exploration of grid topologies in SOM applications, advocating for customization based on specific task requirements.\nIn [AO15], the authors introduce an improvement to self-organizing maps through the use of dynamic binary search tree (BST) structures. This approach, called TTOCONROT, allows dynamic adaptation and transformation of the SOM topology, thereby optimising data representation. The key innovation lies in the concept of 'Neural Promotion', where neurons adjust according to their increasing importance, more accurately reflecting the stochastic distribution of the data. Experimental results show that this method significantly improves the accuracy and dynamics of SOMs, without requiring the user to have an in-depth understanding of the topological properties of the data.\n\"AMSOM: Adaptive Moving Self-organizing Map for Clustering and Visualization\" [SW16] presents AMSOM that introduces significant flexibility by allowing dynamic adjustments to the position of neurons, as well as the addition and deletion of neurons during training, whereas traditional SOMs have a fixed structure during training. This approach addresses two major limitations of conventional SOMs: the rigidity of their neuron grid and the inability to delete neurons once they have been introduced. By offering a more adaptable structure, AMSOM not only improves training performance, but also the quality of data visualisation. Experiments carried out on various datasets have shown that AMSOM leads to a better representation of the input data and provides a useful framework for determining the optimal number of neurons and their structure, thereby improving the efficiency of clustering and visualisation.\nIn [RD21], the authors present an innovative variant of the self-organizing map algorithm by introducing the random placement of neurons on a two-dimensional surface. This approach, which follows a blue noise distribution, makes it possible to create various topologies with random but controllable discontinuities. This flexibility is particularly advantageous for organising high-dimensional data. The algorithm was tested on one-, two- and three-dimensional tasks, as well as on the MNIST handwritten digits dataset. The results, validated by spectral analysis and topological analysis of the data, show that this randomised self-organising map can effectively reorganise itself in the event of neuronal lesion or neurogenesis, demonstrating a remarkable capacity for adaptation.\nDBGSOM (Directed Batch Growing Self-Organizing Map) [VA17] aims to optimise the quality of topology preservation when growing SOM maps. Unlike the usual incremental learning strategies of GSOMs, DBGSOM introduces a batch learning strategy that guides the growth process based on the accumulated error around the candidate neuron at the boundary. This method allows only one new neuron to be added around each candidate neuron, reducing the risk of map distortion and twisting, common problems in traditional GSOMs due to unexpected network growth and poor initialization of neuron weights.\nDistances The impact of using different distance metrics on the cooperative process of the SOM algorithm is explored in [Wil17]. While the Euclidean distance is generally used in standard implementations of the algorithm, the study reveals that other metrics, such as the Manhattan distance and those from the same family as the Euclidean metric, can also converge and produce comparable results. However, the authors point out that simply being a metric is not enough to guarantee satisfactory results, and they present examples of metrics that do not produce adequate maps despite their analogy with the Euclidean metric. This research sheds light on the need to choose distance metrics wisely in SOMs, taking into account their compatibility with the structure and objectives of the data being processed.\nA similarity measure called correntropy-induced metric (CIM) is explored in [CP15]. This approach aims to improve the magnification factor of the mapping, which is often distorted in standard SOMs due to the use of the mean square error, which tends to oversample regions of low probability. The study shows that adapting the SOM according to the CIM is equivalent to reducing the localised cross-information potential, an information theory function that quantifies the similarity between two probability distributions. By exploiting this property, the authors propose a kernel bandwidth adaptation algorithm for Gaussian kernels. The proposed model achieves a mapping with optimal magnification and automatically adapts the kernel function parameters, offering a significant improvement over traditional SOM approaches, without overcomplicating the algorithm.\nIn [CK22] a generalized framework for non-Euclidean SOMs is proposed. Traditionally, SOMs are based on Euclidean geometry, which limits their ability to model similarity relations in complex data. By adopting a non-Euclidean geometry, the authors open up new perspectives for dimension reduction, clustering and similarity discovery in large datasets. This innovative approach offers a new freedom in the translation of similarities into spatial neighbourhood relations, thus significantly improving the ability of SOMs to model complex and hierarchical data.\nConsidering complex data such as images, graphs, matrices and time series, in [DGMS20] the authors propose a multilinear approach to distances in the input vector space, offering greater flexibility in cluster formation. This methodological choice overcomes the traditional limits of fixed geometric shapes in clustering, opening up the possibility of creating clusters with arbitrary shapes.\nThe core of the study lies in the application of this distance metric to a multimodal functional neuroimaging (fMRI) dataset, taken during three distinct cognitive tasks. The aim was to assess the ability of SOMs to cluster this data in a meaningful way. The results obtained from various SOM configurations are evaluated using confusion matrices, topological error rates, activation set change rates, and intra-cluster distance variations.\nThe idea of a bi-modal scaled metric is introduced in [Wil18]. This innovation allows more accurate segmentation of SOM maps into distinct regions, reflecting the expected cluster structure in the data. This is particularly relevant in the context of somatotopic maps, where clusters in the data may correspond to specific regions of the body surface. The use of this bi-scale metric helps to solve a common problem in SOMs: that of map neurons that are not activated by any training data. Thanks to this approach, the anticipated structure of the data is better preserved, and the SOM maps become more representative of the actual distribution of the data.\nIn addition, the study investigates the plasticity of SOM maps using this metric when they are re-trained following the loss of groups of neurons or following changes in the training data. These simulations are of particular importance for understanding how SOM maps can adapt to changes in the data, for example, in a neurobiological context where the loss of a region of the body surface requires the somatotopic mapping to be readjusted."}, {"title": "Neighborhood Relationships", "content": "In [HM15], a new neighborhood function specifically designed for hardware SOMs is proposed to enhance their vector quantization performance. This function, tailored to hardware constraints and utilizing negative powers of two, was tested through simulation and implemented on a Field-Programmable Gate Array (FPGA). The results demonstrate that this neighborhood function enhances the vector quantization performance of hardware SOMs without increasing hardware costs or slowing down operational speed.\nIn [Ols14], the SOM algorithm dynamically adjusts the neighbourhood width of neurons according to the frequency of occurrence of input patterns in the data space. This approach ensures that each neuron on the SOM grid has its own adapted neighbourhood width, enabling better visualisation of the data, especially when significant differences exist in the frequencies of the input patterns. The experimental study carried out on three real data sets confirms the effectiveness of this adaptive SOM approach, highlighting its potential for improving the visual representation and analysis of data.\nAn innovative neighborhood function enabling continuous learning is introduced in [HIM18]. Unlike the traditional neighborhood function, whose intensity and radius decrease over time, this new function relies solely on the distance between the weight vector of the winning neuron and the input vector. This means that the magnitude and radius of the neighborhood function are adjusted based on this distance, rather than learning iterations. This approach allows the SOM to continue its learning uninterrupted, even in the presence of variable input distributions, which is particularly useful for online learning or in dynamic input environments. The use of vector distance in this method provides the SOM with a voluntary learning capability, akin to curiosity observed in the biological brain, enabling continuous adaptation to new information.\n[Ols21] proposes to consider the scattering of input data to better preserve the main structure of the data. This method starts with a preliminary clustering of the input data to capture their scattering. Then, the intra-cluster variances obtained are used to define the neighborhood widths of the best matching units (BMUs). This approach was empirically evaluated on three diverse real-world datasets, varying in size, dimensionality, and type, representing different experimental domains. The performance of this method was compared to seven other data visualization techniques. The results demonstrated that the proposed method outperforms the other techniques in terms of efficiency, utility, and accuracy. This improved SOM method thus distinguishes itself by its ability to adapt more precisely to the structure of the data, thereby providing enhanced and more informative visualization."}, {"title": "Learning Techniques", "content": "Different learning approaches can optimise the effectiveness and efficiency of SOMs, focusing on aspects such as the rapid identification of optimal matching units, improved learning for weak or distant neurons, and the adaptation of learning techniques to the requirements of various applications.\nGeneral Learning Approach The learning efficiency of the least solicited or most distant neurons from the winning neuron is studied in [CBA15a]. In a standard SOM, these neurons receive less exposure to input data, reducing their learning capacity. To avoid this, the authors propose CSOM, a technique that enhances the learning of these so-called \"weak\" or \"distant\" neurons. This approach aims to strengthen learning within the community of winning neurons, which could lead to better knowledge distribution and more precise mapping of input data. By increasing the efficiency of learning in less active neurons, CSOM could potentially improve the overall performance of SOM in various applications, including classification, data visualization, and clustering.\nIn [BHG20], the authors tackle the challenge of computational efficiency in the learning phase, especially when a large number of neurons is involved. Finding the Best Matching Unit (BMU) is a key process in SOM operation, but it can become increasingly time-consuming as the number of neurons increases. The algorithm proposed in this paper aims to significantly accelerate the search for the BMU while minimizing performance loss. This speed improvement could make SOMs more practical and efficient for applications requiring a large number of neurons, such as complex pattern recognition or voluminous data analysis. The proposed approach thus paves the way for more ambitious uses of SOMS, where the size and complexity of models are no longer major constraints.\nAn approach, called 'input information maximisation' [Kam14], focuses on the input neurons by considering mainly the winning neurons. The method is based on evaluating the uncertainty of the input neurons, defined by the difference between the input neurons and the corresponding winning neurons, and then normalised to obtain a clear measure.\nThe central idea is that increasing the input information leads to reduced activation of the input neurons, with the maximum state characterised by the activation of a single neuron and the deactivation of all the others. This property is particularly useful for reducing quantization and topographical errors in SOMs, thus improving the overall quality of the representations obtained.\nThe authors applied this method to two distinct datasets: Senate voting data and voting attitudes. The experimental results confirmed that increasing the input information leads to a reduction in quantification and topographical errors, while allowing a clearer class structure to be extracted.\n\"Two novel hybrid Self-Organizing Map based emotional learning algorithms\" (EmSOM/Em-SOR-SOM)[DG19] introduces two innovative algorithms, Em-SOM and Em-SOR-SOM, for integrating emotional aspects into machine learning. These algorithms aim to improve the modelling of human reactions by taking account of emotions, an essential but often neglected aspect of decision-making.\nEmSOM corrects the shortcomings of Emotional Backpropagation (EmBP) by determining emotional input values from the corresponding SOM blocks. This approach provides a more accurate and nuanced representation of emotions in the learning process.\nEm-SOR-SOM combines the SOR-SOM (Sparse Online SOM) algorithm with emotional learning to improve pattern recognition. This hybridisation exploits the advantages of SOR-SOM, in particular its ability to manage sparse data efficiently.\nThe performance of EmSOM and Em-SOR-SOM has been validated on facial recognition and credit databases. These algorithms have demonstrated their superiority over EmBP and other recent methods, paving the way for advanced applications in pattern recognition and automated decision-making systems.\nFeatures Extraction \"A Convolutional Deep Self-Organizing Map Feature Extraction for Machine Learning\" [SZ20] introduces a novel approach to feature extraction called Unsupervised Deep Self-Organizing Map (UDSOM). This method merges multi-layer SOM architecture with deep learning principles to tackle challenges associated with high-dimensional data handling and feature extraction in Big Data environments.\nUDSOM's key innovation lies in its data processing workflow, which involves segmenting data into sub-regions, applying self-organizing layers, and rectification functions (RELU). Each SOM layer focuses on modeling a local sub-region, and the most active neurons from each SOM are grouped in a sampling layer to generate a new 2D map. Concurrently, data abstraction occurs through a convolution-pooling module and ReLU function application. This architecture facilitates the collection and integration of local information to construct a global representation in upper layers, crucial for effectively handling Big Data. Experimental results demonstrate the effectiveness of UDSOM in various machine learning tasks, highlighting its potential for feature extraction in the context of large-scale datasets.\nAn innovative approach [KRM20] focuses on exploiting extracted features. This method is particularly relevant in the context of embedded applications, where the efficiency and accuracy of unsupervised learning are paramount.\nThe core of this study lies in the comparison of two feature extraction methods: one based on machine learning with Parsimonious Convolutional Autoencoders and the other inspired by neuroscience with Pulse Neural Networks using pulse timing-dependent learning. This contrast between a traditional machine learning approach and a more experimental neuroscience approach offers a fascinating insight into how data features can be effectively extracted and used to improve SOM classification.\nTraining Phase \"VSOM: Efficient, Stochastic Self-Organizing Map Training\" [Ham18] presents VSOM, an efficient implementation for stochastic training of Self-Organizing Maps. VSOM improves on Kohonen's standard stochastic algorithm by replacing iterative constructions with vector and matrix operations. This innovative approach delivers significant performance gains over Kohonen's iterative algorithm and over batchSOM, the fastest implementation of SOM without the need for multi-processing.\nThe quality of the maps produced by VSOM is comparable to that obtained with the original iterative algorithm and surpasses that of batchSOM. Suitable for single-threaded operation, VSOM lends itself particularly well as a replacement for iterative stochastic SOM training, especially in environments such as R that do not handle multi-threading efficiently.\nIn [TH18], the authors introduce a new quality index called the convergence index. This index combines map fitting accuracy and estimated topographic accuracy, thus providing a single, statistically significant number that proves to be more intuitive to use than other quality measures. This research focuses on how the convergence index captures SOMs learning the multivariate distribution of a training dataset. Special attention is paid to the convergence of the marginals and the influence of different parameters governing SOM learning. Surprisingly, it is found that the constant neighborhood function outperforms the popular Gaussian neighborhood function in producing more effective SOM models. This result highlights the importance of choosing an appropriate neighborhood function to optimize the learning and convergence of self-organizing maps.\nClustering In [MSL17], the authors address the concept of collaborative clustering, a technique aimed at revealing the common structures of data spread over different sites. In today's environment, where the amount of data available is constantly increasing, incremental clustering methods are essential. The algorithm presented in this article enables collaborative clustering to be carried out using Self-Organizing Maps in an incremental manner, without requiring any topological modifications to the map. This approach is particularly useful for efficiently managing large volumes of data that are constantly changing.\nExperiments carried out on several datasets validate the proposed method. The article also highlights the influence of batch size on the learning process, a crucial aspect for optimising clustering performance in dynamic and collaborative environments. This work thus represents a significant contribution both to the field of incremental clustering and to that of SOMs applied to collaborative clustering.\n[PCdM17] explores an innovative approach in data clustering using SOMs. This research focuses on improving the generalisation and performance of SOMS using an ensemble method.\nData clustering is an analytical method for identifying groups within data based on similarities. However, clustering results can vary depending on many factors such as algorithm parameters, initialization, stopping criteria, or the use of different attributes or data subsets. SOMs are frequently used for data analysis, particularly for visualisation and clustering.\nThe approach proposed in this article is to use a set of SOM networks working independently, combined by a system that integrates the individual results into a single output. This method aims to achieve better generalisation than using a single neural network. The key concept of this method is the use of cluster validity indices to combine the weights of neurons from maps of different sizes.\n[PCdM15] proposes an innovative method for improving classification accuracy by fusing SOMs of different sizes. This approach, based on the neural network ensemble, aims to obtain a better generalisation of the model. Through factorial experiments and computer simulations using datasets from the UCI Machine Learning Repository and the Fundamental Clustering Problems Suite, the authors demonstrate a significant increase in classification accuracy. The Wilcoxon Signed Rank test validates the feasibility of this method, highlighting its potential for improving SOM-based classification techniques.\nIn \"Improving self organizing maps method for data clustering and classification\" [ZYM15], iSOM bis is presented as an improvement of Self-Organizing Maps to solve visualization and classification difficulties. This model improves the distinction between clusters and sub-clusters by adding a third dimension based on computational distance to the winning neurons. Tested on the Iris Flowers dataset, iSOM showed improved class separation and increased classification accuracy compared with traditional SOMs, offering a more efficient method for analysing complex data."}, {"title": "Visualization", "content": "\"A novel data-driven visualization of n-dimensional feasible region using interpretable self-organizing maps (iSOM)\" [NPR22", "B-matrix\", a graphical representation that visualises both the feasible range of design variables and the objective function.\nB-matrix effectively reduces the complexity inherent in multi-dimensional search spaces, enabling more intuitive data analysis. The flexibility and effectiveness of this method is demonstrated through various analytical and engineering examples, covering dimensions from 2 to 30. This approach represents a significant advance for designers and engineers, offering a new way of approaching and solving complex optimisation problems in many fields.\nIn [PL22": "a modification of the SOM algorithm incorporates relevance scores into the 2D data visualization process. Traditionally, SOMs are effective at preserving the topological relationships of multidimensional data, but they do not take into account the relevance of the results, a crucial aspect in multimedia information retrieval systems. The proposed modification aims to simultaneously optimise the preservation of topological order and result relevance. This approach not only improves data exploration capability, but also enables the"}]}