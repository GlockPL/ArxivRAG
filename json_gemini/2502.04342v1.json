{"title": "Tutorial on Using Machine Learning and Deep Learning Models for Mental Illness Detection", "authors": ["Yeyubei Zhang", "Zhongyan Wang", "Zhanyi Ding", "Yexin Tian", "Jianglai Dai", "Xiaorui Shen", "Yunchong Liu", "Yuchen Cao"], "abstract": "Social media has become an important source for understanding mental health, providing researchers a way to detect conditions like depression from user-generated posts. This tutorial provides practical guidance to address common challenges in applying machine learning and deep learning methods for mental health detection on these platforms. It focuses on strategies for working with diverse datasets, improving text preprocessing, and addressing issues such as imbalanced data and model evaluation. Real-world examples and step-by-step instructions demonstrate how to apply these techniques effectively, with an emphasis on transparency, reproducibility, and ethical considerations. By sharing these approaches, this tutorial aims to help researchers build more reliable and widely applicable models for mental health research, contributing to better tools for early detection and intervention.", "sections": [{"title": "Introduction", "content": "Mental health disorders, especially depression, have become a significant concern worldwide, affecting millions of individuals across diverse populations (Organization, 2020). Early detection of depression is crucial, as it can lead to timely treatment and better long-term outcomes. In today's digital age, social media platforms such as X(Twitter), Facebook, and Reddit provide a unique opportunity to study mental health. People often share their thoughts and emotions on these platforms, making them a valuable source for understanding mental health patterns (De Choudhury et al., 2013; Guntuku et al., 2017).\nRecent advances in computational methods, particularly machine learning (ML) and deep learning (DL), have shown promise in analyzing social media data to detect signs of depression. These techniques can uncover patterns in language use, emotions, and behaviors that may indicate mental health challenges (Shatte et al., 2020; Yazdavar et al., 2020).\nHowever, applying these methods effectively is not without challenges. A recent systematic review (Cao et al., 2024) highlighted issues such as a lack of diverse datasets, inconsistent data preparation, and inadequate evaluation metrics for imbalanced data (Hargittai, 2015; Helmy et al., 2024)\u2014problems that have also led to inaccuracies in other domains (e.g., (Gao et al., 2024; Tan et al., 2025; Weng et al., 2024)). Similarly, Liu et al. (Liu et al., 2024) identified additional linguistic challenges in ML approaches for detecting deceptive activities on social networks, including biases from insufficient linguistic preprocessing and inconsistent hyperparameter tuning, all of which are pertinent to mental health detection. Moreover, complementary insights from related fields underscore the need for continuous improvements in robust model development (Bi et al., 2024; Tao, 2023; Xu et al., 2025; Zhao & Lai, 2024).\nThis tutorial is designed to address these gaps by guiding readers through the steps necessary to create reliable and accurate models for depression detection using"}, {"title": "Method", "content": "This section provides a comprehensive overview of the methodological framework employed in this study, detailing the processes for data preparation, model development, and evaluation metrics. All analyses and model implementations were conducted using Python 3, leveraging popular libraries such as pandas for data manipulation, scikit-learn for machine learning, PyTorch for deep learning, and Transformers for pre-trained language models. These tools enabled efficient preprocessing, hyperparameter optimization, and performance evaluation. The following subsections elaborate on the key steps and methodologies involved in the study."}, {"title": "Data Preparation", "content": "A sufficiently representative dataset is essential for machine-learning-based mental health detection. This study utilized the Sentiment Analysis for Mental Health dataset, available on Kaggle. The dataset integrates textual content from multiple repositories focused on mental health topics, including depression, anxiety, stress, bipolar disorder, personality disorders, and suicidal ideation. The primary sources of these data are social media platforms such as Reddit, Twitter, and Facebook, where individuals frequently discuss personal experiences, emotional states, and mental health concerns.\nThe dataset was originally compiled using platform-specific APIs (e.g., Reddit, Twitter, and Facebook) and web scraping tools, allowing for the collection of substantial volumes of publicly available text data. After the acquisition, duplicates were removed, irrelevant and spam content was filtered, and mental health labels were standardized to ensure consistency across repositories. Personal identifiers were removed to safeguard privacy and ensure compliance with ethical guidelines for data usage. The final dataset was consolidated into a structured CSV file with unique identifiers for each entry.\nAlthough the dataset combines data from multiple platforms to provide a diverse corpus, it is not free from limitations. Differences in platform demographics, such as age, cultural background, and communication styles, may affect the generalizability of models trained on this data. Additionally, linguistic variability, including colloquialisms, slang, and code-switching, reflects the informal nature of social media communication. While this diversity enriches the dataset, it also presents challenges for natural language processing (NLP) techniques, particularly in tokenization and embedding generation. To address these complexities, the preprocessing pipeline was designed to handle diverse linguistic patterns and balance class distributions where needed."}, {"title": "Data Preprocessing", "content": "A standardized preprocessing pipeline was applied to prepare the dataset for training both machine learning (ML) and deep learning (DL) models. These steps ensured consistency in data handling while accommodating the unique requirements of each modeling approach:\n\u2022\tText Cleaning: Social media text often contains noise such as URLs, HTML tags, mentions, hashtags, special characters, and extra whitespace. These elements were systematically removed using regular expressions to create cleaner input for both ML and DL models.\n\u2022\tLowercasing: All text was converted to lowercase to maintain uniformity across the dataset and minimize redundancy in text representation.\n\u2022\tStopword Removal: Commonly used words that provide little semantic value (e.g., \"the,\" \"and,\" \"is\") were excluded using the stopword list available in the"}, {"title": "Feature Transformation for ML Models", "content": "For ML models, an additional step, TF-IDF Vectorization, was necessary to transform the text into structured features. The cleaned text was converted into numerical representations using Term Frequency-Inverse Document Frequency (TF-IDF), which captured term frequencies while down-weighting overly frequent words. The vectorizer was configured to extract up to 1,000 features and account for both unigrams and bigrams (n-gram range: 1-2)."}, {"title": "Model Development", "content": "This study employed a range of machine learning (ML) and deep learning (DL) models to analyze and classify mental health statuses based on textual data. Each model was selected to explore specific aspects of the data, from linear interpretability to handling complex patterns and long-range dependencies. Detailed implementation code for all models, including hyperparameter tuning and evaluation, is available on GitHub. Below, we provide an overview of each model, its methodology, and its performance in the context of binary and multi-class mental health classification tasks."}, {"title": "Logistic Regression", "content": "Logistic regression is one of the most widely used methods for classification tasks and has long been employed in social science and biomedical research (Hosmer & Lemeshow, 2000). In the context of mental health detection, it provides a straightforward yet interpretable modeling framework, translating linear combinations of predictors (e.g., term frequencies, sentiment scores, and linguistic features) into estimated probabilities of class membership through the logit function.\nThe logistic regression model predicts the probability of a binary outcome using the following expression:\n$\\hat{y} = \\frac{1}{1 + exp(-\\overline{w}x - b)},$\nwhere $\\hat{y}$ represents the predicted probability, w is the vector of model coefficients, x is the feature vector, and b is the bias term. For multi-class classification, the model generalizes to predict probabilities for K classes using the softmax function:\n$P(y = k | x) = \\frac{exp(\\overline{w}_{k}x + b_{k})}{\\sum_{j=1}^{K} exp(\\overline{w}_{j} x + b_{j})},$\nwhere k \u2208 {1, . . ., K} represents the class index.\nBoth binary and multi-class logistic regression models were optimized using cross-entropy loss during training and configured to converge with a maximum iteration limit of 1,000. Regularization was applied to prevent overfitting, using 12 (ridge) regularization,"}, {"title": "Support Vector Machine (SVM)", "content": "Support Vector Machines (SVMs) are supervised learning models that are widely used for both classification and regression tasks. Originally introduced by Cortes and Vapnik (1995), SVMs aim to find the optimal hyperplane that maximizes the margin between data points of different classes. The margin is defined as the distance between the closest data points (support vectors) from each class to the hyperplane. By maximizing\nFor a linearly separable dataset, the decision boundary is defined as:\n$f(x) = \\overline{w}^{T}x + b,$\nwhere w is the weight vector, x is the feature vector, and b is the bias term. The optimal hyperplane is determined by solving the following optimization problem:\n$\\min_{w,b} \\frac{1}{2} ||w||^{2},$\nsubject to $y_{i}(\\overline{w}^{T}x_{i} + b) \\geq 1, i = 1, . . ., N,$\nwhere y \u2208 {\u22121,+1} are the class labels.\nFor datasets that are not linearly separable, the optimization problem is modified to include a penalty for misclassifications:\n$\\min_{w,b,\\xi} \\frac{1}{2} ||w||^{2} + C \\sum_{i=1}^{N} \\xi_{i},$\nsubject to $y_{i}(\\overline{w}^{T}x_{i} + b) \\geq 1 - \\xi_{i}, \\xi_{i} \\geq 0, i = 1, . . ., N,$\nwhere $ \\xi_{i}$ are slack variables that allow for misclassifications, and C > 0 is the regularization parameter that controls the trade-off between maximizing the margin and minimizing classification errors.\nKernel methods enable SVMs to handle nonlinearly separable data by mapping the input features into a higher-dimensional space where linear separation becomes possible. This mapping is performed implicitly using a kernel function K(x\u2081, xj), which computes the inner product in the transformed space:\n$K(x_{i}, x_{j}) = \\phi(x_{i})^{T}\\phi(x_{j}),$\nwhere \u03c6(.) represents the mapping function.\nSeveral commonly used kernel functions are available, each suited for different data characteristics:\n1.\tLinear Kernel:\n$K(x_{i}, x_{j}) = \\overline{x}_{i}^{T}\\overline{x}_{j}$"}, {"title": "Tree-Based Models", "content": "Classification and Regression Trees (CARTs) are versatile tools used for analyzing categorical outcomes (classification tasks). The CART algorithm constructs a binary decision tree by recursively partitioning the data based on covariates, optimizing a predefined splitting criterion. For classification tasks, the quality of a split is typically evaluated using impurity measures such as Gini impurity or entropy (Bishop, 2006). The Gini impurity for a node is defined as:\n$G = \\sum_{i=1}^{C} p_{i}(1 \u2212 p_{i}),$\nwhere pi is the proportion of observations in class i at the given node, and C is the total number of classes.\nAlternatively, entropy can be used to measure impurity:\n$H = - \\sum_{i=1}^{C}p_{i} log(p_{i}),$\nwhere pi represents the same class proportions as in the Gini impurity formula. Lower impurity values indicate greater homogeneity within a node.\nAt each step, the algorithm selects the split that minimizes the weighted impurity"}, {"title": "Random Forests", "content": "Random Forests. Random Forests are ensemble learning methods that aggregate multiple decision trees parallelly to enhance classification performance. By building trees on bootstrap samples of the data and introducing random feature selection at each split, Random Forests reduce overfitting and improve generalization. Each tree is trained on a random bootstrap sample, where data points are sampled with replacement from the original dataset, meaning some observations may appear multiple times in the training sample, while others are excluded. Additionally, Random Forests introduce randomness during the tree-building process by selecting a random subset of covariates at each split instead of considering all available covariates. This randomization decorrelates the trees, reduces variance, and enhances the model's robustness. For classification tasks, the final prediction is determined by majority voting across all trees (Breiman, 2001).\nTo further mitigate overfitting, each tree in the Random Forest is grown to its full depth without pruning, fitting the bootstrap sample as accurately as possible. Hyperparameters such as the number of trees (n_estimators), the maximum depth of each tree"}, {"title": "Light Gradient Boosting Machine (LightGBM)", "content": "Light Gradient Boosting Machine (LightGBM). Light Gradient Boosting Machine (LightGBM) is a gradient-boosting framework optimized for efficiency and scalability, particularly in handling large datasets and high-dimensional data. Gradient Boosting Machines (GBM) work by sequentially building decision trees, where each new tree corrects the errors made by the previous ones, leading to highly accurate predictions. However, traditional GBM frameworks can be computationally intensive, especially for large datasets (Friedman, 2001). Unlike traditional Gradient Boosting Machines (GBMs), LightGBM employs a leaf-wise tree growth strategy, which enables deeper splits in dense data regions, enhancing performance by focusing complexity where it is most needed. Additional optimizations, such as histogram-based feature binning, reduce memory usage and accelerate training. These enhancements make LightGBM faster and more resource-efficient than standard GBM implementations, without compromising predictive accuracy (Ke et al., 2017).\nKey hyperparameters tuned for LightGBM included the number of boosting iterations (n_estimators), learning rate, maximum tree depth (max_depth), number of leaves (num_leaves), and minimum child samples (min_child_samples). To address the class imbalance, the class_weight parameter was tested with both \u2018balanced' and None options. Grid search was employed to explore all possible combinations of these hyperparameters, and the weighted F1 score was used as the primary metric for selecting the best configuration.\nLightGBM was applied to both binary and multi-class mental health classification tasks. For binary classification, the model differentiated between Normal and Abnormal statuses. For multi-class classification, it predicted categories such as Normal, Depression, Anxiety, and Personality Disorder using the multiclass objective. Hyperparameter tuning via grid search ensured balanced performance across all categories, guided by the weighted F1 score.\nThe best-performing models demonstrated robust predictive power, evaluated using precision, recall, F1 scores, confusion matrices, and one-vs-rest ROC curves. Additionally, LightGBM's feature importance metrics provided interpretability by highlighting the"}, {"title": "A Lite version of Bidirectional Encoder Representations from Transformers (ALBERT)", "content": "A Lite version of Bidirectional Encoder Representations from Transformers (BERT), known as ALBERT (Lan et al., 2020), is a transformer-based model designed to improve efficiency while maintaining performance. While BERT (Devlin et al., 2019) is highly effective for a wide range of natural language processing (NLP) tasks, it is computationally expensive and memory-intensive due to its large number of parameters. ALBERT addresses these limitations by introducing parameter-sharing across layers and factorized embedding parameterization, which significantly reduces the number of parameters without sacrificing model capacity. Additionally, ALBERT employs Sentence Order Prediction (SOP) as an auxiliary task to enhance pretraining, improving its ability to capture sentence-level coherence. These optimizations make ALBERT a lightweight yet powerful alternative to BERT, capable of achieving competitive performance with reduced memory and computational requirements, making it particularly suitable for large-scale text classification tasks like mental health detection.\nIn this project, ALBERT was employed for both binary and multi-class classification tasks. For binary classification, the model was fine-tuned to differentiate between Normal and Abnormal mental health statuses, while for multi-class classification, it was configured to predict multiple categories, including Normal, Depression, Anxiety, and Personality Disorder. The implementation leveraged the pre-trained Albert-base-v2 model, with random hyperparameter tuning conducted over 10 iterations to optimize the learning rate, number of epochs, and dropout rates. The weighted F1 score served as the primary evaluation metric throughout the tuning process."}, {"title": "Gated Recurrent Units (GRUs)", "content": "Gated Recurrent Units (GRUs) are a type of recurrent neural network (RNN) designed to capture sequential dependencies in data, making them particularly effective for natural language processing (NLP) tasks such as text classification (Cho et al., 2014). Compared to Long Short-Term Memory networks (LSTMs), GRUs are computationally more efficient due to their simplified architecture, which combines the forget and input gates into a single update gate. This efficiency allows GRUs to model long-range dependencies while reducing the number of trainable parameters.\nIn this study, GRUs were employed for both binary and multi-class mental health classification tasks. For binary classification, the model was configured to differentiate between Normal and Abnormal mental health statuses. For multi-class classification, it was adapted to predict categories such as Normal, Depression, Anxiety, and Personality Disorder."}, {"title": "Evaluation Metrics", "content": "When modeling mental health statuses-particularly for conditions like depression or suicidal ideation\u2014class distributions are often skewed. In many real-world scenarios, the \u201cpositive\u201d class (e.g., individuals experiencing depression) is underrepresented compared to the \"negative\u201d class (e.g., no mental health issue). This imbalance renders"}, {"title": "Precision", "content": "Precision measures the proportion of positive predictions that are truly positive:\n$Precision = \\frac{True Positives}{True Positives + False Positives}$ (1)\nFor instance, in depression detection, high precision indicates that most users flagged as \"depressed\u201d indeed exhibit depressive content. While precision minimizes false alarms, focusing on it exclusively can be risky. A model that generates very few positive predictions may achieve artificially high precision while missing many genuinely positive cases."}, {"title": "Recall (Sensitivity)", "content": "Recall captures the proportion of actual positives correctly identified:\n$Recall = \\frac{True Positives}{True Positives + False Negatives}$\nIn depression detection, recall is critical because failing to recognize at-risk individuals (false negatives) can have serious consequences. A model with low recall risks overlooking individuals who need intervention."}, {"title": "F1 Score", "content": "The F1 score serves as the harmonic mean of precision and recall, providing a balance between these two metrics (Powers, 2011):\n$F1 = 2*\\frac{Precision * Recall}{Precision + Recall}$\nThe F1 score is particularly useful in imbalanced classification scenarios because it penalizes extreme trade-offs, such as very high precision coupled with very low recall. In mental health detection, achieving a high F1 score ensures the model can effectively identify positive cases while maintaining a reasonable level of precision in its predictions."}, {"title": "Area Under the Receiver Operating Characteristic Curve (AUROC)", "content": "AUROC provides an aggregate measure of performance across all possible classification thresholds. It evaluates the model's ability to discriminate between positive and negative classes. However, in the presence of severe class imbalance, AUROC may not fully reflect the challenges posed by a majority class dominating the dataset. Nevertheless, it remains valuable for assessing model performance across varying decision thresholds (Davis & Goadrich, 2006)."}, {"title": "Results", "content": "This section presents the findings from the analysis of the dataset and the evaluation of machine learning and deep learning models for mental health classification. First, we provide an Overview of Mental Health Distribution, highlighting the inherent class imbalances within the dataset and their implications for model development. Next, the Hyperparameter Optimization subsection details the parameter tuning process, which ensures that each model performs at its best configuration for both binary and multi-class classification tasks. Finally, the Model Performance Evaluation subsection compares the models' performance based on key metrics, including F1 scores and Area Under the Receiver Operating Characteristic Curve (AUROC). Additionally, nuanced observations, such as the challenges associated with underrepresented classes, are discussed to provide deeper insights into the modeling outcomes."}, {"title": "Overview of Mental Health Distribution", "content": "Before hyperparameter optimization and model evaluation, an analysis of the dataset's class distributions was conducted to highlight potential challenges in classification. The dataset, sourced from Kaggle, contains a total of 51,074 unique statements categorized into three primary groups: Normal (31%), Depression (29%), and Other (40%). The Other category encompasses a range of mental health statuses such as Anxiety, Stress, and Personality Disorder, among others.\nFigure 1 illustrates the expanded distribution of mental health statuses across seven detailed categories in the multi-class classification setup. The dataset shows a significant imbalance, with categories such as Normal, Depression, and Suicidal dominating\nFor the binary classification task, the dataset is divided into two classes: Normal and Abnormal. The distribution, shown in Figure 2, reveals that the Abnormal class (labeled as 1) accounts for approximately twice the number of records as the Normal class (labeled as 0). Although the imbalance is less severe compared to the multi-class scenario, it still necessitates strategies to ensure that the minority class (Normal) is adequately captured during model training."}, {"title": "Hyperparameter Optimization", "content": "Hyperparameter optimization is a critical step in enhancing the performance of machine learning (ML) and deep learning (DL) models. For this study, a grid search or random search approach was employed to systematically explore a predefined range of hyperparameters for each model. The primary evaluation metric used to select the best-performing hyperparameter configuration was the weighted F1 score, as it effectively balances precision and recall, particularly in the presence of imbalanced class distributions. This approach ensures that the selected models perform robustly across both binary and multi-class mental health classification tasks.\nThe optimized hyperparameters for each model, alongside their corresponding weighted F1 scores on the test set, are summarized in Table 1. These results highlight the configurations that achieved the best trade-off between underfitting and overfitting, providing insight into the hyperparameter values critical to the classification tasks."}, {"title": "Model Performance Evaluation", "content": "The evaluation metrics, including F1 scores (Table 2) and Area Under the Receiver Operating Characteristic Curve (AUROC) (Table 3), reveal minimal numeric differences across the models for both binary and multi-class classification tasks. This consistency in performance can be attributed to two primary factors. First, each model underwent rigorous hyperparameter tuning, ensuring only the best configurations were used for evaluation. Second, the dataset size, being of medium volume, provided sufficient information for machine learning models to achieve strong performance, while deep learning models could not fully demonstrate their potential advantages due to the limited data scale.\nIn the binary classification task, all models exhibited competitive F1 scores and AUROC values, effectively balancing precision and recall while distinguishing between normal and abnormal mental health statuses. Deep learning models such as ALBERT and GRU demonstrated slightly superior performance, achieving AUROC values of 0.95 and 0.94, respectively, which highlights their ability to capture complex linguistic patterns. Machine learning models, including Logistic Regression and LightGBM, also performed well, with AUROC scores of 0.93, underscoring their robustness in simpler classification settings.\nIn the multi-class classification task, a slight decline in performance was observed compared to the binary task. This decline aligns with the increased complexity of distinguishing between seven mental health categories. Nevertheless, deep learning models retained their advantage, with GRU and LightGBM achieving the highest micro-average AUROC scores of 0.97, followed closely by ALBERT with an AUROC of 0.95. Machine learning models such as Logistic Regression and Random Forest also performed commendably, with AUROC scores of 0.96, demonstrating their ability to handle multi-class tasks effectively when optimized.\nAnother important observation in the multi-class classification task is the consistently lower AUROC scores for Depression (Class 2) across all machine learning models, with values not exceeding 0.90. While deep learning models demonstrated a slight improvement, their performance for this class remained comparatively weaker than for other categories. This difficulty likely arises from the significant overlap between Depression (Class 2) and other categories in both linguistic and contextual features. The reduced AUROC scores highlight the models' challenges in effectively distinguishing Depression, resulting in higher misclassification rates. These findings emphasize the need for refined feature engineering techniques or more sophisticated model architectures to enhance the separability and accurate classification of this particular class.\nThe minimal differences in performance metrics across models suggest that the combined effects of comprehensive hyperparameter optimization and dataset size contributed significantly to these results. Binary classification consistently outperformed multi-class classification, likely due to its reduced complexity and fewer decision boundaries. While deep learning models demonstrated their ability to capture intricate patterns, machine learning models offered competitive performance, making them practical alternatives for medium-sized datasets.\nPerformance metrics for F1 scores and AUROC values are detailed in Table 2 and Table 3, respectively. This analysis highlights the importance of balancing model complexity with dataset characteristics and emphasizes the critical role of hyperparameter tuning in achieving optimal results."}, {"title": "Discussion", "content": "This tutorial serves as a practical resource to address key methodological and analytical challenges in mental health detection on social media, as identified in the systematic review (Cao et al., 2024). By focusing on best practices and reproducible methods, the tutorial aims to advance research quality and promote equitable outcomes in this important field.\nA critical issue identified in the review is the narrow scope of datasets, which are often limited to specific social media platforms, languages, or geographic regions. This lack of diversity restricts the generalizability of findings. In this tutorial, strategies for expanding data diversity are explored, including integrating datasets across multiple platforms, collecting data from underrepresented regions, and analyzing multilingual content. These efforts aim to make research outcomes more inclusive and applicable to diverse populations.\nText preprocessing emerged as another key challenge, particularly in handling linguistic complexities such as negations and sarcasm. These nuances are critical for accurately interpreting mental health expressions. This tutorial offers practical guidelines for building preprocessing pipelines that address these complexities. Techniques for advanced tokenization, feature extraction, and managing contextual meanings are discussed to enhance the reliability of text-based analyses.\nResearch practices related to model optimization and evaluation were also found to be inconsistent in many studies. Hyperparameter tuning and robust data partitioning are essential for reliable outcomes, yet they are often inadequately implemented. This tutorial provides step-by-step instructions for optimizing models and ensuring fair evaluations, emphasizing the importance of strategies like cross-validation and train-validation-test splits. By following these practices, researchers can reduce bias and improve the validity of their results.\nEvaluation metrics were another area of concern, with many studies relying on accuracy despite its limitations in imbalanced datasets. This tutorial highlights the importance of metrics such as precision, recall, F1-score, and AUROC, which provide a"}]}