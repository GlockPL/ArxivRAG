{"title": "Variational Learning Induces Adaptive Label Smoothing", "authors": ["Sin-Han Yang", "Zhedong Liu", "Gian Maria Marconi", "Mohammad Emtiyaz Khan"], "abstract": "We show that variational learning naturally induces an adaptive label smoothing where label noise is specialized for each example. Such label-smoothing is useful to handle examples with labeling errors and distribution shifts, but designing a good adaptivity strategy is not always easy. We propose to skip this step and simply use the natural adaptivity induced during the optimization of a variational objective. We show empirical results where a variational algorithm called IVON outperforms traditional label smoothing and yields adaptivity strategies similar to those of an existing approach. By connecting Bayesian methods to label smoothing, our work provides a new way to handle overconfident predictions.", "sections": [{"title": "1. Introduction", "content": "Adaptive strategies to Label Smoothing (LS) (Szegedy et al., 2016) aim to adapt the label noise according to the type of data example. Such adaptation can be more effective in practice than its traditional counterpart where the label noise is the same for all examples. Adaptation is useful to handle examples that may have labeling errors, distribution shift, or calibration issues. For such cases, the effectiveness of adaptation has been extensively studied, for example, see Ghoshal et al. (2021); Lee et al. (2022) for generalization improvements, Zhang et al. (2021); Ko et al. (2023) for mis-labelled examples, Park et al. (2023) for miscalibration, and Xu et al. (2024) for out-of-distribution detection. Adaptivity is useful for label smoothing to handle all such cases.\nOne major problem with adaptive label smoothing is that it is not easy to design a good adaptivity strategy. For example, a simple approach is to adapt the label noise by using model's predictions but there are many ways to do this, for examples, Park et al. (2023) set the noise based on the logits, Zhang et al. (2021); Ko et al. (2023) use the predictive probabilities (obtained with softmax), while Lee et al. (2022) use their entropy. All of these are reasonable ideas but the choice of a good strategy for a given problem is not always straightforward. A strategy that reduces miscalibration may not be most effective for handling outliers or mislabeling. Focusing on one issue at a time has given rise to a lot of ad-hoc and heuristic strategies, and, despite their usefulness, designing an adaptive strategy for a task in hand remains tricky. Our goal here is to simplify the process by presenting and analyzing algorithms that naturally induce adaptivity.\nWe show that variational learning naturally induces an adaptive label smoothing. The smoothing arises due to the use of the expectation of the loss in the variational objective, taken with respect to the posterior distribution. The expectation gives rise to a label noise (among other types of noises) which is customized for each example through its features. Our key contribution is to derive the exact form of the label noise (Eq. 10) for many problems and study their behavior. We show extensive empirical results analyzing the label noise induced by Improved Variational Online Newton (IVON) (Shen et al., 2024). We show the following:\n1. Variational learning assigns higher noise to atypical or ambiguous examples (Fig. 1 and Fig. 3).\n2. IVON's adaptive label noise behaves similarly to the proposal of Zhang et al. (2021).\n3. IVON consistently outperforms Label Smoothing in presence of labeling errors, giving up to 9% accuracy"}, {"title": "2. Label Smoothing and Adaptivity Strategies", "content": "Label Smoothing (LS) is a simple technique where the true label vector y (length K) are replaced by a smoothed version. In its simplest form, a convex combination is used where the smoothed labels are defined as\n$y_{i}^{ls} = (1 - \\alpha)y_i + \\alpha u,$\nfor some scalar $\\alpha \\in (0, 1)$ with u as a vector of 1/K with K being the number of classes. This simple technique is effective to penalize overconfident predictions because the noise $\\alpha(u - y_i)$ reduces the importance of the label during training (Pereyra et al., 2017a). Multiple works have studied its effectiveness, for example, to improve calibration and representation (M\u00fcller et al., 2019), to favor flatter solutions (Damian et al., 2021), and improve robustness to mislabelled data (Lukasik et al., 2020; Liu, 2021) due to its connections to loss correction (Patrini et al., 2017). Despite its simplicity, LS has clear practical advantages.\nAdaptive label smoothing aims to inject noise according to the type of data example, for example, during learning, we may want to inject a noise to get the smoothed label\n$y_{i|t}^{ls} = y_i + \\epsilon_{i|t}.$\nThe noise $\\epsilon_{i|t}$ depend on the model parameter $\\Theta_t$ at iteration t, and can be varied according the model's opinion regarding the relevance of the examples. Adaptive label smoothing uses additive noise to reweigh examples during training. Many studies have shown the effectiveness of the adaptive noise, which ranges from improvements in generalization (Ghoshal et al., 2021; Lee et al., 2022), robustness to misla-beled data (Zhang et al., 2021; Ko et al., 2023), improving calibration (Park et al., 2023) and out-of-distribution (OOD) detection (Xu et al., 2024). By adapting label noise, such methods aim to down-weight the problematic examples.\nWhile adaptivity is desirable, it also requires additional effort to design a good strategy to adapt. Each specific issues may require a different type of noise, for instance, what works to reduce miscalibration, may not be most effective for handling OOD detection or mislabling. Focusing on one issue or strategy at a time has given rise to a lot of ad-hoc and heuristic strategies, and, despite their usefulness, clarity of good ways to design adaptive strategy is lacking.\nThe simplest approach is to adapt by using the model pre-dictions based on the logits $f_i(\\Theta_t)$, but there are many ways"}, {"title": "3. Variational Learning Induces Adaptive LS", "content": "Variational learning aims to optimize for distribution over parameters $\\Theta$ which is fundamentally different from tradi-tional deep learning where we minimize empirical risk,\n$l(\\theta) = \\sum_{i=1}^{N} l_i(\\theta) + R_\\theta(\\theta),$\nwith loss $l_i(\\theta)$ for the i'th example in the training dataset. The regularizer $R_\\theta(\\theta)$ is often implicitly defined through various training choices, such as, weight-decay, initializa-tion, and architecture design. In contrast, variational learn-ing aims to find a distribution $q(\\theta) \\in Q$ which minimizes\n$L(q) = \\sum_{i=1}^{N} E_{q} [l_i(\\theta)] + D_{kL}[q(\\theta) ||p(\\theta)].$\nThe second term is the Kullback-Leibler (KL) Divergence where the $p(\\theta) \\propto exp(-R_\\theta(\\theta))$ can be defined implicitly similarly to deep learning. Throughout, we will set $q(\\theta)$ to take Gaussian forms and show that, despite their differences, variational learning can be implicitly seen as minimizing a noisy version of Eq. 6. Existing works have studied the weight-noise (Zhang et al., 2018; Khan et al., 2018) but our goal here is to specifically study its effect on label noise."}, {"title": "3.1. A Simple Example: Logistic Regression", "content": "We start with logistic regression where we can write a closed-form expression for the adaptive label noise. The result extends to all loss functions using generalized linear model. We will consider all such extensions (including neural net-works) afterwards. For now, we consider a loss function for binary labels $y_i \\in \\{0,1\\}$ with model output $f_i(\\theta)$,\n$l_i(\\theta) = -y_i f_i(\\theta) + log (1 + e^{f_i(\\theta)}).$\nIn logistic regression, we have $f_i(\\theta) = \\phi_i^T \\theta$ where $\\phi_i \\in \\mathbb{R}^P$ is the feature vector. For simplicity, let us assume $R_\\theta(\\theta) = ||\\theta||^2$ to be a quadratic regularizer. For such a model, we can solve Eq. 6 with gradient descent (GD),\n$\\Theta_{t+1} = (1 - \\rho_t)\\Theta_t - \\rho_t \\sum_{i=1}^{N} \\phi_i [\\sigma(f_i(\\Theta_t)) - y_i]$\nThe result is obtained by simply taking the derivative of Eq. 8 which gives rise to $\\sigma(f) = 1/(1 + e^{-f})$, a binary ver-sion of the softmax function from Eq. 4. We will now show that, by choosing the family Q appropriately, variational learning can be seen as GD with label noise.\nWe choose the distribution $q_t(\\theta)$ at iteration t to take a Gaus-sian form with mean $\\Theta_t$ and covariance set to the identity,\n$q_t(\\theta) = N(\\Theta | \\Theta_t, I),$\nand perform GD to minimize the variational objective in Eq. 7, now denoted as $L(\\Theta_t)$, with respect to $\\Theta_t$. Below is a formal statement of the result.\nTheorem 1 A gradient update $\\Theta_{t+1} = \\Theta_t - \\rho_t \\nabla_{\\Theta_t} L(\\Theta_t)$ is equivalent to the gradient update in Eq. 9 where the label $y_i$ are replaced by $y_i + \\epsilon_{i|t}$ with noise defined as\n$\\epsilon_{i|t} = \\sigma(f_i(\\Theta_t)) - E_{q_t} [\\sigma(f_i(\\Theta))].$"}, {"title": "3.2. Generalized Linear Model (GLM) with GD", "content": "The result generalizes to any loss function derived using exponential-family distribution, for instance, the following generalization of Eq. 8\n$l_i(\\theta) = -y f_i(\\theta) + A(f_i(\\theta)),$\nwhere $A(f)$ is a convex function called the log-partition function. The regularizer can also be a general convex function. For such models, we can derive the label noise following almost the same procedure as in the previous section. Due to its similarity, we omit the derivation and only give the final form of the noise,\n$\\epsilon_{i|t} = A'(f_i(\\Theta_t)) - E_{q_t} [A'(f_i(\\Theta))].$\nEssentially, we replace the $\\sigma(f)$ by the derivative $A'(f)$. For logistic regression, $A(f) = log(1 + e^f)$, derivative of which is $\\sigma(f)$ and we recover the result in Eq. 10. We can extend this result to multiclass classification by considering $A(f) = log \\sum_{k=1}^{K} e^{f^k}$, derivative of which is the softmax function defined in Eq. 4. Similarly to the binary case, we expect uncertainty in $q_t$ to be amplified near the boundary. The label noise is therefore low for examples where softmax yields probabilities close to 0 or 1."}, {"title": "3.3. Generalized Linear Model with Newton's Method", "content": "We now go beyond GD to Newton's method and show that a specific variational-learning algorithm can be seen as a noisy-label version of Newton's method. This is a useful step before we move to neural networks training. Here, we find that the form of the noise has exactly same form as Eq. 16 but the distribution $q_t$ has a flexible covariance which improves the adaptivity of the label noise.\nWe consider the following Newton's update,\n$\\Theta_{t+1} = \\Theta_t - [\\nabla^2 \\ell(\\Theta_t)]^{-1} \\nabla \\ell(\\Theta_t)$\nwhich is commonly used for generalized linear models. As shown by Khan & Rue (2023), the update can be seen as a special case of a Variational Online Newton (VON) al-gorithm (Khan et al., 2018) to learn a full Gaussian with covariance $\\Sigma_t$,\n$q_t(\\Theta) = N(\\Theta | \\Theta, \\Sigma_t)$\nThe VON updates are given as follows,\n$\\Theta_{t+1} = \\Theta_t - \\rho_t \\Sigma_{t+1} E_{q_t} [\\nabla \\ell(\\Theta)]$\n$\\Sigma_{t+1}^{-1} = (1 - \\rho_t) \\Sigma_t^{-1} + \\rho_t E_{q_t} [\\nabla^2 \\ell(\\Theta)].$\nSetting $\\rho_t = 1$ yields a Newton-like update where gradi-ents $ \\nabla l$ and Hessian $ \\nabla^2 l$ are replaced by their terms where expectations are taken, namely, $E_{q_t} [\\nabla \\ell]$ and $E_{q_t} [\\nabla^2 \\ell]$. Similarly to the previous cases, the label noise in VON arises due to the expectation of the gradient, while expectation of the Hessian gives rise to other types of noise.\nAs shown in in App. A.1, the VON updates in Eq. 18 are equivalent to Newton's update in Eq. 17 where labels are replaced by the noisy ones with noise shown in Eq. 16. The proof technique relies on comparing the form of the surrogates for the two algorithms. Even though the noise has the same form, there is an important difference here. Essentially, the Gaussian $q_t$ now is more flexible because its covariance $\\Sigma_t$ is not fixed but learned using the Hessian. As a result the distribution over $f_i$ now has adaptive variances,\n$q_t(f_i) = N(f_i | f_{i|t}, \\phi_i^T \\Sigma_t \\phi_i).$\nTherefore, now both the location and spread of the Gaus-sians are changed for each example, and they both contribute to the adaptivity. The result shows that second-order meth-ods yield more adaptive label noise than first order methods, and are expected to perform better in practice. We will later present experiments that support this finding."}, {"title": "3.4. Neural Network training with IVON", "content": "We will now show that the label noise expression have simi-lar form for the neural network case, but to derive them we need to use Taylor's approximation. Essentially, the form of the expression then is similar to Eq. 14 there the adaptive nature should roughly stay the same. We validate these findings later through numerical experiments.\nWe will illustrate the derivation for the binary case which can then be extended to other case as we did in previous section. Taylor's approximations is required because the gradient of $l_i$, shown below,\n$\\nabla l_i(\\theta) = \\nabla f_i(\\Theta_t) [\\sigma(f_i(\\Theta_t)) - y_i],$\nreplaces the $\\phi_i$ term in Eq. 9 by $\\nabla f_i(\\Theta_t)$. As a result, we cannot simply move the expectation over $q_t$ to derive the label noise as we did in Eq. 11. However, we can simplify these by using Taylor's approximation.\nWe show this by using a single-sample $ \\Theta^{(1)} \\sim q_t$ Monte-Carlo approximation (multiple samples can also be used),\n$E_{q_t} [\\nabla l_i(\\Theta)] \\approx \\nabla f_i(\\Theta^{(1)}) [\\sigma(f_i(\\Theta^{(1)})) - y_i].$\nThen, we do the following two approximations where we use Taylor's expansion but ignore the second-order terms,\n$\\sigma(f_i(\\Theta^{(1)})) \\approx \\sigma(f_i(\\Theta_t)) + \\sigma'(f_i(\\Theta_t)) \\nabla f_i(\\Theta_t) (\\Theta^{(1)} - \\Theta_t)$\n$\\nabla f_i(\\Theta^{(1)}) \\approx \\nabla f_i(\\Theta_t)$\nWith these approximations, we can write,\n$E_{q_t} [\\nabla l_i(\\Theta)] \\approx \\nabla f_i(\\Theta_t) [\\sigma(f_i(\\Theta_t)) - (y_i + \\epsilon_{i|t})],$\nwhere the noise takes a very similar form to Eq. 14,\n$\\epsilon_{i|t} \\approx \\sigma'(f_i(\\Theta_t)) \\nabla f_i(\\Theta_t) \\Sigma_t^{1/2} e$\nwhere e is a sample from a standard normal distribution. The derivation generalizes to all GLM losses by replacing $\\sigma(\\cdot)$ by $A'(\\cdot)$. It also extends to variational GD and VON."}, {"title": "4. Experiments", "content": "We do extensive experiments to show adaptive label noise via variational learning and its benefits. In Sec. 4.1, we show that IVON adapts the label noise for each example, and generally assigns higher noise magnitude to ambiguous ones. In Sec. 4.2, we show that IVON's smoothed labels are similar to an existing adaptive smoothing method (Zhang et al., 2021). In Sec. 4.3, we show that IVON consistently outperforms LS when datasets have labeling errors in various settings. Additional experiments are reported in App. B, and experiment details are reported in App. \u0421."}, {"title": "4.1. IVON's Adaptive Label Noise", "content": "We demonstrate IVON label noise's adaptivity on MNIST dataset (LeCun & Cortes, 2010). We plot IVON's label noise distribution in Fig. 3, which shows that IVON adds different label noise on each example whereas traditional Label Smoothing defines a uniformly distributed noise for all. By further visualizing the data, we see that IVON in-duces stronger noise to unclear examples, which prevent models from being overconfident in these datapoints."}, {"title": "4.2. Comparisons to Existing Adaptive LS Strategies", "content": "In this section, we show that IVON's label smoothing is sim-ilar to an adaptive method called Online Label Smoothing (OLS) (Zhang et al., 2021). In the CIFAR-10 and CIFAR-100 dataset (Krizhevsky & Hinton, 2009), we compare the smoothed labels of IVON with traditional LS (Szegedy et al., 2016) and Online Label Smoothing (OLS) (Zhang et al., 2021). OLS adjusts the label noise according to the model's predictions, as described in Sec. 2. As Fig. 4 shows, IVON has surprisingly similar smoothed label distributions as the OLS in both datasets, while IVON tends to induce stronger label noises. Variational learning's adaptive label smoothing is similar to existing work's, without needing any additional effort to design or estimate the adaptive label noise."}, {"title": "4.3. Comparisons on Datasets with Labeling Errors", "content": "We compare IVON to Label Smoothing (LS) (Szegedy et al., 2016) and SAM (Foret et al., 2021) in presence of labeling errors, and the results show that IVON consistently outper-forms LS in various settings. To find the best performance of the baselines, we tune several LS's smoothing rates a (de-fined in Eq. 1), and various SAM's adversarial perturbation size p (discussed in Sec. 3.4). We conduct studies on bench-mark datasets with synthetic noise, where the noise level can be adjusted, followed by evaluations on datasets with natural noise, where the noise level is fixed and unknown. For synthetic noise experiments, we use the CIFAR-10 and CIFAR-100 datasets (Yu et al., 2019). For natural noise ex-periments, we use the benchmark Clothing1M (Xiao et al., 2015). All datasets include a clean test set."}, {"title": "4.3.1. SYNTHETIC NOISY DATASETS", "content": "We consider two commonly used corruptions (Patrini et al., 2017; Li et al., 2019; Yu et al., 2019): Symmetric flipping and Pair flipping. In symmetric flipping, a true label is re-placed by a randomly generated class with a probability. In pair flipping, it tries to mimic real world mistakes for simi-lar classes, where a true label is replaced by the next class with a probability. For training dataset, we use previous work's (Yu et al., 2019) code to generate noisy labels. More experiment details are in App. C.2.\nIn CIFAR-10, Fig. 5 shows that IVON outperforms Label Smoothing and SAM in different scenarios. We also ob-serve that SAM is sensitive to the choice of p, while IVON does not need to tune any hyperparameters to perform well."}, {"title": "4.3.2. DATA DEPENDENT LABELING ERRORS", "content": "In this experiment, we try to understand the adaptivity of these methods in the data-dependent noisy dataset. When each class has different noise levels, we expect LS will fail since it adds uniform noises to all classes, while IVON'S adaptivity makes it stand.\nFirst, we create a new transition matrix P of noisy label $y' = Py$, where $y, y' \\in \\mathbb{R}^K, P \\in \\mathbb{R}^{K \\times K}$. We inject difference noise level to each class, so the noise level of each class is different:\n$P_{i,i} = 1 - (\\kappa + \\beta_i), i \\in [1, K].$\nwhere $ \\kappa$ is the starting noise level and $ \\beta$ is the increase factor. Afterwards, we give the same transition probability to the rest of the wrong classes:\n$P_{i,j} = \\frac{\\kappa + \\beta_i}{K-1}, i, j \\in [1, K], i \\neq j.$\nIn experiments, we follow the hyperparameters in CIFAR-10 synthetic noise experiment from Sec. 4.3.1. For LS, we run smoothing rate \\{0,0.1, 0.3, 0.5, 0.7, 0.9\\} and report the best accuracy. For SAM, we run $ \\rho$ for \\{0, 0.05, 0.1, 0.15, 0.2, 0.5\\} and report the best accuracy.\nThe experiment results for $ \\kappa = \\{0.1 \\sim 0.5\\}$ and $\\beta = 0.05$ are in Fig. 7. Overall, IVON outperforms LS and SAM in all noise levels. Meanwhile, IVON can learn in very noisy scenarios $ \\kappa = \\{0.4,0.5\\}$ while baselines can only reach around 10% accuracy. The experiment results support our claim that adaptive label noise induced by variational learning is more effective than traditional label smoothing."}, {"title": "4.3.3. UNCONTROLLED NOISY DATASETS", "content": "We now report results on Clothing1M (Xiao et al., 2015), a large-scale dataset that features natural label noise from the web and consists of 1 million images across 14 categories. We conduct experiments by using ResNet-50 as the model.\nThe results on Clothing1M, illustrated in Fig. 9, demonstrate that IVON outperforms Label Smoothing and is comparable to SAM. This experiment shows that IVON's performance is consistent in the large scale dataset."}, {"title": "5. Conclusion", "content": "In this paper, we show that variational learning induces an adaptive label smoothing similar to an existing adaptive approach (Zhang et al., 2021) but does not require any addi-tional effort to design. We derive the exact form for simple models and extend them to neural networks. We empirically confirm the effectiveness of noise, showing that the IVON method consistently performs better than LS, and compara-bly to SAM, without requiring hyperparameters to achieve desired smoothing. Our work suggests that Bayesian frame-works are naturally suitable for label noise. Specifically, we believe that variational learning algorithms, such as IVON, provide a flexible framework to further add noise to handle both the abnormalities and typicalities in the data."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Derivations", "content": null}, {"title": "A.1. Derivation of GLM with Newton's method", "content": "Newton's update shown in Eq. 17 is equivalent to the following surrogate minimization,\n$\\Theta_{t+1} = arg min_\\Theta \\nabla \\ell^T(\\Theta_t) \\Theta + \\frac{1}{2} (\\Theta - \\Theta_t)^T \\nabla^2 \\ell(\\Theta_t) (\\Theta - \\Theta_t).$\nThis can be verified by simply taking the derivative of the above objective and setting it to zero. We will now show that the VON update give rise to a similar surrogate but where the gradient and Hessian are replaced by their expected values.\nTo do so, we use the result of Khan et al. (2019) who show that each step of VON algorithm can be seen as inference on a linear model. Essentially, the VON update can be expressed as follows (see Nickl et al. (2024, App. C.3) for a derivation):\n$q_{t+1}(\\Theta) \\propto q_t(\\Theta)^{1-\\rho_t} \\prod_{i=1}^{N} exp(-\\Theta^T E_{q_t} [-\\nabla l_i(\\Theta)+\\sqrt{\\nabla^2 l_i(\\Theta)} \\delta e]-\\frac{1}{2} e^T q_t [\\nabla^2 l_i(\\Theta)] \\delta e)$\n$ \\propto q_t(\\Theta)^{1-\\rho_t} exp(-\\Theta^T E_{q_t} [\\nabla \\ell] + \\frac{1}{2} \\Theta^T E_{q_t} [\\nabla^2 l] \\Theta - \\Theta^T E_{q_t} [\\nabla^2 l]\\Theta - \\Theta^T E_{q_t} [\\nabla^2 l]\\Theta_t ),$\nwhere we subtracted $\\Theta_t^T E_{q_t} [\\nabla^2 l]\\Theta_t$ and completed the square. This is a constant which is absorbed in the normalizing constant of $q_{t+1}$. From here, we can simply match the mode $ \\Theta_{t+1}$ of $q_{t+1}$ to the mode of the right hand side. For $ \\rho_t = 1$, this gives us the following minimization problem to recover $\\Theta_{t+1}$:\n$\\Theta_{t+1} = arg min_\\Theta \\Theta^T E_{q_t} [\\nabla l(\\Theta)] + \\frac{1}{2} (\\Theta - \\Theta_t)^T E_{q_t} [\\nabla^2 \\ell(\\Theta)] (\\Theta - \\Theta_t).$\nThis shows that, for $ \\rho_t = 1$, VON updates can be seen as Newton step where gradient and Hessian are replaced by their expected values. The proof is identical to the one shown in the main tex, therefore we omit it.\nTheorem 2 For the loss function of Eq. 8, the VON update in Eq. 18 with $ \\rho_t = 1$ is equivalent to Newton's update in Eq. 17 but where the label $y_i$ are replaced by $y_i + \\epsilon_{i|t}$ with noise defined as\n$\\epsilon_{i|t} = \\sigma(f_{i|t}) - E_{N(e|0,I)} [\\sigma (f_{i|t} + e\\sqrt{\\phi^T \\Sigma_t \\phi_i}) ]$,\nand the Hessian $ \\nabla^2 \\ell(\\Theta_t)$ is replaced by its noisy version $E_{q_t} [\\nabla^2 \\ell(\\Theta)]$."}, {"title": "A.2. IVON pseudo code", "content": "The pseudo-code is given in Alg. 1."}, {"title": "B. Additional Experiments", "content": null}, {"title": "B.1. Hessian Initialization", "content": "We analyze how Hessian initialization ho of IVON affects the accuracy. The results are in Fig. 10. IVON's accuracy can only vary by up to 10% when the Hessian is bigger than 0.05, and this variation is less sensitive compared to SAM's sensitivity to p, as shown in Fig. 5, Fig. 8 and Fig. 9."}, {"title": "C. Experiment details", "content": null}, {"title": "C.1. Experiment details of Sec. 4.1 and Sec. 4.2", "content": "In Fig. 3, we test IVON on a 3-layers convolutional neural networks. In Fig. 4, we do experiments on ResNet-34 model. We uses the PyTorch implementation verison\u00b9 of Online Label Smoothing (Zhang et al., 2021)."}, {"title": "C.2. Experiments on Synthetic Noisy Datasets", "content": "For pairflip setting in CIFAR-10, the classes flipping order is: AIRPLANE \u2192 AUTOMOBILE \u2192 BIRD \u2192 CAT \u2192 DEER \u2192 DOG \u2192 FROG \u2192 HORSE \u2192 SHIP \u2192 TRUCK \u2192 AIRPLANE. In CIFAR-10 experiments, we train a ResNet 34 for 200 epochs with batch size set to 50 and weight decay set to 0.001. For SAM and LS, we set initial learning rate as 0.05 and reduce it by 0.1 at 100 epoch and 150 epoch, following hyper-parameters from previous papers. For IVON (Shen et al., 2024), we follow the original paper to set initial learning rate as 0.2 and anneal the learning rate to zero with a cosine schedule after a linear warmup phase over 5 epochs. We set momentum to 0.9 for all methods, and hessian momentum B2 to 1 - e-5, hessian initial ho to 0.9, scaling parameter \\lambda to the number of training data for IVON. For SAM, we follow the original paper (Foret et al., 2021) and choose best neighborhood size \\rho from [0.01, 0.05, 0.1, 0.2, 0.5]. In CIFAR-100 experiments, we tune the hyperparamters to the best for each method. The hyperparameters are specified in Table 1.\nIn Fig. 6, we fix the Hessian of IVON by setting $ \\beta_2 = 1$ in Line 5 of Alg. 1. Therefore, standard deviation $\\sigma$ defined in Line 8 is fixed since Hessian h is fixed."}, {"title": "C.3. Clothing 1M Details", "content": "The noisy labels in Clothing1M (Xiao et al., 2015) are derived from the text surrounding the images on the web. In constructing the dataset, noisy labels are assigned to images based on this contextual text."}]}