{"title": "HexaCoder: Secure Code Generation via Oracle-Guided Synthetic Training Data", "authors": ["Hossein Hajipour", "Lea Sch\u00f6nherr", "Thorsten Holz", "Mario Fritz"], "abstract": "Large language models (LLMs) have shown great potential for automatic code generation and form the basis for various tools such as GitHub Copilot. However, recent studies highlight that many LLM-generated code contains serious security vulnerabilities. While previous work tries to address this by training models that generate secure code, these attempts remain constrained by limited access to training data and labor-intensive data preparation.\nIn this paper, we introduce HexaCoder, a novel approach to enhance the ability of LLMs to generate secure codes by automatically synthesizing secure codes, which reduces the effort of finding suitable training data. HexaCoder comprises two key components: an oracle-guided data synthesis pipeline and a two-step process for secure code generation. The data synthesis pipeline generates pairs of vulnerable and fixed codes for specific Common Weakness Enumeration (CWE) types by utilizing a state-of-the-art LLM for repairing vulnerable code. A security oracle identifies vulnerabilities, and a state-of-the-art LLM repairs them by extending and/or editing the codes, creating data pairs for fine-tuning using the Low-Rank Adaptation (LoRA) method. Each example of our fine-tuning dataset includes the necessary security-related libraries and code that form the basis of our novel two-step generation approach. This allows the model to integrate security-relevant libraries before generating the main code, significantly reducing the number of generated vulnerable codes by up to 85% compared to the baseline methods. We perform extensive evaluations on three different benchmarks for four LLMs, demonstrating that HexaCoder not only improves the security of the generated code but also maintains a high level of functional correctness.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have made significant progress in various code generation and understanding tasks, such as text-to-code [27, 49, 71], code repair [47, 71], and code summarization [27, 65]. This advancement is due to training with large corpora of open-source code, allowing the models to generate the desired output based on user input. One notable application is GitHub Copilot [22], an AI pair programmer built based on these models. More than one million developers use this product to complete code, generate code documentation, and fix bugs [70]. Furthermore, various other products based on LLMs have been developed to enhance the productivity of software developers [1, 2, 3, 4].\nDespite advances in LLMs to generate functionally correct code, previous studies have shown that pre-trained and instruction-tuned LLMs can produce code with security-relevant vulnerabilities [32, 56]. Pearce et al. [56] have shown that in manually designed security scenarios 40% of the code generated by GitHub Copilot contains security issues. Hajipour et al. [32] proposed an automated approach to evaluate the security vulnerabilities generated by LLMs. Their work showed that other state-of-the-art models [42, 49, 52, 58] also produce code with security vulnerabilities. For instance, they found that OpenAI's GPT-3.5 generated more than 2,000 unique and vulnerable code instances covering various Common Weakness Enumerations (CWEs) [32].\nHe and Vechev [35] tried to increase the security of the models' code outputs by employing controlled code generation. However, their fine-tuning approach remains limited to manually checked data, making adapting models to generate secure code for specific and new types of vulnerability labor-intensive. Furthermore, the trained models are only tested on a limited set of security scenarios proposed by Pearce et al. [56] and Siddiq and Santos [61]. Our experimental evaluation shows that many codes generated by these models [35] still contain vulnerabilities when tested with a diverse set of CodeLMSec benchmark prompts [32]. This indicates the limited representativeness of the current datasets and the ongoing challenges in using LLMs for secure code generation tasks.\nAdditionally, previous LLMs for code generation (CodeLMs), such as those described in the literature [27, 52, 42, 35], have typically been used to generate or complete code in a one-step fashion. Users usually request this one-step inference procedure to complete the code given the provided context. In one-step inference, the models are prompted to complete the code based on provided prefixes or combinations of prefixes and suffixes, with the inclusion of libraries either included within the context or omitted entirely. This process allows for little to no modification of libraries during inference, creating a scenario where the generation of secure code is not guaranteed if the necessary libraries have not been provided in advance by the user. A better alternative is to also suggest modifications to the developer's input to avoid biases, such as missing libraries, which can be used to generate secure code.\nIn this paper, we introduce HexaCoder, a novel approach that combines an oracle-guided data synthesis pipeline with a two-step generation process to improve the security of the generated code. Specifically, we use a security oracle to detect vulnerable code generated by different LLMs and use the oracle's report together with an LLM to repair these vulnerabilities. The security oracle is also used to ensure that the model's code output is free of vulnerabilities. By pairing the fixed codes with their corresponding vulnerable version, we train LLMs to generate secure code using the Low-Rank Adaptation (LoRA) fine-tuning method [38].\nDuring the data synthesis phase, the LLMs repair the code by modifying or extending the included libraries and the main code. As a result, our synthesized data includes the required security-relevant libraries. This suggests that writing secure code may require the use of additional libraries in specific scenarios. Based on this observation, we propose a two-step generation approach to give the models the opportunity to include libraries that can potentially be used to generate secure codes. In our two-step generation approach, we complete the given codes during inference by first providing only the included libraries as input to the models to generate all other potential libraries. In the second code generation step, we provide the updated libraries together with the rest of the code as input to the models. Figure 1 illustrates how our approach automatically synthesizes the secure code and fine-tunes the CodeLMs to enhance the models' capabilities in generating secure codes.\nIn summary, we make the following key contributions:\n1. End-to-End Code Repair Pipeline. We introduce HexaCoder, an approach that enhances the CodeLMs' capabilities in generating secure codes while maintaining their effectiveness in producing functionally correct programs. We achieve this by synthetically generating pairs of vulnerable and fixed codes for the targeted types of security vulnerabilities. Unlike previous approaches, HexaCoder provides a complete end-to-end pipeline for both synthesizing data and enhancing code security aspects of CodeLMs.\n2. Security Fine-Tuning. Using our synthesized data pairs, we fine-tune four different CodeLMs of varying sizes using the LoRA fine-tuning method. Our evaluation shows that this process significantly enhances the models' ability to generate secure code.\n3. Two-step Code Generation. We extend our HexaCoder approach by proposing a two-step generation approach. This approach gives models the opportunity to include relevant libraries in the given code before generating the desired code, reducing the number of vulnerable code instances generated by up to 85% compared to the baseline.\n4. Security Evaluation of Different CodeLMs. We conduct a comprehensive experimental evaluation of HexaCoder to verify its applicability across different CodeLMs. Our evaluation demonstrates that HexaCoder not only trains these models to generate secure code, but also maintains their performance in generating functionally correct programs.\nThe code, fine-tuned models, and synthesized will be available at https://github.com/hexacoder-ai/hexacoder."}, {"title": "2 Related Work", "content": "We begin by introducing LLMs for code generation (CodeLMs), highlighting the challenges they face in generating secure codes. Additionally, we provide an overview of existing research on data synthesis using LLMs."}, {"title": "2.1 LLMs for Code Generation", "content": "LLMs demonstrate remarkable performance in various natural languages and programming language tasks [12, 58, 23]. These include translation, question answering, code completion, and code refinements [12, 65, 16, 47]. This success is attributed to several factors, including the scaling of model sizes from hundreds of millions [20] to hundreds of billions of parameters [23, 19], the use of self-supervised learning objectives, reinforcement learning techniques [28], and the availability of large datasets comprising natural text and source code [42, 23].\nVarious works proposed LLMs for modeling source code data to tackle a wide range of code generation and understanding tasks. These models include Codex [16], CodeT5 [65], CodeGen [52], InCoder [27], DeepSeekCoder [31, 71], along with many others [24, 30, 58, 42, 47]. These models are primarily trained and evaluated on their ability to generate functionally correct programs, often without considering their potential software security issues. In this work, we propose an approach to enhance the capabilities of these models in generating secure codes while preserving their effectiveness in generating the desired codes."}, {"title": "2.2 LLM-Generated Security Vulnerabilities", "content": "LLMs for codes have been trained using a large corpus of open-source projects written by human developers [16, 47, 19, 33]. These codebases may contain a variety of software security issues, including SQL and code injection [56, 32], memory safety issues [62], deprecated APIs [59, 56], improper input validation [32], and cross-site scripting [61, 56, 32]. The models learned the patterns of vulnerable codes by using unsanitized source code data [56, 35, 32]. Various studies and benchmarks show that a high percentage of the code generated by these models may contain a diverse set of security vulnerability issues [56, 61, 32].\nPearce et al. [56] show that 40% of the programs generated by GitHub Copilot contain various security vulnerabilities. They use a set of manually designed scenarios to investigate the security issues that can be generated by GitHub Copilot. Siddiq and Santos [61] expand the scenarios provided by Pearce et al. [56] to other types of Common Weakness Enumerations (CWEs). These works [56, 61] rely on the limited set of manually designed scenarios to evaluate the model, which may lead to overlooking potential security issues that the models could generate. To address this limitation, Hajipour et al. [32] introduced an automated approach to generate a broader range of scenarios and assess the security vulnerabilities produced by LLMs. Additionally, they developed the CodeLMSec benchmark, a diverse dataset of scenarios to evaluate and compare the susceptibility of different models to generating vulnerable code. Other studies [41, 6, 63, 34, 10] have also highlighted the tendency of these models to generate code with various types of security vulnerabilities.\nHe and Vechev [35] and He et al. [36] attempted to enhance the security of LLMs in code generation. Their approach involved using examples of both vulnerable code and its corrected counterparts. He and Vechev [35] introduced a novel prefix-tuning approach called SVEN, designed to control the model's output, guiding it to generate secure (or even vulnerable) code and reducing the likelihood of generating code with vulnerabilities. This work is limited to manually checked examples of source codes. Furthermore, in [32], it has been shown that a high percentage of the codes generated by SVEN's models contain various security vulnerability issues. This highlights the limited representation of the collected training data.\nRecently, He et al. [36] proposed SafeCoder, which focuses on enhancing the code security of instruction-tuned models. They introduce a pipeline to automatically collect code examples from open-source repositories. Despite this advancement, due to the library dependency issues, the dataset they collected includes only a limited number of examples for each CWE. For instance, there are only 13 examples of C/C++ code related to CWE-787 (Out-of-bound Write) in their dataset [36]. Moreover, most of the examples in the datasets of previous work [35, 36] do not contain the necessary libraries. In contrast, we propose an approach to automatically generate a set of vulnerable and fixed code examples, which can be easily extended to cover new types of security vulnerabilities. Our synthesized data includes security-related libraries, enabling models to improve their secure code generation capabilities."}, {"title": "2.3 Data Synthesis using LLMs", "content": "Synthetic data generation has become a widely adopted solution for addressing challenges such as data scarcity [8, 46], the high costs associated with data collection and annotation [29, 14], and privacy concerns [46, 44, 5]. Given the advancement of LLMs in generating various types of data [46], synthetic data generation by LLMs emerges as an effective and low-cost synthetic data generation method [46, 44]. Various works used LLMs to synthesize data for natural language [50, 68], mathematics [48, 69], and code generation tasks [15, 66, 49]. For instance, Self-Instruct [64] introduced a pipeline to enhance the instruction-following capabilities of models by using LLMs to generate data that encompass a wide range of natural language scenarios.\nIn the code generation domain, Code Alpaca [15] automatically synthesized 20k code instruction data by applying Self-Instruct [64] to GPT-3.5 [53]. WizardCoder [49] adapts Evol-Instruct [68] to synthesize instruction-following code. Hajipour et al. [32] propose a few-shot prompting approach to automatically generate targeted vulnerable codes using CodeLMs. While their approach was initially used to evaluate CodeLMs in generating vulnerable codes, in our work, we use the generated vulnerable codes as part of our data synthesis pipeline. In this pipeline, given the vulnerable codes, we use the security oracle together with an instruction-tuned LLM to synthesize the corresponding fixed codes."}, {"title": "3 Technical Background", "content": "In this section, we provide an overview of LLMs, explain the different categories of code security, and discuss potential methods for identifying security issues in software."}, {"title": "3.1 Large Language Models", "content": "In this work, we consider LLMs that are pre-trained on code datasets and process the code data as a sequence of the text represented as tokens [27, 52, 19]. During inference, these models take the user's input, which may be a partial program, a natural language description, or a combination of both. Given the provided input, the models predict the next token at each step until they either generate the end-of-sequence token or reach a pre-set maximum token limit. Given tokenized input x = [x1,...,xn], the LLMs calculate the probability of the entire sequence x by multiplying the conditional probabilities:\n$$P(x) = \\prod_{t=1}^{n} P(x_t | X_{<t}).$$\nIn the left-to-right decoding approach, each token xt can be sampled from the distribution modeled by the LLM using P(xt|X<t) [57, 36]."}, {"title": "3.2 Evaluating Code Security Issues", "content": "Software faults in complex systems can be identified using a variety of security testing methods [60, 72, 18, 43]. These techniques aim to uncover different types of programming errors, suboptimal coding practices, deprecated functions, or potential memory safety issues. Generally speaking, these security evaluation methods fall into two main categories: static analysis [9, 7, 43] and dynamic analysis [60, 25, 55, 26, 72]. Static analysis involves reviewing the program's code without executing it and detecting issues like buffer overflows and improper API use by applying predefined rules and patterns [43]. Dynamic analysis, in contrast, executes the program in a controlled environment to observe its runtime behavior, identifying issues such as memory leaks, race conditions, or other types of spatial/temporal vulnerabilities that arise from specific input sequences [55].\nBuilding on previous research [56, 35, 32], we selected static analysis to identify security vulnerabilities in the generated source code. This method allows for the categorization of various vulnerabilities. Specifically, we used CodeQL, a leading static analysis engine provided by GitHub [39]. CodeQL has been increasingly used in recent studies [56, 61, 35, 32, 36] to evaluate the security of code generated by LLMs. By analyzing the model-generated code with CodeQL, we are able to detect security vulnerabilities and classify them using CodeQL's Common Weakness Enumeration (CWE) framework. This classification allows us to focus on specific types of vulnerabilities when generating and repairing vulnerable code, which we then examine in detail throughout our study."}, {"title": "3.3 Categories of Code Security Issues", "content": "The Common Weakness Enumeration (CWE), maintained by MITRE [51], is a comprehensive catalog of common software and hardware flaws. It includes over 400 distinct types of weaknesses, which are organized into various categories and subcategories, such as SQL injection and cross-site scripting [56]. Each CWE is typically accompanied by specific example(s) and potential ways to mitigate the issues [51]. In our data synthesis pipeline, we use the recommended mitigation strategies provided by MITRE [51] to guide the model in repairing vulnerabilities associated with these specific CWEs.\nIn this work, we focus on 11 CWEs that can be identified using static analysis tools. Notably, 9 of these 11 CWEs are included in MITRE's list of the 25 most dangerous software weaknesses published in 2023 [51]. The list of these CWEs, including a brief description, is provided in Table 1. In our analysis, we decided against using fuzzing for vulnerability detection, as this may require significant computational resources and extensive manual effort for classifying vulnerability types and root cause analysis. In addition, the code sequences generated by LLMs are typically not suitable for a fuzzing campaign because they do not represent a full program and would require a program-specific testing harness."}, {"title": "4 Secure LLM-based Code Generation", "content": "In this work, we propose HexaCoder, an approach designed to enhance the ability of CodeLMs to generate secure code. Our approach involves three key steps:\n1. Synthesis. Synthesizing pairs of vulnerable and fixed codes by guiding an instruction-tuned LLM (e.g., GPT-4 [54]). We guide the LLM with detailed security reports describing the vulnerabilities of the code. This guidance enables the LLM to generate a patched version that addresses these vulnerabilities.\n2. Fine-tuning. These synthesized code pairs are then used to fine-tune the models using the LoRA method [38] and a masked objective loss [35].\n3. Two-Step Generation. Based on the insights of our analysis of synthesized code pairs where the model added new libraries to address vulnerabilities, we introduce a two-step generation approach. This approach enables the models to first incorporate the necessary libraries before actually generating the targeted code."}, {"title": "4.1 Oracle-Guided Secure Code Synthesis", "content": "The straightforward way to teach LLMs and CodeLMs to generate secure codes is to train them with only samples of secure, vulnerable-free codes. However, a dataset consisting only of such code samples is not easy to collect: Automatically validating security issues in open-source code, where code training data is usually collected, can be challenging, as it often requires analyzing complex dependencies in various libraries [32]. Manually analyzing and labeling code is also a labor-intensive task. In addition, even if the model is trained only on secure code, it is not guaranteed that it will not generate vulnerable code during inference.\nTo address this challenge, we propose an oracle-guided code synthesis pipeline in which LLMs are used to synthesize pairs of vulnerable and fixed code samples. These samples are then used to fine-tune the LLM and guide the model to generate secure codes. Figure 2 provides an overview of our secure code synthesizing procedure. To synthesize vulnerable codes, we employ the few-shot prompting approach proposed by Hajipour et al. [32]. This approach employs a few examples of codes with targeted vulnerabilities to generate a diverse set of vulnerable code samples at scale. A security oracle then validates whether the generated code contains the targeted vulnerabilities; only the validated samples are included in the set of vulnerable codes. Each vulnerable code sample, along with its corresponding security report, serves as part of the model input. This security report contains the security oracle report together with a security hint. Based on the input, the model is then prompted to fix the security issues in the vulnerable code. The output of the model is checked again with the security oracle and if the oracle finds no vulnerability, the code is considered fixed and is used as a secure version of the code. The secured codes, together with their corresponding vulnerable versions, form our fine-tuning dataset. Note that in this work we use the GPT-4 model [54] to fix the given codes."}, {"title": "4.1.1 Detail of the Security Reports", "content": "In the process of fixing the security vulnerabilities in a given code, we guide the CodeLM using the security reports, which include both the report provided by CodeQL [39] as a security oracle and an additional security hint. More specifically, we analyze the security issue of each generated vulnerable code using CodeQL security queries. CodeQL then generates a report detailing the identified vulnerabilities in the code. We guide the model using the description and line number of each identified vulnerability."}, {"title": "4.1.2 Details of the Input Prompt", "content": "To synthesize the fixed version of the given vulnerable code, we provide the model with a prompt as input. This prompt includes details about the vulnerabilities, security hints, instructions outlining the model's task, and the vulnerable code itself presented in Markdown language [67].\nFigure 5 shows a summarized version of the prompt template that we use as input for the model. In this template, each variable ({variable}) is replaced with the corresponding content for the given vulnerable code. Specifically, {prog_lang} indicates the programming language of the code, {num_vuls } denotes the total number of vulnerabilities, {vul_count } is used to enumerate the vulnerabilities, {line_num} specifies the line of code in which the corresponding vulnerability was found, {cwe_type} describes the type of vulnerability according to the CWE classification, {cwe_explanation} provides an explanation of the CWE type that is provided by the CodeQL [39] report (Figure 3), {hint} represent the security hint (Figure 4), and {vul_code } contains the actual vulnerable code that needs to be fixed.\nNote that each code can contain one or multiple vulnerabilities. We include all vulnerabilities and their details in a single input prompt and instruct the model to fix all of the issues. In the input prompt, each vulnerability is listed with the variable {vul_count}, followed by the corresponding security hint. We instruct the model to first explain how these issues can be resolved before attempting to fix them. Our initial results indicate that when the model explains the vulnerabilities first, it has a higher success rate in resolving the issues.\nFollowing the provided instruction, given the input prompt, the model outputs an analysis of the vulnerabilities and the fixed code. By using Markdown language in the input prompt, we guide the model in producing output in Markdown. This helps us extract the output code from the model's outputs. We then use the security oracle to evaluate the output code for any security issues, and we select the code with no security issues as our fine-tuning data."}, {"title": "4.2 Fine-tuning CodeLMS", "content": "Our goal in this work is to enhance pre-trained CodeLMs to generate secure codes while preserving their ability to produce functionally correct codes. We accomplish this by fine-tuning the CodeLMs using synthesized code examples. This fine-tuning procedure can involve optimizing all parameters or a parameter-efficient approach [33, 37], such as the LoRA fine-tuning method [38]. We use LoRA fine-tuning as it requires a drastically lower number of parameters to optimize, and more importantly, previous studies showed that the LoRA approach is less prone to catastrophic forgetting in comparison to the full fine-tuning approach [33, 11].\nIn the LORA fine-tuning method [38], we freeze all of the model's weights and inject rank decomposition matrices into the selected layers. This approach significantly reduces the number of trainable parameters, thereby requiring less computational resources. We fine-tune the CodeLMs by employing our synthesized code data and update the parameters through gradient descent by optimizing for the following objective functions:\n$$L = -\\sum_{t=1}^{n} \\sum_{m_t} log P(x_t | x_{<t}).$$"}, {"title": "4.3 Two-step Code Generation", "content": "During the process of fixing security issues in the synthesized vulnerable code, we observe that the model includes new libraries to address some of the vulnerabilities. For example, the model included escape function from the flask Python library to resolve the cross-site scripting vulnerability (CWE-079). This observation led us to propose a two-step generation approach. The CodeLMs have been typically used to complete the codes in a one-step fashion by providing the relevant input context [52, 27, 19]. This context can include the prefix of the codes, such as included libraries with a few lines of codes and/or a natural language description. In the one-step code generation approach, the goal is to generate the next token given the provided context. This gives little to no opportunity to modify the input context, including the libraries.\nTo address this limitation, we introduce our two-step generation approach. In this approach, we complete the given input in two steps: 1. First, condition the CodeLM on the included libraries. 2. Next, update the context with the newly added libraries and condition the CodeLM on the updated context to generate the desired code.\nLet x = [x1,...,xn] be the input context and our goal is to generate the next m tokens y = [y1,..., ym] given the provided input context x. In the one-step generation approach, we autoregressively sample each token yi using P(yix, y1) without modifying the input context x. In contrast, our two-step generation approach treats the tokenized input context x as [X1,...,x1,...,Xn], where [x1,...,x1] represents the included libraries, and [x1+1,...,xn] represents the remaining input context.\nIn the first step, we condition the CodeLM on the included libraries [x1,...,x1] and generate up to m' additional tokens. We then update the input context to x' [X1,...,x1,..., Xu,...,xn] by incorporating the newly included libraries and modules. Note that, in the first step, we only consider the newly added libraries and discard the other types of the generated token. In the second step, we generate up to m tokens y = [y1,...,ym] given the updated input context x'."}, {"title": "5 Experiments", "content": "In the following, we demonstrate how HexaCoder effectively enhances the capabilities of various CodeLMs to generate more secure code while maintaining their utility. We begin by detailing our experimental setup. Then, we study the effectiveness of our approach in synthesizing pairs of vulnerable and secure code. Finally, we compare the performance of our approach with the state of the art method for generating secure code using CodeLMs."}, {"title": "5.1 Setup", "content": "Below, we provide details of our experimental setup."}, {"title": "5.1.1 Models", "content": "For our experiments, we use different models to synthesize the code and also to evaluate the effectiveness of our approach. To generate vulnerable code samples, we follow the few-shot prompting approach proposed by Hajipour et al. [32]. In their work, they use CodeGen-multi with 6B parameters [52], GPT-3.5 (gpt-3.5-turbo-0301) [53], and Codex [16] (code-davinci-002) models to synthesize vulnerable codes. In our code synthesis pipeline, we incorporate these generated samples as our set of vulnerable codes. Additionally, we use GPT-4 [54] (gpt-4-turbo-preview) to fix the given vulnerable codes.\nWe evaluate the effectiveness of our approach by fine-tuning three models in different sizes. In our evaluation, we use CodeGen-350-multi [52], CodeGen-2B-multi [52], InCoder-6B [27], and DeepSeek-Coder-V2-Lite-Base with 16B parameters [71]."}, {"title": "5.1.2 Evaluating Code Security", "content": "We assess the security of the models using state-of-the-art methods [56, 32], which include both manually designed [56] and automatically generated [32] scenarios. These scenarios consist of a few initial lines of code, which can include libraries, function definitions, comments, and portions of the main code. We use these scenarios as input prompts to evaluate the models. Pearce et al. [56] offer only 2 to 3 prompts for each CWE, whereas the CodeLMSec benchmark [32] includes 20 diverse prompts per CWE. By utilizing these two sets that contain Python and C/C++ prompts, we can thoroughly evaluate the security of the models across a wide range of scenarios.\nFollowing the state-of-the-art work [35], we generate for each scenario up to 200 new tokens with a temperature of 0.4 to complete the provided input. We then use CodeQL [39] to evaluate the security issues in the generated code. CodeQL offers queries to identify 29 different CWEs for Python code and 35 CWEs for C/C++ code. Although we focus on 11 specific CWEs, as listed in Table 1, we analyze the generated code for all CWEs supported by CodeQL and report all identified security vulnerabilities in both Python and C/C++ code. In our results, CWEs not listed in Table 1 are categorized as Other."}, {"title": "5.1.3 Evaluating Functional Correctness", "content": "To assess the model's ability to generate functionally correct code, we use the HumanEval benchmark [16, 13], which has been widely adopted in previous studies [52, 35, 36, 31, 71]. We evaluate the model's performance using the pass@k metric. This metric involves generating code solutions for each problem, considering a problem solved if any of the solutions passes all unit tests. We then report the total fraction of problems that were successfully solved. Following the approach in existing work [16, 52, 35], we use an unbiased estimator to sample programs. We run the models using four common sampling temperatures (i.e., 0.2, 0.4, 0.6, and 0.8) and report the highest pass@k achieved. To ensure a fair comparison, following the approach of He and Vechev [35], we generate up to 300 tokens for each program."}, {"title": "5.2 Performance of Our Code Synthesis Approach", "content": "In our data synthesis pipeline, we generate a set of vulnerable and fixed codes. To generate the vulnerable set, we employ the few-shot prompting approach proposed in [32]. To minimize unnecessary computing usage, we use a set of 2,042 vulnerable code samples generated by this work [32]. This set contains 1,519 Python codes and 523 C/C++ codes. Each of these code samples contains at least one vulnerability of a CWE type listed in Table 1. Note that, using CodeQL [39], we validate whether the code contains the targeted vulnerabilities. Only the samples that pass this validation are included in the vulnerable set.\nTo fix the vulnerabilities in each code, as described in Subsection 4.1, we consider each vulnerable code, along with its corresponding security report, as the input of GPT-4 [54]. Given the input, we generate up to 1,000 tokens using GPT-4 [54] with a temperature of 0.1. In an initial study, we found that these two parameters provide the best results considering the budget and the model's performance. Given the provided input to the GPT-4 model, we generate one sample and extract the fixed code from the provided sample. We then check the generated code again with CodeQL [54], and if the code does not contain any vulnerability, we consider it as an instance of our secure codes. Out of 2,042 vulnerable code samples, our approach successfully fixed the vulnerabilities in 1,776 of them, which we refer to as the secure code set. This secure set includes 1,414 Python codes and 362 C/C++ codes."}, {"title": "5.2.1 Importance of Security Reports in Synthesizing Secure Codes", "content": "In our data synthesis pipeline, the security report contains the report provided by CodeQL [39] together with the security hint adapted from the corresponding CWEs pages of MITRE [51] and Semgrep documentation [40]. We incorporate this security report in the input prompt to guide the model in resolving the root cause of the software faults. Here, we examine the impact of each component of the security report on resolving the security issues. For this experiment, we evaluate three variations of the secure code synthesis approach: 1. Without any security report. 2. Using CodeQL output as the security report. 3. Using CodeQL output along with the security hint as the security report.\nTo perform this experiment, we randomly selected 30 vulnerable codes for each CWE to evaluate our secure code synthesis pipeline using three variations of the security report. We limited the selection to 30 programs per CWE to maintain a reasonable compute budget. Table 3 shows the repair rate results using different security report components. We consider a code fixed if CodeQL [39] does not detect any vulnerabilities in it. The first row of this table provides results for the baseline case, in which we do not provide any security report about the vulnerability in the code. In this case, we simply ask the model to identify any vulnerabilities in the code and attempt to repair them directly. We provide the input prompt for this case in Appendix A. The second row of Table 3 presents the results for the scenario where only the CodeQL report is included as the security report in the input prompt. Comparing the first two rows of Table 3, we observe that for all of the CWEs, employing the CodeQL report provides helpful guidance for the model to fix the vulnerabilities. For example, for CWE-094, using the CodeQL report, we were able to repair 80.0% of codes, compared to only 43.33% when we didn't use any security report. The last row of Table 3 shows the results for scenarios where we use both the CodeQL report and the security hint to guide the model. These results demonstrate that by using the CodeQL report along with the security hint, we gained the highest repair rate compared to other approaches. On average, with the full security report, we were able to repair 83.99% of the code, whereas using only CodeQL, we repaired just 63.33% of the code."}, {"title": "5.2.2 Example of a Fixed Code", "content": "In Listing 2, we provide an example of a vulnerable C code with its corresponding fixed code. Note that we only provide part of the generated code, and we chose this example for illustration purposes. The other samples in our dataset have higher complexity. Listing 2a contains an integer overflow (CWE-190) vulnerability at line 17. This vulnerability arises if the user inputs a value that, when multiplied by 2, exceeds the maximum limit for an integer variable, leading to an integer overflow. In Listing 2b, we provide the fixed version of the code generated"}]}