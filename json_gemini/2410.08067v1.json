{"title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs", "authors": ["Shenao Zhang", "Zhihan Liu", "Boyi Liu", "Yufeng Zhang", "Yingxiang Yang", "Yongfei Liu", "Liyu Chen", "Tao Sun", "Zhaoran Wang"], "abstract": "Preference alignment in Large Language Models (LLMs) has significantly improved their ability to adhere to human instructions and intentions. However, existing direct alignment algorithms primarily focus on relative preferences and often overlook the qualitative aspects of responses, despite having access to preference data that includes reward scores from judge models during AI feedback. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning of the high-quality rejected responses. The unawareness of the reward scores also drives the LLM to indiscriminately favor the low-quality chosen responses and fail to generalize to responses with the highest rewards, which are sparse in data. To overcome these shortcomings, our study introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. This dataset is easily integrated with existing direct alignment algorithms and is applicable to any preference dataset. The experimental results across instruction-following benchmarks including Alpaca Eval 2.0, MT-Bench, and Arena-Hard-Auto demonstrate that our approach consistently boosts the performance of DPO by a considerable margin across diverse models such as Zephyr, Mistral, Qwen2, Llama3.1, Gemma2, and SPPO. Additionally, on six academic benchmarks including GSM8K, GPQA, MUSR, TruthfulQA, BBH, and ARC, our method improves their average accuracy. When applying our method to on-policy data, the resulting DPO model outperforms various baselines and achieves state-of-the-art results on AlpacaEval 2.0. Through comprehensive ablation studies, we demonstrate that our method not only maximizes the utility of preference data but also mitigates the issue of unlearning, demonstrating its broad effectiveness beyond mere dataset expansion. Our code is available at https://github.com/shenao-zhang/reward-augmented-preference.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning from Human Feedback (RLHF) has recently seen remarkable success in aligning Large Language Models (LLMs) to follow instructions with human intentions. In this approach, AI-generated feedback serves as a stand-in for human preferences, assessing and ranking responses to prompts to construct a preference dataset. This dataset is then utilized in preference optimization algorithms to fine-tune LLMs. Among them, direct preference alignment (Rafailov"}, {"title": "2 Background", "content": "Consider a language model $\\pi\\in A$ that takes the prompt $x \\in X$ as input and outputs the response $y \\in Y$, where $X$ and $Y$ are spaces of prompts and responses, respectively. Given the prompt $x \\in X$, a discrete probability distribution $\\pi(\\cdot | x) \\in \\Delta_y$ is generated, where $A_y$ is the set of discrete distributions over $y$. We define the true human preference distribution as\n$p*(Y1 > Y2 | x) := En [1(h prefers y1 over y2 given x)],$\nwhere $h$ denotes the human rater and the expectation is over $h$ to account for the randomness of the human raters' choices. After pretraining and Supervised Fine-Tuning (SFT), Reinforcement Learning from Human or AI Feedback (Ouyang et al., 2022; Bai et al., 2022b) is typically employed to enhance the ability of the language model to follow instructions with human preferences.\nRL from AI Feedback (RLAIF). The RLAIF framework involves two major steps: preference dataset construction with AI feedback and preference optimization. As a surrogate for human preference, AI feedback, including LLM-as-Judge (Zheng et al., 2024; Cui et al., 2023) and Reward-Model-as-Judge (Adler et al., 2024; Dong et al., 2024), can be used to rank responses and generate preference pairs. Specifically, consider the judge model $r(x, y) : X \\times Y \\rightarrow R$ that outputs a scalar reward value representing the quality of $y$ under $x$. For each prompt $x \\in X$, two responses, $y_1$ and $y_2$, are independently sampled either from the same reference model (Xiong et al., 2024; Wu et al., 2024) or several different models (Zhu et al., 2023; Zhang et al., 2024). Then $r(x, y_1)$ and $r(x, y_2)$ are evaluated to determine the preferred response $y_w = \\text{argmax}_{y\\in\\{y_1,y_2\\}} r(x, y)$ and dispreferred response $y_l = \\text{argmin}_{y\\in\\{y_1,y_2\\}}r(x, y)$. By sampling responses and ranking them for a set of $N$ prompts, we get a preference dataset: $D_v = \\{x^i, Y_w, Y_l, r(x^i, Y_w), r(x^i, Y_l)\\}_{i=1}^N$.\nDirect Alignment from Preference. The objective for the LLM $\\pi \\in \\Delta_y$ is to maximize the KL-regularized expected reward. Recent works (Azar et al., 2023; Zhao et al., 2023; Tunstall et al., 2023b; Ethayarajh et al., 2024) proposed to align the LLM directly with the preference data by deriving the preference loss as a function of the LLM by the change of variables. Among them, the Direct Preference Optimization (DPO) (Rafailov et al., 2024b) loss has the following form:\n$L_{DPO}(\\pi; D_N) = -E_{(x;y_w;l) \\sim D_N} \\left[ \\text{log} \\sigma \\left( \\beta \\text{log} \\frac{\\pi(Y_w | x)}{\\pi_{\\text{ref}}(Y_w | X)} -  \\beta \\text{log} \\frac{\\pi(\u03b6_l | x)}{\\pi_{\\text{ref}} (Y_lx)} \\right) \\right]$\nwhere $\\beta$ is a hyperparameter corresponding to the KL divergence regularization, $\\sigma(\\cdot)$ is the logistic function, and $\\pi_{\\text{ref}}$ is some reference LLM policy, such as the SFT model."}, {"title": "3 Reward-Conditioning Addresses Limitations of Direct Preference Alignment", "content": "We will first demonstrate the limitations of vanilla direct alignment over the preference data.\nHigh-Quality Rejected Responses are Unnecessarily Unlearned. The dataset $D_N$ often contains preference pairs where the rejected response $y_l$ is only marginally worse than the chosen one $y_w$. Direct alignment algorithms, however, primarily focus on relative preferences and are unaware of the responses' quality values and gaps. Striving to maximize the reparameterized reward gap between the chosen and rejected responses will risk overfitting and the unnecessary unlearning of high-quality responses, potentially diminishing the model's performance by discarding valuable alternatives. Furthermore, in such a finite data regime where only a sample estimate of the true preference is accessible, it can be very possible that $p*(y_l > y_w | x) > 0.5$, i.e., $y_i$ is in fact more preferred than $y_w$ in expectation. This issue becomes even more pronounced when the preference data generated with the imperfect judge model is noisy.\nWe illustrate this limitation with the example in Table 1. For $D_{N=1}$ that contains a single preference pair\u00b9 with reward $r(x, y_1) = 9$ and $r(x, y_2) = 8$, the optimal policy learned from $D_{N=1}$ is $\\pi*(y_1 | x) = 1$. This causes the model to avoid generating $y_2$, a response of nearly equivalent quality.\nLow-Quality Chosen Responses are Indiscriminately Learned. For a similar reason, direct alignment algorithms also indiscriminately reinforce the chosen responses. As illustrated in Table 2, when $D_{N=2}$ contains two preference pairs, where one of the chosen responses, $y_2$, is of low quality, $\\pi*$ still indiscriminately generates $y_2$ with an arbitrary probability $0 \\leq a \\leq 1$, i.e., $\\pi*(y_2 | x) = a$.\nReward Sparsity. Preference data often contains responses that, despite being preferred in pairwise comparisons, exhibit substantial variation in quality. As a result, the optimal responses- those associated with the highest reward values are sparse in the dataset. Since direct alignment algorithms do not account for these reward values, the trained model struggles to differentiate between responses of varying quality and fails to generalize effectively to the sparse optimal responses."}, {"title": "3.2 Reward-Conditioned Policies Resolve the Limitations", "content": "A straightforward way to address the limitations of direct alignment algorithms\u2014specifically, their inability to account for the quality of responses\u2014is to optimize a reward-conditioned policy. In this approach, the LLM policy is trained to generate responses corresponding to different reward values, enabling it to become aware of and adapt to these reward distinctions. By doing so, the LLM not only learns the patterns associated with the preferred responses but also retains the valuable information from the rejected ones, preventing the unlearning of high-quality rejected responses. For example, in Table 1, reward-conditioned policies learn to generate both $y_1$ and $y_2$, instead of unlearning $y_2$."}, {"title": "4 Method", "content": "With the above motivation, we propose a data relabeling method that constructs a reward-augmented dataset by conditioning the preference pairs on the reward values given by the judge model. Specifically, we define the goal-conditioned reward $r_g(x,y) = 1(g = r(x, y))$ as a function of the quality reward $r$. The objective of the reward-conditioned policy $\\pi(y | x; g)$ is thus to minimize the absolute difference between the goal reward $g$ and the response reward $r(x, y)$, which is equivalent to maximizing the goal-conditioned reward $r_g(x, y)$, i.e.,\n$\\text{min}_\\pi E_{g,x\\sim D_n,y\\sim \\pi(\\cdot|x;g)} [[r(x, y) - g|] = \\text{max}_\\pi E_{g,x\\sim D,y\\sim \\pi(\\cdot|x;g)} [r_g(x, y)]. (4.1)$\nTo optimize the RHS of Equation (4.1), we first observe that under the new goal-conditioned reward metric $r_g$, for each preference pair $x^i, Y_w, r_w, Y_l, r_l$ in $D_v$, we have\n$r_{g=r_w}(x, Y_w) = 1, r_{g=r_w}(x, Y_l) = 0, r_{g=r_l}(x, Y_l) = 1, r_{g=r_l}(x, Y_w) = 1$.\nThus, each pair can be relabeled to create two new preference pairs based on two distinct goals: when $g = r_w, Y_w > Y_l$; when $g = r_l, Y_l > Y_w$. Then any direct alignment algorithm can be applied to this new goal-conditioned preference dataset. Compared to fine-tuning on the original dataset $D_N$, the model learns to capture not only desirable behaviors but also undesirable ones from the reward-augmented dataset. This approach helps identify patterns across high- and low-quality"}, {"title": "5 Related Work", "content": "Preference Dataset Construction. In order for the LLMs to follow instructions and better align with human intents, it is common practice to build a preference dataset containing a set of prompts and a pair of responses for each prompt, whose qualities are ranked by humans (Ouyang et al., 2022) or judge models (Bai et al., 2022b). A popular pipeline (Cui et al., 2023; Tunstall et al., 2023b; Wang et al., 2024c; Ivison et al., 2023; Zhu et al., 2023) for constructing offline (i.e., fixed) datasets involves sampling off-policy responses from various LLMs for each prompt in the hope to increase the response diversity. The preference data can also be generated online (Guo et al., 2024) or iteratively (Bai et al., 2022a; Xu et al., 2023; Gulcehre et al., 2023; Hoang Tran, 2024; Xiong et al., 2023; Dong et al., 2024; Calandriello et al., 2024; Rosset et al., 2024) by sampling and ranking on-policy responses from the training LLM. Recent works (Zhang et al., 2024; Cen et al., 2024; Xie et al., 2024) have also proposed systematically exploring the responses online and actively eliciting the preference. The proposed method in this paper is orthogonal to the construction ways of the preference data and can be applied to any dataset created either off-policy or on-policy."}, {"title": "6 Experiments", "content": "We begin by conducting experiments to demonstrate that applying the proposed method to fixed offline preference datasets leads to consistent performance improvements in DPO."}, {"title": "6.2 Ablation Studies", "content": "Our Method Improves the Utility of Preference Data. We provide two pieces of evidence that our method can get more juice out of the preference data compared to directly applying DPO. Firstly, we evaluate SPPO (Wu et al., 2024)\nfine-tuned with DPO on UltraFeedback (UF).\nThe results are shown in Table 4. Since the SPPO model is already trained on Ultra- Feedback from Gemma-2-9B-It, an additional round of DPO training with the same data significantly degrades its performance. In contrast, performing DPO on Reward-Augmented (RA) UltraFeedback results in substantial per- formance gains for SPPO, indicating that our method enhances the utility of the preference data.\nThe second evidence is that after DPO, the implicit reward can be used to relabel and aug- ment the same preference data. Specifically, after training Qwen2-7B-Instruct with DPO on UltraFeedback, we leverage the resulting model $\\pi_{DPO}$ to calculate the implicit reward for each prompt $x$ and response $y$, i.e., $r = \\beta(\\text{log} \\pi_{DPO}(y | x)-\\text{log}\\pi_{Qwen}(y | x))$. Then we perform DPO on Qwen2-7B-Instruct using the Implicit-Reward-Augmented (IRA) UltraFeed- back dataset. The results are shown in Table 5. We observe that augmenting the data with the implicit reward from the DPO (UF) model leads to superior performance even compared to augmenting the data with reward scores from the LLM judge, i.e., DPO (RA). This result highlights that DPO does not fully ex- ploit the potential of the data. Moreover, this\nablation demonstrates that our method is compatible with binarized preference datasets that only contain chosen and rejected response pairs, bypassing the need for reward scores from judge models.\nReward-Augmented Data is Superior Not Just Due to Its Increased Size. In this part, we show that the success of our method is not merely due to the increased size of the training dataset. To illustrate this, we perform DPO on the dataset where reward augmentation is applied to the first half of the UltraFeedback data, which we denote as DPO (Half RA). By doing so, the reward-augmented data is of the same size as the original dataset, but with only half of the prompts and the corresponding responses being utilized. It can be observed from Table 6 that DPO (Half RA) outperforms fine-tuning on the whole UltraFeedback (UF) by a large margin and achieves comparable performance to applying reward augmentation across the entire UF dataset, which is denoted as DPO (RA)."}, {"title": "7 Conclusion", "content": "In this paper, we first investigate the limitations of direct alignment algorithms, which arise from focusing solely on relative preferences while neglecting the qualities of the responses and their gaps. Specifically, since many rejected responses are only slightly worse than the chosen ones, striving to maximize the reparameterized reward gap will cause overfitting and unnecessarily unlearning the high-quality rejected response. Moreover, the directly aligned LLMs often struggle to differentiate between responses of varying quality, indiscriminately learning the low-quality chosen responses and failing to generalize effectively to more optimal responses that are sparse in the preference data. To resolve the above limitations, we introduce a straightforward solution-learning reward-conditioned policies. By optimizing the LLM to generate responses conditioned on their qualities, it can better differentiate between quality levels and learn from the entire spectrum. Motivated by this, we propose a data relabeling method that constructs reward-augmented datasets by conditioning on the quality of responses as the goal quality. In experiments, we fine-tune various LLMs by applying DPO on our reward-augmented data. The results demonstrate that our approach consistently delivers significant performance improvements across various instruction-following benchmarks and increases the average accuracy on academic benchmarks. Comprehensive ablation studies demonstrate that our method not only enhances the utility of preference data but also addresses issues beyond simple dataset expansion, such as mitigating the unlearning limitation. Our method also achieves state-of-the-art results on Alpaca Eval 2.0 when applied to on-policy data."}, {"title": "A Experiment Details", "content": "We use the following prompt during training. Here, the reward values are the quality scores given by the judge models that exist in the preference dataset. The prompt is set as the system prompt whenever the LLM supports, such as Qwen2-7B-Instruct and Llama-3.1-8B-Instruct, and it is prefixed before the original prompt when the LLM doesn't support system prompting, such as Mistral-7B-Instruct-v0.3 and Gemma-2-9B-It.\nAt inference time, we use almost the same prompt, except that the goal score is the highest one, i.e., the overall score is 10.\nIn our experiments using UltraFeedback, we directly leverage the LLM-as-Judge scores provided by GPT-4 in the dataset, which range from 1 to 10. For our method that is applied to on-policy data ranked by external reward models, including PairRM and ArmoRM, we apply linear transformations to normalize the resulting reward scores, ensuring they are scaled within the same 1 to 10 range. For hyperparameters, we utilize a KL regularization coefficient of $\\beta = 0.01$ in DPO, and we adopt the AdamW optimizer (Loshchilov, 2017). The batch size is set to 128, with a learning rate of 5e - 7 and a warmup ratio of 0.1. Furthermore, we observe that for models such as Qwen2-7B-Instruct and Gemma-2-9B-It on UltraFeedback, as well as Llama-3-8B-Instruct on on-policy data, both DPO and our proposed method yield improved performance when employing the conservative DPO (cDPO) technique (Mitchell, 2023). Consequently, for these models, we set the label smoothing hyperparameter from the Alignment Handbook (Tunstall et al., 2023a) to 0.3, while keeping it at 0 for the remaining models."}]}