{"title": "PixLens: A Novel Framework for Disentangled Evaluation in Diffusion-Based Image Editing with Object Detection + SAM", "authors": ["Stefan Stefanache", "Llu\u00eds Pastor P\u00e9rez", "Julen Costa Watanabe", "Ernesto Sanchez Tejedor", "Thomas Hofmann", "Enis Simsar"], "abstract": "Evaluating diffusion-based image-editing models is a crucial task in the field of Generative AI. Specifically, it is imperative to assess their capacity to execute diverse editing tasks while preserving the image content and realism. While recent developments in generative models have opened up previously unheard-of possibilities for image editing, conducting a thorough evaluation of these models remains a challenging and open task. The absence of a standardized evaluation benchmark, primarily due to the inherent need for a post-edit reference image for evaluation, further complicates this issue. Currently, evaluations often rely on established models such as CLIP or require human intervention for a comprehensive understanding of the performance of these image editing models. Our benchmark, PixLens, provides a comprehensive evaluation of both edit quality and latent representation disentanglement, contributing to the advancement and refinement of existing methodologies in the field.", "sections": [{"title": "1 Introduction", "content": "Recent developments in Generative AI, particularly in large-scale text-to-image diffusion models [13, 29], have enabled the development of text-guided image editing models [2,24,37]. Notably, text-guided image diffusion models [13,29,31] have recently demonstrated substantial image generation capabilities. Pre-trained on extensive image-text pairs like LAION [32] using a diffusion objective, these models have achieved state-of-the-art FID [11] scores on benchmarks such as MS-COCO [18] and exhibited success in editing real images [15, 24]. In the domain of image editing models, recent strides have been made, exemplified by InstructPix2Pix [2], ControlNet [37], LCM [22], OpenEdit [20], and VQ-GAN+CLIP [6], which leverage established image generators like Stable Diffusion [29] and GANs [10]."}, {"title": "2 Related Work", "content": "However, progress is hampered by the absence of a widely accepted and standardized methodology for evaluating the performance of image editing models. The absence of a post-edited image reference makes this task somewhat subjective and non-trivial. Efforts to accomplish this task have already been made in the past, however existing evaluation methods may not be universally applicable or suitable for all situations. The majority of benchmarks overly depend on the CLIP score [26], which is not always reliable [8], as discussed in Section 5.2, or on distribution-based metrics such as FID [11]. While FID is useful for assessing the overall quality and diversity of generated images, it does not account for specific edit accuracy or the preservation of image attributes and has been proven to be biased [5], making it even less reliable. This over-reliance on distribution-based metrics and CLIP scores can lead to misleading evaluations, where models appear performant but fail to meet nuanced or task-specific criteria.\nIn light of this challenge, we propose PixLens, an automatic benchmarking framework designed to address the shortcomings of current evaluation methodologies for diffusion-based image editing models. PixLens emphasizes automatic evaluation and leverages SAM-based detection models [16,25,27] for precise object localization through segmentation masks. In addition, it explores the correlation between latent space disentanglement and image editing performance, offering a comprehensive approach to model evaluation. Our contributions include:\nAutomatic evaluation procedures that mitigate subjectivity and provide standardized assessments.\nUtilization of advanced detection and segmentation models to enhance the evaluation process, particularly for complex objects and scenarios.\nComprehensive assessment of both subject and background preservation in edited images, including untargeted elements not intended for modification.\nIntroduction of multiplicity handlers to address issues such as hallucinations in edits and ambiguous scenarios, ensuring robust evaluation outcomes.\nExploration of latent space disentanglement and its influence on image editing performance, providing insights into model behavior.\nStreamlined pipeline setup and usage designed for ease-of-use by image editing model developers, facilitating efficient evaluation workflows.\nBenchmarks such as PIE-Bench [14] and MagicBrush [36], present different automated pipelines for evaluating the quality of edits. Yet, a common limitation is their over-reliance on CLIP [26] scores, which has been shown to be unreliable in certain cases, as noted in [8] and exemplified in Section 5.2. ImagenHub [17] addresses this concern by proposing a unified framework incorporating human evaluation scores, albeit introducing the inconvenience of expert raters.\nEditVAL [1] employs OwL-ViT [23] for automated evaluation. However, when facing hallucinations from the image editing model, such as the addition of unintended objects or drastic changes to unintended parts of the image (e.g.,"}, {"title": "3 PixLens: Benchmark for Text-Guided Image Editing", "content": "This section introduces PixLens, our novel automated benchmark for text-guided image editing. It consists of three primary components: (i) an automated editing quality evaluation procedure, (ii) a subject and background preservation evaluation pipeline, and (iii) a latent space disentanglement analysis."}, {"title": "3.1 Edit Quality Automatic Evaluation", "content": "Evaluated Edit Types and Pipeline Structure. We draw inspiration from Edit VAL's categorization of 13 distinct edit types, although their automatic evaluation pipeline only assesses a subset of 6. Notably, we introduce automatic assessment for object-removal, single-instance-object-removal, and color edit types, which are missing in their benchmark, though we acknowledge limitations regarding color evaluation biases, as discussed in Section 5.2.\nOur benchmark adopts EditVAL's JSON structure for defining edit sets, facilitating easy modification and introduction of new edit types and operations. Specifically, for an edit operation to be evaluated successfully, the following"}, {"title": "Detection and Segmentation Integration.", "content": "EditVAL highlights the effectiveness of CLIP [26] in evaluating the alignment between edited images and the prompts. However, CLIP often falls short in recognizing complex spatial relationships, such as edits where the position or size of the main object in the image is"}, {"title": "Subject Preservation.", "content": "In contrast to existing benchmarks such as EditVAL or MagicBrush, our benchmark uniquely incorporates a comprehensive evaluation of subject preservation. This crucial aspect of image editing quality is often overlooked in related work. While MagicBrush and PIE-Bench use the DINO [4] metric for subject preservation, we assess the fidelity of the subject across multiple dimensions, providing a more nuanced and intuitive understanding of edit performance.\nWhen providing instructions such as \"Move the ball to the left of the image\", we expect that the edited image will retain the same subject as the original input. Based on this concept, we assess subject preservation from four distinct perspectives:\nSSIM [34] & SIFT [21]: Do the structural features remain consistent between the original and edited images?\nAligned IoU: Does the edited image maintain the same size and shape as the original?\nColor Similarity: Is the color of the subject preserved?\nNormalized Euclidean Distance: Does the edited subject retain its original position?"}, {"title": "Background Preservation.", "content": "There remains a lack of consensus regarding the most effective method for evaluating background preservation in masked images. PIE-Bench tackles this issue by employing a variety of metrics such as SSIM, MSE, PSNR and LPIPS [38]. Through experimentation with such techniques, we discovered that employing Mean Squared Error calculations between masked images and their grayscale counterparts yielded the most reliable results. This approach effectively accounted for not only color, but also shape and texture differences, providing a comprehensive assessment of background preservation.\nIn Figures 2, 3, and 4, we highlight the significance of evaluating subject and background preservation in image edits, a key feature of our benchmark distinguishing it from others like TedBench or EditVAL.\nIn Figure 2, we compare two color edits using InstructPix2Pix to alter the boat's color in a lake. Despite both edits being successful (both scored 0.99), changing the boat to yellow also affects the background, resulting in preservation scores of 0.89 and 0.40, respectively. This underscores the importance of assessing how well models disentangle various image attributes beyond specific tasks, aiding developers in refining their editing models."}, {"title": "3.2 Disentanglement Evaluation", "content": "We address a significant gap in current image editing benchmarks: the evaluation of disentanglement. Disentanglement refers to the process of identifying and separating different factors that make up a complex data representation. Modularity [3] is an important concept that states that various factors in a representation space should operate independently and only affect specific subspaces without interacting with each other. Compactness [3] is another crucial aspect, which suggests that each factor's influence should be limited to as small a subset of the representation space as possible, ideally confined to a single dimension for each factor. This ensures that the representation is clear and precise. Lastly, explicitness [28] refers to the clarity and simplicity of the mapping between the representation space, i.e., the code, and the value of a factor, ideally being a straightforward linear relationship. In this framework, we evaluate modularity and"}, {"title": "Setup and Disentanglement Pipeline.", "content": "We start from a blank white image, crucial in circumventing positional-encoding issues that typically arise in the embedding process. We define a comprehensive list of objects and corresponding attributes to facilitate our evaluation. This includes a diverse range of elements as illustrated in the Supplementary Material. For each attribute category (e.g., texture, color, pattern), we generate pairs from the respective attribute lists. We denote $\\text{prompt}$ as the latent representation of an edited white image with a specific prompt. For each attribute pair $(a_1, a_2)$ in a category, we proceed as follows. First, we generate latent representations $z_{a_1}$ and $z_{a_2}$ for the prompts $a_1$ and $a_2$. Then, for each object $o$ in our list, we generate samples with $Z_{start} = z_{a_1 o}$ and $Z_{end} = Z_{a_2 o}$ (e.g., if $o$ is \u201cchair\u201d and $a_1$ is \u201cblue\u201d, then the prompt would be \"blue chair\"). This procedure results in a dataset comprised of tuples of latent encodings of the following form: $(Z_{start}, z_{a_1}, a_2, Z_{end})$."}, {"title": "Computing Scores.", "content": "We employ different metrics to evaluate disentanglement, depending on whether we evaluate isolated samples, samples with the same attributes, or all samples.\nIntra-sample Score: As mentioned in [28], an ideal disentanglement representation should be defined by a simple (linear) function from the code to the factors. A method to verify this is by checking if replacing attribute $a_1$ with attribute $a_2$ in the object $a_1o$ results in the same latent representation as $a_2o$. We evaluate this linearization by assessing the isolated behavior of samples, evaluating the distance between $z_{end}$ and $(Z_{start} + z_{a_2} - z_{a_1})$ and then averaging across all samples. We normalize these values by dividing them by the Euclidean norm of $z_{end}$, denoted as $||Z_{end}||_2$, to facilitate a standardized comparison.\nInter-sample and Intra-attribute Score: This part of the evaluation is based on the principle that going from one attribute $a_1$ to another $a_2$ belonging to the same attribute category should follow a given direction independently of"}, {"title": "4 Automated Evaluation of State-of-the-Art Models", "content": "We conducted a comprehensive evaluation of five recently introduced text-guided image editing methods using the EditVAL dataset's edit operation collection set. This dataset is particularly well-suited for object-oriented edits, making it an ideal choice for our benchmark's evaluation criteria. The evaluated methods include InstructPix2Pix, ControlNet, Latency Consistency Models, OpenEdit, and VQGAN+CLIP. Our automated evaluation uses SAM on top of Grounding DINO [19] for object detection and segmentation, with a bounding box extraction confidence threshold set at 0.1."}, {"title": "4.1 Performance Overview and Analysis", "content": "From the results shown in table 1, we observe that, for simple object manipulation edit types, the scores related to the edit success typically fall within the interval of [0.25, 0.96], with InstructPix2Pix, LCM and VQGAN+CLIP being the top-performing models. On the other hand, in edit types related to spatial manipulation, all models exhibit poor performance, with the potential exception of the LCM and VQGAN+CLIP models, which demonstrate a relatively consistent score. As illustrated in Table 2, LCM is one of the worst models in"}, {"title": "4.2 Disentanglement Analysis and Correlation", "content": "As illustrated in Table 3 disentanglement evaluation, we can see that overall, LCM and InstructPix2Pix perform better in the three assessments, both of"}, {"title": "5 Quantitative Analysis of the Benchmark", "content": "In this section we will present quantitative comparison of our Benchmark with state of the art methods [33,36], emphasizing how PixLens overcomes the limitations of these benchmarks."}, {"title": "5.1 Comparative Analysis with Edit VAL", "content": "Our comparison with EditVAL, illustrated in Table 4, using InstructPix2Pix as a common model evaluated in both benchmarks, demonstrates that PixLens' computed scores exhibit a notably stronger correlation with human assessment than EditVAL's scores. Specifically, when considering three types of score assignments (EditVAL-human, EditVAL-automatic, and PixLens), our aggregated automatic results show a higher correlation (0.903) with the aggregated human-based scores of Edit VAL compared to their automatic method (0.146)."}, {"title": "5.2 Benchmarking Against MagicBrush Dataset", "content": "To compare the proposed benchmark with MagicBrush, we analyzed all the editing instructions in their \"dev\" file, consisting of 528 edits (the distribution of edit types is shown in the Supplementary Material). We then adapted these instructions to be compatible with our benchmark, assigning an edit type label to each and including all necessary information for evaluation, such as the category, from, and to attributes. In Table 5, we present aggregated scores per edit type obtained from evaluating the ground truth images using PixLens. Overall, the edit-specific scores are generally high, as expected since we are assessing images that were manually altered to serve as ground truth examples. Particularly, edit types such as object-replacement and object-addition demonstrate high scores, reflecting their prevalence in the dataset. However, lower scores are observed for object-removal and color edits. In the case of object-removal edits, the presence of false positives from detection models can lead to misidentification of objects, resulting in lower scores despite successful removal. Regarding color edits, our evaluation methodology exhibits bias towards primary colors, impacting the assessment of color-related edits. This limitation arises due to the subjective nature of color evaluation in image editing, which lacks consensus within the field.\nThis evaluation was a critical step in ensuring that the image quality assessment"}, {"title": "1. Absence of Ground Truth Images:", "content": "In MagicBrush, image quality is assessed using L1 and L2 distances between generated and ground-truth images, cosine similarity between CLIP and DINO [4] embeddings, and CLIP-T [30] for prompt-image alignment. However, this method presents two challenges. Firstly, it requires manual generation of ground-truth images using another editing model like DALL-E or tools like Photoshop, resulting in additional costs and impeding scalability. Secondly, MagicBrush's evaluation approach only considers their ground-truth images, overlooking other potentially correct edits. In contrast, our method assigns high scores to all potentially accurate edited images."}, {"title": "2. Unreliability of CLIP Scores:", "content": "The CLIP model's reliability has been called into question, as discussed in [8] and exemplified in Figure 7. Furthermore, CLIP has demonstrated limited capabilities in evaluating certain edit instructions, especially those involving spatial alterations [9]."}, {"title": "3. Detailed Edit-Specific Scores:", "content": "While MagicBrush offers comprehensive evaluation across various prompts with an edit-type independent pipeline, our benchmark specializes in object-oriented images, assessing only 9 edit types. However, it offers advantages such as exhaustive evaluation covering subject and background preservation, along with tailored logic for each edit type, ensuring nuanced and precise evaluation criteria beyond reliance solely on CLIP."}, {"title": "6 Conclusion", "content": "PixLens offers a framework to evaluate different state-of-the-art models for image editing, designed to adapt seamlessly to any predefined evaluation dataset for instruction-based models. With a meticulous evaluation logic covering eight distinct edit types, PixLens employs detection and segmentation techniques to comprehensively assess the quality of the generated images. We employed our framework to evaluate six models over the EditVAL dataset. Notably, our findings reveal that no model excels across all edit types, with spatial manipulations posing a particular challenge for current image editing models. Demonstrating a superior correlation to human preference compared to EditVAL, PixLens presents a more accurate evaluation without depending on the CLIP metric, which has been proven unreliable in certain cases. Furthermore, by integrating disentanglement evaluation alongside traditional assessment metrics, PixLens offers a more comprehensive understanding of the performance of image editing models, setting a new standard for evaluation methodologies in this domain.\nOur benchmark provides a crucial task-dependent evaluation, addressing the limitations of existing models that may overfit to the aforementioned metrics. This ensures a more accurate assessment of image-editing capabilities and aligns closely with the primary objectives of the task. PixLens aims to establish a new standard for evaluating text-guided image editing, marking a significant advancement in image quality assessment methodologies."}, {"title": "6.1 Limitations", "content": "In considering the limitations of PixLens, reliance on detection and segmentation models presents a significant challenge, as these models can produce false positives"}, {"title": "7 Evaluation Methods for Different Edit Types", "content": "In this section, we offer a comprehensive description and pseudo-code for each of the edit types considered in our benchmark. These methods generate the edit-specific scores for the instances under evaluation.\nNotation. We adopt the same prompt format and notation as EditVAL [1]. Specifically, when referencing a specific image and editing operation, we denote the class of the image in the COCO dataset as category. Additionally, we use the from and to attributes from EditVAL. For instance, for an edit of type object-replacement over an image of class \"bench\", and prompt \"Replace the woman with a kid\", the \"bench\" would be the category, and \"woman\" and \"kid\" would be the from and to attributes, respectively.\nFurthermore, we refer to the input and edited images as Io and I1, respectively. M for segmentation masks. The function Detect outputs a list of detected objects and the corresponding masks M. If a variable is not used in the algorithms, it is omitted."}, {"title": "7.1 Object Addition", "content": "From what we have been able to observe in other attempts to benchmark text-guided image editing models, there is no established consensus regarding the interpretation of the instruction \"add an object\" in image manipulation operations. Some argue that the operation should be deemed successful only if the final image contains both the added object and the original category. Conversely, others contend that merely making the object visible in the image should suffice. After careful consideration, we have chosen not to impose any constraints on the detection of the image category, as subject preservation evaluation already verifies the retention of the main features of the primary image category. Consequently, to accommodate models that interpret the target as simply adding a new object in the image, we do not require the detection of the image category in the evaluation process when computing the edit-specific score."}, {"title": "7.2 Size Change", "content": "To evaluate size change edits, we start by detecting and segmenting the object targeted for resizing in both the input and edited images. Next, we validate whether the size change corresponds to the specified direction in the prompt. For instance, if the edit involves enlarging the object, we ensure that the area of the segmentation mask has increased in the edited image compared to the original. The magnitude of the size change does not affect the score, as our prompts do not dictate a specific new size. However, we enforce a minimum change $\\delta$, set to $\\delta = 0.1$ by default, relative to the original object. Additionally, we assess if the object's relative position remains relatively unchanged, factoring this into the final score. This involves checking if the smaller mask area (e.g. the input image's mask in enlargement) is contained within a larger area (e.g. the edited image's mask in enlargement) using a confidence threshold $T$ (set to 0.9 by default). If the ratio of the intersection area to the smaller area exceeds the confidence threshold, the size change edit meets the expected criteria. While some may argue this aspect is covered in subject preservation evaluation, our experiments show this refinement enhances PixLens' alignment with human evaluation standards in size edits."}, {"title": "7.3 Positional Addition", "content": "This edit type involves adding a specific object (defined by the to attribute) to the image at a designated position relative to the category object (Fig. 8"}, {"title": "7.4 Position Replacement", "content": "A successful position-replacement edit operation should alter the relative and absolute position of the category object within the image while preserving its main attributes (color, size, etc.), as well as the rest of the image (background preservation evaluation).\nEvaluation of this edit type presents inherent complexity due to multiplicity. When the input image contains multiple instances of the category object, it's unclear if the editing model should adjust the position of all instances or only one. Additionally, determining which instance to adjust becomes challenging if only one change is expected. In cases where hallucinations occur, creating additional instances of the category object, deciding which instance to evaluate poses additional issues.\nTo automate assessment, we judge whether the position of the largest object (using MultiplicityHandler.LARGEST) in the input image has been correctly modified. Empirical testing showed this approach yields meaningful scores with current prompt-guided image editing models.\nThe evaluation procedure follows a similar process to positional-addition logic. However, we not only assess the relative position of the category object but also verify its final absolute position within the image. For example, if the prompt is \"Move the ball from the left to the right of the image\", we divide the image horizontally into three sections (left, center, and right) and confirm the final center of mass is in the correct section."}, {"title": "7.5 Object Replacement", "content": "This type of edit aims to replace the from object with a new object to. The corresponding editing prompt structure is \"replace from with to\". Similar to other"}, {"title": "7.6 Object Removal", "content": "The goal of this edit type is to evaluate how effectively a specified object category is removed from an image. We achieve this by comparing the number of detected category instances in both the input and edited images. This comparison allows us to determine the success of removing the specified object class. The edit-specific score is computed as the maximum of zero and one minus the ratio of the number of target categories in the edited image to the number in the input image, effectively quantifying the proportion of removed categories."}, {"title": "7.7 Alter Parts", "content": "The aim of this edit type is to modify certain parts of the category object in the image by adding complementary elements. For instance, an instruction could"}, {"title": "7.8 Color", "content": "The objective here is to change the color of the main object in the image while ideally preserving the color of the background and other elements (which is thoroughly evaluated in the background preservation assessment). To achieve this, we generate an image with pixels of the target color using the ImageColor module from the Pillow python library. We then compute 1D histograms for the masked edited image and the target color image over each RGB channel. These histograms are smoothed using a Gaussian kernel to reduce noise. A correlation analysis is then performed between the smoothed target color histograms and the histograms of the masked image. Finally, the mean correlation score across all three RGB channels is computed and outputted as the edit-specific score."}, {"title": "8 PixLens Visual Demonstrations", "content": "We showcase the effectiveness of our evaluation benchmark in assessing various image editing models by examining both successful and unsuccessful edits to"}, {"title": "9 Preprocessing the MagicBrush Dataset for PixLens Evaluation", "content": "In alignment with the methodology outlined in the main paper, we focused our preprocessing efforts on the \"dev\" subset of the MagicBrush dataset, comprising 528 edits alongside their corresponding input and ground truth images, as well as the respective prompts. To adapt these edits to our evaluation benchmark, we manually extracted and defined the necessary attributes for each edit, namely the category, from, and to attributes, based on the provided prompts.\nAdditionally, we filtered out edit operations that did not meet our criteria. Specifically, we discarded edits in which: (i) the ground truth images inaccurately represented the intended edits, as demonstrated in Fig. 11; and (ii) the edits could not be classified into any of the predefined edit types in our evaluation framework, such as edits involving spatial relationships (e.g. \"Let the cow be closer to the farm\") or alterations related to action, viewpoint, or texture."}, {"title": "10 Supplementary Information on Subject Preservation Evaluation", "content": "Subject preservation evaluation entails computing five scores to assess the degree to which the subject is retained:\nSIFT Score \u2191: This score represents the percentage of SIFT descriptors in the images that form good matches, determined using Lowe's ratio test [21]. Typically, values below 0.075 are considered poor, while values above 0.1 are indicative of good preservation. The SIFT score primarily tracks structural preservation.\nAligned IoU \u2191: The Intersection over Union (IoU) of the aligned masks, measuring changes in the size and shape of the subject. The masks are aligned by their first row and column (i.e. so that their top-left corners coincide).\nSSIM [34] \u2191: The Structural Similarity Index Measure is a metric used to quantify the similarity between two images, focusing on luminance, contrast, and structure. It produces values ranging from 1 to 1, with 1 indicating perfect similarity.\nColor Similarity \u2191: This score represents the mean of channel-wise histogram correlations, where values close to 1 indicate high color similarity.\nPosition Score \u2193: The position score reflects the distance between the centroids of the masks, normalized by the image size. Lower values, closer to 0, signify better preservation.\nIt's important to interpret the subject preservation scores in the context of the specific edit type. For instance, the color similarity score may not be relevant when the edit prompt involves a directive like \"Make the bag red\"."}, {"title": "11 Failure Cases of Detection Models", "content": "In our analysis, we discovered that misalignments between the computed evaluation scores from our benchmark and human evaluation usually stem from the inaccuracies of the detection model [27], or from hallucination artifacts generated by the image editing models [2, 22,37]. We categorize these instances into true negatives and false positives. We believe these issues will be significantly reduced with the development of improved zero-shot detection models, though addressing this is beyond the scope of our work.\nFalse Negatives: In certain scenarios, PixLens may inaccurately assign low scores to well-executed edits, especially in the following scenarios:\nObject Replacement: Although the image editing model correctly replaces the intended object, the detection model fails to identify it (see Fig. 13).\nColor Change: When the image editing model slightly alters the color of a multi-colored object, PixLens may assign a low score due to the presence of non-target color pixels, impacting the correlation analysis."}, {"title": "12 Supplementary Information on Disentanglement Evaluation", "content": "In the generated dataset, latent representations are formed by combining attributes and objects. We have chosen four distinct attribute types for this benchmark: texture, color, style, and pattern. The available objects and attributes are as follows:\nObjects: Chair, Bowl, Plate, Nail, Bucket, Backpack, Book, Ball, Clock, Donut.\nTextures: Steel, Wood, Glass, Plastic, Wool, Cotton, Silk.\nColors: Verdant, Red, Azure, Green, Gold, Purple, Black, Pink.\nStyles: Vintage, Modern, Abstract, Realistic, Cartoon, Surreal, Expressionist, Futuristic, Retro.\nPatterns: Striped, Polka Dot, Plaid, Paisley, Floral, Geometric, Abstract, Animal Print, Checkered, Herringbone.\nTherefore, examples of prompts for latent representations include \"Vintage chair\", \"Striped nail\", or \"Red plate\".\nFor the inter-attribute section of the disentanglement evaluation, following the dataset construction outlined in the main part, a linear classifier is trained for 50 epochs. To address computational constraints, we randomly sample seeded data for each class in the final dataset. The test size used to extract accuracy is set to 30% of the total number of samples.\nIn addition to accuracy and balanced accuracy metrics, we provide confusion matrices on the test set for each model to offer deeper insights into attribute classification (refer to Figs. 15 to 18)."}, {"title": "Computing Scores.", "content": "We employ different metrics to evaluate disentanglement, depending on whether we evaluate isolated samples, samples with the same attributes, or all samples.\nIntra-sample Score: As mentioned in [28], an ideal disentanglement representation should be defined by a simple (linear) function from the code to the factors. A method to verify this is by checking if replacing attribute $a_1$ with attribute $a_2$ in the object $a_1o$ results in the same latent representation as $a_2o$. We evaluate this linearization by assessing the isolated behavior of samples, evaluating the distance between $z_{end}$ and $(Z_{start} + z_{a_2} - z_{a_1})$ and then averaging across all samples. We normalize these values by dividing them by the Euclidean norm of $z_{end}$, denoted as $||Z_{end}||_2$, to facilitate a standardized comparison."}, {"title": "Disentanglement intra-sample evaluation examples", "content": "We provide many examples for the four different attributes in Figs. 19 to 22. Each row corresponds to the first part of the disentanglement evaluation, tested in one of the models (left), and each image is the result of using the model with the corresponding prompt in the white image of reference."}]}