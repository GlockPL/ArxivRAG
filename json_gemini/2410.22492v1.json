{"title": "REALCQA-V2 : VISUAL PREMISE PROVING", "authors": ["Saleem Ahmed", "Rangaraj Setlur", "Venu Govindaraju"], "abstract": "We introduce Visual Premise Proving (VPP), a novel task tailored to refine the process of chart\nquestion answering by deconstructing it into a series of logical premises. Each of these premises\nrepresents an essential step in comprehending a chart's content and deriving logical conclusions,\nthereby providing a granular look at a model's reasoning abilities. This approach represents a\ndeparture from conventional accuracy-based evaluation methods, emphasizing the model's ability to\nsequentially validate each premise and ideally mimic human analytical processes. A model adept at\nreasoning is expected to demonstrate proficiency in both data retrieval and the structural understanding\nof charts, suggesting a synergy between these competencies. However, in our zero-shot study using\nthe sophisticated MATCHA model on a scientific chart question answering dataset, an intriguing\npattern emerged. The model showcased superior performance in chart reasoning (27%) over chart\nstructure (19%) and data retrieval (14%). This performance gap suggests that models might more\nreadily generalize reasoning capabilities across datasets, benefiting from consistent mathematical\nand linguistic semantics, even when challenged by changes in the visual domain that complicate\nstructure comprehension and data retrieval. Furthermore, the efficacy of using accuracy of binary\nQA for evaluating chart reasoning comes into question if models can deduce correct answers without\nparsing chart data or structure. VPP highlights the importance of integrating reasoning with visual\ncomprehension to enhance model performance in chart analysis, pushing for a balanced approach in\nevaluating visual data interpretation capabilities.", "sections": [{"title": "1 Introduction", "content": "Understanding and reasoning with visual data is crucial for Multi-Modal Question Answering (MMQA) systems.\nHowever, current methods do not effectively evaluate the sequential reasoning needed for complete comprehension,\nresulting in a gap between visual data representation and semantic interpretation. Current benchmarks in structured\nvisual reasoning, such as those used in [1, 2], primarily rely on one-to-one accuracy metrics, which do not adequately\ncapture the true reasoning capabilities of these models. While formal reasoning tasks like Conjecture/Theorem Proving\nand First-Order Logic (FOL) Verification have been extensively studied in structured domains like mathematics and\nnatural language processing, such formalization is largely absent in vision-language MMQA settings.\nTo bridge this gap, we introduce Visual Premise Proving (VPP), which formalizes reasoning in Chart Question\nAnswering (CQA) by breaking down the reasoning process into logical premises, each representing a step needed\nto understand a chart and draw a conclusion. This approach shifts the emphasis from mere accuracy to validating a\nmodel's ability to replicate human-like analytical processes through step-by-step sequential premise validation.\nOur contributions are threefold:"}, {"title": "1.1 Problem Definition: Visual Reasoning Chains", "content": "The formalization of generalized visual reasoning is still in its early stages and involves tackling complex, higher-order\nreasoning problems. To make this problem tractable, we introduce two key constraints:\nFirst, we limit the scope to First-Order Logic (FOL) problems. FOL verification is inherently complex and often\nNP-hard, particularly when dealing with natural language, which have inherent lexical, syntactic, and contextual\nambiguities unless manually annotated.\nSecond, we focus specifically on Chart Question Answering (CQA). The recently released RealCQA dataset [3] serves\nas an ideal testbed for this task. This dataset, which is template-based (avoiding complex language), manually annotated\n(ensuring consistency and completeness), and grounded in mathematical logic (ensuring coherent quantification), makes\nthe process of logical verification more manageable.\nBy restricting our problem to this structured and controlled environment, we establish a foundational framework for\nformalizing visual reasoning in MMQA. Our current work begins with chart QA and aims to extend to general visual\nquestion answering (VQA) and, eventually, more abstract visual reasoning in future works. The primary challenge lies\nin identifying and verifying visual premises and reasoning chains in diverse and unconstrained environments. Our work\ntakes a step in this direction by focusing on the constrained domain of Chart Question Answering, where we introduce\nthe VPP task to evaluate a model's reasoning capacity in a manner analogous to human chart analysis. This not only\nmeasures the model's final answer accuracy but also its ability to replicate the reasoning process across multiple steps."}, {"title": "2 Background", "content": "We discuss some recent works proposed for multi-modal document understanding domain. Then further specific Chart\nReasoning tasks, datasets and models."}, {"title": "2.1 Vision-Language Models", "content": "Early models, such as LayoutLM [4], used separate encoders for vision and language, combining their outputs at a\nlater stage (late fusion) to generate text from image and text inputs. Dessurt [5] proposed a novel token processing\nmechanism to provide OCR free document processing. Pix2Struct [6], similarly extend the VIT architecture with\ncombined fixed resolution image and text tokens as input and showcase larger generalizability by converting web page\nscreenshots to HTML. These advancements signal a shift towards more integrated and efficient multi-modal methods.\nRecently, the emergence of large vision-language models (LVLMs) like LLAVA, Gemini, GPT-4 etc [7, 8, 9] leverage\nsimilar novel token and early to late fusion strategies for combining visual input with large language models."}, {"title": "2.2 Visual Reasoning over Charts", "content": "Charts convey complex information through visual elements like trend lines and legends, requiring detailed interpretation\nof various visual tasks (see Figure 1). These require prediction and evaluation at each step and are referred to as 'Dense'\nin Table1. Chart-to-text tasks like QA and summarizing involve directly evaluating chart understanding tasks through\nthe text output."}, {"title": "2.2.1 Chart Datasets", "content": "Synthetic datasets like FQA, DVQA, and LeafQA, have extensive scale (180k to 300k charts) and dense text annota-\ntions(components, captions, summaries, question answers), usually leverage real world tabular data-source and plot\nimages using standard libraries like Matplotlib.\nReal-world chart datasets such as, FigureSeer, UB-PMC, and EC400k, are costly to annotate and have limited number of\nimages/annoations available. Scientific charts, from sources like ArXiV and PMC encompass a wide range of technical\nand stochastic data required for academic discourse as compared to business oriented excel charts.\nWhile synthetic charts are easy to scale they lack fidelity with real-world chart images and under-perform with even a\nslight variation in the data distribution. Digital-born scientific publications, especially pose a significant challenge due\nto their complex visual layouts and intricate details, such as dense plot elements, noisy overlapping lines and bars, high\nconcentration of math and special symbols etc.\nFigureSeer [10], consists of ~1k densely annotated line charts from arXiv publications. RealCQA consists of ~ 2Mil\nQA pairs based on 240 templates for ~28k human annotated charts first proposed by UB-PMC from pubmed central\npublications. EC400k provides line, bar and pie plots from business based sources and obfuscates text in charts, making\nany further semantic use impossible. ChartQA, C2T used for chart to text tasks are primarily based on more straight\nforward data from sources like Pew, Statista and Our World in Data(OWID).\nWe base our study on RealCQA dataset as it is the only dataset that provides a combination of challenging real-world\ndata, scientific chart images, and dense annotations of both structural elements and textual information to ensure\nthe creation of verifiable FOL reasoning sequences. Two requirements for creating a valid FOL are (i) a closed set\nof variables and (ii) a closed set of predicates. The closed set of variables includes chart components such as tick\nvalues, axis titles, legend labels, etc manually identified for the chart structure prediction task of UB-PMC. Further\nthe QA templates used for RealCQA were handcrafted by domain experts, and generate reasoning-based questions by\nperforming mathematical comparisons between a given subset of chart components a.k.a our variables. This ensures\ncompleteness of predicate logic. We provide exhaustive details over variables, predicates, premises, and our curated\nFOL sequences for each template in supplementary section."}, {"title": "2.2.2 Chart Understanding Models :", "content": "Dense prediction models like Cached [16] focus on chart structure prediction and proposed a custom local-global context\nmodelling architecture based on Swin-Transformers. Chart-Ocr[13] and SpaDen [17] focuses on using keypoints for\ntabular data extraction, by recreating plot area components like bars, lines etc and end-to-end data extraction with\nlegend mapping."}, {"title": "3 Chart FOL Premise Proving", "content": "We discuss our methodology for the proposed task. First we discuss our base ideation and task formalization, then\ndiscuss some meta charecteristics of the created data from the approach and finally our proposed evaluation metric for\nthe task.\nWe want to build a system for evaluation of visual reasoning from first principles as depicted in Figure 2. Our basic idea\nderives this theory from the way human beings read semantically arranged visual representations of data, i.e. charts.\nWe start from the input an Image $I \\in R^3$ and a question. Each question context is confined to the template variables\nand predicates. This can be systematically characterized into a series of premise-conclusions per question, visually\ngrounded in the plot area."}, {"title": "3.1 VPP Task", "content": "\"We start by defining a Premise-Conclusion Pair, where each premise logically leads to a conclusion. These premises\nare articulated in FOL, utilizing quantifiers such as 'for all' ($\\forall$) and 'there exists' ($\\exists$), alongside predicates applicable to\nthese variables. Subsequently, we establish a Chain of Reasoning. This sequence involves linking each proposition to\nthe next, ensuring that each step logically follows from the previous one as described in Algorithm 1 and the example\nin 3.2.1, and 3.2.2. The task culminates in Proving the Premises, where the final premise-conclusion, the original\nquestion-answer is derived from the interconnected premises through the established reasoning chain. Successfully\ncompleting this task confirms that the conclusion is a logical consequence of the premises. Since each premise is an\nFOL statement we can represent our final premise conclusion as natural language strings, as well as mathematical graph\nrepresentations [24] using abstract syntax trees (AST) uniquely identifying each logical sequence. Representing textual\npremises as graphs where nodes correspond to entities or concepts mentioned in the premises, and edges represent\nlogical or causal relationships between these entities enables problem formulation as a graph problem for visual logic\nverification studied independantly of the natural language complexity. While we have released the AST representation\nof our dataset we currently focus on the NLP version for this work."}, {"title": "3.2 Premise Conclusion Sequence Creation", "content": "We define our step by step process of identifying and creating visual premises for charts in Algorithm 1. We create four\ntypes of chart premises:"}, {"title": "3.2.1 Example Query Template", "content": "Is the difference between the value of Y for legendlabell at the i-th x tick and the j-th x tick greater than the difference\nbetween the value of Y for legendlabel2 at the i-th x tick and the j-th x tick?"}, {"title": "3.2.2 Deconstructed Sequential Premises", "content": "$\\forall i, j\\in [x_o, x_n], i, j\\in [Y_o, Y_m], i \\neq j : $\n$(\\{X_i, X_j\\}, Xtitle)^$\n$(\\{Y_i, Y_j\\}, Ytitle)^$\n$\\exists k_o, k_1 \\in [L_o, L_h]^$\n$\\exists (\\{L_{ko}, L_{k\u2081}\\}, Legend)^$\n$Legend\\_At(Ytitle, \\{(Y_i, L_{ko}), (Y_j, L_{ko}), (Y_i, L_{k\u2081}), (Y_j, L_{k\u2081})\\})^$\n$Value\\_At(\\{(X_i, Y_j)\\}, Ytitle, L_{ko})^$\n$Value\\_At(\\{(X_i, Y_j)\\}, Ytitle, L_{k\u2081})^$\n$((X_{i}, Y_{p}) < (X_{j}, Y_{p})) \\Rightarrow True/False$"}, {"title": "3.3 VPP Evaluation", "content": "The $Acc_{VPP}$ metric evaluates a model's ability to perform complete and correct reasoning over visual premises,\nrequiring strict logical consistency unlike traditional metrics that allow partial correctness. It requires that every\npremise in a reasoning sequence be validated as correct for the entire sequence to be considered accurate, reflecting\nthe principles of formal logic where any single error can invalidate an entire proof. This follows from the nature of\ndeductive reasoning, where conclusions are only as valid as their weakest premise. The Depth of Correct Premises\n(DCP) metric complements $Acc_{VPP}$ by providing insight into the extent of errors within incorrect sequences."}, {"title": "3.3.1 Accuracy of Proven Premises", "content": "Let S denote the total number of sequences, $P_s$ the number of premises in sequence s, and $C_{s,p}$ a binary indicator of\ncorrectness for premise p in sequence s. The $Acc_{VPP}$ metric is defined as:\n$ACCVPP = \\frac{1}{S}\\sum_{s=1}^{S} (\\prod_{p=1}^{P_s} C_{s,p})$\nThis metric assigns a maximum value of 1 for sequences where all premises are true and 0 if any premise is incorrect.\nThe multiplicative approach reflects the logical \"AND\" operation in formal proofs, where the truth value of the\nconclusion depends on all components being true. If a model predicts a final conclusion as True, all intermediate\npremises in its reasoning sequence must also be True; otherwise, an incorrect intermediate premise indicates that the\nmodel relied on an incorrect bias to reach the correct conclusion."}, {"title": "3.3.2 Depth of Correct Premises", "content": "To assess how well a model reasons through incomplete sequences, we use the Depth of Correct Premises (DCP)\nmetric:\n$DCP = \\frac{1}{S - S_{correct}} \\sum_{s=1}^{S} (\\frac{1}{P_s} \\sum_{p=1}^{P_s} C_{s,p})$, where $\\prod_{p=1}^{P_s} C_{s,p} = 0$\nHere, $S_{correct}$ represents the number of sequences where all premises are correct. The DCP metric first identifies\nincorrect sequences and normalizes the sum of correct premises by the total number of premises in each chain. This\nprovides a measure of how far a model correctly reasons within sequences that are not fully accurate, highlighting areas\nfor potential improvement in model training and reasoning formulation.\nWhile metrics like NDCG and MRR offer measures for structured output, they do not assess the depth of correctness\nwithin reasoning sequences. The DCP metric captures how much of the reasoning chain is correct even when the\noverall sequence is not, providing a nuanced understanding of the model's capacity to handle extended reasoning tasks."}, {"title": "4 Experimentation", "content": "We conduct all our experiments using a 12 layer encoder-decoder transformer model with a resolution preserving\nimage tokenizer as introduced in Pix2Struct. We employ the variant pretrained with math and chart question answering\nfollowing Matcha. All our experiments are conducted using 4XNvidia A6000 (24G) machines with a batch size of 2.\nThe model uses a fixed context length of 2048 tokens. And inference with a single text-image pair costs about ~5G\nmemory. ChartPrem model is trained using only structural premises, previous question answer strings and groundtruth\nannotation-json from the original UB-PMC task. 1 full epoch with chart qa (1.5mil) + challenge json (30k) + structure\npremises(52k) takes around 2 weeks in our setting. We train for 150k iterations(see supplementary) and use all types of\nthe premises for evaluation (158k) of our proposed task, using a subset of the extensively generated data premises(60k).\nWe hope by publicly releasing the full dataset, models and evaluation code, it will stimulate the community for further\nexploration especially using the extensive premises for instruction fine-tuning LVLM's and lead to improvement of\nmodels to parse large scale scientific publication repositories."}, {"title": "4.1 Dense Chart Parsing", "content": "We compare our model's performance on dense chart parsing tasks by using structured premises for structure prediction,\nsuch as querying 'What is the type of chart?' or 'What is the title of the dependent axis?' instead of conventional image\nclassification. We compare with the results of the corresponding tasks from the previous challenge[11], and in the\ncolumn 'Direct Prediction' we report results as reported in the challenge report by task specific vision models. The\nnext column is the Matcha-base model in a zero shot setting, and the next column 'Matcha-FT' is the same model\nwhen trained only on the RealCQA QA pairs. The next column, \u2018ChartPrem' is our proposed model trained further on\nthe SP's we created. We provide exhaustive list of our queries in the appendix. These queries are complementary to\nthe underlying binary SP's that we created for the VPP task and have text answers. Thus, training on SP's improves\nperformance on previous dense chart parsing tasks as compared to direct pixel level predictions. While zero-shot\nMatcha is only able to generalize sufficiently to three chart properties categorical labels, logarithmic axis and presence\nof legend, on fine-tuning the performance improves but the vision based task specific models still outperform. Only\non further training with the SP's we see considerable improvement. The worst performance is for dependent axis title\nwhich can include multiple complex math symbols used in scientific charts which the model might not have seen as\nmuch due to the text heavy pre-training datasets. The second worst performance is for categorical x-tick labels and this\nis due to the complex grouped and stacked bar charts which might again have math-symbol intensive labels and at times\nare quite cluttered having 40-50 tilted labels on a single chart."}, {"title": "4.2 NLP QA", "content": "\"We evaluate our premises-trained model against the standard benchmark for question answering tasks from RealCQA,\nusing the RQA9357 subset.We evaluate the task of chart question answering through multiple views. Figure 4 shows\nthe delta performance of each model with comparison to zero-shot matcha-base. These results are presented in Table\n3, where the first column describes the category of QA, the rest are performances by different models. These are\nall reported as accuracy percent over total QA pairs in that category except ranked lists which are evaluated using\nnDCG@10."}, {"title": "4.3 Visual Premise Proving", "content": "We evaluate the ChartPrem model on our VPP metrics."}, {"title": "4.3.1 FOL Solver", "content": "Our first analysis is based on the models capability to prove individual FOL premises. As shown in the first column of\nTable 4. Overall, the ChartPrem model is able to solve 46% of binary FOL performing best on structure and worst on\nMath. this is expected since the current model has not be trained on other premise types. Surprisingly it still get 56% of\ndata premises correct suggesting this style of training/inference can be effective in extracting tabular data used to plot\nthe chart."}, {"title": "4.3.2 Chain of Reasoning Evaluation", "content": "In Table 4 we observed an overall $Acc_{VPP}$ of 0.2 and a DCP of 0.395 for our model which has an impressive 68%\naccuracy on binary reasoning QA as discussed previously.\nThe $Acc_{VPP}$ value of 0.2 indicates that only 20% of the sequences in the dataset were completely correct, where every\npremise within these sequences was validated accurately. This relatively low score suggests that the model struggles\nto completely validate all premises in most sequences. Even for the high 88% FOLS accuracy of SP's the sequential\ncorrectness is only 34%. On the other hand, the DCP value of 0.395 reveals that, on average, approximately 39.5%\nof the premises are correctly validated in sequences where not all premises are correct. This metric is particularly\nrevealing, as it quantifies the extent to which the model can successfully navigate through part of the reasoning chain\nbefore making an error. A DCP less than 0.5 indicates that, in sequences with errors, the model tends to fail before\nreaching the halfway point of the premise sequence on average. This means a specific weaknesses to handle more\nextended chains of reasoning.\nTogether, these metrics suggest that while the model has some capability to process and validate individual premises, it\nstruggles with consistency and completeness in more extended reasoning tasks."}, {"title": "5 Conclusion", "content": "We curated over 10 million text descriptors for a real-world chart dataset and demonstrated that high reasoning QA\naccuracy alone does not fully measure a model's reasoning capabilities. Certain limitations also remain. First, RealCQA\nis the only source providing real-world charts with the necessary annotations. Models trained on our premises could\nbe used to generate logic sequences for other chart datasets, which would still require logic verification. Second, our\nfocus on Charts and the constraints set by first-order logic, while necessary due to the nascent stage of visual reasoning\nresearch, limits the direct applicability of our approach to broader vision tasks. Once the domain has sufficient traction\nover chart/table/document QA one could envision identification of visual logic sequences in the wild. Lastly, our work\nemphasizes the challenge in visual reasoning due to visual complexity but does not address the full spectrum of natural\nlanguage variation. We do paraphrase premises but the original questions still remain template based and addressing\nboth visual and language complexities would exceed the scope of a single study."}, {"title": "6 Appendix", "content": ""}, {"title": "6.1 Order of Reasoning : Vision Tasks", "content": "Visual reasoning as depicted in Figure ?? tasks often require a hierarchical approach that combines different orders of\nlogical reasoning. The taxonomy of reasoning used in such tasks can be broadly classified into zeroth-order, first-order,\nand higher-order reasoning. Each class has its distinct characteristics and plays a vital role in interpreting visual data\nand making decisions based on that interpretation."}, {"title": "6.1.1 Zeroth-Order Reasoning", "content": "Zeroth-order reasoning refers to the direct mapping from inputs to outputs without any inferential or logical deduction.\nIt can be represented by a function $f : X \\rightarrow Y$, where X is the input space, and Y is the output space. An example task\nin computer vision is color-based object categorization, where a predefined function directly classifies objects based on\ncolor histograms."}, {"title": "6.1.2 First-Order Reasoning", "content": "First-order reasoning involves inferential logic based on specific properties and relationships within the data. It is\ngenerally encapsulated by logical constructs such as existential quantifiers, for example, $\\exists x(P(x) \\wedge Q(x))$, meaning\n\"there exists an x such that P(x) and Q(x) are true.\" Deductive, inductive, and conditional reasoning are forms of\nfirst-order reasoning used in tasks like object recognition. Here, specific conclusions about object identities are derived\nfrom general principles or observed examples."}, {"title": "6.1.3 Higher-Order Reasoning", "content": "Higher-order reasoning encompasses more complex logical constructs, such as modal, epistemic, and analogical\nreasoning. It deals with abstract concepts like necessity, possibility, probability, and analogical parallels. For instance,\nthe modal reasoning involves considerations of necessity and possibility, denoted by $\\Box$ and $\\Diamond$ respectively, and can be\nformalized as $(\\Box (P(x) \\rightarrow Q(x)))$, indicating that it is necessarily the case that if P(x) is true, Q(x) will also be true. In\ncomputer vision, this type of reasoning is important for scene graph generation, where the relationship between objects\nin a scene is determined not only by their visual features but also by their possible interactions."}, {"title": "6.1.4 Combinations of Reasoning Orders", "content": "Combining different orders of reasoning allows for the development of robust visual reasoning systems. For example,\nanalogical reasoning can integrate elements of first-order reasoning, such as conditional statements, with higher-order\nparallels between situations. In visual tasks, this is seen in tasks like image captioning, where understanding and\ndescribing a scene involves recognizing objects (first-order) and relating them in a meaningful way (higher-order).\nTransductive reasoning, another combined approach, may involve both first-order and conditional reasoning, bridging\nconclusions from specific cases to specific cases. It is useful in visual tasks such as zero-shot learning, where knowledge\nabout seen objects is transferred to categorize unseen objects."}, {"title": "6.1.5 Formalizing Visual Reasoning in Computer Vision Tasks", "content": "The formalization of reasoning in computer vision tasks is essential to improve the interpretability and reliability of\nalgorithms. As computer vision moves toward more complex tasks such as action recognition and temporal event\nunderstanding, the integration of multi-order reasoning becomes critical. For instance, spatial reasoning combines\nfirst-order logic pertaining to the spatial arrangement of objects with higher-order reasoning that may involve temporal\ndynamics and causal relationships.\nIn summary, the taxonomy of reasoning orders and their combinations plays a foundational role in tackling various\ncomputer vision tasks. By formalizing these reasoning mechanisms, we can design algorithms that are not only more\neffective but also more transparent in their decision-making process."}, {"title": "6.2 Visual Premise Task", "content": "The premise proving task for chart question answering is a meticulous process that requires detailed validation of\nstructural annotations, generation and evaluation of premises, and the aggregation of these premises to answer binary\nquestions. This task is designed to rigorously assess the FOL reasoning capabilities of models in interpreting and\nreasoning about visual data. The proposed framework provides a structured approach to formalizing evaluation in the\nvisual domain, moving beyond domain-specific metrics to a more generalized and logical assessment methodology. We\nfirst discuss the dataset and then the specific task details."}, {"title": "6.2.1 Dataset Details", "content": "Chart Question Answers The underlying chart images and questions are taken from RealCQA we refer reader to for\nexhaustive details.\nChart FOL Details To convert a question about a chart with binary answers to first-order logic (FOL), we need to\nrepresent the relevant structural elements of the chart and the relationships between them."}, {"title": "Chart Variables", "content": "\u2022 X-axis title (Xtitle)\n\u2022 i-th X-axis tick marks (Xi)\n\u2022 Closed range of values of X-ticks [xo, xn]\n\u2022 Y-axis title (Ytitle)\n\u2022 j-th Y-axis tick mark (Yj)\n\u2022 Closed range of values of Y-ticks [yo, ym]\n\u2022 Legend labels (Legendlabel)\n\u2022 k-th legend label (Lk)\n\u2022 Closed range of values of legends, i.e., data series names [lo, lh]\n\u2022 i + 1/j + 1 represents successive i-th/j-th value of the respective variable"}, {"title": "Chart Predicates", "content": "1. $\\exists(\\{X_{io}, ..., X_{in}\\}, Xtitle)$: for all X ticks of Xtitle in C, there exist a given set of X tick values, where\nXi denotes the i-th X tick value.\n2. $\\exists(\\{Y_{jo},...,Y_{jm}\\}, Ytitle)$: for all Y ticks of Ytitle in C', there exist a given set of Y tick values, where Yj\ndenotes the j-th Y tick value.\n3.$\\exists(\\{L_{ho},..., L_{hk}\\}, Legendlabel)$: for all labels in Legendlabel in C, there exists a set of given labels, where\nLk denotes the k-th label.\n4. Value_At(\\{(X_{io}, Y_{.jo}), ..., (X_{in}, Y_{jm})\\}, Ytitle): the value of Ytitle at each data point\n(X_{io}, Y_{.jo}), ..., (X_{in}, Y_{jm}) exists in C.\n5. Value_At(\\{(X_{io}, Y_{jo}, L_{ko}), ..., (X_{in}, Y_{jm}, L_{kh})\\}, Ytitle, Legend): the value of Ytitle for the k-th\ngiven Legend, Lkh, at each data point in \\{(X_{io}, Y_{jo}, L_{ko}), ...\\} exists in C.\n6. Max_Value((Xi, Yj), Ytitle): the data point represented by (Xi, Yj) is the maximum value of Ytitle\nacross all data points in C.\n7. Max_Value((Xi, Yj, Lk), Ytitle, Legendlabel): the data point represented by (Xi, Yj) is the maximum\nvalue of Ytitle for the given Legendlabel across all data points in C.\nThese conditions ensure that the chart C has valid and complete data, as well as allowing for comparison of data across\ndifferent data points and legends."}, {"title": "Chart Premises Creation", "content": ": Our process of creating premises relies on deconstructing the binary reasoning questions\nof the RQA dataset. This is done inspired from a bottom up method of building up from first principles how a human\nbeing reads a chart. This involves certain common steps of identifying the chart structure and a unique premise per\noriginal reasoning question. Mathematical reasoning questions are deconstructed to base arithmetic steps to calculate\nthe particular value. The premises are created as individual statement, conclusion pair per the question templates. We\nuse a T5 transformer to further create 2-3 paraphrases of each for vocabulary diversity. Then for each question the\npremise templates are populated with chart specific values and generate both positive and negative cases."}, {"title": "6.3 Sample Question Templates", "content": "The following are the patterns utilized to query the charts these can found in details in the original dataset, we take the\nreasoning based questions and present here a subset of such question templates.\n'63': 'Is the difference between the value of (?P<y_title>.+) at (?P<x_i\n>.+)\nand (?P<x_j>.+) greater than the difference between any two (?P<x_title\n>.+)?'\n'65': 'Is the sum of the value of (?P<y_title>.+) in (?P<x_i>.+) and (?P<\nx_j>.+)\ngreater than the maximum value of (?P<y_title_extra>.+) across all (?P<\nx_title>.+)?'\n'59': 'Is the value of (?P<y_title>.+) at (?P<x_i>.+) less than that at (?\nP<x_j>.+)?'\n'72': 'Is it the case that in every (?P<x_title>.+), the sum of the value\nof\n(?P<y_title>.+) for (?P<legend1>.+) and (?P<legend2>.+) is greater than\nthe value of\n(?P<y_title_extra>.+) for (?P<legend3>.+)?'\n'62': 'Is the value of (?P<y_title>.+) for (?P<legend>.+) at (?P<x_i>.+)\nless than that at (?P<x_j>.+)?'\n'68': 'Is the difference between the value of (?P<y_title>.+) for (?P<\nlegend1>.+)\nat (?P<x_i>.+) and at (?P<x_j>.+) greater than the difference between the\nvalue of\n(?P<y_title_extra>.+) for (?P<legend2>.+) at (?P<xi_extra>.+) and at (?P<\nxj_extra>.+)?'\n'146': 'Does any (?P<x_title>.+) have equal interquartile range?'\n'166': 'Is the value of median of (?P<y_title>.+) at (?P<x_i>.+)\nless than that at (?P<x_j>.+)?'\n'167': 'Is the value of upper quartile of (?P<y_title>.+) at (?P<x_i>.+)\nless than that at (?P<x_j>.+)?'\n'169': 'Is the maximum value of (?P<y_title>.+) at (?P<x_i>.+)\nless than that at (?P<x_j>.+)?'\n'168': 'Is the value of lower quartile of (?P<y_title>.+) at (?P<x_i>.+)\nless than that at (?P<x_j>.+)?'\n'170': 'Is the minimum value of (?P<y_title>.+) at (?P<x_i>.+)\nless than that at (?P<x_j>.+)?'\n'18': 'Is the number of lines equal to the number of legend labels?'\n'18a': 'Is the number of lines equal to the number of mark labels?'\n'35': 'Does the (?P<y_title>.+) monotonically increase over the (?P<\nx_title>.+)?'\n'116': 'Is the (?P<legend>.+) monotonically increasing?'"}, {"title": "6.4 Premises", "content": "Structural Premises", "conclusions.": "ntemplates = {"}, "n\"SPO\": \"The type of chart is {chart_type}.\",\n\"SP1\": \"The dependant axis is labeled as {y_title}.\",\n\"SP2\": \"The independant axis is labeled as {x_title}.\",\n\"SP3\": \"The dependant axis ranges from a minimum of {ymin} to a\nmaximum of {ymax} in {y_title}.\",\n\"SP4\": \"The independant axis ranges from a minimum of {xmin} to a\nmaximum of {xmax} in {x_title}.\",\n\"SP5\": \"The independant axis is categorical with the labels {\nx_ticks}.\",\n\"SP6\": \"Tick marks corresponding to specified {x_title} values are\npresent on"]}