{"title": "FactLens: Benchmarking Fine-Grained Fact Verification", "authors": ["Kushan Mitra", "Dan Zhang", "Sajjadur Rahman", "Estevam Hruschka"], "abstract": "Large Language Models (LLMs) have shown impressive capability in language generation and understanding, but their tendency to hallucinate and produce factually incorrect information remains a key limitation. To verify LLM-generated contents and claims from other sources, traditional verification approaches often rely on holistic models that assign a single factuality label to complex claims, potentially obscuring nuanced errors. In this paper, we advocate for a shift toward fine-grained verification, where complex claims are broken down into smaller sub-claims for individual verification, allowing for more precise identification of inaccuracies, improved transparency, and reduced ambiguity in evidence retrieval. However, generating sub-claims poses challenges, such as maintaining context and ensuring semantic equivalence with respect to the original claim. We introduce FactLens\u00b9, a benchmark for evaluating fine-grained fact verification, with metrics and automated evaluators of sub-claim quality. The benchmark data is manually curated to ensure high-quality ground truth. Our results show alignment between automated FactLens evaluators and human judgments, and we discuss the impact of sub-claim characteristics on the overall verification performance.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have proven to be powerful tools, demonstrating impressive capabilities in language generation and understanding (Touvron et al., 2023; Brown et al., 2020). However, a well-known limitation of LLMs is their tendency to hallucinate, generating information that is factually incorrect or unsupported by evidence (Ji et al., 2022; Lin et al., 2021). As LLMs become more widespread, especially in applications where factual accuracy is crucial, there has been increasing research on methods to verify the factuality of LLM-generated content as well as claims from other sources.\nPrevious works on building fact-checking benchmarks focus on generating claims with a ground truth label, and in some cases provide the evidence/context to verify the claim. (Aly et al., 2021; Schlichtkrull et al., 2023). Claims are generated using human annotators (Aly et al., 2021), synthetic processes (Bayat et al., 2023; Tang et al., 2024), or considering LLM outputs on Question-Answering tasks (Wang et al., 2023). To increase the complexity of the fact-checking process, the claims are generated from source data of multiple domains & modalities, such as Wikipedia text and/or tables (Thorne et al., 2018; Chen et al., 2019; Aly et al., 2021), Web Pages (Schlichtkrull et al., 2023), Knowledge Graphs (Kim et al., 2023), online posts/chats (Wang et al., 2023; Li et al., 2023), and Q-A tasks from various domains such as statistics, finance, legal, etc (Jacovi et al., 2024a).\nThese works also provide baseline fact-checking pipelines, which typically involves two main stages: (1) the retrieval of relevant evidence using Search APIs and multimodal data-lakes (Tang et al., 2023; Schlichtkrull et al., 2023) and (2) the verification of claims based on that evidence using NLI-based, LLM-based and fine-tuned fact-verification models (Li et al., 2023). Some works also explore delegating these steps entirely to an LLM-based policy framework (Li et al., 2023; Peng et al., 2023).\nDespite this structured pipeline, most existing methods rely on a holistic verification model, where complex claims are assigned a single factuality label, often obscuring the nuanced nature of the errors or inaccuracies in the claims. In this work, we echo the sentiments of Wang et al. (2023); Liu et al. (2019); Pan et al. (2023) for a shift towards fine-grained verification of complex claims, where claims are decomposed into smaller, more manageable sub-claims that can be individually verified. We additionally emphasise on the need to provide evaluation metrics to benchmark such fine-grained verification and enrich existing benchmarks with fine-grained verification labels.\nAs shown in Figure 1, the benefits of fine-grained verification are substantial. By breaking down a complex claim into its constituent sub-claims, verification is more precise, allowing for pinpointing exact locations of factual inaccuracies. Additionally, this approach enables more transparent rationalizations and explanations, as each sub-claim can be linked directly to its corresponding evidence or lack thereof. Fine-grained decomposition also narrows the scope of evidence retrieval, making the subsequent verification process more focused and less prone to ambiguity.\nAchieving fine-grained verification, however, presents its own challenges. Decomposing a raw, complex claim into smaller sub-claims is not simply a matter of splitting it into sentences. Poorly constructed sub-claims can introduce a variety of issues: they may lose the context necessary for proper verification, lack atomicity, or misrepresent the original information by either omitting key details or introducing new ones. Ensuring the quality and verifiability of these sub-claims is therefore critical for the overall success of the verification process.\nTo address these challenges, we introduce FactLens, a benchmark designed specifically for fine-grained fact verification. FactLens provides a novel suite of metrics for evaluating the quality of sub-claim generation and incorporates automated evaluators that combine LLM-based assessments with statistical metrics. The dataset has been manually curated to ensure high-quality sub-claims.\nThrough empirical evaluation, we demonstrate that our sub-claim evaluators align closely with human judgments. Moreover, our end-to-end evaluation shows that these fine-grained scores correlate strongly with improved downstream verification performance. We also present the results of state-of-the-art models on sub-claim generation, revealing the challenges inherent in this task and the need for further research in this area."}, {"title": "2 Evaluating Sub-claims with FactLens", "content": "At the core of fine-grained verification is the decomposition of complex claims into smaller, more specific sub-claims when necessary. The accuracy of the overall verification process depends heavily on the quality of these sub-claims; errors (e.g. oversimplification, omission of important details, or incorrect contextualization) in their formulation can lead to flawed verification outcomes. To detect potential issues early on in the sub-claim generation process, we propose a set of metrics to quantitatively assess sub-claim quality across several dimensions."}, {"title": "2.1 Evaluation Metrics", "content": "The decomposed sub-claims should meet criteria below to fully realize benefits of fine-grained verification such as more precise identification of inaccuracies, enhanced transparency, and reduced ambiguity during evidence retrieval.\nAtomicity Each sub-claim should refer to a single factual unit within the original claim. This ensures if an error occurs, the inaccuracy can be precisely traced back to one or more specific sub-claims. Atomicity measures whether a sub-claim is truly atomic i.e. it focuses on only one relation between a subject and an object. For example, the claim \"The International Olympic Committee (IOC) was established on June 23, 1895, in Paris, France\" is not atomic, as it makes assertions about both the time and location of the IOC's establishment.\nThe decomposition process transforms a single claim into a list of sub-claims. It is crucial this transformation is semantically equivalent, ensuring the combined list of sub-claims faithfully represents the original claim and that each can be independently verified. To address this aspect, we propose the metrics Sufficiency, Fabrication, and Coverage.\nSufficiency To perform fine-grained verification, each sub-claim needs to be independently verifiable. This requires the sub-claims to be properly contextualized to avoid any added ambiguity. Sufficiency measures whether the sub-claim is unambiguous and sufficiently contextualized with respect to the original claim. For example, in the original claim \u201cAmanda Bauer attended the University of Cincinnati. The school's nickname is Bearcats.\", the sub-claim \u201cThe school's nickname is Bearcats\" would be considered low in sufficiency because the reference to school was omitted in the decomposition, making it ambiguous.\nFabrication The decomposition process must not introduce additional information or attempt to correct factual errors. This metric is especially important when evaluating LLM decomposers, as LLMs are known to suffer from hallucination or the generation of made-up information. For example, in the original claim \u201cSydney, the capital of Australia, is known for its Opera House and Harbour Bridge\", a sub-claim \u201cSydney is the capital of New South Wales, Australia\" is considered fabrication. Similarly, with the source claim \u201cNet sales will reach 30 million if the growth rate in 2024 is the same as in 2023\", a sub-claim \u201cThe growth rate of 2024 is the same as in 2023\" is considered fabrication because it treats a condition as a claim.\nCoverage The list of sub-claims must cover all factual assertions in the original claims, leaving no sub-claims missing. For instance, with the claim \"Amanda Bauer attended the University of Cincinnati, whose nickname is Bearcats\", if only one sub-claim is generated as \u201cAmanda Bauer attended the University of Cincinnati\", the coverage will be considered low because the assertion about the university's nickname is missing.\nAdditionally, some dimensions might not directly affect downstream verifiability and accuracy but capture some nice-to-have characteristics of the sub-claims.\nRedundancy This metric measures whether the sub-claims, as a whole, contain redundant facts. When some sub-claims are semantically repetitive, the distribution of the fact-check units might be skewed. For example, if one erroneous sub-claim is repeated three times, the final judgment could shift from \"mostly correct except for one sub-claim\" to \"more than half of the sub-claims were wrong.\" Furthermore, redundancy also introduces unnecessary costs in terms of time and computing resources.\nReadability This metric assesses how readable the sub-claims are to the end-user and imposes a penalty on unnaturally formed sub-claims.\nFor each of these metrics, the sub claims are evaluated by assigning a score of 'low', 'medium' or 'high'. For coverage and redundancy, the scores are assigned at the claim level as we consider the sub-claims as a whole. For all other metrics, scores are assigned to each sub-claim."}, {"title": "2.2 FactLens Evaluator", "content": "FactLens evaluator utilizes an ensemble method of LLM-generated evaluation scores and statistically computed scores (more details in Appendix B.1 and B.2 respectively). We use LLMs as evaluators due to their ability to scale well compared to human evaluators, as well as their reliability and knowledge across diverse domains. However, acknowledging the limitations of LLMs (Bavaresco et al., 2024; Stureborg et al., 2024), our statistical scores rely on entity and semantic-based computations.\nIn Table 1, we report the correlation scores of human annotators with the FactLens Evaluator scores, on a synthetic data (more details in Appendix B.5 and C) that has been carefully curated to cover various types of sub-claim errors. We observe fair to moderate agreement across all dimensions between human evaluations and FactLens Evaluator scores, except for sufficiency. The moderate correlation scores can be attributed to the subjectivity involved in judging such metrics. The dependency on contextual information and evidence for assessing the sufficiency of a sub-claim contributes to the lower correlation scores for this metric. Nevertheless, our results demonstrate that our computation methods for the FactLens Evaluator align moderately well with human judgments on a dataset with varying sub-claim quality."}, {"title": "3 FactLens: Benchmarking Fine-grained Verification", "content": ""}, {"title": "3.1 Dataset Creation", "content": "The FactLens benchmark contains a dataset with ground-truth sub-claims and fine-grained labels. We use 733 instances from CoverBench (Jacovi et al., 2024b), a fact-checking benchmark focused on complex claim verification sampled from diverse sources and domains, as the original claims.\nWe utilize two state-of-the-art LLMS GPT-40 (OpenAI, 2024) and LLaMA-3.1 (Meta, 2024) (details available in Appendix A) to generate candidate sub-claims and measure the quality of these generations using the FactLens evaluator. To ensure the high quality of the generated sub-claims, we engage human annotators (details provided in the Appendix C) to review all sub-claims and manually generate the ground-truth sub-claims, correcting any inaccuracies in the LLM-generated sub-claims (details in Appendix F).\nTo isolate the benefits of fine-grained verification, we do not perform the step of retrieving evidence or context for each sub-claim. Instead, we use the evidence and context provided in CoverBench, along with the generated sub-claims, to perform fact verification. This approach eliminates variability in the results that could arise from different methods and processes of evidence retrieval.\nThe next step in fine-grained verification involves using a 'verifier' model to fact-check each sub-claim against the provided evidence. In this work, we use GPT-40 mini as our verifier model across all experiments to maintain consistency.\nThis verification method enables an enriched fine-grained evaluation. To compare the performance of fine-grained verification labels (for each sub-claim) with the holistic verification label, we aggregate the fine-grained labels as false if at least one of the fine-grained labels is also false; otherwise, the label is considered true."}, {"title": "3.2 Evaluation Results", "content": "In fine-grained verification, our FactLens evaluators can act as judges of generated sub-claims, providing early revision signals if the sub-claims might lead to problematic verifications. To illustrate this, we perform an end-to-end evaluation (as shown in Figure 2) to highlight how the final verification accuracy is affected by the quality of the sub-claims. For example, we expect high-quality sub-claims to exhibit low fabrication scores. We note that for claim decompositions with a fabrication score classified as 'low,' the final fact-checking accuracy is higher compared to those with \u2018medium' or \u2018high' fabrication scores.\nSimilarly, we observe trends where sub-claims with higher atomicity, sufficiency, coverage, and readability scores demonstrate better verification performance. Although sub-claims with lower redundancy scores perform marginally better, the overall verification performance remains similar. This can be explained by the fact that highly redundant sub-claims may simply repeat claims without negatively impacting the final verification label."}, {"title": "4 Conclusion", "content": "In this paper, we introduce the benchmark FactLens to evaluate fine-grained claim verification, enriching existing benchmarks. We also identify important metrics for assessing the quality of fine-grained sub-claims and propose an automated evaluator to provide early signals of decomposition failures and evaluate claim decomposition approaches."}, {"title": "5 Limitations", "content": "In this section, we note some limitations of our work."}, {"title": "5.1 Computation of Metrics", "content": "We utilize two methods for computation of FactLens Evaluation metrics: LLM-based and statistically computed. Using LLMs as evaluators/judges is a research field being explored and improved continuously. However, existing works (Stureborg et al., 2024; Bavaresco et al., 2024) have highlighted the limitations of using LLMs in such evaluation tasks, with their scores being skewed and inconsistent.\nTo mitigate inconsistency, we provided specific instructions in the prompt (Table 3) to LLMs. we measured the agreement & correlation scores of LLMs with human judgement scores, observing fair-moderate agreement across most metrics. Furthermore, we propose our own definitions for computation of the FactLens Evaluator scores.\nHowever, we acknowledge the limitations in our computational approach as well, with it relying on the method for entity extraction, which may produce variable results. We aim to propose more concrete definitions for these metrics in future works."}, {"title": "5.2 Evidence Retrieval", "content": "To ensure there is no variability in the fact-verification task, in this work we utilize the ground truth evidence which is present in the CoverBench dataset. This allows us to solely measure the dependency of fact-verification on the claim-decomposition and sub-claim quality. With our FactLens Benchmark, we provide motivation for fine-grained labels to soon be included across fact-verification benchmarks. In future works, we hope to show how claim decompositions may also improve the evidence search & retrieval process."}, {"title": "5.3 Fact Verification Models", "content": "Previous works compared different verification models in the fact checking task. Tang et al. (2024) contrast the performance of MNLI-based, LLM-based and their proposed fine-tuned models for fact-verification. In this work, we choose to use GPT-40 mini as our only verifier model, as our aim is not to propose stronger models for verification; but to illustrate the benefits of fine-grained verification even using simpler off-the-shelf verifier models."}, {"title": "5.4 What are facts?", "content": "Fact extraction is a domain which still has a lot of room for improvement. Some previous works (Wang et al., 2023) also distinguish between factual claims, opinions and standard sentences. In this work, we utilize the CoverBench claims, which itself is obtained from benchmarks which rely on human or synthetic methods for claim generation. It can be noted that different types of factual claims may be generated with such a process, as some claims may center around a claim that is universally true (eg. \u201cEarth revolves around the Sun.\"), while other claims are dependent heavily on the context/evidence (eg. \"There are 3 players whose home state is Missouri\").\nMoreover, factual claims in real world-scenarios can often be temporal and domain-dependent in nature. For example, a claim such as \u201cThe legal drinking age is 18\" is false in a country such as the United States, however may be true in the United Kingdom, indicating domain dependence. Similarly, evidence retrieved for the claim \"Barack Obama is the President of the US\" is temporal in nature.\nWe note that our results and experiments are also based on existing benchmarks, which do not account for all real-world scenarios."}, {"title": "A Claim Decomposition", "content": "We utilize few-shot prompting to decompose a claim into sub-claims. Table 2 shows the prompt used to capture the objective of generating sub-claims which are atomic, yet contextualized with enough information preserved from the original claim.\nFor the few-shot demonstrations we sample 4 instances from the FEVEROUS dataset ensuring no overlap with the CoverBench data. Finally in the prompt, we randomly select 3 of the 4 demonstrations and shuffle the order, to ensure there is no bias.\nWe utilize GPT-40 and Llama-3.1(405B) models for this task, with temperature = 0."}, {"title": "B Evaluating Claim Decomposition", "content": "To evaluate the sub-claims, our FactLens Evaluator utilizes LLM-generated as well as statistically computed scores."}, {"title": "B.1 Prompt for Evaluating Claim Decomposition", "content": "In Table 3 we provide the prompt to LLMs used for evaluating the claim decompositions across the 6 metrics defined in Section 2.1. We provide clear instructions using which LLMs can judge the claim decompositions across different dimensions. For all metrics except \"coverage\" and \"redundancy\", the sub-claims are passed one at a time to obtain sub-claim level evaluation. \u201cCoverage\" and \"redundancy\" are used to judge the sub-claims as a whole, hence for these metrics, we provide the entire set of sub-claim decompositions for that instance. For \"atomicity\" we ask the LLM to output a label as per specific instructions, while for each of the remaining metrics we prompt the LLM to judge the instance with a \"low\", \"medium\u201d or \u201chigh\u201d score. We utilize the smaller and cheaper Ope-"}, {"title": "B.2 Statistically Computed Evaluation", "content": "To compute the FactLens Evaluation metrics using statistical methods we rely on entity-based and semantic-based calculations. Given one instance, with claim C, we extract all the (Subject, Object) pairs present within it, and from there create a list of S = subjects, and O = objects. After decomposing the claim, we obtain sub-claims c = {c\u2081, c\u2082 ,..., c\u2099 }. For all i in [1, n], we extract the subjects s\u1d62 and object o\u1d62 lists in a similar manner. We next follow these definitions to calculate the following metrics:\nAtomicity To measure atomicity, we use an entity-based computation method of comparing the number of subjects and objects involved in the sub-claim c\u1d62. If c\u1d62 revolves around only one subject and one object eg. \u201cKurt Cobain was a guitarist\", it is labeled 'atomic'.\nif len(s\u1d62) = 1 and len(o\u1d62 ) = 1,\natomicity = 'atomic'\nIf c\u1d62 revoles around one subject, but multiple objects eg. \"Kurt Cobain was a guitarist and a singer\", it is labeled 'non-atomic-1'.\nif len(s\u1d62) = 1 and len(o\u1d62) > 1,\natomicity = 'non-atomic-1'\nHowever, if c\u1d62 revolves around multiple subjects eg. \u201cKurt Cobain was a member of the band Nirvana, which was co-founded with Krist Novoselic\", it is labeled 'non-atomic-2'.\nif len(s\u1d62 ) > 1,\natomicity = 'non-atomic-2'\nSufficiency As sufficiency is a tough metric to judge using semantic techniques, we rely on LLM evaluation scores.\nFabrication To calculate fabrication, we count all subjects in each s\u1d62 that do not appear in S i.e. subjects in the original claim, and all objects in each o\u1d62 that do not appear in O i.e. objects in the original claim.\nIf the count for fab is equal to 0, i.e. no new entities present in the sub-claims, the fabrication is 'low'. Based on thresholding values, we assign scores of 'medium' or 'high' fabrication.\nCoverage To measure coverage, we check if the entities (subjects and objects) in all sub-claims c\u1d62 include all the subjects S and objects O present in the original claim.\nif \u222a(s\u1d62 ) \u2200 i = S and \u222a(o\u1d62 ) \u2200 i = O,\ncoverage = 'high'\nIn case there is no overlap, coverage = \u2018low', and for all other cases coverage = 'medium'\nRedundancy To calculate redundancy, we use semantic-based technique by measuring BertScore (Zhang et al., 2019) between each pair of sub-claims. If there is high similarity between two\nred = \u2211\u2211\u2161(i \u2260 j, BertScore(c\u1d62, c\u2c7c) > T)\nwhere T is a threshold value to find BertScore(.) similarity and n is the number of sub-claims generate for that instance.\nBased on the number of redundant claims found, we assign scores of 'low', 'medium' and 'high'.\nReadability We rely on LLM generated evaluations to measure readability."}, {"title": "B.3 FactLens Evaluation using Ensemble Method", "content": "As previously mentioned, in Table 1 we tabulate the correlation between Human scores and our LLM-generated & statistically computed scores on the synthetic with varying claim decomposition quality. Based on the results across the metrics, we propose to utilize the statistically computed scores for atomicity and coverage (as they are better correlated than the LLM-generated scores), while using LLM-generated evaluations for the rest of the metrics in our experiment results in Section 3.2."}, {"title": "B.4 Model Performance", "content": "We utilize the prompt defined in Table 2 to decompose claims using GPT-40 and Llama-3.1(405B). In Table 4 we tabulate the evaluation performance of both these models on the claim decomposition task using FactLens Evaluator. For each instance, we map the 'low', 'medium', 'high' scores ('non-atomic-2', 'non-atomic-1', \u2018atomic' for atomicity) to numerical values (1, 2, 3 respectively), and report the average for each metric across all 733 instances in the CoverBench dataset."}, {"title": "B.5 Agreement Scores on Synthetic Data", "content": "In addition to the correlation scores in Table 1, we also report agreement scores between Human annotators and FactLensEvaluator. We report the ordinal Krippendorff's Alpha score to measure the agreement. We observe fair to moderate agreement across all dimensions except 'sufficiency', which can be attributed to the dependency on contextual information and evidence to judge sufficiency of a sub-claim.\nThe synthetic data is curated using 10 claims from the FEVEROUS benchmark and generating expert-annotated claim decompositions with perturbations. For each claim we generate 7 claim decompositions: one with perfect quality sub-claims, one LLM generated sub-claim, and others using perturbations resulting in lower quality sub-claims corresponding to each of the following 5 metrics: atomicity, sufficiency, fabrication, coverage and redundancy. We exclude readability in the agreement-scores, as it is an extremely subjective metric."}, {"title": "C Expert Annotations", "content": "For human annotations on the synthetic data (Section 2.2) and the creation for the benchmark, we recruited two in-house expert annotators. The annotators are proficient in English, currently based in the United States of America, with at least a graduate-level degree. For the task, they were provided the same instructions as the prompt to LLMs in Table 3. The annotators were clearly explained the objective of the task and how their annotations would be utilized.\nThe inter-annotator agreement score (Krippendorff Alpha) is high, as tabulated in Table 6."}, {"title": "D Fine-Grained Verification", "content": "We study the benefits of fine-grained fact verification compared to verifying the whole claim in Figure 3.\nIn order to perform verification, we utilize GPT-40 mini to judge if a claim is true or false based on the evidence provided. We obtain the ground truth evidence present in the CoverBench dataset.\nIn order to show the benefits of fine-grained verification, we compare it with the method of holistically verifying the original claim without decompositions.\nIn the first case, we simply pass the original claim C along with the evidence to be verified. In the second case, we pass the claim's decompositions c = {c\u2081, c\u2082, ..., c\u2099 } one at a time. For each c\u1d62 we obtain a verification label, and then aggregate the labels for that instance. If any one sub-claim is judged false the whole instance is marked false, otherwise true.\nWe contrast the performance of the fine-grained verification with holistic verification in Figure 3. Here, we assume the number of sub-claims of an instance is indicative of how complex the claim is. We observe that as the complexity (number of sub-claims) increase, the performance of the fine-grained verification method significantly increases compared to holistic verification."}, {"title": "E Impact of Sub Claim Quality on Verification", "content": "In Figure 2, we showed how sub-claim quality impacts the end-to-end verification result. To truly understand the benefits of fine-grained decompositions and the FactLens metrics, we only consider those instances for which the number of sub-claims was greater than 1. Here, we illustrate further using qualitative examples and weights of a logistic regression model to show the influence of FactLens Evaluator Metrics on fine-grained verification.\nTo deeper understand how each metric influences the final verification, we fit a logistic regression model on the FactLens Evaluator scores on CoverBench. We specifically study the impact of the metrics atomicity, sufficiency, fabrication and coverage as we expect them to influence the final verification more than the \u201cnice-to-have\" metrics: redundancy and readability.\nWe also conducted an analysis to understand how the scores can collectively predict the final verification accuracy by fitting a logistic regression model and examining the coefficients associated with each metric. Combining the four metrics\u2014atomicity, fabrication, coverage, and sufficiency\u2014we achieved a prediction F1 score of 0.71, despite potential noise in the retrieval and verification steps.\nFrom Figure 4, we observe that fabrication has the highest weight in magnitude, implying most influence in predicting whether the final label matches with ground truth. We see a negative weight for fabrication which is expected as lower fabrication indicates better quality sub-claims which in turn should have a positive effect on verification. Atomicity, sufficiency and coverage show a positive weight, as highly atomic, highly sufficient and high coverage sub-claims are expected to have a positive influence on verification."}]}