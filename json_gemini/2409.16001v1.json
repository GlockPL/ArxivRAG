{"title": "Artificial Human Intelligence: The role of Humans in the Development of Next Generation AI", "authors": ["Suayb S. Arslan"], "abstract": "Human intelligence, the most evident and accessible form of source of reasoning, hosted by biological hardware, has evolved and been refined over thousands of years, positioning itself today to create new artificial forms and preparing to self-design their evolutionary path forward. Beginning with the advent of foundation models, the rate at which human and artificial intelligence interact with each other has surpassed any anticipated quantitative figures. The close engagement led to both bits of intelligence to be impacted in various ways, which naturally resulted in complex confluences that warrant close scrutiny. In the sequel, we shall explore the interplay between human and machine intelligence, focusing on the crucial role humans play in developing ethical, responsible, and robust intelligent systems. We slightly delve into interesting aspects of implementation inspired by the mechanisms underlying neuroscience and human cognition. Additionally, we propose future perspectives, capitalizing on the advantages of symbiotic designs to suggest a human-centered direction for next-generation AI development. We finalize this evolving document with a few thoughts and open questions yet to be addressed by the broader community.", "sections": [{"title": "1 Introduction", "content": "In the wake of the artificial intelligence (AI) revolution, industries and societies, more broadly almost everyday life errands have undergone profound transformations. The relentless evolution of AI technologies and digital learning mechanisms prompts critical inquiries into the extent of human contribution in steering these advancements. While modern AI systems showcase unparalleled prowess in automation, decision-making, and closed-domain problem-solving, the indispensability of human engagement and intervention remains unequivocal in shaping the trajectory of next-generation AI systems as it has been the source of inspiration since the very early appearances in the past [1].\nThe complexity of the intelligence problem, whose definition is yet to be established, is tempting to adapt human model in search of any type of intelligent behavior. Paradoxically, any conceivable form of artificial intelligence will inevitably originate either by mimicking human form or be a product of human intellect, at least in the near term, unless artificially generated forms begin building up newer forms of intelligences. Consequently, bias might be an intrinsic part of its initial design. However, implementation constraints, task variability, and interactions with other intelligent forms could guide humans in creating general intelligence to collaborate with [2]. Moreover, this collaboration could guide humanity to discover new inventions or new cognitive technologies, which might potentially expand the range of human thought. This virtuous feedback could eventually lead to artificially generated human-like intelligence, a byproduct of symbiotic evolution of digital and biological computational models."}, {"title": "2 Views on Intelligence", "content": "The notion of intelligence, although seemingly straightforward in everyday language to define or use, is remarkably sophisticated and complex. Attributes like expeditious learning or life-long adaptation, knowledge accumulation and representation, abstract reasoning, symbolic problem-solving, and creativity contribute to its elusive and unfathomable nature. Psychologists have long debated its definition and measurement, sparking controversies regarding human valuation and performance assessment, particularly relevant and applicable to current education system [7]. Despite some consensus within the scientific community on measuring intelligence via standarized tests [8], issues persist around their scope, potential biases, and the spectrum of skills they believe these experiments assess.\nConcurrently, the discourse on machine intelligence faces even greater complexity due to the diverse forms and abilities of machines. Defining intelligence in precise terms, applicable across a wide range of systems, poses significant challenges, especially as tasks once considered benchmarks of human intellect become now remarkably trivial for machines (like ImageNet [9] or EcoSet [10] based categorization tasks). As technology evolves, so too does our perception of intelligence, necessitating a comprehensive and enduring definition that transcends sensory limitations, environmental constraints, and specific hardware types, yet remains practically achievable.\nThe quest for defining intelligence is a multifaceted challenge, labyrinthine in its nature and diverse in its interpretations. Intelligence includes a spectrum of cognitive abilities, adaptive behaviors, and problem-solving skills exhibited predominantly by living organisms. At its center, intelligence refers to the capacity to acquire knowledge, reason abstractly (including counterfactual thinking), solve problems, learn from exploratory experience (through interactions with the three dimensional world model), and adapt to unseen circumstances through generalization, quickly without necessarily using many examples (one-shot learning [11]). Intelligence might manifest itself in diverse forms across species, each tailored to suit the ecological niche and evolutionary context of the organism. Human intelligence, often pointed at as the cornerstone of cognitive evolution, is characterized by its complexity, versatility, and capacity for symbolic thought, language, and cultural and societal intercourse. However, defining human intelligence as the ultimate form of intelligence risks anthropocentric bias and overlooks the diverse manifestations of intelligence found in other organisms [12] or more recently in self-learning machines [13].\nScholars have proposed various models and frameworks to elucidate the multidimensional nature of intelligence. Gardner's theory of multiple intelligences [14] posits that the broad notion of intelligence should comprise distinct modalities, including linguistic, logical-mathematical, spatial, musical, bodily-kinesthetic, inter-personal, intra-personal, and naturalistic intelligences. Each intelligence represents a domain of expertise and proficiency, offering insights into the diverse talents and capabilities of individuals. On the other hand, recent developments in deep learning have enabled rather constrained but various forms of intelligences capable of performing complex tasks and sometimes exhibiting human-like cognitive abilities [15]. This pluralistic account of intelligence allows a rethinking of developmental trajectory with necessary cross modality interactions. However, replicating the full breadth and depth of human intelligence remains elusive, as deep learning systems often excel in narrow domains but struggle with generalization, common-sense reasoning, and context-dependent understanding. Furthermore, ethical considerations surrounding the learning systems raise concerns about the societal impact of new forms of intelligences, including issues of autonomy, accountability, and bias. As we venture into the realms of various forms of artificial and/or non-human intelligence, navigating these challenges requires careful consideration of ethical, social, and philosophical implications.\nOn a lower scale, Brooks [16] highlights that in the early stages of AI research, intelligence was defined as tasks that highly educated scientists found challenging, such as chess, symbolic integration, proving mathematical theorems, and solving complex word algebra problems [17]. Tasks that young children effortlessly accomplished, like visually distinguishing between objects like a coffee cup and a chair, walking on two legs without losing balance, or navigating from their bedroom to the living room, were not initially regarded as activities requiring intelligence. Thus, he argues that a more informal notion of intelligence should be the sort of staff that humans do, pretty much all the time. Winston [18] delineates the objectives of Artificial Intelligence as twofold: the development of practical intelligent systems and the comprehension of human intelligence, arguing in favor of a mutually beneficial enterprise. On the other hand, Legg and Hutter [19] argues that despite the diverse set of definitions, it might still be possible to construct a formal encompassing definition of intelligence (e.g. universal intelligence), which is believed to have strong connections to the theory of optimal learning agents [20]. Optimal learning agents are artificial agents designed to learn and make decisions in a way that maximizes their performance or efficiency in achieving specific goals or tasks. These agents utilize various algorithms and strategies to improve their learning process and decision-making abilities over time through interaction and continuous feedback.\nStudying the intelligence and its critical role in the evaluation of society is yet another challenge to face. Intelligence tests are typically used to provide summative scores for the assessment of academic and vocational success. However, it is argued that these tests could be limiting our understanding of intelligence. For example, the absence of precise and systematic scoring methods that leverage the multiple contributions of a response is one of the ways in which the current use of psychometric tests could be limiting our understanding of intelligence [21]. This limitation is believed to be due to the ways we design and execute tests and handle the test outcomes later for analysis."}, {"title": "3 Origins and the Path leading to AHI", "content": "Neither the origin of intelligence nor a theory for this origin have been adequately considered and addressed by the past literature. In fact, all definitions of intelligence seem to have a strong subjective component. Whether it be Turing or Newell [22], their descriptions of intelligence, involving knowledge-level considerations, beliefs, and intentions to a system, is strictly based on human judgments making both of them subjective. On the contrary, Steel [23] supports an alternative definition which highlights the continuum nature of it which progressively evolves from chemical systems to actual living organisms. Through precise definition of evolving complex adaptive systems, the quest for origins is reduced down to addressing two fundamental questions, namely, (1) how the brain demonstrates its remarkable behavioral plasticity and (2) how a symbolic layer might have emerged early in the development [23-25].\nBased on our past human experience and historical account that intelligence most likely emerged from active perception and tight/attentive mechanisms that entail 3D interactions with the real-world phenomena through multi-sensory input [26,27]. In other words, the biological intelligence requires to exist in multi-dimensional feedback-enabled open environments to emerge and develop [27]. We can think of different models of natural intelligence [14]\u00b9, but one of the observable models that is immediately accessible and intriguing is the human intelligence which has been leveraged by machine intelligence innovators (particularly deep learning community) in the past for so many years to inspire novel architectures, training methodologies, learning techniques, etc. Multi-model sensory input processing is essential to human intelligence as there is considerable evidence to support their joint progression early in childhood [28,29]. In fact, recent work on the computational modellings of visually guided haptic sensory systems advocate for the value of multimodal fusion to improve the visual experience and performance [30]. As different sensory systems can be linked to distinct intelligent behaviors, growing number of studies seem to suggest that their synchronization and fusion lead to more accuracy in completion of specific tasks [31], which demonstrates the potential for integrating more general and non-biological intelligences. One of the most compelling pieces of evidence supporting this finding is the success of multimodal foundation models, which have demonstrated that a missing modality can be compensated for by the presence of other modalities as part of their emergent behavior [32].\nInspired from the past studies, we characterize AHI to describe trichotomous, consisting of human-assisted, human-inspired or human-independent machine intelligences. Since each one of these trejectories are rooted based on human capabilities and human intelligence, we coined the name AHI to describe all. Just like found in the integration of multimodal sensory inputs, this paradigm's objective is to improve system performance by leveraging the distinct learning mechanisms of both humans and various AI approaches, which might result in complementarity in many aspects of processing and eventually in the final task performance. Additionally, the involvement of humans or understanding tasks performed by the human brain can help regulate ethical considerations, symbolic computations, and emotional aspects of the underlying functional systems."}, {"title": "3.1 The current SoTA and Taxonomy", "content": "Current AI systems do not allocate enough emphasis to the definition of the intelligence problem, nor they seem to necessarily align in their objectives with different forms of human and/or non-human intelligences. Most early literature was inclined to adopt the model of human intelligence from a pragmatic perspective: The layered models of mind [33] (see also AlexNet [34] or VGGNet [35] etc.), a representation of layered cortical structures and hierarchical activations of neural populations, inspired inductive biases which eventually got encoded into the model architectures. They typically define the elevation to this intelligence as getting the job done faster, more efficient and accurate than most humans do. While engineering the machines, the evolution of the intelligence (and its definition) will not necessarily be following the same trajectory and hence be compatible with the model of human intelligence and its development. Although many of the most popular architectural models, such as residual networks [36\u201340], primarily designed to address a hardware or an algorithm issue, later has been shown to possess biological parallels both physiologically and functionally [15]. There are already many attempts in literature exploring this implicit comparison with respect to human systems [41\u201343].\nIn this work, we hypothesize that three branches of research can be leveraged to develop AI models in observation of human intelligence. One that could directly be inspired by the human brain, putting emphasis on its structural and functional connectivity, elements of developmental trajectory, and learning mechanisms. Secondly, different machine models can assist each other to create a more collective intelligence which will naturally lead to a human-assisted rather than a human-inspired model development. In this paradigm, models could be developed in such a way that functional complementarity is ensured based on the behavioral aspects of human intelligence as a reference. Finally, models can be developed independently, distributed emergence of independent intelligences, exploring freely the novel ways of task completions and discoveries of new implementation techniques that would otherwise be impossible to implement on biological hardware."}, {"title": "3.2 Human-inspired, Human-assisted, Human-Independent AI", "content": "The distinction between \"human-inspired\" and \"human-assisted\" AI pertains to the extent of human involvement and influence in the development and operation of artificial intelligence systems. A summary of this taxonomy is illustrated geometrically in Fig. 2, along with the temporal progression (directed arrows) of each paradigm in the evolutionary trajectory of the work that has been done in the last few decades (from Past to Future). More specifically, assuming stable centroids of the studies conducted around human and machine intelligences, a human-inspired trajectory will expand future works around these centroids, thereby naturally leading to more commonalities, i.e., increased biological plausibility due to everlasting incremental research around the same or similar ideas. In the human-assisted approach, the trend is reverse i.e., the future overlap between machine and human intelligences tend to shrink allowing hybrid designs to emerge and have better capacity to effectively address intelligence problem. Human assistance is not used as part of the training processs but rather used as the essential component of the system. On the contrary, human-independent trend moves the centroids of the literature works in different directions while their expansion is likely to continue, potentially creating more overlaps between the two distinct intelligences. Fig. 3 illustrates these options in detail and in the following subsections, we shall closely explore verbally each one of these options."}, {"title": "3.2.1 Human-inspired AI: Neuroscience meets Machines", "content": "In Human-inspired AI, machines are designed to mimic or replicate aspects of human intelligence, behavior, or complex cognitive processes. Part of this endevour could be inspired by the brain mechanisms responsible for processing information and navigating attention to complete complex tasks. Human-inspired AI aims at understanding and replicating the neural mechanisms behind human cognitive processes, such as learning, common sense reasoning, perception, and critical decision-making."}, {"title": "3.2.2 Human-assisted AI: Hybrid Systems", "content": "In Human-assisted AI, models are developed with the primary goal of augmenting or enhancing human/machine capabilities rather than replicating machine or human intelligence. Human-assisted AI leverages AI technologies to assist machines or humans in performing tasks more efficiently, accurately, or autonomously. Human-assisted AI, as a bidirectional enterprise, can be characterized as the continuous interaction between human-helping-machines and machines-helping-humans. However, fully realizing the potential of human-AI collaboration requires to address important challenges. First, the necessary conditions that support complementarity \u2013 where human-assisted AI performance is expected to surpass that of AI alone for a given task, requiring humans to recognize when to augment AI that complement critical decision-making processes. Second, accurately assessing human mental models as well as the extent to which humans are prone to failures are crucial, which also brings out the issue of understanding the effects of design choices in human-AI interaction, such as the timing and amount of Human assistance to avoid bias.\nPrimary example of this trend is the foundation models that can interact with humans, such as most recent advanced LLMs [74] (GPT4 [6], Llama [75], etc.) that begin transforming the software industry by providing assistance writing the bulk of the required code as a downstream task, and still subject to thorough validation [76, 77]. This approach (especially the former reference [76]) emphasizes the significance of collaboration between humans and machines, with AI systems providing support, guidance, or automation to complement human skills (critical thinking, reasoning, navigation of complex decision processes) and expertise. Other examples include AI-augmented tools for object recognition, memory enhancement [78], data analysis, medical diagnosis [79], language translation, and robotic assistance in manufacturing or logistics [80]. Yet another hot topic could be mission critical applications where humans and machines could perform tasks using different strategies and hence end up having distinct failure patterns. This complementarity can be utilized as a fusion to bolster overall accuracy performance [81].\nHuman mind organization and sensorimotor systems could be useful for developing computational systems in various ways. One remarkable example from vision is the leverage of eye tracking and analysis of gaze direction, which has the potential to uncover a wealth of information about internal cognitive states. For example, it is argued that gaze can reveal the underlying attentional patterns [82]. More broadly, gaze information could be useful from two standpoints. It can help AI develop an attention mechanism to effectively cope with the information-rich world and determine significant cognitive features [83]. In fact, given the amount of training data required to learn such an adaptive attention process, it may be easier for AI agents to learn attention directly from human gaze data. Secondly, it can help AI interfaces to perceive and understand gaze movements to create naturalistic interactions with humans [84,85]. For instance, gaze-based control mechanisms could allow users to interact with AI systems without physical input devices (keyboards, mouse, etc.), fostering a more immersive and responsive interaction.\nIn summary, while human-inspired AI seeks to replicate human intelligence, human-assisted AI focuses on enhancing human capabilities through collaboration with AI technologies. As collaboration and interaction between these two forms of intelligence occur, both will inevitably influence each other, leading to potential evolution and changes in their characteristics over time. However, both approaches originally have distinct goals and applications, and the choice between them would depend on the specific needs, objectives, and ethical considerations of AI development and deployment."}, {"title": "3.2.3 Human-independent AI: The rise of Open Intelligence", "content": "In our taxonomy, this branch is still considered as human-centered AI because the studies in this category are still framed based on human intelligence as a reference model. For instance, although AlexNet [34] is inspired by the visual cortex and its layered functional structure, designed to mimic human visual perception and processing, the proposed optimization algorithm backpropagation (also referred as back-prop) is not necessarily human-inspired; it is more inspired by how computational constraints shape and efficiently utilize resources, along with mathematical convenience such as sticking to the same set of weights/coefficients in both forward and backward passes [86]. In fact, back-prop is considered to be an optimization method which spreads information across all parameters of the neural network making the network less modular [55] (also see Section 4.3). Hence one can argue that the development of AlexNet is not necessarily categorized as human-inspired but rather neuroscience-inspired at the very high-levels of abstraction subject to SoTA silicon hardware constraints and explainability.\nThe main motivation behind Human-independent AI is twofold: (1) In pursuit of an open intelligence that is independent of human intelligence could potentially be a better model for the natural world we live in as human intelligence seems to have missing elements to tackle important issues of the world such as less advanced predictive capabilities for extended time and range or the inherent limitations of convolution/pooling operations executed by the visual system. This is corroborated by the most recent transformer models which lifts off some of the important inductive biases that previous generation CNNs adopted. Second observation to this idea is the ever growing data scales as machines having less inductive bias would require to build such biases based on the voluminous data. (2) In any reliable/efficient AI system development, we would expect a joint symbiotic life between the intelligence and the hardware that runs it. The current silicon hardware and Von-Neuman architecture is not necessarily the right home for the optimal implementation of human intelligence. This might guide us to think of other forms of intelligences that could optimally run on the available hardware architectures, building on the few-decade experience of distributed microprocessor history of human construct. For more elaborate discussion on the effect of hardware on the implemented intelligence, please see 5.3.1.\nPerhaps it might be more accurate to frame human-independent AI as an effort which characterizes humans as irrelevant to the next generation large-scale AI development. There is a growing inclination in the community believing that the independent design will benefit from some of the findings that come out through interactions with the outside world (experimentations etc.) which might well be counter-intuitive and cannot be adequately represented by the data intrinsics. However, these designs are still bound to subjective evaluations by human-credited metrics and might be compared to human performance to improve their explainability. After all, explainability is a human-construct and only meaningful if it helps with understanding the true nature of phenomenon."}, {"title": "4 Brain-inspired Information processing", "content": "Brain-Inspired Information Processing refers to building computational systems and designing architectures that emulate the brain's structure and functions to achieve advanced cognitive and data processing abilities. This interdisciplinary field draws on neuroscience, cognitive science, and artificial intelligence to develop models and systems that implement brain-like processes for tasks like multimodal perception, learning in development, and critical decision-making/building commonsense. The overall picture is illustrated in Fig. 5 where the elements of brain-inspired information processing as well as the context is introduced in which this paradigm is utilized [87]. To get a grasp on how the brain operates, various neuroimaging techniques are utilized, including but not limited to Electroencephalography (EEG) and Magnetoencephalography (MEG) which measure the brain's electrical and magnetic activity, offering high temporal resolution for tracking dynamic neural activity [88]. Functional Magnetic Resonance Imaging (fMRI), on the other hand, provides detailed spatial resolution by monitoring blood flow changes, helping to map which brain regions are involved in particular cognitive functions [89]. These tools are crucial for revealing the neural dynamics and connections that underpin complex cognitive processes. To identify neural signatures for different brain functions, machine learning algorithms are trained on brain signal patterns associated with specific tasks or mental states. By analyzing the relationships between brain regions, statistical methods (such as random graph theory) help model brain networks and pinpoint crucial regions/nodes involved in processing information [90].\nThe informative features extracted from the biomedical data are typically used to build computational models that mimic the way our brains process information. Translating these measurements into computational frameworks that are truly brain-inspired could be quite challenging though. For example, neural networks can be structured to reflect the hierarchical nature of the brain, with different layers corresponding to various levels of processing found in, say, the visual cortex [89, 91, 92]. Finally, these computational frameworks are refined by comparing them against real brain data, perhaps at higher abstraction levels, ensuring they not only mimic brain activity but also can generalize to new, unseen tasks. However, the resemblance is always subject to questions as it is hard to completely translate brain operations to algorithms and implement them on silicon. By blending neuroimaging with AI and statistical analysis, researchers mainly aim to create brain-inspired models that capture, at least to some extent, the core manifestation of human cognition. These models can then be applied to fields ranging from the most popular foundation models such as LLMs to autonomous systems. In fact, brain-inspired data processing could guide foundation models to embrace emergent behaviours more rapidly as architectural similarity is more likely to demonstrate functional similarities [93] (See also Section 7 for further discussions). This approach seeks to bridge the gap between biological intelligence and machine intelligence, pushing toward more flexible and robust AI systems.\nTo fully adapt brain-inspired processing, it is crucial to tackle several fundamental questions, which can be grouped, to our count, into four key areas: (1) the possibility of reverse engineering of human cognitive abilities, (2) the distinctive features of the human brain, (3) the mechanisms through which our brains achieve modularity and robustness, and (4) exploration of gaps in current state-of-the-art approaches that prevent us from reaching human-level AI capabilities."}, {"title": "4.1 Reverse engineering a human skill", "content": "One of the main pillars of brain-inspired research is to understand how hard it is to fully grasp the various emergent behaviors that leads to a form of intelligence in the brain. This question dates back to 1980s and the difficulty of adapting it depends on the developmental trajectory of that skill. The premise of the past work was that we typically expect the difficulty of reverse-engineering any human skill to be roughly proportional to the amount of time that skill has been evolving in living beings including animals [94]. For example one of the oldest human skills is perception which is largely unconscious and so appear to us to be effortless. Examples are abundant: Anything to do with attention, visualization, motor skills, tactile system, social skills and so on. The hypothesis is that we should expect skills that appear effortless to be difficult to reverse-engineer. The dual to this hypothesis is skills that require effort may not necessarily be difficult to engineer at all."}, {"title": "4.2 What makes humans special and/or not so special?", "content": "Humans exhibit unique strengths and limitations in tasks like recognition, perception, detection, and search, which are shaped by both our biological makeup and cognitive abilities. First and foremost, humans are not adept at \"detection\u201d tasks like blur identification or identifying misaligned lines. Especially without focused attention, their performance significantly degrades, making it difficult to excel in completing such tasks successfully. Additionally, humans struggle with \"hidden correlation\" tasks, as their ability to estimate the joint probability distribution of a large set of random events is very limited [97]. This limitation arises from the cognitive load and complexity involved in processing and integrating multi-modal sources of imprecise information simultaneously.\nHowever, human cognitive system is extremely good at trying/searching (visual search) [98] and finding out things\u00b2 (in other words, quite good at ontologies \u2013 a set of concepts and categories in a subject area or domain that shows their properties and the relations between them) leading to good performance in languages. Humans also have a sparse but adequate understanding of physical reality and causality which leads to data processing efficiencies in their generalization capacities. Due to this valuable feature, their learned world-models usually turn out to be reasonable and in congruent with the required tasks. They are also good at grounding the language with mental simulation. It is widely recognized that the way humans perceive the world is organized around objects - mostly up to 3rd dimension and some lausy understanding of the 4th dimension. These objects act as fundamental building blocks for a variety of higher-level cognitive processes, including language, planning, and reasoning. The notion of the world being comprised of separate parts that can be processed autonomously and combined in countless ways enables humans to make generalizations that go well beyond their direct experiences.\nIn the context of mental simulation, the ladder of causation is pertinent to our discussion [99,100]. We should consider our question with respect to the three rungs of the ladder: (1) The initial stage involves the extraction of meaningful correlations from real-world representations through a process of observation. This endeavor aims to estimate the conditional expectations of events, thereby facilitating the formation of crucial associations. (2) The subsequent stage entails engaging with the environment using an accurate model of the world which might be formed partially in the previous rung. Interactions in this phase result in alterations to the characteristics of objects that are inherently linked or correlated, either directly or through other dependent or confounding variables. Given that such interventions occur over time, they engender a sequence of actions and reactions conducive to causal reasoning. (3) The final stage of the ladder, termed counterfactual thinking, necessitates envisioning alternative scenarios, evaluating them, and adjusting accordingly. This aspect is distinguished by its requirement for imagining changes and their potential repercussions, without necessarily engaging in active intervention. Achieving counterfactuality underscores one of humanity's most demanding cognitive abilities: predictive reasoning."}, {"title": "4.3 A case for Modularity and Robustness", "content": "The brain network modularity can be defined as the degree to which functional brain networks (connectivity) are divided into special subnetworks. Modularity is assumed to be beneficial to develop a robust brain against internal as well as external perturbations. Biological individual differences seem to suggest the mandatory modular organizations for the robustness of persistent activity to various perturbations [101]. Modularity does not typically form in computational models with standard training regimens and architectures. However, modularity can be enforced through various approaches such as changing the training or the optimization process. Biomimetic training for instance is shown to demonstrate more robust behaviour under out-of-distribution scenarios. In addition, more biologically plausible optimizations provide robustness and seem to allow the emergence of invariant representations more quickly [102].\nOne way to achieve robustness in recognition tasks is to retain information in redundant and distributed manner. A widely accepted assumption is that distributed information leads to robustness to internal and external perturbations, whereby lost or corrupt information in one brain region can be compensated for by redundant information in other regions [103]. Redundant modular representations might naturally emerge in neural network models that could learn robust dynamics [101] as long as such representations can be retained in later stages of training. Indeed, developing modularity might allow learned features to be frozen and the rest of the other hardware could be used to learn new tasks allowing learning rates to change over time for different parameters of the neural system [104,105]. As can be seen, modularity has the potential to lead a system to be more robust to external perturbations. However, not every robust system necessarily manifests any sort of modularity in their operational characteristics."}, {"title": "4.4 Missing elements towards achieving Human-level AI", "content": "Here is a corollary that could well easily be established through a rigorous search on the AI research in the last decade or so. It summarizes the main elements missing in any design towards achiving human-level capabilities [27,95, 106].\nWithout higher level abstractions, counterfactuality, predictive reasoning and explicit compositionality, it is almost impossible to achieve or get to human-level AI in the near future.\nTools for few-shot generalization (without exponentially increasing the data size for training) [107], must be totally different than what AI community is allocating their attention on/and may require the next breakthrough in science and engineering. One approach to achieve such immediate generalization could be \u201cbiological plausibility\u201d [108,109] which is often exercised by the research community in diverse ways including the modifications of neuronal structure-architecture [109], inductive bias, training regimen [110], activation patterns etc.\nBiological plausibility in neural network design and training refers to how closely artificial neural networks (ANNs) mimic the structure and function of biological neural systems, like spiking neurons in the human brain [111]. This involves replicating neural architecture, using local learning rules akin to Hebbian learning [112] or even the advanced BCM theory [113], incorporating modification thresholds, learnable activation functions and dynamics, similar to biological neurons, achieving energy efficiency and provide plausible explanation for synaptic scaling [114]. However, biological plausibility must be understood in the context of the scale at which learning is considered. For instance, if a deep network is viewed as an unrolled recurrence in the brain, an argument can be made in favor of its biological plausibility [15]. Conversely, when examining fundamental operations such as convolutions and back-propagation for parameter optimization, an argument can easily be made against its biological plausibility [86, 115].\nNetworks that are biologically plausible can improve interpretability, robustness, adaptability, and energy efficiency, reflecting some of the advantageous properties of biological systems. However, biological plausibility does not necessarily ensure behavioral similarity, which is the resemblance in outputs and responses to those of biological systems. Achieving behavioral similarity requires addressing additional factors such as the complexity of tasks, generalization capabilities, and learning efficiency. While biologically plausible networks may show some behavioral similarities due to their design, achieving full behavioral mimicry involves broader considerations of intelligence, learning, and adaptability beyond just structural and functional resemblances."}, {"title": "5 Challenges and Perspectives in Human-Level AI Development", "content": "There are many challenging and often obscure obstacles in the development of human-level AI. Addressing these issues requires interdisciplinary collaboration across diverse fields such as computer science, neuroscience, psychology, philosophy, and ethics. This multifaceted endeavor comes with perhaps the most intriguing unsolved problems of science and engineering."}, {"title": "5.1 Challenges in the development of human-level AI", "content": "One of the most fundamental issues in the deveopment of human-level AI is concerned with the identification of mechanisms that lead to commonsense reasoning in complex systems such as humans. For one thing, AI systems that possess a reasonable understanding of commonsense knowledge are expected to make sense of everyday situations and make intelligent decisions based on implicit knowledge that humans take for granted. As indicated in [27], common sense in AI are expected to build representations of core physical constructs such as time, space, causality, and establish interactions between physical objects and humans. Through the leverage of abstraction and compositionality, they should support reasoning techniques capable of handling complex and incomplete knowledge. More fundamentally, they also need to possess a human-inspired learning system (also see section 4.4) that continually incorporates new knowledge from various interactions and sources, enabling deep understanding and general intelligence. While carefully putting together bits and pieces, structural resemblences to human anatomy should also bring together explainability and interpretability components, which should naturally fall in place due to the joint consideration of learning and architecture. Creating AI that can explain its decisions and actions in a human-understandable manner is very important for building trust between machines and humans. The mutual trust is especially important in mission-critical applications such as healthcare and autonomous vehicles, where understanding the rationale behind AI-driven decisions can be a matter of life and death.\nAI systems must hold up ethical principles and societal values as part of their design and ensure that algorithms running on them adopt fair, transparent, and unbiased behavioral characteristics. This undertaking involves navigating issues such as privacy, accountability, and the long-term impact of AI on biological intelligence as well as employment and other dynamics of society at large. It is not a surprise that the next generation AI and its implementation will generate significant societal impacts, necessitating moderation and legislative regulations.\nTraining foundation AI models and enhancing their ability to generalize knowledge across different domains and tasks through specialized fine tunings, also known as transfer learning, is another major challenge. This adaptation to novel circumstances involves enabling foundational AI systems to learn novel concepts from only a few samples of data (a.k.a. few-shot learning [107, 124]), leveraging previously acquired knowledge to solve new problems more efficiently. However, as is the case with modeling many cognitive functions of the brain with regards to real-world problem solving, there is no widely accepted mathematical model of transfer learning that connects out-of-distribution generalization to the minimum number of model parameters required for updates. Despite easy to articulate, accomplishing such a goal involves many genuine techniques that would make machines look like humans. Of such mechanisms, meta-learning [125] and self-supervised learning [126] are the two other critical areas of human-inspired research. Developing AI systems that can learn to learn, adapt, and improve over time through meta-learning techniques are desirable properties of the future generations of this technology. The overall goal of learning involves designing algorithms that can automatically discover effective strategies and apply them to new tasks, allowing AI to become more autonomous, capable and self-directed over time.\nIntegrating AI systems with physical bodies (embodiment) and sensory capabilities to enable interaction with the physical world is another significant challenge towards human-level AI. This concept, known as embodied intelligence [127], involves understanding how intelligent behavior emerges from the complex interplay between an agent's hardware components, implementation of the software logic and its dynamic environment. Achieving this integration is crucial for performing complex tasks requiring learning based on sensorimotor skills [128].\nAs the AI systems begin to replace many critical positions of human occupancy in making mission-critical and life-changing decisions, ensuring that they are safe, secure, and robust against adversarial attacks, system failures, and unintended intrusions gradually become a major focus of today's research. This includes developing techniques for verifying and validating AI behavior and mitigating potential risks associated with their deployment. Maybe the missing elements could be as far-stretched as developing self-awareness and characterization of self-assessments. The conceptualization of consciousness and self-awareness in AI systems remains one of the most widely studied and elusive goals [129, 130]. Investigating the nature of consciousness and exploring whether it is possible to create artificial entities that possess subjective experiences such as feeling secure or safe, similar to humans is still largely unknown. Although there have been attempts at mathematical modeling [131,132], these have not yet sufficiently explained the complete conscious experience that humans present."}, {"title": "5.2 Transferability of human-specific attributes to machines", "content": "In order to transfer a human specific attribute (Ethics, Aesthetics, Coinciousness, Morality, Humor, Causality or Trust), it is a priority to define these attributes using a precise language. Without a clear systematic description, it would be hard to comprehend and even have a common consensus on their meanings. However, creating a clear understanding entails improving the precision of their dictionary definitions [133], which might not be an easy task to perform in a mathematical sense. In fact, having precise definition for biological and metabolic events could sometimes be pretty hard to frame, which are analyzed historically using a branch of mathematics known as information theory [134]. In fact, imprecision in the development of human attributes could be beneficial in accurate and robust decision making processes."}, {}]}