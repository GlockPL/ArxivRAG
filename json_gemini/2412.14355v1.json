{"title": "ENABLING REALTIME REINFORCEMENT LEARNING AT SCALE WITH STAGGERED ASYNCHRONOUS INFERENCE", "authors": ["Matthew Riemer", "Gopeshh Subbaraj", "Glen Berseth", "Irina Rish"], "abstract": "Realtime environments change even as agents perform action inference and learn-ing, thus requiring high interaction frequencies to effectively minimize regret. However, recent advances in machine learning involve larger neural networks with longer inference times, raising questions about their applicability in realtime systems where reaction time is crucial. We present an analysis of lower bounds on regret in realtime reinforcement learning (RL) environments to show that minimiz-ing long-term regret is generally impossible within the typical sequential interaction and learning paradigm, but often becomes possible when sufficient asynchronous compute is available. We propose novel algorithms for staggering asynchronous inference processes to ensure that actions are taken at consistent time intervals, and demonstrate that use of models with high action inference times is only constrained by the environment's effective stochasticity over the inference horizon, and not by action frequency. Our analysis shows that the number of inference processes needed scales linearly with increasing inference times while enabling use of models that are multiple orders of magnitude larger than existing approaches when learning from a realtime simulation of Game Boy games such as Pok\u00e9mon and Tetris.", "sections": [{"title": "1 INTRODUCTION", "content": "An often ignored discrepancy between the discrete-time RL framework and the real-world is the fact that the world continues to evolve even while agents are computing their actions. As a result, agents are limited in the types of problems that they can solve because the speed at which they can compute actions dictates a particular stochastic or deterministic time discretization rate. Agents that take infrequent actions require some lower-level program to manage behavior between actions, often through simple policies like remaining still or repeating the last action. Ideally, intelligent agents would exert more control over their environment, but this conflicts with the trend of using larger models, which have high action inference and learning times. Consequently, as typically deployed with sequential interaction, large models, which are often found to be essential for complex tasks, increasingly rely on low-level automation, reducing their control over realtime environments. This paper examines this discrepancy and explores alternative asynchronous interaction paradigms, enabling large models to act quickly and maintain greater control in high-frequency environments.\nFigure 1a shows the standard sequential interaction paradigm of RL. In this setup, the agent receives a state from the environment, learns from the state transition, and then infers an action. Each process must be completed before the agent can process a new state, limiting the action frequency and increasing reliance on low-level automation as the model size grows. In contrast, Figure 1b illustrates the asynchronous multi-process interaction paradigm we propose. Our key insight is that even models with high inference times can act at every step using sufficiently many staggered inference processes. Similarly, sufficiently many asynchronous learning processes can maintain rapid updates without blocking progress, despite high learning times. This work formalizes and empirically tests the benefits and limitations of this approach, making the following contributions:\n1. We formalize how the choice of a particular time discretization induces a new learning\nproblem and how that problem relates to the original problem in Definition 1."}, {"title": "2 REGRET DECOMPOSITION IN REALTIME REINFORCEMENT LEARNING", "content": "Background - Sequential Interaction: Most RL research focuses on agents interacting sequentially with a Markov Decision Process (MDP) [71; 94] $M_{seq} = \\langle S, A, p, r\\rangle$, where S is a set of states, A is a set of actions, r(s, a) is a reward function with outputs bounded by rmax, and p(s'|s, a) is a state transition probability function. Agents take actions based on a policy $\\pi_{\\theta}(a|s)$ that maps states to action probabilities parameterized by \u03b8. An unrealistic implicit assumption of this setting is that the time between decisions is fixed and only depends on the MDP. It is also unrealistically assumed that the environment can be paused while the policy generates an action a from state s.\nAsynchronous Interaction Environments: The standard MDP formalism lacks a crucial element for realtime settings where the environment cannot be \"paused,\" and the agent interacts with it asynchronously, as described by Travnik et al. [102]. In this case, it is necessary to define the environment's behavior when the agent has not selected an action. We believe the most general solution is to use a preset default behavior if there is no available action at by the agent at time-step t. This behavior follows a ~ \u03b2(s), where a \u2208 A\u00df is possibly from a different action space than A, requiring p and r to be defined over AU AB. Now we can define an asynchronous MDP $M_{async} = \\langle S, A, p, r, \u03b2 \\rangle$ as an extension of a sequential MDP $M_{seq}$ with the addition of the default behavior policy \u03b2. Note that \u1e9e does not need to be non-Markovian, because the state space should be defined to include any intermediate computations needed to generate the actions of \u00df. Defining the default behavior as a policy is no more than a useful interpretation of what happens and is equivalent to saying the environment follows a Markov chain $p^{\\beta}(s'|s)$ when no action is available where $p^{\\beta}(s'|s) := \\sum_{a\\in A_{\\beta}} p(s'|s, a)\\beta(a|s)$ with expected reward $r^{\\beta}(s) = \\sum_{a\\in A_{\\beta}} \\beta(a|s)r(s,a)$.\nTime Discretization Rates: The real environment evolves in continuous time, so we must define time discretization rates to describe each component of the agent-environment interface in discrete steps. We treat the environment step time as a random variable TM with sampled values T\u043c ~ T\u043c and expected value $\\overline{\\tau}_M := E[T_M]$. Similarly, the inference time of the policy for a single action is another random variable Te with sampled values $T_\\theta$ ~ $T_{\\theta}$ and expected value $\\overline{\\tau}_\\theta := E[T_\\theta]$."}, {"title": "Definition 1 (Induced Delayed Semi-MDP)", "content": "Any choice of random variables TM, T1, and Te applied to an asynchronous MDP Masync induces a delayed semi-MDP Mdelay :=\n(S, A, p, r, \u0432, TM, T1, Te) where the semi-MDP decision making steps \u012b associated with the actual decisions of the agent \u03c0 happen after $\\lceil T_I/T_M \\rceil$ steps t in the ground asynchronous MDP Masync. The semi-MDP is delayed with respect to Masync because semi-MDP actions \u1fb6\u1fd6 \u2208 A generated by \u3160 are equivalent to actions that are delayed by $\\lceil \\tau_{\\theta}/\\tau_M \\rceil$ in Masync such that $\\pi_{\\theta}(\\tilde{a}_\\tau|S_\\tau) = \\pi_{\\theta}(a_{t+\\lceil \\tau_{\\theta}/\\tau_M \\rceil}|S_t)$ where $s_\\tau = s_t$. If $\\lceil \\tau_{\\theta}/\\tau_M \\rceil > 1$ the transition dynamics are $p^{\\beta}$ and reward dynamics are $r^{\\beta}$ for $\\lceil \\tau_{\\theta}/\\tau_M \\rceil - 1$ steps in Masync until $a_{t+\\lceil \\tau_{\\theta}/\\tau_M \\rceil}$ is applied."}, {"title": "Theorem 1 (Realtime Regret Decomposition)", "content": "The accumulated realtime regret $A_{realtime}(T)$ over time \u0442 of a delayed semi-MDP $M_{delay}$ relative to the oracle policy in the underlying asynchronous MDP $M_{async}$ can be decomposed into three independent terms.\n$A_{realtime}(\\tau) = \\Delta_{learn}(\\tau) + \\Delta_{inaction}(\\tau) + \\Delta_{delay}(\\tau)$ (1)\n$\\Delta_{learn}(\u03c4)$ is the regret experienced even in sequential environments as a result of learning and exploration. The lower bound in the worst case is:\n$\\Delta_{learn}(\\tau) \\in \\Omega(\\sqrt{\\tau/\\overline{\\tau}_I})$ (2)\n$\\Delta_{inaction}(\u03c4)$ expresses the regret as a result of following \u1e9e rather than optimal actions in Masync. The lower bound and upper bound in the worst case is:\n$\\Delta_{inaction}(\\tau) \\in \\Theta((\\tau/\\overline{\\tau}_I) \\times (\\overline{\\tau}_I - \\overline{\\tau}_M)/\\overline{\\tau}_M)$ (3)\n$\\Delta_{delay}(\u03c4)$ expresses the regret as a result of the delay of actions by \u03c0 in the underlying asynchronous Masync. The lower bound in the worst case is:\n$\\Delta_{delay}(\\tau) \\in \\Omega((\\tau/\\overline{\\tau}_I) \\times E[1 - (p_{minimax})^{\\lceil\\overline{\\tau}_\\theta/\\overline{\\tau}_M\\rceil}])$ (4)"}, {"title": "3 ASYNCHRONOUS INTERACTION & LEARNING METHODS", "content": "Figure 3 highlights key differences between the standard sequential RL framework and the asyn-chronous multi-process framework we propose. In the sequential framework, interaction and learning delay each other. In contrast, in the asynchronous framework that we propose, actions and learning can occur at every step with enough processes. However, actions are delayed and reflect past states, which may limit performance in some environments. Note that staggering processes to maintain regular intervals is essential. For example, if all inference processes took a deterministic amount of time with no offset between them, all additional actions in the environment would be overwritten with no benefit from increasing compute. Meanwhile, with staggering we can experience linear speedups."}, {"title": "3.1 BACKGROUND: STAGGERED ASYNCHRONOUS LEARNING", "content": "Parallel vs. Asynchronous Updates: Learning from a transition, i.e., computing gradients, usually takes longer than inference. Thus, performing learning in separate processes is crucial to avoid blocking inference [106], especially for models with a large number of parameters. For this use case, one might be tempted to consider parallel learning processes to increase the effective batch size without increasing wall-clock time per batch as this avoids wasted computation. Indeed, parallel updates are better for training large language models when final performance and compute efficiency are most important. In contrast, asynchronous learning can produce updates even faster than learning from a single transition, making the model more responsive to exploration. However, lock-free asynchronous approaches risk overwriting updates, potentially wasting computation that does not contribute to final performance. Our focus is on maximizing responsiveness in large models, not necessarily compute efficiency. So even overwritten updates are not wasted with respect to regret."}, {"title": "3.2 OUR NOVELTY: STAGGERED ASYNCHRONOUS INFERENCE", "content": "In Remark 1 we highlighted that T1 is fundamentally limited by Te for sequential interaction, which results in persistent regret even as time goes on when T\u0e2d > TM. We will now highlight two novel algorithms for staggering inference processes that can lead to a reduction in T1 when the number of inference processes N\u012b are increased. Algorithm 1 is capable of scaling the expected interaction time with the number of processes by 71 < min(max/N1, TM) where Tmax is the maximum encountered value of Te. Meanwhile, Algorithm 2 is capable of scaling the expected interaction time with the number of processes by 71 = min(70/1,\u012aM). Both algorithms can eliminate inaction.5"}, {"title": "Remark 2 (Inaction of Asynchronous Interaction)", "content": "For any Te when \u03c0 and Masync interact asynchronously with staggering algorithms 1 or 2, there is a value of the number of inference processes N such that for all N1 \u2265 7, \u2206inaction(T)/\u315c \u2192 0 as time goes to \u0442 \u2192 8."}, {"title": "4 RELATED WORK", "content": "Realtime interaction: Previous work such as Travnik et al. [102] has considered the asynchronous nature of realtime environments. However, we are not aware of any prior paper that has formalized the connection between asynchronous and sequential versions of the same environment as we have. Travnik et al. [102] highlight the reaction time benefit of acting before you learn, and Ramstedt & Pal [73] highlight the reaction time benefit of interacting based on a one-step lag. Meanwhile, the interaction frequency of both of these approaches are limited by sequential interaction and thus the drawback highlighted in Remark 1 also applies to them.\nDesigning the interaction rate: Farrahi & Mahmood [21] examined how the choice of T1 affects the learning performance of deep RL algorithms in robotics. They found that low T1 complicates credit assignment, while high T1 complicates learning reactive policies. Karimi et al. [36] proposed a policy that executes multi-step actions with a learned T1 within the options framework, which may aid in slow problems where credit assignment is challenging. However, this approach does not address the action delay issue we focus on and may worsen it by committing to multiple actions based on a delayed state. Our policy, defined in the semi-MDP framework (Definition 1), relies on a low-level policy \u1e9e, similar to the options framework [95]. The key difference is that \u1e9e cannot be modified, preventing intra-option learning and thus making it impossible to improve \u1e9e even when it is sub-optimal. Thus we would rather minimize the use of \u1e9e i.e. minimize inaction.\nReinforcement learning with delays: Reinforcement learning in environments with delayed states, observations, and actions is well-studied. Typically, delays are treated as communication delays inherent to the environment [103; 9]. In contrast, we focus on delays resulting from our computations, which are under our control and part of agent design. Our formulation of delay as part of regret is novel due to this unique focus. Common methods address delay by augmenting the state space with all actions taken since the delayed state or observation [8; 37; 63], but this is infeasible for us since these actions are not available when computation begins. Instead, our approach aligns more with"}, {"title": "5 EMPIRICAL RESULTS", "content": "To show that our proposed method does indeed provide practical benefits for minimizing regret per second with large neural networks in realtime environments, we perform a suite of experiments to validate the theoretical claims made in the previous sections. Our experiments include:\n\u2022 Question 1: an evaluation of the speed of progress through a realtime game strategy game where constant learning is necessary to move forward when action inference times are large.\n\u2022 Question 2: an evaluation of episodic reward in games where reaction time must be fast to demonstrate that asynchronous interaction can maintain performance with models that are multiple orders of magnitude larger than those using sequential interaction.\n\u2022 Question 3: an evaluation of the scaling properties of Algorithms 1 and 2 to demonstrate that the needed number of processes to eliminate inaction, N, scales linearly with increasing inference times Te and parameter counts |0|.\n\u2022 Question 4: an evaluation of the scaling properties of round-robin asynchronous learning [50] to demonstrate that the number of processes needed to learn from every transition also scales linearly with increasing learning times and parameter counts |0|.\nImplementation Details: For our experiments, we implemented the Deep Q-Network (DQN) learning algorithm [61] within our asynchronous multi-process framework, using a discount factor of y = 0.99, a batch size of 16, and an Adam learning rate of 0.001 for the sparse reward Game Boy games (with 0.00001 used for the comparatively dense reward Atari games). A shared experience replay buffer stores the 1 million most recent environment transitions. For preprocessing, we down-sampled monochromatic Game Boy images to 84x84x1, similar to Atari preprocessing [61]. Following the scaling procedure previously established by [13] and [91], we used a 15-layer ResNet model [20] while scaling the number of filters by a factor k to grow the network. The model sizes correspond to: k = 1 (1M parameters), k = 7 (10M), k = 29 (100M), and k = 98 (1B). Models were deployed on multi-process CPUs using Pytorch multiprocessing library on Intel Gold 6148 Skylake cores at 2.4GHz, with one core per process and multiple machines for models using > 40 processes. See Appendix A for further detail regarding our experimental setup and a discussion of limitations. As large models are known to be difficult to optimize in an online RL context, we also tried training common 1M models with an equivalent amount of added delay as the larger models to perform a sanity check. Our initial experiments indicated performance was similar.\nRealtime Environment Simulation: To run a comprehensive set of scaling experiments that would not be feasible with real-world deployment, we need a simulation of a realistic realtime scenario. Towards this end, we considered two games from the Game Boy that are made available for simulation as RL environments through the Gymnasium Retro project [65]. We implemented a realtime version of the Game Boy where it is run at 59.7275 frames per second such that \u0442\u043c = 1/59.7275 and with \"noop\" actions executed as the default behavior B. This exactly mimics the way that humans would interact with the Game Boy as a handheld console [104] and matches the setting in which humans compete over speed runs for these games. We also leveraged three Atari environments [7] run at 60 frames per second with \"noop\" actions executed as \u1e9e to mimic the way that humans interact with the Atari console in the real-world. These are ideal settings for addressing our core empirical questions."}, {"title": "5.1 FASTER PROGRESS THROUGH A REALTIME GAME WITH CONSTANT NOVELTY", "content": "Pok\u00e9mon Blue: Pok\u00e9mon Blue is a valuable environment for our study due to its long play through time and constant novelty over many hours of play. Acting quickly is not a necessity to complete this game as it lets the agent dictate the pace of play, but better players are still differentiated based on their speed of completing the game. Indeed, the game has a large community of \"speed runners\" aiming to complete milestones in record times, with even the fastest milestones taking multiple hours [93]. It is an interesting domain for our study because acting quickly is only beneficial to the extent that the agent displays competent behavior, so action throughput alone will not lead to better results when the quality of play correspondingly suffers. Because Pok\u00e9mon Blue is known as a challenging exploration problem that perhaps even exceeds the scope of previous deep RL achievements [30], we divided the game into two settings based on expert human play: 295 battle encounters (Figure 9a) and 93 catching encounters (Figure 9b). Agents are deployed in these settings and must complete each encounter (by winning a battle or catching a Pok\u00e9mon) before progressing.\nQuestion 1: Can asynchronous approaches achieve faster progress in a realtime strategy game where constant learning is necessary to move forward even when action inference times are large?"}, {"title": "5.2 MAINTAINING PERFORMANCE IN GAMES THAT PRIORITIZE REACTION TIME", "content": "Tetris: We also explore the game Tetris (Figure 9c) that presents a different kind of challenge for our agents where even more of a premium is put on reaction time. In Tetris, the player will lose the game if they wait indefinitely and do not act in time. While a slow policy can eventually win in Pok\u00e9mon, despite taking longer than necessary, a policy that does not act timely cannot progress in Tetris as new pieces must be moved correctly before they fall on existing pieces.\nQuestion 2: Can asynchronous interaction help for games that prioritize reaction time as |0| grows?"}, {"title": "5.3 COMPUTATIONAL SCALING OF ASYNCHRONOUS INTERACTION AND LEARNING", "content": "Question 3: How does N* from Remark 2 scale with T\u0473 and the number of parameters |0|?\nFigure 7: We measure N for Algorithms 1 and 2 when the Game Boy is run at the standard frequency using an e-greedy DQN policy at \u20ac = 0 and \u0454 = 0.5. Figure 7a shows that N scales roughly linearly with Te in all cases, as expected for effective staggering (Remark 2). Figure 7b also demonstrates that N scales roughly linearly with |0|. When \u20ac = 0, the variance in Te is very small and Algorithms 1 and 2 thus perform the same. When \u20ac = 0.5, the variance in Te is high because sampling random actions is very fast, showcasing predictably superior performance for Algorithm 2. The performance of Algorithm 1 is unaffected by stochasticity in To, but the values of Te are.\nNotation for Asynchronous Learning: We also would like to consider the compute scaling properties of round-robin asynchronous learning [50]. We now assume that the time to learn from an environment transition can be treated as a random variable Tc with sampled values TC ~ Tc and expected value TL := E[TC]. No will denote the number of learning processes such that all Nc > N include at least one transition learned from for each environment transition.\nQuestion 4: How does N scale with T\u3125 and the number of parameters |0|?"}, {"title": "6 DISCUSSION", "content": "Implications for Software: One difficulty in deploying RL environments within realtime settings is limitations in current APIs such as Open Al gym [10]. In realtime settings, the environment needs to run in its own separate process so that it is not blocked by the agent (just like the real world). We hope that the contribution of our code and environments can thus help jump-start the community's ability to conduct research on this important setting. A new paradigm of agent-environment interaction is needed where 1 process is used for the environment with N\u2081 processes dedicated for inference and Nc processes dedicated for learning, depending on specific resource constraints.\nImplications for Hardware: Our paper demonstrates the benefits of asynchronous computation within realtime settings and thus advances in hardware that enable this will serve to amplify the impact of our findings. Memory bandwidth is a primary bottleneck in allowing for asynchronous computation with current hardware. So any improvements i.e. in GPU memory bandwidth, in bandwidth across nodes, or the number of GPUs or CPUs per node will make the scalability of asynchronous approaches increasingly viable. Taking a longer-term perspective, hardware architectures that move beyond the von Neumann seperation of memory and compute, such as so called \"neuromorphic\" computing, will also serve to enable larger scale asynchronous computation like we see in the brain.\nBigger Models: In this paper, we have taken a deeper look at RL in realtime settings and the viability of increasing the neural network model size in these environments. Our theoretical analysis of regret bounds has demonstrated the downfall of models that implement a single action inference process as model sizes grow (Remark 1) and we have proposed staggering algorithms that address this limitation for environments that are sufficiently deterministic (Remark 2). Our empirical results playing realtime games corroborate these findings and demonstrate the ability to perform well with models that are orders of magnitude larger. While conventional wisdom often leads researchers to think that smaller models are necessary for realtime settings, our work indicates that this is not necessarily the case and takes a step towards making realtime foundation model deployment realistic."}, {"title": "A.1 POK\u00c9MON RESULTS WITH SMALLER MODEL SIZES", "content": "Due to space restrictions we were not able to include our experiments for the Pok\u00e9mon battling and catching domains with |0| < 100M in the main text. We provide these results for |0| = 1M in Figure 11 and |0| = 10M in Figure 12. As expected by our theory, the difference between asynchronous interaction and sequential interaction is expected to be less when the action inference time is low. In this experiment, the sequential interaction baseline acts every 5 steps at 1M parameters, 8 steps at 10M parameters, and 70 steps at 100M parameters. For the Pokemon game it is expected that there is not much improvement acting at every step as it is well known to speed runners that in common circumstances the game could take as many as 17 frames to respond to non-noop actions in which time intermediate actions are \"buffered\" and not yet registered in the environment [92]. This is a fact commonly exploited by speed-runners to allow for them to manipulate the RNG of the game and execute actions in a \"frame perfect\" manner despite natural human imprecision when it comes to action timing. As a result, the overall results in this domain are in line with expectations as differences are most significant when inference times are greater than this 17 environment step threshold. Moreover, the results reported in Figure 11 and in Figure 12 are mean performances along with 95% confidence intervals, calculated from 10 runs with different random seeds."}, {"title": "A.2 PERFORMANCE AS A FUNCTION OF NON-NOOP ACTIONS", "content": "In Figures 13a and 13b we observe that the difference between progress through the game for sequential and asynchronous algorithms per non-noop action is not statistically significant in the Pok\u00e9mon battling and catching environments. As such, our asynchronous algorithm takes full advantage of the increased throughput of non-noop actions taken in the environment."}, {"title": "A.3 RAINBOW EXPERIMENTS", "content": "We now also include results with Rainbow [26] used for learning rather than the DQN on Atari. We again use a standard k = 1 filter size at the standard 15 convolutional layer ResNet depth now with the Rainbow model for learning based on model inference times that correspond to those used for the DQN. Note that Rainbow performing worse than the DQN for Boxing even without delay was reported in Table 5 of the original paper [26]. Our implementation of Rainbow draws heavily from the following repository https://github.com/davide971/Rainbow/tree/master with hyperparameters taken from that repository other than the 0.0001 learning rate corresponding to our other Atari experiments. These experiments yet again showcase the superior ability of staggered asynchronous inference to maintain performance at larger model sizes and inference times."}, {"title": "B PROOFS FOR EACH THEORETICAL STATEMENT", "content": "Our proofs rely on the following core assumptions, restated from the main text:\n1. The environment step time can be treated as an independent random variable TM with sampled values tm ~ TM and expected value \u012b\u043c := E[TM].\n2. The environment interaction time can be treated as an independent random variable T1 with sampled values T1 ~ T1 and expected value 11 := E[T1].\n3. The action inference time of the policy can be treated as an independent random variable Te with sampled values Te ~ Te and expected value Te := E[To].\n4. Asynchronous learning can learn from every interaction with Mdelay\u00b7\nIssues with the proof in an earlier workshop version of the paper [85] have been corrected here."}, {"title": "B.1 DEFINITION 1", "content": "Most of Definition 1 just recaps the dynamics of how the agent interacts with an asynchronous ground MDP following assumptions 1-3 about the nature of that interaction. All that is left to show is that this"}, {"title": "B.2 THEOREM 1", "content": "To prove Theorem 1 we will demonstrate the validity of each equation of the theorem following the order of presentation in the main text.\nEquation 1: By definition Alearn (7) and Ainaction (7) must be independent contribution to the total regret because learning regret is only incurred when acting in the environment following and inaction regret is only incurred when not acting in the environment and thus following the default behavior policy B. The interaction frequency does not depend on the parameter values of \u03c0 as they change following from the independent random variable assumption. Even when regret from learning is eliminated and regret from inaction is eliminated there is still another independent source of regret that persists Adelay (T) reflecting the lower reward rate of the best possible policy in acting over Mdelay in comparison to the best possible policy acting over Masync.\nEquation 2: The worst case lower bound for the standard notion of regret arising from the need for learning and exploration has been established as \u2206learn(T) \u2208 \u03a9(VT) where T denotes the number of discrete learning steps taken in the environment. Given that we learn from every asynchronous interaction with the environment following assumption 4, then the regret as a function of 7 scales with the expected number of discrete environment steps as a function of Ti.e. E[T(T)] = T/TI because TM and T1 are independent. Therefor, \u2206learn(\u03c4) \u2208 \u03a9(\u221aT/\u012b\u2081). As noted in the footnote in the main text, this analysis equally applies to the known \u2206learn(T) \u2208 \u00d5(\u221aT) minimum upper bound [68].\nEquation 3: The worst case lower bound on Ainaction(T) is derived by considering a worst case environment with two actions a\u2081 and a2 and two states 81 and 82 where the default behavior \u03b2 takes its own action a3 at every state. The reward provided is 1 at s\u2081 and 0 at 82. The next state is s1 regardless of the state if either a\u2081 or a2 is taken and 82 if a3 is taken. In this environment the optimal reward rate is 1 and the reward rate when following \u1e9e is 0.0. The expected number of times a3 is taken during 7 seconds in Masync is then 7/\u30121 \u00d7 (T1 - TM)/\u012bm because \u00df is used for (T1 - TM)/T1 percent of ground MDP actions and the expected number of actions taken over 7 is \u0442/\u0442\u043c when T\u043c and T1 are independent. Therefor, \u0394inaction(\u03c4) \u2208 \u03a9((T/\u30121) \u00d7 (\u30121 - \u3012\u043c)/\u0442\u043c) for this particular environment. Meanwhile, the expected inaction regret is also upper bounded by Ainaction(T) \u2264 max \u00d7 (T/T1) \u00d7 (\u30121 \u2013 TM)/\u012aM where rmax is the maximum possible reward per step because by definition the agent cannot incur regret from inaction when it is acting in the environment. Therefor, we have demonstrated that Equation 3 holds.\nEquation 4: The worst case lower bound on Adelay(T) is derived by considering a worst case environment with n states S = {$1,..., Sn} and n actions A = {a1,..., an} where the default behavior \u1e9e takes its own action ao at every state. If the agent takes the action a\u017c corresponding to the state si for any i \u2208 {1, ..., n} the agent receives a reward of 1, otherwise it receives a reward of 0. The agent stays in the current state si regardless of the action with probability Pminimax and goes to the next state in the cycle si+1 with probability 1 Pminimax where Pminimax \u2265 1/n because the sum over next state probabilities must equal 1. After state sn the agent returns to state 81. The probability of staying in the current state for k consecutive environment steps is (Pminimax)k and in the limit as n \u2192 \u221e this is also the probability that the agent is in the current state k steps later. If the agent is not in the current state k steps later, the agent will apply a sub-optimal action based on"}, {"title": "Tighter Version of Equation 4", "content": "While the definition of Pminimax in the main text holds for the counter example above, this example relies on the fact that transitioning states actually has an impact on the reward of a policy and its value function. In general, it does not matter if the state changes if the change does not impact the optimal policy. So a tighter version of Pminimax would be defined over a \u03c0*-irrelevance state abstraction * of the state space rather than the ground state space, following the terminology of [52]. In a * abstraction every abstract state in * (S) has an action a* that is optimal for all the ground states i and j in that abstract state. As a result, \u03a6\u03c0* (si) = \u03a6\u03c0* (sj) implies that maxa\u2208 A Q\u2122* (si, a) = maxa\u2208A Q\"* (sj, a) For example, a tighter version of Equation 4 could be written with $P_{minimax} := \\min_{s\\in S,a\\in A} \\max_{\\phi_{\\pi^*}(s_i) \\in \\phi_{\\pi^*}(S)} \\sum_{\\pi^*}(s_j)=\\phi_{\\pi^*}(s_i)} P(S_j|s, a)$"}, {"title": "B.3 REMARK 1", "content": "We restate the derivation of the remark from the main text, filling in a bit more detail for clarity. When \u03c0 and Masync interact sequentially, we must have \u03c4\u03c4 > \u03c4\u03b8, so \u2206inaction(T) \u0395\u03a9(\u03c4/TI \u00d7 (TI \u0442\u043c)/\u0442\u043c) \u0454 \u03a9(\u0442/\u0442\u04e9\u0445 (\u0442\u04e9 - \u0442\u043c)/\u043c). This implies that even as \u30f6 \u2192 \u221e, the worst case regret rate Arealtime(T)/\u03c4\u2208\u03a9(\u2206inaction(T)/\u0442) \u2208\u03a9((\u0442\u04e9 - \u0442\u043c)/\u012b\u043c\u0442\u04e9) following from Theorem 1."}, {"title": "B.4 REMARK 2", "content": "Interaction Time of Algorithm 1: At any point in time, by definition max < max as the estimated maximum must be less than or equal the current maximum. Therefore, with N\u2081 equally spaced processes, the interaction time must be T1 = [max/N1/\u0442\u043c] \u00d7 \u0442\u043c and Algorithm 1 ensures equal spacing between processes whenever max does not change. Regardless of the changing value of max, we can also bound the individual interaction time steps T1 \u2264 [Tmax/N1/TM] \u00d7 \u0442\u043c and correspondingly the global average 71 < Tmax/N1 when \u30121 > TM and T1 = TM otherwise.\nInteraction Time of Algorithm 2: As T \u2192 \u221e, Te \u2192 Te due to the law of large numbers and thus \u03b4\u03c4 \u2192 0, which implies that additional waiting times become zero in the limit. Therefore, with N\u2081 equally spaced processes, the interaction time must be T1 = [10/N1/TM] \u00d7 \u0442\u043c and Algorithm 2 ensures equal spacing between processes whenever Te does not change and \u03b4\u03c4 = 0. Thus, as \u2192 \u221e the global average is T1 = Te/N1 when \u30121 > TM and T1 = TM otherwise.\nBringing it Together: Algorithm 1 is capable of scaling the expected interaction time with the number of processes by \u012b1 \u2264 min(Tmax/N1,\u012aM) where Tmax is the maximum encountered value of T\u04e9 as T \u2192 \u221e. This then implies that for N\u2081 > N = [tmax/\u012bm], T1 = \u012bm. Algorithm 2 is capable of scaling the expected interaction time with the number of processes by T1 = min(To/N1, TM) as T\u2192\u221e following the law of large numbers. This then correspondingly implies that for N\u2081 > N = [To/TM], TI = TM\u00b7"}, {"title": "B.5 COMPARISON WITH ACTION CHUNKING APPROACHES", "content": "The action chunking approach learns a policy that produces multiple actions at a time with a single inference step i.e. a policy \u03c0\u03b8 (at, ..., at+k|St) that produces k actions at a time. Clearly action chunking does not address the root cause of regret from delay Adelay(T) that our asynchronous inference framework also suffers from as k steps of delay is built directly into the policy. However, it is less clear on the surface how the action chunking approach relates to learn (7) and Ainaction(T).\nDoes action chunking eliminate \u2206inaction(T)? Action chunking could eliminate regret from inaction if \u0442\u0430\u0445/\u043a = \u0442\u043c. But this is not possible for any value of max with sufficiently large k because t\u0442\u0430\u0445 must depend on k as the size of the outputs produced scales linearly with k. So action chunking can"}, {"title": "Does action chunking impact \u2206learn(T)?", "content": "A hidden term excluded from our bound of \u2206learn(T) in the main text is a dependence on the size of the action space |A| such that learn (7) \u2208 \u03a9(\u221aTA/F1) [31"}]}