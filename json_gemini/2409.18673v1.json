{"title": "Exploiting Motion Prior for Accurate Pose Estimation of Dashboard Cameras", "authors": ["Yipeng Lu", "Yifan Zhao", "Haiping Wang", "Zhiwei Ruan", "Yuan Liu", "Zhen Dong", "Bisheng Yang"], "abstract": "Dashboard cameras (dashcams) record millions of driving videos daily, offering a valuable potential data source for various applications, including driving map production and updates. A necessary step for utilizing these dashcam data involves the estimation of camera poses. However, the low-quality images captured by dashcams, characterized by motion blurs and dynamic objects, pose challenges for existing image-matching methods in accurately estimating camera poses. In this study, we propose a precise pose estimation method for dashcam images, leveraging the inherent camera motion prior. Typically, image sequences captured by dash cameras exhibit pronounced motion prior, such as forward movement or lateral turns, which serve as essential cues for correspondence estimation. Building upon this observation, we devise a pose regression module aimed at learning camera motion prior, subsequently integrating these prior into both correspondences and pose estimation processes. The experiment shows that, in real dashcams dataset, our method is 22% better than the baseline for pose estimation in AUC5\u00b0, and it can estimate poses for 19% more images with less reprojection error in Structure from Motion (SfM).", "sections": [{"title": "I. INTRODUCTION", "content": "In recent decades, the widespread adoption of dashboard cameras has led to the recording of a significant amount of roadside videos every day. These recordings serve as vital resources for reconstructing traffic scenes and offer promis- ing data sources for production and updating high-definition maps as well as spatial comprehension [1]. Leveraging these dashcam videos for the production and updating of high- definition maps holds the potential to significantly reduce mapping costs and enhance update frequency. However, the absence of integrated positioning sensors, such as GNSS receivers, IMUs, or LiDAR technology, presents a significant challenge to the direct utilization of dashcam videos for location-based applications [2].\nTo address this challenge, we propose a novel framework for pose estimation in dashcam imagery. This framework leverages inherent camera motion prior to enhance the ac- curacy of image matching. Since the dashcams are mounted on moving cars, these dashcams exhibit strong motion prior such as forward movement or lateral turns. These motion prior constrain the possible epipolar geometries of dashcam image pairs [3]. As shown in Fig. 1, by approximating these camera motions, we are able to locate keypoints along"}, {"title": "II. RELATED WORK", "content": "Instead of solving relative camera poses from established correspondences, a number of recent works [9], [10], [11] directly regress relative camera poses of a given pair of images from their features. These methods leverage the strong motion prior exhibited in daily life image captur- ing [12], which constrains the motion of the camera within a predictable range or trajectory even in unseen scenarios [13]. The dashboard cameras typically show strong motion prior such as moving forward or turning left/right. Thus we propose to learn to encode such motion prior with a neural network similar to GRelPose [14], but focusing solely on geometric relationships rather than incorporating additional features. We then use the encoded motion prior to improve correspondence estimation and pose estimation, leading to more accurate relative camera pose estimation."}, {"title": "B. Correspondence estimation", "content": "Traditional methods estimate correspondences between hand-crafted [15], [16] or learning-based [17], [18], [19], [20] local features by nearest neighbour search with mutual check or ratio check [21]. In recent developments, detector- based [6], [22] matching methods such as SuperGlue [6] have achieved significant improvements. Recently, detector- free matches [5], [23], [24], [25] enhance the input fea- tures with attention-based GNNs and match similar en- hanced features to correspondences, which improves the correspondence quality. Such methods obviate the necessity for keypoint detection and extraction, performing well in texture-less environments, such as indoor scenes. However, the absence of stable keypoints also makes it challenging to handle downstream applications like SfM. Other methods in- corporate external information to improve performance [26], [27], [28], [29]. However, dashboard images often include compression artefacts of the textureless regions, trails of motion blur, repetitive patterns, and dynamic objects, result- ing in indiscriminative and ambiguous local features [30], where the aforementioned methods struggle to find reliable correspondences by solely relying on feature similarity."}, {"title": "C. Model scoring in pose estimation", "content": "Given estimated correspondences, the RANSAC [31] paradigm is widely adopted to recover the relative camera poses of the image pair. Traditional methods [32], [33], [34] propose model scoring functions based on inlier counting or well-designed maximum likelihood procedures, which are sensitive to the inlier-outlier threshold setting or inlier-outlier distribution. Recently, MQ-Net [35] learns to score the model from point-to-model residual distribution and achieves impressive accuracy. However, in real-world scenarios of dashboard images, outlier correspondences tend to form spatially coherent structures due to repetitive patterns or dynamic objects. The aforementioned methods are prone to trap plausible geometric models from these coherent outliers. We additionally incorporate the estimated motion prior into a model scoring network to resist plausible models."}, {"title": "III. METHODOLOGY", "content": "Given two images A and B with the known intrinsic matrix K, our goal is to recover the relative camera pose (R, t) between these two images. Our pipeline is illustrated in Fig. 2. In the following sections, we first provide a coarse estimate of the relative pose in Section III-A. Next, we ex- plain how to use this initial estimate to find correspondences between the two images in Section III-B, and how to refine the pose estimation accuracy in Section III-C."}, {"title": "A. Motion prior regression", "content": "As the dashcam image sequences often show strong cam- era motion prior that can be used for pose regression, we introduce a neural network to regress the relative camera motion in this part. Fig. 3 shows the overall architecture of the motion prior regression module. The module first extracts dense correspondences by correlating every feature vector of image A with those of image B and then regresses the rotation and translation from the extracted correspondence."}, {"title": "B. Correspondence estimation with motion prior", "content": "In this section, we aim to establish correspondence sets between the image pair (A, B). Fig. 4 shows the overall architecture.\nThe coarse relative pose (Rc, tc) estimated in Section III- A can serve as a prior, reducing the search regions for iden- tifying correct correspondences. Hence, we integrate coarse relative camera poses into a transformer-based matcher [6] using epipolar line encoding along with keypoints position encoding.\nGiven a set of keypoints' features $f_A, f_B$ and their posi- tions $p_A, p_B$ from images A and B, along with the prior coarse pose (Rc, tc) regressed beforehand, the proposed matcher returns the correspondence set CA,B for these keypoints."}, {"title": "Feature extraction.", "content": "For each pair of images A and B, the module initially extracts a set of keypoints, including their features $f_{A, B}$, and positions $p_{A, B}$ in the pixel coordinate by SuperPoint [19]."}, {"title": "Keypoints position encoding.", "content": "The transformer module requires position encoding to distinguish different features. Following SuperGlue [6], the keypoint positions in pixel co- ordinates are initially transformed into the camera coordinate using intrinsic parameters KA, KB. Subsequently, a simple MLP p(\u00b7) is utilized to map them to a higher dimensionality to match the feature dimensions."}, {"title": "Epipolar line encoding.", "content": "Similarly, in the epipolar encod- ing stage, another MLP is employed to encode the epipolar line to align with the feature dimension. Given the coarse relative pose (Rc, tc) of image A, B, with their intrinsic matrix KA, KB, it is able to compute the corresponding fundamental matrix FA, FB. Subsequently, for the keypoints $p_A$, it is able to compute its corresponding epipolar line in image B in the camera coordinateas following:\n$l_B = F_A p_A = (K_B^{-1})^T [t_c]_{\\times} R_c K_A^{-1} p_A$ \n$l_A = F_B p_B = (K_A^{-1})^T [t_c]_{\\times} R_c K_B^{-1} p_B$ \n$l_B$ denotes the corresponding epipolar line of $p_A$ in image B, whereas $l_A$ denotes the corresponding epipolar line of $p_B$ in image A. Following this, the module normalizes $l_A, l_B$ to unit vectors and then applies an MLP $\u03c6_e(\u00b7)$ to encode the epipolar line, which serves as the encoding feature of the epipolar line."}, {"title": "Feature updating.", "content": "With the previously extracted fea- tures $f_{A, B}$, point coordinate encoded features $f_{A, B}^p$ and epipolar line encoded features $f_{A, B}^l$, the module aggre- gates them by simple addition to form the input feature. Subsequently, these merged features undergo a sequence of self-cross attention layers, enabling information exchange within and between the images. Following several layers of self-cross attention, the features of keypoints are updated for both images."}, {"title": "Matching score prediction.", "content": "Afterwards, a scoring matrix $S \u2208 R^{N_A\\times N_B}$ is formulated using the updated features. Here, $S(i, j) = \\langle f_A^i , f_B^j \\rangle$ denotes the inner product between the updated features $f_A, f_B$, where $N_A, N_B$ represent the numbers of keypoints on image A and image B and $f_A^i \u2208 f_A, f_B^j \u2208 f_B$. The elements $S(i, j)$ in the score matrix represent the matching confidence of keypoints $p_A^i$ and $p_B^j$ where $p_A^i \u2208 p_A$ and $p_B^j \u2208 p_B$."}, {"title": "Correspondence estimation.", "content": "The issue can be reframed as an optimal transport problem. In this formulation, each keypoint in image A is allocated to a keypoint in image B based on a cost matrix, derived from the previously calculated score matrix S. The entropy regularization of S facilitates a softer assignment of correspondences. This problem can be effectively addressed using the Sinkhorn algorithm, which iteratively normalizes the rows and columns of the cost matrix, thereby converging towards a soft assign- ment. Consequently, the correspondences are obtained."}, {"title": "C. Pose estimation with motion prior", "content": "Given the correspondences estimated in the preceding section, the primary objective of this section is to determine the relative pose based on these correspondences. Initially, a straightforward method is employing RANSAC and the 5-point algorithm for pose estimation.\nThe typical RANSAC iteratively selects hypotheses con- sisting of five correspondences until reaching the maximum iteration. Each hypothesis is then used to compute the essen- tial matrix E, assessing the Sampson distance and classifying it into inliers or outliers. The hypothesis with the most inliers is chosen, and the rotation and translation are determined accordingly.\nIn this section, we follow the overall RANSAC framework but employ an improved method for hypothesis scoring. Instead of relying solely on inlier count, our scoring mech- anism involves the utilization of a neural network that integrates motion prior and distribution of inliers. Fig. 5 shows the overall architecture."}, {"title": "Hypothesis sampling.", "content": "In our implementation, we first sample N hypotheses from the estimated correspondences, followed by the selection of the top 100 hypotheses based on their inlier count. Here, a hypothesis denotes a grouping of correspondences. Traditionally, a minimum of 5 correspon- dences suffices for estimating relative pose. However, this minimal setup might result in multiple solutions. To alleviate this uncertainty, we randomly sampling 6 correspondences for each hypothesis. Following this, the inlier count is computed for each hypothesis using the remaining correspon- dences, and the top 100 candidates are then determined based on the inlier count. Inliers and outliers are delineated by Sampson distance threshold de. Sampson distance $d(p_A, p_B^i)$ can be computed as:\n$d(p_A^i, p_B^i) = \\frac{(p_B^{iT} F_i p_A^i)^2}{(F_i p_A^i)_1^2 + (F_i p_A^i)_2^2 +(p_B^{iT} F_i)_1^2 + (p_B^{iT} F_i)_2^2}$ \nwhere F\u2081 is the fundamental matrix and $F_i = K_B^T E_i K_A^{-1}$ These candidate hypotheses are then evaluated using a scor- ing neural network to determine the most accurate one."}, {"title": "Hypothesis scoring: motion prior perception.", "content": "The coarse relative pose, established in Sec. III-A, defines the general camera motion prior represented by Ri and ti. For each hypothesis, we calculate the Sampson distance of the 6 seed correspondences under the prior pose fundamental matrix $F_i = K_B^{-T} [t]_\\times R K_A^i$ and feed this feature vector into the hypothesis scoring network to approximate the regressed relative pose."}, {"title": "Hypothesis scoring: inlier distribution perception.", "content": "Compared with the inlier number, the cumulative distri- bution of inlier provides more information on hypothesis quality [35]. Inliers corresponding to each hypothesis are determined by applying a Sampson distance threshold de = 12.6. Subsequently, de is divided into \u044c = 64 bins where each bin denotes 0.2, and the number of inliers within each bin is tabulated. Each bin represents a Sampson distance \u03b4i = de, uniformly distributed between 0 to de. For each bin, its value denotes the ratio of inliers with a Sampson distance below di to all correspondences, given by\n$b_k = \\frac{n_i(k)I(d_i < \\delta_k)}{N_c}, k \u2208 [0, n_b]$ \nHere, bk signifies the k-th bin inlier ratio, ni(k) represents the number of inliers in the k-th bin, ne stands for the total number of correspondences, I(\u00b7) indicates the count that satisfies the condition, and $I(d_i < \\delta_k)$ indicates the inlier count with a Sampson distance below \u03b4\u03ba."}, {"title": "Hypothesis scoring: network implementation.", "content": "As shown in Fig. 5, the scoring network receives inputs from the inlier distribution, comprising m = 64 bins, the 6 epipolar distances under the prior coarse relative pose, and the de- scriptors of hypothesis from the matcher. Subsequently, these inputs are processed by MLPs with ReLU activation and batch normalization layers. The batch normalization layers standardize features across the top 100 hypotheses. Finally, the scoring network produces a score for each hypothesis, and the hypothesis with the highest score is chosen as the output hypothesis, which is subsequently decomposed to derive the final Ri, ti."}, {"title": "Hypothesis scoring: loss function.", "content": "The score network employs binary cross-entropy loss. The network outputs scores for each hypothesis and the estimated pose is utilized for label calculation. Labels are determined based on the angular errors in rotation Rerr and translation terr between the estimated pose and ground truth pose. To ensure than Rerr and terr contribute equally, we average the two errors and map through a continuous linear function from the range [0\u00b0, 20\u00b0] \u2192 [1,0] to obtain the network labels. Errors exceeding 20\u00b0 are considered negative labels."}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "The method is evaluated using the KITTI [7] dataset, the NuScenes dataset [8] and the Real- DashCam(RDC) dataset.\nFor the KITTI dataset, we randomly selected frame inter- vals ranging from 5 to 13 to generate image pairs, resulting in 62,833 pairs for training, 800 pairs for validation, and 2,347 pairs for testing.\nFor the NuScenes dataset, we selected 36 scenes to gen- erate image pairs solely for testing. Images were randomly chosen with frame intervals ranging from 20 to 30, resulting in a total of 14,922 test image pairs.\nFor the RealDashCam (RDC) dataset, it was collected in Beijing by us. It comprises totalling 1,348 images at a resolution of 2284 \u00d7 1123. This dataset is solely utilized for evaluation. These images exhibit low quality and contain multiple dynamic objects and compression artefacts, present- ing significant challenges for accurate pose estimation. The image examples are illustrated in Fig. 6. Image pairs from the RDC dataset are randomly selected with frame intervals ranging from 15 to 25, resulting in 2328 test image pairs."}, {"title": "Baselines:", "content": "In this study, two types of match- ing methods were employed: detector-based methods and detector-free methods. The representative of the detector- based method is SuperGlue [6]. The representative of the detector-free method is LoFTR [5], Aspanformer [23], DKM [24], and RoMA [25]. All of these matching methods utilize RANSAC for pose estimation.\nFor the pose estimation method, two learning-based ap- proaches were employed: NefSAC [12] and MQNet [35], both of which are learning-based and can capture certain reg- ularities of correct sampling, thereby enhancing the accuracy of camera pose estimation. These pose estimation baselines take SuperGlue [6] correspondence as input."}, {"title": "Metrics:", "content": "In accordance with prior methodologies [6], [36], performance assessment is conducted through Area- Under-Curve (AUC) metrics derived from pose accuracy curves. For each estimated relative pose angular disparities between rotations and normalized translation vectors are computed in comparison to the corresponding ground-truth pose. Subsequently, the pose error is determined as the larger value between the angular errors in rotation and translation. Specifically, we report the AUC values corresponding to an- gular errors less than 5\u00b0, 10\u00b0, and 20\u00b0 across all experiment."}, {"title": "Implementation details:", "content": "The pose regressor, matcher, and hypothesis score network are trained separately on the KITTI dataset using the Adam optimizer. The learning rate is set at $1 \\times 10^{-4}$, which undergoes annealing from $1 \\times 10^{-4}$ to $1 \\times 10^{-5}$. The regressor module uses a 5-layer CNN with output dimensions 64/256/512/1024/2048, followed by average pooling and two MLPs for quaternion and transaction regression. The matcher module employs a 4-layer MLP encoder (32/64/128/256) and 9 self- and cross-attention layers. The pose regressor has three branches: descriptor (4-layer MLP with dimensions 512/256/128/64), prior (4-layer MLP with dimensions 6/16/32/64), and inlier (4-layer MLP with dimensions 64/64/128/128). Features from these branches are concatenated and processed through a score net with dimensions 256/256/128/64/32/16/1 to produce the score. The baseline models of matching were fine-tuned using the KITTI dataset, considering only correspondences with depth information. As NefSAC [12] was also trained on the KITTI dataset, we utilized its pretrained weights. Due to the unavailability of the source"}, {"title": "B. Performance", "content": "Pose Estimation Performance: The qualitative com- parison is shown in Fig. 7 and Fig. 8, while the quantitative results of the two datasets are presented in Tab. I."}, {"title": "SfM Performance:", "content": "To further validate the perfor- mance of our matching method, we perform SfM on the RDC dataset. In this experimental setup, we utilize the correspon- dences estimated by our method as inputs to COLMAP to assess the quality of the reconstruction. Our evaluation of the SfM quality encompasses three key metrics: the number of registered cameras (\"registered cameras\"), the number"}, {"title": "C. Ablation Analysis", "content": "To validate our design, we conduct ablation studies on the KITTI dataset for relative pose estimation. The quantitative results of our ablation studies are shown in Tab. III. In this table, \"pos. reg.\" denotes the pose regressor, \"SG\" refers to SuperGlue [6], \"epi. enc.\" signifies the epipolar encoding, and \"prior est. (samp.)\" represents the proposed pose esti- mation method utilizing our sampling implementation. Ablation model 0 refers to the pose regression component, while ablation model 1 represents the SuperGlue [6] matcher with RANSAC pose estimation. Ablation model 2 indicates our matcher integrated with epipolar encoding and RANSAC pose estimation. Ablation model 3 denotes the SuperGlue [6] matcher coupled with our pose estimator incorporating pose prior information. Finally, ablation model 4 encompasses our complete model."}, {"title": "Runtime", "content": "Quantitative results are shown in Tab. IV and the notation consistent with that in Tab. III. The inference time experi- ment was conducted on the KITTI dataset with a maximum iteration limit set at 1000. The detector-free matching method typically exhibits lower efficiency compared to detector- based matching methods due to its generation of dense cor- respondences for nearly all pixels. In the estimation process, the primary limitation is the sampling procedure. However, this procedure is highly optimized in OpenCV RANSAC."}, {"title": "V. CONCLUSION", "content": "In this paper, we present a novel image matching frame- work for dashcam images. We have observed a robust reviewedmotion prior inherent in dashcams, which proves advantageous for the learning of correspondence and filter- ing outliers. Consequently, we employed a pose regression module to regress the motion prior of the camera, encoding them via soft epipolar constraint into the matcher. Simulta- neously, we applied this methodology within random sample consensuses to assess their quality, thereby achieving precise pose estimation. The experimental results validate that our method outperforms all the existing methods and supports all claims made in this paper. We believe that our framework will benefit the production and updating of high-definition map, as well as improve subsequent geo-information analysis tasks."}]}