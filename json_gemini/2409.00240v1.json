{"title": "One-Frame Calibration with Siamese Network in Facial Action Unit Recognition", "authors": ["Shuangquan Feng", "Virginia R. de Sa"], "abstract": "Automatic facial action unit (AU) recognition is used widely in facial expression analysis. Most existing AU recognition systems aim for cross-participant non-calibrated generalization (NCG) to unseen faces without further calibration. However, due to the diversity of facial attributes across different identities, accurately inferring AU activation from single images of an unseen face is sometimes infeasible, even for human experts\u2014it is crucial to first understand how the face appears in its neutral expression, or significant bias may be incurred. Therefore, we propose to perform one-frame calibration (OFC) in AU recognition: for each face, a single image of its neutral expression is used as the reference image for calibration. With this strategy, we develop a Calibrating Siamese Network (CSN) for AU recognition and demonstrate its remarkable effectiveness with a simple iResNet-50 (IR50) backbone. On the DISFA, DISFA+, and UNBC-McMaster datasets, we show that our OFC CSN-IR50 model (a) substantially improves the performance of IR50 by mitigating facial attribute biases (including biases due to wrinkles, eyebrow positions, facial hair, etc.), (b) substantially outperforms the naive OFC method of baseline subtraction as well as (c) a fine-tuned version of this naive OFC method, and (d) also outperforms state-of-the-art NCG models for both AU intensity estimation and AU detection.", "sections": [{"title": "1 Introduction", "content": "Facial expression analysis is important for understanding human emotions and behaviors across various fields, including human-computer interaction, psychology, and security. The Facial Action Coding System (FACS) is a comprehensive system for describing human facial movement developed by Ekman and Friesen [10], widely recognized and extensively used in facial expression analysis for its ability to describe facial movement objectively and systematically. It breaks down facial expressions into individual components of muscle movement, called action units (AUs). Table 1 introduces the primary AUs analyzed in this paper."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Bias in Facial Expression Recognition", "content": "Previous studies have shown that facial expression recognition systems can exhibit biases across groups based on gender [9], race [28], age [15], among others [7, 14], raising significant attention and concerns about the fairness of these systems. Due to the objectiveness of FACS by definition, some researchers believe AU recognition is less subject to bias and use it to investigate and mitigate bias in facial emotion annotation [6] and recognition [12, 33]. However, it was shown that AU recognition is also subject to bias, which researchers have developed methods to mitigate [7, 14].\nAlthough group-based biases attract more attention, they are essentially specific manifestations of the broader issue of identity bias in facial expression recognition. Researchers have proposed two methods of addressing identity bias, either to develop identity-aware/personalized models for facial expression recognition [25, 36, 39, 42] or to apply adversarial training with respect to identities on the models to encourage them to disregard identity-related features [45]."}, {"title": "2.2 Siamese Neural Network", "content": "The Siamese Neural Network was first introduced by Bromley et al. [3] and has been widely applied in facial identity-related tasks, such as face verification [34] and face recognition [40]."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 One-Frame Calibration", "content": "While most existing AU recognition systems aim for cross-participant non-calibrated generalization (NCG) to unseen faces, it is crucial to take the face's neutral appearance into account in AU coding. Thus, we propose one-frame calibration (OFC) for AU recognition: for each face, a single image of its neutral expression is used as the reference image for calibration.\nIn offline benchmarking, OFC primarily applies to video datasets. The selection of the reference image is achieved by manually selecting one image from all frames with zero activation of annotated AUs for each face. The aim of manual selection is to ensure that in the selected reference image, (a) the unannotated AUs are also not activated or only minimally activated, and (b) the face is at an appropriate angle and not partially occluded.\nIn real-life applications of AU recognition systems, the ideal method of selecting the reference image for OFC is to directly ask the user to pose a neutral face before usage."}, {"title": "3.2 Calibrating Siamese Network for OFC", "content": "We propose the Calibrating Siamese Network (CSN) architecture for OFC. The input for CSN consists of the target image for AU recognition and the reference image. The two images are fed into two identical networks with shared weights and joined in an intermediate stage of the network by computing the difference between their feature maps.\nThis architecture design can be integrated with a variety of model designs for AU recognition. To demonstrate its effectiveness in a simple way, we use the classical iResNet-50 (IR50) as the backbone in this work and name it CSN-IR50."}, {"title": "3.2.1 CSN-IR50", "content": "Figure 1 illustrates the architecture of CSN-IR50. The reference image with a neutral expression and the target image for AU recognition are fed into two identical IR50 networks with shared weights; just before reaching stage 4 of the network, the difference between their feature maps is computed and then fed into the rest of the IR50 network until the AU intensities are outputted.\nCSN-IR50 may be more precisely called CSN-IR50-Stage4, emphasizing stage 4 as the merge point, which is the main version of CSN-IR50 we primarily investigate in this work. Stage 4 is selected as the merge point because the feature maps in this stage capture high-level abstractions of the face features and still retain the fine-grained information. We will also compare it with other versions, including CSN-IR50-Stage1, CSN-IR50-Stage2, CSN-IR50-Stage3, CSN-IR50-FC, and CSN-IR50-Output."}, {"title": "3.3 Baseline Models", "content": "We compare the performance of our proposed method with those of various models. Firstly, we directly compare our method of OFC with CSN-IR50 with the vanilla NCG with IR50 to demonstrate the effectiveness of OFC. Secondly, we compare our model with the naive OFC method of baseline subtraction (BS) (IR50 OFC w/ BS) to demonstrate the superiority of our model as an OFC method. (Table 6 shows a comparison with CSN-IR50-Output, which is a fine-tuned version of this naive method (fine-tuned with same parameters as CSN-IR50)). Additionally, we also compare our model performance with that of other SOTA NCG models."}, {"title": "3.3.1 IR50 (NCG)", "content": "In IR50 (NCG), the IR50 is trained on individual images of the training set and directly applied to images of unseen faces during validation."}, {"title": "3.3.2 IR50 (OFC w/ BS)", "content": "In IR50 (OFC w/ BS), the IR50 is trained on individual images of the training set; however, during validation, its output on the reference image for each face is used as the baseline, which is subtracted from outputs on all images of the same face to produce final predictions."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets and Settings", "content": "DISFA [24] contains left-view and right-view facial video recordings of 27 participants with approximately 130,000 frames in total for each view. Each frame is annotated with intensities of 12 AUs on a scale of 0 to 5. Following previous studies, we perform participant-exclusive 3-fold cross-validation on DISFA.\nDISFA+ [22] is an extension of the DISFA dataset. It contains facial video recordings of 9 participants' posed and spontaneous facial expression with each frame being annotated with the same 12 AUs on a scale of 0 to 5. We perform leave-one-participant-out cross-validation on DISFA+.\nUNBC-McMaster [19] is a dataset originally collected for pain detection. Since it also contains frame-level AU intensity annotations of 10 AUs on a scale of 0 to 5 (except that the annotations for AU43 (eye closure) are binary), it is also appropriate for analyzing AU recognition methods. It contains facial video recordings of 25 participants with 48,398 frames in total. We perform participant-exclusive 5-fold cross-validation on UNBC-McMaster.\nWe did not include the widely used BP4D [43] dataset because it is not appropriate for OFC. Unlike the previous mentioned datasets, in BP4D, FACS coders selectively annotate a 20-second segment with the highest density of facial expression for each recording session, and only these segments are released in the dataset. Consequently, for most participants in BP4D, there is no appropriate \u201cneutral face frame\" to use as the reference image for OFC.\nWe evaluate our methods on both AU intensity estimation and AU detection. In AU intensity estimation, the model outputs estimations of intensities of the AUs (real values between 0 to 5). In AU detection, the model outputs predictions of whether each AU occurs in binary format (0 or 1). Following previous studies [30], for AU detection, we consider AU intensities greater or equal to 2 as occurrences, and we only include 8 of the 12 AUs for DISFA."}, {"title": "4.2 Implementation Details", "content": "Each frame is preprocessed with face detection [20], face alignment [5], and a combination of histogram equalization and linear mapping [16] for both training and validation.\nWe use the weights pre-trained on Glink360k [1, 8] for both IR50 and CSN-IR50, and the last layer of the network is modified to adapt to the output format for the AU recognition task.\nFor AU intensity estimation, we train the network to perform both regression and ordinal classification [26] on the AU intensities. The network outputs the estimation of the AUs in two formats: for estimating the intensity of the ith AU \\(Y_i\\), it outputs 1 value \\(\\hat{y}_{i, reg}\\) representing the numerical estimation of the AU intensity (in the format of regression) and 5 values \\(o(\\hat{y}_{i,class(1)})\\), \\(o(\\hat{y}_{i,class(2)})\\), \\(o(\\hat{y}_{i,class(3)})\\), \\(o(\\hat{y}_{i,class(4)})\\), and \\(o(\\hat{y}_{i,class(5)})\\) respectively representing the estimated probability of the AU intensity being higher than or equal to 1, 2, 3, 4, and 5 (in the format of binary classifications). The loss function consists of three parts:\n\\(E_{AUIE} = E_{reg, MSE} + E_{reg, cos} + E_{class},\\)\nwhere \\(E_{reg, MSE}\\), \\(E_{reg,cos}\\), and \\(E_{class}\\) respectively represent a mean squared error (MSE) loss for the numerical estimations\n\\(E_{reg,MSE} = \\sum_{i=1}^{25}W_{i,y_i} (Y_i \u2013 \\hat{Y_{i,reg}})^2,\\)\na cosine similarity loss for the numerical estimations\n\\(E_{reg,cos} = 1- \\frac{\\sum_{i=1}^{25}Y_i\\hat{Y_{i,reg}}}{\\sqrt{(\\sum_{i=1}^{25}Y_i)^2} \\sqrt{(\\sum_{i=1}^{25}\\hat{Y_{i,reg}})^2}}},\\)\nand a cross entropy loss for the binary classification estimations\n\\(E_{class} = \\sum_{i=1}^{25} \\sum_{j=1}^{5} W_{i,j,X_{y_i\\geq j}}CE(X_{y_i\\geq j}, o(\\hat{y}_{i,class(j)})),\\)\nwith the cross entropy function being\n\\(CE(y, \\hat{y}) = -[y_i log(\\hat{y}_i) + (1 - Y_i) log(1 \u2013 \\hat{y}_i)].\\)\nThe weights for the MSE loss and those for the cross entropy loss are both inverse-frequency weighted and normalized within each AU for addressing class imbalance in the datasets (substantially higher number of occurrences for low AU intensities). However, since the number of occurrences of high intensities are too few for most AUs (resulting in too high weights for the MSE loss if used directly), we \"bin\" the intensities into 2 groups, and each group shares the same weight. Specifically, for the MSE loss, we apply one weight for occurrences of intensities of 0 and 1 and another weight for occurrences of intensities of 2, 3, 4, and 5, and these weights are computed based on the total number of occurrences within each intensity group.\nNotably, although we train the network to learn both numerical estimations and binary classification estimations of the AU intensities, only the numerical estimations are used in model validation (and any further model inference).\nFor AU detection, we train the network to directly output the estimated probability of the occurrence of each AU \\(\\sigma(\\hat{y}_i)\\), with an occurrence defined as \\(y_i \\geq 2\\). The loss function uses cross entropy loss:\n\\(E_{AUD} = \\sum_{i=1}^{25}W_{i,X_{y_i\\geq 2}}CE(X_{y_i\\geq 2}, \\sigma(\\hat{y}_i)),\\)\nwith similar inverse-frequency weights normalized within each AU. The detailed equations for weight computation in both AU intensity estimation and AU detection are provided in the technical appendix.\nFor model training, we employ the Adam optimizer with an initial learning rate of \\(10^{-4}\\) for parameters of the last layer and an \\(10^{-5}\\) for other parameters, a weight decay of \\(5 \\times 10^{-4}\\), and a batch size of 64. These hyperparameters were selected based on our prior work on other models for"}, {"title": "4.3 Results of CSN-IR50", "content": ""}, {"title": "4.3.1 Main Results", "content": "Table 2 reports the performance of CSN-IR50 on AU intensity estimation on DISFA in comparison to other methods. Firstly, CSN substantially improves the performance of IR50 for NCG and also greatly outperforms the naive OFC method of IR50 with BS, with a difference of 0.08 in ICC(3,1) and a difference of 0.09 in Mean Absolute Error (MAE). Secondly, in comparison to other SOTA NCG methods, our CSN-IR50 demonstrates a substantially higher ICC(3,1) of 0.67 and a near-best MAE of 0.22 (the best being 0.20). ICC(3,1) measures the consistency between the model estimations and the human expert labels in the dataset. We believe ICC(3,1) is a better metric here because of the high imbalance of DISFA.\nTable 3 reports the performance of CSN-IR50 on AU detection on DISFA in comparison to other models. Firstly, it again substantially improves the performance of IR50 for NCG and outperforms the naive OFC method of IR50 with BS\u00b9 in both F1 score and accuracy. Secondly, in comparison to other SOTA NCG methods, our CSN-IR50 demonstrates a higher F1 score and equal-to-best accuracy of 94.1.\nNote that the comparison between our CSN-IR50 and other SOTA models here is not an apples to apples comparison because CSN-IR50 is for OFC while the SOTA models are for NCG. However, also note that the effectiveness of our proposed CSN architecture is demonstrated only using the simple IR50 as the backbone for simplicity in our paper, and we believe the CSN architecture design has great potential to be integrated with more complicated, advanced backbone models for AU recognition to achieve higher performance."}, {"title": "4.3.2 The Advantage of OFC with CSN", "content": "One interesting question is how/why OFC with CSN-IR50 outperforms the vanilla IR50. Two important observations provide insights into this question.\nFirstly, Table 5 compares within-participant ICC(3,1) averaged over all participants and across-participant ICC(3,1) between different methods on AU intensity estimation on DISFA, DISFA+, and UNBC-McMaster. Within-participant ICC(3,1) only measures the consistency between the model estimations and human expert labels within individual participants; across-participant ICC(3,1) is the version used everywhere else in this paper, as it measures the consistency not only within but also across participants and thus more insightfully captures bias across different participants. As shown in Table 5, although our CSN-IR50 greatly improves the across-participant ICC(3,1) of IR50, its improvement in within-participant ICC(3,1) is much more modest. This suggests that the primary advantage of OFC with CSN-IR50 lies in its ability to calibrate for diverse facial attributes across different identities, which aligns with our original intent for CSN.\nThe comparison of precision and recall in Table 3 offers further insights into how the calibration is achieved: the CSN architecture generally increases precision while decreasing recall for most AUs. In other words, the CSN architecture substantially reduces the misidentification of non-activated AUs as activated (false positives), although this improvement comes with the cost of missing some actual AU activations (false negatives).\nThis reduction of false positives is achieved through mitigating facial attribute biases. More specifically, without face-specific calibration, some facial attributes are easily misidentified as AU activations, and our CSN addresses this issue. See Figure 2 for a variety of case examples. In Figure 2a, IR50 tends to overestimate AU1 (inner brow raiser) intensities due to the participant's wider eyebrow-to-eye distances, because AU1 produces wider distances between eyebrows and eyes; in Figure 2b, IR50 tends to overestimate AU4 (brow lowerer) intensities due to the participant's slight permanent wrinkle at the root of the nose, because AU4 may produce horizontal wrinkles at the root of the nose; in Figure 2c, IR50 tends to misidentify the bridge of eyeglasses as wrinkles caused by AU9 (nose wrinkler) activation, because AU9 produces wrinkles at the root of the nose; in Figure 2d; IR50 tends to misidentify the facial hair as wrinkles caused by AU17 (chin raiser) activation, because AU17 produces wrinkles on the chin boss. Interestingly, the first two examples are issues human FACS coders may also face without a neutral reference, while the latter two examples are facial attribute misidentification problems specific to machine learning models, partially due to insufficient training data, a limitation common in AU datasets. CSN-IR50, effectively addresses these issues by calibrating the AU estimations of different faces based on their neutral appearances."}, {"title": "4.3.3 Comparing Different Versions of CSN-IR50", "content": "The CSN-IR50 we have presented so far is our main version, CSN-IR50-Stage4, in which the two networks for the reference image and the target image respectively merge just before stage 4 of IR50 (see Figure 1). Table 6 compares it with other versions of CSN-IR50 with different merge points on DISFA. (Each version is named after the first module after the merge point.) We can see that our selected merge point, stage 4, is the optimal one providing the best performance. Merging at earlier stages (CSN-IR50-Stage1, CSN-IR50-Stage2, and CSN-IR50-Stage3) provides somewhat suboptimal performances but still outperforms the IR50 baselines. We believe these versions suffer from insufficient processing of individual faces before the merge but still benefit from calibration with the neutral reference. On the other hand, merging later just before the fully connected layer (CSN-IR50-FC) or directly computing the difference between the output AU estimations of the two networks (CSN-IR50-Output) provides substantially worse performance possibly because the more fine-grained information is already lost at that stage. Note CSN-IR50-Output is the fine-tuned version of our naive baseline OFC method (IR50 (OFC w/ BS)). Fine-tuning seems to have a small effect on this method, slightly improving MAE but reducing ICC(3,1) on AU intensity estimation and slightly improving F1 and accuracy on AU detection on DISFA. Performance is much worse than our CSN-IR50-Stage4 model, which only differs in where the merging (difference computation) takes place."}, {"title": "4.3.4 Full Results", "content": "For the results presented in Tables 4 to 6, the full versions including individual values for each AU are in the supplementary material (see Tables S1 to S9)."}, {"title": "5 Discussion and Conclusion", "content": "In this paper, we propose to perform OFC in AU recognition for better generalizing the model to unseen faces and a CSN architecture design for OFC. For simplicity, we demonstrate the effectiveness of the CSN architecture with an IR50 backbone. On the DISFA, DISFA+, and UNBC-McMaster datasets, we show that our OFC CSN-IR50 model (a) substantially outperforms the performance of IR50 with NCG, (b) substantially outperforms IR50 with the naive OFC method of BS, as well as (c) the fine-tuned version of this method we call CSN-IR50-Output (note that it only differs from our model in where the merging takes place), and (d) also outperforms SOTA NCG models for both AU intensity estimation and AU detection. With further analysis, we show that the superiority of OFC with CSN-IR50 lies in its ability to calibrate for diverse facial attributes across different identities. Specifically, it substantially reduce false positives in AU recognition, albeit at the cost of increasing false negatives. With case examples, we demonstrate how the reduction of false positives is achieved through mitigating overestimation and misidentification of AUs due to various facial attribute biases, including eyebrow locations, wrinkles, eyeglasses, and facial hair.\nOne important note is that, while comparison with CSN-IR50-Output and IR50 (OFC w/ BS) is fair and shows large performance improvement, the comparison between our CSN-IR50 and other SOTA models is not an apples to apples comparison because CSN-IR50 is for OFC, enhanced with one important labeled frame of neutral expression from each participant in the validation set, while the SOTA models are for NCG. However, also note that the effectiveness of our proposed CSN architecture is demonstrated only using the simple IR50 as the backbone for simplicity in our paper. Additionally, CSN is not a replacement for existing NCG AU recognition models; rather, it is an augmentation that can be integrated with any existing model. Therefore, as an important future direction, we believe that our CSN architecture design has great potential to be integrated with more complicated, advanced backbone models for AU recognition to achieve higher performance.\nAdmittedly, OFC has limitations in real-life applications because of its reliance on a good reference image\u2014a neutral face at an appropriate angle and not partially occluded. The ideal method of selecting the reference image is to directly asking the user to pose a neutral face before using the system. Despite its great accuracy and efficiency, this method only applies to scenarios where the user willingly uses the system with full awareness (which includes a wide range of applications, such as healthcare, education, and entertainment). One potential solution for other scenarios is to develop a method to automatically select a good reference image from real-time video streaming, which would also be an interesting future direction to explore.\nIn conclusion, we propose to perform OFC with a novel CSN architecture design for AU recognition and demonstrate its remarkable effectiveness with a simple IR50 backbone. We also believe it has great potential to be integrated with better backbone models to achieve higher performance."}]}