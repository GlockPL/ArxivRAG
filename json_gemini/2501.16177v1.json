{"title": "BAG: Body-Aligned 3D Wearable Asset Generation", "authors": ["ZHONGJIN LUO", "YANG LI", "MINGRUI ZHANG", "SENBO WANG", "HAN YAN", "XIBIN SONG", "TAIZHANG SHANG", "WEI MAO", "HONGDONG LI", "XIAOGUANG HAN", "PAN JI"], "abstract": "While recent advancements have shown remarkable progress in general 3D shape generation models, the challenge of leveraging these approaches to automatically generate wearable 3D assets remains unexplored. To this end, we present BAG, a Body-aligned Asset Generation method to output 3D wearable asset that can be automatically dressed on given 3D human bodies. This is achived by controlling the 3D generation process using human body shape and pose information. Specifically, we first build a general single-image to consistent multiview image diffusion model, and train it on the large Objaverse dataset to achieve diversity and generalizability. Then we train a Controlnet to guide the multiview generator to produce body-aligned multiview images. The control signal utilizes the multiview 2D projections of the target human body, where pixel values represent the XYZ coordinates of the body surface in a canonical space. The body-conditioned multiview diffusion generates body-aligned multiview images, which are then fed into a native 3D diffusion model to produce the 3D shape of the asset. Finally, by recovering the similarity transformation using multiview silhouette supervision and addressing asset-body penetration with physics simulators, the 3D asset can be accurately fitted onto the target human body. Experimental results demonstrate significant advantages over existing methods in terms of image prompt-following capability, shape diversity, and shape quality.", "sections": [{"title": "1 INTRODUCTION", "content": "3D wearable assets, such as garments, shoes, and headwear, are vital components in the creation of digital 3D avatars. Generating a large number of high-quality 3D wearable assets is therefore crucial for many practical applications, including video games, filmmaking, and augmented and virtual reality (AR/VR).\nExisting wearable asset generation is dominated by sewing pattern modeling, as seen in works like DressCode [1], or PCA-based approaches, such as BCNet [2]. While these methods offer production-ready quality shapes, they suffer from low diversity, poor prompt-following capabilities, and may fail to generate very complex geometry.\nOn the other hand, powered by advancements in transformers [3] and diffusion models [4], there has been substantial progress in the field of general 3D shape generation. New methods are continuously being proposed, consistently pushing the limitations of geometric quality. Notable recent works include DreamFusion [5], LRM [6], CLAY [7], Tripo 1 and Trellis [8], among others. These methods excel at following input images or text prompts and can generate highly complex shapes. By combining additional remeshing tools like [9] for post-processing, 3D asset generation that meets industrial standards is becoming possible.\nThis brings us to the question: Can we generate body-aligned wearable 3D assets using the most advanced 3D generative models available today? A straightforward approach is to first generate the 3D asset and then put it onto a 3D human body model. However, this usually requires manual intervention, such as dragging and morphing in a 3D graphics user interface, which is both time-consuming and costly, moreover, the generated 3D assets are not guaranteed to match the shape of the target human body, which could lead to poor dressing results.\nTo automate the process of wearing generated 3D assets onto human models, Garment3DGen [10] deforms predefined garment templates to match the generated 3D asset. It then wraps the target human model to wear the generated asset according to the deformation of the garment template. This strategy is limited by the number of available template shapes. Additionally, the deformation usually cause severe surface stretching when the shape of the garment template significantly differs from the generated asset.\nIn this paper, we present an method to directly generate body-aligned 3D assets, such that the assets can be automatically fitted to a given human body shape without human intervention. This is achived by controlling the 3D generation process using human body shape and pose information. Specifically, similar to Zero123+ [11], we first build a general single-image to consistent multiview image diffusion model, and train it on the large Objaverse [12] dataset to achieve diversity and generalizability. Then we train a Control-net [13] to guide the multiview diffusion to generate body-aligned multiview images. The control signal comprises multiview 2D projection of the target human body, with pixel values indicating the XYZ coordinates of the body surface in a canonical space. Our body-conditioned multiview generator produce body-aligned multiview images, which are then fed into a native 3D diffusion model to produce the 3D shape of the asset. Finally, by optimizing a similarity"}, {"title": "2 RELATED WORK", "content": "This section surveys generative models for 3D shapes, garments, and 3D avatars. For clarity, in this paper, we regard single-view 3D reconstruction as an image-conditioned generation problem.\nGeneral 3D Shape Generation. Early methods primarily leveraged Generative Adversarial Networks (GANs) [14] to model 3D distributions, as demonstrated in 3DGAN [15], EG3D [16] and Get3D [17]. However, these methods faced challenges in scaling to more diverse scenarios. DreamFusion [5] and its follow-up works [18, 19] obtain 3D shapes by leveraging 2D generative models. However, these methods are time-consuming and suffer from the Juanus-face problem. LRM [6] utilizes large transformer models to reconstruct the triplane representation of 3D shapes. Recent native 3D generative models employ diffusion models [4] to generate 3D content using various representations, such as point clouds [20, 21], meshes [22], voxel grids [8, 23, 24], octrees [25], triplanes [26-29], irregular primitives [30], vecsets [31], and network weights [32]. To produce shapes with artist-like topology, PolyGen [33] and its follow-up works [9, 34-36] adopted autoregressive models for mesh generation. Shape generation can follow image prompts by incorporating visual features into the generation process with transformers. One can also convert a single image to consistent multi-view images as conditioning signals using models such as Zero123+ [11]. Similar techniques have been adopted in Wonder3D [37], CLAY [7], Tripo 1 and Trellis [8]. In this paper, we introduce body-conditioned multi-view image generation and leverage advanced native 3D generative models to produce 3D wearable shapes that can be automatically draped on 3D human bodies.\nGarment Generation. Sewing patterns are a widely adopted geometric representation for generating garments, as done in Sewformer [38] and DressCode [1]. It is usually follow-uped with a draping process that attach cloth on human body. This can be done via physcis-based approach as done in [39] or data-driven approaches like DrapeNet [40]. While these approaches generate production-ready mesh quality, creating sewing patterns that precisely follow image prompts remains challenging. Some other works model garment geometry as functions from human SMPL [41] body pose and shape parameters, this can be seen in CAPE [42], SMPLicit [43] and NeuralTailor [44]. Among them, CAPE [42] regress cloth gemetry as vertex offsets from body surface mesh, SMPLicit [43] generate clothes as implicit model conditioned with the SMPL human body parameters. Garments can also be represented using vertex-based PCA models, as demonstrated in MultiGarmentNet [45], Cloth3D[46], BCNet [2], and GarverseLOD [47], however, these methods are limited by the number of base garment templates. Garment3DGen [10] leverages Wonder3D to generate a reference garment shape and"}, {"title": "3 METHOD", "content": "The overview of the method is shown in Fig. 2.\n3.1 Body Conditioned Multi-view Image Generation\nThis section outlines the method for constructing a body-aligned multi-view consistency image generation module. This is achived by controlling the 3D generation process using human body shape and pose information.\nConsistent Multiview Image Generation. First, we build a general single-image to consistent multiview image diffusion model similar to Zero123++ [11]. The multi-view images are represented by tiling four images in a 2 \u00d7 2 layout to form a single frame. The four training views are rendered using orthographic cameras, maintaining an absolute elevation angle of 0\u00b0 and four fixed azimuth angles: {0\u00b0, 90\u00b0, 180\u00b0, 270\u00b0 }. Therefore, the first view corresponds to the input front view, while the remaining views represent the relative left, back, and right perspectives of the object. Similar to Zero123++, we reuse the weights of Stable Diffusion 2.0 [52] and finetune the weigths using rendered images of over 600k objects filtered from the original Objaverse [12] dataset.\nBody-Aligned Multiview-Generation. Now that we have a general multiview generator, we can control it to generate body-aligned"}, {"title": "3.2 Similarity Transformation Estimation", "content": "Given body-aligned multiview images, we can leverage multiview-conditioned native 3D diffusion models for asset shape generation, e.g., CLAY [7] and Tripo 1. This paper primarily focuses on body-aligned geometry generation, the textures of the assets are generated using off-the-shelf texturing tool Meshy 2. Due to the non-deterministic nature of the diffusion model, the shape generator cannot guarantee that the output shape will align with the input multi-view images. This misalignment can manifest as differences in scale, translation, and rotation.\nTo address this issue, we optimize the similarity transformation $S_R \\in Sim(3)$ with multiview image supervision, where $s \\in \\mathbb{R}^+$\nis the scaling factor, $R \\in SO(3)$ denote rotation, and $t \\in \\mathbb{R}^3$ is translation. We parameterize rotation with a 3-dimensional axis-angle vector $w \\in \\mathbb{R}^3$. We use the exponential map $exp : so(3) \\rightarrow SO(3), \\hat{w} e = R$ to convert from axis-angle to matrix rotation form, where the operator creates a 3 \u00d7 3 skew-symmetric matrix from a 3-dimensional vector. Given an generated asset mesh with vertex set $V_{raw} = {x \\in \\mathbb{R}^3 i=1..n}$, the transformed vertex set $V_{transformed} = {y_i \\in \\mathbb{R}^3|i=1..,n}$ are obtained via,\n$y_i = sRx_i + t$                                                                                                                                                                       (1)\nWe employ silhouette loss as supervision, which is the pixel-wise mean absolute difference between the rendered multiview silhouettes $S_{rendered}$ and the input multiview silhouettes $S_{input}$. The loss reads\n$L_{silhouette} = \\frac{1}{N} \\sum_{i=1}^N |S_{rendered,i} - S_{input,i}|$.                                                                                                             (2)\nWhere N is the total number of pixels. The input silhouettes are obtained by running SAM [68]. The silhouettes of the mesh are rendered using the Phong shader implemented in PyTorch3D. Optimization is performed using the Adam optimizer."}, {"title": "3.3 Handling Asset-Body Penetration", "content": "The Sim(3)-transformed asset can roughly align with the body, but small penetrations between the asset and body still exist. To address this issue, we further resolve such penetrations using a physics simulation-based solver.\nRepresenting Asset via Single Layer Proxy Mesh. Our native 3D diffusion model represents 3D geometry as occupancy [69] and extracts the surface mesh using the marching cubes algorithm. As a result, the generated assets are watertight but not suitable for physics simulation. For instance, the generated garments are not single-layer meshes; they have thickness, which can cause issues when simulating body-asset collisions. Inspired by ProxyCloth [70], we use single-layer proxy mesh to represent the asset. Specifically, we first put cameras around the human body and remove triangles that can not be observed by the cameras. Then we simplifiy the mesh with Voronoi diagram-based technique [71], which clusters the vertices in into a Centroidal Voronoi Diagram and then extract the proxy from the centroids of neighboring clusters. After removing non-manifold faces and non-manifold vertices, we obtain a uniformly meshed surface. An example of comparison bewteen original visual mesh and proxy mesh is shown in Fig. 4. Then we compute the Linear Blend Skinning (LBS) weights from the asset shape to asset proxy, which can propagate the potential deformation back to the visual mesh.\nPenetration Handling. To physically resolve penetration between the asset and the body, we developed a position-based simulation of compliant constrained dynamics (XPBD) [72] based cloth simulator. The simulator can hanlde the penetration through collision response between the asset and the body. We compute a Signed Distance Field (SDF) for the body to facilitate collision detection. During each simulation step, vertices on the asset with negative SDF values are identified as penetrated into the body. These vertices are projected to a penetration-free state based on their SDF normals and values"}, {"title": "3.4 Input Body And Image Pair Acquisition", "content": "Our method requires that the input image of the asset is aligned with the front view of the body. As shown in Fig. 5, we identified four use cases to obtain these input pairs. The following are the details:\na) Image-based SMPLX Fitting. Given an image of a dressed human, we estimate the SMPLX parameters using the techniques introduced in PyMAF [59, 74, 75].\nb) Sketch-Based Modeling. Users can draw 2D assets on top of the rendered image of a SMPLX model. The sketch is then converted into a colored image using SDXL [52] and T2i-adaptor [76] based sketch-to-image generation.\nc) Image-Based Virtual Try-On (VTON). VTON takes an image of a garment as input and generates an outfitted image of a target human wearing the garment. We leverage Kolors [77] 3 as our VTON tool. We use the rendered image of a textured SMPLX as the target 2D human.\nd) Manual Image Assembly. Users can manually drag images of existing 2D assets onto the rendered image of a textured SMPLX model.\nNote that only case {a} requires human pose estimation; in cases {b, c, d}, the body parameters are provided. In cases {a, b, c}, the"}, {"title": "4 EXPERIMENTS", "content": "Evaluation metric. The quality of shape generation is evaluated using the Chamfer Distance (CD), Normal Consistency (NC), and the average Point-to-Surface Euclidean distance (P2S). The multi-view image generation is evaluated with Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM), and the Learned Perceptual Image Patch Similarity (LPIPS) [79] metric.\nComparison with SOTA. We compare our method with the state-of-the-art (SOTA) single-view garment reconstruction methods, including BCNet [2], ClothWild [80], SewFormer [38], Frankenstein [81], and Garment3DGen [10], both quantitatively and qualitatively. Tab. 1 shows the quantitative results. Our method achieves the best scores against the baselines. Fig. 6 and Fig. 10 provide qualitative results of single-view image conditioned generation. We repose the results"}, {"title": "Ablation Study on body-aligned multi-view generation", "content": "To demonstrate the effectiveness of our body-conditioned multi-view image generation, we conduct an ablation study on various methods for obtaining 3D assets: 1) single-view to 3D asset; 2) single-view to"}, {"title": "Ablation Study on Alignment Strategy", "content": "To demonstrate the significance of our asset-body alignment strategy, we conduct an ablation study on Sim(3) optimization and penetration handling. Fig. 8 presents the comparisons between our strategy and the ablated strategies. As demonstrated, although the generated multi-view images are aligned with the rendered body from multiple 2D perspectives (Fig. 8b), the 3D assets produced from these multi-view images do not effectively guarantee alignment with the input images (Fig. 8c). This discrepancy arises from various multiview-to-3D diffusion models employing different techniques and normalization strategies during training to accommodate their specific architectures and objectives. As a result, the generated 3D meshes are often not perfectly aligned with the input multi-view images. This misalignment manifests as differences in scale, translation, and rotation between the reconstructed 3D mesh and the input multi-view images (Fig. 8c). By employing Sim(3) optimization, the 3D mesh becomes geometrically consistent with the multi-view observations and consequently aligns with the human body (Fig. 8d); however, there are still some penetrations between the asset and the body. As seen in Fig.d, by applying our penetration handling approach, we can achieve a penetration-free state for the asset and the body (Fig. 8e)."}, {"title": "5 LIMITATIONS AND DISCUSSIONS", "content": "This paper presents a method for guiding advanced 3D generative models to directly produce body-aligned 3D assets. Experimental results indicate that our approach offers significant advantages over existing methods in terms of prompt-following capacity, shape diversity, and shape quality. We believe that our method opens up a new era in 3D generative model-based garment modeling. However, a few changes still need to be addressed:\n1) Our method does not explicitly handle multi-layer garment generation. A potential direction for future work is to generate multiple garment layers in parallel and then assemble them on the target human body by resolving asset-to-asset penetrations, as demonstrated in industrial tools such as MetaTailor 4. We will explore this approach in future research. 2) The mesh topology of the generated asset is based on Marching Cube output, which is not yet production-ready. Since characters are intended to be animatable, topology becomes an even more crucial aspect compared to static objects. Recent researches on artistic style mesh generation, such as [9], offers a potential solution for addressing the mesh topology issue. 3) When running SAM, occlusion from the human body or other assets may result in incomplete single-view asset input. While leveraging inpainting tool such as SDXL [52] can recover from occlusions, the results are not always stable. Future work will focus on developing robust inpainting techniques specifically for the human body asset. 4) Our current approach primarily focuses on main-body fitting. Fine-grained asset generation, such as gloves that involve hand fingers, is not included in this version. This could be accomplished using a fine-grained, hand-aligned 3D asset dataset."}]}