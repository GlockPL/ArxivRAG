{"title": "A Tutorial on Clinical Speech AI Development: From Data Collection to Model Validation", "authors": ["Si-Ioi Ng", "Lingfeng Xu", "Ingo Siegert", "Nicholas Cummins", "Nina R. Benway", "Julie Liss", "Visar Berisha"], "abstract": "There has been a surge of interest in leveraging speech as a marker of health for a wide spectrum of conditions. The underlying premise is that any neurological, mental, or physical deficits that impact speech production can be objectively assessed via automated analysis of speech. Recent advances in speech-based Artificial Intelligence (AI) models for diagnosing and tracking mental health, cognitive, and motor disorders often use supervised learning, similar to mainstream speech technologies like recognition and verification. However, clinical speech AI has distinct challenges, including the need for specific elicitation tasks, small available datasets, diverse speech representations, and uncertain diagnostic labels. As a result, application of the standard supervised learning paradigm may lead to models that perform well in controlled settings but fail to generalize in real-world clinical deployments. With translation into real-world clinical scenarios in mind, this tutorial paper provides an overview of the key components required for robust development of clinical speech AI. Specifically, this paper will cover the design of speech elicitation tasks and protocols most appropriate for different clinical conditions, collection of data and verification of hardware, development and validation of speech representations designed to measure clinical constructs of interest, development of reliable and robust clinical prediction models, and ethical and participant considerations for clinical speech AI. The goal is to provide comprehensive guidance on building models whose inputs and outputs link to the more interpretable and clinically meaningful aspects of speech, that can be interrogated and clinically validated on clinical datasets, and that adhere to ethical, privacy, and security considerations by design.", "sections": [{"title": "I. INTRODUCTION", "content": "Speaking is a deceptively complicated and sensitive activity. We must think of the words to convey our message, organize the words in compliance with the rules of a language, and activate the muscles that allow us to produce clear and understandable speech. This process requires coordination across multiple regions of the brain and precise activation of more than 100 muscles. If there is a disturbance to any of these regions of the brain or the physical apparatus itself, it becomes apparent in the resulting speech. The potential for extracting clinically-rich information from such an easy-to-acquire signal has generated considerable excitement in the digital health and wellness communities, and speech has been referred to as the newest vital sign [1]. The promise is that any neurological, mental health, or physical disturbances that impact the speech production process can be passively detected from patients' speech patterns. To that end, there has been considerable interest in different academic communities and in industry in developing AI models for diagnosis, prognosis, and tracking of different clinical conditions using only speech (e.g. see [2], [3] for mental health, [4], [5] for cognition, [6], [7], [8] for nueromuscular disease, [9] for emotion recognition, [10], [11] for speech sound disorder, etc.).Speech has a dual role in neurology applications (Figure 1), capturing both upstream informa-tion, such as the presence and prognosis of neurological changes, and downstream consequences,like the impact on quality of life and social interactions [12]. In Parkinson's disease, for example,speech AI is used not only to detect pre-clinical symptoms (upstream) but also to assess howdysarthria affects social communication (downstream). Similarly, in schizophrenia, speech AIdetects early signs of psychotic episodes and evaluates their effects on social skills and functionalcapacity [13], [12].In the development of speech-based clinical solutions, supervised AI on diagnostic outcomeshas played a central role. Survey articles on dementia [14] and Parkinson's disease [15] showthat most clinical speech AI approaches involve starting with a labeled dataset of paired speechsamples and diagnostic labels, extracting high-dimensional (often clinically uninterpretable)features, and building predictive models based on these labels [16]. However, labeled clinicaldatasets are much smaller and more variable than the large datasets used for consumer-gradeautomatic speech recognition (ASR) models. For instance, studies on speech-based dementiaclassification often work with datasets spanning only tens to hundreds of minutes of speech, instark contrast to the large-scale ASR datasets [17], [18], [19].Recently, researchers have started focusing on the reliability of the supervised AI approach topredict diagnostic labels. For example, research has shown that AI models for disease predictionin fields like neuroimaging often report inflated accuracies that correlate negatively with samplesize, suggesting overfitting and potential issues with generalizability [20], [21]. Our own analysisof 59 speech-based classifiers used for detecting Alzheimer's disease revealed similar concerns[22], with negative correlations between reported accuracy and dataset size (Fig. 2), contrary totypical learning curve expectations. For correctly-trained AI models, as the sample size increases,the accuracy of the model should increase monotonically according to a power law model [23].We attribute these inconsistencies to several factors, primarily data leakage and variability inmodel accuracy estimates due to analytic flexibility. Data leakage, where models unintentionallyaccess information from the test set during training, is a significant concern [24]. Importantly,data leakage can also occur through repeated use of the same test data over time, even withoutdirectly training on it-models may gradually become fine-tuned to the test set through iterativeexperimentation and evaluation. Additionally, the flexibility available to model developers canlead to different outcomes even when working with the same dataset. For instance, a recentstudy [25] involving 46 research teams, each given the same dataset and research question,resulted in widely divergent models and performance estimates. This variability, combined withthe tendency to publish only high-performing models, contributes to systemic overoptimism inour field [22]. In short, this evidence points to systemic overoptimism about how well popularapproaches to clinical speech AI model development actually work."}, {"title": "II. THE HUMAN SPEECH PRODUCTION MECHANISM", "content": "To develop interpretable AI models for clinical speech data, an understanding of how humansproduce speech is of fundamental importance. Measuring changes in what we say (words, syntax)and how we say them (acoustic features, how speech sounds) allows for detection and classifi-cation of differences of interest. While the complete speaking process is mediated by complexneural circuits synergistically activating more than 100 muscles to create the acoustic signal, wecan adopt a simplified stage approach to compartmentalize the various components involved inspeaking. We can then reference these stages to discuss how various diseases/conditions map todeficits in these stages. Levelt's (1989) book, \u201cSpeaking: From Intention to Articulation\u201d presentsa modular model of all known aspects of speech production. It provides a valuable frameworkfor understanding the various processes at play and how things can go wrong [28]. Here weuse a simplified adaptation of Levelt's model, focusing on three stages that are relatively well-understood and supported by abundant research: Conceptualization, Formulation, Articulation.Conceptualization: Before anyone utters a word, they must develop a thought, idea, or message.They also must experience the intent to communicate that thought to someone. Conceptualizationoccurs within the context of an individual's perception of self, of time, and place, and anappreciation for the perspective of the receiver of this message (i.e., Theory of Mind). Con-ceptualization requires adequate functioning of the parts of the brain responsible for judgement,reasoning, memory, emotion, and social motivation [29]. There are several conditions that in-terfere primarily with the Conceptualization Stage of speaking, including psychiatric disorders(schizophrenia, bipolar disorder, major depressive disorder), dementias that include personalitychanges and/or hallucinations (frontotemporal dementia, Lewy body dementia), and the laterstages of Alzheimer's disease. Speech characteristics of impaired conceptualization result fromthe presence of 'negative symptoms' (psychomotor retardation, apathy), 'positive symptoms'(mania, hallucinations), and a distorted sense of reality (disorientation, paranoia, psychosis).Speech characteristics of negative symptoms include reduced speech output, imprecise articula-tion, and a tendency toward monotonicity and low or monoloudness [30]. Positive symptoms areassociated with rapid, pressed speech and a lack of coherence in the message being conveyed[31]. Individuals with a distorted sense of reality may speak very quickly or slowly, changetopics randomly, or produce incoherent speech. Signal for these speech characteristics can befound in both the acoustic speech signal as well as in analysis of the transcripts of the spokenmessage.Formulation: In this second stage, words are selected and sequenced to best convey the con-ceptualized message. The words must be sequenced according to the rules of the language beingspoken. But the word selection and sequencing also involve abstract decisions about the levelof specificity of words to use (\u2018car' versus \u2018convertible'), their emotional valence and intensity('dislike' versus \u2018despise'), and language devices (sarcasm, humor, double entendre). Thesedecision pair with paralinguistic decisions on how the outflow of the speech message (prosody)will enhance the intended meaning. For example, the sentence, \u201cI despise riding in convertibles,except at stop lights,\" might be spoken with an emphasis on 'despise' and a longer-than usualpause at the comma to invoke intrigue with the final clause. This string of richly phenotypedmental words is then translated into a score of sensorimotor commands to muscles necessary toproduce the sounds of the sequence of words, with timing and emphasis that underscores themessage's meaning. Health conditions that interfere with the Formulation Stage typically involvedamage to the cortical and subcortical language circuits of the brain. A cerebrovascular accident(CVA, or stroke) of the left cerebral hemisphere can cause aphasia, characterized by varyingpatterns of difficulty finding the words one wishes to speak, difficulty sequencing the words intomeaningful sentences, and difficulty understanding what others are saying. In contrast, a CVA inthe right cerebral hemisphere can leave language preserved, but one may struggle with perceivingand producing paralinguistic information that signals humor, sarcasm, or emotion. Individualswith early to middle-stage Alzheimer's disease can also struggle to find words, and the wordsthey do find tend to be less specific [32]. As with the Conceptualization Stage, evidence of allthese speech characteristics can be found in both the acoustic speech signal as well as in analysisof the transcripts of the spoken message.Articulation: After the message has been conceptualized and formulated into the mental se-quence of words with an accompanying set of sensorimotor commands, the commands areexecuted to produce speech. First, one inhales more deeply than at rest and begins to release acolumn of air that will travel upward through the vocal tract to be shaped into the words wehear. The ascending column of air will first encounter the larynx where it will be impeded bythe resistance of the closed vocal folds (cords). As pressure from the air builds up beneath them,the vocal folds blow open and begin to vibrate. This vibration chops the ascending column ofair into bursts of audible phonation (voice). The faster the vocal folds vibrate, which is finelycontrolled by laryngeal muscles, the higher the pitch of the voice (fundamental frequency).The loudness of the voice is modulated by adjusting the air pressure beneath the vibratingvocal folds. This requires precise control of the muscles that compress the lungs and those thatregulate the firmness of vocal fold closure during vibration. The higher the air pressure andthe tighter the vocal fold closure, the greater the amplitude of the vibration, and the louderthe voice becomes (decibels). Finally, the clarity of the voice (vocal quality) has to do withphysical characteristics of the vibration of the vocal folds. The more symmetrically the twovocal folds move during vibration, and the more completely they meet in the middle of theairway, the clearer the vocal quality sounds (harmonics-to-noise ratio). As the column of airmoves through the larynx, the portions of the column that encountered the vibrating vocal foldshave been set into audible acoustic energy that will ascend to be filtered through the upper vocaltract to produce voiced sounds (e.g., vowels, nasals, liquids and glides). The portions of the aircolumn that were undisturbed as they passed through open, non-vibrating vocal folds ascend tobe shaped into unvoiced sounds (e.g., unvoiced stops and fricatives). The filter function of theupper vocal tract is created by moving the articulators (tongue, jaw, lips, soft palate) to createtime-varying shapes and sizes of cavities, constrictions, and closures. The muscles of the softpalate and throat work in concert to shunt the acoustic energy through the nasal and/or oralcavities to produce nasal and non-nasal sounds, respectively. Different resonant cavities createdby movements of the tongue, jaw, and lips produce formants (regions of enhanced energy in thefrequency spectrum), whose patterns are generated by the vocal tract filter acting upon the sourcephonation and distinguish among vowel sounds. Constrictions created by close approximationsbetween the lips, tongue, and/or hard palate generate turbulent noise as the air rushes through toproduce fricative sounds like 's.' Closures of the vocal tract allow air pressure to build up andburst through to create stops like 'p.' Clear, highly intelligible speech is the result of maximallydistinctive resonant cavities, narrow and precise constrictions, and firm and precise closures,precisely timed across systems to produce the sounds of the spoken message. Any condition thatinterferes with this highly synchronized movement among the respiratory, phonatory, articulatory,and resonatory systems will be evident in the acoustic signal of speech. Focal conditions havespecific effects on speech. For example, a paralyzed vocal fold will result in breathy phonation,without impact on other speech subsystems. Conditions like amyotrophic lateral sclerosis impactmultiple targets in the brain and spinal cord, causing spastic and/or flaccid paralysis of muscles ofthe limbs, trunk, head, and neck. This results in reduced respiratory support, reduced articulatoryprecision, hypernasality, strained vocal quality, and overall slowed speech. The constellation ofspeech symptoms for any given condition derives from the location and extent of nervous systemdamage [33]. Further, conditions that impact the structure and function of the speech organs alsowill manifest in the speech signal. Cleft palate, laryngeal cancer, chronic obstructive pulmonarydisease, and many other conditions, impact speech in predictable ways [34], [35], [36]."}, {"title": "III. COMPARING CLINICAL SPEECH APPLICATIONS TO OTHER SPEECH APPLICATIONS", "content": "In this section we highlight the important differences between traditional applications ofspeech AI and clinical applications of speech AI and explain that straightforward application oftechniques that have worked well in traditional domains are often not appropriate for clinicalapplications. In the two-dimensional plot in Fig. 6 we depict important differences betweentraditional applications of speech AI and clinical speech AI. The x-axis on the plot representsthe stakes of the application. In this context \u201cstakes\u201d refers to the level of consequence or impactthat outcomes of speech AI applications have. The y-axis represents the amount of variability inthe ground-truth labels used to train the supervised learning algorithms. For example, transcriptsfor speech recognition are reliable; however, there is considerable variability in diagnosing mildcognitive impairment or depression [37], [38]. On this two-dimensional plot, we plot certainapplications of speech AI and refer to them in the description below.One of the primary differentiators is the elevated stakes associated with clinical speech AI,where the outcomes have a direct bearing on human health. For example, a mis-recognitionin ASR might result in a trivial transcription error (e.g. a misunderstanding by Siri or Alexa),whereas inaccuracies in clinical speech AI could lead to misdiagnosis or delayed treatment ofsevere health conditions such as Alzheimer's or Parkinson's disease. Adding to the model buildingchallenge is the high diagnostic label variability associated with these conditions, especiallyprominent in the early stages of neurological diseases. Recent meta-analyses reveal that themisdiagnosis rates in Alzheimer's disease exhibit a wide range with sensitivities from 71% to87%, and specificities from 44% to 71%. Similarly, the diagnosis of Parkinson's disease in theinitial five years is reported to be inaccurate nearly half the time [39]. This contrasts starklywith traditional speech AI like speaker recognition, which has a clear ground truth. Similarly,the ground truth in ASR has relatively low inter-transcriber variability [40]. The variability indiagnostic labels presents an added layer of complexity in clinical speech AI. This variabilitycould be attributed to the evolving nature of diseases, subjective interpretation by differentclinicians, or the presence of comorbid conditions that might obscure the primary diagnosis. Itnecessitates more thorough modeling and validation approaches, as we will highlight throughoutthis tutorial.For many traditional applications of speech AI, models are evaluated using simple measuresof accuracy (e.g. word error rate in ASR; equal error rate in speaker verification). In contrast,clinical models require more comprehensive validation. Beyond measures of accuracy, validationin clinical settings requires a broader set of criteria to ensure that the models are reliable, robust,and clinically viable. This involves conducting rigorous testing across diverse cohorts, capturingreal-world deployment feedback, and undergoing regulatory scrutiny to ascertain the safety andeffectiveness of these models. A starting point for this analysis is the Verification, Analyticalvalidation, Clinical Validation (V3) framework recently proposed for digital health. In sectionVI-VII, we provide an overview of how this validation can be conducted for speech-based digitalhealth tools to ensure that developed models can perform well under various conditions and arecapable of handling the inherent uncertainties associated with clinical data.Traditional speech AI often relies on standard input representations like Mel Frequency Cep-stral Coefficients (MFCCs), the mel-spectrum, other open-source feature sets [41], or deep-learning-based speech representations [42]. However, the diversity in clinical applications callsfor more bespoke, individually validated representations. In contrast to non-clinical applicationswhere speech data is available at large scale, the scarcity of large-scale clinical data requiresreduction of sample complexity; this can be achieved via simpler, interpretable, and individually-validated features. Moreover, the high-dimensional nature of traditional input representationsoften leads to models that lack interpretability a critical aspect in clinical scenarios forensuring reasonable and reliable model outputs. There is great utility in developing acousticmarkers of the underlying speech symptoms. The ground truth for these models can be attaineddirectly from the audio and/or transcript, making it much easier to obtain than clinical diagnosticlabels. For example, breathy voice is a characteristic symptom of Parkinson's disease. Featureparameters that measure breathiness in speech, such as cepstral peak prominence [43], can be"}, {"title": "IV. THE CLINICAL SPEECH PIPELINE", "content": "Clinical AI models are typically trained via supervised learning to analyze a subject's speechdata, extract the clinically-relevant information, predict the clinical labels (e.g. severity score, in-telligibility score, subtype of a speech disorder, etc.), and provide actionable insight to clinicians.As described by the FDA [45], the technical pipeline for training and deployment of clinical AImodels as AI-based software-as-a-medical device (SaMD) tools is illustrated in Figure 7. Thepipeline starts with the design of data collection protocol. It defines how the clinical populationof interest is recruited, how speech elicitation stimuli are designed, and how data is collected.During data collection, participant speech is captured by speech recording devices in the formof acoustic signals. Clinical labels associated with the participant, such as symptoms of disease,clinical scores, and other clinically meaningful labels, are provided by experts and associatedwith the collected speech. In AI parlance, the speech signals and clinical labels become the\"data\" and \"labels\" stored in a structured database. Feature engineering plays an important rolein extracting useful information from the speech data to facilitate clinical speech AI. Domainknowledge in speech signal processing, speech production, clinical speech science, etc., can beincorporated into the process. With a sufficient amount of features and clinical labels, statisticalmodels are trained to identify the relationship between the features and the labels. Validation ofthe trained models is performed using a held-out set of test data to determine the feature andmodel to be deployed. Following deployment, real-world model performance is continuouslymonitored after deployment. With speech data newly acquired during deployment, the modelcan be iteratively updated and redeployed.Errors in each part of the model development process can negatively impact the efficacy ofthe analytic system. For example, if speech data is collected inappropriately (e.g. insufficientconsideration in selection of speech elicitation tasks, improper implementation of recordingspeech, etc.), the extracted features may not carry sufficient clinically-relevant information,leading to failure of statistical modeling and model deployment. If methods of feature extractiondo not consider clinical knowledge, or the statistical models are overly complex, clinicians maylack necessary evidence to interpret the model output. If models are trained on data sets thatare not representative of the deployment scenario, model performance could be impacted. Thesefactors could limit the adoption of the models in clinical settings. Given the spectrum of factorsthat influence model development in clinical speech AI, potential approaches that steer the modeldevelopment towards better interpretability and reliability are discussed in the following sections.In Section V, we will discuss the relationship between degrees of freedom in speech contentand the computational load asserted to the speakers. The discussion provides the researcherswith a mental model for determining the speech elicitation tasks that magnify speech changesfor a given clinical condition. Then the section discusses important considerations regarding datacollection. We provide practical guidelines for speech recording setups for real-world settings,with the goal of obtaining speech data reliably and in a way that captures underlying symptomsassociated with a condition. Finally, the section also considers verification of the recording setupto ensure that acquired data is of sufficiently high quality for the application of interest.Section VI proposes a shift from using speech features to utilizing speech measures as animportant step to develop clinically interpretable and generalizable clinical speech models. Wedraw a distinction between conventional high-dimensional speech features used by the technicalspeech community, and interpretable speech measures which are required for validated clini-cal/scientific studies of speech. We highlight the limitations of high-dimensional uninterpretablespeech features, and describe the benefits of developing speech measures linked to constructsof clinical interest during feature engineering. The section further discusses the validation ofspeech measures for clinical applications, as informed by the V3 validation framework [27]. Wepresent examples of speech measures validated in accordance with this framework across variousstudies. We further highlight the drawback of relying on invalidated features during clinical modeldevelopment. The Section will then discuss technical details of speech measure design. Thesestrategies range from knowledge-driven to data-driven approaches. While the knowledge-drivenapproach can be readily adopted from clinical and speech science research, our further emphasiswill be on introducing the data-driven approach to derive valid and reliable speech measures.This section demonstrates new opportunities to the clinical community on the integration of AIand clinical knowledge in speech analyses. The process of designing novel speech measuresneeds to be coupled with rigorous analytical validation. We will demonstrate existing works thathave performed analytical validation, which will explain the importance and implication of doingSO.The Section VII then shifts the focus on the design of clinical AI model, which predictsclinical labels based on the input speech features. We discuss the most common issues thatimpede the development of generalizable and explainable clinical AI model. We then providestrategies to mitigate these issues through the use of validated and interpretable lower-dimensionalspeech measures as inputs, and the adoption of AI models that intrinsically offer explanability.Similar to speech measures design, as discussed in Section VI, we will introduce the clinicalvalidation framework for the developed model. The process not only concerns the reliability ofthe predicted clinical label, it further validates whether the design of the model (from modelinput to model output) answers the clinical question of interest. We will illustrate examples andcounter-examples to emphasize the importance of clinical validation before deploying the clinicalAI model in real-world settings."}, {"title": "V. SPEECH DATA COLLECTION", "content": "This section covers the key aspects of speech data collection", "46": [47], "48": ".", "49": ".", "50": "For dementia patientswhose cognitive function is impaired", "51": ".", "interest[52": [53], "54": ".", "data": "What is the goal of the model development (e.g., classification, diagnosis, differentialdiagnosis, change (progression/remission/improvement), at a group level or individual, cross-sectional or longitudinal)? The answer to this question estimates the level of sensitivity andspecificity the modelling will require to be successful, and how much speech will need tobe collected and at what intervals.\u2022 What aspects of speech production (Conceptualization, Formulation, Articulation) are mostlikely impacted by the disease or condition to be analyzed and what is their severity level?These deficit patterns delimit the type of pressure the speech task should place on theparticipant to enhance the signal. For example, if the articulation aspect of speech productionis primarily impacted by the disease or condition, and these deficits manifest only mildlyin the study population, maximum performance speech tasks can be selected to amplifyarticulation deficits.\u2022 Are there participant-related limitations that must be considered in task design (e.g., vision,hearing, cognition, reading ability, time to fatigue)?\u2022 What are the language and cultural considerations of the participant population (language(s),native versus non-native speakers, dialect, accent; rhythm and phonology, and semantics andsyntax of the language(s) collected)?\u2022 How will the spoken responses be processed (ASR, digitized acoustic signal) and what typeof measures will be extracted (NLP, spectral-temporal measures)?These considerations can guide to trim the search space for the suitable speech elicitation task.See the example in Table I, which considers speech elicitation task design for the development ofa tool to classify between cognitively intact individuals and those with mild cognitive impairmentbased on speech. Development of the tool calls for speech elicitation tasks that place pressure oncognitive functioning (memory and language formulation) to amplify subtle differences betweenthe groups' performance metrics. It is useful to consider potential speech elicitation tasks interms of the spoken response variability and how difficult the task is to complete.Figure 8 portrays examples of speech elicitation tasks plotted relative to the degrees of freedomof the spoken responses (x-axis) and the degree of computational and/or sensorimotor pressureinherent in completing the task (y-axis). For example, reading aloud a list of words has a lownumber of degrees of freedom because each word has exactly one correct spoken response.Further, reading a word places little pressure on the cognitive-language system and does notrequire higher order executive function or memory. Describing what's happening in a picturerequires a synthesis of the objects and actions depicted in the picture, and inferences abouttheir temporal and spatial relations, thereby making it a more computationally challengingspeech elicitation task than word reading. But because the picture constrains the content ofthe response, degrees of freedom are lower than for tasks with unconstrained topics, such asan open conversation. The reduction in degrees of freedom can provide benefit and can serveas a drawback. For example, a cross-sectional study design may benefit from control over thecontent of speech as an open conversation unconstrained by topic could add noise to the data,obscuring any small group differences. However, the reduced freedom in describing a picturemay limit the richness and spontaneity of language use, which could be a drawback in studiesaiming to capture more naturalistic speech patterns.In the MCI example posed above, we would expect automatic speech recognition (ASR) tobe relatively accurate in a conversational task for native speakers of English in this age range.ASR would be even more accurate in tasks with lower degrees of freedom, in which ASRmodel pre-training could benefit accuracy. But because the intended analysis level is naturallanguage processing on connected speech, a story recall task (immediate and delayed followingintervening distractor tasks) would be a reasonable option for placing pressure on the cognitivesystem, while keeping degrees of freedom relatively constrained. The task is difficult becausethe story's contents need to be understood, remembered, retrieved, and retold. Adding a delayedcondition places further pressure on the system to induce response errors in the context of mildlyimpaired cognition. This also allows for a comparison of performance in the immediate-versus-delayed condition within individuals, providing a metric of performance stability.Different elicitation tasks also impose varying levels of computational load on the humanbrain, as shown in the y-axis of Figure 8. Tasks like picture naming and passage reading induceminimal computational load, as the spoken content is predefined by clinicians and requireslittle effort from the speaker. In contrast, more complex tasks such as picture description,interviews, conversations, and story recall necessitate greater coordination of memory, attention,and organization, thereby imposing a higher cognitive load on the speakers.An example of a high computational load task is illustrated in Table II, in which participantswill provide speech samples intermittently over time to track evidence of chronic obstructivepulmonary disease (COPD) progression in the context of an intervention. This means that thespeech elicitation task must be feasibly executed reliably across a large severity range. Due to thevariability in the patient population, reducing degrees of freedom in the speech elicitation reducespossible confounds. However, a desirable elicitation task also taxes the respiratory system (highcomputational load). A maximum performance tasks that put pressure on the respiratory systeminclude taking a deep breath and sustaining a phonation for as long as possible or counting ashigh as possible or saying the alphabet on a single breath. In the context of significant pulmonaryfunction deficits, reading a passage aloud also would be considered a challenging task in whichspeakers need to pause increasingly frequently to take a breath while reading the passage aloud.Speech production involves the precise integration of language, memory, cognition, and sen-sorimotor functions. Researchers should review the existing literature to identify the stageswhere the speech production of clinical populations diverges from that of healthy populations.Selecting appropriate elicitation tasks is crucial to elicit speech that carries clinically relevantinformation and to examine the upper limits of the subjects' performance. For example, patientswith cleft lip or with oral cancer have damaged sensorimotor functions, while their languageskills, memory and cognition function may be intact. Given the relevant speech problems aremanifested in articulation, structured speech tasks that can control phonetic content of speech,such as isolated pseudo-work tasks, and sentence reading tasks, can be used for data collection[55", "56": ".", "57": [58], "59": ".", "60": ".", "descriptions[61": [62], "individuals[63": ".", "64": ".", "65": ".", "66": [67], "68": "spatiotemporal index (STI) is measured on English-speakingyoung adults using four different sentences varied by length and complexity. Results showed thatthe sentence type had significant effect on the measured STI values. Although these sentences allbelong to reading task, the variability of design in sentences affect the speech feature, meaningthe differences in sentences could result in different interpretations of results. Understandingthe differences between elicitation tasks, the variability within each task, and the relationshipbetween tasks and features is important for clinical model development.The protocol for speech data collection is a critical step to ensure acoustic and/or linguisticinformation pertinent to the clinical condition of the speakers can be accurately measuredfrom the digital speech signal. There is now significant flexibility in selection of devices (e.g.smartphones vs. professional recorder) and platforms (face-to-face vs. remote session) for speechdata collection. However, such convenience comes at the cost of potential contamination of theacquired data by unwanted effects, which can negatively impact subsequent analysis. For instance,given the rising popularity of telepractice, Tran et al. investigated how speech compressionalgorithms employed in data transmission could impact acoustic measures used in dysarthricspeech assessment [69", "laptops[70": ".", "71": ".", "27": "."}]}