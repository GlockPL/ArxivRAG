{"title": "Node Importance Estimation Leveraging LLMs for Semantic Augmentation in Knowledge Graphs", "authors": ["Xinyu Lin", "Tianyu Zhang", "Chengbin Hou", "Jinbao Wang", "Jianye Xue", "Hairong Lv"], "abstract": "Node Importance Estimation (NIE) is a task that quantifies the importance of node in a graph. Recent research has investigated to exploit various information from Knowledge Graphs (KGs) to estimate node importance scores. However, the semantic information in KGs could be insufficient, missing, and inaccurate, which would limit the performance of existing NIE models. To address these issues, we leverage Large Language Models (LLMs) for semantic augmentation thanks to the LLMs' extra knowledge and ability of integrating knowledge from both LLMs and KGs. To this end, we propose the LLMs Empowered Node Importance Estimation (LENIE) method to enhance the semantic information in KGs for better supporting NIE tasks. To our best knowledge, this is the first work incorporating LLMs into NIE. Specifically, LENIE employs a novel clustering-based triplet sampling strategy to extract diverse knowledge of a node sampled from the given KG. After that, LENIE adopts the node-specific adaptive prompts to integrate the sampled triplets and the original node descriptions, which are then fed into LLMs for generating richer and more precise augmented node descriptions. These augmented descriptions finally initialize node embeddings for boosting the downstream NIE model performance. Extensive experiments demonstrate LENIE's effectiveness in addressing semantic deficiencies in KGs, enabling more informative semantic augmentation and enhancing existing NIE models to achieve the state-of-the-art performance. The source code of LENIE is freely available at https://github.com/XinyuLin-FZ/LENIE.", "sections": [{"title": "I. INTRODUCTION", "content": "Node Importance Estimation (NIE) aims to evaluate the significance of nodes within a graph, providing a crucial foundation for practical applications such as resource allocation, data management, and recommendation systems [1]\u2013[3]. Earlier methods for the NIE task, e.g., Google's PageRank [4], compute node importance scores by analyzing the quantity and quality of incoming edges to help users identify the most relevant pages. However, these early NIE methods, including random walk-based [4]\u2013[6] and centrality-based [7]\u2013[10] approaches, are primarily designed for homogeneous graph data and evaluate node importance solely based on graph topology. With the growing complexity of application scenarios, heterogeneous graphs, which contain richer information than homogeneous graphs, have been widely utilized [11]. Effectively leveraging richer information in graph data to more comprehensively and accurately assess node importance has become a key focus of recent NIE research.\nKnowledge Graphs (KGs), a prominent type of complex heterogeneous graphs, encompass real-world knowledge through triplets (head node, relational edge, tail node), which contain rich structural and semantic information. Many recent studies have focused on leveraging this wealth of information in KGs to enhance NIE tasks [2], [12]\u2013[17]. GENI [2], the first NIE method for KGs, employs a GNN-based aggregation and update mechanism to effectively capture structural information within KGs, achieving promising NIE results. Later on, other research efforts have explored incorporating more information from KGs, such as semantic content [15], multiple input signals [12], local and global features [13], and attention to the most important nodes [16], to enhance NIE performance. Notably, several studies have demonstrated that exploiting node descriptions can improve the model performance of NIE [13], [15]\u2013[17], further verifying the benefits of integrating semantic information from KGs into NIE tasks.\nHowever, KG data is often incomplete [18], [19], leading to deficiencies in their semantic information. As illustrated in Figure 1, the semantic information of nodes in KGs would encounter issues such as insufficient descriptions, missing descriptions, and inaccuracies. Specifically, the insufficient descriptions refer to short or incomplete textual descriptions of nodes in KGs; the missing descriptions indicate the absence of textual descriptions for some nodes; and the inaccuracies refer to inaccurate semantic information of nodes regarding their description texts or related triplets. These issues would limit the performance improvement of existing NIE methods that make use of semantic information from KGs.\nMotivated by the recent advancements in Large Language Models (LLMs), we intend to adopt LLMs to improve KGs by addressing the incompleteness and inaccuracies [20], [21]. LLMs excel at providing extra knowledge beyond the given KG, since they are trained on vast amounts of texts [22], [23]. Meanwhile, the techniques like in-context learning and retrial augmented generation [24], [25] enable LLMs to smoothly integrate with KGs, thereby assisting LLMs to generate more precise and specific contents for the given KG. These abilities make LLMs possible to tackle the issues of insufficient descriptions, missing descriptions, and inaccurate semantic information in KGs. Consequently, we introduce LLMs to alleviate these issues restricting the performance of NIE models.\nTo enrich the semantic information for a given KG and accordingly further support the NIE task, we propose the LLMs Empowered Node Importance Estimation (LENIE) method. Concretely, we first develop a clustering-based triplet sampling strategy to extract the semantic knowledge of a node from the KG, which serves as the contexts surrounding that node for the given KG; the clustering-based sampling strategy is used to keep the diversity and coverage of the knowledge offered by the given KG. Second, the node-specific adaptive prompts are designed to combine the sampled triplets with the original node descriptions, enabling LLMs to generate richer and more precise augmented node descriptions, thus utilizing LLMs' understanding of real-world entity and the ability of integrating domain knowledge sampled from the given KG. Finally, the augmented descriptions are adopted to initialize node embeddings with richer and more precise information to boost the performance of various existing NIE models.\nThe main contributions of this work are as follows:\n\u2022 Existing NIE methods have not considered the situation where KG data is incomplete, as evidenced by the fact that the semantic information is possibly insufficient, missing, or inaccurate. We accordingly introduce a novel framework that leverages LLMs for semantic augmentation in KGs for node importance estimation.\n\u2022 Technically, we develop a clustering-based triplet sampling strategy to effectively keep the diversity and coverage of the knowledge offered by the given KG. We also design the node-specific adaptive prompts to smoothly integrate the sampled triplets with the original node descriptions, aiming to guide LLMs in generating richer and more precise augmented node descriptions for further improving the performance of NIE models.\n\u2022 Extensive experiments demonstrate that LENIE can boost the performance of existing NIE models across various KGs and metrics, as well as achieve the new state-of-the-art NIE performance. Furthermore, we also conduct experiments to confirm the effectiveness of key designs in LENIE and show the capability of LENIE in addressing the semantic deficiencies in KGs.\nTo our best knowledge, this is the first attempt to incorporate LLMs into the NIE task. And to facilitate future research in NIE and LLM communities, we release our source code at https://github.com/XinyuLin-FZ/LENIE."}, {"title": "II. RELATED WORK", "content": "NIE has evolved from applications in simple homogeneous graphs to complex heterogeneous graphs, with a growing focus on leveraging the wealth of information within graphs to more accurately assess node importance. Earlier work primarily focused on homogeneous graphs [4]\u2013[8], where nodes and edges each have a single type, and these methods are typically categorized into random walk-based and centrality-based methods. PageRank (PR) [4] is a classic random walk-based method that captures the structural information of a graph and evaluates node importance through random walks with equal probability on the graph. Personalized PageRank (PPR) [5] extends the standard random walk approach by incorporating specific topic information. Random Walk with Restart [6] enhances PR by adding a restart mechanism. Unlike random walk-based methods, centrality-based methods tend to identify critical nodes in the graph. Degree centrality [7] evaluate a node's importance by its number of direct connections. Closeness centrality [8] measures a node's importance by the inverse of the sum of its shortest path distances to all other nodes in the network. More concepts and improved versions of centrality-based methods can be found in [9], [10]. In summary, both methods mentioned above can assess node importance in graphs. However, they focus on homogeneous graphs, as they rely on the topology of the graph structure for the NIE task, making it challenging to leverage the richer information available in more complex graphs.\nIn recent years, GNN-based models have excelled in handling complex heterogeneous graphs like KGs by aggregating and propagating rich information within the graph, achieving state-of-the-art results in numerous NIE tasks [2], [12]\u2013[17]. GENI [2] is the first to apply GNN for estimating node importance in KGs, utilizing an attention mechanism to aggregate graph structure information for the NIE task."}, {"title": "III. METHODOLOGY", "content": "Knowledge Graphs or KGs: A knowledge graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{R}, \\mathcal{T})$, represents relationships between real-world entity nodes, where $\\mathcal{V}$, $\\mathcal{R}$ and $\\mathcal{T}$ denote entities, relationships and triplets, respectively. Specifically, $(h,r,t) \\in \\mathcal{T}$ denotes a set of triplets, where head entity $h$ and tail entity $t$ both come from entity set $\\mathcal{V}$, and relation $r$ comes from relation set $\\mathcal{R}$. In a KG, which is a type of heterogeneous graph, two nodes can be connected by different types of edges, indicating that there can be multiple relationships between two entities.\nSemantic Information in KGs: The defining characteristic of KGs is their rich semantic information, where each node can have its corresponding text description, and the triplets also reflect semantic information to some extent.\nNode Importance: The importance of node $I_i \\in \\mathbb{R}^+$ is typically represented as a non-negative number, reflecting the significance of the entity within the KG. The value of $I_i$ is derived from real-world scores, such as movie ratings or the popularity of singers. Following previous works [2], [15], [16], we utilize the log transformation of real-world scores as $I_i$.\nNode Importance Estimation or NIE: Given a Knowledge graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{R}, \\mathcal{T})$, a set of interested nodes $\\mathcal{V}_s \\subset \\mathcal{V}$, and a set of partially known importance scores $\\{s\\}$ for $\\mathcal{V}_s$, NIE aims to learn a function $f: \\mathcal{V}_s \\rightarrow \\mathbb{R}^+$ that predicts the node importance score for each node in $\\mathcal{V}_s$. To ensure the comparability of the importance scores for nodes in $\\mathcal{V}_s$, the nodes are typically of the same or similar type.\nFigure 2 illustrates the overview of the proposed framework, consisting of three key steps: Semantic Information Extraction from KG, Augmented Node Descriptions Generation, and NIE using downstream GNN-based models.\nSpecifically, we first develop a clustering-based triplet sampling strategy to effectively extract diverse semantic information for each interested node from KGs and integrate this information into triplet text. Next, we design node-specific adaptive prompts by combining the sampled triplet text with the node descriptions (if available) to define the nodes, enabling LLMs to generate accurate and informative augmented node descriptions, thus fully utilizing LLMs' understanding of real-world entites. Finally, the generated augmented node descriptions are encoded into semantically rich embeddings to further improve the performance of downstream NIE models. The details are elaborated in the following sections."}, {"title": "C. Semantic Information Extraction from KG", "content": "Each entity node in a KG is connected to relevant entity nodes through triplets, which serve as the fundamental units representing the structure of the KG and inherently contain rich semantic information. To effectively extract this semantic information, we first perform subgraph extraction for each interested node $v_i \\in \\mathcal{V}_s$, i.e., we collect all triplets that involve $v_i$, as described below:\n$\\mathcal{T}_{v_i} = \\{(h,r,t) | \\{h, t\\} = \\{v_i, n\\}, v_i \\in \\mathcal{V}_s, n \\in \\mathcal{N}_1(v_i), r \\in \\mathcal{R}\\}$ (1)\nwhere $\\mathcal{N}_1(v_i)$ represents the set of all One-hop neighboring nodes of $v_i$, and $\\{h,t\\} = \\{v_i, n\\}$ indicates that if either $h$ or $t$ is $v_i$, the other node in the triplet is the neighboring node $n \\in \\mathcal{N}_1(v_i)$, with $r \\in \\mathcal{R}$ denoting the relationship between $h$ and $t$. Note that the collected triplets must include the name texts of all $h, t$, and $r$ in $\\mathcal{T}_{v_i}$. Subsequently, to better reflect the semantic information, we convert each triplet in $\\mathcal{T}_{v_i}$ into its corresponding sentence text format, as follows:\n$\\mathcal{T}_{v_i} = \\{f_{\\text{sentence}}(h, r, t) | (h, r, t) \\in \\mathcal{T}_{v_i} \\}$ (2)\nwhere $f_{\\text{sentence}}$ is a function that converts a triplet $(h,r,t)$ into a sentence text, such as 'h's r is t.' The above strategy extracts the semantic information of node $v_i$ from the KG in textual form. However, in real-world KGs, some popular nodes with many neighbors have numerous associated triplets, leading to excessively long and redundant extracted text, which increases processing difficulty for language models. Therefore, sampling the triplets connected to node $v_i$ is crucial. The triplet sampling strategy can either utilize random-based triplet sampling strategy or the proposed clustering-based triplet sampling strategy, with the differences explained as follows.\n1) Random-based Triplet Sampling Strategy: To ensure sampling fairness, random-based sampling with equal probability remains one of the most widely used algorithms. This strategy performs random-based triplet sampling without replacement from a large set of triplets, ensuring both fairness and uniqueness of the selected samples, as illustrated below:\n$\\mathcal{T}_{v_i}^o = \\text{RandomSample}(\\mathcal{T}_{v_i}, \\min(k, |\\mathcal{T}_{v_i}|))$ (3)\nwhere $\\mathcal{T}_{v_i}^o$ represents the set of triplet sentences after random-based triplet sampling, $k$ is the desired number of sampled triplets, and $\\min(k, |\\mathcal{T}_{v_i}|)$ ensures that the number of selected samples does not exceed the total number of triplet sentences $|\\mathcal{T}_{v_i}|$ in $\\mathcal{T}_{v_i}$. However, the random-based triplet sampling strategy is influenced by sample distribution. For instance, if $v_i$ shares the same relationship type with many neighboring nodes, triplet sentences associated with that relationship are more likely to be selected. In fact, we aim to extract semantic information about $v_i$ from a diverse range of relationship types or neighboring node types. Therefore, we recommend an alternative sampling strategy based on semantic clustering.\n2) Clustering-based Triplet Sampling Strategy: To extract semantic information from the KG that covers as many relationship and node types as possible, we propose clustering the triplet sentences in the semantic space for sampling. Specifically, all triplet sentences associated with $v_i$ are mapped into the semantic space using a text encoder, as follows:\n$\\mathcal{E}_{v_i} = \\{f_{\\text{text\\_encoder}}(s) | s \\in \\mathcal{T}_{v_i} \\}$ (4)\nwhere $\\mathcal{E}_{v_i}$ represents the set of embeddings with semantic information obtained by encoding each triplet sentence $s$ in $\\mathcal{T}_{v_i}$. Next, the clustering algorithm is applied to find the cluster centers of the embeddings, as illustrated below:\n$\\mathcal{C} = \\{c_1, c_2,..., c_k\\} = f_{\\text{cluster}} (\\mathcal{E}_{v_i}, k)$ (5)\nwhere $c_k$ represents the $k$-th cluster center, and $k$ is the total number of clusters. Note that $k$ also cannot exceed the total number of triplet sentences $|\\mathcal{T}_{v_i}|$ in $\\mathcal{T}_{v_i}$. The clustering algorithm divides all triplet sentences related to $v_i$ into $k$ clusters, with the cluster centers representing the most distinct points in the semantic space. Therefore, selecting the triplet sentences closest to each cluster center in the semantic space is most likely to capture diverse relationship types and neighboring node types, i.e.,\n$\\mathcal{T}_{v_i}^c = \\{s_{m_j} | m_j = \\arg \\min_m d(c_j, e_m), j = 1,2,...,k\\}$ (6)\nwhere $\\mathcal{T}_{v_i}^c$ represents the set of triplet sentences after clustering sampling, $d(c_j, e_m)$ is the distance between cluster center $c_j$ and the embedding $e_m \\in \\mathcal{E}_{v_i}$, and $s_{m_j} \\in \\mathcal{T}_{v_i}$ is the triplet sentence corresponding to $e_m$ that is closest to $c_j$.\nThrough the above strategy, we achieve semantic information extraction for each interested node in the KG by converting it into triplet sentence form. However, this semantic information is dependent on the scale and quality of the KG. To gain a more comprehensive understanding of these real-world nodes, we leverage LLMs, which possess vast knowledge bases, to generate text-based outputs that enhance the semantic information of these nodes."}, {"title": "D. Augmented Node Descriptions Generation", "content": "Leveraging the semantic information extracted from the KG, LLMs can mitigate the negative effects of hallucination and synonym ambiguity, enabling the generation of more accurate and informative description texts for real-world nodes, thereby enhancing the KG's overall semantic information to support the NIE task. To fully leverage the LLMs' understanding of $v_i$, we need to construct adaptive prompts tailored to different $v_i$ by incorporating its semantic information. After applying the strategy described in Section III-C, we concatenate all the sampled triplet sentences of $v_i$ into a single text, as follows:\n$\\text{Triplets}_{v_i} = \\bigcup_{s \\in \\mathcal{T}_{v_i}^o} s$ (7)\nwhere $\\text{Triplets}_{v_i}$ represents the sampled triplet text for node $v_i$, which includes the triplet sentences obtained through sampling. Note that the length of $\\text{Triplets}_{v_i}$ must remain below the input limit of the LLM, allowing room for additional content. Next, we combine all available semantic information of $v_i$ from the KG, i.e., the sampled triplet text and the original node description text (if available), to construct the adaptive prompt, as detailed below:\n$\\text{prompt}_{v_i} = \\tau_{\\text{Triplets}_{v_i}} \\cup \\tau_{\\text{Descriptions}_{v_i}} \\cup \\tau_{\\text{Generation\\_Task}}$ (8)\nwhere $\\text{prompt}_{v_i}$ is composed of $v_i$'s sampled triplet text $\\tau_{\\text{Triplets}_{v_i}}$, description text $\\tau_{\\text{Descriptions}_{v_i}}$, and $\\tau_{\\text{Generation\\_Task}}$ which provides the instruction for the LLMs to perform augmented description text generation. Each interested node $v_i$'s augmented description text $D_{\\text{Generated}_{v_i}}$ is generated by the LLM, guided by its corresponding prompt, as follows:\n$D_{\\text{Generated}_{v_i}} = \\text{LLM}(\\text{prompt}_{v_i}).$ (9)\nThe augmented description text generated by the above method not only incorporates the semantic information of the node from the KG and its own textual description (if available) but also integrates external knowledge provided by the LLM. Additionally, this approach addresses semantic deficiencies in KGs, enhancing the node's semantic representation and improving NIE performance in downstream GNN-based models."}, {"title": "E. Downstream GNN-based NIE", "content": "To assess node importance on KGs with diverse node and edge types, we utilize a GNN-based model that integrates both structural and augmented semantic information, aiming to achieve superior performance in the NIE task. Augmented descriptions $D_{\\text{Generated}}$ for each $v_i \\in \\mathcal{V}_s$ are generated using the proposed method, then encoded into vectors via a text encoder as their initial embeddings $h_i^{(0)}$, as shown below:\n$h_i^{(0)} = f_{\\text{text\\_encoder}} (D_{\\text{Generated}_{v_i}}), v_i \\in \\mathcal{V}_s.$ (10)\nThese $h_i^{(0)}$ capture rich semantic information. Subsequently, the downstream GNN-based NIE model aggregates and updates these information across the graph, as described below:\n$h_i^{(t+1)} = \\text{Update} \\left( h_i^{(t)}, \\mathop{\\text{Aggregate}}_{j \\in \\mathcal{N}(i)} \\left( \\phi(h_i^{(t)}, h_j^{(t)}, h_{ij}^{(t)}) \\right) \\right)$ (11)\nWhere $h_i^{(t)}$ and $h_j^{(t)}$ represent the embeddings of node $v_i$ and its neighboring node $j \\in \\mathcal{N}(i)$ at time step $t$, and $h_{ij}^{(t)}$ is the embedding of the edge between them. The GNN-based model first aggregates information from $v_i$, its neighbors, and the corresponding edges by applying differentiable functions $\\phi$ (e.g., artificial neural networks) and an information aggregation operator (e.g., sum, mean, or max), to compute the message around $v_i$ in the graph. Subsequently, the model updates $v_i$'s embedding by combining its $h_i^{(t)}$ with the message, resulting in the updated embedding $h_i^{(t+1)}$ at time step $t+1$. This approach enables the GNN-based model to facilitate feature interactions across various node and edge types, enhancing performance in the NIE task.\nCurrently, the State-Of-The-Art (SOTA) performance in NIE tasks on KGs is achieved by RGTN [15] and its improved version, LICAP [16], both of which are GNN-based models. Our proposed semantic enhancement method LENIE can further improve their performance. Specifically, RGTN aggregates and updates the LENIE-enhanced embeddings $h^{\\text{LENIE}}$ on the graph as $h_i^{(t)} = f_{\\text{GNN-RGTN}}(h^{\\text{LENIE}})$, while using projection matrices to compute node importance scores $s_i = f_{\\text{scores-RGTN}}(h_i^{(t)})$ for each $v_i$. Building on this, LICAP introduces contrastive learning for pretraining embeddings, aiming to make nodes with similar importance scores have closer embeddings, expressed as $h^{\\text{LICAP}} = f_{\\text{pretraining-LICAP}}(h^{\\text{LENIE}})$, thereby further enhancing the performance of downstream RGTN model. LENIE's focus on semantic information enhancement not only improves the performance of current SOTA models on the NIE task but also benefits a wide range of downstream NIE models. This suggests that LENIE could continue to enhance NIE models in the future, enabling more accurate node importance prediction on KGs."}, {"title": "IV. EXPERIMENTAL SETTINGS", "content": "To assess the effectiveness of the proposed method, we perform experiments on three real-world knowledge graphs. The statistics of them are shown in Table I. Each dataset includes node importance scores, triplets, and textual information. The logarithm of node importance scores are used as ground truth labels for the experiments.\n\u2022 FB15K is a subset of Freebase\u00b9. It comprises 14,951 entities and 1,345 distinct relationships, reflecting the world's knowledge through triplets. The pageviews of Wikipedia pages are used as the nodes' importance.\n\u2022 TMDB5K is a movie knowledge graph generated from TMDB 5000 dataset\u00b2. It includes around 5,000 movie entities, along with entities such as actors, directors, and countries. The popularity scores of movies function as the movie nodes' importance.\n\u2022 MUSIC10K is a music knowledge graph constructed from the 10k Song Dataset\u00b3, with additional information sourced from the Million Song Dataset\u2074. It contains about 22985 entities such as artists, songs, terms of artists and releases. This dataset lacks node description text, so entity names serve as it. The familiarity of artists is defined as the artist nodes' importance.\nWe thank Chen et al. [17] for generously providing the original MUSIC10K dataset. MUSIC10K is further processed into the similar format as used in FB15K and TMDB5K, and we make the well-processed MUSIC10K freely available at https://github.com/XinyuLin-FZ/LENIE. Regarding FB15K and TMDB5K datasets, we directly obtain them from https://github.com/GRAPH-0/RGTN-NIE.\nTo comprehensively evaluate the advantages that integrating LENIE brings to NIE models and to further clarify the importance of enhanced semantic information for NIE tasks, we follow [16] by comparing three distinct types of NIE methods. These three types of NIE methods, covering a total of 10 NIE models, are used as baselines for comparison.\n1) Unsupervised Methods: PR [4] and PPR [5] are two well-known methods that assess node importance based on graph topology, without requiring labels or node features for training.\n2) Non-GNN Supervised Methods: Treating NIE as a regression task, we can utilize labels and node features to train two classical regression models\u2014Linear Regression (LR) and MultiLayer Perception (MLP)\u2014to predict node importance scores.\n3) GNN-based Supervised Methods: These methods leverage labels, structural, and semantic information from KGs to train the learning model.\nWe employ general-purpose GNN-based models, such as GCN [34], GraphSAGE [35], and RGCN [36], tailored for the NIE task, along with SOTA NIE-specific GNN-based models like GENI [2], RGTN [15], and LICAP [16], all serving as benchmarks for comparison."}, {"title": "C. Evaluation Metrics", "content": "To comprehensively evaluate the performance of the NIE models, we employ both regression metrics and ranking metrics, totaling 5 metrics, consistent with [16]. The specific definitions of these metrics are as follows:\n1) Regression metrics: The NIE model's predicted scores align with nodes' ground-truth labels, both reflecting node importance in KGs as numerical values. Therefore, we employ regression metrics, specifically Root Mean Square Error (RMSE) and Median Absolute Error (MedianAE), to evaluate the discrepancy between predicted and actual scores. Lower values indicate better performance.\n2) Ranking metrics: To evaluate the performance from another perspective, comparing the node importance rankings derived from predicted scores $\\{s\\}$ and ground-truth scores $\\{s_i\\}$ can also effectively assesses the alignment between the NIE model's predictions and actual results. Therefore, we employ three ranking metrics: Normalized Discounted Cumulative Gain (NDCG), Spearman's rank correlation coefficient (SPEARMAN), and Overlap (OVER). Higher values indicate better performance."}, {"title": "D. Implementation Details", "content": "LENIE aims to enhance semantic information in KGs to generate semantic embeddings for nodes, thereby improving downstream NIE model performance. In the subsequent experiments, unless otherwise specified, LENIE adopts the recommended clustering-based triplet sampling strategy for semantic extraction, selecting 10 triplets for FB15K, 5 for TMDB5K, and 3 for MUSIC10K according to their average node degrees, with Llama3.15 as the default LLM to generate augmented descriptions of nodes. Throughout the process, the all-mpnet-base-v26 (which maps text to a 768-dimensional dense vector space) serves as the text encoder. In all experiments, node original descriptions are encoded and fed into downstream NIE models, with their performance serving as a baseline for comparison. This setup demonstrates the effectiveness of LENIE's semantic enhancement of KGs in boosting the performance of downstream NIE models.\nAll techniques and models employed in the experiments are implemented in Python. Most experimental settings align with [16], with specific configurations as follows: PR and PPR are from the NetworkX package7, LR, MLP, and K-means from scikit-learn package8, and GCN, GraphSAGE, and RGCN from the DGL package9, each with a hidden layer of 64 dimensions. The LICAP model is sourced from its GitHub repository10. Note that both RGTN and LICAP achieve state-of-the-art performance, with LICAP serving as an improved version of RGTN by enhancing node embeddings through pretraining. To assess the impact of text information augmentation, RGTN is used as the downstream NIE model for LENIE in the main experiments, providing a comparison against other benchmark methods. For all experiments, we conducted a learning rate search across the range [0.1, 0.5, 0.01, 0.05, 0.001, 0.005, 0.0001, 0.0005]. Other hyperparameters were set to each model's default optimal values, and all models were evaluated using 5-fold cross-validation. The learning rate with the lowest RMSE was chosen as optimal, with corresponding performance results recorded as final outcomes. All experiments were conducted on NVIDIA 3090 GPUs."}, {"title": "V. EXPERIMENTS", "content": "In this section, we examine the effectiveness of LENIE and its components by addressing the following questions.\n1) Can the proposed LENIE further boost the performance of existing NIE model and achieve the new state-of-the-art performance? (Section V-A)\n2) How do different semantic augmentation approaches affect the NIE performance? (Section V-B)\n3) Why is the proposed clustering-based triplet sampling superior to random-based sampling? (Section V-C)\n4) Do the augmented descriptions generated by different LLMs improve the NIE performance? (Section V-D)\n5) Can the proposed LENIE framework also improve the performance of various NIE models? (Section V-E)\n6) How are the semantic deficiencies in KGs, such as insufficient descrpitons, missing descriptions, and inaccurate information, addressed by LENIE? (Section V-F)"}, {"title": "A. NIE on Real-World KGs", "content": "The main experimental results of our proposed LENIE method compared to other NIE methods across three real-world KGs are presented in Table II. Conclusions drawn from these results can be explained from the following perspectives.\nLENIE achieves state-of-the-art performance across nearly all metrics in NIE tasks on three real-world KGs. On the FB15K dataset, LENIE's OVER@100 is slightly lower than RGTN by 0.4%, but it shows an average improvement of approximately 0.7% across other metrics, with a maximum increase of 1% in RMSE. On the TMDB5K dataset, LENIE outperforms RGTN across all metrics with an average improvement of around 3.7%, with the largest gain of 6% in OVER@100. Similarly, on the MUSIC10k dataset, LENIE excels across all metrics, showing an average improvement of 6.4% over RGTN and a substantial 19.6% improvement in SPEARMAN.\nEffectively leveraging additional node information in KGs enhances the performance of NIE models. Unsupervised methods like PR and PPR rely solely on graph topology for unsupervised training and generally perform worse on NIE metrics across the three real-world KGs compared to other methods. Non-GNN supervised methods like LR and MLP utilize labels and node features, achieving better overall performance than unsupervised methods; however, they cannot fully exploit the rich information within KGs, limiting further improvement. In contrast, GNN-based supervised methods capture rich structural and semantic information through aggregation and update mechanisms, yielding better overall performance and making it easier to achieve state-of-the-art results.\nEnhancing the semantic information of nodes in KGs is essential for improving NIE model performance. While FB15K and TMDB5K datasets include node original descriptions, MUSIC10K dataset contains only node names, providing limited semantic context. LENIE bridges this gap by integrating semantic information from KGs with LLMs to generate augmented descriptions, significantly enriching node semantics. This enhancement explains LENIE's especially strong impact on downstream RGTN models in MUSIC10K compared to other KGs. These semantic improvements also benefit other NIE models, as discussed further in Section V-E."}, {"title": "B. Ablation Study: Semantic Augmentation", "content": "The semantic augmentation process in LENIE involves semantic extraction from KGs via the clustering-based triplet sampling strategy, followed by prompting LLMs to generate augmented descriptions for nodes in the KGs. To investigate the effectiveness of these designs, we conducted an ablation study, with results presented in Table III. The relevant definitions are as follows:\n\u2022 Vanilla: The original RGTN method without LENIE.\n\u2022 LENIE (concate): LENIE enhances node semantics by concatenating node original description with triplet text extracted from KGs, without using LLMs.\n\u2022 LENIE (random): LENIE enhances node semantics by integrating triplet text extracted via random-based triplet sampling strategy with the original node descriptions to prompt LLMs in generating augmented descriptions.\n\u2022 LENIE (cluster): Building on the previous approach, LENIE applies the clustering-based triplet sampling strategy to extract semantic information from KGs.\nAs shown in Table III, LENIE (concate), which incorporates semantic extraction from KGs, outperforms the Vanilla method (which uses only the original node descriptions) on both TMDB5K and MUSIC10K datasets across most metrics. Furthermore, the LENIE (random) and LENIE (cluster) methods, which additionally leverage LLMs for augmented text generation, achieve overall superior performance across all KGs compared to the Vanilla method. These results validate the effectiveness of both semantic extraction from KGs and LLM-based text generation.\nMoreover, comparing the performance of LENIE (random) and LENIE (cluster) reveals that clustering-based triplet sampling strategy for semantic extraction, followed by LLMs augmented text generation, is more effective than random-based strategy for enhancing NIE model performance. The ablation experiment results above validate the effectiveness of LENIE and its semantic augmentation method."}, {"title": "C. Comparison of Triplet Texts Extracted by Two Strategies", "content": "To further investigate why the clustering-based triplet sampling strategy in LENIE outperforms the random-based triplet sampling strategy for the NIE task, we conducted a textual comparison of the semantic information extracted for the same node using both strategies across three KGs.\nAs shown in Figure 3, under the same sampling count, clustering-based triplet sampling strategy tends to select triplet sentences with the most diverse semantics. The resulting triplet text better guides LLMs in generating more comprehensive descriptions for nodes (details are provided in Section V-F)."}, {"title": "D. Impact of Different LLMs on LENIE", "content": "To assess whether LENIE based on different LLMs also improves NIE model performance, we compared LENIE's effectiveness using the latest versions of Llama, GLM, and Qwen (i.e., Llama3.1-8b-Instruct, GLM4-9b-Chat\u00b9\u00b9, and Qwen2-7b-Instruct12) for the NIE task. All other experimental settings align with Section V-A, except for the choice of LLMs.\nAs shown in Table IV, the experimental results indicate that LENIE with various LLMs enhances most performance metrics across KGs. Among these, Llama3.1 achieves the best overall performance across the three KGs, while Qwen2 performs best on the TMDB5K. GLM4, however, shows relatively lower overall performance. These performance differences among LLM-based LENIE methods may result from variations in training data and application domains across different LLMs. With the"}]}