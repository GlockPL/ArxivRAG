{"title": "I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing", "authors": ["Yiwei Ma", "Jiayi Ji", "Ke Ye", "Weihuang Lin", "Zhibin Wang", "Yonghan Zheng", "Qiang Zhou", "Xiaoshuai Sun", "Rongrong Ji"], "abstract": "Significant progress has been made in the field of Instruction-based Image Editing (IIE). However, evaluating these models poses a significant challenge. A crucial requirement in this field is the establishment of a comprehensive evaluation benchmark for accurately assessing editing results and providing valuable insights for its further development. In response to this need, we propose I2EBench, a comprehensive benchmark designed to automatically evaluate the quality of edited images produced by IIE models from multiple dimensions. I\u00b2EBench consists of 2,000+ images for editing, along with 4,000+ corresponding original and diverse instructions. It offers three distinctive characteristics: 1) Comprehensive Evaluation Dimensions: I2EBench comprises 16 evaluation dimensions that cover both high-level and low-level aspects, providing a comprehensive assessment of each IIE model. 2) Human Perception Alignment: To ensure the alignment of our benchmark with human perception, we conducted an extensive user study for each evaluation dimension. 3) Valuable Research Insights: By analyzing the advantages and disadvantages of existing IIE models across the 16 dimensions, we offer valuable research insights to guide future development in the field. We will open-source I2EBench, including all instructions, input images, human annotations, edited images from all evaluated methods, and a simple script for evaluating the results from new IIE models. The code, dataset and generated images from all IIE models are provided in github: https://github.com/cocoshe/I2EBench.", "sections": [{"title": "1 Introduction", "content": "Instruction-based Image Editing (IIE) which aims to edit an image using a text instruction, provides a user-friendly way for the community to edit images. Over the past few years, significant progress has been made in IIE, with the development of diffusion models and large vision-language models (LVLMs). However, there is a pressing need for a comprehensive benchmark to effectively assess the performance of these models. An ideal evaluation framework should not only measure the editing quality across different dimensions but also align with human perception to ensure reliable measurements. Furthermore, the evaluation should highlight the specific strengths and weaknesses of each model, thereby offering valuable insights for future endeavors in data selection, training strategy selection, and architecture design within this field. However, evaluating an IIE model poses challenges due to the diverse range of editing types and the inherent difficulty in assessing the level of alignment between edited images and given instructions.\nExisting evaluation metrics for IIE could be divided into three categories: 1) conventional metric; 2) user study; 3) benchmark. The first category employs conventional metrics to evaluate IIE models, including CLIP Score , CLIP Text-Image Direction Similarity , PSNR , SSIM and LPIPS . The advantage of this approach is its ease of use. However, a single metric is not suitable for evaluating all types of editing. For instance, CLIP score measures the similarity between images and text, making it less suitable for low-level visual editing tasks like denoising and low-light enhancement. Similarly, PSNR, which measures image similarity, is not adequate for high-level visual editing tasks such as object removal and replacement. The second category involves methods that evaluate the effectiveness of different techniques by soliciting ratings from human participants. This approach directly reflects human preferences and aligns the results with human perception. However, it is a costly method and lacks reproducibility, as the test sets and participants may be not consistent in each evaluation. The final category comprises benchmarks specifically designed for evaluating IIE models. While these benchmarks are tailored for IIE, they have certain limitations. For example, TedBench evaluates only 100 images with commonly occurring editing types, which may not sufficiently demonstrate the capabilities of IIE models. EditBench focuses on mask-guided editing, rendering it unsuitable for evaluating mask-free methods. In EditVal, only a limited set of dimensions related to size or location can be automatically evaluated, limiting its universality.\nIn this paper, we propose I2EBench, a comprehensive benchmark designed to automatically evaluate the performance of IIE models. I2EBench exhibits three attractive characteristics: 1) Comprehensive Evaluation Dimension, 2) Human Perception Alignment, and 3) Valuable Research Insights.\nFirst and foremost, I2EBench offers a comprehensive evaluation dimension. These dimensions are categorized into two main types: High-level Editing and Low-level Editing. High-level editing primarily focuses on understanding instructions or editing specific areas of images, whereas low-level editing is more concerned with editing image details or the entire image. As shown in Fig. 1, both high-level and low-level editing consist of 8 fine-grained editing dimensions, which serve to demonstrate the model's proficiency in high-level and low-level editing. We meticulously collected"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Instruction-based Image Editing", "content": "With the advancements in Generative Adversarial Networks (GAN) and Diffusion models , text-to-image models have made remarkable progress in recent years. As the demand for image editing continues to grow, a multitude of text-based image editing models have emerged. One editing task, known as Prompt-based Image Editing (PIE) , requires users to provide a target description along with the original image. The PIE model then analyzes the target description to modify the input image accordingly, generating a target image that matches the provided description. However, despite the lowered threshold for image editing, the requirement of describing the entire content of the target image in the description still poses challenges in terms of user interaction. To address this limitation, Instruction-based Image Editing (IIE) was proposed, which simplifies the user's role to providing the original image and modification instructions (e.g., \u2018Remove the dog'). One notable implementation, InstructPix2Pix introduces a large-scale dataset for instruction-based image editing. The dataset is created using a fine-tuned GPT-3 and image pairs generated by the Prompt-to-Prompt diffusion model. Additionally, InstructPix2Pix proposes an instruction-based diffusion model for image editing based on this dataset. However, due to the automatic generation and filtering of the InstructPix2Pix dataset, concerns arise regarding its quality and potential noise. To address this, MagicBrush proposes a manually-annotated instruction-guided image editing dataset. In addition to textual instructions, InstructAny2Pix proposes a model that utilizes other modalities, such as audio and image, as instructions. To enhance the level of detail in instructions and improve the accuracy of editing results, MGIE introduces the use of Multimodal Large Language Models (MLLM). SmartEdit, aiming to improve the editing capabilities of IIE models in complex scenes, incorporates MLLM into the IIE model to better comprehend instructions. Despite significant progress, evaluating the editing performance of IIE models remains a crucial concern. Therefore, in this paper, we present I2EBench, a systematic evaluation framework for these models. Our work includes an in-depth analysis of their strengths and weaknesses, offering valuable insights for the future development of IIE models."}, {"title": "2.2 Text-based Image Editing Benchmark", "content": "While numerous benchmarks have been introduced for evaluating vision-language tasks, the evaluation of text-based image editing models often relies on metrics such as CLIP Score , PSNR , SSIM, and LPIPS . Several existing studies have introduced benchmarks to assess the performance of image editing models. TedBench presents a relatively small benchmark consisting of only 100 images and a limited set of highly common editing types. EditBench is specifically designed to evaluate mask-guided image editing methods, which necessitate the availability of additional masks indicating the areas to be edited. In EditVal, the evaluation of certain dimensions relies on manual labor, thereby limiting the reproducibility of performance. Moreover, the remaining dimensions primarily involve modifications to object size or position, lacking comprehensive coverage. While MagicBrush and Emu Edit propose test sets for evaluating editing performance, they still rely on conventional metrics such as L1, L2, CLIP-I, DINO, and CLIP-T, which may not accurately capture the nuances of all editing types. SmartEdit specifically develops a benchmark tailored for complex editing scenarios, but it does not accommodate other editing scenarios. Considering the current absence of a systematic benchmark"}, {"title": "3 I2EBench", "content": "This section provides an overview of the main components of I2EBench. In Sec. 3.1, we provide a concise introduction to the principles, definitions, and evaluation methods of 16 dimensions. Sec. 3.2 outlines the process of data annotation. Lastly, in Sec. 3.3, we present the human evaluation process to assess the correlation between the I2EBench score and the human score. A detailed explanation can be found in the supplementary materials."}, {"title": "3.1 Evaluation Dimension", "content": "In our evaluation of the IIE model's editing quality, we have categorized it into 16 dimensions, each assessing different aspects of editing in a top-down manner. An overview of I2EBench is presented in Fig. 1. High-level Editing Evaluation primarily focuses on assessing the model's ability to accurately understand instructions and make precise edits to local areas of the input image. This evaluation consists of 8 dimensions. Low-level Editing Evaluation, on the other hand, primarily examines global editing and detailed image processing. It also comprises 8 evaluation dimensions. Unlike previous approaches that relied on a single metric, such as CLIP score, to evaluate editing quality for all editing types, we have developed specialized evaluation methods for each of the 16 dimensions. This approach is necessary due to the distinct goals of high-level and low-level editing."}, {"title": "3.1.1 High-level Editing", "content": "Evaluating editing quality in high-level dimensions poses a challenge due to the diverse goals involved, making it impractical to rely on a single metric. The advancement of Multimodal Large Language Models (MLLM) has significantly enhanced automated understanding of images. Therefore, to ensure precise evaluation of the editing quality of IIE models in high-level dimensions, we leverage the exceptional capabilities of the widely recognized GPT-4V model to make judgments for most high-level evaluation dimensions.\nCounting. The Counting dimension pertains to instructions related to the number of objects, such as \"add two apples to the image.\" To assess this dimension, we query GPT-4V about the number of target objects in the image and compare its response with the human-annotated answer.\nDirection Perception. The Direction Perception dimension requires the IIE model to comprehend directions provided in instructions, and accurately make edits when presented with images. We evaluate this dimension by asking GPT-4V if the target object is in the expected position.\nObject Removal. The Object Removal dimension focuses on removing the target object according to the given instruction. To evaluate this dimension, we inquire whether GPT-4V identifies the presence of the target object in the image.\nObject Replacement. The Object Replacement dimension aims to replace the original object with the target object as instructed. To assess this dimension, we query GPT-4V about the presence of the target object in the image.\nBackground Replacement. The Background Replacement dimension involves replacing the original background with the target background as specified in the instruction. To evaluate this dimension, we ask GPT-4V if the background of the image matches the textual instruction.\nColor Alteration. In the Color Alteration dimension, we modify the color of the target object using instructions. To evaluate this dimension, we inquire GPT-4V about the color of the target object in the edited image.\nStyle Alteration. The Style Alteration dimension focuses on changing the style of the image. To evaluate this dimension, we calculate the CLIP similarity between the edited image and \"an image with style\"."}, {"title": "Region Accuracy", "content": "In the editing task, we not only assess whether the target area has been edited correctly but also whether areas that should not be edited have been altered. To evaluate this dimension, we sample input images and instructions from the Object Removal, Object Replacement, and Color Alteration dimensions. We annotate the mask for the area that requires editing. Next, we fill the mask area of the images before and after editing with white and calculate SSIM to evaluate this dimension."}, {"title": "3.1.2 Low-level Editing", "content": "Unlike high-level editing, low-level editing instructions are simpler, lacking specifications regarding object size, orientation, or color. Various low-level editing tasks have undergone extensive development over the years, resulting in a relatively mature evaluation system. Therefore, for low-level editing, we employ the widely recognized metric, namely SSIM , to evaluate the editing quality.\nDeblurring. Deblurring encompasses the procedure of mitigating or eliminating blur from images, resulting in enhanced clarity and sharpness.\nHaze Removal. Haze removal entails the elimination or reduction of atmospheric haze or fog from images, augmenting visibility and reinstating the true colors and intricate details of the scene.\nLowlight Enhancement. Lowlight enhancement refers to the process of improving the quality of images captured in low-light conditions, enhancing brightness, and reducing noise.\nNoise Removal. Noise removal involves the reduction or elimination of unwanted noises in images, resulting in cleaner and more visually appealing visuals.\nRain Removal. Rain removal aims to eliminate or reduce the visual effects of raindrops or rain streaks from images, improving clarity and restoring the original appearance.\nShadow Removal. Shadow removal refers to reducing or eliminating unwanted shadows from images, enhancing visibility, and improving overall image quality.\nSnow Removal. The goal of Snow Removal is to effectively reduce or eliminate snow from images.\nWatermark Removal. Watermark removal involves the removal or elimination of embedded watermarks from images, restoring the original appearance without the presence of the watermark."}, {"title": "3.2 Human Annotation", "content": "Data Annotation. We meticulously curated approximately 140 images from publicly available datasets for each evaluation dimension of I2EBench. These images were then meticulously annotated with textual editing instructions by human annotators, namely original instructions. However, instructions provided by human annotators usually followed a singular sentence pattern. For instance, the prevalent sentence pattern for the object removal dimension was typically \"remove from the image\". To foster increased diversity, we employed ChatGPT to effectively rewrite the original"}, {"title": "3.3 Human Evaluation", "content": "The primary objective of the human evaluation is to ascertain the correlation between human perception and the I2EBench score. To achieve this, we present human evaluators with a textual instruction T, an input image V\u2081, and a set of edited images {V1, V2,\u00b7\u00b7\u00b7, VM} generated by M different IIE models. The evaluators are then tasked with ranking the results based on their judgment. More specifically, we sample N images for each evaluation dimension, leading to a comprehensive collection of N \u00d7 16 \u00d7 2 edited image comparisons. Within each comparison, evaluators are presented with M edited images to assess and rank in relation to one another. We assign a human score to each model based on its ranking among the M models. Specifically, the model ranked first among the M models receives a human score of M, while the model ranked last among the M models receives a human score of 1. Additionally, the model ranked k among the M models is assigned a human score of Mk + 1. To determine the human score for each dimension, we calculate the average of the human scores across all samples within that dimension. Thus, the human score for each model ranges from 1 to \u041c."}, {"title": "4 Experiments", "content": "Dimension Evaluation. For each image and instruction, we utilize official codes from various models for image editing. We calculate the I2EBench scores following the methodology described in Sec.3.1. The I2EBench scores for original and diverse instructions are presented in Fig. 4, Tab.1, and Tab. 2, respectively. Our observations reveal that no single model achieves the best performance across all evaluation dimensions. Regarding low-level editing, InstructDiffusion demonstrates superior results. It attains the highest scores in 4 out of 7 low-level editing evaluation dimensions when using original instructions, and 3 out of 7 when using diverse instructions. For high-level editing, both Magic Brush and InstructAny2Pix perform impressively. MagicBrush achieves the highest scores in 3 evaluation dimensions using original instructions, while InstructAny2Pix achieves the highest scores in 3 dimensions using diverse instructions. In the deblurring dimensions, MGIE stands out significantly. It surpasses the second-place model by 11.92 when using original instructions and by 11.37 when using diverse instructions.\nHuman Evaluation. We ranked different models based on their I2EBench scores and computed I2EBench rank scores using the methodology described in Sec. 3.3. Given that both I2EBench rank scores and human scores range from 1 to 8, a direct comparison can be made between them. Therefore, we conducted correlation analyses and visually presented the results in Fig. 5. Significant positive correlations were observed between the I2EBench rank score and the human score across all dimensions. These findings offer strong evidence supporting the alignment between our proposed benchmark and human perception."}, {"title": "$\nS'_{i} = \\frac{\\left| S_{i}-S^{a}_{i} \\right|}{\\text{MIN}(S_{i}, S^{a}_{i})},\n$", "content": "Insights\nThe editing ability across different dimensions is not robust: Our observations indicate that no single model excels in all evaluation dimensions. This implies that different IIE models have varying strengths in terms of their editing abilities across different dimensions. Thus, it is crucial to acknowledge this limitation and focus on developing an IIE model that demonstrates consistent and competent performance across all dimensions. Future research efforts should prioritize the creation of a robust and versatile IIE model that can effectively handle a wide range of editing tasks across diverse dimensions.\nThe editing ability of different instructions is not robust: To evaluate the robustness of editing models when provided with different instructions, we propose a metric called I2EBench change rate. This metric is defined as follows:"}, {"title": "5 Insights", "content": "The editing ability across different dimensions is not robust: Our observations indicate that no single model excels in all evaluation dimensions. This implies that different IIE models have varying strengths in terms of their editing abilities across different dimensions. Thus, it is crucial to acknowledge this limitation and focus on developing an IIE model that demonstrates consistent and competent performance across all dimensions. Future research efforts should prioritize the creation of a robust and versatile IIE model that can effectively handle a wide range of editing tasks across diverse dimensions.\nThe editing ability of different instructions is not robust: To evaluate the robustness of editing models when provided with different instructions, we propose a metric called I2EBench change rate. This metric is defined as follows:\nwhere S and Sa represent the I2EBench scores of the i-th evaluation dimension when using original and diverse instructions, respectively. The value of S' indicates the I\u00b2EBench change rate for the i-th evaluation dimension. As illustrated in Fig. 6, when it comes to the object removal dimension, InstructPix2Pix, HIVE, InstructionDiffusion and Magic Brush exhibit significant fluctuations in their performance using different instructions. On the other hand, the remaining models demonstrate relatively stable performance across different instructions. One notable distinction between these two categories of models is that the latter employs LLM or MLLM to comprehend instructions, which enhances their resilience to variations in instructions. Given the unpredictable and diverse nature of user editing instructions, it is crucial to develop an editing model that can effectively handle instructions with varying levels of complexity.\nThe editing ability for different categories is not robust: As illustrated in Fig. 7, we have observed distinct variations in the performance of different categories. Notably, the \"Scenery\" and \"Global\" categories consistently demonstrate superior performance compared to the other categories across all the IIE models we evaluated. This discrepancy can be attributed to the inherent inclination of the \"Scenery\" and \"Global\" categories towards global editing, which diminishes the necessity for"}, {"title": "6 Conclusions", "content": "In this paper, we present I2EBench, a comprehensive benchmark specifically designed for instruction-based image editing (IIE). Our benchmark includes a substantial dataset of over 2000+ images and more than 4000+ instructions, covering 16 distinct evaluation dimensions. To evaluate the effectiveness of I2EBench, we conduct experiments using 8 open-source IIE models. Additionally, we complement these experiments with meticulous human evaluations to establish the correlation between I2EBench scores and human perception. Based on the observations derived from I2EBench, we provide valuable insights and recommendations for advancing IIE models. We hope the proposed I2EBench to serve as an indispensable asset, playing a pivotal role in fostering the advancement of IIE models and assessing their efficacy."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 More Details on Evaluation Dimension.", "content": "Counting. When using diffusion models to add some objects to the images, it is important for models to understand the numbers of the objects we want to add according to editing instructions. Thus, we provide the instructions like, add a tie to the man, add three birds to the skateboard, add five dogs to the image. As shown in Figure 8, editing instruction with a larger number means a much more difficult task, so we propose the editing model which can handle the number well should be better when evaluating the performance. To evaluate if the editing model successfully adds the specific number of objects to the image, we use GPT4V (with gpt-4-vision-preview checkpoint) to get the number of objects from the edited images, by designing the prompts, i.e., \"How many ties are on the image?\", \"How many birds are in the image?\", \"How many dogs are in the image?\". To restrain the useless and irrelevant context of the raw outputs of GPT4V, we also design a filter process with ChatGPT (with gpt-4-turbo checkpoint). For example, in Figure 8 (a), the GPT4V output is \"There is one tie visible in the image.\", and the prompt for ChatGPT final judge is shown below."}, {"title": "Direction Perception.", "content": "To evaluate whether the editing model is equipped with the ability of direction perception, as shown in Figure 9, we design a strategy with LVLM which is similar to the Counting evaluation. We first create the instructions and then edit the original image with the instructions. We still use GPT4V as LVLM to ask the edited image if the object is added to the correct direction(as"}, {"title": "Object Removal.", "content": "As shown in Figure 10, this is also a significant dimension for an editing model. We use GPT4V as the LVLM and ChatGPT for final judgment, and the object to be removed in editing instruction is designed including animal, human, scenery, plant, etc. After editing the original image, the GPT4V is used to ask if the object is in the edited image, then get the final judgment with ChatGPT. More specifically, the ChatGPT for object removal is designed like this:"}, {"title": "Object Replacement.", "content": "As shown in Figure 11, this is a common dimension and is evaluated widely for an editing model. We still use GPT4V as the LVLM and ChatGPT for final judgment, and the object to be replaced in editing instruction is designed including animal, human, scenery, plant, etc. After editing the original image, the GPT4V is used to ask if the object is replaced in the edited image, then get the final judgment with ChatGPT. The ChatGPT for object replacement is designed"}, {"title": "Background Replacement.", "content": "A powerful editing model not only requires a keen perception of specific objects but also a robust ability to manipulate backgrounds effectively. For example, in Figure 12, the editing instructions \"Change the background of this photo to snow\" and \"Change the background of this photo to desert\" are provided for the editing models in (a) and (b) respectively, and the second image and the fifth image show that they fail to replace the background, which means the corresponding editing model may get a lower score in the dimension. The final judgment prompt for ChatGPT is designed here:"}, {"title": "Color Alteration.", "content": "This is also a very common function for editing models. We designed an edited class including animal, human, scenery, plant, etc. Some results are shown in 13. The editing prompts \"Change the color of the bear to brown\", \"Change the color of the bus to blue\", and \"Change the color of the flowers to purple\" is used to edit the bear, bus, and flower. Then the GPT4V is used for evaluation to check if the images are correctly edited, and ChatGPT is used to get more stable results,"}, {"title": "Style Alteration.", "content": "This is a much more abstract level editing task, while the other tasks focus on editing the specific objects or background of the image. Some examples are shown in Figure 14. The style alteration task evaluates the editing models on the style understanding dimension. The CLIP model with excellent ability of abstract understanding is used for evaluation, and the CLIP scores are calculated between the images and the captions concatenated with \"an image with\" and the style description, \"an image with cyberpunk style\" for example."}, {"title": "Region Accuracy.", "content": "All of the samples for evaluation in this dimension are selected from the other object-based tasks, for example, object removal, object replacement, color alteration. The scores are calculated with a region SSIM.\nThe region SSIM is designed with the mask annotated before by the Segment Anything Model(SAM). In Figure 16 showing how to use the mask on the original image and the edited image, then the scores of region accuracy can be calculated between the masked original image and the masked edited image:"}, {"title": "Deblurring.", "content": "This is a dimension of low-level editing, some results are shown in Figure 17, and the editing models are used to do the deblurring task with the instruction \"Remove the blurriness from the image\" on the deblurring dataset:"}, {"title": "Haze Removal.", "content": "This is also a low-level dimension evaluation, but it seems much easier than the deblurring task according to the experiment results in Figure 18. All of the images are clearer for human vision after the editing. The editing instruction is designed with \"Remove the haze from the image\" for this haze removal dimension, and the SSIM is used for evaluation:"}, {"title": "Lowlight Enhancement.", "content": "This is another low-level dimension. The results in Figure 19 indicate the editing models are equipped with excellent ability on lowlight enhancement. The original images are nearly beyond the margins of human visual perception, but the edited results are effectively lightened."}, {"title": "Noise Removal.", "content": "It is a low-level dimension for evaluating the edit models. In Figure 20, the instruction \"Remove the noise from the image\" is used to the original image, and SSIM is used to calculate the noise removal score:"}, {"title": "Rain Removal.", "content": "This is a low-level dimension, and the results shown in Figure 21 indicate that rain is hard to perceive. The instruction \"Remove the rain from the image\" is used for the task, and the SSIM is used for calculating the score:"}, {"title": "Shadow Removal.", "content": "This is a low-level dimension, and the results in Figure 22 show some models can complete the task perfectly. The instruction \"Remove the shadow from the image\" is used for the"}, {"title": "Snow Removal.", "content": "This is a low-level dimension, and the results in Figure 23 show some models can remove the snow as expected, but most models can not. The instruction \"Remove the snow from the image\" is used for the snow removal task, and SSIM is used for calculating the snow removal score:"}, {"title": "Watermark Removal.", "content": "This is a low-level dimension, and the results in Figure 24 show some models can remove the watermark perfectly or fade out the watermark to some degree. The instruction \"Remove the watermark from the image\" is used for the watermark removal task, and SSIM is used"}, {"title": "A.2 More Details on Data Annotation.", "content": "Counting. The counting evaluation dimension aims to assess the model's ability to accurately comprehend quantity-related instructions and precisely modify images accordingly. To conduct this evaluation, we manually curated a set of 136 photos from the widely used MS-COCO dataset. Subsequently, we annotated textual editing instructions for each image, based on the content depicted. To promote diversity in the editing instructions, we employed ChatGPT, as illustrated in Fig. 25. We utilized ChatGPT to modify the original instructions into varied alternatives. Specifically, we formulated a prompt for ChatGPT, which was structured as follows: \"Please provide a different expression for the following sentence: <original instruction>\". Besides, for each sample in the high-level editing dimension, we annotate a category, such as animal, object, scenery, plant, human, For instance, in Fig. 26 (right), we present an image featuring a man capturing a photograph of a snowy mountain. The original instruction assigned to this image was \"Add eight birds to the image\", and the ChatGPT-enhanced diverse instruction became \"Decorate the image with a group of eight birds\".\nDirection Perception. The dimension of direction perception aims to assess the editing model's capability to accurately comprehend positional descriptions in instructions and execute corresponding"}, {"title": "Object Removal.", "content": "The Object Removal dimension evaluates the ability of the editing model to accurately eliminate the target object from the image based on the provided instructions. We carefully curated a set of 139 images from the MS-COCO dataset and meticulously annotated them accordingly. For instance, as depicted in Fig. 28 (left), we show a picture of a man surfing. The original instruction for this picture is \"Remove the man from the image\", while ChatGPT's enhanced diversification instruction becomes \"Delete the man from the picture\"."}, {"title": "Object Replacement.", "content": "The objective of the object replacement dimension is to substitute an object in the image with another object based on the provided instruction. We meticulously curated a set of 136 images from the MS-COCO dataset and annotated them with utmost care. As illustrated in Fig. 29 (right), the selected image shows two zebras on the grass. The original instruction given by the human annotator is \"replace zebra with child\", while the diversified instruction after optimization by ChatGPT is \"Replace the zebra with an energetic child\"."}, {"title": "Background Replacement.", "content": "Aside from editing foreground objects, modifying the background is also a crucial evaluation dimension for the editing model. Therefore, the Background Replacement dimension focuses on altering the image's background according to the provided textual instructions. With meticulous curation, we assembled a set of 139 images from the MS-COCO dataset and annotated them meticulously. For instance, as depicted in Fig. 30 (right), we present an image featuring a man cooking in the kitchen. The original instruction for this image is \"Change the background of this photo to snow\", while ChatGPT's enhanced diversification instruction becomes \"Rearrange the elements in this picture to simulate a snowy backdrop\"."}, {"title": "Color Alteration.", "content": "Color Alteration involves modifying object attributes. This dimension aims to assess the model's ability to accurately change the color of the target object based on the provided instructions. Through careful curation, we compiled a set of 138 images from the MS-COCO dataset, subjecting them to meticulous instruction annotation. For instance, as depicted in Fig. 31 (left), we display an image featuring a person skiing. The original instruction associated with this image is \"Change the color of the ground to brown\", while the enhanced and diversified instruction provided by ChatGPT becomes \"Brown the ground's color\"."}, {"title": "Style Alteration.", "content": "Style transfer is a crucial task in computer vision that entails creating a novel image by merging the content of one image with the style of another. It is also reasonable to consider utilizing textual instructions to alter the style of an image. The Style Alteration dimension endeavors to evaluate the editing model's ability to modify an image's style. We meticulously curated a set of 140 images from the MS-COCO dataset and subjected them to detailed annotation. This evaluation is illustrated in Fig. 32 (left). The selected image depicts an urban street scene at night. The original instruction provided by the human annotator is \"Change the image to cyberpunk style,\" while the diversified instruction refined by ChatGPT reads, \"Transform the image into a futuristic and gritty cyberpunk dystopia.\""}, {"title": "Region Accuracy.", "content": "In the task of instruction-based image editing, our focus extends beyond the accuracy of editing the target object; we also consider the preservation of non-target objects. To assess the level of retention of these non-target objects, we introduce the Region Accuracy dimension. To facilitate this evaluation, we select 140 images and their corresponding annotations from the Object Removal, Object Replacement, and Color Alteration dimensions. Additionally, we manually annotate the masks for the edited objects by following the provided instructions. For instance, as"}, {"title": "Deblurring.", "content": "The Deblurring dimension is designed to assess the model's ability to remove image blurring and enhance clarity. We collect a set of 140 blurred and deblurred images from the deblurring datasets, specifically GoPro and HIDE. The original instruction provided for all images is \"Remove the blurriness from the image.\" To introduce diversity in the instructions, we employ ChatGPT, resulting in distinct instructions for each image, such as \"Enhance the clarity of the image,\" \"Clear up the blur in the picture,\" \"Sharpen the image to remove blurriness,\" and so on. In the low-level editing dimension, we annotate each sample's category as \"global.\" Fig. 34 illustrates the process. First, we obtain the input image and ground-truth image from the publicly available deblurring dataset. Next, we manually annotate the image with the original instruction, \"Remove the blurriness from the image\". Finally, we obtain diverse instructions, such as \"Sharpen the image to remove blurriness\" using ChatGPT."}, {"title": "Haze Removal.", "content": "The Haze Removal dimension focuses on assessing the model's capability to eliminate haze from images. To create the evaluation dataset, we initially collect a total of 155 input images and corresponding ground-truth images from the Dense-Haze and Haze4k datasets. As shown in Fig. 35, we initially acquire the input image and the real image from the de-hazing dataset. Then, we manually label the image with the original instruction, \"Remove the haze from the image.\" Finally, utilizing ChatGPT, we generate diverse instructions, such as \"Eliminate the mist in the picture.\""}, {"title": "Lowlight Enhancement.", "content": "The Lowlight Enhancement dimension focuses on evaluating the model's ability to improve image brightness, thereby enhancing image clarity. For this evaluation, we select a set of 140 input images and their corresponding ground-truth images from the LOL dataset."}, {"title": "Noise Removal.", "content": "The Noise Removal dimension focuses on evaluating the model's ability to successfully eliminate noise from images. To conduct this evaluation, we collect a set of 120 input images and their corresponding ground-truth images. We obtain these images by sampling from the CBSD68 dataset and adding noise to sampled images from the MS-COCO dataset. In addition, we gather both original instructions and diverse instructions from both human annotators and ChatGPT. As shown in Fig. 37, We first acquire input images and ground truth images from the denoising image datasets. Next, we manually annotate the images with the original instruction: \"Remove the noise from the image.\" Finally, we utilize ChatGPT to generate diverse instructions like \"Cleanse the image from visual interference through noise removal.\""}, {"title": "Rain Removal.", "content": "The Rain Removal dimension aims to"}]}