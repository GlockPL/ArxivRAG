{"title": "Financial Named Entity Recognition: How Far Can LLM Go?", "authors": ["Yi-Te Lu", "Yintong Huo"], "abstract": "The surge of large language models (LLMs) has revolutionized the extraction and analysis of crucial information from a growing volume of financial statements, announcements, and business news. Recognition for named entities to construct structured data poses a significant challenge in analyzing financial documents and is a foundational task for intelligent financial analytics. However, how effective are these generic LLMs and their performance under various prompts are yet need a better under-standing. To fill in the blank, we present a systematic evaluation of state-of-the-art LLMS and prompting methods in the financial Named Entity Recognition (NER) problem. Specifically, our experimental results highlight their strengths and limitations, identify five representative failure types, and provide insights into their potential and challenges for domain-specific tasks.", "sections": [{"title": "1 Introduction", "content": "As an increasing amount of information is con-tained within documents and text available online, utilizing a series of natural language processing (NLP) techniques to automate the process of ex-tracting meaningful information from unstructured text has become a critical task, especially in the financial domain (Ashtiani and Raahemi, 2023). Among all, named entity recognition (NER) serves as a foundational first step in identifying key enti-ties, such as persons, organizations, and locations, enabling the construction of knowledge graphs and other applications.\nWith the surge of large language models (LLMs), LLMs have demonstrated transformative capabili-ties in generative tasks, leveraging reinforcement learning from human feedback (RLHF) (Christiano et al., 2017). LLMs achieve remarkable perfor-mance across a wide range of NLP tasks with min-imal adaptation (Qin et al., 2024). However, their ability to perform domain-specific tasks, such as NER in the financial domain, remains less explored. For instance, in the sentence \u201cJohnson Brothers re-think plan for St. Paul waterfront Shepard Road Development.\", a generic NER model might incor-rectly classify the company \"Johnson Brothers\"as a person. This understanding is critical, as it could influence numerous applications in finance.\nIn this paper, we aim to evaluate the capabili-ties of state-of-the-art LLMs in performing NER tasks within the financial domain, their response to various prompt types, and their limitations in this context. To achieve this, we conduct a sys-tematic analysis and present experimental results, comparing the effectiveness of leading LLMs with recent fine-tuned approaches. Specifically, we eval-uate three advanced LLMs with different param-eter sizes, GPT-40 (OpenAI, 2024), LLaMA-3.1 (Dubey et al., 2024), and Gemini-1.5 (Google, 2024)-under three distinct prompting techniques: direct prompting, in-context learning, and chain-of-thought (CoT) prompting. We perform our study by investigating the following two research questions (RQs):\n\u2022 RQ1: How do different LLMs perform in NER tasks under various prompts?\n\u2022 RQ2: What types of mistakes do LLMs com-monly make?\nTo sum up, the main contributions of this paper are as follows:\n\u2022 To the best of our knowledge, this is the first study to comprehensively compare state-of-the-art generically trained LLMs on NER tasks in the financial domain.\n\u2022 We analyze LLM performance across three dis-tinct prompting techniques, identify their limi-tations, categorize five representative types of failures and underlying causes, and elicit two future directions based on our findings.\""}, {"title": "2 Related Work", "content": "2.1 Large Language Models in Finance\nLLMs have recently been applied to finance, par-ticularly in automatic information retrieval and fi-nancial analysis (Li et al., 2023b). Li et al., 2023a empirically explore ChatGPT and GPT-4's capa-bilities in analyzing financial texts and compare them to state-of-the-art fine-tuned models. How-ever, existing research mainly focuses on fine-tuned finance LLMs or individual generic LLMs, lacking comparisons of their performance under various prompt designs. This paper addresses this gap by providing a comprehensive evaluation of state-of-the-art LLMs under various prompting styles in the context of financial NER tasks."}, {"title": "3 Study Setup", "content": "To understand current LLMs' capabilities in han-dling financial NER problems, we choose three state-of-the-art LLMs, each with three popular prompting strategies. We further select two rep-resentative transformer-based models and fine-tune them on financial data for comparison.\n3.1 Financial NER Datasets\nIn this study, we use the FiNER-ORD dataset (Shah et al., 2023) as our benchmark. While the CRA NER dataset (Alvarado et al., 2015), based on fi-nancial agreements from the SEC, is widely used for research (Li et al., 2023a) and includes four entity types (person/PER, location/LOC, organi-zation/ORG, and miscellaneous/MISC), it suffers from a skewed distribution of entity types and lim-ited source of data.\nFINER-ORD resolves this imbalance and re-moves the ambiguous miscellaneous category, con-sisting of a manually annotated dataset of 201 finan-cial news articles. This provides a more robust and high-quality benchmark for financial NER tasks and has been adopted in recent research (Xie et al., 2024). As reported by Shah et al., 2023, the entity ratio in FiNER-ORD for ORG, LOC, and PER is 2.29:1.17:1, compared to the heavily skewed ratio of 0.31:0.22:1 in the CRA dataset.\n3.2 Models\nWe evaluate three state-of-the-art LLMs and their lightweight versions on the FiNER-ORD task: GPT-40, GPT-40-mini (OpenAI, 2024), LLaMA-3.1-70B-Instruct, LLaMA-3.1-8B-Instruct, and Gemini-1.5-flash, Gemini-1.5-flash-8B"}, {"title": "3.3 Prompt Design", "content": "We design three types of prompt methods: direct prompt, in-context learning (Dong et al., 2022), and chain-of-thought (Wei et al., 2022). As shown in Figure 1, the direct prompt first gives instructions for the NER task, followed by the given text and the answer format. Next, we conduct few-shot learning (five shots) experiments through in-context learn-ing and CoT prompts. The shots are chosen ran-domly and the same five shots are used in every experiment. For the in-context learning prompt, we simply add the five examples after the NER task instruction of the direct prompt. For the chain-of-thought prompt, we use the instruction \"let's think step by step\" to design intermediate steps for iden-tifying each named entity in the text, as shown in Figure 2."}, {"title": "3.4 Evaluation Metrics", "content": "After obtaining answers from the generated text, we label the identified entities through word match-ing. The evaluation metrics include the entity-level F1 score and the weighted F1 score. The formula for entity-level F1 score is described below, where TP, FP, and FN represent the counts of True Positives, False Positives, and False Negatives, re-spectively.\nPrecision = $\\frac{TP}{(TP + FP)}$, Recall = $\\frac{TP}{(TP + FN)}$\nF1_Score = $\\frac{2* Precision * Recall}{Precision + Recall}$\n(2)\nThe weighted F1 score is defined as follows:\n$W_i = \\frac{No._of_entities_in_classi}{Total_number_of_entities}$\nWeighted_F1 = $\\sum_{i=1}^{N}(wi * F1_Scorei)$"}, {"title": "4 Experiments", "content": "In this work, we conduct experiments to answer the following two research questions.\n4.1 RQ1: How do different LLMs perform in FINER-ORD tasks under different prompts?\nWe present the performance results of three leading LLMs under three distinct prompts in Table 1. The results are measured using the F1 scores for three entity types and the weighted F1 score (shown in the Weighted column). The LLMs are grouped into two sections based on their size, with bold val-ues highlighting the best performance. From these results, we can draw the following observations.\n(1) Fine-tuned language models consistently outperform generic LLMs, the performance gap can be narrowed through prompt design, few-shot learning, and model size. Table 1 demonstrates that fine-tuned language models sur-pass generic LLMs in zero-shot direct prompting. However, the performance of generic LLMs im-proves significantly with diverse zero-shot prompt-ing styles, surpassing the prompt designs proposed by Shah et al., 2023. Additionally, few-shot learn-ing and larger LLMs demonstrate notable advan-tages over their smaller counterparts.\n(2) Chain-of-Thought prompting has limited ef-fect on LLMs performance and can sometimes reduce effectiveness. While few-shot learning gen-erally enhances generic LLMs' performance, Ta-ble 1 shows that the difference between prompt-ing styles is marginal. CoT prompting only im-proves the performance of the GPT-40-mini model, whereas it significantly degrades the performance of the LLaMA 3.1 series. Notably, LLaMA 3.1 frequently suffers from \"implied entities\" errors, where it tends to overanalyze and tag words that merely imply a named entity. This failure type is further discussed in subsequent sections."}, {"title": "4.2 RQ2: What types of mistakes do LLMs commonly make?", "content": "We manually annotate the failure types, summarize the limitations of LLMs, and analyze the underly-ing causes based on their responses, as shown in Table 2. The most common failure cases include:\n(1) Contextual misunderstanding of proper noun. LLMs often fail to classify entities that rely on context correctly, such as domain-specific terms or ambiguous entities. For example, person names that overlap with location names, and or-ganizational entities containing person or location names may be incorrectly categorized.\n(2) Pronouns and generic terms. Terms such as pronouns (\"he\" or \"a woman\"), and generic phrases (\"universities\" or \"automakers\") are some-times misclassified as specific entities.\n(3) Citizenship Terms. Words related to citizen-ship, such as \"Chinese\" or \"British\", are often mis-classified as locations despite referring to national identities.\n(4) Implied entities. LLMs frequently misinterpret terms that imply specific entities. For example, product names like \"iPhone\" or \"Google Maps\" are often mislabeled as organizational entities due to their association with companies.\n(5) Entity omission and boundary errors. LLMs struggle to recognize certain entities, such as abbre-viations or long entities (e.g., long addresses). They may either omit these entities entirely or incorrectly segment them."}, {"title": "5 Discussion", "content": "The findings of our study highlight several potential directions for improving the performance of LLMs on financial NER tasks:\nTuning LLMs for the Financial Domain. A significant proportion of the observed failure cases involve domain-specific proper nouns. Fine-tuning LLMs with financial data could enhance their abil-ity to accurately recognize such entities.\nImplementing self-correction strategies. Our analysis in RQ2 identifies common mistakes made by LLMs in the FiNER-ORD task. Developing self-verification prompting strategies could allow LLMs to recognize and address these errors, thereby re-ducing recurrent failures."}, {"title": "6 Conclusion", "content": "This study presents the first systematic evaluations of generic LLMs in the FiNER-ORD task under different prompt designs, compared to state-of-the-art fine-tuned transformer-based models. Through comprehensive experiments with LLMs and their related lightweight versions, we demonstrate the capabilities and limitations of generic LLMs in handling domain-specific tasks. Our findings cat-egorize five representative types of failures, along with their underlying causes. We release artifcats for future research 1."}]}