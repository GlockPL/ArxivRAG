{"title": "Whither Bias Goes, I Will Go: An Integrative, Systematic Review of Algorithmic Bias Mitigation", "authors": ["Louis Hickman", "Christopher Huynh", "Jessica Gass", "Brandon Booth", "Jason Kuruzovich", "Louis Tay"], "abstract": "Machine learning (ML) models are increasingly used for personnel assessment and selection (e.g., resume screeners, automatically scored interviews). However, concerns have been raised throughout society that ML assessments may be biased and perpetuate or exacerbate inequality. Although organizational researchers have begun investigating ML assessments from traditional psychometric and legal perspectives, there is a need to understand, clarify, and integrate fairness operationalizations and algorithmic bias mitigation methods from the computer science, data science, and organizational research literatures. We present a four-stage model of developing ML assessments and applying bias mitigation methods, including 1) generating the training data, 2) training the model, 3) testing the model, and 4) deploying the model. When introducing the four-stage model, we describe potential sources of bias and unfairness at each stage. Then, we systematically review definitions and operationalizations of algorithmic bias, legal requirements governing personnel selection from the United States and Europe, and research on algorithmic bias mitigation across multiple domains and integrate these findings into our framework. Our review provides insights for both research and practice by elucidating possible mechanisms of algorithmic bias while identifying which bias mitigation methods are legal and effective. This integrative framework also reveals gaps in the knowledge of algorithmic bias mitigation that should be addressed by future collaborative research between organizational researchers, computer scientists, and data scientists. We provide recommendations for developing and deploying ML assessments, as well as recommendations for future research into algorithmic bias and fairness.", "sections": [{"title": "Machine Learning Models in Industrial-Organizational Psychology", "content": "Table 1 defines key terms relevant to ML and algorithmic bias. In general, ML for personnel assessment relies on supervised ML, where algorithms are trained to use predictors to model some outcome (e.g., job performance, dependability; often referred to as the \u201cground truth\"), which \"supervises\" predictor use. The resulting models replicate patterns in their inputs. Any empirical predictor weighting scheme keyed based on observed outcomes\u2014such as those used for biodata (e.g., Cucina et al., 2012) or to weigh multiple assessments together into a composite (e.g., OLS regression; Wainer, 1976)\u2014can be considered supervised ML models. Table S1 in the additional online material provide additional detail on unsupervised and reinforcement learning, but we focus on supervised ML because organizations are rapidly adopting supervised ML (hereafter ML) due to time and cost savings (Campion et al., 2016). The promise of ML models is that they can assess more individuals quicker than humans in a consistent, replicable manner treating all applicants equally. Despite this promise, concerns exist throughout society about algorithmic bias in high-stakes decision-making contexts.\""}, {"title": "Algorithmic Bias and Fairness", "content": "Industrial-organizational psychology has long investigated bias and fairness, yet most computer science research on algorithmic bias mitigation has ignored this body of research."}, {"title": "Machine Learning Algorithm Life Cycle", "content": "To understand algorithmic bias and methods for mitigating it, it is crucial to consider the process of developing and deploying supervised ML algorithms. The four major steps in this process include: 1) generating training data, 2) training the model, 3) testing the model, and 4) deploying the model. We present these as sequential steps, but they are often completed in an iterative fashion. For example, if the model scores exhibit poor validity in testing, the organization may collect additional data and repeat the process until validity improves. Similarly, after deployment, the algorithm's psychometric properties may change over time (SIOP, 2018), eventually necessitating updated training data and model retraining.\nFurther, actions such as creating rigorous documentation of the data involved and decisions made throughout the development process can be used to judge the resulting model's fairness (Landers & Behrend, 2022). Detailed documentation enables auditing the training data and process for potential concerns, which can then spur remediation (Bandy, 2021). Putting each major decision into writing could cause ML model developers to think more deeply about their decisions and the resulting effects (e.g., model cards; Mitchell et al., 2019)."}, {"title": "Generating Training Data", "content": "Generating training data is the process of designing and collecting data and extracting quantitative information from unstructured data (e.g., feature engineering). Important considerations at this stage include the sample characteristics (including demographics), choice"}, {"title": "Training the Model", "content": "Training (or fitting) a supervised model involves mathematically estimating relationships between predictors and the outcome variable. This results in a model with fitted parameters that can score future, unseen cases. The decisions relevant to fairness at this step include the type of model(s) to train and the model's optimization (or loss) function (Landers & Behrend, 2022).\nThe type of algorithm (e.g., random forest, ordinary least squares [OLS] regression, neural network) used for training is relevant because a given model may underfit or overfit the training data, which relates to the bias-variance tradeoff (in this tradeoff, \u201cbias\u201d refers to error from mistaken assumptions made by the algorithm; Yarkoni & Westfall, 2017). When a model overfits the training data, it minimizes prediction errors in the training data (i.e., low bias), but its parameters may change drastically when trained on another random sample drawn from the same population (i.e., high variance), resulting in poor generalizability and cross-validity. When a model underfits the training data, it exhibits greater prediction errors in the training data (i.\u0435., high bias) but is less likely to change drastically when trained on a new sample (i.e., low"}, {"title": "Testing the Model", "content": "Testing the model involves estimating the psychometric properties of the trained model's scores in data not used during model hyperparameter tuning or training. Important considerations include the testing process, which psychometric properties to evaluate, the criteria set for determining adequate validity, and the characteristics of the test data relative to training and deployment data (Landers & Behrend, 2022). The testing process regards the type of cross-validation used. For example, cross-validation can be conducted with a single train/test split, k- folds, or in a temporally lagged sample (Landers & Behrend, 2022). A single train/test split from one sample is least rigorous because it is prone to sampling error, while temporally lagged cross- validation is most rigorous, because it reflects how the model will be used during deployment."}, {"title": "Deploying the Model", "content": "Deploying the model occurs when the decision has been made to use a trained ML model for high-stakes assessment. Important considerations include how applicants react to the way algorithms are used in the assessment process, what information is shared with applicants (Landers & Behrend, 2022), consistency in the ML algorithm scores' psychometric properties compared to those observed during model testing (e.g., that there is minimal shrinkage; Song et al., 2017), and how the scores are used to make selection decisions. Additionally, many concerns from prior stages are relevant to deployment as well (e.g., measurement bias in predictors; similarity of training, test, and deployment populations), but we do not describe them again here."}, {"title": "Transparency and Openness Statement", "content": "We followed the Journal of Applied Psychology methodological checklist, and the data generated by our review is available on OSF together with additional online material: https://osf.io/8tp2j/. Our focus was on the classification, regression, and other metrics (i.e., fairness operationalizations), as well as the bias mitigation methods, their categorization, and descriptions. The study did not involve human subjects data so did not require IRB approval."}, {"title": "Systematic Review", "content": "We conducted a systematic literature review following the PRISMA guidelines. We used multiple search terms in computer science and organizational research databases, as displayed in Figure 3. We also used ancestor and child searches of core articles on the topic. We systematically screened the literature for two types of papers: (1) those that propose or review operational fairness definitions, and (2) those that propose or investigate algorithmic bias mitigation methods. Our initial search uncovered 3,321 potentially relevant articles. After the"}, {"title": "Results", "content": "Figure 1 illustrates the connections among the ML model development life cycle, fairness operationalizations, legal requirements, and bias mitigation methods. Table 4 reports fairness operationalizations across the ML model lifecycle. Tables 5-9 report, respectively, for each segment of the life cycle: 1) conceptual categories of bias mitigation methods, 2) which fairness operationalizations are addressed by each category, and 3) whether the methods are likely legal according to United States anti-discrimination statutes and case law. In each section below, we discuss the sources of bias or fairness operationalizations addressed by each bias mitigation method and connect the fairness operationalizations to psychometric bias and adverse impact."}, {"title": "Generating Training Data", "content": "Our search uncovered several fairness operationalizations related to characteristics of the training data: equivalence of computed features (Tay et al., 2022), adverse/disparate impact (Uniform Guidelines, 1978), training sample representativeness (Tay et al, 2022), and potential bias (Minot et al., 2022). Equivalence of computed features concerns whether predictor variables"}, {"title": "Legal Requirements", "content": "To our knowledge, no laws in the United States or anywhere else govern ML model training data characteristics. This means that training data is not required to be representative of the population to which the algorithm will be applied nor to include underrepresented groups ."}, {"title": "Preprocessing Bias Mitigation Methods", "content": "Bias mitigation approaches at this stage are known as preprocessing techniques since they modify the training data prior to model training. Our search uncovered 264 mentions of preprocessing methods. We uncovered three broad classes of preprocessing: (1) balancing"}, {"title": "Training the Model", "content": "Fairness operationalizations during model training regard causal fairness\u2014or whether demographics affect ML model scores\u2014and include fairness through unawareness (Hardt et al., 2016), disparate treatment, counterfactual fairness, and fair inference (Verma & Rubin, 2018).\nFairness through unawareness (also known as anticlassification and fairness through blindness) considers ML models fair if they do not use demographic information during deployment. Counterfactual fairness (Kusner et al., 2017) is similar to no unresolved discrimination, no proxy discrimination (Verma & Rubin, 2018), and total effect (Makhlouf et al., 2021). All regard whether demographic membership causally affects the predictors in the ML model\u2014or whether the ML model scores depend on a descendent of a demographic variable. This involves using causal graphs (e.g., directed acyclic graphs) to illuminate how demographic variables affect predictor variables and how those predictor variables affect the outcome. Sometimes counterfactual fairness is examined by testing whether the ML model outcome remains the same if the individual were a member of a different demographic group, keeping all other predictors"}, {"title": "Legal Requirements", "content": "Although no United States laws explicitly govern ML model training, anti-discrimination employment (case) law (e.g., CRAs of 1964 and 1991) applies to model training. To avoid disparate treatment, ML models cannot use protected demographic information as predictors."}, {"title": "Inprocessing Bias Mitigation Methods", "content": "During model training, bias mitigation strategies are called inprocessing techniques because they occur while the algorithm processes data during training. Our search uncovered 256 mentions of inprocessing methods. The vast majority of inprocessing methods modify the algorithm's loss function to simultaneously optimize validity and a fairness operationalization. A second approach trains separate models, and a third uses an initial ML model's predictions to reweigh observations (which we describe only in the additional online material). In industrial- organizational psychology, methods that simultaneously optimize validity and fairness are known as multi-objective optimization (e.g., Song et al., 2017). Most work in psychology focused on the normal boundary intersection (NBI) pareto optimization method implemented by De Corte et al. (2007), but Rottman et al. (2023) and Zhang et al. (2023) recently showed how to incorporate such constraints in other algorithms' loss functions (and this has been done in computer science too; e.g., adversarial learning; Zhang et al., 2018). Multi-objective optimization can maintain validity while minimizing adverse impact."}, {"title": "Testing the Model", "content": "Group fairness during model testing regards group-level consistency in scores (i.e., independence) and validity (i.e., separation and sufficiency), including adverse/disparate impact (Uniform Guidelines, 1978), conditional statistical parity, equal accuracy (Berk et al., 2021), and differential functioning (Tay et al., 2022). Additionally, individual fairness regards consistency in scores assigned to individuals who are similar except for their demographics.\nAdverse/disparate impact is well-known to organizational researchers, given that the CRA of 1964 established that substantial adverse impact against members of a protected group constitutes prima facie evidence of discrimination. Computer scientists also refer to adverse impact as demographic/statistical parity and independence (i.e., ML model outcomes should be statistically independent of demographic membership). Other notions have gone further to consider all potential distributional differences, exemplified by the use of Jensen-Shannon and Kullback-Leibler divergence measures that capture differences in two sets of scores' distributions. All are concerned with potential score and outcome differences between groups."}, {"title": "Legal Requirements", "content": "The CRAs of 1964 and 1991 apply to how the models will be used. Because disparate treatment is outlawed, organizations must apply the same model regardless of protected demographics. Further, the ML model scores must be used consistently across legally protected subgroups because any changes to scores or score interpretation based on subgroup membership constitute disparate treatment and/or subgroup norming."}, {"title": "Postprocessing Bias Mitigation Methods", "content": "These approaches are known as postprocessing techniques since they involve modifying model outputs (e.g., Kamiran et al., 2012). Our search uncovered 92 mentions of postprocessing methods. Although such techniques become part of the model ultimately used for scoring (thus, in a way, making them part of model training), they are not developed until after a model has"}, {"title": "Deploying the Model", "content": "Our search uncovered no fairness operationalizations specific to deployment that are not also relevant to earlier stages. Many operationalizations from earlier stages apply to deployment, including measurement bias in the predictor variables, causal fairness, group fairness, and individual fairness/consistency of the ML model scores. In other words, virtually all fairness operationalizations not concerned with the ground truth also apply to model deployment."}, {"title": "Legal Requirements", "content": "Any laws that apply to other selection procedures in the United States apply to deployed ML models, including the Civil Rights Acts of 1964 and 1991 and New York City Local Law 144. Thus, during deployment ML model scores should be monitored for adverse impact. Organizations should use work analysis to establish a link between assessment and job content, and collect multiple types of validity evidence. Additionally, group differences should trigger heightened scrutiny, particularly if those group differences are larger than observed during testing or if they increase over time. Doing so can help ensure validity and legality. Further, disparate treatment is outlawed, annual \u201cbias\u201d audits are required if operating in New York City, and Illinois requires informing job applicants if they will be evaluated by an ML model."}, {"title": "Bias Mitigation Methods", "content": "Because disparate treatment is outlawed, bias mitigation must occur prior to deployment. During deployment, protected demographic information cannot be used: as a predictor variable, to determine which transformation is applied to predictor variables, to determine which ML model will be applied to the individual, or to determine how to postprocess ML scores. Group agnostic approaches to score adjustments are acceptable\u2014such as score banding. However, this"}, {"title": "Discussion", "content": "Because work on algorithmic bias spans many fields, our work takes an important first step toward integrating the vast, varied perspectives on a topic that is central to industrial- organizational psychology. Integrating this literature for diverse audiences is important because some fairness operationalizations and algorithmic bias mitigation methods uncovered by our systematic review are likely outlawed by United States and European employment law. In many cases, there is substantial overlap across fields in how fairness is operationalized, considering the focus on adverse impact, differential functioning, and disparate treatment. However, there are also substantial gaps, including suggestions to use postprocessing methods that utilize demographic information to adjust scores (i.e., disparate treatment) and a lack of emphasis on predictive bias. Inprocessing approaches to multi-objective optimization have been shown to be effective across fields, and preprocessing approaches can provide additional benefits and are"}, {"title": "Methodological and Practical Implications", "content": "Our review focused on works that specifically referred to algorithms or ML. Though we examined their application through the lens of personnel selection, ML methods are flexible and quickly becoming ubiquitous in society. However, an algorithm is, simply, a set of rules for converting inputs to outputs for a certain task. Thus, our review and its implications are potentially applicable to any approach for empirically deriving weights for any model intended for high-stakes decision making. Regardless of whether the context is biodata, traditional selection systems, social media, clinical psychology, criminal justice, or loan decisions, the same concerns, recommendations, and remedies apply. The cross-disciplinary nature of these concerns suggests a need for additional cross-disciplinary reviews and research.\nAlthough most bias mitigation methods focus on the context of two subgroups (e.g., White and Black applicants), it is possible to extend many solutions to handle multiple subgroup comparisons (Kozodoi et al., 2022). Indeed, preprocessing methods for balancing representation and ground truth means (Zhang et al., 2023), inprocessing multi-objective optimization techniques (Rottman et al., 2023), and postprocessing methods (Zehlike et al., 2023) can all be extended for multiple subgroup comparisons. However, increasing the number of subgroup comparisons increases the sample size needed to achieve a solution that will cross-validate with minimal shrinkage, so it should only be considered when sample sizes are adequately large. This limits the applicability of such approaches in cases of intersectional identities, given the rapidly increasing number of subgroups when intersectional identities are considered. Inprocessing approaches for reducing group differences in a single subgroup comparison require at least 300"}, {"title": "Recommendations for Developing, Deploying, and Maintaining ML Models", "content": "In addition to the effectiveness of the different approaches described above, our findings and information gleaned from the review have important implications for organizations. Table 9 reports our recommendations for developing, validating, and maintaining ML models. First, rigorous documentation of all decisions made and code used should be maintained. One example, prescriptive technique for documenting these decisions, any measurable group differences, and the context in which a model is meant to be used is called \u201cmodel cards\" (Mitchell et al., 2019). These cards capture preprocessing, inprocessing, and postprocessing decisions, metrics, intended uses, and ethical considerations and designer recommendations, but they are intended to serve as detailed summaries rather than comprehensive enumerations."}, {"title": "Future Directions", "content": "Inprocessing bias mitigation methods are widely studied, but little work has examined how to effectively conduct multi-objective optimization for end-to-end deep learning with language models. Such models are rapidly being adopted in organizational research and practice (e.g., Speer et al., 2023; Thompson et al., 2023). Modern embedding methods are neural networks trained on masked language prediction, and they can be fine-tuned for other prediction tasks by adding a classifier or regressor layer to the neural network. We noticed very few articles in our review that conducted inprocessing with end-to-end deep learning with language models\nRelatedly, an emerging issue regards the potential use of LLMs such as OpenAI's Generative Pretrained Transformers (GPT) or Anthropic's CLAUDE for assessment and selection tasks. Zero-shot (i.e., no examples) or few-shot (i.e., a few training examples) prompts for LLMs can be used to evaluate the quality of open-ended assessment responses (Hickman & Liu, 2024). Further, LLMs can be used to query documents, such as resumes. However, we know little about the potential biases in these systems, and new approaches are needed for minimizing any biases, given that retraining them is prohibitively expensive. Initial research suggests diversity valuing prompts can mitigate LLM biases (Glazko et al., 2024).\nTo date, we know little about the effects of combining multiple debiasing methods. For example, oversampling to remove group differences in training data and using multi-objective optimization might, in combination, reduce group differences in the predictor composite more than using only one of the methods. Of the available research, ensembling\u2014or using multiple types of preprocessing or inprocessing methods then averaging their results\u2014worsens fairness operationalizations relative to using a single preprocessing or inprocessing method (Badran et al., 2023; Feffer et al., 2022). However, using multiple methods sequentially (e.g., preprocessing to remove ground truth group differences, then inprocessing multi-objective optimization) does improve upon using just one method (Feldman & Peake, 2021). However, additional research is needed to determine when combinations of methods are effective in high-stakes settings."}, {"title": "", "content": "Some approaches to enhancing fairness that are not algorithmic bias mitigation methods deserve note. Participatory design regards involving stakeholders who will be affected by the use of AI in the design and development process (e.g., Trewin et al., 2019). This includes gathering input when generating the training data, selecting predictors, deciding on the algorithm's optimization function and which ground truth to use. Further, stakeholders can be involved in the testing process to evaluate if the trained model is likely to provide a net benefit to stakeholders.\nExplainable AI (XAI) involves explaining the functioning of ML models or their specific outputs in a way that is understandable to humans (Langer, Oster, et al., 2021). Global X\u0391\u0399 methods focus on reporting: the model's accuracy and validity, how the model was developed and what training data was used, cases that receive low or high scores from the trained model, and/or the model's most important predictors. Local XAI methods focus on how a particular case was scored and report influential predictors for the focal case's score, near neighbor example cases that would receive the same score, and/or near neighbor example cases that would receive a higher or lower score (Lai et al., 2023). Thus, XAI holds potential for uncovering fairness issues in the model development process as well as in the model's functioning itself.\nAlthough our review focused on supervised ML, similar concerns exist for unsupervised ML and reinforcement learning. Given the empirical superiority of supervised ML for classification and regression problems (as in personnel assessment), most bias mitigation methods to date have focused on supervised ML. However, the characteristics of the training sample\u2014including group mean differences on the predictors and ground truth variable\u2014are concerns for any type of ML. To the extent that unsupervised ML and reinforcement learning grow in use for psychological, educational, and pre-employment testing, more research will be needed on robust methods for enhancing fairness and reducing bias in them."}]}