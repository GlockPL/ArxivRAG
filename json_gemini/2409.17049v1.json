{"title": "ControlCity: A Multimodal Diffusion Model Based Approach for Accurate Geospatial Data Generation and Urban Morphology Analysis", "authors": ["Fangshuo Zhou", "Huaxia Li", "Rui Hu", "Sensen Wu", "Hailin Feng", "Zhenhong Du", "Liuchang Xu"], "abstract": "Volunteer Geographic Information (VGI), with its rich variety, large volume, rapid updates, and diverse sources, has become a critical source of geospatial data. However, VGI data from platforms like OSM exhibit significant quality heterogeneity across different data types, particularly with urban building data. To address this, we propose a multi-source geographic data transformation solution, utilizing accessible and complete VGI data to assist in generating urban building footprint data. We also employ a multimodal data generation framework to improve accuracy. First, we introduce a pipeline for constructing an 'image-text-metadata-building footprint' dataset, primarily based on road network data and supplemented by other multimodal data. We then present ControlCity, a geographic data transformation method based on a multimodal diffusion model. This method first uses a pre-trained text-to-image model to align text, metadata, and building footprint data. An improved ControlNet further integrates road network and land-use imagery, producing refined building footprint data. Experiments across 22 global cities demonstrate that ControlCity successfully simulates real urban building patterns, achieving state-of-the-art performance. Specifically, our method achieves an average FID score of 50.94, reducing error by 71.01% compared to leading methods, and a MIoU score of 0.36, an improvement of 38.46%. Additionally, our model excels in tasks like urban morphology transfer, zero-shot city generation, and spatial data completeness assessment. In the zero-shot city task, our method accurately predicts and generates similar urban structures, demonstrating strong generalization. This study confirms the effectiveness of our approach in generating urban building footprint data and capturing complex city characteristics.", "sections": [{"title": "1. Introduction", "content": "With the growing demand for geographic information, Volunteer Geographic Information (VGI) platforms have become vital tools for acquiring and updating geospatial data. The open and dynamic nature of VGI data provides significant advantages in the geographic information field. OpenStreetMap (OSM), as a leading example of VGI, relies on contributions from volunteers worldwide, constantly up-dating and refining its data to provide comprehensive geo-graphic information to global users. Due to its open data policy, broad community involvement, and accessibility, OSM has become a widely-used platform for sharing geographic information on a global scale.\nAlthough OSM has demonstrated its value in numerous studies, the heterogeneity of its data raises concerns about data quality. The quality and completeness of OSM data vary significantly across different regions and data types, result-ing in notable asymmetries in geographic data."}, {"title": "2. Related Works", "content": null}, {"title": "2.1. Diffusion Model", "content": "In recent years, diffusion models have made significant advancements in the field of im-age generation, surpassing previously dominant models such as Generative Adversarial Networks (GANs) , Variational Autoencoders (VAE), and Flow models. Diffusion models generate images by converting Gaus-sian noise into the target distribution through an iterative denoising process, which involves two stages: diffusion and denoising. Denoising Diffusion Probabilistic Models (DDPM) improved the training method for diffusion models by employing variational inference to train a parameterized Markov chain, enabling the generation of high-quality samples. To improve the sampling efficiency of diffusion models, Denoising Diffusion Implicit Models (DDIM) introduced a non-Markovian diffusion process, significantly reducing the number of sampling steps and accelerating the generation speed. enhanced the flexibility and efficiency of gener-ative models by introducing stochastic differential equations. Conditional diffusion models extend DDPM by adjusting the output based on additional input information, similar to conditional GANS (CGAN) and conditional VAES (CVAE).\nIn the field of text-driven image generation, diffusion-based methods are currently considered the most promis-ing. These methods typically leverage pre-trained language models, such as CLIP(?), to encode text input into latent vectors. GLIDE uses a cascaded diffu-sion architecture to enable text-guided image generation and editing. Imagen, utilizing the powerful text comprehension capabilities of the large pre-trained lan-guage model T5 , significantly improves high-fidelity image generation. Currently, the most popular text-to-image generation model is based on latent diffusion (LDM), a.k.a. Stable Diffusion. This model is trained on the large-scale LAION-5B image and text dataset, performing the diffusion"}, {"title": "2.2. Controllable Image Synthesis", "content": "Text-guided diffusion models have demonstrated a cer-tain capability in generating images that meet user expec-tations, but they still lack fine-grained control for specific domain tasks. Typically, this fine-grained control signal is expressed in image form, such as using segmentation maps to control the layout and shape of the generated im-age, or employing sketches as structural infor-mation to precisely guide image generation. Additionally, extracting semantic information from input images allows for the generation of personalized images, enabling content control. However, early control methods were often designed for specific tasks, and this task-specific design has limited their broader application in the community. The challenge remains how to build a general framework on top of existing pre-trained diffusion models (e.g., Stable Diffusion) that can support large-scale user adoption.\nTo meet practical demands, researchers have rapidly ex-plored general frameworks to handle various types of spatial conditions. The T2I-Adapter aligns exter-nal control signals with the internal knowledge of pre-trained text-to-image (T2I) diffusion models, enabling more refined control over the generation process. ControlNet introduced a trainable copy of the UNet encoder, encoding additional conditional signals into latent represen-tations, which are injected into the backbone of the T2I diffusion model via zero convolution. IP-Adapter utilizes CLIP to extract global semantic represen-tations from images and achieves content control through decoupled cross-attention. InstantID uses an innovative IdentityNet and lightweight image adapter to achieve personalized facial transfer. Uni-ControlNet and Ctrl-X achieve a flexible combination of structural control and semantic appearance control by designing different architectures. However, while these methods improve image generation control to some ex-tent, they remain insufficient when handling complex spatial conditions, particularly in generating harmonious, natural, and morphologically accurate building footprints under mul-timodal geospatial conditions."}, {"title": "2.3. Urban building layout generation method", "content": "In urban planning and architectural design, traditional procedural city modeling initially relied on manually written rules and constraints to generate city layouts, effectively ensuring the rationality of topological structures. However, this manual process requires de-signers to define rules and design options, limiting the flexi-bility and efficiency of the design process. In recent years, with the rapid develop-ment of AI generation technologies, researchers have begun exploring the use of generative models to automatically produce city layouts that meet specific requirements. LayoutGAN leverages GANs to learn layout design rules and features, generating image layouts that meet design standards. Lay-outVAE, on the other hand, learns dis-tributions from existing layout data, enabling it to generate layouts similar to input data, while also offering diversity and variations. Initially, these layout generation models were pri-marily applied to document and graphic layouts but have gradually expanded to other domains. In the field of interior design, House-GAN introduced a graph-constrained GAN to automatically generate diverse and realistic house layouts that match input bubble diagrams. Graph2Plan combines generative models with user inter-action, creating floor plans based on input layout diagrams and building boundaries that align with user requirements. Additionally, other studies have used deep generative models to synthesize indoor scenes.\nAs research shifts towards large-scale urban building layout synthesis, GAN-based models have increasingly been applied to generate building layouts for different cities, demonstrating GANs' unique adaptability in learning ur-ban morphology. BlockPlanner introduced a vectorized dual-layer graph representation to enable diverse and efficient city block generation, while ESGAN combined deep generative methods with urban condition encoding to generate visually realistic and semantically coherent city layouts. The method proposed by generates realistic city layouts based on ar-bitrary road networks, addressing the limitations of existing methods when handling irregularly shaped city blocks and diverse building morphologies. This approach is capable of generating realistic city maps under larger-scale and more complex layout conditions. InstantCity generates high-resolution building vector data from street networks, showcasing its potential for urban geo-graphic applications in experiments across 16 global cities. However, existing methods typically rely on single-modality data to generate city layouts, focusing mainly on road network constraints, and struggle to scale up to large urban agglomerations. To address this, we propose ControlCity, a method that integrates multimodal data constraints on urban building layouts and morphology. It consolidates the morphological knowledge of multiple cities into a single model, opening the possibility of learning the morphology of global urban agglomerations."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Overview", "content": "We first propose a multimodal aligned data construction pipeline that processes each tile's road network, landuse, OSM attribute data, Wikipedia data, and coordinate data into image prompts, text prompts, and metadata, and aligns them with the target building footprint tiles. This results in the creation of an \"image-text-metadata-building footprint\" quadruple dataset. We used datasets from 22 different cities around the world, encompassing various urban morpholo-gies, to conduct a comprehensive evaluation of the model.\nNext, we enhanced the pre-trained text-to-image gen-eration model (i.e., Stable Diffusion XL) to integrate text, image, and metadata as inputs, guiding the generation of high-resolution target building footprint images.\nIn the experiments, we evaluated the generated raster and vector data from different cities using visual and GIS-related metrics. Additionally, we explored the model's performance in three downstream tasks: urban morphology transfer, zero-shot city generation, and building completeness detection."}, {"title": "3.2. Dataset Construction", "content": "OpenStreetMap (OSM) is an open, free, and editable mapping project created and maintained by volunteers world-wide. OSM data comes from a wide range of sources, including government open data, manual contributions from volunteers, field surveys, and automated extraction using computer vision techniques. OSM ensures the timeliness and accuracy of its data through user error reports, community reviews, and manual corrections by volunteers. Based on these methods, OSM has developed a comprehensive and precise global map database.\nTo evaluate the performance and scalability of our pro-posed method, we extracted the latest building footprint data for 22 cities from OSM. These cities are primarily economically developed regions with high completeness in building data. The selected cities are distributed across var-ious continents and countries, representing major global ur-ban morphologies, providing ample data support for model training and evaluation.\nPrevious studies have demon-strated that the 1000m-per-tile level better preserves the con-tinuity of urban morphology and provides more contextual information, leading to more stable model training, infer-ence, and downstream tasks. Therefore, this study focuses exclusively on tiles at this level. We used a preprocessing pipeline to convert the study area into 1024 \u00d7 1024 raster tiles, which were stored in a WMTS (XYZ tile) directory. Additionally, the pipeline retrieved the corresponding street network and landuse maps for each tile from MapBox.\nIn OSM, each object (feature) is represented as a digi-tized geometric entity, such as polygons, lines, or points, and contains rich geographic attributes (e.g., \"highway,\" \"natu-ral,\" and \"landuse\"). These attributes are expressed as key-value pairs that represent the semantic properties of the ob-ject, e.g., \"building: industrial.\" However, not all attributes are relevant. Based on the attribute filtering methodology of LHRS-Bot , we collaborated with several experts to evaluate the relevance of each attribute to the building footprint tiles, ultimately selecting 185 relevant attributes. Unrelated key-value pairs were filtered out using a preprocessing pipeline, which also aggregated the geometric features belonging to the same tile and calculated the count of each attribute, forming the OSM-Caption feature set. Additionally, the geographic coordinates of the center of each XYZ tile were computed and used as metadata.\nWikipedia's GeoSearch feature is a location-based search tool that allows users to find Wikipedia entries for nearby buildings based on specific coordinates or place names. The semantic information of building footprints (e.g., shape and layout) cannot be fully expressed using only OSM data, but Wikipedia's GeoSearch can compensate for this limitation. Using the center coordinates of each XYZ tile, we searched for buildings within a 500-meter radius and retrieved their detailed descriptions from Wikipedia, which were used as Wikipedia-Caption features.\nAt this point, we have obtained coarse-grained geo-graphic features aligned with the target images, including metadata, OSM-Caption features, Wikipedia-Caption fea-tures, road network maps, and landuse maps. However, the Caption features retrieved directly from Wikipedia are often overly detailed, containing a significant amount of non-building-related information. Additionally, OSM-Caption features are represented as key-value pairs and lack gram-matical structure. These issues result in excessively long geographic feature captions (with the longest containing tens of thousands of words) and disorganized language structure, which negatively impacts the performance of the text encoder and thus limits the model's effectiveness.\nExisting research suggests that using large language models (LLMs) to generate image captions is an effective approach. We applied an LLM to recaption both Wikipedia-Caption and OSM-Caption features. After testing a small portion of data and incorporating feedback from domain experts, we ulti-mately selected GPT-40 mini as the model for generating captions, achieving an optimal balance between generation efficiency and quality. In the end, we built a quadruple data processing pipeline that includes \"image-text-metadata-building footprint\" and used it to create an aligned dataset of images and geographic features, containing 3,140 sample pairs."}, {"title": "3.3. Model Architecture", "content": "Unlike traditional Image-to-Image Conditional Genera-tive Adversarial Networks (GANs), our model architecture does not rely on conventional image transformation meth-ods. Instead, we use the input image data as one of the multimodal conditions, combining it with text and metadata as additional modalities to jointly guide the target image generation process.\nControlCity can generate high-resolution 1024 \u00d7 1024 pixel images, thanks to the application of Stable Diffusion XL (SDXL). SDXL is an advanced Text-to-Image generation model based on the principles of diffusion probabilistic models, capable of generating high-quality images of any resolution from textual descriptions. This model is built upon Latent Diffusion Models (LDMs) and is one of the most cutting-edge methods for text-to-image generation.\nThe SDXL architecture consists of three core compo-nents: a text encoder, an autoencoder, and a diffusion model. The text encoder is composed of a pre-trained combination of OpenCLIP ViT-bigG and CLIP ViT-L, providing power-ful text input processing. The autoencoder (VAE) operates in latent space, effectively compressing and reconstructing image information. The core of the diffusion model includes a noise scheduler and a convolution-based noise prediction network (UNet), which model and predict the noise during the image generation process.\nFor a given building footprint image $x \\in R^{W \\times H \\times 3}$, during each forward pass, the VAE encoder $\\varepsilon$ first encodes the target image $x$ into latent space: $z(x) = \\varepsilon(x)$, while simul-taneously sampling Gaussian noise $e$ of the same size. The pre-trained text encoders, OpenCLIP ViT-bigG and CLIP ViT-L, encode the text prompt $c_t$, and the cross-attention layer is used to guide the denoising process of the diffusion model $\\epsilon_{\\theta}$.\nFor metadata $c_m = (m_{lon}, m_{lat})$, a simple approach would be to treat it as part of the text description. However, discretizing continuous covariates is both unnecessary and suboptimal. To avoid the inherent issues text encoders face when handling numeric information, inspired by Khanna et al. (2023), we encode each piece of metadata using sinusoidal embeddings, similar to timestep embeddings. Specifically, the formula is as follows:\n$\\begin{aligned}\nProj(k, 2i) &= sin (m\\Omega - \\frac{2 \\pi i}{d}), \\\\\nProj(m, 2i + 1) &= cos (k\\Omega - \\frac{2 \\pi i}{d})\n\\end{aligned}$\nwhere $i$ is the index of the feature dimension, and $\\Omega = 1000$. Each piece of metadata is projected using an Multi-Layer Perceptron(MLP) to the same dimensionality as the timestep embedding. Then, the two metadata embedding vectors are summed and combined with the timestep embedding to generate the final conditional vector $c_{m,t}$:\n$c_{m,t} = \\sum_{j=1}^{2} MLP([Proj(m_j, 0), ..., Proj(m_j, d)]) + MLP([Proj(t, 0), ..., Proj(t, d)])$ \nAdditionally, for image modality signals, we selected an additional network for encoding. ControlNet leverages a trainable copy of the UNet encoder from the diffusion model to encode specific control signals into latent representations, which are injected into the backbone of the diffusion model via zero convolution. A common strategy is to train separate ControlNets for street network maps and landuse maps, jointly controlling the generation process. However, this approach struggles to generate coor-dinated and natural results when dealing with complex sig-nals. Therefore, we introduced improvements to ControlNet.\nFirst, we concatenate the street network map $c_s$ and the landuse map $c_l$ along the channel dimension to form a composite image condition $c_i$: $c_i = Concat(c_s, c_l)$. Then, it is encoded into a feature map $c_f$ via a small network $C_f = Conv(c_i)$. The feature map is processed through three downsampling blocks and an intermediate block to extract multi-scale features from the input image, denoted as $F_c = \\{F_{Down}, F_{CADown}, F_{CADown}, F_{Mid}\\}$, and injected into the diffusion model $\\epsilon_{\\theta}$ via zero convolution to guide the denoising process.\nDuring inference, the VAE decoder D is used to recon-struct the final denoised latent encoding $z_{\\theta} (x)$ back into the RGB space, yielding the generated image $x = D(z_{\\theta} (x))$. The overall model architecture is illustrated in Fig. 3."}, {"title": "3.4. Evaluation Metrics", "content": "We evaluated the generated building footprints based on both visual metrics and urban morphology metrics to ensure that the generated images have high visual quality while accurately reflecting the building morphology.\nThe Frechet Inception Distance (FID) is a commonly used metric for evaluating image quality. FID measures the quality of images by calculating the distribution differences between the generated and real images in the feature space of the pre-trained InceptionV3 network. FID is computed as follows:\n$FID = ||\\mu_r - \\mu_g||^2 + Tr(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2})$ \nHere, $\\mu_r$ and $\\mu_g$ are the mean vectors of the real and gen-erated image features, respectively, while $\\Sigma_r$ and $\\Sigma_g$ are the corresponding covariance matrices.\nTo evaluate the model's performance at the building level, we computed the Mean Intersection over Union (MIoU) between the generated images and the ground truth. This metric measures the degree of overlap between buildings in raster data by dividing the number of overlapping pixels between the generated and real images by the total number of building area pixels. A higher MIoU score indicates that the generated building footprints are more similar to the real buildings in terms of shape and size. Additionally, we vector-ized the generated raster images into geospatial polygons to quantitatively assess differences in urban morphology at the GIS level. A Site Cover measures the percentage difference in building area between the total polygons in each tile. % GN Count measures the ratio of the number of polygons in the generated set compared to the real set. The definitions of these metrics are as follows:\n$\\begin{aligned}\nMIOU &= \\frac{Area \\ of \\ Intersection_{Tile}}{Area \\ of \\ Union_{Tile}} \\\\\n\\Delta Site \\ Cover &= 100 \\times (\\frac{Tile \\ Building \\ Area_{GN}}{Tile \\ Area} - \\frac{Tile \\ Building \\ Area_{GT}}{Tile \\ Area}) \\\\\n\\% GN \\ Count &= 100 \\times (\\frac{Polygons \\ per \\ Tile_{GN}}{Polygons \\ per \\ Tile_{GT}})\n\\end{aligned}$\nIf the generated dataset scores close to 0% in A Site Cover, it indicates that the predicted building area closely matches the real data, suggesting high-quality predictions. Similarly, scores approaching 100% or 1 in % GN Count and MIoU suggest that the number of generated buildings accurately reflects the real building count, and the predicted building regions have a high degree of overlap with the real ones. This indicates high accuracy in both location and shape."}, {"title": "3.5. Experiment Setup", "content": "We designed four experiments to study the model's per-formance and potential applications. Experiment 1 aims to evaluate the model's performance across 10 cities. We assess the differences between the generated data and the ground truth using visual and urban morphology metrics, and com-pare them with InstantCity (a GAN-based model) trained on the same dataset. This comparison allows for a comprehensive analysis of the advantages and limitations of our proposed method. It is important to note that InstantCity has not released its data or model weights, so our experimental results are based on a reproduction of its method. Experiment 2 investigates the model's ability to transfer learned urban morphology to new areas, i.e., regions not seen during training. Specifically, the model will use the knowledge trained on four cities from Experiment 1 to generate building footprints for an additional eight cities. For each of the four cities, one similar and one morphologically different city will be selected. We will again compare the results with InstantCity to analyze the model's style transfer performance. This experiment also explores the feasibility of using this method in urban planning research by transfer-ring building morphology from one trained city to another. Experiment 3 investigates the model's generalization ability in unknown city regions. Unlike the style transfer in Exper-iment 2, Experiment 3 aims to explore whether the model can generalize the knowledge learned from multiple cities to entirely unseen city regions. The goal is to assess whether the model can generate generalized building footprints in un-trained areas, rather than transferring specific morphologies. Experiment 4 explores the potential of applying diffusion models for assessing the completeness of OSM building data. To do this, we introduce random errors into the building data to reduce its completeness, creating a degraded OSM building dataset consisting of four cities (which originally had complete building data). Using our model, we generate the corresponding dataset and compare it with the degraded set to evaluate its ability to classify under-mapped areas across diverse urban regions."}, {"title": "4. Experiment and Results", "content": null}, {"title": "4.1. Experiment 1 - Analyze model performance", "content": "We selected 10 cities worldwide (i.e., Beijing, Frankfurt, Jakarta, London, Los Angeles, New York, Rotterdam, Seat-tle, Shanghai, and Singapore) to evaluate the accuracy and generalizability of the model-generated building footprints. All data were sourced from OSM and processed into raster tiles, with each tile covering an area of approximately 1200m \u00d7 1200m.\nAs shown in Fig. 4, we present some of the generated im-ages along with their corresponding conditional inputs (i.e., road networks and landuse), real images, and the images gen-erated by Pix2PixHD (GAN-based) trained on the same data. The road networks are categorized into three types based on their importance, represented by different colors and line thicknesses, while landuse is also color-coded. Pix2PixHD generated relatively accurate building footprints in some cities (e.g., Los Angeles and New York), but failed entirely in Beijing and Shanghai. In contrast, ControlCity produced building footprints that more closely matched reality across all cities, accurately capturing the architectural forms of different urban areas. For example, Beijing demonstrated a structured grid layout with diverse building forms, Frank-furt exhibited irregular courtyard styles, Jakarta featured densely packed small buildings, while Los Angeles and Seat-tle showed regular rectangular blocks and detached house layouts.\nTypically, road networks, as a constraint, can only con-trol the external shape of buildings and cannot predict in-ternal details. In the example of New York, while both methods accurately generated the external building outlines, ControlCity's predictions of internal building details exhib-ited significantly greater similarity compared to Pix2PixHD.\nThis may be attributed to the combined effect of multimodal conditions: first, the text prompts derived from OSM and Wikipedia provide additional information about architec-tural styles, while landuse conditions help further distin-guish between different building types (e.g., commercial vs. residential areas). Additionally, the inclusion of metadata allows the model to reference building morphologies from geographically similar locations during training. This trend was also reflected in the results from Frankfurt, Rotterdam, and Singapore.\nWe conducted a quantitative evaluation of the model using the visual and urban morphology metrics described in Section 3.4, with the results shown in Table 1. The average 4 Site Cover was calculated by taking the absolute value of each 4 Site Cover and then averaging them. Frechet In-ception Distance(FID) reflects the visual similarity between the generated and real images, with lower scores indicat-ing higher image quality. As shown in Table 1, although there are performance variations between cities, overall, ControlCity outperforms Pix2PixHD. Pix2PixHD achieved favorable FID scores only in four cities-Jakarta, Los An-geles, New York, and Seattle-while in other cities, the generated images either lacked many buildings or deviated significantly from reality. In contrast, ControlCity generated images that more accurately captured building shapes, sizes, and regional morphology, demonstrating higher visual qual-ity. For instance, the FIDs for Beijing, Frankfurt and London were 337.52, 245.10, and 272.89, respectively, which were considerably higher than ControlCity's scores.\nTo eliminate the influence of regional differences across cities, we extracted an 8.5\u00d78.5km area (i.e., 7x7 tiles) from each city, computed the urban morphology metrics, and averaged them to compare the models' stability. The average results in Table 1 indicate that ControlCity outperformed Pix2PixHD in the majority of cities, achieving state-of-the-art performance. Fig. 5 shows the stitched images generated by both methods, further validating the consistency between the visual and urban morphology metrics. The results gen-erated by ControlCity exhibit better continuity and density in urban morphology, which is particularly evident in the examples from Frankfurt and Rotterdam. In Frankfurt, most of the tiles predicted by Pix2PixHD failed to generate valid buildings, while ControlCity demonstrated strong stability, a trend observed across other cities as well.\nThe 4 Site Cover values generated by the Pix2PixHD method were all negative, indicating that the total building area produced was smaller than the real value. This is because Pix2PixHD models the relationship between road networks and building footprints exclusively, failing to gen-erate useful information in areas where there is no clear relationship between building polygons and road networks. In contrast, ControlCity utilizes not only road network condi-tions but also text prompts, metadata, and landuse, providing sufficient information for generating building footprints. As a result, the A Site Cover of our model varies across different cities. Pix2PixHD's % GN Count is lower than the real value in most cases, due to two reasons: first, as mentioned above, Pix2PixHD fails to generate effective predictions in some regions of certain cities; second, during vectorization, some polygon connections are converted into larger single polygons. In contrast, our method tends to generate more buildings than the real data across all cities. Observations show that ControlCity is more inclined to generate small buildings within unconstrained building interiors.\nThe MIoU measures the overlap between generated and real buildings. ControlCity's average MIoU is 0.36, sig-nificantly higher than Pix2PixHD's 0.26, indicating better overall prediction accuracy for ControlCity. For example, the MIoU in London dropped from 0.45 for ControlCity to 0.14 for Pix2PixHD, and in Shanghai, it dropped from 0.20 to 0.14. However, in New York, where the building patterns are more regular, both methods performed similarly, as the architectural patterns are simpler and easier to learn.\nFig. 6 further corroborates the above conclusions by analyzing the distribution of metric data. By analyzing the sample images and metric tables, we observe that Control-City consistently generates building footprints that closely align with real-world data, demonstrating higher realism and accuracy, whereas Pix2PixHD performs well only in cities with more regular building patterns. Notably, ControlCity is capable of integrating multiple urban morphologies within a single model for training, while Pix2PixHD requires a sepa-rate model for each city. Therefore, considering both visual and urban morphology metrics, ControlCity demonstrates greater accuracy and stability in generating urban building footprints."}, {"title": "4.2. Experiment 2 - Cross-region style transfer", "content": "In Experiment 1, we demonstrated that the ControlCity model produces more accurate and stable results in terms of building density, distribution, and morphology compared to Pix2PixHD when generating building footprints. In this experiment, our objective is to test the model's ability to transfer urban morphology knowledge. Specifically, we aim to assess whether the model can successfully apply the learned building morphology from one city to another, thus evaluating its potential for rapid urban modeling. This ex-periment is based on the hypothesis from InstantCity that if the target city's morphological features are similar to those of the training city, a pre-trained model can be directly transferred and applied to other cities. For example, Chicago and Philadelphia share similar building morphology, so if the model has been trained on Chicago's data, no additional training is needed for Philadelphia, as their morphology is similar. Conversely, for cities with distinct building mor-phologies, e.g., Chicago and Los Angeles, the model should be able to generate Chicago-style building morphology in specified regions of Los Angeles.\nTo test this hypothesis, we selected four source cities from Experiment 1 (i.e., New York, Seattle, London, and Jakarta) and generated building footprint data for eight addi-tional target cities (i.e., Detroit, Jersey, Chicago, San Fran-cisco, Manchester, Paris, Manila, and Sumatra). The ap-proach involved using the source city names in the text prompts (aligned with their morphology during model fine-tuning), while the image conditions and metadata were based on the target area's information. For comparison, we selected an 8.5\u00d78.5km area from each generated city for evaluation. Table 2 presents the comparative results of the two models (ControlCity and Pix2PixHD) based on the evaluation met-rics, while Fig. 7 shows the building mosaics generated for some of the cities.\nAccording to the data in Table 2, ControlCity shows relatively low FID values in most cities, particularly in Manchester (57.85) and Chicago (42.67). This may be due to the similar urban morphology between London and Manch-ester, and Seattle and Chicago. In contrast, Pix2PixHD has significantly higher FID values across all cities, especially in Manchester (300.96) and Paris (377.08), consistent with the results from Experiment 1, indicating that Pix2PixHD struggles to learn London's complex urban morphology. In terms of ASite Cover, ControlCity shows slight reduc-tions in site coverage in some cities, e.g., Paris (-13.12%) and San Francisco (-5.72%), whereas Pix2PixHD displays more pronounced changes, e.g., Paris (-38.51%) and Manch-ester (-7.05%), highlighting Pix2PixHD's limitations in con-trolling site coverage changes. ControlCity's % GN Count fluctuates across different cities but remains relatively sta-ble overall, performing well in Manchester (133.22) and Sumatra (92.81). In contrast, Pix2PixHD's % GN Count is lower in most cities, particularly in Manchester (23.94) and Paris (34.29), indicating its instability in generating polygon counts. ControlCity has higher MIoU values in most cities, with Paris having the highest MIoU (0.50), indicating better accuracy in building footprint prediction, while Pix2PixHD shows generally lower MIoU values, reflecting its weakness in spatial layout prediction.\nCombining the tabular data with the visual results from Fig. 7, we observe that ControlCity outperforms the Pix2PixHD model in terms of image generation quality, prediction accuracy, site coverage, and % GN Count. As shown in the figure, ControlCity is capable of simulating realistic urban patterns in previously unseen city areas. For morpholog-ically similar cities, the shape transfer allows the model to generate building density and texture that more closely resemble real conditions. For example, the generated results from Jakarta to Sumatra demonstrate consistency in building density. In city pairs with larger morphological differences, the model transfers the source city's morphology to the target city. For example, the London-to-Paris results show that the model transferred London's courtyard style to Paris' grid-like urban layout, rather than directly recreating Paris' actual morphology.\nIn this experiment, we found that ControlCity is more adept than Pix2PixHD at transferring learned morphology knowledge from one city to another, while maintaining stable performance. This suggests that, in practical applica-tions, designers may no longer need to rely on traditional urban modeling methods, which involve manually creating rules and constraints to generate city layouts. Assuming that the growth of new urban areas follows existing patterns, the model can simulate future city expansions and help designers quickly generate specific urban morphologies in a given area for initial concept designs."}, {"title": "4.3. Experiment 3 - Zero-shot evaluation in unknown regions", "content": "In Experiment 2, we explored the feasibility of transfer-ring urban morphology to other regions, aiming to evaluate how well building morphology features learned from one city can be transferred to other cities or regions. The goal of this experiment was to verify whether the model can generate building morphologies for unknown cities in a zero-shot scenario. Specifically, we fine-tuned a pre-trained mul-timodal text-to-image generation model using aligned data from multiple cities to learn their building morphologies. Based on this hypothesis, after training on multiple cities, the model can extract and aggregate building layout features from various cities, forming a set of general urban patterns that remain effective when predicting building morphologies for unseen cities.\nTo test this hypothesis, we selected four global city regions with different urban morphologies for evaluation. Specifically, we replaced the city names in the prompts with the target city names and used the image conditions and metadata of the target regions. Our objective was to evaluate whether the model could successfully integrate knowledge from multiple cities to generate building layouts that match the reality of the target city. Through joint training on data from multiple cities, we expected the model to not only rec-ognize the uniqueness of each city but also extract common features across cities, ultimately building a widely adaptable urban morphology prediction model. Table 3 presents the performance results of the model across different cities, while Fig. 8 provides generation examples for the four cities.\nAs seen in the results from Table 3, the model's perfor-mance varies across different cities, reflecting its ability to predict building layouts in different urban areas. For exam-ple, Munich has the lowest FID score (41.37), indicating that the model's generated urban morphology for Munich has the smallest discrepancy from reality, demonstrating good gen-eralization capability. While Amsterdam ranks third in terms of FID, it has the highest MIoU value (0.42), indicating that the generated building layout in this city has the highest overlap with reality. This suggests that, after joint training on multiple cities, the model can provide highly accurate building layout predictions in certain cities.\nAdditionally, the performance of 4 Site Cover remained relatively stable, with the worst result in Amsterdam (4 Site Cover -8.6), indicating that the difference between the generated building area and the real situation was small. Regarding the % GN Count metric, Berlin showed an outlier with a value of 249.49, far exceeding other cities. This means that in the case of Berlin, the model generated significantly more buildings than the actual"}]}