{"title": "Privacy-preserving Universal Adversarial Defense for Black-box Models", "authors": ["Qiao Li", "Cong Wu", "Jing Chen", "Zijun Zhang", "Kun He", "Ruiying Du", "Xinxin Wang", "Qingchuang Zhao", "Yang Liu"], "abstract": "Deep neural networks (DNNs) are increasingly used in critical applications such as identity authentication and au- tonomous driving, where robustness against adversarial attacks is crucial. These attacks can exploit minor perturbations to cause significant prediction errors, making it essential to enhance the resilience of DNNs. Traditional defense methods often rely on access to detailed model information, which raises privacy concerns, as model owners may be reluctant to share such data. In contrast, existing black-box defense methods fail to offer a universal defense against various types of adversarial attacks. To address these challenges, we introduce DUCD, a universal black-box defense method that does not require access to the target model's parameters or architecture. Our approach involves distilling the target model by querying it with data, creating a white-box surrogate while preserving data privacy. We further enhance this surrogate model using a certified defense based on randomized smoothing and optimized noise selection, enabling robust defense against a broad range of adversarial attacks. Comparative evaluations between the certified defenses of the surrogate and target models demonstrate the effectiveness of our approach. Experiments on multiple image classification datasets show that DUCD not only outperforms existing black- box defenses but also matches the accuracy of white-box defenses, all while enhancing data privacy and reducing the success rate of membership inference attacks.", "sections": [{"title": "I. INTRODUCTION", "content": "DEEP neural networks (DNNs) are widely applied across various fields, including large language models [1], [2], and federated learning [3]\u2013[5]. However DNNs have demon- strated vulnerability to adversarial examples, which are inputs with subtle perturbations designed to cause incorrect predic- tions [6]. This vulnerability poses significant risks in safety- critical applications such as biological identification [7]\u2013[10] and network traffic [11]\u2013[15], where adversarial examples can lead to severe consequences, including identity theft and traffic accidents. To mitigate these risks, various defense methods have been proposed, broadly categorized into empirical and certified defenses [16]. Empirical defenses, such as adver- sarial detection [17] and adversarial training [18], have been effective in enhancing DNN robustness; however, they remain susceptible to adaptive attacks and lack provable robustness guarantees. In contrast, certified defenses offer a provable guarantee of robustness by ensuring that, within a certified lp radius (e.g., l1, l2, or l\u221e), adversarial attacks cannot successfully perturb the model's predictions [19]. Existing method and research gap. Although certified defense methods have demonstrated notable effectiveness [20], the majority of them require access to white-box models. However, such prior knowledge is not always available to defenders, particularly third-party entities, due to privacy and security concerns from model owners. Implementing these de- fenses typically demands substantial resources, including high- performance GPUs and expert knowledge, making them less feasible in practice. Consequently, recent studies have focused on providing provable robustness in black-box settings, where defenders must offer a robustness-certified model based on limited information from model owners. Existing methods, such as those proposed by Teng et al. [21] and Salman et al. [22], employ Gaussian noise for denoised smoothing based on surrogate models. However, these approaches fall short in practical black-box defense scenarios, as they assume that the target model adheres to a specific structure. To address this gap, Zhang et al. [16] utilize query-based techniques and zero-order optimization to estimate information for denoised smoothing. Unfortunately, these methods only achieve certified defenses under l2-norm constraints, making them vulnerable to lo-norm attacks, which are prevalent in real-world scenar- ios. Moreover, they offer a restricted certified radius, further limiting their practical effectiveness. This highlights the need for more general and practical black-box certified defenses that can overcome these limitations. DUCD. In this paper, we present a practical and ef- fective universal defense method designed to address two critical properties: (i) Model-agnosticism, which ensures that the method can be applied to any target model without requiring prior knowledge, and (ii) Norm-universality, which provides provable robustness against attacks across different lp-norms. A key requirement for such a defense method is to achieve the highest possible certified radius to enhance its practical effectiveness. However, existing black-box defenses have struggled to simultaneously meet the criteria of model- agnosticism and norm-universality. To bridge this gap, we propose a novel universal defense approach that maximizes the certified radius while maintaining model privacy in black-box settings. Our method requires only query access to the target"}, {"title": "II. PRELIMINARIES", "content": "A. Randomized Smoothing\nRandomized smoothing introduces random noise or per- turbations to input samples, ensuring that minor changes in inputs do not alter the predictions of the DNNs. By randomly sampling input space and averaging the results, smoothed predictions are obtained. This averaging minimizes the impact"}, {"title": "B. Robustness Guarantee", "content": "Let PA denote the probability that the target classifier f outputs the most probable class ca, and pe denote that of the second most probable class CB. The certified radius is the minimum lp norm of a perturbations, \u03b4, that satisfies the following robustness boundary conditions:\n$\\begin{array}{l}\\mu_{a}(x-\\delta) \\\\ \\mu_{a}(x)\\end{array} \\geq t_{a}=P_{A},$\n$\\begin{array}{l}\\mu_{a}(x-\\delta) \\\\ \\mu_{a}(x)\\end{array} \\leq t_{B}=P_{B},$\n$P(\\begin{array}{l}\\mu_{a}(x) \\\\ \\mu_{a}(x+\\delta)\\end{array} \\geq t_{a})=P(\\begin{array}{l}\\mu_{a}(x) \\\\ \\mu_{a}(x+\\delta)\\end{array} \\leq t_{a})$,\n$t_{a}=\\Phi^{-1}(p_{a}), t_{B}=\\Phi^{-1}(p_{B}).$\nHere, tA and tB are auxiliary parameters that satisfy Eq. (2). \u0424-1 is the inverse function of $\\mu_{a}(x - \\delta)/\\mu_{x}(x)$. The certified radius of the smoothed classifier g in the lp norm is:\nR=\\frac{\\sigma}{2}(t_{a}-t_{B}).$\nTherefore, for any $||\\delta||_{p} < R$, we have $g(x + \\delta) = c_{a}$. This means that for any possible target classifier f and any possible x, the output of the smoothed classifier g will not exceed P(CA = g(x)) \u00b1 R.\nAs a result, when the noise level o is high or the probability value of the maximum class CA is large, or the probability values of other classes are low, the robustness radius R will increase. When the robustness radius R \u2192 \u221e, we have pa \u2192 1 and p\u0432 \u2192 0. Since the Gaussian distribution exists in the entire input space Rd, there are cases with Pa = 1 where f(x + c) = CA."}, {"title": "C. Threat Model", "content": "We outline the threat model for adversarial attacks, covering the attacker's goals and knowledge of the target model. We consider a malicious third party as an attacker. The attacker acquires relevant prior knowledge of the model and crafts an adversarial example using this knowledge. The attacker selects a sample for manipulation, initiating subtle modifications, aiming to misclassify it. We next delineate the attacker's goals and knowledge.\nThe attacker's main goal is to generate adversarial examples using prior knowledge of the model, guiding the target model to make incorrect predictions and thus degrading its perfor- mance. More specifically, the attacker's goal can be defined as the perturbation goal and the target effectiveness goal as follows:\nGoal - i: perturbation goal. This goal implies that the perturbations added to the sample should be hard to detect. It is intended to minimize the added perturbations.\nGoal - ii: target effectiveness goal. This goal pertains to the model's misclassification behavior concerning the adversarial example during testing, aiming to ensure successful misclassification of the target model.\nWe consider a white-box attack where the attacker pos- sesses specific knowledge. Firstly, the attacker can choose the samples to target. Secondly, the attacker has prior knowledge of the target model, including its structure, parameters, and training data. Additionally, the attacker is capable of adaptive attacks. This means that the attacker, upon discovering the implementation of a defense by the target model, can modify their strategy and launch a new attack against the updated defense. It's important to note that this adaptive attack remains a white-box attack."}, {"title": "III. METHODOLOGY", "content": "In this section, we present overview of methodogy and detail each step.\nA. Overview\nIn this section, we present a Distillation-based Universal Black-box model Defense (DUCD). The key idea is to employ queries to generate a surrogate model of the target model, thereby converting the black-box scenario into a white-box one. Leveraging this surrogate model, we propose a certified defense strategy aimed at developing a universal defense.\nLet f(x) be a pre-trained black-box prediction model that maps an input x to a predicted output. The black-box scenario studied in this paper is one where the owner of the model is unwilling to share model parameters and training data. Therefore, the only interaction with the black box is to submit an input and receive the corresponding prediction. Formally, given a black-box base model f, the goal is to devise a certified classifier D exclusively using input-output queries Q(f), where the Q(f) operation provides the output label or logits of input x for f, and derive a robust certified model D(x) resilient to adversarial perturbations."}, {"title": "B. Distillation-based Surrogate Model Generation", "content": "The goal of the surrogate model A is to accurately fit the predictions of the target black-box model T on its input space Dr. Specifically, we aim to find the optimal parameter \u03b8\u03b1 for the surrogate model A such that the error between A(x) and T(x) is minimized for all x \u2208 Dr. Then for all x \u2208 Db:\n$\\arg \\min _{\\theta_{A}} P_{x \\sim D_{b}}[\\arg \\max T(x) \\neq \\arg \\max A(x)].$\nIn order to achieve distillation-based surrogate model gen- eration, DUCD leverages the idea of knowledge distillation, transferring knowledge from the target model to a white-box surrogate model [24]. In DUCD, the target model serves as a teacher, from which the surrogate model learns as a student. As illustrated in Fig. 1, the logits generated by the teacher network are utilized as the training targets for the student network's outputs on the training dataset.\nTo address the issue of gradient vanishing during training, we utilize the l\u2081 norm loss as our loss function. Despite the non-differentiability of the l\u2081 norm loss, empirical studies have demonstrated its resilience to gradient vanishing and its ability to yield improved results [25]. The loss function is defined as follows:\n$Loss = \\sum_{i=1}^{C}|t_{i} - a_{i}|, i \\in 1, ..., C.$\nwhere ti and a\u017c are the logits belonging to the black box model and the surrogate model, and C is the number of training dataset categories.\nThe surrogate model is iteratively trained on the dataset, using the predicted output of the black-box model as the target. This iterative approach aims to align the predictions of the surrogate model with those of the black-box model. Training termination is controlled based on the query budget constraints. After training, the surrogate model effectively mimics the black-box model's predictions."}, {"title": "C. Universal Black-box Defense Using Surrogate Models", "content": "Universal defense necessitates both model-agnosticism and norm-universality. Additionally, it should yield a higher cer- tified radius for any lp perturbation. After obtaining the surrogate model through distillation, we apply randomized smoothing and noise selection to create a universal defense"}, {"title": "Algorithm 1 Scalar optimization", "content": "Input: Lower bound of probability pa; upper bound of probability PB; scalar value of perturbation \u5165; perturbation \u03b4; probability density function of the noise \u03bc\u03b1; Monte Carlo samples' number n; K's threshold Km; dichoto- mous search iterations number N.\nOutput The scalar A that minimizes K.\nInitial scalars \u03bb\u03b1 and A such that K > 0 and K < 0.\n\u03bb = (\u03bb\u03b1 + \u03bb\u03b9)/2.\nwhile N > 0 and |K| > Km do\nif K > 0 then\n\u03bb\u03b1 = \u03bb.\nelse\n\u03bb\u03b9 = \u03bb\n\u03bb = (\u03bb\u03b1 + \u03bb\u03b5)/2.\nCompute K with \u03bb.\nN=N-1.\nreturn \u03bb.\nThe first stage utilizes scalar optimization to find a scaling factor A that adjusts the perturbation 8 to the robust boundary. Let |K| represent the distance between the perturbation & and the robustness boundary. In this paper, we use a binary search method to find the scaling factor that minimizes K. When K = 0, the perturbation & lies exactly on the robustness boundary. By fixing the direction of 8, it is necessary to find two scalars such that K > 0 and K < 0. First, we compute K based on the scalar da. If K > 0, then the scaled perturbation \u03bb\u03b1\u03b4 lies within the robustness boundary. By adjusting the scalar, we can find A such that K < 0, and vice versa. In the proposed approach, K is iteratively computed with \u03bb = (\u03bb\u03b1 + \u03bb\u03cc). If K > 0, then let a = \\; otherwise, let \u03bb = \u03bb. This iterative process is repeated until K is less than a threshold or the number of iterations reaches a specified limit. The steps are detailed in Algorithm 1."}, {"title": "Algorithm 2 The computation of ta and to", "content": "Input: Lower bound of probability Pa; upper bound of probability PB; scalar value of perturbation \u5165; perturbation \u03b4; probability density function of the noise \u03bc\u03b1; Monte Carlo samples' number n.\nOutput: Auxiliary parameters ta and tB.\nSample n noise values, e \u2208 Rn\u00d7d, from the discrete probability density function.\nUse these n noise samples, \u03bc\u03b1, \u03bb and \u03b4 to compute \u03bc\u03b1 (x-\n\u03bb\u03b4).\nUse the Monte Carlo method to estimate the cumulative distribution function \u03a6 for \u03bc\u03b1 (x \u2013 \u03bb\u03b4).\nreturn $t_{a}=\\Phi^{-1}(p_{a}) ; t_{B}=\\Phi^{-1}(p_{B})$"}, {"title": "Algorithm 3 Certified classifier", "content": "Input: Train set X; label set C; abstain threshold ; base classifier f; Monte Carlo sampling number n; certification threshold\nOutput: Certified classifier g\nTRAIN\nfor Xi, Ci in X, C do\nCalculate the class distribution f(x\u2081 + \u20ac) based on the labels ci of Xi\nSample n noise samples to approximate the class distribution f(xi + \u20ac)\nCount the number of occurrences of each class in the sample counts\nfor each c do\nif c\u2260 ca and counts[c] > counts[CA] * ( then\nUpdate g\nreturn g\nPREDICT\nSample n noise samples to approximate a class distribu- tion f(x + \u20ac)\nSample a example xi from the class distribution f (xi + \u20ac)\nfor each c do\nif c\u2260 ca and counts[c] > counts[CA] * ( then\nreturn abstain\nreturn CA\nCERTIFICATION\nSample n noise samples to approximate the class distri- bution f(xi + \u20ac)\nCount the number of occurrences of each class in the sample counts\nTake the class with the most occurrences in counts CA\nfor each c do\nif c\u2260 ca and counts[c] > counts[CA] * ( then\nreturn abstain\nCalculate the frequency of class CA counts[CA]/n\nif counts[CA]/n > 1 then\nreturn CA\nelse\nreturn abstain\nCertified classifier based on randomized smoothing. The certified defense based on randomized smoothing utilizes noise selection during training to approximate the input's category distribution. The prediction result is determined by selecting the most frequent category. If the occurrences of other categories exceed a predefined threshold, the classifier abstains from making a prediction. A randomized smoothing-based classifier for certified defense employs noise sampling in training to estimate the class distribution in the samples, ultimately predicting the class with the highest occurrence. If the occurrence count of other classes surpasses the predefined threshold, the abstain result is returned. The base classifier generates n samples by applying n different noise perturbations to input x. The prediction will be CA if it appears more frequently than any other class. The"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we provides a comprehensive analysis of our method's performance.\nA. Experimental Setup\nDatasets. We evaluate our proposed DUCD method on three benchmark datasets: MNIST [27], SVHN [28], and CIFAR10 [29] Furthermore, we split the CIFAR10 training set into two halves (dubbed CIFAR-S): one half is used to train the target model, while the other half is used for distillation. CIFAR-S is designed for scenarios where the model owner is unwilling to disclose the original dataset, but an independent and identically distributed dataset is available. In addition, for scenarios where only a similar dataset is available, we employ CIFAR10.1 [30] for distillation, which is similar to CIFAR10 but with a slightly different distribution.\nMetrics. The evaluation of the surrogate model involves assessing its absolute accuracy and relative accuracy. The absolute accuracy refers to the test accuracy of the surrogate model, while the relative accuracy represents the ratio of the test accuracy of the surrogate model to that of the target model. Additionally, we employ the 'approximate certified test set accuracy', commonly used in prior studies [19], [26], [31], [32], to compute the certification radius for lp-norm perturbations on input samples. Certified accuracy is then measured as the percentage of correctly classified data for which the certified radius exceeds a specified threshold R. Following [33], we evaluate the overall certified robustness by plotting the correlation curve between the certified accuracy and certified radius for noise sampled with varying \u03c3, and computing the area under the curve:\n$score = \\int_{0}^{+\\infty} max_{\\sigma}(Acc_{\\sigma}(R)) dR, \\sigma\\in \\Sigma.$\nEnvironments. All experiments are conducted on Ubuntu 20.04 with an Intel(R) Xeon(R) Gold 6133 CPU @ 2.50GHz and an NVIDIA GeForce RTX 4090 GPU. The experiments are implemented in Python 3.8 with PyTorch 1.12.0, CUDA 12.0, and the adversarial-robustness-toolbox [34].\nB. Surrogate Model Evaluation\nRecent studies in knowledge distillation demonstrate that a relatively small student model can often perform on par with a larger teacher model [25], [35]. Therefore, we employ ResNet-18 as the surrogate model architecture in the following experiments.\nFirstly, we generate surrogate models for Resnet-34 classi- fiers under various query budgets. Fig. 3 shows the accuracies"}, {"title": "C. Universarlity Evaluation", "content": "Noise selection. We optimize the hyperparameters of the noise PDF using a grid search. In the experiment, we use a noise distribution that follows x e-x/a|\u03b2, where we optimize \u03b2 and set a such that \u03c3 = 1. This choice of \u03c3 = 1 helps achieve a larger certified radius and good performance across different certified radii, as shown in previous work [19]. The grid search is conducted on the SVHN dataset, where a model is trained and evaluated for each round of search. For each pair of parameters a and \u03b2, we train a ResNet-18 model with randomized smoothing. Finally, as an approximation to Eq. (8), we compute the robust score on a set of \u03c3 = [0.25, 0.50, 0.75, 1.00] to evaluate the overall performance. To obtain the certified radii, we use Monte Carlo sampling of size 1000.\nAs indicated by the results in Table III, the optimal \u1e9e for l\u2081 norm is 0.75, while that for 12 and l\u221e norms is 2.75. This suggests that the commonly used Laplacian noise with \u03b2 = 1 and Gaussian noise with \u03b2 = 2 are not optimal for certified defense across 11, 12, and lo\u221e norms. A smaller \u1e9e strikes a better balance between the certified radius and certified accuracy. The results show that the choice of noise distributions is crucial for the performance of the certified defense.\nCertified radius. In theory, randomized smoothing provides certified robustness against input perturbations under any lp- norm constraints. Consequently, our experimental evaluation focuses on assessing the certified radii under different noise probability density functions (PDFs) for p = 1, p = 2, and p = \u221e. To identify the optimal PDF for each lp perturbation, we compute the certified radii for various PDFs, all with the same variance, across a range of PA values within the interval (0.5, 1.0]."}, {"title": "D. Defense Performance", "content": "We evaluate DUCD's defense performance against existing methods, including a black-box defense ZO-AE-DS [16]. Note"}, {"title": "E. Adaptive Attacks", "content": "We empirically evaluate the defense performance of differ- ent defense methods on CIFAR10 against popular adversarial attacks, including AutoPGD [37], ACG [38], HSJA [39], and"}, {"title": "F. Purification", "content": "We introduce a purification experiment that employs a ran- domized smoothing certification process. This process filters out uncertified inputs, ensuring that only certified samples are forwarded to the classifier. As demonstrated in the study by [26], certified samples exhibit greater robustness, making them more resilient to minor noise. Through the randomized smoothing certification process, the target classifier is more likely to accurately classify certified samples.\nWe conduct experiments using the CIFAR10 dataset with a fixed number of 1000 Monte Carlo samples. The purification success rate is defined as the percentage of certified samples out of all inputs. We generated plots for various noise inten-"}, {"title": "G. Privacy Evaluation", "content": "Finally, we conduct a membership inference attack [41] to evaluate the privacy protection performance of the proposed surrogate model generation method. The goal of a membership inference attack is to determine whether a specific data point is in the training dataset of the target model [42], which can violate privacy guarantees, especially in scenarios where the training dataset contains sensitive or personal information."}, {"title": "V. RELATED WORK", "content": "In this section, we brief existing works on empirical defense and certified defense.\nEmpirical defense. Empirical defense is obtained by re- searchers through experimentation based on existing attacks. In practice, they exhibit good defensive performance against specific adversarial attacks. Empirical defense includes adver- sarial detection and adversarial training. Adversarial detection methods [44]\u2013[46] aim to identify and discard adversarial examples. While they ensure the robustness of model predic- tions, they don't enhance the model's intrinsic defense against attacks. Adversarial training, on the other hand, improves DNNs' robustness by training them on datasets comprising both adversarial and normal examples. Pioneering research by Madry et al. [47] theoretically framed this approach, leading to significant strides in empirical defense. Subsequent methods have varied in focus: some have optimized input [48], others have refined training strategies [49]\u2013[53], and others still have explored the mechanisms underpinning adversarial training [54]. Although the methods mentioned above improve the robustness of the target model, they are designed based on existing attacks and therefore vulnerable to re-attacks specifi- cally designed to bypass them.\nCertified defense. Certified defense is designed to achieve provable security for models, ensuring reliable robustness for machine learning classifiers against adversarial perturbations. A range of certified defense methods have been proposed. Lecuyer et al. [55] first introduced a certified defense that used differential privacy to offer robust proofs for classifiers. Cohen et al. [19] later established a certified defense based"}, {"title": "VI. CONCLUSION", "content": "This paper focuses on enhancing the robustness of pre- trained black-box models, aiming to reduce the strictness of defense assumptions and explore how to provide a universal defense mechanism that protects privacy and defends against all norm attacks through query operations alone. To achieve this goal, we propose an innovative method that does not rely on the internal details of the model. Instead, it uses query results to construct a universal defense system capable of resisting various adversarial attacks. Specifically, we design a knowledge distillation-based surrogate model generation technique to create a proxy model functionally similar to the target model. Combined with optimized randomized smooth- ing, this proxy model generates a robust certified classifier"}, {"title": "VII. CONCLUSION", "content": "with a larger certified radius. This classifier effectively defends against adversarial attacks of any norm and maintains privacy protection.\nAlthough our proposed method achieves the effectiveness of white-box certified defenses, it performs poorly against loo attacks. Even with rigorous theoretical derivations and proofs, defenses based on randomized smoothing is theoretically ca- pable of defending against attacks of all norms, including loo. However, our experiments demonstrate that in the face of real adversarial attacks, randomized smoothing fails to effectively defend against lo norm attacks. This remains a critical issue for future exploration and resolution in certified defenses."}]}