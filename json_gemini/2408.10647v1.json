{"title": "Privacy-preserving Universal Adversarial Defense for Black-box Models", "authors": ["Qiao Li", "Cong Wu", "Jing Chen", "Zijun Zhang", "Kun He", "Ruiying Du", "Xinxin Wang", "Qingchuang Zhao", "Yang Liu"], "abstract": "Deep neural networks (DNNs) are increasingly used in critical applications such as identity authentication and au- tonomous driving, where robustness against adversarial attacks is crucial. These attacks can exploit minor perturbations to cause significant prediction errors, making it essential to enhance the resilience of DNNs. Traditional defense methods often rely on access to detailed model information, which raises privacy concerns, as model owners may be reluctant to share such data. In contrast, existing black-box defense methods fail to offer a universal defense against various types of adversarial attacks. To address these challenges, we introduce DUCD, a universal black-box defense method that does not require access to the target model's parameters or architecture. Our approach involves distilling the target model by querying it with data, creating a white-box surrogate while preserving data privacy. We further enhance this surrogate model using a certified defense based on randomized smoothing and optimized noise selection, enabling robust defense against a broad range of adversarial attacks. Comparative evaluations between the certified defenses of the surrogate and target models demonstrate the effectiveness of our approach. Experiments on multiple image classification datasets show that DUCD not only outperforms existing black- box defenses but also matches the accuracy of white-box defenses, all while enhancing data privacy and reducing the success rate of membership inference attacks.", "sections": [{"title": "I. INTRODUCTION", "content": "DEEP neural networks (DNNs) are widely applied across various fields, including large language models [1], [2], and federated learning [3]\u2013[5]. However DNNs have demon- strated vulnerability to adversarial examples, which are inputs with subtle perturbations designed to cause incorrect predic- tions [6]. This vulnerability poses significant risks in safety- critical applications such as biological identification [7]\u2013[10] and network traffic [11]\u2013[15], where adversarial examples can lead to severe consequences, including identity theft and traffic accidents. To mitigate these risks, various defense methods have been proposed, broadly categorized into empirical and certified defenses [16]. Empirical defenses, such as adver- sarial detection [17] and adversarial training [18], have been effective in enhancing DNN robustness; however, they remain susceptible to adaptive attacks and lack provable robustness guarantees. In contrast, certified defenses offer a provable guarantee of robustness by ensuring that, within a certified lp radius (e.g., l1, l2, or l\u221e), adversarial attacks cannot successfully perturb the model's predictions [19]. Existing method and research gap. Although certified defense methods have demonstrated notable effectiveness [20], the majority of them require access to white-box models. However, such prior knowledge is not always available to defenders, particularly third-party entities, due to privacy and security concerns from model owners. Implementing these de- fenses typically demands substantial resources, including high- performance GPUs and expert knowledge, making them less feasible in practice. Consequently, recent studies have focused on providing provable robustness in black-box settings, where defenders must offer a robustness-certified model based on limited information from model owners. Existing methods, such as those proposed by Teng et al. [21] and Salman et al. [22], employ Gaussian noise for denoised smoothing based on surrogate models. However, these approaches fall short in practical black-box defense scenarios, as they assume that the target model adheres to a specific structure. To address this gap, Zhang et al. [16] utilize query-based techniques and zero-order optimization to estimate information for denoised smoothing. Unfortunately, these methods only achieve certified defenses under l2-norm constraints, making them vulnerable to l\u221e-norm attacks, which are prevalent in real-world scenar- ios. Moreover, they offer a restricted certified radius, further limiting their practical effectiveness. This highlights the need for more general and practical black-box certified defenses that can overcome these limitations. DUCD. In this paper, we present a practical and ef- fective universal defense method designed to address two critical properties: (i) Model-agnosticism, which ensures that the method can be applied to any target model without requiring prior knowledge, and (ii) Norm-universality, which provides provable robustness against attacks across different lp-norms. A key requirement for such a defense method is to achieve the highest possible certified radius to enhance its practical effectiveness. However, existing black-box defenses have struggled to simultaneously meet the criteria of model- agnosticism and norm-universality. To bridge this gap, we propose a novel universal defense approach that maximizes the certified radius while maintaining model privacy in black-box settings. Our method requires only query access to the target"}, {"title": "II. PRELIMINARIES", "content": "A. Randomized Smoothing\nRandomized smoothing introduces random noise or per- turbations to input samples, ensuring that minor changes in inputs do not alter the predictions of the DNNs. By randomly sampling input space and averaging the results, smoothed predictions are obtained. This averaging minimizes the impact of noise and perturbations on DNN predictions, enhancing their robustness. The essence of randomized smoothing is to construct a smoothed classifier from the target classifier. Given a target classifier \\(f(x) : R^d \\rightarrow Y\\), where \\(R^d\\) represents the input space and \\(Y\\) indicates the output space, a randomly smoothed classifier, \\(g\\), is defined as:\n\\begin{equation}\ng(x) = \\underset{c \\in Y}{\\arg \\max} P(f(x+\\epsilon)), \\epsilon \\sim N(0, \\sigma^2I).\n\\end{equation}\nFor a given input \\(x\\), the smoothed classifier \\(g\\) outputs the class probabilities that the target classifier \\(f\\) is most likely to predict in the presence of random noise \\(\\epsilon \\sim N(0, \\sigma^2I)\\). The target classifier \\(f\\) outputs the most likely category \\(c_A\\) for input \\(x\\) with probability \\(p_A\\), and outputs the next most likely category with probability \\(p_B\\). In the case of a classification task, for \\(\\epsilon \\sim N(0, \\sigma^2I)\\), assuming \\(c_A \\in Y\\) and \\(p_A, p_B \\in [0, 1]\\), we have:\n\\begin{equation}\nP(f(x + \\epsilon) = c_A) \\geq p_A \\geq p_B \\geq \\underset{c / c_A}{\\max} P(f(x + \\epsilon) = c).\n\\end{equation}\nReplacing \\(p_A\\) with a lower bound \\(\\underline{p_A}\\) and \\(p_B\\) with an upper bound \\(\\overline{p_B}\\) in the above equations still holds.\nB. Robustness Guarantee\nLet \\(p_A\\) denote the probability that the target classifier \\(f\\) outputs the most probable class \\(c_A\\), and \\(p_B\\) denote that of the second most probable class \\(c_B\\). The certified radius is the minimum \\(l_p\\) norm of a perturbations, \\(\\delta\\), that satisfies the following robustness boundary conditions [19]:\n\\begin{equation}\n\\begin{aligned}\n& P \\left(\\frac{\\mu_{\\alpha}(x-\\delta)}{\\mu_{\\alpha}(x)} \\leq t_A \\right) \\geq \\underline{p_A}, \\quad\n& P \\left(\\frac{\\mu_{\\alpha}(x-\\delta)}{\\mu_{\\alpha}(x)} \\geq t_B \\right) \\geq \\overline{p_B},\n\\quad\n& P \\left(\\frac{\\mu_{\\alpha}(x)}{\\mu_{\\alpha}(x+\\delta)} \\leq t_A \\right) \\geq p_A,\n\\quad P \\left(\\frac{\\mu_{\\alpha}(x)}{\\mu_{\\alpha}(x+\\delta)} \\geq t_B \\right) \\geq p_B,\n\\quad t_A = \\Phi^{-1}(p_A), t_B = \\Phi^{-1}(p_B).\n\\end{aligned}\n\\end{equation}\nHere, \\(t_A\\) and \\(t_B\\) are auxiliary parameters that satisfy Eq. (2). \\(\\Phi^{-1}\\) is the inverse function of \\(\\mu_{\\alpha}(x - \\delta)/\\mu_x(x)\\). The certified radius of the smoothed classifier \\(g\\) in the \\(l_p\\) norm is:\n\\begin{equation}\nR = \\frac{\\alpha}{\\sigma}(t_A - t_B).\n\\end{equation}\nTherefore, for any \\(||\\delta||_p < R\\), we have \\(g(x + \\delta) = c_A\\). This means that for any possible target classifier \\(f\\) and any possible \\(x\\), the output of the smoothed classifier \\(g\\) will not exceed \\(P(c_A = g(x)) \\pm R\\).\nAs a result, when the noise level \\(\\sigma\\) is high or the probability value of the maximum class \\(c_A\\) is large, or the probability values of other classes are low, the robustness radius \\(R\\) will increase. When the robustness radius \\(R \\rightarrow \\infty\\), we have \\(p_A \\rightarrow 1\\) and \\(p_B \\rightarrow 0\\). Since the Gaussian distribution exists in the entire input space \\(R^d\\), there are cases with \\(P_A = 1\\) where \\(f(x + \\epsilon) = c_A\\)."}, {"title": "C. Threat Model", "content": "We outline the threat model for adversarial attacks, covering the attacker's goals and knowledge of the target model. We consider a malicious third party as an attacker. The attacker acquires relevant prior knowledge of the model and crafts an adversarial example using this knowledge. The attacker selects a sample for manipulation, initiating subtle modifications, aiming to misclassify it. We next delineate the attacker's goals and knowledge.\nThe attacker's main goal is to generate adversarial examples using prior knowledge of the model, guiding the target model to make incorrect predictions and thus degrading its perfor- mance. More specifically, the attacker's goal can be defined as the perturbation goal and the target effectiveness goal as follows:\n*   Goal - i: perturbation goal. This goal implies that the perturbations added to the sample should be hard to detect. It is intended to minimize the added perturbations.\n*   Goal - ii: target effectiveness goal. This goal pertains to the model's misclassification behavior concerning the adversarial example during testing, aiming to ensure successful misclassification of the target model.\nWe consider a white-box attack where the attacker pos- sesses specific knowledge. Firstly, the attacker can choose the samples to target. Secondly, the attacker has prior knowledge of the target model, including its structure, parameters, and training data. Additionally, the attacker is capable of adaptive attacks. This means that the attacker, upon discovering the implementation of a defense by the target model, can modify their strategy and launch a new attack against the updated defense. It's important to note that this adaptive attack remains a white-box attack."}, {"title": "III. METHODOLOGY", "content": "In this section, we present overview of methodogy and detail each step.\nA. Overview\nIn this section, we present a Distillation-based Universal Black-box model Defense (DUCD). The key idea is to employ queries to generate a surrogate model of the target model, thereby converting the black-box scenario into a white-box one. Leveraging this surrogate model, we propose a certified defense strategy aimed at developing a universal defense.\nLet \\(f(x)\\) be a pre-trained black-box prediction model that maps an input \\(x\\) to a predicted output. The black-box scenario studied in this paper is one where the owner of the model is unwilling to share model parameters and training data. Therefore, the only interaction with the black box is to submit an input and receive the corresponding prediction. Formally, given a black-box base model \\(f\\), the goal is to devise a certified classifier \\(D\\) exclusively using input-output queries \\(Q(f)\\), where the \\(Q(f)\\) operation provides the output label or logits of input \\(x\\) for \\(f\\), and derive a robust certified model \\(D(x)\\) resilient to adversarial perturbations.\nB. Distillation-based Surrogate Model Generation\nThe goal of the surrogate model \\(A\\) is to accurately fit the predictions of the target black-box model \\(T\\) on its input space \\(D_T\\). Specifically, we aim to find the optimal parameter \\(\\theta_A\\) for the surrogate model \\(A\\) such that the error between \\(A(x)\\) and \\(T(x)\\) is minimized for all \\(x \\in D_T\\). Then for all \\(x \\in D_b\\):\n\\begin{equation}\n\\underset{\\theta_A}{\\arg \\min} P_{x \\sim D_b} [\\arg \\max T(x) \\neq \\arg \\max A(x)].\n\\end{equation}\nIn order to achieve distillation-based surrogate model gen- eration, DUCD leverages the idea of knowledge distillation, transferring knowledge from the target model to a white-box surrogate model [24]. In DUCD, the target model serves as a teacher, from which the surrogate model learns as a student. As illustrated in Fig. 1, the logits generated by the teacher network are utilized as the training targets for the student network's outputs on the training dataset.\nTo address the issue of gradient vanishing during training, we utilize the \\(l_1\\) norm loss as our loss function. Despite the non-differentiability of the \\(l_1\\) norm loss, empirical studies have demonstrated its resilience to gradient vanishing and its ability to yield improved results [25]. The loss function is defined as follows:\n\\begin{equation}\nLoss = \\sum_{i=1}^{C} |t_i - a_i|, i \\in 1, ..., C.\n\\end{equation}\nwhere \\(t_i\\) and \\(a_i\\) are the logits belonging to the black box model and the surrogate model, and \\(C\\) is the number of training dataset categories.\nThe surrogate model is iteratively trained on the dataset, using the predicted output of the black-box model as the target. This iterative approach aims to align the predictions of the surrogate model with those of the black-box model. Training termination is controlled based on the query budget constraints. After training, the surrogate model effectively mimics the black-box model's predictions.\nC. Universal Black-box Defense Using Surrogate Models\nUniversal defense necessitates both model-agnosticism and norm-universality. Additionally, it should yield a higher cer- tified radius for any \\(l_p\\) perturbation. After obtaining the surrogate model through distillation, we apply randomized smoothing and noise selection to create a universal defense"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we provides a comprehensive analysis of our method's performance.\nA. Experimental Setup\nDatasets. We evaluate our proposed DUCD method on three benchmark datasets: MNIST [27], SVHN [28], and CIFAR10 [29] Furthermore, we split the CIFAR10 training set into two halves (dubbed CIFAR-S): one half is used to train the target model, while the other half is used for distillation. CIFAR-S is designed for scenarios where the model owner is unwilling to disclose the original dataset, but an independent and identically distributed dataset is available. In addition, for scenarios where only a similar dataset is available, we employ CIFAR10.1 [30] for distillation, which is similar to CIFAR10 but with a slightly different distribution.\nMetrics. The evaluation of the surrogate model involves assessing its absolute accuracy and relative accuracy. The absolute accuracy refers to the test accuracy of the surrogate model, while the relative accuracy represents the ratio of the test accuracy of the surrogate model to that of the target model. Additionally, we employ the 'approximate certified test set accuracy', commonly used in prior studies [19], [26], [31], [32], to compute the certification radius for \\(l_p\\)-norm perturbations on input samples. Certified accuracy is then measured as the percentage of correctly classified data for which the certified radius exceeds a specified threshold \\(R\\). Following [33], we evaluate the overall certified robustness by plotting the correlation curve between the certified accuracy and certified radius for noise sampled with varying \\(\\sigma\\), and computing the area under the curve:\n\\begin{equation}\nscore = \\int_{0}^{+\\infty} \\underset{\\sigma}{\\max} (Acc_{\\sigma} (R)) dR, \\sigma \\in \\Sigma.\n\\end{equation}\nEnvironments. All experiments are conducted on Ubuntu 20.04 with an Intel(R) Xeon(R) Gold 6133 CPU @ 2.50GHz and an NVIDIA GeForce RTX 4090 GPU. The experiments are implemented in Python 3.8 with PyTorch 1.12.0, CUDA 12.0, and the adversarial-robustness-toolbox [34].\nB. Surrogate Model Evaluation\nRecent studies in knowledge distillation demonstrate that a relatively small student model can often perform on par with a larger teacher model [25], [35]. Therefore, we employ ResNet-18 as the surrogate model architecture in the following experiments.\nFinally, we conduct a membership inference attack [41] to evaluate the privacy protection performance of the proposed surrogate model generation method. The goal of a membership inference attack is to determine whether a specific data point is in the training dataset of the target model [42], which can violate privacy guarantees, especially in scenarios where the training dataset contains sensitive or personal information."}, {"title": "V. RELATED WORK", "content": "In this section, we brief existing works on empirical defense and certified defense.\nEmpirical defense. Empirical defense is obtained by re- searchers through experimentation based on existing attacks. In practice, they exhibit good defensive performance against specific adversarial attacks. Empirical defense includes adver- sarial detection and adversarial training. Adversarial detection methods [44]\u2013[46] aim to identify and discard adversarial examples. While they ensure the robustness of model predic- tions, they don't enhance the model's intrinsic defense against attacks. Adversarial training, on the other hand, improves DNNs' robustness by training them on datasets comprising both adversarial and normal examples. Pioneering research by Madry et al. [47] theoretically framed this approach, leading to significant strides in empirical defense. Subsequent methods have varied in focus: some have optimized input [48], others have refined training strategies [49]\u2013[53], and others still have explored the mechanisms underpinning adversarial training [54]. Although the methods mentioned above improve the robustness of the target model, they are designed based on existing attacks and therefore vulnerable to re-attacks specifi- cally designed to bypass them.\nCertified defense. Certified defense is designed to achieve provable security for models, ensuring reliable robustness for machine learning classifiers against adversarial perturbations. A range of certified defense methods have been proposed. Lecuyer et al. [55] first introduced a certified defense that used differential privacy to offer robust proofs for classifiers. Cohen et al. [19] later established a certified defense based on randomized smoothing, utilizing Gaussian noise to ensure rigorous robustness guarantees for the \\(l_2\\) norm. Subsequent works, including [21], [23], [56], [57], tailored their ap- proaches to \\(l_p\\)-norm perturbations, but these methods are only robust for specific \\(l_p\\)-norm disturbances. Several recent studies [32], [58]\u2013[60] enhanced randomized smoothing to ensure model robustness within the \\(l_2\\) and \\(l_{\\infty}\\) norms, albeit at the cost of a somewhat reduced robust radius. While these methods successfully amplify model robustness and provide provable security, they are based on a white-box model assumption and rely heavily on prior knowledge of the model's architecture, parameters, and dataset. This dependence significantly ham- pers the practical application of the certified defense.\nIn addition to the above, researchers have developed sev- eral black-box certified defense methods. Salman et al. [22] pioneered an approach using a proxy model to approximate the black-box model, constructing the defense strategy around this surrogate. Carlini et al. [61] further enhanced this method with a denoising diffusion probability model, though its high training cost can be prohibitive for those with limited com- putational resources. However, these approaches [62] do not fully qualify as true black-box defenses, as they require prior knowledge of the target model's type and behavior. More recent studies [16], [63] propose certified defenses based solely on querying the target classifier. However, these methods are limited to defending against \\(l_2\\) norm perturbations, lacking norm-universality."}, {"title": "VI. CONCLUSION", "content": "This paper focuses on enhancing the robustness of pre- trained black-box models, aiming to reduce the strictness of defense assumptions and explore how to provide a universal defense mechanism that protects privacy and defends against all norm attacks through query operations alone. To achieve this goal, we propose an innovative method that does not rely on the internal details of the model. Instead, it uses query results to construct a universal defense system capable of resisting various adversarial attacks. Specifically, we design a knowledge distillation-based surrogate model generation technique to create a proxy model functionally similar to the target model. Combined with optimized randomized smooth- ing, this proxy model generates a robust certified classifier with a larger certified radius. This classifier effectively defends against adversarial attacks of any norm and maintains privacy protection.\nAlthough our proposed method achieves the effectiveness of white-box certified defenses, it performs poorly against \\(l_{\\infty}\\) attacks. Even with rigorous theoretical derivations and proofs, defenses based on randomized smoothing is theoretically ca- pable of defending against attacks of all norms, including \\(l_{\\infty}\\). However, our experiments demonstrate that in the face of real adversarial attacks, randomized smoothing fails to effectively defend against \\(l_{\\infty}\\) norm attacks. This remains a critical issue for future exploration and resolution in certified defenses."}]}