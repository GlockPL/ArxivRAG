{"title": "COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations", "authors": ["Vinicius G. Goecks", "Nicholas Waytowich"], "abstract": "The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAS. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information in both text and image formats and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, with added benefits of enhanced adaptability and alignment with commander intentions. COA-GPT's capability to rapidly adapt and update COAs during missions presents a transformative potential for military planning, particularly in addressing planning discrepancies and capitalizing on emergent windows of opportunities.", "sections": [{"title": "I. INTRODUCTION", "content": "The future battlefield presents an array of complex and dynamic challenges for Command and Control (C2) personnel, requiring rapid and informed decision-making across multi-faceted domains. As warfare continues to evolve, integrating and synchronizing diverse assets and effects across air, land, maritime, information, cyber, and space domains becomes increasingly critical. The C2 operations process, encompassing planning, preparation, execution, and continuous assessment, must adapt to these complexities while dealing with real-time data integration and operating under conditions of Denied, Degraded, Intermittent, and Limited (DDIL) communication [1], [2].\nIn this high-stakes environment, maintaining decision ad-vantage \u2013 the ability to make timely and effective decisions faster than adversaries \u2013 is paramount. The Military Decision Making Process (MDMP) [3], a cornerstone of C2 plan-ning [4], faces the pressing need to evolve with the accelerating pace and complexity of modern warfare [5]. This necessitates executing the MDMP within increasingly shorter timescales to exploit fleeting opportunities and respond adaptively to the dynamic conditions of the battlefield.\nThe development of Courses of Action (COAs), fundamen-tal to the decision-making process, exemplifies these chal-lenges. COA development has traditionally been a meticulous and time-consuming process, heavily dependent on the expe-rience and expertise of military personnel. However, the rapid evolution of modern warfare demands more efficient methods for developing and analyzing COAs.\nEnter Large Language Models (LLMs), a transformative technology in the field of natural language processing [6]\u2013[8]. LLMs have shown immense potential in various applications, including disaster response [9]\u2013[11] and robotics [12]\u2013[14], by processing extensive data to generate human-like text from given prompts. This research paper proposes COA-GPT, a framework that explores the application of LLMs to expe-dite COA development in military operations via in-context learning.\nCOA-GPT leverages LLMs to swiftly develop valid COAs, integrating military doctrine and domain expertise directly into the system's initial prompts. Commanders can input mission specifics and force descriptions \u2013 in both text and image formats, receiving multiple, strategically aligned COAs in a matter of seconds. This breakthrough significantly reduces COA development time, fostering a more agile decision-making process. It enables commanders to rapidly iterate and refine COAs, keeping pace with the ever-changing conditions of the battlefield and maintaining a strategic edge over adver-saries.\nOur main contributions are:\n\u2022\n\u2022\nCOA-GPT, a novel framework leveraging LLMs for ac-celerated development and analysis of COAs, integrating military doctrine and domain expertise effectively.\n\u2022 Empirical evidence showing that COA-GPT outperforms existing baselines in generating COAs, both in terms of speed and alignment with military strategic goals.\nDemonstration of COA-GPT's ability to improve COA development through human interaction, ensuring that generated COAs align closely with commanders' inten-tions and adapt dynamically to battlefield scenarios."}, {"title": "II. MILITARY RELEVANCE", "content": "The current C2 processes in military operations are predom-inantly linear and involve numerous sequential steps, which can be cumbersome and slow in the fast-paced environment of modern warfare [2], [3]. The advent of AI in military planning, particularly the COA-GPT system, presents an opportunity to radically streamline these processes. COA-GPT's ability to quickly generate, analyze, and refine multiple COAs can transform traditional C2 operations, combining and executing multiple planning steps concurrently to optimize efficiency.\nWith COA-GPT, the MDMP can be significantly enhanced. By enabling simultaneous development and analysis of COAs, COA-GPT facilitates quicker decision-making and more ef-fective control of operations. This leads to faster generation of actionable intelligence, thereby enabling commanders to make more informed decisions swiftly. The integration of AI-based tools like COA-GPT in C2 processes can reduce the physical footprint of military operations, diminishing the need for extensive personnel and logistics support, supporting the vision of distributed commander posts.\nThe proposed COA-GPT system has the potential to revolu-tionize C2 operations by enabling rapid generation, analysis, and comparison of multiple COA alternatives. This system can integrate with war gaming simulators and process data from battlefield sensors in real-time, offering commanders the ability to quickly adapt to the dynamic battlefield envi-ronment. This vision aligns with the ideas presented in [5], which emphasize the integration of COA analysis during the development phase for rapid optimization and comparison of multiple COAs. However, COA-GPT extends this concept by leveraging the combined strengths of human expertise and AI, fostering creativity, exploration, and informed decision-making across the C2 process.\nCOA-GPT is designed to present COAs to personnel in an intuitive manner, using language and annotated maps, facilitating the analysis of COA alternatives and allowing C2 personnel to adapt AI-generated COAs using their domain expertise and situational awareness, and ultimately making informed decisions on final COA selections.\nIn practical terms, C2 personnel will dynamically interact with COA-GPT to modify proposed COAs. They can specify objectives, input data in both text and image formats, issue corrections to fine-tune plans, and set constraints to avoid un-desirable actions. In response to unforeseen changes, feedback can be relayed to COA-GPT to adapt the plan to new risks and opportunities. Personnel can also adjust COA selection criteria to balance multiple objectives, such as prioritizing control over an enemy asset or minimizing danger to forces. This interactive process ensures that the final COA selected is aligned with the commander's strategic intent and situational requirements."}, {"title": "III. RELATED WORK", "content": "A. Planning with Large Language Models\nThe integration of Large Language Models (LLMs) in plan of action development is revolutionizing various sectors, including disaster management and military operations. In the realm of disaster response, the DisasterResponseGPT [15] algorithm stands out. This algorithm leverages the capabilities of LLMs to quickly generate viable action plans, integrating essential disaster response guidelines within the initial prompt. Similar to the proposed COA-GPT, DisasterResponseGPT facilitates the rapid generation of multiple plans from user-inputted scenarios, significantly reducing response times and enabling real-time plan adjustments based on ongoing feed-back. This methodology parallels the objectives of COA-GPT, however it hasn't been shown to incorporate COA analysis and spatial information during COA generation.\nSimilar to military operations, disaster response demands rapid, informed decision-making under severe time constraints and high-pressure conditions [16]\u2013[18]. Traditionally, action plan development in such contexts has been a laborious process, heavily reliant on the experience and expertise of the personnel involved. Given the stakes involved, where delays can result in loss of life, there is a critical need for more efficient and reliable plan development methodologies [16], [19]\u2013[21]. COA-GPT addresses similar challenges in military operations, offering a swift and adaptive approach to COA generation and analysis.\nThe recent studies by Ahn et al. (2022) [12] and Mees et al. (2023) [14] expand the scope of LLM applications to include robotic and visual affordances, illustrating how LLMS can be grounded in real-world contexts. These developments are crucial in bridging the gap between high-level seman-tic knowledge and practical, executable actions in dynamic environments. Likewise, COA-GPT seeks to apply LLMs in a military context, translating abstract strategic concepts and battlefield information into concrete, actionable military plans.\nContinuing to show the potential of LLMs for planning, the introduction of Voyager by Wang et al. (2023) [22], an LLM-powered embodied lifelong learning agent in Minecraft, repre-sents a groundbreaking step in autonomous, continual learning and skill acquisition in complex, ever-changing environments. Similarly, the STEVE-1 [23] model showcases the potential of LLMs in guiding agents to follow complex instructions in virtual settings, leveraging advancements in text-conditioned image generation and decision-making algorithms. The adapt-ability and learning capabilities demonstrated in these models are qualities that COA-GPT harness for generating and refining in real-time COAs for military scenarios.\nB. Al for Military Planning and Operations\nThe application of AI and LLMs in military planning and operations is a field of growing interest and significant potential. The First-Year Report of ARL Director's Strategic Initiative, focusing on Artificial Intelligence for Command and Control (C2) of Multi-Domain Operations [24], exemplifies this trend. The report discusses ongoing research into whether deep reinforcement learning (DRL) could support agile and adaptive C2 in multi-domain forces. This is particularly rel-evant for commanders and staff aiming to exploit rapidly evolving situations and fleeting windows of superiority on the battlefield. COA-GPT embodies this concept of agility and adaptability, leveraging LLMs to rapidly develop and modify COAs in response to evolving battlefield conditions. This approach effectively counters the inherent brittleness and extensive training demands typical of DRL methods.\nExploring the synergy between gaming platforms and mil-itary training, Goecks et al. (2023) provide insight into how AI algorithms, when combined with gaming and simulation technologies, can be adapted to replicate aspects of military missions [25]. Similarly, COA-GPT can be viewed as part of this trend, utilizing LLMs to enhance the strategic planning phase of military operations and gaming and simulation tech-nologies to evaluate them.\nIn a similar vein, Waytowich et al. (2022) [26] demonstrate the application of DRL in commanding multiple heteroge-neous actors in a simulated command and control task, mod-eled on a military scenario within StarCraft II. Their findings indicate that agents trained via an automatically generated curriculum can match or even surpass the performance of human experts and state-of-the-art DRL baselines. This ap-proach to training AI systems in complex scenarios aligns with the objectives of COA-GPT, which seeks to combine human expertise with Al efficiency in generating military COAs, and it is used here as the main baseline comparison for the proposed method, COA-GPT.\nIn addition to these developments, Schwartz (2020) [27] delves into the application of AI in the Army's MDMP, specifically in the COA Analysis phase. This research is par-ticularly relevant to COA-GPT, as it demonstrates how AI can assist commanders and their staff in quickly developing and optimizing multiple courses of action in response to the com-plexities of modern, hyper-contested battlefields. The study also highlights the increasing importance of Multi-Domain Operations (MDO) and the challenges presented by near-peer adversaries equipped with advanced Anti-Access Area Denial (A2AD) capabilities. The COA-GPT aligns with this vision by providing a tool to enhance the MDMP, particularly in the rapid generation and analysis of COAs.\nIn contrast to these AI-driven approaches, traditional mili-tary planning processes, as outlined in U.S. Army's ATP 5-0.2 [28], involve comprehensive guidelines for staff members in large-scale combat operations. While providing a consoli-dated source of key planning tools and techniques, these tra-ditional methodologies often lack the agility and adaptability offered by modern AI systems in rapidly changing combat environments. COA-GPT represents a paradigm shift from these traditional methods, bringing the speed and flexibility of LLMs to military planning processes."}, {"title": "IV. METHODS", "content": "In this research, we leveraged the in-context learning ca-pabilities of LLMs to create COA-GPT, a virtual assistant designed to efficiently generate COAs for military operations. COA-GPT is prompted to understand that it serves as a mil-itary commander's assistant to aid C2 personnel in developing COAs. It is aware that its inputs will include mission objec-tives, terrain information, and details on friendly and threat forces as provided by the commander in text and/or image format. Furthermore, COA-GPT has access to a condensed version of military doctrine, covering forms of maneuver (en-velopment, flank attack, frontal attack, infiltration, penetration, and turning movement), offensive tasks (movement to contact, attack, exploitation, and pursuit), and defensive tasks (area defense, mobile defense, and retrograde).\nAs depicted in Figure 1, the COA-GPT assistant commu-nicates with C2 personnel via natural language. It receives mission-related information such as objectives, terrain details, friendly and threat force description and arrangement, and any planning assumptions the C2 staff might have. For the LLM in the backed, we use OpenAI's GPT-4-Turbo (named \"gpt-4-1106-preview\" in their API) for text-only experiments and GPT-4-Vision (named \u201cgpt-4-vision-preview\") for tasks where mission information is given in both text and image format.\nUpon receiving this information, COA-GPT generates sev-eral COA options, each with a designated name, purpose, and visual representation of the proposed actions. Users can select their preferred COA and refine it through textual suggestions. COA-GPT processes this feedback to fine-tune the selected COA. Once the commander approves the final COA, COA-GPT conducts an analysis and provides performance metrics.\nThe generation of COAs by COA-GPT is remarkably swift, completing in seconds. Including the time for commander interaction, a final COA can be produced in just a few minutes. This efficiency underscores COA-GPT's potential to transform COA development in military operations, facilitating rapid adjustments in response to planning phase discrepancies or emerging opportunities.\""}, {"title": "V. EXPERIMENTS", "content": "A. Scenario and Experimental Setup\nOur evaluation of the proposed method is conducted within the Operation TigerClaw scenario [24], which is implemented as a custom map in the StarCraft II Learning Environment (PySC2) [29]. This platform enables artificial intelligence (AI) agents to engage in the StarCraft II game. Operation TigerClaw [24] presents a combat scenario where Task Force 1-12 CAV is tasked with seizing OBJ Lion by attacking across the Thar Thar Wadi, eliminating the threat force. This objective is depicted in Figure 2.\nIn PySC2, the scenario is realized by mapping StarCraft II units to their military equivalents, adjusting attributes like weapon range, damage, unit speed, and health. For instance, M1A2 Abrams combat armor units are represented by mod-ified Siege Tanks in tank mode, mechanized infantry by Hellions, among others [24]. The Friendly Force consists of 9 Armor, 3 Mechanized infantry, 1 Mortar, 2 Aviation, and 1 Reconnaissance unit. The Threat Force includes 12 Mechanized infantry, 1 Aviation, 2 Artillery, 1 Anti-Armor, and 1 Infantry unit. A specially designed StarCraft II map [24] depicts the TigerClaw area of operations, as shown in Figure 3.\nB. COA Generation\nCOA-GPT processes mission objectives and terrain infor-mation in text format for all experimental scenarios, which are described as follows:\n\u2022\nMission Objective. \"Move friendly forces from the west side of the river to the east via multiple bridges, destroy all hostile forces, and ultimately seize objective OBJ Lion East at the top right corner of the map (coordinates x: 200, y: 89).\"\n\u2022 Terrain Information. \"The map is split in two major portions (west and east sides) by a river that runs from north to south. There are four bridges that can be used to cross this river. Bridge names and exit coordinates are as follows: 1) Bridge Bobcat (x: 75, y: 26), 2) Bridge Wolf (x: 76, y: 128), 3) Bridge Bear (x:81, y: 179), and 4) Bridge Lion (x: 82, y: 211).\"\nAdditionally, for the experiments using LLM with image processing capabilities, COA-GPT takes as input a frame of a Common Operational Picture (COP) that overlays force arrangements in a satellite image of the terrain, as depicted in Figure 4.\nInformation regarding Friendly and Threat forces, detailing all assets present in the scenario, is fed to COA-GPT in JSON format:\n\u2022 Example Friendly Force Asset: \"{'unit_id': 4298113025, 'unit_type': 'Armor', 'alliance': 'Friendly', 'position': 'x': 12.0, \u2018y': 203.0}\"\n\u2022 Example Threat Force Asset: \u201c{'unit_id': 4294967297, 'unit_type': 'Mechanized infantry', 'alliance': \u2018Hostile', 'position': 'x': 99.0, 'y': 143.0}\"\nAdditionally, COA-GPT is programmed with knowledge of specific game engine functions to command each asset, such as:\n\u2022 attack_move_unit(unit_id, target_x, target_y): Directs a friendly unit to move to a specified coordinate, engaging hostile units encountered en route.\n\u2022 engage_target_unit(unit_id, target_unit_id, target_x, target_y): Orders a friendly unit to engage a specified hostile unit. If the target is out of range, the friendly unit will move to the target's location before engaging.\nFor integration with the PySC2 game engine, COA-GPT generates COAs in JSON format. These are subsequently translated into function calls understood by the PySC2 en-gine. A complete COA includes a mission name, strategy description, and specific commands for each asset. Within each simulation rollout, each asset adheres to its assigned command throughout the game, with COA-GPT limited to issuing only one command per asset at the beginning of the simulation."}, {"title": "C. Human Feedback", "content": "As depicted in Figure 5, the COA generated by our system is transformed into a graphical format accompanied by a concise mission summary for presentation to human evaluators. The evaluators can provide textual feedback, which COA-GPT then uses to refine and generate a new COA for subsequent feedback rounds.\nTo ensure a consistent and fair comparison across all generated COAs, we have standardized the human feedback process. The specific instructions provided in each feedback iteration for the COAs that incorporated human input are as follows:\n\u2022\nFirst Iteration: \"Make sure both our aviation units directly engages the enemy aviation unit.\"\n\u2022 Second Iteration: \"Make sure only our Scout unit is commanded to control Bridge Bobcat (x: 75 y: 26) and our other assets (not the aviation) are split in two groups and commanded to move towards both enemy artillery using the attack_move command.\"\nAfter receiving final approval from the human evaluators, COA-GPT proceeds to simulate the scenario multiple times, gathering and compiling various evaluation metrics for analy-sis."}, {"title": "D. Evaluation Metrics", "content": "The evaluation of the generated COAs is based on two key metrics recorded during the COA analysis: total reward and casualty figures. These metrics include:\n\u2022\nTotal Reward. This metric represents the total game score, which is tailored specifically for the TigerClaw mission scenario. It includes positive rewards for strategic advancements and neutralizing enemy units, and negative rewards for retreats and friendly unit losses. Specifically, agents gain +10 points for each unit advancing over bridges and for each enemy unit neutralized. Conversely, they lose -10 points for retreating over a previously crossed bridge and for each friendly unit lost.\n\u2022 Friendly Force Casualties. This metric counts the num-ber of friendly force units lost during the simulated engagement. It reflects the operational cost in terms of friendly unit losses.\n\u2022 Threat Force Casualties. This metric tracks the num-ber of enemy (threat force) units eliminated during the simulation, indicating the effectiveness of the COA in neutralizing the opposition."}, {"title": "E. Baselines", "content": "In this study, we benchmark our proposed method against the two best-performing published approaches for the task: Autocurriculum Reinforcement Learning from a Single Hu-man Demonstration [26] and the Asynchronous Advantage Actor-Critic (A3C) algorithm in Reinforcement Learning [26], [30]. Our comparison includes:\n\u2022\nAutocurriculum Reinforcement Learning [26]. Utiliz-ing a single human demonstration, this method develops a tailored curriculum for training a reinforcement learning agent via the A3C algorithm [30]. The agent, which con-trols each friendly unit individually, accepts either image inputs from the environment or a vectorial representation detailing unit positions. We evaluate our method against both input modes, referred to as \"Autocurr.-Vec\" and \"Autocurr.-Im\", respectively, in our results section.\n\u2022 Reinforcement Learning. As detailed in [26], this base-line employs the A3C algorithm, using either images or vector representations as inputs. Unlike the previous method, it does not incorporate a guiding curriculum for the learning agent. Both input scenarios are evaluated against our method, labeled as \u201cRL-Vec\u201d and \u201cRL-Im\", respectively, in our results.\n\u2022 COA-GPT. This serves as an ablation study of our proposed approach, as detailed in Section IV, but without human feedback. Labeled \u201cCOA-GPT\u201d (text-only inputs) and \u201cCOA-GPT-V\u201d (for experiments with vision models, text and image inputs), this version generates COAs based solely on the initial mission data provided by the C2 personnel, without further human input.\n\u2022 COA-GPT with Human Feedback. This is our fully realized method, outlined in Section IV. Beyond the initial mission information from the C2 personnel, this version also incorporates the impact of human feedback on the COA performance. We assess the changes after the first iteration of feedback, denoted as \"COA-GPT+H1\", and after the second iteration, labeled \u201cCOA-GPT+H2\" in the results. Similarly, for experiments using the LLM with vision capabilities the experiments are labelled \"COA-GPT-V+H1\" and \"COA-GPT-V+H2\"."}, {"title": "F. Results", "content": "For a comprehensive evaluation of COA-GPT, we generated five COAs for each method variant and conducted ten simu-lations for each, totaling 50 evaluation rollouts per baseline. All results discussed in this section represent the mean and standard deviation calculated across these 50 rollouts for each evaluation metric. The data for the Reinforcement Learning and Autocurriculum Reinforcement Learning baselines are sourced directly from their respective published work [26].\nFigure 7 presents a comparison of all baseline methods in terms of the average and standard deviation of the total rewards received during evaluation rollouts. COA-GPT, even in the absence of human interaction and relying solely on textual scenario representations, produces Courses of Action (COAs) that surpass the performance of all baselines on average. This includes the previously established state-of-the-art method, Autocurriculum Reinforcement Learning, which utilizes im-age data. COA-GPT-V, which also uses a single image as additional input, has equivalent performance compared to the previous baselines. Additionally, Figure 7 illustrates that the effectiveness of the COAs generated by COA-GPT is enhanced further (as indicated by a higher mean total reward) and exhibits reduced variability (evidenced by a lower standard deviation) when subjected to successive stages of human feedback. When taking into consideration the performance after human feedback, COA-GPT with vision models (COA-GPT-V+H1 and COA-GPT-V+H2) achieve higher mean total rewards compared to all previous baselines.\nFigure 6 illustrates the evolution of generated COAs in response to human feedback. Initially, the COA generated without human input (Figure 6a) depicts the planned move-ment of friendly forces across bridges and their engagement with hostile units. After the first round of human feedback (\u201cMake sure both our aviation units directly engages the enemy aviation unit.\u201d), we see a strategic pivot (Figure 6b); the friendly aviation units are now directed to engage the enemy's aviation assets directly. This adjustment reflects the human commander's intent to prioritize air superiority. The second iteration of feedback \u201cMake sure only our Scout unit is commanded to control Bridge Bobcat (x: 75 y: 26) and our other assets (not the aviation) are split in two groups and commanded to move towards both enemy artillery using the attack_move command.\u201d as seen in Figure 6c), results in a more nuanced approach: the friendly forces are divided, with specific units tasked to target enemy artillery positions. Additionally, the reconnaissance unit is ordered to secure the northern bridge and the friendly aviation is still tasked to engage the threat aviation, demonstrating COA-GPT successfully followed the commander's intent conveyed via textual feedback.\nFigures 8 and 9 provide a comparative analysis of the mean and standard deviation of friendly and threat force casualties, respectively, during the evaluation rollouts. In Fig-ure 8, we observe that the COA-GPT and COA-GPT-V, even when enhanced with human feedback (COA-GPT+H1, COA-GPT+H2, COA-GPT-V+H1 and COA-GPT-V+H2 models), exhibits higher friendly force casualties compared to other baselines. This outcome may be linked to COA-GPT's lower control resolution, offering a single strategic command at the beginning of the episode rather than continuous command inputs throughout the episode as seen in the baseline methods. While this approach facilitates a more interpretable COA de-velopment process for human operators, it potentially increases casualty rates due to the limited tactical adjustments during COA execution.\nIn contrast, Figure 9 will show that the lethality of COA-GPT towards threat forces remains consistent with other baselines, despite operating with lower control resolution. This indicates that COA-GPT and COA-GPT-V variations are capable of matching the effectiveness of other methods in neutralizing threats, even with the potential handicap of less granular command capabilities."}, {"title": "VI. CONCLUSIONS", "content": "This research presents an advancement in the field of military planning and decision-making through the develop-ment of COA-GPT. In the face of increasingly complex and dynamic future battlefields, COA-GPT addresses a critical need in C2 operations for rapid and informed decision-making. By leveraging the power of LLMs, both with text-only and text and images as input, COA-GPT substantially accelerates the development and analysis of COAs, integrating military doctrine and domain expertise via in-context learning.\nThe results from comprehensive evaluations further vali-date the effectiveness of COA-GPT. Notably, it demonstrates superior performance in developing COAs aligned to com-mander's intent, outperforming other methods including state-of-the-art Autocurriculum Reinforcement Learning algorithms. The enhanced adaptability and reduction in variability with human feedback highlight COA-GPT's potential in real-world scenarios, where dynamic adaptation to changing conditions is critical.\nMoreover, the ability of COA-GPT to generate actionable COAs within seconds, without the need for extensive pre-training, exemplifies its potential for rapid deployment in diverse military scenarios. This feature, coupled with its flexi-bility in adapting to new situations and the intuitive interaction with human commanders, underscores COA-GPT's practical utility and operational efficiency.\nIn conclusion, COA-GPT represents a transformative ap-proach in military C2 operations, facilitating faster, more agile decision-making and maintaining a strategic edge in modern warfare. Its development and successful application pave the way for further innovations in military AI, potentially reshaping how military operations are planned and executed in the future."}]}