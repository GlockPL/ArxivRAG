{"title": "Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning", "authors": ["Yifang Chen", "Shuohang Wang", "Ziyi Zhang", "Hiteshi Sharma", "Nikos Karampatziakis", "Donghan Yu", "Kevin Jamieson", "Simon Shaolei Du", "Yelong Shen"], "abstract": "Reinforcement learning with human feedback (RLHF), as a widely adopted approach in current large language model pipelines, is bottlenecked by the size of human preference data. While traditional methods rely on offline preference dataset constructions, recent approaches have shifted towards online settings, where a learner uses a small amount of labeled seed data and a large pool of unlabeled prompts to iteratively construct new preference data through self-generated responses and high-quality reward/preference feedback. However, most current online algorithms still focus on preference labeling during policy model updating with given feedback oracles, which incurs significant expert query costs. We are the first to explore cost-effective proxy reward oracles construction strategies for further labeling preferences or rewards with extremely limited labeled data and expert query budgets. Our approach introduces two key innovations: (1) on-policy query to avoid OOD and imbalance issues in seed data, and (2) active learning to select the most informative data for preference queries. Using these methods, we train a evaluation model with minimal expert-labeled data, which then effectively labels nine times more preference pairs for further RLHF training. For instance, our model using Direct Preference Optimization (DPO) gains around over 1% average improvement on AlpacaEval2, MMLU-5shot and MMLU-0shot, with only 1.7K query cost. Our methodology is orthogonal to other direct expert query-based strategies and therefore might be integrated with them to further reduce query costs.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning from human feedback (RLHF) has gained significant attention in recent years. Traditional approaches represented by Proximal Policy Optimization (PPO) (Ouyang et al., 2022), maintains one or several standalone reward models to finetune the policy model online by maximizing the rewards. Recently, people start to using Direct Preference Optimization (DPO) (Rafailov et al., 2024) and its variants due to their stable training properties. Some approaches query preferences directly from experts (e.g., humans, GPT) while others utilize a cheaper, offline-trained reward/preference model as a proxy oracle. However, all these methods suffer from the scarcity of human preference-labeled data.\nClassic works such as Bai et al. (2022); Cui et al. (2023); Zhu et al. (2023) aim to build high-quality, model-independent preference datasets offline. However, these methodologies can lead to distribution shift issues when the training model differs from the exploratory models used to generate the dataset. Recent research has shifted focus to online techniques, also referred to as \"self-improvement\" or \"iterative\" methods (Wang et al., 2023; Yuan et al., 2024; Rosset et al., 2024; Xiong et al., 2023; Dong et al., 2024; Tran et al., 2023; Wu et al., 2024; Xu et al., 2023; Xie et al., 2024). These methods leverage a set amount of labeled seed data and a large pool of unlabeled prompts, with the goal of continuously constructing new preference data through responses generated by the model itself, and potentially through external reward/preference feedback. The primary cost in this process comes from feedback queries from experts.\nDespite advances, most current online methods still focus on saving expert query costs directly for policy model training with fixed preference feedback oracles, as described in Sec.2 and App.A. Given the high complexity of generative models, they either demand significant amounts of preference/reward labeling from expensive experts or rely on offline-trained reward models like PairRM (Jiang et al., 2023), which itself requires substantial high-quality preference data. Conversely, the cost-saving strategies for constructing the proxy reward oracle remain under-explored.\nInspired by the success of proxy reward oracles,\nwe hypothesize that a smaller dataset is sufficient to train a weak evaluation model that can effectively label a much larger set of RLHF data. Furthermore, inspired by the successes of online methods, we incorporate on-policy query techniques into the training of evaluation models. The term \"on-policy,\" although not equivalent, is a key part of the \"online\" pipeline, as it emphasizes constructing the data using the target model being trained.\nIn this paper, we focus on cost-effective labeling strategies through constructing proxy reward oracles with only a limited amount of labeled seed data and a small expert query budget. For instance, our approach uses seed SFT data that is more than 10 times smaller than many existing works, and the query budget is on the same order as the seed data. Our objective is not to develop a new state-of-the-art model but to propose a methodology under these stringent conditions that can potentially be combined with other methods. The most closely related work to our study is the self-rewarding LM (Yuan et al., 2024), where a cost-efficient method is used in training a proxy reward oracle but with a different focus and setting than ours. We also investigate a modified off-policy version by adapting their methods to our setting.\nWe highlight our proposed pipelines in Fig.1. Specifically, our contributions can be summarized as threefold:\n\u2022 We first propose a random on-policy expert query strategy for labeling the preference data used to train the proxy reward oracle. Our empirical study validates that a weak evaluation model trained with a small amount of data can effectively label about nine times more preference pairs. For instance, with only 1.7K query budget, DPO training on Llama-2 7B with 15K preference pairs labeled by us yields over a 1% increase in performance on AlpacaEval2 and MMLU 5-shot metrics compared to the initial supervised fine-tuning model. In comparison, directly using the queried rewards to label the preference data without training an proxy oracle result in less than a 0.1% improvement under the same query budget. (Fig. 2)\n\u2022 Building on the success of the on-policy approach, we further explore replacing the random query strategy with coreset-type (e.g., k-center) active learning strategies, to select the most informative prompts and responses from the large unlabeled. Our active learning strategy results in additional performance gains from random on-policy strategy of 0.34% to 0.6% on the MMLU-5shot and 0.4% to 0.53% on MMLU-shot metrics under a properly managed budget. (Tab. 1)\n\u2022 Lastly, we also investigate other methods, such as off-policy data query strategy derived from the self-rewarding LM (Yuan et al., 2024) and"}, {"title": "2 Related works", "content": "variants of Self-play finetuning (SPIN) (Wang et al., 2023). Note that they are not directly comparable to our setting but help supporting the advantage of our on-policy + AL design, (Tab. 1, Sec. 5.4)\nTraining without reward oracle. The Self-play finetuning (SPIN) (Chen et al., 2024) relies solely on seed SFT data by consistently pairing a ground truth response as a positive sample with a model-generated response as a negative, thereby obviating the need for expert queries. However, the efficacy of this method heavily depends on the availability of a large seed data volume to avoid over-fitting. For example, while the original SPIN methodology utilizes a dataset of 50K IFT instances, our study is constrained to a considerably smaller set of only 3K instances. The pipeline is shown in Fig.4.\nUsing seed SFT data to train proxy oracle Yuan et al. (2024) proposed a method for training the evaluation model using SFT seed data within a self-rewarding framework. Although their experimental setup differs from ours, their strategies can be adapted to our context as off-policy query detailed in Sec. 5.1.1. Specifically, they use a single model to serve both as the policy and evaluation models, generating evaluations of seed data from the initial SFT model and then updating the model based on self-generated data. However, the success of this self-iterative process relies on a significantly more powerful model, LLama2-70B, whereas our focus is a more general methodology for any model, including weaker models like Llama2-7B. To adjust for this disparity, we query GPT for evaluating the seed data and use the generated evaluation-inclusive data to train a standalone evaluation model. Another adaptation is that the original paper uses Llama2-70B-chat to generate instructions relevant to the seed data to avoid distribution shift. However, this should be counted into the expert query budget. Here, we replace this self-instruct step with fixed pool of unlabeled prompts in our setting. The pipeline is shown in Fig.1.\nUsing external resources to train proxy oracle Directly querying preference or reward feedback from experts during the online process is expensive. Existing works like (Wu et al., 2024; Tran et al., 2023) utilized an offline-trained model, PairRM, proposed by Jiang et al. (2023) as a proxy feedback oracle. Recently, Dong et al. (2024) further"}, {"title": "3 Proxy-reward-oracle based self-improvement with limited data", "content": "Given a pretrained model Mo, our approach assumes a small set of labeled seed SFT data in the format (instruction, response, reward). Note that the reward label is optional, since most standard instruction fine-tuning datasets do not contain reward information. In such cases, using rewards for seed data will also require an expert query budget. Additionally, we have access to a large pool of unlabeled instruction-only data, X, and expert which provides preference feedback (e.g., GPT-4). Note that X is sourced differently from the seed data and therefore has a different distribution.\nOur goal is to label X by efficiently leveraging the expert's feedback and the intrinsic capabilities of Mo. In practice, it is not always feasible to label X by querying superior LLMs such as GPT, considering the cost to label (large-scale) data can be formidable. Therefore, we propose to efficiently build a reward feedback oracle Meval as a proxy to assist in labeling X, while minimizing the expert querying cost as much as possible. The performance of this proxy oracle will be measured by the final performance of the model trained on the newly labeled preference data. Since the problem setting is with strictly constrained budget to query expert LLMs, then using external datasets and benchmarks to train the proxy reward oracle (e.g., related works mentioned in Sec 2) is out of the scope.\nEssentially, we utilize two types of data during the entire process. Following the same notation as Yuan et al. (2024), we use Instruction Fine Tuning (IFT) to denote samples with the format [prompt, response] for policy model training that generates instruction-following responses. On the other hand, we use Evaluation Fine Tuning (EFT) to denote samples with the format [prompt + response + evaluation criterion, justification + reward score"}, {"title": "4 Our strategy: active on-policy query.", "content": "Now we are ready to present our on-policy active EFT query strategies, starting with the detailed pipelines as follows. (See visualization in Fig. 1.)\nDetailed steps of our on-policy +AL pipeline.\n1. Given the pretrained model Mo and the initial seed data IFTseed, SFT on IFTseed (or only on its high reward part if available) to obtain M\u2081.\n2. Given a set of N unlabeled prompts X, for each \\(x \\in X\\), generate a set of k responses Yo, Y1,..., Yk using M\u2081. Denote the entire pool of responses as Y\u2081 and the whole N * k size generated samples as IFT1.\n3. Use active query strategies (explained below) to select a \\(n < N * k\\) budget subset of IFT1, query expert (e.g. GPT) for their evaluation results based on the evaluation criterion templates, and therefore construct EFT1.\n4. Based on a pretrained model Mo, SFT on EFT1 to get a weak evaluation model Meval.\n5. Generate rewards for the rest of unqueried IFT1 using Meval. For each prompt, choose the highest and lowest samples to form a DPO pair and denote the whole set as DPO1\n6. Finally trained M2 based on M\u2081 using DPO1.\nThe key contribution of our pipeline comes from the third step. Firstly, we emphasize on-policy EFT querying, where the term 'on-policy' refers to sampling from the target model we are training, rather than utilizing external resources. I.e., we generate EFT1 based on responses from the policy model M1, rather than relying on the initial EFTseed. Secondly, rather than randomly selecting a subset of IFT1 for querying, we employ Active Learning (AL) strategies to identify and select a more informative subset.\nFocus on one iteration and DPO In this study, we limit our focus to a single iteration, rather than multiple iterations, to specifically analyze the impact of on-policy querying and AL strategies. It"}, {"title": "4.1 Random (passive) on-policy query", "content": "is entirely feasible to extend our pipeline to multiple iterations, the single-iteration here allows us to isolate and understand the effects more clearly.\nThe previous method involves training Meval via EFTseed. However, this approach faces two main challenges: Firstly, due to the distribution shift from seed data to unlabeled prompts X, the IFT1 generated by the policy model may fall into the out-of-distribution (OOD) domain of evaluation model. Secondly, we observe that the reward distribution for seed data is often biased towards higher values. This bias arises because EFTseed, derived from IFTseed, typically consists of human-annotated, high-quality entries, which benefits the SFT phase but can leads to over-fitting when training evaluation models. An example of this can be seen in the left part of Figure 3 (specific dataset details will be provided later). Training with a balanced reward distribution can mitigate such bias issues, but also significantly reduces the effective number of training EFTs (e.g., less than 20% of total EFTseed are used for training Meval), thus limiting potential improvements.\nTo address these two problems, we first propose random on-policy query for constructing Meval. This method involves randomly selecting a subset of n prompts from X and generating responses using our target policy M\u2081 (i.e. on-policy). This not only avoid OOD problem when using Meval to label the rest of IFT1, but also natural leads to more balanced rewards distribution among EFT1 as shown in the right part of Fig. 3.\nVariant: random on-policy query with balanced training. Although EFT\u2081 exhibits a more diverse reward distribution than EFTseed, we can further enforce the strict balance by setting the number of samples for each reward score to be equal. Later we show that the unbalanced and balanced version each may leads to different advantages."}, {"title": "4.2 Active on-policy query: coresetEFT and coresetIFT", "content": "Active querying for LLM finetuning has been studied in Bhatt et al. (2024) but they focused on query response for IFT dataset that is for supervised fine-tuning. Their results show that actively learning a generative model is challenging. However, our goal here is to actively learn a weak evaluator Meval, which is more close to classification tasks, where\nnumerous of AL strategies has been proved to be effective.\\(e.g. \\) Sener and Savarese (2018); Geifman and El-Yaniv (2017); Citovsky et al. (2021); Ash et al. (2019, 2021)) The similar conjecture has also been proposed in Dong et al. (2024) where they believe that, reward feedback, as a discriminative task, might be easier than generative tasks\nWhile there exists many AL strategies, here we focused on the classical coreset selection. (In some place, people will call this K-center selection.) The main idea is to annotate inputs that are diverse in the representation space. Sener and Savarese (2018) proposed a k-center objective that chooses k size subset S as centers of balls with equal radius:\n\\(S = \\underset{S' \\subset X, |S'|=k}{argmin} \\underset{i \\in X}{max} \\underset{j \\in S'}{min}|| f(x_i) - f (x_j)||, (1)\\)\nwhere f is a feature extractor that maps prompts into feature space in Rd and is derived from the pretrained model h. For decoder-only architectures, we use the first hidden state as the feature. To optimize this NP-hard objective (Cook et al., 1998), we follow the greedy methods proposed by Sener and Savarese (2018), which enjoy a 2-multiplicative approximation guarantee to the optimal selection.\nTwo ways of extracting embedding. In classical AL problems, the inputs and embedding models are straightforward since the queries are trained on the same data as the model whose final performance is of interest, and the training set inputs are fixed. In our scenario, we are not directly comparing the performance of Meval, and part of EFT1 is generated by the model itself. Therefore, we propose two methods for extracting embeddings and study the benefits of each.\n\u2022 coresetEFT: We use the instruction for each EFT sample (i.e IFT prompts + M\u2081 generated response + evaluation criterion).\n\u2022 coresetIFT: We use the instruction for each IFT sample as input and used seed IFT trained M\u2081 as the embedding extractor model.\nWhile both methods involves information provided by unlabeled prompts and the SFT trained M1, the coresetEFT explicitly consider the embedding of the generated outputs. Therefore, coresetEFT can be reduced to standard active learning for discriminative problem (i.e classification) where the learner take aims to find the decision boundary a. On the other hand, the second coresetIFT makes assumption that, the evaluation made by trained evaluator mainly depends on the prompts instead of the generated response."}, {"title": "4.3 Summary of our approaches", "content": "As summary, we proposed three approaches \u2013 random on-policy and two active on-policy strategies coresetIFT and coresetEFT. We also adapt the SPIN and self-rewarding methods mentioned in Sec. 2 to our settings as additional investigation. We will focus on the following three questions:\n\u2022 Q1. Is a weak evaluator Meval trained on a small budget n of EFT\u2081 sufficient to construct a larger preference set, and is the performance of M2 always positively correlated with the size n?\n\u2022 Q2. Can an active learning strategy further improve the performance over random on-policy?\n\u2022 Q3. How does on-policy+AL strategy compare with other candidate approaches like off-policy query and variants of SPIN?"}, {"title": "5 Experiments", "content": "5.1 Experimental setup\nModels and dataset We choose pretrained model Mo to be Llama-27B (Touvron et al., 2023) and the first round conversion of OpenAssistant (oasst1) (K\u00f6pf et al., 2024) as the initial IFTseed whose size is around 3K. We specifically select the high-reward data from oasst1 as SFT data. For EFTseed, the original reward scores from oasst1 lacked justification and did not conform to our evaluation templates. Consequently, we constructed an EFTseed by querying GPT using all 3K IFTseed. (Only when applying the off-policy query approach.) For unlabeled prompts X, we selected random N subsets of prompts, ranging from 2.8K to 16.8K, from the Supernatural Instruction dataset (Wang et al., 2022) and generate k = 4 responses for each prompt.\nTrain weak evaluator Meval For each EFT dataset used to train M\u2081val, we randomly selected 300 (or 200 when train with EFTseed) sample as validation set, with the remainder forming the training set to address training randomness. Specifically, we trained Meval using EFT train set over three random seeds for three epochs, and choose the best checkpoint using the validation set.\nRandomness and the impact of initial SFT model We trained three different versions of M\u2081 using the same IFTseed but with varying random seeds to mitigate training randomness and to explore the influence of the initial model quality on the data synthesis pipeline. Each M\u2081 version was then used to generate responses and construct DPO pairs. Each DPO set was subsequently trained with"}, {"title": "5.1.1 Other approaches investigated in ablation studies", "content": "three random seeds. In all the results in the rest of the paper, unless specified, we report the average accuracy and sometimes square root of the total variance (denote as \u221atv) across all nine random seeds. tv = E[Var(M2 | M\u2081)] + Var(E[M2 | M1])\nEvaluation metric We evaluate the performance of our EFT query strategy by measuring the performance change from the initial SFT model M\u2081 to the final policy model M2. Here we first use AlpacaEval2 Li et al. (2023), MMLU-0shot, MMLU-5shot (Suzgun et al., 2023) as the downstream metrics to assess the performance of three proposed strategies. Then, we add BBH-COT and BBH-NOCOT Hendrycks et al. (2020) where the prompts from supernatural instruction is less relevant to further investigate those methods through ablation studies.\nWe compare our approach with SPIN and off-policy query as explained in Sec. 2. Both methods have different original settings from ours, so we adapt and reproduce their approaches in our setting.\nTrain with EFTseed (Off-policy query) Due to the high bias in EFTseed, we select only 200 samples among the all 3K queried EFTseed as the training set to ensure an equal number of rewards per class during training. The number of query budgets and exact query strategies under this setting depends on whether the initial rewards of IFTseed are known or not. Suppose the rewards of IFTseed are roughly known in advance; then we only need to query and construct EFTseed for an equal number of samples for each reward class, leading to a 200 (train) + 300 (validation) query budget. We refer to this method as balanced off-policy query. Otherwise, if the rewards are unknown, which is common in most SFT datasets, then we need to query the entire set of seed SFT data to find 200 balanced samples, given that IFTseed is highly biased. We refer to this method as off-policy query.\nSPIN and its variants We not only compare the original SPIN with our proposed methods, but also highlight the disadvantages of SPIN under the setting where no unlabeled prompts are available, as shown in Tab. 4. Specifically, we train Meval using EFTseed and employ it to evaluate responses generated by M\u2081 for each prompt in IFTseed instead of X."}, {"title": "5.2 Main Results: performance across different strategy and query budget n", "content": "For the original SPIN, we choose human-annotated responses from oasst1 (i.e., IFTseed) as positive samples and randomly generated \u1ef9 by M1 as negative samples. For a hybrid version, denoted as SPIN+Meval, we use the ground truth response as the positive sample and the Meval generated response with the lowest reward as the negative one. Finally, for the method denoted as Meval, both positive and negative samples are selected based on their evaluation by Meval"}, {"title": "5.3 Ablation study: Sufficiency of training weak evaluator with low budget EFT", "content": "We have demonstrated the advantages of on-policy and active learning strategies with a fixed N = 16.8K, using the AlpacaEval2 and MMLU metrics. Here, we further study the labeling ability of Meval across different values of N with a fixed query budget n to address Q1, which give an affirmative answer that with limited budget n, using that to train a proxy oracle is better than direct label preference. In this ablation study, we focus on the random on-policy strategy as it generally exhibits similar trends to AL.\nMeval trained on 1.5K EFT1 can effectively label more than 9x DPO pairs. Despite training"}, {"title": "5.4 Ablation study: Compare with potential approaches adapted from previous works", "content": "In this section, we further answer Q3. We first explore the effectiveness of our proposed methods"}, {"title": "6 Conclusion", "content": "This work is the first to explore cost-effective proxy reward oracle construction strategies for labeling a larger set of preferences with extremely limited labeled seed data. We identify two key techniques: on-policy query and active learning. The results convey a clear message: with a limited query budget, reward feedback should be used to train a proxy reward/preference oracle instead of being directly used for labeling."}, {"title": "7 Limitations", "content": "Focus on methodology instead of achieving state-of-the-art This paper focuses more on the methodology rather than achieving state-of-the-art results. Therefore, we do not use more recent models like Llama3-8B or Mistral-7B, or more recent datasets like Ultra-feedback. Given that the performance of self-improvement methods highly relies on the capability of the initial pretrained model and high-quality data, our choice may limit large numerical increases. Additionally, as mentioned in Sec. 4, we focus only on one iteration, while most existing works validate that multiple iterations can further improve model performance. Therefore, the main direction for future work is to apply our methods in more advanced setups to achieve state-of-the-art models.\nLimited downstream metrics As we mentioned earlier, the effectiveness of our algorithm highly depends on the quality of initial pretrained models and datasets. Here, we did not test on all the standard metrics like MT-bench or GSM8k since our choice of model and dataset are naturally not good at those benchmarks. After switching to more advanced setups, we should conduct a more thorough investigation.\nFailure of using external resources Many existing works employ externally trained models, especially some existing reward models. It is important to combine our methods with these external resources.\nCombine with existing iterative DPO methods As mentioned in Sec. 2 and App. A, many existing works assume a fixed reward/preference oracle and focus on optimizing the algorithm by proposing new loss functions or adding an extra exploratory policy. These directions seem orthogonal to our methods. It is important to combine our approach with these to see whether our approaches are truly universally applicable to all those methods."}, {"title": "A More related works", "content": "A.1 Efficient query with fixed reward/preference oracle\nMany existing works focus on efficient query by assuming the reward/preference oracle is good enough. In the other word, they want to select the data that is informative for training the policy model, which is the generative tasks, instead of informative for training the evaluation model, which is the discriminative tasks. This motivation is highly related classical reinforcement learning topics. Specifically, works such as (Xu et al., 2023; Touvron et al., 2023) start using iterative DPO with the same loss as original DPO paper. Later works like Rosset et al. (2024); Wu et al. (2024); Gao et al. (2024); Xie et al. (2024) proposes to use more advanced loss instead of DPO loss. ((Rosset et al., 2024) propose new loss in their theoretical section but in their experiment they still use something like original DPO loss). Finally, while all of those above works are focusing on on-policy query, (Xiong et al., 2023; Dong et al., 2024) further propose to maintain an extra exploratory strategy to cover more space, therefore combine the on-policy and off-policy strategy.\nA.2 Pipeline for SPIN and direct query"}, {"title": "B More experimental setup", "content": "B.1 Hyperparameters\nHyper-parameter for training M1, M2 When training M\u2081, we use SFT training pipelines with batch size 128, 2 epochs, learning rate 2e \u2013 5 and warmup rate 0.03. When training M2, we use DPO training pipelines with batch size 32, 2 epochs for 16800 number of unlabeled prompts and 3 epoch for others, learning rate 5e - 7 and warmup rate 0.01.\nHyper-parameter for training Meval and Meval When training the evaluation models using either on-policy generated EFT1 or EFTseed, we use the same setting as training M1.\nHyper-parameter for generating EFT1 and corresponding DPO For each instruction in X and IFTseed (when compare with SPIN), we generate k = 4 responses with maxLength = 1024. Then to give the reward feedback for each generated response, we call Meval three times (therefore get at most three EFT) and compute the average reward. We do not explicitly use the justification feedback, but such justification serves as chain-of-thought to help generate proper reward. Also not all response can received rewards, sometimes the Meval can fail to give any reasonable evaluation. In that case, we will directly discard the sample. Therefore, among 16.8K prompts, we only get about 15K DPO pairs.\nB.2 Example of EFT\nWe use the exact same approach as present in Figure.2 in (Yuan et al., 2024) and therefore omit the details here."}, {"title": "C More experimental results", "content": "C.1 Visualization of Table. 1\nwe show the visualization of Table. 1 in Fig. 5.\nC.2 Ablation Study: Compare with previous work\nFixed Ground Truth as Positive Sample Significantly Reduces Performance SPIN is strictly worse than our proposed methods as shown in Tab. 4. One might attribute this inferior performance of SPIN to the lack of using unlabeled prompts X, we argue that the core issue persists even when using only EFTseed and IFTseed. In Tab. 4, by comparing SPIN and SPIN+Meval with Meval, we clearly see that, even"}]}