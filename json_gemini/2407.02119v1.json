{"title": "Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning", "authors": ["Yifang Chen", "Shuohang Wang", "Ziyi Zhang", "Hiteshi Sharma", "Nikos Karampatziakis", "Donghan Yu", "Kevin Jamieson", "Simon Shaolei Du", "Yelong Shen"], "abstract": "Reinforcement learning with human feedback (RLHF), as a widely adopted approach in current large language model pipelines, is bottlenecked by the size of human preference data. While traditional methods rely on offline preference dataset constructions, recent approaches have shifted towards online settings, where a learner uses a small amount of labeled seed data and a large pool of unlabeled prompts to iteratively construct new preference data through self-generated responses and high-quality reward/preference feedback. However, most current online algorithms still focus on preference labeling during policy model updating with given feedback oracles, which incurs significant expert query costs. We are the first to explore cost-effective proxy reward oracles construction strategies for further labeling preferences or rewards with extremely limited labeled data and expert query budgets. Our approach introduces two key innovations: (1) on-policy query to avoid OOD and imbalance issues in seed data, and (2) active learning to select the most informative data for preference queries. Using these methods, we train a evaluation model with minimal expert-labeled data, which then effectively labels nine times more preference pairs for further RLHF training. For instance, our model using Direct Preference Optimization (DPO) gains around over 1% average improvement on AlpacaEval2, MMLU-5shot and MMLU-0shot, with only 1.7K query cost. Our methodology is orthogonal to other direct expert query-based strategies and therefore might be integrated with them to further reduce query costs.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning from human feedback (RLHF) has gained significant attention in recent years. Traditional approaches represented by Proximal Policy Optimization (PPO) (Ouyang et al., 2022), maintains one or several standalone reward models to finetune the policy model online by maximizing the rewards. Recently, people start to using Direct Preference Optimization (DPO) (Rafailov et al., 2024) and its variants due to their stable training properties. Some approaches query preferences directly from experts (e.g., humans, GPT) while others utilize a cheaper, offline-trained reward/preference model as a proxy oracle. However, all these methods suffer from the scarcity of human preference-labeled data.\nClassic works such as Bai et al. (2022); Cui et al. (2023); Zhu et al. (2023) aim to build high-quality, model-independent preference datasets offline. However, these methodologies can lead to distribution shift issues when the training model differs from the exploratory models used to generate the dataset. Recent research has shifted focus to online techniques, also referred to as \"self-improvement\" or \"iterative\" methods (Wang et al., 2023; Yuan et al., 2024; Rosset et al., 2024; Xiong et al., 2023; Dong et al., 2024; Tran et al., 2023; Wu et al., 2024; Xu et al., 2023; Xie et al., 2024). These methods leverage a set amount of labeled seed data and a large pool of unlabeled prompts, with the goal of continuously constructing new preference data through responses generated by the model itself, and potentially through external reward/preference feedback. The primary cost in this process comes from feedback queries from experts.\nDespite advances, most current online methods still focus on saving expert query costs directly for policy model training with fixed preference feedback oracles, as described in Sec.2 and App.A. Given the high complexity of generative models, they either demand significant amounts of preference/reward labeling from expensive experts or rely on offline-trained reward models like PairRM (Jiang et al., 2023), which itself requires substantial high-quality preference data. Conversely, the cost-saving strategies for constructing the proxy reward oracle remain under-explored.\nInspired by the success of proxy reward oracles,\nwe hypothesize that a smaller dataset is sufficient to\ntrain a weak evaluation model that can effectively\nlabel a much larger set of RLHF data. Furthermore,\ninspired by the successes of online methods, we in-\ncorporate on-policy query techniques into the train-\ning of evaluation models. The term \"on-policy,\"\nalthough not equivalent, is a key part of the \"on-\nline\" pipeline, as it emphasizes constructing the\ndata using the target model being trained.\nIn this paper, we focus on cost-effective label-\ning strategies through constructing proxy reward\noracles with only a limited amount of labeled seed\ndata and a small expert query budget. For instance,\nour approach uses seed SFT data that is more\n10 times smaller than many existing works, and\nthe query budget is on the same order as the seed\ndata. Our objective is not to develop a new state-of-\nthe-art model but to propose a methodology under\nthese stringent conditions that can potentially be\ncombined with other methods. The most closely\nrelated work to our study is the self-rewarding LM\n(Yuan et al., 2024), where a cost-efficient method\nis used in training a proxy reward oracle but with\na different focus and setting than ours. We also in-\nvestigate a modified off-policy version by adapting\ntheir methods to our setting.\nWe highlight our proposed pipelines in Fig.1.\nSpecifically, our contributions can be summarized\nas threefold:"}, {"title": "2 Related works", "content": "Training without reward oracle. The Self-play\nfinetuning (SPIN) (Chen et al., 2024) relies solely\non seed SFT data by consistently pairing a ground\ntruth response as a positive sample with a model-\ngenerated response as a negative, thereby obviating\nthe need for expert queries. However, the efficacy\nof this method heavily depends on the availability\nof a large seed data volume to avoid over-fitting.\nFor example, while the original SPIN methodology\nutilizes a dataset of 50K IFT instances, our study\nis constrained to a considerably smaller set of only\n3K instances.\nUsing seed SFT data to train proxy oracle\nYuan et al. (2024) proposed a method for training\nthe evaluation model using SFT seed data within\na self-rewarding framework. Although their ex-\nperimental setup differs from ours, their strategies\ncan be adapted to our context as off-policy query\ndetailed in Sec. 5.1.1. Specifically, they use a sin-\ngle model to serve both as the policy and evalua-\ntion models, generating evaluations of seed data\nfrom the initial SFT model and then updating the\nmodel based on self-generated data. However, the\nsuccess of this self-iterative process relies on a\nsignificantly more powerful model, LLama2-70B,\nwhereas our focus is a more general methodol-\nogy for any model, including weaker models like\nLlama2-7B. To adjust for this disparity, we query\nGPT for evaluating the seed data and use the gener-\nated evaluation-inclusive data to train a standalone\nevaluation model. Another adaptation is that the\noriginal paper uses Llama2-70B-chat to generate\ninstructions relevant to the seed data to avoid distri-\nbution shift. However, this should be counted into\nthe expert query budget. Here, we replace this self-\ninstruct step with fixed pool of unlabeled prompts\nin our setting.\nUsing external resources to train proxy oracle\nDirectly querying preference or reward feedback\nfrom experts during the online process is expensive.\nExisting works like (Wu et al., 2024; Tran et al.,\n2023) utilized an offline-trained model, PairRM,\nproposed by Jiang et al. (2023) as a proxy feed-\nback oracle. Recently, Dong et al. (2024) further"}, {"title": "3 Proxy-reward-oracle based self-improvement with limited data", "content": "Given a pretrained model Mo, our approach as-\nsumes a small set of labeled seed SFT data in the\nformat (instruction, response, reward). Note that\nthe reward label is optional, since most standard\ninstruction fine-tuning datasets do not contain re-\nward information. In such cases, using rewards for\nseed data will also require an expert query budget.\nAdditionally, we have access to a large pool of un-\nlabeled instruction-only data, X, and expert which\nprovides preference feedback (e.g., GPT-4). Note\nthat X is sourced differently from the seed data\nand therefore has a different distribution.\nOur goal is to label X by efficiently leveraging\nthe expert's feedback and the intrinsic capabilities\nof Mo. In practice, it is not always feasible to label\nX by querying superior LLMs such as GPT, con-\nsidering the cost to label (large-scale) data can be\nformidable. Therefore, we propose to efficiently\nbuild a reward feedback oracle Meval as a proxy\nto assist in labeling X, while minimizing the ex-\npert querying cost as much as possible. The per-\nformance of this proxy oracle will be measured\nby the final performance of the model trained on\nthe newly labeled preference data. Since the prob-\nlem setting is with strictly constrained budget to\nquery expert LLMs, then using external datasets\nand benchmarks to train the proxy reward oracle\n(e.g., related works mentioned in Sec 2) is out of\nthe scope.\nEssentially, we utilize two types of data during\nthe entire process. Following the same notation as\nYuan et al. (2024), we use Instruction Fine Tuning\n(IFT) to denote samples with the format [prompt,\nresponse] for policy model training that generates\ninstruction-following responses. On the other hand,\nwe use Evaluation Fine Tuning (EFT) to denote\nsamples with the format [prompt + response + \nevaluation criterion, justification + reward score"}, {"title": "4 Our strategy: active on-policy query.", "content": "Now we are ready to present our on-policy active\nEFT query strategies, starting with the detailed\npipelines as follows. (See visualization in Fig. 1.)\nDetailed steps of our on-policy +AL pipeline.\n1. Given the pretrained model Mo and the initial\nseed data IFT seed, SFT on IFTseed (or only on its\nhigh reward part if available) to obtain M\u2081.\n2. Given a set of N unlabeled prompts X, for\neach \\(x \\in X\\), generate a set of k responses\n\\(Yo, Y1,..., Yk\\) using M\u2081. Denote the entire pool\nof responses as \\(Y_1\\) and the whole N * k size\ngenerated samples as IFT1.\n3. Use active query strategies (explained below)\nto select a n < N * k budget subset of IFT1,\nquery expert (e.g. GPT) for their evaluation\nresults based on the evaluation criterion tem-\nplates, and therefore construct EFT1.\n4. Based on a pretrained model Mo, SFT on EFT1\nto get a weak evaluation model Meval.\n5. Generate rewards for the rest of unqueried IFT1\nusing Moval. For each prompt, choose the high-\nest and lowest samples to form a DPO pair and\ndenote the whole set as DPO1\n6. Finally trained M2 based on M\u2081 using DPO1.\nThe key contribution of our pipeline comes from\nthe third step. Firstly, we emphasize on-policy\nEFT querying, where the term 'on-policy' refers\nto sampling from the target model we are training,\nrather than utilizing external resources. I.e., we\ngenerate EFT1 based on responses from the pol-\nicy model M1, rather than relying on the initial\nEFTseed. Secondly, rather than randomly selecting\na subset of IFT1 for querying, we employ Active\nLearning (AL) strategies to identify and select a\nmore informative subset.\nFocus on one iteration and DPO In this study,\nwe limit our focus to a single iteration, rather than\nmultiple iterations, to specifically analyze the im-\npact of on-policy querying and AL strategies. It"}, {"title": "4.1 Random (passive) on-policy query", "content": "The previous method involves training Meval via\nEFTseed. However, this approach faces two main\nchallenges: Firstly, due to the distribution shift\nfrom seed data to unlabeled prompts X, the IFT1\ngenerated by the policy model may fall into the\nout-of-distribution (OOD) domain of evaluation\nmodel. Secondly, we observe that the reward\ndistribution for seed data is often biased towards\nhigher values. This bias arises because EFT seed,\nderived from IFTseed, typically consists of human-\nannotated, high-quality entries, which benefits the\nSFT phase but can leads to over-fitting when train-\ning evaluation models. An example of this can be\nseen in the left part of Figure 3 (specific dataset\ndetails will be provided later). Training with a bal-\nanced reward distribution can mitigate such bias\nissues, but also significantly reduces the effective\nnumber of training EFTs (e.g., less than 20% of\ntotal EFTseed are used for training Meval), thus lim-\niting potential improvements.\nTo address these two problems, we first propose\nrandom on-policy query for constructing Meval.\nThis method involves randomly selecting a subset\nof n prompts from X and generating responses\nusing our target policy M\u2081 (i.e. on-policy). This\nnot only avoid OOD problem when using Meval\nto label the rest of IFT1, but also natural leads to\nmore balanced rewards distribution among EFT1\nas shown in the right part of Fig. 3.\nVariant: random on-policy query with balanced\ntraining. Although EFT\u2081 exhibits a more diverse\nreward distribution than EFTseed, we can further\nenforce the strict balance by setting the number of\nsamples for each reward score to be equal. Later\nwe show that the unbalanced and balanced version\neach may leads to different advantages."}, {"title": "4.2 Active on-policy query: coresetEFT and coresetIFT", "content": "Active querying for LLM finetuning has been stud-\nied in Bhatt et al. (2024) but they focused on query\nresponse for IFT dataset that is for supervised fine-\ntuning. Their results show that actively learning a\ngenerative model is challenging. However, our goal\nhere is to actively learn a weak evaluator Meval,\nwhich is more close to classification tasks, where"}, {"title": "4.3 Summary of our approaches", "content": "As summary, we proposed three approaches \u2013 random on-policy and two active on-policy strategies\ncoresetIFT and coresetEFT. We also adapt the\nSPIN and self-rewarding methods mentioned in\nSec. 2 to our settings as additional investigation.\nWe will focus on the following three questions:\n\u2022 Q1. Is a weak evaluator Meval trained on a small\nbudget n of EFT\u2081 sufficient to construct a larger\npreference set, and is the performance of M2\nalways positively correlated with the size n?\n\u2022 Q2. Can an active learning strategy further im-\nprove the performance over random on-policy?\n\u2022 Q3. How does on-policy+AL strategy compare\nwith other candidate approaches like off-policy\nquery and variants of SPIN?"}, {"title": "5 Experiments", "content": "Models and dataset We choose pretrained model\nMo to be Llama-27B (Touvron et al., 2023) and the\nfirst round conversion of OpenAssistant (oasst1)\n(K\u00f6pf et al., 2024) as the initial IFTseed whose\nsize is around 3K. We specifically select the high-\nreward data from oasst1 as SFT data. For EFTseed,\nthe original reward scores from oasst1 lacked justi-\nfication and did not conform to our evaluation tem-\nplates. Consequently, we constructed an EFTseed\nby querying GPT using all 3K IFTseed. (Only when\napplying the off-policy query approach.) For unla-\nbeled prompts X, we selected random N subsets\nof prompts, ranging from 2.8K to 16.8K, from the\nSupernatural Instruction dataset (Wang et al., 2022)\nand generate k = 4 responses for each prompt.\nTrain weak evaluator Meval For each EFT\ndataset used to train M\u2081val, we randomly selected\n300 (or 200 when train with EFTseed) sample as val-\nidation set, with the remainder forming the training\nset to address training randomness. Specifically,\nwe trained Moval using EFT train set over three\nrandom seeds for three epochs, and choose the best\ncheckpoint using the validation set.\nRandomness and the impact of initial SFT\nmodel We trained three different versions of M\u2081\nusing the same IFTseed but with varying random\nseeds to mitigate training randomness and to ex-\nplore the influence of the initial model quality on\nthe data synthesis pipeline. Each M\u2081 version was\nthen used to generate responses and construct DPO\npairs. Each DPO set was subsequently trained with"}, {"title": "5.1.1 Other approaches investigated in ablation studies", "content": "We compare our approach with SPIN and off-policy\nquery as explained in Sec. 2. Both methods have\ndifferent original settings from ours, so we adapt\nand reproduce their approaches in our setting.\nTrain with EFTseed (Off-policy query) Due to\nthe high bias in EFTseed, we select only 200 sam-\nples among the all 3K queried EFTseed as the train-\ning set to ensure an equal number of rewards per\nclass during training. The number of query bud-\ngets and exact query strategies under this setting\ndepends on whether the initial rewards of IFT seed\nare known or not. Suppose the rewards of IFT seed\nare roughly known in advance; then we only need\nto query and construct EFTseed for an equal number\nof samples for each reward class, leading to a 200\n(train) + 300 (validation) query budget. We refer to\nthis method as balanced off-policy query. Other-\nwise, if the rewards are unknown, which is common\nin most SFT datasets, then we need to query the\nentire set of seed SFT data to find 200 balanced\nsamples, given that IFTseed is highly biased. We\nrefer to this method as off-policy query.\nSPIN and its variants We not only compare the\noriginal SPIN with our proposed methods, but also\nhighlight the disadvantages of SPIN under the set-\nting where no unlabeled prompts are available, as\nshown in Tab. 4. Specifically, we train Meval using\nEFTseed and employ it to evaluate responses gener-\nated by M\u2081 for each prompt in IFTseed instead of\nX."}, {"title": "5.2 Main Results: performance across different strategy and query budget n", "content": "With a fixed number of unlabeled prompts at\nN=16.8K, we evaluate the performance of random\non-policy, coresetIFT on-policy, and coresetEFT\non-policy strategies across various n budgets using\nthe AlpacaEval2 and MMLU metrics. As shown\nin Tab. 1, our random on-policy strategies gener-\nally result an effective proxy reward oracle, and\nAL module further improves performance. This an-\nswers Q2. More comparisons with other candidate\napproaches are investigated in Sec. 5.4. Below, we\nprovide further discussion on the performance of\nour approaches.\nOver-Optimizing Meval Can Degrade Perfor-\nmance. For all three methods, a consistent in-\ncrease is observed only in the MMLU5shot metric."}, {"title": "5.3 Ablation study: Sufficiency of training weak evaluator with low budget EFT", "content": "We have demonstrated the advantages of on-policy\nand active learning strategies with a fixed N =\n16.8K, using the AlpacaEval2 and MMLU met-\nrics. Here, we further study the labeling ability\nof Meval across different values of N with a fixed\nquery budget n to address Q1, which give an af-\nfirmative answer that with limited budget n, using\nthat to train a proxy oracle is better than direct label\npreference. In this ablation study, we focus on the\nrandom on-policy strategy as it generally exhibits\nsimilar trends to AL.\nMeval trained on 1.5K EFT1 can effectively la-\nbel more than 9x DPO pairs. Despite training"}, {"title": "5.4 Ablation study: Compare with potential approaches adapted from previous works", "content": "In this section, we further answer Q3. We first\nexplore the effectiveness of our proposed meth-"}, {"title": "6 Conclusion", "content": "This work is the first to explore cost-effective proxy\nreward oracle construction strategies for labeling\na larger set of preferences with extremely limited\nlabeled seed data. We identify two key techniques:\non-policy query and active learning. The results\nconvey a clear message: with a limited query bud-\nget, reward feedback should be used to train a proxy\nreward/preference oracle instead of being directly\nused for labeling."}, {"title": "7 Limitations", "content": "Focus on methodology instead of achieving\nstate-of-the-art This paper focuses more on the\nmethodology rather than achieving state-of-the-art\nresults. Therefore, we do not use more recent mod-\nels like Llama3-8B or Mistral-7B, or more recent\ndatasets like Ultra-feedback. Given that the perfor-\nmance of self-improvement methods highly relies\non the capability of the initial pretrained model and\nhigh-quality data, our choice may limit large nu-\nmerical increases. Additionally, as mentioned in\nSec. 4, we focus only on one iteration, while most\nexisting works validate that multiple iterations can\nfurther improve model performance. Therefore,\nthe main direction for future work is to apply our\nmethods in more advanced setups to achieve state-\nof-the-art models.\nLimited downstream metrics As we mentioned\nearlier, the effectiveness of our algorithm highly\ndepends on the quality of initial pretrained models\nand datasets. Here, we did not test on all the stan-\ndard metrics like MT-bench or GSM8k since our\nchoice of model and dataset are naturally not good\nat those benchmarks. After switching to more ad-\nvanced setups, we should conduct a more thorough\ninvestigation.\nFailure of using external resources Many ex-\nisting works employ externally trained models, es-\npecially some existing reward models. It is impor-\ntant to combine our methods with these external\nresources.\nCombine with existing iterative DPO methods\nAs mentioned in Sec. 2 and App. A, many existing\nworks assume a fixed reward/preference oracle and\nfocus on optimizing the algorithm by proposing\nnew loss functions or adding an extra exploratory\npolicy. These directions seem orthogonal to our\nmethods. It is important to combine our approach\nwith these to see whether our approaches are truly\nuniversally applicable to all those methods."}]}