{"title": "Run-Time Adaptation of Neural Beamforming for Robust Speech Dereverberation and Denoising", "authors": ["Yoto Fujita", "Aditya Arie Nugraha", "Diego Di Carlo", "Yoshiaki Bando", "Mathieu Fontaine", "Kazuyoshi Yoshii"], "abstract": "This paper describes speech enhancement for real-time automatic speech recognition (ASR) in real environments. A standard approach to this task is to use neural beamforming that can work efficiently in an online manner. It estimates the masks of clean dry speech from a noisy echoic mixture spectrogram with a deep neural network (DNN) and then computes a enhancement filter used for beamforming. The performance of such a supervised approach, however, is drastically degraded under mismatched conditions. This calls for run-time adaptation of the DNN. Although the ground-truth speech spectrogram required for adaptation is not available at run time, blind dereverberation and separation methods such as weighted prediction error (WPE) and fast multichannel nonnegative matrix factorization (FastMNMF) can be used for generating pseudo ground-truth data from a mixture. Based on this idea, a prior work proposed a dual-process system based on a cascade of WPE and minimum variance distortionless response (MVDR) beamforming asynchronously fine-tuned by block-online FastMNMF. To integrate the dereverberation capability into neural beamforming and make it fine-tunable at run time, we propose to use weighted power minimization distortionless response (WPD) beamforming, a unified version of WPE and minimum power distortionless response (MPDR), whose joint dereverberation and denoising filter is estimated using a DNN. We evaluated the impact of run-time adaptation under various conditions with different numbers of speakers, reverberation times, and signal-to-noise ratios (SNRs).", "sections": [{"title": "I. INTRODUCTION", "content": "Robust speech enhancement is a key technique in practical automatic speech recognition (ASR) systems that work in real time in real environments. For this purpose, one may use blind source separation (BSS) methods such as multichannel nonnegative matrix factorization (MNMF) [1], [2], independent low-rank matrix analysis (ILRMA) [3], and FastMNMF [4], [5]. Among these, FastMNMF is a state-of-the-art method that has been shown to outperform MNMF and ILRMA [5]. These methods are based on unsupervised learning (maximum likelihood estimation) of probabilistic models of mixture signals and are thus essentially free from the condition mismatch problem of supervised learning methods. However, these methods are hard to use for real-time systems due to the computationally demanding iterative optimization required at run time.\nIn recent years, deep neural networks (DNNs) have widely been used for speech enhancement. This approach, for example, performs direct mapping from a noisy mixture into multiple speech sources [6], mixture-conditioned deep speech generation [7], [8] for single-channel data, and neural beamforming with a DNN-based mask estimator [9] for multichannel data. In general, while supervised training of a DNN is computationally demanding, inference with the DNN works fast even on edge devices [10]. Considering the low-latency and high-performance nature and the potential contribution to the ASR [11], we focus on DNN-based beamforming (a.k.a. neural beamforming) as a front end of real-time distant ASR.\nIn neural beamforming, a DNN is used for estimating speech masks in the time-frequency domain. To separate clean speech from noisy speech mixture, the spatial covariance matrices (SCMs) of speech and noise are computed from the estimated masks and a enhancement filter used for beamforming is computed from the SCMs. The DNN is trained in a supervised manner by using pairs of noisy mixture and clean speech, often with the directions of the target speakers [10], [12]. However, it is not realistic to collect training data covering diverse acoustic environments that the model is potentially applied to. This makes the model less robust to unseen acoustic environments.\nA promising solution to this problem is a run-time adaptation of neural beamforming [10], [13]. The biggest challenge in this task is that no ground-truth data (clean speech) are available unlike in standard offline benchmarks. To solve this problem, one can fine-tune a DNN-based mask estimator using pseudo ground-truth data given by FastMNMF. In a dual-process system [10], a light-weight minimum variance distortionless response (MVDR) beamforming (front end) is used for streaming speech enhancement, where the mask estimator is fine-tuned with the target speech separated by asynchronously-running FastMNMF (back end), often with the direction of the target speaker. It has been shown that the ASR performance tends to improve along with the amount of fine-tuning data (e.g., multi-party conversation data) [10]. This system also uses weighted prediction error (WPE) [14], [15], a popular blind dereverberation method, before MVDR beamforming and FastMNMF for improved ASR. However, since the adaptation is only applied to the mask estimator for beamforming, the adaptation capability of this system is thus limited to MVDR beamforming only.\nIn this paper, we propose run-time adaptation of neural beam-forming for joint speech dereverberation and denoising. Since WPE works stably in various environments thanks to the unsupervised nature [10], we aim to draw its full potential with its neural extension. Specifically, we use weighted power minimization distortionless response (WPD) beamforming [16], [17], a unified version of WPE and a minimum power distortionless response (MPDR), where a DNN-based mask estimator is used to estimate a unified dereverberation and denoising filter. Both the speech dereverberation and denoising functions of the system can be adapted to a test environment while considering the mutual dependency of both tasks. We comprehensively investigate acoustic conditions in which the adaptation effectively contributes to the improvement of speech enhancement and ASR."}, {"title": "II. RELATED WORK", "content": "This section reviews speech dereverberation based on WPE [14], [15], speech enhancement based on MPDR beamforming, joint dereverberation and denoising based on WPD beamforming [16], [17], and BSS based on FastMNMF [5]."}, {"title": "A. Dereverberation", "content": "WPE is a well-known blind dereverberation method based on an autoregressive model of late reverberation. Let $x_{ft} \\in \\mathbb{C}^{M}$ be the short-time Fourier transform (STFT) spectrum of an observed mixture captured by an M-channel microphone array at frequency $f \\in [1, F]$ and time frame $t \\in [1, T]$, where F is the number of frequency bins and T is the number of frames. We assume $x_{ft}$ can be decomposed as follows:\n$x_{ft} = d_{ft} + r_{ft}$, (1)\nwhere $d_{ft} \\in \\mathbb{C}^{M}$ is direct signals with early reflections, $r_{ft} \\in \\mathbb{C}^{M}$ is the spectrum of late reverberation. The late reverberation is assumed to be the weighted sum of past observations as follows:\n$r_{ft} = \\sum_{\\tau=b}^{L} W_{f\\tau} x_{f,t-\\tau}$, (2)\nwhere $W_{f\\tau} \\in \\mathbb{C}^{M \\times M}$ is a mixing filter for delay $\\tau$, L is a tap length, and b is a prediction delay representing the boundary between the early reflections and late reverberation. The target $d_{ft}$ is thus given by\n$d_{ft} = x_{ft} - \\sum_{\\tau=b}^{L} W_{f\\tau} x_{f,t-\\tau}$. (3)\nThe filter is estimated by minimizing the weighted power of the estimated direct signal as follows:\n$W_f = \\underset{W_f}{\\text{argmin}} \\mathbb{E}_t \\left[ \\frac{\\left| \\left| x_{ft} - \\sum_{\\tau=b}^{L} W_{f\\tau} x_{f,t-\\tau} \\right| \\right|^2}{\\sigma_{ft}} \\right]$, (4)\nwhere $\\sigma_{ft} = \\mathbb{E}_t [f_{ft} f_{ft}^{H}]$ represents the time-varying power spectral density (PSD) of the target speech. The PSD can be obtained through iterative updates of the estimated target speech and its power [18], or by source mask estimation using a DNN [19]."}, {"title": "B. Speech Enhancement", "content": "The multichannel signal model in (1) can be rewritten by decomposing a target speech signal as the product of a steering vector $a_f \\in \\mathbb{C}^{M}$ and a source $s_{ft} \\in \\mathbb{C}$, while considering as noise other components including non-target components, early reflections, and late reverberations as follows:\n$x_{ft} = a_f s_{ft} + n_{ft}$, (5)\nwhere $n_{ft}$ is the spectrum of noise. The target speech $\\hat{a}_{ft}$ is estimated by applying an enhancement filter $w_{f0} \\in \\mathbb{C}^{M}$ to the mixture $x_{ft}$ as follows:\n$\\hat{a}_{ft} = w_{f0} x_{ft}$. (6)\nIn MPDR beamforming [20], the filter is estimated by minimizing the power of the observed mixture $x_{ft}$, while maintaining a distortionless response in the direction of the steering vector $a_f$ as follows:\n$w_{f0}^{MPDR} = \\underset{w_{f0}}{\\text{argmin}} \\mathbb{E}_t [\\left| w_{f0}^{H} x_{ft} \\right|^2] \\quad \\text{s.t.} \\quad w_{f0}^{H} a_f = 1$. (7)\nThe closed-form solution of the optimal filter is given by\n$w_{f0} = \\frac{K_f^{-1} a_f}{a_f^{H} K_f^{-1} a_f}$, (8)\nwhere $K_f = \\mathbb{E}_t [x_{ft} x_{ft}^{H}]$ is the SCM of the mixture."}, {"title": "C. Joint Speech Dereverberation and Denoising", "content": "WPD beamforming is formulated by integrating WPE and MPDR beamforming for jointly dereverberation and enhancement. Specifiacally, using (3) and (6), the signal obtained with WPD beamforming is given by\n$\\hat{a}_{ft} = w_{ft}^{H} \\left( x_{ft} - \\sum_{\\tau=b}^{L} W_{f\\tau} x_{f,t-\\tau} \\right) = w_{ft}^{H} \\mathbf{x}_{ft}$, (9)\nwhere $w_{ft} \\in \\mathbb{C}^{(L-b+1)M}$ is an integrated filter consisting of $\\{w_{ft}\\} \\tau=0,b,...,L$, and $\\mathbf{x}_{ft} \\in \\mathbb{C}^{(L-b+1)M}$ is the concatenation of the current and past observations $\\{x_{f,t-\\tau}\\} \\tau=0,b,...,L$."}, {"title": "III. PROPOSED METHOD", "content": "This section describes the proposed adaptive joint dereverberation and denoising system based on a dual-process architecture consisting of mask-based WPD beamforming with FastMNMF-guided fine-tuning. This system uses the WPD beamforming to perform low-latency speech dereverberation and denoising, resulting in a single-channel speech signal useful for the ASR system. To be adaptive to dynamic environments, its DNN-based mask estimator is fine-tuned at run time using speech signals dereverberated and separated with high-latency yet environment-robust WPE and FastMNMF."}, {"title": "A. Joint Neural Speech Dereverberation and Denoising", "content": "Given a mixture spectrogram $X = \\{x_{ft} \\in \\mathbb{C}^{M}\\} \\substack{F,T \\ f=1,t=1}$ with target speaker DOAs $\\phi \\approx \\{\\phi_l \\in [0,2\\pi]\\}\\{l=1}$, a DNN $\\mathcal{F}_{\\Theta}$ parameterized by $\\Theta$ is used for estimating TF masks $\\omega = \\{\\omega_{ft}\\} \\substack{F,T \\ f=1,t=1}$ as follows:\n$\\omega = \\mathcal{F}_{\\Theta} (X, \\phi)$. (18)\nThe PSD $\\sigma_{ft}$ and SCM $R_f$ of the target speech can be computed using the mask estimate as follows:\n$\\sigma_{ft} = \\frac{1}{M} \\sum_{m=1}^{M} |w_{ft} x_{ftm}|^2$, (19)\n$R_f = \\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{s}_{ft} \\mathbf{s}_{ft}^H$, (20)\nwhere $\\mathbf{s}_{ft} = \\left[ w_{ft} x_{ft}, w_{f,t-b} x_{f,t-b}, ..., w_{f,t-L} x_{f,t-L} \\right] \\in \\mathbb{C}^{(L-b+1)M}$. The WPD filter $W_f^{WPD}$ is then computed from $K_f = \\mathbb{E}_t \\left[ \\mathbf{x}_{ft} \\mathbf{x}_{ft}^H \\right]$ and $R_f$ as in (12). Finally, the target speech signal $\\hat{d}_{ft}$ corresponding to the DOA $\\phi_l$ is obtained by applying $W_f^{WPD}$ to $\\mathbf{x}_{ft}$ as in (9)."}, {"title": "B. Pretraining of Mask Estimator", "content": "The DNN-based mask estimator $\\mathcal{F}_{\\Theta}$ is pretrained using triples consisting of an M-channel mixture, a reference speech, and a target DOA. It is optimized to minimize the negative signal-to-distortion ratio (SDR) between the estimated time-domain speech signal $\\hat{d} \\in \\mathbb{R}^{S}$ and the reference time-domain speech signal $d_{ref} \\in \\mathbb{R}^{S}$ given by\n$\\mathcal{L} = -10 \\log_{10} \\left( \\frac{\\hat{d}^T \\hat{d}}{(\\hat{d} - d_{ref})^T (\\hat{d} - d_{ref})} \\right)$, (21)\nwhere S denotes the number of samples. The estimated time-domain speech signal $\\hat{d}$ is obtained by applying the inverse STFT to the estimated speech signal in the STFT domain $\\{\\hat{d}_{ft}\\} \\substack{F,T \\ f=1,t=1}$."}, {"title": "C. Run-Time Adaptation of Mask Estimator", "content": "To make the mask estimator $\\mathcal{F}_{\\Theta}$ adaptive to environmental changes, we fine-tune it at run time using triples of the observed mixture $\\{x_{ft}\\} \\substack{F',T' \\ f=1,t=1}$, the pseudo ground-truth speech signal $\\hat{d}_{ref} \\in \\mathbb{R}^{S}$, and the pseudo ground-truth DOA $\\phi_p$ to minimize the negative SDR loss in (21), where $F'$ and $T'$ are the number of frequency bins and that of frames of the mixture used for fine-tuning, respectively.\nTo obtain the pseudo ground-truth speech signal, we first dereverberate the mixture $x_{ft}$ using WPE as follows:\n$x_{ft}^{dry} = x_{ft} - \\sum_{\\tau=b}^{L} W_{f\\tau} x_{f,t-\\tau}$, (22)\nwhere $x_{ft}^{dry}$ is the dereverberated mixture and $W_{f\\tau}$ is a filter for delay $\\tau$. The filter is estimated as (4) with the PSD $\\sigma_{ft}$ obtained through iterative updates of the estimated dereverberated signal $x_{ft}^{dry} x_{ft}^{dry}$ and its power $\\sigma_{ft}$ [18]. Then, we separate the sources $\\{x_{nft}\\} \\substack{N \\ n=1}$ from the dereverberated mixture using FastMNMF as follows:\n$x_{nft} = (w_{nft}^{BSS})^H x_{ft}^{dry}$, (23)\nwhere the separation filter $w_{nft}^{BSS}$ is calculated as in (17). We measure the signal quality of each separated source signal using the reference-less non-intrusive scale-invariant SDR [23], [24]. We take $N'(\\leq N)$ separated signals that satisfy a predefined threshold $\\alpha$ and consider these as pseudo ground-truth speech signals $\\{\\hat{d}_{ref_n}\\} \\substack{N' \\ n=1}$. Finally, the corresponding $N'$ pseudo ground-truth DOAs $\\{\\phi_{p,n}\\} \\substack{N' \\ n=1}$ are estimated based on multiple signal classification (MUSIC) [25]."}, {"title": "IV. EVALUATION", "content": "We report a comprehensive evaluation conducted for assessing the performance of our adaptive system in various simulated acoustic environments."}, {"title": "A. Dataset", "content": "For the mask estimator $\\mathcal{F}_{\\Theta}$, we made a training dataset comprising 36,000 triples and a validation dataset comprising 3,600 triples. Each triple consisted of a 2-second 7-channel mixture signal, a 2-second 7-channel target speech signal, and the corresponding target DOA. Each mixture was composed of two speech signals by different speakers, who were stationary during the recording, and a diffuse noise signal. The speech signals were randomly taken from the training set (for the training dataset) and the development set (for the validation dataset) of Librispeech [26], while the diffuse or moving noise signals were randomly taken from the DEMAND dataset [27]. Both mixture and target speech signals were simulated using Pyroomacoustics [28] by considering a 7-channel circular microphone array, configured to match the geometry constraints of the DEMAND dataset, within a 2-dimensional room. The room length and width were randomly sampled from uniform distributions $\\mathcal{U}(7.6m, 8.4m)$ and $\\mathcal{U}(5.6m, 6.4m)$, respectively. The 2-dimensional coordinates of the array center were sampled from $\\mathcal{U}(3.6m, 4.4m)$ and $\\mathcal{U}(2.6m, 3.4m)$, respectively."}, {"title": "B. Experimental Settings", "content": "Both the front and back ends operated in the STFT domain. The STFT coefficients were computed using a window size of 1024 (F = 513) with a hop length of 256.\nFor the mask-based WPD beamforming, the prediction delay and the tap size of the convolutional filter were set to b = 3 and L = 8, respectively. The TF mask was estimated using a DNN as in [10]. The DNN was composed of a preprocessing network, a direction attractor network (DAN), and a bidirectional long short-term memory (BLSTM) network. The preprocessing network took as input the concatenation of the log magnitude of the mixture, the inter-channel phase difference, and the beamforming output by delay-sum beamforming, while the DAN took the target DOA as input. Given the outputs of these two networks, the BLSTM then estimates a TF mask. This mask estimator was pretrained on the training dataset using the AdamW optimizer with a learning rate of $10^{-4}$ and a batch size of 4. The model was trained for 20 epochs, and the model with the lowest validation negative SDR loss was selected for evaluation.\nFor the joint adaptation of mask-based WPD beamforming, the prediction delay, the tap size, and the number of iterations for parameter updates in WPE were set to b = 3 and L = 13, and 3, respectively. The number of sources, the number of bases, and the number of iterations for parameter updates in FastMNMF were set to N = 5, K = 16, and 200, respectively. The threshold for non-intrusive SI-SDR was set to $\\alpha$ = 10.0. The window size of the mixture given as the input to FastMNMF was set to 30 seconds. The learning rate was set to 4 \u00d7 10-5,"}, {"title": "C. Experimental Results", "content": "Figure 2 shows the evaluation results of the joint adaptation of dereverberation and denoising using mask-based WPD beamforming with different durations of fine-tuning data and various simulation settings. The upper left plot in the figure indicates that our adaptation method improved WER, SDR, STOI, and PESQ across different numbers of stationary speakers, which proves the effectiveness of our adaptation. These improvements seem to benefit from the robust separation capability of FastMNMF for stationary sources.\nHowever, when we used a large amount of fine-tuning data, the WER was slightly degraded as the RT60 increased or the SNR decreased. This would be because the pretrained mask-based WPD beamforming already has a strong capability of dereverberation, and FastMNMF is less robust to a mixture with moving sources. In contrast, when we used a small amount of fine-tuning data, the ASR performance hardly changed and the other metrics, STOI, PESQ, and SRMR improved in noisy or reverberant conditions. This suggests that 30 seconds of fine-tuning data is optimal for practical use."}, {"title": "V. CONCLUSION", "content": "This paper proposes a run-time adaptation method for the joint neural dereverberation and denoising with mask-based WPD beamforming using fine-tuning data obtained using WPE and FastMNMF. Evaluations showed robust improvements of the ASR performance across different numbers of stationary speakers, RT60s, and SNRs when we used a small amount of fine-tuning data. For future work, BSS methods that are capable of dealing with more various acoustic conditions should be investigated to improve the ASR performance even in noisy conditions with moving sources."}]}