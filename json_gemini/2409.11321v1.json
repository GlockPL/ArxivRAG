{"title": "SOAP: IMPROVING AND STABILIZING SHAMPOO USING ADAM", "authors": ["Nikhil Vyas", "Depen Morwani", "Rosie Zhao", "Itai Shapira", "David Brandfonbrener", "Lucas Janson", "Sham Kakade"], "abstract": "There is growing evidence of the effectiveness of Shampoo, a higher-order preconditioning method, over Adam in deep learning optimization tasks. However, Shampoo's drawbacks include additional hyperparameters and computational overhead when compared to Adam, which only updates running averages of first- and second-moment quantities. This work establishes a formal connection between Shampoo (implemented with the 1/2 power) and Adafactor a memory-efficient approximation of Adam showing that Shampoo is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to the design of a simpler and computationally efficient algorithm: Shampoo with Adam in the Preconditioner's eigenbasis (SOAP).\nWith regards to improving Shampoo's computational efficiency, the most straightforward approach would be to simply compute Shampoo's eigendecomposition less frequently. Unfortunately, as our empirical results show, this leads to performance degradation that worsens with this frequency. SOAP mitigates this degradation by continually updating the running average of the second moment, just as Adam does, but in the current (slowly changing) coordinate basis. Furthermore, since SOAP is equivalent to running Adam in a rotated space, it introduces only one additional hyperparameter (the preconditioning frequency) compared to Adam. We empirically evaluate SOAP on language model pre-training with 360m and 660m sized models. In the large batch regime, SOAP reduces the number of iterations by over 40% and wall clock time by over 35% compared to AdamW, with approximately 20% improvements in both metrics compared to Shampoo. An implementation of SOAP is available at https://github.com/nikhilvyas/SOAP.", "sections": [{"title": "1 INTRODUCTION", "content": "With ever-increasing costs of LLM training, optimization efficiency has become a central question in the field of deep learning. Several recent works have tackled this challenge by addressing both the memory (Zhao et al., 2024a; Wang et al., 2024) and compute (Anil et al., 2020) footprint of optimizers. In Algoperf (Dahl et al., 2023), a recent optimization efficiency benchmark, Shampoo (Gupta et al., 2018a), a second-order algorithm, outperformed all other submissions, including Adam (Kingma & Ba, 2015), reducing wall-clock time by 28%. Higher-order preconditioning has also been applied in large-scale training runs, such as Gemini-1.5 Flash (Gemini Team, 2024).\nThe success of Shampoo has drawn increasing attention from the deep learning community. Several works have explored ways to scale Shampoo by improving its memory and compute efficiency (Wang et al., 2024; Anil et al., 2020; Shi et al., 2023). Other research (Morwani et al., 2024) has examined the theoretical foundations of Shampoo and proposed minor adjustments (such as using power 1/2 rather than 1/4) that align with prior empirical findings (Anil et al., 2020). Moreover, Morwani et al. (2024) also showed that Shampoo with the aforementioned modifications is close to the optimal Kronecker approximation of the Adagrad (Duchi et al., 2011b) optimizer.\nOur first contribution is demonstrating that the variant of Shampoo proposed by Morwani et al. (2024) is equivalent\u00b9 to running Adafactor (Shazeer & Stern, 2018; Zhai et al., 2022) in the eigenbasis provided by Shampoo's preconditioner."}, {"title": "2 NOTATION AND BACKGROUND", "content": "We denote the weight matrix of a neural network layer by $W \\in \\mathbb{R}^{m \\times n}$, and the corresponding gradient by $G \\in \\mathbb{R}^{m \\times n}$. At a given time step $t$, these are denoted as $W_t$ and $G_t$, respectively. For a batch of inputs at time $t$, denoted by $B_t$, the loss and its gradient evaluated at $W_t$ are represented as $\\mathcal{B}_t(W_t)$ and $\\nabla_W \\mathcal{B}_t(W_t)$, respectively.\nAdagrad (Duchi et al., 2011b) is an online learning second-order algorithm that maintains a preconditioner $H \\in \\mathbb{R}^{mn \\times mn}$. If the vectorized gradient at time $t$ is denoted by $g_t$ (i.e., $g_t = vec(Gt) \\in \\mathbb{R}^{mn}$), then the update of the preconditioner and the vectorized weights $w_t \\in \\mathbb{R}^{mn}$ with learning rate $\\eta$ is given by\n$H_t = H_{t-1} + g_t g_t^T$; $W_t = W_{t-1} - \\eta H_t^{-1/2} g_t$.\nAdam (Kingma & Ba, 2015), a widely used first-order optimization algorithm in deep learning is a diagonal approximation of Adagrad. It maintains an exponential moving average of the gradients $G_t$ (denoted as $M_t$) and of element-wise squared gradients $G_t^2$ (denoted as $V_t$) for a given weight matrix $W$. Its update rule with learning rate $\\eta$ is given by\n$W_t = W_{t-1} - \\eta \\frac{M_t}{\\sqrt{V_t}}$,\nwhere the division is performed element-wise.\nAdafactor (Shazeer & Stern, 2018; Zhai et al., 2022), a variant of Adam, replaces $V_t$ with its best rank-1 approximation $V_t'$ to reduce memory usage. While the original Adafactor paper (Shazeer & Stern, 2018) proposed additional modifications, such as changes to the learning rate schedule, we focus on the version of Adafactor proposed in recent works (Zhai et al., 2022; Zhao et al., 2024c), whose update with learning rate $\\eta$ is given by\n$W_t = W_{t-1} - \\eta \\frac{M_t}{\\sqrt{V_t'}}$.\nShampoo (Gupta et al., 2018b) is a second-order optimization algorithm that approximates Adagrad and maintains two preconditioners, $L_t \\in \\mathbb{R}^{m \\times m}$ and $R_t \\in \\mathbb{R}^{n \\times n}$, for a given weight matrix $W \\in \\mathbb{R}^{m \\times n}$. The updates for the preconditioners and the weights with learning rate $\\eta$ are as follows:\n$L_t \\leftarrow L_{t-1} + G_t G_t^T$; $R_t \\leftarrow R_{t-1} + G_t^T G_t$; $W_t \\leftarrow W_{t-1} - \\eta L_t^{-1/4} G_t R_t^{-1/4}$.\nIn practice, Shampoo is implemented with several other modifications such as layerwise learning rate grafting and exponents other than -1/4. We will use the DistributedShampoo (Shi et al., 2023) implementation which has these variations available as hyperparameters."}, {"title": "3 RELATED WORK", "content": "We begin by discussing works that are closely related, including George et al. (2018); Anil et al. (2020) and Zhao et al. (2024a). Subsequently, we cover extended related works.\nKFAC (Martens & Grosse, 2015) is a well-known second-order optimization algorithm designed for neural networks. E-KFAC (George et al., 2018) builds upon KFAC in a manner analogous to our extension of Shampoo, introducing a diagonal preconditioner that is updated between KFAC inversion steps. However, E-KFAC's algorithm is not identical to running Adam in KFAC's eigenbasis, as the diagonal preconditioner is not Adam.\nAnil et al. (2020) introduced several algorithmic and numerical improvements to develop a practical and scalable version of Shampoo (Gupta et al., 2018b). Notably, they empirically found that using an exponent of 1/2 outperforms the original exponent of 1/4 in Shampoo. Of particular interest to our work is Appendix B of Anil et al. (2020), where, inspired by E-KFAC, they describe an algorithm that is essentially equivalent to SOAP for 2D layers. However, no experiments were provided, and the authors claimed that unpublished experiments showed no empirical improvement over Shampoo. This discrepancy between our findings may be due to some of the implementation details of SOAP.\nGaLore (Zhao et al., 2024a) was recently proposed as a method to reduce Adam's memory footprint by maintaining momentum in a low-rank subspace derived from the singular value decomposition (SVD) of the gradients. Their algorithm's full-rank version bears similarity to ours, with some notable distinctions. Firstly, their projection subspace is"}, {"title": "4 ALGORITHM", "content": "We begin by describing an equivalence between Shampoo and running Adafactor in the eigenbasis of the Shampoo preconditioner. For simplicity we omit momentum but the equivalence also holds with momentum. For this equivalence we use Shampoo with the following modifications from the original Shampoo optimizer (Gupta et al., 2018b):\n1.  We use power 1/2 instead of power 1/4. This was already recommended in practical implementations (Anil et al., 2020; Shi et al., 2023) and a theoretical connection between optimal Kronecker approximation of Adagrad (Duchi et al., 2011b) preconditioner and Shampoo with power 1/2 was established in Morwani et al. (2024).\n2.  We also use the scalar correction to per layer learning rates described in Ren & Goldfarb (2021); Morwani et al. (2024).\n3.  Instead of the running average of $L$ and $R$ across time steps, we use dataset averages.\nWith these changes in place (first occurrence of these changes is highlighted in red in the algorithm below) we formally define the two algorithms whose equivalence we show in Algorithms 1 and 2."}, {"title": "4.1 THEORY", "content": null}, {"title": "5 EXPERIMENTAL METHODOLOGY", "content": "Hyperparameter tuning: We begin with hyperparameter values suggested by prior research for both AdamW and Distributed Shampoo (e.g., $\\beta_2 = 0.95$). Initially, we conduct a learning rate sweep to determine the optimal learning rate for each optimizer. Once the optimal learning rate is identified, we perform two-dimensional sweeps for each of the remaining hyperparameters, where we vary the selected hyperparameter alongside the learning rate. The purpose of these sweeps is to demonstrate that our default hyperparameter settings are near-optimal, disregarding potential interactions between two non-learning-rate hyperparameters. A detailed discussion of the hyperparameter sweeps is provided in Appendix A.\nThroughput Measurement: We evaluate the throughput of each optimizer by measuring the number of tokens processed per second. At present, we perform these measurements on a single H100 GPU and utilize gradient accumulation to accommodate large batch sizes. While this approach may seem to disadvantage AdamW\u2014 as the overhead of Shampoo/SOAP is compared against multiple gradient accumulation steps- it is important to note that the overhead of Shampoo/SOAP can be amortized across layers by distributing the updates across multiple GPUs. This technique is employed in the distributed implementation of Shampoo (Shi et al., 2023). A comprehensive comparison of distributed implementations of these algorithms is left to future work.\nEfficiency Benefits: Simply running SOAP for the same duration as Shampoo and AdamW cannot be directly used to calculate the efficiency benefit (in terms of training steps or wall-clock time) of using SOAP since we use a cosine schedule. Therefore, we run SOAP on .5, .625, .75 and .875 fraction of the training data and fit a scaling law of the form $a + bN^{-\\beta}$ through the final losses obtained, where $N$ represents the number of training points and $a$, $b$, $\\beta$ are the parameters of the fit. We show these points and the corresponding scaling laws obtained in Figure 2. This scaling law is then used to calculate the efficiency benefit in terms of training steps and wallclock time as shown in Figure 2. Here, the horizontal lines represent the final losses of AdamW and Shampoo."}, {"title": "6 LANGUAGE MODELING EXPERIMENTS", "content": "In this section we focus on empirically comparing AdamW, DistributedShampoo, and SOAP on language modeling tasks."}, {"title": "6.1 MEASURING EFFICIENCY BENEFITS", "content": "In Figure 1 (left and middle) and Figure 3 we show train loss curves of 360m and 660m models with 2m token batch size for AdamW, Shampoo, and SOAP, where SOAP outperforms the other two. To directly calculate the efficiency benefit of SOAP, we also run SOAP with cosine decay for a shorter lr schedule, as shown in Figures 1 and 3. This allows us to approximate the following efficiency benefits (when setting batch size to 2m and preconditioning frequency to 10): $\\geq$ 40% reduction in number of iterations and $\\geq$ 35% reduction in wall clock time as compared to AdamW; $\\approx$ 20% reduction in iterations and wall clock time as compared to Shampoo. Precise efficiency benefit calculations are presented in Figure 2(left and middle)."}, {"title": "6.2 EFFECT OF FREQUENCY OF FINDING EIGENVECTORS/INVERSE", "content": "In Figure 1 (right), we compare SOAP and Shampoo with respect to preconditioning frequency. We observe the following:\n*   For all frequencies we tried from 1 to 100, both optimizers outperform AdamW.\n*   At frequency 1, SOAP and Shampoo are quite close in performance.\n*   At higher frequencies, the performance of both SOAP and Shampoo degrades but SOAP's performance degrades significantly slower than Shampoo's."}, {"title": "6.3 EFFECT OF BATCH SIZE", "content": "In this section, we examine the impact of batch size on the performance of the Shampoo and SOAP optimizers. Specifically, we reduce the batch size by a factor of 8, from 2m to 256k. To maintain the same FLOPS overhead for the eigenvector decomposition steps as in the 2m setting, we increase the preconditioning frequency by a factor of 8, from 10 to 80. In Figure 4, we present the optimal runs for each optimizer. Our results show that SOAP consistently outperforms both Shampoo and AdamW, demonstrating a reduction of 25% or more in the number of iterations compared to AdamW, and approximately a 10% reduction compared to Shampoo. In Figure 2 (right), we show that SOAP also improves in wall-clock time by $\\geq$ 15% over AdamW and approximately 10% over Shampoo.\nNote that we present these results as a preliminary analysis for small batch size runs. It is quite likely that our increase in preconditioning frequency by a factor of 8 is not optimal and a better trade-off is achievable. Furthermore, the overhead of SOAP can likely be ameliorated by doing $L$ and $R$ updates in lower precision (instead of fp32)."}, {"title": "7 FURTHER EFFICIENCY IMPROVEMENTS", "content": "In this section, we discuss space and time complexity of SOAP and provide an overview of potential avenues for further space and compute efficiency improvements in SOAP."}, {"title": "7.1 ONE SIDED EIGENBASIS", "content": "As described in Section 3, Zhao et al. (2024a) have an algorithm similar to ours. One of the differences is that they only project the smaller side of the layer using the eigenbasis while using identity as the rotation matrix for the larger side i.e. if $m < n$ we set $Q_R = I_n$ in Algorithm 3 and if $m > n$ we set $Q_L = I_m$. Doing this leads to a reduction in space usage as well as reduction of optimizer time overhead, which is discussed in Sections 7.2.1 and 7.3.1.\nIn Figure 5, it is evident that the one-sided projection results in slightly reduced performance compared to the original SOAP optimizer. However, it still performs on par with, or marginally better than, Shampoo, while maintaining greater computational efficiency. Further investigation into the potential for these variants to surpass the computational efficiency of original SOAP optimizer is left for future work."}, {"title": "7.2 SPACE USAGE OF SOAP", "content": "For a $m \\times n$ matrix where $m > n$ we require\n$2m^2 \\text{ (for } L, Q_L) + 2n^2 \\text{ (for } R, Q_R) + 3mn \\text{ (for gradient, } M, V)$"}, {"title": "7.2.1 IMPROVING SPACE USAGE OF SOAP", "content": "The most direct way to reduce memory is using low precision to store the $L$, $R$, $Q_L$, $Q_R$, $V$ matrices, which is done by Dettmers et al. (2022); Wang et al. (2024). Orthogonal to the low precision approaches, there are two algorithmic approaches to improving the space usage of SOAP:\n*   Using Adafactor instead of Adam as the diagonal preconditioner after rotating by $Q_L$ and $Q_R$. This reduces the space usage by $mn$.\n*   Using one sided version of SOAP (Section 7.1). This reduces space usage from $2m^2 + 2n^2 + 3mn$ to $2 \\min(m, n)^2 + 3mn$.\n*   Combining these approaches yields space usage of $2 \\min(m, n)^2 + 2mn$.\nFor standard transformer architectures the last variant which combines the two approaches would yield less space usage overall compared to AdamW (which uses $3mn$).\nWe try these approaches in Figure 5. We observe that using Adafactor instead of AdamW yields very small reductions in performance while using one-sided preconditioner results in larger reductions. Nonetheless even after combining these two approaches the resulting optimizer outperforms AdamW while having a smaller space requirement than AdamW. Regarding space usage we also note that Adafactor (with momentum added back) itself utilizes only $2mn$ space usage and has been shown to perform comparable to AdamW for ViT training (Zhai et al., 2022) and for language model training (Zhao et al., 2024c). Further space reduction beyond Adafactor has been studied in the Adalomo (Lv et al., 2024a), GaLore (Zhao et al., 2024a), and AdaMeM (Vyas et al., 2024) papers."}, {"title": "7.3 TIME OVERHEAD OF SOAP", "content": "There are two types of overhead of Shampoo and SOAP over AdamW: the overhead per step and the overhead when changing the preconditioner (or for SOAP, the preconditioner's eigenbasis). Let us first analyze the first one. For SOAP per step for a layer of size $m \\times n$ we have an overhead of\n$m^3 \\text{ (updating } L) + n^3 \\text{ (updating } R) + (2m^2n + 2mn^2) \\text{ (projecting and projecting back on both sides)}.\nWe note that this is more than the overhead of Shampoo which is $m^3 + n^3 + m^2n + n^2m$. This can be observed in Figure 2 (bottom, right) but not in the other figures since there the second type of overhead is the dominant term.\nThe second type of overhead is due to changing the preconditioner for Shampoo (or for SOAP, preconditioner's eigenbasis i.e. $Q_L$ and $Q_R$). The DistributedShampoo (Shi et al., 2023) implementation of Shampoo uses a direct call to torch.linalg.eigh for this. Following Wang et al. (2024) we use Algorithm 4 which uses power iteration based approach which calls torch.linalg.qr. We note that torch.linalg.qr is faster than torch.linalg.eigh (Documentation, 2024). In Figure 6 (right) we see that using power iteration based approach (torch.linalg.qr) performs as well as fresh eigenvector decomposition (torch.linalg.eigh)."}, {"title": "Effect of frequency on overhead", "content": "In Figure 6 (left), we observe that the overhead decreases as the preconditioning frequency increases, i.e., the frequency of invoking Algorithm 4. If the only additional computation occurred in Algorithm 4, we would expect the overhead to scale as $1.0/\\text{(preconditioning frequency)}$, approaching zero. However, empirical results (Figure 6 left) show that the overhead approaches an asymptote greater than zero. This is attributable to the additional matrix multiplications required to update $L$, update $R$, project the gradient, and reproject the gradient (for each layer) in the optimizer. Currently, these operations are performed in float32; reducing the precision of these operations, as proposed in Wang et al. (2024), could lower this asymptote."}, {"title": "7.3.1 IMPROVING TIME OVERHEAD OF SOAP", "content": "The per step overhead of SOAP can be reduced by using low precision to store the $L$, $R$, $Q_L$, $Q_R$, $V$ matrices (Dettmers et al., 2022; Wang et al., 2024), which in turn will speed up computation done using these matrices. This approach cannot be used for reducing the overhead for the preconditioner update in popular deep learning frameworks such as Pytorch since torch.linalg.qr does not support precision lower than float32. Orthogonal to the low precision approach we can improve the per step time overhead of SOAP by the following algorithmic approaches:\n*   Using Adafactor instead of Adam (Section 7.2) as the diagonal preconditioner after rotating by $Q_L$ and $Q_R$. In this version of SOAP the overhead can be improved by from $m^3 + n^3 + 2m^2n + 2n^2m$ to $m^3 + n^3 + m^2n + n^2m + \\max(m, n)^2 \\min(m, n) + \\min(m, n)^3$ by merging the project and project back steps for the smaller dimension.\n*   Using one sided version of SOAP (Section 7.1). This reduces overhead from $m^3 + n^3 + 2m^2n + 2n^2m$ to $\\min(m, n)^3 + 2 \\min(m, n)^2 \\max(m, n)$.\n*   Combining these approaches yields an overhead of $\\min(m, n)^2 \\max(m, n) + 2 \\min(m, n)^3$\nUsing one-sided version also reduces the second type of overhead from a calls to torch.linalg.qr on a $m \\times m$ and a $n \\times n$ matrix to only a single call to $\\min(m, n) \\times \\min(m, n)$ matrix."}, {"title": "8 DISCUSSION AND FUTURE WORK", "content": "We study an optimizer called SOAP: ShampoO with Adam in the Preconditioner's eigenbasis. We show that SOAP outperforms both AdamW and Shampoo in language modeling tasks and show that it is more robust to changes in preconditioning frequency than Shampoo. For future work, we would like to explore further improvements to the design of SOAP, in particular, related to using lower precision for the preconditioners as well as a better distributed implementation. We would also like to explore the performance of SOAP on other domains such as vision."}, {"title": "A EXPERIMENTAL SETUP", "content": "Many aspects of our setup such as models are the same as in Zhao et al. (2024c). We will restate those details verbatim for completeness.\nWe train language models on C4 tokenized with the T5 tokenizer (Raffel et al., 2020) and report results in terms of validation loss."}, {"title": "A.1 SWEEPING OVER HYPERPARAMETERS", "content": "AdamW, 2m batch size: Starting from the default hyperparameters above we do the following sweeps:\n1.  We sweep over learning rate in {.1, .0316, .01, ..., 3.16e-4}.\n2.  (360m) We sweep over the cross product of best 3 learning rates and $\\beta_1 \\in \\{0.9, 0.95, 0.99\\}$.\n3.  (360m) We sweep over the cross product of best 3 learning rates and $\\beta_2 \\in \\{0.9, 0.95, 0.99\\}$.\nThe last two of the sweeps did not yield any benefit for the 360m model with 2m batch size hence we only sweep over learning rate for the 660m model with 2m batch size.\nDistributedShampoo, 2m batch size: Starting from the default hyperparameters above we do the following sweeps:\n1.  We sweep over learning rate in {.1, .0316, .01, ..., 3.16e-4}.\n2.  (360m) We sweep over over the cross product of best 3 learning rates from above and $\\epsilon_{shampoo} \\in \\{1e-11, 1e-12, 1e-13\\}$.\n3.  (360m) We sweep over over the cross product of best 3 learning rates from above and $\\beta_{shampoo} \\in \\{.9, .95, .975\\}$.\n4.  Let $\\epsilon_{1d}, \\epsilon_{2d}$ denote the exponents used in DistributedShampoo for 1D and 2D parameters respectively. We also sweep over the cross product of best 3 learning rates from above and $(\\epsilon_{1}, \\epsilon_{2d})$ in $\\{(2, 2), (2.5, 2.5), (3, 3), (2, 4)\\}$."}, {"title": "B GALORE", "content": "We tried GaLore for 210m model, and while it outperformed AdamW it performed worse than Shampoo. Hence we do not try GaLore for higher model sizes.\nHyperparameter sweeps: We did the following sweeps:\n1.  We swept the cross product over learning rate $(3.16e-4, 1e-3, 3.16e-3, 1e-2)$, preconditioning frequency (10, 50, 200), both sided and one sided versions. Frequency 200 had the best results matching the observation of Zhao et al. (2024a)."}]}