{"title": "Adaptive High-Frequency Transformer for Diverse Wildlife Re-Identification", "authors": ["Chenyue Li", "Shuoyi Chen", "Mang Ye"], "abstract": "Wildlife ReID involves utilizing visual technology to identify specific individuals of wild animals in different scenarios, holding significant importance for wildlife conservation, ecological research, and environmental monitoring. Existing wildlife ReID methods are predominantly tailored to specific species, exhibiting limited applicability. Although some approaches leverage extensively studied person ReID techniques, they struggle to address the unique challenges posed by wildlife. Therefore, in this paper, we present a unified, multi-species general framework for wildlife ReID. Given that high-frequency information is a consistent representation of unique features in various species, significantly aiding in identifying contours and details such as fur textures, we propose the Adaptive High-Frequency Transformer model with the goal of enhancing high-frequency information learning. To mitigate the inevitable high-frequency interference in the wilderness environment, we introduce an object-aware high-frequency selection strategy to adaptively capture more valuable high-frequency components. Notably, we unify the experimental settings of multiple wildlife datasets for ReID, achieving superior performance over state-of-the-art ReID methods. In domain generalization scenarios, our approach demonstrates robust generalization to unknown species.", "sections": [{"title": "1 Introduction", "content": "Wildlife Re-Identification (ReID) aims to accurately identify specific individual animals in images or videos captured at different time points or locations. In contrast to regular animal classification, wildlife ReID necessitates a more advanced level of differentiation among individuals within the same species. This task is crucial for monitoring the living conditions, migratory habits, and reproductive situations of targeted wild animals, with broad applications in the conservation of endangered species, ecological research, and livestock"}, {"title": "2 Related Works", "content": "Wildlife Re-Identification. Recently, the rapid development of deep learning has made convolutional neural networks (CNNs) widely adopted for wildlife re-identification, leveraging them for feature extraction and metric learning. While person ReID technology has reached a mature state, wildlife ReID remains in an early stage. Many of them are specific to particular species, restricting their applicability. We can divide them into the following categories. (1) Global Feature Learning: Building on traditional person ReID methods, many approaches use whole animal images to extract distinctive features. Wang et al. developed a multi-stream feature fusion network aimed at extracting and integrating both local and global features of giant pandas. For manta ray ReID, where distinctive patterns vary unpredictably, Moskvyak et al. proposed a specialized loss to reduce feature distance between different views of the same individual, enhancing pose-invariant feature learning. Such methods do not require prior knowledge of specific species when designing feature extraction mechanisms, making them particularly suitable for practical applications across a diverse range of species. However, existing methods primarily rely on traditional person ReID technologies. (2) Species-specific Feature Extraction: Some wildlife have distinctive patterns, and these methods crop these specific pattern areas for local identification. These methods are adept at extracting discriminative features from specific body parts in a range of animal species, such as dolphin fins , the heads of cattle, tail features of whales , elephant ears , the pelage patterns of ringed seals , etc. However, in practical applications, significant variations in perspective and partial occlusions often occur, making it challenging to capture clear local features for each individual, resulting in unreliable images. These methods typically face difficulties in being generalized to animals of other species. (3) Auxiliary Information Integration: Li et al. leveraged pose key point estimation outcomes to segment tiger images into seven components, facilitating local feature learning. Following this, Zhang et al. adopted a simplified pose definition for either side of a yak's head, serving as an auxiliary supervisory signal to improve feature learning. Yet, these approaches require supplementary annotations and depend on disparate auxiliary information specific to various animals, making it challenging to apply a standardized method across multiple species.\nTransformer Based Object ReID. The extensively studied realms of person and vehicle ReID have long been dominated by CNN-based methods, but the"}, {"title": "3 Method", "content": "In this section, the specific details of the proposed method are presented. As shown in Fig.2, our adaptive high-frequency Transformer, with the core objective of enhancing high-frequency feature learning, incorporates three strategies."}, {"title": "3.1 ViT ReID Baseline.", "content": "Our model is built on the ReID baseline with vision transformer as backbone. Given an image $I \\in R^{H \\times W \\times C}$, with height H, width W, and C channels. The ViT model divides the image into N fixed-size patches. These patches are then reshaped into a sequence of flattened vectors. The patches are linearly transformed into a D-dimensional embedding space. A learnable embedding, class token, is appended to this sequence for the purpose of capturing a global representation of the image, resulting in the sequence $X \\in R^{(N+1) \\times D}$. To capture the spatial information of the patches, positional embeddings $E_{pos}$ are introduced and combined with the patch embeddings, yielding the input $X + E_{pos}$, where $E_{pos} \\in R^{(N+1) \\times D}$. The final input to the transformer's encoder is thus a combination of patch embeddings, positional embeddings, and the class token. To optimize the model's parameters, a combination of loss functions is employed. After learning, the class token is further used as a global feature representation, denoted as c. The triplet loss, denoted by $L_{tri}$, and the ID loss, denoted by $L_{ID}$, are integral to the network optimization process for ReID tasks."}, {"title": "3.2 Adaptive High-Frequency Transformer", "content": "Frequency-Domain Mixed Augmentation. For wildlife, high-frequency details such as fur patterns and small spots may be unstable due to variations in lighting and posture. In this part, we propose a data augmentation strategy named Frequency-Domain Mixed Augmentation(FMA), aimed at enhancing the robustness of models to simulate the detail changes caused by environmental factors such as seasonal fur variations or occlusions by mud, allowing the model to focus more on stable and essential features of the images. In brief, we transform the spatial representation of an image into a frequency domain and extract high-frequency information to obtain a representation dominated by high frequencies. The frequency domain representation of the original image is mixed with the high-frequency representation, thereby generating an augmented representation. Operating in the frequency domain is motivated by the recognition"}, {"title": "High-Frequency Information Extraction.", "content": "We first transform an input image $I \\in R^{H \\times W \\times C}$ into a single-channel image; each pixel in the image is located at coordinates (x,y) with a value I(x, y). The Fourier transformation is utilized to convert the spatial representation of the image into the frequency domain F(I):\n\n$F(I) (u, v) = \\Sigma_{x=0}^{H-1}\\Sigma_{y=0}^{W-1}I(x,y)e^{-j2\\pi(\\frac{ux}{H}+\\frac{vy}{W})}$."}, {"title": "Frequency-Domain Mixing.", "content": "The FMA method blends the high-frequency components with the original image within the frequency domain, thereby sharpening the model's focus on stable features and improving its adaptability to environmental changes. We define a frequency mixing function, which randomly mixes $F_h(I)$ with $F(I)$:\n\n$F^\\sim(I) = (1 - M_a) \\cdot F_h(I) + M_a \\cdot F(I)$.\n\n$M_a$ is a matrix of the same size as $F_h(I)$ and $F(I)$, with a randomly selected square area covering a(randomly ranging from 0 to 0.5) proportion of the total area set to 1, and the rest set to 0. The inverse Fourier transform of $F^\\sim(I)$ provides the augmented image for model training, to enhance feature stability recognition. Through this augmentation process, our model is more robust to the inevitable environmental factors of wildlife. The augmented high-frequency representation, denoted by $I_h$ and serving as the input high-frequency representation, typically represents finer details and edges within the image. $I_h$ is derived by converting the augmented frequency domain representation $F^\\sim(I)$ back to the spatial domain. This inverse transformation is given by:\n\n$I_h(x,y) = \\frac{1}{HW}\\Sigma_{u=0}^{H-1}\\Sigma_{v=0}^{W-1}F^\\sim (I) (u, v) e^{j2\\pi(\\frac{ux}{H}+\\frac{vy}{W})}$."}, {"title": "Object-aware Dynamic Selection.", "content": "High-frequency information reflects distinct discriminative features in images of various wild animals, demonstrating the versatility of high-frequency information across multiple species. However, the complex backgrounds in natural environments also fall under high-frequency information. Directly leveraging the high-frequency information of input images"}, {"title": "3.3 Feature Equilibrium Loss", "content": "Our model simultaneously takes visual image inputs and high-frequency augmented inputs, both of which are crucial for discriminative feature learning. The strategies we proposed above primarily guide the model to focus on high-frequency information. However, this needs to be established without compromising the learning of original visual information. Therefore, we further introduce the feature equilibrium loss to constrain the high-frequency features and visual features of the same individual from deviating excessively in the feature space.\nConsider $f^h \\in R^{B \\times Z \\times D}$ denote the encoded high-frequency embeddings excluding class tokens, and $f^o \\in R^{B \\times Z \\times D}$ represent the encoded embeddings of the original sequence corresponding to $f^h$. The proposed $L_F$ aims to minimize the discrepancy between these two sets of embeddings, ensuring the retention of vital domain-specific features in the transformed embeddings. Specifically, the feature equilibrium loss is defined as:\n\n$L_F = \\frac{1}{B} \\Sigma_{b=1}^{B} \\Sigma_{z=1}^{Z} \\left\\| f_{b, z}^o, f_{b, z}^h \\right\\|_F,$\n\n$\\left\\| f_{b, z}^o, f_{b, z}^h \\right\\|_F$ represents the difference between the high-frequency feature $f_{b,z}^h$ and the original feature $f_{b,z}^o$, for the z-th token in the b-th input, detailed as:\n\n$\\left\\| f_{b, z}^o, f_{b, z}^h \\right\\| = \\begin{cases} 0.5 \\left( f_{b, z}^o - f_{b, z}^h \\right)^2, & \\text{if } \\left| f_{b, z}^o - f_{b, z}^h \\right| < 1, \\\\ \\left| f_{b, z}^o - f_{b, z}^h \\right| - 0.5, & \\text{otherwise.} \\end{cases}$\n\nFeature equilibrium loss aggregates the differences across all selected tokens, ensuring a comprehensive measure of the discrepancy between the high-frequency and original features for each token. By minimizing $L_F$, we encourage the model to preserve the essential features of the original input, while still leveraging the detailed textures and patterns enhanced in the high-frequency components, to ensure that the model learning does not overemphasize the high-frequency details at the expense of the original feature. This balance maintains visual and spatial consistency with the original feature while emphasizing high-frequency feature, thus improving the overall efficacy of feature extraction. The final loss function is denoted as:\n\n$L_{overall} = L + \\lambda L_F,$\n\nwhere $\\lambda$ represents the weight of $L_F$."}, {"title": "4 Experiments", "content": "Datasets. Existing studies for wildlife ReID exhibit inconsistent experimental settings regarding dataset partitioning, making it challenging to conduct fair comparisons between different approaches. Some methods only offer raw datasets without partition. In addition, most datasets are partitioned with varying standards, such as identity overlap between training and testing sets in some cases and no overlap in others. To facilitate subsequent research in this field and provide a standardized data benchmark for related studies, we have divided the data into training and testing sets in a uniform manner, allocating 70% of the identities for the training set and the remaining 30% for the testing set, while ensuring no overlap between the identities in the training and testing sets. To test the universality of our method, We endeavored to encompass a diverse array of animal datasets, including species such as giant pandas [40], elephants [20], seals [32], giraffes [36], sharks [13], tigers [23], and pigeons [21]. During the testing phase, each image in the test set is treated as a query, with the remainder of the test set, excluding the query image, forming the gallery.\nEvaluation Metrics. In ReID tasks, two commonly used metrics are Cumulative Matching Characteristics (CMC) and mean Average Precision (mAP). We employed both CMC and mAP for evaluation. Notably, in wildlife ReID, most datasets lack explicit camera information, hence we include all correct matches in our evaluations. Given that in many instances, simple samples and minor viewpoint changes lead to higher Rank-k accuracy, we introduce a new metric, the mean Inverse Negative Penalty (mINP) [48], which reflects the accuracy of identifying the most challenging matches.\nImplementation Details. All of our experiments are conducted on PyTorch with Nvidia 3090 GPUs. We use the pre-trained vision transformer on imageNet-1K as the backbone. All images are resized to 256 \u00d7 256 and undergo data augmentation during training, which includes random rotations of 15 degrees, random adjustments to brightness and contrast with a 50% probability each, and padding of 10 pixels. We configure \u03bc to be 0.5 and \u03bb to be 0.1. The patch size is set to 16 \u00d7 16. During training, the SGD optimizer is used. The initial learning rate is 0.001, employing cosine learning rate decay. The training is conducted over 150 epochs with a batch size of 32, comprising 8 identities, each with 4 images. During the testing phase, distance matrices are computed using only the original features.\nThe existing wildlife ReID experiments mostly employ different settings and do not provide publicly clear dataset divisions, making comparison impossible. Therefore, we compare our approach with the current state-of-the-art person ReID methods on our own divided dataset. Our model significantly outperforms existing ReID models based on CNN and ViT architectures, as shown in Table.1. To demonstrate the universality of our model, we evaluate it on multiple wildlife datasets, including terrestrial, aquatic, and aerial species. Among these, species like elephants and sharks, which lack distinct pattern features, require emphasis"}, {"title": "4.2 Ablation Studies", "content": "Ablation studies are performed on the panda and pigeon datasets to validate the effectiveness of our method. We first compare it with the ViT baselines, described in Sec.3.1. Then, we sequentially added our core designs to the baseline to test the improvements introduced by our design, as shown in Table.4."}, {"title": "Pure High-Frequency Augmentation.", "content": "To validate the effectiveness of enhancing high-frequency feature learning for wildlife ReID, a straightforward approach involves directly extracting high-frequency information from the images for augmentation. We conducted relevant experiments in this part. In Table.4, Pure High-Frequency Augmentation refers to training exclusively with the high-frequency information of the original images without undergoing our frequency-domain mixed augmentation operations. Compared to the baseline, our method"}, {"title": "The Effectiveness of Our Methods.", "content": "Frequency-Domain Mixed Augmentation. We conducted experiments to verify that our frequency domain mixing augmentation enhances model robustness. This approach emphasizes the model's reliance on stable features, such as general body shape, while maintaining sensitivity to high-frequency details, despite environmental factors that may obscure high-frequency information. Experimental results show a 0.9% increase in mAP and a 0.2% increase in mINP on the panda dataset compared to Pure High-Frequency Augmentation, with improvements also observed in the pigeon dataset. 2) Object-aware Dynamic Selection. To confirm that our Object-aware Dynamic Selection (ODS) method can more effectively learn high-frequency target information and reduce background interference, we continued testing with Frequency-Domain Mixed Augmentation. Experiments validating the effectiveness of ODS demonstrate increased mAP on multiple animal datasets compared to the Baseline. Additionally, visualized attention maps indicate that ODS enables the model to focus more on the target than both the Baseline and Pure High-Frequency Augmentation. Compared to PHA [51], which we have reproduced to closely resemble the source version, our ODS demonstrates superior performance. PHA amplifies uncertain local high-frequency features, which may lead to a bias toward background noise. In contrast, our method extracts object-aware high-frequency information and reduces background noise. 3) Feature Equilibrium Loss. We conducted experiments to compare the model's performance with and without feature equilibrium loss. Feature equilibrium loss balances original and high-frequency features, reducing their disparity. The results show that this balance enhances nuanced consistency."}, {"title": "Parameter Analysis.", "content": "We conduct a thorough evaluation of the effects exerted by the ratio \u03bc across several datasets and the feature equilibrium loss weight \u03bb on the panda dataset. The experimental outcomes depicted in Fig.3 elucidate the feature equilibrium loss performs better when assigning lower weights, with weight 0.1 the best. The optimal value of \u03bc varies across different datasets. Due to the absence of bounding boxes, the proportion of the target within the entire image varies across different datasets. This variation can lead to the selection of too few targets or excessive background when adjusting \u03bc."}, {"title": "Visualization Analysis.", "content": "Fig.4 exhibits the attention maps of the Baseline, Pure High-Frequency Augmentation(Pure HF), and our model. It is evident that the Baseline attention significantly surpasses that of Pure HF in terms of focus on the target object. Pure HF is considerably affected by background noise, resulting in less attention to the target. In contrast, our model is capable of not"}, {"title": "5 Conclusion", "content": "In this paper, we analyze the challenges unique to wildlife ReID compared to conventional ReID tasks and propose a versatile adaptive high-frequency Transformer architecture tailored to achieve effective performance across diverse wildlife species. Specifically, we propose enhancing feature learning by focusing on high-frequency information that can capture the distinct characteristics of various animals. Extensive experimental evaluations on different wildlife species demonstrate the state-of-the-art performance of our model in the ReID task. Experiments under domain generalization settings also showcase the generalization capability of our model to unknown species."}, {"title": "Limitations.", "content": "This part discusses the limitations of our method. It is somewhat influenced by the baseline choice due to its reliance on the dynamic selection process based on baseline attention, meaning poor baseline attention could lead to selecting high-frequency information with more background noise. Besides, the potential for variability in the optimal selection of the value of \u03bc across different datasets suggests that our approach may not achieve complete adaptability. In the future, we will attempt to design more flexible strategies to dynamically adjust the value of \u03bc according to the specific features of each dataset."}]}