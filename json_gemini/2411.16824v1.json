{"title": "Beyond Sight: Towards Cognitive Alignment in LVLM via Enriched Visual Knowledge", "authors": ["Yaqi Zhao", "Yuanyang Yin", "Lin Li", "Mingan Lin", "Victor Shea-Jay Huang", "Siwei Chen", "Weipeng Chen", "Baoqun Yin", "Zenan Zhou", "Wentao Zhang"], "abstract": "Does seeing always mean knowing? Large Vision- Language Models (LVLMs) integrate separately pre-trained vision and language components, often using CLIP-ViT as vision backbone. However, these models frequently en- counter a core issue of \"cognitive misalignment\" between the vision encoder (VE) and the large language model (LLM). Specifically, the VE's representation of visual in- formation may not fully align with LLM's cognitive frame- work, leading to a mismatch where visual features exceed the language model's interpretive range. To address this, we investigate how variations in VE representations influence LVLM comprehension, especially when the LLM faces VE- Unknown data\u2014images whose ambiguous visual represen- tations challenge the VE's interpretive precision. Accord- ingly, we construct a multi-granularity landmark dataset and systematically examine the impact of VE-Known and VE-Unknown data on interpretive abilities. Our results show that VE-Unknown data limits LVLM's capacity for accurate understanding, while VE-Known data, rich in distinctive features, helps reduce cognitive misalignment. Building on these insights, we propose Entity-Enhanced Cognitive Alignment (EECA), a method that employs multi-granularity supervision to generate visually enriched, well- aligned tokens that not only integrate within the LLM's embedding space but also align with the LLM's cognitive framework. This alignment markedly enhances LVLM per- formance in landmark recognition. Our findings underscore the challenges posed by VE-Unknown data and highlight the essential role of cognitive alignment in advancing mul- timodal systems.", "sections": [{"title": "1. Introduction", "content": "Large Vision Language Models (LVLMs) [6, 9, 15-17, 19, 22, 27, 41, 45] have recently achieved significant advance- ments. By mapping visual inputs into the embedding space of large language models, LVLMs harness the powerful interpretative capabilities of language models [1, 26, 36] to address complex tasks like visual question answering, grounding, counting, etc.\nDespite the impressive advancements in LVLMs, these models still struggle with fundamental recognition tasks. As illustrated in Figure 1, even state-of-the-art models like GPT-40 and Qwen2-VL fail to recognize iconic landmarks from images, although they can describe these landmarks accurately when prompted with text alone. This raises criti- cal questions: Why do these challenges persist? To what ex- tent do these models truly understand what they perceive? Drawing inspiration from [8], we attribute this issue to a broader problem that we call cognitive misalignment-a fundamental disconnect between the representations gener- ated by the vision encoder (VE) and the interpretive frame- work of the large language model (LLM).\nCurrently, most LVLMs [3, 6, 19] integrate separately pretrained vision [29, 34, 44] and language models [1, 28, 36] through different adapters [2, 16, 19] to achieve mul- timodal comprehension. However, this simple connection often results in fundamental misalignment, as recent studies suggest [37, 39, 43], which limits the potential of LVLMs. In this paper, we start by examining the cognitive frame- work of the vision encoder (VE) in LVLMs (Section 3). To systematically investigate this, we define evaluation metrics based on CLIP's training paradigm to assess VE knowledge. Using these metrics, we categorize data into VE-Known and VE-Unknown. Our study empirically demonstrates the fol- lowing insights:\n\u2022 Enhanced alignment with VE-Known data: VE- Known data, comprising images with rich and distinctive features, provides a strong, discriminative foundation that enhances the utilization of visual knowledge in LVLMs. This data enables smoother alignment between the vision encoder (VE) and the language model (LLM), allowing for more effective cognitive integration.\n\u2022 Challenges with VE-Unknown data: In contrast, VE- Unknown data, characterized by weak alignment with"}, {"title": "2. Preliminary", "content": "Notation. In typical Large Vision Language Models (LVLMs) [6, 7, 15, 18-20, 22, 38, 41, 42], an adapter is used to connect VE and LLM seamlessly. This adapter, de- noted by $g_{\\Theta}$, can be a simple linear layer or a more complex attention module. For clarity, we define the features out- put by the vision encoder as visual patches and the features after passing through the adapter as visual tokens.\nIn our framework, the vision encoder $f_v$ and text encoder $f_t$ are from CLIP's dual modules. The vision encoder $f_v$ processes an image $I_i$ to produce $V_i = f_v(I_i) \\in \\mathbb{R}^{P\\times d}$, where $d$ is the feature dimension and $P$ is the number of visual patches. The text encoder $f_t$ maps text $T_i$ to $t_i = f_t(T_i) \\in \\mathbb{R}^{d}$. During LVLM training, the LLM input is constructed as follows:\n$X_v = g_{\\Theta}(f_v(I_i)), X_t = \\phi(T_i),$  (1)\nwhere $g_{\\Theta}$ transforms the vision encoder's output to visual tokens $X_v \\in \\mathbb{R}^{N\\times C}$, and $\\phi$ maps text $T_i$ to text tokens $X_t \\in \\mathbb{R}^{N_t\\times C}$ in the LLM's embedding space. The visual tokens $X_v$ and text tokens $X_t$ are then concatenated and fed into the LLM to generate a response.\nCognitive misalignment: issues and challenges. We identified a critical issue in LVLMs, illustrated through the example in Figure 1, which reveals a phenomenon we term cognitive misalignment. This issue arises from discrepan- cies between Vision Encoder's representations and the lan- guage model's cognitive framework. For instance, when asked, \"Do you know the landmark Holy Trinity Church (Gornji Milanovac)? Please describe the landmark,\" the model provides an accurate description based on its tex- tual knowledge. However, when shown an image that corre- sponds to this description and asked, \"What is the location"}, {"title": "3. From Sight to Insight", "content": "In this section, we explore the cognitive alignment between the VE and the LLM within LVLMs. By constructing a fine-tuned dataset and evaluating visual knowledge across different levels, we aim to systematically analyze the quality of visual outputs and their impact on achieving cognitive alignment with the LLM's understanding.\n3.1. Study setup\nDataset construction. To explore cognitive alignment between Vision Encoders (VE) and large language mod- els (LLMs) in LVLMs, we use an entity-related dataset where both the visual and language models have their own distinct representations and understandings. This al- lows us to systematically examine how well visual out- puts align with the LLM's understanding space. In light of this, we construct a fine-tuning dataset, termed the Multi- Granularity Landmark Dataset (MGLD), comprising ap- proximately 200k samples from the GLDv2 [40] dataset, following the process illustrated in Figure 2 (detailed in Appendix B). In this section, we focus solely on image-text pairs ($I_i, T_i$) in Stage 1 and Q-A pairs ($q_i, a_i$) in Stage 2.\nSpecifically, for Stage 1, each landmark in the original dataset corresponds to multiple images, and we pair the landmark name $T_i$ with the image $I_i$ that has the highest CLIP similarity to its corresponding landmark names. This selection process ensures that each chosen image aligns closely with the landmark identity. In Stage 2, we generate Question-Answer (Q-A) pairs to support landmark recog- nition. We randomly select a question from a predefined set and provide it to GPT-40 along with the selected im- age and corresponding landmark name. The model then responds with answers describing the landmark. Then, the dataset used in this section can be denoted as $D = \\{(I_i, T_i, q_i, a_i)\\}_{i=1}^{N}$, where $I_i$ represents the image, $q_i$ is a question related to the landmark, and $a_i$ is the corre- sponding answer, including both image descriptions and location-related information. $T_i$ is the ground-truth land- mark name(e.g., \"Eiffel Tower\").\nMeasuring knowledge of vision encoder. We calculate the CLIP similarity between the visual representation of each image $I_i$ and all the landmark names $T_j$ (e.g., \"Eiffel Tower\") in the dataset, using the CLIP vision and text en- coders. The similarity, denoted as $Sim_{CLIP}(I_i, T_j)$, is com- puted as follows:\n$Sim_{CLIP}(I_i, T_j) = \\frac{(f_v(I_i), f_t(T_j))}{\\| f_v (I_i) \\| \\| f_t (T_j) \\|}$   (2)\nwhere $f_v(I_i)$ and $f_t(T_j)$ represent the visual and textual embeddings of the image and the landmark name, respec- tively.\nWe record the Similarity Score between the image $I_i$ and its corresponding ground-truth label $T_i$, denoted as"}, {"title": "3.2. Visual patterns of different knowledge", "content": "Firstly, we analyze the characteristics of different types of visual knowledge, focusing on how variations in visual representation affect cognitive alignment between VE and LLM. As shown in Figure 3, we used t-SNE to visualize the visual knowledge patterns extracted by the VE across distinct subsets. Specifically, we constructed three datasets: HDS-25k (top 25k samples with the highest $RSR^i$), HSS- 25k (top 25k samples with the highest $Sim_{CLIP}^i$), and LCS- 25k (25k samples with both low RSR and low $Sim_{CLIP}^i$). Additionally, Figure 4 shows the distribution of major cate- gories in each subset, offering insights into their visual class composition.\nThe t-SNE results reveal distinct patterns across these subsets. The HDS subset shows dispersed visual repre- sentations within categories, indicating that the visual en- coder captures fine-grained intra-class diversity, which en- hances subtle landmark recognition. Conversely, the HSS subset forms compact, well-separated clusters, suggesting a focus on broad, shared patterns that aid in distinguish-"}, {"title": "3.3. Uncovering the impact of visual knowledge", "content": "After analyzing the visual patterns of different knowledge extracted by the VE, we investigate a key question: how do the strength and quality of VE representations influence cognitive alignment with the LLM's understanding? To an- swer this, we evaluated models trained with various data selection strategies, starting from the LLaVA [19] initializa- tion and optimizing both the LLM and the adapter. Strong alignment-indicated by a higher proportion of recognized responses (sum of Strongly Known and Known) \u2014 suggests that visual features are both robust and supportive of cross- modal cognitive integration.\nTo measure improvement, we compared the proportion of recognized responses to the LLaVA baseline, focusing on the percentage increase in accurate landmark identifi- cations. Our findings, presented in Table 1 and Figure 5, reveal three insights:\n1. VE-Known data mitigates cognitive misalignment. As shown in Table 1, VE-Known subsets (HDS and HSS) consistently outperform BRS, especially with smaller datasets. The t-SNE analysis indicates that these subsets have richer, more discriminative visual features, facilitating smoother knowledge transfer from the VE to the LLM.\n2. VE-Unknown data exacerbates cognitive misalign- ment. The Low Clarity Set (LCS), which includes VE- Unknown instances with ambiguous representations, shows the lowest performance (see Table 1). While VE- Unknown data may contribute partially to LVLM learn- ing, its low confidence and information loss hinder effec-"}, {"title": "4. Open the Eyes of LVLM", "content": "After examining the cognitive framework of the VE, a natu- ral question arises: how can we fully \"open the eyes\u201d of the LVLM to achieve cognitive alignment between visual and language components? In this section, we take initial steps toward answering this question. First, we design a data an- notation pipeline to ensure consistency between the visual input and the language model's output (see Section 4.1). Building on this, we introduce the Entity-Enhanced Cog- nitive Alignment (EECA) framework, which supervises the adapter's visual tokens to retain richness and discriminative power, minimizing information loss (see Section 4.2). This approach encourages transformed visual tokens to \u201cmimic\" VE-Known representations, facilitating alignment with the LLM's cognitive framework."}, {"title": "4.1. Multi-granularity data annotation pipeline", "content": "To construct a landmark instruction dataset that enhances the model's recognition and differentiation capabilities, we designed a 3-stage data pipeline as shown in Figure 2. Stage 1 and 2 have been introduced in Section 3.1. Here, we focus on providing a detailed description of Stage 3.\nThe final stage, Multi-granularity data annotation, en- riches each landmark with hierarchical labels and unique entities, providing a layered structure that captures both broad classifications and fine-grained details. Firstly, each landmark is assigned a hierarchical label (e.g., \u201cmountain,\u201d \"lake,\u201d \u201cchurch,\u201d), placing it within a general category. In contrast, entities provides a finer level of detail, capturing landmark-specific attributes. For instance, the Eiffel Tower might be associated with entities like \u201ciron lattice structure\u201d and \u201ctriangular silhouette\u201d, which highlight its distinctive physical attributes. These entities are annotated based on both image content and QA pairs, making them directly rel- evant to the LLM's answer outputs.\nThe resulting dataset, which we denote as $D = \\{(I_j, q_j, a_j, h_j, e_j)\\}_{j=1}^{N}$, is structured such that each entry contains an image $I_j$, a question $q_j$, an answer $a_j$ incorpo- rating both descriptive and location-specific information, a hierarchical label $h_j$ representing the broader category, and unique entities $e_j$ detailing fine-grained, landmark-specific attributes. This dataset construction enables EECA to cap- ture the unique identity of each landmark through visual tokens, facilitating cognitive alignment between the vision encoder and the language model."}, {"title": "4.2. Entity-Enhanced cognitive alignment", "content": "Architecture overview. Within this enriched dataset, we train our model using EECA, as illustrated in Figure 6.\nEECA promotes cognitive alignment by supervising the transformation from visual patches to tokens, ensuring that the tokens retain rich, discriminative information linked to specific entities in MGLD. These entities are annotated to align with the LLM's cognitive framework, supporting cross-modal understanding.\nInspired by recent work [17, 32], we employ a dual- branch visual architecture to leverage high-resolution (HR) information, enhancing the richness of visual representa- tions. Starting with a high-resolution image $I_H$, we gen- erate a low-resolution version $I_L$ via bilinear interpolation. $I_H$ is divided into four sub-images, which, along with $I_L$, are fed to a shared vision encoder, producing low-resolution and high-resolution visual features, $V_L$ and $V_H$.\nIn the low-resolution branch, a 2-layer MLP adapter out- puts visual tokens $X_L \\in \\mathbb{R}^{N_{VL}\\times C}$. In the high-resolution branch, a shared perceiver resampler [2] generates high- resolution visual tokens $X_H \\in \\mathbb{R}^{N_{VH}\\times C}$. To make HR information efficient, EECA supervises the compression of HR features, aligning visual tokens with the LLM's em- bedding space. The LLM then integrates textual and vi- sual information for comprehensive understanding. The EECA framework is optimized with two loss functions: Entity-Aware Contrastive Loss and Hierarchical Classifica- tion Loss, detailed below.\nEntity-Aware contrastive loss. Given the concatenated visual tokens $X = [X_L; X_H]$, where $X_H = (X_{H}^1...,X_{H}^{N_{VH}}) \\in \\mathbb{R}^{N_{VH}\\times C}$ represents high-resolution tokens and $X_L$ represents low-resolution tokens, this loss function encourages the model to incorporate fine-grained, entity-specific details from $X$ into the primary represen- tation in $X$."}, {"title": "5. Experiments", "content": "In this section, we analyze the effectiveness of EECA on landmark recognition tasks. To evaluate the impact of our approach, we conduct experiments on different variations and data subsets, comparing performance across multiple configurations.\n5.1. Experimental setup\nOur experiments are conducted within the LLaVA-1.5 [19], using a dual-branch visual architecture for enhanced feature representation. We jointly optimize both the LLM and the two adapters within the EECA framework. Further exper- imental details, including hyperparameter configurations, are provided in Appendix A. For data preparation, we con- struct the dataset according to the process described in Fig- ure 2, creating subsets with varying data selection methods (See Section 3.1).\n5.2. Results and analysis\nEffectiveness of EECA. We evaluate EECA against three configurations. Baseline serves as a reference without entity prompts. Entity Prompt (Inference Only) includes entities only during inference, confirming that entity prompts do not negatively impact performance. Entity Prompt (Training + Inference) incorporates entities during both training and inference, yielding significantly higher accuracy and indi- cating alignment with the LLM's cognitive framework. As shown in Figure 5, with only 25k data, EECA reaches the performance of the 125k reference dataset, and with 50k data, it achieves the best results. EECA thus provides bal- anced improvements without extensive reliance on entity prompts during inference, demonstrating effectiveness in enhancing performance without overfitting. Overall, EECA offers a more efficient, focused approach, achieving strong alignment with the LLM's cognitive framework."}, {"title": "5.3. Generalizability of EECA", "content": "To validate EECA's generalizability, we applied it to three distinct data subsets representing different levels of visual knowledge. Results in Table 4 show that EECA consis- tently improves performance across all subsets, demonstrat- ing adaptability even in challenging VE-Unknown scenar- ios.\nFor LCS-25k (VE-Unknown data), adding the HR branch significantly boosts performance, indicating that en- riched visual information in the HR branch effectively re- duces visual ambiguity by enhancing feature richness and separability. In contrast, HDS-25k and HSS-25k (VE- Known data) achieve the largest gains with the entity-aware contrastive loss ($L_e$) and hierarchical classification loss ($C_h$), showing that VE-Known data benefit from targeted supervision to extract discriminative features.\nOverall, these results underscore EECA's robustness and adaptability, addressing visual ambiguity in VE-Unknown while enhancing feature discrimination in VE-Known."}, {"title": "6. Related Work", "content": "Large vision language models. The integration of vision and language models has advanced Large Vision Language"}, {"title": "7. Conclusion and Discussion", "content": "This study revisits the question: does seeing always mean knowing? In Large Vision Language Models (LVLMs), we find that this is often not the case. Our investigation reveals a critical cognitive misalignment between the vi- sion encoder (VE) and the large language model (LLM), where VE's visual representations do not fully align with the LLM's cognitive framework. Our results show that LVLM performance is closely tied to the knowledge within the VE: VE-Known data alleviate cognitive misalignment,"}]}