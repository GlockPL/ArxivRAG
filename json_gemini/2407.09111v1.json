{"title": "Inference Optimization of Foundation Models on Al Accelerators", "authors": ["Youngsuk Park", "Kailash Budhathoki", "Liangfu Chen", "Jonas K\u00fcbler", "Jiaji Huang", "Matth\u00e4us Kleindessner", "Jun Huan", "Volkan Cevher", "Yida Wang", "George Karypis"], "abstract": "Powerful foundation models, including large language models (LLMs), with Transformer architectures have ushered in a new era of Gener- ative AI across various industries. Industry and research community have witnessed a large number of new applications, based on those foundation models. Such applications include question and answer, customer services, image and video generation, and code comple- tions, among others. However, as the number of model parameters reaches to hundreds of billions, their deployment incurs prohibi- tive inference costs and high latency in real-world scenarios. As a result, the demand for cost-effective and fast inference using AI accelerators is ever more higher. To this end, our tutorial offers a comprehensive discussion on complementary inference optimiza- tion techniques using AI accelerators. Beginning with an overview of basic Transformer architectures and deep learning system frame- works, we deep dive into system optimization techniques for fast and memory-efficient attention computations and discuss how they can be implemented efficiently on AI accelerators. Next, we describe architectural elements that are key for fast transformer inference. Finally, we examine various model compression and fast decoding strategies in the same context.", "sections": [{"title": "OVERVIEW", "content": "The substantial size of modern large language models (LLMs), such as Llama-2/3 70B [109], Claude 3 Opus 137B [8], and Groq-1 314B [118], presents significant challenges in both training and in- ference phases. Training LLMs, in particular, demands considerable resources and has been the subject of extensive research. In con- trast, inference consumes fewer computational resources but occurs much more frequently once the model has been trained. This phase is crucial as it encompasses various applications where the value of LLMs is realized, including text translation, sentiment detection, code generation, text summarization, and question answering. Customers naturally demand faster and more cost-effective infer- ence. To meet the user demands, it is essential to reduce latency-the time required to complete a generation-and to increase through- put, which is the number of requests processed per unit of time. The latency and throughput of LLMs depend on multiple factors, such as the hardware utilized, the capability of software frameworks to op- timally leverage the available hardware, and the model architecture itself. Therefore, efforts to improve speed and costs benefit from optimizations across all these dimensions. To this end, this section provides an overview of the characteristics of LLM inference, along with the corresponding systems and hardware requirements."}, {"title": "LLM Inference", "content": "Transformer models have revolutionized the landscape of LLMs by introducing a highly effective architecture for natural language processing tasks, as shown in Figure 1 [112]. These models, charac- terized by their attention mechanisms, have significantly enhanced the capacity of models to understand and generate human-like text. Their versatility and scalability in training have established them as the backbone of many state-of-the-art LLMs today. Transformer models can include an encoding component only (e.g., BERT [27]), a decoding component only (e.g., GPT [13], Llama-2 [109], Claude 3 [8], Groq-1 [118], Mistral 7B [49]), or both (e.g., BART [65]). Cur- rently, modern LLMs predominantly employ a decoder-only archi- tecture, generating output sequences by predicting one token at a time, conditioned on the input sequence and previously generated tokens-a process known as auto-regression. Consequently, our discussion primarily focuses on decoder-only Transformer models."}, {"title": "Computational and Memory Requirements", "content": "Modern computer chips employ specialized tensor units to effi- ciently perform tensor computations, such as matrix multiplication, which are fundamental in large foundation model workloads. Ex- amples of these units include Nvidia TensorCore [86], AMD Ma- trixCore [4], and the systolic arrays found in Google TPU [50, 52] and AWS Trainium [14]. These tensor units are designed to process high-performance tensor computations such as matrix multiplica- tion to meet the extensive demands of LLM workloads, especially during the training phase. Inference tasks, however, present a distinct challenge, as pow- erful tensor units alone are insufficient for optimal performance. To address memory-bound during decoding process, modern chips incorporate high-bandwidth memory, typically in the form of Static Random Access Memory (SRAM). SRAM offers low latency and high throughput, suitable for the substantial memory requirements of inference workloads. However, the high cost of SRAM limits its capacity, requiring careful data manipulation to optimize its usage. High performance kernels. Inference-purposed kernels, such as DeepSpeed-Inference [6], FasterTransformer [84], and transformers- neuronx [11], adhere to these guidelines to efficiently process the workloads. They can be designed by experienced performance- tuning experts or generated by machine learning compilers. In either case, a deep understanding of both chip architecture and in-ference workloads is essential for efficiently mapping and schedul- ing computations onto the hardware. By leveraging this knowledge, these kernels can fully optimize the utilization of high-bandwidth memory and tensor units, ultimately enhancing the efficiency of inference workloads on modern computer chips. Hardware Accelerators. While the majority of the LLM workloads are now done on GPUs following the SIMT (single instruction, mul- tiple threads) paradigm, LLM inference actually can also be acceler- ated with systolic array and High Bandwidth Memory (HBM) based systems (e.g. Google TPUs [50, 52], AWS Trainium/Inferentia [14] and Intel Gaudi [41]) with lower power consumption and lower cost accordingly. Systolic array based systems can accelerate matrix multiplication with instruction-level parallelism [51]. To accelerate memory access speed of a large amount of data, HBM is used as a replacement of Double Data Rate (DDR) and careful memory planning is required as the capacity of HBM is limited compared to the model size [135]. There are also systems that utilize FPGAs [66] for compute acceleration, and systems that utilize inter-node con- nectivity [137] for large-scale transformer inference. Techniques to Mitigate Memory Bound. In addition, to mitigate the memory-bound issues in LLM inference, practitioners employ various techniques that can be broadly categorized into two main approaches. First, semantic-preserving methods aim to reduce mem-ory usage while maintaining the original prediction via system opti- mization (Section 2). Examples includes KV caches [90], FlashAtten-tion [24], and FlashDecoding [91]. Conversely, architectural/algorithmic"}, {"title": "Distributed Solution Frameworks", "content": "The memory-bound nature of LLM inference and the limited capac- ity of HBM on individual accelerators present significant challenges in meeting the growing demands of LLM workloads. LLMs with hundreds of billions of parameters typically do not fit on a single node for inference, let alone a single accelerator. Consequently, a distributed solution becomes necessary. However, implement- ing such a solution for LLM inference introduces challenges like efficient model partitioning, communication, and load balancing. Addressing these challenges is crucial for enabling scalable pro- cessing of large-scale LLM inference workloads. Typically, we can employ a combination of multiple parallel strategies to achieve state-of-the-art performance for LLM inference, each with its own advantages and disadvantages. Tensor parallelism is designed to distribute large chunks of ten- sor computation workloads across multiple accelerators and aggre- gate the final results via collective communication. This approach can help reduce end-to-end latency when collective communica- tion is efficient (e.g., NVIDIA NVLink [30], AWS Neuron Collective Communication [10]). However, if the tensor computation work- load is small, the extra overhead in collective communication can diminish overall performance. Since inter-node communication is typically higher than intra-node communication, tensor parallelism is most effectively utilized within a single node. Pipeline parallelism is employed to distribute model layers across accelerators. As both model weights and KV cache for each layer can be distributed to different accelerators, and only the in-puts/outputs of the layers need to be transferred across devices, pipeline parallelism is relatively independent of the collective com-munication bandwidth. This strategy allows for the distribution of models that are too large for a single node. To increase hardware utilization, overlapping different pipeline stages is typically nec-essary. Pipeline parallelism is preferable over tensor parallelism when the entire model does not fit on a single node for inference. Sequence parallelism [68] is a critical technique for supporting long context. The core concept of sequence parallelism involves distributing sequences along the sequence dimension, enabling the parallel decoding of small batches of long sequences. This tech-nique is implemented by solutions such as FlashDecoding [91] and PagedAttention V2 [58]. Expert parallelism (EP) facilitates the distribution of Mixture of Expert (MoE) models [104] across multiple accelerators. The MoE model architecture is designed to skip inactive expert computation, while still maintaining the capability to achieve high accuracy com-pared to dense models. Since expert weights are typically large, distributing and dynamically loading these weights can be costly. To reduce collective communication and avoid the dynamic loading of expert weights, EP keeps each expert within a small group of accelerators [92]. As the input/output data is considerably smaller"}, {"title": "SYSTEM OPTIMIZATION", "content": "We explores semantic-preserving optimizations for LLM inference from a systems perspective. By strategically organizing computa- tions, significant improvements in inference speed and memory efficiency can be achieved without compromising the semantic integrity of the model. In this seciton, we discuss on reducing redundant computations through the use of key-value caches (Sec- tion 2.1), optimizing attention implementation to minimize memory access (Section 2.2), enhancing throughput via handling batches of requests (Section 2.3), and reducing unused memory fragmenta-tion via distributing sequences (Section 2.4). These optimizations were mainly developed based on GPUs, but the main concepts are largely applicable to other AI accelerators with some specific im-plementation tweak. The following subsections delve into each of these approaches in detail, examining their theoretical foundations, practical implementations and challenges therein."}, {"title": "Fast Attention Computation via Caching", "content": "Generating tokens in an autoregressive fashion is a widely adopted approach like GPT [13] and Llama [109], yet it can pose computa-tional challenges. During the auto-regressive generation, decoding step to generate every next token requires to fetch previous tokens. This requires to compute their hidden representation of keys and values in attention mechanism, which could be repetitive during the sequence of token generation. KV- cache [90] stores and reuses these past key-value pairs, eliminating the need of recalculation for every new token. This technique significantly improves the efficiency of inference, by reducing the quadratic complexity of attention computation w.r.t. a sequence length to be linear. However, the memory footprint of KV cache growing linearly w.r.t. the sequence length can be substantial, as it requires addi-tional memory to store the cached keys and values. To address this, several techniques has been introduced to reduce the memory space required for the KV cache. Low-bit precision data types have been utilized in KVQuant [45], which brings million-scale context length support on a single A100-80G GPU. StreamingLLM [123] introduced the concept of attention sink, which preserves decent ac-curacy by leveraging initial tokens without exhausting the long con-text window size. Generalized block-sparse attention patterns, e.g. BigBird [128]), allow the training of long context support, without degrading accuracy at inference stage. Heavy-Hitter Oracle [134] is a cache eviction policy which retains Heavy Hitters tokens, i.e., tokens contributing most of the value in attention scores, based on local statistics at each decoding step. However, all of these can lead to a potential degradation of accuracy. The aforementioned KV cache strategies can be implemented differently depending on hardware. To be specific, the KV cache memory space size can be formulated as $2bshdln$ bytes, where $b$ is batch size, $s$ is sequence length, $h$ is number of KV heads, $d$ is size of the attention head, $l$ is the number of layers, $n$ is size of each data element in number of bytes. The size of $b$ is determined at runtime for batch inference. $d$ and $l$ are fixed by the model configuration. This leaves the optimization space for reducing KV cache memory"}, {"title": "Efficient Attention Computation", "content": "Modern LLMs have extended the support of context length from the order of thousands to millions within a few years from less than 1k (e.g., GPT-2 [13]) to 200k+ (e.g., Claude 3 [8]). The main challenge of expanding the context window lies in the extensive computa- tional requirements and memory consumption for the attention computation. As the model considers more tokens simultaneously, the compute/time complexity and memory demands of calculations increase significantly, scaling quadratically with the size of the context window. FlashAttention [23, 24] was introduced to address these challenges, which reformulates the attention computation as a sequence of matrix multiplications and applies block-sparse decom-position. By processing attention in smaller blocks, FlashAttention reduces the memory footprint of attention computation, avoiding the need to materialize the entire attention matrix in memory at once. The key advantage of FlashAttention is its ability to minimize data movement between different memory hierarchies. By care- fully selecting the block size based on the memory hierarchy and capacity of the device, FlashAttention ensures that the data can be efficiently processed without requiring multiple transfers between memory levels. For example, on GPUs, the block size is typically small to fit within the L2 cache, minimizing expensive memory"}, {"title": "Continuous Batching", "content": "LLM inference is inherently memory-bound if only one sequence is processed. To increase the throughput for a large number of input prompts, the most straightforward approach was to allocate a fixed time window for decoding a fixed number of sequences. This is commonly known as static batching, which has been implemented in FasterTransformer [84] and many others [11, 90]. The advantage of static batching comes from the minimized latency for decod-ing with small batch sizes. As batch size gets bigger to achieve higher throughput, a mechanism in improving effective utilization of batched decoding is needed. Static batching results in resource waste as some sequences reach the end earlier than the others in the same batch. Orca [31] proposed the idea of a dynamic sequence eviction strategy. The strategy essen-tially removes the sequences that generated EOS token, and inserts new prompts into the decoding batch. The approach is commonly referred to as continuous batching. In addition to the proposed mechanism in handling continuous batching, Ocra also introduced the idea of flattening multiple input prompts and concatenate them into the prefill kernel, in order to reduce padding and kernel launch overhead. The block diagonal causal attention mask is commonly used to achieve a throughput gain with FlashAttention."}, {"title": "PagedAttention and its Derived Applications", "content": "Since the length of output tokens is unpredictable, the most straight-forward approach was to maintain the maximal sequence length for each decoding request. As most part of the reserved memory won't be actually used, this would introduce a large amount of internal memory fragmentation. As illustrated in the Figure 3, internal memory fragmentation refers to the memory space that is allocated but not effectively utilized for sequence decoding. Exter-nal memory fragmentation indicates the device memory space that is free but not allocated for usage. To reduce both internal and ex-ternal memory fragmentation, PagedAttention [59] introduced the"}, {"title": "STRUCTURED TRANSFORMER ARCHITECTURES", "content": "Beyond optimizing the serving of a given model, also the model architectures themselves have developed and moved towards archi- tectures that enable faster and more efficient inference, while still being similarly powerful. In the following we discuss changes to the attention mechanism, reducing its number of key and value heads"}, {"title": "Multi-/Grouped Query Attention", "content": "Falcon [3] and Llama 2 70B [109] employ techniques known as multi-query attention (MQA) [103] and grouped-query attention (GQA) [2] respectively. When it comes to inference, memory and computational challenges arise from the repeated loading of decoder weights and attention keys/values in decoding steps. In multi-head attention, distinct queries brings linear increase on the number of heads for keys and values, requiring larger memory bound and prohibiting potential latency improvement. However, MQA involves employing multiple query heads alongside a single key/value head, thereby accelerating decoder inference. GQA an advancement over MQA, strikes a balance by utilizing an interme-diate number of key-value heads (more than one but fewer than the query heads). The GQA model efficiently partitions the query into $n_{heads}$ segments akin to the original multi-head attention mecha-nism, while dividing the key and value into handful of groups. For example, Llama-3 70B [109] uses 64 query heads which are grouped onto 8 key-value heads. This arrangement allows a handful of query heads to share the same key-value heads to interact. By leverag-ing repeated key-value pairs, the GQA approach enhances overall model performance while preserving quality. When it comes to the MQA/GQA inference strategy in a dis-tributed setting, there are a number of approaches. If possible, the common practice is to evenly distribute KV heads across multiple accelerators. This assumes that the number of KV heads are divisi-ble by the number of accelerators. For handling the case where there are more accelerators than number of KV heads, Pope et al. [90] has"}, {"title": "Mixture of Experts for Transformer", "content": "Mixture of Experts (MoE) [104] architecture from Figure 5 is de-signed to activate part of expert computation by skipping inactive ones, while maintaining the capability in achieving high accuracy. This allows for pretrained models to utilize significantly less compu-tational resources and thus increase the model's size or the dataset it handles within the same computational budget compared to a dense model both in training and inference. This MoE compo-nent becames a popular design choice in favor of fast inference among Transformer class [29, 49, 62]. Among many variance of MoE [36, 55, 64, 133, 138, 141], it typically tries to comprise two primary components: First, sparse MoE layers replace conventional dense feed-forward network (FFN) layers. These MoE layers are comprised of a set number of \"experts\" (e.g., 8 in Mistral [49]), where each expert functions as an individual neural network. While these experts are typically FFNs in practice, they can also encompass more intricate networks or even form a hierarchical MoE structure [34]. Second, a gate network or router determines the allocation of to-kens to specific experts. Notably, tokens can be directed to multiple experts. This decision is governed by the routing mechanism as a critical design choice for efficient inference and training. The router, comprising learned parameters, is pretrained concurrently with the remainder of the network and plays a pivotal role in token allocation within MoEs. Routers for sparse MoEs can be categorized into two main vari-ants: Token Choice, which assigns experts to individual tokens, and Expert Choice, which assigns tokens to individual experts. Token"}, {"title": "Other Architectures", "content": "Sliding Window Transformer (SWT) [12] is a variant of the self-attention mechanism designed to handle long sequences more ef-ficiently by dividing the input sequence into smaller, overlapping chunks or \"windows.\" For each token, the attention score is com-puted only over a window of length $w$ sequence rather than the entire (previous) sequence. This attention mechanism sequentially slides across the input sequence to compute all localized attention scores. As the layers of the SWT get deeper, the localized attention mechanism extends the receptive fields w.r.t. input tokens, preserv-ing a comprehensive understanding of the entire sequence, similar to a CNN. Each SWT requires only linear complexity $O(nw)$, mitigating the quadratic complexity $O(n^2)$ in standard self-attention. Mixture-of-Depth [93] allows some tokens to take paths across layers dynamically, skipping certain layers based on specific criteria, e.g., CALM [99] with exit criteria during forward pass, instead of all tokens passing through every layer of Transformer. This approach enables the model to allocate computational resources more efficiently, focusing more layers on complex parts of the input while using fewer layers for simpler parts. The mixture of depth can help reduce computational costs and improve forward/backward speed without significantly compromising model performance."}, {"title": "MODEL COMPRESSION", "content": "Model compression techniques [22] compress a model or input, thereby reducing the memory footprint and latency of LLMs. These methods come with challenges as they typically introduce trade-offs between inference improvement and accuracy. Quantization of model weights (Section 4.1) has essentially has become a stan-dard nowadays. Pruning parts of models has posed more challenges but also seen much progress targeted specifically to LLMs (Sec-tion 4.2). Lastly, entirely compressed models can be trained through distillation from a large teacher model (Section 4.3)."}, {"title": "Quantization", "content": "Quantization [37] is a model-compression technique that represents weights or activations of the network with low-precision data types (e.g., 8-bit integer) instead of high-precision data types (e.g., FP32), therewith reducing the storage when loading the model/activations in hardware (see Figure 6). Reduced data precision poses trade-off between latency-throughput-accuracy. It also requires support from the target hardware to realize maximum speedup [111]. Quantiza-tion is applied either during training or after training. Post-training quantization (PTQ) methods quantize weights or activations of a pre-trained model (e.g., LLM.int8() [25], ZeroQuant-V2 [126], SmoothQuant [122], GPTQ [33], Quip# [110], OmniQuant [102], AQLM [28], PV-Tuning [79], Outlier Suppression+ [116], QLORA [26])."}, {"title": "Pruning", "content": "Pruning is a compression technique to remove redundant param-eters from the model. The goal is to maintain prediction quality of the model while shrinking its size, and thereby increasing its efficiency. Pruning requires strategies to identify which parts to remove and, potentially, how to adapt the remaining parts in order to compensate for quality degradation. Structured Pruning removes whole components of the network such as neurons, attention heads, and layers [9, 56, 78, 108, 121, 130]. For example, SliceGPT [9] effectively decreases the embedding di-mension of the model, whereas LLM-Pruner [78] scores coupled structures in the decoder-layer and removes the least important ones. Sheared Llama [120] and LoRAPrune [130] similarly remove entire structures. When pruning larger structures like channels, blocks, or embedding dimensions, the speedups can easily be real-ized end-to-end (e.g., [78, Table 3], [9, Table 2]). Unstructured Pruning removes individual weights of the network. Clearly, weights that are 0 can be ignored without any loss in ac-curacy, but also very small weights can be set to zero. Pruning weights that are not small enough will finally lead to degradation of the model, which sets the limit for the speedup. Given a desired sparsity ratio and a matrix W, the simplest strategy is to prune the weights with the smallest magnitude, which corresponds to minimizing the Frobenius norm between the dense matrix W and its sparse approximation $\\hat{W}$, i.e., $||W - \\hat{W}||^2$. This approach, re-ferred to as magnitude pruning, quickly leads to drastic accuracy degradation [32, 106]. Wanda [106] and RIA [132] improve over"}, {"title": "Distillation", "content": "Knowledge distillation (KD) ([15, 44, 76, 117]) is a model compres-sion technique in which we train a small model (called student) to match closely the performance of a larger model or an ensem-ble of models (called teacher). To this end, KD connects a student model with the teacher model by a distillation loss, which penal-izes differences in the outputs of the two models at certain layers (see Figure 7). The standard KD approach-also called last-layer-only approach-trains the student to match the performance of the teacher at the last layer (e.g., [44, 96]). Another approach-also called layer-wise approach-trains the student to match the hidden representation of the teacher at each layer (e.g., [107]). Layer-wise-distillation approaches report improved results on downstream tasks compared to last-layer-distillation approaches [71], but they stipulate the same number of layers in the student as the teacher. In general, KD approaches are flexible with regard to the exact struc-ture of the student model, which allows optimizing the student for various target hardwares. Another advantage is that the distillation process runs entirely after training the large teacher model. Distillation does not affect the training of a teacher model, but distillation effort by itself can be a major training effort for the following reasons. First, the number of steps can be similar to pre-training a small model. Second, the distillation loss usually is a combination of the pure student/teacher loss together with an original loss, for which typically the original pre-training data is recommended [44]. To compute the distillation loss, we also need to make a forward pass of a teacher model to get logits. But there"}, {"title": "FAST DECODING", "content": "As discussed, vanilla auto-regressive decoding is memory bound. Speculative decoding (SD) [18, 63] exploits the fact that multiple draft tokens can be verified in a single forward pass of the target model. The draft tokens are then accepted based on a rejection sampling scheme [18, 63] or deterministic approaches [54]. Pro-cessing the draft tokens requires additional computations in the target model, but the main bottleneck remains the loading of the weights. Hence, the verification of the additional draft tokens comes at negligible additional latency. But once draft tokens are accepted, multiple tokens are decoded with a single call to the target model, resulting in an overall latency reduction. Noticeably also, opposed to the compression techniques in Section 4, the output distribution provably remains the same [18, Theorem 1]. Beyond the verification also the draft token generation adds to the latency. We classify SD methods broadly into two categories, based on whether or not they use a separate model for drafting. Seminal work [63] uses a smaller model from the target model's family as draft model, e.g., T5-small as the draft model for T5-XXL, whereas Chen et al. [19] train a separate draft model from scratch. Choosing an appropriate draft model for a target model can be tricky. In light of this, some SD methods take advantage of the target model itself. For example, self-speculative decoding [129] drafts tokens using the target model but skips some of its intermediate layers. Medusa [16] trains multiple feed-forward heads on top of the last Transformer layer. The i-th head is responsible for predicting (t + i)-th token into the future. EAGLE [70] improves the heads by introducing auto-regression on features at the last Transformer layer. PaSS[83] appends k special \"look-ahead\" tokens to the prompt as input, and generates k draft tokens in parallel using the target model itself. Lookahead Decoding [35] applies Jacobi method [98, 105] that drafts multiple tokens in parallel. In some applications (e.g., Question Answering), one can draft tokens by matching their prefix in a document [5], or a database [43]."}, {"title": "CONCLUSION", "content": "This paper provides a comprehensive overview of efficient infer-ence methods for LLMs, covering system optimization, structured Transformer architectures, model compression, and algorithmically faster decoding, especially in the context of AI accelerator. These techniques aim to facilitate effective computation, often considering input-output (IO) communication during attention score calcula-tion, reducing extensive and repetitive self-attention mechanisms, minimizing memory idleness and compressing models themselves. Inference optimization is not only crucial for Transformer-based LLMs, but also other foundation models like Stable Diffusion [94] or the Transformer alternative of State Space Models (SSMs) [38, 39]. Several of the techniques presented in this paper have been suc-cessfully applied to these models too; e.g., in Stable Diffusion with FlashAttention [20], quantization [42, 69, 100, 114], sparsity [67], and distillation [80, 95], or in SSMs with Mixture of Experts [7, 88]. Nevertheless, many of the challenges remain largely unresolved, particularly when dealing with extremely long context lengths and sequences, necessitating tailored efforts depending on the types of devices used. We are confident that researchers and developers will continue to strive towards narrowing these gaps, thereby enhancing the accessibility of Generative Al systems."}]}