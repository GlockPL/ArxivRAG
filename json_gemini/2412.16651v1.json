{"title": "PB-UAP: HYBRID UNIVERSAL ADVERSARIAL ATTACK FOR IMAGE SEGMENTATION", "authors": ["Yufei Song", "Ziqi Zhou", "Minghui Li", "Xianlong Wang", "Hangtao Zhang", "Menghao Deng", "Wei Wan", "Shengshan Hu", "Leo Yu Zhang"], "abstract": "With the rapid advancement of deep learning, the model robustness has become a significant research hotspot, i.e., adversarial attacks on deep neural networks. Existing works primarily focus on image classification tasks, aiming to alter the model's predicted labels. Due to the output complexity and deeper network architectures, research on adversarial exam-ples for segmentation models is still limited, particularly for universal adversarial perturbations. In this paper, we propose a novel universal adversarial attack method designed for seg-mentation models, which includes dual feature separation and low-frequency scattering modules. The two modules guide the training of adversarial examples in the pixel and frequency space, respectively. Experiments demonstrate that our method achieves high attack success rates surpassing the state-of-the-art methods, and exhibits strong transferability across different models.", "sections": [{"title": "I. INTRODUCTION", "content": "With the development of deep learning, semantic segmentation models are playing an increasingly important role in complex scenarios such as autonomous driving [1], medical image analysis [2], and remote sensing [3]. By classifying semantic information at the pixel level, segmentation mod-els [4]\u2013[6] achieve accurate object segmentation. However, recent works [7], [8] demonstrate that segmentation models are vulnerable to adversarial attacks, where imperceptible noise is added to images, leading to incorrect model predictions.\nExisting adversarial attacks can be categorized into sample-wise [9], [10] and universal adversarial perturbations (UAPs) [11]\u2013[14], where UAP refers to a single perturbation applied to various examples, causing the model to produce er-roneous outputs across different inputs. Despite the promising attack performance of UAPs in classification tasks, the UAP methods for segmentation models [15], [16] still fall short. The earliest UAP method [15] for segmentation builds on UAP [17] by averaging gradients over each batch to mislead the segmen-tation model. Another study [16] enhances the generalization of attacks across different segmentation models by utilizing feature similarities of input examples in the first layer of the model, based on the UAPGD [18] method. Although these methods optimize the traditional UAP approach, they still do not achieve satisfactory attack performance. We attribute this limitation to the direct adaptation of classification-oriented UAP techniques, which overlooks task-specific knowledge critical to segmentation.\nIn this paper, we propose Pixel Blind UAP (PB-UAP), a novel universal adversarial attack in segmentation tasks that disrupts image features in both spatial and frequency domains. Our method aims to disable the models segmentation ability across diverse images using a single UAP (see Fig. 1). Unlike classification models that focus on global features, segmen-tation models concentrate more on contextual relationships within images [4], [5], [19]. Therefore, our intuition is to destroy inter-class and intra-class semantic correlations in the image to mislead the model into incorrectly segmenting the input images. In the spatial domain, we deviate the output features of adversarial examples from both the output features of benign examples and the ground truth labels, aiming to undermine inter-class semantic correlations. In the frequency domain, given that pixels within the same class primarily be-long to the low-frequency components of images, we separate the low-frequency features between adversarial and original examples to disrupt intra-class semantic correlations.\nIn conclusion, our main contributions are three-folds. 1) We propose an effective universal adversarial attack for seg-mentation tasks, dubbed PB-UAP, disrupting image features in both the spatial and frequency domains. 2) We propose a dual feature separation and low-frequency scattering strategy that overcomes the limitations of inter-class and intra-class semantic correlations. 3) Experimental results on three models and two benchmark datasets demonstrate that PB-UAP sig-nificantly outperforms state-of-the-art methods, and exhibits strong transferability across different models."}, {"title": "II. BACKGROUND AND RELATED WORKS", "content": "A. Semantic Segmentation Models\nSegmentation models typically incorporate architectures such as Image Pyramid [20], Encoder-Decoder [21], Context Module [22], Spatial Pyramid Pooling [23], Atrous Convolu-tion (AC) [5], and Atrous Spatial Pyramid Pooling (ASPP) [6]. These architectures enhance the model's ability to capture image details by introducing multi-scale feature extraction, strengthening contextual information, or expanding the recep-tive field, resulting in more precise segmentation outcomes. Specifically, Deeplabv1 [5] and PSPNet [4] uses AC structure to enhance global context awareness while preserving local details without sacrificing resolution. After Deeplabv2 [6], subsequent versions [24], [25] adopted the ASPP structure, which enhances the model's understanding of complex scenes and object boundaries through multi-scale feature aggregation. Different models have distinct architectures, resulting in vary-ing vulnerabilities to adversarial attacks.\nB. Universal Adversarial Attacks for Segmentation\nDeep learning models are vulnerable to poisoning at-tacks [26], [27], backdoor attacks [28]\u2013[31], and adversarial attacks [32], [33], among which standard universal adversarial attack methods [17], [18] show outstanding performance in classification tasks. However, when applied to segmentation tasks, they exhibit limitations in disrupting the model's under-standing of contextual information. The first UAP method [15] for segmentation extends UAP [17] by averaging gradients over each batch, enabling targeted attacks on street scenes. However, this approach neglects the semantic interdependen-cies between classes, limiting the generalization of adversarial examples. To address this, a subsequent work [34] enhances generalization by applying data augmentation techniques that inject high-frequency information into the training images. An-other research [16] demonstrate that similar feature represen-tations in the initial layers of different models improve cross-model attack transferability. Despite these advancements, the attack success rate remains hindered by intra-class semantic dependencies. Therefore, existing work does not address the limitations of classification-based UAP methods in segmenta-tion tasks, indicating the need for a more effective strategy."}, {"title": "III. METHODOLOGY", "content": "A. Problem Formulation\nIn semantic segmentation, adversarial attacks can be defined as adding imperceptible perturbations to deceive the model, causing incorrect classifications for every pixel in the image. Specifically, let $f(x)$ denote the segmentation model, where $x \\in R^{W\\times H \\times C}$ represents the input image, and the model's output consists of the predicted class labels for each pixel. This optimization problem can be formulated as maximizing the cross entropy loss function $L_{CE}$ by adjusting the perturbation $\\delta$ within the constraint set $S$, thereby increasing the discrep-ancy between the predictions and the ground truth labels $y_{true}$:\n$\\delta^* = \\arg \\max_{\\delta \\in S} L_{CE} (f(x + \\delta), y_{true}).$ \nB. Intuition Behind PB-UAP\nA successful universal adversarial attack on segmentation task should apply a UAP to induce incorrect predictions for all pixels across different examples, requiring the attack to consider both the global context and local details of the images. Specifically, it faces the following two challenges:\nChallenge I: Inter-class Semantic Correlations Limit the Universality of Attacks. Segmentation tasks involve com-plex semantic correlations, including class boundaries, object shapes, and structures, making the optimization of perturba-tions more challenging. A universal adversarial attack must generate a single perturbation that generalizes across diverse examples, addressing variations in class boundaries, shapes, and structures. This requires identifying a common vulnera-bility in the model. To this end, we propose a dual feature deviation strategy, applying gradient ascent on the features of the model's final layer and the pixel-level labels. The former disrupts the model's global semantic understanding, weakening its reliance on class boundaries, while the latter ensures the perturbation affects diverse target labels, bypassing semantic constraints related to shapes and structures.\nChallenge II: Intra-class Semantic Correlations Weaken the Attacks. Intra-class semantic correlations refer to the spa-tial relationships between pixels of the same target in an image. This correlation is evident in adjacent pixels having similar visual features and semantic information. Specifically, these pixels form coherent and similar local features in the image. When adversarial perturbations affect these regions, the model smooths the impact of the perturbations by understanding the local context. This contextual integration makes it difficult for perturbations to have significant effects within these regions, thereby reducing the success rate of adversarial attacks. Con-sidering that in the frequency domain analysis of an image, the low-frequency components contain the overall structure and most of the smooth information of the image, while the high-frequency components, such as the edges and textures of objects, represent less semantic information, and perturbations in these areas are influenced by local context smoothing. Based on this consideration, we use low-frequency scattering to disrupt pixel correlations. Specifically, we separate the low-frequency components of adversarial examples from the original examples and increase the perturbation strength in the low-frequency regions to improve the effectiveness of the attack.\nC. PB-UAP: A Complete Illustration\nIn this section, we introduce PB-UAP, a hybrid spatial-frequency universal adversarial attack method designed for semantic segmentation tasks. The pipeline of PB-UAP is illus-trated in Fig. 2, including a spatial attack based on dual feature deviation and a frequency attack based on low-frequency scattering. Specifically, we separate the output features of adversarial examples and benign examples at the final layer of the model, while separating the outputs of adversarial examples from the ground truth labels, in order to disrupt the"}, {"title": "IV. EXPERIMENTS", "content": "A. Experimental Setup\nDatasets and Models. We use two public segmentation datasets to evaluate the attack performance of our method: PASCAL VOC 2012 [36] and CITYSCAPES [37]. We choose PSPNet, Deeplabv1, and Deeplabv3+ with MobileNet and ResNet50 backbones as victim models.\nParameter Setting. Following [17], [18], [38], we set the upper bound of UAP to 10/255. For our experiments, we set the hyperparameters k, \u5165, and the batch size to 1, 0.3, and 5, respectively.\nDual Feature Deviation In this module, we define two loss functions as optimization objectives, including the deviation loss between the output features of adversarial examples and both the output features of original examples and the ground truth labels. First, Eq. (3) and Eq. (4) calculate the difference between output features of the adversarial examples and the ground truth labels. \u03b4 denotes the perturbation, M is a binary matrix, where zeros and ones represent the pixel locations in the segmentation mask that are misclassified and correctly classified, respectively. $f(x+d)$ and $Y_{true}$ are the outputs of the adversarial example and the ground truth labels, respectively. $J_{suc}$ and $J_{fail}$ represent the loss for pixels in which the attack succeeds and fails, respectively.\n$J_{suc} = -(L_{CE}(f(x + d), Y_{true})) * M,$\n$J_{fail} = -(L_{CE}(f(x + d), Y_{true})) * M,$\nEq. (5) illustrates the process of assigning different optimiza-tion weights to correctly and incorrectly classified pixels. The hyperparameter \u5165, with a value of 0.3, represents the weight assigned to correctly classified pixels, while 1 - A represents the weight assigned to incorrectly classified pixels.\n$I_{pd} = \u03bb \u00b7 J_{suc} + (1 \u2212 \u03bb) \u00b7 J_{fail},$\nSecondly, Eq. (6) calculates the difference between the outputs of adversarial examples and benign examples, where $L_{MSE}$ is the mean-square error loss function.\n$I_{fd} = -(L_{MSE}(f(x + d), f(x))).$\nLow-frequency Scattering In segmentation tasks, the low-frequency regions of an image contain most of the semantic information for each object, where semantic consistency be-tween adjacent pixels exhibits a strong spatial correlation. This spatial correlation smooths the effect of perturbations, thereby reducing attack performance. To disrupt this correlation, we use the discrete wavelet transform (DWT) [35] with a low-pass filter L to decompose the image into its low-frequency component $c_u$. Next, we use the inverse discrete wavelet transform (IDWT) to reconstruct the low-frequency component into an image \u03c6(x), as detailed in Eq. 7. Subsequently, we per-form low-frequency scattering by calculating the mean square error between the low-frequency images of the adversarial and original examples, thereby disrupting the correlation between adjacent pixels, as shown in Eq. 8.\n$c_u = LxL^T, \u03c6(x) = L^T c_uL = L^T (LxL^T)L,$\n$I_{ls} = \u2212(L_{MSE} (\u03c6(x + \u03b4), \u03c6(x)).$\nB. Attack Performance\nIn this section, we comprehensively evaluate the effective-ness of PB-UAP. We conduct attack experiments on three segmentation models and two datasets, with two backbone net-works. We calculate the mIoU of both benign and adversarial examples for each experimental setup.\nAccording to the experimental results in Tab. I, the mIoU dropped to between 3.17% and 18.77% in all experimental setups, indicating that PB-UAP can effectively influence the output of segmentation models.\nC. Transferability Study\nWe investigate the attack transferability of PB-UAP across different models, using MobileNet as the backbone network. As shown in Fig. 3(a) (b), the UAPs generated using different proxy models exhibit excellent performance on other models. These results demonstrate that PB-UAP possesses strong transferability and reliability.\nD. Comparison Study\nTo demonstrate the superiority of our method, we compare PB-UAP with five previous popular UAP schemes, including SegPGD [7] and TranSegPGD [8], which are state-of-the-art adversarial attacks designed for segmentation models. We select PASCAL VOC [36] as the attacked dataset, with model and backbone settings as described in Section 4.2.\nThe results in Tab. II indicate that PB-UAP outperforms all methods significantly. We also provide visualizations of the segmentation results of the adversarial examples generated\nE. Abaltion Study\nThe effect of different modules. We investigate the effect of various modules on the attack performance of PB-UAP, with Deeplabv3+ as the model and MobileNet as the backbone network. The results in Fig. 5 (a) show that no variants can compete with the complete method, implying the indispens-ability of each component for PB-UAP.\nThe effect of perturbation budget. As shown in Fig. 5 (b), we evaluate PB-UAP's attack performance in different values of e, using MobileNet as the backbone network. With the increase in e, there is a corresponding enhancement in attack performance. Notably, our attack still maintains high efficacy at the 8/255 setting, with an average mIoU exceeding 16.22%."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose PB-UAP, a universal adversarial attack specifically designed for the characteristics of seg-mentation models. PB-UAP can effectively induce incorrect"}]}