{"title": "Logit Scaling for Out-of-Distribution Detection", "authors": ["Andrija Djurisic", "Rosanne Liu", "Mladen Nikolic"], "abstract": "The safe deployment of machine learning and AI models in open-world settings hinges critically on the ability to detect out-of-distribution (OOD) data accurately, data samples that contrast vastly from what the model was trained with. Current approaches to OOD detection often require further training the model, and/or statistics about the training data which may no longer be accessible. Additionally, many existing OOD detection methods struggle to maintain performance when transferred across different architectures. Our research tackles these issues by proposing a simple, post-hoc method that does not require access to the training data distribution, keeps a trained network intact, and holds strong performance across a variety of architectures. Our method, Logit Scaling (LTS), as the name suggests, simply scales the logits in a manner that effectively distinguishes between in-distribution (ID) and OOD samples. We tested our method on benchmarks across various scales, including CIFAR- 10, CIFAR-100, ImageNet and OpenOOD. The experiments cover 3 ID and 14 OOD datasets, as well as 9 model architectures. Overall, we demonstrate state-of-the-art performance, robustness and adaptability across different architectures, paving the way towards a universally applicable solution for advanced OOD detection.", "sections": [{"title": "1 Introduction", "content": "In the forthcoming era of artificial intelligence (AI), ensuring the safety and reliability of AI models is of utmost importance. One of the key components in achieving this is the effective detection of out- of-distribution (OOD) data on which the trained models usually severely underperform [1, 2]. Such underperformance poses a significant challenge to model reliability and safety. This is particularly crucial in areas such as healthcare, autonomous driving, and finance where the ability to detect OOD instances can profoundly influence outcomes. Thus, ensuring models can accurately recognize and manage OOD samples not only enhances their performance but also plays a vital role in preventing potential errors in critical applications.\nThe flip side of OOD is in-distribution (ID) data, which usually correspond to the exact train- ing data of the model, or data samples from the same distribution as the training data. AI and machine learning research preceding the current phase of large models rely on standardized train- ing sets that are made open and accessible, but the situation has changed starting from the large models era, where the majority of state-of-the-art models have their training data undisclosed or dif- ficult to access. In light of this change, we are in greater need of OOD detection methods that do"}, {"title": "2 Related work", "content": "The field of out-of-distribution detection has sig- nificantly expanded in recent years, driven by the necessity for safer ML model deployment. Base- line methods such as those proposed by Hendrycks and Gimpel [2], Liu et al. [9], and Lee et al. [10] have laid the groundwork for much of the subse- quent research in this area. We discuss four major groups of OOD detection methods relevant for understanding our own work.\nOOD detection methods leveraging train- ing data statistics. A number of earlier tech- niques, including ReAct [3], DICE [7], and KNN [11] employ statistics derived from training data in their OOD detection process. Specifically, ReAct adjusts the penultimate layer activations"}, {"title": "3 Logit Scaling for OOD Detection", "content": "Our method is inspired by findings reported in ReAct [3], which highlight that out-of-distribution samples often induce abnormally high activations. These findings are illustrated by Figure 2. Sev- eral methods tried to exploit this discrepancy of patterns observed in out-of-distribution and in-distribution samples [3\u20135, 7]. Our subsequent research, notably ASH-S [4], first observed that treating activations on a sample-based approach enhances the efficacy of OOD detectors and also lifts the requirement of accessing the training set at detection time. Still, it relied on activation prun- ing which resulted in modest losses of accuracy. Now we make a further step forward by proposing a method which avoids any network modification and relies only on the modification of the popular energy score.\nThe energy score, as introduced by Liu et al. [9], is one of the most frequently employed techniques for detecting out-of-distribution samples. The pro- cess of detecting OOD samples using energy score operates in the following manner: raw inputs are processed by the network, the logits are com- puted, and fed into the energy score function. This function then produces a score used to classify whether a sample is in-distribution (ID) or out-of- distribution (OOD). The energy score is defined as a scalar value computed by the formula:\n$E(x; f) = - \\log \\sum_{i=1}^{C} e^{f_i (x)}$\nwhere $x$ is the given input, $C$ is the number of classes and $f_i(x)$ is the logit for the class i.\nOur approach modifies the aforementioned pro- cess by extending it with the computation of a scaling factor S(x) derived from the feature representation of an individual sample x:\n$E(x; f) = - \\log \\sum_{i=1}^{C} e^{S(x) f_i (x)}$\nThe scaling factor $S$ is computed in the follow- ing manner. We denote the feature representation of an input sample x from the network's penulti- mate layer as h(x) \u2208 Rm. Our method calculates"}, {"title": "4 Experiments", "content": "In this section, we introduce benchmarks, outline evaluation metrics, and present the experimental results of our approach, with a focus on evaluating the effectiveness of LTS and its applicability across different architectures."}, {"title": "4.1 OOD detection benchmark", "content": "We tested our method on four different benchmarks presented in Table 2.\nCIFAR-10 and CIFAR-100 benchmarks.\nThe setup for CIFAR-10 and CIFAR-100 bench- marks is derived from Sun et al. [3], Djurisic et al."}, {"title": "4.2 OOD evaluation metrics", "content": "We evaluate our method using standard OOD detection metrics following the work of Hendrycks and Gimpel [2]: AUROC (Area Under the Receiver Operating Characteristic Curve), FPR@95 (False Positive Rate at 95% True Positive Rate) and AUPR (Area Under the Precision-Recall curve).\nAUROC measures model's ability to differentiate between ID and OOD classes. Higher AUROC val- ues indicate better model performance. FPR@95 measures the proportion of false positives (incor- rectly identified as positive) out of the total actual negatives, at the threshold where the true pos- itive rate (correctly identified positives) is 95%. Lower FPR@95 indicates better model perfor- mance. AUPR is providing a single scalar value that reflects the model's ability to balance preci- sion and recall. A higher AUPR indicates a better performing model."}, {"title": "4.3 OOD detection performance", "content": "LTS demonstrates excellent performance in out-of- distribution detection. As highlighted in Table 3,"}, {"title": "4.4 LTS applicability accross architectures", "content": "A recent study by Zhao et al. [8] revealed that many previous methods fail to maintain robust out-of-distribution performance across different architectures. Specifically, modern transformer- based architectures such as ViT [30], SwinTrans- former [31], as well as MLP-Mixer [32] present significant challenges for existing OOD detec- tion methods. The OptFS [8] method was the first to demonstrate consistent performance across eight different architectures. In our experiments, we replicate the exact experimental setup of OptFS and demonstrate that our method performs consistently well across all tested architectures, significantly reducing FPR@95 while maintain- ing a comparable level of AUROC compared to OptFS. Figure 4 illustrates the performance of LTS across five different architectures. Detailed results for all tested architectures can be found in Appendix B. Integration of LTS within the different architectures is described in Appendix C."}, {"title": "4.5 Ablation studies", "content": "Determining the optimal value of p. In this section we evaluate the performance of LTS across various values of p, which represents the propor- tion of top activations used in computing the scaling factor S. The detailed analysis is shown in Figure 5. The appropriate choice of p is highly dependent upon the specific task as well as net- work architecture in use. In Figure 5, we present a comprehensive sweep across five architectures: ResNet-50, MobileNet-V2, ViT-16/B, Swin Trans- former S and MLP-Mixer-B and 4 different tasks from ImageNet-1k benchmark. Based on our anal- ysis, we have found that p value of 5% generally yields optimal results.\nCompatibility with other OOD detection methods. LTS is not compatible with methods that are relying on scaling activations in middle layers of the network due to its operational charac- teristics. For instance, ASH-S and SCALE perform scaling of activations on the penultimate layer,"}, {"title": "5 Conslusion", "content": "In this study, we introduced LTS, extremely simple, post-hoc, off-the-shelf method for detecting out-of- distribution samples. LTS operates by deriving a scaling factor for each sample based on activations from the penultimate layer, which is then applied to adjust the logits. We conducted thorough test- ing of LTS, demonstrating that its performance surpasses many existing methods. Additionally, we have shown its robustness and effectiveness across a diverse set of architectures. Looking ahead, we plan to explore strategies to consistently maintain performance on specific OOD tasks irrespective of architectural differences."}, {"title": "A Detailed CIFAR-10 And CIFAR-100 Results", "content": "Table 7 and Table 8 supplement Table 3 in the main text, as they display the full results on each of the 6 OOD datasets for models trained on CIFAR-10 and CIFAR-100 respectively."}, {"title": "B LTS Performance Across Eight Architectures", "content": "Table 9 supplements Figure 4 and presents detailed performance of LTS across 8 different architec- tures on ImageNet benchmark, along with other methods."}, {"title": "C Application of LTS to different architectures", "content": "Figure 6 illustrates the integration of LTS within the Vision Transformer and ResNet-50 architec- tures. We observe that ResNet-50, DenseNet-101 and MobileNet V2 have non-negative activations at penultimate layer, unlike ViT, Swin Transformer and MLP-Mixer. To mimic that, in OOD detection time we apply ReLU on penultimate layer activa- tions of ViT, Swin Transformer and MLP-Mixer before feeding them into LTS. We emipirically observed that such a modification leads to increase in OOD detection performance."}]}