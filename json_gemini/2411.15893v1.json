{"title": "Distribution-aware Online Continual Learning for Urban Spatio-Temporal Forecasting", "authors": ["Chengxin Wang", "Gary Tan", "Swagato Barman Roy", "Beng Chin Ooi"], "abstract": "Urban spatio-temporal (ST) forecasting is crucial for various urban applications such as intelligent scheduling and trip planning. Previous studies focus on modeling ST correlations among urban locations in offline settings, which often neglect the non-stationary nature of urban ST data, particularly, distribution shifts over time. This oversight can lead to degraded performance in real-world scenarios. In this paper, we first analyze the distribution shifts in urban ST data, and then introduce DOST, a novel online continual learning framework tailored for ST data characteristics. DOST employs an adaptive ST network equipped with a variable-independent adapter to address the unique distribution shifts at each urban location dynamically. Further, to accommodate the gradual nature of these shifts, we also develop an awake-hibernate learning strategy that intermittently fine-tunes the adapter during the online phase to reduce computational overhead. This strategy integrates a streaming memory update mechanism designed for urban ST sequential data, enabling effective network adaptation to new patterns while preventing catastrophic forgetting. Experimental results confirm DOST's superiority over state-of-the-art models on four real-world datasets, providing online forecasts within an average of 0.1 seconds and achieving a 12.89% reduction in forecast errors compared to baseline models.", "sections": [{"title": "1 Introduction", "content": "Modern Intelligent Transportation Systems (ITS) [1, 49] rely on extensive sensor networks deployed across urban areas to monitor traffic conditions. These sensors produce vast amounts of spatio-temporal (ST) data that exhibit both spatial correlations and temporal dynamics. Accurate forecasting of these urban ST data is crucial for smart city applications, such as intelligent scheduling [19, 27, 55], traffic management [63, 68, 78], and trip planning [15, 16, 33].\nSpatio-temporal (ST) correlations [14, 29, 64] play a critical role in urban forecasting, as traffic conditions at one location are influenced both by its historical data and by data from neighboring locations. This has driven substantial efforts in developing advanced ST networks to capture urban ST dependencies [14, 20, 52]. Nonetheless, most existing studies are conducted in offline settings with static data distributions, assuming stationary relationships between data over time. In contrast, urban ST data are inherently dynamic with constantly evolving distributions, which renders offline models ineffective for practical deployment. To address distribution shifts in streaming data, online continual learning has proven effective in forecasting tasks such as long-term time series forecasting (LTSF) [47, 66], natural language processing (NLP) [25, 45], and stock prediction [54, 82]. However, these methods cannot be directly applied to urban ST forecasting for the following two reasons.\nFirst, existing methods either immediately update upon receiving new data [25, 47, 66], or delay updates until a new batch or domain is available [5, 38]. Immediate updates are essential for tasks characterized by rapidly changing temporal patterns, such as Long-Term Series Forecasting (LTSF) and Natural Language Processing (NLP), whereas batch updates are more common in scenarios that experience abrupt domain shifts, such as image classification. However, urban ST data typically undergo gradual shifts over time due to static urban zoning and functionality [48, 74], rendering both immediate and batch updates impractical. As illustrated in Figure 1, taxi demand in a city like Chicago might appear stable over weeks, yet significant variations emerge over longer spans, such as months. Specifically, the estimated distributions in regions, such as Region 23 and Region 64, demonstrate substantial similarity between two consecutive weeks, for example, from 08/01/2020 to 14/01/2020 and from 15/01/2020 to 21/01/2020. This data characteristic necessitates a finer-grained update approach to avoid performance degradation and excessive computational costs associated with over-frequent or delayed model updates.\nSecond, previous efforts have encountered challenges in adapting to the varied distribution shifts across different urban locations during ST forecasting. Each urban area exhibits distinct shift patterns due to location-specific factors such as urban functionality [48, 74]. As Figure 1 illustrates, the estimated data distributions in Region 23 (a residential area) and Region 64 (a transportation hub) differ significantly, and their shift patterns vary considerably over extended periods, for example, from 15/01/2020 to 31/03/2020. Traditional adaptation strategies, such as model fine-tuning [47, 66], which updates all model parameters with the most recent data, or parameter-efficient tuning [25, 45], which selectively fine-tunes specific layers, can address these distribution shifts, but often at the cost of high computational demands and frequent unnecessary updates. Although recent advancements in parameter-efficient tuning [25, 26, 45] and continuous learning [59, 79, 80] have shown promise in managing these shifts effectively, they also ignore the unique shifts at individual locations, thus falling short of the specific needs of urban ST forecasting.\nTo address the above challenges, we propose a novel Distribution-aware Online continual learning framework for urban Spatio-Temporal forecasting (DOST) to handle distribution shifts in streaming urban ST data. DOST leverages inherent urban ST behaviors to enhance online urban forecasting, which considerably improves performance and efficiency. In particular, to explicitly model the varying distribution shifts across urban locations over time, we introduce an adaptive ST network with a plug-and-play adapter named Variable-Independent Adapter (VIA). VIA customizes adapters for respective urban locations, which effectively update the network in response to location-specific distribution shifts. In addition, we introduce a novel Awake-Hibernate (AH) learning strategy to align network updates with the gradual shift characteristic of urban ST data. This strategy alternates between awake and hibernate phases to minimize unnecessary updates and reduce computational costs. During the awake phases, VIA is precisely fine-tuned to quickly adapt to new distribution patterns, while in the hibernate phases, all model parameters are frozen to conserve resources. The network fine-tuning employs a Streaming Memory Update (SMU) mechanism, which adopts a small episodic memory selected from recent updates to ensure timely and effective adaptation without overfitting or catastrophic forgetting.\nOur main contributions are summarized as follows:\n\u2022 We propose DOST, a novel distribution-aware online continual learning framework tailored for urban ST forecasting, which effectively balances between update phases to align with gradual urban distribution shifts, thus avoiding inefficient training cycles.\n\u2022 We introduce the Variable-Independent Adapter VIA, which enables the network to adapt to diverse and evolving urban distribution shifts across different locations.\n\u2022 Extensive experimental results confirm DOST's superiority over state-of-the-art models on four real-world datasets, which delivers online forecasts in real-time, within only 0.1 seconds, and reduces forecast errors by 12.89% compared to baseline models."}, {"title": "2 PRELIMINARIES", "content": "Definition 1 (Urban Spatio-Temporal Data): Urban data, including region-based data (e.g., taxi demand and crowd flow) and road-based data (e.g., vehicle speed and traffic volume), are spatio-temporal (ST) data as they have both spatial and temporal dimensions. Region-based data covers N community-defined regions, while road-based data involves N specific roads. Both data types evolve temporally within each region or road.\nDefinition 2 (External Factors): Urban ST data often correlate with external factors, such as time of day and day of the week, as human activities and traffic patterns are shaped by daily routines and weekly cycles.\nOnline Urban Spatio-Temporal Forecasting: In real-world applications, data arrives sequentially, denoted as \\(X_{\u03c4:0}\\). The objective is to process this ongoing data stream and forecast future traffic conditions across urban locations for the next H time intervals at any given time step \\(\u03c4\\). In other words, given the past observed data within a look-back window L, i.e., \\(X_t = X_{\u03c4-L+1:\u03c4} = (X_{\u03c4-L+1}, X_{\u03c4-L+2}, ..., X_\u03c4) \u2208 \\mathbb{R}^{N\u00d7L\u00d7d}\\), and the spatial adjacency matrix of urban locations \\(A \u2208 \\mathbb{R}^{N\u00d7N}\\), the aim is to predict future urban ST conditions \\(\\hat{y}_\u03c4 = X_{\u03c4:\u03c4+H} = (X_{\u03c4+1}, X_{\u03c4+2},...,X_{\u03c4+H}) \u2208 \\mathbb{R}^{N\u00d7H\u00d7d}\\), where d represents the dimension of the data features."}, {"title": "3 DOST", "content": "In this section, we present our proposed model DOST, as depicted in Figure 2. DOST employs two main strategies to tackle distribution shifts in sequential urban ST forecasting: an adaptive spatio-temporal network for online learning (Section 3.1) and an awake-hibernate learning strategy for efficient model updates (Section 3.2)."}, {"title": "3.1 Adaptive Spatio-Temporal Network", "content": "Recognizing that future urban conditions are influenced by past ST correlations, various offline ST networks have been designed to capture these dependencies. These networks typically utilize ST Modules to capture spatial dependencies via graph neural networks (GNNs) [69, 73] and attention mechanisms [83, 87], and exploit temporal dependencies with convolutional neural networks (CNNs) [69, 70], recurrent mechanisms (RNNs) [2, 34], attentions [71, 87], and multilayer perceptrons (MLP) [51, 81]. However, these networks cannot adapt to location-specific time-evolving distributions in online urban ST forecasting. To address this, we introduce an adaptive spatio-temporal network to learn online distribution shifts, as illustrated in Figure 2 (a). This network contains Traditional Modules from offline ST networks to model the ST correlations, a Variable-Independent (VI) Adapter to learn location-specific shifts, and an Awake Decider to support the awake-hibernate learning strategy.\n3.1.1 Traditional Modules. Traditional (i.e., offline) ST networks typically consist of three modules: Input Embedding, Spatio-Temporal (ST) Module, and Decoder. In general, they begin with an Input Embedding that transforms the raw inputs \\(X_\u03c4 \u2208 \\mathbb{R}^{N\u00d7L\u00d7d}\\) into high-dimensional representations through a fully connected layer FC(\u00b7):\n\\[h = FC(X_T; W_e),\\] (1)\nwhere \\(h \u2208 \\mathbb{R}^{N\u00d7L\u00d7d_h}\\), \\(d_h\\) denotes the feature dimensions.\nThen, the spatio-temporal correlations can be captured via the ST Module, which can be many existing ST networks, based on the input embedding h and the spatial adjacency matrix A:\n\\[\\hat{h} = f_{st} (h, A; W_{st}),\\] (2)\nwhere \\(\\hat{h} \u2208 \\mathbb{R}^{N\u00d7d_o}\\) denotes the high-level ST representations, \\(d_o\\) represents the feature dimensions, \\(W_{st}\\) are the learnable parameters, and \\(f_{st}\\) denotes functions of the ST Module. By default, we employ the ST network in GWNet [70] as our ST Module. Finally, future urban ST conditions can be predicted via a Decoder, e.g., a fully connected layer:\n\\[\\hat{y} = FC(\\hat{h}; W_d),\\] (3)\nwhere \\(\\hat{y} \u2208 \\mathbb{R}^{N\u00d7H\u00d7d}\\), and \\(W_d\\) is the learnable parameters.\n3.1.2 Variable-Independent Adapter (VIA). Distribution shifts can vary significantly across different city locations. For example, data distribution in school regions may change dramatically during school holidays, whereas it remains stable in CBD regions. To address these location-specific shifts, we propose a Variable-Independent Adapter (VIA), as shown in Figure 3. VIA consists of N sub-adapters, each designed to handle the distribution shifts in a specific location, ensuring accurate adaptation without interference from irrelevant changes in other locations.\nFor each urban location n, VIA employs a sub-adapter to transform the original input embedding, learned from Equation 1, into a location-specific adapted embedding:\n\\[H^{(n)} = f_a(h^{(n)}; W_a^{(n)}) + h^{(n)} = \u03c3(W_{a2}^{(n)}(\u03c3(W_{a1}^{(n)} h^{(n)}))) + h^{(n)}, \\] (4)\nwhere \\(H^{(n)} \u2208 \\mathbb{R}^{L\u00d7d_h}\\), \\(f_a\\) represents the non-linear transformation function, i.e., multi-layer perceptron (MLP) layers, \\(\u03c3\\) refers to ReLU function, \\(W_a^{(n)}\\) is learnable parameters specific to location n, \\(W_{a1} \u2208 \\mathbb{R}^{d_h\u00d7d_m}\\), and \\(W_{a2} \u2208 \\mathbb{R}^{d_m\u00d7d_h}\\). The final adapted features for all urban locations \\(H \u2208 \\mathbb{R}^{N\u00d7L\u00d7d_h}\\) can be obtained by concatenating \\(H^{(n)}\\) for each location n. The skip connection [22] is used to preserve the untransformed features. Thus, Equation 2 in DOST is updated to:\n\\[\\hat{h} = f_{st} (H, A; W_{st}),\\] (5)\nNote that VIA functions as a plug-and-play component before the variable mixing networks, i.e., ST Module. This design is crucial due to the complex ST correlations in urban ST data, necessitating the variable mixing networks to model these dependencies [50]."}, {"title": "3.1.3 Awake Decider", "content": "Urban ST data distribution typically remains stable over short periods, such as weeks, reducing the necessity for continuous network updates. We introduce the Awake Decider to alternate network updates between awake and hibernate phases, thus preventing unnecessary updates in real-time forecasting. Recognizing weekly periodic patterns in urban ST data [53, 60], the Awake Decider leverages external factors, i.e., date and time, to align the scheduling of awake and hibernate phases with the data's weekly patterns. Specifically, each awake-hibernate (AH) cycle consists of an awake phase \\(L_a\\) and a hibernate phase \\(L_h\\). Both phases are proportionate to the week's span \\(L_w\\), with \\(L_h = \u03bbL_a \u221d L_w\\), where \\(\u03bb\\) is the AH parameter."}, {"title": "3.2 Awake-Hibernate Learning Strategy", "content": "Urban ST data distributions exhibit gradual shifts: they remain stable over short spans, such as consecutive weeks, but undergo significant changes over longer periods due to natural factors like seasonal variations that affect human activity patterns [7]. Traditional offline ST networks, trained on static datasets, often struggle to adapt to these shifts, leading to less accurate predictions over extended inference periods. To address this gradual distribution shift, we introduce an Awake-Hibernate (AH) learning strategy, depicted in Figure 2 (b). This strategy intermittently fine-tunes the adapter to align with the nature of urban ST distribution shifts, ensuring precise and timely forecasts over time. Designed for online sequential data, our AH learning strategy features a key update mechanism, i.e., the Streaming Memory Update (SMU) Mechanism, and comprises three phases: Warm up, Awake, and Hibernate.\n3.2.1 Streaming Memory Update (SMU) Mechanism. The awake phase involves fine-tuning the network to adapt to new incoming patterns. However, continuously updating the network can cause catastrophic forgetting [11, 40]. For example, during a one-week update phase, the model could forget Monday's patterns by Sunday, despite daily variations. Recent studies [6, 38] have shown that retaining a memory of previously trained samples can reduce forgetting and stabilize training. Inspired by these works, we design a Streaming Memory Update Mechanism, as illustrated in Figure 4. This mechanism fine-tunes the network with a tiny Episode Memory (EM) for multi-step ahead forecasting in streaming data, which comprises three main components: a Memory Placeholder (MP) to track recent samples, a Streaming Memory Buffer (SMB) to store the most relevant samples, and a Streaming Memory Update (SMU) to update the adapters with the selected EM from the SMB.\nMemory Placeholder (MP) is designed to track recent samples for multi-step ahead predictions. At each time step, the network receives only the current observations, which means the ground truth for the current sample, i.e., \\(X_{\u03c4+1:\u03c4+H}\\), is not immediately available. The MP addresses this by maintaining both past and current observations, thus enabling the network to select the most recent samples without revisiting the data sequence. Specifically, upon receiving new data at each t, the MP discards the oldest data, \\(X_{T-L-H}\\), and incorporates the new data \\(X_\u03c4\\), thus updating the MP to hold the most recent observations as \\(MP = X_{\u03c4-L-H+1:\u03c4} \u2208 \\mathbb{R}^{(L+H)\u00d7N\u00d7d}\\) at \u03c4. The MP then extracts recent observations \\(MP_x = X_{\u03c4-L-H+1:\u03c4-H} \u2208 \\mathbb{R}^{L\u00d7N\u00d7d}\\) and ground truths \\(MP_y = X_{\u03c4-H+1:\u03c4} \u2208 \\mathbb{R}^{H\u00d7N\u00d7d}\\) to serve as inputs for the Streaming Memory Buffer.\nStreaming Memory Buffer (SMB) selectively stores the most relevant samples. We consider samples from the latest AH cycle as the most relevant for two reasons inherent to urban ST data: (1) distant past data can become irrelevant for future predictions due to evolving patterns; (2) recurrent patterns often emerge within a single AH cycle due to gradual shifts and weekly periodicity. Therefore, we design a SMB M with M memory slots to selectively retain observations and ground truths from the most recent hibernate phase and the current awake phase up to time t. At each time step \\(\u03c4\\), given the recent observations \\(MP_x\\) and ground truths \\(MP_y\\) from the MP, M is updated via reservoir sampling [58]. The probability of storing a sample in the SMB is \\(p = M/L_{ah}\\), where \\(L_{ah} = L_h + L_a\\) is the total duration of an AH cycle. We reset M at the start of each hibernate phase to ensure that the SMB only stores the most relevant samples. Thus, M at time t can be represented as:\n\\[M_\u03c4 =\n\\begin{cases}\n{(x, y) \u2208 (MP_x, MP_y) | sampled with p} & \\text{if } t\\neq 0 \\ (mod\\ L_{ah}),\\\\\n\\emptyset & otherwise.\n\\end{cases}\\] (6)\nStreaming Memory Update (SMU) efficiently updates the model with newly emerging patterns from incoming data while preventing catastrophic forgetting. It is tailored for urban ST streaming, differing from previous methods in three key aspects: (1) SMU selects episodic memory from the most relevant samples, instead of randomly selecting episodic memory from all past observations [6, 89]; (2) rather than updating the network based on the very latest sample [47, 66], SMU does not explicitly incorporate the very latest sample for network updates; (3) SMU updates the network only during the awake phase, instead of fine-tuning the network immediately upon receiving new data [5, 13].\nAt each awake phase time step, we employ SMU to fine-tune the adapter with a tiny Episodic Memory (EM), which is selected from the SMB. Specifically, at each time step t, given the updated SMB M, we first randomly select a tiny EM \\(M_e\\) of size \\(M_e\\) from M, where \\(M_e << M\\). The selected EM \\(M_e\\) only includes data samples from the most recent AH cycle up to the current time t, serving to: (1) incorporate recent patterns from the latest AH cycle to prevent catastrophic forgetting, and (2) introduce randomness through non-sequential samples to prevent overfitting. The adapter, i.e., VIA, is then optimized based on errors for the selected \\(M_e\\), followed by generating forecasts for t. Note that \\(M_e\\) might not contain the very latest samples, i.e., \\(X_{\u03c4-H}\\) and \\(Y_{\u03c4-H}\\) as random sampling from M is adopted. However, this is sufficient to capture recent patterns, as distribution shifts within an AH cycle are generally stable."}, {"title": "3.2.2 Warm Up Phase", "content": "In real-world scenarios, data typically arrive in a sequential order, introduce challenges for model convergence due to limited randomness [42]. To address this, during the warm up phase, we divide historical data into training and validation sets. We shuffle the training set to introduce randomness, aiding in preventing overfitting, while the validation set remains unshuffled. In the validation phase, the samples are updated to the SMB M using reservoir sampling [58], preparing the model for the first online awake phase. To this end, this warm up phase can ensure the effective handling of upcoming data streams."}, {"title": "3.2.3 Awake Phase and Hibernate Phase", "content": "DOST operates in alternating awake and hibernate phases during the online phase. The transition between these phases is controlled by the Awake Decider (see Section 3.1.3). During the Awake Phase, updates are applied to both the SMB and the model. Specifically, the SMB M is updated with newly received samples, and the network is fine-tuned using the SMU mechanism to adapt to new patterns. For computational efficiency, only the adapters are fine-tuned, while traditional modules remain frozen. In contrast, the Hibernate Phase focuses solely on updating the SMB, suspending model updates due to the stability of short-term shifts to save computational costs. M is reset at the onset of this phase and then updated with new samples, preparing it for the upcoming AH cycle."}, {"title": "3.2.4 Algorithm", "content": "Algorithm 1 outlines the proposed AH learning strategy for online urban ST forecasting. Specifically, given the incoming data stream, the algorithm operates at each timestep. At each timestep \u03c4, \\(MP \u2208 \\mathbb{R}^{(L+H)\u00d7N\u00d7d}\\) stores observed samples (\\(X_{\u03c4-L-H+1},\u00b7\u00b7\u00b7, X_{\u03c4-1}, X_\u03c4\\)), allowing M to incorporate the most recent observations \\(MP_x\\) and ground truths \\(MP_y\\) for the time step \u03c4 \u2013 H via reservoir sampling. During the awake phase, the SMU is executed to update the VIA with a small, randomly selected subset of memory from the SMB. At the beginning of each hibernate phase, the SMB is reset. The algorithm involves both awake and hibernate phases during the online phase, and the time complexity differs between these phases. Considering the execution time of f(\u00b7) as \\(T_f\\), during the awake phase, the time complexity to update the SMB with the current observation is O(NL + NH + M). The time complexity for the streaming memory update is O(\\(M_eT_f\\)), and for forecasting, it is O(\\(T_f\\)). Since (NL + NH + M) \u00ab \\(M_eT_f\\), the total time complexity during the awake phase is O(\\(M_eT_f\\)). During the hibernate phase, the time complexity is O(\\(T_f\\))."}, {"title": "3.3 Optimization", "content": "Following the prior works [33, 72], we adopt the Mean Absolute Error (MAE) as our loss function. The loss during the warm up phase is formulated as follows:\n\\[L(\u03b8_t, \u03b8_a) = MAE(\\hat{y}, y),\\] (7)\nwhere \\(\\hat{y}\\) represents the predicted ST conditions, y denotes the actual ST conditions, and \\(\u03b8_t\\) and \\(\u03b8_a\\) refer to the sets of learnable parameters of the traditional modules and adaptive modules, respectively. For online adaptation, the loss during the awake phase is calculated as MAE based on the samples in the Episodic Memory:\n\\[L(\u03b8_a) = MAE(\\hat{y}, y).\\] (8)\nWith this loss function, we can then train DOST in an end-to-end manner effectively via popular gradient-based optimizers such as Adam [31], AdamW [39] and etc."}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\n4.1.1 Datasets. We evaluate DOST on four real-world datasets, i.e., Chicago-T\u00b9, Singapore-T\u00b2, METR-LA [34] and PEMS-BAY [34]. Chicago-T and Singapore-T are region-based datasets, while METR-LA and PEMS-BAY are road-based datasets. Detailed statistics of datasets are described in Table 1."}, {"title": "4.1.2 Evaluation Metrics", "content": "We follow the previous studies [50, 62] to evaluate our model performance using three metrics: Mean Absolute Error (MAE), Root Mean Squared Errors (RMSE) and Weighted Mean Absolute Percentage Error (WMAPE)."}, {"title": "4.1.3 Implementation Details", "content": "DOST is trained on an NVIDIA GeForce RTX 3090 GPU using AdamW optimizer [39] with a learning rate of 0.001. We adopt an early-stop strategy, setting a patience parameter of 10 and a maximum number of epochs of 150 for all experiments. We set the look-back window L to 12, forecast horizon H to 12, AH parameter \\(\u03bb\\) to 1, SMB slot M to 1000, and EM size \\(M_e\\) to 8. For the dimensions, \\(d_h\\) = 32, \\(d_o\\) = 256, and \\(d_m\\) = 4. \\(L_a\\) is set to the total time intervals for a week, i.e., 672 for Chicago-T and Singapore-T datasets, and 2016 for METR-LA and PEMS-BAY datasets. The data is divided into warm up and online phases in a 2:6 ratio. The warm up phase is further partitioned into a 4:1 ratio for training and validation."}, {"title": "4.1.4 Baselines", "content": "We compare DOST against several widely used baselines for urban ST forecasting and Long-term Time Series Forecasting (LTSF). HA [3] is a classical method. We also evaluate six strong models specifically for urban ST forecasting, i.e., STGCN [73], GWNET [70], AGCRN [2], MTGNN [69], GMSDR [36], and PDFormer [28]. Additionally, we examine six state-of-the-art LTSF methods capable of predicting traffic conditions, though not specifically tailored for short-term urban ST forecasting. These methods include REVIN [30], PatchTST [43], Dlinear [76], OnlineTCN [89], FSNet [47], and OneNet [66]. Among them, OnlineTCN, FSNet, and OneNet are designed for online forecasting."}, {"title": "4.2 Experimental Results & Analysis", "content": "4.2.1 Performance Comparison. Table 2 presents the prediction results of various baseline models and DOST across four datasets. The results indicate that: (1) Models with advanced ST networks yield better results on most datasets, e.g., Singapore-T, METR-LA, and PEMS-BAY. This is because the significant spatial indistinguishability in urban ST data [50] requires advanced ST models to capture complex correlations. LTSF methods, which focus mainly on temporal correlations, struggle to learn these complex ST patterns. (2) Online forecasting models, e.g., FSNet and DOST, excel on datasets like Chicago-T, which have extended test phases. Conversely, methods designed to address out-of-distribution issues, e.g., REVIN and PatchTST, struggle with long-term distribution changes because they learn from fixed samples. This highlights the effectiveness of online settings in real-world urban ST forecasting, where the ability to continuously adapt to new data patterns is crucial during prolonged testing phases. (3) DOST, tailored for urban ST forecasting, significantly outperforms all baseline models across various datasets. Unlike traditional ST networks, DOST tackles distribution shifts via the AH learning strategy. It also addresses complex spatial correlations and location-specific shifts through its ST Modules and VIA, distinguishing it from general online learning methods. DOST reduces MAE by an average of 12.89% compared to baseline models across various datasets. T-test results across the four datasets confirm DOST's consistent superiority over the leading baselines."}, {"title": "4.2.2 Evaluating Strategies in DOST", "content": "The two strategies in DOST, i.e., adaptive ST network and AH learning strategy, can be seamlessly integrated with many existing offline urban ST forecasting methods. Table 3 demonstrates the effectiveness of our"}, {"title": "4.2.3 Speed Comparison and Memory Usage", "content": "Table 4 presents the inference time and memory usage of DOST with various baselines on the Singapore-T dataset, which contains 13,093 test samples. Results are reported on a GTX 3090 GPU with 24,268 MB of memory.\nThe results indicate that: (1) Offline methods such as STGCN, GWNET, MTGNN, and PatchTST have faster inference times than advanced online methods like FSNet and OneNet because they do not require backpropagation during the online phase. (2) DOST achieves the best performance with reasonable computational costs, outperforming advanced online methods by updating only the adapter rather than all parameters. It requires 0.0266 s per sample during the hibernate phase and 0.1099 s in the awake phase for both fine-tuning and forecasting. This efficiency confirms DOST's effectiveness in real-world urban ST forecasting. (3) DOST excels in performance with manageable memory usage. It employs the GWNET backbone to capture complex ST correlations, thus requiring more memory than FSNet and OneNet. However, by fine-tuning only the adapter, DOST avoids excessive memory usage for gradient storage. Even though SMB needs memory to store past observations, with reasonable sizes for SMB M, spatial size N, look-back L, and prediction horizon H, the space complexity of SMB O(MN(L+H)) does not significantly increase overall memory usage."}, {"title": "4.3 Ablation Study", "content": "Figure 5 illustrates the effectiveness of each component in DOST. w/o VIA denotes DOST without the VIA, updating the default network with the AH strategy; w/o AH refers to DOST without the AH learning strategy, using only the adaptive ST network; w/o VIAH refers to DOST without both VIA and AH. w VSA replaces VIA with vanilla MLP layers and updates the network using AH. w/o Reset does not reset the SMB at hibernate phase start; w/o SMU refers to DOST without SMU mechanism, instead it adopts the online update strategy from existing works [47], updating the model directly with the latest observations.\nThe results indicate that: (1) w/o VIA and w/o AH outperform w/o VIAH, demonstrating the effectiveness of our strategies for urban ST forecasting in an online setting. (2) w VSA exhibits a performance decline, indicating that updating the network without considering location-specific distributions is insufficient. This proves the importance of VIA, which can adapt to various distribution shifts across locations over time. (3) The inferior results of the w/o Reset verify our presupposition that samples from the distant past are not relevant to current forecasting. (4) Increased errors in w/o SMU highlight the benefits of our SMU as it leverages historical knowledge to mitigate catastrophic forgetting and introduces randomness to avoid overfitting."}, {"title": "4.4 Study on AH Learning Strategy", "content": "Table 5 presents the prediction results and the total inference time (in seconds) of our model using various online continual learning strategies, including: w/o H omits the hibernate phase, updating the model at every time step. w ER adopts the learning strategy from ER [6], utilizing a memory buffer and current observations. w ERH extends w ER by including the hibernate phase. w SMUR adopts the SMU mechanism and adds the most recent samples to the episodic memory. Full fine-tunes all network parameters.\nThe results indicate that: (1) Omitting hibernate phases not only introduces computational costs but also leads to performance degradation, likely due to overfitting caused by too frequent updates. Additionally, the superior performance of w ERH over w ER demonstrates the beneficial role of hibernate phases in enhancing online"}, {"title": "4.5 Effects of Hyperparameters", "content": "In Figure 6, we study the effects of hyperparameters in DOST on the PEMS-BAY dataset. The results indicate that: (1) Increasing \\(d_m\\) from 4 to 16 lowers MAE and RMSE due to improved network capability. However, \\(d_m\\) = 8 requires 128K more parameters compared to \\(d_m\\) = 4. Since our model performs well with \\(d_m\\) = 4, we select it as our default setting. (2) DOST performs best at \\(\u03bb\\) = 1, with performance decreasing at higher \\(\u03bb\\) values, suggesting that the data distribution changes gradually over time. Eliminating either the awake phase (\\(\u03bb\\) = 0) or the hibernate phase (\\(\u03bb\\) = \u221e) results in lower performance, highlighting the need for intermittent updates as the data distribution evolves. Surprisingly, even with an extended hibernate phase (\\(\u03bb\\) = 2), DOST performs better than without any hibernate phase, indicating that too frequent updates can cause overfitting and catastrophic forgetting. (3) A larger SMB size (M) helps reduce MAE and RMSE, as it provides more historical pattern knowledge. However, an excessively large M is unnecessary, as the SMB is reset at the end of each AH cycle. (4) The absence of episodic memory, i.e., \\(M_e\\) = 0, greatly diminishes performance. Conversely, small episodic memory sizes, i.e., \\(M_e\\) = 8, are sufficient to introduce historical patterns and randomness, which helps mitigate overfitting to recent data and thus improves results."}, {"title": "5 Related Work", "content": "Urban Spatio-Temporal Forecasting is a crucial task in smart city development, serving numerous urban applications and attracting notable attention. Early attempts employed traditional time series models, such as Autoregressive Integrated Moving Average (ARIMA) [56, 67", "10": "and Support Vector Regression (SVR) [24, 65", "85": "or attentions [83, 87", "23": "Convolutional Neural Networks (CNNs) [60, 70", "57": ".", "75": "and often overlook temporal dynamics, where data arrives sequentially and distributions shift in real-world online deployment. Similarly, recent graph-free models employing normalization [12", "51": "also neglect these temporal dynamics. Some studies [79, 80"}]}