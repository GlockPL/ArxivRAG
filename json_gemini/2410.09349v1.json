{"title": "Inference and Verbalization Functions During In-Context Learning", "authors": ["Junyi Tao", "Xiaoyin Chen", "Nelson F. Liu"], "abstract": "Large language models (LMs) are capable of in-context learning from a few demonstrations (example-label pairs) to solve new tasks dur- ing inference. Despite the intuitive impor- tance of high-quality demonstrations, previ- ous work has observed that, in some settings, ICL performance is minimally affected by ir- relevant labels (Min et al., 2022). We hy- pothesize that LMs perform ICL with irrele- vant labels via two sequential processes: an inference function that solves the task, fol- lowed by a verbalization function that maps the inferred answer to the label space. Im- portantly, we hypothesize that the inference function is invariant to remappings of the label space (e.g., \u201ctrue\u201d/\u201cfalse\u201d to \u201ccat/\u201ddog\"), en- abling LMs to share the same inference func- tion across settings with different label words. We empirically validate this hypothesis with controlled layer-wise interchange intervention experiments. Our findings confirm the hypothe- ses on multiple datasets and tasks (natural lan- guage inference, sentiment analysis, and topic classification) and further suggest that the two functions can be localized in specific layers across various open-sourced models, including GEMMA-7B, MISTRAL-7B-v0.3, GEMMA-2- 27B, and LLAMA-3.1-70B.", "sections": [{"title": "1 Introduction", "content": "Large language models (LMs) are capable of in-context learning (ICL)\u2014the ability to solve novel tasks from solely a handful of demonstration exam- ples provided in-context during inference (Brown et al., 2020). Previous work has found that, in certain settings, ICL performance is minimally af- fected by using demonstrations with irrelevant la- bel words (Min et al., 2022). How do LMs man- age to perform in-context learning with irrelevant and even misleading label words? This research seeks to offer a causal explanation for such model behaviors, going beyond mere summaries of input- output patterns. We hypothesize that the model develops and utilizes certain abstractions to consis- tently work well on ICL tasks despite remappings of the label spaces. Specifically, we propose and test the following hypotheses:\nH\u2081 Causal mechanism: LMs sequentially ap- ply two functions when performing ICL: First, an inference function that constructs a representa- tion of the answer to the ICL input; and second, a verbalization function that maps this answer representation to the output label space specified by the demonstrations.\nH2 Invariant to label remapping: The infer ence function is invariant to remappings of the out- put label space (e.g., \u201ctrue\u201d/\u201cfalse\u201d to \u201ccat/\u201ddog\")."}, {"title": "2 Methods", "content": "We aim for an explanation that reveals the causal structure employed by the model. Successful causal explanation is marked by its ability to al- low us to make counterfactual predictions, that is, answering the \u201cwhat-if-things-had-been-different questions\" should we perform certain interventions (Woodward, 2003). Typically, direct observation of outcomes from distinct interventions on the same unit is not possible (Holland, 1986). But, it is not an issue for us, because we can let the model generate outputs under any conceivable intervention on the units. Moreover, we can make counterfactual state- ments with mere interventional information with deterministic neural network models (Bareinboim et al., 2022), which is also typically not possible (Pearl and Mackenzie, 2018). These enable a con- crete operationalization of our hypothesis testing."}, {"title": "2.1 Testing Hypotheses with Counterfactual Scenarios", "content": "Framework for testing our hypotheses. Our approach is to (1) define a counterfactual scenario aligned with our hypotheses, wherein certain model behaviors emerge only if our hypotheses hold true, (2) employ appropriate intervention methods to re- alize this scenario, and (3) conduct the intervention- based experiments and interpret their results.\nDesign a counterfactual scenario. Imagine a scenario corroborating our hypotheses, where we \"concatenate\" the verbalization function and inference function induced from different model runs with distinct label words. Assuming H2 (in- variant to label remapping) is true, this con- catenation should function effectively, enabling the verbalization function to successfully de- code the answer representation produced by the inference function and correctly verbalize it, lead- ing to a hypothetical counterfactual output where the answer from the first run appears in the label words of the second run. Their functionalities must remain unaffected by the concatenation for inference function to transfer the answer repre- sentation to verbalization function, collectively producing the counterfactual output.\nOperationalize the setting. With our \"ladder\" hypotheses, we can modify the model's low-level internal representation as a protocol for interven- ing on the high-level inference function and verbalization function. Specifically, the two functions being localized separately (H3.1 sepa- rate localization) enables us to isolate their causal effects, and H3.2 (consistent localization) makes it possible to conduct the intervention with the coun- terfactual value of the representation\u2014the value it would take under alternative input scenarios in the same position\u2014rather than a value the neuron might never actually assume. With these, our objec- tive becomes to find a layer (or sequence of layers), lmid, such that taking the last token representation from one run and plugging it into another would change the model's output in a way that reflects the hypothetical counterfactual condition.\nInterpreting results. Our hypotheses are sup- ported if the model consistently generates a suf- ficient proportion of counterfactual outputs. Additionally, this will also provide information about the specific (though not exact) locations of the two functions within the model-lmid should occur after the layer where the answer representa- tion is sufficiently developed. This, however, does not tell us the exact starts and ends of the two func- tions. It is possible that only a subset of layers be- fore lmid is actively involved in implementing the inference function and after lmid is actively in- volved in implementing the verbalization func- tion, while some others may be redundant.\nOur hypotheses can be weakened by the absence of evidence, i.e., no specific layer found to allow ef- fective concatenation of the functions; and they can falsified if our interventions yield consistent results that contradict our predictions, which may indicate that the model systematically employs mechanisms different than those hypothesized."}, {"title": "2.2 Using Interchange Interventions to Realize Counterfactual Scenarios", "content": "Interchange intervention. We will first intro- duce the formulation of interchange intervention (Geiger et al., 2020, 2021, 2022, 2024a,b; Huang et al., 2024) and then apply it to our context. Con- sider a model M that takes an input string x and generates an output string y. We denote the en- tire set of internal representations of the model M created during this inference as M(x) and the pre- dicted token y = \u0442(\u041c(x)).\nLet's conduct interchange intervention on a set of intermediate representations Z of the model M by replacing them with the value of z and denote the post-intervention model as Mz\u2190z. The difference between \u315c(M(x)) and \u315c(Mz\u2190z(x)) manifests"}, {"title": "3 Experiments", "content": "Models. We choose open-sourced base models of various sizes and families, including GEMMA- 7B (Team et al., 2024), MISTRAL-7B-v0.3 (Jiang et al., 2023), GEMMA-2-27B (Team et al., 2024), and LLAMA-3.1-70B (Dubey et al., 2024).\nDatasets. We first conduct experiments on three NLI datasets: MultiNLI (Williams et al., 2018), RTE (Wang et al., 2018), and ANLI (Nie et al., 2020). We cast MultiNLI and ANLI into binary classification tasks by dropping the \u201cneutral\" class to facilitate the remapping of label spaces and to avoid ambiguities inherent in the \u201cneutral\" exam- ples. We further test if our findings generalize to other tasks by using two sentence classification datasets: IMDb (Maas et al., 2011) and AGNews (Zhang et al., 2015). Due to computational limits, we only take a subset of 300 test examples that are sampled randomly and fixed across different settings on the dataset (see Table 2 for details).\nImplementation details. We conduct interven- tion on each layer in the model except the last one, since exchanging the last token representation at that layer is equivalent to replacing the output to- ken. ICL settings are summarized in Table 3. All flip rates are averaged over three runs with evenly sampled demonstration examples, i.e., balanced for each class. Model outputs are decoded greedily by always selecting the top predicted token.\""}, {"title": "3.1 Experiment with Remapped Label Spaces", "content": "Modulate the label words. For each task, we ex- periment with a diverse set of label words. For example, in addition to the default label words"}, {"title": "3.2 Experiment with Reconstructed Tasks on MultiNLI", "content": "Purpose. To address the potential concern men- tioned earlier, we design a complementary ex- periment to test if the plugged-in representations are task-agnostic. We want to control that the intervened model can only know about the in- tended tasks (that can lead to the counterfactual output) by decoding a task-relevant representation from the source model, i.e., the representations we take from the source model and plug into the in- tervened model. We conclude that the plugged-in representations are not task-agnostic if the model can still achieve high flip rates; and this conclusion can be strengthened if the high flip rates occur in a similar location as in the previous experiments with remapped label spaces, that is, the results validate the location of the inference function.\nSet desiderata for alternative tasks. We want the alternative tasks that (1) maximally preserve the input space to allow the model to continue em- ploying similar mechanisms, (2) secure a balanced dataset with an adequate number of examples for both positive and negative labels, and (3) allow"}, {"title": "4 Discussion", "content": "Validation of H\u2081: Causal mechanism. This hy- pothesis is confirmed if we can \u201cconcatenate\" an inference function with another verbalization function and observe the generation of the counterfactual output that reflects the answer obtained by the inference function and the corre- sponding label space of the verbalization func- tion. We achieve such a scenario with the exper- iment with remapped label spaces, as manifested by the high flip rates we observe during the middle layers of the model.\nConcern may be raised that the intervened model just re-solves the task on its own during the down- stream processes, that is, both inference func- tion and verbalization function are performed in the middle and the late layers. Our experiment with alternative tasks shows that this is not true. This experiment ensures that only the source model knows the answer that leads to the hypothetical counterfactual output, which means high flip rates can only be achieved if the plugged-in repre- sentation involves a sufficiently developed repre- sentation of the answer. Therefore, the high flip rates we observe during and after the middle layers support the hypothesis that the inference function"}, {"title": "5 Related Work", "content": "Understanding ICL. A variety of work has sought to better understand how language mod- els perform in-context learning. Min et al. (2022) show that ICL performance on a variety of text clas- sification tasks is minimally affected by randomly changing the demonstration labels, hypothesizing that demonstrations may primarily help with de- termining the label space (i.e., the verbalization function), the distribution of the input examples, and how the output text should be formatted. (Xie et al., 2022) hypothesize that LMs learn latent tasks during pre-training, primarily using ICL demonstra- tions to identify which latent task is most pertinent to the provided ICL input. Other work (Aky\u00fcrek et al., 2023; von Oswald et al., 2022; Dai et al., 2022) hypothesizes that LMs perform implicit gra- dient descent on a latent model during ICL. Wei et al. (2023) and Pan et al. (2023) confirmed these observations and further indicated that this con- sistency depends on the size of the model. Our study offers a deeper analysis of these behaviors at the representational level. Wei et al. (2023) study how model size affects whether LMs prefer to rely on semantic priors about a task, as opposed to the example-label mappings provided in the demonstra- tions. They find that smaller models tend to rely on semantic priors during pre-training, since they ignore flipped labels presented in the ICL demon- strations. In contrast, larger models make use of these flipped labels, indicating that they rely less on their semantic task priors. Pan et al. (2023) disentan- gle task learning and task recognition in in-context learning; intuitively, LMs use their priors to per- form novel tasks during ICL (since a few demon- strations are unlikely to provide complete informa- tion about a complex task), but they also do not completely rely on their task priors (since they can handle ICL settings with arbitrary label mappings).\nImplicit functions in language models. Another line of prior work has sought to characterize what functions might be implicitly implemented by LMs. Olsson et al. (2022) identify \u201cinduction heads\", a unique type of attention head that replicates re- peating patterns from prior contexts. Hendel et al. (2023) argue ICL compresses the demonstrations into a task vector, which is then used to generate the output for the ICL input. Merullo et al. (2023) provide evidence that Transformer LMs perform ICL in three layer-wise stages: argument formu- lation, function application, and saturation. Todd et al. (2024) show that ICL creates function vec- tors of the demonstrated task, and these function vectors can trigger execution of the task in other settings (e.g., zero-shot prediction or natural text)."}, {"title": "6 Conclusion", "content": "We hypothesize that when LMs perform ICL with irrelevant or misleading label words, they first ap- ply an inference function to obtain a represen- tation of the answer and then apply a verbaliza tion function to verbalize the answer as one of the label words specified in the demonstrations (H\u2081). In addition, the inference function is invariant to remappings of label words (H2), and both the func- tions can be localized separately and consistently within certain layers of the model (H3). To vali- date our hypotheses, we design experiments based on interchange intervention to realize counterfac- tual scenarios that would only occur if all of our hypotheses hold true. Our experiments across a variety of tasks, datasets and models indicate that our hypotheses hold across a variety of settings. Our findings contribute to a growing body of work on mechanistically understanding how language models perform in-context learning."}, {"title": "7 Limitations", "content": "Coverage of models and tasks. Since our inter- vention study requires access to the model's inter- mediate representations, we can only experiment with open-source models. There is no guarantee that similar findings can be observed in the state- of-the-art close models, and how the MoE architec- tures will affect our findings remains unclear. In addition, we acknowledged that we only focus on classification tasks on which the notion of \"verbal- ization\" or remapping of label spaces can be more naturally defined. Future studies could extend to more complex tasks, such as those that involve answering open-ended questions.\nFurther specifying the inference function. Does the model truly carry out natural language inference when processing datasets like MultiNLI, RTE, and ANLI? Our current evidence does not allow us to definitively answer this question, and the notion of \"textual entailment\" itself is not with- out its controversies. We cannot claim about what the model is actually doing during the layers of the inference function. Moreover, when experi- menting with reconstructed tasks on MultiNLI, we do not know the true relationships between the al- ternative tasks (lexical and domain classification) and NLI. Nonetheless, these ambiguities do not undermine our argument\u2014we do not assert that the model performs an \u201cNLI-inference\" function on NLI datasets. By inference function we refer to the (general) processes the model uses to derive the answer representation.\nPredicting ICL performance. Prior studies have shown that the model's ICL performance can be (sometimes significantly) harmed by the irrelevant labels (Min et al., 2022; Pan et al., 2023; Wei et al., 2023). We do not conduct a systematic analysis and explanation of how different factors\u2014such as prompt templates, label word choices, and the num- ber of shots\u2014causally contribute to the model's performance on ICL tasks with irrelevant labels. A nuanced understanding of the underlying mecha- nism would require analysis tailored to each model and setting. In particular, our current research does not address which specific features of label words influence ICL performance positively or negatively, which could be an interesting topic to explore.\nUnknown training data. Concern arises whether the model's good performance on ICL tasks with irrelevant label words stems from its prior exposure to similar cases. We cannot rule out the possibil- ity of data leakage during pre-training, primarily because we do not have access to the pretraining data of the models we study. We are hopeful, how- ever, that our use of a diverse range of irrelevant labels in our prompts reduces the likelihood that these specific cases were present in the pre-training dataset, thus alleviating potential concerns.\""}, {"title": "A Probing Study", "content": "To identify the low-level implementations that align with our hypothesized high-level abstractions, we start with identifying processes shared across different model runs and then verify if they sat- isfy all properties we hypothesize. As a proxy, we conduct a pilot study where we use probing to ex- amine how the intermediate representations, i.e., the neural activations, correlate with the high-level concept of the answer to the ICL task and the label words specified in the prompt.\nProbe. A probe (Alain and Bengio, 2016; Peters et al., 2018; Tenney et al., 2019; Clark et al., 2019; Hupkes et al., 2018) is usually a linear classifier or shallow MLP that takes the intermediate repre- sentation as the input and output labels for some property, aimed at testing how easily the represen- tations can be linearly separated. A high probing accuracy on a hold-out test set indicates that the information about the property is encoded in the intermediate representation.\nExperimental details. We implement probes as logistic regressions with Scikit-learn (Pedregosa et al., 2011) and the L-BFGS optimization algo- rithm (Liu and Nocedal, 1989). We train one probe for each layer with representations generated on the RTE training set by GEMMA-7B. Probes are then applied to the RTE validation set with different label words. Results are averaged over three trials with different sets of demonstrations. We repeat this process for each layer.\nExperiments. We train probes to predict the out- put labels on the last token representations of one run with default label words (\"true\"/\"false\") in the NLI task. We first apply to probe to the representa- tions produced by runs with default label words to observe the development of the representation of the output throughout the forward pass. Then, we test if this probe can generalize to the representa- tions produced by runs with remapped label words (e.g., when the default label space \u201ctrue\u201d/\u201cfalse\u201d is remapped to \u201ccat\u201d/\u201cdog\u201d). This aims to identify if there are shared processes in runs with different label words. Specifically, the process on certain layers is interpreted as being shared if the probing accuracy on these layers are similar across different runs; on the contrary, if the probing accuracy on certain layers of one run diverges from the prob- ing accuracy on another run, we conclude that the model implements different processes on these lay- ers in each run.\nFindings. For probes trained and tested on the runs with default label words (Figure 6, Middle), the probing accuracy is monotonically increasing, suggesting the development of the representation of the output label. For probes trained on the runs with default label words and tested on runs with different label words, the probes achieve high accuracy on other label words in layers 15-20 and then drop to a random baseline in the last 8 layers. In other"}, {"title": "B Illustrations of the Reconstructed Task Intervention", "content": "Figure 7 illustrates and summarizes our recon- structed task intervention outlined in Section 3.2."}, {"title": "C Implications of Poor ICL Performance", "content": "We do not expect to observe effective intervention results on cases where the ICL performance is in- adequate, and such cases will not weaken our argu- ment. This is because the poor ICL performance is very likely to come from the model's failure to develop the necessary abstractions for solving the task, which would include recognizing the task, in- ferring the correct answer, recognizing the label space, and verbalizing the answer correctly. If the model achieves high ICL performance with some"}, {"title": "D Prompting Strategy to Improve ICL Performance", "content": "We slightly adjust the selection of irrelevant label words and prompting strategies for each setting, to facilitate the model in achieving adequate ICL performance.\nWe observe that reproducing the phenomena of interest on base models can be challenging with some settings. For instance, 7-8 models on ANLI are difficult to handle with irrelevant label words despite performing adequately with default label words. And AGNews is generally hard for all mod- els because it is a four-class classification and natu- rally requires more shots.\nWe adjust our template selection based on the corresponding ICL performance. We always start with a 16-shot demonstration with the \"sentence\" template, aiming for ICL performance above 0.7. If this benchmark is not met, we increase the demon- stration to 32 shots. Should challenges persist, we modify the template to \u201csent_label\" and consider adding specific keywords in front of the answer to implicitly hint for the intended tasks, such as \"topic:\" for AGNews and \u201csentiment:\u201d for IMDb.\nNote on the notion of generalizability. By the generalizability of our findings, we do not mean that a single set of irrelevant labels should work universally across all models in all settings. Again, our study of causal mechanisms focuses on cases where the model achieves high ICL performance, which means it develops the necessary causal ab- stractions required to solve the task we want to study. Since LMs are sensitive to contexts and to different prompting strategies, the required settings unsurprisingly vary model by model.\""}, {"title": "E Predicted Tokens from the experiment with remapped label spaces", "content": "We present the full results of the intervened model's prediction distributions on all five datasets used in our intervention with remapped label spaces: MultiNLI (Figure 9), RTE (Figure 10), ANLI (Fig- ure 11), IMDb (Figure 12), AGNews (Figure 13 & 14)."}]}