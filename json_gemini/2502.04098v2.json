{"title": "Efficient Few-Shot Continual Learning in Vision-Language Models", "authors": ["Aristeidis Panos", "Rahaf Aljundi", "Daniel Olmeda Reino", "Richard E. Turner"], "abstract": "Vision-language models (VLMs) excel in tasks such as visual question answering and image captioning. However, VLMs are often limited by their use of pretrained image encoders, like CLIP, leading to image understanding errors that hinder overall performance. On top of that, real-world applications often require the model to be continuously adapted as new and often limited data continuously arrive. To address this, we propose LoRSU (Low-Rank Adaptation with Structured Updates), a robust and computationally efficient method for selectively updating image encoders within VLMs. LoRSU introduces structured and localized parameter updates, effectively correcting performance on previously error-prone data while preserving the model's general robustness. Our approach leverages theoretical insights to identify and update only the most critical parameters, achieving significant resource efficiency. Specifically, we demonstrate that LoRSU reduces computational overhead by over 25x compared to full VLM updates, without sacrificing performance. Experimental results on VQA tasks in the few-shot continual learning setting, validate LoRSU's scalability, efficiency, and effectiveness, making it a compelling solution for image encoder adaptation in resource-constrained environments.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have revolutionized natural language understanding and generation, enabling significant advancements across diverse applications. As intelligent agents are increasingly expected to operate in real-world multimodal environments, integrating visual understanding becomes essential. Vision-Language Models (VLMs) extend LLMs by incorporating visual information, either through pre-trained vision encoders or end-to-end multimodal training. These models have demonstrated state-of-the-art performance on vision-language tasks such as visual question answering (VQA) and image captioning, highlighting their potential for general-purpose multimodal reasoning (Chen et al., 2024; Wang et al., 2024a).\nApproaches that rely on pretrained image encoders typically use variants of the CLIP model (Radford et al., 2021), which is kept frozen in the vision-language binding process (Liu et al., 2024b). CLIP is a widely deployed vision transformer that has strong zero-shot capabilities in various tasks and domains. However several existing works have highlighted various weaknesses of CLIP on out of domain data (Liu et al., 2024b; Zhu et al., 2023; Chen et al., 2023; Li et al., 2023; Tong et al., 2024). When deploying VLMs as visual assistants in new domains, it is then expected that VLMs can be updated using a few images gathered from the target environment whenever deficiencies are noted.\nContinual learning allows a model to be continuously updated as new data from new tasks or domains are encountered. Recent literature on continual learning (CL) of vision language models focus on updating either the LLM (Srivastava et al., 2024) or language projection layers (Das et al., 2024), maintaining a frozen image encoder.\nIn vision language models, the LLM component provides reasoning and factual knowledge, while the image encoder's role is to extract robust and accurate visual features. In this work, we argue that adapting VLMs to new visual domains or tasks is more effective and efficient when the image encoder is updated rather than the LLM. Figure 1 highlights this issue using images from the Toyota Smart Home dataset (TSI) (Das et al., 2019) dataset: in the first column, LLaVA (Liu et al., 2024a) struggles to recognize the person's action in the original image but accurately describes the same action in a generated image from OpenAI's DALL-E 2. This example underscores that the visual shift, rather than the LLM's understanding of the action, is the main source of weakness.\nMotivated by the above limitations, we introduce a novel parameter-efficient fine-tuning (PEFT) method called LORSU (Low-Rank Adaptation with Structured Updates) for selectively updating specific modules of the transformer blocks of image encoders within VLMs. The right column of Figure 1 illustrates the (correct) responses of LLaVA after updating the image encoder separately with our method on"}, {"title": "2. Related Work", "content": "Continual Learning. Our work falls within the continual learning literature, where a model needs to be updated incrementally as new data arrive, accumulating knowledge over tasks and reducing forgetting of previously acquired information (De Lange et al., 2021).\nContinual Learning for Multimodal Language Models. Wu et al. (2024) provide a survey on continual learning for LLMs, highlighting challenges of computational efficiency and forgetting. Srivastava et al. (2024) explored continual multi-modal learning on VQA datasets, keeping the vision encoder frozen. He et al. (2023b) examined continual instruction tuning with sequential VQA datasets, proposing a method where the projection head is expanded for each new task. Das et al. (2024) introduced a pseudo-rehearsal strategy for vision-language models, updating only the language projection layer. Our method adapts only the vision encoder, preserving language capabilities.\nContinual Learning with Few-Shot Updates. Verwimp et al. (2023) posits that an ideal continual learning solution would enable continual correction of model's mistakes at a lower computational cost than retraining from scratch. However, most continual few-shot learning from pre-trained models focuses on classification tasks and introduces solutions that cannot scale to large multimodal models. Panos et al. (2023) update the vision encoder on the first task only, later adapting a covariance matrix for incoming tasks. Goswami et al. (2024) calibrate the covariance matrix for new classes based on semantic similarity. Zhao et al. (2024) introduce few and slow updates, proposing a transfer loss function and a cross-classification loss to mitigate catastrophic forgetting. Few-shot updates can also be viewed through the lens of model editing (Sinitsin et al., 2020). MEND (Mitchell et al., 2022) scales model editing to large language models by transforming the gradient obtained from fine-tuning, through a low-rank decomposition fed to auxiliary networks designed to make fast, local edits to a pre-trained model, requiring a set of unrelated examples to prevent forgetting. ROME (Meng et al., 2022) applies causal tracing to identify layers where incorrect factual knowledge is stored, applying a low-rank update. However, ROME does not scale to continual updates or non-association types of updates. Cheng et al. (2023) studied multi-modal editing, showing negligible deterioration in multi-modal task performance when updating language models but severe forgetting when updating vision encoders. To the contrary, our method focuses on adapting the vision encoder rather than updating the factual knowledge in the LLM, yet achieving strong performance gains and negligible forgetting.\nContinual Learning of Pre-Trained Image Encoders. SPT (He et al., 2023a) estimates a mask of updates based on parameter sensitivity, performing low-rank or"}, {"title": "3. Low-Rank Adaptation with Structured Updates", "content": "Few-shot continual learning is a highly practical and challenging scenario, where models must incrementally adapt to new tasks with limited supervision while retaining previously acquired knowledge. This setting closely mirrors real-world applications, such as interactive AI assistants and autonomous systems, where models receive a continuous stream of novel data but only sparse supervision per update.\nTo address the challenge of efficiently fine-tuning large-scale visual encoders and transformer-based models under the few-shot continual learning setting, without causing catastrophic forgetting (i.e., degradation in performance on previously learned tasks), we propose a novel parameter-efficient fine-tuning method called Low-Rank Adaptation with Structured Updates (LoRSU).\nLORSU updates specific parameters within each transformer block in a resource-efficient manner, mitigating the risk of generic knowledge loss when fine-tuning for new tasks. Specifically, we selectively update a subset of parameters from the first linear layer in the MLP block of each transformer layer, as proposed in (Zhang et al., 2024). While this approach reduces the fine-tuning burden, it may limit model flexibility as the remaining parameters in the transformer block remain fixed. To enhance flexibility, we further update the most informative attention heads based on the gradient of the task-specific loss.\nMore specifically, let a dataset $D_t = \\{x_n, y_n\\}_{n=1}^l$ for the current task t where $x_n$ is an image with text description $y_n$. We define $L(\\theta; D_t) := L_t(\\theta)$ as the loss used for training the model and $\\theta \\in \\mathbb{R}^d$ is the full set of model's parameters. The standard Multi-head Self-Attention Mechanism (MSA) (Vaswani et al., 2017), comprised of H $D_h$-dimensional heads, is defined as the concatenation of multiple self-attention (SA) blocks where $q^{(i)} = W_q^{(i)}ZT, k^{(i)} = W_k^{(i)}ZT, v^{(i)} = W_v^{(i)}ZT\\in \\mathbb{R}^{D_h\\times N}$, are the query, key and value matrices, which are used to compute the self-attention outputs as follows\n$A^{(i)} = softmax(q^{(i)}k^{(i)}/\\sqrt{D_h}) \\in \\mathbb{R}^{N\\times N},$ (1)\n$SA_i(Z) = A^{(i)}v^{(i)} \\in \\mathbb{R}^{N\\times D_h}, i = 1,..., H.$ (2)\n$Z\\in \\mathbb{R}^{N\\times D}$ is the input matrix of N tokens of dimension D and $W_q^{(i)}, W_k^{(i)}, W_v^{(i)}$, and $W_o^{(i)}$ are the query, key, and value matrices of learnable parameters for head i, respectively. The final MSA function is defined as $MSA(Z) = Concat [SA_1 (Z), ...,SA_H(Z)]W_o\\in \\mathbb{R}^{N\\times D}, W_o\\in \\mathbb{R}^{HD_h\\times D}$\nSince we care to update the parameters of the heads that cause the largest changes in $L_t(\\theta)$, we compute the gradient of the loss with respect to the parameters of each head and then we update only those heads with the largest cumulative contribution to the loss change. Since the matrices $W_q^{(i)}, W_k^{(i)}, W_v^{(i)}$ are all the parameters of head i, we can define an importance score for each head by adding the squared values of their corresponding gradients $G_q^{(i)} = \\nabla_{W_q^{(i)}} L_t, $"}, {"title": "4. Experiments", "content": "We conduct a series of experiments under three different few-shot continual learning (CL) settings (CL-5, CL-20, and CL-50 shots) to thoroughly investigate the performance of LoRSU based on ten VQA datasets. By adopting this paradigm, we aim to assess the adaptability and efficiency of LoRSU under constrained learning conditions, ensuring that it remains both computationally feasible and effective in improving downstream performance. Specifically, we seek to address the following key questions: 1) How does our method, LoRSU, compare to other fine-tuning and CL baselines that use the CLIP loss to update the image encoder? 2) Does updating the image encoder separately and then reintegrating it into the corresponding VLM enhance downstream VQA performance? 3) What is the effect of using the perplexity loss instead of the CLIP loss to update the image encoder? 4) What are the benefits of choosing a subset of attention heads to be fine-tuned using LoRSU? and 5) What are the computational benefits of LoRSU?\n4.1. Datasets\nWe evaluate the performance of LoRSU on ten visual question answering (VQA) datasets falling in two broad categories: regular VQA datasets and classification datasets converted to VQA datasets.\nRegular VQA datasets. We consider four standard VQA datasets used for benchmarking VLMs' performance (Duan et al., 2024): VSR (Liu et al., 2023), the Visual Spatial Reasoning corpus consists of caption-image pairs labeled as True or False, where each caption describes the spatial relation between two objects in the image. VLMs evaluate whether the caption accurately reflects the image. HM (Kiela et al., 2020), the Hateful Memes dataset designed to detect multimodal hateful memes. MMVP (Tong et al., 2024), the Multimodal Visual Patterns dataset is a challenging benchmark which has been built on images that CLIP perceives as similar despite their clear visual differences. VisOnly (Kamoi et al., 2024), a novel dataset created to directly assess the visual perception abilities of VLMs in"}, {"title": "5. Discussion", "content": "In this work, we introduced LoRSU, a novel parameter-efficient fine-tuning method, specifically designed for few-shot continual learning scenarios with VLMs. Unlike existing approaches, LoRSU operates without relying on a replay buffer, making it uniquely suited for resource-constrained settings. Through extensive experiments, we demonstrate that LoRSU achieves both computational efficiency and the preservation of the model's generic knowledge by using"}]}