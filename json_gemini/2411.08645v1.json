{"title": "A System Level Performance Evaluation for Superconducting Digital Systems", "authors": ["Joyjit Kundu", "Debjyoti Bhattacharjee", "Nathan Josephsen", "Ankit Pokhrel", "Udara De Silva", "Wenzhe Guo", "Steven Van Winckel", "Steven Brebels", "Manu Perumkunnil", "Quentin Herr", "Anna Herr"], "abstract": "Superconducting Digital (SCD) technology offers significant potential for enhancing the performance of next generation large scale compute workloads. By leveraging advanced lithography and a 300 mm platform, SCD devices can reduce energy consumption and boost computational power. This paper presents a cross-layer modeling approach to evaluate the system-level performance benefits of SCD architectures for Large Language Model (LLM) training and inference. Our findings, based on experimental data and Pulse Conserving Logic (PCL) design principles, demonstrate substantial performance gain in both training and inference. We are, thus, able to convincingly show that the SCD technology can address memory and interconnect limitations of present day solutions for next-generation compute systems.", "sections": [{"title": "I. INTRODUCTION", "content": "The ever increasing demand for Generative Artificial Intelligence is shaping the roadmap of next generation compute systems [1], [2]. Large language models (LLMs) in particular have dominated this domain given their versatility and performance scaling [3]. However, training typical LLMs is very compute intensive and requires processing a huge amount of data with significant compute time resulting in an unprecedented level of carbon footprint [4]. For instance, training a GPT-3 model costs around 10M USD with an estimated energy consumption of ~1300 MWh [5], [6]. While LLM inference is computationally cheaper compared to training, the cumulative cost is extremely high in the long run, as multiple users queries are serviced [7]. This is obviously not sustainable given the trend of increasing LLM size [8]. Furthermore, the astronomical total cost of ownership (TCO) for advanced CMOS technology [9] used to build training and inference hardware, only serves to exacerbate the problem. Finally, semiconductor technology constraints like memory and interconnect scaling walls [10], [11] adversely affect future AI models that will become increasingly memory or communication bound [8], [12], requiring massive bandwidths to feed compute. Thus, there is a desperate need for a paradigm shift in compute system technology to tackle this global challenge.\nSuperconducting Digital (SCD) Electronics [13]\u2013[16] has been one of the promising beyond-CMOS technologies with a potential for tremendous improvements across the whole stack. Unique physics and material advantages facilitate active devices with 'sub-attoJoule' energy scales (at 'ps' time scales), quantum accurate encoding of digital information [13]-[15], wires with negligible dissipation and dispersion up to 100's GHz frequencies. The energy dissipation per switching event in SCD does not depend on the process node (like CMOS) but instead is relative to thermal noise. Given these benefits, superconducting architectures stand out by being able to operate at ~20\u00d7 higher frequencies for a fraction of the on-chip power (100\u00d7 less) [17], [18] and have 10000\u00d7 more energy efficient communication at the on-chip clock rate [19], [20]. Cryogenic cooling can be applied to the entire system as opposed to individual dies, resulting in extremely high volumetric packaging density with all components of a system being both physically and electrically close [21].\nDespite such promising device data, it is unclear how such technology primitives would pan out when estimating overall performance while designing a full system for future AI or HPC workloads. It is imperative to perform early cross-layer system level evaluations to understand the competitiveness of SCD technology, compared to existing mature technologies. Our key contributions here are as follows:\n\u2022 Construction of SCD-based system-architecture for LLM training and inference, via parametric architectural building blocks in a bottom-up manner, from SCD devices, pulse-conserving logic (PCL), superconducting Josephson SRAM (JSRAM) and packaging solutions.\n\u2022 Performance projection of the SCD system architecture for LLM training and inference based on an analytical performance modeling framework.\n\u2022 Showcasing the potential improvements when using SCD system compared to contemporary GPU systems."}, {"title": "II. BACKGROUND", "content": "A. Advanced SCD process technology\nThe fabrication of Nb based SCD devices have been reported previously. However, these have a temperature budget of <200\u00b0C which limits effective scaling to lower dimensions for advanced process nodes, thus not meeting the process specifications required for complex integrated circuits [22]-[24]. To address these challenges, we have developed NbTiN based SCD building blocks that allow advanced integration (meeting the requirements for advanced digital systems). Compared to the SOA Nb, NbTiN is a stable material, compatible with temperature budget of 420\u00b0C and is scalable down to critical dimensions (CDs) of <50nm [25] [26]. Specifically, we use a legacy semi-damascene [27] [28] integration process"}, {"title": "B. Pulse Conserving Logic and JSRAM", "content": "Digital circuits have been constructed in various low-power superconducting logic families like RSFQ, RQL, and AQFP [16]. These efforts have been held back by the lack of large scale design automation targeting superconducting logic. Superconducting circuits that are DC-powered waste energy in power distribution, while circuits that are AC-powered face challenges in static timing analysis and automated synthesis.\nPulse-conserving logic (PCL) [18] is a SCD logic family that enables automated circuit design using commercially available tools. In a PCL circuit, each digital signal comprises two physical wires (positive and negative sense) and inversion is achieved by swapping them. This dual-rail encoding eliminates the delay in logical inversion (inherent to the data encoding in other AC powered SCD logic families) and greatly simplifies mapping of PCL gates into a standard cell library, a subset of which is shown in Fig. 1f and 1g. The PCL library is compatible with a slightly modified RTL-GDS flow. Fig. 1h illustrates the translation of an RTL design to a PCL circuit using off-the-shelf synthesis followed by a modification stage that performs phase assignment, phase matching, and single-to-dual rail conversion. A commercial place and route solution that can route wires with targeted inductance was used. JSRAM [18] is a SCD memory technology complementary to PCL, which, implemented in the advanced NbTiN/aSi/NbTiN process described in the previous subsection, enables memory density of 4 MB/cm\u00b2, a 600\u00d7 increase over older SFQ-compatible memory technology, with XY addressing analogous to CMOS SRAM. It represents a key intermediate stage of the memory hierarchy with lower density but higher locality than cryo-DRAM."}, {"title": "III. DESIGN SPECIFICATIONS", "content": "In this section, we describe the fundamental design blocks that serve as inputs towards building the SCD blade for LLM training and inference.\nHigh Throughput Compute Core: A regular array of bf16 MAC units is used for a TPU like high throughput compute core. Our bf16 MAC (8-bit add, 8-bit multiply and 32 bit accumulate) consists of ~8k JJs. This regular MAC array architecture is banked for scale-out. Due to the lack of RC overhead for superconducting data transmission lines and utilization of dedicated low latency memory hierarchy (detailed below), we achieve a very high packing density and utilization for the high throughput compute core. The peak floating point (bf16) performance achieved is ~2.45 PetaFLOPs (30GHz\u00d7400k MACs\u00d72Ops/MAC) at 80% utilization of the MACs (>400K) in a 144mm\u00b2 die footprint.\nControl Complex: A simple dual core (in-order) complex manages the distribution of kernels fragments and appropriate instructions to the high throughput core. The 'Control Complex' maintains local directories for coherency for the global addressing. It also assists in power/clock gating (if any) locally.\nMemory Hierarchy: The 8JJ single port (1R/1W) JSRAM configuration detailed in Fig. 1e serves as the 'High Density' (HD) on-chip memory variant for our SCD stack. HD JSRAM is used for the L1 and L2 data caches in our study. Multi-port variants, 2R/1W JSRAM (14 JJs) and 3R/2W (29 JJs), serve as the 'High Performance' (HP) on-chip memory variants. These are used for register files, high speed buffers and L1 instruction caches in our system configurations based on the required parallelism, performance and capacity specifications.\nSwitch: Like conventional designs, our SCD switch consists of a central crossbar that connect the input ports (+ associated buffers) to the control unit and output ports (+ associated buffers). The building block of the crossbar is in-turn the superconducting MUX based cross-point unit. Our crossbar is hierarchical in nature with a first level of cross-point units used to route each packet to the appropriate output port, and the second level serving as an aggregation point.\nMain Memory Datalink: A custom interface, as shown in Fig. 2a is designed for data transfer between the 4K (Superconducting Compute) and 77K (Cryo-Memory) domains in our system. Physically, this dual-temperature interface connects the interposers in both domains via Cu transmission lines across a glass bridge. This datalink is DC coupled and thus requires no specific encoding. It performs translation for voltage levels, data rates, data direction and also protocol based on the amplification and PHY specifications at either ends(Fig. 2b). For the given baseline specifications, our datalink can achieve a peak bidirectional bandwidth of 30TBps (20TBps Downlink and 10TBps Uplink). This can be increased or decreased based on the power budget, available metal layers, channel reach, reliability, noise & dispersion etc.\nCryo-DRAM block: The cryo-DRAM block consists of an array of cryo-DRAM packages integrated on a Si interposer. These have cryo-DRAM packages have no customizations or internal design changes and are simply regular DDR-X/LPDDR-X packages operating at 77K. As such these have inherent power benefits that have been extensively reported in [30]\u2013[32].\nA more detailed look into the power breakdown, micro-architecture and ISA lie outside the scope of this paper and will be pursued as future work."}, {"title": "IV. PROPOSED SYSTEM ARCHITECTURE", "content": "A. SCD Processing and Network Unit (SPU and SNU)\nA single SPU (detailed in Fig. 3a), consists of a high compute throughput die (section III), a host controller die, multiple HD JSRAM based Memory dies and HP JSRAM die all of which are vertically stacked by means of NbTiN through-silicon vias (TSVs). The HD JSRAM dies serve both, the private L1 dcaches of the high throughput and control complex cores, as well as the shared L2 of the control complex. The HP JSRAM die contains the register files and L1 icaches for both the high throughput core and control cores. The control complex as well as the local switch lie at the base of the SPU physical stack of dies. The SNU (Fig. 3a) is another vertical stack of dies with a base die serving as switch for off-node or main memory communications. The JSRAM dies in each SNU die-stack are composed of banked HD arrays and function as slices of the shared and distributed L2 cache for all the high throughput cores (per SPU) in the blade. These help in bridging the latency gap for off-blade communication.\nB. SCD blade and interconnect\nWe propose a SCD blade (Fig. 3d), where, a 2D array of SPUs are interconnected via their local switches to construct a 2D torus intra-node network. The SNU die-stack lies at the edges of the 2D array of SPUs, and can even enable potential vertical stacking of SCD blades. This is made possible by extending NbTiN TSVs in the SNU to connect to the substrate of the neighbouring blade (and optimizing its aspect ratio).\nC. System Architecture Parameters\nTo assess the potential of SCDs for LLM workloads, we instantiate the system with the baseline parameters specified in Fig. 3c. Our blade consists of 8\u00d78 SPUs (maximum ~100 per blade, limited by the interposer stitching). The Cryo-DRAM capacity per blade is 2 TB accounting for 8 \u00d7 8 quad die packages of LPPDDRx/DDRx bonded on a interposer (77K) similar to the Si interposer housing SPU arrays. Every SPU can access the shared DRAM space at a bandwidth of 0.47 TBps. The entire 8 \u00d7 8 blade can sustain a cumulative bi-directional main memory bandwidth of 30 TBps at an average read/write latency of 30 ns. Due to the unique nature of superconducting interconnects and packaging technology, we can scale both the effective DRAM and network BW as we scale the number of SPUs. The above specifications serve as the baseline for the explorations on LLM training and inference via our performance model."}, {"title": "V. ANALYTICAL PERFORMANCE MODELLING", "content": "In this section, we describe the principles of the performance modeling approach (Fig. 4). The measured SCD technology data (section II) is used to create the basic design blocks, which are further used to derive the high-level system architectural parameters, shown in Fig. 3c.\nGiven a workload (e.g., LLM training/inference), the modeling framework, Optimus, ingests a detailed task graph with the LLM model parameters such as number of layers, attention heads, hidden dimension, input/output shapes, sequence length, batch-size, working precision, etc. Using the above parameters and a chosen combination of parallelization strategies, such as data parallelism (DP), tensor model parallelism (TP) and pipeline parallel (PP), the workload is mapped onto the underlying system architecture. In DP, the model is replicated and data is sharded; in TP the model is sharded and data is replicated and in PP the model is sharded layer wise and data is divided into small chunks to inject in a pipeline fashion. Each parallelism option comes with its own advantages and trade offs [33]. At its core, Optimus relies on a hierarchical roofline model for a single accelerator to determine if a given kernel in the task graph is compute or memory (on-chip/off-chip) bound. For compute bound kernels, the execution time is primarily determined by the compute throughput, while for memory-bound kernels, it is dominated by the data transfer time from the respective memory level. For a given system architecture and workload, we assess the most optimal mapping, reducing communication overhead [34]. The performance modeling framework has been validated against"}, {"title": "VI. RESULTS AND DISCUSSION", "content": "Here, we present the projected performance numbers using the performance modeling framework. As described in the previous sections, the SCD system has a huge memory and network bandwidth advantage compared to contemporary systems, which memory bound applications can exploit. To test this hypotheses, we model both LLM training and inference on the SCD system and project its performance in comparison with equivalent number of GPUs (H100s: peak throughput of 0.9895 PFLOPs, DRAM bandwidth of 3.35 TBps).\nFirst, we look at LLM training to investigate the potential speed-up of SCD systems. The majority of the matrix multiplication (GEMM) kernels in LLM training are compute-bound on GPUs; however, the GEMMs involved in the attention computation and non-linear operations are typically memory bandwidth bound [36] (depends on kernel flop-to-byte ratio and hardware compute throughput-to-bandwidth ratio). Fig. 5, gives the achieved throughput in PFLOPs/SPU for training GPT3-(76B model) on 64 SPUs, considering fixed model parallelism with a TP degree of 8 and a PP degree of 8. We perform an exploration of the total effective bandwidth available to an SPU. The minimum memory bandwidth available to each SPU is 0.47 TBps as discussed in Sec. IV-C (Fig. 3c). We then perform a sweep for the memory bandwidth available per SPU from0.5 TBps to 64 TBps and estimate the achieved throughput. As seen in Fig. 5, the performance scales with bandwidth monotonically and tend to saturate around 2 PFLOPs/SPU at 16 TBps of bandwidth. To further analyze this, we look at the compute versus memory-boundedness of GEMMs involved in the forward pass as a function of the memory bandwidth. As seen in the inset, at lower bandwidths, most of the GEMM time is spent in memory-bound kernels,and thus performance can improve when bandwidth is increased. We observe a gradual crossover from a memory-bound scenario to compute-bound one for a memory bandwidth \u226516 TBps per SPU. The modest improvement in the performance beyond 16 TBps is due to the remaining memory-bound operations in the task-graph, for instance, non-linear operations like, softmax, layer-norm etc. Key takeaway: Achieved throughput grows with available bandwidth initially and slowly saturates as most kernels turn compute-bound vs being memory-bound.\nWe now compare the performance of training different GPT models on SPUs vs the same number of GPUs.Fig. 6, shows the break up of compute, communication and others (pipeline bubble + weight update time) for three different models (for TP=8, PP=8, DP=1 and B=64 and bf16 precision). We set the total effective bandwidth available to each SPU in a blade (64 SPUs) at 16 TBps. In all the three cases, we observe that the SCD system is significantly faster than the GPU system (64 H100s). The speed-up varies from 3.5\u00d7 \u2013 4.4\u00d7 for this particular set up and model sizes. As expected, the SCD system enables both faster compute and faster communication \u2013 the primary gain coming from faster data movement. The inset exhibits the achieved throughput in terms of PFLOPS per processing units for GPU and SCD systems with the same experimental set up. We should note that in this case the maximal achieved throughput is around 1.5 PFLOPs/SPU while in the previous set up it is around 2 PFLOPs/SPU. This is primarily due the increased batch size (64 versus 128), that leads to improved throughput for both GPU and SPU. However, the SCD systems benefit more where the data transfer overhead is larger.\nKey takeaway: SCD system promises to offer significantly higher throughput compared to contemporary GPU systems across different LLM models for training due to higher cumulative memory and interconnect bandwidth.\nWe now move to LLM inference. Inference is known to be a memory-bound workload due to minimal data reuse in the involved kernels [35] and thus, we expect SCD systems to perform better in this case vs LLM training. In, the following study, we vary the effective DRAM bandwidth per SPU from 0.5 TBps to 32 TBps to see its impact on the inference latency. Fig. 7 shows how the inference latency goes down as we increase the bandwidth seen by each SPU. Going from 0.5 TBps (8.8s) to 32 TBps (0.52s), we see a speed up of 17\u00d7, thus confirming the importance of memory bandwidth in inference. The performance scaling tend to saturate beyond 8 TBps since we start hitting the DRAM latency bound limit. Here, we consider the Llama-405B model with a batch size of 8, bf16 precision, summarization and prediction of 200 tokens; DRAM latency is set to 30ns. Tensor parallelism is utilized here, where the number of SPUs is same as the TP degree.\nSince the collective DRAM bandwidth to a blade can be very high for the given SCD architecture, we analyze the sensitivity of the achieved throughput with respect to DRAM latency keeping the effective DRAM bandwidth for each SPU at 16 TBps. Inset (a) in Fig. 7 shows that the achieved PFLOPs/SPU goes down almost linearly with increasing DRAM latency (varied from 10ns- 200ns). We see that lower DRAM latency is crucial to achieve high performance in the case of inference. Inset (b) shows the variation of achieved throughput with inference latency (H100 versus SPU) as we vary the batch size from 4 to 128 (DRAM latency is set at 30ns). As the batch size is increased, the achieved PFLOPs/SPU increases along with the inference latency- this trade off helps determining the number of queries that can be batched without sacrificing user experience.\nKey takeaway: LLM inference performance continue to scale with increase in available DRAM bandwidth when the DRAM latency is low.\nNext, we investigate the speed-up in inference across different LLM models (MoE-132B/38B, Llama-70B, Llama-405B) for different batch sizes at a single SCD blade level compared to equivalent number of GPUs (H100s). We see a massive speed-up of 9\u00d7-11\u00d7 depending on the LLM model (see Fig. 8a, DRAM latency is set to 30 ns and bandwidth to 16 TBps). SCD performs best for Llama-70B among these models since the fraction of communication overhead is maximal in this case. For the Mixture-of-Experts model, the speed-up is a bit lower than that for Llama models since only 4 experts are active among 16. Hence, the communication overhead is less compared to the other models. The right panel shows the robustness of the speed-up with respect to the batch size for Llama-405B model. In addition, we also show the required KV-cache size as a function of batch size. We see that, at a batch size of 128, the KV-cache size is very close the maximum memory capacity of 64 GPUs (5TB), thus potentially limiting scaling up of batch sizes further for GPU systems. The speedup of our SCD system gradually decreases for large batch sizes (till we hit the memory capacity limit of GPUs) relative to GPU systems since the compute to data-transfer ratio now goes up.\nKey takeaway: The SCD system offers even more performant execution of LLM inference compared to training on GPU systems (due to memory boundedness of inference). The speed-up is robust across different LLM models and batch sizes.\nLastly, we explore the potential use of the large L2 caches (~ 4.19 GB). The required kv-cache size for the popular llama models are, llama2-7B: 2 GB, llama2-13B: 3 GB and llama2-70B: 10 GB. Thus, one can possibly fit the entire kv-cache of the two smaller llama models (7B and 13B) onto the L2 cache of the SCD architecture. For GPU systems, the cache size is in the order of MBs (e.g., 50 MB for H100) and cannot be shared across GPUs. Thus, the GEMMs/GEMVs involving Key and Value can be accelerated by making them L2 or compute bound from a traditional DRAM bound case. Since L2 cache also comes with a huge bandwidth jump compared to DRAM, one can exploit it by properly managing the memory. Our early estimates suggest a speed-up of ~ 2-4\u00d7 for the relevant GEMMs/GEMVs (depending on the software overhead of launching the kernels) if such a scheme is implemented. Since we do not have a clear estimate of the software overhead in those cases, it is hard to model the impact accurately."}, {"title": "VII. CONCLUSIONS AND FUTURE OUTLOOK", "content": "In this paper, we present a comprehensive study on performance modeling of SCD technology for LLMs training and inference using a bottom-up approach to derive high level system architectural parameters based superconducting process and device level data. We show that the significantly high memory and interconnect bandwidth of the SCD technology can substantially accelerate both LLM training and inference, serving as a very promising solution to the memory and interconnect wall problems. Although, we limit this study to projecting the performance of a single SCD blade, we expect the performance to scale with the number of blades to be explored in our future investigations. Another interesting direction could be to look at the impact of huge JSRAM capacity on LLM inference exploiting its massive bandwidth and negligible latency. Such unusual SRAM capacity will further lead to new ways of mapping and memory management approaches."}]}