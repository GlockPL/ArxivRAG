{"title": "Are Expressive Models Truly Necessary for Offline RL?", "authors": ["Guan Wang", "Haoyi Niu", "Jianxiong Li", "Li Jiang", "Jianming Hu", "Xianyuan Zhan"], "abstract": "Among various branches of offline reinforcement learning (RL) methods, goal-conditioned supervised learning (GCSL) has gained increasing popularity as it formulates the offline RL problem as a sequential modeling task, therefore bypassing the notoriously difficult credit assignment challenge of value learning in conventional RL paradigm. Sequential modeling, however, requires capturing accurate dynamics across long horizons in trajectory data to ensure reasonable policy performance. To meet this requirement, leveraging large, expressive models has become a popular choice in recent literature, which, however, comes at the cost of significantly increased computation and inference latency. Contradictory yet promising, we reveal that lightweight models as simple as shallow 2-layer MLPs, can also enjoy accurate dynamics consistency and significantly reduced sequential modeling errors against large expressive models by adopting a simple recursive planning scheme: recursively planning coarse-grained future sub-goals based on current and target information, and then executes the action with a goal-conditioned policy learned from data relabeled with these sub-goal ground truths. We term our method as Recursive Skip-Step Planning (RSP). Simple yet effective, RSP enjoys great efficiency improvements thanks to its lightweight structure, and substantially outperforms existing methods, reaching new SOTA performances on the D4RL benchmark, especially in multi-stage long-horizon tasks.", "sections": [{"title": "Introduction", "content": "Offline reinforcement learning (RL) has emerged as a promising approach to solving many real-world tasks using logged experiences without extra costly or unsafe interactions with environments (Fujimoto, Meger, and Precup 2019; Levine et al. 2020; Zhan et al. 2022). Unfortunately, the absence of online interaction also poses the counterfactual reasoning challenge in out-of-distribution (OOD) regions, causing catastrophic failures in the conventional RL paradigm due to extrapolation error accumulation during value function learning using temporal difference (TD) updates (Fujimoto, Meger, and Precup 2019; Kumar et al. 2019; Li et al. 2023). To remedy this, Goal-Conditioned Supervised Learning (GCSL) has gained much attention as an alternative paradigm since it reformulates offline RL problems as sequential modeling tasks, where policy extraction can be learned in a supervised manner, thus bypassing problematic value update in conventional TD learning. However, this comes with the price of much higher requirements of accurately describing the data dynamics in sequential modeling, which naturally favors more expressive models. Recent advances in highly expressive models have yielded remarkable achievements across a diverse range of domains such as computer vision (Liu et al. 2021) and natural language processing (Vaswani et al. 2017), which have also been extended to offline RL to accurately capture policy distributions (Wang, Hunt, and Zhou 2022; Hu et al. 2023a; Hansen-Estruch et al. 2023; Ada, Oztop, and Ugur 2023; Wang et al. 2023; Chen et al. 2023), or reduce the accumulative compounding errors in sequential data modeling (Chen et al. 2021; Janner, Li, and Levine 2021; Janner et al. 2022; Ajay et al. 2022; Villaflor et al. 2022; Jiang et al. 2022; Mazoure et al. 2023; Niu et al. 2024). Although they sometimes provide encouraging improvements over previous approaches, they also suffer from noticeable performance deterioration when they fail to accurately represent the behavior policy and/or environment dynamics in long-horizon tasks.\nMoreover, the integration of expressive models in offline RL inevitably increases both the computational load and inference latency. This poses a significant challenge for many real-world applications that are latency-sensitive or resource-constrained. Besides, methods that employ such expressive models are notoriously data-hungry to train, making them impractical for scenarios with expensive samples (Zhan et al. 2022; Cheng et al. 2024). In Fig. 1, we compare key metrics of recent offline RL methods, including policy performance, training time, and inference latency. The use of large expressive models in prior offline RL methods such as DT (Chen et al. 2021), TAP (Zhang et al. 2022), TT (Janner, Li, and Levine 2021), and Diffuser (Janner et al. 2022) sometimes results in marginal performance gains. However, this incremental improvement is offset by the exponentially increased computational load and inference latency, particularly when compared to models that do not employ such expressive models. This prompts us to ask the question:\nAre expressive models truly necessary for offline RL?\nWhile prior attempts introduce highly expressive models to combat the accumulated compounding error in long-sequence modeling, these methods still operate in a fine-grained (step-by-step) manner (Chen et al. 2021; Wang, Hunt, and Zhou 2022; Janner et al. 2022), which is inherently tied to the temporal structure of the environment and is susceptible to rapidly accumulated approximation errors over longer horizons (Levine et al. 2020). In this study, we provide a novel perspective to avoid step-wise accumulated compounding error in sequential modeling while only relying on a lightweight model architecture (as simple as 2-layer MLPs). The core of our method is the Recursive Skip-step Planning (RSP) scheme, which employs an iterative two-phase approach to solve tasks: it recursively plans skip-step future sub-goals conditioned on the current and target information, and then executes a goal-conditioned policy based on these coarse-grained predictions. During the recursive planning phase, RSP bypasses step-wise sub-goal prediction and focuses more on longer-horizon outcomes through recursively skipping steps. In essence, RSP can generate long-horizon sub-goals with just a few planning steps, which uses exponentially fewer steps than what is required by previous fine-grained sequence modeling methods, therefore smartly bypassing the long-horizon compounding error issue while enjoying great computational efficiency.\nRSP can be easily implemented using simple two-layer shallow MLPs for recursive skip-step dynamics models and the goal-conditioned policy. The entire learning process can be conducted in a supervised learning fashion, eliminating the need for complex stabilization tricks or delicate hyperparameter tuning in value learning. Notably, RSP not only exhibits great training efficiency but also enjoys very low inference complexity, while it achieves comparable or even superior performance against existing offline RL algorithms on D4RL benchmark (Fu et al. 2020), including those equipped with expressive models. This advantage is particularly evident in complex tasks such as AntMaze and Kitchen, demonstrating the effectiveness of the long-horizon modeling capability by adopting our recursive skip-step planning scheme."}, {"title": "Related Work", "content": "Model-free Offline RL. Most prior methods incorporate pessimism during training to alleviate the distributional shift problem (Levine et al. 2020). One solution is leveraging various behavior regularizations to constrain the learned policies close to the behavior policy in offline dataset (Fujimoto, Meger, and Precup 2019; Wu, Tucker, and Nachum 2019; Kumar et al. 2019; Fujimoto and Gu 2021; Li et al. 2023). Some other works introduce pessimism during policy evaluation by assigning low confidences or low values for the value functions in out-of-distribution (OOD) regions (Kumar et al. 2020; Kostrikov et al. 2021; Lyu et al. 2022; An et al. 2021; Bai et al. 2021; Niu et al. 2022). In-sample learning methods (Kostrikov, Nair, and Levine 2021; Xu et al. 2023, 2022; Garg et al. 2023; Xiao et al. 2023; Wang et al. 2024; Zheng et al. 2024) have recently emerged as an alternative, optimizing on samples exclusively from the offline dataset and thus eliminating any OOD value queries. Moreover, an alternative in-sample learning approach performs RL tasks via supervised learning by conditioning other available sources, also called Reinforcement Learning via Supervised Learning) (RvS) (Kumar, Peng, and Levine 2019; Schmidhuber 2019; Emmons et al. 2021; Feng et al. 2022). These approaches are super stable and easy to tune compared to other methods. Existing methods, however, rely on shallow MLPs to model their actors, performing well on simple tasks but falling short on more complex long-horizon planning tasks, such as AntMaze and Kitchen tasks. To combat this, one popular direction is to increase policy capacity by employing highly expressive models to accurately approximate the actor, which obtains certain degrees of performance gains (Wang, Hunt, and Zhou 2022; Hansen-Estruch et al. 2023; Ada, Oztop, and Ugur 2023; Chen et al. 2023; Lu et al. 2023). However, this marginal improvement comes at the cost of extra model complexity and exponentially increased computational burden, inevitably limiting their applications.\nSequential Modeling for Offline RL. Different from traditional TD methods, this avenue treats offline RL problems as general sequence modeling tasks, with the motivation to generate a sequence of actions or trajectories that attain high rewards. In this formulation, shallow feedforward models are typically believed to suffer from a limited modeling horizon due to accumulated modeling errors caused by deficient model capacities (Janner et al. 2019; Amos et al. 2021; Zhan, Zhu, and Xu 2022). To combat this, recent innovations in sequential modeling techniques shift towards achieving precise long-horizon modeling through the use of high-capacity sequence model architectures such as Transformer (Chen et al. 2021; Janner, Li, and Levine 2021; Jiang et al. 2022; Wang et al. 2022; Konan, Seraj, and Gombolay 2023; Hu et al. 2023b; Wu, Wang, and Hamaya 2023; Villaflor et al. 2022) and diffusion models (Janner et al. 2022; Ajay et al. 2022; Niu et al. 2024). However, Fig. 1 manifests that the adoption of expressive models imposes a large amount of computational load and complexity. In this study, RSP adopts a recursive coarse-grained paradigm for sub-goal prediction solely with shallow 2-layer MLPs, achieving exceptional results while bypassing training inefficiency and high inference latency issues led by expressive models."}, {"title": "Preliminaries", "content": "In this paper, we consider the standard Markov Decision Process (MDP), denoted by M = (S, A, T, p, r, \u03b3), which is characterized by states s \u2208 S, actions a \u2208 A, transition probabilities T(st+1|at, st), initial state distribution p(so), reward function r and the discount factor \u03b3\u2208 (0, 1). In each episode, the agent is given a reward function r(st, at) and embodies with a policy \u03c0(\u03b1 | s) to interact with the environment. Let T := (so,A0, s1, A1,\u2026\u2026) denote an infinite length trajectory. We employ \u03c0(\u03c4) to represent the probability of sampling trajectory 7 from the policy \u03c0(\u03b1 | s). The primary goal is to optimize the expected cumulative discounted rewards incurred along a trajectory, where the objective and the corresponding Q-functions are expressed as:"}, {"title": "RL via Goal-Conditioned Supervised Learning", "content": "Reformulating RL as GCSL, previous works (Schmidhuber 2019; Emmons et al. 2021) typically employ hindsight relabeling to perform conditional imitation learning. The core idea underlying this approach is that any trajectory can be considered a successful demonstration of reaching certain goals, i.e. future states or accumulating rewards within the same trajectory. GCSL demonstrates significant potential in the offline setting, largely due to the availability of that goal information (Kumar, Peng, and Levine 2019; Chen et al. 2021; Emmons et al. 2021; Feng et al. 2022).\nThe general procedure of GCSL is simple and straightforward, as shown in 1, consisting of two main steps. In the first step, GCSL relabels the dataset by adding an additional goal information e to each state-action pair and forms a state-action-goal tuple, i.e., (s, a, e). The goal can be the target state, cumulative return, or language description (Lynch et al. 2020; Ghosh et al. 2021; Kumar, Peng, and Levine 2019; Chen et al. 2021; Feng et al. 2022), if applicable. For the state st, the goal et is randomly sampled from the future steps from the current step t along a valid trajectory in the dataset. The key idea behind this resampling is to consider any future goal et is reachable by taking action at and state St. In the second step, GCSL learns a goal-conditioned policy within the relabeled dataset via maximum log-likelihood:"}, {"title": "Recursive Skip-Step Planning", "content": "Why Non-recursive Planning Might Struggle?\nFine-grained planning using single-step dynamics f(St+1 St,g) often faces significant challenges due to the rapid accumulation of step-wise errors in sequential modeling, led by such a limited planning horizon. Although expressive models like Transformers (Chen et al. 2021; Janner, Li, and Levine 2021) and Diffusers (Janner et al. 2022; Ajay et al. 2022) aim to mitigate these issues by modeling long-term dependencies, their complex architectures significantly increase computational load. This approach ultimately struggles to balance prediction accuracy with efficiency, remaining constrained within the limitations of fine-grained sequential modeling.\nTo overcome this, coarse-grained planning uses skip-step dynamics, predicting intermediate sub-goals via f(St+k|St,9) with an extended planning horizon k. However, setting an appropriate skip-step horizon k is non-trivial: too short a horizon fails to alleviate error accumulation as in fine-grained planning, while too long a horizon weakens the learning signal, compromising policy performance in long sequential tasks. Thus, a delicate balance must be achieved between the prediction stability of the sub-goal St+k and the control precision of the policy \u03c0(at|St, St+k, g). Non-recursive planning may struggle to select sub-goal information that provides both adequate long-term guidance from the final goal and sufficient supervision for current action.\nCoarse-grained Planning in a Recursive Way\nThe dilemma for non-recursive planning suggests using a higher-level dynamics model to predict distant sub-goals f'(st+k'|st, g) and conditioning intermediate predictions on the current-level dynamics f(st+k|St, St+k', g). This hierarchical approach stabilizes sub-goal predictions St+k in the same way that sub-goals enhance action extraction \u00e2t. By recursively introducing next-level sub-goal prediction with more distant horizons, sub-goal approximation error is reduced at each level. As these sub-goals approach the final target, their error also inherently diminishes. Planning in such a recursive manner thus effectively mitigates error accumulation in long-horizon tasks.\nIn the RSP framework, each current state st in the dataset is first relabeled with a fixed-horizon future target state St+k as the sub-goal, resulting in an augmented dataset D = {(St,at, St+k, 9)}. This allows us to revise the optimization objective of goal-conditioned policies in Equation 2 to focus on goal-conditioned sub-goal prediction:\nAs explained in the previous section, planning with a learned coarse-grained dynamics model f is insufficient for balancing the minimization of action and sub-goal prediction errors. To further reduce the sub-goal prediction error, we extend it to a recursive skip-step prediction strategy, as depicted in Fig. 2, predicting high-level sub-goals to stabilize the lower-level sub-goal predictions in sequential modeling. To standardize the recursive process, we set K(n) := t + K/2n, n \u2208 [0,..., N \u2013 1] where N is the recursion depth, K is the skip step of the highest-level sub-goal, and SK(n) are sub-goal ground truths. The highest-level sub-goal skips K steps while the lowest-level sub-goal skips k = K/2N-1 steps from the current step. For N-level recursion, we need to relabel the dataset D = {(st, at, g)} with additional N skip-step sub-goal ground truths:\nwhere k(0) = (st, g); \u2200n \u2208 [1,..., \u039d], \u043a(n) = SK(n-1) U K(n-1) contains all the information required by the n-th level dynamics model to fit the sub-goal ground truth SK(n-1). For brevity, we omit t and refer to \u03ba(n) in the following descriptions. With all the definitions above, the highest(first)-level coarse-grained dynamics model could be denoted by fo(SK(0)|st, g). Subsequently, we can learn more coarse-grained dynamics models to push the limits of the benefits of coarse-grained planning, as depicted in Fig. 2. In terms of the n-th dynamics model, the recursion solution naturally extends to the optimization objective in Eq. 5:\nwhere the current level sub-goal generation is conditioned on all the sub-goals predicted with higher-level dynamics for stability, which proves a significant design in our experiments.\nDuring the evaluation process, we can formulate the recursive sub-goal planning as:\nwhere (0) = (st,g); \u2200n \u2208 [1,..., \u039d], \u03ba(\u03b7) = SK(n-1) U K(n-1). Eventually, we get the predicted sub-goals and leverage K(N) for policy extraction."}, {"title": "Policy Extraction", "content": "Different from other planning methods, the coarse-grained planning approach employed by RSP does not explicitly integrate the corresponding policy into the learning process, necessitating a separate policy extraction step. To maintain the simplicity and efficiency of RSP, we extract the policy by maximizing the log-likelihood within a supervised learning framework. We introduce a goal-conditioned policy, distinct in that it incorporates all the sub-goals planned from prior coarse-grained dynamics models:\nAt the evaluation stage, given the current state st, the action is determined by both the coarse-grained dynamics model and the goal-conditioned policy with:\nwhere \u03ba(N) = (St, SK(N-1), ..., SK(0), g) is obtained from Eq. 6 recursively.\nThe final procedure in Algorithm 2 consists of three steps. Initially, it relabels the dataset D, augmenting each transition with the future skip-step sub-goal and final goal ground truths. Subsequently, RSP learns N coarse-grained dynamics models from Eq. 5 and a goal-conditioned policy from Eq. 7 that determines the action based on the sub-goals predicted with all the dynamics models. Finally, RSP plans the sub-goal sequence recursively with Eq. 6 and executes action from sub-goals with Eq. 8."}, {"title": "Discussions", "content": "In this section, we provide an in-depth explanation of why recursive skip-step planning might outperform from the perspective of model rollout prediction errors. We conduct experiments on different (lowest-level) skip-step horizons (k = 4, 8, 16, 32, 64) and recursion depths (N = 1, 2, 3, 4) in the long-horizon Antmaze-Ultra-Diverse task, where trajectories can extend up to 3000 steps. For example, [16,32,64] in Fig. 3 denotes N = 3 and k = 16. We plot the root mean squared errors (RMSE) between the model rollout predictions and the ground truths of skip-step state trajectories, averaged over 10 seeds. Specifically, we obtain the next skip-step prediction $t+k through the recursive approach of RSP; we iteratively query the next skip-step prediction \u015dt+nk from the last prediction $t+(n-1)k and finally obtain rollout sub-goal predictions st,..., $t+(n-1)k, St+nk, \u00b7\u00b7\u00b7, \u015ct+1024; then calculate RMSE between the predictions and ground truths.\nIn Fig. 3, as k increases from 4 to 64, the accuracy of the model rollout prediction improves, aligning more closely with the ground truth over longer-horizon rollouts. This suggests that coarse-grained prediction outperforms fine-grained prediction in mitigating accumulated compounding errors as the skip-step horizon k increases, thereby enhancing long-horizon planning capabilities. Besides, as the recursion depth N increases, we observe that the RMSE of model rollout predictions achieves a lower asymptotic value and remains closer to the ground truth over longer-horizon rollouts, demonstrating the effectiveness of recursive planning.\nChoices of recursion depth and skip-step horizon. At first glance, Fig. 3 indicates that increasing recursion depth and skip-step horizon improves the planning performance of RSP. However, these plots assume that an oracle policy model perfectly follow the skip-step state predictions. In practice, deviations from optimal state trajectories can easily occur due to suboptimal policies, especially when the policy model \u03c0(at St, St+k,\u2026\u2026, g) is conditioned on a distant skip-step state prediction st+k, which offers minimal guidance for current actions. Therefore, selecting recursion depth and horizon requires to strike the delicate balance between the expressiveness of policy models and the long-horizon capability of dynamics models. Given that we use a 2-layer MLP for policy learning for simplicity in experiments, we opt for k = 32 and N = 1 in recursive skip-step dynamics models across all tasks in order to provide sufficient long-horizon planning ability while easing the burden for policy learning. Practitioners using more advanced policy modeling and learning methods may consider enlarging the horizon and deepening the recursion process accordingly."}, {"title": "Experiments", "content": "In this section, we conduct comprehensive evaluations to showcase the effectiveness of RSP. Specifically, we compare RSP against other competitive baselines on the standard offline RL benchmark, D4RL (Fu et al. 2020). The results indicate that despite its simplicity, RSP can obtain on-par or superior performance compared to others using expressive models, especially on the long-horizon Antmaze and multi-stage Kitchen tasks (Section D4RL Benchmark Results). Moreover, RSP exhibits fast training and inference time, with training for only around 180 seconds and an inference latency of under 1ms (Section Training Cost and Inference Latency). At last, we provide extensive ablation studies on some critical components of RSP including the planning horizon and recursion depth (Section Ablation Studies).\nD4RL Benchmark Results\nEvaluation setups We evaluate the performance of various methods using a diverse set of challenging tasks from D4RL benchmark (Fu et al. 2020), including Antmaze navigation, Kitchen Manipulation, Adroit hand manipulation, and Mujoco locomotion tasks. The Antmaze navigation tasks require intricate long-horizon planning to guide an ant navigate from the starting point to the goal location, which are pretty challenging for traditional offline RL to solve. To test the planning capabilities on extremely long-horizon tasks (> 2000 steps), we consider a more extensive and challenging Ultra Antmaze environment. Kitchen tasks involve a 9-DoF Franka robot interacting with a kitchen scene to complete a combination of household sub-tasks, which requires multi-skill long-horizon composition ability. In contrast, the Adroit hand manipulation tasks involve controlling a dexterous robot hand to complete tasks like opening a door or rotating a pen, with planning horizons typically ranging from 100 to 200 steps, which are relatively simple. Finally, the Mujoco locomotion tasks are believed as the simplest tasks among all these tasks. They only focus on controlling different robots to move forward as fast as possible, which are relatively simple since the control pattern is repetitive. To assess performance, we normalize the final scores by following the benchmark standards (Fu et al. 2020), where 0 represents random policy performance and 100 means expert-level performance. We use 10 random seeds and 100 episodes per seed for every task in our evaluation, with mean and standard deviation reported. All the results are obtained on a server with 512G RAM, 4\u00d7 A800 PCIe 80GiB (GPU) and 2\u00d7 AMD EPYC 7T83 64-Core Processor (CPU).\nBaselines We compare RSP against the following state-of-the-art (SOTA) baselines. BC is the simplest imitation learning approach that imitates the offline dataset; RvS (Emmons et al. 2022) is an SOTA conditioned imitation learning method that casts offline RL as supervised learning problems; CQL (Kumar et al. 2020) is an offline RL method that penalizes Q-values for OOD regions; IQL (Kostrikov, Nair, and Levine 2021) is an SOTA in-sample learning offline RL method; DT (Chen et al. 2021) and TT (Janner, Li, and Levine"}, {"title": "Ablation Studies", "content": "In this section, we conduct extensive ablation studies on the critical components of RSP including the recursion depth N and the skip-step horizon of the lowest-level sub-goal for policy extraction k (referred to as \"horizon\" hereafter for simplicity). The results are presented in Fig. 4.\nWe first fix the horizon k = 8 and conduct ablation studies on different recursion depths, i.e. N = 1, 2, 3, 4. The results in Fig. 4(a) indicate that a deeper recursion process consistently improves performance. However, performance in some tasks tends to plateau once N > 2. This is because increasing recursion depth can introduce cumulative errors from each prediction level even though each gets more accurate, which may counterbalance the benefits of RSP for low-level policy learning and result in no further performance improvement. Then we can conclude the following implementation insights: (1) choose minimal recursion depth that achieves saturated performance: N = 2 is sufficient for most benchmark tasks or those with similar configurations. There is no need to risk RSP performance by unnecessarily increasing depth; (2) advance policy learning as mentioned in Section Discussions: Strong policy models may exhibit robustness to slight inaccuracies that accumulate in sub-goal predictions, enabling safe extension of recursion depth and fully harnessing the potential of RSP. Other design considerations in policy formulation, such as how to appropriately condition actions on predicted sub-goals, could further enhance performance.\nNext, we fix the recursion depth N = 1 and ablate on the choices of k = 1, 4, 16, 32. A larger horizon corresponds to more coarse-grained planning. We observe that the horizon has minimal impact on short-horizon tasks, such as Adroit and MuJoCo, as shown in Fig. 4(b). However, reducing the horizon significantly leads to performance degradation in long-horizon or multi-stage tasks, such as Antmaze and Kitchen, as seen in Fig. 4(c) and (d)."}, {"title": "Conclusion", "content": "In this study, we propose a novel recursive skip-step planning framework for offline RL, which leverages a set of coarse-grained dynamics models to perform recursive sub-goal prediction, and use a goal-conditioned policy to extract planned actions based on these sub-goals. Our method can smartly bypass the long-horizon compounding error issue in existing sequence modeling-based offline RL methods through hierarchical recursive planning, while still maintaining the advantage of learning in a completely supervised manner. Notably, even using lightweight MLP networks, our method can provide comparable or even better performance as compared to sequence modeling methods that use heavy architectures like Transformers or diffusion models, while providing much better training and inference efficiency. Our work highlights the need to rethink the existing design principles for offline RL algorithms: instead of relying on increasingly heavier models, maybe it's time to introduce more elegant and lightweight modeling schemes to tackle existing challenges in offline RL."}, {"title": "Additional Experimental Details", "content": "Training Implementations\nWe use feed-forward MLPs for all our policies and dynamics models, with two layer neural networks with 1024 units each and ReLU nonlinearities used for all tasks. We list the basic hyperparameters in Table 2. We try to keep the same series of hyperparameters for all the experiment tasks but in Kitchen environment we have to add dropouts and decrease the batch size for less overfitting due to the extremely limit dataset provided by D4RL (Fu et al. 2020). For the recursive dynamics models, [32] denotes \u201cRecursion step N = 1\" and \"Horizon k = 32\", which implies that we use one dynamics model to predict the probability of the state after 32 steps, i.e. f(st+32|st, g), and extract actions from \u03c0(\u03b1t|St, St+32, 9). Similarly, [8,16,32] denotes \u201cRecursion Depth N = 3\" and \u201cHorizon k = 8\". For consistency, we adopt [32] across all the tasks in our experiments and achieve superior performance.\nBaseline Scores\nMost baselines didn't report results of the extremely challenging Antmaze-Ultra task so we reproduce BC, DT and RvS and report results of IQL, TT, TAP from TAP paper (Zhang et al. 2022). We collect IQL results on other Antmaze-v2 tasks from DOGE (Li et al. 2023) since IQL paper reports scores on v0. We also report self-reproduced results on Adroit-Expert task due to lack of reported scores. Other results are from the original papers."}]}