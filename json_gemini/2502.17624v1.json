{"title": "THEORY-GUIDED PSEUDO-SPECTRAL FULL WAVEFORM\nINVERSION VIA DEEP NEURAL NETWORKS", "authors": ["Christopher Zerafa", "Pauline Galea", "Cristiana Sebu"], "abstract": "Full-Waveform Inversion seeks to achieve a high-resolution model of the subsurface through the\napplication of multi-variate optimization to the seismic inverse problem. Although now a mature\ntechnology, FWI has limitations related to the choice of the appropriate solver for the forward problem\nin challenging environments requiring complex assumptions, and very wide angle and multi-azimuth\ndata necessary for full reconstruction are often not available.\nDeep Learning techniques have emerged as excellent optimization frameworks. Data-driven methods\ndo not impose a wave propagation model and are not exposed to modelling errors. On the contrary,\ndeterministic models are governed by the laws of physics.\nSeismic FWI has recently started to be investigated as a Deep Learning framework. Focus has been\non the time-domain, while the pseudo-spectral domain has not been yet explored. However, classical\nFWI experienced major breakthroughs when pseudo-spectral approaches were employed. This work\naddresses the lacuna that exists in incorporating the pseudo-spectral approach within Deep Learning.\nThis has been done by re-formulating the pseudo-spectral FWI problem as a Deep Learning algorithm\nfor a theory-driven pseudo-spectral approach. A novel Recurrent Neural Network framework is\nproposed. This is qualitatively assessed on synthetic data, applied to a two-dimensional Marmousi\ndataset and evaluated against deterministic and time-based approaches.\nPseudo-spectral theory-guided FWI using RNN was shown to be more accurate than classical FWI\nwith only 0.05 error tolerance and 1.45% relative percent-age error. Indeed, this provides more\nstable convergence, able to identify faults better and has more low frequency content than classical\nFWI. Moreover, RNN was more suited than classical FWI at edge detection in the shallow and deep\nsections due to cleaner receiver residuals.", "sections": [{"title": "1 Introduction", "content": "Full waveform inversion (FWI) endeavours to attain a high-resolution representation of subsurface structures through the\napplication of multivariate optimization to the seismic inverse problem [1]. The foundational role of optimization theory\nwithin FWI stems from its essence in reconstructing the parameters of the investigated system from indirect observations,\nwhich are bound by a forward modelling process [2]. It is worth noting that the selection of the forward problem holds a\nconsequential sway on the precision of the FWI outcome. Particularly intricate environments necessitate more intricate"}, {"title": "2 Theoretical Background", "content": "The forward problem in FWI is based on the wave equation. It is a second order, partial differential equation involving\nboth time and space derivatives. For an isotropic medium is given by:\n\n$\\frac{1}{c(m)^2}\\frac{\\partial^2p(m,t)}{\\partial t^2} - \\nabla^2p(m,t) = s(m,t)$,\n\nwhere $p(m, t)$ is the pressure wave-field, $c(m)$ is the acoustic p-wave velocity and $s(m, t)$ is the source [11]. To solve\nthe wave equation numerically, it can be expressed as a linear operator.\nBased on the Born approximation in scattering theory [12], consider the first model calculated to be $x_0$. After the first\npass via forward modelling, the model needs to be updated by the model parameter perturbation $\\Delta x$. This newly\nupdated model is then used to calculate the next update and the procedure continues iteratively until the computed mode\nis close enough to the true model based on a residual threshold criterion. At each iteration k, the misfit function $\\pounds(x_k)$\nis calculated from model $x_{k-1}$ of the previous iteration giving:\n\n$\\pounds(x_k) = \\pounds(x_{k-1} + \\Delta x_k)$.\n\nAssuming that the model perturbation is small enough with respect to the model, Equation 2 can be expanded via Taylor\nexpansions up to second orders as:\n\n$\\pounds(x_k) = \\pounds(x_{k-1}) + \\delta x_k^T \\frac{\\partial\\pounds}{\\partial x} + \\frac{1}{2} \\delta x_k^T \\frac{\\partial^2\\pounds}{\\partial x^2} \\delta x_k$.\n\nTaking the derivative of Equation 3 and minimizing to determine the model update leads to:\n\n$\\delta x \\approx -H^{-1}\\nabla\\pounds,$"}, {"title": "3 Proposed FWI as a Theory-Guided DNN", "content": "Neural networks are a subset of tools in artificial intelligence which when applied to inverse problems can approximate\nthe non-linear function of the inverse problem $F^{-1} : D \\rightarrow M$. That is, using a neural network, a non-linear mapping\ncan be learned to minimize:\n\n$||m- g_{\\Theta}(d)||^2,$\n\nwhere $\\Theta$ the large data set of pairs $(m, d)$ used for the learning process [5]. Recurrent Neural Networks (RNNs) are a\nclass of neural networks designed to process sequences of data by maintaining an internal memory to capture temporal\ndependencies. They are particularly adept at handling tasks like natural language processing and time series analysis\n[14]."}, {"title": "3.1 Long Short-Term Memory (LSTM)", "content": "Standard RNN architectures suffer from the vanishing gradient problem [15]. Namely, the sensitivity of deeper neurons\neither decays or blows up exponentially as it passes through the recurrent connections [16].\nIn 1997, [17] introduce a modified architecture type known as Long Short-Term Memory (LSTM) which mitigates the\nvanishing gradient problem. This NN introduces a set of recurrent connections known as memory blocks (the input,\noutput and forget gates) and a cell state."}, {"title": "3.2 LSTM as a Substitute for Wave Propagation", "content": "Consider the discretized finite-difference stencil for wave propagation given as,\n\n$p^{n+1} = dt^2 \\lbrace c^2 \\nabla^2 [F^{-1} [k^2 p^n] + s^n] \\rbrace + 2p^n - p^{n-1},$\n\nit is clear how pressure waves p and source impulse s at current time step n are not affected by the future values n + 1,\nbut only dependent on the previous state of pressure at n - 1. This is, by definition, a finite impulse with directed\nacyclic graph under graph theory definitions [18]. With slight modification to the LSTM blueprint in Figure 1b, a Deep\nLearning architecture supporting forward modelling can be cast as a LSTM cell that considers the pressure wave at\ntime n 1, produces the modelled shot record at current time n and stores this in memory for the next step n + 1.\nMeasuring all outputs at each moment in time would equal to the measurements of the wavefield locally at a geophone.\nThis LSTM architecture is shown in Figures 2a as an unrolled graph and Figures 2b as the building block components\nwithin an LSTM."}, {"title": "4 Experimental Results", "content": "4.1 Forward Modelling using RNNS\nRNN should be able to model the different wave field components if it is to replace the forward modelling component. A\n25Hz Ricker wavelet was propagated through a 2D 1500ms\u00af\u00b9 constant velocity model with a multi-source\nmulti-receiver geometry setup. The 25Hz source wavelet goes into the hyper-resolution realm for FWI and is beyond\nthe resolution that will be investigated on the synthetic model, however, this allows for gauging the limit of accuracy.\nThis model setup was forward propagated for 5333 time-steps at 1ms, with a 10m grid spacing. Namely, this implies\nthat 5333 LSTM cells were employed for the forward modelling. The resulting direct waves\nwith True being the analytical solution calculated using a 2D Green's function, RNN Time and RNN Freq are the RNN\nimplementations for forward modelling using Time and Fourier spatial derivatives respectively. Qualitatively, there is\nno visible difference between either approach.\nReflected and transmitted arrivals were tested using a simple step velocity model ranging from 1500ms-1 to 2000ms-1\nas shown in Figure 4a.Figure 4b is the forward-modelled wavefield for the two receiver locations (RCV-1 and RCV-2), top and bottom respectively. RCV-1 at ground level interacts with the direct wave at 125ms and the reflected wave at\n250ms. RCV-2 is below the acoustic impedance layer at 30m and shows the transmitted wave. Comparing these to the\nanalytical solution, either are able to model the wave components perfectly.\nThe remaining wavefield components are scattering waves. A constant velocity model of 1500 ms\u00af\u00b9 was created\nwith a 1550ms-1 point scatterer. RNN implementations were modelled to be depended non-linearly\non the scattering amplitude and then approximately linearised. The results are given in Figure 5b. The direct wave\nwas not included in the scattered wavefield reconstruction. Similarly to previous components, scattering is modelled\nsuccessfully."}, {"title": "4.2 Gradient Comparison", "content": "Classical FWI approaches generally use the adjoint state method to calculate gradients or the finite differences approach\n(although computationally expensive), whereas DNN frameworks use automatic differentiation. [19] already showed\nequivalence for the time-domain, and we now confirm the same for pseud-spectral RNN approach.\nA random 1D model was generated, randomly perturbed and the gradient of the cost function was evaluated along the\ntrace."}, {"title": "4.3 Hyper-Parameter Tuning", "content": "Similarly to the approach shown in [6], a benchmark 1D 4-layer synthetic profile, with velocities [2, 3, 4, 5]kms\u00af\u00b9,\nwas used to identify the ideal parameters for the RNN architecture. This is illustrated as the Black line in Figure 7.\nClassical 1D second-order finit-difference modelling was used to generate the required true receiver data. Multiple\nlearning rates for the different loss optimizers were investigated to try and identify the ideal combination.Figure 7\nshows the best combination for all losses with an ideal batch size of three.\nLeft side of Figure 7 shows the inverted velocity profiles, with Red being the initial velocity profile. For Stochastic\nGradient Descent, the learning rates was found to be both between zero and one. This is as expected and follows\nconventional loss optimization. On the other hand, the other loss optimizers had to be scaled to beyond one due to the\nmagnitude differences brought by accumulated squared-norms of the gradients as investigated by [20]. This is allowed\nprovided the scaling coefficient is between zero and one. For Adagrad, following from [21], the $ \\beta $ hyper-parameter was\nfixed at 0.9 and learning rate found to be 20. Adadelta, RMSprop and Adam optimal learning rates were identified at\n1000, 1 and 2 respectively.\nThe right side of Figure 7 gives the loss progression. All optimizers iteratively reduce the error with additional shots\nand on similar scales. Stochastic Gradient Descent and Adagrad do this relatively sooner than the rest, yet the inverted\nvelocity is not as good as the other optimizers. RMSprop follows a rather slow gradual decrease in loss, which then\nsudden increases. This is expected given that RMSprop updates are derived from a moving average of the square\ngradients and require an inertial start.\nBased on this investigation, Adam with a learning rate of 2 was identified as the best optimizer. This provided the most\nstable inversion for either RNN Time or Freq, with the most update and reasonable error loss performance. Mis-match\nin the shallow part of the velocity is due to the choice of batch-size within the RNN update process. Figure 8 shows the\nAdam optimizer fixed with learning rate 2 and inverted for batch sizes ranging from one to five. The smaller the batch\nsize, the greater the error since the inversion is more localized and amplifies the gradient onset error. The larger the batch size, the better the inversion as more data is being used. This poses a limitation since batch size is limited by the Graphical Processing Unit RAM."}, {"title": "4.4 Marmousi Model Experiment", "content": "The Marmousi-2 model [22] was used to evaluate theory-guided RNN on an industry-standard dataset. This was\nre-sampled to a 50m\u00d750m grid and smoothed to create the initial model. These velocity models are plotted in Figure 9.\nTrue synthetic receivers were computed by forward modelling through the RNN framework. 56 shots at 300m intervals\nat depth 200m were generated with a Perfectly Matched Layer at the boundaries. Receivers were set at 50m intervals\nand modelled for 12s duration."}, {"title": "4.4.1 Training of RNN", "content": "As in standard RNN approaches, the receiver dataset was split into training and development datasets with a 75%-25%\nsplit. The training was run for 100 epochs, with early stopping on an NVIDIA Titan V Graphical Processing Unit\ncourtesy of Istituto Nazionale di Geofisica e Vulcanologia. Development loss was calculated every 5th training shot.\nFigure 10 gives the RNN performance for training and development datasets using Adam optimizer with learning\nrate of 2.0 and batch size 1. The horizontal labels shows the epoch number and respective number of shots evaluated\nfor training and development. Computational run times are of 14 hours per approach. Both RNN Time and RNN\nFreq follow similar reductions in loss per epoch and indicate that either implementation converge to an optimal loss.\nL-BFGS-B loss for classical FWI is shown and is discussed is in the next section."}, {"title": "4.4.2 Comparison with classical FWI", "content": "3.5Hz FWI with Sobolev space norm regularization [23] was used to compare against theory-guided FWI. This results\nin a minimum update resolution of 414m, and the iterative update process started from frequency 1Hz and iteratively\nupdated by a factor of 1.2. The optimization algorithm was L-BFGS-B, with 50 iterations per frequency band in each\nupdate. Forward shot modelling was done every 100m, starting from 100m offset, and receivers spaced every 100m.\nFigure 10 plots the cost function versus the number of shot evaluation equivalent for classical FWI and RNN. The RNN\nframework is more computationally efficient since either RNN approach converge significantly quicker than L-BFGS-B.\nRNN Freq provides a more stable convergence and is better performant then RNN Time."}, {"title": "5 Discussion", "content": "5.1 Inversion Paradigm\nTheory-guided inversion inherits advantages and problems from either inverse theory and deep learning. It faces\nchallenges of cycle-skipping and local-minima, whilst it benefits from the use of automatic differentiation to calculate\nthe gradient. This reduces development time as it avoids the need to manually implement and verify the adjoint state\nmethod. Furthermore, being at the intersection of physics and computer science, it is inherently strengthened by\ncontributions from two communities of researchers. This opens up the possibility of considering other deep learning\ntechniques such as dropout or other acyclic-graph architectures such as directed acyclic graphs [24].\nIn classical FWI, the wavefield at the final time step is affected by the wavefield during the initial time steps. Back-\npropagation must occur over the entire sequence of time steps for theory-guided RNN. Application of back-propagation\nthrough thousands of layers is not a typical application in deep learning applications and automatic differentiation is\nnot designed to efficiently handle such situations. Strategies common to other FWI frameworks to reduce memory\nrequirements could be translated into the field. Examples would include not saving the wavefield at every time step\n[25], applying compression to the wavefield time slices [26, 27], saving wavefields to disk rather then memory [28],\nand regenerating wavefields during back-propagation rather than storing them [29, 30]."}, {"title": "5.2 Training Datasets for Real Data", "content": "For theory-guided RNN, excluding part of the data from training for use as a development dataset is standard practice in\ndeep learning, but not within classical FWI. For a real-world problem, the size of the seismic dataset relative to the\nmodel parameters generally has fewer data samples and could potentially prove problematic. Hyper-parameter tuning\nfor the optimal parameters for RNN demonstrate that practice can result in convergence to a good model, yet this does\nnot prove a similar result is achievable when using the entire dataset."}, {"title": "5.3 Forward Modelling and Multiples", "content": "All shots considered within the forward problem for either FWI and RNN framework were within the water column for\nthe Marmousi model. This implies that receiver data have surface-related multiples, together with all other inter-layer"}, {"title": "5.4 Implications of Data Volume and Computational Power", "content": "More data is directly correlated with better modelling for NN frameworks, and this ability is limited by the resources\navailable. Similar to classical FWI, computational power is a limitation within the frameworks presented. This was\nalready identified within the RNN approach with the limit from the Graphical Processing Unit RAM, constraining the\nmodel size and batch processing. A larger batch-size for RNN processing would intuitively imply that the optimization"}, {"title": "6 Conclusion", "content": "In this manuscript, a theory-guided approach for FWI using RNN was derived. This was developed theoretically,\nqualitatively assessed on synthetic data and tested on the Marmousi dataset. Theory-guided RNN as an analogue\nof FWI was implemented for 2D experiments and different wavefield components compared to an analytical 2D\nGreen's function and time implementation. Based on these results, RNN Time is able to model the wavefield within a\nmaximum 0.06 error tolerance and 1.74% RPE. RNN Freq is overall more accurate with 0.05 error tolerance and 1.449%\nRPE. Assessment on the gradients indicates how the adjoint state and RNN Freq gradients in general overestimate\nfinite difference calculation, whilst RNN Time under-estimates it with an infinitesimal error. RNN Freq produced a\nperturbation on the onset of the gradient which was attributed to modelling artefact and could be mitigated in future\nversions of this approach. Based on the model size and compute available, the ideal loss was Adam with a learning\nrate of 2 and batch size of 1. Model batch size proved to be a limitation for practical implementations, yet RNN\nis computationally more efficient than the classical FWI presented in this work. RNN freq provides more stable\nconvergence and is better performant. Overall, RNN frameworks are able to identify faults, but amplitudes are not fully\ninverted properly.\nRNN approach benefits from the wider community of active researchers. The reduction in development time is a direct\nintegration from Computer Science to geophysics. Vice-versa, Deep Learning frameworks can adopt strategies common\nto FWI.\nThe forward modelling approach used through this work was critiqued for the use of multiples. Whether to use or not to\nuse multiples within forward modelling is model dependent and should be evaluated for RNN Freq. Similar to classical\nFWI, computational power was identifiable as a limitation within these DNN frameworks. Although this is currently a\nlimitation, it will not be in the near future due to the relative quick development of GPUs. A corollary to the whole\napproach was addressed in the form of the maturity of the approach. 35 years of advances applied to these frameworks\nwould be expected to yield very good results. Finally, other areas of DNN that can be applied to FWI were presented.\nAlternative architectures such as Transformers and use of Fourier Recurrent Units are readily available. Potential of\ntransfer learning and solving differential equations using NN were presented as future directions of research for these\nframeworks."}, {"title": "A LSTM Components", "content": "A.1 Forget gate\nThe forget gate uses a sigmoid function to decide what information should be passed between hidden states. Values\nfrom this gate range between 0 and 1, indicating the level of information to be forgotten.\nA.2 Input gate\nThe input gate uses a sigmoid activation function, accepts the previous hidden state and current input and decides which\nvalues will be updated. The current input and previous hidden state are passed into the tanh function to squeeze values\nbetween -1 and 1 and get a potential new candidate.\nA.3 Cell state\nThe cell state acts as a mechanism to transfers information through the sequence. This enables information from earlier\ntime steps to be available at later time steps, thus reducing the effects of vanishing gradient. The preservation of gradient\ninformation by LSTM is illustrated in Figure 16."}, {"title": "A.4 Output gate", "content": "The output gate determines the next hidden state. It uses a sigmoid activation on the current state and previous hidden\nstate, and multiples this new cell state with a tanh to decide which part of the data should be pushed forward through\nthe sequence."}, {"title": "A.5 RNN Hyper-Parameter Tuning", "content": "Similarly to the approach shown in [6], a benchmark 1D 4-layer synthetic profile, with velocities [2, 3, 4, 5] kms\u00af\u00b9,\nwas used to identify the ideal parameters for the RNN architecture. Classical 1D second-order FD modelling was used\nto generate the required true receiver data. Batch size is used as a discriminator throughout Figure 18. The results\nindicate that the larger the batch size used, the better the inversion as more data is being used. However, given fore-sight\nthat this hyper-parameter tuning will be used a large dataset that might not fit in Graphical Processing Unit RAM, this\nwas fixed at batch size one."}, {"title": "A.6 RNN Inversion Update Progress", "content": "Complementary to inverted Marmousi models in \u00a7 4.4.2, Figure 19 gives the update progress at epoch 10, 25, 40, 55,\n70, 85 and 100 for RNN Time and RNN Freq, together with residual. Furthermore, classical FWI progress is included\nat different update frequency scales. In addition, receivers are provided in Figure 20."}, {"title": "B Classical FWI", "content": "B.1 Inversion\nFWI with Sobolev space norm regularization was used as the deterministic version of FWI within this work. The\nmaximum frequency of the inversion process was set to be 3.5Hz. The iterative update process started from frequency\n1Hz and iteratively updated by a factor of 1.2 until reaching a maximum frequency of 3.45Hz. The optimization\nalgorithm was L-BFGS-B, with 50 iterations per frequency. Figure 21 is the loss update for L-BFGS-B and Stochastic\nGradient Descent. Figure 22 shows the progression of the frequency updates."}, {"title": "B.2 Ray-Tracing", "content": "Pre-cursor to FWI is ray-tracing modelling to assess areas of update from standard FWI formulation. Open source\nversion of fteikpy Python library provided by [41] was adapted and utilized on the Marmousi-2. This implementation\ncomputes accurate first arrival travel-times in 2D heterogeneous isotropic velocity models. The algorithm solves a\nhybrid Eikonal formulation with a spherical approximation near-source and a plane wave approximation in the far field.\nThis reproduces properly the spherical behaviour of wave fronts in the vicinity of the source [41]."}]}