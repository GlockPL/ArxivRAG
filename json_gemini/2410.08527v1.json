{"title": "SCALING LAWS FOR PREDICTING DOWNSTREAM PERFORMANCE IN LLMS", "authors": ["Yangyi Chen", "Binxuan Huang", "Yifan Gao", "Zhengyang Wang", "Jingfeng Yang", "Heng Ji"], "abstract": "Precise estimation of downstream performance in large language models (LLMs)\nprior to training is essential for guiding their development process. Scaling laws\nanalysis utilizes the statistics of a series of significantly smaller sampling language\nmodels (LMs) to predict the performance of the target LLM. For downstream\nperformance prediction, the critical challenge lies in the emergent abilities in LLMs\nthat occur beyond task-specific computational thresholds. In this work, we focus\non the pre-training loss as a more computation-efficient metric for performance\nestimation. Our two-stage approach consists of first estimating a function that\nmaps computational resources (e.g., FLOPs) to the pre-training Loss using a series\nof sampling models, followed by mapping the pre-training loss to downstream\ntask Performance after the critical \u201cemergent phase\". In preliminary experiments,\nthis FLP solution accurately predicts the performance of LLMs with 7B and 13B\nparameters using a series of sampling LMs up to 3B, achieving error margins of 5%\nand 10%, respectively, and significantly outperforming the FLOPs-to-Performance\napproach. This motivates FLP-M, a fundamental approach for performance\nprediction that addresses the practical need to integrate datasets from multiple\nsources during pre-training, specifically blending general corpora with code data\nto accurately represent the common necessity. FLP-M extends the power law ana-\nlytical function to predict domain-specific pre-training loss based on FLOPs across\ndata sources, and employs a two-layer neural network to model the non-linear\nrelationship between multiple domain-specific loss and downstream performance.\nBy utilizing a 3B LLM trained on a specific ratio and a series of smaller sampling\nLMS, FLP-M can effectively forecast the performance of 3B and 7B LLMs across\nvarious data mixtures for most benchmarks within 10% error margins.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) form the basis\nfor numerous real-world applications and scaling laws analysis\nserves as the foundation for LLMs develop-\nment.\nThe key idea of scaling laws involves train-\ning a sequence of language models (LMs) to\ngather data (e.g., expended compute and corre-\nsponding model performance). This data is then\nused to build a predictive model that estimates\nthe performance of a substantially larger target\nLLM.\nPrevious efforts focus on predicting the target\nLLM's pre-training loss and establish a power-\nlaw relation between the computational resource\nexpended (e.g., floating-point operations per second (FLOPs)) and the final loss achieved."}, {"title": "2 RELATED WORK", "content": "2.1 SCALING LAWS\nEstimating the performance of the target LLM prior to training is essential due to the significant\nresources required for pre-training. The scaling laws of LLMs guide the systematic exploration in scaling up computational\nresources, data, and model sizes. Previous efforts in this\nfield demonstrate that LLMs' final pre-training loss on a held-out validation set decreases with an\nincrease in expended FLOPs during pre-training. The following work subsequently establishes the scaling laws for computer vision\nmodels, vision-language models mixed quantization, graph self-supervised learning"}, {"title": "2.2 DATA MIXTURE", "content": "Creating the pre-training dataset necessities collecting data from different sources making the data mixture a critical factor in the\nstudy of scaling laws. propose the data mixing laws to predict the pre-training loss\nof the target LLM given the mixing ratios. build the regression model to predict\nthe optimal data mixture regarding the pre-training loss optimization, and further\nshow that the optimal data composition depends on the scale of compute. In this work, we focus on\nintegrating the data mixture factor to better predict the downstream performance."}, {"title": "3 FLP: DOWNSTREAM PERFORMANCE PREDICTION", "content": "We introduce a two-stage approach to predicting downstream performance in LLMs based on two\nestablished findings: (1) Predicting the target pre-training loss and establishing the power-law relation\nis feasible as it does not involve an emergent phase. (2)\nWhen pre-training loss goes below a task-specific threshold, there is an observed correlation between\npre-training loss and downstream task performance. In this sec-\ntion, we present FLP as a proof-of-concept for this framework with a straightforward implementation."}, {"title": "3.1 FLOPS \u2192 Loss", "content": "We follow the previous practice to use the analytical power law function to characterize the relation\nbetween expended FLOPs C and the pre-training loss L:\n$L(C) = \\left(\\frac{C}{C_N}\\right)^{\\alpha_N}$ , (1)\nwhere $C_N$ and $ON$ are constant terms to be estimated. In FLP, we train a series of N LMs within the\nsame model family in the same pre-training distribution, progressively increasing model size and\ntraining tokens to achieve even sampling. Then we measure their pre-training loss in our curated\nvalidation dataset to obtain N pairs of $(Ci, Li)$ to estimate the constants in Eq. 1."}, {"title": "3.2 Loss \u2192 PERFORMANCE", "content": "Based on our empirical observation of the scatter plots showing (pre-training loss, performance)\ndata points (see \u00a7A), we select the analytical linear function to characterize the relation between the\npre-training loss L on general validation data and the task performance P:\n$P(L) = w_0 + w_1 * L$, (2)"}, {"title": "4 VALIDATION OF FLP FRAMEWORK", "content": "4.1 SAMPLING AND TARGET LMS\nWe train a series of 12 sampling LMs up to 3B parameters to predict the performance of target LLMs\nwith 7B and 13B parameters. The configurations of LMs are shown in Tab. 1. We first determine\nthe number of training tokens required for the 7B LLM (approximately 180 times the model size),\nconsidering practical needs and inference-time costs. In real-world applications, prioritizing inference\nefficiency often involves training smaller LMs with a higher token-to-parameter ratio beyond the\noptimal factor of 20x. Our preliminary experiments indicate that scaling\nlaws remain applicable even in this over-training regime (within 2.8% error margins). We then\nproportionally scale down this number to determine the required training tokens for the sampling LMs."}, {"title": "4.2 DATA: PRE-TRAINING, VALIDATION, EVALUATION", "content": "Pre-Training We use the RedPajama v1 which consists of 1.2T tokens in total,\nand the data is sourced from Arxiv, C4, Common Crawl, GitHub, Stack Exchange, and Wikipedia.\nValidation We curate a validation dataset to measure the final pre-training loss, which includes 5\ndistinct domains: math, code, scientific paper, Wikipedia, and general language corpus. Specifically,\nwe utilize subsets from GitHub, ArXiv, Wikipedia, and the English portion of C4, all from the\nRedPajama validation sets, along with Proof Pile for the math domain."}, {"title": "4.3 EXPERIMENTAL SETTING", "content": "Baseline We consider directly using the expended FLOPs C to predict the downstream performance\nP, and experiment with the following analytical form for comparison:\n$P(C) = \\left(\\frac{C}{C_M}\\right)^{\\alpha_M}$, (3)"}, {"title": "4.4 RESULTS", "content": "The downstream performance prediction results are visualized in Fig. 2. Across all evaluation tasks,\nFLP fit curve can better predict the performance of target LLMs with 7B and 13B parameters using the\nsampling LMs up to 3B. In contrast, while FP more effectively fits the data points of sampling LMs,\nit has difficulty accounting for the \u201cemergent phase\u201d characterized by rapid performance shifts, due to\nthe scarcity of data points from this period. As a solution, FLP utilizes pre-training loss as a more fine-\ngrained indicator to monitor performance changes and effectively incorporates data from intermediate\ncheckpoints, enhancing sample efficiency. The evaluation results of relative prediction error are\nshown in Fig. 3. Unlike the suboptimal predictions of FP, FLP delivers precise forecasts, maintaining\nrelative error margins of 5% and 10% across all benchmarks for 7B and 13B LLMs, respectively.\nCompared to FP, FLP is less effective at fitting the data points of sampling LMs, especially in\nHumanEval and TriviaQA. The reason is that we do not align with the \u201cnon-emergent\" phase of the\nLoss-to-Performance curve, where LMs exhibit random performance when pre-training loss is beyond\nthe task-specific threshold. Thus, FLP predicts higher pre-training loss for LMs with fewer FLOPS,\nresulting in below-random performance. This issue is not within the scope of FLP, as it is specifically\ndesigned to predict the performance of LLMs trained with significantly larger FLOPs in practice.\nIn addition, we discuss additional results in Appendix for the presentation purpose since adding these\ndata points may distort the vertical axis scaling in Fig. 2. We compare FLP further with the analytical\nforms and approaches proposed in GPT-4 and Llama-3. The results are shown in \u00a7B and \u00a7C respectively. We also evaluate the feasibility\nof employing FLP to predict the performance of a 13B LLM on MMLU\nusing intermediate checkpoints from a 7B LLM (\u00a7D). Overall, the results demonstrate the general\neffectiveness and applicability of FLP."}, {"title": "5 FLP-M: DATA MIXING FOR DOWNSTREAM PERFORMANCE PREDICTION", "content": "Motivated by the encouraging results of FLP (\u00a74), we propose FLP-M, a fundamental approach to\nmeet the practical needs of integrating data from various sources. In our work, we focus on mixing general corpus with code data, considering two distinct\nyet overlapping data sources. This intersection offers a more realistic perspective than treating them\nas distinct domains as real-world corpus often spans multiple domains, necessitating\nan analysis of the interdependence between data sources when formulating our analytical functions.\nCompared to the straightforward implementation of FLP (\u00a73), FLP-M operates on fine-grained,\ndomain-specific pre-training loss, due to the observation that the average loss on the entire validation\nset fails to effectively reflect performance variations in downstream tasks in the data mixing context\n(\u00a77.2). This may be due to the fact that changes in pre-training data mixtures simultaneously impact\nmultiple capabilities of the LMs. For instance, an increase in code data loss coupled with a decrease in\ngeneral data loss may leave the average validation loss unchanged, yet result in LMs with distinct capa-\nbilities and downstream performance. Note that unlike the pre-training data mixture, the validation set\nis deliberately curated by domain, as creating smaller, domain-specific validation sets is manageable."}, {"title": "5.1 FLOPS \u2192 DOMAIN LOSS", "content": "Given the FLOPs CG spent on the general corpus and CC spent on the code data, we naturally extend\nthe power law function to the following analytical form to predict the domain-specific pre-training\nloss LD on domain D:\n$L_D (C_G, C_C) = \\left(\\frac{C_G + C_C}{C_T}\\right)^{a_C} \\times \\left(\\frac{C_G}{C_T}\\right)^{a_{C_1}} \\times \\left(\\frac{C_C}{C_T}\\right)^{a_{C_2}}$\nwhere CT, CG, CC, ac, $ac_1$, and $C_2$ are constants to be estimated. In FLP-M, we first select\na sequence of total compute {C}=1 spent on pre-training. For each selected Ci, we experiment\nwith various ratios to mix two data sources, and decompose Ci into CF and CF. We measure the\ndomain-specific pre-training loss LP on a domain-specific subset D of validation data to obtain (CG,\nCC, LP) data pairs. Then we can estimate the constants in Eq. 5. We also experiment with other\npotential analytical forms in \u00a77.2."}, {"title": "5.2 DOMAIN LOSS \u2192 PERFORMANCE", "content": "Given the pre-training loss {LD}B=1 on K domains, we train a two-layer neural network with a\nhidden layer size of 3 and the ReLU activation function to predict the downstream\nperformance. The network is optimized using the regression loss with L2 regularization and the\nAdam optimizer, employing a learning rate of 0.05 that linearly decays to 0 within\n2,000 steps and a weight decay of 0.01. In FLP-M, we adopt the same strategy as in FLP to fetch the\nintermediate checkpoints and only retain the results that the LMs achieve above-random performance\n(see \u00a73). Thus, for LM\u00bf, we can obtain a sequence of effective data points, where\nLP is the pre-training loss on domain D and P\u2081 is the LM's performance. Then we can use these\ndata points to train the neural network. We also explore other functions for fitting in \u00a77.2."}, {"title": "6 EXPERIMENT FOR FLP-M", "content": "6.1 SAMPLING AND TARGET LMS\nWe train a series of sampling LMs with sizes of {0.12B, 0.2B, 0.32B, 0.5B, 0.72B, 1B}, and the corre-\nsponding training token numbers are shown in Tab. 1. We train the LMs on the general and code data\nmixture with {0, 0.1, 0.2, 0.3, 0.4, 0.5} as the mixing ratios of code data to reflect real-world usage.\nWe also add one sampling LM of 3B size and 0.3 mixing ratio. For evaluation, we train 3B LLMs with\nthe other mixing ratios and a 7B LLM with 0.3 as the mixing ratio due to the limited compute budget."}, {"title": "6.2 DATA: PRE-TRAINING, VALIDATION, EVALUATION", "content": "Pre-Training For general corpus, we use DCLM, a curated high-quality pre-training\ncorpus including heuristic cleaning, filtering, deduplication, and model-based filtering. For code\ndata, we use The Stack v2 which initially contains over 3B files in 600+\nprogramming and markup languages, created as part of the BigCode project. We mix these two data\nsources to create the pre-training data mixture using the ratios specified in \u00a76.1.\nValidation We use the same validation data mixture specified in \u00a74.2 that includes 5 distinct domains.\nEvaluation The evaluation benchmarks and settings are the same as those in \u00a74.2."}, {"title": "6.3 EXPERIMENTAL SETTING", "content": "Baseline We implement FLP within this data mixing context as a baseline, which first predicts the\naverage pre-training loss on the validation set and uses this to estimate downstream performance\nvia linear regression.\nImplementation of FLP-M We adopt the same implementation as in FLP (details in \u00a74.3). The dis-\ntinction is that we individually measure the pre-training loss on each domain of the validation mixture."}, {"title": "6.4 RESULTS", "content": "The downstream performance prediction results are visualized in Fig. 4. We update the x-axis to\n\u201cpredicted performance\u201d to improve clarity, as the presence of two variables (CG, CC) complicated\n3D visualization. Overall, we find that FLP-M demonstrates better performance compared to FLP\nwhen considering the data mixing as an extra factor in scaling laws analysis. Using average validation\nloss as an indicator for assessing the performance of LMs pre-trained on mixed data sources, such\nas general text and code, is limited. Thus, the average loss fails to trace performance variations in\ndownstream tasks because changes in data mixtures can affect different capabilities of the LMs. In\ncontrast, FLP-M effectively leverages the domain-specific validation loss to capture the capabilities\nimprovement in LMs, and thus can better predict the downstream performance. In our experiments,\nFLP-M accurately predicts the performance of 3B LLMs across various data mixtures and the 7B\nLLM with 0.3 data mixing ratio with error margins within 10% for most benchmarks.\nHowever, on TriviaQA, despite significantly outperforming FLP, FLP-M shows higher relative\nprediction error, ranging from 20% to 30%. This discrepancy can be explained by the substantial\nperformance improvement when scaling LLMs from under 1B to 3B parameters (increasing from\nbelow 12 to over 28). In our sampling LMs configurations (see Tab. 1), we lack sufficient data points\nto adequately characterize the phase of accelerated performance improvement. To better model this\ntrend, a practical solution is to add several sampling LMs between 1B and 3B parameters."}, {"title": "7 FURTHER ANALYSIS", "content": "7.1 \u039f\u03a1\u03a4\u0399MIZING DATA MIXTURE USING FLP-M SCALING LAWS\nWe demonstrate how the derived scaling laws using FLP-M can be effectively applied to optimize\ndata mixtures, enhancing downstream performance. We focus on 1B LMs in this analysis due to\ncompute constraints. For each dataset, we use the FLP-M to estimate the function that maps expended\nFLOPs in each data source to the downstream performance. Then we use this function to predict\nperformance across mixing ratios from 0 to 0.5, in intervals of 0.01.\nAmong all evaluation datasets, the estimated scaling laws function exhibits non-monotonic behavior\non the RACE and ARC datasets, reaching its peak at mixing ratios of 0.01 and 0.22, respectively.\nTo verify, we train 1B LMs with these two mixing ratios and measure their performance on the\ncorresponding benchmarks. The results are shown in Fig. 6. We find that the selected optimal\nmixing ratio can reliably yield better performance compared to the six mixing ratios adopted for the\nsampling LMs, highlighting FLP-M as a practical approach for optimizing data mixtures to enhance\nperformance on specific target tasks."}, {"title": "7.2 ABLATION STUDY", "content": "We conduct further analysis to better understand the two stages in FLP-M. Specifically, we compare\nvarious approaches to estimate the FLOPs-to-Loss and Loss-to-Performance curves in FLP-M.\nFLP-M: FLOPS \u2192 Loss We experiment\nwith several candidate analytical forms listed\nin Tab. 3. We assess their performance in\nestimating the average pre-training loss across\nthe entire validation set, as well as the domain-\nspecific pre-training losses on corresponding\nsubsets. We present the fit curves in Fig. 13\n(\u00a7E), and the relative prediction errors for\npre-training loss estimation are shown in Fig. 7.\nFor average pre-training loss prediction, using\nmore complex analytical models that account\nfor the individual impact of each data source\ncan lead to performance degradation. However,\n$L_D (C_G, C_C) =$\nAnalytical Form\n$M_1 $\n$\\left(\\frac{C_G + C_C}{C_T}\\right)^{a_C}$\n0.029\n$M_2 $\n$\\left(\\frac{C_G}{C_T}\\right)^{a_{C_1}} \\times \\left(\\frac{C_C}{C_T}\\right)^{a_{C_2}}$\n0.026\n$M_3 $\n$\\left(w_0*C_G+w_1*C_C\\right)^{a_C}$\n0.017\n$M_4$ (Ours)\n$\\left(\\frac{C_G + C_C}{C_T}\\right)^{a_C} x \\left(\\frac{C_G}{C_T}\\right)^{C_1}\\times\\left(\\frac{C_C}{C_T}\\right)^{C_2}$\n0.014\nLD (CG, CC) =\nM1Average Error\nrelying solely on the total compute for prediction (M1) can cause high prediction errors in certain\ndomains (e.g., code) and are not stable for various mixing ratios. More complex analytical models\ngenerally perform better in predicting domain-specific loss. Among them, M4, the adopted model\nin FLP-M, provides more stable (within 2.5% relative prediction error across most domains) and\noverall more accurate predictions (achieving the lowest average error shown in Tab. 3).\nFLP-M: Loss \u2192 Performance We experiment with various approaches to estimate the function\nthat maps the pre-training loss to the downstream performance. In this study, we utilize the actual\npre-training loss of target LLMs, rather than the predictive loss used in \u00a75. We consider the following\ncandidates with different inputs:\n(1) FLOPs: We adopt the analytical form used to predict the pre-training loss based on training\ncompute (see Eq. 5), only changing the target metric to the downstream performance.\n(2) Average Loss (Average): We implement a linear regression model to map the average\npre-training loss on the whole validation set to the downstream performance.\n(3) Domain Loss via Linear Combination (Domain-Linear): We apply a linear regression model\nto correlate pre-training loss across domains with downstream performance.\n(4) Domain Loss via Neural Network (Ours) (Domain-Neural): We implement a two-layer\nneural network to map the pre-training loss across domains to the downstream performance.\nThe network configuration and optimization process are introduced in \u00a75.\nThe fit curves are shown in Fig. 14 (\u00a7E) and the results of relative prediction error are shown in Fig. 8.\nConsistent with the findings in \u00a74, directly estimating the performance based on expended compute\n(FLOPs) leads to highly inaccurate predictions (FLOPs vs. Loss). Pre-training loss serves as a more"}, {"title": "8 CONCLUSION", "content": "This paper introduces a two-stage FLP solution to predict downstream performance in LLMs by\nleveraging pre-training loss. Encouraged by promising preliminary results, we propose FLP-M, a core\nsolution for performance prediction that addresses the practical challenges of integrating pre-training\ndata from diverse sources. The effectiveness of FLP-M is validated through extensive experiments."}, {"title": "LIMITATIONS", "content": "Our approach FLP-M is generally applicable across various data sources, yet currently, it is demon-\nstrated only in binary cases involving code and text data due to computational constraints. Our\nspecific emphasis on the mixing ratio of code is deliberate, reflecting its practical significance in\nreal-world applications. This limitation marks a key area for future expansion."}, {"title": "APPENDIX", "content": "A LINEAR RELATION BETWEEN LOSS AND PERFORMANCE\nWe gather data points from intermediate checkpoints of all sampling LMs and visualize the rela-\ntionship between pre-training loss and corresponding task performance in Fig. 9. We observe a\ngenerally linear trend across all benchmarks, which motivates our selection of linear analytical form\nto characterize the mapping from pre-training loss to downstream performance."}, {"title": "B ANALYTICAL FORM TO FIT FLOPS-TO-PERFORMANCE CURVE", "content": "We also experiment with the analytical form proposed in Achiam et al. (2023) to estimate the\nFLOPs-to-Performance curve:\n$log P(C) = \\left(\\frac{C}{C_M}\\right)^{a_M}$, (6)\nwhere CM and M are constant terms to be estimated. The fit curves are shown in Fig. 10. We\nobserve that FLP still consistently outperforms FP across all evaluation benchmarks. In addition, FP\ncan yield very unstable predictions on certain datasets, like HumanEval and TriviaQA, due to a lack\nof sufficient data for accurate modeling."}, {"title": "C COMPARE WITH LLAMA-3 APPROACH", "content": "We compare with the Llama-3 approach for downstream task prediction. They\nsuggest initially estimating the negative log-likelihood (NLL) of the target answer based on the\ncomputational cost in FLOPs, followed by using this NLL to model the task performance through a\nsigmoid function. The comparison results are shown in Fig. 11. We find that the two-stage approach\nproposed in Dubey et al. (2024) fails to effectively estimate the performance based on our data points,\ncompared to FLP."}, {"title": "D MMLU EXPERIMENT", "content": "Our sampling LMs, up to 3B, exhibit random\nperformance (i.e., 25%) on the MMLU bench-\nmark. Consequently, these\nmodels do not provide effective data points for esti-\nmation. Accordingly, we utilize intermediate check-\npoints from 7B LLMs to estimate the performance\nof 13B LLMs on MMLU using FLP. The results are\nshown in Fig. 12, and the relative prediction error is\n3.54%. FLP can also effectively predict the perfor-\nmance on MMLU by leveraging intermediate LMs\ncheckpoints that emerge on this task."}, {"title": "E FLP-M: FIT CURVE FOR ABLATION STUDY", "content": "The FLOPs-to-Loss fit curves are in Fig. 13 and the Loss-to-Performance fit curves are in Fig. 14.\nWe observe that M4 in Tab. 3 offers more stable and accurate predictions for domain-specific loss,"}, {"title": "F USING DOMAIN LOSS IN FLP", "content": "We explore the application of FLP-M during pre-training on a consistent distribution (the experimental\nsetting described in \u00a74), and compare it with FLP. The fitting curves are shown in Fig. 15 and the re-\nsults of relative prediction error are shown in Fig. 16. We show that FLP-M fails to effectively predict\nthe performance of target LLMs when sampling LMs are pre-trained on a fixed distribution. This in-\neffectiveness is attributed to the closely related domain-specific validation losses among the sampling\nLMs within the same training distribution, which suggests that decomposing the pre-training vali-\ndation loss yields no additional information in this pre-training setting. Thus, estimating five domain-"}]}