{"title": "Performance-Aware Self-Configurable Multi-Agent Networks: A Distributed Submodular Approach for Simultaneous Coordination and Network Design", "authors": ["Zirui Xu", "Vasileios Tzoumas"], "abstract": "We introduce the first, to our knowledge, rigorous approach that enables multi-agent networks to self-configure their communication topology to balance the trade-off between scalability and optimality during multi-agent planning. We are motivated by the future of ubiquitous collaborative autonomy where numerous distributed agents will be coordinating via agent-to-agent communication to execute complex tasks such as traffic monitoring, event detection, and environmental exploration. But the explosion of information in such large-scale networks currently curtails their deployment due to impractical decision times induced by the computational and communication requirements of the existing near-optimal coordination algorithms. To overcome this challenge, we present the Alter-NAting Coordination and Network-Design Algorithm (Anaconda), a scalable algorithm that also enjoys near-optimality guarantees. Subject to the agents' bandwidth constraints, Anaconda enables the agents to optimize their local communication neighborhoods such that the action-coordination approximation performance of the network is maximized. Compared to the state of the art, Anaconda is an anytime self-configurable algorithm that quantifies its suboptimality guarantee for any type of network, from fully disconnected to fully centralized, and that, for sparse networks, is one order faster in terms of decision speed. To develop the algorithm, we quantify the suboptimality cost due to decentralization, i.e., due to communication-minimal distributed coordination. We also employ tools inspired by the literature on multi-armed bandits and submodular maximization subject to cardinality constraints. We demonstrate Anaconda in simulated scenarios of area monitoring and compare it with a state-of-the-art algorithm.", "sections": [{"title": "I. INTRODUCTION", "content": "In the future, distributed teams of agents will be coordinating via agent-to-agent communication to execute tasks such as target tracking [1], environmental mapping [2], and area monitoring [3]. Such multi-agent tasks are modeled in the robotics, control, and machine learning literature via maximization problems of the form\n\n$\\max f(\\lbrace a_i \\rbrace_{i\\in N}), \\\\ a_i \\in V_i, \\forall i \\in N$\n\nwhere $N$ is the set of agents, $a_i$ is agent $i$'s action, $V_i$ is agent $i$'s set of available actions, and $f: 2^{\\cup_{i \\in N} V_i} \\leftrightarrow \\mathbb{R}$ is the objective function that captures the task utility [2]-[14]. Particularly, in information gathering tasks, $f$ is often submodular [15]: submodularity is a diminishing returns property, and it emanates due to the possible information overlap among the information gathered by the agents [4]. For example, in target monitoring with multiple cameras at fixed locations, $N$ is the set of cameras, $V_i$ is the available directions the camera can point at, and $f$ is the number of targets covered by the collective field of view of the cameras.\n\nBut solving the problem in eq. (1) in real-time is challenging since it is NP-hard [16]. Although polynomial-time algorithms exist that achieve near-optimal solutions for the problem in eq. (1), in the presence of real-world communication delays [17], these algorithms often require high times to terminate -communication delays are caused by the finite speed of real-world communication channels. The reason is that, for an increasing number of agents, the current algorithms require a combination of high number of communication rounds and large inter-agent message lengths, which collectively increase the total delay.\nFor example, the algorithm in [14], although it achieves the approximation bound $1 - 1/e$ for eq. (1), which is the best possible [18], can require tenths of minutes to terminate even for 10 agents [11]. This is due to algorithm in [14] requiring near-cubic communication rounds in the number of agents, and inter-agent messages that carry information about all agents, instead of only local information. Similarly, the Sequential Greedy algorithm [15], which is the gold standard in robotics, control, and machine learning [2]-[14], although it sacrifices some approximation performance to enable faster decision speed -achieving the bound $1/2$ instead of the bound $1 - 1/e$- still requires (i) inter-agent messages that carry information about all the agents and, in the worst case, (ii) a quadratic number of communication rounds over directed networks [19, Proposition 2], resulting in a communication complexity that is cubic in the number of agents, as we will discuss later in Remark 3.\nIn a similar vein, the Resource-Aware distributed Greedy (RAG) algorithm [11] aims to sacrifice even further approximation performance in favor of more scalability, by requiring the agents to receive information only from and about neighbors such that each message contains information only about the neighbor that sends it. In more detail, the messages are received directly from each neighbor, via multi-channel communication, assuming a pre-defined directed communication network that respects all agents' communication bandwidths. That way, RAG enables (i) parallel decision-making, instead of only sequential that the Sequential Greedy requires, reducing the total number of communication rounds it (RAG) requires, and (ii) inter-agent messages that contain information about one agent only, instead of multiple agents that the Sequential Greedy requires, thus making the communication of such shorter messages faster in the presence of finite communication speeds. Due to RAG's communication-minimal protocol, RAG has an approximation performance that is the same as the Sequential Greedy when all agents are neighbors with all other -fully centralized coordination\u2014 but, when agents coordinate with a few others only, RAG suffers a suboptimality cost as a function of the network topology. Therefore, the following research question arises:\nSubject to the agents' bandwidth constraints, how to enable each agent to optimize its coordination neighborhood such that the suboptimality cost due to decentralization is minimized, that is, the action-coordination performance of the multi-agent network is maximized?\nTo our knowledge, no current work provides distributed algorithms that design the network topology to rigorously optimize the coordination's approximation performance. Heuristic methods are proposed for network optimization yet without being rigorously tied to optimizing the coordination performance [20], [21]. Although [11] quantifies the suboptimality cost due to decentralization, it cannot be leveraged to enable the agents to optimize their neighborhoods since it requires the agents to have oracle access to the actions of non-neighbors, which is impossible in practice in favor of scalability. Works also provide fundamental limits in using the Sequential Greedy for distributed submodular optimization where the agents can select actions ignoring the actions of some of the previous agents [7]\u2013[9]. Particularly, [7], [9] assume a Directed Acyclic Graph (DAG) information-passing communication topology. In this context, these works characterize Sequential Greedy's worst-case performance via graph-theoretic properties of the network. For example, [9] characterizes the worst-case approximation bound by considering the worst-case over all submodular functions, and proves a bound that scales inversely proportional to the independence number of the information graph. Intuitively, the bound scales inversely proportionally to the maximum number of groups of agents that can plan independently. In contrast, in this work, we provide algorithms that allow for the distributed co-design of the network topology based on the submodular function $f$ at hand and the agents' bandwidth constraints. Even if all agents plan independently (fully decentralized network), the guaranteed bound can still be $1/2$, depending on $f$'s curvature (Theorem 1), instead of scaling inversely proportional to the number of agents.\nContributions. We provide the first, to our knowledge, rigorous approach that enables multi-agent networks to self-configure their communication topology to balance the trade-off between scalability and optimality during multi-agent coordination. To this end, we initiate the study of the problem Distributed Simultaneous Coordination and Network Design (Section II), and present a scalable online algorithm with near-optimality guarantees (Sections III to V).\nSubject to the agents' bandwidth and communication constraints, the algorithm enables the agents to optimize their local communication neighborhoods such that the action-coordination approximation performance of the whole network is maximized. The optimization occurs over multiple rounds of local information exchange, where information relay via multi-hop communication is prohibited to curtail the explosion of information exchange, and, thus, to keep low delays due to limited communication speeds. We overview the algorithm in more detail in Fig. 1.\nTo enable the algorithm, we introduce the first, to our knowledge, quantification of the suboptimality cost during distributed coordination as a function of each agent's neighborhood (Section IV). To this end, we capture the action overlap through $f$ between each agent and its neighbors via a mutual-information-like quantity that we term Mutual Information between an Agent and its Neighbors (Definition 3). The algorithm has the following properties:\na) Anytime Self-Configuration: The algorithm enables each agent to select actions and neighbors based on local information only. Therefore, the algorithm enables the multi-agent system to fluidly adapt to new near-optimal actions and communication topology whenever either new agents are included to or existing agents are removed from the network.\nb) Decision Speed: For sparse networks, where the size of each agent's neighborhood is not proportional to the size of the whole network, the algorithm is an order faster than the state-of-the-art algorithms when accounting for the impact that the message length has to communication delays (Section V). The result holds true when the communication cost to the decision speed is at least as high as the computational cost. Particularly, we quantify the algorithm's decision speed in terms of the time needed to perform function evaluations and to communicate with finite communication speeds.\nc) Approximation Performance: The algorithm enjoys a suboptimality bound against an optimal fully centralized algorithm for eq. (1) (Section IV). Particularly, the bound:\n*   Captures the suboptimality cost due to decentralization, i.e., due to communication-minimal distributed coordination. For example, if the network returned by the algorithm is fully connected, then the algorithm's approximation bound becomes 1/2. This is near-optimal since the best approximation bound for eq. (1) is $1 - 1/e \\sim 0.63$ [16].\n*   Holds true given any network topology, including directed and (partially) disconnected networks. In contrast, the current algorithms, such as the Sequential Greedy algorithm [15], cannot offer suboptimality guarantees when the network is disconnected since they require the agents to be able to relay information about all others."}, {"title": "II. DISTRIBUTED SIMULTANEOUS COORDINATION AND NETWORK DESIGN", "content": "We define the problem Distributed Simultaneous Coordination and Network Design. To this end, we use the notation:\n*   $V_{\\mathbb{N}} \\equiv \\Pi_{i \\in \\mathbb{N}} V_i$ is the cross product of sets $\\{V_i\\}_{i \\in \\mathbb{N}}$.\n*   $[T] \\triangleq \\{1, ..., T\\}$ for any positive integer $T$;\n*   $f(a|A) \\triangleq f(A\\cup \\{a\\}) - f(A)$ is the marginal gain of set function $f : 2^V \\to \\mathbb{R}$ for adding $a \\in V$ to $A \\subseteq V$.\n*   $|A|$ is the cardinality of a discrete set $A$.\n*   $\\mathbb{E}$ is the set of (directed) communication edges among the agents. $\\mathbb{E}$ is designed in this paper.\nWe also lay down the following framework about the agents' communication network, and their function $f$.\nCommunication network. The communication network $\\mathbb{G} = (\\mathbb{N}, \\mathbb{E})$ among the agents is unspecified a priori, with the goal in this paper being that the agents must optimize the network themselves to best execute the given task.\nThe resulting communication network can be directed and even disconnected. When the network is fully connected (all agents receive information from all others), we call it fully centralized. In contrast, when the network is fully disconnected (all agents are isolated, receiving information from no other agent), we call it fully decentralized.\nCommunication neighborhood. When a communication channel exists from agent $j$ to agent $i$, i.e., $(j \\to i) \\in \\mathbb{E}$, then $i$ can receive, store, and process information from $j$. The set of all agents that $i$ receives information from is denoted by $N_i$. We refer to $N_i$ as agent $i$'s neighborhood.\nCommunication constraints. Each agent $i$ can receive information from up to $\\alpha_i$ other agents due to onboard bandwidth constraints. Thus, it must be $|N_i| \\leq \\alpha_i$.\nAlso, we denote by $M_i$ the set of agents than have agent $i$ within communication reach -not all agents may have agent $i$ within communication reach because of distance or obstacles. Therefore, agent $i$ can pick its neighbors by choosing at most $\\alpha_i$ agents from $M_i$. Evidently, $N_i \\subseteq M_i$.\nDefinition 1 (Normalized and Non-Decreasing Submodular Set Function [15]). A set function $f : 2^V \\to \\mathbb{R}$ is normalized and non-decreasing submodular if and only if\n*   (Normalization) $f(\\emptyset) = 0$;\n*   (Monotonicity) $f(A) \\leq f(B), \\forall A \\subseteq B \\subseteq V$;\n*   (Submodularity) $f(s|A) \\geq f(s|B), \\forall A \\subseteq B \\subseteq V \\text{ and } s \\in V$.\nIntuitively, if $f(A)$ captures the number of targets tracked by a set $A$ of sensors, then the more sensors are deployed, more or the same targets are covered; this is the non-decreasing property. Also, the marginal gain of tracked targets caused by deploying a sensor $s$ drops when more sensors are already deployed; this is the submodularity property.\nDefinition 2 (2nd-order Submodular Set Function [22], [23]). $f : 2^V \\to \\mathbb{R}$ is 2nd-order submodular if and only if\n\n$f(s|C) - f(s|A \\cup C) \\geq f(s|B \\cup C) - f(s|A \\cup B \\cup C), (2)$\nfor any disjoint $A, B, C \\subseteq V (A \\cap B \\cap C = \\emptyset)$ and $s \\in V$.\nIntuitively, if $f(A)$ captures the number of targets tracked by a set $A$ of sensors, then marginal gain of the marginal gains drops when more sensors are already deployed.\nProblem 1 (Distributed Simultaneous Coordination and Network Design). Each agent $i \\in \\mathbb{N}$ needs to select a neighborhood $N_i$ of size at most $\\alpha_i$, and an action $a_i$ such that the agents jointly solve the optimization problem\n\\begin{aligned}\n& \\max_{\\substack{N_i \\subset M_i \\\\ \\forall i \\in \\mathbb{N}}} \\max_{\\substack{a_i \\in V_i \\\\ \\forall i \\in \\mathbb{N}}} f(\\lbrace a_i \\rbrace_{i\\in \\mathbb{N}}) \\text{ s.t. } |N_i| \\leq \\alpha_i, \\\\\n\\end{aligned}\n\nwhere each agent $i$ selects their action $a_i$ after coordinating actions with its neighbors only, without having access to information about non-neighbors, and where $f: 2^{\\cup_{i \\in N} V_i} \\leftrightarrow \\mathbb{R}$ is a normalized, non-decreasing submodular, and 2nd-order submodular set function.\nProblem 1 implies that the network and action optimizations are coupled: when the network is fully decentralized (all agents coordinate with no other), the achieved value of $f$ can be lower compared to the value that can be achieved when the network is instead fully centralized (all agents coordinate with all others). For example, consider the target monitoring scenario in Section I: in the fully decentralized setting, all cameras may end up covering the same targets, thus $f$ will equal the number of targets covered by one camera only. In contrast, in the fully centralized setting, the cameras can coordinate and end up covering different targets, thus maximizing the total number of covered targets $f$.\nRemark 1 (Decision speed vs. Optimality). As demonstrated by the above example, the more centralized a network is, the higher the value of $f$ that can be achieved. But a more centralized network can also lead to lower decision speeds due to an explosion of information passing among all the agents since all agents will coordinate with all others. The goal of this paper is to develop a communication-efficient distributed algorithm that not only requires just a few coordination rounds for convergence; it also needs only short messages to be communicated among agents, thus, accounting for real-world communication delays due to limited communication speeds [24]. For this reason, in particular, Problem 1 requires each agent to (i) coordinate actions only with its neighbors, and (ii) receive information only about them, instead of also about non-neighbors. This is in contrast to standard distributed methods that allow information about the whole network to travel to all agents via multi-hop communication, hence often not reducing the amount of information flowing in the network compared to fully centralized coordination, and hence often introducing impractical communication delays [11], [17]."}, {"title": "III. ALTERNATING COORDINATION AND NETWORK-DESIGN ALGORITHM (Anaconda)", "content": "We present the AlterNAting COordination and Network-Design Algorithm (Anaconda) for Problem 1. Anaconda aims to approximate a solution to Problem 1 by alternating the optimization for action coordination and neighbor selection. A description of the algorithm is given in Fig. 1.\nBoth action coordination and neighborhood selection take the form of Multi-Armed Bandit (MAB) problems, therefore, in the following, we first present the MAB problem (Section III-A). Then, we will present the algorithms ActionCoordination (Section III-B) and NeighborSelection (Section III-C).\nA. Multi-Armed Bandit Problem\nThe adversarial Multi-Armed Bandit (MAB) problem [25] involves an agent selecting a sequence of actions to maximize the total reward over a given number of time steps [25]. The challenge is that, at each time step, the reward associated with each action is unknown to the agent a priori, becoming known only after the action has been selected. To rigorously present the MAB problem, we use the notation:\n*   $V$ denotes the available action set;\n*   $v_t \\in V$ denotes the agent's selected action at time $t$;\n*   $r_{v_t, t} \\in [0,1]$ denotes the reward that the agent receives by selecting action $v_t$ at $t$.\nProblem 2 (Multi-Armed Bandit [25]). Assume an operation horizon of $T$ time steps. At each time step $t \\in [T]$, the agent must select an action $v_t$ such that the regret\n\n$MAB-Reg_T \\triangleq \\max_{\\nu\\in V} \\sum_{t=1}^{T} r_{\\nu,t} - \\sum_{t=1}^{T} r_{\\nu_t, t} (4)$\n\nis sublinear in $T$, where for full-information feedback, the rewards $r_{\\nu,t} \\in [0,1]$ for all $\\nu \\in V$ become known to the agent after $\\nu$ has been executed at each $t$; whereas for bandit feedback, only the reward $r_{\\nu_t, t} \\in [0,1]$ becomes known to the agent after $\\nu$ has been executed.\nProblem 2 asks for MAB-Reg$_T$ to be sublinear, i.e., MAB-Reg$_T$/T $\\to$ 0 for T $\\to$ +$\\infty$, since this implies that the agent asymptotically chooses optimal actions even though the rewards are unknown a priori [25].\nProblem 2 presents two versions of MAB, one with full-information feedback, and one with bandit feedback. The difference between them is that, at each $t \\in [T]$, in full-information feedback the rewards of all $\\nu \\in V$ are revealed, even though only one action is selected; while in bandit feedback, only the reward of the selected action is revealed.\nB. Action Coordination\nWe present ActionCoordination and its performance guarantee. To this end, we introduce the coordination problem that ActionCoordination instructs the agents to simultaneously solve and show that it takes the form of Problem 2 with full-information feedback. We use the definitions:\n*   $A_t \\triangleq \\{a_{i, t}\\}_{i \\in \\mathbb{N}}$ is the set of all agents' actions at $t$;\n*   $A^{OPT} \\in arg \\max_{a_i \\in V_i, \\forall i \\in \\mathbb{N}} f(\\lbrace a_i \\rbrace_{i\\in \\mathbb{N}})$ is the optimal actions for agents $\\mathbb{N}$ that solve eq. (1);\n*   $N^{*}$ is the optimal neighborhood corresponding to $\\{a_{i,t}\\}_{t \\in [T]}$ that solves eq. (8);\n*   $\\kappa_f \\triangleq 1 - \\min_{v \\in V} [f(V) - f(V \\backslash \\{v\\})]/f(v)$ is the curvature of $f$ [26]. $\\kappa_f$ measures how far $f$ is from modularity: if $\\kappa_f = 0$, then $f(V) - f(V \\backslash \\{v\\}) = f(v), \\forall v \\in V$, i.e., $f$ is modular. In contrast, $\\kappa_f = 1$ in the extreme case where there exist $v \\in V$ such that $f(V) = f(V \\backslash \\{v\\})$, i.e., $v$ has no contribution in the presence of $V \\backslash \\{v\\}$.\nThe intuition is that the agents should select actions simultaneously such that each agent $i$ selects an action $a_{i,t}$ that maximizes the marginal gain $f(a|\\{a_{j,t}\\}_{j\\in N_{i,t}})$. But since the agents select actions simultaneously, $\\{a_{j, t}\\}_{j\\in N_{i, t}}$ become known only after agent $i$ selects $a_{i,t}$ and communicates with $N_{i,t}$, i.e., computing $f(a|\\{a_{j,t}\\}_{t\\in N_{i, t}})$ becomes feasible for all $a \\in V_i$ only in hindsight. To this end, ActionCoordination instructs each agent $i$ to select actions $\\{a_{i,t}\\}_{t \\in [T]}$ such that the action regret\n\n$\\max_{a \\in V_i} \\sum_{t=1}^{T} f(a|\\{a_{j, t}\\}_{j\\in N_{i, t}}) - \\sum_{t=1}^{T} f(a_{i, t} | \\{a_{j, t}\\}_{j\\in N_{i, t}}), (5)$\n\nis sublinear in $T$. Thus, the action coordination problem takes the form of Problem 2 with full-information feedback, where the reward of each action $a \\in V_i$ is the marginal gain, i.e., $r_{a,t} = f(a|\\{a_{j,t}\\}_{j\\in N_{i, t}})$.\nActionCoordination implements a Multiplicative Weights Update (MWU) procedure to converge to an optimal solution to eq. (5) -the MWU procedure has been introduced to solve Problem 2 with full-information feedback [27]. In more detail, ActionCoordination starts by initializing a learning rate $\\eta_1$ and a weight vector $w_t$ for all available actions $a \\in V_i$ (Algorithm 2's lines 1-2). Then, at each time step $t \\in [T]$, ActionCoordination executes in sequence the steps:\n*   Compute probability distribution $p_t$ using $w_t$ (lines 3-4);\n*   Select action $a_{i, t} \\in V_i$ by sampling from $p_t$ (line 5);\n*   Send $a_{i,t}$ to NeighborSelection and receive neighbors' actions $\\{a_{j,t}\\}_{j\\in N_{i, t}}$ (line 6);\n*   Compute marginal gain of selecting $a \\in V_i$ as reward $r_{a, t}$ associated with each $a \\in V_i$, and update weight $w_{a,t+1}$ for each $a \\in V_i$ (lines 7-9).$\\mathnormal{^1}$\nProposition 1 (Approximation Performance). The agents select actions via ActionCoordination such that:\n\n$\\sum_{t=1}^{T} f(A_t) \\geq \\frac{1-\\kappa_f}{1+\\kappa_f} T f(A^{OPT}) - \\tilde{O} \\bigg( \\sum_{i\\in \\mathbb{N}} I_{f,t}(a_{i, t}; N_{i,t}) \\bigg) - \\tilde{O}(\\sqrt{T}) (6)$\n\nwhere $\\tilde{O}(\\cdot)$ hides log terms.\nProposition 1 implies that the approximation performance of ActionCoordination increases for network designs $\\{N_{i,t}\\}_{i\\in \\mathbb{N}}$ with higher value $I_{f,t}(a_{i,t}; N_{i,t})$. Particularly, the $-\\sum_{i\\in \\mathbb{N}} I_{f,t}(a_{i,t}; N_{i, t})$ plays the role of $C(\\{N_i\\}_{i\\in \\mathbb{N}})$ in Introduction. Intuitively, $I_{f,t}(a_{i,t}; N_{i, t})$ captures the utility overlap between agent $i$'s action and the actions of its neighbors: for example, when the network is fully disconnected ($N_{i, t} = \\emptyset, \\forall i \\in \\mathbb{N}$), then $I_{f,t}(a_{i,t}; N_{i,t}) = 0$.\nDefinition 3 (Mutual Information between an Agent and Its Neighbors). At any $t \\in [T]$, given an agent $i \\in \\mathbb{N}$ with an action $a_{i,t}$ and neighbors $N_{i,t}$, the mutual information between the agent and its neighbors is denoted by$\\mathnormal{^2}$\n\n$I_{f,t}(a_{i,t}; N_{i,t}) = f(a_{i,t}) - f(a_{i,t}|\\{a_{j,t}\\}_{j\\in N_{i,t}}). (7)$\n\nNeighbor Selection will next leverage Lemma 1 to enable the agents to distributively select a network topology that optimizes the approximation bound of ActionCoordination.\nLemma 1 (Monotonicity and Submodularity of $I_{f,t}$). Given an $a \\in V_i$ and a non-decreasing and 2nd-order submodular function $f:2^V \\to \\mathbb{R}$, then $I_{f,t}(a; \\cdot)$ is non-decreasing and submodular in the second argument.\n\\hrulefill\n$\\mathnormal{^1}$The coordination algorithms in [12]\u2013[14] instruct the agents to select actions simultaneously at each time step as ActionCoordination, but they lift the coordination problem to the continuous domain and require each agent to know/estimate the gradient of the multilinear extension of $f$, which leads to a decision time at least one order higher than ActionCoordination [11].\n$\\mathnormal{^2}$The quantity in eq. (7) extends the definition of submodular mutual information [28] to the multi-agent setting introduced herein."}, {"title": "C. Neighbor Selection", "content": "We present Neighbor Selection. To this end, we introduce the neighbor-selection problem that NeighborSelection instructs the agents to simultaneously solve and show that it takes the form of Problem 2 with bandit feedback.\nSince ActionCoordination's suboptimality bound improves when $I_{f,t}(a_{i,t}; N_{i,t})$ increases, NeighborSelection instructs each agent $i$ to select its neighbors by solving the cardinality-constrained maximization problem:\n\n$\\max_{\\substack{N_{i,t} \\subset M_i, |N_{i, t}| \\leq \\alpha_i}} \\sum_{t=1}^T \\sum_{i \\in \\mathbb{N}} I_{f,t}(a_{i,t}; N_{i,t}), (8)$\n\nwhere $a_{i,t}$ is given by ActionCoordination (Fig. 1). The problem in eq. (8) is a submodular optimization problem since we prove that $I_{f,t}(a_{i,t}; N_i)$ is submodular in $N_i$.\nBut $I_{f,t}(a_{i,t}; N_{i,t})$ is computable in hindsight only: the $\\{a_{j,t}\\}_{j\\in N_{i,t}}$ become known only after agent $i$ has selected and communicated with $N_{i,t}$. Therefore, eq. (8) takes the form of cardinality-constrained bandit submodular maximization [1], [29], [30], which is an extension of Problem 2 to the submodular multi-agent setting.\nSolving eq. (8) using algorithms for Problem 2 with bandit feedback will lead to exponential-running-time algorithms due to an exponentially large $V$ per eq. (4) [30]. Therefore, NeighborSelection instead extends [30, Algorithm 2], which can solve eq. (8) in the full-information setting, to the bandit setting [1]. Specifically, NeighborSelection decomposes eq. (8) to $\\alpha_i$ instances of Problem 2 with bandit feedback, and separately solves each of them using the EXP3-IX algorithm [31], which can handle bandit feedback."}, {"title": "IV. APPROXIMATION GUARANTEE", "content": "We present the suboptimality bound of Anaconda. Thus, the bound compares Anaconda with an optimal fully centralized algorithm that maximizes $f$ per eq. (1).\nTheorem 1 (Approximation Performance). Anaconda instructs over $t \\in [T]$ each agent $i \\in \\mathbb{N}$ to select actions $\\{a_{i, t}\\}_{t \\in [T]}$ and neighborhoods $\\{N_{i, t}\\}_{t \\in [T]}$ that guarantee:\n*   If the network is fully centralized, i.e., $N_{i, t} = \\mathbb{N} \\backslash \\{i\\}$,\n\n$\\mathbb{E} [f(A_t)] \\geq \\frac{1}{1 + \\kappa_f} f(A^{OPT}) - \\tilde{O} (\\vert \\mathbb{N} \\vert \\sqrt{T} ) / \\Phi(T). (9)$\n\n*   If the network is fully decentralized, i.e., $N_{i, t} = \\emptyset$,\n\n$\\mathbb{E} [f(A_t)] \\geq \\frac{1-\\kappa_f}{1 + \\kappa_f - \\kappa_f^2} f(A^{OPT}) - \\tilde{O} (\\vert \\mathbb{N} \\vert \\sqrt{T} ) / \\Chi(T). (10)$\n\n*   If the network is anything in between fully centralized and fully decentralized, i.e., $N_{i, t} \\subseteq M_i \\subseteq \\mathbb{N} \\backslash \\{i\\}$,\n\n\\begin{aligned}\n& \\mathbb{E} [f(A_t)] \\geq \\frac{1-\\kappa_f}{1 + \\kappa_f - \\kappa_f^2} f(A^{OPT}) \\\\\n& - (1-\\kappa_f) \\mathbb{E} \\bigg[ \\frac{\\sum_{i \\in \\mathbb{N}} \\sum_{t} I_{f,t}(a_{i, t}; N_{i,t})}{\\vert\\mathbb{N}\\vert} \\bigg] \\\\\n& - \\tilde{O} \\bigg(\\frac{\\alpha \\vert \\mathbb{N} \\vert \\sqrt{\\vert M \\vert}}{T} \\bigg) - \\log(2/\\delta)\\tilde{O} \\bigg(\\frac{\\alpha \\vert \\mathbb{N} \\vert \\sqrt{\\vert M \\vert}}{T} \\bigg) / \\Psi(T) (11)\n\\end{aligned}\n\nwhere $\\alpha \\triangleq \\max_{i \\in \\mathbb{N}} \\alpha_i$, and $\\vert M\\vert \\triangleq \\max_{i \\in \\mathbb{N}} \\vert M_i\\vert$.\nParticularly, each case above holds with probability at least $1 - \\delta$, for any $\\delta \\in (0,1)$, and the expectation is due to Anaconda's internal randomness. $\\tilde{O}(\\cdot)$ hides log terms.\nTheorem 1 quantifies both the suboptimality of Anaconda due to decentralization, and the convergence speed of Anaconda. Both are affected as follows:\n*   Effect of online co-design of network topology and agent actions: $\\Psi$ in eq. (11) captures the convergence speed of the network and action selection and its impact to the suboptimality bound \u2014similarly in eqs. (9) and (10) for $\\Phi$ and $\\Chi$. Particularly, $\\Psi$ vanishes as $T \\to \\infty$, having no impact on the suboptimality bound anymore, and its vanishing speed captures how fast the agents converge to near-optimal actions and neighborhoods.\n*   Effect of resource-minimal distributed communication and action coordination: After $\\Psi$ vanishes as $T \\to \\infty$, the bound in eq. (11) depends on $I^*$ that captures the suboptimality due to decentralization such that the higher $I^*$ is the better is Anaconda's approximation performance. Particularly, $I^*$ depends on the neighborhoods of each agent $i$, and the larger agent $i$'s neighborhood can be, the higher $I^*$ can be since $I_{f,t}(a_{i,t}; N_i)$ is non-decreasing in $N_i$. Then, the better Anaconda's suboptimality bound can be per eq. (11). In contrast, if $\\alpha_i = 0$ for all agents $i \\in \\mathbb{N}$, then $I_{f,t}(a_{i,t}; \\emptyset) = 0$, and as $T \\to \\infty$, eq. (10) and eq. (11) become the same.\nOverall, eqs. (9) to (11) imply that Anaconda's global suboptimality bound will improve if the agents can communicate and coordinate over a more centralized network, with the bound covering the range $[(1 - \\kappa_f)/(1 + \\kappa_f - \\kappa_f^2),1/(1 + \\kappa_f)]$ as the network covers the spectrum from being fully decentralized (eq. (10)) to fully centralized (eq. (9)). Importantly, the $1/(1 + \\kappa_f)$ suboptimality bound in the fully centralized case recovers the bound in [26] and is near-optimal since the best possible bound for in (3) is $1 - \\kappa_f/e$ [32].$\\mathnormal{^3}$\nIn all, asymptotically (as $T \\to \\infty$), Anaconda enables the agents to individually select near-optimal actions and communication neighborhoods.\nRemark 2 (On the Tightness of Approximation Bounds). The approximation bound in Theorem 1 are not necessarily tight. Particularly, the bound in eq. (11) does not converge to $1/(1 + \\kappa_f)$ when the network becomes fully centralized. As part of our future work, we will investigate tight bounds."}]}