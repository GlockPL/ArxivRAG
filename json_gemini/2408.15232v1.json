{"title": "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations", "authors": ["Yucheng Jiang", "Yijia Shao", "Dekun Ma", "Sina J. Semnani", "Monica S. Lam"], "abstract": "While language model (LM)-powered chatbots and generative search engines excel at answering concrete queries, discovering information in the terrain of unknown unknowns remains challenging for users. To emulate the common educational scenario where children/students learn by listening to and participating in conversations with their parents/teachers, we create Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all the questions, Co-STORM lets users observe and occasionally steer the discourse among several LM agents. The agents ask questions on the user's behalf, allowing the user to discover unknown unknowns serendipitously. To facilitate user interaction, Co-STORM assists users in tracking the discourse by organizing the uncovered information into a dynamic mind map, ultimately generating a comprehensive report as takeaways. For automatic evaluation, we construct the WildSeek dataset by collecting real information-seeking records with user goals. Co-STORM outperforms baseline methods on both discourse trace and report quality. In a further human evaluation, 70% of participants prefer Co-STORM over a search engine, and 78% favor it over a RAG chatbot.", "sections": [{"title": "Introduction", "content": "Recent advancements in language models (LMs) (Bai et al., 2022; OpenAI, 2023; Gemini Team, 2024) and retrieval-augmented generation (RAG) (Lewis et al., 2021) have led to more capable chatbots and emerging generative search engines (Liu et al., 2023a). Compared to traditional search engines and information retrieval (IR) models (Robertson, 1977), these systems fulfill user queries by generating direct responses, effectively addressing known unknowns, where users are aware of their information needs.\nHowever, a gap remains in using these systems for complex information-seeking scenarios, such as academic research, market analysis, and decision-making, where the system should expose users to their unknown unknowns to facilitate knowledge discovery. While the concept of unknown unknowns originally referred to unexpected risks in the military, it is linked to the serendipitous discovery of information in the information research context (Foster and Ford, 2003; Agarwal, 2015). Specifically, Kirzner (1997) directly contrasts such discovery (\"the realization that one had overlooked something in fact readily available\") with successful search (\"the deliberate production of informa-"}, {"title": "Complex Information Seeking", "content": "2.1 Problem Formulation\nPirolli (2009) defines complex information-seeking as part of the broader sensemaking process, involving collecting, sifting, understanding, and organizing information from large collections to generate a knowledge product. Prevalent in domains such as investigative journalism, scientific research, and market analysis, this task has the following properties: (1) it requires seeking information from multiple sources to address various facets of a topic rather than retrieving a document that best matches a query; (2) it involves ongoing user interaction rather than processing a single query; (3) it produces report-like curated information product rather than a single short-form answer. As shown in Table 1, none of the existing information-seeking assistance systems (e.g., Robertson, 1977; Chen et al., 2017; Reddy et al., 2019; Shao et al., 2024) can fully support this task.\nGiven a user with an initial topic of interest t and an initial goal g, and a repository of information R, the task is to interact with the user and write a custom long-form report tailored to the users' interest;"}, {"title": "WildSeek: An In-the-Wild Information Seeking Dataset", "content": "To study users' interests in complex information-seeking tasks in the wild, we utilized data collected from the open-source STORM web application\u00b2, which generates comprehensive long-form reports based on users' specified topics of interest and goals for using the site. Each data point is a pair comprising a topic and the user's goal. To improve the quality of the dataset, we retain only those data points that are well motivated by applying rule-based filtering followed by binary classification using an LM (gpt-40-2024-05-13). Next, we use the same LM to predict the taxonomy class of each topic, followed by manual review and refinement. Finally, we downsample the data to create a dataset with 100 data points across 24 different domains."}, {"title": "Method", "content": "\"Tell me and I forget. Teach me and I remember. Involve me and I learn.\" \u2014 Benjamin Franklin"}, {"title": "Collaborative Discourse Protocol", "content": "Nussbaum (2008) emphasizes the importance of collaborative discourse in fostering deep understanding and critical thinking in human learning. Since it is difficult to assemble a group of human experts for collaborative discourse on any topic at any time, we propose Co-STORM (Figure 2) to emulate this process with multiple LM agents to assist human information seeking and learning. Formally, the collaborative discourse, D = {U1, U2, ..., Un}, consists of turn-based textual utterances ui from one of three roles: the user (\u00a73.3), experts with diverse perspectives (\u00a73.4), and a moderator guiding the discourse and injecting questions (\u00a73.5). The discourse begins with N experts, P = {P1, ..., pn }, discussing the topic t for one turn per expert to warm up the conversation. Co-STORM dynamically maintains a mind map (\u00a73.2) to track the discourse and construct shared knowledge between the user and the system.\nInspired by the utterance intent taxonomy for information-seeking conversations proposed by Qu et al. (2019), we associate each agent utterance ui with an agent intent type ai, where ai can be one of the following: ORIGINAL QUESTION (which initiates a new question), INFORMATION REQUEST (which seeks additional information from the prior utterance), POTENTIAL ANSWER (which offers a possible answer to a previously posed question), FURTHER DETAILS (which provides supplementary information to a previous answer). We group ORIGINAL QUESTION and INFORMATION REQUEST as question-asking and the other two intents as question-answering.\nTraum (2003) underscores the necessity of discourse management in multiparty dialogues. While existing systems support either just user initiatives (e.g., QA systems) or just agent initiatives (e.g., STORM), Co-STORM adopts a mixed-initiative approach. When the user actively engages, the system continues the discourse based on the user's question or argument, allowing for a more targeted discussion. Otherwise, the system automatically generates the next turn. The user controls who takes the initiative, as Co-STORM allows the user to take a turn anytime.\nIf the user does not take the turn at timestamp i, Co-STORM needs to determine which LM agent should generate the next utterance ui. Its protocol is to let different experts, P1, ..., PN, take turns in sequence. To prevent them"}, {"title": "Tracking the Discourse with a Mind Map", "content": "Having shared knowledge or a shared conceptual space is critical for collaboration (Roschelle and Teasley, 1995). To help users track the discourse and reduce their cognition load, Co-STORM uses a tree-structured mind map M to dynamically organize collected information in the discourse D. Specifically, M = (C, E) is a hierarchical organization of concepts C, where its directed edges E characterize latent parent-child relationships among topics (e.g., in Figure 2, \"Drug Discovery Acceleration\" is a subtopic of \u201cImpact and Applications\u201d). Each concept c\u2208 C is associated with a subset of retrieved information Ic \u2286 I. To ensure M is an intent-driven organization of information, each piece of information is also associated with the"}, {"title": "User Participation", "content": "When the user injects an utterance u, Co-STORM uses u as the query to retrieve information to"}, {"title": "Simulating the Roundtable Participant", "content": "Following STORM (Shao et al., 2024) which uses perspective-guided question asking to improve the question diversity and quality, Co-STORM personalizes simulated experts with different expertise to represent different perspectives. Co-STORM retrieves the background of topic t with a search query and gives it to an LM to generate the expert list P = {p1, ..., pn}. For example, for the topic \u201cAlphaFold3\" in Figure 2, the LM suggests an \u201cAI Expert"}, {"title": "Simulating the Moderator", "content": "If all the simulated participants are experts, we discover that the discourse tends to consist mostly of utterances with intent FURTHER DETAILS, leading to repetition and niche discussions. The moderator plays an important role of injecting new directions into the discourse. To generate the moderator's utterance, Co-STORM instructs the LM to generate\n...reranking score is\ncos(i, t)\u03b1(1 \u2013 cos(i, q))1-\u03b1,where i, t, q are corresponding text embeddings and \u03b1 is a hyperparameter. This reranking function prioritizes information that does not directly answer the original question but is relevant to the topic t. Co-STORM concatenates these reranked sources along with concept names in C to avoid repetitive concepts. This combined context is used to prompt the LM to generate the question for the moderator turn and an updated list of experts, P'."}, {"title": "Co-STORM Implementation", "content": "The LM component of Co-STORM is implemented using zero-shot prompting via the DSPy framework (Khattab et al., 2023) and gpt-40-2024-05-13 (see full prompts in Appendix D). We ground Co-STORM on the Internet using the You.com search API\u2074 although the system is compatible with other search engines or IR systems. Hyperparameters N, K, L, \u03b1 are set to 3, 10, 2, and 0.5, respectively. The text embeddings in Eq. 1 are obtained from text-embedding-3-small. We set the LM temperature as 1.0 and top_p as 0.9 for all experiments. For human evaluation, we develop a web application (Figure 12) for users to interact with Co-STORM in real time."}, {"title": "Automatic Evaluation", "content": "Automatic evaluation enables scalable testing and allows for consistent simulation of user behavior. We compare Co-STORM with the following baselines: (1) RAG Chatbot, a baseline that retrieves information from the search engine and interacts with the user through a one-question-one-answer paradigm. (2) STORM + QA, a baseline that uses the STORM framework (Shao et al., 2024) to generate a report for a given topic to provide general information. It then allows the user to ask follow-up questions and provides corresponding answers retrieved with the search engine."}, {"title": "Evaluation Setup", "content": "We use the WildSeek dataset where each data point consists of an initial topic t and goal g. We simulate the user with an LM (gpt-40-2024-05-13) prompted with t, g, the discourse history D, and the instruction for question generation. To ensure a fair comparison, we terminate the information-seeking session once it reaches 30 search queries for Co-STORM and both baselines. For all methods, the final report is generated using the two-stage approach of outline generation followed by section-by-section article generation, as proposed by STORM (Shao et al., 2024), based on the interaction history. We evaluate the system quality by assessing the final report and the interaction history (i.e., discourse) with the automatic metrics defined in \u00a75.2."}, {"title": "Automatic Metrics", "content": "We evaluate the final report on four aspects, Relevance, Broad Coverage (Breadth), Depth, and Novelty, as indicators of the quality of the whole information-seeking process.\u2075 We employ Prometheus 2 (Kim et al., 2024), a 7B evaluator LM, to score the report based on a 5-point rubric. To further quantify the diversity of the collected information, we also report the Information Diversity as the average pairwise dissimilarity of I,\n(1 / (|I|(|I| \u2013 1))) \u03a3i,j\u2208I, i\u2260j cos(i, j)\nwhere i, j are corresponding text embeddings obtained from OpenAI's text-embedding-3-small.\nSince the discourse itself is valuable for human learning, we also evaluate the discourse trace using a 5-point rubric to grade each turn. This grading assesses Novelty, Intent Align-"}, {"title": "Automatic Evaluation Results", "content": "Table 3 presents the evaluation results for report quality and the quality of question-answering turns in the discourse. The question-answering turns and the final report are the primary sources for human learning when they interact with Co-STORM. STORM + QA considers multiple perspectives in researching the given topic, indeed leading to improved performance across all four grading dimensions of the report quality compared to the RAG Chatbot. However, Co-STORM outperforms it, particularly in the Depth and Novelty aspects, by simulating collaborative discourse with multiple agentic roles, akin to a thought-provoking round table discussion. For discourse quality, the question-answering turns in Co-STORM significantly outperform both baselines in terms of Consistency and Engagement. This improvement is attributed to collaborative discourse setup, where the LM is prompted to generate the answer only when the retrieved information matches the current question according to the discourse history (see Listing 2). The utterance polishing step (see Figure 2) also helps as it serves as a self-improving mechanism."}, {"title": "Ablation Studies", "content": "As discussed in \u00a73, a major innovation of Co-STORM is the orchestration of two types of LM agents. To assess the benefit, we compare Co-"}, {"title": "Human Evaluation", "content": "Human evaluation is essential for assessing systems designed for collaborative discourse, as it captures the complexities of human interaction, reflects familiar real-world interactions, and provides critical insights into the system's effectiveness. We compare Co-STORM with two baselines: (1) RAG Chatbot as detailed in section \u00a75. (2) Traditional Search Engine."}, {"title": "Evaluation Setup", "content": "We conduct an IRB-approved human evaluation to compare Co-STORM with RAG Chatbot and Search Engine by recruiting 20 volunteers on the Internet. Participants are randomly split into two groups: one compared Co-STORM with Google Search, while the other group compared it with the RAG Chatbot. Participants are asked to seek information on two topics, one on each system, from the same domain and sharing the same goal. Note that we mitigate topic familiarity bias by using two different topics within the same domain. We have prepared five different domains, with each assigned to 2 users in each group. To counterbalance, one user starts with Co-STORM and switches to the baseline for the other topic, and vice versa.\nAfter seeking information for each topic, participants are instructed to rate their experience based on four grading aspects defined in \u00a75.2 (Relevance, Breadth, Depth, Novelty/Serendipity), using a 5-point Likert scale. After completing both tasks, participants are asked to provide pairwise preferences regarding the required effort, user engagement, addressing echo chamber issues, and overall experience. We also collect open-ended feedback and allow participants to optionally leave comments on each discourse turn and the mind map snapshots when interacting with Co-STORM."}, {"title": "Human Evaluation Results", "content": "shows the human rating results and Figure 4 shows the pairwise comparison results. Co-\nSTORM helps users find broader and deeper information relevant to their goals. Participants found that Co-STORM uncovers information with greater breadth and depth compared to the search engine and the RAG Chatbot. Specifically, Co- STORM is rated strictly higher in Breadth by 50% of the participants and strictly higher in Depth by 60% of the participants than the search engine. Compared to the RAG Chatbot, Co-STORM receives strictly higher scores in Breadth from 67% of participants and in Depth from 56% of participants. This finding aligns with the automatic evaluation results shown in Table 3. While helping users discover more information, Co-STORM remains aligned with their goals, as participants also rated Co-STORM higher in Relevance compared.\nCo-STORM provides more serendipitous information with less mental effort required. Participants found that Co-STORM requires less effort, better mitigates the echo chamber issue, and provides a better overall experience. In a more fine-grained evaluation, participants evaluated 32% of Co-STORM 's total utterances, rating 89% of them as effectively \"steering the discourse towards a new and interesting direction\". One participant noted, \"Co-STORM allows for almost full automation and much better understanding as it brings up topics that the user may not even think of\u201d. Moreover, participants found the mind map helpful. In total, they evaluated 80 snapshots of the dynamic mind map, finding it accurately tracked the discourse 71% of the time. One participant remarked, \u201cCo-STORM is so much less mentally taxing for me to use\u201d.\nCo-STORM should support more customization. Among the 19 participants, 4 noted that the RAG Chatbot better follows instructions that have a clear target and mentioned they expect Co-STORM to generate more concise utterances and provide less information in such cases. We view dynamically adapting Co-STORM to users' evolving mental states and personalizing their preferences as a meaningful direction for future work."}, {"title": "Related Works", "content": "Information-Seeking Support in NLP NLP research supporting human information-seeking has mainly focused on building question-answering (QA) systems (Chen et al., 2017; Lee et al., 2019; Dasigi et al., 2021; Levy et al., 2021; Yuan et al., 2020). These works often assume that the answer can be found within a single document (Clark et al., 2020) or that users can formulate complex queries (Yang et al., 2018; Chen et al., 2021; Ahmadvand et al., 2023), assumptions that do not hold true in complex information seeking (Butler, 2000; Booth et al., 2009; Bystr\u00f6m and J\u00e4rvelin, 1995).\nSome more recent works have proposed long-form QA systems (Xu et al., 2023, 2024) and automatic expository writing systems (Balepur et al., 2023; Shen et al., 2023; Shao et al., 2024) to synthesize information from multiple sources. Some other studies have explored conversational search (Kumar and Callan, 2020; Nakamura et al., 2022). However, these works typically ignore human interaction or only passively answer user questions. We construct a multi-agent system with a human-in-the-loop protocol to support effective user interaction for complex and evolving information needs.\nAs LMs advance, a growing body of research explores their use in multi-agent applications (Wu et al., 2023; Nakajima, 2023; Liu et al., 2023b; Wang et al., 2024). Several studies show that multi-agent debate enhances factuality and reasoning compared to using a single LM (Du et al., 2023; Liang et al., 2023), and cooperative role-playing frameworks improve performance on coding or mathematical benchmarks (Li et al., 2023; Hong et al., 2023). While these studies primarily focus on automating tasks, the potential applications extend further. For instance, Generative Agents (Park et al., 2023) instantiate an interactive environment with twenty-five LM agents to study emergent social behaviors, and Michael et al. (2023) show that multi-agent debates help humans supervise model outputs. Our work aligns"}, {"title": "Conclusion", "content": "We propose Co-STORM, an information-seeking assistance system that emulates collaborative discourse among users and multiple LM agents. By creating an interactive environment where users can both observe and participate, Co-STORM enhances learning and the complex information-seeking process. To facilitate automatic evaluation, we construct the WildSeek dataset, which captures the information-seeking needs and intents of real Internet users. Experimental results, including extensive human assessments, show that Co-STORM outperforms traditional search engines and RAG chatbots in surfacing unknown unknowns for human learning and reducing users' mental effort."}, {"title": "Limitations", "content": "We design Co-STORM to create an immersive human learning experience by enabling humans to participate in LM agent conversations. Despite the advantages demonstrated through both automatic and human evaluations, several limitations remain. First, the system could better tailor the collaborative discourse to the user's prior knowledge, skipping basic facts for knowledgeable users and introducing concepts progressively for novices. Second, while Co-STORM employs an effective discourse management mechanism, users sometimes desire more control over the discourse, including managing expert perspectives and customizing the utterance length. Third, extending Co-STORM to support multiple languages would significantly enhance its usefulness and impact. Although current LMs often possess multilingual capabilities, implementing a multilingual Co-STORM requires integrating search engines or retrieval models capable of accessing diverse language sources. Furthermore, managing content across different languages demands robust content moderation and the ability to identify conflicting information to ensure a reliable human learning experience. Finally, compared to the RAG Chatbot, Co-STORM has higher latency due to the need to decide the utterance intent and update the mind map. Although the current latency is acceptable for real-time interaction, as demonstrated in human evaluations, further improving the efficiency of the LM system would provide a smoother user experience."}, {"title": "Dataset Details", "content": "We constructed WildSeek using a web application we built that hosts the open-sourced STORM project (Shao et al., 2024), as detailed in \u00a72.2. User privacy was strictly maintained by explicitly obtaining consent each time users logged into our web application. No personally identifiable information was collected, and the entire dataset was manually reviewed to ensure compliance with this standard. We rejected topics that were illegal, harmful, violent, racist, sexual, non-English, based on personal experience, or contained personal information.\nAt the time of dataset construction, 8,777 users accessed our web application, resulting in the collection of 6,608 unique topic and purpose pairs. Participants were sourced from the general internet and all held valid Google accounts, in accordance with our IRB-approved policy. To ensure broad coverage, we conducted topic classification using gpt-40-2024-05-13 and human inspection, and then downsampled the collected data to 100 cases, covering 24 fine-grained categories in 6 domains: Science, Health and Fitness, Culture and Society, Lifestyle and Leisure, Social Science and Humanities, and Others. We applied rule-based filtering to exclude non-informative or trivial information-seeking purposes, and the 100 selected topic-purpose pairs were manually labeled by the authors. Table 5 includes example data points from each domain and Figure 5 shows the full taxonomy of the WildSeek dataset."}, {"title": "Mind Map Insert Operation", "content": "As revealed in human evaluation results (see \u00a76), the mind map is crucial for helping users track the discourse and the collected information. Co-STORM dynamically updates the mind map through insert and reorganize operations. In this section, we conduct controlled experiments on different implementations of insert and verify the quality of the mind map updates.\nDynamically organizing collected information into a mind map is challenging. Unlike classic document classification tasks (Zhang et al., 2024, 2023) and recursive summarization tasks (Sarthi et al., 2024; Gao et al., 2023), where either the hierarchical organization or the information to be organized is fixed, mind map insertion involves an evolving hierarchical organization of concepts\n\u2026reranking score is\ncos(i, t)\u03b1(1 \u2013 cos(i, q))1-\u03b1,"}, {"title": "Full Prompts in Co-STORM", "content": "In \u00a73.1, we introduce Co-STORM 's collaborative discourse protocol which includes three key roles: the user, experts, and a moderator. We implement the perspective-guided expert and moderator pipeline using zero-shot prompting of gpt-40-2024-05-13. Listing 2 and Listing 3 documents the full prompts for simulating the expert and the moderator respectively. Co-STORM uses a hierarchical mind map to track the discourse (\u00a73.2) and the mind map insert operation is detailed in Appendix B. Prompts used for the mind map operations can be found in Listing 1."}]}