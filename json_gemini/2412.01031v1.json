{"title": "EVALUATING AUTOMATED RADIOLOGY REPORT QUALITY THROUGH FINE-GRAINED PHRASAL GROUNDING OF CLINICAL FINDINGS", "authors": ["Razi Mahmood", "Pingkun Yan", "Diego Machado Reyes", "Ge Wang", "Mannudeep K. Kalra", "Parisa Kaviani", "Joy T. Wu", "Tanveer Syeda-Mahmood"], "abstract": "Several evaluation metrics have been developed recently to automatically assess the quality of generative AI reports for chest radiographs based only on textual information using lexical, semantic, or clinical named entity recognition methods. In this paper, we develop a new method of report quality evaluation by first extracting fine-grained finding patterns capturing the location, laterality, and severity of a large number of clinical findings. We then performed phrasal grounding to localize their associated anatomical regions on chest radiograph images. The textual and visual measures are then combined to rate the quality of the generated reports. We present results that compare this evaluation metric with other textual metrics on a gold standard dataset derived from the MIMIC collection and show its robustness and sensitivity to factual errors.", "sections": [{"title": "1. INTRODUCTION", "content": "With the evolution of AI models, it is now possible to produce realistic-looking natural language radiology reports, particularly for chest X-rays [1, 2, 3, 4]. Figure 1e shows a sample report using GPT-4 [5] on the chest X-ray image shown on Figure 1a. While this appears good on surface, upon closer examination and comparing to the ground truth report shown in Figure 1b, several mistakes can be found including the potential conclusion of pneumonia and missed pulmonary hypertension in the hilar regions. In general, the reports produced by generative AI tools can have false predictions, omissions, incorrect finding locations or incorrect severity assessments. Identifying such factual errors, therefore, requires quality measures that can pay attention to both presence and absence of findings, their locations, laterality and severity. Further, they should be robust to the different ways in which a finding is described. Current methods for evaluating such descriptions have so far been based on lexical or textual semantics-based scoring metrics[6, 5, 7, 8, 9, 10, 11]. While some metrics cover clinical entities and their relations[9, 11], generally scoring metrics do not explicitly capture the textual mention differences in the anatomy, laterality and severity. Further, phrasal grounding of the findings in terms of anatomical localization in images is not exploited in the quality scoring.\nIn this paper, we propose a metric that captures both fine-grained textual descriptions of findings as well as their phrasal grounding information in terms of anatomical locations in images. We present results that compare this evaluation metric to other textual metrics on a gold standard dataset derived from MIMIC collection of chest X-rays and validated reports, to show its robustness and sensitivity to factual errors."}, {"title": "2. OVERALL APPROACH", "content": "Our overall approach to evaluating report quality is illustrated in Figure 2. Given a chest X-ray image and its associated ground truth report, we first extract fine-grained finding (FFL) patterns from the ground truth report as described in[12]. This creates a structured description of the report using a normalized vocabulary for findings derived from a chest X-ray lexicon[12]. Next, we extract all important anatomical region bounding boxes as defined in the ChestImagenome dataset[13] and assign them to the relevant findings based on the anatomical location mentioned in the FFL textual pattern. Next, to evaluate an automated report generated for the same image by available methods[1, 2, 14, 15], we similarly extract FFL patterns from the automated report. The overlap in FFL patterns of automated and ground truth reports is evaluated in terms of precision, recall and F1-score. A geometric comparison is then initiated with the pair of FFL patterns from the ground truth and automated regions using the bounding boxes of the referred anatomical locations within each FFL to do phrasal grounding of the underlying findings. The spatial overlap of the anatomical regions indicated in the findings constitutes a geometric similarity measure per FFL pattern. A bipartite graph is formed from the FFL pairs using the spatial overlap measure as edge weights. The mean IOU score derived from the maximum matching in the bi-partite graph serves as the phrasal grounding metric. The final quality score is then the average of the F1 and mean IOU scores, reflecting a match in both the textual description of findings and their implied anatomical locations. The rest of the section describes the above approach in detail."}, {"title": "Extraction of FFL patterns", "content": "A fine-grained finding pattern (FFL) $F_i$ describes a finding in terms of its presence or absence and modifiers as described in[2]. For our report evaluation purpose, we restrict the modifiers to cover location, laterality and severity of a finding so that an FFL is denoted by:\n$F_i = T_i N_i C_i A_i L_i S_i$                                                                                                                 (1)\nwhere $T_i$ is the finding type, $N_i = yes|no$ indicates a present or absent finding respectively, $C_i$ is the normalized core finding name, $A_i$ is the anatomical location, $L_i$ reflects laterality, and $S_i$ reflects the severity of the core finding $C_i$. The normalized values for each category of information captured in the FFL patterns was derived from a comprehensive clinician-curated chest X-ray lexicon described in [12, 16].\nTo extract the FFL patterns, adopting a vocabulary driven approach described in earlier work[2, 16], we detect the core finding and their modifiers using the chest X-ray lexicon. We then group noun phrases and apply negation detection and pattern completion as described in [2, 16]. The FFL pattern completion step uses a priori domain knowledge to incorporate anatomical locations. For example, \"alveoli\" would be inserted for an alveolar finding, even when not specified in the report sentence. Thus the final FFL patterns show more information than the original sentence. Further, due to the normalization of names through the lexicon, they are robust to variations in descriptions of the same finding across reports.\nFigures 1c and f show the FFL patterns derived from the sentences in the ground truth and automated reports from Figures 1b and e respectively. As can be seen, FFL labels capture the essence of a report adequately and much more comprehensively than core findings alone. The FFL label extraction algorithm reported in [2] is known to be highly accurate in terms of the coverage of findings with around 3% error mostly due to negation sense detection."}, {"title": "3. DEVELOPING REPORT EVALUATION SCORE", "content": "We now describe our clinical accuracy score using the structured representation of the findings in terms of FFL patterns and their phrasal grounding. Specifically, given a ground truth radiology report G and a predicted automated report P, we extract FFL pattern set from sentences within these reports as $F_G$ and $F_P$ from Equation 1. For each FFL pattern $F_i$, we also form prefix patterns obtained by successively removing modifier descriptions as:\n$W_i(F_i) = T_i|N_i|C_i|M_1|...|M_j$                                                                                              (2)\nwith $M_i$ is the jth modifier retained in the FFL pattern. By creating prefixes of patterns at modifier boundaries, we can assess the quality of matching at various levels of granularity. FFL F-1 score\nGiven two prefix versions of FFL patterns between ground truth report and generated automated report, we can calculate the true positives (tp), and false positives (fp)and false negatives (fn) to computer F1 score as:\n$tp = |W_j(F_{Gi}), s.t. W_j(F_{Gi}) = W_j(F_{Pk})|$                                                                   (3)\n$fp = |W_j(F_{Pk})| - tp_j, fn = |W_j(F_{Gi})| - tp_j$                                                                  (4)\n$F1_{G,P} = \\frac{2tp}{2tp + fp + fn}$                                                                                                        (5)\nHere $W_j(F_{Gi}$ and $W_j(F_{Pk}$ are the matching FFL patterns from ground truth and automated report respectively.\nMIOU score\nTo evaluate the geometric overlap between the findings, we consider FFL patterns that indicate the same core finding prefix (i.e. match in $T_i|N_i|C_i$). Since the same core finding can be observed in multiple locations (e.g. left upper lobe, and right lower lobe), several possible matches exist between pairs of FFL patterns of ground truth and generated reports. To compute the overlap between the indicated spatial locations in the pairs, we use the IOU score. Specifically, let the anatomical location bounding box in an FFL pattern $F_{Pk}$ of a predicted report be denoted by $B_{Pk} = < x_1, y_1, x_2, y_2>$, and let $B_{Gi} = < x_{g1},y_{g1},x_{g2},y_2 >$ be the anatomical location of the corresponding ground truth finding. Then the IOU score is given by:\n$I_{ki} = \\frac{|B_{Pk} \\cap B_{Gi}|}{|B_{Pk} \\cup B_{Gi}|}$                                                                                                  (6)\nTo find the best pair of corresponding findings, we treat the FFL patterns of ground truth report and automated report as a bipartite graph and perform a maximum matching using the IOU score to weigh the edges. The resulting cost of the"}, {"title": "4. RESULTS", "content": "We now present results of applying the quality score to assess report quality on a benchmark dataset of chest X-ray images with validated ground truth reports and extracted FFL patterns.\nDataset\nFor our experiments, we selected the gold dataset of 439 chest x-rays and their ground truth reports from the publicly available clinician validated ChestImagenome[13] collection built from the MIMIC dataset[18]. The dataset also provided FFL patterns covering 60 findings extracted from the finding and impression sections of the ground truth reports to serve for our report quality evaluation. Further, 36 anatomical locations were marked in each of the images and validated by 2 clinicians.\nTo evaluate report quality, we experimented with open source radiology report generation tools, namely, XrayGPT[14] and RGRG[15], and an internal tool based on GPT-4 being piloted in our hospital. We ran the report generation tool on the benchmark dataset and retained their automated reports. We then extracted the FFL patterns and recorded the bounding boxes of their indicated anatomical locations for the computation of the report quality score.\nEvaluating through proposed measure\nNext, we evaluated the report quality using our lexical measure reported in Section 3 using prefix patterns restricting to core finding, anatomy (with laterality), severity. The result is shown for the 3 report generators evaluated in Table 1.\nFrom this table, we observe that the RGRG report generator has the highest lexical quality with their FFL patterns matching closely with the ground truth FFL patterns at all levels of granularity. We also notice that all methods improve in report quality when evaluated on the basis of their core finding. The Mean IOU scores were evaluated for the FFL prefixes that retained the anatomical location and are as shown in the last column of that table.\nComparison to evaluation scores:\nTo compare our approach to other report evaluation scores, we selected representative methods for word overlap scores (BLEU[6]), semantic textual matching (BERTscore[8]) and clinical accuracy F1-score (Radgraph[9]). The result is shown in Table 2 from which we observed that the lexical comparison scores under-estimated the accuracy of the reports due to lexical mismatch in the reported descriptions. The clinical accuracy score, as it was trained on fewer findings (14 findings), overestimated the performance by giving higher scores due to missed findings in their model. Our approach gave balanced estimates of report quality indicating 36-48% spatial overlap of their locations and 33-44% overlap in their descriptions. Finally, we observe that all reporting metrics rated the RGRG report as the best even in this evaluation.\nSensitivity of the report quality score\nTo measure sensitivity, we created 500 additional synthetic reports by perturbing each ground truth reports to introduce a range of errors in findings in terms of negation reversal, substitutions, and alteration in location and severity The FFL pattern extraction, and spatial localization of findings was completed on the synthetic reports and all quality scores were re-evaluated by comparing the synthetic reports to the associated ground truth reports. The interval change of scores was taken as a measure of sensitivity of the report evaluation score to the factual errors. The result is shown in Table 3 for all report evaluation measures. As can be seen, the lexical and semantic score changes remained generally low, while the Radgraph clinical accuracy F1 score showed less sensitivity to location variations. In comparison, our quality score showed good range of variation to reflect quality in such fine-grained characterization."}, {"title": "5. CONCLUSIONS", "content": "In this paper, we present a new approach to evaluating the quality of generated chest X-ray radiology reports. Our approach captured fine-grained finding patterns along with phrasal grounding of findings and is shown to be sensitive to factual errors in radiology reports making it suitable as an evaluation metric for fact-checking of radiology reports."}]}