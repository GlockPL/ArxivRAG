{"title": "Autoformalization of Game Descriptions using Large Language Models", "authors": ["Agnieszka Mensfelt", "Kostas Stathis", "Vince Trencsenyi"], "abstract": "Game theory is a powerful framework for reasoning about strategic interactions, with applications in domains ranging from day-to-day life to international politics. However, applying formal reasoning tools in such contexts is challenging, as these scenarios are often expressed in natural language. To address this, we introduce a framework for the autoformalization of game-theoretic scenarios, which translates natural language descriptions into formal logic representations suitable for formal solvers. Our approach utilizes one-shot prompting and a solver that provides feedback on syntactic correctness to allow LLMs to refine the code. We evaluate the framework using GPT-40 and a dataset of natural language problem descriptions, achieving 98% syntactic correctness and 88% semantic correctness. These results show the potential of LLMS to bridge the gap between real-life strategic interactions and formal reasoning.", "sections": [{"title": "1 Introduction", "content": "Game theory (von Neumann and Morgenstern 1944) is a mathematical framework that facilitates the analysis of competitive and cooperative interactions among rational decision-makers. It can be applied to a wide range of scenarios, from family decision-making to business strategies to nuclear conflict. However, these scenarios are typically expressed in natural language, making it challenging to apply formal reasoning tools directly. Moreover, the inherent ambiguities and complexities of natural language complicate the task of automatically converting problem descriptions into formal representations suitable for solvers.\nRecent advancements in Large Language Models (LLMs) offer a solution to this challenge. LLMs exhibit remarkable abilities in handling the nuances of natural language in the context of translation. They have proved to be successful in converting natural language into formal representations, including mathematical (Wu et al. 2022; He-Yueya et al. 2023; Jiang et al. 2022) and logical expressions (Cosler et al. 2023; Pan et al. 2023; Feng et al. 2023; Yang, Ishay, and Lee 2023; Chen et al. 2023), which is known as autoformalization (Wu et al. 2022). This paper explores the novel application of LLMs in the context of game theory, specifically focusing on translating natural language descriptions of game-theoretic scenarios into formal representations that enable formal reasoning in real-world situations.\nTo the best of our knowledge, this work is the first to leverage LLMs for autoformalization within the domain of game theory. The results pave the way for more easily accessible applications of game theory, bridging the gap between natural language and formal reasoning in strategic interactive scenarios. The main contributions of this paper are as follows:\n\u2022 Dataset creation. We developed a dataset consisting of 105 natural language descriptions of scenarios that can be modelled using game theory. This dataset includes both standard and non-standard descriptions, with and without numerical payoffs, to vary the translation difficulty level.\n\u2022 Autoformalization framework development. We propose a novel framework for translating natural language descriptions into formal representations, enabling formal reasoning in real-world game-theoretic scenarios.\n\u2022 Evaluation. We evaluate the performance of GPT-40 within our framework by assessing its ability to translate natural language descriptions of strategic interactions into formal representations. Our evaluation includes both zero-shot and one-shot prompting and examines the model's capacity to generate formal specifications for games that differ structurally from provided examples.\nThe remainder of this paper is organized as follows. Section 2 provides a background overview, including LLMs, game theory, and general game playing. Section 3 details the formal solver, the proposed framework, experimental setup, and the dataset of game descriptions. Section 4 presents the evaluation results on the dataset. Section 5 reviews related work. Finally, Section 6 provides the conclusions and outlines future work."}, {"title": "2 Background", "content": "Large Language Models Multi-layered LSTMs are sequential models that perform well on complex sequential tasks such as language translation (Sutskever, Vinyals, and Le 2014). Following the emergence of transformers and the success of pre-trained models (Qiu et al. 2020), such architectures had become the go-to solution for state-of-the-art Large Language Models (LLMs). LLM performance depends on billions of parameters and huge training data sets (Zhao et al. 2023). While LLMs generalize well over a wide"}, {"title": "Game Theory", "content": "Game theory provides a mathematical framework to reason about strategic interactions and decision-making analytically (Osborne 2004). Following the formalization by (Osborne and Rubinstein 1994), we define a game theoretic encounter a game - by its players, the available strategies and payoffs.\nPlayers Game theory assumes a simple definition of rationality to hold for all of its actors the players and that each actor is aware that all others are also rational in the same sense. Such players are expected to act intelligently and voluntarily, with a full understanding of the game and to be acting to optimize their own utility score (Myerson 1984).\nStrategies A player's strategy maps the state of the game to an available action, also considering the accumulated history (Osborne and Rubinstein 1994).\nPayoff A payoff is a numerical value associated with a specific outcome of strategic interaction; it is the reward each player receives as a result of a combination of their actions (Gibbons 1992).\nGame Based on the outlines of (Osborne and Rubinstein 1994) and (Rasmusen 2006), our games are formally defined in the following way:\n\u2022 N: the set of n players;\n\u2022 $A_i$: $\u2200i \u2208 N$, a non-empty set of actions available to the player;\n\u2022 $u_i$: a utility function mapping $A_i \u2192 R$, $\u2200i \u2208 N$, producing $\u03c0_i$.\nIn the context of our experiments, we refer to non-cooperative games in normal form (Rasmusen 2006). A general example is shown in Table 1. Our five games are characterised by such 2 \u00d7 2 payoff matrices, having two players and two actions \u2013 Cooperate and Defect. Taking the row player's perspective, these actions lead to four possible outcomes:\n\u2022 T: the Temptation to defect, resulting from the opponent choosing D against the player's C;\n\u2022 R: the Reward for mutual cooperation;\n\u2022 P: the Punishment for mutual defection;\n\u2022 S: the Sucker's Payoff, resulting from the player cooperating while the opponent chooses to defect.\nThe relation of these four outcomes can be used to classify strategic interaction into different games (Rasmusen 2006)."}, {"title": "Prisoner's Dilemma", "content": "The Prisoner's Dilemma is a symmetric game defined by a payoff structure that values the temptation to defect over mutual cooperation: T > R > P > S. An example payoff matrix is (-1,-1) for mutual cooperation, (0,-10) for the temptation to defect, (-10,0) for the sucker's payoff, and (5,5) for mutual defection. The negative payoffs represent years spent in prison."}, {"title": "Hawk-Dove", "content": "The Hawk-Dove game describes a situation where T is associated with the highest reward, but the punishment for mutual defection yields the lowest utility. The definitive relation T > R > S > P is demonstrated by the following payoff assignments: R:(0,0), S:(-1,1), T:(1,-1), P:(-10,-10)."}, {"title": "Matching Pennies", "content": "Matching Pennies is a zero-sum game. In our generalised version of the game, if both players choose the same action, the row player gets higher payoff while the opponent gets lower payoff. If the player choices are in disagreement, the yields are the reverse resulting in the asymmetric outcome relations: R, P > T, S and T, S > R, P."}, {"title": "Stag Hunt", "content": "The Stag Hunt involves a payoff structure which values mutual cooperation over the temptation to defect. For instance, mutual cooperation gives both players a payoff of 3, the temptation to defect yields 2 while the sucker gets 0, and mutual defection will result in a reward of 1 utility for both resulting in the outcome relation R > T > P > S."}, {"title": "Battle of the Sexes", "content": "In this asymmetric coordination game, both players have a preferred action, while disagreement is the least preferred outcome for both (Osborne 2004). For instance, mutual cooperation yields (2, 1), mutual defection gives (1, 2), and distinct player choices result in 0 for both. From the row player's perspective, the situation is characterised by the relation R > P > T, S, and P > R > T, S for the column player."}, {"title": "General Game Playing", "content": "The idea behind general game playing (Genesereth, Love, and Pell 2005) is about how to construct intelligent systems that can process the rules of arbitrary new games and learn to play these games without human intervention. Key to this idea is the notion of a Game Description Language (GDL), proposed as a formal and machine-processable language for describing the"}, {"title": "3 Methods", "content": "Our methods rely on a logic programming solver that provides a game game-independent formulation of an initial state, a set of legal moves, the effects of these moves on the state, and a description of the final states of a game. This formulation can generate or test all possible evolutions of a specific game, given we provide it with the domain-dependent part describing the specifics of that game. If we then feed the solver a logic programming description of a specific game and a textual description of that game to an LLM, we can then prompt the LLM to generate the formulations of different games from textual descriptions. We then devise an algorithm that allows us to do just that, viz., automatically formalise logic programs of new games systematically from textual descriptions of these games."}, {"title": "3.1 Solver", "content": "Our solver consists of a game-independent part specifying the rules of any game in extensive form, a game-dependent part expressing the rules of a specific game, and a set of auxiliary predicates used on top of these representations to support processing a game. We represent all parts in Prolog: variables are denoted by uppercase letters, predicates and function symbols by lowercase letters. The symbol : - is read as if, and \\+ is read as not (negation by failure). An underscore'' is used to denote a variable whose value is unused within a specific definition. In this setting, the state of a game is represented as a situation understood by histories of moves starting in an initial situation denoted by a constant (e.g. s0). The special binary function do (M, S) represents the situation resulting from the execution of move M in situation S, as in the Situation Calculus."}, {"title": "Game-independent part", "content": "We specify legal transitions of an extended form game from an initial situation S to a final situation F as:\ngame (F,F):-\nfinal(F).\ngame (S,F):-\n\\+ final (S),\nlegal (M,S),\ngame (do (M,S),F).\nThe final situation F is returned when reached. In a non-final situation S, the game accepts a legal move M, and the game continues in the next do (M, S) situation, until the final situation F is reached. To reason about what holds in a situation, we use Situation Calculus:\nholds (F, S):-\ninitially (F, S).\nholds (F, do (M, S)):\neffect (F, M, S).\nholds (F, do (M, S)):\nholds (F, S),\n\\+ abnormal (F, M, S).\nA fluent F holds in the initial situation, a new fluent F is initiated by the effects of a move M executed in a situation S, and a fluent F persists after a move is made, provided it is not abnormal; abnormal fluents are terminated (do not persist). Rules of the form:\nfinally (F, S): Conditions.\nreturn derived fluents F describing the result of the game, when the Conditions hold in the final situation S."}, {"title": "Game-dependent part", "content": "For a specific game, we define game-dependent predicates for the initial state initial/1, the legal moves legal/2, what holds in the initial game situation via initially/2, the effects of a move on a situation via effect/3, what stops persisting in a situation after the execution of a move via abnormal/3, the final situation final/1, and the result of the game via finally/2 definitions. To exemplify a game definition, we show how to describe a PD game with initial situation s0, defined as:\ninitial (s0).\nWhat holds in so we specify as:\ninitially (player(p1), s0).\ninitially (player (p2), s0).\ninitially (role(p1,row), s0).\ninitially (role(p2,col), s0).\ninitially (control(p1), s0).\ninitially (control (p2), s0).\nPlayer names are represented by unique identifiers (p1 and p2), their roles (p1 is the row player, while p2 is the column player), and then the fact that initially any of them can play next (we use a control/1 fluent to indicate that,"}, {"title": "3.2 Framework", "content": "To generate a formal game specification for a given game, we use the one-shot prompting approach. The prompt consists of game-independent predicates (\u0393), an example: a natural language description of Prisoner's Dilemma (NLPD) and game-specific predicates for PD (\u00c9PD), and a natural language description of a game to be translated (NLNG). An LLM is prompted iteratively. Once the game-specific predicates (ENG) are generated, they are validated for syntactic correctness using a Prolog solver. If the generated predicates contain syntax errors, the LLM is re-prompted with the solver's error trace and additional instructions for correcting the Prolog code. This correction process is repeated up to max_attempts. An overview of the algorithm is provided in Listing 1."}, {"title": "3.3 Game descriptions", "content": "To evaluate the ability of GPT-40 to translate natural language game descriptions into their formal specification, we"}, {"title": "3.4 Experimental parameters", "content": "To perform autoformalization we employed state-of-the-art model GPT-40. The experimental parameters used in the evaluation are shown in Table 3. The source code and the logs generated in the experiment are publicly available\u00b9."}, {"title": "4 Results and Discussion", "content": "Our experimental results include both zero-shot and one-shot prompting, as well as testing the model's capability to generate formal specifications for games that differ structurally from the provided examples. Although the outcomes from zero-shot prompting demonstrate the weaknesses of LLMs to produce formal description for our game-independent part, one-shot prompting with an example is able to provide results that adapt successfully the two-player game formulations for different game instances."}, {"title": "4.1 Zero-shot prompting", "content": "We started by performing zero-shot prompting, including only the predicates for the game-independent part in the prompt. The goal was to generate the game-dependent part from a textual description without an example. For PD, the following code sequence was generated as an answer. The first part of the answer contained the representation of the players and the legal moves:\nsuspect (suspect1).\nsuspect (suspect2).\nlegal (confess (S), _ ):- suspect (S).\nlegal (silent(S), _):- suspect (S).\nAlthough players represented as suspects is acceptable, players' moves being legal in any state is not; the underscore ('_') indicates that state is unnecessary when checking whether a move is legal. This is obviously undesired, as it is illegal to make two consecutive moves, but the formulation allows it. The generated answer also contains what holds initially:\ninitially (state(0, 0), s0).\nHere clearly the generated code is problematic as the term state(0,0) has unclear parameters, including their 0 value. Also, problematic is the code generated subsequently, for when a state is final:\nfinal (state (_, _)).\nThis implies that any state term of the form state (-, -) is final. So already the solution so far is unusable. The situation deteriorates with the effects of moves. We show below an effect rule, specified as:\neffect (state (Y1, Y2), silent (suspect1),\nstate (NY1, NY2)) :-\nholds (state (Y1, Y2), s0),\n(% If suspect2 also silent\nholds (state (_, _), do (silent(\nsuspect2), s0))\n-> NY1 is Y1 + 1, NY2 is Y2 + 1\n; % If suspect2 confesses\n).\nNY1 is Y1, NY2 is 10\nThe above effect rule has the state as the first and third arguments, but the state is not in situation calculus format. As a result, only what happens at the initial state will be inferred correctly. Also, the first argument is intended as a fluent in"}, {"title": "4.2 2-player simultaneous move games", "content": "In the one-shot prompting method, we provided game-specific axioms for prisoner's dilemma, \u00c9PD, as an example. This resulted in the generation of game-specific predicates for the natural language description variants presented in Table 2, analogous to those defined for PD (see Sect. 3.1). The main difference was the definition of the payoff matrix. Table 4 summarizes the syntactic and semantic accuracy of the generated predicates. Syntactic accuracy refers to the percentage of syntactically correct Prolog programs produced on the first translation attempt, before any revisions were made. The semantic correctness of the generated predicates (e.g. the relationships between payoffs in the payoff matrix) was reviewed manually. During the inspection of non-standard natural language descriptions, some instances were found to be incomplete or ambiguous. As a result, semantic accuracy was calculated based on 48 samples for the numerical variants and 47 samples for the non-numerical variants.\nThe generated code was syntactically correct in 108 out of 110 cases (98%). In one of the incorrect cases, the error involved the use of a comment delimiter ('//'), not valid in Prolog. In the second case, the code triggered a warning about singleton variables. Notably, despite receiving feedback from the Prolog solver, GPT-4o was unable to fix the syntactic errors. This contrasts with our previous work, where the model was able to correct reasoning errors in game-theoretic scenarios based on solver feedback (Mensfelt, Stathis, and Trencsenyi 2024). A potential explanation could be that the feedback prompt, which included the solver trace and debugging guidelines, may not have been specific enough. However, when the model was re-prompted in a separate call, syntactically correct code was generated on the first attempt. For analyzing semantic correctness, the syntactically incorrect codes were replaced with corrected versions.\nRegarding semantic correctness, the standard examples were translated into their formal specification with 100% accuracy. For the non-standard examples, the accuracy"}, {"title": "4.3 Towards Generalisation", "content": "Sequential PD In order to explore whether the LLM can generalise over different aspects of a game, we also run our system to see what is generated in the case of the sequential PD. This is a turn-based two-player game in which there is a first-mover that chooses to cooperate or defect, and after observing this choice, the second-mover responds with an action of the same set. In terms of the game specification, the LLM should produce the same specification as the game-dependent part of Section 3.1, with the only differences (a) a revised payoff matrix and (b) the way the initial state is specified, to allow only for one player to play first, and not both, i.e. it should contain only one player having the control of the game at state so. Indeed, as shown below, the system generated in the initial state two players a and b, the role of a is to play first, and of b second, and the control is given to only to a:"}, {"title": "Rock-Paper-Scissors", "content": "We also explored whether the LLM can generalise over the number of moves of the game and the specification of the possible moves. For this purpose, we gave it the textual description of the Rock-Paper-Scissors game, a simultaneous zero-sum game that has three possible outcomes: a draw, a win, or a loss. A player who decides to play rock will beat another player who chooses scissors (\"rock crushes scissors\"), but will lose to one who has played paper (\u201cpaper covers rock", "scissors cuts paper\\\"). If both players choose the same shape, the game is tied and is usually replayed to break the tie. Again, our system produced the same specification as the game-dependent part of Section 3.1, with the only differences (a) a revised payoff matrix and (b) the way possible moves were specified, to allow for three moves instead of two, as shown below": "npossible (choice (P, 'rock'), S) :-\nholds (player(P), S).\npossible (choice (P, 'paper'), S) :-\nholds (player (P), S).\npossible (choice(P, 'scissors\u2032), S) :-\nholds (player(P), S).\nThis demonstrates that our original formulation of a domain-specific game allowed our system to generalise well to different aspects of a new game."}, {"title": "5 Related Work", "content": "LLMs have gained significant interest as potential building blocks for agents in game-theoretical simulations (Akata et al. 2023; Fan et al. 2023; Guo 2023; Lor\u00e8 and Heydari 2023). Despite their promise, a major obstacle is the inherent challenge that reasoning tasks pose for LLMs (Rae et al. 2022), (Nye et al. 2021). While techniques such as Chain-of-Thought (Wei et al. 2023) can enhance performance in some of the tasks, even state-of-the-art models are not free from logical and arithmetic errors (Imani, Du, and Shrivastava 2023). An alternative approach to leveraging LLMs for reasoning in game-theoretic scenarios is to utilise them for translating natural language into formal representations, followed by employing a formal solver for the actual reasoning process. This method, known as autoformalization (Wu et al. 2022), has been successful in translating natural language into mathematical formalisms (Wu et al. 2022; He-Yueya et al. 2023; Jiang et al. 2022), providing a method to integrate LLMs with formal tools to solve reasoning tasks.\nThe autoformalization approach was also successfully applied to creating logical representations of natural language descriptions. Yang et al. (Yang, Ishay, and Lee 2023) used GPT-3 and few-shot prompting to translate natural language sentences into their formal representation, passed as an input to answer set programs. GPT-3 autoformalization coupled with ASP achieved high accuracy on NLP benchmarks, however, errors in translation from natural language occurred in some cases. Pan et al. (Pan et al. 2023) combined in-context learning with self-refinement based on the evaluation from the solver. This improved the performance over standard LLM and chain-of-thought prompting on logical reasoning benchmarks. This approach was also not free from translation errors. To address this problem in the context of translation to temporal logics, Cosler et al. (Cosler et al. 2023) introduced an interactive approach in which the user can edit sub-translations. Fine-tuning of LLMs was demonstrated to be another successful method for increasing translation accuracy in both temporal logics (Chen et al. 2023) and deductive datasets (Feng et al. 2023).\nThe aforementioned works considered standard logical reasoning tasks and benchmarks. To the best of our knowledge, this work is the first to perform autoformalization in the context of game theory."}, {"title": "6 Conclusions and Future Work", "content": "To evaluate the ability of our framework and GPT-40 to perform autoformalization in the domain of logical representation of game-theoretic scenarios, we developed a dataset of natural-language descriptions. The descriptions involved scenarios ranging from family decision-making to business to warfare, possible to model by 2-players simultaneous-move games. The descriptions were developed in two versions, including and lacking numerical payoffs.\nThe preliminary assessment demonstrated that with zero-shot prompting GPT-40 was not able to autoformalize the standard description of the Prisoner's Dilemma. One-shot prompting, however, showed high syntactic and semantic accuracy. Interestingly, even in the case of real-world examples with no numeric payoffs, the LLM was mostly able to infer an appropriate payoff matrix. To evaluate the ability of generalization within the proposed framework, we used the natural-language descriptions of sequential Prisoner's Dilemma and Rock-Paper-Scissors. The formal representation of both games, differing in structure from non-sequential PD provided as an example in the prompt, were syntactically and semantically correct.\nWe acknowledge several limitations of this work. The semantic correctness was investigated manually, which limits the scalability of the evaluation. We plan to address this in future work by not only generating the predicates for the game description but also executing the code to find payoffs for specific actions, which will allow us to automatize the evaluation. The class of games that we considered was limited to 2-players simultaneous-move games and two games that differ only in the sequential nature of decision-making and the number of possible actions. We will expand the set of considered games to include games from outside this class and outside game theory.\nOverall, the results show the promise of using LLMs as a bridge between real-life strategic interactions and the game theory framework, facilitating the use of formal tools in finding an optimal action in such scenarios."}]}