{"title": "SRSA: A Cost-Efficient Strategy-Router Search Agent for Real-world Human-Machine Interactions", "authors": ["Yaqi Wang", "Haipei Xu"], "abstract": "Recently, as Large Language Models (LLMs) have shown impressive emerging capabilities and gained widespread popularity, research on LLM-based search agents has proliferated. In real-world situations, users often input contextual and highly personalized queries to chatbots, challenging LLMs to capture context and generate appropriate answers. However, much of the prior research has not focused specifically on authentic human-machine dialogue scenarios. It also ignores the important balance between response quality and computational cost by forcing all queries to follow the same agent process. To address these gaps, we propose a Strategy-Router Search Agent (SRSA), routing different queries to appropriate search strategies and enabling fine-grained serial searches to obtain high-quality results at a relatively low cost. To evaluate our work, we introduce a new dataset, Contextual Query Enhancement Dataset (CQED), comprising contextual queries to simulate authentic and daily interactions between humans and chatbots. Using LLM-based automatic evaluation metrics, we assessed SRSA's performance in terms of informativeness, completeness, novelty, and actionability. To conclude, SRSA provides an approach that resolves the issue of simple serial searches leading to degenerate answers for lengthy and contextual queries, effectively and efficiently parses complex user queries, and generates more comprehensive and informative responses without fine-tuning an LLM. The code is available at https://anonymous.4open.science/r/SRSA-3A04/.", "sections": [{"title": "I. INTRODUCTION", "content": "As large language models (LLMs) have demonstrated im-pressive emerging capabilities and gained widespread pop-ularity, researchers have begun leveraging these models tobuild LLM-based agents. Specifically, they adopt LLM asthe main component of the brain or controller of theseagents and expand their perception and action space throughstrategies such as multi-modal perception and tool utilization[1]. Meanwhile, the concept of retrieval-augmented generation(RAG) has emerged as a way to address the limitations ofLLM, particularly their tendency to \"hallucinate\" inaccurateinformation [2] and their difficulty maintaining up-to-dateknowledge within its parameters.\nRecent works like 'Metacognitive Retrieval-AugmentedLarge Language Models' [3] created a search agent that hasmetacognition, achieving better reasoning and planning ability.RA-ISF [4] considered self-knowledge before searching and"}, {"title": "II. RELATED WORK", "content": "Prompt engineering involves designing and refining theprompts that guide language models to generate specific out-puts. This process can significantly enhance the model's per-formance on various tasks without modifying the underlyingarchitecture. Methods of prompt engineering include manuallycrafting prompts that effectively lead the model to desiredanswers and using automated techniques to optimize promptsbased on their performance. Additionally, some advancedstrategies involve dynamic prompts that adjust based on thecontext or through continuous learning. This area is crucial forusing pre-trained models in novel applications and improvinginteraction with LLMs [11].\nHere, we will introduce two techniques that can signifi-cantly improve the effect of prompt engineering: Chain ofThought (CoT) [12] is a reasoning process in which the modelgradually derives a series of intermediate steps or sub-goalsbefore generating the final answer. These intermediate stepsform a step-by-step process that ultimately guides the modelto the correct result. Moreover, it has recently become anindispensable means to improve the performance of LLM incomplex reasoning tasks. In-context learning (ICL) [13] hasbeen widely used for LLM prompting, which augments LLMgeneration by providing a few examples in the prompt. Itefficiently benefits the generation quality because it does notneed model training or parameter adjustment.\nRAG is a technique that allows language models to captureinformation from external knowledge to generate better an-swers. It retrieves relevant information based on the query andthen uses it to guide the LLM in developing a response withinthe retrieved data. According to previous studies, RAG is apowerful tool for improving the accuracy of LLM responsesand processing long contexts [14]. In our paper, we usedRAG to obtain more accurate information, making the LLM'sresponse more comprehensive and high-quality.\nWith the impressive capabilities of LLMs, researchers havestarted leveraging them to build AI agents. LLMs serve asthe core \"brain\" or controller of these agents, with strategieslike multimodal perception and tool utilization extending theirperception and action spaces [1]. So, LLM-based Agent isan intelligent entity that treats LLM as a brain. It can usetools (such as search engines and calculators), interact with theenvironment, and respond and plan what to do next based onthe results of the interaction. There are many successful studieson LLM-based agents, such as MetaGPT [15] featuring themulti-agent framework, Data Interpreter [16] as a data scientistand Devin, and the AI software engineer [17].\nOne of the popular LLM-based Agent frameworks is ReAct(reason+Act) [7], which utilizes chain-of-thought reasoningand repeatedly interacts with the environment, thinking andplanning the next step based on the results of the interaction.Another one is depth-first search-based decision tree (DFSDT)[18]. It allows LLM first to generate step-by-step reasoning.When a failure is found, it returns to the state before the rea-soning track fails and re-performs reasoning, and the reasoning"}, {"title": "III. METHODOLOGY", "content": "To better bridge the gap between complex human contextualqueries and effective search engine inputs, we developed thedynamic Strategy-Router Search Agent (SRSA) framework.\nAs illustrated in Figure 1,the SRSA framework begins byprocessing the initial user query through an LLM, incorporat-ing a time module to ensure temporal relevance. This stepis crucial for handling real-time, context-dependent queriesoften encountered in authentic human-machine interactions."}, {"title": "A. Framework Overview", "content": "The LLM then generates two key outputs: The most appro-priate search strategy for the query and overview searchingsuggestions tailored to the chosen strategy.\nThe core of SRSA, the strategy router, then classifies androutes the query to one of three sophisticated search strategies:\nDirect Search Rewrites the original query and searchesonline to retrieve supporting documents. It is designed forsimple questions that an LLM can answer effectively withsupporting information from a single-round retrieval.\nParallel Search Generates and simultaneously pursuesmultiple related sub-questions, then aggregates and sum-marizes the results. This strategy excels at handlingqueries that usually have two or more parallel conceptsor tasks.\nPlanning Search Utilizes overview suggestions to planwhat to search for at each step and automatically thinksand plans subsequent searches based on the initial results.Queries will be classified into this strategy if the queryrequires a sequence of searches, where each step's inquirydepends on the information obtained from the previoussearch.\nEach search strategy module will not directly produce thefinal answer to the user's query but will output a detailedsupporting documents or reference passage. This referencepassage is then used as a prompt to generate the final answer.\nOur SRSA framework dynamically balances efficiency andeffectiveness. The strategy router plays a crucial role inthis balance, intelligently categorizing queries and directingthem to the most appropriate search strategy. This approachensures that simple queries are handled swiftly and efficiently,while complex, multi-faceted questions receive the depth ofprocessing they require."}, {"title": "B. Strategy-Router Module", "content": "Given that $M(a|b)$ denotes the function using LLM togenerate text, where b is the fixed prompt and a are changeableinputs, the search strategy routing prompt $p_s$, the user'squery Q, the chosen strategy S and the generated suggestionsfor searching $sug(S)$, the strategy-router module could beformalized as\n$S, sug(S) = M(Q|p_s)$\nS will be one of the following: D for direct search, P forparallel search strategy and R for planning search strategy.Now, denote $S(\\cdot)$ as the function that takes the search query asinput, and outputs the supporting documents for the generationof the final answer. Since there are three search strategies, $S(\\cdot)$will be one of the following functions: $D(\\cdot)$, $P(\\cdot)$, $R(\\cdot)$.\nIf a query is classified into directsearch strategy, $sug(D)$ can be parsed into a rephrased query$Q_r$, which is more concise and appropriate version of theuser's lengthy contextual query for input into the searchengine.\n$Q_r = Parser(sug(D))$\n$D(Q) = Search(Q_r)$"}, {"title": "a) Direct Search D:", "content": "This involves generating relatedsub-questions from the main query and executing thesesearches in parallel, similar to the question decompositiontasks described in recent works like Least-to-Most [26]. The results are then aggregated.\n$\\lbrace S_{par1}, S_{par2},..., S_{parn} \\rbrace = M(Q, sug(P)|p_{parallel})$\nThis represents the set of generated sub-questions$\\lbrace S_{parm} \\rbrace_{m=1}^{n}$ from Q, where $p_{parallel}$ is the prompt for theparallel search strategy. The number of sub-questions n isdynamically decided by LLM and is undefined. The parallelsearch results are a concatenation of all search results of eachsub-questions:\n$P(Q) = \\bigcup_{i=1}^{n} Search(S_{para_i})$"}, {"title": "b) Parallel Search P:", "content": "The general workflow of plan-ning search strategy is shown in Figure 2, similar to IR-CoT [10] but it includes an additional summarization process,selects only a subset of the search results as final referencedocuments, rather than using all the search results. Thismodule consists of several iterations. In each iteration, theagent compresses the search results from the previous steps(except for the first step) to filter out irrelevant or nonsensicalinformation. Then the agent performs reasoning to: 1) evaluatethe quality of the search results; if they are of poor quality,determine why and rewrite the search query; 2) identifyinteresting or informative search results from the last iterationand decide whether to explore these points further for deeperinsights; 3) assess whether the search results are sufficient toanswer the user's query. If the agent concludes that the currentsearch results stored in iteration memory are enough to provide"}, {"title": "c) Planning Search R:", "content": "The final answer is generated based on the original query Q, search results referenceS(Q), and a RAG prompt $p_{RAG}$:\n$FinalAnswer = M(Q, S(Q)|p_{RAG})$"}, {"title": "d) Final Answer Generation:", "content": "In the experiments section, we begin by introducing theconstruction of a context-rich question dataset, the ContextualQuery Enhancement Dataset(CQED). Subsequently, we de-scribe the setup of our experiments, including the configurationof parameters for our SRSA and details of the prompts, as wellas the selection and introduction of baseline models, the choiceof search engine API, and the metrics used, as well as how toleverage a LLM for automatic evaluation."}, {"title": "IV. EXPERIMENTS", "content": "We first construct a new dataset that focuses on long-context, user-situated searches, which introduces new chal-lenges to the agent. This task requires a deep understandingof the context and aims to provide users useful and detaileddinformation."}, {"title": "A. Dataset Construction", "content": "Some existing datasetsfor testing Retrieval-Augmented Generation (RAG), such asPopQA [27], TriviaQA [28], HotpotQA [29], and the AI2Reasoning Challenge (ARC) [30], typically provide referencedocuments along with a deterministic question, such as \"Whatis Henry Feilden's occupation?\" These datasets evaluate themodel's RAG ability by testing whether it can extract correct\nTo further refine the capabilities of our search agent in real-world interactions, we've developed a dataset named \"Con-textual Query Enhancement Dataset (CQED).\" This dataset iscrafted to evaluate the agent's ability to navigate complex,user-scenario-based queries that demand high levels of speci-ficity and contextual understanding, as illustrated in Figure 3.Each query in CQED is designed to simulate real-worldsituations, such as users seeking detailed product information,travel plans for specific dates or accurate flight schedules.This design necessitates nuanced searches by the agent toeffectively assist an LLM in generating accurate responses.\nA distinctive challenge posed by the CQED is the signifi-cant semantic divergence between the user's queries and thecontent typically retrievable through standard search methods.Additionally, since a single question may contain multipleexplicit or implicit queries, it is challenging for search agents togenerate comprehensive and satisfactory answers."}, {"title": "1) Why do we need a new dataset:", "content": "The construction of theCQED is illustrated in Figure 4. Initially, domains such asshopping, research, travel, and digital devices were establishedas the primary areas of focus for the dataset. Relevant posts,including titles and content, were retrieved from Reddit usingPRAW [31].\nTo construct the dataset, We conduct multiple rounds ofconversations with the LLM-based chatbot (Claude 3.5 Son-net). In each iteration, the conversational LLM-based chatbotis given 100 crawler results. It is then asked to generate 20scenario-based questions that users might ask. From these, 1to 3 high-quality answers are manually selected and refinedby humans. In the next iteration, previously selected querieswill be presented in the prompt so that the chatbot canlearn from these selections and modifications. This process isrepeated multiple times to generate a total of 182 answers."}, {"title": "2) Dataset Construction Process:", "content": "We set up two baseline agents for comparison with ourSRSA. One baseline agent uses a single-round search tool,while the other is a ReAct agent with a query rewritingmodule. The maximum number of ReAct iterations is set to 5,as this has been shown to be a reasonable number in this work[3]. We initially tried setting this number to 3, but it provedinsufficient, leading to frequent termination due to reaching themaximum iterations. To ensure reproducibility, the temperaturefor LLMs is set to 0, max_token equals the context windowlength of the model, and $n = 1$ (the number of responsesgenerated for each input message) to minimize costs.\nFor the pre-trained LLMs used in the experiments, we\nselected Google's gemma-2-2b-it [32], Meta's Meta-Llama-3-8B-Instruct [33], and Mistral-7B-Instruct-v0.3 [34]. For eval-uation, we employed GPT-40-mini as the judging model,presenting it with the answers from the three search agentssimultaneously. This allows the judging model to compare theresults of our SRSA with those of the baseline agents."}, {"title": "B. Experiments Setup", "content": "There are two baseline agents used forcomparison: The first baseline agent is a single-round searchAgent, referred to as the \"simple search agent\" in the followingsections. This baseline agent performs a one-time search,and the search result is used to generate the final output.The second baseline agent is a ReAct-based search agentthat integrates the ReAct flow (Thought, Action, Observation)with the search tool. It includes a rephrasing module thatreformulates the contextual query into a precise and clearquestion for the search engine. The rephrasing module isincorporated into the ReAct-based search agent for a bettercomparison, as our SRSA has a similar processing component."}, {"title": "1) Baseline Models:", "content": "Studies have shown that LLM canbe used to automatically evaluate text robustly [39], [40]. Weuse the LLM as an automatic evaluation machine, with ourevaluation metrics including:\nMeasures the degree ofinformation richness in the answer and the amount ofuseful information provided. Higher scores are awardedto answers that include a greater proportion of relevantcontent valuable to users.\nAssesses how well the an-swer addresses all aspects of the user's question. This isevaluated by breaking down the query into its constituentconcepts and checking if the answer covers all theseconcepts.\nEvaluates the extent to which theanswer provides information that requires searching andis not common knowledge. Answers that include lessobvious or difficult-to-obtain information score higher.\nMeasures the extent to whichthe user can take specific actions based on the answer.More specific and actionable answers receive higherscores, while abstract or vague answers score lower.\nEach metric is scored on a scale of 0-5, with explanationsprovided to justify the scores. Evaluators reference specificparts of the answers or summarize them to support theirscoring decisions. The scoring process involves a comparisonbetween different agent types (simple search, ReAct-basedsearch, and SRSA) to ensure relative performance is accuratelyreflected.\nTo ensure that the results of the automatic LLM evaluationmet our expectations, we manually evaluated 8 data sets.However, each example is very long. Using too many examplesin one prompt will confuse the referee model, and the middle"}, {"title": "3) Evaluation Metrics:", "content": "To obtain compelling results, we implemented t-test todetermine whether there is a statistically significant differencebetween the two models. We calculated the sample meansand sample variances, then derived the t-statistic and p-value,assuming that the results of the automatic evaluation follow atruncated normal distribution. The t-stat is defined as\n$t=\\frac{\\overline{X_1}-\\overline{X_2}}{\\sqrt{\\frac{s_1}{n_1}+\\frac{s_2}{n_2}}}$\nwhere $\\overline{X_1}$ and $\\overline{X_2}$ are the sample means of the two groups,$\ns_1$ and $s_2$ are the sample variances of the two groups, and $n_1$and $n_2$ are the sample sizes of the two groups."}, {"title": "C. Experiments Results", "content": "According to the previous section, we used three LLMmodels, which we now refer to as Gemma, Llama, and Mistral.Unfortunately, during our experiments, we found that Gemmaand Llama were unable to effectively follow formatted outputbased on the system prompt. Our SRSA requires LLMs to havestrong formatted output capabilities, allowing the program toparse and advance the process based on the LLM's format-ted output. The ReAct-based search agent also has similarrequirements for LLMs. When executing the strategy routermodule, Gemma and Llama were unable to output which classof strategy they chose in a formatted manner, so they wereautomatically categorized as Direct. However, this doesn'tmean that the test data from these two models is useless.We can compare the results of Gemma and Llama runningSRSA when choosing 'Direct' as a targeted strategy with theresults of a simple search agent, demonstrating the importanceof rephrasing in situations where lengthy queries require asearch.\nFortunately, our Mistral model was able to follow the outputof our SRSA model well, so our conclusions are primarilybased on the results from this Mistral model.\nWe can assess the capabilities ofthe rephrasing module by comparing the results of SRSA'sdirect search strategy with those of the simple search agentbecause the only difference is that SRSA's direct searchstrategy rewrites the query before doing a search. As shownin Figure 5, the Llama-based SRSA, which defaulted to thedirect search strategy due to its difficulty in properly followingformatting requirements, still significantly outperforms thesimple search agent in terms of informativeness and com-pleteness. The statistical analysis shows t-statistics of 3.6302and 6.3128, with p-values of 0.0003 and 0.0000, respectively.Similarly, the Gemma-based SRSA using the Direct strategyalso shows significantly higher scores in informativeness andcompleteness compared to the simple search agent, with t-statistics of 4.2606 and 5.9207, and p-values of 0.0000 and0.0000."}, {"title": "1) Power of Rephrasing:", "content": "This finding suggests that ReAct's iterative querying processstruggles when handling complex, context-rich queries thatrequire long and serial search steps. The results imply that asquery complexity increases, ReAct's ability to extract relevantinformation diminishes, leading to less reasonable responses.This is because, during the multiple search steps in the Reactworkflow, the results obtained at each stage, such as websnippets, often end up being either only loosely related to theuser's query or irrelevant. These irrelative documents defectthe generated answers, as discussed in this work [8]. This isthe inspiration for our design of the 'Planning' search strategyin SRSA. Each search requires a compressing and filteringprocess to ensure that when the final answer is generated, thereference documents provided to LLM are highly relevant."}, {"title": "2) Degeneration of ReAct Search Agent for Contextual Queries:", "content": "Figure 6 and Figure 7highlight the strength of the search router; the two figures areall based on the Mistral model. As shown in Figure 7a, theMistral-based SRSA significantly outperforms both the Reactagent and the simple search agent in terms of informativenessand completeness, with t-statistics of 14.9827 and 4.8846,respectively, and p-values of 0.00 for both.\nHowever, there are two metrics, novelty and actionability,where SRSA does not demonstrate a significant improvement;instead, it performs similarly to the simple search agent."}, {"title": "3) Comparison between Baselines:", "content": "In terms of novelty, this may be because performing moresearches does not significantly enhance novelty. A single-round search can still uncover information beyond commonknowledge, and the simple search agent generates answersdirectly from the search results, which often include \"infor-mation that human users may not know or find difficult tothink of without searching.\" As for actionability, while thesimple search agent can provide actionable recommendations,it may lack completeness. For instance, the suggestions mightbe incomplete or insufficient to fully address all of the user'sconcerns or expectations.\nOverall, by dynamically selecting the optimal strategy basedon query characteristics, the search router enables SRSA tooutperform other agents. In particular, it makes the answersmore complete, which means it will answer all the users'needs. Besides, the proportion of helpful information in theanswers is also higher."}, {"title": "4) Power of Search Router:", "content": "Figure 7b offers a closer lookat the specific strategies employed by the search router. Theplanning strategy emerges as the most powerful in terms ofinformativeness and other metrics, except completeness, wherethe parallel strategy performs better. This suggests that whendealing with complex, multi-step queries, the planning strategyexcels at ensuring detailed and accurate responses. In contrast,the parallel strategy's advantage in completeness indicates itsability to capture a broader range of relevant information inless complex queries.\nIt is important to note that our initial intention with thestrategy router was to categorize each query into its appropriatestrategy, thereby reducing unnecessary LLM inference whilemaintaining the quality of results. To validate the capabilityof the search router, we need to verify that the answers toqueries assigned to different strategies do not significantlydiffer in quality. Additionally, we aim to demonstrate thatwithout the search router, when all questions are processedthrough a single strategy, there are significant differences inthe quality of query answers.\nTable II presents a comparison of the direct, parallel, andplanning strategies across different metrics. The results showthat there are few significant differences between strategieswith the search router, as indicated by the p-values. Onlythe comparison between Direct and Planning strategies forInformativeness shows a significant difference (p < 0.05).These findings support the effectiveness of the search routerin appropriately assigning queries to strategies, thereby main-taining consistent quality across different approaches whilereducing computational cost.\nFurthermore, by correctly assigning different queries tosuitable strategies, the model achieves consistent performanceacross queries categorized under different strategies. Thismeans that classification into the direct strategy is sufficient forcertain queries to obtain a good answer, eliminating the needfor more complex strategies, like the planning strategy. Oursearch router successfully identifies these queries, assigningthem to simpler strategies, thereby reducing cost.\nThis approach demonstrates the search router's ability tobalance between response quality and computational effi-ciency. By intelligently routing queries to the most appropriatestrategy, SRSA can provide high-quality answers while con-trolling the computation cost. This optimization is particularlyvaluable in real-world applications where both response qual-ity and system efficiency are crucial."}, {"title": "V. LIMITATION", "content": "In constructing the dataset, considerations regarding thelegality and reliability of data sources were paramount. Thedata was extracted using PRAW from Reddit, raising potentialconcerns about copyright and data privacy. Besides, the datasetsize was limited since manual screening takes too muchtime and money. Moreover, there is no perfect method toevaluate the quality of a dataset; it is only manual evaluation,which is very subjective. Moreover, in our experiments, onlyone LLM model (Mistral) successfully implemented the fullSRSA framework, which may affect the generalizability of theresults. Besides, the number of baselines might not be enough;other reasoning frameworks like IR-CoT [10] might be addedto the baselines to get more convincing results."}, {"title": "VI. CONCLUSION", "content": "We created a context-rich scenario dataset, CQED, to simu-late real human queries and introduce a Strategy-Router SearchAgent (SRSA) that enhances the informativeness, complete-ness, and applicability of responses while utilizing a frozenlarge language model. By dynamically routing queries ofdifferent complexity into appropriate search strategies, SRSAnot only improves the quality of answers but also optimizesinference time, increasing efficiency. Our search agent outper-forms the basic single-round search agent and addresses thedegeneration issues observed in the ReAct-based search agent,which underperforms compared to a single-round agent. Inconclusion, this paper introduced the SRSA to deliver high-quality responses in real-world, context-rich, and personalizedhuman-chatbot interactions at a relatively low computationalcost."}]}