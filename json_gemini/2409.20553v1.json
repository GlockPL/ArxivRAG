{"title": "Maia-2: A Unified Model for Human-AI Alignment in Chess", "authors": ["Zhenwei Tang", "Difan Jiao", "Reid McIlroy-Young", "Jon Kleinberg", "Siddhartha Sen", "Ashton Anderson"], "abstract": "There are an increasing number of domains in which artificial intelligence (AI) systems both surpass human ability and accurately model human behavior. This introduces the possibility of algorithmically-informed teaching in these domains through more relatable AI partners and deeper insights into human decision-making. Critical to achieving this goal, however, is coherently modeling human behavior at various skill levels. Chess is an ideal model system for conducting research into this kind of human-AI alignment, with its rich history as a pivotal testbed for AI research, mature superhuman AI systems like AlphaZero, and precise measurements of skill via chess rating systems. Previous work in modeling human decision-making in chess uses completely independent models to capture human style at different skill levels, meaning they lack coherence in their ability to adapt to the full spectrum of human improvement and are ultimately limited in their effectiveness as AI partners and teaching tools. In this work, we propose a unified modeling approach for human-AI alignment in chess that coherently captures human style across different skill levels and directly captures how people improve. Recognizing the complex, non-linear nature of human learning, we introduce a skill-aware attention mechanism to dynamically integrate players' strengths with encoded chess positions, enabling our model to be sensitive to evolving player skill. Our experimental results demonstrate that this unified framework significantly enhances the alignment between AI and human players across a diverse range of expertise levels, paving the way for deeper insights into human decision-making and AI-guided teaching tools. Maia-2 implementation is available here", "sections": [{"title": "Introduction", "content": "There are an increasing number of domains in which artificial intelligence (AI) systems both surpass human ability and accurately model human behavior. This combination of machine mastery over a domain and computational understanding of human behavior in it introduces the possibility of algorithmically-informed teaching and learning. AI-powered aids could guide people along reliable and efficient improvement paths, synthesized from their knowledge of both human trajectories and objective performance. Relatable AI partners, on the other hand, could learn to act alongside human counterparts in synergistic and complementary ways.\nResearchers have begun to tackle this challenge in the model system of chess. Once held to be an ideal testbed for developing artificial intelligence, it is now the perfect domain to pursue human-AI alignment. The AI community finally surpassed all human ability in chess approximately 20 years ago, a milestone achievement and watershed cultural moment. Now, superhuman AI chess engines are ubiquitous and widely used. Despite this transformation, chess has never been more popular, becoming a mainstream activity in many countries during the last few years. There is now both unprecedented demand for chess education, as well as mature superhuman AI that could in principle help meet it.\nHowever, existing models fall short of being effective learning tools and relatable partners. Traditional chess engines such as Stockfish and AlphaZero are unimaginably strong, but they don't play in ways that humans can easily understand or learn from. Comparing one's own decisions with those of traditional engines, it is easy to see how near-perfect AI would have improved upon your play but hard to see how you could realistically do the same. Recent work has resulted in the development of Maia, a suite of models that aim to mimic human behavior in chess at various skill levels by learning to predict actual human moves from a wealth of online gameplay data [1]. While substantially more human-like, these models still cannot power effective algorithmic teaching tools because of several limitations.\nFirst and foremost, Maia models players at different skill levels completely independently; games by players at one skill level and those by an adjacent skill level are fed into separate instances of the same architecture and result in separate models. This has the downside that predictions from one model are independently made of predictions from any other. Viewed as a whole, they are volatile: the Maia models might predict that at one level players will approach a position correctly, then at the next level they will make a horrible mistake, then at the next level they will do fine again, and so on. In a word, they fail to cohere. People don't improve along volatile paths, they steadily get better. The unrealistically incoherent predictions made by separate models don't suggest realistic pathways that people can take in order to get better. In order to serve as algorithmic teachers or learning aids, our models of human behavior must be coherent.\nBuilding a coherent model of human skill in chess is difficult, because the breadth of skill in chess is almost incomprehensibly large. Decisions made by beginners bear only the faintest of relations to those made by masters. A difference of 200 points in chess rating systems roughly equates to a 75% win rate for the higher-rated player-typically higher than the best record of any team in the entire National Basketball Association. On the online chess platform we study, there are players who are 2600 rating points apart or 13 successive steps of 75%-vs.-25% dominance apart from each other. Capturing this breadth of skill in a single model, in a coherent, smooth fashion, is a challenge.\nWe contribute a unified modeling approach for human-AI alignment in chess that coherently captures human style across different skill levels and directly captures how people improve. Since our model builds directly on original Maia, we call it Maia-2. Maia-2 consists of a standard residual network tower that processes chess positions into features, and our novel contribution of a skill-aware attention module with channel-wise patching. This innovation takes the position representation outputted by the residual network tower and simple player skill encodings and learns how player skill levels interact with chess positions to produce the moves humans make. Unlike previous models, Maia-2 only requires the current board position as input (as opposed to six), which dramatically reduces training time and increases flexibility (e.g. for applying the model in non-game contexts where there may be no 6-board history). In addition to policy and value heads like in previous work, we also add an additional auxiliary information head that helps the model learn a deeper understanding of human chess moves.\nWe evaluate Maia-2 along two key dimensions: move prediction accuracy and coherence. Testing it against the original Maia models, Stockfish, and AlphaZero, Maia-2 emerges as the most accurate human move predictor by far, surpassing original Maia by almost 2 full percentage points. Analyzing move prediction accuracy by skill level, Maia-2 matches and surpasses all other models on all skill levels. Furthermore, Maia-2's gains in perplexity are similarly striking, reducing average perplexity from a previous record of 4.67 bits down to 4.07 bits. Maia-2 achieves these accuracy gains while being substantially more coherent than the original Maia models. For example, call a model's treatment of a position monotonic if it assigns a monotonically increasing probability to the"}, {"title": "Related Work", "content": "This paper draws on the long history of chess at the forefront of AI research [2, 3, 4, 5]. We engage with 3 distinct approaches to building chess AI: heuristic [6], learned [7], and textual [8].\nHeuristic search. The original approach to computer chess was heuristics-based [4, 9]. This method was famously used by IBM's Deep Blue to defeat Garry Kasparov [2] and is currently used by Stockfish [6], one of the strongest chess engines in the world.\nLearned search. Alpha(Zero) Go [7, 10] is a set of neural networks that learn to play Go with methods that generalized to other games, including chess, with AlphaZero [10]. The convolutional neural network stack they introduced is what Maia-2's is based on.\nChess as text. Large language models [11, 12, 13] have recently been found to perform well on tasks that the models were not explicitly trained on [14], including playing chess without fine-tuning [15, 16, 17]. This has lead to chess knowledge being one of the tested features in BIG-Bench [18], a popular LLM evaluation suite. Additionally, fine-tuning a language model can lead to systems that not only play chess, but can also generate comments, describe positions, and create other simple analyses of a game [19, 20, 8]."}, {"title": "Human-AI Alignment in Chess", "content": "Building a chess engine that can defeat any human has been a solved problem for over 20 years. This has led to a new research agenda in extracting useful knowledge from these superhuman systems. A direct way of doing this is to probe an AI chess engine in a human representation space. Without any prior human knowledge or guidance, evidence of human chess concepts learned by AlphaZero is found and measured by linear probes [21]. Going further, AlphaZero also encodes knowledge that extends beyond existing human knowledge but is ultimately learnable by humans [22].Another direction was the creation of a \u2018behavioral stylometry' model that can identify chess players from the moves they play [23].\nHuman-like chess AI. An alternative approach to creating systems that can act as guides to humans is demonstrated by Maia [1, 24], in which a model is trained to predict the next move a human at a given skill level will play, instead of optimizing for winning the game. In addition to predicting human actions, the models have been fine-tuned to predict a given player's actions [24]. This method suggests that players have a distinct and measurable style that can be learned by observing a sufficient number of games. The prediction accuracy can be improved via a reinforcement learning-style search [25]."}, {"title": "Methodology", "content": "We propose a unified model architecture to capture human decision-making in chess across a broad spectrum of skill levels. Since this model builds upon the previous Maia move-matching models, we call it Maia-2. As shown in Figure 1, Maia-2 first encodes active and opponent skill levels and the chess positions, respectively. Then the encoded skill levels and positions are fused using our skill-aware attention with channel-wise patching architecture. The fused representations are then used for move prediction (policy head), auxiliary information prediction (info head), and game outcome prediction (value head). We now discuss each of these components in detail."}, {"title": "Skill Level Encoder", "content": "Instead of directly incorporating player ratings as numerical inputs, we use categorical skill level embeddings for two reasons. First, player behavior and decision-making in chess are not linearly related to their rating. Categorical embeddings allow for capturing complex, non-linear relationships between player strength and their moves. They can encode nuanced differences in play style and strategy that are not directly proportional to player ratings. Second, Generalization across similar skill levels: Players within a certain skill level may exhibit similar playing styles, strategies, and common mistakes. Categorical embeddings group players into these ranges, helping the model to better generalize across players with similar strengths, as opposed to treating each rating as a numerical input.\nLet \\(E \\in \\mathbb{R}^{|E|\\times d_s}\\) be the matrix of player rating embeddings, where each row corresponds to the embedding of a skill level with dimension \\(d_s\\): \\(E = [e_{(0,1000]}, e_{(1000,1100]}, ..., e_{(2000,+\\infty]}]\\). Given the skill levels \\(a\\) and \\(o\\) of an active player (i.e. the player to move) and the opponent player, we look up the embedding matrix \\(E\\) by rows to map the skill levels to active and opponent skill embeddings: \\(e_a = E[a], e_o = E[o]\\).\nNote that previous work [1, 25] uses completely independent models for human-AI alignment at different skill levels\u2014e.g. decisions by 1100-rated chess players are encoded in one model and decisions by 1500-rated players are encoded in a separate model. Further, these models ignore opponent skill level, meaning that predictions cannot vary as a function of opponent strength. However, the active player's decisions may be significantly affected by the opponent's skill level in certain types of situations, or even in general. Players may adjust their strategy based on their perception of the opponent's skill, e.g. a higher-skill opponent might prompt more (or less) cautious play, while against a lower-skill opponent a player may pursue more aggressive tactics. Thus, the interaction between the skill levels of both players is an important component of matching human moves. Unlike existing models that ignore opponent skill level (and actually only consider games in which both players are at the same skill level), we explicitly model not only opponent skill but also the complex interplay between the two players' skill levels, and how it affects human decision-making."}, {"title": "Position Encoder", "content": "Position representation. We use a well-established method [10, 1] to represent each chess position as a multi-channel tensor \\(P_{input} \\in \\mathbb{R}^{C_{board}\\times8\\times8}\\), which includes channels for each type of chess piece, which color is to move, and states of the position that are not derivable from the position alone (castling rights and en passant), where \\(C_{board}\\) denotes the number of channels. One important departure from previous work is that we only use the current chess position, and not the last few chess positions that occurred in the game (models have typically incorporated the six most recent positions in the game). Many games with perfect information, including chess, can be modeled as alternating Markov games [26, 7], where future states are independent of past states given the current game state. Therefore, the current chess position theoretically encapsulates all the information necessary to make future decisions. Although human decision-making in chess may sometimes subtly depend on the historical lead-up to the current position, these effects are anecdotally small. In exchange,"}, {"title": "Bridging Skill Levels and Positions", "content": "A central challenge we face is learning how players at different skill levels interact with chess positions differently. How does an expert player evaluate and process a chess position to come up with a move, and how does this differ from a novice? The relationship between positions and skill levels is complicated by the non-linearity in how players of various skill levels interpret and react to chess positions. This complexity presents a significant challenge in human move prediction using a unified model for diverse skill levels. To bridge skill levels and positions\u2014decision-makers and decisions-we propose skill-aware attention with channel-wise patching.\nChannel-wise patching. In contrast to the area-wise patching approach in Vision Transformers (ViTs) [28], we employ channel-wise patching. Each channel is flattened and linearly transformed, regarding the number of channels in \\(P_{encoded}\\), i.e., \\(C_{patch}\\), as the sequence length.\n\n\\(P_{patched} = Patching(P_{encoded}) \\in \\mathbb{R}^{C_{patch} \\times 64}\\)\n\\(P = P_{patched} W + b \\in \\mathbb{R}^{C_{patch} \\times d_{att}},\\)\n\nwhere \\(W \\in \\mathbb{R}^{64\\times d_{att}}\\) and \\(b \\in \\mathbb{R}^{d_{att}}\\) denote the parameters of the linear projection from the patching dimension to the hidden dimension of the skill-aware attention blocks \\(d_{att}\\). This is particularly suitable for patching encoded chess positions as inputs to Transformer-like architectures, where channels are essentially feature maps that represent different learned latent concepts. These concepts in feature maps are then interactively selected and aggregated considering skill levels via skill-aware attention.\nSkill-aware Attention. Given position representations \\(P_{patched}\\) and skill level representations \\(e_a\\) and \\(e_o\\), our proposed skill-aware multi-head self-attention is computed as follows.\nFor each head \\(k\\), we learn weight matrices \\(W_q^k \\in \\mathbb{R}^{d_{att} x d_h}\\), \\(W_K^k \\in \\mathbb{R}^{d_{att} x d_h}\\), and \\(W_V^k \\in \\mathbb{R}^{d_{att} x d_h}\\), where \\(d_h\\) denote the dimension of each head. The queries \\(Q_k\\), keys \\(K_k\\), and values \\(V_k\\) for each head are computed as:\n\n\\(Q_k = P_{patched} W_q^k, K_k = P_{patched} W_K^k, V_k = P_{patched} W_V^k\\)\n\nIn order to fuse player skill levels and chess positions progressively and interactively, we inject skill level embeddings into queries within the multi-head self-attention: \\(Q'_k = Q_k + (e_a\\bigoplus e_o)W^*\\), where \\(W^* \\in \\mathbb{R}^{2d_s\\times d_h}\\) denotes the weight matrix for feature transformation to the query space, and \\(\\bigoplus\\) is the concatenation operator. We choose to incorporate skill levels in queries because queries directly influence how attention is distributed across patched channels. Using skill-aware queries \\(Q'_k\\), the attention mechanism can adjust its focus to reflect the strategic considerations and positional understanding of players at different skill levels. This adjustment allows Maia-2 to adaptively prioritize features of the positions that are more relevant to the skill levels involved, enhancing the model's contextual sensitivity. The skill-aware scaled dot-product attention for each head is thus defined as:\n\n\\(h_k = softmax(\\frac{Q'_k K_k^T}{\\sqrt{d_k}}) V_k\\)\n\nThe outputs of all heads \\(h_1, h_2,..., h_h\\) are concatenated and then linearly transformed: \\(P_{att} = \\sigma((h_1 h_2...h_n)W^O)\\), where \\(W^O \\in \\mathbb{R}^{hd_hd_{att}}\\) denote the weight matrix for multi-head attention and \\(\\sigma(\\cdot)\\) denotes the activation function. We apply the vanilla ViT's feed-forward network"}, {"title": "Model Training", "content": "Infusing auxiliary information. To enhance the model's understanding of the game state, we inject auxiliary information as labels, including legal moves represented by multi-hot vectors and human move information: one-hot vectors of which piece is moved, which piece is captured (if any), the move's originating square, the move's destination square, and whether or not the move will deliver a check. For nuanced moves like castling, we ensure both the king's and rook's moves are accurately represented with 2-hot vectors. These segments are then concatenated into a comprehensive multi-hot vector to be used as labels for classification, serving a dual purpose: 1) It offers a more granular understanding of human moves by providing detailed context beyond just the move indices produced by the policy head labels, enriching the model's insight of player decisions; and 2) It ensures the model also learns about objective (i.e. chess-specific as opposed to behavioral) knowledge in chess, which is essential for developing a comprehensive understanding of both human moves and the fundamental mechanics of the game.\nData balancing. Chess games between players of significantly different skill levels are relatively rare but help us understand how players of lower skill levels approach games against far stronger opponents and vice versa. While previous work has ignored these games completely, they play a central role in our approach. Since games between players of similar skill levels vastly outnumber more uneven matchups, we use a data balancing strategy to effectively train our unified model for aligning players across all skill levels, in which games between players of different skill levels are over-sampled. To be precise, we pre-process the data in chunks of \\(N_{chunk}\\) games each. We then scan each data chunk to find games satisfying various (active player skill, opponent skill) combinations. Each skill combination can include at most \\(N_{range}\\) games. We continue scanning the data chunk until all skill combinations have \\(N_{range}\\) games or the data chunk is fully consumed. Note that the higher the balancing factor \\(N_{range}\\), the less likely it is that rare skill combinations will fully reach \\(N_{range}\\), which will lead to less balanced data overall. On the other hand, if the balancing factor is too small, fewer games will be selected from each data chunk, which is data-inefficient. We choose a fair compromise between data efficiency and balanced data so that our training data encompasses a broad spectrum of skill levels without biasing excessively towards the more frequently-occurring equal-skill matchups.\nData filtering. Online chess platforms feature a variety of game types, including blitz, rapid, and classical, each representing games played at different time controls (amount of time given to each player for the whole game). We use data from Lichess, a well-known large open-source chess platform, and its open database. In Lichess, since each game type is given a separate rating, ratings across different game types are not comparable (e.g. a rating of 1800 in \"Rapid\" is significantly weaker than a rating of 1800 in \u201cBlitz\u201d on Lichess). Previous work [1, 24] mixes player ratings across these game types together for training and evaluation. Instead of mixing data across game types, we focus on Rapid games only, which are medium-length games that lie between the fast-paced decisions of \"Blitz\" games and the slower, more strategic considerations of \u201cClassical\u201d games. \"Blitz\" and \"Bullet\" games, characterized by their quick pace, are composed of many decisions made under time pressure, introducing randomness that may not accurately reflect player intentions and skills. In contrast, Classical games are played less frequently, leading to data scarcity. Rapid chess is an ideal compromise between quality of play and quantity of data. In addition, we follow the procedures in [1] to filter valid positions within each game (more details can be found in Appendix Section B).\nTraining objectives. With the fused skill level and position representation \\(P\\) as input, we construct the policy head on top to predict human moves, which is optimized using cross-entropy loss with one-hot labels representing the recorded human move. We also build the auxiliary information head to infuse additional knowledge into Maia-2 as introduced in Section 3.4. This head is trained using bit-wise binary cross-entropy loss with multi-hot labels. Finally, following previous work [1, 24] we"}, {"title": "Results", "content": "We empirically evaluate Maia-2 along two key dimensions: move prediction accuracy, how well it can predict human moves at varying skill levels, and move prediction coherence, how aligned its predictions are across skill levels."}, {"title": "Experimental Setup", "content": "Maia-2. We train Maia-2 on Lichess games played between Jan 2013 and Nov 2023 (inclusive), with the exception of Dec 2019, since that is the month used for testing in the original Maia paper (and we also test on this month for consistency) [1]. After game filtering and balancing, we end up with a training set of 169M games (9.1B positions) played between May 2018 and Nov 2023 (inclusive).\nTo control for differences in training sets when comparing Maia-2 against Maia-1, we also train Maia-2subset with identical model architecture and training configurations as Maia-2, except it only has access to the same training data that Maia-1 had (i.e. games played from Jan 2013 to Nov 2019 in the Lichess Database). Training dataset statistics are reported in Appendix Tables 7, 9, and 10.\nModel comparison. We compare Maia-2 with several baselines. The first is Stockfish [6], the strongest chess engine in the world at the time of writing. Since the most common method of using Stockfish to play in a \"human-like\" way is to limit its search depth, we test it at various search depths. The second is Leela, an open-source counterpart to AlphaZero [10], which we also test at various strengths, since chess commentators have anecdotally observed that its decisions are more reminiscent of human-like play at lower strengths. Finally, our main model comparison will be with Maia (which we will refer to as \"Maia-1\" to avoid confusion with our model), the state-of-the-art model for human-like chess play [1]. Maia-1 is actually a set of 9 separate models, each trained on a different set of players at different skill levels from 1100 to 1900. Maia-1100 models the weaker players, Maia-1500 the intermediate players, and Maia-1900 the higher-skill players.\nEvaluation Datasets. We evaluate using three main test sets. To enhance fair comparison with baseline models, we use the benchmarking Maia-1 Testset [1] for performance comparisons where both players have identical skill levels. We report the results on Maia-1 Testset by grouping players into three categories: Skilled (Rapid rating up to 1600, which slightly exceeds the initial rating of 1500), Advanced (Rapid rating between 1600 and 2000), and Master (Rapid rating over 2000, roughly comprising the top 10% of players[29]). In addition, we aim to evaluate move prediction across diverse skill combinations, which Maia-1 Testset excludes. Therefore, we construct the Cross-skill Testset, a new testing dataset from Lichess games played in Dec 2023, ensuring that positions under each skill level combination have sufficient data to be statistically reliable. Detailed statistics of this dataset are summarized in Appendix Table 8. Finally, we construct Grounded Testset by extracting 450,000 positions from Dec 2023 games in Lichess Database where Stockfish evaluations are available. Such recorded evaluations serve as grounded facts to measure move quality."}, {"title": "Move Prediction Accuracy", "content": "In Table 1, we show the top-1 move prediction accuracy of all models across all groups of players on the Maia-1 Testset.\nMaia-2. Maia-2 demonstrates strong and consistent performance across all skill levels, surpassing all baselines. Specifically, despite Maia-1 models being specifically trained to mimic chess moves by players at specific skill levels, Maia-2 emerges as a unified one-for-all model that is consistently effective across the entire spectrum of chess skills. The largest improvement is on Advanced players, where Maia-2 gains 1.5 percentage points over the nearest competitor (Maia 1500). Recall that human move prediction has a large amount of intrinsic noise: different players of equal skill facing the same"}, {"title": "Move prediction perplexity", "content": "While top-1 accuracy gains are important, they may overshadow larger improvements in prediction quality. To account for this, we also measure the perplexity of move predictions, which reflects the model's confidence in its predictions. A lower perplexity indicates the model is more confident and accurate in human move prediction, as it corresponds to a higher likelihood (lower log-likelihood) of the correct human move. As shown in Table 2, Maia-2 consistently yields substantially lower perplexity in all groups of skill levels compared to Maia-1. In particular, Maia-2 significantly outperforms Maia-1 in Advanced and Master moves with relatively large margins, demonstrating the effectiveness of our unified modeling approach across diverse skill levels."}, {"title": "Adaptive move predictions", "content": "We now evaluate Maia-2's ability to predict compare across diverse skill combinations using the Cross-skill Testset.\nAs shown in Figure 2, Maia-2 consistently outperforms both Maia 1100 and Maia 1900 in almost all combinations of active player and opponent player skill levels. In particular, although Maia 1100 and Maia 1900 demonstrate competent performance within their respective domains of expertise, their predictive accuracy decreases substantially outside of these targeted skill levels. This is because Maia-1 models are static and cannot respond to varied skill levels and adjust their predictions accordingly. In contrast, our proposed unified modeling approach with skill-aware attention enables Maia-2 to adapt its predictions to account for the skill levels of both the active player and the opponent player, so that varying skill level configurations correspondingly can result in better aligned human move predictions. More results on comparisons with other Maia-1 versions can be found in Figure 7 in the Appendix."}, {"title": "Quality", "content": "One of our key motivations for creating a unified model of human chess behavior is to guide the development of future algorithmic learning tools. As such, understanding the mistakes that people make is of fundamental interest. Can Maia-2 predict mistakes better than Maia-1? Figure 11 in the Appendix shows the move prediction accuracy on the Grounded Testset as a function of move quality, measured by win-rate loss, which is calculated following the same procedures as prior studies [1, 24]. All models generally decrease in their ability to predict worse moves, since humans are generally trying to avoid mistakes, and high-quality moves are more certain whereas lower-quality moves can be more random and thus hard to predict. Nevertheless, Maia-2 outperforms all versions of Maia across most of the move quality range, demonstrating the effectiveness of our unified modeling approach for human move prediction. Maia-2's overall gains are not constrained to any specific move quality type, but are spread across the entire range."}, {"title": "Move Prediction Coherence", "content": "Maia-2's accuracy across the spectrum of human skill is certainly desirable, but perhaps an even more important dimension is prediction coherence as skill varies. A central drawback of Maia-1 is that it models players at different skill levels independently from each other, which results in particularly volatile predictions: the same position might elicit very different predicted behavior from models of"}, {"title": "Prediction smoothness", "content": "We measure the coherence of Maia-2's predictions by testing for smoothness features in its entire set of predictions. Call a model's treatment of a position monotonic if the predicted probability of the correct move increases with skill monotonically. In the Grounded Testset of 100K positions, we find that Maia-1 only treats 1% of them monotonically. In stark contrast, however, Maia-2 treats 27% of them monotonically, clearly demonstrating that Maia-2 is much more coherent. Similarly, call a model's treatment of a position transitional if it predicts a suboptimal move for some prefix of skills and then transitions to an optimal move for all subsequent skill levels. Again, Maia-2 treats substantially more positions transitionally-around 22% of them compared with 17% for Maia-1."}, {"title": "Move prediction agreement", "content": "As a first test, we measure move prediction coherence as we vary active player skill and opponent player skill in Maia-2. The results shown in Figure 4 reveal several trends. First, increasingly varying either the active or opponent rating results in lower agreement, suggesting that Maia-2 smoothly varies its predictions with skill. Second, comparing the two heatmaps reveals that Maia-2 has clearly learned that varying one's own skill has much larger effects than varying the opponent's-changing one's own skill against a fixed opponent can change the decision up to 22% of the time, but changing the opponent's skill while fixing our own skill will only change the decision up to 6% of the time. This is intuitive, as players must change their decisions in order to play at a higher level, while in theory one's opponent shouldn't affect one's decision. Of course, humans are not optimal agents and sometimes take their opponent's skill level into account when deciding on a move-willfully or not which is reflected in our results."}, {"title": "Chess concept understanding", "content": "Human chess players of varying strengths differ in their ability to recognize important features and patterns on the board. Stronger players are adept at discerning"}, {"title": "Discussion", "content": "Maia-2 is a unified model architecture that can accurately and coherently capture human decision-making in chess across a broad spectrum of skill levels. As such, Maia-2 can be regarded as a foundation model for human-AI alignment in chess, serving as a base model for general-purpose explorations of human-like AI. For example, fine-tuning Maia-2 with personal historical data of increasing active player skill levels and diverse opponent player skill levels can yield personalized move predictions, facilitating algorithmic education tools and more relatable AI partners. Also, the prior knowledge and general understanding acquired during pre-training can help Maia-2 to be data efficient when extending to grandmaster level moves, which are relatively rare and thus cannot be learned with the previous skill-independent modeling approaches.\nThe development of human-like models raises ethical concerns discussed in previous work [24, 30]. We believe Maia-2 poses limited risk while offering large potential benefits. Our data is highly aggregated, with almost 1 billion games being used for training, and chess as a domain is generally low-risk. Meanwhile, helping people improve in chess could lead to increased cognitive skills, confidence boosts, and help with general life satisfaction. Our vision is for Maia-2 to power AI partners and training aids; it cannot currently replace skilled human tutors and coaches.\nOur work has limitations and natural extensions. First, we are excited by the applications that Maia-2 will enable, such as more relatable AI partners and AI-powered learning aids, the development of which are out of scope for the current work. Maia-2 does not yet incorporate search, although previous work has demonstrated that with proper regularization it can help improve move prediction performance [25]. Relatedly, we group the strongest players in a single bucket, although modeling the very best players in the world remains difficult due to the complexity and depth of their moves."}, {"title": "More Experimental Results", "content": "Case study: Smoothness. We evaluate the smoothness of Maia-1 and Maia-2 by a case study in puzzle solving: a Mate-in-1 puzzle of 1500 skill level is presented to both Maia-1 and Maia-2, where smoothness can be evaluated by checking whether the predictions are monotonic and transitional as skill level increases. Call a model's treatment of a position as monotonic if the predicted probability of the correct move increases with skill monotonically, we can observe from Figure 6 that the Maia-2 predicted probabilities of the best move (in green arrows) increase monotonically from 0.22 to 0.45 as the skill levels rise from 1100 to 1900, while Maia-1 predictions are rather turbulent. Similarly, we call a model's treatment of a position transitional if it predicts a suboptimal move for some prefix of skills and then transitions to an optimal move for all subsequent skill levels. As shown in Figure 6, Maia-2 can mimic weaker players to whom the puzzle is hard to solve, while stronger Maia-2 with skill level configured above or equal to 1500 can successfully solve the puzzle. However, Maia 1100 surprisingly solved the puzzle, while the stronger Maia-1 models, e.g., Maia 1700 failed to make the optimal move. Therefore, in the considered case, as opposed to Maia-1, Maia-2 yields smooth predictions provided that its treatment of this position is monotonic and transitional."}, {"title": "Hyperparameter Settings.", "content": "To maintain a consistent perspective from both sides of players, we implemented board flipping to train and test Maia-2; that is, positions with black to move were mirrored such that all analyses could be conducted from the white side's viewpoint. We further refined our dataset through game and position filtering, selecting only rapid games from Lichess with available clock information and disregarding the initial 10 plys of each game as well as positions where either player had less than thirty seconds remaining. The filtration is significant to eliminate the noise introduced by rushed decisions under time constraints, which could skew the true representation of a player's skill. The choice of exclusive rapid games is also informed by the distinct rating systems"}, {"title": "Chess position representation", "content": "We follow the well-established prior works [10", "1": "to represent chess positions as multi-channel 8 \u00d7 8 matrices", "including": "n\u2022 Piece Representation: The first 12 channels categorize the board's pieces by type and color", "Turn": "A single channel ("}]}