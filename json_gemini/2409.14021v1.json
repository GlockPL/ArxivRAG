{"title": "BrainDreamer: Reasoning-Coherent and Controllable Image Generation from EEG Brain Signals via Language Guidance", "authors": ["Ling Wang", "Chen Wu", "Lin Wang"], "abstract": "Can we directly visualize what we imagine in our brain together with what we describe? The inherent nature of human perception reveals that, when we think, our body can combine language description and build a vivid picture in our brain. Intuitively, generative models should also hold such versatility. In this paper, we introduce BrainDreamer, a novel end-to-end language-guided generative framework that can mimic human reasoning and generate high-quality images from electroencephalogram (EEG) brain signals. Our method is superior in its capacity to eliminate the noise introduced by non-invasive EEG data acquisition and meanwhile achieve a more precise mapping between the EEG and image modality, thus leading to significantly better-generated images. Specifically, BrainDreamer consists of two key learning stages: 1) modality alignment and 2) image generation. In the alignment stage, we propose a novel mask-based triple contrastive learning strategy to effectively align EEG, text, and image embeddings to learn a unified representation. In the generation stage, we inject the EEG embeddings into the pre-trained Stable Diffusion model by designing a learnable EEG adapter to generate high-quality reasoning-coherent images. Moreover, BrainDreamer can accept textual descriptions (e.g., color, position, etc.) to achieve controllable image generation. Extensive experiments show that our method significantly outperforms prior arts in terms of generating quality and quantitative performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Human visual perception has long remained enigmatic. This has attracted neuroscientists and AI researchers to investigate the mech-anisms of human vision. With the advancement of deep learning, generative models have reached a point where they can directly generate similar observed images from brain signals [2, 10, 27, 26, 35, 17, 3, 12, 34]. However, directly generate images from EEG signals may not align with human cognitive processes or human-computer interaction paradigms, as such direct generation could result in significant inaccuracies or omissions in detail. When we imagine a scene, it often undergoes iterative refinement from coarse to fine details and can be supplemented or corrected based on different descriptions. Therefore, in addition to possessing the ability to generate observed images from brain signals directly, the generative models should also be capable of accepting textual descriptions to assist in image generation. This capability has profound implications for virtual reality (VR) content creation, where content creation tools that incorporate brain signals and textual inputs could offer a more intuitive and human-centered approach. For instance, a user imagining a rough scene in a VR environment could iteratively refine it through mental imagery and further enhance it with verbal descriptions. By incorporating textual input to refine fine-grained details or correct errors in the generated imagery, these models could become more useful for VR content creation. Additionally, such advancements open new possibilities in fields like personalized entertainment and art creation. In this paper, we investigate how to generate observed images from EEG brain signals in a manner that aligns more closely with human visual perception. EEG is a non-invasive technique that stands as the most commonly used method for capturing the electrophysiological dynamics of the brain [25, 31]. EEG data typically refers to time-series electro-physiological signals that are recorded using electrodes positioned on the human scalp [29, 4]. These recordings are often conducted while subjects are presented with stimuli, e.g., watching some images within a specific time frame. Exploring the connection be-tween EEG signals and brain activity is a highly meaningful en-deavor [2, 26]. It enables the recording of our momentary thoughts, and more significantly, it holds the potential to assist in the treat-ment of individuals with conditions such as cognitive impairments.\nRecently, there have been some explorations for generating images from the EEG data [2, 10, 27, 26, 35]. While they demonstrate promising outcomes, they encounter various limitations. Firstly, EEG data are captured non-invasively and thus are inherently noisy. Some methods [10, 27, 26, 35] disregard the noise, leading to lower quality in the generated images. Secondly, several methods [2, 26] attempt to utilize single-label information to guide the network in learning deep features from EEG data to achieve alignment with EEG-Image pairs. However, in most cases, single-label information is insufficient to depict complex image scenarios, resulting in inaccurate instances in the generated images. In a nutshell, these methods either ignore the noise introduced by non-invasive EEG acquisition or fail to achieve a precise mapping between EEG and image modality, resulting in poor-quality generated images. Moreover, previous works [2, 10, 27, 26, 35] solely focus on generating observed images from EEG data, disregarding the supplementary information such as textual descriptions, which does not conform to human visual perception.\nTo address the aforementioned challenges, we propose a novel framework, called BrainDreamer, as depicted in Fig. 1. It can mimic human reasoning and generate high-quality images from EEG brain signals. It can also accept supplementary information such as textual descriptions, which better conform to human visual perception. It enhances user engagement by aligning VR content more closely with an individual's mental imagery and preferences, fostering a more intuitive and responsive interaction. This methodology opens new possibilities for creating VR content that is not only visually compelling but also more attuned to the nu-ances of human perception, making VR experiences more natu-ral and human-centered. Specifically, our method consists of two key learning stages: modality alignment and image generation. In the modality alignment stage, we leverage Contrastive Language-Image Pre-training (CLIP) [22] to assist in aligning EEG, text, and image embeddings. CLIP learns a multi-modal embedding space shared by the text and image feature and contains a wide range of visual concepts. We design a novel mask-based triple contrastive learning strategy to map EEG embeddings into the CLIP embeddings space (Sec. 3.3). Prior approaches [2, 10, 27, 26, 35] pre-dominantly emphasized the alignment of EEG data with images, neglecting the significant semantic information that text can offer. Introducing text information to supervise EEG embeddings can make them more flexible and controllable. Also, we employ the masked modeling [7, 15] on the image and EEG data. During the training, random masks are applied to both the image and EEG data to discard certain information. Such an approach not only enhances the robustness of features (e.g., reducing noise interference and alleviating inter-individual differences in EEG data) but also reduces the training cost [15].\nIn the image generation stage, we design an EEG adapter to inject EEG embeddings into the pre-trained Stable Diffusion [23](Sec. 3.4). Specifically, the EEG adapter consists of a frozen EEG encoder and a feature projection module that is dedicated to reducing the domain gap between EEG embeddings and CLIP embeddings. Subsequently, the output of the feature projec-tion module is considered as scale and shift factors. Then, the EEG adapter employs a Feature-wise Linear Modulation (FiLM) [20] to inject EEG embeddings into the pre-trained Stable Diffusion to generate images based on the scale and shift factors. This offers a lower computational overhead compared to commonly used cross-attention methods. Particularly, BrainDreamer is capable of incorporating additional textual descriptions, such as color and po-sition information, to assist in the generation of images. This way, our BrainDreamer can generate high-quality, controllable, and reasoning-coherent images from EEG signals, as demonstrated in Figs. 1, and 8. For example, when we input text [\"the main object is red\"] or [\"on the beach, seaside\"], our method can make the main subject color of the generated image red or the background of the generated image is a beach, located by the seaside. Our approach allows for a more interactive, adaptive and fluid content cre-ation process, where users can iteratively refine their contextually rich visual outputs based on feedback and preferences.\nIn summary, our main contributions are three-fold: (I) We propose BrainDreamer that can mimic human reasoning and gener-ate high-quality images from EEG brain signals. Moreover, our"}, {"title": "2 RELATED WORK", "content": "Image Generation from EEG Signals. The rapid development of learning-based methods has made it possible to extract meaningful representations from brain signals, such as EEG. Brain2Image [10] is the first to generate observed images of ImageNet from EEG fea-tures. EEGStyleGAN-ADA [26] improves image synthesis from EEG signals by leveraging learned EEG representations in con-trastive settings. As diffusion models have shown remarkable achievements in image generation, DM-RE2I [35] attempts to in-corporate EEG features into the diffusion model by adding them to the time step. The current state-of-the-art method, DreamDiffu-sion [2], utilizes the pre-trained Stable Diffusion model to generate high-quality images from EEG signals. It employs large-scale ad-ditional datasets (approximately 120K samples) for self-supervised pre-training and aligns EEG embeddings with CLIP image embed-dings to enhance the EEG encoder's representation capability of EEG features. However, it requires fine-tuning all parameters of the diffusion models, which brings about significant training costs. Moreover, it merely focuses on aligning EEG data with images, overlooking the significant semantic information that text can offer. Furthermore, these methods can only directly generate images from EEG brain signals, without the ability to make adjustments to the generated images based on supplementary information (e.g., textual descriptions). This does not align with human visual perception.\nPre-training Vision-Language Models (VLMs). The recent ad-vancements in vision and language models have significantly ad-vanced the integration and comprehension of both image and lan-guage modalities. CLIP [22] achieves the alignment of image and text features by leveraging contrastive learning on large-scale image-text datasets. It maps images and text into a unified embed-ding space and demonstrates excellent zero-shot capabilities across various visual and language tasks. BLIP [14] refines pre-trained methods by enhancing the quality of image and text data by the image captioner. Building upon this, BLIP2 [13] leverages pre-trained VLMs to enhance the model's multimodal understanding capabil-ity while significantly reducing training costs. InstructBLIP [16] allows us to perform various tailored vision tasks through instruc-tion tuning. Furthermore, state-of-the-art VLMs such as GPT-4 [1] and Gemini [30], they possess powerful representational capabili-ties and can handle a wide range of multimodal tasks. In this work, We leverage CLIP and propose a mask-based triple contrastive loss (see Eq. 4) to assist us in aligning EEG, text, and image embed-dings. In contrast to previous works, we introduce text supervision to assist in achieving better embedding alignment.\nDiffusion Models. In recent years, we have witnessed the in-credible results of diffusion probabilistic models [8] in control-lable image generation, particularly in text-to-image generation. GLIDE [18] adopts a cascaded text-guided diffusion architecture to support both image generation and editing. Imagen [24] encodes text to embeddings using language models as the condition into dif-fusion model, achieving high fidelity of the generated image. Stable Diffusion [23] moves the execution of the diffusion process to the latent space instead of the original pixel space, which significantly reduces the computation cost. In addition, It extends the types of control conditions beyond just text, allowing for the inclusion of depth maps, semantic segmentation maps, and more. Subsequently, a series of fine-tuning works based on Stable Diffusion emerged. ControlNet [36] controls diffusion model with task-specific condi-tions by fine-tuning a \"trainable copy\" of any off-the-shelf diffusion model. UniControl [21] incorporates task instruction, which en-ables a single model to handle multiple condition-to-image tasks. To achieve efficient fine-tuning, IP-Adapter [33] employs a decou-pled cross-attention strategy to reduce the trainable parameters to 22M. We design an EEG adapter to inject EEG embeddings into the pre-trained Stable Diffusion model, enabling image generation with the integration of text descriptions."}, {"title": "3 THE PROPOSED METHOD", "content": "3.1 Prelimiaries and Background\nDiffusion models are one type of generative model, consisting of a forward (a.k.a, the forward process) and a backward process. In the forward process, Gaussian noise with variance \\( \\beta_t \\in (0,1) \\) at time t is added to the input \\( x_0 \\) for producing the noisy input. At each step \\( t\\in \\{0,..., T\\} \\), the intermediate sample \\( x_t \\) is computed as:\n\\[ x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t, \\]\nwhere \\( \\epsilon_t \\sim N(0,I) \\) is the Gaussian noise at step t, \\( \\alpha_t = 1 - \\beta_t \\) and \\( \\bar{\\alpha}_t = \\prod_{i=1}^{t} \\alpha_i \\). When t is large enough, the input \\( x_t \\) is nearly a standard Gaussian distribution.\nA network \\( \\mathcal{E}_\\theta \\) is learned by predicting the noise \\( \\epsilon_t \\) conditioned on c (e.g., text prompts) at a randomly picked time-step t. The optimization of the diffusion model is defined as follows:\n\\[ L_{dm} = \\mathbb{E}_{x_0, c, t, \\epsilon} [|| \\epsilon_t - \\epsilon_\\theta(x_t=\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t, c, t)||_2 ], \\]\nwhere \\( || \\cdot ||_2 \\) is the L2 loss.\nWe implement our method based on the pretrained Stable Diffusion [23], which is a latent text-to-image diffusion model. This model relies on an autoencoder that converts an image x into a latent z with encoder \\( \\mathcal{E} \\) to achieve better efficiency and stabilized training. Then generates it with decoder \\( \\mathcal{D} \\) after completing the forward pro-cess. In the sampling stage, Stable Diffusion also randomly drops out c to reduce reliance on the conditions. In other words, the pre-dicted noise is calculated based on the prediction of both the condi-tional model \\( \\epsilon_\\theta(z_t, c, t) \\) and unconditional model \\( \\epsilon_\\theta(z_t, t) \\):\n\\[ \\epsilon(z_t, c, t) = w \\epsilon_\\theta (z_t, c, t) + (1-w) \\epsilon_\\theta(z_t, t), \\]\nwhere \\( z_t = \\tau(x_t) \\). w often named guidance scale or guidance weight, is a scalar value that adjusts the alignment with condition c.\n3.2 Overview\nAs shown in the Figs. 3, our BrainDreamer adopts a two-stage pipeline, which is effective and robust. First, we leverage the pre-trained CLIP image encoder and text encoder to assist us in training the EEG encoder. We design a mask-based triple contrastive learn-ing strategy (see Sec. 3.3) to map EEG embeddings into the CLIP embedding space. Subsequently, we construct an EEG adapter(see Sec. 3.4), consisting of an EEG encoder and a feature projection module, where the parameter weights of the EEG encoder remain fixed. The EEG Adapter injects EEG embeddings into the pre-trained Stable Diffusion model in a FiLM manner, which offers a lower computational overhead compared to commonly-used cross-attention method. It is worth noting that textual descriptions are not mandatory during the generation. However, we encourage inputting a small amount of abstract textual descriptions to assist the model in better generating the corresponding images. This is because the semantic information conveyed by brain signals often represents an object alone, lacking background, color, or spatial information [2]. Finally, both text embeddings extracted by CLIP text encoder and EEG embeddings are fed into the pre-trained Stable Diffusion to accomplish controlled high-quality image generation.\n3.3 Mask-based Triple Contrastive Learning\nOur method is primarily based on a pre-trained Stable Diffusion model, which primarily handles latent features in the CLIP embed-ding space. Therefore, we aim to map EEG embeddings to the CLIP embedding space, enabling pre-trained Stable Diffusion to generate higher-quality images based on EEG embeddings. Previ-ous works [2, 26] have attempted to align the EEG embeddings with the CLIP image embeddings, and then input the EEG embeddings to the generative model for image generation. We believe that such image-EEG alignment is incomplete due to the following reasons: 1) Most generative models are text-to-image models (e.g., Imagen and Stable Diffusion), which are more sensitive to text embeddings. Despite CLIP's impressive image-text representation capabilities, there still exists a certain degree of domain gap between image and text modalities. 2) Textual information offers greater flexibility and controllability, which is why text embeddings are often considered as \"ground truth\" in most multi-modal works [37, 32]. Therefore, we also incorporate textual information to map EEG embeddings to the CLIP embedding space in addition to image information. We design a mask-based triple contrastive learning strategy for embed-ding alignment. Given the EEG encoder \\( F_E \\), the frozen CLIP im-age encoder \\( F_I \\) and the frozen CLIP text encoder \\( F_T \\), along with a sampled batch of triplets \\( (E_i, I_i, T_i) \\) for the EEG signals, its cor-responding observed image, and the associated text, the contrastive loss for alignment is formulated as:\n\\[\\begin{split} L = & \\frac{1}{B} \\sum_{i=1}^{B} \\left( \\log \\frac{\\exp(\\frac{f^E_i \\cdot f^I_i}{\\tau})}{\\sum_{j=1}^{B}\\exp(\\frac{f^E_i \\cdot f^I_j}{\\tau})} + \\log \\frac{\\exp(\\frac{f^E_i \\cdot f^I_i}{\\tau})}{\\sum_{j=1}^{B}\\exp(\\frac{f^E_j \\cdot f^I_i}{\\tau})} \\right) \\\\ & \\quad \\text{LEI:EEG-Image contrastive} \\\\ & + \\frac{1}{B} \\sum_{i=1}^{B} \\left( \\log \\frac{\\exp(\\frac{f^E_i \\cdot f^T_i}{\\tau})}{\\sum_{j=1}^{B}\\exp(\\frac{f^E_i \\cdot f^T_j}{\\tau})} + \\log \\frac{\\exp(\\frac{f^E_i \\cdot f^T_i}{\\tau})}{\\sum_{j=1}^{B}\\exp(\\frac{f^E_j \\cdot f^T_i}{\\tau})} \\right) \\\\ & \\quad \\text{LET:EEG-Text contrastive} \\\\ & + \\frac{1}{B} \\sum_{i=1}^{B} \\left( \\log \\frac{\\exp(\\frac{f^I_i \\cdot f^T_i}{\\tau})}{\\sum_{j=1}^{B}\\exp(\\frac{f^I_i \\cdot f^T_j}{\\tau})} + \\log \\frac{\\exp(\\frac{f^I_i \\cdot f^T_i}{\\tau})}{\\sum_{j=1}^{B}\\exp(\\frac{f^I_j \\cdot f^T_i}{\\tau})} \\right) \\\\ & \\quad \\text{LIT:Image-Text contrastive} \\end{split}\\]\nwhere B is the number of shapes in a batch; t is a learn-able temperature; \\( f^E_i = Norm(F_E(E_i)) \\), \\( f^I_i = Norm(F_I(I_i)) \\), \\( f^T_i = Norm(F_T(T_i)) \\) and Norm is normalization. In addition, we employ masked modeling on the image and EEG data. This not only en-hances the robustness of the features but also reduces training costs. The objective of training the EEG encoder is to minimize L.\n3.4 EEG Adapter\nEEG adapter is designed to enable the pre-trained Stable Diffusion model to generate images with EEG signals. As shown in Fig. 3, the EEG adapter consists of a frozen EEG encoder and a feature projec-tion module. Previous methods [2] simply feed EEG embeddings into the frozen cross-attention layers of Stable Diffusion to gen-erate corresponding images, overlooking the domain gap between EEG embeddings and text embeddings, despite prior efforts to align the embeddings. To address this issue, we introduce a feature projection module to reduce the domain gap between the embed-dings. Furthermore, we employ the FiLM mechanism to inject em-beddings into models instead of common cross-attention methods. Previous works, e.g., [19] have demonstrated that FiLM is more suitable for incorporating category information (e.g., EEG signals), while the cross-attention method is more applicable to sequential information (e.g., text description). In addition, compared to the additional 15% Gflops overhead introduced by cross-attention, the computational cost associated with FiLM is almost negligible [19]. Given the query features Z and the EEG embeddings \\( c_e \\), the output of EEG adapter is computed as follows:\n\\[\\begin{aligned} &\\alpha, \\beta = FP(c_e), \\\\ &Z^{\\prime \\prime} = Z \\odot \\alpha + \\beta, \\end{aligned}\\]\nwhere \\( \\odot \\) denotes element-wise multiplication and the FP() is the feature projection module which consists of two linear layers, a nor-malization layer, and an activation function (see Fig. 3).\n3.5 Image Generation based on EEG and Texts\nAlthough textual descriptions are not mandatory, BrainDreamer en-courages the input of textual descriptions to assist in generating de-sired images. Given the query features Z and the text embeddings \\( c_t \\), the output of cross-attention Z' can be defined as follows:\n\\[ Z^{\\prime} = \\text{Attention}(Q, K, V) = \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}})V, \\]\nwhere Q = \\( ZW_q \\), K = \\( c_t W_k \\), V = \\( c_t W_v \\) are the query, key, and values matrices of the attention operation respectively, and \\( W_q \\), \\( W_k \\), \\( W_v \\) are the weight matrices of the linear projection layers. Then we simply add the output of EEG adapter to the output of text cross-attention. This process is formulated as follows:\n\\[Z^{new} = \\text{Softmax} (\\frac{QK^T}{\\sqrt{d}})V+ \\lambda (Z \\odot \\alpha + \\beta). \\]\nwhere \\( \\lambda \\) is weight factor, and the model becomes the original text-to-image diffusion model if \\( \\lambda \\rightarrow 0 \\). Hence, the training objective Eq. 2 is updated to:\n\\[L = \\mathbb{E}_{z, c, c_t, c_e, t} [|| \\epsilon_t - \\epsilon_\\theta (Z_t, C_t, C_e,t)||_2]. \\]\nAnd the Eq. 3 is updated to:\n\\[\\epsilon(Z_t, C_t, C_e,t) = w \\epsilon_\\theta(Z_t, C_t, C_e,t) + (1 -w)\\epsilon(z_t,t). \\]"}, {"title": "4 EXPERIMENTS", "content": "4.1 Implementation Details and Benchmark Datasets\nImplementation Details. We align with the optimal settings estab-lished by [2], and adjust our parameters: EEG time series length as 512, embedding dimension as 1280, channels as 128.\nIn the first stage of our fine-tuning of the EEG encoder, only the parameters of the EEG encoder are updated, and the text encoder and image encoder are frozen at this step. The joint fine-tuning of the EEG encoder made it compatible with the vanilla CLIP model. The mask ratio for image and EEG data is set to 0.5. For CLIP, we use the ViT-L/14 model to extract feature embeddings. The archi-tecture of EEG encoder is the same as ViT-Large [5]. Regarding our generative model architecture, we utilize version 1.5 of the Stable Diffusion model. There are 16 cross-attention layers in this model, and we add a new EEG Adapter for each of these layers.\nIn the second stage of training the EEG Adapter, we use the AdamW optimizer with a fixed learning rate of 0.0001 and weight decay of 0.01. To enable classifier-free guidance, we drop EEG em-beddings and text embeddings individually by using a probability of 0.05 and drop EEG embeddings and text embeddings simulta-neously by using a probability of 0.05. In the inference stage, we adopt DDIM [28] sampler with 50 steps and set the guidance scale w to 7.5. When only using the EEG prompt, we set the text prompt to empty and \\( \\lambda = 1.0 \\).\nBenchmark Datasets. In our research, we utilize the EEG-image dataset [10] to verify the effectiveness of our method. EEG-image contains EEG signals paired with 2,000 images from Im-ageNet, which includes 11,466 EEG sequences captured through 128-channel electrodes. These EEG recordings corresponded with 40 unique image classes, with each class represented by 50 distinct images sourced from the ImageNet dataset. The EEG recordings were obtained from 6 subjects, and our training and testing were conducted on the same subject, specifically, Subject 4.\n4.2 Experimental results\nExperimental settings. Our experiment is conducted in two set-tings. 1) EEG-to-Image Generation. It indicates generating im-ages from only EGG input. 2) (EEG+Text)-to-Image Generation. In this setting, we also provide textual descriptions related to color and background to guide EEG-to-image generation. EEG Classifi-cation Accuracy (ECA) and CLIP Similarity (CS) are employed for"}, {"title": "4.4 Disccusion", "content": "4.4.1 Analysis about the Necessity of Textual Guidance In-teraction\nEEG signals provide coarse-grained information about the main ob-ject, while text offers background information that EEG cannot pro-vide; combining EEG with text for image generation is reasoning-coherent because they are complementary. The textual descriptions are only about the content of the background or the object's fine-grained features like color, without providing information about the main object, which is only provided by EEG signals. As shown in the first two columns of Fig. 9, images directly generated from EEG, although they reflect the correct primary object information that it is a boat, dog, and cat, respectively, the colors and the back-ground information are incorrect. If one wishes to generate a com-plete and correct image through EEG signals, the complete user interaction logic is that the user completes the imagination, the hel-met collects the EEG signals, and then the user describes the finer-grained features, and finally, the computer completes the generation of an image that is correct in the main body and rich in details as imagined by user.\nThis process of combining EEG and text for image generation is analogous to the process of creating a suspect sketch, where the sketch artist begins with a coarse-grained description of the suspect based on the eyewitness's overall impression, such as gender, race, and general body shape. This is similar to how EEG signals provide a rough representation of the main object in the user's imagination. As the sketch progresses, the artist collects more fine-grained de-tails, like specific facial features, hairstyles, and clothing, to refine the image. Similarly, in the EEG-text fusion approach, the EEG provides the foundational image information about the main object, while text descriptions add the finer details and background infor-mation that the EEG cannot capture. Both processes rely on the complementary strengths of two sources of information to create a more accurate and complete representation. Just as the suspect sketch becomes more precise through iterative detailing based on eyewitness feedback, the EEG-text image generation process also follows a logic that mirrors human cognition: the user imagines the main object, the EEG captures the core features, and text inputs fill in the missing finer details. This analogy illustrates the reasonable-ness of the EEG-text approach, highlighting how the combination of these two modalities creates a coherent, detailed, and contextu-ally rich image that faithfully represents the user's mental picture.\n4.4.2 Explanation about Reasoning-coherent Interaction of EEG-Image Generation\nEEG signals provide coarse-grained information about the main ob-ject, e.g., 'Cat', while text offers details and background informa-tion, e.g., 'Black, on the bed', that EEG cannot provide. Therefore, the generated image could be a black cat on the bed. Such an im-age generation from EEG and language is regarded as reasoning-coherent because it mimics human imagination.\nThis method represents an innovative and novel form of human-computer interaction, where the brain's neural activity, captured through EEG signals, directly interfaces with computational mod-els to create visual content. By integrating EEG signals with textual descriptions, this approach leverages the strengths of both modali-ties: the intuitive, subconscious processing of the brain for identi-fying main objects, and the deliberate, conscious articulation pro-vided by language for specifying details and context. This dual-modality interaction not only allows for a more natural and intu-itive user experience but also opens up new possibilities for users to express their imagination and creativity. Unlike traditional im-age generation methods that rely solely on explicit text input or pre-defined templates, this EEG-text fusion approach dynamically translates human thought patterns into rich visual representations, fostering a deeper connection between human cognitive processes and machine-generated outputs. By aligning closely with how hu-mans naturally envision and describe their surroundings, this tech-nique enhances user engagement and could pave the way for more immersive applications in areas, such as virtual reality, personalized content creation, and cognitive neuroscience research.\n4.5 Evaluations on Real-world EEG Data\nTo evaluate the effectiveness and user experience of BrainDreamer, we conducted a user study with several participants. The EEG sig-nal collection and study procedures were approved by the ethical committee. After collecting EEG signals, we fine-tuned the EEG encoder using real-world data. The study's objective was to as-sess the feasibility of generating images from EEG signals and the effectiveness of our proposed method. Specifically, it aimed to as-sess how well users could interpret these generated images with and without accompanying textual guidance and to measure the per-ceived quality and coherence of the images produced.\nParticipants and Devices. Due to the difficulty of collecting EEG signals, we recruited 2 university students (1 male and 1 female) as participants. We collect EEG signals when participants look at the image stimuli. To collect brain responses, participants were wear-ing electrodes and caps (actiCAP\u00b9), which were connected to am-plifiers (actiCHamp Plus\u00b2), and the amplified signals were finally"}, {"title": "5 CONCLUSION", "content": "In this paper, we proposed a novel end-to-end language-guided gen-erative framework BrainDreamer that can mimic human reasoning and generate high-quality images from EEG brain signals. We de-signed a novel mask-based triple contrastive learning strategy to effectively align EEG, text, and image embeddings. Moreover, we have designed an EEG adapter to inject EEG embeddings into a pre-trained Stable Diffusion model, enabling us to achieve reasoning-coherent image generation. In the generation stage, we can provide abstract textual descriptions (e.g., background information) to assist in the image generation, ensuring that the results align more closely with our expectations. Our BrainDreamer is more aligned with hu-man visual perception, and the generation quality and quantitative performance are better than existing methods. We also conducted a real-world user study to confirm the effectiveness of BrainDreamer and show its practical potential for applications like personalized content creation in virtual reality. Its ability to generate reasoning-coherent and controllable images from brain signals takes a step forward in human-computer interaction, bridging the gap between human imagination and machine-generated visual content. Future work will focus on enhancing the model's robustness and gener-alization to variations in EEG signals, while also expanding the framework's potential applications.\nLimitations. In Fig. 12, we show some unsatisfactory results of our BrainDreamer. Although these results exhibit a high level of coarse-grained matching with the ground truth and indeed interact with our textual descriptions, they are still imperfect. In cases (a) and (b), our BrainDreamer fails to recognize the main subjects in the images, resulting in instance mismatch. Particularly in (b), the wall is incorrectly colored red instead of the chair. In cases (c) and (d), BrainDreamer exhibits insufficient coloring, particularly in (d), where only half of the butterfly is red. Such images are highly unrealistic. Combining EEG signals with textual descriptions or other supplementary information to achieve instance-level image generation will be the focus of our future work."}]}