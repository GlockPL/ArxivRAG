{"title": "MTMamba: Enhancing Multi-Task Dense Scene Understanding by Mamba-Based Decoders", "authors": ["Baijiong Lin", "Weisen Jiang", "Pengguang Chen", "Yu Zhang", "Shu Liu", "Ying-Cong Chen"], "abstract": "Multi-task dense scene understanding, which learns a model for multiple dense prediction tasks, has a wide range of application scenarios. Modeling long-range dependency and enhancing cross-task interactions are crucial to multi-task dense prediction. In this paper, we propose MTMamba, a novel Mamba-based architecture for multi-task scene understanding. It contains two types of core blocks: self-task Mamba (STM) block and cross-task Mamba (CTM) block. STM handles long-range dependency by leveraging Mamba, while CTM explicitly models task interactions to facilitate information exchange across tasks. Experiments on NYUDv2 and PASCAL-Context datasets demonstrate the superior performance of MTMamba over Transformer-based and CNN-based methods. Notably, on the PASCAL-Context dataset, MTMamba achieves improvements of +2.08, +5.01, and +4.90 over the previous best method in the tasks of semantic segmentation, human parsing, and object boundary detection, respectively.", "sections": [{"title": "1 Introduction", "content": "Multi-task dense scene understanding is an essential problem in computer vision [36] and has a variety of practical applications, such as autonomous driving [20, 23], healthcare [19], and robotics [48]. It aims to train a model for simultaneously handling multiple dense prediction tasks, such as semantic segmentation, monocular depth estimation, surface normal estimation, and object boundary detection.\nThe prevalent multi-task architecture follows an encoder-decoder framework, consisting of a task-shared encoder for feature extraction and task-specific decoders for predictions [36]. This framework is very general and many variants have been proposed [37, 42, 43, 46] to improve its performance in multi-task scene understanding. One promising approach is the decoder-focused method [36] with the aim of enhancing cross-task interaction in task-specific decoders through well-designed fusion modules. For example, derived from the convolutional neural network (CNN), PAD-Net [42] and MTI-Net [37] incorporate a multi-modal distillation module to promote information fusion between different tasks in the decoder and achieve better performance than the encoder-decoder framework. Since the convolution operation mainly focuses on local features [2], recent methods [43, 46] propose Transformer-based decoders with attention-based fusion modules. These methods leverage the attention mechanism to capture global context information, resulting in better performance than CNN-based methods. Previous works demonstrate that enhancing cross-task correlation and modeling long-range spatial relationships are critical for multi-task dense prediction.\nVery recently, Mamba [13], a new architecture derived from state space models (SSMs) [14, 15], has shown better long-range dependencies modeling capacity and superior performance than Transformer models in various domains, including language modeling [12, 13, 39], graph reasoning [1, 38], medical images analysis [30, 41], and point cloud analysis [22, 49]. However, all of these works focus on single-task learning, while how to adopt Mamba for multi-task training is still under investigation. Moreover, achieving cross-task correlation in Mamba remains unexplored, which is critical for multi-task scene understanding.\nTo fill these gaps, in this paper, we propose MTMamba, a novel multi-task architecture featuring a Mamba-based decoder and superior performance in multi-task scene understanding. The overall framework is shown in Figure 1. MTMamba is a decoder-focused method with two types of core blocks: the self-task Mamba (STM) block and the cross-task Mamba (CTM) block, illustrated in Figure 2. Specifically, STM, inspired by Mamba, can effectively capture global context information. CTM is designed to enhance each task's features by facilitating knowledge exchange across different tasks. Therefore, through the collaboration of STM and CTM blocks in the decoder, MTMamba not only enhances cross-task interaction but also effectively handles long-range dependency.\nWe evaluate MTMamba on two standard multi-task dense prediction benchmark datasets, namely NYUDv2 [35] and PASCAL-Context [6]. Quantitative results demonstrate that MTMamba largely outperforms both CNN-based and Transformer-based methods. Notably, on the PASCAL-Context dataset, MTMamba outperforms the previous best by +2.08, +5.01, and +4.90 in semantic segmentation, human parsing, and object boundary detection tasks, respectively. Qualitative studies show that MTMamba generates better visual results with more accurate details than state-of-the-art Transformer-based methods.\nOur main contributions are summarized as follows:\nWe propose MTMamba, a novel multi-task architecture for multi-task scene understanding. It contains a novel Mamba-based decoder, which effectively models long-range spatial relationships and achieves cross-task correlation;\nWe design a novel CTM block to enhance cross-task interaction in multi-task dense prediction;"}, {"title": "2 Related Works", "content": "Multi-task learning (MTL) is a learning paradigm that aims to simultaneously learn multiple tasks in a single model [50]. Recent MTL research mainly focuses on multi-objective optimization [24\u201326, 34, 44, 45, 47] and network architecture design [37, 42, 43, 46]. In multi-task dense scene understanding, most existing works focus on designing architecture [36], especially designing specific modules in the decoder to achieve better cross-task interaction. For example, based on CNN, Xu et al. [42] introduce PAD-Net, incorporating an effective multi-modal distillation module to promote information fusion between different tasks in the decoder. MTI-Net [37] is a complex multi-scale and multi-task CNN architecture with an information distillation across various feature scales. As the convolution operation mainly captures local features [2], recent approaches [43, 46] utilize the attention mechanism to grasp global context and develop Transformer-based decoders for multi-task scene understanding. For instance, Ye & Xu [46] introduce InvPT, a Transformer-based multi-task architecture, employing an effective UP-Transformer block for multi-task feature interaction at different feature scales. MQTransformer [43] designs a cross-task query attention module to enable effective task association and information exchange in the decoder.\nPrevious works demonstrate long-range dependency modeling and enhancing cross-task correlation are critical for multi-task dense prediction. Unlike existing methods, we propose a novel multi-task architecture derived from Mamba to capture global information better and promote cross-task interaction."}, {"title": "2.2 State Space Models", "content": "State space models (SSMs) are a mathematical representation of dynamic systems, which models the input-output relationship through a hidden state. SSMs are general and have achieved great success in a wide variety of applications such as reinforcement learning [16], computational neuroscience [10], and linear dynamical systems [18]. Recently, SSMs are introduced as an alternative network architecture to model long-range dependency. Compared with CNN-based networks [17, 21], which are designed for capturing local dependence, SSMs are more powerful for long sequences; Compared with Transformer-based networks [8, 40], which require the quadratic complexity of the sequence length, SSMs are more computation- and memory-efficient.\nMany different structures have been proposed recently to improve the expressivity and efficiency of SSMs. Gu et al. [14] propose structured state space models (S4) to improve computational efficiency, where the state matrix is a sum of low-rank and normal matrices. Many follow-up works attempt to enhance the effectiveness of S4. For example, Fu et al. [11] design a new SSM layer H3 to fill the performance gap between SSMs and Transformers in language modeling. Mehta et al. [32] introduce a gated state space layer using gated units for improving expressivity.\nRecently, Gu & Dao [13] further propose Mamba with the core operation S6, an input-dependent selection mechanism of S4, which achieves linear scaling in sequence length and demonstrates superior performance over Transformers on various benchmarks. Mamba has been successfully applied in image classification [27, 53], image segmentation [41], and graph prediction [38]. Different from them, which use Mamba in the single-task setting, we consider a more challenging multi-task setting and propose novel self-task and cross-task Mamba modules to capture intra-task and inter-task dependence."}, {"title": "3 Methodology", "content": "In this section, we first introduce the background knowledge of state space models and Mamba in Section 3.1. Then, we introduce the overall architecture of the proposed MTMamba in Section 3.2. Subsequently, we delve into a detailed exploration of each part in MTMamba, including the encoder in Section 3.3, the Mamba-based decoder in Section 3.4, and the output head in Section 3.5."}, {"title": "3.1 Preliminaries", "content": "SSMs [13\u201315], originated from the linear systems theory [5, 18], map input sequence $x(t) \\in \\mathbb{R}$ to output sequence $y(t) \\in \\mathbb{R}$ though a hidden state $h\\in \\mathbb{R}^N$ by a linear ordinary differential equation:\n$$h'(t) = Ah(t) + Bx(t),$$\n$$y(t) = Ch(t) + Dx(t),$$\nwhere $A \\in \\mathbb{R}^{N \\times N}$ is the state matrix, $B \\in \\mathbb{R}^N$ is the input matrix, $C \\in \\mathbb{R}^N$ is the output matrix, and $D \\in \\mathbb{R}$ is the skip connection. Equation (1) defines the evolution of the hidden state $h(t)$, while Equation (2) determines the output is composed of a linear transformation of the hidden state $h(t)$ and a skip connection from $x(t)$. For the remainder of this paper, D is omitted for explanation (i.e., D = 0).\nSince the continuous-time system is not suitable for digital computers and real-world data, which are usually discrete, a discretization procedure is introduced to approximate it by a discrete-time one. Let $\\Delta\\in \\mathbb{R}$ be a discrete-time step. Equations (1) and (2) are discretized as\n$$h_t = \\overline{A}h_{t-1} + \\overline{B}x_t,$$\n$$Y_t = Ch_t,$$\nwhere $x_t = x(\\Delta t)$, and\n$$\\overline{A} = exp(\\Delta A), \\overline{B} = (\\Delta A)^{-1}(exp(\\Delta A) \u2013 I) \u00b7 \\Delta B \\approx \\Delta B, C = C.$$\nIn S4 [14], $(A, B, C, \\Delta)$ are trainable parameters learned by gradient descent and do not explicitly depend on the input sequence, resulting in weak contextual information extraction. To overcome this, Mamba [13] proposes S6, which introduces an input-dependent selection mechanism to allow the system to select relevant information based on the input sequence. This is achieved by making B, C, and $\\Delta$ as functions of the input $x_t$. More formally, given an input sequence $x \\in \\mathbb{R}^{B\\times L\\times C}$ where B is the batch size, L is the sequence length, and C is the feature dimension, the input-dependent parameters $(B, C, \\Delta)$ are computed as\n$$B = Linear(x) \\in \\mathbb{R}^{B\\times L\\times N},$$,\n$$C = Linear(x) \\in \\mathbb{R}^{B\\times L\\times N},$$,\n$$\\Delta = SoftPlus (A + Linear(x)) \\in \\mathbb{R}^{B\\times L\\times C},$$,\nwhere $\\Delta \\in \\mathbb{R}^{B\\times L\\times C}$ is a learnable parameter, SoftPlus(\u00b7) is the SoftPlus function, and Linear() is the linear layer. $A \\in \\mathbb{R}^{L\\times C}$ is a trainable parameter as in S4. After computing $(A, B, C, \\Delta)$, $(\\overline{A}, \\overline{B}, C)$ are discretized via Equation (5), then the output sequence $y \\in \\mathbb{R}^{B\\times L\\times C}$ is computed by Equations (3) and (4)."}, {"title": "3.2 Overall Architecture", "content": "An overview of MTMamba is illustrated in Figure 1. It contains three components: an off-the-shelf encoder, a Mamba-based decoder, and task-specific heads."}, {"title": "3.3 Encoder", "content": "We take the Swin Transformer [28] as an example. Consider an input RGB image $x \\in \\mathbb{R}^{3\\times H\\times W}$, where H and W are the height and width of the image, respectively. The encoder employs a patch-partition module to segment the input image into non-overlapping patches. Each patch is regarded as a token, and its feature representation is a concatenation of the raw RGB pixel values. In our experiment, we use a standard patch size of 4 \u00d7 4. Therefore, the feature dimension of each patch is 4 \u00d7 4 \u00d7 3 = 48. After patch splitting, a linear layer is applied to project the raw token into a C-dimensional feature embedding. The patch tokens, after being transformed, sequentially traverse multiple Swin Transformer blocks and patch merging layers, which collaboratively produce hierarchical feature representations. Specifically, the patch merging layer [28] is used to 2\u00d7 downsample the spatial dimensions (i.e., H and W) and 2\u00d7 expand the feature dimension (i.e., C), while the Swin Transformer block focuses on learning and refining the feature representations. Formally, after forward passing the encoder, we obtain the output from four stages:\n$$f_1, f_2, f_3, f_4 = encoder(x),$$,\nwhere $f_1, f_2, f_3$, and $f_4$ have a size of $C\\times\\frac{H}{8}\\times\\frac{W}{8}$, $2C \\times\\frac{H}{16}\\times\\frac{W}{16}$, $4C\\times\\frac{H}{32}\\times\\frac{W}{32}$, and $8C \\times\\frac{H}{32}\\times\\frac{W}{32}$, respectively."}, {"title": "3.4 Mamba-based Decoder", "content": "Extend SSMs to 2D images. Different from 1D language sequences, 2D spatial information is crucial in vision tasks. Therefore, SSMs introduced in Section 3.1 cannot be directly applied in 2D images. Inspired by [27], we incorporate the 2D-selective-scan (SS2D) operation to address this problem. This method involves expanding image patches along four directions, generating four unique feature sequences. Then, each feature sequence is fed to an SSM (such as S6). Finally, the processed features are combined to construct the comprehensive 2D feature map. Formally, given the input feature z, the output feature $\\overline{z}$ of SS2D is computed as\n$$\\begin{aligned}\n&z_v = expand(z, v), \\quad \\text { for } v \\in \\{1,2,3,4\\},\\\\\n&\\overline{z}_v = S6(z), \\quad \\text { for } v \\in \\{1,2,3,4\\},\\\\\n&\\overline{Z} = sum(\\overline{z}_1, \\overline{z}_2, \\overline{z}_3, \\overline{z}_4),\\\\\n\\end{aligned}$$\nwhere $v \\in \\{1,2,3,4\\}$ is the four different scanning directions, $expand(z, v)$ is to expand 2D feature map z along direction v, S6(\u00b7) is the S6 operation introduced in Section 3.1, and $sum(.)$ is the element-wise add operation.\nMamba-based Feature Extractor (MFE). We introduce a Mamba-based feature extractor to learn the representation of 2D images. It is a critical module in the proposed Mamba-based decoder. As shown in Figure 2(a), motivated by [13], MFE consists of a linear layer used to expand the feature dimension by a controllable expansion factor a, a convolution layer with an activation function for extracting local features, an SS2D operation for modeling long-range dependency, and a layer normalization to normalize the learned features. More formally, given the input feature z, the output $\\overline{z}$ of MFE is calculated as\n$$\\overline{Z} = (LNSS2D \\circ \\sigma \\circ Conv \\circ Linear)(z),$$,\nwhere $LN(.)$ is the layer normalization, $\\sigma(\\cdot)$ is the activation function and the SiLU function is used in our experiment, $Conv(.)$ is the convolution operation.\nSelf-Task Mamba (STM) block. We introduce a self-task Mamba block for learning task-specific features based on MFE, which is illustrated in Figure 2(a). Inspired by [13], we use an input-dependent gate to adaptively select useful representations learned from MFE. After that, a linear layer is used to reduce the feature dimension expanded in MFE. Specifically, given the input feature z, the computation in the STM block is as\n$$\\begin{aligned}\n&Z_{LN} = LN(z), \\\\\n&\\overline{z} = MFE(Z_{LN}), \\quad g = \\sigma(Linear(Z_{LN})),\\\\\n&z = \\overline{z} * g, \\\\\n&\\overline{Z} = z + Linear(\\overline{z}),\n\\end{aligned}$$\nwhere is the element-wise multiplication.\nCross-Task Mamba (CTM) block. Although the STM block can effectively learn representations for each individual task, it lacks inter-task connections to share information which is crucial to the performance of MTL. To tackle this problem, we design a novel cross-task Mamba block (as shown in Figure 2(b)) by modifying the STM block to achieve knowledge exchange across different tasks. Specifically, given all tasks' features $\\{z^t\\}_{t=1}^T$ where T is the number of tasks, we first concatenate all task features and then pass it through an MFE to learn a global representation $\\overline{z}^{sh}$. Each task also learns its corresponding feature $\\overline{z}^t$ via its own MFE. Then, we use an input-dependent gate to aggregate the task-specific representation $z^t$ and global representation $\\overline{z}^{sh}$. Thus, each task adaptively fuses the global representation and its features. Formally, the forward process in the CTM block is as\n$$\\begin{aligned}\n&Z_{LN}^t = LN(z^t), \\quad \\text { for } t \\in \\{1,2, \\ldots, T\\},\\\\\n&Z_{IN}^{sh} = LN(concat(z^1, z^2, \\ldots, z^T)), \\\\\n&\\overline{z}^t = MFE(Z_{LN}^t), \\quad \\text { for } t \\in \\{1,2, \\ldots, T\\},\\\\\n&\\overline{z}^{sh} = MFE(Z_{IN}^{sh}), \\\\\n&g^t = \\sigma(Linear(Z_{LN}^t)), \\quad \\text { for } t \\in \\{1,2, \\ldots, T\\},\\\\\n&z^t = g^t \\overline{z}^t + (1 - g^t) \\overline{z}^{sh}, \\quad \\text { for } t \\in \\{1,2, \\ldots, T\\},\\\\\n&\\overline{z}^t = z^t + Linear(\\overline{z}^t), \\quad \\text { for } t \\in \\{1,2, \\ldots, T\\},\\\\\n\\end{aligned}$$\nwhere $concat()$ is the concatenation operation, $\\hat{\\sigma}(\\cdot)$ is the activation function and instead of SiLU used in STM block, we use the sigmoid function which is more suitable for generating the gating factors $g^t$ used in Equation (23).\nStage Design. As shown in Figure 1, the Mamba-based decoder contains three stages. Each stage has a similar design and comprises patch expand layers, STM blocks, and a CTM block. The patch expand layer is used to 2x upsample the feature resolution and 2\u00d7 reduce the feature dimension. For each task, its feature will be expanded by a patch expand layer and then fused with multi-scale features from the encoder via skip connections to complement the loss of spatial information caused by down-sampling. Then, a linear layer is used to reduce the feature dimension and two STM blocks are responsible for learning task-specific representation. Finally, a CTM block is applied to enhance each task's feature by knowledge exchange across tasks. Except for the CTM block, other modules are task-specific. More formally, the forward process of i-stage (i = 1,2,3) is formulated as\n$$\\begin{aligned}\n&r_1^t = PatchExpand(z_{i-1}^t) \\text { for } t \\in \\{1,2, \\ldots, T\\},\\\\\n&r_2^t = Linear(concat(r_1^t, f_{4-i})), \\text { for } t \\in \\{1,2, \\ldots, T\\},\\\\\n&r_3^t = STM(STM(r_2^t)), \\text { for } t \\in \\{1,2, \\ldots, T\\},\\\\\n&\\{z_i^t\\}_{t=1}^T = CTM(\\{r_3^t\\}_{t=1}^T),\\\\\n\\end{aligned}$$\nwhere $z_0^t = f_4$, PatchExpand(.) is the patch expand layer, STM(.) and CTM(.) are STM and CTM blocks, respectively."}, {"title": "3.5 Output Head", "content": "After obtaining each task's feature from the decoder, each task has its own output head to generate its final prediction. Inspired by [4], each output head contains a patch expand layer and a linear layer, which is lightweight. Specifically, given the t-th task feature $z^t$ with the size of $C \\times \\frac{H}{4} \\times \\frac{W}{4}$ from the decoder, the patch expand layer performs 4\u00d7 up-sampling to restore the resolution of the feature maps to the input resolution H \u00d7 W, and then the linear layer is used to output the final pixel-wise prediction."}, {"title": "4 Experiments", "content": "In this section, we conduct extensive experiments to demonstrate the effectiveness of the proposed MTMamba in multi-task dense scene understanding."}, {"title": "4.1 Experimental Setups", "content": "Datasets. Following [43, 46], experiments are conducted on two widely used benchmark datasets with multi-task labels: NYUDv2 [35] and PASCAL-Context [6]. The NYUDv2 dataset comprises a variety of indoor scenes, containing 795 and 654 RGB images for training and testing, respectively. It consists of four tasks: 40-class semantic segmentation (Semseg), monocular depth estimation (Depth), surface normal estimation (Normal), and object boundary detection (Boundary). The PASCAL-Context dataset, derived from the PASCAL dataset [9], includes both indoor and outdoor scenes and provides pixel-wise labels for tasks like semantic segmentation, human parsing (Parsing), and object boundary detection, with additional labels for surface normal estimation and saliency detection tasks generated by [31]. It contains 4,998 training images and 5,105 testing images.\nImplementation Details. We use Swin-Large Transformer [28] pretrained on the ImageNet-22K dataset [7] as the encoder. All models are trained with a batch size of 8 for 50,000 iterations. The AdamW optimizer [29] is adopted with a learning rate of 10\u22124 and a weight decay of 10\u22125. The polynomial learning rate scheduler is used in the training process. The expansion factor a in MFE is set to 2. Following [46], we resize the input images of NYUDv2 and PASCAL-Context as 448 \u00d7 576 and 512 \u00d7 512, respectively, and use the same data augmentation including random color jittering, random cropping, random scaling, and random horizontal flipping. We use l1 loss for depth estimation and surface normal estimation tasks and the cross-entropy loss for other tasks.\nEvaluation Metrics. Following [46], we use mean intersection over union (mIoU) for semantic segmentation and human parsing tasks, root mean square error (RMSE) for monocular depth estimation task, mean error (mErr) for surface normal estimation task, maximal F-measure (maxF) for saliency detection task, and optimal-dataset-scale F-measure (odsF) for object boundary detection task. Besides, we use the average relative MTL performance Am (defined in [36]) as the overall performance metric."}, {"title": "4.2 Comparison with State-of-the-art Methods", "content": "We compare the proposed MTMamba method with two types of MTL methods: CNN-based methods, including Cross-Stitch [33], PAP [51], PSD [52], PAD-Net [42], MTI-Net [37], ATRC [3], and ASTMT [31], and Transformer-based methods, i.e., InvPT [46] and MQTransformer [43].\nTable 1 shows the comparison on the NYUDv2 dataset. As can be seen, the proposed MTMamba method shows superior performance in all four tasks. For example, the performance of the semantic segmentation task has notably improved from the Transformer-based methods (i.e., InvPT and MQTransformer),"}, {"title": "4.3 Model Analysis", "content": "Effectiveness of STM and CTM blocks. The proposed MTMamba contains two types of core blocks: STM and CTM blocks. We perform an experiment on the NYUDv2 dataset to study the effectiveness of each block with a Swin-Large Transformer encoder. The results are shown in Table 3. \"Swin only\" (resp. \"STM only\") denotes each task only uses two specific Swin Transformer (resp. STM) blocks in each decoder stage. \"Single-task\" is the single-task counterpart of \"Swin only\", which indicates each task has its task-specific model. \"STM+CTM\" is the default method of MTMamba, i.e., adding a shared CTM block in each decoder stage compared with \"STM only\".\nAccording to Table 3, \"STM only\" largely outperforms \u201cSwin only\u201d, demonstrating that the STM block is more effective than the Swin Transformer block. Furthermore, \"STM only\" has fewer parameters and FLOPs than \"Swin only\", which shows the efficiency of the STM block. Compared with \"STM only\","}, {"title": "5 Conclusion", "content": "In this paper, we propose MTMamba, a novel multi-task architecture with a Mamba-based decoder for multi-task dense scene understanding. With two novel blocks (STM and CTM blocks), MTMamba can effectively model long-range dependency and achieve cross-task interaction. Experiments on two benchmark datasets demonstrate that the proposed MTMamba achieves better performance than previous CNN-based and Transformer-based methods."}]}