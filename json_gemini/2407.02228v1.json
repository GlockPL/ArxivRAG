{"title": "MTMamba: Enhancing Multi-Task Dense Scene Understanding by Mamba-Based Decoders", "authors": ["Baijiong Lin", "Weisen Jiang", "Pengguang Chen", "Yu Zhang", "Shu Liu", "Ying-Cong Chen"], "abstract": "Multi-task dense scene understanding, which learns a model for multiple dense prediction tasks, has a wide range of application sce- narios. Modeling long-range dependency and enhancing cross-task inter- actions are crucial to multi-task dense prediction. In this paper, we pro- pose MTMamba, a novel Mamba-based architecture for multi-task scene understanding. It contains two types of core blocks: self-task Mamba (STM) block and cross-task Mamba (CTM) block. STM handles long- range dependency by leveraging Mamba, while CTM explicitly mod- els task interactions to facilitate information exchange across tasks. Ex- periments on NYUDv2 and PASCAL-Context datasets demonstrate the superior performance of MTMamba over Transformer-based and CNN- based methods. Notably, on the PASCAL-Context dataset, MTMamba achieves improvements of +2.08, +5.01, and +4.90 over the previous best method in the tasks of semantic segmentation, human parsing, and object boundary detection, respectively. The code is available at https://github.com/EnVision-Research/MTMamba.", "sections": [{"title": "1 Introduction", "content": "Multi-task dense scene understanding is an essential problem in computer vi- sion [36] and has a variety of practical applications, such as autonomous driv- ing [20, 23], healthcare [19], and robotics [48]. It aims to train a model for si- multaneously handling multiple dense prediction tasks, such as semantic seg- mentation, monocular depth estimation, surface normal estimation, and object boundary detection.\nThe prevalent multi-task architecture follows an encoder-decoder framework, consisting of a task-shared encoder for feature extraction and task-specific de- coders for predictions [36]. This framework is very general and many variants have been proposed [37,42,43,46] to improve its performance in multi-task scene understanding. One promising approach is the decoder-focused method [36] with the aim of enhancing cross-task interaction in task-specific decoders through well-designed fusion modules. For example, derived from the convolutional neu- ral network (CNN), PAD-Net [42] and MTI-Net [37] incorporate a multi-modal distillation module to promote information fusion between different tasks in the decoder and achieve better performance than the encoder-decoder frame- work. Since the convolution operation mainly focuses on local features [2], recent methods [43, 46] propose Transformer-based decoders with attention-based fu- sion modules. These methods leverage the attention mechanism to capture global context information, resulting in better performance than CNN-based methods. Previous works demonstrate that enhancing cross-task correlation and modeling long-range spatial relationships are critical for multi-task dense prediction.\nVery recently, Mamba [13], a new architecture derived from state space mod- els (SSMs) [14,15], has shown better long-range dependencies modeling capacity and superior performance than Transformer models in various domains, includ- ing language modeling [12, 13,39], graph reasoning [1,38], medical images anal- ysis [30, 41], and point cloud analysis [22, 49]. However, all of these works focus on single-task learning, while how to adopt Mamba for multi-task training is still under investigation. Moreover, achieving cross-task correlation in Mamba remains unexplored, which is critical for multi-task scene understanding.\nTo fill these gaps, in this paper, we propose MTMamba, a novel multi- task architecture featuring a Mamba-based decoder and superior performance in multi-task scene understanding. The overall framework is shown in Figure 1. MTMamba is a decoder-focused method with two types of core blocks: the self- task Mamba (STM) block and the cross-task Mamba (CTM) block, illustrated in Figure 2. Specifically, STM, inspired by Mamba, can effectively capture global context information. CTM is designed to enhance each task's features by facili- tating knowledge exchange across different tasks. Therefore, through the collab- oration of STM and CTM blocks in the decoder, MTMamba not only enhances cross-task interaction but also effectively handles long-range dependency.\nWe evaluate MTMamba on two standard multi-task dense prediction bench- mark datasets, namely NYUDv2 [35] and PASCAL-Context [6]. Quantitative results demonstrate that MTMamba largely outperforms both CNN-based and Transformer-based methods. Notably, on the PASCAL-Context dataset, MT- Mamba outperforms the previous best by +2.08, +5.01, and +4.90 in semantic segmentation, human parsing, and object boundary detection tasks, respectively. Qualitative studies show that MTMamba generates better visual results with more accurate details than state-of-the-art Transformer-based methods.\nOur main contributions are summarized as follows:\nWe propose MTMamba, a novel multi-task architecture for multi-task scene understanding. It contains a novel Mamba-based decoder, which effectively models long-range spatial relationships and achieves cross-task correlation;\nWe design a novel CTM block to enhance cross-task interaction in multi-task dense prediction;"}, {"title": "2 Related Works", "content": "Multi-task learning (MTL) is a learning paradigm that aims to simultaneously learn multiple tasks in a single model [50]. Recent MTL research mainly focuses on multi-objective optimization [24-26, 34, 44, 45, 47] and network architecture design [37, 42, 43, 46]. In multi-task dense scene understanding, most existing works focus on designing architecture [36], especially designing specific modules in the decoder to achieve better cross-task interaction. For example, based on CNN, Xu et al. [42] introduce PAD-Net, incorporating an effective multi-modal distillation module to promote information fusion between different tasks in the decoder. MTI-Net [37] is a complex multi-scale and multi-task CNN architecture with an information distillation across various feature scales. As the convolution operation mainly captures local features [2], recent approaches [43,46] utilize the attention mechanism to grasp global context and develop Transformer-based de- coders for multi-task scene understanding. For instance, Ye & Xu [46] introduce InvPT, a Transformer-based multi-task architecture, employing an effective UP-Transformer block for multi-task feature interaction at different feature scales. MQTransformer [43] designs a cross-task query attention module to enable ef- fective task association and information exchange in the decoder.\nPrevious works demonstrate long-range dependency modeling and enhancing cross-task correlation are critical for multi-task dense prediction. Unlike existing methods, we propose a novel multi-task architecture derived from Mamba to capture global information better and promote cross-task interaction."}, {"title": "2.2 State Space Models", "content": "State space models (SSMs) are a mathematical representation of dynamic sys- tems, which models the input-output relationship through a hidden state. SSMs are general and have achieved great success in a wide variety of applications such as reinforcement learning [16], computational neuroscience [10], and linear dy- namical systems [18]. Recently, SSMs are introduced as an alternative network architecture to model long-range dependency. Compared with CNN-based net- works [17,21], which are designed for capturing local dependence, SSMs are more powerful for long sequences; Compared with Transformer-based networks [8,40], which require the quadratic complexity of the sequence length, SSMs are more computation- and memory-efficient.\nMany different structures have been proposed recently to improve the ex- pressivity and efficiency of SSMs. Gu et al. [14] propose structured state space models (S4) to improve computational efficiency, where the state matrix is a sum of low-rank and normal matrices. Many follow-up works attempt to enhance the effectiveness of S4. For example, Fu et al. [11] design a new SSM layer H3 to fill the performance gap between SSMs and Transformers in language model- ing. Mehta et al. [32] introduce a gated state space layer using gated units for improving expressivity.\nRecently, Gu & Dao [13] further propose Mamba with the core operation S6, an input-dependent selection mechanism of S4, which achieves linear scaling in sequence length and demonstrates superior performance over Transformers on various benchmarks. Mamba has been successfully applied in image classifica- tion [27,53], image segmentation [41], and graph prediction [38]. Different from them, which use Mamba in the single-task setting, we consider a more challeng- ing multi-task setting and propose novel self-task and cross-task Mamba modules to capture intra-task and inter-task dependence."}, {"title": "3 Methodology", "content": "In this section, we first introduce the background knowledge of state space mod- els and Mamba in Section 3.1. Then, we introduce the overall architecture of the proposed MTMamba in Section 3.2. Subsequently, we delve into a detailed exploration of each part in MTMamba, including the encoder in Section 3.3, the Mamba-based decoder in Section 3.4, and the output head in Section 3.5."}, {"title": "3.1 Preliminaries", "content": "SSMs [13-15], originated from the linear systems theory [5, 18], map input se- quence \\(x(t) \\in \\mathbb{R}\\) to output sequence \\(y(t) \\in \\mathbb{R}\\) though a hidden state \\(h \\in \\mathbb{R}^{N}\\) by a linear ordinary differential equation:\n\\[\nh'(t) = Ah(t) + Bx(t), \\tag{1}\n\\]\n\\[\ny(t) = Ch(t) + Dx(t), \\tag{2}\n\\]\nwhere \\(A \\in \\mathbb{R}^{N \\times N}\\) is the state matrix, \\(B \\in \\mathbb{R}^{N}\\) is the input matrix, \\(C \\in \\mathbb{R}^{N}\\) is the output matrix, and \\(D \\in \\mathbb{R}\\) is the skip connection. Equation (1) defines the evolution of the hidden state \\(h(t)\\), while Equation (2) determines the output is composed of a linear transformation of the hidden state \\(h(t)\\) and a skip connec- tion from \\(x(t)\\). For the remainder of this paper, D is omitted for explanation (i.e., D = 0).\nSince the continuous-time system is not suitable for digital computers and real-world data, which are usually discrete, a discretization procedure is intro- duced to approximate it by a discrete-time one. Let \\(\\Delta \\in \\mathbb{R}\\) be a discrete-time step. Equations (1) and (2) are discretized as\n\\[\nh_{t} = \\overline{A}h_{t-1} + \\overline{B}x_{t}, \\tag{3}\n\\]\n\\[\ny_{t} = Ch_{t}, \\tag{4}\n\\]\nwhere \\(x_{t} = x(\\Delta t)\\), and\n\\[\n\\overline{A} = exp(\\Delta A), \\ \\overline{B} = (\\Delta A)^{-1}(exp(\\Delta A) - I) \\cdot \\Delta B \\approx \\Delta B, C = C. \\tag{5}\n\\]\nIn S4 [14], \\((A, B, C, \\Delta)\\) are trainable parameters learned by gradient descent and do not explicitly depend on the input sequence, resulting in weak contextual information extraction. To overcome this, Mamba [13] proposes S6, which in- troduces an input-dependent selection mechanism to allow the system to select relevant information based on the input sequence. This is achieved by making B, C, and A as functions of the input \\(x_{t}\\). More formally, given an input sequence \\(x \\in \\mathbb{R}^{B \\times L \\times C}\\) where B is the batch size, L is the sequence length, and C is the feature dimension, the input-dependent parameters \\((B, C, \\Delta)\\) are computed as\n\\[\nB = Linear(x) \\in \\mathbb{R}^{B \\times L \\times N}, \\tag{6}\n\\]\n\\[\nC = Linear(x) \\in \\mathbb{R}^{B \\times L \\times N}, \\tag{7}\n\\]\n\\[\n\\Delta = SoftPlus(A + Linear(x)) \\in \\mathbb{R}^{B \\times L \\times C}, \\tag{8}\n\\]\nwhere \\(\\Delta \\in \\mathbb{R}^{B \\times L \\times C}\\) is a learnable parameter, SoftPlus(\\(\\cdot\\)) is the SoftPlus func- tion, and Linear(\\(\\cdot\\)) is the linear layer. \\(A \\in \\mathbb{R}^{L \\times C}\\) is a trainable parameter as in S4. After computing \\((A, B, C, \\Delta)\\), \\((A, B, C)\\) are discretized via Equation (5), then the output sequence \\(y \\in \\mathbb{R}^{B \\times L \\times C}\\) is computed by Equations (3) and (4)."}, {"title": "3.2 Overall Architecture", "content": "An overview of MTMamba is illustrated in Figure 1. It contains three compo- nents: an off-the-shelf encoder, a Mamba-based decoder, and task-specific heads."}, {"title": "3.3 Encoder", "content": "We take the Swin Transformer [28] as an example. Consider an input RGB image \\(x \\in \\mathbb{R}^{3 \\times H \\times W}\\), where H and W are the height and width of the image, respectively. The encoder employs a patch-partition module to segment the input image into non-overlapping patches. Each patch is regarded as a token, and its feature representation is a concatenation of the raw RGB pixel values. In our experiment, we use a standard patch size of 4 \u00d7 4. Therefore, the feature dimension of each patch is 4 \u00d7 4 \u00d7 3 = 48. After patch splitting, a linear layer is applied to project the raw token into a C-dimensional feature embedding. The patch tokens, after being transformed, sequentially traverse multiple Swin Transformer blocks and patch merging layers, which collaboratively produce hierarchical feature representations. Specifically, the patch merging layer [28] is used to 2\u00d7 downsample the spatial dimensions (i.e., H and W) and 2\u00d7 expand the feature dimension (i.e., C), while the Swin Transformer block focuses on learning and refining the feature representations. Formally, after forward passing the encoder, we obtain the output from four stages:\n\\[\nf_{1}, f_{2}, f_{3}, f_{4} = encoder(x), \\tag{9}\n\\]\nwhere \\(f_{1}, f_{2}, f_{3}\\), and \\(f_{4}\\) have a size of \\(C \\times \\frac{H}{4} \\times \\frac{W}{4}\\), \\(2C \\times \\frac{H}{8} \\times \\frac{W}{8}\\), \\(4C \\times \\frac{H}{16} \\times \\frac{W}{16}\\), and \\(8C \\times \\frac{H}{32} \\times \\frac{W}{32}\\), respectively."}, {"title": "3.4 Mamba-based Decoder", "content": "Extend SSMs to 2D images. Different from 1D language sequences, 2D spatial information is crucial in vision tasks. Therefore, SSMs introduced in Section 3.1 cannot be directly applied in 2D images. Inspired by [27], we incorporate the 2D-selective-scan (SS2D) operation to address this problem. This method involves expanding image patches along four directions, generating four unique feature sequences. Then, each feature sequence is fed to an SSM (such as S6). Finally, the processed features are combined to construct the comprehensive 2D feature map. Formally, given the input feature z, the output feature z of SS2D is computed as\n\\[\n\\text{\\~{}z}_{v} = expand(z, v), \\text{ for } v \\in \\{1, 2, 3, 4\\}, \\tag{10}\n\\]\n\\[\n\\breve{z}_{v} = S6(\\text{\\~{}z}_{v}), \\text{ for } v \\in \\{1, 2, 3, 4\\}, \\tag{11}\n\\]\n\\[\n\\hat{z} = sum(\\breve{z}_{1}, \\breve{z}_{2}, \\breve{z}_{3}, \\breve{z}_{4}), \\tag{12}\n\\]\nwhere \\(v \\in \\{1, 2, 3, 4\\}\\) is the four different scanning directions, expand(z, v) is to expand 2D feature map z along direction v, S6(\\(\\cdot\\)) is the S6 operation introduced in Section 3.1, and sum(\\(\\cdot\\)) is the element-wise add operation.\nMamba-based Feature Extractor (MFE). We introduce a Mamba-based feature extractor to learn the representation of 2D images. It is a critical module in the proposed Mamba-based decoder. As shown in Figure 2(a), motivated by [13], MFE consists of a linear layer used to expand the feature dimension by a con- trollable expansion factor a, a convolution layer with an activation function for extracting local features, an SS2D operation for modeling long-range de- pendency, and a layer normalization to normalize the learned features. More formally, given the input feature z, the output z of MFE is calculated as\n\\[\nz = (LN \\circ SS2D \\circ \\sigma \\circ Conv \\circ Linear)(z), \\tag{13}\n\\]\nwhere LN(\\(\\cdot\\)) is the layer normalization, \\(\\sigma(\\cdot)\\) is the activation function and the SiLU function is used in our experiment, Conv(\\(\\cdot\\)) is the convolution operation.\nSelf-Task Mamba (STM) block. We introduce a self-task Mamba block for learn- ing task-specific features based on MFE, which is illustrated in Figure 2(a). Inspired by [13], we use an input-dependent gate to adaptively select useful rep- resentations learned from MFE. After that, a linear layer is used to reduce the feature dimension expanded in MFE. Specifically, given the input feature z, the computation in the STM block is as\n\\[\nz_{LN} = LN(z), \\tag{14}\n\\]\n\\[\n\\text{\\~{}z} = MFE(z_{LN}), \\ g = \\sigma(Linear(z_{LN})), \\tag{15}\n\\]\n\\[\nz = \\text{\\~{}z} * g, \\tag{16}\n\\]\n\\[\nz = z + Linear(\\hat{z}), \\tag{17}\n\\]\nwhere is the element-wise multiplication.\nCross-Task Mamba (CTM) block. Although the STM block can effectively learn representations for each individual task, it lacks inter-task connections to share information which is crucial to the performance of MTL. To tackle this prob- lem, we design a novel cross-task Mamba block (as shown in Figure 2(b)) by modifying the STM block to achieve knowledge exchange across different tasks. Specifically, given all tasks' features \\(\\left\\{z^{t}\\right\\}_{t=1}^{T}\\) where T is the number of tasks, we first concatenate all task features and then pass it through an MFE to learn a global representation \\(\\hat{z}^{sh}\\). Each task also learns its corresponding feature \\(\\hat{z}^{t}\\) via its own MFE. Then, we use an input-dependent gate to aggregate the task-specific representation \\(\\hat{z}^{t}\\) and global representation \\(\\hat{z}^{sh}\\). Thus, each task adaptively fuses the global representation and its features. Formally, the forward process in the CTM block is as\n\\[\nz_{LN}^{t} = LN(z^{t}), \\text{ for } t \\in \\{1, 2, \\dots, T\\}, \\tag{18}\n\\]\n\\[\nz_{IN}^{sh} = LN(concat(z^{1}, z^{2}, \\dots, z^{T})), \\tag{19}\n\\]\n\\[\n\\text{\\~{}z}^{t} = MFE(z_{LN}^{t}), \\text{ for } t \\in \\{1, 2, \\dots, T\\}, \\tag{20}\n\\]\n\\[\n\\text{\\~{}z}^{sh} = MFE(z_{IN}^{sh}), \\tag{21}\n\\]\n\\[\ng^{t} = \\sigma(Linear(z_{LN}^{t})), \\text{ for } t \\in \\{1, 2, \\dots, T\\}, \\tag{22}\n\\]\n\\[\nz^{*t} = g^{t} \\text{\\~{}z}^{t} + (1 - g^{t}) \\text{\\~{}z}^{sh}, \\text{ for } t \\in \\{1, 2, \\dots, T\\}, \\tag{23}\n\\]\n\\[\nz^{t} = z^{*t} + Linear(\\hat{z}^{t}), \\text{ for } t \\in \\{1, 2, \\dots, T\\}, \\tag{24}\n\\]\nwhere concat() is the concatenation operation, \\(\\sigma(\\cdot)\\) is the activation function and instead of SiLU used in STM block, we use the sigmoid function which is more suitable for generating the gating factors \\(g^{t}\\) used in Equation (23).\nStage Design. As shown in Figure 1, the Mamba-based decoder contains three stages. Each stage has a similar design and comprises patch expand layers, STM blocks, and a CTM block. The patch expand layer is used to 2x upsample the feature resolution and 2\u00d7 reduce the feature dimension. For each task, its feature will be expanded by a patch expand layer and then fused with multi-scale features from the encoder via skip connections to complement the loss of spatial information caused by down-sampling. Then, a linear layer is used to reduce the feature dimension and two STM blocks are responsible for learning task-specific representation. Finally, a CTM block is applied to enhance each task's feature by knowledge exchange across tasks. Except for the CTM block, other modules are task-specific. More formally, the forward process of i-stage (i = 1,2,3) is formulated as\n\\[\nr_{i}^{t} = PatchExpand(z_{i-1}^{t}) \\text{ for } t \\in \\{1, 2, \\dots, T\\}, \\tag{25}\n\\]\n\\[\nr_{i}^{t} = Linear(concat(r_{i}^{t}, f_{4-i})), \\text{ for } t \\in \\{1, 2, \\dots, T\\}, \\tag{26}\n\\]\n\\[\nr_{i}^{t} = STM(STM(r_{i}^{t})), \\text{ for } t \\in \\{1, 2, \\dots, T\\}, \\tag{27}\n\\]\n\\[\n\\left\\{z_{i}^{t}\\right\\}_{t=1}^{T} = CTM(\\left\\{r_{i}^{t}\\right\\}_{t=1}^{T}), \\tag{28}\n\\]\nwhere \\(z_{0}^{t} = f_{4}\\), PatchExpand(\\(\\cdot\\)) is the patch expand layer, STM(\\(\\cdot\\)) and CTM(\\(\\cdot\\)) are STM and CTM blocks, respectively."}, {"title": "3.5 Output Head", "content": "After obtaining each task's feature from the decoder, each task has its own output head to generate its final prediction. Inspired by [4], each output head contains a patch expand layer and a linear layer, which is lightweight. Specifically, given the t-th task feature \\(z^{t}\\) with the size of \\(C \\times \\frac{H}{4} \\times \\frac{W}{4}\\) from the decoder, the patch expand layer performs 4\u00d7 up-sampling to restore the resolution of the feature maps to the input resolution H \u00d7 W, and then the linear layer is used to output the final pixel-wise prediction."}, {"title": "4 Experiments", "content": "In this section, we conduct extensive experiments to demonstrate the effective- ness of the proposed MTMamba in multi-task dense scene understanding."}, {"title": "4.1 Experimental Setups", "content": "Following [43, 46], experiments are conducted on two widely used benchmark datasets with multi-task labels: NYUDv2 [35] and PASCAL-Context [6]. The NYUDv2 dataset comprises a variety of indoor scenes, containing 795 and 654 RGB images for training and testing, respectively. It consists of four tasks: 40-class semantic segmentation (Semseg), monocular depth estimation (Depth), surface normal estimation (Normal), and object boundary detection (Boundary). The PASCAL-Context dataset, derived from the PASCAL dataset [9], includes both indoor and outdoor scenes and provides pixel-wise labels for tasks like semantic segmentation, human parsing (Parsing), and object bound- ary detection, with additional labels for surface normal estimation and saliency detection tasks generated by [31]. It contains 4,998 training images and 5,105 testing images.\nWe use Swin-Large Transformer [28] pretrained on the ImageNet-22K dataset [7] as the encoder. All models are trained with a batch size of 8 for 50,000 iterations. The AdamW optimizer [29] is adopted with a learning rate of \\(10^{-4}\\) and a weight decay of \\(10^{-5}\\). The polynomial learning rate scheduler is used in the training process. The expansion factor a in MFE is set to 2. Following [46], we resize the input images of NYUDv2 and PASCAL-Context as 448 \u00d7 576 and 512 \u00d7 512, respectively, and use the same data augmentation including random color jittering, random cropping, random scaling, and ran- dom horizontal flipping. We use \\(l_{1}\\) loss for depth estimation and surface normal estimation tasks and the cross-entropy loss for other tasks."}, {"title": "4.2 Comparison with State-of-the-art Methods", "content": "We compare the proposed MTMamba method with two types of MTL methods: CNN-based methods, including Cross-Stitch [33], PAP [51], PSD [52], PAD- Net [42], MTI-Net [37], ATRC [3], and ASTMT [31], and Transformer-based methods, i.e., InvPT [46] and MQTransformer [43].\nAs can be seen, the proposed MTMamba method shows superior performance in all four tasks. For example, the performance of the semantic segmentation task has notably im- proved from the Transformer-based methods (i.e., InvPT and MQTransformer)."}, {"title": "4.3 Model Analysis", "content": "Effectiveness of STM and CTM blocks. The proposed MTMamba contains two types of core blocks: STM and CTM blocks. We perform an experiment on the NYUDv2 dataset to study the effectiveness of each block with a Swin-Large Transformer encoder. The results are shown in Table 3. \"Swin only\" (resp. \"STM only\") denotes each task only uses two specific Swin Transformer (resp. STM) blocks in each decoder stage. \"Single-task\" is the single-task counterpart of \"Swin only\", which indicates each task has its task-specific model. \"STM+CTM\" is the default method of MTMamba, i.e., adding a shared CTM block in each decoder stage compared with \"STM only\".\nAccording to Table 3, \"STM only\" largely outperforms \u201cSwin only\u201d, demon- strating that the STM block is more effective than the Swin Transformer block. Furthermore, \"STM only\" has fewer parameters and FLOPs than \"Swin only\", which shows the efficiency of the STM block. Compared with \"STM only\", \"STM+CTM\" performs better, confirming the CTM block is beneficial. More- over, the default configuration (i.e., \"STM+CTM\") significantly outperforms \"Single-task\" on all tasks, demonstrating the effectiveness of MTMamba.\nAs shown in Figure 2, the MFE module is SSM- based and is the core of both STM and CTM blocks. We conduct an experiment by replacing all MFE modules in MTMamba with the attention module on the NYUDv2 dataset. As shown in Table 4, MFE is more effective and efficient than attention.\nAs shown in Figure 2, in both STM and CTM blocks, we use an input-dependent gate to select useful representations adaptively from MFE modules. The linear layer is a simple but effective option for the gate function. We conduct an experiment by replacing all linear gates in MTMamba with the attention-based gate on the NYUDv2 dataset. As shown in Table 5, the linear gate (i.e., MTMamba) performs comparably to the attention gate in terms of \\(A_m\\), while the linear gate is more efficient.\nEffectiveness of cross-task interaction in CTM block. The core of the CTM block is the cross-task interaction, i.e., Equation (23), where we fuse task-specific representation \\(\\hat{z}^{t}\\) and shared representation \\(\\hat{z}^{sh}\\) via a task-specific gate \\(g^{t}\\). In this experiment, we study its effectiveness by comparing it with the cases of \\(g^{t} = 0\\) and \\(g^{t} = 1\\). The experiments are conducted with a Swin-Large Transformer encoder on the NYUDv2 dataset. The results are shown in Table 6. As can be seen, using a specific \\(\\text{\\~{}z}^{t}\\) (i.e., the case of \\(g^{t} = 0\\)) or shared \\(\\text{\\~{}z}^{sh}\\) (i.e., the case of \\(g^{t} = 1\\)) degrades the performance, demonstrating that the adaptive fusion is better.\nIn this experiment, we investigate the per- formance of the proposed MTMamba with different scales of Swin Transformer encoder on the NYUDv2 dataset. The results are shown in Table 7. As can be seen, as the model capacity increases, all the tasks perform better accordingly."}, {"title": "4.4 Qualitative Evaluations", "content": "Visualization of learned features. Figure 3 shows the comparison of the final de- coder feature between the proposed MTMamba and Transformer-based method captures more discriminative features, resulting in better segmentation perfor- mance.\nVisualization of predictions. We conduct qualitative studies by comparing the output predictions from our proposed MTMamba against the state-of-the-art Transformer-based method, InvPT [46]. Figures 5 and 4 show the qualitative results on the NYUDv2 and PASCAL-Context datasets, respectively. As can be seen, our method has better visual results than InvPT in all tasks. For example, as highlighted with yellow circles in Figure 5, MTMamba generates more accurate results with better alignments for the semantic segmentation task and clearer object boundaries for the object boundary de- tection task. Figure 4 demonstrates that MTMamba produces better predictions with more accu- rate details (like the fingers as highlighted) for both semantic segmentation and human parsing tasks and more distinct boundaries for the object boundary de- tection task. Hence, both qualitative study (Figures 5 and 4) and quantitative study (Tables 1 and 2) show the superior performance of the proposed MT- Mamba method."}, {"title": "5 Conclusion", "content": "In this paper, we propose MTMamba, a novel multi-task architecture with a Mamba-based decoder for multi-task dense scene understanding. With two novel blocks (STM and CTM blocks), MTMamba can effectively model long-range dependency and achieve cross-task interaction. Experiments on two benchmark datasets demonstrate that the proposed MTMamba achieves better performance than previous CNN-based and Transformer-based methods."}]}