{"title": "Driv3R: Learning Dense 4D Reconstruction for Autonomous Driving", "authors": ["Xin Fei", "Wenzhao Zheng", "Yueqi Duan", "Wei Zhan", "Masayoshi Tomizuka", "Kurt Keutzer", "Jiwen Lu"], "abstract": "Realtime 4D reconstruction for dynamic scenes remains a crucial challenge for autonomous driving perception. Most existing methods rely on depth estimation through self-supervision or multi-modality sensor fusion. In this paper, we propose Driv3R, a DUSt3R-based framework that directly regresses per-frame point maps from multi-view image sequences. To achieve streaming dense reconstruction, we maintain a memory pool to reason both spatial relationships across sensors and dynamic temporal contexts to enhance multi-view 3D consistency and temporal integration. Furthermore, we employ a 4D flow predictor to identify moving objects within the scene to direct our network focus more on reconstructing these dynamic regions. Finally, we align all per-frame pointmaps consistently to the world coordinate system in an optimization-free manner. We conduct extensive experiments on the large-scale nuScenes dataset to evaluate the effectiveness of our method. Driv3R outperforms previous frameworks in 4D dynamic scene reconstruction, achieving 15\u00d7 faster inference speed compared to methods requiring global alignment.", "sections": [{"title": "1. Introduction", "content": "Real-time and accurate dense reconstruction of dynamic scenes is a challenging task for the perception of autonomous driving and robotics. Compared to data fusion from multi-modality sensors, such as cameras, LiDAR, and radar, relying solely on multi-view cameras provides a more computationally efficient and low-cost solution. However, achieving accurate depth estimation without 3D ground truth supervision and precise representations of dynamic objects introduces significant challenges to this task.\nTo address these challenges, several efficient 3D representations have been proposed to enable scene reconstruction from multi-view cameras and perform downstream tasks, such as novel view synthesis, depth estimation, and pose prediction. Mildenhall et al. [31] encode"}, {"title": "2. Related Work", "content": "Depth Estimation For Autonomous Driving. Due to the absence of dense ground-truth depth in large-scale autonomous driving datasets, previous research either adopts a self-supervised approach for depth estimation or incorporates additional supervisory signals, such as LiDAR, optical flow and object motion, to enhance prediction accuracy. Among these methods, R3D3 leverages both temporal and spatial information from multi-view cameras by iterating between geometric estimations and further refines the monocular depth, enabling accurate and efficient dense depth prediction in dynamic scenes. We thus adopt the depth predictions from the pretrained R3D3 model as supervision for moving objects in our Driv3R model.\nStatic 3D Reconstruction. Static 3D reconstruction for objects and scenes has seen considerable advancement with the rise of learning-based approaches. These methods aim to learn meshes, point clouds, voxels, implicit neural fields or explicit representations from the training data. DUSt3R takes a pioneering step to directly regress point maps from arbitrary input image pairs leveraging strong 3D priors learned from the large-scale training data. However, DUSt3R requires computationally expensive global alignment to optimize pointmaps and camera poses into a consistent coordinate system. To address this, Spann3R maintains a spatial memory pool that enables incremental 3D reconstruction in a consistent coordinate system from input sequences, eliminating the need for alignment optimization. However, these methods perform poorly to reconstruct dynamic scenes, which are crucial for autonomous driving perception.\nOptical Flow. Identifying dynamic objects is crucial for the accurate reconstruction of 4D scenes. Optical flow, which estimates per-pixel motion across image sequences, plays a key role in detecting moving objects in 2D images. Previously, the estimation of optical flow is treated as an energy minimization process or discrete optimization problem, while more recent works tend to adopt end-to-end differentiable neural networks for improved efficiency and accuracy . The lightweight RAFT model constructs multi-scale 4D correlation volumes and updates the flow field using a recurrent network,"}, {"title": "3. Proposed Approach", "content": "Given multi-view images $I_t = \\{I_{t,c}\\}_{c=1}^C$ from RGB cameras with corresponding camera poses $\\{T_{t,c}, K_{t,c}\\}_{t=1}^C$ for each timestamp $t \\in T$, our Driv3R aims to learn 4D dense pixel-wise point cloud representations $\\{P_t | t \\in T\\}$ in the global world coordinate system.\nOur Driv3R is composed of three stages and allows for end-to-end training. First, we construct a memory pool for spatial and temporal information interaction inspired by Spann3R [41]. Then, we introduce a 4d flow predictor to identify dynamic objects within the scene, guiding the network to focus more on reconstructing these regions during training. Finally, we employ a multi-view aligner to register all point maps into the consistent world coordinate system in an optimization-free manner. Leveraging the depth inference from the pretrained R3D3 [35] model as supervision, Driv3R enables consistent 4D dense point cloud reconstruction and precise modeling of moving objects within the dynamic scenes."}, {"title": "3.2. Temporal-Spatial Memory Pool", "content": "Spann3R [41] takes multi-view images as input and manages an external spatial memory to predict per-image pointmaps in a consistent coordinate system. Inspired by this, we maintain a temporal-spatial memory pool to reason both temporal relationships and spatial contexts within the multi-view input sequences. To elaborate, given frames from various sensors and timestamps in the input sequence, denoted as $I_{t,c}$ and $I_{t',c'}$, a ViT [12] first encodes both images into feature maps $f_{t,c}, f_{t',c'}$. Then, we update $f_{t,c}$ by extracting memories from the temporal-spatial memory pools as described in Equation 1, which allows the encoded feature $f_{t,c}$ to fully interact with information obtained from previous timestamps and viewpoints.\n$f'_{t.c} = softmax(\\frac{I_{t,c}K^T - V}{\\sqrt{s}}) + q_{t,c},$\nwhere $q_{t,c}, K, V$ represent the query for the current frame, and the key and value from memory pools, respectively. Subsequently, we decode the feature pair $(f_{t,c}, f_{t',c'})$ via two interconnected decoders. The feature decoded by the target decoder is used to generate the query for the next step, while the feature decoded by the reference decoder is applied to memory encoding and regression of per-frame point maps and confidence maps (as shown in Equation 2).\n$f_{tar}, I_{t,c} f_{ref} = Decoder(f_{t,c}, f_{t',c'}),$\n$P_{t,c}, C_{t,c} = MLP(f_{ref}),$\n$K_{t,c}, V_{t,c} = Encoder(f_{tef}, f_{t',c'}, P_{t,c}).$\nCompared to Spann3R [41], our architecture efficiently handles both spatial and temporal information storage, management, and interaction. Specifically, we maintain a separate memory pool for each sensor with each key-value pair labeled by its corresponding timestamp. During pool updates, new keys are added to the relevant sensor pool based on cosine similarities with the existing memory keys, allowing us to identify which key-value pairs have the closest spatial and temporal relationships with the current frame when using the memory pool to update the feature generated from"}, {"title": "3.3. 4D Flow Predictor", "content": "To enhance the ability of our Driv3R model to reconstruct dynamic objects in the input scene, we design a 4D flow predictor based on RAFT [38] model. Given image sequence from a single sensor $\\{I_1, I_2, ..., I_T\\}$ and corresponding pointmaps $\\{P_1, P_2, ..., P_T \\}$ obtained in Section 3.3, we first generate a set of pairs $\\{(I_{i1}, I_{i2})\\}_1^M$ that include frames which are temporally adjacent. Then, we use the pretrained RAFT model to predict flow maps $\\{(F_{12}, F_{21})\\}_{1}^M$ for each image pair. To further capture the 4D motions of objects, we apply cross-projection on the pointmaps to obtain the flow map induced by the motion of the sensor (as shown in Equation 3). Thus, we derive the coarse dynamic mask ${\\Omega_1, \\Omega_2, ..., \\Omega_T\\}$ for each frame by simply averaging the corresponding masks across all frame pairs.\nNext, we incorporate the pretrained SAM2 [34] model for segmentation to further refine the coarse dynamic masks. To illustrate, for each mask at frame t, we first binarize ${\\Omega_t}$ and feed each connected mask region along with the original image into SAM2, using the segmentation output to augment the binary coarse mask. This helps to fill in the missing parts of the initial mask, thereby ensuring comprehensive coverage of the dynamic objects. Finally, we obtain the refined masks of dynamic objects ${\\Omega_1, \\Omega_2, ..., \\Omega_T\\}$. The overall process of 4D flow predictor can be formulated as:\n$E_{12} = K_{i_2}^{(e)}T_{i_2}^{(e)} - p_{i_2}^{(e)} - K_{i_1}^{(e)}(T_{i_1}^{(e)})^{-1}p_{i_1}^{(e)},$\n$T_{ij}^{(e)} = PoseEstimate(P_{i_1}), j = 1,2.$\n$\\Omega_t = \\frac{1}{S} (\\sum_{21=t}||F_{12} - E_{12}|| + \\sum_{12=t}||F_{21} - E_{21}||),$\n$\\Omega_t = S(B(\\Omega), I_t) + B(\\Omega),$\nwhere $PoseEstimate(\u00b7)$ denotes the estimation of camera extrinsics and intrinsics from the given point map as described in DUSt3R . S(\u00b7) and B(\u00b7) stand for SAM2 augmentation and binary operation, respectively."}, {"title": "3.4. Multi-view Aligner", "content": "After regressing per-frame point cloud predictions from multi-view input sequences, pointmaps from multiple sensors, denoted as $\\{P_{t,c}, t \\in T,c = 1,2,...,C'\\}$, are initially represented within their respective coordinate systems. Therefore, we employ a multi-view aligner to align these point maps into the global world coordinate system in an optimization-free manner. Specifically, we first obtain the camera parameters through pose estimation for each frame within the respective coordinate system of its input sequence, which allows us to project the predicted point maps into per-frame depth maps. Therefore, each depth"}, {"title": "3.5. Training of Driv3R", "content": "Loss. Due to the sparsity of point clouds obtained from LiDAR in autonomous driving, we use the dense depth estimation results from the pretrained R3D3 [35] model as supervision. After the warm-up training steps, we supervise only the points aligned with dynamic objects, which are identified by the 4d flow predictor in Section 3.3, to calculate the confidence-aware loss (as shown in Equation 6). Following DUSt3R [43], both the predicted and \"ground truth\" point maps from R3D3 prediction are normalized by average distances. Furthermore, we also add a scale loss to encourage the scale of predicted point maps to be smaller than the predicted ones from the pretrained R3D3 model.\n$L = L_{confidence}^{dynamic} - L_{scale}^{dynamic}$\nTwo-stage Training. Our Driv3R model is trained in two stages to address the limitations of training memory. In the first stage, the input sequences consist of images from a single sensor, which means the memory pool is used solely for temporal information interactions. In the second stage, the input sequences consist of images from different sensors and timestamps with overlapping fields of view, which allows the memory pool to further reason about spatial relations and ensures the output pointmaps are spatially consistent. Finally, we adopt the optimization-free multi-view aligner to formulate the complete 4D dense point cloud predictions in the global world coordinate system."}, {"title": "4. Experiments", "content": "The nuScenes [4] dataset consists of 1,000 diverse driving scenes, each lasting approximately 20 seconds with keyframes annotated at a frequency of 2Hz. The 1,000 scenes in nuScenes are officially split into 700 for training, 150 for validation, and 150 for testing. Each keyframe contains RGB cameras from six surrounding cameras and a sparse point cloud collected from lidar. We divide each sample into sequences in the temporal order and group consecutive sets of 5 frames as inputs to Driv3R. We only use multi-view camera data during training and inference and adopt the pretrained R3D3 [35] model to generate noisy depth supervision."}, {"title": "4.2. Implementation Details", "content": "Given that the initial resolution of images in nuScenes [4] is 1600x900, we split each camera into two virtual cameras with a resolution of 224x224, so that we can leverage the pretrained memory encoder in Spann3R [41] and ensure the multi-view inputs are able to cover the entire panoramic fields of view. Our Driv3R model is trained on 8 NVIDIA A6000 GPUs with a batch size of 4. As described in Section 3.5, our model is trained in two stages to address memory limitations. We first train Driv3R for 30 epochs using temporal sequences. Then, we fine-tune the model for 20 additional epochs with a small learning rate, allowing the memory pool to further reason spatial relationships in sequences composed of multi-view images from different timestamps. For fair comparisons, we fine-tune all baseline models not trained on nuScenes [4] for the same number of epochs using predicted points from R3D3 [35] as supervision. We provide more details in Sec A.1."}, {"title": "4.3. Evaluation Metrics", "content": "Following previous work in dense 3D reconstruction [1, 41], we use accuracy, completion, and normal consistency to evaluate the quality of reconstruction. Additionally, we also calculate the standard metrics to assess the quality of depth estimation, including Abs Rel, Sq Rel, RMSE, and the percentage of prediction with \u03b4 < 1.25, \u03b4 < 1.252 and \u03b4 < 1.253. The predicted point maps and ground-truth LiDAR point clouds are transformed into a consistent global coordinate system for the assessment of reconstruction quality, and into respective sensor coordinate systems for the evaluation on depth predictions."}, {"title": "4.4. Results and Analysis", "content": "Depth Estimation. Table 1 shows that our Driv3R achieves competitive results on nuScenes [4] compared to both multi-view depth estimation frameworks and methods that directly regress per-frame pointmap . Notably, the visual results in Figure 4 demonstrate that the spatial memory pool introduced in Spann3R [41] is not capable of fully capturing the temporal relationships within the input sequences, thus can cause significant blur in the reconstruction areas of fast-moving dynamic objects. Additionally, while MonST3R [52] can achieve more delicate depth estimation in some scenarios, its global alignment process is both computationally expensive (as discussed in Section 4.4) and highly sensitive to the accuracy of dynamic masks. In contrast, Driv3R leverages the strengths of Spann3R in static reconstruction and R3D3 [35] in depth estimation of dynamic objects, therefore is more efficient and robust for 4D dynamic reconstruction.\nDynamic Objects and Scene Reconstruction. To highlight the ability of Driv3R to accurately reconstruct dynamic scenes, we sample 3,508 input sequences from the nuScenes [4] validation set, which contain a higher number of dynamic objects identified by the 4d flow predictor. As shown in Table 2, Driv3R outperforms all previous methods in both reconstruction and depth estimation on the NuScenes dynamic subset. Notably, our model even achieves slightly better reconstruction results on dynamic scenes compared to MonST3R [52], which requires global alignment with flow optimization, while performing inference far more efficiently without the optimization. Visualization results further demonstrate that the point clouds of fast-moving objects predicted by Spann3R [41] often suffer from incompleteness, blurring, and inaccuracy. In comparison, our Driv3R reconstructs the dynamic regions of fast-moving objects more accurately due to the temporal interaction with the memory pool and the guidance of the dynamic masks from the 4D flow predictor. Furthermore, the 4D global point cloud generated by the optimization-free multi-view aligner shown in Figure 5 maintains strong 3D consistency as the encoded features from different viewpoints can share spatial information within the memory pool. Due to the point maps from Driv3R still containing floaters on the edges, we only retain the regions with higher confidence and align them to the global coordinate system via the optimization-free multi-view aligner for visualization.\nEfficiency Analysis. We investigate the inference efficiency of Driv3R in comparison to dominant methods that regress per-frame point maps or depth maps from input images. The input resolution is set to 224x224 for DUSt3R, MonST3R, Spann3R and Driv3R, while For R3D3 , FSM , and SurroundDepth , we evaluate the models at their respective required resolutions. All models are evaluated on a single A6000 GPU with a batch size of 1. To assess inference FPS, we use image sequences from a single sensor as input and also account for the time consumed by global alignment when evaluating inference time for DUSt3R and MonST3R. Additionally, we set the input sequence length to 5 and evaluate the total reconstruction time t4d and memory required to reconstruct the complete 4D scene from multi-view images. For DUSt3R, MonST3R, and Spann3R, we simply merge point maps from different sensors to assess the inference time for 4D reconstruction."}, {"title": "Ablation of modules", "content": "To further investigate the designs of Driv3R, we conduct ablations on the nuScenes [4] dynamic subset. We begin with a vanilla model without the temporal-spatial memory pool or the 4D flow predictor. Next, we introduce temporal feature interactions within the memory pool. Furthermore, the memory pool is extended, and our model performs both temporal interactions across input sequences and spatial interactions between different"}, {"title": "5. Conclusion", "content": "We have presented Driv3R to learn dense 4D reconstruction on dynamic scenes for autonomous driving. Our key innovation is a memory pool that reasons both temporal relationships across sequences and spatial contexts across viewpoints. We also use a 4D Flow Predictor to identify moving objects, guiding the network to focus on dynamic regions. With an optimization-free multi-view aligner, Driv3R generates consistent 4D point maps in the global coordinate system. On the large-scale NuScenes [4] dataset, Driv3R outperforms existing methods in depth estimation and scene reconstruction, achieving 15\u00d7 higher inference speed than methods relying on global alignment.\nLimitations. Although Driv3R efficiently reconstructs large-scale dynamic scenes, the input length is constrained by memory. During training, it requires about 10 GB of memory for a 5-frame sequence from 6 multi-view cameras, mainly due to memory pool storage. Furthermore, using sparse LiDAR points for dynamic objects as supervision does not yield optimal results, and point predictions from the R3D3 [35] pre-trained model can be inaccurate in some cases. Future work can focus on improving memory storage and adapting to the fully self-supervised training."}, {"title": "A. Additional Implementation Details", "content": null}, {"title": "A.1. Training Configuration", "content": "The comprehensive configuration for the two-stage training of Driv3R is shown in Table 5. Based on Spann3R [41] pretrained weight, we first train Driv3R for 30 epochs using only temporal sequences as input. Next, we fine-tune the model for 20 more epochs with a smaller learning rate. In this stage, temporal sequences from multiple sensors are adopted as input, allowing the memory pool to further refine its reasoning of spatial contexts. For all the baseline models that have not been trained on nuScenes [4] dataset, we fine-tune their pretrained model with the same training configuration for a fair comparison."}, {"title": "A.2. Training Loss", "content": "As illustrated before, the training loss is a combination of confidence loss and scale loss. Following DUSt3R [43], the confidence loss is defined as:\n$L_{conf} = \\sum_c \\sum_t \\sum_{i \\in M_{t,c}} C_{i,c} L_{reg} (i) \u2013 \u03b1 log C_{i,c}$,\n$L_{scale} = max(0, X(M_{t,c}) \u2013 X_{r3d3}(M_{t,c}))$,\nwhere $M_{t,c}$ represents the dynamic mask of timestamp t and sensor c generated from the 4D Flow Predictor, while \u03b1 controls the overall range of confidence score and is set to 0.5 during the training. Additionally, the scale loss is formulated as Equation 8, which guides our Driv3R model to predict points of dynamic objects within a narrower range compared to the \"ground truth\" predictions from the R3D3 [35] model, thereby preventing the predictions of fast-moving objects from becoming unstable."}, {"title": "A.3. Model Architecture", "content": "In this section, we further elaborate on the implementation details of the temporal-spatial memory pool in Driv3R. Different from Spann3R [41], we maintain a separate memory pool for each sensor where key-value pairs are stored in chronological order based on their timestamps. During the first stage of training, the features of the input sequences are updated using only the memory pool corresponding to their respective sensor. However, during the spatial training stage, we first pre-define the frames from the previous k timestamps (set to 4 in our experiment) that overlap with the field of view of the current frame as the most closely related frames. Then, we apply cross-attention to update the features of the current frame using only these closely related frames to address the limitations of training memory.\nFurthermore, we maintain working memory and long-term memory for each pool inspired by Spann3R [41]. The working memory consists of the most recent 5 frames during the first training stage, and the most similar 5 frames during the second spatial training stage. For long-term memory, we apply memory pruning based on the accumulated attention weights, preventing the memory from exceeding its limit during large-scale training and inference."}, {"title": "B. Additional Results and Visualization", "content": "We present more visualization results of the depth maps and 4D global point clouds with corresponding quantitative results for Driv3R, Spann3R [41], MonST3R [52], and R3D3 [35] on the large-scale nuScenes [4] dataset. As illustrated in Figure 6, our Driv3R effectively integrates the strengths of R3D3 [35] for dynamic object prediction and Spann3R [41] for static background modeling, achieving comparable depth prediction while offering a 15x faster inference speed compared to MonST3R [52]. Notably, our model outperforms MonST3R in several scenarios due to its robustness in handling the inaccuracy of dynamic masks. Furthermore, we provide more visualizations of local and global 4D reconstruction in Figure 7, where Driv3R achieves better representations of fast-moving objects and ensures both temporal and spatial consistency."}, {"title": "C. Additional Discussion and Future works", "content": "Although Driv3R outperforms previous methods in the 4D reconstruction of large-scale dynamic scenes, the point predictions still exhibit floating artifacts on the edges, and there are several failure cases involving both fast-moving objects and the background. These limitations primarily arise from the accuracy constraints of the point predictions in the R3D3 [35] model, as well as the lack of high-quality and continuous data sequences containing a large number of dynamic and fast-moving objects. Due to the lack of ground-truth dense point clouds for autonomous driving and inaccurate depth prediction of existing methods, our future research will focus on adapting our model to an efficient self-supervised approach, such as utilizing reprojection loss and geometry consistency loss. Furthermore, to address the issue of data scarcity, a more efficient data sampling strategy should be adopted instead of splitting sequence data based on temporal order. Moreover, future work could involve joint training strategies on multiple large-scale datasets, even those outside the context of autonomous driving, to further improve the model performance and robustness."}]}