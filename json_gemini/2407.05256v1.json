{"title": "Unlocking Textual and Visual Wisdom:\nOpen-Vocabulary 3D Object Detection Enhanced\nby Comprehensive Guidance from Text and Image", "authors": ["Pengkun Jiao", "Na Zhao", "Jingjing Chen", "Yu-Gang Jiang"], "abstract": "Open-vocabulary 3D object detection (OV-3DDet) aims to\nlocalize and recognize both seen and previously unseen object categories\nwithin any new 3D scene. While language and vision foundation models\nhave achieved success in handling various open-vocabulary tasks with\nabundant training data, OV-3DDet faces a significant challenge due to\nthe limited availability of training data. Although some pioneering ef-\nforts have integrated vision-language models (VLM) knowledge into OV-\n3DDet learning, the full potential of these foundational models has yet\nto be fully exploited. In this paper, we unlock the textual and visual wis-\ndom to tackle the open-vocabulary 3D detection task by leveraging the\nlanguage and vision foundation models. We leverage a vision foundation\nmodel to provide image-wise guidance for discovering novel classes in 3D\nscenes. Specifically, we utilize a object detection vision foundation model\nto enable the zero-shot discovery of objects in images, which serves as the\ninitial seeds and filtering guidance to identify novel 3D objects. Addition-\nally, to align the 3D space with the powerful vision-language space, we\nintroduce a hierarchical alignment approach, where the 3D feature space\nis aligned with the vision-language feature space using a pre-trained VLM\nat the instance, category, and scene levels. Through extensive experimen-\ntation, we demonstrate significant improvements in accuracy and gener-\nalization, highlighting the potential of foundation models in advancing\nopen-vocabulary 3D object detection in real-world scenarios.", "sections": [{"title": "1 Introduction", "content": "3D object detection serves as a fundamental component in understanding 3D\nscenes, playing a pivotal role in various applications [1, 5, 13, 21] such as au-\ntonomous driving and robot interaction. However, conventional approaches to\n3D object detection [14, 15, 22, 24, 28] often operate under the assumption that\nthe detection targets during testing remain consistent with those observed dur-\ning training. This assumption fails to reflect the dynamic and evolving nature\nof real-world scenarios, where the objects within scenes can vary and expand\nover time. Consequently, the capability of open-vocabulary 3D object detection,\nwhich enables the localization and recognition of both seen and previously un-\nseen objects within new scenes, becomes essential for their practical deployment\nin real-world settings.\nTo achieve open-vocabulary capacity, image-based methods [7, 8, 23] lever-\nage internet-scale image-text pairs to train a unified feature alignment space.\nIn contrast to significant achievements in its 2D counterpart, open-vocabulary\n3D object detection (OV-3DDet) [2, 12, 29] faces critical challenge due to the\nscarcity of training data, which impedes the 3D detection models from effectively\nacquiring the ability for open-vocabulary inference. Fortunately, the success of\nlarge language and vision foundation models [7, 9, 20, 27], such as vision-language\nmodels (VLMs) and large language models (LLMs), holds promise for benefiting\n3D open-vocabulary learning. Several previous works [2, 12, 26, 30] have demon-\nstrated the feasibility of leveraging VLMs by using images as a medium to align\ntext and images with 3D space for open-vocabulary learning in the 3D domain.\nFor example, OV3DET [12] employs Detic [27] to generate 2D bounding boxes\n(bboxes), which are then back-projected to 3D space to generate 3D bboxes. It\naligns the 3D-image-text feature space using CLIP [23] through category-level\ncontrastive learning. Another recent work, CoDA [2], also leverages CLIP but\nto provide semantic priors for selecting novel 3D objects from the class-agnostic\n3D object detector. CoDA aligns 3D features to image features at the instance\nlevel and 3D features to text features at the category level. Additionally, in-\nstead of using images as a medium, L3DET [29] directly augments 3D scenes\nby injecting novel objects from external object-level datasets into the scenes. It\naligns 3D features to text features extracted from LLMs (i.e. RoBERTa [10])\nvia category-level contrastive learning.\nDespite the efforts of prior works, they have not fully capitalized on foun-\ndational models or effectively integrated valuable 3D information with these\nmodels. For instance, L3DET overlooks the utilization of VLMs, which excel\nin zero-shot tasks and exhibit a smaller domain gap with 3D data compared\nto LLMs. Furthermore, its simplistic injection augmentation approach may lead\nto unrealistic scenes with incongruous contextual information, thereby under-\nmining the effectiveness of 3D object detection. Similarly, CoDA only employs\nVLMs as a prior for filtering novel 3D objects, relying solely on a class-agnostic\n3D object detector trained on available annotations for 3D object discovery.\nConsequently, the discovery of 3D novel objects is constrained by the 3D in-\nputs, making it challenging to detect classes with small size, sparse density, or\ninsignificant structures. On the other hand, OV3DET heavily relies on vision\nfoundation model and overlooks the valuable 3D inputs, which could provide\nrich geometry clues for 3D object discovery. Moreover, these methods predomi-\nnantly focus on feature alignment either at the instance level or category level,\nneglecting to align the feature space comprehensively.\nTo address these limitations, we propose a novel Image-guided Novel class\ndiscovery and Hierarchical feature space Alignment (INHA) approach. INHA\nleverages foundation models to unlock the full potential of text and image in-\nformation for open-vocabulary 3D object detection. Figure 1 illustrates the two\nkey components of INHA. In image-guided novel object discovery (Figure 1a),\nwe harness the power of vision foundation models to search and select 3D novel\nobjects. Specifically, we use an off-the-shelf open-vocabulary 2D detector to lo-\ncate 2D objects in the image. For detected 2D objects, we utilize their centroids\nas initial seeds to generate additional 3D object proposals and leverage their\n2D bounding boxes to select reliable novel objects. This approach leads to im-\nprovement in the recall of 3D novel object discovery, as demonstrated in Fig-\nure 6. The discovered novel objects are combined with the given base objects\nto retrain the 3D detector, enhancing its class-agnostic 3D detection capability.\nAdditionally, we introduce a hierarchical feature space alignment mechanism,\naligning the 3D feature space with the vision-language feature space at instance-\nlevel, category-level, and scene-level, as shown in Figure 1b. The incorporation\nof scene-level alignment is to capture the occurrence relation of classes across\nmodalities. Since each scene contains a set of objects, direct comparison of this\nset to a text description of the scene is feasible. To accomplish this, we design\na Permutation-Invariant Scene feature Extraction (PISE) module to extract 3D\nscene features and align them with the embedding of the scene description text.\nOur main contributions can be summarized as follows:\nWe propose a novel framework named INHA that exploits comprehensive\nguidance from text and images through language and vision foundation mod-\nels, enhancing 3D open-vocabulary learning capacity.\nWe introduce an image-guided novel object discovery (IGND) mechanism to\neffectively integrate valuable 3D information with image information from\nvision foundation models, facilitating the discovery of more 3D novel objects.\nWe design a permutation-invariant scene feature extraction (PISE) module\nto encode class occurrence relations in a scene. Additionally, we present hier-\narchical alignment of the 3D feature space with the vision-language feature\nspace at instance, category, and scene levels.\nExtensive experiments demonstrate the effectiveness of our proposed method.\nOur method achieves state-of-the-art performance in open-vocabulary 3D\nobject detection on two benchmark datasets, SUN RGB-D and ScanNetv2."}, {"title": "2 Related Work", "content": "3D Object Detection endeavors to locate and identify 3D objects within\na given scene, with numerous approaches proposed to address this challenge.\nVoteNet [16", "17": "for pro-"}]}