{"title": "Unlocking Textual and Visual Wisdom: Open-Vocabulary 3D Object Detection Enhanced by Comprehensive Guidance from Text and Image", "authors": ["Pengkun Jiao", "Na Zhao", "Jingjing Chen", "Yu-Gang Jiang"], "abstract": "Open-vocabulary 3D object detection (OV-3DDet) aims to localize and recognize both seen and previously unseen object categories within any new 3D scene. While language and vision foundation models have achieved success in handling various open-vocabulary tasks with abundant training data, OV-3DDet faces a significant challenge due to the limited availability of training data. Although some pioneering efforts have integrated vision-language models (VLM) knowledge into OV-3DDet learning, the full potential of these foundational models has yet to be fully exploited. In this paper, we unlock the textual and visual wisdom to tackle the open-vocabulary 3D detection task by leveraging the language and vision foundation models. We leverage a vision foundation model to provide image-wise guidance for discovering novel classes in 3D scenes. Specifically, we utilize a object detection vision foundation model to enable the zero-shot discovery of objects in images, which serves as the initial seeds and filtering guidance to identify novel 3D objects. Additionally, to align the 3D space with the powerful vision-language space, we introduce a hierarchical alignment approach, where the 3D feature space is aligned with the vision-language feature space using a pre-trained VLM at the instance, category, and scene levels. Through extensive experimentation, we demonstrate significant improvements in accuracy and generalization, highlighting the potential of foundation models in advancing open-vocabulary 3D object detection in real-world scenarios.", "sections": [{"title": "1 Introduction", "content": "3D object detection serves as a fundamental component in understanding 3D scenes, playing a pivotal role in various applications [1, 5, 13, 21] such as autonomous driving and robot interaction. However, conventional approaches to 3D object detection [14, 15, 22, 24, 28] often operate under the assumption that the detection targets during testing remain consistent with those observed during training. This assumption fails to reflect the dynamic and evolving nature of real-world scenarios, where the objects within scenes can vary and expand over time. Consequently, the capability of open-vocabulary 3D object detection, which enables the localization and recognition of both seen and previously unseen objects within new scenes, becomes essential for their practical deployment in real-world settings.\nTo achieve open-vocabulary capacity, image-based methods [7, 8, 23] leverage internet-scale image-text pairs to train a unified feature alignment space. In contrast to significant achievements in its 2D counterpart, open-vocabulary 3D object detection (OV-3DDet) [2, 12, 29] faces critical challenge due to the scarcity of training data, which impedes the 3D detection models from effectively acquiring the ability for open-vocabulary inference. Fortunately, the success of large language and vision foundation models [7, 9, 20, 27], such as vision-language models (VLMs) and large language models (LLMs), holds promise for benefiting 3D open-vocabulary learning. Several previous works [2, 12, 26, 30] have demonstrated the feasibility of leveraging VLMs by using images as a medium to align text and images with 3D space for open-vocabulary learning in the 3D domain. For example, OV3DET [12] employs Detic [27] to generate 2D bounding boxes (bboxes), which are then back-projected to 3D space to generate 3D bboxes. It aligns the 3D-image-text feature space using CLIP [23] through category-level contrastive learning. Another recent work, CoDA [2], also leverages CLIP but to provide semantic priors for selecting novel 3D objects from the class-agnostic 3D object detector. CoDA aligns 3D features to image features at the instance level and 3D features to text features at the category level. Additionally, instead of using images as a medium, L3DET [29] directly augments 3D scenes by injecting novel objects from external object-level datasets into the scenes. It aligns 3D features to text features extracted from LLMs (i.e. RoBERTa [10]) via category-level contrastive learning.\nDespite the efforts of prior works, they have not fully capitalized on foundational models or effectively integrated valuable 3D information with these models. For instance, L3DET overlooks the utilization of VLMs, which excel in zero-shot tasks and exhibit a smaller domain gap with 3D data compared to LLMs. Furthermore, its simplistic injection augmentation approach may lead to unrealistic scenes with incongruous contextual information, thereby undermining the effectiveness of 3D object detection. Similarly, CoDA only employs VLMs as a prior for filtering novel 3D objects, relying solely on a class-agnostic 3D object detector trained on available annotations for 3D object discovery. Consequently, the discovery of 3D novel objects is constrained by the 3D inputs, making it challenging to detect classes with small size, sparse density, or insignificant structures. On the other hand, OV3DET heavily relies on vision foundation model and overlooks the valuable 3D inputs, which could provide rich geometry clues for 3D object discovery. Moreover, these methods predominantly focus on feature alignment either at the instance level or category level, neglecting to align the feature space comprehensively.\nTo address these limitations, we propose a novel Image-guided Novel class discovery and Hierarchical feature space Alignment (INHA) approach. INHA leverages foundation models to unlock the full potential of text and image information for open-vocabulary 3D object detection. Figure 1 illustrates the two key components of INHA. In image-guided novel object discovery (Figure 1a), we harness the power of vision foundation models to search and select 3D novel objects. Specifically, we use an off-the-shelf open-vocabulary 2D detector to locate 2D objects in the image. For detected 2D objects, we utilize their centroids as initial seeds to generate additional 3D object proposals and leverage their 2D bounding boxes to select reliable novel objects. This approach leads to improvement in the recall of 3D novel object discovery, as demonstrated in Figure 6. The discovered novel objects are combined with the given base objects to retrain the 3D detector, enhancing its class-agnostic 3D detection capability. Additionally, we introduce a hierarchical feature space alignment mechanism, aligning the 3D feature space with the vision-language feature space at instance-level, category-level, and scene-level, as shown in Figure 1b. The incorporation of scene-level alignment is to capture the occurrence relation of classes across modalities. Since each scene contains a set of objects, direct comparison of this set to a text description of the scene is feasible. To accomplish this, we design a Permutation-Invariant Scene feature Extraction (PISE) module to extract 3D scene features and align them with the embedding of the scene description text.\nOur main contributions can be summarized as follows:\nWe propose a novel framework named INHA that exploits comprehensive guidance from text and images through language and vision foundation models, enhancing 3D open-vocabulary learning capacity.\nWe introduce an image-guided novel object discovery (IGND) mechanism to effectively integrate valuable 3D information with image information from vision foundation models, facilitating the discovery of more 3D novel objects.\nWe design a permutation-invariant scene feature extraction (PISE) module to encode class occurrence relations in a scene. Additionally, we present hierarchical alignment of the 3D feature space with the vision-language feature space at instance, category, and scene levels.\nExtensive experiments demonstrate the effectiveness of our proposed method. Our method achieves state-of-the-art performance in open-vocabulary 3D object detection on two benchmark datasets, SUN RGB-D and ScanNetv2."}, {"title": "2 Related Work", "content": "3D Object Detection endeavors to locate and identify 3D objects within a given scene, with numerous approaches proposed to address this challenge. VoteNet [16] introduced a point voting strategy, leveraging PointNet [17] for processing 3D points. Evolving from VoteNet, MLCVNet [19] introduces additional modules for point voting, aiming to capture multi-level contextual information and enhance overall detection performance. The advent of transformer-based methods has significantly shaped the landscape of 3D object detection. Notably, GroupFree [11] utilizes a transformer as the prediction head, eliminating the need for manually crafted grouping and harnessing the transformer's capabilities to improve detection accuracy. In recent years, the trend has shifted towards end-to-end models, with 3DETR [14] standing out as the pioneer in employing an end-to-end transformer architecture for 3D object detection, utilizes bipartite graph matching to establish associations between predictions and ground truth. However, traditional 3D object detection methods only detect objects seen during the training stage and can't handle the open-vocabulary scenario.\nOpen-vocabulary Object Detection aims to detect objects that include both seen and unseen classes in an image. Since the success of CLIP in associating language and the 2D domain, research on zero-shot learning and open-vocabulary has become a trend. For example, Detic [27] uses ImageNet to train the classifiers of detectors, expanding the vocabulary of detectors to tens of thousands of concepts. GLIP [8] and MDETR [6] treat detection as the grounding task and adopt a text query for the input image to predict corresponding boxes. GLIPv2 [25] reformulates the detection task, introduces a novel region-word level contrastive learning task, and includes masked language modeling. The success of open-vocabulary 2D detection methods provides the potential to facilitate OV-3DDet. Our method also leverages a pre-trained language-vision model to provide guidance for 3D detection.\nOpen-vocabulary 3D Object Detection (OV-3DDet) presents a substantial challenge, aiming to detect and locate objects in a 3D space, encompassing both known and unknown object categories. In OV3DET [12], a 2D detector is employed to generate pseudo-labels. These labels are utilized for training a class-agnostic detector, followed by a contrastive learning step that aligns the 3D feature space with image and language features. L3DET [29] contributes to the field by enriching 3D scene datasets through the introduction of novel objects and associated text descriptions. It leverages cross-domain, category-level contrastive learning to align feature spaces between point clouds and language, facilitating effective cross-modal reasoning. CoDA [2] takes a distinctive approach by integrating 3D box geometry priors and 2D semantic open-vocabulary priors for novel object identification. Discovered novel objects contribute to subsequent detector training, aligning the 3D feature space with the vision-language feature space through class-agnostic knowledge distillation and class-aware contrastive learning. However, CoDA exhibits inaccuracies and insufficiencies in novel class discovery and feature alignment. While PLA [4] introduces alignment at the instance, category, and scene levels, its primary focus lies within the segmentation setting, resulting in a higher cost at the point-wise level rather than the object-wise level. In response to these challenges, we propose an alternative approach."}, {"title": "3 Method", "content": "3.1 Problem Definition\nIn OV-3DDet, we are given the point cloud of a scene, denoted as \\(P = \\{p_i \\in \\mathbb{R}^3\\}_{i=1}^N\\), as well as the associated image of the scene, denoted as \\(I \\in \\mathbb{R}^{3 \times H \times W}\\). For each point cloud, the objects present in the scene are denoted as \\(O_{3D} = \\{(B_n, C_n)\\}_{n=1}^N\\) where \\(B_n \\in \\mathbb{R}^7\\) represents the 3D bounding box parameters, including the center, size, and heading angle, and \\(c_n \\in \\mathcal{C}\\) represents the category of the object. Among the objects in a scene, some are initially annotated with labels, denoted as base objects \\(O_{3D}^{base} = \\{(B_o^{base}, C_o^{base})\\}\\), \\(o\\) belongs to the base label space \\(\\mathcal{C}^B\\), while the remaining unlabeled objects are 3D novel objects \\(O_{3D}^{novel} = \\{(B_v^{novel}, C_v^{novel})\\}\\), where \\(v\\) belongs to the novel label space \\(\\mathcal{C}^N\\), and \\(\\mathcal{C}^B + \\mathcal{C}^N = \\mathcal{C}\\). Our objective is to train an open-vocabulary 3D object detector capable of localizing and recognizing both base and novel objects in any new point cloud.\n3.2 INHA Overview\nOur proposed Image-guided Novel class discovery and Hierarchical feature space Alignment (INHA) approach adopts 3DETR [14] as our 3D object detector, which comprises PointNet++, Transformer encoder, Transformer decoder, a bounding box regression head, and a classification head. The training process consists of three stages, illustrated in Figure 2. In the first stage, we train a base class-agnostic 3D object detector. More specifically, we remove the classification-related training and solely utilize bounding box regression loss \\(L_{box}\\) [14] for training the detector. After the first stage, the detector is capable of detecting objects in a point cloud without considering their class information. In the second stage, we incorporate both the 2D detector and the 3D detector to discover novel objects (cf. Sec. 3.3). The discovered objects are stored and then used for further training the 3D detector. This process enhances the 3D detector's ability to handle novel classes. In the final stage, we introduce a hierarchical paradigm (cf. Sec. 3.4) that aligns the feature space at instance, class, and scene levels."}, {"title": "3.3 Image-Guided Novel Class Discovery", "content": "The image provides rich appearance cues that aid in identifying cluttered objects. Leveraging the capabilities of recent open-vocabulary 2D object detectors, even extremely small or occluded novel objects can be identified in images. For instance, a distant object can be easily discerned by a 2D detector using only a few pixels. In contrast, 3D object detectors struggle to recognize distant objects with very few points. However, 3D object detectors excel in capturing rich 3D geometrical information, resulting in more precise localization predictions. Recognizing the complementary nature of these modalities, we present the Image-Guided Novel Class Discovery (IGND) module, illustrated in Figure 3, to discover more 3D novel objects. In this module, we utilize a pre-trained open-vocabulary 2D detector, i.e. Detic [27], to extract valuable object-level information (2D object bboxes) from images. This information is then effectively integrated with valuable 3D data to guide the discovery of 3D novel objects. Specifically, this guidance occurs in two key steps: a) lifting the centroids of the 2D objects to 3D space to provide supplementary query seeds for generating more 3D object proposals, and b) utilizing the bounding boxes of the 2D objects to select reliable novel 3D bounding boxes. Through these two guided processes, we enhance the recall rates of novel classes, as depicted in Figure 6.\nImage-guided Query Seed Initialization. The transformer-based 3D detector [12, 14] typically employs position encoding of multiple points as initial query seeds to propose 3D objects, which are conversely sampled by object-agnostic sampling algorithms, e.g. random sampling or the farthest point sampling [18]. The quality of query seeds significantly influences the effectiveness of novel object proposals. Given that 2D detectors can reliably identify novel objects, we elevate the centers of 2D objects to 3D space to obtain supplementary query seeds. Let \\(O_{2D} = \\{(b_m, c_m)\\}_{m=1}^M\\) denote the detected M 2D objects in an image, where \\(b_m \\in \\mathbb{R}^4\\) and \\(c_m \\in \\mathcal{C}\\). We lift the centers of 2D bounding boxes to 3D space. These lifted 3D points are then encoded into position embeddings [6, 14] and added to query seeds, which can be used to facilitate novel object proposals within the Transformer decoder framework [14].\nImage-guided Novel Object Selection. The geometrical correlation in a scene between 3D point clouds and paired 2D images provides a bridge to associate 2D detectors and 3D detectors. We first project the predicted 3D objects B to the 2D image coordinate. The projected boxes are denoted as \\(\\hat{b}\\) and can be obtained using camera parameters. Then we select the novel objects with the guidance from 2D objects. For each 2D box \\(b_m\\), we match it with the projected box \\(\\hat{b}\\) that has the highest overlapping area. We then filter out matched samples that have an overlap area less than a threshold, or those that are included in the base objects. Let \\(\\mathcal{N}_{ij} = n(b_i, b_j)\\) denote the Intersection over Union (IoU) [14] between two 2D boxes, we select novel objects as:\n\\[\\mathcal{B}^{N} = \begin{cases}  b_i | n(b_m, \\hat{b}_i) \\geq \\epsilon \\frac{max_{j} n(b_m, b_j)}{\\mathcal{N}_{mj}}  , \\mathcal{N}_{mi} \\geq \\epsilon  , c_m \\in \\mathcal{C}^{N} \\       \\hat{b_i} = arg max_{j} n(b_m, b_j),   m = 1, ..., M  \\end{cases}\\]     (1)\nwhere \\(\\epsilon\\) is a threshold used to filter out cases with low IoU. We then match the corresponding 3D objects based on these selected 2D boxes. These 3D boxes are stored in the novel object memory bank and are subsequently used to retrain the"}, {"title": "3.4 Hierarchical Cross-modal Feature Alignment", "content": "Pretrained large vision-language models (VLMs) have demonstrated remarkable success, showcasing powerful feature representation and generalization capabilities. Consequently, we align the 3D feature space with the pretrained vision-language model in a hierarchical design. Specifically, this alignment takes place at three levels: instance, category, and scene levels. We will elaborate on each level below.\nInstance-level 3D-Image Alignment. Building upon that image and point cloud have a natural correlation in geometry information, e.g. shape, we directly associate 3D object features and pair 2D object features at instance level. Let \\(f^{3D}\\) denote the 3D object feature, and \\(f^{2D}\\) denote the corresponding cropped image feature generated from the VLM, i.e. CLIP [20]. We mitigate the distance between \\(f^{3D}\\) and \\(f^{2D}\\) by using L1-norm loss:\n\\[\\mathcal{L}^{3d \rightarrow rgb}_{ins} = \\Arrowvert f^{3D} - f^{2D}\\Arrowvert_1\\]      (2)\nThe alignment at the instance level emphasizes the consistency in 3D features and image features. While not considering general class information in the language domain yet.\nClass-level Cross-modal Alignment. We further align the 3D feature with the vision-language feature at the category level. Inspired by [12], we categorize features from the three modalities by their class and use contrastive learning to bring together features of the same class while pushing apart those of different classes. In this arrangement, we use \\(\\{g_i^S\\}_{i=1}^S\\), to represent the set of S features for all modalities (i.e. point cloud, image and text) in a batch. The class labels for these features are denoted as \\(\\{C_i\\}_{i=1}^S\\). We construct positive pairs by using samples from the same class and negative pairs by utilizing samples from different classes, then calculate the contrastive loss:\n\\[\\mathcal{L}_{cls}^{3d \rightarrow rgb,text} = - \\frac{1}{S}  \\sum_{i=1}^S log  \\frac{\\sum_{k=1}^S 1(i=k, C_i = C_k) e^{s_i s_k / \\tau}}{ \\sum_{j=1}^S e^{s_i s_j/ \\tau}}       \\] (3)\nHere, \\(\tau\\) is the temperature parameter, \\(1(\\cdot)\\) is the indicator function, which yields 1 when the condition is met and 0 otherwise.\nScene-level Cross-modal Alignment. The objects within a scene often exhibit strong correlations, where certain objects are more likely to coexist. For example, in a living room, a bed is typically accompanied by a dresser, but not a refrigerator. Utilizing this correlation prior is beneficial for aligning the 3D scene-level feature space. Here, we align the 3D scene features with the language semantic feature space at the scene level. To generate the text scene feature, we first create a scene-level caption containing all class names present in the scene. This caption is then processed by the CLIP text encoder to produce the scene-level text feature \\(\\{z_{text}^i\\}_{i=1}^L\\), L signifies the number of scenes in a batch. For scene-level 3D feature extraction, we introduce the Permutation-Invariant Scene-level feature Extraction (PISE) module. This involves concatenating all individual 3D object features within a scene and projecting them into a high-dimensional space. To address permutation invariance, we utilize a max pooling operation on these high-dimensional features. The detailed architecture of the PISE module is illustrated in Figure 5. Let \\(\\{z_{3D}^i\\}_{i=1}^L\\), represent the scene-level 3D feature extracted by the PISE module. Subsequently, we employ a contrastive loss to align the 3D scene feature with the corresponding text scene feature:\n\\[\\mathcal{L}_{scene}^{3d \\rightarrow text} = -   \\frac{1}{L}   \\sum_{i=1}^L   log   \\frac{e^{z_{3D}^i z_{text}^i }}{ \\sum_{i=1}^L e^{z_{3D} z_{text}}}   \\]  (4)\nConsidering all the alignment losses above, and along with the box regression loss, the total loss used in the third stage is:\n\\[L_{align} = L_{box} + \\lambda_1 L_{ins}^{3d \\rightarrow rgb} + \\lambda_2 L_{cls}^{3d \\rightarrow rgb,text} + \\lambda_3 L_{scene}^{3d \\rightarrow text}\\] (5)"}, {"title": "4 Experiment", "content": "4.1 Datasets\nOur proposed approach is evaluated on two challenging 3D indoor detection datasets: SUN RGB-D, which consists of 5,285 training samples with oriented 3D bounding box labels for 46 object categories. We select the 10 most frequent classes as base classes and the remaining 36 most frequent classes as novel classes; ScanNetv2 [3], it has 1,201 training samples with 200 object categories. We use the 10 most frequent classes as base classes and the remaining 50 most frequent classes as novel classes. All configurations above are the same as in [2].\nAdditionally, we adapt the settings in OV3DET [12], using generated pseudo-labels trained on all classes. However, L3DET [29] has a different configuration compared to [12], where it uses 10 classes for seen class training and another 10 classes for unseen class. For a fair comparison, we select the 10 overlapping novel classes (toilet, bed, chair, sofa, dresser, table, cabinet, bookshelf, pillow, and sink) between [12] and the novel classes in [29] for validation. We denote this benchmark as ScanNet-10.\n4.2 Baselines and Evaluation Metrics\nBaselines. For benchmark datasets SUN RGB-D and ScanNet-10, we select Det-PointCLIP [26], Det-PointCLIPv2 [30], Det-CLIP [25], 3D-CLIP [20], and CODA [2] as our comparative methods [2]. For the ScanNet-10 benchmark, we compared our method with L3DET [29], OV-3DET [12], and CoDA [2].\nMetrics. Regarding validation metrics, we employ mean Average Precision (mAP) and mean Average Recall (mAR) [14], with an IoU threshold set to 0.25. Among these two metrics, mAP is our primary metric.\n4.3 Implementation Details\nTraining Strategy and Hyperparameters. Our training procedure includes 3 stages. In the first stage, i.e. base detector training, we train the 3D detector for 1000 epochs for a fair comparison with [2]. In the second stage, i.e. novel class discovery, we train for an additional 200 epochs. In the final stage, i.e. feature space alignment, we train another 50 epochs to align the 3D feature space to the vision-language feature space. For the first stage, we configure a batch size of 8 and a learning rate of 0.0004, utilizing 128 queries. Subsequently, during the second and third stages, we specifically adjust the learning rate to 0.0001, batch size to 16, and increase the query size to 196 for enhanced detector training and feature alignment. We set the temperature \\(\tau\\) to 1.0 and the threshold \\(\\epsilon\\) to 0.75. Additionally, the hyperparameters \\(\\lambda_1\\), \\(\\lambda_2\\), and \\(\\lambda_3\\) are initially set to 0.02 during warm-up and later adjusted to 1, 1, and 0.5, respectively.\nModel Selection and Prompt Setting. We modify 3DETR [14] as our 3D detector, removing the classification head and using only the bounding box regression loss. Additionally, we employ the pretrained Detic [27] as our 2D detector. For feature alignment, we leverage the pretrained CLIP [20] to encode image and text features. In the class-level and scene-level feature space alignment, we generate text prompts for language feature encoding. Specifically, for the class-level prompt, we generate feature embeddings using the template \"A photo of [class name].\", where [class name] represents the category name. For the scene-level prompt, we concatenate the class names in a scene into a list and insert the list into the template \"A room with [class list].\", where [class list] denotes the position to insert the list. Additionally, we include the non-object description prompt, i.e., \"A photo of nothing.\" to represent areas without discriminative objects.\n4.4 Main Results\nResults on SUN RGB-D. We evaluate our method and baseline methods on the SUN RGB-D dataset, and the results are shown in Table 1. As can be seen, our method outperforms all other methods on both base class and novel class in terms of both mean average precision and mean average recall. Specifically, compared to the state-of-the-art method CoDA, our method has a 30% higher performance on \\(\\text{mAP}_{Novel}\\) and a 10% higher performance on the base class \\(\\text{mAP}_{Base}\\). This highlights that our method not only finds more novel objects but also better aligns the 3D feature space to the vision-language feature space, yielding superior performance.\nResults on ScanNetv2. We also evaluate our method and baseline methods on the ScanNetv2 dataset, and the results are shown in Table 2. As can be seen, our method continually outperforms other methods on both base class and novel class. We have a 19% higher performance on novel class \\(\\text{mAP}_{Novel}\\) and a 16.4% higher performance on base class \\(\\text{mAP}_{Base}\\). All the results highlight the significant novel class discovery of our method.\n4.5 Results on ScanNet-10\nAs OV3DET [12] leverages a 2D detector to generate pseudo labels for all classes, we adopt this pseudo-labeling setting for comparison. We evaluate our methods alongside several others on the ScanNet-10 benchmark, and the results are presented in Table 3. Notably, L3DET [29] employs synthetic data to expand the novel category on top of the 10 base classes. To ensure a fair comparison between [12] and [29], we select the overlapping 10 novel classes. From the results, it is evident that our method outperforms the others and achieves the best performance in terms of mean average precision.\n4.6 Ablation Study\nEffect of Image-Guided Novel Class Discovery. To assess the effectiveness of our proposed components, we conduct an ablation experiment on the SUN RGB-D dataset, evaluating various versions of our method. The results are summarized in Table 4. The inclusion of the IGND module leads to a significant improvement in novel object discovery, resulting in higher performance on the mean average recall (\\(\\text{mAR}_{Novel}\\)) for novel classes. We visualize the changes in mean average recall and mean average precision throughout the training process, as depicted in Figure 6. It is evident that with the introduction of IGND, the mean average recall of novel classes notably increases (specifically at the epoch marked by IGND), further underscoring the effectiveness of the image-guided novel object discovery module.\nEffect of Hierarchical Cross-modal Feature Alignment. When incorporating instance-level, class-level, and scene-level feature space alignment, respectively, the mean average precision (including both \\(\\text{mAP}_{Novel}\\) and \\(\\text{mAP}_{Base}\\)) demonstrates a gradual improvement, as illustrated in Table 4. Moreover, as depicted in Figure 6, the implementation of our hierarchical multi-modality feature space alignment (initiated at the epoch beginning with Align) leads to a significant enhancement in the mean average precision on novel classes. These findings underscore the effectiveness of our proposed hierarchical design in aligning the feature spaces.\nEffect of Image-guided Query Seed Initialization. To evaluate the efficacy of the Image-Guided Query Seed module, we conduct experiments wherein the module is excluded while maintaining a constant query size of 196 on SUN RGB-D. The results are presented in Table 5. Notably, the results indicate that the absence of guided query seeds adversely impacts the mean average recall for Novel classes. These outcomes underscore the importance of integrating additional query seeds derived from 2D object centers. Such integration enables the model to identify more objects, thereby emphasizing the effectiveness of leveraging vision models to enhance 3D detection capabilities.\n4.7 Visualization\nQualitative Results of IGND. Our proposed Image-Guided Novel Class Discovery (IGND) effectively discovers novel classes, as evidenced by the high-quality results depicted in Figure 4. In the 2nd row of the figure, the fabulous results in 2D object detection from the open-vocabulary 2D detector, i.e., Detic [27], showcase the power of the vision foundation model. By effectively leveraging 2D objects detected by the vision foundation model, our IGND accurately identifies novel classes, even including objects not annotated in the ground truth. For example, in the 6th column of Figure 4, IGND discovers a garbage bin"}, {"title": "5 Conclusion", "content": "In this study, we delve into the challenging realm of open-vocabulary 3D object detection. While vision language foundation models have propelled 2D detection forward in open-vocabulary scenarios, their potential for 3D detection remains underutilized. To address this gap, we introduce a novel framework called Image-Guided Novel Object Discovery and Hierarchical Multi-Modality Feature Space Alignment framework. Specifically, we integrate 2D detection model and 3D detector to identify novel objects. Additionally, we hierarchically align the 3D features with the vision-language feature space at the instance, category, and scene levels. Our approach capitalizes the power of foundation models to extract comprehensive guidance information from both text and images, which is then effectively integrated with 3D inputs for open-vocabulary 3D object detection. Extensive experiments validate the effectiveness of our proposed methods, demonstrating the efficacy in various datasets."}]}