{"title": "DEXTRACK: TOWARDS GENERALIZABLE NEURAL\nTRACKING CONTROL FOR DEXTEROUS MANIPULA-\nTION FROM HUMAN REFERENCES", "authors": ["Xueyi Liu", "Jianibieke Adalibieke", "Qianwei Han", "Yuzhe Qin", "Li Yi"], "abstract": "We address the challenge of developing a generalizable neural tracking controller\nfor dexterous manipulation from human references. This controller aims to man-\nage a dexterous robot hand to manipulate diverse objects for various purposes\ndefined by kinematic human-object interactions. Developing such a controller is\ncomplicated by the intricate contact dynamics of dexterous manipulation and the\nneed for adaptivity, generalizability, and robustness. Current reinforcement learn-\ning and trajectory optimization methods often fall short due to their dependence\non task-specific rewards or precise system models. We introduce an approach that\ncurates large-scale successful robot tracking demonstrations, comprising pairs of\nhuman references and robot actions, to train a neural controller. Utilizing a data\nflywheel, we iteratively enhance the controller's performance, as well as the num-\nber and quality of successful tracking demonstrations. We exploit available track-\ning demonstrations and carefully integrate reinforcement learning and imitation\nlearning to boost the controller's performance in dynamic environments. At the\nsame time, to obtain high-quality tracking demonstrations, we individually op-\ntimize per-trajectory tracking by leveraging the learned tracking controller in a\nhomotopy optimization method. The homotopy optimization, mimicking chain-\nof-thought, aids in solving challenging trajectory tracking problems to increase\ndemonstration diversity. We showcase our success by training a generalizable\nneural controller and evaluating it in both simulation and real world. Our method\nachieves over a 10% improvement in success rates compared to leading baselines.\nThe project website with animated results is available at DexTrack.", "sections": [{"title": "1 INTRODUCTION", "content": "Robotic dexterous manipulation refers to the ability of a robot hand skillfully handling and manipu-\nlating objects for various target states with precision and adaptability. This capability has attracted"}, {"title": "2 RELATED WORK", "content": "Equipping robots with human-level dexterous manipulation skills is crucial for future advancements.\nPrevious approaches either rely on model-based trajectory optimization or model-free reinforce-\nment learning (RL). Model-based methods face challenges due to the complexity of dynamics, often\nrequiring approximations (Pang et al., 2023; Jin, 2024; Pang & Tedrake, 2021). Model-free ap-\nproaches, using RL (Rajeswaran et al., 2017; Chen et al., 2023; 2021; Christen et al., 2022; Zhang\net al., 2023; Qin et al., 2022; Liu et al., 2022; Wu et al., 2023; Gupta et al., 2016; Wang et al., 2023;\nMordatch et al., 2012), focus on goal-driven tasks with task-specific rewards, limiting their gener-\nalization across diverse tasks. Our work explores a general controller for dexterous manipulations.\nBesides, learning via mimicking kinematic trajectories has recently become a popular train to equip\nthe agent with various expressive skills (Jenelten et al., 2023; Luo et al., 2023b;a). DTC (Jenelten\net al., 2023) proposes a strategy that can combine the power of model-based motion planning and\nRL to overcome the sample inefficiency of RL. In the humanoid motion tracking space, PHC (Luo\net al., 2023b) proposes an effective RL-based training strategy to develop a general humanoid mo-\ntion tracker. Recently, OmniGrasp (Luo et al., 2024) proposes to train a universal grasping and\ntrajectory following policy. The policy can generalize to unseen objects as well as track novel mo-\ntions. However, their considered motions are still restricted in grasping and trajectory following,\nleaving the problem of tracking more interesting and difficult trajectories such as those with subtle\nin-hand manipulations largely not explored. In this paper, we focus on these difficult and challeng-\ning manipulations. Moreover, we are also related to recent trials on combining RL with imitation\nlearning. To overcome the sample inefficiency problem of RL and to facilitate the convergence,\nvarious approaches have been developed aiming to augment RL training with demonstrations (Sun\net al., 2018; Hester et al., 2017; Booher et al., 2024; Liu et al., 2023). In our work, we wish to\nleverage high-quality demonstrations to guide the agent's explorations. Unlike previous work where\ndemonstrations are readily available, acquiring a sufficient amount of high-quality robot tracking\ndemonstrations remains a significant challenge in our task."}, {"title": "3 METHOD", "content": "Terminologies and notations. Dexterous manipulation \"tracking\" involves controlling a robotic\nhand to mimic a kinematic hand-object state sequence, the goal trajectory, denoted as \\{\\hat{s}_n\\}_{n=0}^N.\nThese \"kinematic references\" are retargeted from human manipulation trajectories, with $\\hat{s}_n$ rep-\nresenting the robot hand state and object pose at timestep n. A \u201ctracking demonstration\u201d pairs a\nkinematic reference {\\hat{s}_n} with an expert action sequence {a_n}, guiding the robot from $s_0 = \\hat{s}_0$ to"}, {"title": "3.1 LEARNING A NEURAL TRACKING CONTROLLER FROM DEMONSTRATIONS", "content": "Given a collection of human-object manipulation trajectories and a set of high-quality robot track-\ning demonstrations, our goal is to learn an effective and generalizable neural tracking controller. In\nthe beginning, we retarget human hand-object manipulation to a robotic hand as a data preprocess-\ning step. We combine RL and IL to develop a generalizable and robust neural tracking controller.\nTaking advantage of imitating diverse and high-quality robot tracking demonstrations, we can ef-\nfectively let the tracking controller master diverse manipulation skills and also equip it with high\ngeneralization ability. Jointly leveraging the power of RL, the controller avoids overfitting to suc-\ncessful tracking results limited to a narrow distribution, thereby maintaining robust performance in\nthe face of dynamic state disturbances. Specifically, we design an RL-based learning scheme for\ntraining the tracking controller, including the carefully-designed action space, observations, and the\nreward tailored for the manipulation tracking task. We also introduce an IL-based strategy that lets\nthe tracking controller benefit from imitating high-quality demonstration data. By integrating these\ntwo approaches, we effectively address the complex problem of generalizable tracking control.\nNeural tracking controller. In our formulation, the neural tracking controller acts as an agent that\ninteracts with the environment according to a tracking policy $\\pi$. At each timestep $n$, the policy\nobserves the observation $o_n$ and the next goal $\\hat{s}_{n+1}$ (designated as the target state for the robotic\nhand and the object). The policy then computes the distribution of the action. Then the agent sample\nan action $a_n$ from the policy, i.e., $a_n \\sim \\pi(\\cdot|o_n, \\hat{s}_{n+1})$. The observation $o_n$ primarily contains the\ncurrent state $s_n$ and the object geometry. Upon executing the action with the robotic dexterous\nhand, the hand physically interacts with the object, leading both the hand and the object to transition\ninto the next state according to the environment dynamics, represented as $s_{n+1} \\sim p(s_n, a_n)$. An\neffective tracking controller should ensure that the resulting hand and object states closely align with\ntheir respective next goal states.\nReinforcement learning. In the RL-based training scheme, the agent receives a reward $r_n =$\nr(Sn, an, Sn+1, Sn+1) after each transition. The training objective is to maximize the discounted"}, {"title": "3.2 MINING HIGH-QUALITY ROBOT TRACKING DEMONSTRATIONS USING NEURAL\nCONTROLLER THROUGH A HOMOTOPY OPTIMIZATION SCHEME", "content": "To prepare a demonstration from a kinematic reference trajectory ($\\hat{s}_0, ..., \\hat{s}_N$) for training the track-\ning controller, we need to infer the action sequence ($a^*_0, ..., a^*_{N-1}$) that successfully tracks the ref-\nerence sequence. A straightforward approach is to leverage RL to train a single-trajectory tracking"}, {"title": "5 ABLATION STUDIES", "content": "Diversity and quality of robot tracking demonstrations. We propose enhancing the diversity and\nquality of tracking demonstrations using the tracking controller and homotopy generator. We ablate\nthese strategies by creating two variants: \u201cOurs (w/o data, w/o homotopy)", "Ours (w/o data)": "which uses only\nthe homotopy optimization scheme to improve demonstrations. Despite using the same number of\ndemonstrations, both variants produce lower-quality data. As shown in Table 1, they underperform\ncompared to our full method, underscoring the importance of data quality in training the controller.\nScaling the number of demonstrations. To in-\nvestigate the relationship between the tracking con-\ntroller's performance and the number of demonstra-\ntions, we vary the size of the demonstration dataset\nduring training and tested performance on the TACO\ndataset. Specifically, in the final training iteration,\nwe down-sampled the dataset to 0.1, 0.3, 0.5, and\n0.9 of its original size and re-trained the model. As\nshown in Figure 5, there is a clear correlation be-\ntween the amount of demonstrations and model per-\nformance. Since the curve has not plateaued, we hy-\npothesize that increasing the amount of high-quality\ndata could further improve performance."}, {"title": "6 CONCLUSIONS AND LIMITATIONS", "content": "We propose DexTrack to develop a generalizable tracking controller for dexterous manipulation.\nLeveraging high-quality tracking demonstrations and a pre-trajectory tracking scheme, we refine\nthe controller through bootstrapping. Extensive experiments confirm its effectiveness, establishing\na strong foundation for future advancements. Limitations. A key limitation is the time-consuming\nprocess of acquiring high-quality demonstrations. Future work could explore faster, approximate\nmethods for homotopy optimization to speed up training."}, {"title": "A ADDITIONAL TECHNICAL EXPLANATIONS", "content": "Overview figure of the method with details. In figure 6, to present a comprehensive overview of\nour method, we draw a detailed overview figure that includes more important details than the figure\nof the original method in Section 3."}, {"title": "A.1 DATA PREPROCESSING", "content": "Kinematic retargeting. We curate kinematic robot-object interaction data from human references\nby retargeting robot hand manipulation sequences from human hand trajectories. For instance, given\na human hand-object interaction trajectory describing the human hand pose sequences represented\nin MANO and the object pose sequences (Hhuman), O as well as the description of the articulated\nrobot hand, we retarget Hhuman to acquire the robot hand trajectory H. We manually define corre-\nspondences between the robot hand mesh and the MNAO hand mesh. After that, the sequence of the\nrobot hand DoF positions is optimized so that the resulting robot hand mesh sequence is close to the"}, {"title": "A.2 TRACKING CONTROLLER TRAINING", "content": "Controlling a floating-base articulated hand. The articulated hand is represented in the reduced\ncoordinate (@finger. We additionally add three translation joints and three revolute joints to control the\nglobal position and orientation of the hand, resulting in 0 = (0)trans, (rot, (rot). For the Allegro hand\nand the LEAP hand that we use in our experiments, (finger is a 16-dimensional vector. Therefore, \u03b8\nis a 22-dimensional vector.\nObservations. The observation at each timestep n encodes the current hand and object state, the\nnext goal state, baseline trajectory, actions, and the object geometry:\n$o_n = \\{s_n, \\dot{s}_n, \\hat{s}_{n+1}, s^b_h, a_n, feat_{obj}, aux_n\\}.$\n(8)\nwhere $aux_n$ is the auxiliary features, computed as follows:\n$aux_n = \\{\\hat{s}_{n+1} - \\hat{s}_n, f_n, \\hat{S}_{n+1} \\hat{S}_{n, }\\}.$\n(9)\nwhere $\\hat{s}_{n+1} - s_n$ calculates the difference between two states, including the hand state difference\nand the object state difference, $f_n$ indicates the hand finger positions in the world space.\nReward. Our reward for manipulating tracking encourages the transited hand state and the object\nstate to be close to their corresponding reference states and the hand-object affinity:\nr = W_{o,p}r_{o,p} + W_{o,q}r_{o,q} + W_{wrist}r_{wrist} + W_{finger}r_{finger} + W_{affinity}r_{affinity},\n(10)"}, {"title": "A.3 \u041d\u043e\u043cOTOPY GENERATOR LEARNING", "content": "Mining effective homotopy optimization paths. The maximum number of iterations K is set to 3\nin our method to balance between the time cost and the effectiveness.\nWe need to identify neighbors for each tracking task so that we can avoid iterating over all tasks and\nreduce the time cost. We use the cross-kinematic trajectory similarity to filter neighboring tasks. We\npre-select Knei =10 neighbouring tasks for each tracking task."}, {"title": "A.4 ADDITIONAL EXPLANATIONS", "content": "In the reward design, we do not include the velocity-related terms since it is impossible for us to\nget accurate velocities from kinematic references. One can imagine calculating the finite differences\nbetween adjacent two frames as the velocities. However, it may not be accurate. Therefore, we do\nnot use them to avoid unnecessary noise."}, {"title": "B ADDITIONAL EXPERIMENTS", "content": ""}, {"title": "B.1 DEXTEROUS MANIPULATION TRACKING CONTROL", "content": "Training the tracking controller on two datasets. In the main experiments, the training data and\nthe test data come from the same dataset. We adopt such a setting considering the large cross-\ndataset trajectory differences. Specifically, GRAB mainly contains manipulation trajectories with\ndaily objects, while TACO mainly covers functional tool-using trajectories. However, jointly us-\ning the trajectories from such two datasets to train the model can potentially offer us a stronger\ncontroller considering the increased labeled data coverage. Therefore, we additionally conduct this\nexperiment where we train a single model using trajectories provided by the two datasets and test\nthe performance on their test sets respectively. Results are summarized in Table 4, which can still\ndemonstrate the effectiveness of our approach."}, {"title": "B.2 REAL-WORLD EVALUATIONS", "content": "More results. For direct tracking results transferring setting, we present the quantitative success\nrates evaluated on our method and the best-performed baseline in Table 7 (for the dataset GRAB)\nand Table 8 (for the dataset TACO). As observed in the table, the tracking results achieved by our\nmethod can be well transferred to the real-world robot, helping us achieve obviously better results\nthan the baseline methods. It validates the real-world applicability of our tracking results.\nPlease refer to the main text (Sec. 4) for the quantitative comparisons between the transferred con-\ntrollers. We include more qualitative results in Figure 9 to demonstrate the real-world application\nvalue of our method."}, {"title": "B.3 ANALYSIS ON THE HOMOTOPY OPTIMIZATION SCHEME", "content": "We conduct further analysis of the proposed homotopy optimization scheme and the homotopy path\ngenerator to demonstrate their effectiveness. As shown in Figure 11, by optimizing through the\nhomotopy optimization path, we can get better results in per-trajectory tracking.\nLifting thin objects. As demonstrated in Figure 11a, for the originally unsolvable tracking problem\nwhere we should manage to lift a very thin flute up from the table, we can finally ease the tracking"}, {"title": "B.4 FAILURE CASES", "content": "Our method may fail to perform well in some cases where the object is from a brand new category\nwith challenging thin geometry, as demonstrated in Figure 12."}, {"title": "C ADDITIONAL EXPERIMENTAL DETAILS", "content": "Datasets. Our dexterous robot hand-object manipulation dataset is created by retargeting two public\nhuman-object datasets, namely GRAB Taheri et al. (2020), containing single-hand interactions with\ndaily objects, and TACO Liu et al. (2024b), featured by functional tool using interactions. We\nretarget the full GRAB dataset and the fully released TACO dataset, obtaining 1269 and 2316 robot\nhand manipulation sequences respectively. For GRAB, we use sequences of the subject sl, with\n197 sequences in total, as the test dataset. The training dataset is constructed by remaining sequences\nfrom other subjects. For the TACO dataset, we create one training set with four distinct test sets with\ndifferent generalization levels for a detailed evaluation of the model's generalization performance.\nSpecifically, the whole dataset is split into 1) a training dataset, containing 1565 trajectories, 2) test\nset SO where both the tool object geometries and the interaction triplets are seen during training\ncontaining 207 trajectories in total, 3) test set S1 where the tool geometry is novel but the interaction\ntriplets are seen during training with 139 trajectories, 3) test set S2 with novel interaction triplets but\nseen object categories and geometries, containing 120 trajectories in total, and 4) test set S3 with 285\ntrajectories where both the object category and interaction triplets are new to the training dataset.\nFigure 13 and Figure 14 draw the examples of unseen objects from seen categories and the objects\nfrom new categories respectively. The original data presented in TACO often contains noisy initial\nframes where the hand penetrates through the table or the object. Such noise, though seems subtle,\nwould affect the initial dynamics, however. For instance, if the hand initially penetrates through the\ntable, a large force would be applied to the hand at the beginning, which would severely affect the\nsimulation in subsequent steps. Moreover, if the hand initially penetrates through the object, the\nobject would be bounced away at the start of the simulation. To get rid of such phenomena, we\nmake small modifications to the original sequences. Specifically, we interpolate the phone pass\nsequence of the subject s2 from the GRAB dataset with such TACO sequences as the final modified\nsequence. Specifically, we take hand poses from the initial 60 frames of the GRAB sequence. We\nthen linearly interpolate the hand pose in the 60th frame of the GRAB sequence with the hand pose\nin the 60th frame of the TACO sequence. For details, please refer to code in the supplementary\nmaterial (refer \u201cREADME.md\" for instructions).\""}, {"title": "C", "content": "Squality (L) [to (T) + tc(T) + to(T) .\\\n= ET L 3 (25)\nThe \u201crobustness\u201d can be measured by the performance gap of the model on tracking\ntasks with \u201chigh-quality\u201d references and those with \u201cperturbed\u201d thus \u201clow-quality\u201d track-\ning tasks. Denote the \u201chigh-qualtiy\" tracking task distribution as H while those with low\nquality as L. Thus we can quantify the \"robustness\" as:\nSr(\u03c0) := \\frac{S_{quality}(H)}{\\min(L_H (\\pi), \\epsilon)}(26)\nAs the quality of the trajectory distribution gets worse and the tracking error decreases, the\n\"robustness score\" would increase"}, {"title": "D .123456789"}]}