{"title": "Interpretable Pre-Trained Transformers\nfor Heart Time-Series Data", "authors": ["Harry J. Davies", "James Monsen", "Danilo P. Mandic"], "abstract": "Decoder-only transformers are the backbone of the popular generative pre-trained\ntransformer (GPT) series of large language models. In this work, we apply the\nsame framework to periodic heart time-series data to create two pre-trained general\npurpose cardiac models, namely PPG-PT and ECG-PT. We demonstrate that both\nsuch pre-trained models are fully interpretable. This is achieved firstly through\naggregate attention maps which show that the model focuses on similar points\nin previous cardiac cycles in order to make predictions and gradually broadens\nits attention in deeper layers. Next, tokens with the same value, that occur at\ndifferent distinct points in the ECG and PPG cycle, form separate clusters in high\ndimensional space based on their phase as they propagate through the transformer\nblocks. Finally, we highlight that individual attention heads respond to specific\nphysiologically relevent features, such as the dicrotic notch in PPG and the P-wave\nin ECG. It is also demonstrated that these pre-trained models can be easily fine-\ntuned for tasks such as classification of atrial fibrillation. In this specific example,\nthe fine-tuning took 11 minutes of computer time, and achieved a leave-one-subject-\nout AUCs of 0.99 and 0.93 for ECG and PPG respectively. Importantly, these\nfine-tuned models are also fully explainable, with attention shifting to regions in\nthe context that are strongly indicative of atrial fibrillation.", "sections": [{"title": "1 Introduction", "content": "The generative pretrained transformer (GPT) [1] [2] is a large language model (LLM) which forms\nthe basis of the chat-GPT models [3]. It consists of layers of decoder-only transformers [4] that are\ntrained to predict the next token (words, sub-words, or characters) using all previous tokens provided\nto the model as context. The attention mechanism is masked such that tokens inputted to the model\ncan only communicate with past tokens and thus a context window of N length effectively provides\nN different training examples, whereby the model tries to predict the same context window shifted by\none token into the future. The whole premise of the GPT model is that in order to accurately predict\nthe next token, across a large and diverse array of texts, a model must gain an efficient and general\ncomprehension of these texts. Given that language is a way that we summarise and communicate\nthe world around us, this efficient and general understanding allows the pre-trained model to be fine\ntuned for a myriad of complex tasks [3].\nThe premise of this paper is that we can use this same GPT idea, i.e. stacks of decoder-only\ntransformers trained to predict the next token, and apply it to physiological time series instead of\nlanguage. With the hypothesis that if our models gain an efficient and general understanding of\nthese physiological time series, such as photoplethysmography (PPG) or electrocardiography (ECG),\nthen these large models will be simple to fine-tune for downstream tasks such as the classification\nof heart conditions. The ECG signal refers to the monitoring of the electrical activity of the heart\nthrough external non-invasive electrodes that measure the potential difference across the heart [5][6].\nIn this work, we use only single-lead ECG. The photoplethysmography (PPG) signal refers to the\nnon-invasive measurement of blood volume [7]. The PPG works by shining light through tissue with\nan LED, which is absorbed by the blood, and then either measuring the transmitted light or reflected\nlight with a corresponding photo-diode. The PPG is thus able to measure changes in blood volume\nrelated to pulse and even breathing [8]. In PPG, for example, in order to accurately predict the next\ntime point the model must gain knowledge on underlying several phenomena, such as the subtle\nchanges in heart rate and pulse amplitude due to breathing and blood pressure. We can therefore\nhypothesise that a large model that is good at predicting the next token of PPG will be straightforward\nto fine-tune for specific PPG related tasks.\nAs GPT and other large language models rapidly gain capability and thus decision making power in\nour lives, it is of crucial importance that LLMs do not remain black boxes. The black box nature of\ndeep learning models is also an immediate problem in healthcare, where decisions made by artificial\nintelligence have the potential to directly impact our health and even our lives. It is therefore critical\nthat models are developed which give clear insights and explanations into why decisions are made. A\nlogical way to interpret the operation of transformer-based LLMs is to look at the attention weights of\nthe individual transformer blocks. This attention mechanism provides information on which tokens a\nmodel looks at in order to make a decision, and thus it is common to use heat maps that visualise\nthis attention as an avenue for interpretability [9]. Language is complex, and there are often several\npathways to achieve the same result. In many instances this makes it difficult to decipher attention,\nand means that in the context of LLMs it cannot always be relied upon for explainability [10].\nHowever, both the PPG and ECG signals are far less complicated than language, and downstream\ntasks, such as screening for heart conditions, offer more constraints on the possible pathways to\nachieve a correct diagnosis. It is therefore a core focus of this paper, not just to provide generalised\nmodels for PPG and ECG related tasks, but to provide models that are fully interpretable in their\noperation so that they may be safely used in a healthcare setting.\nTo this end, we firstly present two pretrained decoder-only time series transformer models for use with\nheart signals, namely PPG-PT and ECG-PT. Furthermore, we demonstrate that, through i) aggregate\nattention of different transformer layers, ii) changes in cosine similarity between core PPG and ECG\nfeatures in embedding space upon propagation through transformer layers, and iii) the analysis of the\nattention weights of individual attention heads in the final transformer block, that both the PPG-PT\nand ECG-PT models behave exactly as expected for the task of predicting the next time series point\nin PPG and ECG. Finally, we demonstrate how these models can be easily fined tuned to detect atrial\nfibrillation (AF), a common type of abnormal heart rhythm. The changes in attention from the task of\nnext token prediction, to the task of classification of AF, can be used to further explain why a model\nhas made a decision. Clinicians can thus be provided with both a classification and the reasoning\nbehind it, allowing for a collaborative relationship between a clinician and AI in diagnostics and\ntreatment."}, {"title": "2 The Pre-Trained Transformer Models", "content": "Both the ECG and PPG signals are periodic, and in most applications we can ignore any mean offset.\nWith this in mind, it is simple to divide each PPG and ECG signal into a range of finite tokens,\nforming a vocabulary from which each signal can be constructed. To tokenise the PPG signals, they\nwere resampled to 50Hz and each 10 second context window (500 samples) was scaled to between\n100 and 0. This signal was then rounded to give integer values, yielding a total vocabulary size\nof 101 tokens. The 10 second context window was chosen so as to be long enough to preserve\nlow frequency variations in the PPG (such as respiratory variations), but not too long as to run into\nmemory issues when training. For the ECG, the process was so, apart from resampling to 100Hz and\nthus using a context window that corresponded to 5 seconds instead of 10 seconds. A higher sampling\nfrequency was required in the ECG tokenisation in order to preserve the high frequency components\nof the QRS complex. In GPT models, common sequences of token values can be combined during\ntokenisation, in order to allow for a longer context length for the same number of tokens. This method\nof tokenisation was not implemented in the PPG-PT and ECG-PT models as further extending the\nlength of the context window was not necessary.\nUpon tokenisation, each token was embedded with a token embedding table and a position embedding\ntable. The dimensions of the token embedding were the vocabulary size times the embedding\ndimension ($d_{model}$), giving a $d_{model}$ dimensional vector for every possible token in the vocabulary.\nThe dimensions of the position embedding table were the maximum context length times $d_{model}$,\ngiving a $d_{model}$-dimensional vector for every possible position in the context window. In our models,\nboth of these embeddings are learnt as the model trains. In the original transformer paper, the\npositional embeddings utilised sine and cosines functions of different frequencies [4]. The token\nand positional embeddings are added together, which means that the attention mechanism in the\nsubsequent transformer blocks has information on both the specific token and the position of the\ntoken in the context."}, {"title": "2.2 Multi-Head Masked Self Attention and the Transformer Block", "content": "Once tokens have been embedded with a token embedding and positional embedding that are added\ntogether, thus allowing the model to use information on both the position of the token in the context\nwindow (length N) and the value of the token, they are fed to transformer blocks in their high\ndimensional vector form. In multi-head attention, the embedding space (of size $d_{model}$) is divided up\ninto lower dimensional spaces of equal size ($d_k$), before the results of attention are concatenated after\nthe attention mechanism to reconstruct the original embedding space size ($d_{model}$) [4].\nThe attention mechanism works by allowing tokens to communicate with each other and thus update\neach others information. Each attention head transforms tokens into keys (K), using a linear layer\nwhich compresses the tokens from $d_{model}$ dimensions to $d_k$ dimensions. It also separately transforms\ntokens into queries (Q), which again use another linear layer to compress the tokens from $d_{model}$\ndimensions to $d_k$ dimensions. One can think of queries (Q) as each token broadcasting what it is\nlooking for, and keys (K) as each token broadcasting what it contains. The dot product is then taken\nbetween both the queries and the keys, such that if there is a match between the information a token\nis looking for in Q and the information a token emits in K, there will be a spike in the corresponding\nattention weights $QK^T$. This dot product is also scaled by the $\\sqrt{d_k}$, and the soft max of each row\nis taken (resulting in attention rows that sum to 1). This scaled-dot product results in an N\u00d7N\nmatrix of attention weights, which show how tokens communicate with each other, for all tokens in\nthe context window. In masked self attention, the mechanism used to train GPT models, the upper\ntriangle of this weight matrix is set to zero. This means that tokens can only communicate with\nthemselves or with past tokens in the sequence, and allows for efficient training by allowing for N\nseparate training examples with an input block of N token length. During training of the model,\ndropout (the random zeroing of weights) is also applied to the weight matrix. The final step to masked\nself attention is to multiply the weight matrix with a value matrix. The value matrix, V, is formed\nfrom another linear layer that compresses the tokens from $d_{model}$ dimensions to $d_k$ dimensions. A\ncombination of all of these steps produces the following attention function [4].\n$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$ (1)\nAs previously mentioned, the results of each head, which are of dimension $d_k = d_{model}/(\\text{# Heads})$,\nare next concatenated to give the original dimension $d_{model}$. This output from multi-head attention\nis then added to input to the multi-head attention, forming a residual connection that allows the\nmodel to bypass a transformer block if needed. Finally, this new combined output is normalised and\npassed through a feed-forward network which firstly expands the embedding dimension to 4 \u00d7 $d_{model}$,\napplies a ReLU activation function, and then compresses the dimension back to $d_{model}$. During\ntraining, dropout is applied to this feed-forward network. The feed-forward network after multi-head\nattention can be thought of as allowing the model to process the results of self-attention, before these\nresults are fed into another multi-head attention block. The combination of the multi-head attention\nand this feed-forward network constitute one transformer block. After the final transformer block, the\noutput is passed through a final linear layer, which converts the output from a dimension of $d_{model}$\nto the size of the vocabulary. When passed through a softmax activation function, this provides a\nprobabilities for all tokens in the vocabulary, at all N points."}, {"title": "2.3 Architecture", "content": "Our models were developed in PyTorch [11] and adapted from a tutorial given by Andrej Karpathy\n[12] titled \"Let's build GPT: from scratch, in code, spelled out\". For both the photoplethysmography\npre-trained transformer (PPG-PT) and electrocardiography pre-trained transformer (ECG-PT) we\nused an embedding dimension ($d_{model}$) of 64. The original GPT paper used an embedding dimension\nof 768 [1], but given that PPG and ECG are assumed to be far less complex than language, 64 was\nfound to be sufficient in our case. We used 8 transformer blocks, each with 8 attention heads, as\ndescribed in Section 2.2 of this paper and the original transformer paper [4]. It was found during\nearly training that bigger models performed better, so we trained the largest model possible, with the\ntrade-offs of context length (N = 500 samples) and batch size (64), in order to utilise almost all of\nthe 12GB of RAM on our NVIDIA RTX A2000 GPU. This architecture resulted in 443,493 trainable\nparameters. In the areas where dropout was applied (the feedforward subsection of the transformer\nand the attention weight matrix), dropout was set to 0.2."}, {"title": "2.4 Training", "content": "Three datasets were used for training the PPG-PT base model: 1) Capnobase \u201crespiratory benchmark\"\ndataset [13], which consists of high quality ECG and PPG recorded over 42 subjects of 8 minutes\neach; 2) BIDMC\u00b9 dataset [14] which consists of PPG and ECG from 53 subjects for 8 minutes\neach; 3) the \"cuffless blood pressure dataset\" [15], a subset of MIMIC II [16], consisting of 12,000\nrecordings of PPG signals of varied quality in a hospital setting. The combination of all of these\ndatasets resulted in over 128 million total tokens for training.\nFor the training of the ECG-PT base model, we used subsets of the \"PhysioNet Computing in\nCardiology Challenge 2020\" dataset [17] [16], which consists of tens of thousands of 12-lead ECG\nrecordings, across tens of thousands of patients in a hospital setting. From each of these recordings,\nwe extracted 10 second examples of Lead I ECG. Within this dataset are a diverse range of cardiac\nabnormalities as well as many healthy subjects, as the dataset was originally constructed for the\nidentification of different heart conditions. Once tokenised, this dataset resulted in over 42 million\ntokens for training.\nFor each model, training data was split into 90% training and 10% validation datasets. The data was\nnot shuffled to ensure that validation data primarily consisted of unseen subjects. The PPG-PT model\nwas trained over 500,000 iterations with a batch size of 64, and the ECG-PT model was trained over\n560,000 iterations with the same batch size. After every 2000 iterations, the models were evaluated\nfor a further 200 iterations to measure validation loss. In both cases the learning rate was set as\n3 \u00d7 10-4 within the AdamW optimiser. To train PPG-PT, it took just over 5 days on an RTX A2000\n12GB GPU, and to train ECG-PT it took almost 6 days. Optimisation loss was measured using cross\nentropy, by mapping the N \u00d7 101 model outputs (logits) where the 101 represents all possible tokens,\nto the target tokens which were the next token values for all points in the input. The final validation\nloss of the PPG-PT model was 1.2, corresponding to roughly a third of prediction being perfect.\nThe final validation loss of the ECG-PT model was 1.8 (1 in 6 predictions are perfect). The higher\nvalidation loss for ECG-PT was likely due to the high proportion of abnormal heart rhythms in the\ntraining and validation datasets."}, {"title": "2.4.2 Atrial Fibrillation Fine-Tuning", "content": "The final linear layer, which converts the output from $d_{model}$ dimensions to the dimensions of the\nvocabulary size was replaced with a linear layer which instead converts the output from $d_{model}$\ndimensions to 1 dimension. This new output then was passed through a sigmoid activation function\nto scale the values between 0 and 1, and the final value was used to classify the signal as either\nhealthy (0) or atrial fibrillation (1). This same conversion of the final linear layer of the model could\nbe used for any suitable classification or regression task, by simply scaling the number of output\ndimensions to be in line with the specific task. During training, just the new linear layer and the final\ntransformer block were trained, whilst all other layers were frozen. In this example, the learning rate\nwas maintained at 3 \u00d7 10-4 within the AdamW optimiser, and a binary cross entropy loss function\nwas used.\nTo train and evaluate the fine tuning of the model for classification of atrial fibrillation (AF), the\n\"MIMIC PERform AF\" dataset [18] was used, which is a subset of MIMIC III dataset [19]. This\ndataset contains 20 minutes of continuous PPG and ECG recordings from each of 35 subjects, 19\nof whom have AF and 16 of whom did not have AF. The signals, originally recorded with a sample\nfrequency of 125Hz, were band-pass filtered to between 1 and 15Hz and then downsampled to 50Hz\nin the case of the PPG, and band-pass filtered between 1 and 45Hz and downsampled to 100Hz in\nthe case of the ECG. A window length of 500 samples was maintained, corresponding to 10 seconds\nof PPG (50Hz) and 5 seconds of ECG (100Hz), and sliding windows were extracted from the data\nand tokenised with a shift of 50 samples each time, in order to artificially inflate the volume of\ndata for the model to train on. In both cases of the ECG and PPG, the models were trained in a\nleave-one-subject-out fashion by training on 34 subjects and testing on 1, over 1000 iterations each\nwith a batch size of 128. Each model took 11 minutes to fine tune on an RTX A2000 GPU, which is a\nfraction of the over 5 days that it took to train each of the base models."}, {"title": "2.5 Cross Entropy Loss vs Mean Squared Error", "content": "Conventionally, time series transformer models produce a continuous output values for each token\nprediction and are thus trained with a mean squared error loss function [20]. However, unlike many\ntime series prediction tasks, we were able to leverage the highly periodic nature of the PPG and ECG\nsignals to create models with a well-defined small vocabulary. This meant that we were able to train\nthe model in a similar fashion to a conventional large language model, with a cross-entropy loss\nfunction. It should be noted that we did also experiment with continuous output values and mean\nsquared error as a loss function, but these results were less able to capture long range trends and\nimportantly lacked interpretability in the attention weights. A full investigation into this finding is\nbeyond the scope of this work."}, {"title": "3 Generative Model Evaluation", "content": "In training, for all points in the context window the model outputs logits to which a softmax function\ncan be applied, giving a predicted probability distribution of the next token over all possible tokens.\nFor generation of future tokens, the final probability distribution is taken, representing the possible\ntoken values for the next token. The multinomial function in PyTorch is then used to sample this\ndistribution and generate a predicted next token. Each new predicted token can then be appended\nto the previous context, and the last 500 tokens in this new context are then used to predict the next\ntoken. This process can be repeated until a maximum number of generated tokens has been reached.\nAnother way to examine the predictive capabilities of each model would be to predict the next token\nas the one with the highest probability, instead of sampling from a distribution. The problem with\nthis second approach is that sometimes in low frequency periods, such as a trough in PPG or a period\nbetween P-QRS-T regions in the ECG, taking just the token with maximum likelihood could allow\nthe model to become stuck in predicting the sample token value over and over again. Because of\nthis issue, the first approach of sampling from a probability distribution was chosen to evaluate the\nmodels.\nFor the evaluation of generative capabilities of each model, the pre-processed subset of the Bed-based\nballistocardiography dataset [21] [22] was used. This dataset consists of simultaneously recorded\nPPG and ECG from 40 subjects, sampled at 1kHz. In the preprocessed subset, the PPG was low-pass\nfiltered with a cut off of 10Hz and the ECG was band-pass filtered between 1 and 40Hz. As with\nthe previous datasets, we resampled the PPG to 50Hz, and the ECG to 100Hz, before converting\nthe signals to tokens. The 750 sample windows were extracted and split into 500 samples to form\nthe context and 250 samples to test the prediction accuracy of the model. For each prediction point,\nwe calculated the absolute distance between the predicted token and true token. We were then able\nto calculate the median and inter-quartile range of absolute prediction error against the prediction\nhorizon for both the PPG-PT and ECG-PT models. In Figure 1(a,c) example generation waveforms\nand maximum absolute errors are highlighted for both the PPG-PT and ECG-PT models. Observe\nthat the errors stem not from a failure to predict the morphology of the PPG or ECG, but from slight\ntemporal misalignment between the prediction and the ground truth, as is common with periodic\nphysiological signals [23]. The median absolute distance between the prediction and ground truth\nin PPG-PT starts at 1 token and grows to 21 tokens across the 5 second (250 samples) prediction\nhorizon, as shown in Figure 1(b). For ECG-PT, the median error starts at 1 token and grows to 3\ntokens across the 2.5 second (250 samples) prediction horizon, as shown in Figure 1(d). It should be\nnoted that we expect the errors to be significantly higher in the PPG-PT model when compared with\nthe ECG-PT model, as due to the comparatively broader shape of the peaks in the PPG waveform,\nerrors in temporal misalignment increase the prediction error across the whole waveform. This is in\ncontrast to ECG where the vast majority of the signal is flat, and thus slight temporal misalignment is\nmainly penalised around the QRS complex and the T wave.\nWhilst these prediction errors show us that the models are good at extrapolating PPG and ECG\nwaveforms into the future, this can never be perfect given that there may be changes in heart rate,\nrespiration, and even arrhythmia events that are impossible to predict from the context. The important\nresult here is the ability of the models to generalise to unseen distinct morphologies of PPG and\nECG as context. It is clear that the PPG-PT condenses knowledge of peak width and the shape of the\ndicrotic notch (the sub-peak) in the PPG examples and is thus able to generate PPG with the same\nproperties. This is even more obvious in the case of ECG, where both examples have very different\nP, Q, R, S and T wave morphologies, and ECG-PT was able to replicate this well. This is the first\nindicator that the PPG-PT and ECG-PT models clearly pay attention to the relevent features of both\nsignals."}, {"title": "4 Interpretability of aggregate model attention", "content": "As is the case with GPT models [1], our pre-trained transformer models (PPG-PT and ECG-PT) are\ntrained to predict the next token at each point, by using the attention mechanism to collate knowledge\nfrom previous tokens. It is then natural to ask ourselves: given the task of predicting the next token\nin a periodic signal, which tokens would one look at in order to make that prediction? The obvious\nanswer to this question is to look at tokens which are at the same point in the previous cycles as\nthe token that we want to predict. For example, if the current token to predict was a peak, it would\nbe logical to look at all previous peaks, with an emphasis on peaks of similar height and width, in\norder to make an accurate prediction. However, the information we start with is just the position of a\ntoken in the context window and the value of a token. Given that the fundamental frequency changes\nbetween contexts and that different points in the same cycle can have the same value, we cannot\nrely on the initial position and value information alone to find similar points in the same cycle. In\norder to understand the point at which a token lies in a cycle, it is therefore necessary to first look\nat the context of surrounding tokens. Once this relationship between a token and its local cycle is\nunderstood, attention can broaden to look at all cycles.\nThis mechanism of broadening attention is indeed found in both of our PPG-PT and ECG-PT models.\nThe last row in the attention weight matrix corresponds to the final token in the context. To examine\nthe attention span of the model, we calculated the central point of attention for this final row of\nattention in each attention head in each transformer block, across all context windows in the generative\ntest set. In Table 1 it is shown that for PPG-PT, the mean central point of attention is 0.43 seconds in\nthe first transformer block, and thus the focus is within the immediate cycle. By the last transformer\nblock, this central point of attention has broadened to 2.31 seconds, indicating that the PPG-PT model\nwas updating the current token based on the tokens in previous cycles of PPG. The same effect is\nseen in ECG-PT, with a broadening of attention from 0.33 seconds in the first block to 1.88 seconds\nin the last block."}, {"title": "5 Vector similarities between points of interest", "content": "In the previous section, we demonstrated that the pre-trained transformer models logically focus\non similar points in previous cycles of PPG or ECG when trying to predict the next token. This\nstrongly indicates that the models are able to distinguish different points in the cycle of PPG and\nECG waveforms. In this section, we aim to solidify this by examining how the cosine similarity of\nembedded tokens with the same value, which occur at different distinct points in the PPG and ECG\ncycle, change upon propagation through the model. To investigate this, we chose tokens at similar\npoints on rising slopes and falling slopes on the PPG signal, and on the T-wave of the ECG signal.\nThe similarity of these tokens was calculated with respect to the final rising slope in each context\nwindow. If the models are effective at distinguishing different points in the cycle, we would expect\nrising slopes to cluster with each other in vector space, and falling slopes to form a separate cluster in\nvector space. The parallel of this experiment in a large language model would be to look at the vector\nsimilarity of homonyms (words that have the same spelling but can have multiple different meanings),\nand examine how the vector similarity changes based on specific context in a sentence. For example,\nwe would expect the token \"bat\" to form separate clusters in vector space upon propagation through\nan LLM based on if it contextually refers to a club to hit a cricket ball or refers to the flying mammal.\nIt is highlighted in Figure 3 that, upon input to the pre-trained models, falling slopes and rising slopes\nof the PPG and ECG are shuffled in vector space. This is because the inputs to the models have the\nsame token embedding that has not yet been updated based on the context of previous tokens. Notably,\nas these tokens propagate through the models, rising slopes increase in vector similarity and cluster\nbelow a cosine similarity of 1 (identical vector) and falling slopes form their own independent cluster.\nThis further demonstrates that these large pre-trained models can clearly identify the relationship\nbetween different points in cycle of both PPG and ECG."}, {"title": "6 Attention Maps of Individual Attention Heads", "content": "Now that we know that the base pre-trained PPG-PT and ECG-PT models look at similar points\nin previous cycles in order to predict the next token, and that the models have knowledge of the\ncontext of different points within a PPG and ECG cycle, the final step is to examine if the individual\nattention heads look for specific high level features in the PPG and ECG signals. To investigate this,\nwe examined example contexts from the generative evaluation dataset which contained the common\nfeatures of each signal. For example, a PPG context which has well-defined peaks, troughs, and a\ndicrotic notch, and an ECG context which has a well defined P-wave and QRS complex.\nFor each model, upon inputting the context, we looked at the final row in the attention weights of each\nhead in the last transformer block, corresponding to the weights associated with predicting the next\ntoken. Upon saving these weights, we inputted a new context corresponding to the original shifted by\n1 sample into the future. We then saved these weights and added the final N-1 weights to the last\nN-1 original weights. We repeated this shift and add operation a further N-2 times, and divided each\nof the N weights element-wise by the number of additions made to them. This gave us an average\nmap of attention to the context for the next 500 token predictions, for each self-attention head in the\nfinal transformer block. We then used the findpeaks function in MATLAB, with a minimum peak\ndistance of 15 tokens and a minimum peak height of 0.5 of the maximum attention weight in the\nwindow, to highlight the local maximas in attention.\nFigure 4 highlights that individual attention heads do indeed pay attention to important features in the\nsignal of interest. In particular, PPG-PT has an attention head that looks for peaks, another that looks\nfor troughs and another which looks for the dicrotic notch. In addition, we find that ECG-PT has a\nhead which looks for the Q portion of the QRS, another which looks for R-peaks and another that\nlooks for P waves. Therefore, on top of an ability to distinguish all separate points in a PPG and ECG\ncycle, the pre-trained models presented in this paper also pay specific attention to some of the most\nphysiological relevent features."}, {"title": "7 Change in Attention when Screening For Atrial Fibrillation", "content": "Through pre-training we have obtained models which have a strong grasp on the important features of\nPPG and ECG, and generalise well to unseen morphologies. However, rather like in LLMs, predicting\nthe next token of PPG and ECG lacks utility. To fully make use of these pre-trained models, we fine\ntune the models to classify atrial fibrillation (AF), as outlined in Section 2.4.2. Atrial fibrillation is the\nmost common arrhythmia, characterised by an irregular heart rate where there can be rapid increases\nin heart rate and periods where it slows down dramatically [24]. The fine-tuned AF-PPG-PT model\nachieved a leave-one-subject-out area under the curve (AUC) of 0.93 when cross validated across\nall test subjects, and 0.96 when two subjects with poor signal to noise ratio were excluded. The\nfine-tuned AF-ECG-PT model achieved an AUC of 0.99 when cross validated across all test subjects.\nIt is worth reiterating that these results were achieved with 11 minutes of fine-tuning computer time,\ncorresponding to roughly 0.2% of the total time it took to train the base models.\nThe key result of fine-tuning is not just the accuracy, but the interpretability of the results. To ascertain\nwhy a model made a classification, we simply aggregated the final row of soft-maxed attention\nweights across all heads in the final transformer layer for the base model and compared with the\nfine-tuned model. Any increases in attention weights therefore indicates that the model deems the\ncorresponding tokens valuable for classification of AF. In Figure 5(a) two representative attention\nmaps are displayed for the classification of a PPG context as AF. It is clear in both cases that the\nmodel increases attention on periods where beats occur later than expected on earlier than expected,\nwhich is the obvious characteristic of AF. This is even clearer in the fine-tuned ECG attention, given\nthat peaks in ECG are more precisely localised in time. In Figure 5(b), an example is highlighted\nwhere a beat is expected to occur based on the previous beat timing, but it does not and thus the\nmodel attention spikes. A second example is also highlighted, where based on the previous beat\ntiming a beat occurs much earlier than expected, and model attention therefore spikes exactly at this\npoint. These interpretable maps of shifts in attention, demonstrating the reason why the model has\nmade the classification, can easily be provided along with with the probability of AF."}, {"title": "8 Conclusion", "content": "This work has conclusively demonstrated that GPT-like models, trained to predict the next token in\nperiodic physiological time series, are fully interpretable in their operation. This has been illustrated\nthrough aggregate attention maps which are logical for the task of predicting the next token and\nthrough the clustering of different PPG and ECG features in vector space. This is further shown\nthrough individual attention heads that respond strongly to specific features such as the dicrotic notch\nin PPG or the P-wave in ECG. Moreover, we have shown that this interpretability is carried forward\nwhen fine-tuning for the classification of atrial fibrillation, where attention shifts to regions in the\ninput context that most indicate the presence of the arrhythmia. This work represents a step forward\nin the explainability of large transformer networks when applied to healthcare settings."}]}