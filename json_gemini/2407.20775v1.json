{"title": "Interpretable Pre-Trained Transformers for Heart Time-Series Data", "authors": ["Harry J. Davies", "James Monsen", "Danilo P. Mandic"], "abstract": "Decoder-only transformers are the backbone of the popular generative pre-trained transformer (GPT) series of large language models. In this work, we apply the same framework to periodic heart time-series data to create two pre-trained general purpose cardiac models, namely PPG-PT and ECG-PT. We demonstrate that both such pre-trained models are fully interpretable. This is achieved firstly through aggregate attention maps which show that the model focuses on similar points in previous cardiac cycles in order to make predictions and gradually broadens its attention in deeper layers. Next, tokens with the same value, that occur at different distinct points in the ECG and PPG cycle, form separate clusters in high dimensional space based on their phase as they propagate through the transformer blocks. Finally, we highlight that individual attention heads respond to specific physiologically relevent features, such as the dicrotic notch in PPG and the P-wave in ECG. It is also demonstrated that these pre-trained models can be easily fine-tuned for tasks such as classification of atrial fibrillation. In this specific example, the fine-tuning took 11 minutes of computer time, and achieved a leave-one-subject-out AUCs of 0.99 and 0.93 for ECG and PPG respectively. Importantly, these fine-tuned models are also fully explainable, with attention shifting to regions in the context that are strongly indicative of atrial fibrillation.", "sections": [{"title": "1 Introduction", "content": "The generative pretrained transformer (GPT) [1] [2] is a large language model (LLM) which forms the basis of the chat-GPT models [3]. It consists of layers of decoder-only transformers [4] that are trained to predict the next token (words, sub-words, or characters) using all previous tokens provided to the model as context. The attention mechanism is masked such that tokens inputted to the model can only communicate with past tokens and thus a context window of N length effectively provides N different training examples, whereby the model tries to predict the same context window shifted by one token into the future. The whole premise of the GPT model is that in order to accurately predict the next token, across a large and diverse array of texts, a model must gain an efficient and general comprehension of these texts. Given that language is a way that we summarise and communicate the world around us, this efficient and general understanding allows the pre-trained model to be fine tuned for a myriad of complex tasks [3]."}, {"title": "2 The Pre-Trained Transformer Models", "content": ""}, {"title": "2.1 Tokenisation and Embedding", "content": "Both the ECG and PPG signals are periodic, and in most applications we can ignore any mean offset. With this in mind, it is simple to divide each PPG and ECG signal into a range of finite tokens, forming a vocabulary from which each signal can be constructed. To tokenise the PPG signals, they were resampled to 50Hz and each 10 second context window (500 samples) was scaled to between 100 and 0. This signal was then rounded to give integer values, yielding a total vocabulary size of 101 tokens. The 10 second context window was chosen so as to be long enough to preserve low frequency variations in the PPG (such as respiratory variations), but not too long as to run into memory issues when training. For the ECG, the process was so, apart from resampling to 100Hz and thus using a context window that corresponded to 5 seconds instead of 10 seconds. A higher sampling"}, {"title": "2.2 Multi-Head Masked Self Attention and the Transformer Block", "content": "Once tokens have been embedded with a token embedding and positional embedding that are added together, thus allowing the model to use information on both the position of the token in the context window (length N) and the value of the token, they are fed to transformer blocks in their high dimensional vector form. In multi-head attention, the embedding space (of size dmodel) is divided up into lower dimensional spaces of equal size (dk), before the results of attention are concatenated after the attention mechanism to reconstruct the original embedding space size (dmodel) [4].\nThe attention mechanism works by allowing tokens to communicate with each other and thus update each others information. Each attention head transforms tokens into keys (K), using a linear layer which compresses the tokens from dmodel dimensions to dk dimensions. It also separately transforms tokens into queries (Q), which again use another linear layer to compress the tokens from dmodel dimensions to dk dimensions. One can think of queries (Q) as each token broadcasting what it is looking for, and keys (K) as each token broadcasting what it contains. The dot product is then taken between both the queries and the keys, such that if there is a match between the information a token is looking for in Q and the information a token emits in K, there will be a spike in the corresponding attention weights QKT. This dot product is also scaled by the \u221adk, and the soft max of each row is taken (resulting in attention rows that sum to 1). This scaled-dot product results in an N\u00d7N matrix of attention weights, which show how tokens communicate with each other, for all tokens in the context window. In masked self attention, the mechanism used to train GPT models, the upper triangle of this weight matrix is set to zero. This means that tokens can only communicate with themselves or with past tokens in the sequence, and allows for efficient training by allowing for N separate training examples with an input block of N token length. During training of the model, dropout (the random zeroing of weights) is also applied to the weight matrix. The final step to masked self attention is to multiply the weight matrix with a value matrix. The value matrix, V, is formed from another linear layer that compresses the tokens from dmodel dimensions to dk dimensions. A combination of all of these steps produces the following attention function [4].\nAttention(Q, K, V) = softmax(QK^T / \u221a{d_k})V \t{1}\nAs previously mentioned, the results of each head, which are of dimension dk = dmodel/(# Heads), are next concatenated to give the original dimension dmodel. This output from multi-head attention is then added to input to the multi-head attention, forming a residual connection that allows the model to bypass a transformer block if needed. Finally, this new combined output is normalised and passed through a feed-forward network which firstly expands the embedding dimension to 4 \u00d7 dmodel, applies a ReLU activation function, and then compresses the dimension back to dmodel. During training, dropout is applied to this feed-forward network. The feed-forward network after multi-head attention can be thought of as allowing the model to process the results of self-attention, before these results are fed into another multi-head attention block. The combination of the multi-head attention and this feed-forward network constitute one transformer block. After the final transformer block, the output is passed through a final linear layer, which converts the output from a dimension of dmodel"}, {"title": "2.3 Architecture", "content": "Our models were developed in PyTorch [11] and adapted from a tutorial given by Andrej Karpathy [12] titled \"Let's build GPT: from scratch, in code, spelled out\". For both the photoplethysmography pre-trained transformer (PPG-PT) and electrocardiography pre-trained transformer (ECG-PT) we used an embedding dimension (dmodel) of 64. The original GPT paper used an embedding dimension of 768 [1], but given that PPG and ECG are assumed to be far less complex than language, 64 was found to be sufficient in our case. We used 8 transformer blocks, each with 8 attention heads, as described in Section 2.2 of this paper and the original transformer paper [4]. It was found during early training that bigger models performed better, so we trained the largest model possible, with the trade-offs of context length (N = 500 samples) and batch size (64), in order to utilise almost all of the 12GB of RAM on our NVIDIA RTX A2000 GPU. This architecture resulted in 443,493 trainable parameters. In the areas where dropout was applied (the feedforward subsection of the transformer and the attention weight matrix), dropout was set to 0.2."}, {"title": "2.4 Training", "content": ""}, {"title": "2.4.1 Pre-Training", "content": "Three datasets were used for training the PPG-PT base model: 1) Capnobase \u201crespiratory benchmark\" dataset [13], which consists of high quality ECG and PPG recorded over 42 subjects of 8 minutes each; 2) BIDMC\u00b9 dataset [14] which consists of PPG and ECG from 53 subjects for 8 minutes each; 3) the \"cuffless blood pressure dataset\" [15], a subset of MIMIC II [16], consisting of 12,000 recordings of PPG signals of varied quality in a hospital setting. The combination of all of these datasets resulted in over 128 million total tokens for training.\nFor the training of the ECG-PT base model, we used subsets of the \"PhysioNet Computing in Cardiology Challenge 2020\" dataset [17] [16], which consists of tens of thousands of 12-lead ECG recordings, across tens of thousands of patients in a hospital setting. From each of these recordings, we extracted 10 second examples of Lead I ECG. Within this dataset are a diverse range of cardiac abnormalities as well as many healthy subjects, as the dataset was originally constructed for the identification of different heart conditions. Once tokenised, this dataset resulted in over 42 million tokens for training.\nFor each model, training data was split into 90% training and 10% validation datasets. The data was not shuffled to ensure that validation data primarily consisted of unseen subjects. The PPG-PT model was trained over 500,000 iterations with a batch size of 64, and the ECG-PT model was trained over 560,000 iterations with the same batch size. After every 2000 iterations, the models were evaluated for a further 200 iterations to measure validation loss. In both cases the learning rate was set as 3 \u00d7 10-4 within the AdamW optimiser. To train PPG-PT, it took just over 5 days on an RTX A2000 12GB GPU, and to train ECG-PT it took almost 6 days. Optimisation loss was measured using cross entropy, by mapping the N \u00d7 101 model outputs (logits) where the 101 represents all possible tokens, to the target tokens which were the next token values for all points in the input. The final validation loss of the PPG-PT model was 1.2, corresponding to roughly a third of prediction being perfect. The final validation loss of the ECG-PT model was 1.8 (1 in 6 predictions are perfect). The higher validation loss for ECG-PT was likely due to the high proportion of abnormal heart rhythms in the training and validation datasets."}, {"title": "2.4.2 Atrial Fibrillation Fine-Tuning", "content": "The final linear layer, which converts the output from dmodel dimensions to the dimensions of the vocabulary size was replaced with a linear layer which instead converts the output from dmodel dimensions to 1 dimension. This new output then was passed through a sigmoid activation function to scale the values between 0 and 1, and the final value was used to classify the signal as either healthy (0) or atrial fibrillation (1). This same conversion of the final linear layer of the model could"}, {"title": "2.5 Cross Entropy Loss vs Mean Squared Error", "content": "Conventionally, time series transformer models produce a continuous output values for each token prediction and are thus trained with a mean squared error loss function [20]. However, unlike many time series prediction tasks, we were able to leverage the highly periodic nature of the PPG and ECG signals to create models with a well-defined small vocabulary. This meant that we were able to train the model in a similar fashion to a conventional large language model, with a cross-entropy loss function. It should be noted that we did also experiment with continuous output values and mean squared error as a loss function, but these results were less able to capture long range trends and importantly lacked interpretability in the attention weights. A full investigation into this finding is beyond the scope of this work."}, {"title": "3 Generative Model Evaluation", "content": "In training, for all points in the context window the model outputs logits to which a softmax function can be applied, giving a predicted probability distribution of the next token over all possible tokens. For generation of future tokens, the final probability distribution is taken, representing the possible token values for the next token. The multinomial function in PyTorch is then used to sample this distribution and generate a predicted next token. Each new predicted token can then be appended to the previous context, and the last 500 tokens in this new context are then used to predict the next token. This process can be repeated until a maximum number of generated tokens has been reached. Another way to examine the predictive capabilities of each model would be to predict the next token as the one with the highest probability, instead of sampling from a distribution. The problem with this second approach is that sometimes in low frequency periods, such as a trough in PPG or a period between P-QRS-T regions in the ECG, taking just the token with maximum likelihood could allow the model to become stuck in predicting the sample token value over and over again. Because of this issue, the first approach of sampling from a probability distribution was chosen to evaluate the models.\nFor the evaluation of generative capabilities of each model, the pre-processed subset of the Bed-based ballistocardiography dataset [21] [22] was used. This dataset consists of simultaneously recorded PPG and ECG from 40 subjects, sampled at 1kHz. In the preprocessed subset, the PPG was low-pass filtered with a cut off of 10Hz and the ECG was band-pass filtered between 1 and 40Hz. As with the previous datasets, we resampled the PPG to 50Hz, and the ECG to 100Hz, before converting the signals to tokens. The 750 sample windows were extracted and split into 500 samples to form the context and 250 samples to test the prediction accuracy of the model. For each prediction point, we calculated the absolute distance between the predicted token and true token. We were then able to calculate the median and inter-quartile range of absolute prediction error against the prediction horizon for both the PPG-PT and ECG-PT models. In Figure 1(a,c) example generation waveforms"}, {"title": "4 Interpretability of aggregate model attention", "content": "As is the case with GPT models [1], our pre-trained transformer models (PPG-PT and ECG-PT) are trained to predict the next token at each point, by using the attention mechanism to collate knowledge from previous tokens. It is then natural to ask ourselves: given the task of predicting the next token in a periodic signal, which tokens would one look at in order to make that prediction? The obvious answer to this question is to look at tokens which are at the same point in the previous cycles as the token that we want to predict. For example, if the current token to predict was a peak, it would be logical to look at all previous peaks, with an emphasis on peaks of similar height and width, in order to make an accurate prediction. However, the information we start with is just the position of a token in the context window and the value of a token. Given that the fundamental frequency changes between contexts and that different points in the same cycle can have the same value, we cannot rely on the initial position and value information alone to find similar points in the same cycle. In order to understand the point at which a token lies in a cycle, it is therefore necessary to first look at the context of surrounding tokens. Once this relationship between a token and its local cycle is understood, attention can broaden to look at all cycles."}, {"title": "5 Vector similarities between points of interest", "content": "In the previous section, we demonstrated that the pre-trained transformer models logically focus on similar points in previous cycles of PPG or ECG when trying to predict the next token. This strongly indicates that the models are able to distinguish different points in the cycle of PPG and ECG waveforms. In this section, we aim to solidify this by examining how the cosine similarity of embedded tokens with the same value, which occur at different distinct points in the PPG and ECG cycle, change upon propagation through the model. To investigate this, we chose tokens at similar points on rising slopes and falling slopes on the PPG signal, and on the T-wave of the ECG signal. The similarity of these tokens was calculated with respect to the final rising slope in each context window. If the models are effective at distinguishing different points in the cycle, we would expect rising slopes to cluster with each other in vector space, and falling slopes to form a separate cluster in vector space. The parallel of this experiment in a large language model would be to look at the vector similarity of homonyms (words that have the same spelling but can have multiple different meanings), and examine how the vector similarity changes based on specific context in a sentence. For example,"}, {"title": "6 Attention Maps of Individual Attention Heads", "content": "Now that we know that the base pre-trained PPG-PT and ECG-PT models look at similar points in previous cycles in order to predict the next token, and that the models have knowledge of the context of different points within a PPG and ECG cycle, the final step is to examine if the individual attention heads look for specific high level features in the PPG and ECG signals. To investigate this, we examined example contexts from the generative evaluation dataset which contained the common features of each signal. For example, a PPG context which has well-defined peaks, troughs, and a dicrotic notch, and an ECG context which has a well defined P-wave and QRS complex.\nFor each model, upon inputting the context, we looked at the final row in the attention weights of each head in the last transformer block, corresponding to the weights associated with predicting the next token. Upon saving these weights, we inputted a new context corresponding to the original shifted by 1 sample into the future. We then saved these weights and added the final N-1 weights to the last N-1 original weights. We repeated this shift and add operation a further N-2 times, and divided each of the N weights element-wise by the number of additions made to them. This gave us an average map of attention to the context for the next 500 token predictions, for each self-attention head in the final transformer block. We then used the findpeaks function in MATLAB, with a minimum peak distance of 15 tokens and a minimum peak height of 0.5 of the maximum attention weight in the window, to highlight the local maximas in attention.\nFigure 4 highlights that individual attention heads do indeed pay attention to important features in the signal of interest. In particular, PPG-PT has an attention head that looks for peaks, another that looks for troughs and another which looks for the dicrotic notch. In addition, we find that ECG-PT has a head which looks for the Q portion of the QRS, another which looks for R-peaks and another that looks for P waves. Therefore, on top of an ability to distinguish all separate points in a PPG and ECG cycle, the pre-trained models presented in this paper also pay specific attention to some of the most physiological relevent features."}, {"title": "7 Change in Attention when Screening For Atrial Fibrillation", "content": "Through pre-training we have obtained models which have a strong grasp on the important features of PPG and ECG, and generalise well to unseen morphologies. However, rather like in LLMs, predicting the next token of PPG and ECG lacks utility. To fully make use of these pre-trained models, we fine tune the models to classify atrial fibrillation (AF), as outlined in Section 2.4.2. Atrial fibrillation is the most common arrhythmia, characterised by an irregular heart rate where there can be rapid increases in heart rate and periods where it slows down dramatically [24]. The fine-tuned AF-PPG-PT model achieved a leave-one-subject-out area under the curve (AUC) of 0.93 when cross validated across all test subjects, and 0.96 when two subjects with poor signal to noise ratio were excluded. The fine-tuned AF-ECG-PT model achieved an AUC of 0.99 when cross validated across all test subjects. It is worth reiterating that these results were achieved with 11 minutes of fine-tuning computer time, corresponding to roughly 0.2% of the total time it took to train the base models.\nThe key result of fine-tuning is not just the accuracy, but the interpretability of the results. To ascertain why a model made a classification, we simply aggregated the final row of soft-maxed attention weights across all heads in the final transformer layer for the base model and compared with the fine-tuned model. Any increases in attention weights therefore indicates that the model deems the corresponding tokens valuable for classification of AF. In Figure 5(a) two representative attention maps are displayed for the classification of a PPG context as AF. It is clear in both cases that the model increases attention on periods where beats occur later than expected on earlier than expected, which is the obvious characteristic of AF. This is even clearer in the fine-tuned ECG attention, given that peaks in ECG are more precisely localised in time. In Figure 5(b), an example is highlighted where a beat is expected to occur based on the previous beat timing, but it does not and thus the model attention spikes. A second example is also highlighted, where based on the previous beat timing a beat occurs much earlier than expected, and model attention therefore spikes exactly at this point. These interpretable maps of shifts in attention, demonstrating the reason why the model has made the classification, can easily be provided along with with the probability of AF."}, {"title": "8 Conclusion", "content": "This work has conclusively demonstrated that GPT-like models, trained to predict the next token in periodic physiological time series, are fully interpretable in their operation. This has been illustrated through aggregate attention maps which are logical for the task of predicting the next token and through the clustering of different PPG and ECG features in vector space. This is further shown through individual attention heads that respond strongly to specific features such as the dicrotic notch in PPG or the P-wave in ECG. Moreover, we have shown that this interpretability is carried forward when fine-tuning for the classification of atrial fibrillation, where attention shifts to regions in the input context that most indicate the presence of the arrhythmia. This work represents a step forward in the explainability of large transformer networks when applied to healthcare settings."}]}