{"title": "Enhancing Financial Fraud Detection with Human-in-the-Loop Feedback and Feedback Propagation", "authors": ["Prashank Kadam"], "abstract": "Abstract\u2014Human-in-the-loop (HITL) feedback mechanisms can significantly enhance machine learning models, particularly in financial fraud detection, where fraud patterns change rapidly, and fraudulent nodes are sparse. Even small amounts of feedback from Subject Matter Experts (SMEs) can notably boost model performance. This paper examines the impact of HITL feedback on both traditional and advanced techniques using proprietary and publicly available datasets. Our results show that HITL feedback improves model accuracy, with graph-based techniques benefiting the most. We also introduce a novel feedback propagation method that extends feedback across the dataset, further enhancing detection accuracy. By leveraging human expertise, this approach addresses challenges related to evolving fraud patterns, data sparsity, and model interpretability, ultimately improving model robustness and streamlining the annotation process.", "sections": [{"title": "I. INTRODUCTION", "content": "Financial fraud detection is essential for maintaining the security and integrity of financial systems. As fraud techniques become more sophisticated, traditional detection methods struggle to effectively identify and prevent fraud. Machine learning (ML) techniques have emerged as powerful tools, leveraging large datasets to detect patterns and anomalies indicative of fraud. However, these systems face challenges such as the need for extensive labeled data, the dynamic nature of fraud, and the complexity of domain-specific knowledge. Human-in-the-loop (HITL) feedback mechanisms offer a promising solution to these challenges by incorporating human expertise into the ML process.\nHITL involves active human participation in the machine learning pipeline, providing critical insights, annotations, and feedback to enhance model performance. It addresses key issues such as limited labeled data, model interpretability, and adapting to evolving fraud patterns. In financial fraud detection, HITL systems leverage domain knowledge to identify subtle patterns that automated models might overlook. This allows for more accurate model training and validation, reducing false positives and ensuring better fraud detection.\nFraud detection presents several challenges ideal for HITL, including imbalanced datasets, adversarial fraudsters, and complex fraud patterns. Fraudsters continually adapt to evade detection, creating an environment where models must be updated frequently. Graph-based approaches, which model transactions as networks, have shown promise in capturing complex fraud patterns but require expert optimization.\nIn this paper, we introduce a HITL framework for financial fraud detection, combining human expertise with advanced ML techniques. Our approach incorporates annotation from proprietary and public datasets, interactive model training, and a novel feedback propagation algorithm. We evaluate the impact of HITL feedback using standard metrics, demonstrating improvements in detection accuracy, robustness, and interpretability.\nBy integrating HITL into fraud detection systems, we improve data annotation, model interpretability, and adaptability to dynamic fraud patterns. Our proposed framework combines advanced ML techniques with a novel feedback propagation method, significantly enhancing fraud detection performance across various algorithms. This research highlights the potential of HITL to improve both traditional and state-of-the-art methods in financial fraud detection while introducing a novel technique for propagating feedback signals throughout the dataset."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "A. Human-in-the-Loop (HITL) Feedback\nHuman-in-the-Loop (HITL) systems have become integral to modern machine learning, addressing challenges in data processing and model training by enhancing data quality, model interpretability, and performance [1]. HITL systems combine human intuition with machine learning, especially in data annotation where labeled data is scarce. For instance, systems proposed by Zhang et al. [2] and Liu et al. [3] demonstrate significant performance improvements with human feedback. HITL has also been used to refine models in tasks like question answering [4] and reading comprehension [5].\nBeyond annotation, HITL has been applied in domains such as computer vision, NLP, and medical applications. Gentile et al. [6] utilized HITL for interactive dictionary expansion, while Liu et al. [3] explored it in person re-identification. HITL systems handle complex tasks across fields, such as scene categorization [7], syntactic parsing [8], network anomaly detection [9], and outlier detection [10]. Ristoski et al. [11] proved HITL's utility in relation extraction.\nB. Financial Fraud Detection\nFinancial fraud detection has evolved from manual assessments to rule-based detection systems [26]. These systems, while easy to implement, lack adaptability to changing fraud patterns. Machine learning addressed these limitations with classifiers like Logistic Regression [14], Support Vector Machines [16], and Autoencoders with One-class SVMs [19]. KNN-clustering [20], Naive Bayes [22], and ensemble techniques like Random Forest [24] and XGBoost [25] further advanced fraud detection.\nGraph representation learning significantly improved fraud detection. Techniques like DeepWalk [27], node2vec [28], and LINE [29] evolved to advanced methods like Graph Convolutional Networks (GCNs) [31] and Graph Attention Networks (GAT) [33]. Semi-supervised methods [32], sampling-based GNNs [34], and Care-GNN [35] have addressed fraud detection challenges. Recent techniques like Split-GNN [38], BOLT [48], and RioGNN [49] demonstrated state-of-the-art results. GTAN [45] further improved results with a Gated Temporal Attention mechanism.\nDespite these advancements, challenges like adapting to dynamic fraud patterns and data sparsity remain, which could be mitigated by HITL."}, {"title": "III. METHOD", "content": "A. Manual Annotation\nManual annotation is a key part of our HITL framework, involving experts with domain knowledge in financial fraud annotating both proprietary and publicly available datasets. The annotation process consists of:\nData Preprocessing: Data is cleaned by standardizing formats, handling missing values, and performing exploratory analysis to understand feature distributions.\nAnnotation Guidelines: Clear instructions are provided to ensure consistency, defining fraudulent and non-fraudulent transactions. The isFraud label is added to each anchor (e.g., transaction or review) and scaled from 0 to 100 based on SME feedback.\nQuality Control: A multi-layered process ensures high-quality annotations through cross-checking, discussion, and periodic review to correct errors.\nB. Feedback Propagation\nDue to the infeasibility of manually annotating all transactions, we define an algorithm to propagate the feedback signal through the transaction graph. Typically, only 0.1-0.2% of transactions can be annotated, so propagation extends human feedback to the broader dataset.\n1) Graph Construction: We construct a transaction graph G, where each node represents a transaction with attributes such as email, phone number, and payment details. Nodes are connected if they share attributes, with edge weights corresponding to the number of shared attributes and their predefined importance (e.g., a phone number carries more weight than an email in telecom datasets).\nThe edge weight $W_{ij}$ between two transaction nodes i and j can be calculated as follows:\n$W_{ij} = \\sum_{k=1}^{m} w_k \\delta(a_i^k, a_j^k)$ (1)\nwhere m is the number of attributes. $w_k$ is the predefined weight of the k-th attribute. $a_i^k$ and $a_j^k$ are the k-th attributes of transactions i and j, respectively. $\\delta(a_i^k, a_j^k)$ is an indicator function that equals 1 if $a_i^k = a_j^k$ and 0 otherwise.\nThus, the transaction graph G can be defined as G = (V, E, W), where V is the set of transaction nodes. E is the set of edges between nodes. W is the set of edge weights.\n2) Initialization: To begin with, the transactions that have been annotated as fraudulent by Subject Matter Experts (SMEs) are given an isFraud score $S_i = 100$ where $S_i \\in [0,100]$. This score is different from the actual target label and is stored as a property on the transaction node.\n3) Propagation: In this phase, all the neighboring nodes of the annotated nodes are scored iteratively up to n hops. The score is further discounted as we move away from the originally annotated nodes and the propagation is stopped at n hops when the highest change in score for a given step of propagation falls below the convergence criterion $\\epsilon$ (usually set as a lower value). The score of the immediate neighbor j of a node i after one step of feedback propagation can be given by:\n$S_j^{(h)} = S_j^{(h)} + S_i^{(h-1)} \\times \\frac{W_{ij}}{max(W)} \\times Sim(i, j)$ (2)\nwhere $S_j^{(h)}$ is the score of node j at hop h. $S_i^{(h-1)}$ is the score of node i at hop h \u2013 1. $W_{ij}$ is the edge weight between nodes i and j. max(W) is the maximum possible edge weight in the graph. Sim(i, j) is the similarity between nodes i and j."}, {"title": "Algorithm 1 Feedback Propagation Algorithm", "content": "Once the highest change in score for a given step of propagation falls below the convergence criterion, the propagation is stopped:\n$\\Delta S_{max}^{(h)} < \\epsilon$ (3)\nwhere $\\Delta S_{max}^{(h)}$ is the highest change in score at hop h between two distinct nodes. $\\epsilon$ is the convergence criterion.\nRequire: Transaction graph G = (V, E, W), Initial scores Si for annotated nodes, Maximum number of hops n, Convergence criterion e\nEnsure: Updated scores Si for all nodes in G\n1: Initialize isFraud scores Si by manual annotation\n2: Set h \u2190 1\n3: while h < n do\n4: $\\Delta S_{max} \\leftarrow 0$\n5: for each edge (i, j) \u2208 E do\n6: if $S_i^{(h-1)} > 0$ then\n7: Calculate $S_j^{(h)}$ using Equation 2\n8: $\\Delta S = |S_j^{(h)} - S_j^{(h-1)}|$\n9: if $\\Delta S > \\Delta S_{max}$ then\n10: $\\Delta S_{max} = \\Delta S$\n11: end if\n12: end if\n13: end for\n14: if $\\Delta S_{max} < \\epsilon$ then\n15: break\n16: end if\n17: h \u2190 h + 1\n18: end while\n19: return Si for all nodes"}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets\n1) PFFD: Our Proprietary Financial Fraud Dataset (PFFD) consists of 1.25 million transactions sourced from e-commerce clients. Each transaction includes hashed details such as Name, Address, Email, Phone, Device, and Payment data, ensuring privacy. The target label, Fraud Score (fs), ranges from 0 to 1000, with 1.07% of the data labeled as fraudulent. The isFraud label, scaled from 1 to 100, serves as a feedback signal, annotated by SMEs. The graph has an average node degree of 14.71 with around 29,000 hypernodes.\n2) Yelp Dataset: We also tested our approach on the Yelp-Fraud dataset [35]. Each node represents a review, and attributes like product and user ID are stored as properties. We modeled this graph similarly to the PFFD, enabling us to apply the same techniques to detect fraudulent reviews.\nB. Training and Evaluation\nSMEs annotated 3457 nodes with isFraud labels in the PFFD dataset. Cosine Similarity [47] was used as the similarity measure during feedback propagation to evaluate connections between transactions. Training was done in mini-batches, with a 60:20:20 train, validation, and test split. During training, each mini-batch was used to create sub-graphs, which were fed into both tabular and graph ML algorithms. Regularization techniques like dropout and L2 were applied to prevent overfitting. Hyperparameter tuning was conducted during validation, and testing was done chronologically across 10 equal parts. After each test set, SMEs provided additional annotations, and feedback propagation was applied to update node scores.\nC. Baselines\nWe compared HITL with several baselines, including Logistic Regression [14], SVM [16], Random Forest [24], and advanced GNN models such as GCN [31], RGCN [39], and GAT [33]. We also compared with state-of-the-art methods like CareGNN [35], Semi-GNN [32], and BOLT [48], ensuring a comprehensive evaluation across tabular and graph-based algorithms.\nD. Experimental Setup\nData was loaded into a Neo4j graph database, with sub-graphs extracted for mini-batches using Cypher queries. The training, validation, and test sets were sequentially loaded to avoid future data leakage. Implementations from PyTorch Geometric were used for GNNs, and sklearn [51] was used for tabular methods. All training was conducted on the Databricks platform with NC12 v3 instances supporting 2 NVIDIA Tesla V100 GPUs.\nTo ensure fair evaluation, the test set was divided into 10 equal parts, with 150 transactions from each class randomly sampled for annotation by SMEs. After annotation, feedback propagation was performed across n hops, and subsequent evaluations were triggered.\nEvaluation Metrics: Given the imbalanced nature of the data, we used ROC-AUC and Recall to measure model performance.\nE. Ablation Studies\nWe performed ablation studies across three different conditions:\nModels without Feedback\nModels with HITL Feedback\nModels with HITL Feedback and Feedback Propagation\nThe results in Table I reflect performance across these scenarios, helping quantify the contributions of HITL and feedback propagation in enhancing model accuracy."}, {"title": "V. RESULTS", "content": "A. Overall Performance Improvement\nAll the algorithms perform better with the inclusion of feedback and show further improvement with feedback propagation. Even a small number of annotated transactions, when aided with feedback propagation, were able to catch changing patterns in fraud over time. The inclusion of human annotations results in higher quality training data, leading to improved model performance. Models trained with annotated data exhibit better generalization to unseen fraud patterns and lower false positive rates. Table I shows the AUC and Recall values for each of the algorithms without HITL, with HITL, and with HITL and Feedback Propagation. For the PFFD dataset, the average improvement in AUC from without HITL is 7.24%, and further from HITL to HITL with Feedback Propagation is 2.19%. For Recall, the improvements are 6.81% and 2.33% respectively. For the Yelp fraud dataset, the improvements in AUC are 5.32% and 2.07%, and in Recall are 4.31% and 2.81%.\nB. Tabular vs. Graph Algorithms\nTabular algorithms show the least amount of improvement with feedback, while graph algorithms benefit considerably. This difference is due to the interactions with the immediate neighbors that graph algorithms take into consideration, allowing them to better capture complex relationships between transactions. For tabular algorithms on the PFFD dataset, the average improvement in AUC from \u201cwithout feedback\" (w/o FB) to \"with feedback\" (w/ FB) is 3.46%, while the improvement from w/ FB to \"with feedback propagation\" (w/ FP) is 2.72%. Similarly, the average improvement in Recall from w/o FB to w/ FB is 2.83%, and from w/ FB to w/ FP is 1.64%. In the case of graph algorithms on the PFFD dataset, the average improvement in AUC from w/o FB to w/ FB is 9.06%, and from w/ FB to w/ FP is 2.21%. The average improvement in Recall from w/o FB to w/ FB is 9.09%, and from w/ FB to w/ FP is 2.59%. For tabular algorithms on the Yelp dataset, the average improvement in AUC from w/o FB to w/ FB is 3.71%, and from w/ FB to w/ FP is 2.05%. The average improvement in Recall from w/o FB to w/ FB is 2.37%, and from w/ FB to w/ FP is 2.66%. For graph algorithms on the Yelp dataset, the average improvement in AUC from w/o FB to w/ FB is 6.11%, and from w/ FB to w/ FP is 1.94%. The average improvement in Recall from w/o FB to w/ FB is 4.99%, and from w/ FB to w/ FP is 3.14%.\nC. Progressive Improvement During Testing\nAfter each test set and feedback propagation cycle during testing, the performance of almost all algorithms improves further. This progressive improvement demonstrates the dynamic adaptability of the models in response to newly annotated data and propagated feedback signals. Figure 2 shows progressive improvements in the performance of all the evaluated algorithms without HITL, with HITL, and with HITL and Feedback Propagation.\nD. Ablation Study Findings\nAblation studies reveal that both human annotations and feedback propagation are crucial for achieving the best performance. Removing either component results in a significant drop in accuracy and other performance metrics. For instance, in the PFFD dataset, the average improvement in AUC from without HITL to HITL is 7.24%, and further from HITL to HITL with Feedback Propagation is 2.19%. The Recall improvements for the same transitions are 6.81% and 2.33% respectively. In the Yelp dataset, the AUC improvements from without HITL to HITL and from HITL to HITL with Feedback Propagation are 5.32% and 2.07% respectively, while the Recall improvements are 4.31% and 2.81% respectively. This underscores the importance of combining human expertise with advanced machine learning techniques to develop robust fraud detection systems.\nThese results validate the proposed HITL framework and highlight the potential of integrating human expertise into machine learning models for financial fraud detection. The combination of high-quality annotations and effective feedback propagation leads to robust, adaptable, and interpretable fraud detection systems."}, {"title": "VI. CONCLUSION", "content": "This study demonstrates the significant benefits of integrating human-in-the-loop (HITL) feedback and feedback propagation in financial fraud detection. By incorporating human expertise through manual annotations and propagating this feedback through transaction graphs, we achieve notable improvements in model performance. Our experiments confirm that graph-based models benefit the most from HITL feedback, showing significant gains in precision, recall, and robustness. The progressive improvements with each feedback cycle validate the adaptability of our approach, effectively capturing evolving fraud patterns even with minimal annotations. Ablation studies further emphasize the importance of both human annotations and feedback propagation, with either component's removal leading to a drop in accuracy. In conclusion, the proposed HITL framework offers a robust and interpretable solution for financial fraud detection, showcasing the powerful synergy between human expertise and machine learning. Future work could extend this framework to other domains and explore further enhancements in feedback mechanisms and propagation techniques."}]}