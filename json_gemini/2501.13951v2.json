{"title": "A Layered Multi-Expert Framework for Long-Context Mental Health Assessments", "authors": ["Jinwen Tang", "Qiming Guo", "Wenbo Sun", "Yi Shang"], "abstract": "Long-form mental health assessments pose unique challenges for large language models (LLMs), which often exhibit hallucinations or inconsistent reasoning when handling extended, domain-specific contexts. We introduce Stacked Multi-Model Reasoning (SMMR), a layered framework that leverages multiple LLMs and specialized smaller models as coequal \"experts\". Early layers isolate short, discrete subtasks, while later layers integrate and refine these partial outputs through more advanced long-context models. We evaluate SMMR on the DAIC-WOZ depression-screening dataset and 48 curated case studies with psychiatric diagnoses, demonstrating consistent improvements over single-model baselines in terms of accuracy, F1-score, and PHQ-8 error reduction. By harnessing diverse \"second opinions\", SMMR mitigates hallucinations, captures subtle clinical nuances, and enhances reliability in high-stakes mental health assessments. Our findings underscore the value of multi-expert frameworks for more trustworthy AI-driven screening.", "sections": [{"title": "I. INTRODUCTION", "content": "Mental health remains a pressing public health concern, complicated by stigma, limited clinical services, and socioeconomic barriers [1], [2]. Advances in large language models (LLMs) such as GPT and Mistreal offer potential for discreet, scalable mental health screenings [3], helping individuals who might otherwise be reluctant to seek professional help due to judgment or resource constraints.\nHowever, using LLMs in high-stakes mental health assessments poses distinct challenges. First, mental health data are inherently subjective: while instruments like Patient Health Questionnaire-8 (PHQ-8) provide structured guidelines, the interpretation of symptoms can vary significantly from one conversation or individual to another [4], [5]. Subtle linguistic nuances and personal expressions often shape how distress is perceived. Second, long and complex transcripts\u2014such as multi-turn interviews or narrative case studies\u2014can overwhelm advanced models, leading to hallucinations or inconsistencies [6], [7]. Third, specialized tools like PHQ-8 offer only partial mitigation; a single LLM \u201cexpert\" might still fail to capture nuanced details without additional context or repeated checks [8]. Although iterative reassessments and third-party evaluations can refine accuracy, they often rely on structured prompts or assume a known \"best\" configuration.\nIn response, we propose Stacked Multi-Model Reasoning (SMMR), a framework in which diverse \"expert\" models collectively assess the same input without any being deemed categorically superior. SMMR leverages multiple reasoning steps, consolidating \"second opinions\" in a layered manner:\n1) Multiple Experts, One Pipeline: Early layers employ smaller or specialized LLMs to generate preliminary assessments, treating each as an independent perspective.\n2) Iterative Refinement: Subsequent layers integrate and reconcile these varying outputs, using long-context models to produce a cohesive final judgment.\n3) Robust Mental Health Screening: By adopting a multi- expert approach, SMMR reduces hallucinations and inconsistencies common in extended conversations, improving diagnostic reliability for clinical interviews and narrative case studies.\nWe evaluate SMMR on two complex mental health datasets: the DAIC-WOZ dataset [9], designed for studying psychological distress (anxiety, depression, PTSD), and a curated set of narrative case studies drawn from real-world clinical scenarios. Our experiments demonstrate that SMMR offers a practical way to provide \u201csecond opinions\u201d within a unified workflow-without requiring a single preselected model or intricate prompt engineering [10], [11]. By treating all models initially as independent experts, then effectively aggregating imperfect prompts and diverse outputs, SMMR navigates the subjective and nuanced landscape of mental health assessments more reliably, pointing toward safer AI-assisted screenings."}, {"title": "II. BACKGROUND", "content": "Artificial intelligence (AI) is reshaping mental health care by providing increasingly sophisticated analytical tools and treatment supports. Early systems relied on simple word-counting and text analysis [12], but the advent of large language models (LLMs) has radically expanded these capabilities. Modern AI-driven applications\u2014such as semi-automated screening platforms, interactive self-reporting interfaces, and responsive mental health chatbots now enable more nuanced interpretation of human language and broader access to mental health services [13].\nDespite these advances, many solutions remain constrained by short-context analyses or single-model pipelines. For instance, Ohse et al. [14] investigate NLP models like BERT and GPT-4 for depression detection, yet their approach primarily relies on individual model outputs. Agrawal [15] highlights the potential of enhanced prompt engineering to improve explainability and intervention planning in mental health AI, though it too centers on a single-model framework. Similarly, Tang and Shang [8] propose a GPT-based system for pre-screening mental health disorders, demonstrating that domain-specific fine-tuning can improve early detection. However, these methods do not fully address the extended, subjective nature of clinical transcripts or case studies where long-format reasoning and multiple perspectives may be crucial.\nIn the following section, we introduce Stacked Multi-Model Reasoning (SMMR), a layered framework designed to overcome the limitations of single-model or short-context approaches. By integrating multiple \"expert\" LLMs and specialized models within a unified pipeline, SMMR offers more robust error checks, interpretive consistency, and the capacity to reconcile diverse viewpoints in subjective mental health data."}, {"title": "III. METHOD", "content": "Motivation and Overview: Long-form mental health assessments often require careful synthesis of complex, multi-turn data. Even advanced Large Language Models (LLMs) can become prone to hallucinations or inconsistent reasoning in these extended contexts. To address this problem, we propose Stacked Multi-Model Reasoning (SMMR), a framework that treats each LLM (or smaller specialized model) as an independent \"expert,\" without a priori knowledge of which model might be \u201cbest.\u201d By layering multiple models each offering a second opinion\u2014SMMR leverages collective insights to mitigate the weaknesses of any single model. Figure 1 illustrates the conceptual architecture of SMMR, while Algorithm 1 provides pseudocode for the overall process.\nIn this initial step, SMMR applies multiple single-step models, each independently processing the input X. These models could be smaller LLMs or specialized classifiers that excel at short-context tasks. Since we do not assume any model to be inherently superior, all outputs are aggregated on equal footing, forming R\u2081 for further refinement.\nSubsequent layers introduce long-context LLMs capable of handling extended or detailed inputs. Each model in layer k takes the aggregated output Rk\u22121 from the previous layer and refines it, generating new outputs r. These outputs are then aggregated into Rk. The iterative process allows each layer to provide a \"second opinion,\" reconciling discrepancies and minimizing hallucinations or inconsistencies that could arise from any single model.\nIn the last layer, a single, long-context Reliable Model MN synthesizes the refined outputs from RN-1 into the ultimate result Rfinal. This top-tier model is selected for its stable performance on nuanced, long-context data-making it especially well-suited for high-stakes mental health evaluations.\nTo ensure optimal performance and computational efficiency, the SMMR framework incorporates a dynamic stopping mechanism. After each layer processes and refines the aggregated outputs from the preceding layer, the framework evaluates the current performance using predefined metrics (e.g., Accuracy, F1-score, MAE). If the performance metrics improve compared to the previous layer by a significant threshold 8, SMMR proceeds to the next layer for further refinement. This iterative process continues until adding another layer does not result in performance gains beyond the threshold. The final output used for evaluation is thus the result from the layer that achieved the highest performance metrics, optimizing both accuracy and resource utilization.\nBy dividing the reasoning process into distinct layers and integrating diverse model outputs at each stage, SMMR effectively mitigates the risk of hallucinations and maintains stronger consistency for complex mental health tasks."}, {"title": "B. Datasets and Task Setup", "content": "In order to demonstrate the effectiveness of our Stacked Multi-Model Reasoning (SMMR) framework, we evaluate on two complementary mental health datasets: (1) an externally sourced dataset (DAIC-WOZ) [9], and (2) a curated collection of narrative case studies. Our primary task involves predicting mental health risk and severity, operationalized through PHQ-8 scores or binary labels, based on extended transcripts or descriptive case data. By adopting a layered approach that draws on multiple \u201cexpert\u201d models, SMMR aims to reduce hallucinations, enhance diagnostic fidelity, and improve the reliability of long-context LLM-based assessments.\nA private dataset consists of 187 labeled interviews designed to assess psychological distress such as anxiety, depression, and PTSD. Each interview includes a PHQ-8 score and its corresponding binary label. Following the standard protocol in [9], we split the dataset into training and testing subsets. To create a more realistic, long-context input for our models, each conversation was concatenated into a single data stream by aligning segments according to the speaker's starting time. The consolidated dataset includes references to the speaker, the content of each segment, and punctuation marks consisting of a period followed by a slash (./) to denote the end of each speaking turn. We evaluate model outputs using:\n\u2022 PHQ-8 Score Estimation (0-24): We compare the predicted PHQ-8 score to the ground truth, measuring accuracy with metrics such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).\n\u2022 Binary Classification (PHQ-8 > 10): The model's output is thresholded at 10 to indicate the presence or absence of clinically significant depressive symptoms.\nCase Study Dataset: To complement the interview-style data from the DAIC-WOZ database, we collected 48 narrative case studies featuring professionals' concluded psychiatric diagnoses. These cases, sourced from academic texts and clinical literature, include detailed demographic, behavioral, and contextual information. We manually extracted binary conclusions and types of disorders from the diagnoses provided by clinical professionals to verify the presence or absence of mental health concerns. Of these case studies, six are formatted as conversational transcripts, while the remaining 42 are presented in a descriptive format.\nBecause these cases do not provide PHQ-8 labels, we adopt an alternative evaluation scheme:\n\u2022 Mental Concern (0, 1, 2): Determines whether the case indicates no mental health issue (0), presence of a mental health issue (1), or if the conclusion is indeterminate from the data (2).\n\u2022 Disorder-Type Identification: Extracts the specific mental health disorder(s) (if any) from the text. We measure accuracy by comparing the model-identified disorders to a reference list of ground-truth labels, considering minor variations in naming as valid matches.\nModel Selection: We initially conducted a pilot test with the smaller, local Mistral model to explore on-premise feasibility. However, the majority of outputs were invalid or incomplete, likely due to context window constraints and the domain-specific nature of the data. Consequently, we omitted local models and go with GPT models but note that future work could revisit them if they become more robust.\nTo Evaluate: For each dataset instance-whether an interview transcript from DAIC-WOZ or a narrative case study-SMMR processes the entire text in a layered manner:\n1) Layer 1 applies multiple smaller or specialized models in parallel to gather initial assessments.\n2) Layers 2 through N 1 employ advanced long-context LLMs to refine, reconcile, and aggregate these preliminary outputs.\n3) Layer N employs a single, reliable long-context model to finalize the assessment and generate the ultimate decision, whether it be a binary classification or a PHQ-8 score.\n4) Dynamic Stopping: The SMMR framework automatically terminates additional layering when further layers do not yield performance improvements beyond a predefined threshold.\nThis pipeline effectively creates multiple \"checkpoints\" for error correction and multi-expert verification, ultimately aiming to improve the reliability of mental health evaluations in lengthy and complex conversations."}, {"title": "IV. RESULTS", "content": "We evaluated the SMMR framework on both DAIC-WOZ and our curated case studies, comparing its performance against single-model baselines. Tables II and III present the key metrics, reflecting a consistent advantage for SMMR-enhanced approaches."}, {"title": "A. DAIC-WOZ Performance", "content": "Table II highlights the performance of GPT-3.5-turbo and GPT-4-turbo across training, testing, and validation splits. Alongside classification measures such as Accuracy (Acc.), F1, Macro F1 (MF1.), Macro Precision (MPrec.), Macro Recall (MRec.), and ROC AUC (for PHQ-8 > 10 classification), we also track Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) for PHQ-8 scoring. Overall, the SMMR-enhanced models outperform baselines on every split, with particularly notable improvements in binary classification accuracy and F1. For example, GPT-3.5-turbo sees its testing accuracy jump from 0.55 to 0.76, while MAE decreases significantly (6.04 to 4.22), suggesting better handling of nuanced long-context data.\nImportantly, SMMR also boosts consistency across different subsets. The validation accuracy for GPT-3.5-turbo, for instance, jumps from 0.60 to 0.80, showing that leveraging multiple \"experts\" and iterative refinement reduces hallucinations and more accurately captures PHQ-8 severity signals."}, {"title": "B. Case Study Dataset Performance", "content": "Table III summarizes results on our 48 narrative case studies, where we evaluate binary detection of mental health concerns and the correctness of disorder-type identification. Although the baseline metrics here are relatively high for GPT-4 variants (exceeding 0.90 accuracy), SMMR demonstrates small but meaningful gains and, in some configurations, yields a 100% valid output rate. Notably, GPT-3.5 sees its accuracy rise from 0.91 to 0.93 under SMMR, while its F1 measure improves from 0.95 to 0.97. Beyond classification, SMMR also stabilizes disorder-type identification; for instance, the average correctness (Ave.) for GPT-4 climbs from 7.02 to 7.40, indicating richer extraction of relevant diagnostic details.\nIn summary, these findings confirm that layering multiple models curbs inconsistencies and incomplete responses, leading to more robust and thorough assessments in both interview transcripts and case narratives."}, {"title": "V. CONCLUSION", "content": "In this study, we introduced Stacked Multi-Model Reasoning (SMMR) as a layered solution for long-context mental health assessments. By progressively refining the outputs of multiple \"expert\" models, SMMR demonstrated notable improvements in diagnostic accuracy, F1-scores, and PHQ-8 estimation on both the DAIC-WOZ dataset and a curated set of case studies. These gains highlight how a structured, multi-expert approach can mitigate hallucinations, capture subtle clinical cues, and enhance consistency in sensitive, high-stakes domains.\nDespite these encouraging results, several limitations warrant attention. First, our model diversity was limited to commercial GPT variants; although these models are powerful, our findings may not generalize to local or open-source LLMs without further experimentation. Second, mental health assessments impose strict confidentiality and ethical constraints. While SMMR can reduce errors and inconsistencies, thorough clinical validation and prospective studies remain necessary to confirm its real-world safety and efficacy. Additionally, the data scarcity inherent in mental health research restricts the breadth of our evaluation, and the computational overhead of running sequential layers can pose scalability challenges. Finally, conflict resolution among layered \u201cexperts\u201d raises questions about decision transparency, bias, and the need for IRB-reviewed protocols when dealing with vulnerable populations.\nLooking ahead, we plan to expand SMMR by refining conflict-resolution strategies, exploring multi-modal data (e.g., audio and video cues), and investigating cost-effective deployments that balance performance with resource usage. We also intend to collaborate with clinical professionals to ensure that SMMR's layered outputs align with human judgment in ethically rigorous settings. Taken together, these developments could make SMMR a robust foundation for safer and more reliable AI-driven mental health screening, addressing the complexity and subjectivity that define this crucial application area."}]}