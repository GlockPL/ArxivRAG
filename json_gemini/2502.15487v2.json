{"title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models", "authors": ["Martina Miliani", "Serena Auriemma", "Alessandro Bondielli", "Emmanuele Chersoni", "Lucia C. Passaro", "Irene Sucameli", "Alessandro Lenci"], "abstract": "Large Language Models (LLMs) are increasingly used in tasks requiring interpretive and inferential accuracy. In this paper, we introduce ExpliCa, a new dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely integrates both causal and temporal relations presented in different linguistic orders and explicitly expressed by linguistic connectives. The dataset is enriched with crowdsourced human acceptability ratings. We tested LLMs on ExpliCa through prompting and perplexity-based metrics. We assessed seven commercial and open-source LLMs, revealing that even top models struggle to reach 0.80 accuracy. Interestingly, models tend to confound temporal relations with causal ones, and their performance is also strongly influenced by the linguistic order of the events. Finally, perplexity-based scores and prompting performance are differently affected by model size.", "sections": [{"title": "Introduction", "content": "Understanding cause-effect relationships is one of the hallmarks of human cognition (Pearl, 2009). The question of whether Large Language Models (LLMs) truly comprehend causal relationships in natural language texts, or merely perform as 'stochastic parrots' (Bender et al., 2021) by replicating statistical associations in their pretraining data, remains a topic of debate (Ze\u010devi\u0107 et al., 2023; Merrill et al., 2024). This question is crucial for the application of LLMs in domains that demand interpretive and inferential accuracy. The recent growth in causal research and benchmarking highlights the fundamental need for more reliable and interpretable models (Chen et al., 2024). LLMs should be able to interpret not only cause-effect relations but also the relations between causal and temporal aspects (Ning et al. (2018) as an example), which are often connected and overlapping (e.g., typically an effect temporally follows its cause).\nA common task for evaluating causal reasoning skills is Pairwise Causal Discovery (PCD), which focuses on determining the existence of a causal relation between two events and, if so, establishing which event serves as the cause and which one as the effect (Gao et al., 2023; Wan et al., 2024). However, this formulation does not directly take into account the tight bound between the cause-effect and the before-after relationships.\nIn this paper, we present ExpliCa, a dataset designed to evaluate LLMs in commonsense causal reasoning through PCD tasks. Our formulation of the PCD task for ExpliCa allows us to take into account also the entanglement of causal and temporal relations between events. This is achieved by considering sentence pairs and connective words that overtly express these relationships. In ExpliCa, each sentence describes an event (e.g. Martina has less chance of getting the flu and Martina has been vaccinated), and each connective word explicitly indicates either a causal relationship (i.e., so and because) or a temporal one (i.e., before and after). We collected acceptability ratings for each connective word with respect to each sentence pair, to account for both causal and temporal relations. To our knowledge, ExpliCa is the first dataset containing both causal and temporal explicit relations between events annotated by native speakers via crowdsourcing, rather than expert annotators.\nWe conducted a nuanced evaluation of a group of LLMs on ExpliCa. Its goal is to shed light on several key aspects. First, we aim to estimate whether and to what extent LLMs can model and distinguish causal and temporal relations, similarly to humans. Second, we want to assess potential differences between LLMs' competence and performance in our PCD task. Recent studies (Hu and Levy, 2023; Kauf et al., 2024) identified a discrepancy between the linguistic competence of models, as measured by log-probability, and the accuracy of their responses elicited via prompting, with the"}, {"title": "Related Work", "content": "The study of causality and its linguistic expressions garnered renewed and intensified interest, particularly in the context of evaluating the reasoning abilities of LLMs. Recent advancements led to the development of several specialized datasets, aimed at testing the causal reasoning of LLMs through hypothetical scenarios. Notable examples include CLadder (Jin et al., 2023), which assesses causal reasoning using questions based on formal rules; CausalBench (Wang, 2024), used for tasks related to mathematics, coding, and textual data; and CausalNet (Ashwani et al., 2024), which covers both causal and counterfactual questions. Unlike ExpliCa, these datasets focus on implicit notions of causality, which are not overtly expressed in the linguistic structures of the text.\nTo evaluate LLMs, several datasets have also been annotated with explicit causal relationships between events in texts. These range from multilingual educational content, as in MECI (Lai et al., 2022), or financial news (Mariko et al., 2022), to more diverse sources, such as CREST (Hosseini et al., 2021). Although these datasets do not explore the temporal dimensions of causality, many causal annotation schemas are derived from datasets that do annotate temporal relationships between events (e.g., BECauSE, Dunietz et al. 2017) or vice-versa (e.g., Temporal and Causal Reasoning, Ning et al. 2018), including those from news sources (e.g., Causal Time Bank, Mirza et al. 2014; Event StoryLine Corpus, Caselli and Vossen 2017), and from short commonsense narratives (e.g., CaTeRS, Mostafazadeh et al. 2016). However, such datasets do not leverage crowdsourcing annotation by native speakers for both causal and temporal relations as in ExpliCa. In our dataset, the ground truth is given by English native speakers' annotation collected via crowdsourcing, with the aim of addressing the complexity of distinguishing truly causal from merely temporal relations between events. A key challenge in evaluating LLMs using datasets with direct textual annotations of"}, {"title": "The ExpliCa Dataset", "content": "ExpliCa is designed to evaluate LLMs on commonsense causal reasoning through PCD tasks. It is composed of sentence pairs, where each sentence describes an event. Sentence pairs were in part adapted from sentences in existing datasets and in part manually crafted. Approximately a third of the sentence pairs were based on sentences from DeScript (Wanzare et al., 2016), e-Care (Du et al., 2022), and BIG-Bench (Srivastava et al., 2022).\nExpliCa includes 600 English sentence pairs, selected to be equally divided into three subsets: i.) CAUSAL subset, where the relationship is most likely causal (and possibly also temporal); ii.) TEMPORAL subset where the relation is expected to be only temporal; iii.) UNRELATED subset, including sentences that are thematically related but neither causally nor temporally. Sentence pairs are linked through words that explicitly signal either a causal or temporal relationship. These connectives act as linguistic cues, enabling a causal or temporal interpretation of events based on the compositional meaning of the sentences. Other than the TYPE of the relation intercurring among the events, the connectives specify the ORDER of the events in the relations, which can be ICONIC where the effect follows the cause and events are presented in their chronological order or ANTI-ICONIC, where the order is reversed. We join each sentence pair with each connective, also considering the reverse order. This way, we obtained 4,800 unique items (600 pairs \u00d7 4 connectives \u00d7 2 orders).\nAn English native speaker examined a sample of the dataset to validate if the connectives correctly express the nature of the relation and the order between the events. We detail the connectives below:\n\u2022 then - indicates a temporal relation in an iconic order: The first event precedes the second event;\n\u2022 after - indicates a temporal relation in an anti-iconic order: The first event follows the second event;\n\u2022 so - indicates a causal relation in an iconic order: The first event causes the second event;"}, {"title": "Lexical Association Bias", "content": "If unrelated sentence pairs presented very different lexical elements compared to causally or temporally related ones, this might affect a LLM behavior, and lead to biased results. In order to address this aspect, we computed the statical association strength via Pointwise Mutual Information (PMI) and Local Mutual Information (LMI) (Church and Hanks, 1990; Evert, 2004) between pairs of lexemes (nouns, verbs, or adjectives), one belonging to the first sentence and the other to the second sentence. MI scores were computed on UkWaC (Ferraresi et al., 2008). We averaged the MI scores of all the possible pairs of lexemes from a single sentence pair to obtain an item-level MI score. We applied the Wilcoxon test to check if there were statistically significant differences in the item-level-MI scores across the CAUSAL, TEMPORAL, and UNRELATED groups. We found that the statistical association of the sentence pairs in the three groups was not significantly different, both for LMI (W: 41, 576, p-value: 0.4312) and PMI (W: 38, 318, p-value: 0.4009). Thus, we can conclude that our dataset is free from lexical association biases."}, {"title": "Human Ratings", "content": "Each of the 4, 800 items in ExpliCa was annotated via crowdsourcing by 15 native English speakers. We asked the participants to assess the acceptability of each item by giving a rating from 1 to 10. The ratings were then averaged for each item, obtaining the average acceptability rating for each connective in each sentence pair, in both event directions. We assigned to each sentence pair a relation TYPE and a relation ORDER label based on the connective deemed more acceptable for humans. For example, if then obtained the highest average rating for a sentence pair (in a specific direction), this is labeled as TEMPORAL TYPE with an ICONIC ORDER. Sentence pairs for which no connective had a rating higher than 6 and with mean rating below 5 are labeled UNRELATED. Tab. 1 summarizes the cardinality of the different classes in the dataset."}, {"title": "Frequency Bias", "content": "The frequency of linguistic constructions affects the performances of LLMs (McCoy et al., 2023). For example, if the construction {listen, after, turn up} were more frequent than {listen, then, turn up}, a model might be biased towards the former.\nTo control for such frequency biases in ExpliCa, we proceeded as follows. We PoS-tagged and lemmatized the sentence pairs with SpaCy. Then, we extracted the verb from each sentence, and the connective used to join them. For copulative verbs, we also considered the noun or the adjective following the copula. We retained particles in phrasal verbs (e.g., turn up), and in sentences containing multiple verbs (e.g., with embedded clauses), we considered up to two verbs, prioritizing those most salient to the sentence meaning (e.g., Michele chose the pizza he wanted to eat [want, eat]). Then, we used the SketchEngine APIs to query the enTenTen21 corpus to compute the co-occurrence of the elements in the triplet {1st sentence verb, connective, 2nd sentence verb}. We divided the co-occurrences into frequency bins based on quartile ranges: RARE, UNCOMMON, COMMON, and FREQUENT. Fig. 2 shows the number of sentence pairs for each bin. While there are differences between classes, their distribution on the frequency spectrum shows no significant trends. This suggests that our dataset is relatively free from frequency biases."}, {"title": "Experiments", "content": "We evaluated models on ExpliCa using human ratings as ground truth across four tasks: three prompting tasks, and one perplexity evaluation. Prompting experiments were conducted under various settings, including few-shot and zero-shot setups, and employing either greedy search or the Outlines framework (Willard and Louf, 2023) for generating answers. We used accuracy as evaluation metric. To study the effect of the model parameter scale, we then compared several models of the Qwen2.5 family on the acceptability rating task and perplexity. Finally, we compared the models' rating distributions in the acceptability tasks to the human ones, and assessed their correlation.\nExperiments were ran on a single Nvidia A100 80GB GPU, for around 120 GPU hours. OpenAI models were queried via the proprietary API for an estimate of 7 GPU hours.\nPrompting Evaluation. This aimed to assess LLMs' generation abilities and analyze how performance varies based on task modeling. Specifically, we defined three tasks:\ni.) acceptability ratings - we adopted the same design used in the survey with human participants (Sec. 3.2). Items for which the model failed to provide a rating were assigned a score of -1;\nii.) cloze test - given a test item consisting of two sentences linked by a connective, we masked the connective and asked the model to choose the most suitable one out of a list of candidates. An out-of-list answer was considered a miss;\niii.) multiple-choice task - the model received a sentence pair with the four connectives marked as A, B, C, D, and tasked to return the letter corresponding to the appropriate connective. Failure to provide one of the options was considered a miss.\nWe collected data from a single prompt per task, and each underwent a selection process. We drafted a first prompt, and then used ChatGPT to obtain four more variants of it. We averaged the perplexity of all the open models on each prompt and chose the one with the lowest average perplexity.\nWe randomized the order of few-shot examples, options to choose from, and correct answers during inference. In the few-shot scenario, the models saw one example for each connective.\nPerplexity Evaluation. We computed the perplexity (PPL) of each item in the dataset, and grouped those corresponding to the same sentence pair. Then, we chose the connective from the item with the lowest PPL. We derived from the connective the TYPE and ORDER of the relation and computed models' accuracy by comparing these results with the human ground truth obtained with crowdsourcing annotation. We call this accuracy as Accuracy Perplexity Score (APS)."}, {"title": "Models", "content": "We selected 7 generative LLMs. Specifically, we selected two open-weights models (Mistral-7B-Instruct-v0.3 and falcon-7b-instruct), three partially open models (Meta-Llama-3.1-8B-Instruct, gemma-2-9b-it, and Qwen2.5-7B-Instruct), and two commercial models, gpt4o and gpt4o-mini. Perplexity evaluation was not performed on commercial models as it is not permitted through the API. We used Qwen2.5 instruct models of different sizes (from 0.5B to 32B parameters) for analyzing the impact of model scale."}, {"title": "Results and Discussion", "content": "We carried out an in-depth analysis of several interesting aspects emerging from the experiments, as outlined in the following.\nCore results are summarized in Fig. 3, which shows the average accuracy across all three prompting tasks for a specific model in a specific setting (e.g., Outlines in zero-shot). Violet bars represent the APS (a single numerical value) for each open model. These results are complemented by Tab. 2, which compares performances across prompting tasks, aggregated by model type.\nThe main trends observed are: i.) performance variability is high among models of the same scale in prompting tasks, despite similar APSs across open models; ii.) GPT4o outperforms all models, but its mini variant does not follow this trend and is still far from human performance; iii.) accuracy standard deviation shows that most open models exhibit inconsistent results across tasks, particularly with greedy search (Sec. 5.1); iv.) prompting performance is systematically lower than competence-level APSs in open models (Sec. 5.2)."}, {"title": "Modeling PCD with Prompting", "content": "Tab. 2 shows that, on average, PCD performed best when responses were framed using Outlines in a few-shot setting (0.40 overall, 0.61 for GPT models). However, the highest task-specific accuracy was achieved in the acceptability rating task under a zero-shot setting with greedy search (0.49).\nMoreover, the Outlines framework proved to be beneficial for all models on the multiple-choice task. Conversely, for the cloze test and acceptability rating task, performances either remained roughly the same or decreased substantially for both open models and GPTs. Strikingly, the top average accuracy of 0.61 in Tab. 2 still reveals a strong gap with human data.\nFig. 3 shows that each model performs best under different settings. Each of GPT40, Gemma, and Qwen achieved the highest mean accuracy (0.72, 0.53, and 0.51, respectively) in a distinct setup, surpassing GPT4o-mini (0.50). Standard deviation analysis (sd) indicates that task selection significantly affects smaller models' performance, especially with greedy search (sd 0.2 as in Tab. 2). Notably, GPT4o-mini exhibits a much wider performance range across both few-shot and zero-shot settings."}, {"title": "Modelling PCD with Perplexity", "content": "The APS results of the open models are shown in Fig. 3. The scores obtained by relying solely on perplexity are significantly higher (0.63 overall average) compared to those achieved in the prompting tasks. Although Falcon performs poorly in the latter, it is the model with the best APS (0.66). By leveraging perplexity, the lowest APS too, reached by Qwen with 0.59, surpasses the other open models in prompting tasks and GPT4o-mini as well. Although this result does not match GPT4o performances, it seems to confirm that models' competence about causal relations encoded in their probabilistic predictions is more accurate than their prompting performances (Hu and Levy, 2023).\nFrom these analyses we conclude that ExpliCa"}, {"title": "In-depth Analysis on ExpliCa", "content": "The ExpliCa design allows us to provide a more in depth analysis of causal reasoning and LLMs.\nRelations' type and order. First, we focus on the relation TYPE between the events expressed in each dataset item, and their ORDER. We report results on the acceptability rating prompting task, with zero-shot and greedy search (i.e., where the models performed best overall). We specifically focus on GPT40 and two open models, namely Gemma and Falcon, with the latter achieving the best APS. In Fig. 4, we see that models perform best on the CAUSAL relations in ICONIC order, with the exception of Falcon. Worse performances are obtained for CAUSAL ANTI-ICONIC and TEMPORAL ICONIC, with the latter often being mistaken for CAUSAL ICONIC.\nAn interesting pattern emerges by comparing APSs and Accuracy on acceptability ratings. APSs for a model are always higher than the respective acceptability ratings' accuracy, except for TEMPORAL ANTI-ICONIC items. However, by observing the models' behavior, we saw that both Gemma and Falcon tend to favor the TEMPORAL ANTI-ICONIC connective despite showing lower perplexity for"}, {"title": "Correlation and distribution variation.", "content": "We compared human ratings with model-generated ones in terms of their distribution and correlation. We show a comparison of models acceptability ratings with humans' for the causal-temporal and unrelated subsets in Fig. 5. We consider ratings of GPT4o and Gemma, and the (normalized) perplexity scores for Falcon, as it obtained the best APSs. A noticeable difference exists between human ratings of CAUSAL and TEMPORAL items vs. UNRELATED and model scores.\nFig. 6 shows Spearman correlation computed among human ratings and models' acceptability ratings collected in zero-shot scenario. We observed"}, {"title": "Size Effect", "content": "We analyzed how the model size affects performance on PCD. We selected the Qwen model family, available in a wide range of sizes, and used the acceptability rating and perplexity tasks as a testbed. Fig. 7 shows that Qwen's performance and APS improve with model size, except for a slight drop for the latter in the largest variant. Nonetheless, the improvement rates are quite different. Performances seem to linearly scale with size, whereas the APS growth curve is flatter and eventually plateaus. The initial APS is markedly higher (2x) than the respective accuracy; accuracy and APS are near equal at the 14B mark; the accuracy of the 32B keeps improving while the APS is stale. A similar trend holds true also for each relation, despite showing a higher variability. For example, the 0.5B variant is quite proficient with ANTI-ICONIC relations but almost incapable of modelling ICONIC ones. Model's scale seem to correlate with less differences in performances among relations.\nThis might suggest that size mainly affects the model's performance on prompting tasks, rather than their competence about causal and temporal relations. These are likely to be already encoded though partially, as observed above \u2013 in smaller"}, {"title": "Conclusion and Future Work", "content": "In this paper, we introduced ExpliCa, a dataset designed to evaluate LLMs on PCD tasks with the aim to assess their reasoning abilities on explicitly expressed relations. ExpliCa is the first dataset to incorporate both causal and temporal relations between events, with acceptability ratings provided by native speakers via crowdsourcing.\nResults indicate that ExpliCa is particularly challenging, even for commercial models like GPT40-mini, which is outperformed by open models when leveraging probabilistic scores, and GPT4o, which struggles to reach 0.80 accuracy. We observed that LLMs exhibit variable performance according to both the evaluation setup and the relation TYPE and ORDER. The models' rating distribution do not approximate the humans' ratings, especially for certain types of relations and linguistic orders. Moreover, our results suggest that the competence on causal relations measured with perplexity is significantly more accurate than prompting performance (especially in smaller models).\nFor future works, we aim to further explore the effect of model size on different model families, and we plan to adopt ExpliCa also to investigate how models interpret implicit causality.\nSo far, the presented results reveal that, despite their increasing size, the knowledge of causal relations is still suboptimal in LLMs, which also show a strong tendency to confound temporal relations with causal ones, compounded with a limited abstraction from the surface linguistic presentation order of the events. e can conclude that, although LLMs have undoubtedly progressed along the 'ladder of causation' (Pearl, 2009), they have still several steps to climb."}, {"title": "Limitations", "content": "This study has several limitations that should be acknowledged. The prompts used across different models were not specifically optimized for each of them. However, this decision was necessary to maintain the feasibility of our experiments and ensure a fair comparative evaluation among all models and tasks. Computational constraints played a significant role in shaping our methodology, influencing aspects such as batch size, model capacity, and the overall scope of the analysis. While we examined causal reasoning through various tasks, the selection of prompts, possible choices, and connectives represents only a subset of all potential analytical strategies. Additionally, the number of closed-source models included in the analysis was limited, and the dataset size was relatively small, further constraining the generalizability of the findings. Experiments on model scaling were confined to the Qwen model, where we observed that larger models tend to equalize performance on tasks and competence. However, this observation requires further validation to confirm that it is not influenced by factors such as the specific prompt or model used, which would bolster the robustness of our claims. Among the CAUSAL sentence pairs in ExpliCa, 50 are labeled as 'socially challenging', indicating content that touches on sensitive or potentially offensive topics such as religion, abortion, immigration, gender identity, drug abuse, and bribery. We acknowledge that some sentences may be offensive to certain groups, but these themes were added to evaluate whether bias-mitigation strategies in LLMs would impact PCD performance. Due to space constraints, we plan to explore such aspects more in depth in future works. Finally, we acknowledge that we focused only on LLMs and we did not include the so-called Large Reasoning Models (LRM) like OpenAI 01 (OpenAI, 2024b) or the very recent DeepSeek-R1 (DeepSeek-AI, 2025), which may have an advantage in such a task. This choice was mainly due to financial constraints, but they will be explored in future works."}, {"title": "Prompt Engineering", "content": "The prompts used during our experiments were selected by computing models' perplexity over five prompts per task, as described in Sec. 4. Tab. 4 shows the perplexity score given by the models to each prompt variant in zero-shot setting. Then the prompts with the lower average perplexity for each task were selected. The selected prompts are reported in the boxes below in the few-shot setting."}, {"title": "Models Details", "content": "Open and partially open models used for our experiments are available via the Hugging Face model library (https://huggingface.co/models); all"}, {"title": "More Models' Results", "content": "Tab. 5 shows the results plotted in Fig. 3. Similar to GPT40-mini, Gemma displays a high instability with greedy search, although it is notably reduced when using the Outlines framework. Llama follows a similar trend, whereas Falcon is quite stable also when using greedy search in zero-shot settings. In contrast, Mistral's results remain nearly the same across all scenarios, with a range similar to GPT4o. On the contrary, Qwen shows the opposite pattern, with more stable performances using greedy search.\nResults on all tasks and setups with greedy search are reported in Fig. 8 and Tab. 6. Results with Outlines are reported in Fig. 9, Tab. 7.\nTab. 8 shows the overall APSs achieved by the models and those achieved according to various relations' conditions. APS changes according to the relation between the events contained in each item. Gemma and Llama best detect events in ICONIC ORDER, whereas Falcon, Mistral, and Qwen are better in identifying CAUSAL relations. All the models struggle at recognizing TEMPORAL ANTI ICONIC relations, achieving really low results for this condition (0.19).\nTab. 9, shows the results plotted in Fig. 4, Sec. 5.3. In the table are reported APS and results on the acceptability rating task, with zero-shot and greedy search, i.e. where the models performed best overall. We specifically focus on GPT4o and two open models, namely Gemma and Falcon, with the latter achieving the best APS.\nPost-hoc analyses were conducted on the obtained results to see if they might be affected by the frequency of the triplets {1st sentence verb, connective, 2nd sentence verb} in each item. Fig. 10"}, {"title": "Error Analysis", "content": "Other than what is described in Sec. 5.3, from the confusion matrixes in Fig. 12, we observed that GPT tends to consider TEMPORAL RELATIONS as CAUSAL ones more often. However, most of the mistakes are within the same ORDER (i.e., ANTI-ICONIC), whereas, open models tend to make more mistakes under both TYPE and ORDER of the relation. Gemma, in the acceptability rating task, interprets CAUSAL relations as TEMPORAL, but it tends to confuse also the ORDER of the TEMPORAL ones. Differently, Falcon is inclined to interpret all the items as TEMPORAL in ANTI-ICONIC ORDER. We also can observe that Falcon and Gemma errors in a different manner according to the way the PCD task is modeled. These results further underline a discrepancy between models' internal representation and prompted knowledge, not only across different models but also referring to the same one."}, {"title": "Correlation & Distribution", "content": "In Fig. 5, Sec. 5.3, we see that for both humans and models, the acceptability ratings are lower for the unrelated sentence pairs, showing that models appear to understand this difference. For the CAUSAL-TEMPORAL subset, in the case of normalized perplexities for Falcon, all values are heavily skewed toward the lower end of the spectrum. On the contrary, ratings from Gemma are gener-"}]}