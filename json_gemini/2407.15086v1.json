{"title": "MaxMI: A Maximal Mutual Information Criterion for Manipulation Concept Discovery", "authors": ["Pei Zhou", "Yanchao Yang"], "abstract": "We aim to discover manipulation concepts embedded in the unannotated demonstrations, which are recognized as key physical states. The discovered concepts can facilitate training manipulation policies and promote generalization. Current methods relying on multimodal foundation models for deriving key states usually lack accuracy and semantic consistency due to limited multimodal robot data. In contrast, we introduce an information-theoretic criterion to characterize the regularities that signify a set of physical states. We also develop a framework that trains a concept discovery network using this criterion, thus bypassing the dependence on human semantics and alleviating costly human labeling. The proposed criterion is based on the observation that key states, which deserve to be conceptualized, often admit more physical constraints than non-key states. This phenomenon can be formalized as maximizing the mutual information between the putative key state and its preceding state, i.e., Maximal Mutual Information (MaxMI). By employing MaxMI, the trained key state localization network can accurately identify states of sufficient physical significance, exhibiting reasonable semantic compatibility with human perception. Furthermore, the proposed framework produces key states that lead to concept-guided manipulation policies with higher success rates and better generalization in various robotic tasks compared to the baselines, verifying the effectiveness of the proposed criterion.", "sections": [{"title": "1 Introduction", "content": "Recent advances in foundation models have shown great potential in promoting robot capabilities in perception, reasoning and control among many other Embodied AI tasks. Despite the remarkable ability to generalize in certain scenarios, their accuracy in grounding high-level manipulation concepts to low-level physical states is still limited due to the scarcity of annotated robot training data. For example, when presenting images to multimodal foundation models (e.g., GPT-4V(ision) ) and ask if a manipulation concept (e.g., \"the peg is aligned with the hole\") is achieved or not, we would usually obtain incorrect answers, which signifies the incapability of current models in accurately understanding the physical conditions. The ability to faithfully tell whether a manipulation concept is realized given the low-level physical state is critical for the efficiency and safety of a robot learning system that integrates the foundation models. Especially, given the many efforts that leverage foundation models to perform planning using high-level concepts in the form of step-by-step instructions or to provide training signals in the form of value functions , all of which rely on an accurate connection (grounding) between the concepts described in language and the physical states.\nOne way to enhance the groundedness of foundation models is to collect more labeled training data, which inevitably induces a heavy annotation effort by asking humans to analyze the data and provide detailed descriptions of the key physical states, corresponding to manipulation concepts defined by human semantics. We refer to the grounding achieved from human annotations as top-down, since the reliance on high-level human understanding is assumed at the first place. However, we propose that manipulation concepts exist on their own due to the significance in the corresponding physical states, but not the attached human semantics. In other words, we believe that manipulation concepts are symbols of and derived from sets of physical states that possess certain regularities. For example, all the states that are instantiations of the manipulation concept \"turn on the faucet\" share the regularity or effect of running water, which is invariant of which language we use to describe the concept. If these key states are discovered, we can then assign them a description (one label) characterizing the set in the form of concept. Accordingly, we denote the above grounding process as bottom-up, since a semantic name is only assigned after the concept is discovered from low-level states, which, in contrast, help reduce the annotation effort as required in the top-down grounding process.\nIn this work, we investigate the possibility of the aforementioned bottom-up concept grounding. More explicitly, we aim for a learning framework that can au-"}, {"title": "MaxMI", "content": "tomatically discover the key states in unannotated robot demonstrations, which deserve being assigned a concept name or description. To achieve this goal, we propose a Maximal Mutual Information (MaxMI) criterion to characterize the regularities that endow a set of physical states with sufficient significance. Specifically, the proposed MaxMI criterion measures the mutual information between a (key) state random variable (concept) and its preceding state variable (e.g., the low-level physical states can be treated as instantiations of the concept or state random variable). Moreover, the proposed criterion promotes that the mutual information quantity should be a maximal when the discovered state variable signals a manipulation concept. We further turn this metric into the training loss of a concept (key states) discovery network by leveraging a differentiable mutual information estimator. After training, the discovery network receives observations from a demonstration and outputs the key states corresponding to a discovered concept.\nOur experiments show that the concept discovery network trained with the MaxMI criterion can accurately localize physical states that align well with human semantics, alleviating the need for human supervision or heuristic rules. Moreover, we explicitly evaluate its efficacy by using discovered manipulation concepts to guide the training of manipulation policies. Our experiments verify that the discovered concepts effectively mitigate compounding errors in manipulation tasks, and the resulting policies outperform state-of-the-art methods in various robotic manipulation tasks. These results underscore the potential of the proposed MaxMI metric in self-supervised manipulation concept discovery and enhancing the grounding of manipulation concepts to the physical states. To summarize, this work contributes the following: 1) An information-theoretic criterion named MaxMI that characterizes the significance of the physical states for manipulation concept discovery. 2) A framework that trains a neural network to discover manipulation concepts and accurately localize the corresponding physical states within a demonstration sequence. 3) A comprehensive evaluation of the proposed concept discovery framework with various ablations and for concept-guided manipulation policy learning."}, {"title": "2 Related Work", "content": "Learning from Demonstrations. Learning from Demonstration (LfD) refers to training robotic agents for complex tasks through expert demonstrations, which efficiently avoids costly self-exploration . LfD methods include Inverse Reinforcement Learning (IRL), online Reinforcement Learning (RL) with demonstrations, and Behavior Cloning (BC) . IRL infers reward functions from observed behaviors, which could be computationally demanding , while online RL with demonstrations combines dynamic online RL with offline guidance from the demonstrations . Further, BC falls into the regime of supervised learning to map input states to actions, but its performance is limited by the number of demonstrations and may subject to imitation errors due to overfitting . Our approach aims to minimize the imitation errors by leveraging key state guidance that can promote compositional generalization.\nFoundation Models for Task Planning. Many recent works investigate the usage of foundation models , such as Large Language Models (LLMs), for embodied planning, either in the form of code generation or by leveraging open-source tools for diverse interaction tasks . Besides virtual environment, foundation models are also utilized to enable embodied agents to interact with the dynamic real-world environment . Specifically, foundation models can help convert language instructions and environmental information into control signals . Moreover, LLMs and Vision Language Models (VLMs) can be used to construct reward or value functions using carefully-crafted prompting strategies . However, all these models still lack the capability to accurately identify low-level physical states corresponding to high-level concepts due to the lack of annotated robot data.\nHierarchical Planning. Hierarchical planning offers a structured approach to decomposing complex tasks into simpler, manageable sub-tasks across various abstraction levels, thereby facilitating the policy learning processes , particularly, with recent advancements that leverage the Chain of Thought (CoT) prompting technique . One can provide manually decomposed high-level skills to facilitate learning with structures , which may incur a heavy annotation burden. There are also works studying unsupervised skill discovery, thus, eliminating the need for human annotations . Especially, AWE extracts waypoints that can be used to linearly approximate a demonstration trajectory and leverages them to facilitate behavior cloning. Our method focuses on the abstraction of manipulation concepts through unsupervised discovery based on an information-theoretic metric, identifying highly informative states for guiding the policy learning."}, {"title": "3 Method", "content": "We aim to characterize the significance of a set of physical states, which shall enable self-supervised discovery of the states that are worth conceptualizing and can be used as guidance to facilitate the training of manipulation policies. We first detail the problem setup and illustrate the key observations that motivate the Maximal Mutual Information (MaxMI) criterion. We then describe a learning framework that leverages the proposed metric for discovering semantically meaningful manipulation concepts (or key states), as well as the policy training pipeline that serves as a testbed for the usefulness of the discovered concepts."}, {"title": "3.1 Problem Statement", "content": "Specifically, our goal is to develop a mechanism for localizing (key) physical states within a demonstration trajectory that are instantiations of a set of manipulation concepts, e.g., \"the peg is aligned with the hole.\" Formally, given a dataset of \\(N\\) pre-collected demonstrations \\(\\mathcal{D} = {\\tau^{i}}_{i=1}^{N}\\) for a task \\(T\\), where each"}, {"title": "Maximal Information at Key States", "content": "We propose that key states corresponding to manipulation concepts exist due to their physical significance but not human semantics, i.e., the state depicted by \"the peg is aligned with the hole\" is invariant to the language used to describe it (e.g., English or French). In other words, we hypothesize that a key state is"}, {"title": "3.3 Key State Discovery", "content": "Discovering a key state now amounts to determining the temporal indices \\({t_{k}^{i}}\\)_{k=1}^{K}\\) that maximizes the measure in Eq. 3 by training a localization neural network (Fig. 3). We denote the localization network as \\(\\varphi\\), which takes as input a trajectory \\(\\tau^{i} = \\{s_{j}^{i}\\}_{j=1}^{T}, s_{j}^{i} \\in \\mathbb{R}^{Q}\\) and a learnable concept embedding \\(e_{k} \\in \\mathbb{R}^{P}\\) representing the key concept \\(\\mu_{k}\\). We omit the actions in the trajectory, as by definition the key state should be sufficiently determined by the states themselves. Also, we normalize each trajectory to the length of \\(T\\) by interpolation to focus on the learning efficiency of the proposed MaxMI criterion instead of designing an autoregressive network architecture.\nThe concept embedding \\(e_{k}\\) is repeatedly concatenated with the states \\(s_{i}\\), and the neural network \\(\\varphi\\) maps the augmented physical states into a distribution corresponding to the key state selection probability, i.e., \\(\\varphi: \\mathbb{R}^{(Q+P) \\times T} \\rightarrow \\mathbb{R}^{T}\\). We can also write \\(p^{i} = \\varphi(\\tau^{i}, \\mu_{k}) \\in \\mathbb{R}^{T}\\) for the probability of each state in \\(\\tau^{i}\\)"}, {"title": "MaxMI", "content": "being the key state described by concept \\(\\mu_{r}\\). The estimated key state location can then be obtained by applying an argmax over the predicted distribution \\(p^{i}\\). However, due to the non-differentiability of the argmax function, we instead introduce a fixed temporal coordinate vector \\(x = [0, 1, 2, . . . ,T-1]\\), and compute the predicted key state location as the dot product between x and the probability \\(p_{i}\\), e.g., \\(t^{i} = xp^{i}\\), which makes the entire process differentiable.\nThe overall structure of the proposed Key State Localization Network (KSL-Net) is shown in Fig. 3. A convolutional encoder is employed to fuse the states and concept embedding, followed by a max-pooling to aggregate information from various time-steps, and an MLP is used to output the key state selection probability.\nDiscovery objectives. As discussed, the predicted key state indices \\(\\{t_{k}^{i}\\}_{i=1}^{N}\\) should maximize the MaxMI criterion (Eq. 3) for a (to be discovered) concept \\(\\mu_{k}\\). However, the total number of concepts existing in the trajectories are not known beforehand. To resolve this issue, we assume that the maximum number of concepts for the manipulation tasks is \\(K\\), e.g., 10, and train the KSL-Net \\(\\varphi\\) by minimizing the following:\n\\begin{equation}\n\\mathcal{L}^{\\text{CMaxMI}}(\\varphi; \\mathcal{D}) = - \\sum_{k=1}^{K} \\psi(\\{(\\tau^{i}, s_{t_{k}^{i}} - \\Delta t)\\}_{i=1}^{N}),\n\\end{equation}\nwhere the neural information estimator \\(\\psi\\) is pretrained and fixed .\nHowever, training solely with Eq. 4 would encourage all the discovered concepts to concentrate on one key state that has the largest mutual information value, even though there are many other (local) maximas that satisfy the maximal mutual information criterion. To alleviate the clustering effect, we propose a second regularization term that forces the discovered key states to be different by penalizing small distances between the localized indices of different key states (concepts):\n\\begin{equation}\n\\mathcal{L}^{\\text{Cdiv}}(\\varphi; \\mathcal{D}) = \\sum_{i=1}^{N} \\sum_{0<u<v<K} \\mathbb{S}_{p}(\\mid t_{u}^{i} - t_{v}^{i} \\mid),\n\\end{equation}\nwhere \\(|\\mid\\) is the L-1 norm, and \\(\\mathbb{S}_{p}\\) is the softplus function: \\(f(x) = \\log(1+\\exp(x))\\). The final loss for training the key state localization network \\(\\varphi\\) is:\n\\begin{equation}\n\\mathcal{L}^{\\text{discover}} = \\mathcal{L}^{\\text{CMaxMI}}(\\varphi; \\mathcal{D}) + \\lambda \\mathcal{L}^{\\text{Cdiv}}(\\varphi; \\mathcal{D}),\n\\end{equation}\nwith \\(\\lambda > 0\\) the weighting between the significance of the discovered concepts and their diversity. Since \\(K\\) might be larger than the actual number of key states involved in a manipulation task, we further apply non-maximum suppression to reduce the redundancy in the discovered concepts. The quality of the discovered concepts (localized key states) is studied in the experiments. Moreover, we evaluate the effectiveness of the proposed concept discovery framework (e.g., the MaxMI criterion) by training concept-guided manipulation policies using the localized key states."}, {"title": "Manipulation Policy with Discovered Concepts", "content": "MaxMI\nWe leverage the CoTPC framework  to learn concept-guided manipulation policy. Specifically, the policy network predicts the action and estimates the key states (corresponding to different concepts) simultaneously during the training phase. In this way, the concept guidance is injected into the action prediction process and can promote learning efficiency and generalization. Instead of using manually annotated key states, we apply the localized key states from our concept discovery framework. Given a trajectory \\(\\tau^{i} = \\{(s_{i},a_{i})\\}_{t}\\) and its localized key states \\(\\{\\hat{t}_{k}\\}_{k}\\), the CoTPC policy training loss is:\n\\begin{equation}\n\\mathcal{L}^{\\text{policy}} = \\sum_{i=1}^{N} \\sum_{t} ||\\hat{a_{i}} - a_{i}||_{2} + \\lambda \\sum_{i=1}^{N} \\sum_{k=0}^{K-1} ||\\hat{s_{\\hat{t}_{k}}} - s_{\\hat{t}_{k}}||_{2},\n\\end{equation}\nwhere \\(\\hat{a}\\) and \\(\\hat{s}\\) are the predictions from the policy network introduced in . By optimizing Eq. 7, we now have a policy that can be applied to manipulation tasks, whose success rate in various settings shall indicate its performance as well as the usefulness of the discovered key states."}, {"title": "4 Experiments", "content": "In this section, we thoroughly evaluate the proposed key state discovery strategy. We first validate its effectiveness against baselines on complex multi-step tasks from ManiSkill2  and Franka Kithen , and then assess its performance and generalization in novel scenarios with unseen configurations and objects. Additionally, an ablation study is performed to explore various key state discovery approaches and the effect of the regularization term."}, {"title": "4.1 Experimental Setup", "content": "Baselines. The evaluation and comparison are performed with the following major baselines: Behavior Transformer (BeT) , Decision Diffuser (DD) , Decision Transformer (DT) , and Chain-of-Thought Predictive Control (CoTPC) . For more information about these methods, please refer to the appendix.\nMulti-Step Manipulation Tasks. We follow the experimental setting used in the most recent state of the art , and choose the same set of four complex multi-step object manipulation tasks from the Maniskill2 environments . These four tasks are shown in Fig. 4. Please refer to the appendix for more details about the tasks.\nExperimental Setting. We train the policies following the behavioral cloning (BC) paradigm without densely labeled rewards . Similar to CoTPC, we set the state space as the observation space, assuming full observability unless stated otherwise. For fair comparisons, we utilize the same set of 500 randomly sampled trajectories for a task. These trajectories, varying in initial states and environmental configurations, are sourced from multiple experts, added with diversity and randomness. In the following sections, we detail the evaluation metric and experimental results."}, {"title": "Discovered Concepts vs. Manually Annotated Concepts", "content": "In Fig. 4, we compare manually annotated concepts with those discovered by our proposed method for four different tasks. Human annotations, restricted by their labor-intensive nature, are often sparse, thus providing limited information. Conversely, our method's flexibility in adjusting the number of key states enables the identification of a diverse range of concepts. This diversity, as evidenced in Fig. 4, underscores the capability of our method in enriching the concept guidance for more effective policy training.\nOn the other hand, Fig. 4 shows that, even without explicitly injecting human semantics in the concept discovery process, the key concepts annotated by humans and a subset of the concepts discovered by our method exhibit a high degree of similarity. This suggests that our approach is able to discover concepts aligning with human understanding of the manipulation process. This alignment not only validates the efficacy of our concept discovery method but also highlights its potential in capturing intrinsic task-relevant features that resonate with human semantics, thereby enhancing the interpretability and applicability of our method in complex scenarios.\nAlso note that, since our approach allows a flexible selection of the number of discovered concepts, more fine-grained concepts can be discovered that help promote the guided policy learning. To prevent redundant manipulation concepts due to state noise, we consider concepts occurring closely in time as identical."}, {"title": "4.3 Quantitative Results", "content": "Evaluation Metric. Our primary evaluation metric is the task success rate. For complex multi-step tasks, we also report the success rate of completing intermediate sub-tasks, e.g., the Peg Insertion Side task, where the final objective is to insert the peg into a horizontal hole in the box. The intermediate sub-tasks include \"peg grasping by the robotic arm\" and \"alignment of the peg with the hole.\" During the evaluation phase, both seen (during training) and novel environmental configurations such as different initial joint positions of the robot are tested. For the Turn Faucet task, we also perform an additional evaluation on faucets with geometric structures not present in the training set.\nMain Results. Tab. 1 and Tab. 2 present the results on the seen and unseen environmental configurations (including zero-shot with novel objects), respectively. Tab. 1 demonstrates the superior performance of our proposed method in comparison to various baseline methods across multiple tasks. It shows that the baselines have difficulties in handling complex multi-step tasks, while the policies trained with our discovered concepts consistently achieve the best performance, verifying the effectiveness of the proposed method in localizing physically meaningful key states. Due to the poor performance of baseline methods in dealing with novel configurations, we follow  to report the comparison with the two most effective baselines in unseen scenes. As evidenced in Tab. 2, our approach still outperforms these baselines, suggesting that our proposed key state discovery significantly enhances the generalization of the trained manipulation policies in new scenarios."}, {"title": "Franka Kitchen", "content": "Franka Kitchen To assess the efficacy in accomplishing long-horizon tasks, we train and test the concept-guided policies in the Franka Kitchen environment , where a 9-DoF Franka robot operates within a virtual kitchen. This environment features a total of seven object-interaction tasks. We utilize the dataset introduced by  with 566 demonstrations. Each demonstration contains a sequence of four object-interaction tasks. The performance is measured by how many object-interaction tasks can be completed in one episode, i.e., the more tasks completed in one episode the better the policy. We check the ratio of the episodes that successfully achieve {1,2,3,4,5} tasks out of 100 trials. The results are reported in Tab. 3, with the maximum roll-out steps equal to 280. It shows that the discovered key concepts can effectively reduce the decision horizon, such that the policy trained with these concepts can achieve more subtasks within fixed steps in one roll-out."}, {"title": "4.4 Ablation of Different Key State Selection Mechanisms", "content": "We further conduct an ablation with various key state selection strategies. The results are detailed in Tab. 4. Please note that these policies are trained using the CoTPC framework . The baselines mainly include those utilizing multimodal encoders, such as CLIP , BLIP , BLIP2 , and FLAVA ,"}, {"title": "4.5 Effect of the Regularization Term", "content": "Now we study the impact of the regularization term, e.g., the pairwise distance (PD) penalty. The numerous results are presented in Tab. 5, which validate the necessity of each term. As shown in Fig. 5, key states in a demonstration discovered by different terms vary a lot. The pairwise distance term is observed to enforce diversity among the discovered concepts but alone can not ensure semantic meaningfulness, thereby simply exhibiting a uniform distribution across the trajectory. In contrast, relying exclusively on the MaxMI criterion tends to discover clustered concepts around the state with the highest mutual information value, thus reducing the diversity of the concepts. The full model can discover more meaningful key states while maintaining diversity."}, {"title": "5 Conclusion", "content": "We propose an information-theoretic metric that characterizes the physical significance of the key states in robotic manipulation tasks. We further leverage the proposed metric to develop a self-supervised manipulation concept discovery pipeline that can produce meaningful key states. When used as guidance for training policies, these key states can lead to higher performance than a broad spectrum of baselines. Additionally, we validate the necessity of the proposed terms through an extensive ablation study. Our investigation also shows that the proposed MaxMI criterion alone may not guarantee the diversity of the discovered concepts due to its local modeling characteristic, which we deem a limit. We propose that future studies can resolve this issue by extending the metric to a multi-scale concept discovery framework so that global information on the trajectories can be accessed."}]}