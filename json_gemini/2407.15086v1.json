{"title": "MaxMI: A Maximal Mutual Information Criterion for Manipulation Concept Discovery", "authors": ["Pei Zhou", "Yanchao Yang"], "abstract": "We aim to discover manipulation concepts embedded in the unannotated demonstrations, which are recognized as key physical states. The discovered concepts can facilitate training manipulation policies and promote generalization. Current methods relying on multimodal foundation models for deriving key states usually lack accuracy and semantic consistency due to limited multimodal robot data. In contrast, we introduce an information-theoretic criterion to characterize the regularities that signify a set of physical states. We also develop a framework that trains a concept discovery network using this criterion, thus bypassing the dependence on human semantics and alleviating costly human labeling. The proposed criterion is based on the observation that key states, which deserve to be conceptualized, often admit more physical constraints than non-key states. This phenomenon can be formalized as maximizing the mutual information between the putative key state and its preceding state, i.e., Maximal Mutual Information (MaxMI). By employing MaxMI, the trained key state localization network can accurately identify states of sufficient physical significance, exhibiting reasonable semantic compatibility with human perception. Furthermore, the proposed framework produces key states that lead to concept-guided manipulation policies with higher success rates and better generalization in various robotic tasks compared to the baselines, verifying the effectiveness of the proposed criterion. Our source code can be found at https://github.com/PeiZhou26/MaxMI.\nKeywords: self-supervised concept discovery robotic manipulation", "sections": [{"title": "1 Introduction", "content": "Recent advances in foundation models have shown great potential in promoting robot capabilities in perception [31], reasoning [6], and control [5], among many other Embodied AI tasks [13]. Despite the remarkable ability to generalize in certain scenarios, their accuracy in grounding high-level manipulation concepts to low-level physical states is still limited due to the scarcity of annotated robot training data. For example, when presenting images to multimodal foundation models (e.g., GPT-4V(ision) [36]), and ask if a manipulation concept (e.g., \"the peg is aligned with the hole\") is achieved or not, we would usually obtain incorrect answers, which signifies the incapability of current models in accurately understanding the physical conditions (Fig. 1). The ability to faithfully tell whether a manipulation concept is realized given the low-level physical state is critical for the efficiency and safety of a robot learning system that integrates the foundation models. Especially, given the many efforts that leverage foundation models to perform planning using high-level concepts in the form of step-by-step instructions [6,8] or to provide training signals in the form of value functions [9,22], all of which rely on an accurate connection (grounding) between the concepts described in language and the physical states.\nOne way to enhance the groundedness of foundation models is to collect more labeled training data, which inevitably induces a heavy annotation effort by asking humans to analyze the data and provide detailed descriptions of the key physical states, corresponding to manipulation concepts defined by human semantics. We refer to the grounding achieved from human annotations as top-down, since the reliance on high-level human understanding is assumed at the first place. However, we propose that manipulation concepts exist on their own due to the significance in the corresponding physical states, but not the attached human semantics. In other words, we believe that manipulation concepts are symbols of and derived from sets of physical states that possess certain regularities. For example, all the states that are instantiations of the manipulation concept \"turn on the faucet\" share the regularity or effect of running water, which is invariant of which language we use to describe the concept. If these key states are discovered, we can then assign them a description (one label) characterizing the set in the form of concept. Accordingly, we denote the above grounding process as bottom-up, since a semantic name is only assigned after the concept is discovered from low-level states, which, in contrast, help reduce the annotation effort as required in the top-down grounding process.\nIn this work, we investigate the possibility of the aforementioned bottom-up concept grounding. More explicitly, we aim for a learning framework that can au-"}, {"title": "MaxMI", "content": "tomatically discover the key states in unannotated robot demonstrations, which deserve being assigned a concept name or description. To achieve this goal, we propose a Maximal Mutual Information (MaxMI) criterion to characterize the regularities that endow a set of physical states with sufficient significance. Specifically, the proposed MaxMI criterion measures the mutual information between a (key) state random variable (concept) and its preceding state variable (e.g., the low-level physical states can be treated as instantiations of the concept or state random variable). Moreover, the proposed criterion promotes that the mutual information quantity should be a maximal when the discovered state variable signals a manipulation concept. We further turn this metric into the training loss of a concept (key states) discovery network by leveraging a differentiable mutual information estimator. After training, the discovery network receives observations from a demonstration and outputs the key states corresponding to a discovered concept.\nOur experiments show that the concept discovery network trained with the MaxMI criterion can accurately localize physical states that align well with human semantics, alleviating the need for human supervision or heuristic rules. Moreover, we explicitly evaluate its efficacy by using discovered manipulation concepts to guide the training of manipulation policies. Our experiments verify that the discovered concepts effectively mitigate compounding errors in manipulation tasks, and the resulting policies outperform state-of-the-art methods in various robotic manipulation tasks. These results underscore the potential of the proposed MaxMI metric in self-supervised manipulation concept discovery and enhancing the grounding of manipulation concepts to the physical states. To summarize, this work contributes the following: 1) An information-theoretic criterion named MaxMI that characterizes the significance of the physical states for manipulation concept discovery. 2) A framework that trains a neural network to discover manipulation concepts and accurately localize the corresponding physical states within a demonstration sequence. 3) A comprehensive evaluation of the proposed concept discovery framework with various ablations and for concept-guided manipulation policy learning."}, {"title": "2 Related Work", "content": "Learning from Demonstrations. Learning from Demonstration (LfD) refers to training robotic agents for complex tasks through expert demonstrations, which efficiently avoids costly self-exploration [38,41]. LfD methods include Inverse Reinforcement Learning (IRL), online Reinforcement Learning (RL) with demonstrations, and Behavior Cloning (BC) [2]. IRL infers reward functions from observed behaviors, which could be computationally demanding [12,55,60], while online RL with demonstrations combines dynamic online RL with offline guidance from the demonstrations [19, 39, 48]. Further, BC falls into the regime of supervised learning to map input states to actions, but its performance is limited by the number of demonstrations and may subject to imitation errors due"}, {"title": "3 Method", "content": "We aim to characterize the significance of a set of physical states, which shall enable self-supervised discovery of the states that are worth conceptualizing and can be used as guidance to facilitate the training of manipulation policies. We first detail the problem setup and illustrate the key observations that motivate the Maximal Mutual Information (MaxMI) criterion. We then describe a learning framework that leverages the proposed metric for discovering semantically meaningful manipulation concepts (or key states), as well as the policy training pipeline that serves as a testbed for the usefulness of the discovered concepts."}, {"title": "3.1 Problem Statement", "content": "Specifically, our goal is to develop a mechanism for localizing (key) physical states within a demonstration trajectory that are instantiations of a set of manipulation concepts, e.g., \"the peg is aligned with the hole.\" Formally, given a dataset of N pre-collected demonstrations $D = \\{T^i\\}_{i=1}^{N}$ for a task T, where each"}, {"title": "MaxMI", "content": "demonstration $\\tau^i = \\{(s_i, a_i)\\}_{t=1}^{T_i}$ consists of a sequence of state-action pairs, we train a neural network to find the $N \\times K$ temporal indices $\\{t_{k}^i\\}_{k=1}^{K}$, such that $s_{t_{k}^i}$ is the key state in trajectory $\\tau^i$ corresponding to the $k$-th manipulation concept $\\mu_k$. Next, we introduce the Maximal Mutual Information (MaxMI) criterion to enable the training without incurring costly manual annotations."}, {"title": "3.2 Maximal Information at Key States", "content": "We propose that key states corresponding to manipulation concepts exist due to their physical significance but not human semantics, i.e., the state depicted by \"the peg is aligned with the hole\" is invariant to the language used to describe it (e.g., English or French). In other words, we hypothesize that a key state is"}, {"title": "Computing $\\mathbb{I}(S_{t_k}; S_{t_k-\\Delta t})$.", "content": "Computing $\\mathbb{I}(S_{t_k}; S_{t_k-\\Delta t})$. Given the set of key states $\\{s_{t_k^i}\\}_{i=1}^{N}$ (manually annotated) corresponding to the concept $\\mu_k$ and the preceding states $\\{s_{t_k^i -\\Delta t}\\}_{i=1}^{N}$, we can compute the mutual information quantity $\\mathbb{I}(S_{t_k}; S_{t_k - \\Delta t})$ using the neural network proposed in [20], which leverages an attention architecture to predict the mutual information between two random variables from their samples.\nWe choose the neural estimator due to its efficiency, robustness, and differentiability, which is critical for training the proposed self-supervised concept discovery pipeline in the following. Specifically, the neural network $\\psi$ in [20] takes in the paired sequences $\\{(s_{t_k^i}, s_{t_k^i - \\Delta t})\\}_{i=1}^{N}$, and outputs the mutual information estimate according to the empirical joint distribution $p(s_{t_k}, s_{t_k-\\Delta t})$ of the samples:\n$$\\mathbb{I}(S_{t_k}; S_{t_k-\\Delta t}) = \\psi(\\{(s_{t_k^i}, s_{t_k^i -\\Delta t})\\}_{i=1}^{N}).$$\nWith $\\psi$, we can easily compute $\\mathbb{I}(S_{t_{k+\\sigma}}; S_{t_{k-\\Delta t}+\\sigma}), \\sigma\\in [-1,1]$. In Fig. 2, we illustrate the procedure for computing $\\mathbb{I}(S_{t_{k+\\sigma}}; S_{t_{k-\\Delta t}+\\sigma})$, with $t_k$ and $\\Delta t$ fixed (top), and display the resulted plots (bottom). Our observations are:\na. The quantity $\\mathbb{I}(S_{t_{k+\\sigma}}; S_{t_{k-\\Delta t}+\\sigma})$ increases (under noise) when approaching a key state, i.e., $\\sigma$ varies from -l to 0. This aligns with our hypothesis that a key state is committed to more constraints than the non-key ones, thus, helps reduce the uncertainty of its preceding state, which in turn contributes to the increase in the mutual information.\nb. Moreover, $\\mathbb{I}(S_{t_{k+\\sigma}}; S_{t_{k-\\Delta t}+\\sigma})$ arrives at a maximal value around the key state ($\\sigma$ = 0) and then starts to decrease, since the constraints that help inform the preceding state are satisfied and then become ineffective when the key state is achieved. For example, when the peg is grasped by the gripper, the gripper can move anywhere before the next key state \"the peg is aligned with the hole\" is in effect."}, {"title": "Therefore, the proposed hypothesis is confirmed with these observations and\nwe can formalize a key state as the one that maximizes:", "content": "$$\\mathcal{L}^{key}(S_{t_k}) = \\mathbb{I}(S_{t_k}; S_{t_k-\\Delta t}),$$"}, {"title": "which is functionally equivalent to the following quantity utilizing the neural estimator $\\psi$:", "content": "$$\\mathcal{L}^{key}(S_{t_k}) = \\psi(\\{(s_{t_k^i}, s_{t_k^i -\\Delta t})\\}_{i=1}^{N}).$$"}, {"title": "We name the above as the Maximal Mutual Information (MaxMI) criterion of key states. We can then locate the instantiations of a key state, which satisfies\nthe aforementioned observations, by maximizing the MaxMI metric. Next, we elaborate on the training objectives of the proposed self-supervised key state\ndiscovery framework.", "content": "We name the above as the Maximal Mutual Information (MaxMI) criterion of key states. We can then locate the instantiations of a key state, which satisfies the aforementioned observations, by maximizing the MaxMI metric. Next, we elaborate on the training objectives of the proposed self-supervised key state discovery framework."}, {"title": "3.3 Key State Discovery", "content": "Discovering a key state now amounts to determining the temporal indices $\\{t_k^i\\}$ that maximizes the measure in Eq. 3 by training a localization neural network (Fig. 3). We denote the localization network as $\\phi$, which takes as input a trajectory $\\tau^i = \\{s_t^i\\}_{t=1}^{T}, s_t^i \\in \\mathbb{R}^Q$ and a learnable concept embedding $e_k \\in \\mathbb{R}^P$ representing the key concept $\\mu_k$. We omit the actions in the trajectory, as by definition the key state should be sufficiently determined by the states themselves. Also, we normalize each trajectory to the length of T by interpolation to focus on the learning efficiency of the proposed MaxMI criterion instead of designing an autoregressive network architecture.\nThe concept embedding $e_k$ is repeatedly concatenated with the states $s_t^i$, and the neural network $\\phi$ maps the augmented physical states into a distribution corresponding to the key state selection probability, i.e., $\\phi : \\mathbb{R}^{(Q+P) \\times T} \\rightarrow \\mathbb{R}^{T}$.\nWe can also write $p^i = \\phi(\\tau^i, \\mu_k) \\in \\mathbb{R}^{T}$ for the probability of each state in $\\tau^i$"}, {"title": "Discovery objectives.", "content": "Discovery objectives. As discussed, the predicted key state indices $\\{t_k^i\\}_{i=1}^{N}$ should maximize the MaxMI criterion (Eq. 3) for a (to be discovered) concept $\\mu_k$. However, the total number of concepts existing in the trajectories are not known beforehand. To resolve this issue, we assume that the maximum number of concepts for the manipulation tasks is $K$, e.g., 10, and train the KSL-Net $\\phi$ by minimizing the following:\n$$\\mathcal{L}^{CMaxMI}(\\phi; D) = - \\sum_{k=1}^{K} \\psi(\\{(s_{t_k^i}, s_{t_k^i -\\Delta t})\\}_{i=1}^{N}),$$"}, {"title": "where the neural information estimator $\\psi$ is pretrained and fixed [20].", "content": "where the neural information estimator $\\psi$ is pretrained and fixed [20].\nHowever, training solely with Eq. 4 would encourage all the discovered concepts to concentrate on one key state that has the largest mutual information value, even though there are many other (local) maximas that satisfy the maximal mutual information criterion. To alleviate the clustering effect, we propose a second regularization term that forces the discovered key states to be different by penalizing small distances between the localized indices of different key states (concepts):\n$$\\mathcal{L}^{div}(\\phi; D) = \\sum_{i=1}^{N} \\sum_{0<u<v<K} Sp(-\\|t_{k_u}^i - t_{k_v}^i\\|),$$"}, {"title": "where || is the L-1 norm, and $S_p$ is the softplus function: $f(x) = log(1+exp(x))$.\nThe final loss for training the key state localization network $\\phi$ is:", "content": "where || is the L-1 norm, and $S_p$ is the softplus function: $f(x) = log(1+exp(x))$.\nThe final loss for training the key state localization network $\\phi$ is:\n$$\\mathcal{L}^{discover} = \\mathcal{L}^{CMaxMI}(\\phi; D) + \\lambda \\mathcal{L}^{div}(\\phi; D),$$"}, {"title": "with $\\lambda > 0$ the weighting between the significance of the discovered concepts and their diversity. Since $K$ might be larger than the actual number of key states involved in a manipulation task, we further apply non-maximum suppression to reduce the redundancy in the discovered concepts. The quality of the discovered concepts (localized key states) is studied in the experiments. Moreover, we evaluate the effectiveness of the proposed concept discovery framework (e.g., the MaxMI criterion) by training concept-guided manipulation policies using the localized key states.", "content": "with $\\lambda > 0$ the weighting between the significance of the discovered concepts and their diversity. Since $K$ might be larger than the actual number of key states involved in a manipulation task, we further apply non-maximum suppression to reduce the redundancy in the discovered concepts. The quality of the discovered concepts (localized key states) is studied in the experiments. Moreover, we evaluate the effectiveness of the proposed concept discovery framework (e.g., the MaxMI criterion) by training concept-guided manipulation policies using the localized key states."}, {"title": "3.4 Manipulation Policy with Discovered Concepts", "content": "We leverage the CoTPC framework [24] to learn concept-guided manipulation policy. Specifically, the policy network predicts the action and estimates the key states (corresponding to different concepts) simultaneously during the training phase. In this way, the concept guidance is injected into the action prediction process and can promote learning efficiency and generalization. Instead of using manually annotated key states, we apply the localized key states from our concept discovery framework. Given a trajectory $\\tau^i = \\{(s_i, a_i)\\}_t^T$ and its localized key states $\\{t_k^i\\}_k$, the CoTPC policy training loss is:\n$$\\mathcal{L}^{policy} = \\sum_{i=1}^{N} \\sum_{t} \\|\\hat{a_t} - a_t\\|_2 + \\lambda \\sum_{i=1}^{N} \\sum_{k=0}^{K-1} \\|\\hat{s_{t_k^i}} - s_{t_k^i}\\|_2,$$\nwhere $\\hat{a_t}$ and $\\hat{s_{t_k^i}}$ are the predictions from the policy network introduced in [24]. By optimizing Eq. 7, we now have a policy that can be applied to manipulation tasks, whose success rate in various settings shall indicate its performance as well as the usefulness of the discovered key states."}, {"title": "4 Experiments", "content": "In this section, we thoroughly evaluate the proposed key state discovery strategy. We first validate its effectiveness against baselines on complex multi-step tasks from ManiSkill2 [15] and Franka Kithen [14, 16], and then assess its performance and generalization in novel scenarios with unseen configurations and objects. Additionally, an ablation study is performed to explore various key state discovery approaches and the effect of the regularization term."}, {"title": "4.1 Experimental Setup", "content": "Baselines. The evaluation and comparison are performed with the following major baselines: Behavior Transformer (BeT) [42], Decision Diffuser (DD) [1], Decision Transformer (DT) [7], and Chain-of-Thought Predictive Control (CoTPC) [24]. For more information about these methods, please refer to the appendix.\nMulti-Step Manipulation Tasks. We follow the experimental setting used in the most recent state of the art [24], and choose the same set of four complex multi-step object manipulation tasks from the Maniskill2 environments [15]. These four tasks are shown in Fig. 4. Please refer to the appendix for more details about the tasks.\nExperimental Setting. We train the policies following the behavioral cloning (BC) paradigm without densely labeled rewards [24]. Similar to CoTPC, we set the state space as the observation space, assuming full observability unless stated otherwise. For fair comparisons, we utilize the same set of 500 randomly sampled trajectories for a task. These trajectories, varying in initial states and environmental configurations, are sourced from multiple experts, added with diversity and randomness. In the following sections, we detail the evaluation metric and experimental results."}, {"title": "4.2 Discovered Concepts vs. Manually Annotated Concepts", "content": "In Fig. 4, we compare manually annotated concepts with those discovered by our proposed method for four different tasks. Human annotations, restricted by their labor-intensive nature, are often sparse, thus providing limited information. Conversely, our method's flexibility in adjusting the number of key states enables the identification of a diverse range of concepts. This diversity, as evidenced in Fig. 4, underscores the capability of our method in enriching the concept guidance for more effective policy training.\nOn the other hand, Fig. 4 shows that, even without explicitly injecting human semantics in the concept discovery process, the key concepts annotated by humans and a subset of the concepts discovered by our method exhibit a high degree of similarity. This suggests that our approach is able to discover concepts aligning with human understanding of the manipulation process. This alignment not only validates the efficacy of our concept discovery method but also highlights its potential in capturing intrinsic task-relevant features that resonate with human semantics, thereby enhancing the interpretability and applicability of our method in complex scenarios.\nAlso note that, since our approach allows a flexible selection of the number of discovered concepts, more fine-grained concepts can be discovered that help promote the guided policy learning. To prevent redundant manipulation concepts due to state noise, we consider concepts occurring closely in time as identical."}, {"title": "Thus, we further employ a non-maximal suppression to prune repeatedly discovered concepts, with further details available in the appendix.", "content": "Thus, we further employ a non-maximal suppression to prune repeatedly discovered concepts, with further details available in the appendix."}, {"title": "4.3 Quantitative Results", "content": "Evaluation Metric. Our primary evaluation metric is the task success rate. For complex multi-step tasks, we also report the success rate of completing intermediate sub-tasks, e.g., the Peg Insertion Side task, where the final objective is to insert the peg into a horizontal hole in the box. The intermediate sub-tasks include \"peg grasping by the robotic arm\" and \"alignment of the peg with the hole.\" During the evaluation phase, both seen (during training) and novel environmental configurations such as different initial joint positions of the robot are tested. For the Turn Faucet task, we also perform an additional evaluation on faucets with geometric structures not present in the training set.\nMain Results. Tab. 1 and Tab. 2 present the results on the seen and unseen environmental configurations (including zero-shot with novel objects), respectively. Tab. 1 demonstrates the superior performance of our proposed method in comparison to various baseline methods across multiple tasks. It shows that the baselines have difficulties in handling complex multi-step tasks, while the policies trained with our discovered concepts consistently achieve the best performance, verifying the effectiveness of the proposed method in localizing physically meaningful key states. Due to the poor performance of baseline methods in dealing with novel configurations, we follow [24] to report the comparison with the two most effective baselines in unseen scenes. As evidenced in Tab. 2, our approach still outperforms these baselines, suggesting that our proposed key state discovery significantly enhances the generalization of the trained manipulation policies in new scenarios."}, {"title": "Franka Kitchen", "content": "Franka Kitchen To assess the efficacy in accomplishing long-horizon tasks, we train and test the concept-guided policies in the Franka Kitchen environment [14, 16], where a 9-DoF Franka robot operates within a virtual kitchen. This environment features a total of seven object-interaction tasks. We utilize the dataset introduced by [42] with 566 demonstrations. Each demonstration contains a sequence of four object-interaction tasks. The performance is measured by how many object-interaction tasks can be completed in one episode, i.e., the more tasks completed in one episode the better the policy. We check the ratio of the episodes that successfully achieve {1,2,3,4,5} tasks out of 100 trials. The results are reported in Tab. 3, with the maximum roll-out steps equal to 280. It shows that the discovered key concepts can effectively reduce the decision horizon, such that the policy trained with these concepts can achieve more subtasks within fixed steps in one roll-out."}, {"title": "4.4 Ablation of Different Key State Selection Mechanisms", "content": "We further conduct an ablation with various key state selection strategies. The results are detailed in Tab. 4. Please note that these policies are trained using the CoTPC framework [24]. The baselines mainly include those utilizing multimodal encoders, such as CLIP [37], BLIP [29], BLIP2 [29], and FLAVA [46],"}, {"title": "4.5 Effect of the Regularization Term", "content": "Now we study the impact of the regularization term, e.g., the pairwise distance (PD) penalty. The numerous results are presented in Tab. 5, which validate the necessity of each term. As shown in Fig. 5, key states in a demonstration discovered by different terms vary a lot. The pairwise distance term is observed to enforce diversity among the discovered concepts but alone can not ensure semantic meaningfulness, thereby simply exhibiting a uniform distribution across the trajectory. In contrast, relying exclusively on the MaxMI criterion tends to discover clustered concepts around the state with the highest mutual information value, thus reducing the diversity of the concepts. The full model can discover more meaningful key states while maintaining diversity."}, {"title": "5 Conclusion", "content": "We propose an information-theoretic metric that characterizes the physical significance of the key states in robotic manipulation tasks. We further leverage the proposed metric to develop a self-supervised manipulation concept discovery pipeline that can produce meaningful key states. When used as guidance for training policies, these key states can lead to higher performance than a broad spectrum of baselines. Additionally, we validate the necessity of the proposed terms through an extensive ablation study. Our investigation also shows that the proposed MaxMI criterion alone may not guarantee the diversity of the discovered concepts due to its local modeling characteristic, which we deem a limit. We propose that future studies can resolve this issue by extending the metric to a multi-scale concept discovery framework so that global information on the trajectories can be accessed."}]}