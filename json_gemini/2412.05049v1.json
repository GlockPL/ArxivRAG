{"title": "OCEAN: Open-World Contrastive Authorship Identification", "authors": ["Felix M\u00e4chtle", "Jan-Niclas Serr", "Nils Loose", "Jonas Sander", "Thomas Eisenbarth"], "abstract": "In an era where cyberattacks increasingly target the software supply chain, the ability to accurately attribute code authorship in binary files is critical to improving cybersecurity measures. We propose OCEAN, a contrastive learning-based system for function-level authorship attribution. OCEAN is the first framework to explore code authorship attribution on compiled binaries in an open-world and extreme scenario, where two code samples from unknown authors are compared to determine if they are developed by the same author. To evaluate OCEAN, we introduce new realistic datasets: CONAN, to improve the performance of authorship attribution systems in real-world use cases, and SNOOPY, to increase the robustness of the evaluation of such systems. We use CONAN to train our model and evaluate on SNOOPY, a fully unseen dataset, resulting in an AUROC score of 0.86 even when using high compiler optimizations. We further show that CONAN improves performance by 7% compared to the previously used Google Code Jam dataset. Additionally, OCEAN outperforms previous methods in their settings, achieving a 10% improvement over state-of-the-art SCS-Gan in scenarios analyzing source code. Furthermore, OCEAN can detect code injections from an unknown author in a software update, underscoring its value for securing software supply chains.", "sections": [{"title": "1. Introduction", "content": "According to the World Economic Forum's Global Risk Report 2024, cyberattacks are among the five biggest risks in 2024 [17]. In particular, the software supply chain, where externally developed code is introduced into critical parts of a network, represents a rising attack surface [12]. The ability to detect and attribute malicious code is an essential part of protection [9]. Authorship attribution has been extensively studied in fields such as literary criticism, forensic linguistics, and legal proceedings [4], [24], [39]. Similar techniques have also been used to identify code authorship as programmers leave stylistic signatures through distinct coding methodologies, preferences, and design patterns. Previous studies have demonstrated the feasibility of identifying programmers through their coding styles in both source code and binary forms [2], [5], [7], [10], [13], [23], [25], [26], [28], [29], [32], [34], [42].\nTraditionally, research has viewed the attribution of code authorship as a closed world classification problem. Using techniques ranging from random forests to modern machine learning methods, an unseen or new code sample is attributed to a known author from the training data set. Hence, only a finite set of authors is considered, and sample training data must be available for every author [2], [10], [23], [26], [28], [29], [32]. This inadvertently limits the applicability of the results as an application in a real-world scenario rarely provides a priori knowledge and sample data for every attributable author. Some techniques consider a limited open world scenario that reformulates the attribution as a membership problem, considering whether an author is part of the training dataset or not [5], [7], [13], [25], [42]. While this limited open world scenario enables applicability to authors beyond the known authors, it allows only coarse-grained attribution.\nOu et al. [34] adopt a genuinely open world approach to authorship attribution, where no predefined set of authors is assumed. Instead, they compare two source code extracts to predict whether they originate from the same author. While this approach, termed extreme authorship identifica-tion, enhances applicability in real-world scenarios, it remains constrained by two limitations. First, their evaluation is restricted to file-level granularity and relies on the original source code. Second, the dataset used for evaluation has been shown to lack representation of realistic production-level code [1], [10].\nWe address this critical gap by proposing a novel ap-proach, OCEAN, designed to predict whether two functions from a binary file belong to the same author. To ensure a realistic evaluation, we train and assess our method using two newly curated datasets derived from large-scale open-source software projects. Our approach leverages contrastive learning to train UniXcoder [21], a state-of-the-art machine learning model. To the best of our knowledge, OCEAN is the first authorship identification method to operate on binary files within an extreme open world scenario.\nTraining and evaluation on diverse and realistic data sets is paramount to the trustworthiness of reported performance and the transferability of results [8]. Hence, we introduce two novel datasets containing C and C++ programs. We focus on C and C++, as they hold the largest market share (37.2%) among compiled programming languages, according to the Stack Overflow Developer Survey 2024 [35]. The first dataset, CONAN, consists of 464 annotated binaries created from Conan [11], a popular package man-"}, {"title": "2. Background", "content": ""}, {"title": "2.1. Extreme Authorship Identification", "content": "The goal of extreme authorship identification is to deter-mine whether two given functions, 21 and 22, were written by the same developer [34]. Formally, given two program functions, 21 and 22, where x1 is written by author a\u2081 and x2 by a2, the objective is to determine whether a\u2081 = a2. Consequently, the goal is to develop a classifier, c(x1, x2) \u2208"}, {"title": null, "content": "{0, 1], which returns a value indicating the likelihood that X1 and 12 were written by the same author. The classifier should satisfy the condition\n$$c(X_1, X_2) = \\begin{cases}\n1 & \\text{if } a_1 = a_2 \\\\\n0 & \\text{otherwise}\n\\end{cases}$$\n}"}, {"title": "2.2. Program Representations for Authorship Iden-tification", "content": "Programs can be represented in many ways, and the choice of representation can significantly impact the effec-tiveness of the authorship attribution (Table 5).\nSource code is widely used in related work due to its high-level nature and the availability of contextual information [2], [10], [13], [23], [25], [26], [32], [34], [42]. The presence of comments and variable names, such as specific naming conventions or commenting styles, can provide additional clues about an author's identity.\nRaw binaries are the most low-level representation and provide little information about the original source code, as this sparse representation introduces significant noise. The amount of noise makes it difficult for classification models to detect author-specific elements, derived from the source code [6].\nDecompiled raw binaries of C/C++ code can bridge the gap between high-level and low-level representations. Al-though decompiled code might appear similar to the original source code, much information, including variable names and comments, is lost during compilation. The decompiler's style also influences the representation, as it follows specific patterns and conventions that may not exactly match the original author's style [7].\nAssembly instructions and P-code provide intermediate representations that balance between high-level source code and low-level binary data. Assembly instructions turn raw binary code into a human-readable syntax that can easily be assembled back into an executable binary. P-code, or pseudocode, is a form of intermediate code used by certain compilers to facilitate debugging and portability. In the decompiler Ghidra [3], there are two flavors of P-code: Raw P-code, which is a direct translation of machine instructions into a generic, low-level set of operations, and high-level P-code, which undergoes further analysis and transformation to aid in decompilation and readability. All representations still include register names and jump addresses, which could affect the performance of the authorship contribution, as they are not necessarily connected to the author's identity and therefore introduce noise. We mitigate the noise by replacing such occurrences with the fixed string HEXSTR. For example, we adapt a sequence like JUMP 0x1a2b to JUMP HEXSTR and call those adjusted representations cleaned (e.g., assembly (c)).\nAn example of a single function in all these representa-tions is given in Appendix A."}, {"title": "2.3. Contrastive Learning", "content": "Contrastive learning is an ML paradigm in which a model learns to differentiate between similar and dissimilar"}, {"title": "3. Datasets for Code Attribution", "content": "Many recent code attribution systems [2], [5], [7], [10], [25], [26], [34], [42] were evaluated using the Google Code Jam dataset (GCJ) [20], consisting of solutions for small programming tasks from an annual programming challenge held by Google. The GCJ dataset seems appealing for authorship identification as it links multiple source code files to individual authors. However, critics have pointed out that GCJ does not resemble real-world production code but rather ad-hoc solutions to specific challenges [1], [10]. Moreover, many samples are contaminated and contain auto-generated comments, such as the @author comment, which make authorship attribution trivial and prevent a robust evaluation of such systems for harsher real-world settings without such information. Other C/C++ datasets for code attribution often share similar problems or are, like the recent dataset of Li et al. [26], rather small, limiting the robustness of the evaluation and impede the effective use of data-hungry state-of-the-art ML techniques. Given these limitations, a dataset comprising real-world production code is essential for a more accurate evaluation of authorship identification systems."}, {"title": "3.1. New Real World Datasets", "content": "We present two new datasets (see Table 1) to address the challenges in the robust evaluation of code authorship attribution systems and leverage them for the development of OCEAN. We hope these datasets will facilitate the robust evaluation of future code attribution systems and enable fair comparisons of different approaches.\nCONAN: We leverage the Conan Package Manager [11] to construct our new dataset called CONAN. Similar to Maven or pip, the Conan Package Manager allows the inclusion of various libraries. Its default build routine can be modified to include custom flags while ensuring successful compilation of all libraries. For each library, we fetched the linked GitHub project and removed header-only and dynamically linked libraries to obtain complete and standalone binaries. CONAN consists of 464 projects of which 255 are written in C and 209 in C++.\nSNOOPY: Prior work showed that relying on a single dataset can introduce data snooping [8], a common flaw in au-thorship identification evaluations [2], [10], [13], [23], [25], [29], [32], [34]. Data snooping occurs when a model learns dataset-specific structural hints, leading to biased evalua-tions. To mitigate this risk, we manually collected and com-piled additional projects from GitHub, ensuring a diverse and unbiased dataset. Specifically, we chose Bitcoin [31], Curl [40], GLibC [36], Nginx [15], OpenSSH [37], PHP [18] and Redis [38] as control dataset. We call this dataset SNOOPY as it prevents data snooping. Utilizing SNOOPY ensures the evaluation of authorship attribution systems to be robust and generalizable.\nLabeling and pair construction: To annotate ownership information, we compiled all binaries with debug flags, which ensures that all function names remain in the binary. Those names allow us to uniquely identify and map each binary function to its corresponding source code, provided the function names are unique. In cases where function names are not unique, we discard them.\nFor each compiled binary, we use Ghidra's scripting API to extract all functions. Ghidra is an open-source reverse engineering tool developed by the NSA that provides ex-tensive capabilities for analyzing binary code [3]. Using its scripting API, we created custom scripts to automatically extract functions across all representations described in Sec-tion 2.2. The extracted function names are then matched to the original source code in the respective Git project, which is subsequently added to the dataset as an additional representation.\nTo attribute ownership to each function in the project, we employed the git-author tool by Meng et al. [30]. This tool analyzes the git history of the source code to attribute ownership. By examining the commit history, it determines which lines of code were contributed by which authors. The tool aggregates these information and records the ownership as a percentage that reflects each author's contribution to each function."}, {"title": "4. OCEAN", "content": "In this section, we present OCEAN, a fully automated framework for function-level authorship identification in C/C++ binaries. OCEAN leverages the CONAN dataset, contrastive learning, and supports various representation extraction techniques to build a robust classifier capable of distinguishing authors based on their coding style. As shown in Table 2, previous work mainly focused on the closed world scenario, or a simplified variant of the much harder open world scenario, a membership inference against the training set. Ou et al. [34] where the first to perform"}, {"title": "4.1. Overview", "content": "Training. We utilize CONAN for training (Step I), which consists of program binaries together with the source code authored by multiple programmers (see also Figure 1 for a full overview of the pipeline). In the representation extraction phase (Step II), we extract functions in the desired representation from each binary using Ghidra, a state-of-the-art reverse engineering tool [3]. Given a representation and the function-level authorship information of CONAN, we build positive and negative pairs for training (Step III). Positive pairs consist of samples from the same author, while negative pairs combine samples from different authors. Us-ing these pairs, we train a contrastive learning model to create embeddings for the samples (Step IV). The model learns to map samples from the same author (positive pairs) closer in the embedding space while pushing samples from different authors (negative pairs) apart.\nInference. During the inference phase OCEAN embeds samples pairwise into a high-dimensional vector space (Step V). To determine the authorship similarity, OCEAN lever-ages the cosine similarity between the embeddings of the two samples (Step VI). If the similarity is above a predefined threshold 0, OCEAN classifies them to be authored by the same individual."}, {"title": "4.2. Contrastive Learning", "content": "Contrastive learning is a technique where the model learns to distinguish between similar and dissimilar pairs of data. Therefore, it is particularly useful for tasks that require the model to generate informative and discriminative embeddings. In our context, these embeddings are crucial, as the cosine distance between two embeddings is used to determine whether the two inputs leading to the embeddings are from the same author.\nIn OCEAN, we use contrastive learning during training to ensure that the model can generate these rich embeddings. Specifically, we use a loss function that decreases the cosine similarity for negative pairs (different authors) and increases it for positive pairs (same author). Therefore, we adopt the loss function from Ai et al. [4]:\n$$L= -\\sum_{i}\\log\\frac{\\sum_{a_i=a_j}\\exp{(\\text{cos-sim}(e_i, e_j) /\\tau)}}{\\sum_{k}\\exp{(\\text{cos-sim}(e_i, e_k) /\\tau)}}$$\nHere, ei corresponds to the embedding generated by a model for the i-th sample, i.e., ei = f(xi) and \u03c4 is a fixed constant for the temperature, e.g., 0.1. The intuition behind the loss function is, that the numerator becomes large for positive samples (ai = aj), while the denominator adds a penalty for all negative samples that have a high cosine similarity. Thus, the fraction gets closer to one as the classifier improves. The fraction is encapsulated in a logarithm to turn the multiplication of probabilities into a sum, which makes the loss easier to calculate. Thus, the sign is inverted with a minus, so that the smaller the loss, the closer the fraction is to one. This loss function ensures that negative samples have reduced cosine similarity values compared to positive samples.\nAfter training, the model f(x) can generate embeddings for any given sample. To compare two samples x1 and x2, we compute the cosine similarity between their embeddings as follows:\n$$c(x_1, x_2) = 1 \\Leftrightarrow \\frac{f(x_1) \\cdot f (x_2)}{|| f(x_1)|| || f (x_2) ||} > 0$$\nAs the model is trained to have a higher cosine similarity for samples from the same author, we infer that if the cosine similarity c(x1,x2) exceeds a predefined threshold 0, the samples are likely to be from the same author. Otherwise, they are assumed to be from different authors."}, {"title": "4.3. OCEAN for Security", "content": "As OCEAN can compare authors, it can detect when new authors have contributed to a dependency during a software update. This ability can be used to identify po-tential code injections in the software supply chain. We hypothesize that although many functions are modified or added during typical updates, a function injected by an attacker will exhibit stylistic discrepancies detectable by authorship identification techniques. This technique could"}, {"title": null, "content": "be used, for example, to flag suspicious functions for further manual review by security analysts.\nOn every update, we can compare each changed or added function to all functions of the previous version. To achieve this compare, we use our previously described pipeline (see Section 4.1) to generate embedding vectors representing the stylistic features of each function in the old binary. Thus, we have a corpus of vectors for the old functions. For the updated binary, we collect all changed functions and compare each of these functions to all functions in the old corpus using the cosine distance. If the distance exceeds a certain threshold, we flag the new functions as possibly written by a new author.\nHowever, given the multiple comparisons involved, ad-hering to a single predefined threshold for each individual comparison is not statistically robust. The high number of comparisons increase the likelihood of finding at least one function whose distance exceeds the threshold by chance. So, we need to calibrate the model [14] and dynamically ad-just the threshold based on the number of changed functions to control false discovery rates. To determine the threshold for detecting potentially malicious code, we use statistical models. Since our distances are between 0 and 1, we use a Beta distribution to approximate the similarities observed for samples from the same author. Given the Beta distribution, we can calculate the expected minimum of a multitude of comparisons.\n$$f_{min}(x) = n \\cdot f(x) \\cdot (1 \u2212 F(x))^{n\u22121}$$\nWhere n is the number of comparisons, f(x) is the probability density function of the Beta distribution, and F(x) is its cumulative distribution function, both estimated from the data. The expected minimum distance E(Xmin) is calculated as:\n$$E(X_{min}) = \\int_0^1 xf_{min}(x) dx$$\nHence, \u03b8 = E(Xmin) enables us to establish a dynamic threshold that adapts based on the number of comparisons. If a function has a bigger minimal distance to the corpus than \u03b8 it was probably modified or inserted by a new author."}, {"title": "4.4. Implementation", "content": "Our implementation is based on ContraX [4], a frame-work that uses contrastive learning to perform authorship identification on written text. We have extended ContraX to support several models from related work [5], [27], [28], [32], [34], [42] as well as some new ones. These include Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM) networks, and Feedforward Neural Net-works (FNN) using two different embedding styles: Term Frequency-Inverse Document Frequency (TF-IDF) and N-grams. In addition, we use a pre-trained BERT model, specifically UniXcoder [21], which has been trained on code-related tasks to improve its code understanding. No-tably, these tasks are unrelated to authorship identification. Research suggests that pre-trained models for code-related tasks often outperform models trained from scratch, due to their ability to leverage learned representations from a large corpus of code [41]. While random forest or decision trees are often used in related work [2], [10], [13], [23], [25], we did not use those models in our approach because they do not naturally generate embeddings, which are necessary for contrastive learning.\nEach model requires a custom pre-processing pipeline to fit within our framework. We ensured that every model received raw input, which was subsequently converted to the necessary format for the respective model. Therefore, all models share a common interface, simplifying the inte-gration process. For the LSTM, we implemented a custom Byte Pair Tokenizer, which was trained using the training dataset. For the FNN, we used two different configurations for preprocessing: The first configuration involved TF-IDF at the character level, while the second employed character-level n-grams ranging from n = 1 to n = 3. Both configura-tions were pre-trained using the training data and returned a maximum of 1000 features. For the CNN, we followed the methodology outlined by Alrabaee et al. [5] and converted the bytes of the input data into a 100x100 matrix, effectively creating a 100x100 pixel grayscale image. If the input was too small, zeros were added for padding; if it was too large, the input was truncated. The hyperparameters of our models are shown in Table 7 in the Appendix. The output of each model is a 768-dimensional vector, rather than a classification value, to facilitate their integration into the"}, {"title": "5. Experiments and Results", "content": "To evaluate OCEAN and benchmark it against the pre-vious state-of-the-art solution in open world extreme author-ship identification (SCS-Gan by Ou et al. [34]), we examine several research questions. To the best of our knowledge OCEAN is the only solution supporting compiled binaries in the extreme open world setting. Therefore, we perform the comparison with SCS-Gan using only source-code rep-resentations. We evaluate the performance of OCEAN on binary representations in a more general approach without comparisons to previous work, as there seems to be no scheme in the same setting enabling a fair comparison. Additionally, we evaluate CONAN against the Google Code Jam (GCJ) dataset. After answering all research questions, we further demonstrate the capabilities of OCEAN in two case studies."}, {"title": "5.1. Research Questions", "content": "\u2022 RQ1: How does OCEAN compare to SCS-Gan in terms of classification performance on source code representa-tions?\n\u2022 RQ2: Does CONAN enable the development of authorship identification systems with better real world performance than GCJ?\n\u2022 RQ3: What is the best combination of program rep-resentation and model in OCEAN to perform extreme authorship identification on binary representations?\n\u2022 RQ4: How to leverage OCEAN in a real world applica-tion?\n\u2022 RQ5: How do Ghidra warnings affect our performance?\n\u2022 RQ6: How robust is OCEAN against compiler optimiza-tions?"}, {"title": "5.2. Experiment Setup", "content": "For all experiments, unless otherwise noted, we use function level granularity and binaries compiled with 00 flags, i.e., no optimization. A function is attributed to an author if they contributed at least 51% of it, as 51% is the minimum threshold for the majority of the function to be written by that author. Functions longer than 1000 tokens are truncated to fit the input requirements of the model [21]. This limit was chosen to provide support for long inputs while also maintaining computational efficiency. If a func-tion is shorter, it is padded to 1000 tokens. Additionally, we always evaluate pairs from the same binary to make sure that we evaluate stylistic detection rather than binary detection.\nOCEAN vs SCS-Gan: To answer RQ1, we compare OCEAN to the state of the art in extreme author identi-fication, namely SCS-Gan [34] by Ou et al. To perform this comparison, we initially contacted the authors to retrieve"}, {"title": "RQ1: OCEAN outperforms SCS-Gan by a large margin in the F1 score.", "content": "CONAN vs. GCJ: To answer RQ2, we compare CONAN (cf. Section 3.1) to the GCJ dataset, which is commonly used in related work [2], [5], [7], [10], [25], [26], [34], [42]. We trained all models that work in our contrastive learning setup (see RQ3) on GCJ and CONAN respectively. The evaluation was conducted on the test split of the training datasets and SNOOPY. As all samples in GCJ are written by a single author, we discarded all samples from CONAN and SNOOPY that were written by more than one author to allow"}, {"title": "RQ2: Using the CONAN dataset improves the per-formance on real-world codebases, indicating its su-perior value for real world applications compared to the GCJ dataset.", "content": "Models vs. Representations: In order to address RQ3, we performed a grid search over ML models and code repre-sentations. Specifically, we evaluated the performance of a pretrained Bert Model, namely UniXcoder [21], UniXcoder without pretraining, a CNN, a FNN, and an LSTM model on five different code representations: Assembly, Source Code, P-Code, the raw binary, and decompiled C code,"}, {"title": "RQ3: The best representation for authorship iden-tification on binaries is decompiled code without comments together with a strong model.", "content": "Threshold determination: We have evaluated OCEAN using AUROC, a metric that does not require a specific threshold. However, in real-world applications, the system should output a clear verdict. Hence, RQ4 investigates the threshold to determine if the author is the same or different.\nWe trained the classification model on the CONAN dataset and used SNOOPY as an unseen dataset to generate a pair of functions from the same author and a pair of functions from different authors. Using the trained model, we generated embeddings for all these pairs and computed the cosine similarities. Figure 2 presents a histogram of the cosine similarities of those pairs. The median cosine similarity for different authors is 0.73, while the median cosine similarity for the same authors is 0.94.\nTo determine the optimal threshold 0, we can use the for-mula from Section 4.3 together with the data from Figure 2 and n = 1 as we only compare one pair. This calculation results in the threshold 0 = 0.72. This threshold is the expected minimum for same authors and defines when an author is considered different."}, {"title": "RQ4: OCEAN is applicable in real-world scenarios because it allows the use of a threshold to make unambiguous verdicts based on two observable dis-tributions.", "content": "Ghidra performance\nAs Ghidra is the core of our data generation, the perfor-mance of our model is inherently tied to Ghidra's function-ality. If Ghidra encounters certain errors, such as parsing failures or incorrect disassembly, our entire data generation process is compromised. Therefore, RQ5 evaluates how warnings within Ghidra affect our performance. During"}, {"title": "RQ5: Samples that trigger warnings in Ghidra do not affect the performance of OCEAN. In contrast, some slightly increase the performance of the model.", "content": "Robustness: So far, we have only evaluated binaries com-piled with 00 flags, which lack compiler optimization and are therefore larger and slower. Such binaries are rarely used in practice, as developers typically use compiler flags for optimization. Therefore, RQ6 investigates whether op-timization affects our performance. For this experiment, we use the CONAN dataset, which uses a set of debugging compilation flags in supported projects and the original com-pilation flags for the remaining dependencies, thus providing"}, {"title": "RQ6: Compiler optimizations do not degrade the performance of OCEAN, supporting its applicability in real-world scenarios where optimized binaries are prevalent.", "content": "5.3. Case Study\nHaving assessed the overall performance of our author-ship identification system, we now demonstrate its practical applications and capabilities by exploring two case studies."}, {"title": "CS1: Can OCEAN distinguish two different authors in a compiled and unseen application?", "content": "\u2022 CS2: Can OCEAN be used to identify a new author between two software versions?\nVisualization of two authors: To address CS1, we compare the embeddings vectors of functions written by two authors. The expectation is that the vectors for each author will form a cluster. As target, we choose PHP because it has the highest number of functions written by two authors, making the case study more representative. For both authors, we use all their samples, i.e., 1456 for Author A and 986 for Author B. We then create embeddings using a model that has been trained on decompiled and cleaned code from the CONAN dataset. For every resulting embedding vector, we calculate the cosine distance to all other vectors. Using t-Distributed Stochastic Neighbor Embedding (t-SNE) we visually represent these relationships in Figure 4.\nIt can be seen that there are indeed two distinct clusters, with each dot representing a single function written by one of the two authors. While the majority of dots clearly belong to one of the two clusters, there are a few dots that appear in the opposite cluster. In particular, in the upper left of Author B's cluster, there is a small sub-cluster of dots written by Author A. However, the clear separation of the majority of points indicates that OCEAN can effectively distinguish between authors based on their coding patterns."}, {"title": "CS1: OCEAN is effective in distinguishing between authors based on their coding patterns, as evidenced by the formation of two clusters.", "content": "Security: To answer CS2, we test the feasibility of detecting a new author between two software versions. Therefore, we simulate a compromised software supply chain, by introducing malware in a software update. We used the malicious code from Zhou et al. [43] and merged it into a single function. The malicious sample encrypts the entire file system, a common approach for malware often referred to as ransomware. Next, we transformed the code from using Windows-based encryption libraries to using Linux-based equivalents. To minimize our influence on the injected code, we used ChatGPT4-omni [33] to rewrite the code and removed all comments that were not in the original malware sample. After injecting the malware into the target software,"}, {"title": null, "content": "we used the style check tools from the target project to match the style and coding guidelines used in the project. Specifically, we injected the malware into OpenSSH subset of SNOOPY, between version v9.7 and v9.8. So v9.7 is un-modified while v9.8 contains the malware by Zhou et al. We chose OpenSSH due to its widespread use and critical role in secure communications. Using Ghidra, we decompiled both versions and automatically identified functions that changed or have been added. Version v9.7 contains 1293 functions in total. During the update, 406 functions were changed or added, one of which was the injected malware. We treated all functions that contained at least 50 assembly instructions as potentially malicious, as OpenSSH includes many functions with very little statements after decompilation in Ghidra. This left us with 137 potentially malicious functions. Each of these functions was compared to the 1293 functions in the old version of the binary, and we measured the minimum distance for each comparison. Using the analysis from Section 4.3 with the data from Figure 2 and n = 1293, we get the threshold \u03b8 = 0.0119. Hence, we treat every distance greater than 0.0119 as potentially malicious, as shown in Figure 5. Our injected malware clearly stood out as malicious, by being the only function above the defined threshold."}, {"title": "CS2: OCEAN effectively detects a new author be-tween software versions, thus helping to identify a compromised software supply chain.", "content": "6. Related Work\nAuthorship identification has long intrigued scholars", "4": [24], "39": ".", "5": [7], "29": "and Omi et al. [32", "10": "also considered multiple authors per file in the closed world scenario. They used a Random Forest and a Neural Network to distinguish between functions that have been written by a single author in a multi-author project. In their work", "25": "explored a semi-open world", "13": "and Zafar et al. [42", "2": "proposed a classifier that operates at line-level granularity. They preprocessed source code by creating pairs of lines and trained a recurrent neural network (RNN) to classify if two lines were from the same author. They iteratively joined lines until blocks were formed", "23": "improved on the approach of Abuhamad et al. by extracting path-based features from each line of code before merging lines of code using a Siamese RNN. A Siamese Network consists of two identical neural networks that process input pairs and learn to distinguish between them. In the end they used a Random Forest classifier for the final author prediction. However", "applicability": "They can not consider compiled binaries because the decompiler could mix the style of"}]}