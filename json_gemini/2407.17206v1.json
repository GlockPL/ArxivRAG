{"title": "Take a Step and Reconsider: Sequence Decoding for Self-Improved Neural Combinatorial Optimization", "authors": ["Jonathan Pirnay", "Dominik G. Grimm"], "abstract": "The constructive approach within Neural Combinatorial Optimization (NCO) treats a combinatorial optimization problem as a finite Markov decision process, where solutions are built incrementally through a sequence of decisions guided by a neural policy network. To train the policy, recent research is shifting toward a 'self-improved' learning methodology that addresses the limitations of reinforcement learning and supervised approaches. Here, the policy is iteratively trained in a supervised manner, with solutions derived from the current policy serving as pseudo-labels. The way these solutions are obtained from the policy determines the quality of the pseudo-labels. In this paper, we present a simple and problem-independent sequence decoding method for self-improved learning based on sampling sequences without replacement. We incrementally follow the best solution found and repeat the sampling process from intermediate partial solutions. By modifying the policy to ignore previously sampled sequences, we force it to consider only unseen alternatives, thereby increasing solution diversity. Experimental results for the Traveling Salesman and Capacitated Vehicle Routing Problem demonstrate its strong performance. Furthermore, our method outperforms previous NCO approaches on the Job Shop Scheduling Problem.", "sections": [{"title": "1 Introduction", "content": "Combinatorial optimization (CO) problems, which are characterized by their discrete nature and often NP-hard complexity, are essential in many areas, including logistics, manufacturing, process design, and scheduling. Traditional methods often rely on heuristics that require domain expertise and struggle with scalability. In recent years, Neural Combinatorial Optimization (NCO) has emerged as a research area that aims to let deep neural networks learn to generate heuristics from the instance distribution of the problem at hand [4]. Among various strategies within NCO, the constructive approach formulates a CO problem as a finite Markov decision process and represents a solution to an instance as a sequence of incremental decisions. A neural network computes a policy to guide these decisions.\nThe policy network is typically trained with supervised learning (SL) techniques [38, 14, 9, 21, 8, 24] or reinforcement learning (RL) [3, 17, 27, 12, 22, 15, 28, 40, 29, 42, 13, 43]. Both approaches have particular challenges: SL-based methods require a large number of high-quality expert solutions to be used as labels, which are usually obtained from existing (exact) solvers. In the case of larger instances or complex problems, it can be challenging, if not infeasible, to pre-generate high-quality solutions. Conversely, RL-based methods do not necessitate the use of expert solutions, yet they are susceptible to the sparse reward problem [10] and high hyperparameter sensitivity [32]. Furthermore, RL-based approaches typically use policy gradient methods (in particular, variants of REINFORCE [39]), where gradients are derived from complete trajectories, resulting in high computational cost. While state-of-the-art constructive RL-based approaches such as POMO [22] demonstrate remarkable performance on the training distribution, they exhibit limited generalizability to larger problem instances. To further complicate matters, it has recently been shown [24, 8] that the poor generalization is likely due to the lightweight decoder structure of commonly used architectures [17]. Instead, Luo et al. [24] and Drakulic et al. [8] suggest increasing the decoder size, which leads to significant memory requirements for policy gradient methods.\nTo bridge SL- and RL-based methods, recent work [24, 6, 30, 25] diverges from RL to a more straightforward, self-improved learning (SIL) approach: During training, the current policy network is decoded (e.g., by sampling) to generate (or refine) solutions to randomly generated instances. The best solutions generated are used as pseudo-labels, which the network is trained to imitate via SL. Repeating this process creates a \"self-improving\" loop. As no gradients need to be collected during decoding, large architectures can be trained in this way. The challenge lies in identifying an effective decoding method that is (a) capable of rapidly generating solutions for potentially thousands of instances that can be utilized in a single training epoch; (b) able to provide diverse sequences for sufficient exploration but can be adapted to exploit the model in later training stages; and, (c) generalizable across different problems, with few hyperparameters to adjust to the specific problem at hand. Given (a), time-consuming search methods such as Monte Carlo Tree Search (MCTS) are unsuitable. Although naive Monte Carlo i.i.d. sampling from the policy is theoretically sound [6], its output is often not diverse. Furthermore, it can require a large number of samples (even at low annealing temperatures or Top-p/-k sampling [11]) to let the model improve in later stages of training [19, 33, 22, 30]. On the other hand, sampling sequences without replacement (WOR) [19, 33, 26] yields diverse sequences. However, for CO problems, it has been observed that the advantage of sampling WOR over sampling with replacement diminishes with increasing solution length [33, 30].\nIn this paper, we propose a simple yet effective decoding mechanism for sequence models that exploits the diversity and paralleliza-"}, {"title": "2 Related work", "content": "Constructive NCO The first application of neural networks to directly predict solutions to CO problems is attributed to the Pointer Network of Vinyals et al. [38]. Originally trained via SL, Bello et al. [3] employ REINFORCE [39] with a learned value baseline. Since then, as in many other areas of deep learning, variants of the Transformer [36] have become the standard architecture choice for many NCO models [17, 7, 22, 23, 8, 24, 44]. To circumvent learning a value function, the policy networks are usually trained with self-critical policy gradient methods [31] over complete trajectories. In particular, POMO [22] exploits problem symmetries and samples solutions from every possible starting node for a single instance, thereby significantly diversifying the solutions found. POMO and similar RL-methods perform remarkably well on training distributions of up to 100 nodes in routing problems. However, they do not scale well to larger instance sizes. The recent methods BQ [8] and LEHD [24] attribute the poor generalization to the light decoder structure of the used Attention Model [17]. In contrast, they propose significantly increasing the decoder size (e.g., up to nine transformer blocks in BQ). Drakulic et al. [8] and Luo et al. [24] train their models with SL on expert solutions to instances with only 100 nodes for routing problems and achieve state-of-the-art results when generalizing up to 1,000 nodes. However, the size of the architecture makes it challenging to train with policy gradient methods.\nSelf-improved learning To overcome the difficulties associated with RL and SL for NCO, recent studies propose a \"self-improving\" training paradigm. The central concept during training is to use the current policy and generate solutions (i.e., sequences) to random instances, which are then used as pseudo-labels to train the network with SL in a next-token prediction setup. In the appendix of their LEHD paper, Luo et al. [24] describe a self-improvement method where the model is pre-trained with RL on small routing problems to be computationally feasible. The resulting model is used to generate solutions to a set of randomly generated larger problem instances. Exploiting problem symmetries, the solutions are further improved by re-unrolling the policy along random subtours. The policy is then trained to imitate the resulting set of solutions. With impressive results, Luo et al. [25] further develop this approach for up to 100,000 nodes. However, the method is limited to routing problems where an optimal solution of a complete tour guarantees the optimality of any subtour. Corsini et al. [6] propose a \"self-labeling\" strategy for the JSSP. They utilize vanilla Monte Carlo i.i.d. sampling with the current model during training to obtain increasingly optimal solutions for the model to imitate. Pirnay and Grimm [30] employ a similar training strategy. They improve the sampling process by sampling solutions WOR over multiple rounds, with each round guiding the policy towards sequences that perform better than expected. This method is close to ours in the spirit of diversifying sequences by sampling WOR in multiple steps. However, their method requires scaling the advantages with problem- and training-dependent hyperparameters, which can be complex to tune.\nSequence decoding There is a plethora of search methods at inference time to improve on the greedy output of a sequence model besides pure sampling or beam search, e.g. [3, 13, 5, 11]. Related to our approach is Simulation-guided Beam Search [5], which performs beam search in an MCTS way by coupling the pruning step in beam search with greedy rollouts. However, due to its purely exploitative nature, it is an unsuitable choice for sequence decoding in the context of SIL. In general, AlphaZero-type algorithms [34] share similarities with SIL. However, running multiple simulations in the MCTS for a single action choice is time-consuming and non-trivial to parallelize. Concerning sampling, considering a diverse set of solutions can significantly improve training [22, 15]. A prominent way of diversification is sampling WOR [19, 33, 26]. In this work, we use SBS [19], as it can be parallelized in a manner analogous to regular beam search."}, {"title": "3 Preliminaries", "content": "We consider a CO problem with n discrete decision variables. A solution to a problem instance is given by a tuple $(a_1,..., a_n)$, representing the n variable assignments (we assume a given numerical order). Let $S$ be the space of all possible solutions $\\bar{a}_{1:n} := (a_1,..., a_n)$. The goal is to find a solution that maximizes a predefined objective function $f: S \\rightarrow \\mathbb{R} \\cup {-\\infty}$. Here, f maps infeasible solutions to -$\\infty$.\nThe constructive approach formulates the problem autoregressively, where a value is chosen for $a_1$, then for $a_2$ given $a_1$, and so on, until a full solution $a_{1:n}$ is created. The policy network $\\pi_\\theta$ to guide these incremental choices is a sequence model with parameters $\\theta$. For a partial solution $\\bar{a}_{1:d} = (a_1,...,a_d)$, with d < n, the policy $\\pi_\\theta$ computes the conditional distribution $\\pi_\\theta(a_{d+1}|\\bar{a}_{1:d})$ over the choices $a_{d+1}$ for the (d + 1)th decision variable. In particular, to"}, {"title": "4 Method", "content": "This section presents our main contribution, a sequence decoding method for efficient SIL. We also briefly recall SBS [19], which forms the backbone of our method. In the following, we omit the parameters $\\theta$ in the subscript of $\\pi_\\theta$."}, {"title": "4.1 Sequence decoding as tree traversal", "content": "As common for neural sequence models, we can view decoding $\\pi$ for a problem instance as traversing a search tree from root to leaf. The root node corresponds to an empty sequence. A node in the tree at depth d corresponds uniquely to a partial solution $\\bar{a}_{1:d} = (a_1,...,a_d)$, and the direct children of this node represent the possible assignments to the (d + 1)th decision variable. To be explicit, let $Ch(\\bar{a}_{1:d})$ be the set of direct children of $\\bar{a}_{1:d}$, then any $\\bar{b}_{1:d+1} = (b_1,..., b_{d+1}) \\in Ch(\\bar{a}_{1:d})$ satisfies $b_i = a_i$ for $1 \\leq i \\leq d$. Thus, a leaf node corresponds uniquely to a complete solution.\nBefore explaining how we search the tree, we briefly describe how to maintain the tree. When decoding the model for an instance, we create a search tree in memory and expand nodes as needed. When expanding a node $\\bar{a}_{1:d}$, we query the model $\\pi(\\cdot|\\bar{a}_{1:d})$ for the transition probabilities of its children. As described later, the transition probabilities will be modified during the search. Hence, for each node $\\bar{a}_{1:d}$, we additionally keep an unnormalized total probability $p(\\bar{a}_{1:d})$ which is set to the total probability $\\pi(\\bar{a}_{1:d})$ when the node is created. For a node $\\bar{a}_{1:d}$ with parent $\\bar{a}_{1:d-1}$, we denote by $\\bar{\\pi}(a_d|\\bar{a}_{1:d-1})$ the normalized transition probability\n$\\bar{\\pi}(a_d|\\bar{a}_{1:d-1}) = \\frac{p(\\bar{a}_{1:d})}{\\sum_{\\bar{b}_{1:d} \\in Ch(\\bar{a}_{1:d-1})} p(\\bar{b}_{1:d})}.$  (2)"}, {"title": "4.2 Stochastic Beam Search", "content": "Ranking nodes by their total log-probability, a beam search of some beam width $k \\in \\mathbb{N}$ is a standard decoding method to obtain a set of k unique high-probability sequences from the sequence model $\\pi$. In beam search, all nodes within the current beam can be evaluated by $\\pi$ in parallel, which aligns well with the effectiveness of GPUs on batches. Besides being classically a deterministic inference method, the k sequences found with beam search often lack diversity. Kool et al. [19] present SBS, an elegant modification of beam search to sample k sequences without replacement (WOR) from the sequence model $\\pi$. The main idea is to perform regular beam search but perturb the total log-probability $log \\pi(\\bar{a}_{1:d})$ by adding noise sampled from a standard Gumbel distribution. The Gumbel noise is sampled under the condition that the maximum perturbed log-probability of sibling nodes is equal to their parent's. This persists a node's perturbation down its subtree.\nThe authors show that by sampling WOR from the distribution of complete sequences, SBS can obtain a set of sequences with high diversity. As it only changes the scoring of nodes, SBS can be implemented and, importantly, parallelized as a regular deterministic beam search."}, {"title": "4.3 Take a step and reconsider", "content": "We can now introduce our proposed decoding method. We summarize the method in Algorithm 1, illustrate it in Figure 2, and give a brief walkthrough below."}, {"title": "5 Experiments", "content": "We evaluate the efficiency of our sequence decoding method within the SIL framework (cf. Section 3.2) on the Euclidean TSP and CVRP and the standard JSSP.\nCode Our code in PyTorch and models are available at https://github.com/grimmlab/step-and-reconsider. We perform training and evaluation using four NVIDIA GeForce RTX 3090 with 24GB RAM."}, {"title": "5.1 Routing problems", "content": "Traveling Salesman Problem A problem instance of the two-dimensional Euclidean TSP in the unit square is given by the coordinates of N nodes $x_1,...,x_N \\in [0, 1]^2 \\subset \\mathbb{R}^2$. The goal is to find a roundtrip that minimizes the total tour length, where the distance between two nodes $x_i, x_j$ is given by the Euclidean norm $|| x_i - x_j ||_2$. A complete tour is constructed sequentially by choosing one unvisited node after another (see Figure 1).\nCapacitated Vehicle Routing Problem In the CVRP, a delivery vehicle of capacity $D\\in \\mathbb{R}_{>0}$ needs to visit N customer nodes $X_1,..., X_N \\in [0,1]^2 \\subset \\mathbb{R}^2$. Each customer i has a demand $\\delta_i \\in \\mathbb{R}_{>0}$, which must be fulfilled by the vehicle. A feasible solution is given by a set of subtours which all start and end at a given depot node, where all customers are visited, and the sum of customer demands satisfied by each subtour does not exceed the capacity D. The aim is to find a feasible solution with minimal total tour length. Following the standard constructive formulation [17, 8, 24], visiting the depot is not seen as a separate step: a complete tour is constructed by deciding for each unvisited customer node whether it is reached via the depot or directly from the previous customer. This ensures solution alignment, as the length of two feasible solutions is the same, even if they contain a different number of subtours. Our problem setup is identical, so we refer to [24] for details.\nData generation and optimal solutions We consider TSP and CVRP instances of size $N \\in \\{100, 200, 500\\}$. We generate random problem instances in the standard way by sampling node coordinates uniformly from the unit square. Additionally, for the CVRP, demands are sampled uniformly from $\\{1, ..., 9\\}$. The capacity of the delivery vehicle is set to 50, 80, and 100 for a corresponding number of nodes 100, 200, and 500. For both problems, we train only on instances of size N = 100 and test generalization on the larger sizes. For comparison with SL, we generate a random training dataset of one million instances and a validation set of 10k instances. The test set consists of 10k instances for N = 100 (same set used in [17] for the TSP and [24] for the CVRP) and 128 instances for $N \\in \\{200,500\\}$ (same sets used in [8] for the TSP and [24] for the CVRP). For the TSP, we obtain optimal solutions from the Concorde solver [1] to use as labels in SL and compute optimality gaps. For the CVRP, (near) optimal solutions are obtained from HGS [37].\nPolicy network architecture We evaluate our approach using two recent state-of-the-art architectures for routing problems: the BQ architecture by Drakulic et al. [8] and the LEHD architecture by Luo et al. [24]. Both architectures are based on the Transformer [36] with a heavy decoder structure. In the original works, the models obtain strong generalization results but are trained with SL on expert data as the large architectures are unsuitable for training with RL. For CVRP with BQ, we stick to the original setup of nine transformer blocks with 12 attention heads and a latent dimension of 192. For TSP with BQ, the number of layers is the same, but with eight attention heads and a latent dimension of 128. For CVRP and TSP with LEHD, we use six transformer blocks in the decoder with eight heads and a latent dimension of 128. Similar to BQ, we use ReZero normalization [2] also for LEHD, as we found the training to be more stable (compared to no normalization as suggested in the original paper). For BQ and LEHD, the hidden dimension of the feedforward network in a transformer block is set to 512.\nTraining For the SIL training, we decode in each epoch solutions in parallel to 1,000 random instances. We use a beam width of k = 64 and step size s = 10. Generating the solutions takes"}, {"title": "5.2 Job Shop Scheduling Problem", "content": "Problem setup The standard JSSP of size $J \\times M$ is a CO problem with J jobs, each consisting of M operations with given processing times. Each job operation must run on exactly one of M machines (precedence constraint), which are assigned to the operations bijectively. A machine can only process one operation at a time. The op-"}, {"title": "5.3 Sampling comparison", "content": "Our decoding method relies on SBS to sample sequences WOR, which is already an established method to diversify the model output and enhance exploration. SBS is also the basis for the sampling method in GD [30]. Therefore, we compare the quality of the best solution obtained when decoding the policy with our method and sampling sequences WOR (with SBS) and GD. For each considered problem class TSP, CVRP, and JSSP, we take a checkpoint from the middle of the training process when the policy still has room for improvement and exploration is advantageous. We then decode solutions with beam width k and step size s using our method, and also with SBS and GD using the same computational budget. To ensure that we allow SBS and GD at least the same computational budget, we count the number of times we transition from a node in the search tree to a child node. One can show that when l is the length of a complete solution, our decoding method with parameters k and s takes g(k, s) node transitions with\n$g(k, s) = k \\cdot \\frac{k \\left( \\frac{tl}{s} \\right)}{2},$ (3)\nwhere $t = \\lfloor \\frac{l}{s} \\rfloor$.\nSampling k sequences WOR with SBS takes kl node transitions. In particular, we allow k [h(s)] transitions for SBS and GD, where\n$h(s) = \\frac{g(k, s)}{kl} = \\frac{k \\frac{tl - st^2 + st}{2}}{kl} = \\frac{2tl - st^2 + st}{2l}$  (4)\nFor example, for l = 100, k = 64 and s = 10, we have h(s) = 5.5, so we grant SBS to sample 6k = 384 sequences from the root. The"}, {"title": "6 Conclusion", "content": "The SIL paradigm, where the neural policy iteratively learns from its own decoded predictions, offers a promising path for NCO to overcome the training complexities and generalization challenges associated with RL methods. However, it requires the construction of a multitude of ever-improving solutions for a substantial number of problem instances during training. Despite this need, there needs to be more guidance apart from standard sequence decoding techniques from natural language processing on how to effectively build these solutions principled and exploratively. In this paper, we have proposed a novel sequence decoding technique for constructive NCO that is strikingly simple, does not rely on problem specifics, and works particularly well for longer planning horizons. We have achieved this by following a sampled, seemingly good solution for a limited number of steps and, importantly, replanning it by considering previously unseen alternatives. We have demonstrated our method on three prominent CO problems, showing comparable performance to training directly on expert solutions and the ability to surpass existing SIL methods. Notably, we have achieved new state-of-the-art results for NCO on the prominent JSSP Taillard benchmark. Due to its flexibility, our method can in principle also be used in other problem-specific SIL approaches, such as [25]."}]}