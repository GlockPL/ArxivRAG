{"title": "Take a Step and Reconsider: Sequence Decoding for Self-Improved Neural Combinatorial Optimization", "authors": ["Jonathan Pirnay", "Dominik G. Grimm"], "abstract": "The constructive approach within Neural Combinatorial Optimization (NCO) treats a combinatorial optimization problem as a finite Markov decision process, where solutions are built incrementally through a sequence of decisions guided by a neural policy network. To train the policy, recent research is shifting toward a 'self-improved' learning methodology that addresses the limitations of reinforcement learning and supervised approaches. Here, the policy is iteratively trained in a supervised manner, with solutions derived from the current policy serving as pseudo-labels. The way these solutions are obtained from the policy determines the quality of the pseudo-labels. In this paper, we present a simple and problem-independent sequence decoding method for self-improved learning based on sampling sequences without replacement. We incrementally follow the best solution found and repeat the sampling process from intermediate partial solutions. By modifying the policy to ignore previously sampled sequences, we force it to consider only unseen alternatives, thereby increasing solution diversity. Experimental results for the Traveling Salesman and Capacitated Vehicle Routing Problem demonstrate its strong performance. Furthermore, our method outperforms previous NCO approaches on the Job Shop Scheduling Problem.", "sections": [{"title": "Introduction", "content": "Combinatorial optimization (CO) problems, which are characterized by their discrete nature and often NP-hard complexity, are essential in many areas, including logistics, manufacturing, process design, and scheduling. Traditional methods often rely on heuristics that require domain expertise and struggle with scalability. In recent years, Neural Combinatorial Optimization (NCO) has emerged as a research area that aims to let deep neural networks learn to generate heuristics from the instance distribution of the problem at hand [4]. Among various strategies within NCO, the constructive approach formulates a CO problem as a finite Markov decision process and represents a solution to an instance as a sequence of incremental decisions. A neural network computes a policy to guide these decisions.\nThe policy network is typically trained with supervised learning (SL) techniques [38, 14, 9, 21, 8, 24] or reinforcement learning (RL) [3, 17, 27, 12, 22, 15, 28, 40, 29, 42, 13, 43]. Both approaches have particular challenges: SL-based methods require a large number of high-quality expert solutions to be used as labels, which are usually obtained from existing (exact) solvers. In the case of larger instances or complex problems, it can be challenging, if not infeasible, to pre-generate high-quality solutions. Conversely, RL-based methods do not necessitate the use of expert solutions, yet they are susceptible to the sparse reward problem [10] and high hyperparameter sensitivity [32]. Furthermore, RL-based approaches typically use policy gradient methods (in particular, variants of REINFORCE [39]), where gradients are derived from complete trajectories, resulting in high computational cost. While state-of-the-art constructive RL-based approaches such as POMO [22] demonstrate remarkable performance on the training distribution, they exhibit limited generalizability to larger problem instances. To further complicate matters, it has recently been shown [24, 8] that the poor generalization is likely due to the lightweight decoder structure of commonly used architectures [17]. Instead, Luo et al. [24] and Drakulic et al. [8] suggest increasing the decoder size, which leads to significant memory requirements for policy gradient methods.\nTo bridge SL- and RL-based methods, recent work [24, 6, 30, 25] diverges from RL to a more straightforward, self-improved learning (SIL) approach: During training, the current policy network is decoded (e.g., by sampling) to generate (or refine) solutions to randomly generated instances. The best solutions generated are used as pseudo-labels, which the network is trained to imitate via SL. Repeating this process creates a \"self-improving\" loop. As no gradients need to be collected during decoding, large architectures can be trained in this way. The challenge lies in identifying an effective decoding method that is (a) capable of rapidly generating solutions for potentially thousands of instances that can be utilized in a single training epoch; (b) able to provide diverse sequences for sufficient exploration but can be adapted to exploit the model in later training stages; and, (c) generalizable across different problems, with few hyperparameters to adjust to the specific problem at hand. Given (a), time-consuming search methods such as Monte Carlo Tree Search (MCTS) are unsuitable. Although naive Monte Carlo i.i.d. sampling from the policy is theoretically sound [6], its output is often not diverse. Furthermore, it can require a large number of samples (even at low annealing temperatures or Top-p/-k sampling [11]) to let the model improve in later stages of training [19, 33, 22, 30]. On the other hand, sampling sequences without replacement (WOR) [19, 33, 26] yields diverse sequences. However, for CO problems, it has been observed that the advantage of sampling WOR over sampling with replacement diminishes with increasing solution length [33, 30].\nIn this paper, we propose a simple yet effective decoding mechanism for sequence models that exploits the diversity and parallelization capabilities of Stochastic Beam Search (SBS) [19] in an MCTS-like manner: We maintain a search tree where each node represents a partial solution and leaf nodes are complete solutions. Given a beam width k and a step size s, we sample k leaf nodes WOR from the model using SBS. We then remove the probability mass of the leaves from the tree, which marks the sequences as sampled. We select the best solution from the sampled leaves and, assuming it corresponds to a sequence (a1,..., an), follow it for s steps. After shifting the tree's root to the partial solution (a1,..., as), we repeat the process of finding a better solution until we have traversed the entire tree.\nThe decoding method is fast, generalizable, and possesses only two intuitive hyperparameters, namely k and s. These can be readily adapted to the available computational resources and problem length. By marking found sequences as sampled, we consistently consider unseen alternative solutions. By following the best solution for s steps, we gradually reduce the length of the problem. This forces more diversity in the sampling process.\nWe summarize our contributions as follows:\n\u2022 In the recent spirit of simplifying and scaling the training process of NCO methods, we propose a novel and straightforward sequence decoding method for SIL.\n\u2022 We train two state-of-the-art architectures [8, 24] for the Traveling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with 100 nodes in an SIL setting using our decoding method. Our method matches the performance of SL on expert trajectories when evaluated on the training distribution and shows similarly strong generalization performance on larger instances.\n\u2022 We further evaluate our method on the Job Shop Scheduling Problem (JSSP), consistently outperforming current state-of-the-art NCO methods.\n\u2022 We additionally show on various policies that our proposed decoding method significantly outperforms SBS-based sampling with the same computational budget."}, {"title": "Related work", "content": "Constructive NCO The first application of neural networks to directly predict solutions to CO problems is attributed to the Pointer Network of Vinyals et al. [38]. Originally trained via SL, Bello et al. [3] employ REINFORCE [39] with a learned value baseline. Since then, as in many other areas of deep learning, variants of the Transformer [36] have become the standard architecture choice for many NCO models [17, 7, 22, 23, 8, 24, 44]. To circumvent learning a value function, the policy networks are usually trained with self-critical policy gradient methods [31] over complete trajectories. In particular, POMO [22] exploits problem symmetries and samples solutions from every possible starting node for a single instance, thereby significantly diversifying the solutions found. POMO and similar RL-methods perform remarkably well on training distributions of up to 100 nodes in routing problems. However, they do not scale well to larger instance sizes. The recent methods BQ [8] and LEHD [24] attribute the poor generalization to the light decoder structure of the used Attention Model [17]. In contrast, they propose significantly increasing the decoder size (e.g., up to nine transformer blocks in BQ). Drakulic et al. [8] and Luo et al. [24] train their models with SL on expert solutions to instances with only 100 nodes for routing problems and achieve state-of-the-art results when generalizing up to 1,000 nodes. However, the size of the architecture makes it challenging to train with policy gradient methods.\nSelf-improved learning To overcome the difficulties associated with RL and SL for NCO, recent studies propose a \"self-improving\" training paradigm. The central concept during training is to use the current policy and generate solutions (i.e., sequences) to random instances, which are then used as pseudo-labels to train the network with SL in a next-token prediction setup. In the appendix of their LEHD paper, Luo et al. [24] describe a self-improvement method where the model is pre-trained with RL on small routing problems to be computationally feasible. The resulting model is used to generate solutions to a set of randomly generated larger problem instances. Exploiting problem symmetries, the solutions are further improved by re-unrolling the policy along random subtours. The policy is then trained to imitate the resulting set of solutions. With impressive results, Luo et al. [25] further develop this approach for up to 100,000 nodes. However, the method is limited to routing problems where an optimal solution of a complete tour guarantees the optimality of any subtour. Corsini et al. [6] propose a \"self-labeling\" strategy for the JSSP. They utilize vanilla Monte Carlo i.i.d. sampling with the current model during training to obtain increasingly optimal solutions for the model to imitate. Pirnay and Grimm [30] employ a similar training strategy. They improve the sampling process by sampling solutions WOR over multiple rounds, with each round guiding the policy towards sequences that perform better than expected. This method is close to ours in the spirit of diversifying sequences by sampling WOR in multiple steps. However, their method requires scaling the advantages with problem- and training-dependent hyperparameters, which can be complex to tune.\nSequence decoding There is a plethora of search methods at inference time to improve on the greedy output of a sequence model besides pure sampling or beam search, e.g. [3, 13, 5, 11]. Related to our approach is Simulation-guided Beam Search [5], which performs beam search in an MCTS way by coupling the pruning step in beam search with greedy rollouts. However, due to its purely exploitative nature, it is an unsuitable choice for sequence decoding in the context of SIL. In general, AlphaZero-type algorithms [34] share similarities with SIL. However, running multiple simulations in the MCTS for a single action choice is time-consuming and non-trivial to parallelize. Concerning sampling, considering a diverse set of solutions can significantly improve training [22, 15]. A prominent way of diversification is sampling WOR [19, 33, 26]. In this work, we use SBS [19], as it can be parallelized in a manner analogous to regular beam search."}, {"title": "Preliminaries", "content": "3.1 Problem formulation\nWe consider a CO problem with n discrete decision variables. A solution to a problem instance is given by a tuple $(a_1,..., a_n)$, representing the n variable assignments (we assume a given numerical order). Let S be the space of all possible solutions $\\bar{a}_{1:n} := (a_1,..., a_n)$. The goal is to find a solution that maximizes a predefined objective function $f: S \\rightarrow \\mathbb{R} \\cup {\\{-\\infty\\}}$. Here, f maps infeasible solutions to -\u221e.\nThe constructive approach formulates the problem autoregressively, where a value is chosen for $a_1$, then for $a_2$ given $a_1$, and so on, until a full solution $\u0101_{1:n}$ is created. The policy network $\u03c0_\u03b8$ to guide these incremental choices is a sequence model with parameters \u03b8. For a partial solution $\\bar{a}_{1:d} = (a_1,..., a_d)$, with d < n, the policy $\u03c0_\u03b8$ computes the conditional distribution $\u03c0_\u03b8(a_{d+1}|\\bar{a}_{1:d})$ over the choices $a_{d+1}$ for the (d+1)th decision variable. In particular, to"}, {"title": "Self-improved training cycle", "content": "The model $\u03c0_\u03b8$ is trained using the following simple SIL strategy, which we generalize from [6, 30, 25]. We present an illustration of the training cycle in Figure 3.2. We randomly initialize parameters \u03b8 and keep best parameters \u03b8'. Initially, we set \u03b8' \u2190 \u03b8. We repeat the following steps for each training epoch:\n(1) Generate a set of random problem instances. For each instance, we use $\u03c0_{\u03b8'}$ to decode multiple solutions in some way, and keep the best solution found, according to the objective function f. The random instances and their best solutions found (used as pseudo-labels) build the training set for this epoch.\n(2) Train $\u03c0_\u03b8$ on the generated data to predict the next sequence element from a partial solution with a cross-entropy loss: For a batch of size B with solutions $\\bar{a}_{1:n}^j = (a_1^j, . . . , a_n^j)$ for j \u2208 {1, ..., B}, we uniformly sample a partial solution $\\bar{a}_{1:d_j}^j$ for each j and $d_j < n$. The loss to be minimized is then given by\n$$L_\u03b8 = -\\frac{1}{B} \\sum_{j=1}^B \\log \u03c0_\u03b8(a_{d_j+1}^j|\\bar{a}_{1:d_j}^j).$$\n(3) At the end of the epoch, evaluate $\u03c0_\u03b8$ and $\u03c0_{\u03b8'}$ greedily on a fixed validation set. If $\u03c0_\u03b8$ outperforms $\u03c0_{\u03b8'}$, update the best parameters \u03b8' \u2190 \u03b8.\nIn (1), for many problems, it is desirable to generate thousands of instances in each epoch (for comparison, the models in BQ [8] and LEHD [24] are trained with SL on 1M random instances and their optimal solutions). Consequently, the efficiency of the SIL strategy is strongly determined by the method used to decode the sequence model."}, {"title": "Method", "content": "This section presents our main contribution, a sequence decoding method for efficient SIL. We also briefly recall SBS [19], which forms the backbone of our method. In the following, we omit the parameters \u03b8 in the subscript of $\u03c0_\u03b8$.\n4.1 Sequence decoding as tree traversal\nAs common for neural sequence models, we can view decoding $\u03c0_\u03b8$ for a problem instance as traversing a search tree from root to leaf. The root node corresponds to an empty sequence. A node in the tree at depth d corresponds uniquely to a partial solution $\\bar{a}_{1:d} = (a_1,..., a_d)$, and the direct children of this node represent the possible assignments to the (d + 1)th decision variable. To be explicit, let $Ch(\\bar{a}_{1:d})$ be the set of direct children of $\\bar{a}_{1:d}$, then any $\\bar{b}_{1:d+1} = (b_1,..., b_{d+1}) \u2208 Ch(\\bar{a}_{1:d})$ satisfies $b_i = a_i$ for $1 \u2264 i \u2264 d$. Thus, a leaf node corresponds uniquely to a complete solution.\nBefore explaining how we search the tree, we briefly describe how to maintain the tree. When decoding the model for an instance, we create a search tree in memory and expand nodes as needed. When expanding a node $\\bar{a}_{1:d}$, we query the model $\u03c0(\u00b7|\\bar{a}_{1:d})$ for the transition probabilities of its children. As described later, the transition probabilities will be modified during the search. Hence, for each node $\\bar{a}_{1:d}$, we additionally keep an unnormalized total probability $p(\\bar{a}_{1:d})$ which is set to the total probability $\u03c0(\\bar{a}_{1:d})$ when the node is created. For a node $\\bar{a}_{1:d}$ with parent $\\bar{a}_{1:d-1}$, we denote by $\u03c0(\\bar{a}_{d}|\\bar{a}_{1:d-1})$ the normalized transition probability\n$$\u03c0(\\bar{a}_{d}|\\bar{a}_{1:d-1}) = \\frac{p(\\bar{a}_{1:d})}{\\sum_{\\bar{b}_{1:d}\u2208Ch(\\bar{a}_{1:d-1})} p(\\bar{b}_{1:d})}.$$ \n4.2 Stochastic Beam Search\nRanking nodes by their total log-probability, a beam search of some beam width k \u2208 N is a standard decoding method to obtain a set of k unique high-probability sequences from the sequence model $\u03c0_\u03b8$. In beam search, all nodes within the current beam can be evaluated by $\u03c0_\u03b8$ in parallel, which aligns well with the effectiveness of GPUs on batches. Besides being classically a deterministic inference method, the k sequences found with beam search often lack diversity. Kool et al. [19] present SBS, an elegant modification of beam search to sample k sequences without replacement (WOR) from the sequence model $\u03c0_\u03b8$. The main idea is to perform regular beam search but perturb the total log-probability $\\log \u03c0(\\bar{a}_{1:d})$ by adding noise sampled from a standard Gumbel distribution. The Gumbel noise is sampled under the condition that the maximum perturbed log-probability of sibling nodes is equal to their parent's. This persists a node's perturbation down its subtree.\nThe authors show that by sampling WOR from the distribution of complete sequences, SBS can obtain a set of sequences with high diversity. As it only changes the scoring of nodes, SBS can be implemented and, importantly, parallelized as a regular deterministic beam search.\n4.3 Take a step and reconsider\nWe can now introduce our proposed decoding method. We summarize the method in Algorithm 1, illustrate it in Figure 2, and give a brief walkthrough below."}, {"title": "Routing problems", "content": "Traveling Salesman Problem A problem instance of the two-dimensional Euclidean TSP in the unit square is given by the coordinates of N nodes $x_1,..., x_N \u2208 [0, 1]^2 \u2282 \\mathbb{R}^2$. The goal is to find a roundtrip that minimizes the total tour length, where the distance between two nodes $x_i, x_j$ is given by the Euclidean norm $|| x_i - x_j ||_2$. A complete tour is constructed sequentially by choosing one unvisited node after another (see Figure 1).\nCapacitated Vehicle Routing Problem In the CVRP, a delivery vehicle of capacity $D \u2208 \\mathbb{R}_{>0}$ needs to visit N customer nodes $X_1,..., X_N \u2208 [0, 1]^2 \u2282 \\mathbb{R}^2$. Each customer i has a demand $\u03b4_i \u2208 \\mathbb{R}_{>0}$, which must be fulfilled by the vehicle. A feasible solution is given by a set of subtours which all start and end at a given depot node, where all customers are visited, and the sum of customer demands satisfied by each subtour does not exceed the capacity D. The aim is to find a feasible solution with minimal total tour length. Following the standard constructive formulation [17, 8, 24], visiting the depot is not seen as a separate step: a complete tour is constructed by deciding for each unvisited customer node whether it is reached via the depot or directly from the previous customer. This ensures solution alignment, as the length of two feasible solutions is the same, even if they contain a different number of subtours. Our problem setup is identical, so we refer to [24] for details.\nData generation and optimal solutions We consider TSP and CVRP instances of size N \u2208 {100, 200, 500}. We generate random problem instances in the standard way by sampling node coordinates uniformly from the unit square. Additionally, for the CVRP, demands are sampled uniformly from {1, ..., 9}. The capacity of the delivery vehicle is set to 50, 80, and 100 for a corresponding number of nodes 100, 200, and 500. For both problems, we train only on instances of size N = 100 and test generalization on the larger sizes. For comparison with SL, we generate a random training dataset of one million instances and a validation set of 10k instances. The test set consists of 10k instances for N = 100 (same set used in [17] for the TSP and [24] for the CVRP) and 128 instances for N \u2208 {200, 500} (same sets used in [8] for the TSP and [24] for the CVRP). For the TSP, we obtain optimal solutions from the Concorde solver [1] to use as labels in SL and compute optimality gaps. For the CVRP, (near) optimal solutions are obtained from HGS [37].\nPolicy network architecture We evaluate our approach using two recent state-of-the-art architectures for routing problems: the BQ architecture by Drakulic et al. [8] and the LEHD architecture by Luo et al. [24]. Both architectures are based on the Transformer [36] with a heavy decoder structure. In the original works, the models obtain strong generalization results but are trained with SL on expert data as the large architectures are unsuitable for training with RL. For CVRP with BQ, we stick to the original setup of nine transformer blocks with 12 attention heads and a latent dimension of 192. For TSP with BQ, the number of layers is the same, but with eight attention heads and a latent dimension of 128. For CVRP and TSP with LEHD, we use six transformer blocks in the decoder with eight heads and a latent dimension of 128. Similar to BQ, we use ReZero normalization [2] also for LEHD, as we found the training to be more stable (compared to no normalization as suggested in the original paper). For BQ and LEHD, the hidden dimension of the feedforward network in a transformer block is set to 512.\nTraining For the SIL training, we decode in each epoch solutions in parallel to 1,000 random instances. We use a beam width of k = 64 and step size s = 10. Generating the solutions takes about 2 minutes on our setup. Using the generated best solutions as pseudo-labels, we train the model on 1,000 batches of 1,024 uniformly sampled subtours as in [8]. We apply the same training structure to LEHD. We use the Adam [16] optimizer with an initial learning rate of 2e-4, clipping gradients to unit norm. To evaluate the improvement of the model, we test the policy on the pre-generated validation set after each epoch. We train the policy until we see no improvement on the validation set for 50 epochs. The setup is the same for both routing problems. However, we found the generalization performance for the CVRP to improve noticeably when, after the regular training, finetuning the model on solutions where the policy was heavily exploited. To this end, we decode another 30k solutions with k = 256 and s = 1 and Top-p sampling with p = 0.8 and continue to train the model for another 100 epochs. For comparison with SL, we use the same training setup but sample batches from the pre-generated training set of 1M instances.\nIn total, this amounts to training the BQ model with SIL for ~3k epochs on the TSP (~4.5k with SL) and ~3k epochs on the CVRP (~1.5k with SL). The LEHD model converges faster. With SIL, it is trained for ~2k epochs on the TSP (~2k with SL) and ~1k epochs on the CVRP (~1k with SL).\nBaselines The primary baselines are given by SL with the corresponding identical BQ or LEHD architecture. Furthermore, we include four common constructive NCO baselines, namely (a) the widely used Attention Model (AM) with a beam search of width 1,024 [17], (b) its multi-decoder counterpart (MDAM) [41] with beam search of width 50, (c) POMO [22], the state-of-the-art constructive method on the training distribution, with their most potent inference technique, and (d) Simulation-guided Beam Search (SGBS) [5] with POMO backbone and parameters (\u03b2, \u03b3) set to (10, 10) for TSP and (4,4) for CVRP. As comparison partners for SIL methods, we list the results (TSP only) of (e) LEHD pre-trained with RL on small-scale instances and finetuned with SIL (LEHD RL+SIL) as reported in [24], and (f) the Gumbeldore (GD) training strategy (GD SIL (BQ resp. LEHD)) [30], where the sampling process is pushed toward regions with higher advantage.\nResults We summarize the results in Table 1 and group them by the used architectures. Bold indicates the best optimality gap per group. Results for LEHD RL+SIL and GD SIL are taken from the original papers [24, 30]. On the TSP, we obtain excellent greedy results that even outperform the SL counterpart on the training distribution of N = 100, with similarly strong generalization capabilities. In particular, we outperform GD SIL, which has a more complex decoding strategy. The same dynamic can be observed on the CVRP with BQ, with worse but still strong generalization results. Our SIL method with LEHD is close to, but does not fully reach, the SL results for CVRP. We note a general gap of about 1% between the LEHD SL results for CVRP in the original paper [24] and our reproduced results, which we attribute to the slightly different training method we aligned with BQ. At the bottom, we group the non-greedy results of our trained BQ model using beam search and our sequence decoding method as an inference technique (coupled with Top-p sampling)."}, {"title": "Job Shop Scheduling Problem", "content": "Problem setup The standard JSSP of size J \u00d7 M is a CO problem with J jobs, each consisting of M operations with given processing times. Each job operation must run on exactly one of M machines (precedence constraint), which are assigned to the operations bijectively. A machine can only process one operation at a time. The op-"}, {"title": "Sampling comparison", "content": "Our decoding method relies on SBS to sample sequences WOR, which is already an established method to diversify the model output and enhance exploration. SBS is also the basis for the sampling method in GD [30]. Therefore, we compare the quality of the best solution obtained when decoding the policy with our method and sampling sequences WOR (with SBS) and GD. For each considered problem class TSP, CVRP, and JSSP, we take a checkpoint from the middle of the training process when the policy still has room for improvement and exploration is advantageous. We then decode solutions with beam width k and step size s using our method, and also with SBS and GD using the same computational budget. To ensure that we allow SBS and GD at least the same computational budget, we count the number of times we transition from a node in the search tree to a child node. One can show that when l is the length of a complete solution, our decoding method with parameters k and s takes g(k, s) node transitions with\n$$g(k, s) = k \\cdot k(\\frac{tl - \\frac{s t^2}{2} + \\frac{st}{2}}{st}),$$ \nwhere $t = \\floor{\\frac{l}{s}}$.\nSampling k sequences WOR with SBS takes $kl$ node transitions. In particular, we allow $k \\cdot h(s)$ transitions for SBS and GD, where\n$$h(s) = \\frac{g(k, s)}{kl} = \\frac{2tl - st^2 + st}{2l}.$$ \nFor example, for $l = 100$, $k = 64$ and $s = 10$, we have $h(s) = 5.5$, so we grant SBS to sample 6k = 384 sequences from the root. The"}, {"title": "Conclusion", "content": "The SIL paradigm, where the neural policy iteratively learns from its own decoded predictions, offers a promising path for NCO to overcome the training complexities and generalization challenges associated with RL methods. However, it requires the construction of a multitude of ever-improving solutions for a substantial number of problem instances during training. Despite this need, there needs to be more guidance apart from standard sequence decoding techniques from natural language processing on how to effectively build these solutions principled and exploratively. In this paper, we have proposed a novel sequence decoding technique for constructive NCO that is strikingly simple, does not rely on problem specifics, and works particularly well for longer planning horizons. We have achieved this by following a sampled, seemingly good solution for a limited number of steps and, importantly, replanning it by considering previously unseen alternatives. We have demonstrated our method on three prominent CO problems, showing comparable performance to training directly on expert solutions and the ability to surpass existing SIL methods. Notably, we have achieved new state-of-the-art results for NCO on the prominent JSSP Taillard benchmark. Due to its flexibility, our method can in principle also be used in other problem-specific SIL approaches, such as [25]."}]}