{"title": "Privacy-Preserving Federated Learning via Homomorphic Adversarial Networks", "authors": ["Wenhan Dong", "Chao Lin", "Xinlei He", "Xinyi Huang", "Shengmin Xu"], "abstract": "Privacy-preserving federated learning (PPFL) aims to train a global model for multiple clients while maintaining their data privacy. However, current PPFL protocols exhibit one or more of the following insufficiencies: considerable degradation in accuracy, the requirement for sharing keys, and cooperation during the key generation or decryption processes. As a mitigation, we develop the first protocol that utilizes neural networks to implement PPFL, as well as incorporating an Aggregatable Hybrid Encryption scheme tailored to the needs of PPFL. We name these networks as Homomorphic Adversarial Networks (HANs) which demonstrate that neural networks are capable of performing tasks similar to multi-key homomorphic encryption (MK-HE) while solving the problems of key distribution and collaborative decryption. Our experiments show that HANs are robust against privacy attacks. Compared with non-private federated learning, experiments conducted on multiple datasets demonstrate that HANs exhibit a negligible accuracy loss (at most 1.35%). Compared to traditional MK-HE schemes, HANs increase encryption aggregation speed by 6,075 times while incurring a 29.2\u00d7 increase in communication overhead.", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) has emerged as a promising paradigm for collaborative model training without direct data sharing [18, 25]. While initially believed to preserve privacy [20, 35], recent studies have revealed vulnerabilities in FL, demonstrating that client-side gradients can potentially leak sensitive training data [6, 14, 26, 39].\nTo prevent data reconstruction in FL settings, researchers have been exploring various strategies, notably differential pri- vacy (DP) [13, 16, 32] and homomorphic encryption (HE) [7, 24, 30, 33, 36, 37]. DP stands out for its computational efficiency but may potentially reduce the performance of the FL model. Regarding HE, although it preserves the model's performance, it may compromise the data privacy of all honest participants if a client conspires with an external attacker to share the key (collusion attacks) [5, 11]. To mitigate this problem, Multi-Key Homomorphic Encryption (MK-HE) [7] has been proposed, which is designed to prevent collusion attacks without compromising the model's performance. However, the implementation of MK-HE introduces its own challenges, such as cooperation during the key generation or decryption processes [27]. These issues underscore the persistent dilemma faced in FL, that is, how to find the right trade-off between data privacy and the practical constraints of model performance as well as resource allocation.\nTo address these challenges, we propose Homomorphic Adversarial Networks (HANs), which offer multiple advantages over existing privacy-preserving methods in FL (comparisons shown in Table 1). Unlike traditional MK-HE methods, HANS do not require key distribution or collaborative decryption, which significantly simplifies the implementation in FL scenarios. Moreover, HANs demonstrate strong resistance to collusion attacks and provide efficient One-Time Pad (OTP) capabilities, further enhancing their privacy features.\nIn classical cryptographic paradigms, key management strategies bifurcate into symmetric and asymmetric encryption. Symmetric cryptosystems, while computationally efficient, employ a single shared key for both encryption and decryption, presenting significant key distribution challenges in distributed environments. Asymmetric cryptosystems utilize public-private key pairs, enhancing key distribution security but remaining susceptible to specific collusion attacks in FL contexts. Multi-key asymmetric schemes offer increased security at the cost of additional computational and communication overhead for key distribution and collaborative decryption protocols.\nOur comprehensive analysis revealed that extant key management approaches are suboptimal for the unique cryptographic requirements of FL environments. To address these limitations, we propose a novel Aggregatable Hybrid Encryption (AHE)"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Privacy-Preserving Federated Learning (PPFL)", "content": "Regarding PPFL, we mainly discuss differential privacy and holomorphic encryption.\nDifferential Privacy is a frequently utilized tool for privacy protection. These studies [2, 13, 15, 17, 29, 31] have utilized DP to secure data and user privacy. However, if there is a need to prevent the reconstruction of data, the inclusion of DP can significantly compromise the accuracy of the models.\nHomomorphic Encryption facilitates the execution of computations directly on a ciphertext to yield an encrypted outcome. Aono et al. [4] proposed the application of HE for the safeguarding of gradient updates during the FL training procedure. Chen et al. [8] proposed an FL framework specifically designed for wearable healthcare, which manages to achieve model aggregation by deploying HE. The application approach of this HE is expedient. Apart from encryption and decryption, it necessitates no significant alterations and imposes no extraordinary constraints on the algorithm. Importantly, the accuracy of learning is preserved with HE, as no noise infiltrates the model updates during either the encryption or decryption stages. [11] employs an enhanced Paillier algorithm to expedite computation. [36] proposed BatchCrypt, a FL framework based on batch encryption, with the goal of reducing computational expenses. However, traditional HE schemes require key sharing which relies on the assumption that there is no collusion between the Server and Clients [4].\nIn order to prevent collusion attacks, MK-HE allows multiple parties to utilize distinct keys for encryption. The decryption process necessitates the collaborative involvement of all parties. Ma et al. [23] introduced a PPFL framework based on xMK-CKKS, demonstrating resilience against collusion involving fewer than N-1 participant devices and the Server.\nHowever, this approach involves additional computational overhead during key generation and decryption, and requires collaboration among multiple clients for decryption. SecFed, an innovative federated learning framework, harnesses multi- key homomorphic encryption and trusted execution environments to safeguard multi-user privacy while boosting computational efficiency [5]. This secure system also integrates an offline protection mechanism to address user dropout issues effectively."}, {"title": "2.2 Cryptography Based on Generative Adversarial Networks", "content": "In recent years, using neural networks, especially Generative Adversarial Networks (GANs), for encryption has become an emerging direction in cryptography research. [1] proposed a method to learn symmetric encryption protocols based on GANS. They used two neural networks for encryption and decryption respectively, and introduced an attacker network to evaluate security. Subsequent works built upon this foundational research, further refining the approach [3, 21, 22, 28]. These studies introduced various enhancements, including diverse attack models and encrypted training schemes, thereby advancing the field of neural network-based cryptography.\nWhile these works have significantly contributed to the application of neural networks in cryptography, they still face certain limitations. Primarily, they focus on symmetric encryption without addressing homomorphic computation. Moreover, they do not adequately tackle the challenges of key distribution or negotiation, which are crucial aspects of practical cryptographic systems.\nInspired by these studies, particularly their loss function design and the application of GANs in training encryption neural networks, we propose HANs to address the aforementioned limitations and provide an enhanced solution for privacy protection within the FL context."}, {"title": "3 HANS System Definition", "content": "In this section, we formally define the encryption scheme and threat model within the context of PPFL. For a detailed explanation of the PPFL problem setting and system goals, please refer to Appendix A.\nThe security model involves a Server, multiple Clients, and an attacker. For ease of illustration, we assume three Clients in this scenario, each aiming to protect the privacy of its local dataset $D_i$ from both the Server and other Clients.\nWe begin by introducing the design concept of AHE, followed by its formal definition and application in modeling. Subsequently, we discuss the threat model and present the design, training, and security analysis of HANs. The detailed phases of the system are further elaborated in Appendices B, C, and E."}, {"title": "3.1 Design Concept of AH\u041d\u0415", "content": "The AHE algorithm is tailored to meet the specific challenges of PPFL. A key innovation is the design of the OTP-based key system. In traditional cryptography, public key encryption and private key decryption secure end-to-end communication, with the private key held securely by authorized parties. However, in PPFL, while individual gradient information must remain confidential, aggregated gradients should be accessible to all participants, necessitating a unique approach to key management. In this context, the key used to decrypt aggregated gradients does not need to be kept secret.\nTo address these challenges, we introduce a novel AHE scheme, with the following key concepts:\n\u2022 Public key: A key that can be made public, used for computing the aggregated plaintext.\n\u2022 Private key: Confidential key for encrypting original ciphertext.\n\u2022 Original plaintext: The plaintext containing gradient information from a single client, which should not be accessed by other clients or servers.\n\u2022 Original ciphertext: The ciphertext that corresponds to the original plaintext and is encrypted by a private key.\n\u2022 Aggregated plaintext: The combined gradient information derived from multiple original ciphertexts and their correspond- ing public keys. In PPFL contexts, this aggregated plaintext may be shared openly among all participants.\n\u2022 Original model: The initial HANs model distributed to clients by servers or third parties. It is potentially vulnerable to information leakage due to the absence of fully trusted distributors.\n\u2022 PPU: A process that clients can use to transform the original model into a private model. The specific algorithm and process are detailed in Appendix E.\n\u2022 Private model: The result of applying PPU to the original model. Each client securely stores their private model, treating it with confidentiality equivalent to private keys.\n\u2022 Public dataset: A small, noisy dataset maintained by each client to facilitate the PPU process without exposing their private model. It serves as a proxy for PPU participation.\nThis AHE scheme, tailored for PPFL, uses private keys for encryption and public keys for aggregation. It protects individual client data while enabling efficient aggregation without trusted third parties. By redefining key roles and introducing concepts like the original model, private model, and public dataset, it enables secure, decentralized collaboration.\nAHE primitives differ from traditional cryptography. We will clarify the capabilities and significance of the following attack methods in the AHE context:"}, {"title": "3.2 Definition of AHE", "content": "Here we further define the algorithms of AHE. It is worth noting that both the private and public keys are always real numbers rather than integers. This significantly expands the key space of AHE, thereby enhancing the security of the algorithm.\n1. $(pk, sk) \\leftarrow KeyGen(k)$. Generates a public key pk and private keys $sk = \\{ska,skB\\}$.\n2. $c \\leftarrow Enc(m, ska, skb, \\psi)$. Encrypts real number $m \\in [-\\psi, \\psi]$ using two private keys $ska$ and $sk\u00df$.\n3. $magg \\leftarrow Agg(C1,C2,C3, pk1, pk2, pk3)$. Aggregates three ciphertexts and outputs the sum of the plaintexts.\nEach private key consists of two real numbers, $ska$ and $sk\u00df$, generated from a security parameter k. The aggregated result only reveals the sum of the plaintexts, ensuring security as individual ciphertexts cannot be reversed."}, {"title": "3.3 Usability in Modeling", "content": "In traditional HE schemes like CKKS, the error introduced by HE must be relatively small compared to the ciphertext modulus [9]. However, in the context of PPFL, our criteria can be somewhat relaxed. Our primary objective is to ensure that the difference between the homomorphically aggregate values $magg$ and the actual value $mreal$ does not significantly affect the model's overall performance. Specifically, we require the original model to have high performance, so that after undergoing the PPU phase, it can maintain an acceptable level of performance."}, {"title": "3.4 Threat Model in AHE Setting", "content": "Attack Process. The adversary aims to exfiltrate the dataset of client $D_i$ through a three-step process:\n1. Step 1: The adversary intercepts the encrypted messages $\u0109_i$ and public keys $pk_i$ transmitted between the client and the server during the PPFL process. $intercept(\u00b7)$ is an interception algorithm capable of capturing all information transmitted through a communication channel: $(\u0109_i, pk_i) \\leftarrow intercept()$, where $c_i = Enc(\u03b8_i, sk_{ia}, sk_{ib}, \u03c8)$ where $\u03b8_i$ represent model parameters of $client_i$\n2. Step 2: Currently, there is no technology that can obtain plaintext information solely by analyzing the ciphertext c of HANS. Consequently, an attacker attempting a COA would be unsuccessful. Instead, the attacker would likely resort to a KMA. While the attacker may have access to the Original Model, they cannot know the target's private model. To break the ciphertext, they would utilize the original model to train two models $crack_1(\u00b7)$ and $crack_2(\u00b7)$.\nDetailed descriptions of these architectures and information on how to obtain them will be provided in the following section. We use $d^{attack}$ to represent model parameters cracked by the attacker: $\u03b8^{attack1} \\leftarrow crack_1(c_i, pk_i)$ and $\u03b8^{attack2} \\leftarrow crack_2(c_i)$\n3. Step 3: Using the cracked information, the adversary attempts to reconstruct the dataset of client $D_i$. $reconstruct()$ is an algorithm capable of reconstructing datasets based on gradients: $D^{attack1} \\leftarrow reconstruct(\u03b8^{attack1})$ and $D^{attack2} \\leftarrow reconstruct(\u03b8^{attack2})$\nAn attack is deemed successful if, upon reconstruction, either one of the two datasets by the adversary is similar to the authentic client dataset:\nAttack successful. $\\Leftrightarrow D^{attack1} \\sim D_i \\cup D^{attack2} \\sim D_i$\nWe presume the adversary possesses robust reconstruction capabilities. Under these conditions, a successful attack implies the adversary's ability to effectively decrypt gradient information. It is imperative to note that we protect our private model parameters with confidentiality equivalent to that of private keys. We assume attackers cannot access these private model pa- rameters, just as they cannot access private keys. Thus, attacks are only considered successful if the adversary achieves their objective without prior knowledge of the private model parameters."}, {"title": "3.5 Pseudo N-1 Collusion Attacks", "content": "In addition to attacks and challenges targeting the model's inherent encryption capabilities, the unique characteristics of HANS may lead to two types of pseudo N-1 collusion attacks. These attacks attempt to overcome the limitations of traditional N-2 collusion attacks by leveraging additional information to achieve an effect approximating N-1 collusion. However, due to the PPU mechanism, their effectiveness remains significantly limited. The basic ideas behind these two attacks are:\n1. Pseudo N-1 Collusion Attack based on Original Model (PCAOM): In this attack, the adversary uses another trusted client's original model to substitute for that client's private model. This is a KMA where the attacker attempts to simulate collusion among N-1 clients by using the publicly available original model, while in reality only N-2 clients are colluding.\n2. Pseudo N-1 Collusion Attack based on Public Dataset (PCAPD):\nThis attack utilizes the noisy public dataset's information generated during the PPU process. The attacker uses this public data to approximate the behavior of another trusted client, thereby achieving an effect similar to N-1 collusion. This is an enhanced version of a COA.\nDetailed formal definitions of these two attacks can be found in Appendix D."}, {"title": "3.6 Design and Training of HANS", "content": "Under the framework of the AHE scheme, the core of HANs lies in its carefully designed optimization objectives and training process, which work together to achieve a balance between privacy protection and accurate aggregation. This section introduces the main optimization objectives of HANs, with detailed implementation specifics available in the Appendices B and C.\nThe design of HANs primarily revolves around three main optimization objectives:\n1. Attacker's Optimization Objective:\n$O_{Enc} = \\underset{\\Theta_{Eve}}{\\operatorname{argmax}} ((L_{Eve}^{client} (\\Theta_{client}, \\Theta_{Eve}^{client})) +L_{Eve}^{client} (\\Theta_{client}, \\Theta_{Eve}^{client}))$\nThis objective aims to maximize the attacker's error in reconstructing the original data. A larger value of the loss indicates stronger privacy protection.\n2. Aggregation Optimization Objective:\n$O_{agg} = \\underset{}{\\operatorname{argmin}}(L_{agg} (Alice, Bob, Carol, Agg))$\nThis objective ensures that the encrypted data can still be accurately aggregated. A smaller value of the loss indicates higher aggregation accuracy.\n3. Comprehensive Optimization Objective:\n$O_{Enc} = \\underset{\\Theta}{\\operatorname{argmin}} (\\lambda \u0395\u03b8 + \\underset{i\u2208Clients}{\u2211} (max(0, \u03b3 \u2013 L'_{Eve} (\u03b8i, \u03b8'_{Eve})) + max(0, \u03b3 \u2013 L'_{Eve} (\u03b8i, \u03b8'_{Eve}))))$\nThis objective balances privacy protection and aggregation accuracy. Here, \u03b3 represents a privacy coefficient controlling the trade-off between security and accuracy, and \u03bb is a balancing parameter for aggregation accuracy.\nThe optimization objectives in HANs are designed to balance privacy protection and model usability throughout the training process. By employing a multi-stage optimization strategy\u2014encompassing computational pre-training, security enhancement, security assessment, and performance-security balancing, HANs ensures robust security without compromising performance. Additionally, the PPU mechanism allows clients to securely adopt and update their models while protecting sensitive data. This approach provides flexibility and adaptability, ensuring the privacy of encryption model encryption. Detailed descriptions of the optimization objectives, the multi-stage optimization process, and the PPU mechanism can be found in Appendix B, C, and E, respectively."}, {"title": "3.7 Security Discussion of HANS", "content": "Neural network-based cryptosystems, such as HANs, preclude conventional mathematical security proofs due to their inherent opacity. Nonetheless, we conduct an indirect security assessment by examining the constraints on attacker-accessible informa- tion.\nAppendix G elucidates the potential information exposure across HANs' lifecycle phases: training, PPU, and operational deployment. Our analysis indicates that HANs' architectural design substantially mitigates the efficacy of exploitable information (e.g., model parameters, public datasets, cryptographic keys, and ciphertexts) in practical attack scenarios.\nThe observed limitations on actionable information, in conjunction with HANs' empirically demonstrated resilience against diverse attack vectors, provide compelling indirect evidence for its security robustness and operational reliability."}, {"title": "4 Experimental Analysis", "content": "This section aims to validate the accuracy, security, and efficiency of HANs. All experiments were conducted using an A800 GPU. We have included the detailed design of the loss function, along with the specific training, distribution processes, and PPU, in the Appendix B, C, and E.\nTo improve experimental efficiency, we ensured that the encryption model structure used by each client was consistent. However, the attacker models were allowed to vary in structure to accommodate different attack strategies.\nThe encryption model consists of linear layers, convolutional layers, and residual blocks. The initial linear layer expands the dimensionality of the plaintext and private keys, while the convolutional layers obscure the relationship between them. Multiple residual blocks further enhance the complexity of the input transformation, and the output layer compresses the data to the target ciphertext length. The attacker models mirror the architecture of the encryption model, with input dimensions adjusted to accommodate the ciphertext input.\nWe employed the AdamW optimizer with a learning rate of le-5 and weight decay of 1e-6, combined with a cosine annealing scheduler to ensure training stability and generalization. Training data were generated by simple addition, enabling the model to handle diverse inputs and avoid overfitting."}, {"title": "4.1 Training Optimization and PPU Enhancements", "content": "We evaluated four attacker types: Atk 1 (using ciphertext and public keys), Atk 2 (using only ciphertext), and their double residual block versions, Atk 1 (Dbl) and Atk 2 (Dbl), as shown in Table 2.\nFor the encryption model, lower Average and Maximum Differences indicate better performance. For attackers, higher Average signify greater difficulty in data reconstruction.\nThe results show that Atk 1 faces greater challenges in data reconstruction compared to Atk 2, likely due to the added complexity of public key information. Even with increased complexity in Atk 1 (Dbl) and Atk 2 (Dbl), their performance improvements were marginal, suggesting that simply increasing model complexity is insufficient to break HANs' encryption mechanism.\nThe PPU process further enhanced security. Both CPPU and IPPU stages progressively increased the difficulty for attacker models, as evidenced by higher average and maximum differences. The narrowing performance gap between standard and double versions of the attacks further underscores the limitations of relying solely on increased model complexity to breach HANs' security.\nOverall, these results demonstrate the stability and attack resistance of the encryption model across different scenarios, showing that it effectively resists attempts to enhance attack success through increased computational complexity. While these metrics offer valuable insights into model performance and security, they do not provide absolute thresholds for meeting System Goals A.2. Therefore, further investigation in practical FL scenarios is required to fully evaluate the performance and security of HANS."}, {"title": "4.2 Performance and Security Analysis of HANs in FL", "content": "Table 3 presents a comparison between traditional additive aggregation and HANs aggregation on the MNIST [10], FashionM- NIST [34], and CIFAR-10 [19] datasets.\nThe Accuracy difference shows the impact of HANs on model performance. On MNIST, there is a 0.48% accuracy improve- ment, which could be due to the additional noise introduced during aggregation acting as a form of regularization on simpler datasets. However, there is a slight drop in accuracy for FashionMNIST (-0.27%) and CIFAR-10 (-1.35%).\nThe Average difference and Maximum differences quantify the discrepancies between parameters aggregated using HANS and traditional methods. Despite larger differences in some parameters, overall model performance remains nearly unaffected, demonstrating that HANs can maintain strong model performance while ensuring privacy.\nTo validate the security of our proposed scheme, we employ simple models in conjunction with the original Deep Leak- age from Gradients (DLG) attack. While recent research has advanced to more complex models and efficient reconstruction techniques [12, 38], the use of DLG on simpler models is sufficient for our security verification purposes.\nWe evaluated HANs' defense against DLG attacks using the MNIST dataset, which is known to be vulnerable [39]. Without HANS, DLG attacks were effective, but with HANs encryption, dataset reconstruction was unsuccessful."}, {"title": "4.3 Resistance to Pseudo N-1 Collusion Attacks", "content": "In evaluating the security of HANs, we conducted experiments on pseudo N-1 collusion attacks. Table 4 presents the results of the PCAOM and PCAPD.\nThese attack methods attempt to simulate the effect of N-1 collusion attacks, but their effectiveness is significantly limited due to the PPU mechanism. The experimental results show that after implementing PPU, PCAOM has a MAD of 0.31067, while PCAPD has a MAD of 0.30340. Notably, before the implementation of PPU, the result of PCAOM was equivalent to the Average value of HANs, indicating that the PPU mechanism effectively enhanced the system's security.\nThe lower half of the table 4 provides attack results for five specific samples from different orders of magnitude. Notable differences between the estimated and original values can be observed, ranging from 0.15399 to 0.41589. This further confirms the effectiveness of HANs in resisting these advanced attacks.\nThe similar performance of both attack methods suggests that the PPU mechanism successfully limits the amount of poten- tially leaked information, thereby enhancing the overall security of the system."}, {"title": "4.4 Operating Efficiency", "content": "To evaluate the computational efficiency of HANs, we conducted a series of experiments assessing encryption time, aggregation time, and communication overhead across various scenarios. Table 5 shows the performance metrics of HANs for different batch"}, {"title": "5 Conclusion", "content": "This work introduces Homomorphic Adversarial Networks (HANs) with Aggregatable Hybrid Encryption for Privacy-Preserving Federated Learning (PPFL). HANs leverage neural networks to emulate multi-key homomorphic encryption, offering a novel ap- proach that balances privacy, performance, and efficiency. Our method enables independent key generation and aggregation without collaborative decryption, while resisting N-2 client collusion. The innovative Privacy-Preserving Update mechanism enhances security through private model updates, effectively mitigating potential vulnerabilities in the initial public model. Experimental results demonstrate HANs' ability to maintain model accuracy within 1.35% of non-private federated learning. HANS also significantly outperform traditional multi-key homomorphic encryption schemes, achieving a 6,075\u00d7 increase in computational efficiency. The introduction of these neural network-based protocols not only improves the practical implementation of PPFL but also opens new research directions in federated learning privacy protocols and neural network-based cryptography."}, {"title": "A Problem Overview", "content": ""}, {"title": "A.1 Problem Setting", "content": "In the setting of FL, there are two primary roles: (1) Clients, who possess local training datasets and are responsible for complet- ing local model training. Clients have the obligation to ensure the privacy and security of the dataset. (2) The Server, responsible for coordinating with Clients to update global model parameters, also initializes the model and global hyperparameter settings.\nSuppose that we have m separate Clients. Each Client is represented by $C_i$, where i \u2208 [1,m] and Client $C_i$ has a local training dataset $D_i$. In each step, there are three sub-steps:\n1. Broadcast. Server broadcasts the current global model parameters $w^{t\u22121}$ to each Client $C_i$, where t represents the index of the current iteration round.\n2. Local training. Each Client $C_i$ receives global model parameters $w^{t\u22121}$ and using local training datasets $D_i$ to obtain the new local model parameters $w_i^t$ in parallel and sends the local model parameter $w_i^t$ back to the Server. During the updating step, the Client typically employs stochastic gradient descent for local epochs. In scenarios where communication cost is not a primary concern, setting local epochs to 1 can be an effective approach [25].\n3. Aggregation. The Server receives all model parameters $\\{w_1^t,w_2^t,...,w_m^t\\}$ from the Client and aggregates them into global parameters $w_i^t$ by averaging them.\nDefinition of \u03b4-accuracy loss. Suppose that M is a deep learning model training on datasets $D$, where $D = D_1 \\cup D_2 \\cup\u2026 \\cup D_m$. We use \u0192 to denote the accuracy of model M. For FL, M denotes the model after all train rounds, and its corresponding accuracy is \u0192. We say that it is \u03b4-accuracy loss, if it satisfies \u0192 \u2212 f < \u03b4."}, {"title": "A.2 System Goals", "content": "1. Input privacy. Our objective is to preserve the privacy of the client dataset $D_i$ during all processes, even if attackers gain access to either partial true gradient information or noisy gradient information $w^{attack}$, which is insufficient to reconstruct $D_i$.\n2. Model utility. After several rounds of encryption and aggregation, the final global parameters of the model $w^{final}$ are accurately computed, ensuring that the model can be used as intended.\nWe assume that all parties involved in the agreement will correctly complete model training and aggregation according to the FL agreement."}, {"title": "A.3 Threat Model", "content": "We consider the threat model where the adversary aims to steal the gradient information $w_i^t$ and $w_i^t$ transmitted between the client and the server, and then use it to reconstruct the dataset $D_i^{attack}$ to match a specific client's dataset $D_i$."}, {"title": "B HANS Training Design: Loss Function Formulation and Rationale", "content": "In this section, we provide a complete and rigorous derivation of the optimization objectives for HANs, expanding upon the high- level summary presented in Section 3.6. The derivation includes the mathematical formulation of the encryption and aggregation processes, as well as the multi-stage optimization strategy employed to balance privacy and performance.\nFirstly, we would like to clarify that all distances mentioned in this paper refer to the Manhattan distance. During model training, we employ MSEloss $L_m$ as our loss function, which enhances our ability to train the model effectively. However, when evaluating the model, we utilize Llloss, as it allows for a more intuitive analysis of errors.\nWe have formally defined the goals of each party in the previous section, and now we will present the specific training methods. We will use \u03b8 to represent model parameters in HANs.\nThe adversary Eve's goal is simple: to accurately reconstruct $w$ to achieve the attacker's objective. We will employ two attack methodologies, namely attacks with and without the use of a public key. The utilization of dual attack methods can ensure the resilience of the attacker. Additionally, if we can effectively thwart both attacks simultaneously, it will demonstrate the defensive efficacy of our solution.\nTo ensure the security of each Client's data, where $Clients = \\{Alice, Bob,Carol\\}$, we have designed separate attackers for each Client. We train each attacker independently. We use the Alice's attacker as an example for discussion. We denote the attacker attacking Alice's output on input C without the public key as $Attack(lice, C)$, and the attacker attacking Alice's output with the public key as $Attack(lice, (sk_1 + sk_2),C)$. The loss function for the two attack models is designed as follows:\n$L_{Alice}^{Eve} (\\Theta_{Alice}, \\Theta_{Eve}^{lice}, W_{Alice}, sk_{1a}, sk_{2a}) = L_m(W, Attack(\\Theta_{lice}, Enc_{sk1a, sk2a}(W_{Alice})))$"}, {"title": "CHANs Training Implementation: A Multi-stage Optimization Process", "content": "In the previous chapter, we elaborated on the design of the HANs model, including the formulation of the loss function and the rationale behind its design. These design considerations laid the theoretical foundation for our training process. However, in practical implementation, we discovered that directly applying the designed optimization objective 2 for training might lead to two main challenges:\n\u2022 Imbalance between Security and Usability: The model might overemphasize Input Privacy while neglecting Usability in Modeling, resulting in the Enc in HANs generating ciphertexts unrelated to the plaintext. In this case, while security is ensured, the model's practical utility is severely compromised.\n\u2022 Insufficient Aggregation Functionality: Even when Enc generates ciphertexts that contain plaintext meaning and are sufficiently secure, the Aggregate model might not be adequately trained to complete the aggregation task. This leads to the entire system being unable to effectively process and integrate encrypted data from multiple sources.\nTo address these challenges, we have decomposed the training process into five crucial stages:\n1. Computational Pre-training: Utilizing optimization formula 1, aiming to satisfy Usability in Modeling.\n2. Security Enhancement Training: Employing optimization formula 2, with the objective of achieving Input Privacy while maintaining Usability in Modeling.\n3. Security Assessment: This phase fixes all Enc models and the Aggregate in HANs, continuing to train each Attack model until convergence. As we have not yet determined a definitive security boundary, it is necessary to simulate it in an FL scenario for additional security confirmation.\n4. Performance-Security Balance Adjustment: HANs models trained through the first two stages often prioritize security at the expense of performance. Therefore, we conduct small-scale training using loss function A, followed by a final security validation on the trained HANs model. If it fails, we repeat the performance-security balance adjustment; if it passes, we proceed to the fifth stage.\n5. Aggregation Alignment: Having ensured the security of individual Enc models through previous training, this stage fixes all Enc models and trains the Aggregate model using optimization formula 1 until convergence, concluding the comprehensive training process for the HANs model."}, {"title": "D Pseudo N-1 Collusion Attacks", "content": "PPFL scenarios based on multi-key homomorphic encryption typically can only resist N-2 collusion attacks. This is because if only one client remains honest and trustworthy, colluding clients can easily obtain that client's real data by subtracting their uploaded data from the aggregated model gradients. This scenario is not one that multi-key homomorphic encryption is designed to defend against, and our method is no exception. However, due to the unique characteristics of HANs, attackers may potentially employ two types of pseudo N-1 collusion attacks. We will now formally define these two attacks."}, {"title": "D.1 Pseudo N-1 Collusion Attack Based on the Original Model (PCAOM)", "content": "PCAOM is a KMA. Let $clients = client_1,..., client_N$ be a set of N clients in a FL system using HANs. This attack can be described in the following steps:\n1. Initial Setup:\n\u2022 A trusted client Alice ($client_a \u2208 clients$) encrypts a gradient message $m_A$: $c_A = Enc(m_A, sk_{a1},sk_{a2})$\n\u2022 Alice's public key $pk_a = sk_{a1} + sk_{a2}$ is transmitted and intercepted.\n2. Attacker's Preparation:\n\u2022 The N-2 colluding attackers acquire the latest aggregation model Aggregation().\n\u2022 The attacker ($client_{att} \u2208 clients\\{client_a,client_\u00df\\}$) generates and encrypts $m_{att}$:\n$c_{att} = Enc(m_{att}, sk_{att1}, sk_{att2})$\n\u2022 The attacker's public key: $pk_{att} = sk_{att1}+ sk_{att2}$\n3. Exploitation of Bob's Original Model:"}, {"title": "D.2 Pseudo N-1 Collusion Attack Based on Public Dataset (PCAPD)", "content": "This represents an enhanced version of a COA", "follows": "n1. Initial Setup:\n\u2022 A trusted client Alice ($client_a \u2208 clients$) encrypts a message $m_A$: $c_A = Enc(m_A", "Preparation": "n\u2022 The N-2 colluding attackers acquire the latest aggregation model.\n\u2022 An attacker ($client_{att"}, "clients\\{client_a,client_\u00df\\}$) generates and encrypts $m_{att}$:\n$c_{att} = Enc(m_{att}, sk_{att1}, Sk_{att2})$\n\u2022 The attacker's public key: $pk_{att} = sk_{att1} + sk_{att2}$\n3. Exploitation of Bob's Public Dataset:\n\u2022 The attacker obtains Bob's ($client_e \u2208 clients$) public dataset from the last round of the PPU process.\n\u2022 From this dataset, the attacker extracts:\nA noisy plaintext $m_B^{pub} = m_B + noise$, where noise is unknown to the attacker\nThe corresponding ciphertext $c_B^{pub} = Enc(m_B,sk_{B1}, sk_{B2})$\nThe public key $pk^{pub} = sk_{B1} + sk_{B2}$\n4. Aggregation:\n$m_{agg} \\leftarrow Aggregate(c_A, c_{pub}, c_{attack}, pk_a, pk_{pub}, pk_{attack})$\n5. Guessing Alice's Message:\n$m_{guess} = m_{agg} - m_{pub} - m_{attack}$\nNote that this guess includes an error term due to the noise in $m_{pub}$.\nThe"]}