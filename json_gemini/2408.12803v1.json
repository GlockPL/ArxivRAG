{"title": "Multi-Treatment Multi-Task Uplift Modeling for Enhancing User Growth", "authors": ["Yuxiang Wei", "Zhaoxin Qiu", "Yingjie Li", "Yuke Sun", "Xiaoling Li"], "abstract": "As a key component in boosting online user growth, uplift modeling aims to measure individual user responses (e.g., whether to play the game) to various treatments, such as gaming bonuses, thereby enhancing business outcomes. However, previous research typically considers a single-task, single-treatment setting, where only one treatment exists and the overall treatment effect is measured by a single type of user response. In this paper, we propose a Multi-Treatment Multi-Task (MTMT) uplift network to estimate treatment effects in a multi-task scenario. We identify the multi-treatment problem as a causal inference problem with a tiered response, comprising a base effect (from offering a treatment) and an incremental effect (from offering a specific type of treatment), where the base effect can be numerically much larger than the incremental effect. Specifically, MTMT separately encodes user features and treatments. The user feature encoder uses a multi-gate mixture of experts (MMOE) network to encode relevant user features, explicitly learning inter-task relations. The resultant embeddings are used to measure natural responses per task. Furthermore, we introduce a treatment-user feature interaction module to model correlations between each treatment and user feature. Consequently, we separately measure the base and incremental treatment effect for each task based on the produced treatment-aware representations. Experimental results based on an offline public dataset and an online proprietary dataset demonstrate the effectiveness of MTMT in single/multi-treatment and single/multi-task settings. Additionally, MTMT has been deployed in our gaming platform to improve user experience.", "sections": [{"title": "1 Introduction", "content": "To offer a better personalized experience and increase user engagement, online marketing platforms usually provide incentives such as advertisements [18], discounts [8], and bonuses [1]. Although these incentives are crucial for generating additional revenue and activity, they are often costly, and individual users can have varied responses to different incentives. For example, some users will not play the next game without a bonus, while others will continue to play regardless. Consequently, accurately modeling individual users' responses and identifying the target user groups that are likely to be positively affected by incentives is essential for enhancing marketing benefits [32].\nOne of the fundamental challenges to measuring the response is the existence of the counterfactual problem, where an individual is either treated (treatment group) or not treated (control group). Therefore, in the same context, we can not simultaneously observe a user's response to a certain incentive or no incentive. Such a problem can be referred to as causal inference [33]. To resolve this, uplift modeling [10] has been proposed to estimate the individual treatment effect (ITE) (a.k.a. uplift) that describes how individual user responds to an incentive [35].\nThe current uplift modeling frameworks predominantly concentrate on directly modeling the response functions of both treatment and control groups to infer counterfactual predictions. Among them, meta-learner-based methods leverage existing models to estimate the Individual Treatment Effect (ITE) of personalized treatments. For example, S-learner [16] estimates the conditional average outcome of treatment and control group, then calculates ITE through subtraction. Building on this, other two-step meta-learners were proposed with other additional operations, including X-Learner [16], DR-Learner [14], R-Learner [22], etc. Nevertheless, these methods are prone to be influenced by the sample imbalance between the"}, {"title": "2 Related Works", "content": "The MTMT model has been deployed on our online gaming platform, serving millions of users."}, {"title": "2.1 Uplift Modeling", "content": "Uplift modeling aims to establish a difference in the users' behaviors when applying or not applying certain treatments by measuring the corresponding ITE. Existing uplift research mainly focuses on three settings: 1) single-treatment single-task setting. For example, meta-learner-based methods [14, 16, 22] integrate existing models to predict ITE. Besides this, tree-based methods [2, 31] gradually divide subpopulations by different metrics and estimate the ITE at the leaf node. Due to their superior feature extraction abilities, deep neural networks have gained much popularity in uplift modeling. They either directly model the uplift from learned representations [13], or separately estimate the natural response and treated response, then calculate the uplift by subtracting [4, 10, 25-27, 39]. 2) single-treatment multi-task setting. Huang et al. [12] interpret the user behaviors as a sequential chain, and include two chained tasks to estimate their uplifts. They first extract user and treatment features with an encoder, then design a network with two branches that have shared parameters to predict the uplifts of the two tasks. 3) multi-treatment single-tasks setting. Zhao et al. [38] extend several meta-learners to the multi-treatment setting. Other works adapt tree-based methods to estimate multiple treatments' uplifts [36, 37]. For the neural network-based approach, [21, 28, 30] design independent heads to estimate each treatment group's response and the control group's response. Liu et al. [17] employ the treatments as the input and explicitly extract its features, then predict the uplifts of each treatment through a single head. This paper considers the multi-treatment multi-task uplift modeling and explores its application in our online gaming server."}, {"title": "2.2 Multi-task Learning", "content": "Multi-task learning has been widely applied in computer vision, recommendation systems, and other fields. To boost the online business, multiple targets often influence marketing strategies. While focusing on a single task, existing models tend to ignore useful information from the training signals of related tasks [24]. The introduction of multiple tasks mitigates the sample bias, where tasks with more training samples and be informative to tasks with fewer samples [34]. In addition, training a multi-task model can reduce the cost of maintaining several models for each task. In the era of deep learning, a popular research line of multi-task learning is parameter sharing, where parameters are shared between different tasks. This includes hard parameter sharing [9], which encodes representations of tasks into a shared embedding, and then applies task-specific heads to predict the outcome of each task. Extended from this, soft parameter sharing [6] applies separate branches to model tasks and share information between branches by weighted addition or attention. Additionally, there is expert sharing [20], which utilizes several expert models to embed features and weigh the influence of each sub-task, and subsequently apply weights to calibrate the output of each expert."}, {"title": "3 Problem Definition", "content": "Assume the observed data to be D = {[xi, (fi, ti), Yi]}_{i=1}^n, where xi \u2208 \\mathbb{R}^d is the d-dimensional user features, t\u00a1 \u2208 {0,1} is the base treatment that denotes whether offering incentives to users, ti \u2208 \\mathbb{R}^m is the secondary treatment features with m treatments, yi \u2208 \\mathbb{R}^k is the k tasks, and n is the number of samples.\nWe follow the Neyman-Rubin causal inference framework [23] to define the estimation of ITE. Let y(0) denotes the potential outcome of the i-th user in the control group on task k, and y (m) denotes the outcome of receiving treatment m. Due to the counter-factual problem, we can only observe the outcome of a user in one treatment group, including the control group. As a result, there is no ground truth for supervised training. Instead, we estimate the expected response difference between the corresponding treatment and control. Let \\tau_i^k (xi) be i-th user's uplift when receiving a treatment, \\tau_i^m (xi) be the incremental uplift under the m-th treatment and k-th task. Then the overall treatment effect can be computed as:\n$\\Tau^m(xi) = \\tau_i^k (xi) + \\tau_i^m (xi) \\times {\\mathbb{I}(t = 1)} = E(y^{(1)} \u2013 y^{(0)}|xi) + E(y^{(m)} \u2013 y^{(1)}|xi, t = 1)$\nwhere  \\tau_i^k (xi)  is the i-th user's overall uplift score on the k-th task, \\Uparrow signifies if an individual is in the treatment group."}, {"title": "4 Methodology", "content": null}, {"title": "4.1 Architecture", "content": "The illustration of the proposed multi-treatment multi-task (MTMT) uplift modeling network is shown in Fig. 2. Given a sample {xi, (fi, ti), Yi}, the user features xi are first encoded by the user feature encoder to generate the representations for each task {\\Phi_0, \\Phi_1, \u2026\u2026\u2026, \\Phi_k}. Meanwhile, the base treatment feature fi (indicates control or treatment) and the secondary treatment features ti (indicates specific types of treatments) are encoded separately to generate the corresponding representations \u00ea and e. The feature representations of each task are projected by the corresponding classifier head to compute its natural response (0) of task k when not treated. Additionally, the embeddings of base and secondary treatments are fed separately into the corresponding user-treatment interaction module, in which the cross-correlations between treatment and user features are computed. The resultant treatment-aware features are further enhanced and used to estimate the response difference for receiving treatment and the incremental uplift for receiving treatment m."}, {"title": "4.2 Task-Oriented Feature Encoder", "content": "To explicitly model inter-task relationships and learn task-specific representations, we adopt the multi-gate mixture-of-experts (MMOE) [20] as the user feature encoder. Mixture-of-experts is a form of ensemble learning that integrates numerous expert models (e.g., vanilla CNN) to learn a shared representation and use the combined predictions to improve accuracy [7]. Extended from this, MMOE introduces an additional gating network to filter useful information from the shared representation of each task. To obtain the encoded representation \\Phi_i^k of for task k:\n$\\Phi^k = G^k (x_i) \\bullet {\\Phi^1 (x_i), \\Phi^2 (x_i),\u2026\u2026,\\Phi^n(x_i)}$\nwhere \\Phi^j is the j-th expert network and there are n experts and Gk is the gating network for k-th task. We choose ResNet18 [11] as the backbone for each expert. Note that the embeddings from the experts are stacked and then filtered by the corresponding gate to produce a task's representation. The gating network can be a simple linear projection from the input with additional activation:\n$G^k = softmax(W_k x_i)$\nwhere Wk \u2208 \\mathbb{R}^{n \\times d} is the trainable weights, n is the number of experts, and d is the feature dimension.\nThe representations \\Phi_i^k for each task only contain non-treatment information and are therefore used to estimate the natural response of the control groups by a linear projection head:\n$\\overline{y}^{(0)} = wproj \\Phi_i^k$\nSince most treatments are binary or discrete, we first one-hot encode the base and secondary treatment seprately and multiply the resultant sparse vector with a corresponding learnable dense matrix A\u00a1 \u2208 \\mathbb{R}^{v \\times m} to produce embeddings:\n$\\hat{\\epsilon_i} = A_i t_i,  \\epsilon_i = A_i t_i$\nwhere v is the embedding dimension and m is the number of possible treatments.\nIt should be noted that both user and treatment feature encoders can be substituted with other feature representation learning networks, provided they have the appropriate dimensions. For instance, a single ResNet18 can be used for user feature extraction, adapting the Multi-Task Multi-Treatment (MTMT) model to a single-task setting. The flexibility of the proposed design allows for seamless online implementation across various problem settings."}, {"title": "4.3 User-Treatment Feature Interaction", "content": "To explicitly utilize treatment features and model their relationships with user features, we propose the user-treatment feature interaction module based on self-attention [29]. The structure is shown in Fig. 3. We treat the treatment embeddings e\u00a1 as the query and the user feature embeddings o as the key and value. Subsequently, we compute how each treatment attends to each user feature, then use the resultant attention scores to generate treatment-aware embeddings:\n$\\hat{w}_i^m = softmax(\\frac{W_{\\epsilon_i} \\times (W_U \\Phi_i^k)^T}{\\sqrt{d_U}} )WU'$\nwhere \\hat{w} linearly projects the treatment embedding and \\hat{W}_U, WU' linearly projects the user feature embeddings. \\sqrt{d_U} is a scaling factor. We then process \\Phi_i^k by a user-treatment feature enhancer to further refine the useful information and the resultant embeddings are projected to estimate the base uplift score \\hat{\\tau^k} (xi) and the incremental uplift score \\tau_i^m (xi) for treatment m:\n$\\hat{\\tau^k} (x_i) = W \\cdot MLP(\\overline{\\Phi}_i^m),   \\tau_i^m (x_i) = W \\cdot MLP(\\overline{\\Phi}_i^m)$\nHere we use a multi-layer perception (MLP) as the feature enhancer and W is the projection matrix."}, {"title": "4.4 Multi-Treatmet Multi-Task Uplift Estimation", "content": "Given the natural response \u04ef (0), the base uplift score \\hat{\\tau^k} (xi), and the incremental uplift score \\tau_i^m (xi), we can estimate the user's response by:\n$\\overline{y}^{(m)} = \\overline{y}^{(0)} + \\hat{\\tau^k} (x_i) + \\tau_i^m (x_i)$\nHere (xi) indicates how the user acts to a treatment, and (xi) indicates the incremental effect of the specific treatment m, on top of the base uplift. Intuitively, \\tau_i^m (xi) is only effective when the user is in the treatment group. For online deployment when the treatment information is unknown, we permute all possible combinations of treatments and rank the resultant base and incremental uplifts.\nCompared to previous methods for multi-treatment uplift modeling, the proposed approach separately estimates the effects of \"whether to treat\" and \"what to treat\", thereby bridging the numerical gap between the base uplift and the uplift variations among treatments."}, {"title": "4.5 Training and Inference", "content": "We use the natural response and treated response for each task to compute the overall loss across the entire sample space:\n$\\sum_{k=1}^{N} [\\sum_{i \\epsilon C}  L(y_i, \\overline{y}^{(0)}) + \\sum_{i \\epsilon T} L(y_i, \\overline{y}^{(m)})]$\n$\\sum_{k=1}^{N} [\\sum_{i \\epsilon C} L(y_i, \\overline{y}^{(0)}) + \\sum_{i \\epsilon T} L(y_i,  \\overline{y}^{(0)} +  \\hat{\\tau^k} (x_i) + \\tau_i^m (x_i))]$\nwhere L is the mean-squared error loss function, C denotes the control group, and T denotes the treatment group. At inference, we only use Eq.7 to directly compute the base and incremental uplifts, which are subsequently ranked and used to determine whether to offer treatment and which treatment to offer."}, {"title": "5 Experiments", "content": "We conduct extensive experiments to verify the effectiveness of the proposed MTMT in single-treatment single-task, single-treatment multi-task, multi-treatment single-task, and multi-treatment multi-task settings. We mainly focus on the following questions:\n\u2022 RQ1: Can the proposed MTMT outperform other baseline methods on public and product datasets?\n\u2022 RQ2: How does each design contribute to the overall performance of MTMT?\n\u2022 RQ3: Whether the model produces interpretable results that are consistent with our online observations?"}, {"title": "5.1 Experimental Setup", "content": null}, {"title": "5.1.1 Datasets.", "content": "\u2022 CRITEO [5]: CRITEO is an open-sourced uplift modeling dataset for online advertising. The data is created by compiling data from various incremental tests, with a specific type of randomized trial in which a portion of the population is randomly excluded from advertising targeting. We include about 14 million samples, each has 12 continuous features, and use visit as the target.\n\u2022 Product: We include a product dataset containing over 10 million samples collected from the our online gaming platform. There are about 700 discrete and continuous features that describe users' static profiles and their recent gaming histories. To minimize the influence of confounding factors in uplift modeling, we gather data from randomized controlled trials. In these trials, treatments are assigned randomly to ensure an even distribution of potential outcomes between the treatment and control groups. For the multi-task scenario, we employ two binary labels: whether the user logs in the next day (short-term activity) after receiving or not receiving the treatment and whether the user plays more games in the next 7 days (long-term activity). For the multi-treatment scenario, we identify the base treatment as \"whether to give a bonus\" and the secondary treatment as \"the type of bonus\" (bonus type A and bonus type B), both of which are binary-valued. Note that in the practical application, certain bonus types are only available for a subset of users. Therefore, we collect an additional multi-treatment dataset by selecting users who are accessible by all types of bonuses."}, {"title": "5.1.2 Baselines.", "content": "To demonstrate the performance of MTMT, we include a set of popular methods proposed for uplift modeling, including: S-Learner [16], T-Learner [16], CFR [26], DragonNet [27], EUEN [13], DESCN [39], FlexTENet [4], EFIN [17], and M3TN [28].\nFor the multi-treatment problem, we employ EFIN, M3TN, and HydraNet [30]. Furthermore, we extend S-Learner and T-Learner to handle multiple treatments. For S-Learner, we directly apply the multi-treatment features as the input and iterate all possible treatment assignments to compute the corresponding uplift score. For T-Learner, we use multiple branches to process each treatment individually."}, {"title": "5.1.3 Evaluation Metrics.", "content": "Following the previous works, we adopt Area Under the QINI Curve (QINI), Area Under the Uplift Curve (AUUC), and the uplift score at first 30% (LIFT@30) to evaluate the uplift ranking capability of different models. Note that for easier and fairer comparison, we employ the normalized QINI and AUUC."}, {"title": "5.1.4 Implementation Details.", "content": "We train all models on NVIDIA A100, with Pytorch 2.1.2 and Python 3.11. We use the AdamW optimizer [15] with a learning rate of 0.001 and cosine annealing learning rate scheduler [19]. Additionally, we use a batch size of 15360 and set the maximum epochs as 50. For the detailed parameter setting of MTMT, we employ the standard ResNet without the classifier head as the expert in the user feature encoder and set the number of experts to 4."}, {"title": "5.2 RQ1: Performance Comparison", "content": "We evaluate the single-treatment single-task variation of the proposed methods and the baselines for single-treatment and multi-treatment uplift estimation. The results are presented in Table 1 and Table 2. Note that for MTMT in the single-treatment case, we only use base uplift \\hat{\\tau^k} (xi) from Eq.7 to decide whether a treatment should be offered.\nIn Table 1, we test all models based on two tasks on the product dataset: short-term activity and long-term activity. We further show how the multi-task version of MTMT performs on the two tasks. From the table, MTMT has 0.164 QINI on the CRITEO dataset. Meanwhile, the best-performing baseline on the CRITEO dataset is FlexTENet, which reaches 0.0779 QINI. On the product dataset, MTMT still maintains its advantage over other baselines on both tasks. For the multi-task setting, MTMT's performance slightly degrades, while its overall performance is still much better than the baselines.\nTo further validate MTMT's performance in the multi-treatment setting, we present the results in Table 2. We separately calculate the metrics on the two types of treatments, bonus A and bonus B. From the table, among the baselines, M3TN performs the best on bonus A and EFIN performs the best on bonus B, while MTMT outperforms all its comparatives on the two types of treatments."}, {"title": "5.3 RQ2: Ablation Study", "content": "We conduct ablation studies to validate the effectiveness of the key design of MTMT. Specifically, for the architecture, we remove the user-treatment feature enhancer and substitute the user-treatment feature interaction module with matrix multiplication. For the multi-treatment case, we remove the \"secondary treatment features\" branch as in Fig.2 and estimate uplifts of multiple treatments with a single output. For the multi-task case, instead of estimating ITE for each task, we jointly estimate the ITE for all tasks. The results are presented in Table3. From the table, changing the network architecture or modifying the ITE estimation process can result in performance degradation."}, {"title": "5.4 RQ3: Interpretable Analysis", "content": "To demonstrate the model produces interpretable results that are consistent with our online observations, we visualize the density distributions of the base treatment effect (\\hat{\\tau^k} (xi)) and the incremental treatment effect (\\tau_i^m (xi)) using our product dataset. As shown in Fig. 4, the average of \\hat{\\tau^k} (xi) (0.055) is much larger than the average of th(xi), which corresponds to our observations in the online gaming server (Fig. 1).\nTo demonstrate the effectiveness of the proposed user-treatment interaction module, we visualize the generated correlations between user features and treatments. The illustration can be found in the first section of appendix."}, {"title": "5.5 Online Deployment", "content": "We aim to apply uplift modeling to personalize the distribution of bonuses to enhance the flow experience for users on our online gaming platform. Different bonuses are offered to users to ensure a better gaming experience, thereby motivating user activity and willingness to spend money. The online deployment process is illustrated in Fig. 5. After training MTMT offline with treatment and user features, the model ranks users into buckets based on their estimated base and treatment effects. For each bucket, the gaming server determines whether to offer a user a bonus and, if so, which bonus to offer, based on certain constraints."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we explored uplift modeling within a multi-treatment, multi-task framework for an online gaming platform. To effectively extract features for different tasks and accurately estimate the effects of various treatments, we proposed the Multi-Treatment Multi-Task (MTMT) uplift modeling framework that explicitly utilizes user-treatment features to estimate the task-wise uplifts. Additionally, to precisely capture the uplift differences between various treatments, we proposed separately estimating the base treatment effect and the incremental treatment effect. The base treatment effect pertains to whether a treatment should be given, while the incremental treatment effect addresses the specific type of treatment. Extensive experiments and ablation studies validated the effectiveness of our methods. We also provided interpretable analyses to demonstrate how user features correlate with treatments. In future work, we plan to extend our models to accommodate non-binary treatments and non-binary tasks, thereby increasing their applicability to a broader range of scenarios."}]}