{"title": "pFedGPA: Diffusion-based Generative Parameter Aggregation for Personalized Federated Learning", "authors": ["Jiahao Lai", "Jiaqi Li", "Jian Xu", "Yanru Wu", "Boshi Tang", "Siqi Chen", "Yongfeng Huang", "Wenbo Ding", "Yang Li"], "abstract": "Federated Learning (FL) offers a decentralized approach to model training, where data remains local and only model parameters are shared between the clients and the central server. Traditional methods, such as Federated Averaging (FedAvg), linearly aggregate these parameters which are usually trained on heterogeneous data distributions, potentially overlooking the complex, high-dimensional nature of the parameter space. This can result in degraded performance of the aggregated model. While personalized FL approaches can mitigate the heterogeneous data issue to some extent, the limitation of linear aggregation remains unresolved. To alleviate this issue, we investigate the generative approach of diffusion model and propose a novel generative parameter aggregation framework for personalized FL, pFedGPA. In this framework, we deploy a diffusion model on the server to integrate the diverse parameter distributions and propose a parameter inversion method to efficiently generate a set of personalized parameters for each client. This inversion method transforms the uploaded parameters into a latent code, which is then aggregated through denoising sampling to produce the final personalized parameters. By encoding the dependence of a client's model parameters on the specific data distribution using the high-capacity diffusion model, pFedGPA can effectively decouple the complexity of the overall distribution of all clients' model parameters from the complexity of each individual client's parameter distribution. Our experimental results consistently demonstrate the superior performance of the proposed method across multiple datasets, surpassing baseline approaches.", "sections": [{"title": "Introduction", "content": "To meet with the increasing needs of privacy protection, federated learning (FL) has been a popular machine learning paradigm and research topic for many years (McMahan et al. 2017a). Typically, multiple devices such as smartphones, sensors, and IoTs (Internet of Things) collaboratively train a global model under the coordination of a central server. FL has been extended to various application domains, including healthcare and finance (Nguyen et al. 2022). In healthcare, FL trains models on distributed patient data, enabling cooperative research without compromising privacy (Yang et al. 2019). In finance, FL helps developing fraud detection models using data from multiple financial institutions while ensuring confidentiality (Li et al. 2019).\nOne challenge in FL is handling the heterogeneity of data distributions across various devices (Zhao et al. 2018; Kairouz et al. 2021). For example, in healthcare applications, patient data collected from different hospitals can vary greatly due to differences in patient demographics, medical equipment, and local practices. In linear aggregation methods such as FedAvg (McMahan et al. 2017a), data in different clients are assumed to share the same distribution. Non-IID data in local clients can lead to unstable results (Zhao et al. 2018; Karimireddy et al. 2020), as illustrated in Fig. 1. In this case, the parameters $\\theta_1$ and $\\theta_2$ from non-IID datasets are aggregated to form $\\theta_{\\text{Fedavg}} = (\\theta_1 + \\theta_2)/2$. While $\\theta_1$ and $\\theta_2$ are located in the high-probability regions of the parameter space and are well-optimized for their respective tasks, their average $\\theta_{\\text{Fedavg}}$ falls into a low-probability region, causing a collapse. This example highlights the limitations of linear aggregators and the need of more ingenious aggregation methods.\nTo handle Non-IID client data, various personalized FL methods have been proposed. They aim to learn a customized model for each individual client to fit its local data distribution. Although many efforts have been devoted to develop advanced model adaptation methods to obtain better"}, {"title": "Related Work", "content": "local models, the limitation of linear aggregation remains since most of the advanced personalized FL methods still highly rely on the linear aggregation (Collins et al. 2021; Sattler, M\u00fcller, and Samek 2020; Yang et al. 2023; Xu, Tong, and Huang 2023). For instance, a simple but tough-to-beat baseline is combining FedAvg with local fine-tuning, where the quality of the global model by FedAvg may directly affect the resulted local models.\nTo build better FL models, a fundamental challenge is to explicitly model the relationship between data distributions and model parameters within specific optimization constraints. However, when considering the parameter space where the model distribution resides, we recognize that the intrinsic difficulty arises from modeling this distribution as a low-dimensional manifold within a high-dimensional space (Zhou et al. 2022, 2023). Simple network architectures like multi-layer perceptrons (MLPs) or basic Transformers struggle to effectively learn such complex distributions and scale robustly. Moreover, they lack effective optimization objectives and training algorithms.\nInspired by the remarkable success of diffusion models in achieving state-of-the-art results in image generation (Dhariwal and Nichol 2021) and recent attempts in model parameter generation (Wang et al. 2024), we propose to leverage the diffusion models to capture the distribution of model parameters in the parameter space. Diffusion models, which convert the complex data distribution to an analytically tractable prior, estimate the score function at each time step and iteratively refine samples through stochastic processes (Song et al. 2020). Although initially applied to image data, this versatile approach shows promise for modeling high-dimensional parameter distributions (Zhang et al. 2024; Wang et al. 2024).\nBuilding on above idea, we introduce our framework, pFedGPA, which employs a diffusion model on the server to handle the parameters of clients, enabling the server to learn the distributions of all clients' model parameters using a powerful generative model. Subsequently, we investigate the mechanisms of generative parameter aggregation and personalized model updates within this framework. We consider the generation process as an alternative to achieve the better model aggregation. Our method effectively guides the optimization direction for newly joined clients and significantly accelerates the initialization. Furthermore, we develop a novel parameter inversion method inspired by image editing techniques applied within diffusion models, which transforms the original parameters into a latent representation and then refines them to generate new parameters that retain the implicit semantics of the original data while incorporating the learned patterns from the diverse parameter distributions. Furthermore, we explore the configuration of the diffusion model architecture tailored for processing model parameter data. Our experimental results demonstrate the effectiveness of the proposed method, consistently achieving superior performance across multiple datasets compared to baseline approaches. The main contributions of our work are concluded as follows:\n\u2022 We present a novel FL framework that employs diffusion models to generate personalized parameters with the"}, {"title": "FL with Data Heterogeneity", "content": "To address the non-IID data challenge, several strategies for improving the global models have been proposed. FedProx (Li et al. 2020) introduced a local regularization term to optimize each client's model.SCAFFOLD (Karimireddy et al., 2020b) introduces control variates to correct the drift in local updates. Clustering-based methods (Sattler, M\u00fcller, and Samek 2020; Li et al. 2020; Ghosh et al. 2020) cluster similar local models into groups and assign each group a global model. Moreover, self-supervised methods have been incorporated to define the similarity between model representations for correcting local training (Li, He, and Song 2021). Personalized FL (Smith et al. 2017) focuses on tailoring models for individual clients by combining information across clients. Decentralized MAML (Fallah, Mokhtari, and Ozdaglar 2020; Li et al. 2017) adapts the model-agnostic meta-learning framework to a federated setting, allowing clients to learn models through local adaptation (Finn, Abbeel, and Levine 2017; Nichol, Achiam, and Schulman 2018). Another category includes model mixing and layer adaptation strategies, where clients learn a mixture of global and local models (Deng, Kamani, and Mahdavi 2020; Hanzely and Richt\u00e1rik 2020; Zhang et al. 2023). Decoupling model layers is also a popular and effective approach (Arivazhagan et al. 2019; Collins et al. 2021; Xu, Yan, and Huang 2022). For instance, FedRep (Collins et al. 2021) and FedBABU (Oh, Kim, and Yun 2022) train base layers globally using FedAvg, with personalized layers finetuned locally. Personalized aggregation based on model similarity has also been investigated (Zhang et al. 2021; Huang et al. 2021; Ye et al. 2023)."}, {"title": "Diffusion Models", "content": "Diffusion models have emerged as a powerful technique of generative AI, particularly excelling in producing high-quality images. Their superior properties have led to widespread applications in various vision (Lugmayr et al. 2022) and multi-modal tasks (Rombach et al. 2022; Kawar et al. 2023; Kumari et al. 2023).\nThe foundational work by (Sohl-Dickstein et al. 2015), studied non-equilibrium thermodynamics, highlighting its potential in generative modeling. (Ho, Jain, and Abbeel 2020) advanced this with Denoising Diffusion Probabilistic Models (DDPMs), improving the denoising process for better image synthesis. (Song and Ermon 2019) proposed Score-Based Generative Modeling, achieving remarkable results in diverse applications (Meng et al. 2021; Xu et al."}, {"title": "Model Parameter Generation", "content": "Model parameter generation has progressed from basic gradient optimization to advanced meta-learning and Bayesian methods. Early efforts primarily focused on optimizing parameters using gradient-based techniques like Stochastic Gradient Descent (SGD) and its variants (Amari 1993). Meta-learning emerged to enable models to adapt quickly to new tasks with minimal data, exemplified by methods that use hypernetworks as metaknowledge to dynamically generate model parameters from input data (Zhmoginov, Sandler, and Vladymyrov 2022). Bayesian deep learnig used Variational Bayes to infer the distributions of model parameters, which are assumed to be Gaussian (Wilson and Izmailov 2020).\nRecently, diffusion models have emerged as a powerful paradigm for model parameter generation. G.pt (Peebles et al. 2022) trains conditional diffusion models using model checkpoints as datasets. MetaDiff (Zhang et al. 2024) introduces a diffusion-based meta-learning method for few-shot tasks. HyperDiffusion (Erko\u00e7 et al. 2023) uses diffusion models to generate new neural implicit fields. Additionally, p-diff (Wang et al. 2024) examines the quality of model parameters generated through diffusion. These works provide preliminary experiments and analyses on using diffusion models for model parameter generation."}, {"title": "Problem Definition and Preliminaries", "content": "FL aims to collectively train a centralized model for $n$ edge-distributed clients. Each client $i$ has access to $m_i$ data samples $D_i \\sim P_i$ on $X \\times Y$, and $N$ is the total number of data samples overall clients. Generally, the data distributions for each client are different, i.e., $P_i \\neq P_j$ for any pair $i, j \\in {1, ..., n}$.\nLet $l_i: Y \\times Y \\rightarrow \\mathbb{R}^+$ denote the loss function corresponding to client $i$, and $F_i: \\Theta \\times X \\rightarrow Y$ denote the local model parameterized by $\\theta^i$. The goal of conventional FL is to optimize the following objectives:\n$$\\theta^* = \\arg \\min_{\\theta} \\sum_{i=1}^n \\mathbb{E}_{(x,y)\\sim P_i}[l_i(F_i(\\theta, x), y)], \\qquad(1)$$\nwhere $\\theta^*$ represents the globally optimal parameters. The representative method for solving Eq. (1) is FedAvg ((McMahan et al. 2017b)). In each round, clients perform"}, {"title": "Federated Learning Setting", "content": "several epochs of SGD on their local loss functions and send the updated models to the server. The server then aggregates the models from all clients linearly by\n$$\\theta = \\sum_{i=1}^n \\frac{M_i}{N} \\theta_i, \\qquad(2)$$\nand broadcasts the averaged model $\\theta$ back to the clients. Moreover, due to data heterogeneity, the unified parameters may not be locally optimal for each client. Therefore, personalized FL adjusts the optimization objective to:\n$$\\Theta^* = \\arg \\min_{\\Theta:=\\{\\theta^i\\}_{i=1}^n} \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}_{(x,y)\\sim P_i}[l_i(F_i(\\theta^i; x), y)]. \\qquad(3)$$\nwhere $\\Theta^*$ denotes the collection of locally optimal parameters. The challenge lies in how to aggregate model parameters from heterogeneous clients on the server and produce parameters that incorporate insights from all clients while still adapting to specific data distributions. In this work, we propose to train a diffusion model at the server to address these challenges."}, {"title": "Diffusion Probabilistic Models", "content": "Here, we focus on the Denoising Diffusion Probabilistic Models formulation, primarily because of its prevalence and consistency with the Denoising Score Matching using Langevin Dynamics. DDPMs gradually add noise to data, transforming it into standard Gaussian noise, and then learn to denoise step-by-step, generating new data. It involves a forward process and a reverse process as described below.\nGiven training data $x_0$ from a target distribution $q(x_0)$, the forward process gradually adds Gaussian noise to diffuse $x_0$ into $x_1, x_2, ..., x_T$, where $x_T$ is approximately sampled from the standard Gaussian distribution $q(x_T) \\approx \\mathcal{N}(x_T; 0, I)$. This process can be formulated as follows:\n$$q(x_{1:T}|x_0) = \\prod_{t=1}^T q(x_t|x_{t-1}) \\qquad(4)$$\n$$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I) \\qquad(5)$$\nwhere $\\beta_1, ..., \\beta_T \\in (0,1)$ is a variance schedule. The diffusion model aims to approximate the noise added at each time step $t$ using a neural estimator $\\epsilon_{\\phi}(x_t, t)$. The training objective $L_{ddpm}$ is given by the Eq. (6) below:\n$$L_{ddpm} = \\mathbb{E}_{t\\sim[1, T], x_0\\sim q_0, \\epsilon_t \\sim \\mathcal{N}(0,1)} [||\\epsilon_t - \\epsilon_{\\phi}(x_t, t)||^2], \\qquad(6)$$\nwhere $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_t$ and $\\bar{\\alpha}_t = \\prod_{j=1}^t(1 - \\beta_j)$.\nDuring the reverse process, we iteratively reconstruct the target data $x_0$ from the random noise $x_T \\sim \\mathcal{N}(0, I)$. At each time step $t$, we sample $x_{t-1}$ from the estimated reverse Markov chain $p_{\\theta}(x_t|x_{t-1})$ parameterized by the noise estimator $\\epsilon_{\\phi}(x_t, t)$. That is,\n$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\phi}(x_t, t)) + \\sigma_t z_t. \\qquad(7)$$\nwhere $z_t \\sim \\mathcal{N}(0,I)$ and $\\sigma_t^2 = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t} \\beta_t$ represents the derived variance."}, {"title": "Methodology", "content": "In this section, we present our framework pFedGPA in detail. Given a collection of clients with their own specific data distributions in a FL system, our goal is to integrate the diverse parameter distributions captured by the diffusion model on the server, which reflects the underlying data distributions of the clients. Meanwhile, the diffusion model aims to generate new, personalized model parameters for each client by leveraging these learned parameter distributions. In the following subsections, we first illustrate the parameter integration process using the diffusion model. Next, we explain the personalized parameter generation process. Finally, we delve into the architecture design of the diffusion model. Further details about the training procedure are presented in Algorithm 1."}, {"title": "Generative Parameter Aggregation", "content": "In our framework, we deploy a diffusion model on the server, where both the input and output are the weights of the clients' models. The core of this approach is to enable the server to learn the distribution of all clients' model parameters using a powerful generative model, a distribution which resides on a low-dimensional manifold in the parameter space.\nLet $H: \\Phi \\times \\Theta \\rightarrow $ denotes the diffusion model parameterized by $\\phi$ and we modify the optimization objective in Eq. (3) based on the parameter generation to derive:\n$$\\phi^*, \\Theta^* = \\arg \\min_{\\Phi, \\Theta:=\\{\\theta^i\\}_{i=1}^n} \\sum_{i=1}^n L_i(\\phi, \\theta^i), \\qquad(9)$$\nwhere $L_i(\\phi, \\theta^i) = \\mathbb{E}_{(x,y)\\sim P_i} [l_i(F_i(H(\\phi; \\theta^i); x), y)]$ and ${\\theta^i\\}_{i=1}^n$ are the stationary points of the system H under well-fitted state $\\phi^*$ that represent the optimal model parameters of each client.\nMoreover, in (Shamsian et al. 2021), the hypernetwork takes a trainable vector descriptor as the input to differentiate each client. However, This approach does not scale efficiently. When new clients join the network, it often requires training suitable embedding vectors for them from scratch. As the network scale continues to grow, it also becomes necessary to fine-tune all clients' embedding vectors to achieve stronger representational capabilities while keeping consistency across clients. In diffusion models, if the input also includes client descriptors $e_{\\text{client}}$, it can be implemented as a form of classifier-free guidance, utilizing a linear combination of conditional and unconditional score estimates to provide client-specific guidance:\n$$\\tilde{\\epsilon}_{\\phi}(\\theta, e_{\\text{client}}) = (1 + w)\\epsilon_{\\phi}(\\theta, e_{\\text{client}}) - w\\epsilon_{\\phi}(\\theta), \\qquad(10)$$\nwhere $w$ controls the strength of the guidance. The advantage is no need to train a separate classifier. However, here we adopt a classifier guidance approach because, in the FL setting, each client inherently serves as a classifier model relative to the diffusion model. In turn, the diffusion model provides global guidance to local clients. That is:\n$$\\tilde{\\epsilon}_{\\phi}(\\theta) = \\frac{\\epsilon_{\\phi}(\\theta)}{(1 + w)\\nabla_{\\theta} \\log P_y(x|\\theta)}, \\qquad(11)$$\nwhere $\\log P_y(x|\\theta)$ represents the training loss of the client model in probabilistic form (e.g., cross-entropy loss). Another benefit is that utilizing a unified model to estimate the overall score of model distributions, rather than conditionally fitting each client's distribution, promotes information sharing across clients while maintaining the adaptability of personalized models. For a newly joined client, this method can accomplish initialization within a few iterations. During each iteration, the client sends fine-tuned model parameters to the server, where the diffusion model alternately provides global guidance to update them. This process is depicted in Fig. 2."}, {"title": "Parameter Inversion", "content": "For existing clients, we aim to generate parameters that align with their local data distributions while incorporating patterns learned from the diffusion model, ensuring the new parameters effectively fit the original data. Inspired by the inversion technique (Wu and De la Torre 2023) in image editing, we propose a parameter inversion method. We view the above difficulty as a zero-shot inversion problem within diffusion models. Given a parameter data, we can transform it into a latent code and then aggregate it cohesively with denoising sampling."}, {"title": "Diffusion Model Designs", "content": "We use the latent diffusion model (Rombach et al. 2022) as the generative model and adopt the main architecture from p-diff (Wang et al. 2024). The latent diffusion model contains an autoencoder and a diffusion model. First, the autoencoder is trained through the reconstruction loss, producing a low-dimensional latent space. Then, the diffusion model operates within this latent space, learning to progressively denoise samples from noise to approximate the distribution of the latent representations. This approach allows the diffusion model to generate high-quality samples with reduced computational costs and memory usage compared to operating directly in the original high-dimensional space.\nWe flatten the model parameters into one-dimensional vectors, and both the U-Net blocks within the diffusion model and the encoder and decoder in the autoencoder use 1-D convolution layers instead of the traditional 2-D convolution layers. In addition, we introduced noise into the input and latent representations to improve the robustness and generalization of the generated results. Experiments in p-diff showed that noise augmentation plays a key role in producing models that are stable and achieve high performance. Unlike generating model parameters trained on IID data in p-diff, our diffusion model is designed to fit model parameters coming from different data distributions in FL. For smaller models, we consider generating the entire model,"}, {"title": "Analysis", "content": "As demonstrated in (Song et al. 2020), DDPMs can be viewed as discretizations to SDEs, coherently bridging diffusion probabilistic modeling and noise conditional score networks into a unified framework. Thus, we can reformulate $L_{ddpm}$ equivalently as denoising score matching:\n$$\\mathbb{E}_{q(x_0)} \\mathbb{E}_{q(x_t|x_0)} [||s_{\\phi}(x_t, t) - \\nabla_{x_t} \\log q(x_t|x_0)||^2], \\qquad(8)$$\nwhere $s_{\\phi}(x_t, t) = \\frac{\\epsilon_{\\phi}(x_t, t)}{\\sqrt{(1-\\bar{\\alpha}_t)}}$. This allows us to use DDPMS to effectively estimate the scores of data distributions."}, {"title": "Parameter Inversion", "content": "Detailed explanation is as follow. First, in the forward process of diffusion algorithm, Gaussian noises are added at each time step, diffusing $\\theta_0$ to $\\theta_1,..., \\theta_T$. Next, We concatenate all the added noises with the final $\\theta_T$ to define a latent code of $\\theta_0$. Consequently, the latent space has a much higher dimensionality than the original data. The formulations are as follows:\n$$\\theta_1,..., \\theta_T \\sim p(\\theta_{1:T}|\\theta_0), \\qquad(12)$$\n$$\\epsilon_t = (\\theta_t - \\sqrt{1-\\beta_t} \\theta_{t-1})/\\sqrt{\\beta_t}, t = T, ..., 1, \\qquad(13)$$\n$$\\gamma := (\\theta_T, \\epsilon_T, \\epsilon_{T-1}, ..., \\epsilon_1), \\qquad(14)$$\nwhere $\\gamma$ represents the implicit semantics of $\\theta_0$. We can perfectly reconstruct $\\theta_0$ from the latent code $\\gamma$:\n$$\\theta_T \\twoheadrightarrow \\theta_{T-1} ... \\twoheadrightarrow \\theta_0, \\qquad(15)$$\nhere $\\theta_t \\twoheadrightarrow \\theta_{t-1}$ means $\\theta_{t-1} = (\\theta_t - \\sqrt{\\beta_t} \\epsilon_t)/\\sqrt{1 - \\beta_t}$. Furthermore, we aim to encode the latent code $\\gamma$ into the generation process of the new parameter $\\theta_0$. To achieve this, during the denoising process of the diffusion model, we start sampling from $\\theta_T$, and at each time step, we substitute the randomly sampled Gaussian noise $z_t$ in Eq. (7) with the deterministic $\\epsilon_t$ to inject the implicit semantics of $\\theta_0$:\n$$\\theta_{t-1} = \\mu_{\\phi}(\\theta_t, t) + \\frac{\\sigma_t \\epsilon_t}, \\qquad(16)$$\nwhere $\\mu_{\\phi}(\\theta_t, t)$ is the mean estimator in Eq. (7):\n$$\\mu_{\\phi}(\\theta_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} (\\theta_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_{\\phi}(\\theta_t, t)). \\qquad(17)$$\nThis process is depicted in Fig. 3, which illustrates how the generated parameters retain the implicit semantics of the original parameters while effectively incorporating global information. This approach ultimately enhances the generalization performance of the model."}, {"title": "Diffusion Model Designs", "content": "We use the latent diffusion model (Rombach et al. 2022) as the generative model and adopt the main architecture from p-diff (Wang et al. 2024). The latent diffusion model contains an autoencoder and a diffusion model. First, the autoencoder is trained through the reconstruction loss, producing a low-dimensional latent space. Then, the diffusion model operates within this latent space, learning to progressively denoise samples from noise to approximate the distribution of the latent representations. This approach allows the diffusion model to generate high-quality samples with reduced computational costs and memory usage compared to operating directly in the original high-dimensional space.\nWe flatten the model parameters into one-dimensional vectors, and both the U-Net blocks within the diffusion model and the encoder and decoder in the autoencoder use 1-D convolution layers instead of the traditional 2-D convolution layers. In addition, we introduced noise into the input and latent representations to improve the robustness and generalization of the generated results. Experiments in p-diff showed that noise augmentation plays a key role in producing models that are stable and achieve high performance. Unlike generating model parameters trained on IID data in p-diff, our diffusion model is designed to fit model parameters coming from different data distributions in FL. For smaller models, we consider generating the entire model,"}, {"title": "Discussion", "content": "In our experiments, training a round of the diffusion model takes about an hour on a single Nvidia 4090 24GB GPU, with the entire FL process completing in four hours. However, the diffusion model's training time significantly exceeds the time between communication rounds, causing a bottleneck in the overall process. In real-world applications, it is expected that the server would have more hardware resources, thereby enabling the training of larger diffusion models, which can in turn generate larger local models. Additionally, employing larger batch sizes could further accelerate the training process. In contrast, the inference time remains relatively short, enabling the trained diffusion model to quickly generate parameters for clients.\nConcerns The trained parameters of the diffusion model, which are much larger than those of the clients, are never transmitted. Consequently, we do not introduce any additional communication costs compared to other methods. Moreover, in our framework, only the local model parameters are exchanged between clients and the server, without transmitting additional local feature statistics, thereby maintaining the same privacy levels as other methods that exchange only parameters.\nBeyond the advantages demonstrated in our experiments, generative aggregation methods have the potential to be integrated with other FL approaches, such as prototype-enhanced algorithms (Tan et al. 2022; Xu, Tong, and Huang 2023). These methods can complement each other effectively: generative models learn the distribution of model parameters, while prototype learning focuses on the distribution of data representations. This integration could enable the development of novel inversion strategies that may lead to the generation of more accurate personalized parameters in the future."}, {"title": "Practical Considerations", "content": "Communication and Privacy"}, {"title": "Future Work", "content": ""}, {"title": "Conclusion", "content": "In this paper, we introduce a novel diffusion-based parameter aggregation method for personalized FL. In our framework, the server deploys a diffusion model to consolidate the uploaded parameters and generates personalized parameters for each client with the global guidance by using a newly developed inversion mechanism. Experimental results on three datasets verify the effectiveness and further advantages of our method."}]}