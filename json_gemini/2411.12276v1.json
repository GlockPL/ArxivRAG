{"title": "LIBCLL: AN EXTENDABLE PYTHON TOOLKIT FOR\nCOMPLEMENTARY-LABEL LEARNING", "authors": ["Nai-Xuan Ye", "Tan-Ha Mai", "Hsiu-Hsuan Wang", "Wei-I Lin", "Hsuan-Tien Lin"], "abstract": "Complementary-label learning (CLL) is a weakly supervised learning paradigm for multiclass classi-\nfication, where only complementary labels-indicating classes an instance does not belong to-are\nprovided to the learning algorithm. Despite CLL's increasing popularity, previous studies highlight\ntwo main challenges: (1) inconsistent results arising from varied assumptions on complementary\nlabel generation, and (2) high barriers to entry due to the lack of a standardized evaluation platform\nacross datasets and algorithms. To address these challenges, we introduce libcll, an extensible\nPython toolkit for CLL research. libcll provides a universal interface that supports a wide range of\ngeneration assumptions, both synthetic and real-world datasets, and key CLL algorithms. The toolkit\nis designed to mitigate inconsistencies and streamline the research process, with easy installation,\ncomprehensive usage guides, and quickstart tutorials that facilitate efficient adoption and implementa-\ntion of CLL techniques. Extensive ablation studies conducted with libcll demonstrate its utility in\ngenerating valuable insights to advance future CLL research.", "sections": [{"title": "Introduction", "content": "In many real-world applications, training effective classifiers typically depends on obtaining high-quality, accurate\nlabels. However, acquiring such labels is often difficult and costly. To address this challenge, many researchers\nhave turned their attention to weakly supervised learning (WSL), a methodology aimed at training reliable classifiers\nusing only incomplete, imprecise, or inaccurate data [1, 2]. Numerous WSL studies have been conducted to extend\nour understanding of machine learning capabilities, covering topics such as complementary labels [3, 4], multiple\ncomplementary labels [5, 6], noisy labels [7], and learning from partial labels [8].\nThis work focuses on complementary-label learning (CLL), a WSL problem where each label indicates only a class\nto which a data instance does not belong [3]. CLL aims to train models with these complementary labels while still\nenabling accurate predictions of the ordinary labels during testing. CLL makes machine learning more practical in\nscenarios where obtaining ordinary labels is difficult or costly [3]. Additionally, CLL broadens our understanding of\nmachine learning's practical potential under limited supervision.\nCurrent research on CLL has introduced numerous learning algorithms [4, 9, 10, 11] that have been evaluated using\na diverse range of datasets, from synthetic datasets based on varied complementary-label generation assumptions to\nreal-world datasets [12]. However, the performance of these algorithms often varies significantly across studies due to\ndifferences in underlying label-generation assumptions, the absence of a standardized evaluation platform, and the use\nof diverse network architectures [4, 9, 3, 11]. Establishing a fair, reproducible, and stable evaluation environment is\ntherefore essential for advancing CLL research. For instance, variations in network architectures, such as the use of\nResNet18 [13, 12] versus ResNet34 [9, 4], contribute to inconsistencies in performance and hinder fair comparisons\nacross studies. Furthermore, most CLL research has not publicly released implementations [6, 11, 4, 14], particularly\nregarding details like loss calculation and data pre-processing. This lack of accessibility presents a challenge for\nresearchers seeking to validate and build upon existing work in CLL.\nTo enable meaningful comparisons among CLL algorithms and create a user-friendly environment for implementation\nand innovation, we introduce libcll, a complementary-label learning toolkit built with PyTorch-Lightning. This toolkit"}, {"title": "Preliminaries and related works", "content": ""}, {"title": "Complementary-Label Learning", "content": "In ordinary multi-class classification, a dataset $D = \\{(x_i, y_i)\\}_{i=1}^N$ is provided to the learning algorithm, where $N$\ndenotes the number of samples, and the dataset is i.i.d. sampled from an unknown distribution. For each $i$, $x_i \\in \\mathbb{R}^d$\nrepresents the $d$-dimension feature of the $i^{th}$ sample and $y_i \\in \\mathbb{R}^K$, where $K$ denotes the total number of classes in\nthe dataset, with $K > 2$. The set $[K] = \\{1, 2, ..., K\\}$ represents the possible classes to which $x_i$ can belongs. The\nobjective of the learning algorithm is to learn a classifier $f(x) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^K$ that minimizes the classification risk:\n$E_{(x,y)\\sim D}[l(f(x), e_y)]$, where $l$ is the loss function and $e_y$ is one-hot vector corresponding to label $y$. The predicted"}, {"title": "Assumptions of complementary labels", "content": "Complementary-label learning aims to develop a classifier under the guidance of weak supervision from complementary\nlabels. For synthetic data generation, prior studies assume the distribution of complementary labels only depends on\nthe ordinary labels instead of features; thus, $P(\\tilde{y} | x,y) = P(\\tilde{y} | y)$. The transition probability $P(\\tilde{y} | y)$ is often\nrepresented by a $K \\times K$ transition matrix $T$, with $T_{ij} = P(\\tilde{y} = j | y = i)$.\nThe transition matrix can be further classified into three categories:\n\u2022 Uniform [3]: All complementary labels are uniformly and randomly selected from $K \u2013 1$ classes. Based on\nthis assumption, the transition matrix is $T = \\frac{1}{K-1}(\\mathbb{1}_K \u2013 I_K)$.\n\u2022 Biased [9]: Any transition matrix that is not uniform is biased.\n\u2022 Noisy [11]: A portion of the true labels are mislabeled as complementary labels. Thus, the diagonals of the\ntransition matrix are not necessarily zero.\n\u2022 MCL [6]: Each instance has more than one complementary label.\nAdditionally, due to the difficulty of obtaining a transition matrix in real-world scenarios, CONU [15] proposed SCAR\n(Selected Completely At Random), where the generation of complementary labels is independent of both instances'\nfeatures and ground-truth labels; that is, $P(\\tilde{y} | x, y) = P(\\tilde{y}) = c_k$, where $c_k$ is a constant related only to the k-th class.\nFurthermore, some studies, such as MCL [6], assume that each instance can have multiple complementary labels. This\napproach involves randomly generating a label set $\\hat{y}$ where $1 < |\\hat{y}| < K \u2212 1$ and then asking annotators whether the\ngiven label set $\\hat{y}$ contains the true label."}, {"title": "Previous methods on CLL", "content": "In this section, we present a timeline of key developments in CLL, as illustrated in Figure 2. We implement three\nprimary categories of CLL methods in libc11: URE (unbiased risk estimator), CPE (complementary probability\nestimation), and MCL (multiple complementary label) methods. Additionally, we include several bridging works that\nconnect CLL with other learning frameworks."}, {"title": "Library design", "content": "The code structure of libcll is highly modular and seamlessly integrates with PyTorch. Each component can be added,\nmodified, or removed individually to support diverse experimental designs. In the following paragraphs, we will outline\nthe definitions of strategies, datasets, models, evaluation, and reproducibility.\nStrategies in CLL algorithms are used to calculate the loss. All strategies inherit from the base class\nlibcll.strategies.Strategy, which itself extends pytorch_lightning.LightningModule. This means ev-\nery strategy defined in libcll can be integrated into any other PyTorch Lightning framework. Additionally,\nlibcll.strategies.Strategy already includes implementations for the validation and testing steps, as well as\nevaluation metrics. Users only need to create a new class that inherits from it and modify the training_step to\nincorporate new complementary-label learning methods into the library."}, {"title": "Benchmark experiments", "content": ""}, {"title": "Experimental Setups", "content": "In the subsequent experiments in Sections 4.2 through 4.5, we evaluate the performance of all CLL algorithms\navailable in libcll. Each section utilizes distinct transition matrices and records test accuracy for each dataset and\nalgorithm, enabling a comprehensive assessment of performance. Additionally, we calculate the average rank of test\naccuracy across all experiments, which is displayed in the 'Avg Rank' column of each table as the primary metric\nfor performance comparison. We use a one-layer MLP model (d-500-c) for the MNIST, KMNIST, FMNIST, Yeast,\nTexture, Control, and Dermatology datasets, and ResNet34 for the CIFAR10, CIFAR20, MicroImageNet10 (MIN10),\nMicroImageNet20 (MIN20), CLCIFAR10, CLCIFAR20, CLMicroImageNet10 (CLMIN10), and CLMicroImageNet20\n(CLMIN20) datasets. Following the setup in [12], we apply standard data augmentation techniques, including\nRandomHorizontalFlip, RandomCrop, and normalization, to each image in the CIFAR and MicroImageNet series\ndatasets."}, {"title": "Uniform distribution", "content": "In this section, we establish baselines for each algorithm under the standard CLL setting, where the correct transition\nmatrix is provided to T-agnostic methods, and complementary labels (CLs) are uniformly sampled from the comple-\nmentary set. An exception is made for CLCIFAR10, CLCIFAR20, CLMIN10, and CLMIN20, whose CLs are derived\nfrom human annotations and are thus noisy. To evaluate the adaptability of current algorithms to real-world scenarios,\nwe divide the datasets into synthetic and real-world sets.\nThe results are shown in Table 2 and 3. As observed, FWD and CPE-F demonstrate the best overall performance on\nboth synthetic and real-world datasets, with CPE-F slightly outperforming CPE-T, consistent with findings from [11].\nSurprisingly, providing the transition matrix to the learner in URE-TNN and URE-TGA does not consistently yield\nbetter performance compared to URE-NN and URE-GA, particularly on non-uniform datasets such as CLCIFAR10,\nCLCIFAR20, CLMIN10, and CLMIN20. We suggest that the transition matrices of these datasets may be ill-conditioned,\nleading to instability in URE."}, {"title": "Biased distribution", "content": "To examine the impact of disturbances in the complementary-label distribution, we follow the procedure from [10] to\ngenerate two biased distributions with varying levels of deviation from a uniform distribution, as follows: For each\nclass y, the complementary classes are randomly divided into three subsets, with probabilities assigned as p1, p2, and\np3 within each subset. We consider two cases for (P1, P2, P3): (a) Strong: (0.75, 0.24, 0.01) to simulate a stronger\ndeviation. (b) Weak: (0.45, 0.30, 0.25) to simulate a milder deviation. Since these configurations are applicable only"}, {"title": "Noisy distribution", "content": "Following the steps outlined in [11], we simulate more restrictive environments by introducing both noisy comple-\nmentary labels and incorrect transition matrices to the learners. We achieve this by generating noisy datasets through\nthe interpolation of a strong deviation matrix, $T_{strong}$, and a uniform noise transition matrix, $\\frac{1}{K} \\mathbb{1}_K$. The resulting\ncomplementary labels follow the distribution $(1 \u2013 \\lambda)T_{strong} + \\lambda \\frac{1}{K} \\mathbb{1}_K$, while only $T_{strong}$ is provided to the learners,\nwhere $\\lambda$ controls the weight of the noise.\nThe results, presented in Table 5, indicate a performance drop across all methods, particularly as the noise factor $\\lambda$\nincreases. This demonstrates that, while T-aware methods can manage some degree of deviation and noise in the\ntransition matrix, they become increasingly vulnerable as the gap widens between the provided transition matrix and\nthe actual distribution with higher noise levels. This highlights why CPE-T outperforms other T-aware methods in noisy\nsettings, as it includes a trainable transition layer that mitigates this gap."}, {"title": "Multi-label distribution", "content": "To demonstrate the versatility of CLL, we assign three complementary labels to each instance, sampled from a uniform\ndistribution without repetition. For real-world datasets, we use three human-annotated complementary labels per sample.\nAfter generating multiple complementary labels, one-hot encoding is applied for MCL series loss calculations. For"}, {"title": "Conclusion", "content": "In this study, we introduce libcll, an open-source PyTorch library designed to advance research in complementary-\nlabel learning (CLL). The primary goal of libcll is to provide a standardized platform for evaluating CLL algorithms,\naddressing challenges in standardization, accessibility, and reproducibility. This library enables users to easily customize\nvarious components of the end-to-end CLL process, including data pre-processing utilities, implementations of CLL\nalgorithms, and comprehensive metric evaluations that reflect realistic conditions. To demonstrate libcll's flexible\nand modular design, we conduct diverse experiments encompassing multiple CLL algorithms, datasets ranging from\nsynthetic to real-world, and various distribution assumptions.\nOur experiments reveal that CPE and FWD are the most effective approaches for handling uniform, biased, and noisy\ncomplementary-label distributions. In cases where there is a substantial discrepancy between the known transition\nmatrix and the actual distribution, such as when using an estimated transition matrix in real-world scenarios, we strongly\nrecommend CPE. For multi-complementary-label learning in synthetic scenarios, SCL-NL algorithms are recommended.\nHowever, we note a limitation in handling deviations from a uniform distribution. For MCL from real-world datasets,\nour findings consistently show that CPE and FWD algorithms outperform other existing algorithms."}, {"title": "Limitations and Future Work", "content": "There are several ways to further enhance the comprehensiveness of the library. Currently, our strategy does not include\nother T-aware algorithms that may be competitive with CPE and FWD. Additionally, there is growing interest in\nleveraging complementary labels from similar instances to improve performance, a framework and functions for which\nare not yet supported in our library. In future work, we plan to integrate more recently published CLL algorithms and\ndevelop frameworks to accommodate more flexible labeling types in datasets."}, {"title": "Broader impacts", "content": "The library has the potential to advance algorithms for learning from complementary labels, enabling classifiers to be\ntrained with limited information. However, this capability may increase the risk of compromising user privacy. We\nrecommend that practitioners remain mindful of privacy concerns when using collected datasets and CLL algorithms."}]}