{"title": "Neural varifolds: an aggregate representation for quantifying the geometry of point clouds", "authors": ["Juheon Lee", "Xiaohao Cai", "Carola-Bibian Sch\u00f6nlieb", "Simon Masnou"], "abstract": "Point clouds are popular 3D representations for real-life objects (such as in LiDAR and Kinect) due to their detailed and compact representation of surface-based geometry. Recent approaches characterise the geometry of point clouds by bringing deep learning based techniques together with geometric fidelity metrics such as optimal transportation costs (e.g., Chamfer and Wasserstein metrics). In this paper, we propose a new surface geometry characterisation within this realm, namely a neural varifold representation of point clouds. Here the surface is represented as a measure/distribution over both point positions and tangent spaces of point clouds. The varifold representation quantifies not only the surface geometry of point clouds through the manifold-based discrimination, but also subtle geometric consistencies on the surface due to the combined product space. This study proposes neural varifold algorithms to compute the varifold norm between two point clouds using neural networks on point clouds and their neural tangent kernel representations. The proposed neural varifold is evaluated on three different sought-after tasks \u2013 shape matching, few-shot shape classification and shape reconstruction. Detailed evaluation and comparison to the state-of-the-art methods demonstrate that the proposed versatile neural varifold is superior in shape matching and few-shot shape classification, and is competitive for shape reconstruction.", "sections": [{"title": "1 Introduction", "content": "Point clouds are preferred in more and more applications including computer graphics, autonomous driving, robotics and augmented reality. However, manipulating/editing point clouds data in its raw form is rather cumbersome. Neural networks have made breakthroughs in a wide variety of fields ranging from natural language processing to computer vision. Point cloud data in general lack underlying grid structures. As a result, convolution operations on point cloud data require special techniques including voxelisation [1, 2, 3], graph representations [4, 5, 6] or point-wise convolutions [7, 8, 9]. Geometric deep learning and its variants have addressed technical problems of translating neural networks on point cloud data [5]. With advanced graph theory and harmonic analysis, convolutions on point cloud data can be defined in the context of spectral [4, 10] or spatial [11, 6] domains. Although geometric deep learning on point clouds has successfully achieved top performance in shape classification and segmentation tasks, capturing subtle changes in 3D surface remains challenging due to the unstructured and non-smooth nature of point clouds. A possible direction to learn subtle changes on 3D surface adopts some concepts developed in the field of theoretical geometric analysis. In other words, deep learning architectures might be improved by incorporating theoretical knowledge from geometric analysis. In this work, we introduce concepts borrowed from geometric measure theory, where representing shapes as measures or distributions has been instrumental.\nGeometric measure theory has been actively investigated by mathematicians; however, its technicality may have hindered its popularity and its use in many applications. Geometric measure-theoretic concepts have recently been introduced to measure shape correspondence in non-rigid shape matching [12, 13, 14] and curvature estimation [15, 16]. We introduce the theory of varifolds to improve learning representation of 3D point clouds. An oriented d-varifold is a measure over point positions and oriented tangent k-planes, i.e., a measure on the Cartesian product space of Rn and the oriented Grassmannian manifold G(d, n). Varifolds can be viewed as generalisations of d-dimensional smooth shapes in Euclidean space Rn. The varifold structure not only helps to better differentiate the macro-geometry of the surface through the manifold-based discrimination, but also the subtle singularities in the surface due to the combined product space. Varifolds provide representations of general surfaces without parameterization. They not only can represent consistently point clouds that approximate surfaces in 3D, but are also scalable to arbitrary surface discretisation (e.g., meshes). In this study, we use varifolds to analyse and quantify the geometry of point clouds.\nOur contributions:\n\u2022 Introduce the notion of neural varifold as a learning representation of point clouds. Varifold representation of 3D point clouds coupling space position and tangent planes can provide both theoretical and practical analyses of the surface geometry.\n\u2022 Propose two algorithms to compute the varifold norm between two point clouds using neural networks on point clouds and their neural tangent kernel representations. The reproducing kernel Hilbert space of the varifold is computed by the product of two neural tangent kernels of positional and Grassmannian features of point clouds. The neural varifold can take advantage of the expressive power of neural networks as well as the varifold representation of point clouds.\n\u2022 Apply the usage of neural varifold in evaluating shape similarity between point clouds on various tasks including shape matching, few-shot shape classification and shape reconstruction."}, {"title": "2 Related works", "content": "Geometric deep learning on point clouds. PointNet is the first pioneering work on point clouds. It consists of a set of fully connected layers followed by symmetric functions to aggregate feature representations. In other words, PointNet is neural networks on a graph without edge connections. In order to incorporate local neighbourhood information with PointNet, PointNet++ [8] applied PointNet to individual patches of the local neighbourhood, and then stacked them together. PointCNN [17] further refined the PointNet framework with hierarchical X-Conv which calculates inner products of X-transformation and convolution filters of point clouds. Dynamic graph CNN (DGCNN) [6] adopted the graph neural network framework to incorporate local neighbourhood information by applying convolutions over the graph edges and dynamically updating graph for each layer. Furthermore, the tangent convolution architecture [18] incorporated 3D surface geometry by projecting point clouds on local tangent plane, and then applying convolution filters.\nVarifolds. Geometric measure theory provides various tools for understanding, characterising and analysing surface geometry in various contexts, e.g., currents [12], varifolds [13, 15, 16] or normal cycles [19]. Despite their potential use for many applications, few studies have explored real-world applications of varifolds in the context of non-rigid surface registration [13]."}, {"title": "3 Varifold representations for point clouds", "content": "The notion of varifold arises in geometric measure theory in the context of finding a minimal surface spanning a given closed curve in R\u00b3, which is known as Plateau's problem [20]. Intuitively, the concept of a varifold extends the idea of a differentiable manifold by replacing the requirement for differentiability with the condition of rectifiability [21]. This modification enables the representation of more complex surfaces, including those with singularities. For instance, Figure 1 in [21] presents straightforward examples of varifolds. Let \u03a9\u2282C Rn be an open set. A general oriented d-varifold V on 2 is a non-negative Radon measure on the product space of \u03a9 with the oriented Grassmannian G(d, n). In this study, we focus on a specific class of varifolds, the rectifiable varifolds, which are concentrated on d-rectifiable sets and can represent non-smooth surfaces such as 3D cubes.\nDefinition 3.1 (Rectifiable oriented d-varifolds). Let \u03a9C Rn be an open set, X be an oriented d-rectifiable set, and @ be a non-negative measurable function with 0 > 0 Hd-almost everywhere in X. The rectifiable oriented d-varifold V = v(0, X) in \u03a9 is the Radon measure on \u03a9 \u00d7 G(d,n) defined by V = 0\u0389\u03c7\u03bf\u03c2 & \u03b4\u03c4\u2082x, i.e.,\n$\\int_{\\Omega \\times G(d,n)} \\varphi(x,T)d\\mu(x,T) = \\int_{X} \\varphi(x,T_xX)\\theta(x)dH^d(x), \\forall \\varphi \\in C_0(\\Omega\\times \\tilde{G}(d, n)),$\nwhere $C_0$ denotes the class of continuous functions vanishing at infinity.\nThe mass of a d-rectifiable varifold V = v(0, X) is the measure ||V|| = 0H%. The non-negative function @ is usually called multiplicity. We assume in the rest of the paper that 0 = 1 for simplicity.\nVarious metrics and topologies can be defined on the space of varifolds. The mass distance defined as follows is a possible choice for a metric:\n$d_{mass}(\\mu, \\nu) = \\sup \\bigg{\\big| \\int_{\\Omega \\times \\tilde{G}(d,n)} \\varphi d\\mu - \\int_{\\Omega \\times \\tilde{G}(d,n)} \\varphi d\\nu \\big|, \\varphi \\in C_0(\\Omega \\times \\tilde{G}(d, n)), ||\\varphi||_{\\infty} \\le 1\\bigg\\}.$  (1)\nHowever, the mass distance is not well suited for point clouds. For example, given two varifolds associated with Dirac masses \u03b4\u03b5 and \u03b4\u03bf, their distance remains bounded away from 0 as it is always possible to find a test function & such that |\u03c6(0) \u2013 \u03c6(\u03b5)| = 2, regardless of how close the two points are. The 1-Wasserstein distance is not a more suitable choice in our context since it cannot compare two varifold measures with different mass. For example, given two Dirac masses (1 + \u03b5)\u03b4\u03bf and \u03b4\u03bf, the 1-Wasserstein distance between them goes to infinity as \u03b5\u03c6(0)| \u2192 \u221e.\nDefinition 3.2 (Bounded Lipschitz distance). Being \u03bc and v two varifolds on a locally compact metric space (X, d), we define\n$d_{BL} (\\mu, \\nu) = \\sup \\bigg{\\big| \\int_{X \\times \\tilde{G}(d,n)} \\varphi d\\mu - \\int_{X \\times \\tilde{G}(d,n)} \\varphi d\\nu \\big|, $\n$\\varphi \\in C_L^1(\\Omega \\times \\tilde{G}(d, n)), ||\\varphi||_{Lip} \\le 1, ||\\varphi||_{\\infty} \\le 1\\bigg\\}.$ (2)\nThe bounded Lipschitz distance (flat distance) can handle both problems, we refer for more details to [22] and the references therein. Although the bounded Lipschitz distance dBL can provide theoretical properties for comparing varifolds, in practice, there is no straightforward way to numerically evaluate it. Instead, the kernel approach has been used to evaluate and compare varifolds numerically [13, 14].\nProposition 3.3. [14]. Let kpos and k\u0171 be continuous positive definite kernels on Rn and \u011e(d,n), respectively. Assume in addition that for any x \u2208 Rn, kpos(x,\u00b7) \u2208 Co(Rn). Then kpos & kg is a positive definite kernel on R\u2033 \u00d7 \u011e(d,n), and the reproducing kernel Hilbert space (RKHS) W associated with kpos \u25ca kg is continuously embedded in Co(R\u2033 \u00d7 G(d,n)), i.e., there exists cw > 0 such that for any \u00a2 \u2208 W, we have ||||\u221e < cw||||w.\nLet Tw: W\u2192 Co(Rn \u00d7 \u011e(d, n)) be the continuous embedding given by Proposition 3.3 and tw* be its adjoint. Then varifolds can be viewed as elements of the dual RKHS W*. Let \u03bc and v be two varifolds. By the Hilbert norm of W*, the pseudo-metric can be induced as follows\n$d_{W^*}(\\mu,\\nu)^2 = ||\\mu - \\nu||^2_{W^*}$\n$= ||\\mu||^2_{W^*} - 2\\langle\\mu,\\nu\\rangle_{W^*} + ||\\nu||^2_{W^*}$. (3)\nThe above pseudo-metric (since Tw* is not injective in general) is associated with the RKHS W, and it provides an efficient way to compute varifold by separating the positional and Grassmannian components. Indeed, one can derive a bound with respect to dBL if we further assume that RKHS W is continuously embedded into CJ(Rn \u00d7 \u011e(d, n)) [13], i.e.,\n$||\\mu - \\nu||_{W^*} = \\sup_{\\varphi \\in W, ||\\varphi||_W\\le 1} \\int_{\\mathbb{R}^n\\times G(d,n)} \\varphi d(\\mu - \\nu) \\le c_W d_{BL}(\\mu,\\nu).$"}, {"title": "Neural tangent kernel", "content": "The recent advances of neural network theory finds a link between kernel theory and over-parameterised neural networks [23, 24]. If a neural network has a large but finite width, the weights at each layer remain close to its initialisation. Given training data pairs {xi, Yi}1, where xi \u2208 Rdo and yi \u2208 R, let f(0; x\u2081) be a fully connected neural network with L-hidden layers with inputs \u00e6i and parameters 0 = {W(0),b(0),\u2026\u2026\u2026, W(L), 6(L)}. Let d\u0127 be the width of the neural network for each layer h. The neural network function f can be written recursively as\n$f^{(h)} (x) = W^{(h)}g^{(h)}(x)+b^{(h)},\\ g^{(h+1)}(x) = \\varphi(f^{(h)}(x)),\\ h = 0, ..., L,$\nwhere g(0) (x) = x and is a non-linear activation function.\nAssume the weights W(h) \u2208 Rdn+1\u00d7dn_and bias b(h) \u2208 Rdh at each layer h are initialised with Gaussian distribution W(h) ~ N(0, 02/dh) and b(h) ~ N(0, 0), respectively. Consider training a neural network by minimising the least square loss function\n$l(\\theta) = \\frac{1}{2} \\sum_{i=1}^M (f(\\theta; x_i) - y_i)^2$. (5)\nSuppose the least square loss l(0) is minimised with an infinitesimally small learning rate, i.e., de = -1(0(t)). Let u(t) = (f(0(t); xi))i\u2208[M] \u2208 RM be the neural network outputs on all xi at time t, and y = (Yi)i\u2208[M] be the desired output. Then u(t) follows the evolution\n$\\frac{du}{dt} = -H(t)(u(t) - y),$\nwhere\n$H(t)_{ij} = \\frac{\\partial f(\\theta(t); x_i)}{\\partial \\theta} \\frac{\\partial f(\\theta(t); x_j)}{\\partial \\theta}$. (7)\nIf the width of the neural network at each layer goes to infinity, i.e., dh \u2192 \u221e, with a fixed training set, then H(t) remains unchanged. Under random initialisation of the parameters \u03b8, \u0397 (0) converges in probability to a deterministic kernel H* \u2013 the \u201cneural tangent kernel\u201d (i.e., NTK) [23]. Indeed, with few known activation functions 4 (e.g., ReLU), the neural tangent kernel H* can be computed by a closed-form solution recursively using Gaussian process [25, 24]. For each layer h, the corresponding covariance function is defined as\n$\\Sigma^{(0)} (x_i, x_j) = \\sigma^2 + \\frac{\\sigma_W^2}{d_0} x_i^T x_j,$\n$\\Lambda^{(h)} = \\begin{bmatrix} \\Sigma^{(h-1)} (x_i, x_i) & \\Sigma^{(h-1)} (x_i, x_j) \\\\ \\Sigma^{(h-1)} (x_j, x_i) & \\Sigma^{(h-1)} (x_j, x_j) \\end{bmatrix} \\in \\mathbb{R}^{2 \\times 2},$\n$\\Sigma^{(h)} (x_i, x_j) = \\sigma_b^2 + \\sigma_W^2 E_{(u,v)\\sim N(0,\\Lambda^{(h)})} [\\varphi(u)\\varphi(v)].$\nIn order to compute the neural tangent kernel, derivative covariance is defined as\n$\\dot{\\Sigma}^{(h)} (x_i, x_j) = E_{(u,v)\\sim N(0,\\Lambda^{(h)})} [\\dot{\\varphi}(u)\\dot{\\varphi}(v)].$\nThen, with (0) (xi, Xj) = \u03a3(0) (xi, xj), the neural tangent kernel at each layer (h) can be computed as follows\n$\\Theta^{(h)} (x_i, x_j) = \\Sigma^{(h)} (x_i, x_j) + \\dot{\\Sigma}^{(h-1)} (x_i, x_j).$\nThe convergence of (L) (xi, xj) to H is proven in Theorem 3.1 in [24]."}, {"title": "3.1 Neural varifold computation", "content": "In this section, we present the kernel representation of varifold on point clouds via neural tangent kernel. We first introduce the neural tangent kernel representation of popular neural networks on point clouds [7, 24] by computing the neural tangent kernel for position and Grassmannian components, individually.\nGiven the set of n point clouds S = {S1,S2,\u2026\u2026, sn}, where each point cloud s\u2081 = {P1,P2,\u2026,Pm} is a set of points, and n, m are respectively the number of point clouds and the number of points in each point cloud. Note that the number of points in each point cloud needs not be the same (e.g., |81|7|82|). For simplicity, we below assume different point clouds have the same number of points. Consider PointNet-like architecture that consists of L-hidden layers fully connected neural network shared by all points. For (i, j) \u2208 [m] \u00d7 [m], the covariance matrix (h) (p2, P3) and neural tangent kernel (h) (pz, p\u2081) at layer h are defined and computed in the same way of Equations (10) and (12). Assuming each point pe consists of positional information and surface normal direction such that p; \u2208 R3 \u00d7 S2, the varifold representation can be defined with neural tangent kernel theory in two different ways. One way is to follow the Charon-Trouv\u00e9 approach [13] by computing the position and Grassmannian kernels separately. While the original Charon-Trouv\u00e9 approach uses the radial basis kernel for the positional elements and a Cauchy-Binet kernel for the Grassmannian parts, in our cases, we use the neural tangent kernel representation for both the positional and Grassmannian parts. Let p = {x\u00ee, z\u00ee} \u2208 R\u00b3 \u00d7 S\u00b2 be a pair of position x \u2208 R\u00b3 and its surface normal z\u00ee \u2208 S2, \u00ec = 1, . . ., m. The neural varifold representation is defined as\n$\\Theta^{varifold} (p_i, p_j) = \\Theta^{POS} (x_i, x_j) \\Theta^{G} (z_i, z_j).$ (13)\nWe refer the above representation as PointNet-NTK1. As shown in Corollary 3.4 below, PointNet-NTK1 is a valid Charon-Trouv\u00e9 type kernel. From the neural tangent theory of view, PointNet-NTK1 in Equation (13) has two infinite-width neural networks on positional and Grassmannian components separately, and then aggregates information from the neural networks by element-wise product of the two neural tangent kernels.\nCorollary 3.4. In the limit of resolution going to infinity, neural tangent kernels \u2299pos and \u2299G are continuous positive definite kernels on positions and tangent planes, respectively. The varifold kernel varifold = Pos\u2299\u2299 is a positive definite kernel on R\u2033 \u00d7 \u011e(d, n) and the associated RKHS W is continuously embedded into Co(R\u2033 \u00d7 \u011e(d,n)).\nThe other way to define a varifold representation is by treating each point as a 6-dimensional feature P\u2081 = {x, z\u00ee } \u2208 R6. In this case, a single neural tangent kernel corresponding to an infinite-width neural network can be used, i.e.,\n$\\Theta^{varifold} (p_i, p_j) = \\Theta(\\begin{bmatrix} x_i \\\\ z_i \\end{bmatrix}, \\begin{bmatrix} x_j \\\\ z_j \\end{bmatrix}).$\nWe refer it as PointNet-NTK2. Since PointNet-NTK2 does not compute the positional and Grassman-nian kernels separately, it is computationally cheaper than PointNet-NTK1. It cannot be associated in the limit with a Charon-Trouv\u00e9 type kernel, in contrast with PointNet-NTK1, but it remains theoretically well grounded because the explicit coupling of positions and normals is a key aspect of the theory of varifolds that provides strong theoretical guarantees (e.g., convergence, compactness, weak regularity, second-order information, etc.). Furthermore, PointNet-NTK2 falls into the category of neural networks proposed for point clouds [7, 8] that treat point positions and surface normals as 6-feature vectors, and thus PointNet-NTK2 is a natural extension of current neural networks practices for point clouds.\nPointNet-NTK1 and PointNet-NTK2 in Equations (13) and (14) are computing NTK values between two points pa and p\u2081. The above forms can compute only pointwise-relationship in a single point cloud. However, in many point cloud applications, two or more point clouds need to be evaluated. Given the set of point clouds S, one needs to compute a Gram matrix of size \u00een \u00d7 n \u00d7 m \u00d7 m, which is computationally prohibitive in general. In order to reduce the size of the Gram matrix, we aggregate information by summation/average in all elements of varifold, thus forming an \u00een \u00d7 \u00een matrix, i.e.,\n$\\Theta^{varifold} (S_i, S_j) = \\sum_{i<m} \\sum_{j<m} \\Theta^{varifold} (p_i \\in S_i, p_j \\in S_j).$\nAnalogous to Equation (3), the varifold representation varifold can be used as a shape similarity metric between two sets of point clouds si and sj. The varifold metric can be computed as follows\n$||S_i - S_j ||_{varifold} = \\Theta^{varifold} (S_i, S_j) - 2\\Theta^{varifold}(S_i, S_j) + \\Theta^{varifold} (S_i, S_j).$"}, {"title": "4 Experiments", "content": "Dataset and experimental setting. We evaluate the varifold kernel representations and conduct comparisons on three different sought-after tasks: point cloud based shape matching between two different 3D meshes, point cloud based few-shot shape classification, and point cloud based 3D shape reconstruction. The details of each experiment setup are available at Appendix A.1, and the high-level pseudo-codes for each task are available at Appendex A.4. For ease of reference, we below shorten PointNet-NTK1, PointNet-NTK2, Chamfer distance, Charon-Trouv\u00e9 varifold norm and Earth Mover's distance as NTK1, NTK2, CD, CT and EMD, respectively."}, {"title": "4.1 Shape matching", "content": "To evaluate the surface representation using neural varifolds and make comparison with existing shape similarity metrics, synthetic shape matching experiments are conducted. We train MLP networks with 2 hidden layers with width of 64 and 128 units respectively. These networks use various shape similarity metrics as loss functions to deform the given source shape into the target shape (more details are available at Appendix A.1.1).\nFigure 1 shows five examples of shape matching based on various shape similarity metric losses. The neural network trained with CD captures geometric details well, except for the airplane. For hippocampi, CD over smoothes sharp edges; and for the bunny, it over smoothes the ears. While CD matches airplane wing shapes, it is noisier than CT, NTK1, and NTK2 methods. The EMD-trained network performs well on the dolphin shape but struggles with geometric details and surface consistency for other shapes, likely due to insufficient parameters for the transportation plan. More iterations and a lower convergence threshold make training inefficient. Networks trained with NTK1 and NTK2 metrics penalise broken meshes and surface noise, resulting in better mesh quality. NTK2 over smoothes high-frequency features on the dolphin, while NTK1 achieves good results. NTK1 and NTK2 show superior shape matching for airplane fuselage and wings. The network trained with CT gives acceptable results except for the airplane; however, one main disadvantage is that CT's radial basis kernel is sensitive to point cloud density, requiring hyperparameter o adjustments for each pair of point clouds to avoid poor results.\nTable 1 presents the quantitative eval-uation of the shape matching task. Each column indicates that the shape matching neural network is trained with a specific shape similarity met-ric as the loss function. In the case of dolphin, when the evaluation metric is the same as the loss function used to train the network, the network trained with the same evaluation met-ric achieves the best results. This is natural as the neural network is trained to minimise the loss function. It is worth highlighting that the shape matching network trained with the NTK1 loss achieves the second best score for all evaluation metrics ex-cept for itself. In other words, NTK1 can capture common characteristics of all shape similarity metrics used to train the network. Furthermore, in the case of shape matching be-tween two different cups, our neural varifold metrics (NTK1 and NTK2) achieve either the best or second best results regardless which shape eval-uation metric is used. This indicates that the neural varifold metrics can capture better geometric details as well as surface smoothness for the cup shape than other metrics. In the case of shape matching between the source hippocampus and the target hippocampus, the network trained with CT excels in the CD metric, while the network trained with NTK1 achieves superior results with respect to NTK1 and NTK2 metrics. The shape matching network trained with NTK2 outperforms in the EMD, CT and NTK2 metrics. In the case of the bunny, CT shows the best results with respect to CD and CT, while NTK1 shows the best matching results with respect to EMD and NTK1. NTK2, on the other hand, shows the second best results with respect to all metrics except for itself. In the case of airplane, CT shows the best matching results with respect to CD, EMD and CT. However, the CT metric itself shows the negative value, i.e. unstable. This is mainly because the RBF kernel used in CT is badly scaled. NTK1 shows the second best shape matching results with respect to all metrics except for itself. The detailed analysis for the role of the NTK layers on shape matching is available at Appendix A.5.2."}, {"title": "4.2 Few-shot shape classification", "content": "In this section, the proposed NTKs are firstly compared with the current state-of-the-art few-shot classification methods on the ModelNet40-FS benchmark [26]. ModelNet40-FS benchmark [26] divided different shape categories in ModelNet40 datasets for pre-training the network with 30 classes and then evaluated few-shot shape classification on 10 classes. The experiment was conducted in the standard few-shot learning setup, i.e. N-way K-shot Q-query. The definition of N-way K-shot Q-query is available at Appendix A.1. Table 2 shows the shape classification results on two different few-shot classification setups, i.e., 5way-1shot-15query and 5way-5shot-15query. In the case of the 5way-1shot classification, the current state-of-the-art method PCIA achieves the best results by around 7% margin in comparison to the second best method NTK2 (pre-trained). In the case of the 5way-5shot classification, NTK2 outperforms PCIA by around 0.8% margin. Note that PCIA requires to train backbone networks with PCIA modules and needs to fix the size of query. NTKs, on the other hand, can directly use the extracted backbone network features without further training the few-shot layer weights and do meta-learning in any arbitrary N-way K-shot Q-query settings. If NTKs are used without pre-trained backbone features, i.e., directly using positional and normal coordinates, then the results are subpar in comparison to other meta-learning approaches. This is understandable as few-shot architectures built on top of the backbone features, while NTKs without a pre-trained model, can only access the raw features, and thus cannot take advantages of the powerful feature learning capability of the neural networks. Interestingly, NTK1 outperforms NTK2 without pre-trained features, while NTK2 (DGCNN) outperforms NTK1 (DGCNN). This is because we use the pre-trained DGCNN on point clouds with spatial coordinates (x,y,z) as a backbone network for extracting both positional and normal features. Relatively low performance on NTK1 (DGCNN) is mainly because there is no appropriate architecture treating position and normal features separately.\nSmall-data tasks are common when data is limited. In the shape classification experiment, we restrict data availability and assume no pre-trained models, requiring training with 1, 5, 10, or 50 samples. Table 3 shows ModelNet classification accuracy with limited samples. Kernel-based approaches excel in small-data tasks. In particular, with only one sample, kernel methods outperform finite-width neural networks like PointNet and DGCNN on both ModelNet10 and ModelNet40, with NTK2 and NTK1 achieving the best results, respectively. Interestingly, the CT kernel performs as well as NTK1 and NTK2 on ModelNet10 but drops significantly on ModelNet40. Similar results occur with five samples: NTK1 and NTK2 achieve 81.3% and 81.7% on ModelNet10, while CT, PointNet, and DGCNN lag by 3.1%, 5.1%, and 5.9%, respectively. On ModelNet40, NTK1 outperforms all other methods more significantly than on Model-Net10. As the number of training samples increases, finite-width neural networks significantly improve their performance on both ModelNet10 and ModelNet40. With ten samples, NTK1 and NTK2 achieve around 86.1% accuracy, outper-forming other methods on ModelNet10 by 2-3%, although DGCNN surpasses NTK and PointNet on ModelNet40. With 50 samples, PointNet and DGCNN outperform NTK approaches by about 1% on ModelNet10 and 3-5% on ModelNet40. NTK1 and NTK2 show similar performance on ModelNet10 (with 0.3% difference), while NTK1 slightly outperforms NTK2 on ModelNet40 by 0.6-1.6%. Notably, NTK1 and NTK2 consistently outperform the CT varifold kernel.\nKernel-based learning is known for its quadratic computational complexity. However, NTK1 and NTK2 are computationally com-petitive in both few-shot learning and limited data scenarios. For in-stance, training NTK1 and NTK2 on ModelNet10 with 5 samples takes 47 and 18 seconds, respec-tively, compared to 254 and 502 seconds for training PointNet and DGCNN for 250 epochs on a sin-gle 3090 GPU. The shape classi-fication performance on the full ModelNet data is available at Ap-"}, {"title": "4.3 Shape reconstruction", "content": "Shape reconstruction from point clouds is tested for NTK1, SIREN, neural splines, and NKSR. NTK2 is excluded as it is unsuitable for this task. Implementation details are in Appendix A.2. Reconstruction quality is evaluated with CD and EMD metrics. Figure 2 shows examples (e.g., airplane and cabinet) with 2048 points. NTK1 performs better in surface completion and smoothness. Additional visualizations are in Appendix A.6.\nQuantitatively, Table 4 shows the mean and median of using the CD and EMD for 20 shapes ran-domly selected from each of the 13 different shape categories in the ShapeNet dataset. For the CD, NTK1 shows the best aver-age reconstruction results for the airplane, cabinet, car and vessel categories; SIREN shows the best reconstruction results for the chair, display and phone categories; and the neural splines method shows the best reconstruction results for the rest 6 categories. NTK1 based reconstruction achieves the lowest mean EMD for vessel and cabinet, while neural splines and SIREN achieve the lowest mean EMD for 7 and 5 categories, respectively. NKSR does not achieve the lowest mean CD and EMD for all the categories. In addition, the shape reconstruction results with different number of points (i.e., 512 and 1024) are available at Appendix A.5.4.\nSIREN shows the lowest distance for both CD and EMD followed by NTK1. Surprisingly, the neural splines method underperforms in both the CD and EMD when we consider all the 13 categories. The performance of NTK1 on shape reconstruction is clearly comparable with these state-of-the-art methods. This might be counter-intuitive as it regularises the kernel with additional normal information, this is probably because there is no straightforward way to assign normals on the regular grid coordinates, where the signed distance values are estimated by the kernel regression."}, {"title": "5 Limitations", "content": "While the proposed neural varifold has advantages over standard baselines, it has limitations. First, it is based on the simpler PointNet architecture. Future research should explore its performance with more advanced architectures like graph convolutions or voxelised point clouds. Second, the quadratic computational complexity of the kernel regime poses a challenge for large datasets. Kernel approximation methods, such as Nystrom approximation, could reduce this complexity, and their performance compared to exact kernels should be evaluated."}, {"title": "A.1 Experimental setup", "content": "A.1.1 Shape matching\nFor point cloud based shape matching, MLP networks consisting of 2 hidden layers (with width size of 64 and 128, respectively) were trained for computing displacement between two shapes, such that one can deform the source shape to the target shape. The neural networks were trained with different shape similarity metric losses including neural varifold. Point clouds of the given shapes were extracted by collecting the centre of triangular meshes of the given shapes, and the corresponding normals were computed by cross product of two edges of the meshes. The first example is deforming the source unit sphere into the target dolphin shape; the second is matching two different cup designs; the third is matching between two hippocampi; the fourth is the shape matching bewteen sphere and Stanford bunny; and the fifth is the shape matching between two different designs of airplane. The data is acquired from the PyTorch3D, SRNFmatch and KeOps GitHub repositories [27, 28, 29]. This experiment evaluates how well the source shape can be deformed based on the chosen shape similarity measure as the loss function. A simple 3-layer MLP network was solely trained with a single shape similarity measure loss, with the learning rate fixed to 1E-3 and the Adam optimiser. The network was trained with popular shape similarity measures including the CD (Chamfer distance), EMD (Earth Mover's distance), CT (Charon-Trouv\u00e9 varifold norm), and the proposed neural varifold norms (NTK1 and NTK2). In the case of CD and EMD, we followed the same method used for shape reconstruction. For varifold metrics, we used Equation (16); note that it is a squared distance commonly used for optimisation. For the numerical evaluation as a metric in Table 1, the square-root of Equation (16) was used. To be consistent with shape classification experiments, we chose the 5-layer NTK1 and 9-layer NTK2 to train and evaluate the similarity between two shapes. The detailed analysis for the role of the neural network layers on shape matching is available at Appendix A.5.2. The final outputs from the networks were evaluated with all of the shape similarity measures used in the experiments."}, {"title": "A."}]}