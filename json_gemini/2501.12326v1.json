{"title": "XUI-TARS:\nPioneering Automated GUI Interaction with Native Agents", "authors": ["Yujia Qin", "Yining Ye", "Junjie Fang", "Haoming Wang", "Shihao Liang", "Shizuo Tian", "Junda Zhang", "Jiahao Li", "Yunxin Li", "Shijue Huang", "Wanjun Zhong", "Kuanye Li", "Jiale Yang", "Yu Miao", "Woyu Lin", "Longxiang Liu", "Xu Jiang", "Qianli Ma", "Jingyu Li", "Xiaojun Xiao", "Kai Cai", "Chuang Li", "Yaowei Zheng", "Chaolin Jin", "Chen Li", "Xiao Zhou", "Minchao Wang", "Haoli Chen", "Zhaojian Li", "Haihua Yang", "Haifeng Liu", "Feng Lin", "Tao Peng", "Xin Liu", "Guang Shi"], "abstract": "This paper introduces UI-TARS, a native GUI agent model that solely perceives the screen-\nshots as input and performs human-like interactions (e.g., keyboard and mouse operations).\nUnlike prevailing agent frameworks that depend on heavily wrapped commercial models\n(e.g., GPT-40) with expert-crafted prompts and workflows, UI-TARS is an end-to-end model\nthat outperforms these sophisticated frameworks. Experiments demonstrate its superior per-\nformance: UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating\nperception, grounding, and GUI task execution (see below). Notably, in the OSWorld bench-\nmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15 steps, outperforming\nClaude's 22.0 and 14.9 respectively. In AndroidWorld, UI-TARS achieves 46.6, surpassing\nGPT-40's 34.5. UI-TARS incorporates several key innovations: (1) Enhanced Perception:\nleveraging a large-scale dataset of GUI screenshots for context-aware understanding of\nUI elements and precise captioning; (2) Unified Action Modeling, which standardizes\nactions into a unified space across platforms and achieves precise grounding and interaction\nthrough large-scale action traces; (3) System-2 Reasoning, which incorporates deliberate\nreasoning into multi-step decision making, involving multiple reasoning patterns such as task\ndecomposition, reflection thinking, milestone recognition, etc. (4) Iterative Training with\nReflective Online Traces, which addresses the data bottleneck by automatically collecting,\nfiltering, and reflectively refining new interaction traces on hundreds of virtual machines.\nThrough iterative training and reflection tuning, UI-TARS continuously learns from its\nmistakes and adapts to unforeseen situations with minimal human intervention. We also\nanalyze the evolution path of GUI agents to guide the further development of this domain.\nUI-TARS is open sourced at https://github.com/bytedance/UI-TARS.", "sections": [{"title": "1 Introduction", "content": "Autonomous agents (Wang et al., 2024b; Xi et al., 2023; Qin et al., 2024) are envisioned to operate with minimal\nhuman oversight, perceiving their environment, making decisions, and executing actions to achieve specific\ngoals. Among the many challenges in this domain, enabling agents to interact seamlessly with Graphical User\nInterfaces (GUIs) has emerged as a critical frontier (Hu et al., 2024; Zhang et al., 2024a; Nguyen et al., 2024;\nWang et al., 2024e; Gao et al., 2024). GUI agents are designed to perform tasks within digital environments\nthat rely heavily on graphical elements such as buttons, text boxes, and images. By leveraging advanced\nperception and reasoning capabilities, these agents hold the potential to revolutionize task automation, enhance\naccessibility, and streamline workflows across a wide range of applications.\nThe development of GUI agents has historically relied on hybrid approaches that combine textual representa-\ntions (e.g., HTML structures and accessibility trees) (Liu et al., 2018; Deng et al., 2023; Zhou et al., 2023).\nWhile these methods have driven significant progress, they suffer from limitations such as platform-specific\ninconsistencies, verbosity, and limited scalability (Xu et al., 2024). Textual-based methods often require\nsystem-level permissions to access underlying system information, such as HTML code, which further limits\ntheir applicability and generalizability across diverse environments. Another critical issue is that, many\nexisting GUI systems follow an agent framework paradigm (Zhang et al., 2023; Wang et al., 2024a; Wu et al.,\n2024a; Zhang et al., 2024b; Wang & Liu, 2024; Xie et al., 2024), where key functions are modularized across\nmultiple components. These components often rely on specialized vision-language models (VLMs), e.g.,\nGPT-40 (Hurst et al., 2024), for understanding and reasoning (Zhang et al., 2024b), while grounding (Lu et al.,\n2024b) or memory (Zhang et al., 2023) modules are implemented through additional tools or scripts. Although\nthis modular architecture facilitates rapid development in specific domain tasks, it relies on handcrafted\napproaches that depend on expert knowledge, modular components, and task-specific optimizations, which are\nless scalable and adaptive than end-to-end models. This makes the framework prone to failure when faced\nwith unfamiliar tasks or dynamically changing environments (Xia et al., 2024).\nThese challenges have prompted two key shifts towards native GUI agent model: (1) the transition from\ntextual-dependent to pure-vision-based GUI agents (Bavishi et al., 2023; Hong et al., 2024). \"Pure-vision\"\nmeans the model relies exclusively on screenshots of the interface as input, rather than textual descriptions\n(e.g., HTML). This bypasses the complexities and platform-specific limitations of textual representations,\naligning more closely with human cognitive processes; and (2) the evolution from modular agent frameworks\nto end-to-end agent models (Wu et al., 2024b; Xu et al., 2024; Lin et al., 2024b; Yang et al., 2024a; Anthropic,\n2024b). The end-to-end design unifies traditionally modularized components into a single architecture,\nenabling a smooth flow of information among modules. In philosophy, agent frameworks are design-driven,\nrequiring extensive manual engineering and predefined workflows to maintain stability and prevent unexpected\nsituations; while agent models are inherently data-driven, enabling them to learn and adapt through large-scale\ndata and iterative feedback (Putta et al., 2024).\nDespite their conceptual advantages, today's native GUI agent model often falls short in practical applications,\ncausing their real-world impact to lag behind its hype. These limitations stem from two primary sources:\n(1) the GUI domain itself presents unique challenges that compound the difficulty of developing robust\nagents. (1.a) On the perception side, agents must not only recognize but also effectively interpret the high\ninformation-density of evolving user interfaces. (1.b) Reasoning and planning mechanisms are equally\nimportant in order to navigate, manipulate, and respond to these interfaces effectively. (1.c) These mechanisms\nmust also leverage memory, considering past interactions and experiences to make informed decisions. (1.d)\nBeyond high-level decision-making, agents must also execute precise, low-level actions, such as outputting\nexact screen coordinates for clicks or drags and inputting text into the appropriate fields. (2) The transition\nfrom agent frameworks to agent models introduces a fundamental data bottleneck. Modular frameworks\ntraditionally rely on separate datasets tailored to individual components. These datasets are relatively easy\nto curate since they address isolated functionalities. However, training an end-to-end agent model demands\ndata that integrates all components in a unified workflow, capturing the seamless interplay between perception,\nreasoning, memory, and action. Such data, which comprise rich workflow knowledge from human experts,\nhave been scarcely recorded historically. This lack of comprehensive, high-quality data limits the ability of\nnative agents to generalize across diverse real-world scenarios, hindering their scalability and robustness.\nTo address these challenges, this paper focuses on advancing native GUI agent model. We begin by reviewing\nthe evolution path for GUI agents (\u00a7 2). By segmenting the development of GUI agents into key stages based\non the degree of human intervention and generalization capabilities, we conduct a comprehensive literature\nreview. Starting with traditional rule-based agents, we highlight the evolution from rigid, framework-based\nsystems to adaptive native models that seamlessly integrate perception, reasoning, memory, and action. We"}, {"title": "2 Evolution Path of GUI Agents", "content": "GUI agents are particularly significant in the context of automating workflows, where they help streamline\nrepetitive tasks, reduce human effort, and enhance productivity. At their core, GUI agents are designed to\nfacilitate the interaction between humans and machines, simplifying the execution of tasks. Their evolution\nreflects a progression from rigid, human-defined heuristics to increasingly autonomous systems that can adapt,\nlearn, and even independently identify tasks. In this context, the role of GUI agents has shifted from simple\nautomation to full-fledged, self-improving agents that increasingly integrate with the human workflow, acting\nnot just as tools, but as collaborators in the task execution process.\nOver the years, agents have progressed from basic rule-based automation to an advanced, highly automated,\nand flexible system that increasingly mirrors human-like behavior and requires minimal human intervention\nto perform its tasks. As illustrated in Figure 2, the development of GUI agents can be broken down into\nseveral key stages, each representing a leap in autonomy, flexibility, and generalization ability. Each stage is\ncharacterized by how much human intervention is required in the workflow design and learning process."}, {"title": "2.1 Rule-based Agents", "content": "Stage 1: Rule-based Agents In the initial stage, agents such as Robotic Process Automation (RPA)\nsystems (Dobrica, 2022; Hofmann et al., 2020) were designed to replicate human actions in highly structured\nenvironments, often interacting with GUIs and enterprise software systems. These agents typically processed\nuser instructions by matching them to predefined rules and invoking APIs accordingly. Although effective for\nwell-defined and repetitive tasks, these systems were constrained by their reliance on human-defined heuristics\nand explicit instructions, hindering their ability to handle novel and complex scenarios. At this stage, the\nagent cannot learn from its environment or previous experiences, and any changes to the workflow require\nhuman intervention. Moreover, these agents require direct access to APIs or underlying system permissions,\nas demonstrated by systems like DART (Memon et al., 2003), WoB (Shi et al., 2017), Roscript (Qian et al.,\n2020) and FLIN (Mazumder & Riva, 2021). This makes it unsuitable for cases where such access is restricted\nor unavailable. This inherent rigidity constrained their applicability to scale across diverse environments.\nThe limitations of rule-based agents underscore the importance of transitioning to GUI-based agents that\nrely on visual information and explicit operation on GUIs instead of requiring low-level access to systems.\nThrough visual interaction with interfaces, GUI agents unlock greater flexibility and adaptability, significantly\nexpanding the range of tasks they can accomplish without being limited by predefined rules or the need for\nexplicit system access. This paradigm shift opens pathways for agents to interact with unfamiliar or newly\ndeveloped interfaces autonomously."}, {"title": "2.2 From Modular Agent Framework to Native Agent Model", "content": "Agent frameworks leveraging the power of large models (M)LLMs have surged in popularity recently. This\nsurge is driven by the foundation models' ability to deeply comprehend diverse data types and generate relevant\noutputs via multi-step reasoning. Unlike rule-based agents, which necessitate handcrafted rules for each\nspecific task, foundation models can generalize across different environments and effectively handle tasks by\ninteracting multiple times with environments. This eliminates the need for humans to painstakingly define\nrules for every new scenario, significantly simplifying agent development and deployment.\nStage 2: Agent Framework Specifically, these agent systems mainly leverage the understanding and\nreasoning capabilities of advanced foundation models (e.g., GPT-4 (OpenAI, 2023b) and GPT-40 (Hurst et al.,\n2024)) to enhance task execution flexibility, which become more flexible, framework-based agents. Early\nefforts primarily focused on tasks such as calling specific APIs or executing code snippets within text-based\ninterfaces (Wang et al., 2023; Li et al., 2023a,b; Wen et al., 2023; Nakano et al., 2021). These agents marked a\nsignificant advancement from purely rule-based systems by enabling more automatic and flexible interactions.\nAutonomous frameworks like AutoGPT (Yang et al., 2023a) and LangChain allow agents to integrate multiple\nexternal tools, APIs, and services, enabling a more dynamic and adaptable workflow.\nEnhancing the performance of foundation model-based agent frameworks often involves designing task-\nspecific workflows and optimizing prompts for each component. For instance, some approaches augment these\nframeworks with specialized modules, such as short- or long-term memory, to provide task-specific knowledge\nor store operational experience for self-improvement. Cradle (Tan et al., 2024) enhances foundational agents'\nmultitasking capabilities by storing and leveraging task execution experiences. Similarly, Song et al. (2024)\npropose a framework for API-driven web agents that utilizes task-specific background knowledge to execute\ncomplex web operations. The Agent Workflow Memory (AWM) module (Wang et al., 2024g) further optimizes\nmemory management by selectively providing relevant workflows to guide the agent's subsequent actions.\nAnother common strategy to improve task success is the incorporation of reflection-based, multi-step reasoning\nto refine action planning and execution. The widely recognized ReAct framework (Yao et al., 2023) integrates\nreasoning with the outcomes of actions, enabling more dynamic and adaptable planning. For multimodal tasks,\nMMNavigator (Yan et al., 2023) leverages summarized contextual actions and mark tags to generate accurate,\nexecutable actions. SeeAct (Zheng et al., 2024b) takes a different approach by explicitly instructing GPT-4V\nto mimic human browsing behavior, taking into account the task, webpage content, and previous actions.\nFurthermore, multi-agent collaboration has emerged as a powerful technique for boosting task completion rates.\nMobileExperts (Zhang et al., 2024c), for example, addresses the unique challenges of mobile environments"}, {"title": "2.2 From Modular Agent Framework to Native Agent Model", "content": "by incorporating tool formulation and fostering collaboration among multiple agents. In summary, current\nadvancements in agent frameworks heavily rely on optimizing plan and action generation through prompt\nengineering, centered around the capabilities of the underlying foundation models, ultimately leading to\nimproved task completion.\nKey Limitations of Agent Frameworks Despite greater adaptability compared to rule-based systems,\nagent frameworks still rely on human-defined workflows to structure their actions. The \"agentic workflow\nknowledge\" (Wang et al., 2024g) is manually encoded through custom prompts, external scripts, or tool-usage\nheuristics. This externalization of knowledge yields several drawbacks:\n\u2022 Fragility and Maintenance Overhead: whenever tasks, interfaces, or usage scenarios evolve, the\nworkflow's manual rules or prompts must be re-crafted or extended by developers an error-prone and\nlabor-intensive process.\n\u2022 Disjoint Learning Paradigms: framework-based methods rarely integrate new experience data to update\nthe underlying LLM/VLM parameters. Instead, they rely on offline prompt-engineering or workflow\ndesign. As tasks deviate from the original domain, these frameworks often fail, limiting adaptability.\n\u2022 Module Incompatibility: complex tasks demand multiple modules (e.g., visual parsing, memory stores,\nlong-horizon planning) that must coordinate via prompts or bridging code. Inconsistencies or errors in\nany module can derail the entire pipeline, and diagnosing these issues typically requires domain experts\nto debug the flow.\nThus, while agent frameworks offer quick demonstrations and are flexible within a narrow scope, they\nultimately remain brittle when deployed in real-world scenarios, where tasks and interfaces continuously\nevolve. This reliance on pre-programmed workflows, driven by human expertise, makes frameworks\ninherently non-scalable. They depend on the foresight of developers to anticipate all future variations, which\nlimits their capacity to handle unforeseen changes or learn autonomously. Frameworks are design-driven,\nmeaning they lack the ability to learn and generalize across tasks without continuous human involvement.\nStage 3: Native Agent Model In contrast, the future of autonomous agent development lies in the creation\nof native agent models, where workflow knowledge is embedded directly within the agent's model through\norientational learning. In this paradigm, tasks are learned and executed in an end-to-end manner, unifying\nperception, reasoning, memory, and action within a single, continuously evolving model. This approach is\nfundamentally data-driven, allowing for the seamless adaptation of agents to new tasks, interfaces, or user\nneeds without relying on manually crafted prompts or predefined rules. Native agents offer several distinct\nadvantages that contribute to their scalability and adaptability:\n\u2022 Holistic Learning and Adaptation: because the agent's policy is learned end-to-end, it can unify\nknowledge from perception, reasoning, memory, and action in its internal parameters. As new data or\nuser demonstrations become available, the entire system (rather than just a single module or prompt)\nupdates its knowledge. This empowers the model to adapt more seamlessly to changing tasks, interfaces,\nor user demands.\n\u2022 Reduced Human Engineering: instead of carefully scripting how the LLM/VLM should be invoked\nat each node, native models learn task-relevant workflows from large-scale demonstrations or online\nexperiences. The burden of \u201chardwiring a workflow\u201d is replaced by data-driven learning. This significantly\nreduces the need for domain experts to handcraft heuristics whenever the environment evolves.\n\u2022 Strong Generalization via Unified Parameters: Although manual prompt engineering can make the\nmodel adaptable to user-defined new tools, the model itself cannot evolve. Under one parameterized\npolicy and a unified data construction and training pipeline, knowledge among environments like certain\napp features, navigation strategies, or UI patterns can be transferred across tasks, equipping it with strong\ngeneralization.\n\u2022 Continuous Self-Improvement: native agent models lend themselves naturally to online or lifelong learn-\ning paradigms. By deploying the agent in real-world GUI environments and collecting new interaction\ndata, the model can be fine-tuned or further trained to handle novel challenges.\nThis data-driven, learning-oriented approach stands in contrast to the design-driven, static nature of agent\nframeworks. As for now, the development of GUI agent gradually reached this stage, which representative\nworks like Claude Computer-Use (Anthropic, 2024b), Aguvis (Xu et al., 2024), ShowUI (Lin et al., 2024b),\nOS-Atlas (Wu et al., 2024b), Octopus v2-4 (Chen & Li, 2024), etc. These models mainly utilize existing world\ndata to tailor large VLMs specifically for the domain of GUI interaction."}, {"title": "2.3 Active and Lifelong Agent (Prospect)", "content": "Stage 4: Action and Lifelong Agent Despite improvements in adaptability, native agents still rely heavily on\nhuman experts for data labeling and training guidance. This dependence inherently restricts their capabilities,\nmaking them contingent upon the quality and breadth of human-provided data and knowledge.\nThe transition towards active and lifelong learning (Sur et al., 2022; Ramamoorthy et al., 2024) represents a\ncrucial next step in the evolution of GUI agents. In this paradigm, agents actively engage with their environment\nto propose tasks, execute them, and evaluate the outcomes. These agents can autonomously assign self-rewards\nbased on the success of their actions, reinforcing positive behaviors and progressively refining their capabilities\nthrough continuous feedback loops. This process of self-directed exploration and learning allows the agent\nto discover new knowledge, improve task execution, and enhance problem-solving strategies without heavy\nreliance on manual annotations or explicit external guidance.\nThese agents develop and modify their skills iteratively, much like continual learning in robotics (Ayub et al.,\n2024; Soltoggio et al., 2024), where they can learn from both successes and failures, progressively enhancing\ntheir generalization across an increasingly broad range of tasks and scenarios. The key distinction between\nnative agent models and active lifelong learners lies in the autonomy of the learning process: native agents still\ndepend on humans, whereas active agents drive their own learning by identifying gaps in their knowledge and\nfilling them through self-initiated exploration.\nIn this work, we focus on building a scalable and data-driven native agent model, which paves the way for this\nactive and lifelong agent stage. We begin by exploring the core capabilities necessary for such a framework\n(\u00a7 3) and then introduce UI-TARS, our instantiation of this approach (\u00a7 4)."}, {"title": "3 Core Capabilities of Native Agent Model", "content": "The native agent model internalizes modularized components from the previous agent framework into several\ncore capabilities, thereby transitioning towards an end-to-end structure. To get a more profound understanding\nof the native agent model, this section delves into an in-depth analysis of its core capabilities and reviews the\ncurrent evaluation metrics and benchmarks."}, {"title": "3.1 Core Capabilities", "content": "As illustrated in Figure 3, our analysis is structured around four main aspects: perception, action, reasoning\n(system-1&2 thinking), and memory.\nPerception A fundamental aspect of effective GUI agents lies in their capacity to precisely perceive and\ninterpret graphical user interfaces in real-time. This involves not only understanding static screenshots, but\nalso dynamically adapting to changes as the interface evolves. We review existing works based on their usage\nof input features:\n\u2022 Structured Text: early iterations (Li et al., 2023a; Wang et al., 2023; Wu et al., 2024a) of GUI agents\npowered by LLMs are constrained by the LLMs' limitation of processing only textual input. Consequently,\nthese agents rely on converting GUI pages into structured textual representations, such as HTML,\naccessibility trees, or Document Object Model (DOM). For web pages, some agents use HTML data\nas input or leverage the DOM to analyze pages' layout. The DOM provides a tree-like structure that\norganizes elements hierarchically. To reduce input noise, Agent-E (Abuelsaad et al., 2024) utilizes\na DOM distillation technique to achieve more effective screenshot representations. Tao et al. (2023)\nintroduce WebWISE, which iteratively generates small programs based on observations from filtered\nDOM elements and performs tasks in a sequential manner.\n\u2022 Visual Screenshot: with advancements in computer vision and VLMs, agents are now capable of\nleveraging visual data from screens to interpret their on-screen environments. A significant portion of\nresearch relies on Set-of-Mark (SoM) (Yang et al., 2023b) prompting to improve the visual grounding\ncapabilities. To enhance visual understanding, these methods frequently employ Optical Character\nRecognition (OCR) in conjunction with GUI element detection models, including ICONNet (Sunkara\net al., 2022) and DINO (Liu et al., 2025). These algorithms are used to identify and delineate interactive\nelements through bounding boxes, which are subsequently mapped to specific image regions, enriching\nthe agents' contextual comprehension. Some studies also improve the semantic grounding ability and\nunderstanding of elements by adding descriptions of these interactive elements in the screenshots. For\nexample, SeeAct (Zheng et al., 2024a) enhances fine-grained screenshot content understanding by\nassociating visual elements with the content they represent in HTML web.\n\u2022 Comprehensive Interface Modeling: recently, certain works have employed structured text, visual\nsnapshots, and semantic outlines of elements to attain a holistic understanding of external perception. For\ninstance, Gou et al. (2024a) synthesize large-scale GUI element data and train a visual grounding model\nUGround to gain the associated references of elements in GUI pages on various platforms. Similarly,\nOSCAR (Wang & Liu, 2024) utilizes an Ally tree generated by the Windows API for representing\nGUI components, incorporating descriptive labels to facilitate semantic grounding. Meanwhile, DUAL-\nVCR (Kil et al., 2024) captures both the visual features of the screenshot and the descriptions of associated\nHTML elements to obtain a robust representation of the visual screenshot.\nAnother important point is the ability to interact in real-time. GUIs are inherently dynamic, with elements\nfrequently changing in response to user actions or system processes. GUI agents must continuously monitor\nthese changes to maintain an up-to-date understanding of the interface's state. This real-time perception is\ncritical for ensuring that agents can respond promptly and accurately to evolving conditions. For instance,\nif a loading spinner appears, the agent should recognize it as an indication of a pending process and adjust\nits actions accordingly. Similarly, agents must detect and handle scenarios where the interface becomes\nunresponsive or behaves unexpectedly.\nBy effectively combining these above aspects, a robust perception system ensures that the GUI agent can\nmaintain situational awareness and respond appropriately to the evolving state of the user interface, aligning its\nactions with the user's goals and the application's requirements. However, privacy concerns and the additional\nperceptual noise introduced by the DOM make it challenging to extend pure text descriptions and hybrid\ntext-visual perceptions to any GUI environment. Hence, similar to human interaction with their surroundings,\na native agent model should directly comprehend the external environment through visual perception and\nground their actions to the original screenshot accurately. By doing so, the native agent model can generalize\nvarious tasks and improve the accuracy of actions at each step."}, {"title": "Action", "content": "Effective action mechanisms must be versatile, precise, and adaptable to various GUI contexts. Key\naspects include:"}, {"title": "3.1 Core Capabilities", "content": "\u2022 Unified and Diverse Action Space: GUI agents (Gur et al., 2023; Bonatti et al., 2024) operate across\nmultiple platforms, including mobile devices, desktop applications, and web interfaces, each with distinct\ninteraction paradigms. Establishing a unified action space abstracts platform-specific actions into a com-\nmon set of operations such as click,type, scroll, and drag. Additionally, integrating actions from\nlanguage agents\u2014such as API calls (Chen et al., 2024b; Li et al., 2023a,b), code interpretation (Wu et al.,\n2024a), and Command-Line Interface (CLI) (Mei et al., 2024) operations-enhances agent versatility.\nActions can be categorized into atomic actions, which execute single operations, and compositional\nactions, which sequence multiple atomic actions to streamline task execution. Balancing atomic and\ncompositional actions optimizes efficiency and reduces cognitive load, enabling agents to handle both\nsimple interactions and the coordinated execution of multiple steps seamlessly.\n\u2022 Challenges in Grounding Coordinates: accurately determining coordinates for actions like clicks, drags,\nand swipes is challenging due to variability in GUI layouts (He et al., 2024; Burger et al., 2020), differing\naspect ratios across devices, and dynamic content changes. Different devices' aspect ratios can alter\nthe spatial arrangement of interface elements, complicating precise localization. Grounding coordinates\nrequires advanced techniques to interpret visual cues from screenshots or live interface streams accurately.\nDue to the similarity of actions across different operational spaces, agent models can standardize actions from\nvarious GUI contexts into a unified action space. Decomposing actions into atomic operations reduces learning\ncomplexity, facilitating faster adaptation and transfer of atomic actions across different platforms."}, {"title": "Reasoning with System 1&2 Thinking", "content": "Reasoning is a complex capability that integrates a variety of\ncognitive functions. Human interaction with GUIs relies on two distinct types of cognitive processes (Groves\n& Thompson, 1970): system 1 and system 2 thinking.\n\u2022 System 1 refers to fast, automatic, and intuitive thinking, typically employed for simple and routine tasks,\nsuch as clicking a familiar button or dragging a file to a folder without conscious deliberation.\n\u2022 System 2 encompasses slow, deliberate, and analytical thinking, which is crucial for solving complex\ntasks, such as planning an overall workflow or reflecting to troubleshoot errors.\nSimilarly, autonomous GUI agents must develop the ability to emulate both system 1 and system 2 thinking to\nperform effectively across a diverse range of tasks. By learning to identify when to apply rapid, heuristic-based\nresponses and when to engage in detailed, step-by-step reasoning, these agents can achieve greater efficiency,\nadaptability, and reliability in dynamic environments."}, {"title": "System 1 Reasoning", "content": "represents the agent's ability to execute fast, intuitive responses by identifying patterns\nin the interface and applying pre-learned knowledge to observed situations. This form of reasoning mirrors\nhuman interaction with familiar elements of a GUI, such as recognizing that pressing \u201cEnter\u201d in a text field\nsubmits a form or understanding that clicking a certain button progresses to the next step in a workflow. These\nheuristic-based actions enable agents to respond swiftly and maintain operational efficiency in routine scenarios.\nHowever, the reliance on pre-defined mappings limits the scope of their decision-making to immediate, reactive\nbehaviors. For instance, models such as large action models (Wu et al., 2024b; Wang et al., 2024a) excel at\ngenerating quick responses by leveraging environmental observations, but they often lack the capacity for\nmore sophisticated reasoning. This constraint becomes particularly evident in tasks requiring the planning and\nexecution of multi-step operations, which go beyond the reactive, one-step reasoning of system 1. Thus, while\nsystem 1 provides a foundation for fast and efficient operation, it underscores the need for agents to evolve\ntoward more deliberate and reflective capabilities seen in system 2 reasoning."}, {"title": "System 2 Reasoning", "content": "represents deliberate, structured, and analytical thinking, enabling agents to handle\ncomplex, multi-step tasks that go beyond the reactive behaviors of system 1. Unlike heuristic-based reasoning,\nsystem 2 involves explicitly generating intermediate thinking processes, often using techniques like Chain-of-\nThought (CoT) (Wei et al., 2022) or ReAct (Yao et al., 2023), which bridge the gap between simple actions\nand intricate workflows. This paradigm of reasoning is composed of several essential components.\n\u2022 First, task decomposition focuses on formulating plannings to achieve overarching objectives by de-\ncomposing tasks into smaller, manageable sub-tasks (Dagan et al., 2023; Song et al., 2023; Huang et al.,\n2024). For example, completing a multi-field form involves a sequence of steps like entering a name,\naddress, and other details, all guided by a well-structured plan.\n\u2022 Second, long-term consistency is critical during the entire task completion process. By consistently\nreferring back to the initial objective, agent models can effectively avoid any potential deviations that\nmay occur during complex, multi-stage tasks, thus ensuring coherence and continuity from start to finish."}, {"title": "3.2 Capability Evaluation", "content": "\u2022 Third, milestone recognition allows the agent model to estimate the current state of progress, analyze\nobservations, and determine the subsequent goals. This ensures that multi-step workflows are executed\neffectively without losing direction.\n\u2022 Fourth, trial and error endows agent models with additional opportunities to hypothesize, test, and\nassess potential actions, thereby enhancing the precision of decision-making, particularly in ambiguous\nand complex scenarios.\n\u2022 Finally, reflection equips agent models with the capability to evaluate past actions, identify mistakes,\nand make adjustments to improve future performance (Shinn et al., 2023; Renze & Guven, 2024). This\niterative process enhances reliability and helps prevent repetitive errors.\nThe development of UI-TARS places a strong emphasis on equipping the model with robust system 2 reasoning\ncapabilities, allowing it to address complex tasks with greater precision and adaptability. By integrating\nhigh-level planning mechanisms, UI-TARS excels at decomposing overarching goals into smaller, manageable\nsub-tasks. This structured approach enables the model to systematically handle intricate workflows that\nrequire coordination across multiple steps. Additionally, UI-TARS incorporates a long-form CoT reasoning\nprocess, which facilitates detailed intermediate thinking before executing specific actions. Furthermore, UI-\nTARS adopts reflection-driven training process. By incorporating reflective thinking, the model continuously\nevaluates its past actions, identifies potential mistakes, and adjusts its behavior to improve performance over\ntime. The model's iterative learning method yields significant benefits, enhancing its reliability and equipping\nit to navigate dynamic environments and unexpected obstacles."}, {"title": "Memory", "content": "The memory is mainly used to store the supported explicit knowledge and historical experience\nthat the agent refers to when making decisions. For agent frameworks, an additional memory module is often\nintroduced to store previous interactions and task-level knowledge. Agents then retrieve and update these\nmemory modules during decision-making progress. The memory module can be divided into two categories:\n\u2022 Short-term Memory: this serves as a temporary repository for task-specific information, capturing\nthe agent's immediate context. This includes the agent's action history, current state details, and the\nongoing execution trajectory of the task, enabling real-time situational awareness and adaptability. By\nsemantically processing contextual screenshots, CoAT (Zhang et al., 2024d) extracts key interface details,\nthereby enhancing comprehension of the task environment. CoCo-Agent (Ma et al., 2024) records layouts\nand dynamic states through Comprehensive Environment Perception (CEP).\n\u2022 Long-term Memory: it operates as a long-term data reserve, capturing and safeguarding records of\nprevious interaction, tasks, and background knowledge. It retains details such as execution paths from\nprior tasks, offering a comprehensive knowledge base that supports reasoning and decision-making for\nfuture tasks. By integrating accumulated knowledge that contains user preferences and task operation\nexperiences, OS-copilot (Wu et al., 2024a) refines its task execution over time to better align with user\nneeds and improve overall efficiency. Cradle (Tan et al., 2024) focuses on enhancing the multitasking\nabilities of foundational agents by equipping them with the capability to store and utilize task execution\nexperiences. Song et al. (2024) introduce a framework for API-driven web agents that leverage task-\nspecific background knowledge to perform complex web operations.\nMemory reflects the capability to leverage background knowledge and input context. The synergy between\nshort-term and long-term memory storage significantly enhances the efficiency of an agent's decision-making\nprocess. Native agent models, unlike agent frameworks, encode long-term operational experience of tasks\nwithin their internal parameters, converting the observable interaction process into implicit, parameterized\nstorage. Techniques such as In-Context Learning (ICL) or CoT reasoning can be employed to activate this\ninternal memory."}, {"title": "3"}]}