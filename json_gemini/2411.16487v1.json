{"title": "When Babies Teach Babies: Can student knowledge sharing outperform Teacher-Guided Distillation on small datasets?", "authors": ["Srikrishna Iyer"], "abstract": "We present our submission to the BabyLM challenge, aiming to push the boundaries of data-efficient language model pretraining. Our method builds upon deep mutual learning, introducing a student model search for diverse initialization. We address the limitation of treating students equally by formulating weighted mutual learning as a bi-level optimization problem. The inner loop learns compact students through online distillation, while the outer loop optimizes weights for better knowledge distillation from diverse students. This dynamic weighting strategy eliminates the need for a teacher model, reducing computational requirements. Our evaluations show that teacher-less methods can match or surpass teacher-supervised approaches.", "sections": [{"title": "Introduction", "content": "The substantial computational and memory requirements of large language models pose significant challenges for deployment on intelligent edge systems, where resources are often constrained. As the demand for real-time processing and low-latency responses increases in edge computing environments, the need for lightweight and memory-efficient models becomes critical. Recent research, notably the Chinchilla paper (Hoffmann et al. (2024)), demonstrated that a 70B parameter model trained on 1.4 trillion tokens outperformed larger models with less data, highlighting the intricate balance between model size and training data. This massive data requirement equivalent to over 10,000 times the words a 13-year-old encounters is becoming a significant bottleneck. To address these challenges, several techniques have emerged such as network pruning (Han et al."}, {"title": "Related Work", "content": "The vanilla distillation Hinton et al. (2015) method consists of two stages, firstly train a large teacher model, followed by transfer of soft logits to a smaller student model. Also known as Offline distillation, it keeps the teacher fixed, only allowing a one-way knowledge transfer. To reduce memory consumption of training a large teacher model, Zhang et al. (2018) proposed an online distillation framework called mutual learning where a group of student (or student) models were trained simultaneously. Although, online distillation eliminated the teacher model, similar networks in online distillation may prevent the students from learning knowledge from the students Zhang et al.. Recent approaches have attempted to induce diversity in online distillation to improve overall performance. Chen et al. (2020) proposed inducing data diversity by training student models with varying image augmentations. However, this method relies heavily on data augmentations, which can be unpredictable in real-world deployment scenarios. Du et al. (2020) introduced an adaptive ensemble knowledge distillation method using multiple diverse teacher models to train a student model. While this approach shows promise, it requires maintaining several teacher models, leading to increased memory usage and computational overhead. The reported accuracy improvements are also relatively modest, typically ranging from 0.5% to 1% across benchmarks. Our approach closely resembles to that of Zhang et al.. They present a diversity induced weight mutual learning approach for distillation. They introduce diversity by assigning varying pruning ratios to different student models. Although this method reduces memory consumption, the manual assignment of pruning ratios may not generalize well across different architectures and tasks. The reported performance gains are limited, with improvements of less than 0.5% on most benchmarks. As shown by Liu et al. (2017), while pruning induces sparsity within networks and can reduce computational complexity (measured in FLOPs), the relationship between pruning percentage and actual model size reduction is not always linear. Moreover, in Zhang et al., we observe a performance drop when pruning beyond 30%, indicating a trade-off between model compression and accuracy."}, {"title": "Diversity Induced Weighted Mutual Learning", "content": null}, {"title": "Diversifying student models", "content": "In our approach to create diverse student models for the Diversity Induced Weight Mutual Learning (DWML) framework, we employ Bayesian optimization to efficiently search for optimal architectural configurations. Given a teacher model with N parameters, we aim to generate p student models, where the i-th student model targets approximately  $N_i$  parameters, defined as:\n$N_i=\\frac{N}{i+1}, i \\in {1, 2, ..., p}$"}, {"title": "Weighted Mutual Learning using Bi-level optimisation", "content": "Building upon the work of (Zhang et al.), we introduce a modified approach to Weighted Mutual Learning using bi-level optimization. Our method replaces the pruning-based initialization with Bayesian optimization for student model selection.\nThe overall loss function for training M peer models is defined as:\n$loss = (1 \u2013 \\alpha) \\sum_{i=1}^M w_i L_{CE}(z_i, Y) + \\alpha \\sum_{i=1}^M \\sum_{j=1}^M w_j KL(z_i, z_j)$"}, {"title": "Training", "content": null}, {"title": "ROBERTa-base", "content": "Our models are based on RoBERTa-base (Liu et al., 2019). This model has shown reasonably good performance on small text corpus. We use the raw ROBERTa-base as a baseline in the evaluations. We use it as our teacher model to distill student models using knowledge distillation (KD) and the teacher supervised version of weighted deep mutual learning (KD_DWML). Details about the hyperparameters found from the search are shown in 3. The models were pre-trained (and finetuned for GLUE, SuperGLUE tasks) using 1 Nvidia H100 GPU with 80GB VRAM."}, {"title": "Dataset", "content": "We pretrain all our language models on the 10M and 100M datasets of the BabyLM challenge from 2023 (Warstadt et al., 2023). We adopt the same preprocessing pipeline from (Samuel et al., 2023)"}, {"title": "Results", "content": "This section provides the results of the empirical evaluation of DWML. First, we compare our method to baselines, then we compare our method with other distillation methods and then we perform an ablation study of different DWML variations."}, {"title": "BabyLM Challenge evaluation", "content": "We use the BabyLM evaluation pipeline to assess our models. This pipeline measures syntactic understanding through the Benchmark of Linguistic Minimal Pairs (BLiMP & BLiMP supplemental,Warstadt et al. (2020)). It evaluates general knowledge using the Elements of World Knowledge (EWoK, Ivanova et al. (2024)) benchmark. For overall natural language understanding, it uses GLUE (Wang et al. (2018)) and SuperGLUE (Wang et al. (2019). If applicable, we divide the training set into a train-development split and report the mean statistics over multiple runs on the hidden validation split.The detailed scores are shown in section D.\nBLIMP Our ROBERTa-DWML demonstrates consistent improvements over RoBERTa-base across both dataset sizes. On the 10M dataset, DWML achieves 51.6% compared to ROBERTa-base's 49.6%, showing a 2% improvement. This gain is maintained in the 100M dataset, where DWML scores 52.1% versus ROBERTa-base's 49.8%. While these improvements are modest, they demonstrate that our teacher-less approach can enhance syntactic understanding with minimal computational overhead. It's worth noting that BabyLlama's multi-teacher distillation approach (Timiryasov and Tastet, 2023) significantly outperforms all models (73.1% on 100M), though this comes at the cost of substantial computational requirements in maintaining and training with multiple teacher models (GPT-2 and LLaMA), which may not be practical for resource-constrained applications.\nBLIMP Supplemental The supplemental BLiMP results further validate the effectiveness of our DWML approach. For the 10M dataset, ROBERTa-DWML (52.3%) outperforms ROBERTa-base (48.9%) by a margin of 3.4%.\nIn the 100M setting, we observe a similar trend with DWML (48.4%) showing improvement over the base model (46.8%). These consistent gains come with minimal additional computational cost over the base model. While BabyLlama achieves substantially higher performance (60.6% on 100M), this improvement requires significant computational resources for managing multiple teacher models during training and inference, a trade-off not examined in their original work.\nEWOK On the world knowledge tasks, ROBERTa-DWML maintains competitive performance relative to ROBERTa-base. In the 10M dataset, DWML (50.3%) performs slightly below the base model (51.6%), while in the 100M dataset, DWML (51.6%) shows improvement over the base model (50.25%). These results demonstrate the capability of our lightweight approach in preserving world knowledge. While BabyLlama leads with 52.1% on the 100M dataset through its multi-teacher architecture, the relatively small performance gap (0.5%) raises questions about whether the significant computational overhead of maintaining multiple teacher models is justified for world knowledge tasks in resource-constrained environments.\nGLUE All the models were fine-tuned on the GLUE and SuperGLUE datasets and then evaluated on their linguistic performance. On the GLUE benchmark, ROBERTa-DWML shows marginal improvements over ROBERTa-base across both dataset sizes. For the 10M dataset, DWML achieves 43.1% compared to RoBERTa-base's 42.5%, representing a modest 0.6% gain. This pattern continues in the 100M setting, where DWML (44.0%) slightly outperforms the base model (43.4%). These results suggest that our teacher-less approach maintains general language understanding capabilities"}, {"title": "Comparison with Other Distillation Methods", "content": "To evaluate the effectiveness of our proposed distillation method, in Table 2 we compare its performance against other distillation techniques using accuracy scores. Our framework is compared to Self-Distillation (SD, Zhang et al. (2019)), a method that allows a small-sized student model to distill knowledge within its network. Knowledge distillation (KD,Hinton et al. (2015)) is the vanilla distillation framework that uses a student network to approximate the output logits of a pretrained teacher network. Deep mutual learning (DML,Zhang et al. (2018)) an ensemble of students learn collaboratively (without a teacher) and teach each other. The main difference between DML and our diversity induced weight mutual learning (DWML) framework is the usage of dynamically learned student weights using a bi-level optimization objective.Knowledge distillation based diversity induced weight mutual learning (KD_DWML) is the teacher-supervised version of DWML. The GPU utilization and training times are shown in Table 9 and Figure 4. They clearly show a trade-off between training times(mins) and GPU Utilization(%). While our approach DWML had the lowest GPU utilization among all, the training time was reported the highest.\nBLIMP Filtered On the BLiMP Filtered dataset, teacher-less methods demonstrate superior performance, with SD and DWML achieving 51.73% and 51.58% respectively, significantly outperforming their teacher-supervised counterparts KD (47.65%) and KD_DWML (47.47%). Among all approaches, our DWML framework shows strong performance, ranking second only to SD with a marginal difference of 0.15%. Notably, DWML substantially outperforms traditional KD by 3.93% and DML by 4.14%, validating the effectiveness of our dynamic weighting strategy in the absence of teacher supervision. Compared to the RoBERTa-base baseline (49.62%), both teacher-less methods show clear improvements, with DWML achieving a 1.96% gain, suggesting that peer learning alone can enhance syntactic understanding.\nBLIMP Supplement The BLiMP Supplement results further reinforce the advantage of teacher-less methods, with SD achieving the highest score of 56.53%. Our DWML method (52.25%) outperforms DML (45.19%) by a substantial margin of 7.06%, though it falls behind SD. While KD (55.82%) and KD_DWML (53.65%) show competitive performance, the superior performance of SD demonstrates that teacher supervision isn't necessary for strong syntactic understanding. All distillation methods except DML surpass the ROBERTa-base baseline (48.9%) by a significant margin, with our DWML showing a 3.35% improvement, further validating the effectiveness of peer learning for syntactic tasks."}, {"title": "Ablation studies", "content": "We compare the following modifications to the original DWML architecture :\n1.  Varying number of students : The effect of using different number of student networks during training.\n2.  Varying \u03b1 ratio between label and peer supervision: The effect of using different \u03b1 in equation 2 that balances KL divergence and cross-entropy loss.\n3.  Effect of dynamic student weights : Determining if learning peer weights during training affect model performance (average accuracy %)\n4.  Effect of model size : Determining if model sizes affected model performance (average accuracy %)\nEffect of Varying number of student models\nEffect of Varying Alpha\nEffect of Dynamic Relative Student Importance\nEffect of Model Size"}, {"title": "Conclusion", "content": "In this paper, we introduced Diversity Induced Weighted Mutual Learning (DWML) as an alternative to teacher-supervised knowledge distillation. While our approach showed modest improvements over the ROBERTa-base baseline, it was the simpler Self-Distillation method that achieved the strongest performance. Our ablation studies on our approach (DWML) revealed that two-peer configurations offered optimal efficiency, a balanced loss function (\u03b1 = 0.5) was crucial, and model performance correlated strongly with both dynamically learned importance weights and model size. Regarding computational efficiency, while DWML showed the lowest average GPU utilization, it required longer training times. Hence, in answering our research question about whether student knowledge sharing can match teacher-guided distillation on small datasets, we found that teacher-less methods can indeed match or exceed teacher-supervised approaches, but not necessarily through complex peer learning mechanisms. The success of simpler methods like SD suggests that the field might benefit from focusing on refined single-model approaches rather than elaborate multi-model frameworks. Future work should investigate why simpler teacher-less methods outperform more complex peer learning approaches, explore better neural architecture search techniques, and develop methods to reduce training time while maintaining low resource utilization."}]}