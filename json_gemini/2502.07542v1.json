{"title": "Exoplanet Transit Candidate Identification in TESS Full-Frame Images via a Transformer-Based Algorithm", "authors": ["Helem Salinas", "Rafael Brahm", "Greg Olmschenk", "Richard K. Barry", "Karim Pichara", "Stela Ishitani Silva", "Vladimir Araujo"], "abstract": "The Transiting Exoplanet Survey Satellite (TESS) is surveying a large fraction of the sky, generating a vast database of photometric time series data that requires thorough analysis to identify exoplanetary transit signals. Automated learning approaches have been successfully applied to identify transit signals. However, most existing methods focus on the classification and validation of candidates, while few efforts have explored new techniques for the search of candidates. To search for new exoplanet transit candidates, we propose an approach to identify exoplanet transit signals without the need for phase folding or assuming periodicity in the transit signals, such as those observed in multi-transit light curves. To achieve this, we implement a new neural network inspired by Transformers to directly process Full Frame Image (FFI) light curves to detect exoplanet transits. Transformers, originally developed for natural language processing, have recently demonstrated significant success in capturing long-range dependencies compared to previous approaches focused on sequential data. This ability allows us to employ multi-head self-attention to identify exoplanet transit signals directly from the complete light curves, combined with background and centroid time series, without requiring prior transit parameters. The network is trained to learn characteristics of the transit signal, like the dip shape, which helps distinguish planetary transits from other variability sources. Our model successfully identified 214 new planetary system candidates, including 122 multi-transit light curves, 88 single-transit and 4 multi-planet systems from TESS sectors 1-26 with a radius > 0.27 RJupiter, demonstrating its ability to detect transits regardless of their periodicity.", "sections": [{"title": "1 INTRODUCTION", "content": "The amount of astronomical data collected from both space and ground-based telescopes has increased rapidly. Given the vast amount of data, the task of analysis and examination becomes quite costly and is also prone to human error. Process automation plays an important role in helping to improve the efficiency and accuracy of detection and classification.\nDuring the last decade, more than a million stars have been observed in the search for transiting exoplanets. Among the most notable contributors is NASA's Kepler space telescope, launched in 2009 (Borucki et al. 2010). Kepler has identified nearly 4,000 potential planet candidates, of which approximately 2,700 have been confirmed as exoplanets to date. Kepler's successor, NASA's Transiting Exoplanet Survey Satellite (TESS; Ricker et al. 2014) was launched in April 2018 and is currently monitoring most of the sky searching for transiting exoplanet candidates. To date, TESS has discovered 561 confirmed exoplanets and identified thousands of additional planet candidates, significantly expanding our understanding of planetary systems beyond our solar system. TESS monitors millions of stars across approximately 90% of the sky, emphasizing the detection of exoplanets around nearby and bright stars. TESS observes each sector for ~27 days before reorienting to observe the next sector. The data products produced by the TESS mission include two types of data: small summed image subarrays (\u201cpostage stamps\") centred on pre-selected targets, also known as target pixel files (TPFs), and summed full-frame images (FFIs), which capture collections of pixels observed simultaneously (Guerrero et al. 2021). From these data products, the primary goal of TESS is to discover hundreds of transiting planets smaller than Neptune, including dozens comparable in size to Earth (Ricker et al. 2015).\nThe current standardized identification process of transiting exoplanet candidates from photometric time series involves two main steps. First, long-term flux variations are removed, and periodic transit-like signals are identified by phase-folding the light curve to different orbital periods, times of transit and transit duration (e.g. Tenenbaum et al. 2012). This process yields Threshold Crossing Events (TCEs), which include both potential exoplanets and false positives like eclipsing binaries and instrumental artifacts. The second step involves vetting TCEs by rejecting false positives based on the properties of the phase-folded transiting light curves, stellar properties, and follow-up observations. Typically, experts manually examine possible exoplanet transit signals, a labour-intensive process prone to human error and increasingly demanding due to large data volumes. To address this, several efforts have been made to automate the classification of light curves.\nMeanwhile, with the advent of Deep Learning (DL) LeCun et al. (2015), neural network (NN) architectures have revolutionized numerous fields of scientific research, leading to substantial advancements in image processing, classification, facial recognition, voice recognition, and Natural Language Processing (NLP; Krizhevsky et al. 2017; Ciregan et al. 2012; He et al. 2016; Voulodimos et al. 2018; Vaswani et al. 2017). DL techniques, particularly convolutional neural networks (CNN; Krizhevsky et al. 2012), have been notably applied to classification tasks, where classification in machine learning refers to the task of predicting the class to which the input data belongs. In particular, it was used to classify transit signals for the validation of exoplanet candidates and has also been applied in the transit search process, though to a smaller extent. In the field of exoplanet transit signal classification, 1D CNNs have been commonly used (e.g. Osborn et al. 2020; Rao et al. 2021; Tey et al. 2023). Specifically, the approaches focus on validating transit signals obtained through phase-folding light curves, which depend on prior transit parameters such as period, duration and depth derived from initial detection processes. These methods utilize both local and global views of folded light curves for distinguishing exoplanet transits from false positives, but they do not perform the initial transit search or parameter extraction themselves. Regarding the search for new exoplanet candidates, Olmschenk et al. (2021) developed a pipeline for detecting and vetting periodic transiting exoplanets. Their approach utilizes the TESS FFIs light curves and employs a 1D-CNN for candidate identification. Another approach, as demonstrated by Cui et al. (2021), employs a 2D object detection algorithm based on YOLOv3 network (Redmon & Farhadi 2018), which was trained on Kepler data to detect exoplanet signals.\nAs mentioned above, proposed DL pipelines have primarily focused on classifying transiting exoplanet signals, where transit detections are confirmed by aligning several repeated transits with the orbital period, resulting in enhanced signal detection. In contrast, few efforts have explored new techniques for discovering candidates without requiring prior transit parameters. Consequently, while current standardized pipelines for detecting transiting exoplanets have reached a high level of efficiency, they have at least two important limitations, namely: i) they require de-trending processes to remove long-term flux variations of stellar origin, which could vary the trend of the sequence, introduce artifacts into the light curve, or remove exoplanet transits; and ii) they rely on the periodicity of the transit signal in the light curve. These pipelines could be therefore missing exoplanets that present strong transit timing variations (TTV) and/or those that present just one transit in the light curve (single transiters).\nRegarding single transiters, many signals in observed light curves arise from stochastic processes-whether instrumental or astrophysical-which can often mimic single transit events (Foreman-Mackey et al. 2016). To accurately determine the orbital period of this single transit, detecting additional transits in multiple sectors is necessary, given that interruptions in the light curves typically require at least three transit events to reliably constrain the orbital period. This is because the presence of a single transit event does not provide sufficient information to confirm the periodic nature of the signal. This leads to considerable challenges that hinder a search for single transit events. For instance, the transit probability for long-period planets could be lower, particularly for those with orbital periods that exceed the duration of continuous observations (Hawthorn et al. 2024). This is also due to the fact that transit probability $P_{tr}$ is proportional to $r*/a$ and for long-period planets, the semi-major axis $a$ is larger, resulting in a lower $P_{tr}$. Despite the challenges associated with detecting single transiters, a small number of named \"monotransiters\" have been confirmed, where a single transit-like feature appears in the light curve but does not repeat (Gill et al. 2020a,b; Lendl et al. 2020).\nDue to observational biases, the vast majority of well-characterized transiting giant planets have orbital periods shorter than 10 days [e.g.](Hartman et al. 2019; Magliano et al. 2023). The existence of these planets, known as hot Jupiters, has challenged theories regarding the formation and evolution of giant planets in general (Pollack et al. 1996). To solve these open questions, it is crucial to discover and characterize giant planets with longer periods (warm Jupiters) that are not extremely irradiated by their host stars, and their physical and orbital properties can be compared to outcomes of different formation and migration processes (Dawson & Johnson 2018). While TESS has significantly increased the sample of well-characterized transiting warm Jupiters (e.g. Dawson et al. 2021; Grieves et al. 2022; Brahm et al. 2023; Battley et al. 2024), an important fraction of them might still be undiscovered because they are presented as single transiters in TESS data (e.g. Gill et al. 2020c).\nIn this work, we propose an approach to identify exoplanet transit signals within a light curve without requiring prior transit parameters. This enables us to discover new exoplanet candidates directly from the complete light curve, avoiding the initial dependence on traditional techniques that phase-fold the light curve to identify periodic transit-like signals. We propose an architecture based on a sequential model inspired by the Transformer (Vaswani et al. 2017). This type of architecture has demonstrated its effectiveness in handling complex scenarios, outperforming other model types such as recurrent neural networks (RNN; Cho et al. 2014) and CNNs, in particular for sequential data (Hawkins 2004; Lakew et al. 2018; Karita et al. 2019).\nOur architecture learns the characteristics of a planetary transit signal, regardless of whether the signal is periodic throughout the light curve. While instrumental systematics are typically removed during preprocessing, to compute light curves, stellar-origin variations remain in the data. Our NN is therefore exposed to both long-term and short-term stellar variability, including activity-related fluctuations on timescales of days. Specifically, the architecture is designed to identify and differentiate between transit signals and the variability of stars. This type of architecture has been explored and applied by Salinas et al. (2023) for classifying exoplanet transit signals and distinguishing them from false positives, and our NN extends this approach to the search for new exoplanet candidates. In particular, our architecture is designed to make predictions directly from the light curve. The NN captures temporal patterns and long-range dependencies within light curves without depending on prior transit parameters, such as transit depth, transit period and transit duration. This independence from periodicity allows our NN to detect both multi-transit and single-transit signals, making it particularly effective for identifying single transiters that have often been missed by previous approaches focused primarily on light curves with multiple transits of the same planet.\nThis article is organized as follows: Section 2 introduces the key concepts that inspired this work, where we briefly describe the Transformer encoder related to the original architecture (Vaswani et al. 2017) in order to understand our proposal for exoplanet transit signal identification. Section 3 describes the photometric data used in our work. Section 4 explains the proposed methodology in detail, including the NN training process. The model analysis and results are described in Section 5. In Section 6, we describe the exoplanet candidates found. Finally, in Section 7 we present overall conclusions and future work."}, {"title": "2 BACKGROUND THEORY OF TRANSFORMER ENCODER", "content": "Recent developments in DL have introduced new approaches to processing sequential data. Among these, the Transformer model proposed by Vaswani et al. (2017) has recently demonstrated significant success in capturing long-range dependencies compared to previous approaches focused on sequential data (Hawkins 2004; Lakew et al. 2018; Karita et al. 2019). Initially developed for Natural Language Processing, it has since been adapted to various domains, including time-series analysis. In NLP, data typically consists of discrete tokens, such as words or characters, arranged in a sequence to form sentences or documents. In astronomy, by contrast, the data often comes in the form of continuous measurements, such as light curves that represent stellar brightness over time. (In the present time-series context, for example, the term range refers to the length of the time baseline between separate elements within the data.) Both are sequential data, where the order of elements within the sequence is essential to uncovering patterns and relationships.\nThe original Transformer model consists of an encoder and a decoder (Vaswani et al. 2017). In this work, we focus on the encoder. The encoder processes the input sequence to generate a representation, which captures the essential patterns and features of the data.\nThe core of the Transformer encoder is a mechanism called self-attention, which builds upon the original attention mechanism proposed by Bahdanau et al. (2014). Self-attention enables the NN to \"attend\" to all parts of the input sequence simultaneously. This mechanism allows the NN to evaluate the importance of each element in the sequence in relation to every other element, weighing how much each data point should contribute to the final representation. It enables the NN to capture both local and long-range dependencies. Unlike traditional methods that focus on short-range dependencies, self-attention allows the Transformer to understand the full temporal structure of the data (Lakew et al. 2018; Karita et al. 2019).\nThe input sequence is represented as a series of data points, denoted as $(x_1, x_2, ..., x_n)$, where $n$ is the length of the sequence. This input is first encoded into a continuous vector space through the embedding layer, and each $x_i$ is represented as a vector $x_{emb} \\in R^d$, where $d$ is the embedding dimension. Vector embedding is a way to represent data symbols (such as words, observations, or images) in a continuous, transformed space. Embeddings can be numeric representations or generated through NNs. As shown in Figure 1(a), this representation is then combined with a positional encoding, which incorporates information about the order of the data points, capturing their relationships in the temporal dimension (Vaswani et al. 2017), such as patterns and relationships that depend on the temporal order of the data, such as periodic trends or sequential dependencies. The positional encoding is essential because the Transformer architecture, unlike RNN or CNN, does not have an inherent way to understand the order of elements in a sequence (Gehring et al. 2017). This combined representation is then passed to the encoder. Within the encoder, the self-attention mechanism captures both local and global dependencies, enabling the extraction of complex features and patterns.\nThe self-attention mechanism used within the encoder to process the input sequence, in this case a light curve. The self-attention process involves three components: queries Q, keys K and values V.  each element in the sequence generates a query, which asks how much attention it should pay to the other elements. The query is compared to keys, which represent the context, and an attention score is calculated. The score is calculated using the scaled dot-product of Q and the transpose of K. These values are then passed through a softmax function to convert the values into a probability distribution that sums to 1, while sharpening the distribution. This score determines how much focus each element should give to others.\nFinally, V, weighted by the attention scores, are then combined to produce the final output of the self-attention feature map. This feature map encapsulates the relationships across the sequence and contributes to the encoded representation $z$. In the case of multihead self-attention (MSA), multiple attention mechanisms are applied in parallel, capturing different aspects of the relationships across the sequence. The decoder then uses this encoded representation $z$ to produce outputs such as probability estimates for the presence of planetary transit signals into a light curve. This approach enables the model to detect exoplanet transit signals directly from the complete light curve without prior transit parameters even if only one transit is exhibited during the entire observation sequence.\nTransformers have shown significant potential for encoding time series data in astronomy (e.g. Allam Jr & McEwen 2021; Zerveas et al. 2021; Morvan et al. 2022). In the field of exoplanet science, Salinas et al. (2023) implemented a Transformer model specifically tailored to distinguish exoplanet transit signals from false positives, highlighting the capability of the architecture to identify and interpret the characteristics of exoplanet signals. In the context of exoplanet transit signal detection, the ability of the Transformer to model long-range dependencies is highly beneficial, as it can identify signal patterns associated with stellar variability that might otherwise be mistaken for false positives. This allows the model to accurately distinguish true planetary transits within the entire light curve, even amid the variability of the stars."}, {"title": "3 TESS DATA", "content": "For the present work, we have used light curve data obtained from the Mikulski Archive for Space Telescopes (MAST). The light curves are extracted from the TESS full-frame images (FFIs), which were captured at a cadence of 30 minutes during its primary mission. The time stamps for these measurements are given in Barycentric TESS Julian Date (BTJD) format, which is specific to the TESS mission. BTJD is defined as BTJD = BJD \u2013 2457000.0 days, where BJD refers to the Barycentric Julian Date. The use of BTJD ensures precise timing by taking into account the motion of Earth and other relativistic effects. To obtain the light curves, we use the outputs of the TESS Science Processing Operations Center (SPOC) pipeline (Caldwell et al. 2020) SPOC pipeline processes the data of targets selected from the FFIs to create target pixel and light curve files for up to 150,000 targets per sector. By selecting a specific set of target stars from these FFIS, SPOC generates the same outputs as for the pre-selected 2-min cadence targets, including calibrated target pixel files and simple aperture photometry (SAP) flux."}, {"title": "3.1 TESS-SPOC FFI light curves", "content": "For the primary mission of TESS, the FFIs are captured at a 30-minute cadence and are crucial for large-scale surveys, enabling the simultaneous monitoring of thousands of stars. To optimize the utility of FFI data, SPOC employs a selection process that prioritizes targets based mainly on their crowding metric (Bryson et al. 2020) and brightness. Specifically, targets with a crowding metric of \u2265 0.5 are selected, ensuring that at least 50% of the flux within the photometric aperture originates from the target star itself. Additionally, SPOC focuses on TESS Input Catalog (TIC) objects with TESS magnitudes of T\u2264 16, balancing the need for both bright and isolated targets to maximize the scientific return from the FFI data (Caldwell et al. 2020). Building on this, our architecture is trained with the light curves processed by SPOC from TESS sectors 1-26. Each sector was observed continuously for about 27 days, with FFIs captured at a 30-minute cadence.\nThe light curves computed by SPOC from the FFIs include the stellar flux (PDCSAP_FLUX), which has been corrected for instrumental systematics and shows fewer systematic trends. In addition to the flux, SPOC also provides detailed information on the centroid position of the target star and the background flux. The centroids are a time series that represents the pixel position of the centre of the light, which varies throughout the observation. This information allows us to determine the location of the transit within the pixels. When the centroid shifts during a transit event, we can identify whether the observed flux variations are coming from the target star or from nearby sources. The background time series represents the estimated flux contribution from nearby sources, such as nearby stars or unresolved objects, to the target aperture. It is derived from pixels located outside the photometric aperture but within the same CCD frame. This estimation aims to minimize the influence of nearby stars, though it is not always possible to completely exclude their effect, but their contribution is minimized. Both time series allow the distinction between true transits and other astrophysical phenomena that might affect the measured light curve caused by background contamination or blended stellar sources. We use these data sets; PDCSAP_FLUX, centroids, and background time series as inputs to our architecture, enhancing the accuracy and reliability of exoplanet transit detection."}, {"title": "4 METHODOLOGY", "content": "In this section, we describe our methodology, which is designed to determine the confidence level of potential transiting exoplanets within a given FFI light curve. Our work is mainly motivated by the need to detect exoplanet candidates observed by the TESS mission. To determine the confidence level, our proposed architecture considers the flux, centroid, and background time series as input of our NN. Our methodology consists of three main steps, which we describe in this section. First, in Section 4.1, we detail the data preprocessing steps used to construct our input representation, such as the time series and the data augmentation techniques employed. Section 4.2 discusses the ground truth data utilized in our analysis. Finally, Section 4.3 presents the design of our proposed NN architecture, with a description of the experimental setup and training details."}, {"title": "4.1 Data preprocessing", "content": "The preprocessing of the light curves provided by SPOC prepares them for model input through necessary transformations. In this process, we ensure that the time series are in a consistent format, allowing the NN to process and detect significant patterns within the light curves."}, {"title": "4.1.1 Flux, centroid, and background time series", "content": "Each light curve is processed to obtain a uniform length of 1000 data points, reflecting the approximate median length of single-sector light curves derived from TESS FFI data. Light curves longer than 1000 data points are truncated to this length. And, for light curves with fewer data points, we extend them by repeating the initial segment until they reach 1000 points. This standardization allows the same sequence sizes for the input for our model. Following length adjustment, we normalize the flux time series values. Specifically, we subtract the mean and divide by the standard deviation, and these values are linearly transformed to a range of -1 to 1.\nWe also incorporate two more time series. First, we included the centroid information into our input. For this, we use the CCD row and column positions of the target centroid provided in the SPOC dataset, with $x_{row}$ representing the row position and $y_{col}$ representing the position of the column. To characterize the centroid displacement, we compute its absolute magnitude as $r_{centroid} = \\sqrt{x_{row}^2 + y_{col}^2}$. Then, the centroid time series is then normalized to the range -1 to 1, following the same procedure as the flux. Finally, we include background time series from the light curves, which is also normalized in the same way as the flux and centroid time series. This ensures that the background data, along with flux and centroid information, are on a comparable scale to maintain consistency across all input features."}, {"title": "4.1.2 Augmentation of training data", "content": "Through various augmentation methods, the NN is allowed to learn more generalized features and prevent network overfitting. We applied four techniques, exclusively to the training dataset. First, we add white noise to the flux values with a standard deviation randomly chosen from a uniform distribution between 0 and the mean of the flux. Second, we use random roll augmentation, which cyclically shifts the time series data for flux, centroid, and background by a random number of positions. This shift generates variations in the data without altering the core characteristics of the signal, thus preventing the model from overfitting to specific patterns. Third, we apply random split and swap augmentation, where the time series data is split at a random index, and the resulting segments are swapped, maintaining temporal relationships while generating new variations. As fourth and last, mirror augmentation is used, which involves flipping the time series data along the time dimension. By reversing the order of data points, this technique creates a mirrored version of the original series, introducing further variations while preserving the overall patterns and relationships within the data.\nAs mentioned before, these augmentation methods are specifically applied to the training data and not to the validation or test datasets, ensuring that the model performance evaluation remains unbiased and reflective of its generalization capabilities. Specifically, the augmentation technique applied to each sample is randomly selected in every iteration, meaning that the model always sees different variations of the data. The stochastic nature of this augmentation strategy allows the model to be exposed to varying patterns and conditions, making it less likely that the model will memorize specific patterns present in the training data. This approach not only broadens the range of scenarios the model is trained on but also significantly enhances its robustness and ability to generalize to new, unseen data (Shorten & Khoshgoftaar 2019). This helps the model to perform optimally during the inference phase."}, {"title": "4.2 Ground truth data set", "content": "For our ground truth dataset, we collected samples for both positive and negative cases. We categorized the collected labels into three primary sets of light curves: known transiting planets, eclipsing binaries (EBs), and non-transit signals. The first set, representing positive cases, was drawn from the TESS Exoplanet Follow-up Program (ExoFOP-TESS) catalog, which provided labels for light curves corresponding to confirmed and known planets. This dataset was utilized for training, validation, and testing of our NN. It is important to mention that we excluded any unconfirmed planet candidates listed by ExoFOP-TESS, as many of these are later identified as false positives during follow-up observations.\nThe second set consisted of EBs collected from three main catalogues. First, we used the catalogue from Pr\u0161a et al. (2022), which provides a collection of EBs observed by TESS. Second, we incorporated false positives classified as EBs from the ExoFOP-TESS catalogue. These targets were initially identified as planetary candidates but were later reclassified as EBs following detailed follow-up observations. Finally, we added a catalogue based on ML predictions from Yu et al. (2019), which includes labelled EBs and is publicly available. EB contamination are among the most common false positives due to their periodic patterns, which are similar to true exoplanet transits. These strong negative cases were crucial for training the model to distinguish between true transits and false positives.\nThe third set consists of non-transit signals derived from a catalogue based on machine learning predictions from Yu et al. (2019). This dataset included labels for instrumental systematics (IS), stellar variability (V), and a Junk (J) class, which represents a mix of IS and V signals. With these non-transit light curves, we expanded the variety of negative cases. In addition, we included light curves labelled as false positives (FP) by the ExoFOP-TESS catalogue, ensuring that the selected false positives were not associated with EBs.\nSince each target may have been observed in multiple sectors, and exoplanet transits appear in specific sectors based on their periodicity, we filtered the light curves to include only those that captured the exoplanet transits. The same approach was applied to EBs, where we selected the light curves containing EB transits. This approach has the advantage of capturing the unique noise characteristics from each sector, as each sector introduces its own variations, which help train the model to recognize signals under different observational conditions.\nAdditionally, we trained our network using light curves artificially injected, following a process inspired by Olmschenk et al. (2021). We injected signals from one light curve into another, selected from the three primary sets of light curves described above. This involved injecting light curves containing transit signals from confirmed exoplanets into non-transit light curves. This approach allows the network to learn from a variety of signals, incorporating real stellar variability and noise conditions. To achieve this, we randomly selected a light curve from the base set and normalized it by applying a median scaling to produce a relative magnification signal. Next, we randomly sampled a non-transit light curve and scaled its values by the previously generated relative magnification signal. We repeated this process, injecting light curves from EBs into non-transit light curves. Thus, light curves injected with known or confirmed exoplanets signals were labelled as positive ground truth, while those injected with EBs signals were labeled as negative ground truth. Non-transit light curves remained as negative ground truth, ensuring they continued to represent non-planetary signals.\nOur training setup involves showing the network three types of light curves in each iteration. Specifically, we ensured that each batch contained an equal number of light curves from planets, EBs, and non-transit signals. This balanced distribution forces the network to learn and differentiate between exoplanet transit signals, EB signals, and other false positives, improving its ability to distinguish each type of signal.\nThe ground truth dataset was divided into training, validation, and testing subsets. Specifically, 80% of the dataset was designated for training, 10% for validation, and 10% for testing. The testing subset provides a final assessment to measure the model's generalization capabilities across different light curves and transit signals."}, {"title": "4.3 Neural network architecture", "content": "Our proposed architecture integrates convolutional embeddings with a Transformer encoder to process light curve data. The process begins with convolutional embedding, which operates on raw inputs such as flux, centroid, and background, extracting local features through localized patterns in the data. These embeddings are then combined with positional encodings, which inject information about the positions of data points in the sequence. Since transformer-based NN do not inherently process sequential information, positional encodings help the model understand the order of the data, enabling it to capture temporal dependencies and the relative positioning of data points (Vaswani et al. 2017). The combined sequence of embeddings is then passed into the Transformer encoder. Then, the transformer encoder outputs are passed through average pooling to condense the information, followed by a multi-layer perceptron (MLP) head, which maps the features into a classification space. This final stage produces the probability of the input light curve representing a light curve with exoplanet transit (class 1) or a light curve without exoplanet transit (class 0). The combination of convolutional embeddings for local feature extraction and Transformer encoders for global contextualization enables the model to robustly identify exoplanet signals, even when faced with complex stellar variability."}, {"title": "4.3.1 Tokens and convolutional embedding", "content": "The input time series data, including flux, centroid, and background, are first segmented into tokens, representing key features within the series. The first step for each piece of information is to generate an input sequence, for this, we generate a sequence of observations in terms of the flux, centroid and background time series, represented as $\\{(x_1, c_1, b_1), (x_2, c_2, b_2), (x_3, c_3, b_3), ..., (x_n, c_n, b_n)\\}$, where $x_i, c_i, b_i$ denote the flux, centroid, and background values, respectively, at each time step i. Here, n is the length of time steps in the sequence. Each concatenated vector is defined as:\n$x' = (x_i, c_i, b_i)$  where $x' \\in R^m$, and m is the number of variables of our time series. After concatenation, the sequence of these concatenated vectors $X = {x_1, x_2, ..., x_n}$ is the input for the convolutional operation.\nAs second step, we build the model's vector embeddings. To obtain this embedding, we apply two layers of 1D convolution to our input sequence X. Each resulting embedding vector, or token, captures essential information about the input signal. This embedding process, where each input element is processed by CNN kernels that slide across the time series, transforming each local window into a token. These tokens extract and capture local patterns and short-term dependencies within the convolution window to identify patterns of exoplanet transit signals. In this context, the CNN focuses on the time variations in brightness that occur over short intervals, allowing the model to detect the distinct shape of the exoplanet transit signal as it passes in front of its host star.\nWe denote the convolution filter coefficients as $w \\in R^{n \\times k}$, and the output of the convolution operation at position i is the embedding token $v'$ which is computed as:\n$v' = (x_i, c_i, b_i) \\otimes w_k = \\sum_k w_k \\times (x_i, c_i, b_i)$  In the first CNN layer, the kernel size k = 1 slides, transforming an m-dimensional space (representing the concatenated values of flux, centroid, and background) into a d-dimensional space at each time step t. The output of the first CNN layer is the input of a second convolution. The number of filters in the second convolution layer is set to match the model dimension d. The final embedding after the second convolution layer is expressed as:\n$X_{emb} = CNN_2(CNN_1(X))$  where, $X_{emb} \\in R^{T \\times d}$ is the embedded representation, with T being the number of tokens generated after the convolution process, X represents the input features (flux, centroid, background) and d is the dimension of the output embedding space."}, {"title": "4.3.2 Encoder", "content": "In this work we used the encoder inspired on transformer encoder proposed by Dosovitskiy et al. (2020) to capture global patterns and long-range dependencies within the time series light curves beyond the short-term correlations identified by the convolutional embedding. The Transformer encoder allows the model to weigh the importance of different time steps", "follow": "n$Z'_l = MSA(Norm(Z_{l-1"}, "Z_{l-1}, for l = 1, ..., L$  where $Z_{l-1}$ is the input to l-th encoder layer, Norm represents layer normalization, L is the number of layers of the Transformer encoder, and MSA is the MSA operation. The layer normalization, as well as the residual connections, are integral to the encoder to contribute to the stability and efficiency of the model during training (Wang et al. 2019). These techniques help prevent issues like vanishing or exploding gradients (Ba et al. 2016). In addition, the layer ensures that the inputs to the attention mechanism are consistently scaled (Dosovitskiy et al. 2020), stabilizing the distribution of activations. Consequently, the attention mechanism focuses more effectively on different parts of the light curve data, which includes complex patterns such as stellar variability. This stabilization not only improves the ability of the model to manage and interpret these transit signals but also improves training stability and convergence speed (Wang et al. 2019).\nOnce the self-attention mechanism processes the input embeddings to captures contextual relationships, each time step passes through a position-wise feed-forward network, which also includes a residual connection:\\"]}