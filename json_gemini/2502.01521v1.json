{"title": "Toward Task Generalization via Memory Augmentation in Meta-Reinforcement Learning", "authors": ["Kaixi Bao", "Chenhao Li", "Yarden As", "Andreas Krause", "Marco Hutter"], "abstract": "In reinforcement learning (RL), agents often struggle to perform well on tasks that differ from those encountered during training. This limitation presents a challenge to the broader deployment of RL in diverse and dynamic task settings. In this work, we introduce memory augmentation, a memory-based RL approach to improve task generalization. Our approach leverages task-structured augmentations to simulate plausible out-of-distribution scenarios and incorporates memory mechanisms to enable context-aware policy adaptation. Trained on a predefined set of tasks, our policy demonstrates the ability to generalize to unseen tasks through memory augmentation without requiring additional interactions with the environment. Through extensive simulation experiments and real-world hardware evaluations on legged locomotion tasks, we demonstrate that our approach achieves zero-shot generalization to unseen tasks while maintaining robust in-distribution performance and high sample efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) has made significant strides in training agents to perform complex tasks [1, 2, 3, 4, 5], achieving high performance in controlled settings. However, RL agents often struggle when faced with tasks outside their training distribution\u2014a limitation known as the out-of-distribution (OOD) generalization challenge. This limitation restricts the applicability of RL policies to a broader range of tasks beyond those explicitly encountered during training [6, 7].\nA common way to address this challenge is to increase training diversity to better encompass potential testing conditions. Domain randomization [8, 9, 10], for instance, achieves this by varying environment parameters, thereby exposing agents to a wide range of scenarios during training. However, as the extent of randomization increases, this method often suffers from reduced sample efficiency [11]. Additionally, increased variations during training can result in over-conservative strategies, especially in environments where these variations are not fully observable. In such settings, the agent may struggle to discern the current task parameterizations and only executes conservative moves, thereby sacrificing optimality. This raises a crucial question: how can an RL agent achieve robust performance across diverse environment variations, or tasks, without being explicitly exposed to them during training?\nA promising method to improve the generalization of the policy is through experience augmentation [12], where training experiences are augmented to simulate a broader range of potential test-time conditions. However, pure augmentation, without properly accounting for task context, may similarly cause over-conservativeness resulting from exposing the policy to excessive unobservable variations. Building on the contextual modeling capability of Recurrent Neural Networks (RNNs), which have been shown to effectively address partial observability [13], we enhance experience augmentation in RL by incorporating memory mechanisms that respect the necessary task context, as shown in Fig. 1.\nIn this work, we introduce memory augmentation, a novel approach to improve task generalization by augmenting training experience to simulate plausible OOD tasks and incorporating memory mechanisms to facilitate zero-shot task adaptation. Our approach trains a single unified policy that can perform well across both in-distribution (ID) and unseen OOD tasks. In summary, our contributions include:\n\u2022\tWe propose memory augmentation, a novel approach to improve task generalization without additional interactions with the environment.\n\u2022\tWe validate our approach across a variety of legged locomotion experiments. Results demonstrate that our memory-augmented policies generalize effectively to unseen OOD tasks while maintaining robust ID performance."}, {"title": "II. RELATED WORK", "content": "A. Task generalization in RL\nIn RL, task generalization arises as a challenge when the agent is required to perform well on tasks that differ from those encountered during training. These differences may stem from variations in scenes, states, or reward structures [14]. Domain randomization, originally introduced for sim-to-real transfer in robotics [8, 9, 10], addresses this issue by randomizing environment parameters during training to encompass a broader range of potential test-time conditions. In robotic control, this often involves dynamics randomization, where parameters like masses, friction coefficients, joint damping, and actuator gains are varied during training to enhance policy robustness [10, 1, 2, 3, 15]. While this effectively broadens the agent's exposure to diverse conditions, increased randomization often leads to reduced sample efficiency [11].\nAlternatively, many works incorporate known physics princi-ples or state structures into task design to approach task general-ization. Examples include foot-placement dynamics [16], object invariance [17], granular media interactions [18], frequency domain parameterization [7], rigid body dynamics [19], and semi-structured Lagrangian dynamics models [20].\nB. Experience augmentation\nWhile many data augmentation techniques in RL have focused on augmenting image-based observations to improve generalization across visual changes [21, 22, 23, 24], fewer studies have investigated augmentation at the induced state-action trajectory level-referred to as experience augmenta-tion-which is the focus of our work.\nOne example of experience augmentation is PlayVirtual [25], which augments cycle-consistent virtual trajectories by leveraging forward and backward dynamics models in a latent space to improve feature representation learning. Meanwhile, in HER [12], which aims to address the sparse reward problem in RL, failed trajectories are transformed into meaningful learning signals through goal relabeling. This relabeling mechanism, as implemented in HER and its extensions [26, 27, 28, 29], can also be viewed as a form of experience augmentation. However, these approaches primarily focus on improving learning efficiency with limited training data rather than explicitly addressing task generalization.\nIn legged locomotion control, symmetry-based augmentation has been applied to state-action trajectories to encourage symmetry-invariant behavior [30, 31]. Inspired by these works, our approach leverages the inherent task structure to augment state-action trajectories, generating synthetic experience of novel scenarios to enhance task generalization.\nC. Meta-RL\nMeta-RL enables agents to quickly adapt to new tasks by leveraging experience gained during meta-training. Through exposure to multiple related tasks, the agent learns a context-aware policy [32, 33, 34, 35] or a good initialization [36, 37, 38], which allows it to adapt to novel, unseen tasks with minimal additional training [39]. In the field of memory-based meta-RL, RNNs have been widely employed for their ability to capture task context from past interactions, enabling zero-shot adaptation [32, 33]. Building on this framework, Nagabandi et al. [40] developed a dynamics model prior to enable rapid online adaptation in dynamic environments, while Zargarbashi et al. [41] trained a morphology-agnostic locomotion policy for legged robots."}, {"title": "III. PRELIMINARIES", "content": "A. Partially observable Markov Decision Process and Meta-RL\nWe model the problem as a Partially Observable Markov Decision Process (POMDP). We define a POMDP as the tuple M = (S, O, A, p, r, \u03b3, \u03c1o), where S is the set of states, O is the set of observations, A is the set of actions, p: S\u00d7A\u00d7S \u2192 [0, 1] is the state transition function, r : S \u00d7 A \u2192 R is the reward function, y is the discount factor, and \u03c1o : S \u2192 [0, 1] is the initial state distribution.\nIn POMDPs, observations provide only partial information about the true underlying state, requiring the agent to reason about the task context. In conventional RL, the agent aims to learn a policy \u03c0 that maximizes the expected cumulative reward:\n$J(\u03c0) = \u0395\u03c4~\u03c1\u03c0 (\u03c4) \\sum_{t=0}^\\infty y^tr (St, at)$,\nwhere T = ($0, ao, $1,...) represents a trajectory sampled from the POMDP M under the policy \u03c0.\nIn this work, we frame the problem of learning across multiple tasks as a meta-RL problem, with the objective of optimizing over a distribution of tasks p(T). In our setting, this corresponds to a distribution of POMDPs M(T). We focus on the scenario where each task T\u2208 T exhibits unique dynamics while sharing a common reward function. The goal is to learn a policy \u03c0 that maximizes the expected cumulative reward across tasks:\n$J(\u03c0) = \u0395T~p(T), T~p (T/T) \\sum_{t=0}^\\infty y^tr (St, at)$\nB. Proximal Policy Optimization\nIn this work, we apply Proximal Policy Optimization (PPO) [42] as the RL algorithm. At the k-th learning step, the agent collects trajectories by executing its policy, parameterized by \u03b8k. To optimize the policy, PPO employs importance sampling to address the discrepancy between the current policy \u03c0\u03b8k(at St) and the policy from the previous update \u03c0\u03b8k\u22121 (at | St) used to collect the data. The policy gradient is expressed as:"}, {"title": "IV. APPROACH", "content": "A. Task-structured augmentation\nTo augment the training experience, our approach leverages a set of transformation functions, denoted by G, where each g \u2208 G represents a transformation that reflects the underlying structure that we assume to exist in the task space. These transformations are used to generate synthetic experience in Taug from the original data collected in TID.\nFormally, let \u03c4 = {(ot, at, ot+1, rt)}t=0T represent a trajectory collected by the current policy \u03c0\u03b8 within the training set TID. Each transformation g = (go,ga) \u2208 G consists of functions go: O \u2192 O and ga: A \u2192 A, which map an original observation-action pair (o, a) to a new pair (og, ag) = (go(o), ga(a)). We denote s9t as the latent state corresponding to the transformed observation of. For all g\u2208 G, s\u2208 S, a \u2208 A, it is assumed that the following properties hold:\n(a) Transition probability invariance: The probability of reaching the next state from a given state-action pair remains identical in their respective tasks. Formally, this is expressed as:\nT(s9t+1|s9t, a9t) = T(st+1|st, at).\n(b) Initial state distribution invariance: The initial state distribution remains invariant across original and transformed trajectories. Formally, this is expressed as:\n\u03c1o(s9) = \u03c1o(s).\n(c) Reward invariance: Each original state-action pair (st, at) and its transformed counterpart (s9t, a9t) receive the same rewards in their respective tasks. Formally, this is expressed as:\nr(s9t, a9t) = r(st, at).\nThese properties ensure alignment between the original and transformed tasks, maintaining the coherence of task dynamics. The resulting augmented trajectory \u03c4g = {(o9t, a9t, o9t+1, rt)}t=0T represents the agent's simulated experience within an augmented task T9\u2208 Taug, generated without real environment interactions. The training dataset is then constructed as the union of the original and augmented trajectories:\nDtrain = DID \u222a Daug = {\u03c4}1Nminibatch \u222a \u222ag\u2208G{\u03c4g}1Nminibatch\nAn example of such transformations in G includes symmetry transformations, which have been used in RL for manipulation tasks [27] and legged locomotion control [30, 31]. Specifically, for quadrupedal robots, symmetry transformations can simulate mirrored conditions (e.g., joint failures) across left-right and front-back body parts. Similarly, for humanoid robots, they can replicate mirrored conditions across left-right body parts. These transformations leverage the morphological symmetry inherent to legged robots and uphold the essential invariance properties (a)-(c) [44].\nB. Training with augmented data\nFollowing the method of Mittal et al. [30], we approach data augmentation in PPO by constructing augmented policies such that \u03c0\u03b8(a|s9) = \u03c0\u03b8 (a|s), \u2200g \u2208 G, s \u2208 S, a \u2208 A. Based on properties (a)-(c), it can be shown that for a state-action pair (s, a) and its transformed counterpart (sg, a9), the following invariances hold [30]:\nA\u03c0\u03b8k (s9, ag) = A\u03c0\u03b8k (s, a), and \u03c1\u03c0\u03b8k (s9) = \u03c1\u03c0\u03b8k (s). (2)\nUsing Eq. 2, the policy update rule in Eq. 1 becomes\n$\u2207\u03b8J(\u03c0\u03b8) = \u0395\u03c4\u223c\u03c1\u03c0\u03b8k \u2211t=0\u221e\u03b7t(\u03b8)A\u03c0\u03b8k \u2207\u03b8 log \u03c0\u03b8(at|st) + \u2211g\u2208G\u0395\u03c4\u223c\u03c1\u03c0\u03b8k \u2211t=0\u221e\u03b7t(\u03b8)A\u03c0\u03b8k \u2207\u03b8 log \u03c0\u03b8(ag|sg), (3)$\nwith $\u03b7t(\u03b8) = \\frac{\u03c1\u03c0\u03b8k(st)\u03c0\u03b8k(at|st)}{\u03c1\u03c0\u03b8k(st)\u03c0\u03b8k(at|st)}$, which retains the action probability of the original data. For further details, we refer readers to [30]."}, {"title": "V. EXPERIMENTS", "content": "A. Environment and task setup\nWe evaluate the performance of our approach across eight legged locomotion experiments with the ANYmal D quadruped and Unitree G1 humanoid robots in Issac Lab [45]. The experiments are designed around two core task settings: position tracking and velocity tracking, with variations introduced to simulate joint failures and payload distributions:\n\u2022\tJoint failure: A joint failure is simulated by disabling the motor torque.\nQuadruped robot: During training, a joint in the left front leg (LF HAA, LF HFE, or LF KFE) is randomly disabled, categorized as an ID joint failure. Joint failures in other legs are considered OOD failures.\nHumanoid robot: A joint in the left hip (pitch, roll, or yaw) is randomly disabled during training (ID). Joint failures in the right hip are considered as OOD.\nFor both robots, a joint failure occurs with an 80% probability during training.\n\u2022\tPayload: A payload is simulated by imposing a vertical external force.\nQuadruped Robot: During training, a payload with a mass uniformly sampled from [30kg, 40kg] is applied at a random position on the left front section of the robot's base (ID). Payloads with the same mass range applied to other sections of the base are considered OOD.\nHumanoid Robot: During training, a payload with a mass uniformly sampled from [40 kg, 50 kg] is applied at a random position on the left shoulder of the robot (ID). Payloads applied to the right shoulder are considered OOD.\nWe train a memory-based policy both with and without experience augmentation, referred to as Memory-Aug and Memory-ID, respectively. To assess the role of memory-based task inference, we also train a standard MLP policy with experience augmentation [30, 31], referred to as Baseline-Aug, and a standard MLP policy without augmentation, referred to as Baseline-ID.\nAdditionally, we include randomization-based policies, where the agent is exposed to all variations mentioned above (ID + OOD) during training. These are referred to as Baseline-Rand for the MLP-only architecture and Memory-Rand for the memory-enabled architecture.\nWe include all implementation details in the Appendix, including observations, rewards, network architectures and hyperparameter values.\nB. Task performance and data efficiency\nWe evaluate performance using the average episodic return, calculated as the mean of undiscounted cumulative rewards across 1000 episodes. Training for each policy was conducted using five different seeds. Figure 4 presents detailed evaluations of different policies in quadruped position tracking under both ID and OOD joint failure scenarios.\nAs shown in Fig. 4b, Baseline-Aug exhibits ID performance comparable to Baseline-Rand, but falls short of matching the ID performance of Baseline-ID (see Fig. 4a). These findings underscore how variations introduced through randomization or augmentation can cause a simple MLP policy to adopt suboptimal behaviors in partially observable settings. Lacking the ability to infer latent task context from partial observations, such policies resort to over-conservative strategies, limiting their ability to optimize performance in specific tasks.\nIn contrast, Memory-Aug achieves performance comparable to Memory-ID on ID tasks, as shown in Fig. 4a. This result highlights how the memory module effectively captures task context from partial observations and utilizes this capability in experience augmentation, thereby preserving robust ID performance.\nIn Fig. 4b, Memory-Aug demonstrates performance on par with Memory-Rand in both ID and OOD tasks. Notably, Memory-Aug achieves this without requiring any additional environment interactions for these OOD tasks, demonstrating significantly higher sample efficiency than Memory-Rand.\nTo verify that these findings are not coincidental, we evaluate the approaches across all experiments. As shown in Fig. 5, the trend of ID performance degradation in Baseline-Rand and Baseline-Aug persists, particularly in experiments involving the quadruped robot, where task diversity is more pronounced. In contrast, Memory-Rand and Memory-Aug consistently achieve comparable ID performance to Memory-ID. This again highlights the importance of the memory mechanism in partially observable settings with substantial task diversity. By leveraging its context inference capability, this memory module allows the agent to maintain robust performance in ID tasks. Furthermore, Memory-Aug achieves performance comparable to Memory-Rand in both ID and OOD tasks. However, unlike Memory-Rand, Memory-Aug eliminates the need for additional environment interactions, effectively improving sample efficiency.\nC. Behavior analysis with memory augmentation\nWe observe distinct strategies adopted by different policies for quadruped position tracking with impaired joint, particularly in response to HFE and KFE joint failures. Agents trained with Baseline-Rand and Baseline-Aug often attempt to walk directly toward the goal using very small steps, as shown in the left part of Fig. 6. This over-cautious behavior reflects a tendency to uniformly reduce reliance on all joints rather than identifying and compensating for the specific failed joint. Despite their careful movements, the joint impairment often leads to stumbling or passive change in body orientation during tracking.\nIn contrast, agents trained with Baseline-ID and Memory-ID demonstrate a more adaptive strategy under ID joint failure. They tend to first turn sideways toward the goal, then move laterally while dragging the impaired leg to minimize reliance on the failed joint (see upper row of Fig. 6). This intentional body orientation adjustment at the start enables them to maintain greater stability while effectively progressing toward the goal. Notably, agents trained with Memory-Rand and Memory-Aug exhibit similar adaptive behavior, as shown in the middle and bottom rows on the right side of Fig. 6.\nTo further support these findings, we conduct a quantitative analysis of feet air time, calculated as the total duration the feet remain off the ground, and feet contact frequency, measured as the total number of instances the feet make contact with the ground. These metrics provide insight into how effectively the agents adapted their motion to compensate for joint failures. As illustrated in Fig. 7, in ID tasks, Baseline-Rand and Baseline-Aug demonstrate significantly shorter feet air time and more frequent ground contact compared to Baseline-ID. This aligns with our observation that agents employing these policies tend to take smaller, more cautious steps during goal tracking. This conservative behavior persists in OOD tasks, as evidenced by the low feet air time and high contact frequency of Baseline-Rand and Baseline-Aug under OOD joint failures.\nIn contrast, Memory-Rand and Memory-Aug exhibit feet air time and contact frequency comparable to Memory-ID in ID tasks, while maintaining consistent levels in OOD tasks. This highlights the ability of Memory-Rand and Memory-Aug to effectively adapt their motion strategies in response to both ID and OOD joint failures.\nWe attribute this context-aware behavior to the memory-based task inference capability provided by the RNN. By leveraging past experience, the RNN enables the agent to infer the underlying task context from partial observations, including the presence and nature of joint failures. This capability enables the agent to dynamically adapt its behavior across diverse scenarios, adopting strategies that compensate for impairments and optimize task performance."}, {"title": "VI. HARDWARE EXPERIMENTS", "content": "We tested our Memory-Aug policy in position tracking under joint failure on the ANYmal D robot. To facilitate sim-to-real transfer, we introduced randomization in terrain profiles with a noise range of [0.0, 0.05] during training.\nAs shown in Figure 8 and the supplementary video, the robot successfully tracked multiple goals consecutively, while adapting effectively to both ID and OOD joint failures. These results demonstrate the ID robustness and OOD generalization of the memory-augmented policy on real robots.\nAdditionally, we also tested Baseline-Aug policy trained under the same terrain profiles. As shown in Figure 9, the robot exhibited limited adaptability under ID joint failure, frequently losing balance during goal tracking."}, {"title": "VII. LIMITATIONS", "content": "The main limitation of our work lies in the reliance on prior knowledge of the task structure, which is essential for augmenting the training experience to simulate diverse task conditions. This dependency restricts the selection of transformations to those that satisfy the necessary invariance assumptions, potentially limiting generalization in cases where such assumptions are difficult to establish. That said, this reliance on prior knowledge also offers the advantage of systematically utilizing our understanding of the system's dynamics. By incorporating domain-specific insights, we can design targeted transformations that align with the inherent properties of the task. In the future, we aim to broaden the applicability of our method by exploring more flexible augmentation techniques to simulate a wider range of OOD task conditions and enhance generalization across diverse and unstructured environments."}, {"title": "VIII. CONCLUSION", "content": "In this work, we have presented an RL framework that integrates task-structured experience augmentation with memory mechanisms to enhance task generalization. By simulating varied task conditions through augmented training experiences and incorporating memory-based task inference, our approach trains a single unified policy capable of handling both ID and OOD tasks without additional environment interactions.\nThis framework effectively addresses the limitations of policies lacking context-awareness, which tend to adopt over-conservative strategies in highly randomized and partially observable environments. Through extensive simulation experiments, we demonstrate that our approach achieves zero-shot generalization to OOD tasks while maintaining robust ID performance.\nFurthermore, our approach demonstrates performance comparable to domain randomization in both ID and OOD tasks, while the latter requiring explicit environment interactions for these tasks. Notably, our method achieves this without requiring additional environment interactions for the OOD tasks, thereby overcoming the sample inefficiency associated with domain randomization.\nCrucially, the memory mechanism proves essential in partially observable settings with substantial task diversity, allowing the agent to infer task context from past interactions. This facilitates zero-shot task adaptation and ensures reliable performance across both ID and OOD tasks. Moreover, our hardware experiments on a quadruped robot validate the sim-to-real transfer, demonstrating that our policy performs well on both ID and OOD tasks in real-world scenarios.\nWe believe this work inspires further advancements in developing adaptive RL agents capable of handling the complexities and uncertainties of real-world environments."}, {"title": "APPENDIX", "content": "A. Implementation details\nOur experiments focus on two primary task settings:\n\u2022\tPosition tracking: The robot's objective is to track a target position, sampled uniformly in polar coordinates with a radius between 1 m and 5 m around its initial position (i.e., r ~ U([1.0, 5.0]), \u03b8 ~ U([\u2212\u03c0, \u03c0])). The target position is dynamically converted to Cartesian coordinates xcmd = [xcmd, ycmd] \u2208 R2, relative to the robot's base frame based on its current position. This relative target position is provided as input to the policy, which outputs the target joint position q*. The episode length is set to 8 s.\n\u2022\tVelocity tracking: The robot's objective is to track a velocity command, vcmd = [vxcmd, vycmd, \u03c9zcmd] \u2208 R3, where vxcmd and vycmd represent the target forward and lateral linear velocities, and \u03c9zcmd denotes the target yaw angular velocity. At the start of each episode, the velocity commands are sampled as follows:\nANYmal quadruped robot: The forward and lateral linear velocities are sampled uniformly between -1.0 and 1.0 m/s:\nvxcmd ~ U([\u22121.0, 1.0]), vycmd ~ U ([\u22121.0, 1.0]).\nG1 humanoid robot: The forward linear velocity is sampled uniformly between 0.0 and 1.0 m/s:\nvxcmd ~ U([0.0, 1.0]),\nwhile the lateral velocity is fixed at zero:\nvycmd = 0.0.\nA target heading direction is sampled uniformly from [-\u03c0,\u03c0]. The yaw command, \u03c9zcmd, is dynamically computed based on the target and current heading. The policy is trained to output the target joint position q*.\nB. Evaluation details\nThe feet air time metric, shown in Fig. 6, is calculated as the total duration the feet remain off the ground, expressed as:\n$\u2211_{j=1}^{Nfeet}(tair, j \u2013 0.5) - 1_{new-contact, j}$\nThe contact frequency measures the total number of times the feet make contact with the ground and is given by:\n$\u2211_{j=1}^{Nfeet}1_{new-contact,j}$"}]}