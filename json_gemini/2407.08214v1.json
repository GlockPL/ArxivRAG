{"title": "TOWARDS STABLE TRAINING OF PARALLEL CONTINUAL LEARNING", "authors": ["Yuepan Li", "Fan Lyu", "Yuyang Li", "Wei Feng", "Guangcan Liu", "Fanhua Shang"], "abstract": "Parallel Continual Learning (PCL) tasks investigate the training methods for continual learning with multi-source input, where data from different tasks are learned as they arrive. PCL offers high training efficiency and is well-suited for complex multi-source data systems, such as autonomous vehicles equipped with multiple sensors. However, at any time, multiple tasks need to be trained simultaneously, leading to severe training instability in PCL. This instability manifests during both forward and backward propagation, where features are entangled and gradients are conflict. This paper introduces Stable Parallel Continual Learning (SPCL), a novel approach that enhances the training stability of PCL for both forward and backward propagation. For the forward propagation, we apply Doubly-block Toeplit (DBT) Matrix based orthogonality constraints to network parameters to ensure stable and consistent propagation. For the backward propagation, we employ orthogonal decomposition for gradient management stabilizes backpropagation and mitigates gradient conflicts across tasks. By optimizing gradients by ensuring orthogonality and minimizing the condition number, SPCL effectively stabilizing the gradient descent in complex optimization tasks. Experimental results demonstrate that SPCL outperforms state-of-the-art methjods and achieve better training stability.", "sections": [{"title": "1 Introduction", "content": "In the era of Internet of Things (IoT) and embodied artificial intelligence, effectively utilizing multi-source data to train multi-task models becomes important. For example in applications including autonomous driving Shaheen et al. [2022] and monitoring systems Doshi and Yilmaz [2020], they rely on the perception from multiple sensors to improve the decision. When multi-source data become dynamic over long periods, experiencing increments due to external factors, model training becomes dynamic as well. This learning task is referred to as Parallel Continual Learning (PCL) Lyu et al. [2023]. In contrast to traditional Continual Learning (CL) Parisi et al. [2019], De Lange et al. [2021], which requires models to learn new tasks one by one, PCL allows multi-task training at any time. PCL can continuously adapt to new environmental variables or changing conditions without requiring system downtime or pending.\nIn traditional CL, catastrophic forgetting Kirkpatrick et al. [2017] happens when a model learns new information and unintentionally erases previous knowledge. More than catastrophic forgetting, PCL faces significant training conflict challenge Lyu et al. [2023]. Training conflicts arise because different tasks inherently exhibit significant differences and varied training progress. This discrepancy often leads to conflicting multi-task gradients, resulting in unstable model updates. To solve the challenges, Lyu et al. Lyu et al. [2023] propose MaxDO, which uses asymmetric gradient distance metric and minimizes gradient discrepancies among tasks, addressing instability and interference issues in feature extractors. However, while MaxDO reduces training conflicts and catastrophic forgetting, its performance can still be susceptible to the variations in task relevance and the scale of gradient changes across different tasks, which may destabilize the training process."}, {"title": "2 Related Work", "content": "CL and PCL. CL enables models to continuously learn new information while retaining old knowledge, crucial for intelligent systems in dynamic environments Parisi et al. [2019]. A primary challenge of CL is catastrophic forgetting Kirkpatrick et al. [2017], where CL models lose previous knowledge when learning new tasks, highlighting traditional machine learning's limitations in multitasking and adaptive scenarios. To address catastrophic forgetting, key strategies in existing research include regularization-based methods, rehearsal-based methods, and architecture-based methods De Lange et al. [2021]. Regularization methods Kirkpatrick et al. [2017], Aljundi et al. [2018], Chaudhry et al. [2018b], Du et al. [2022], like Elastic Weight Consolidation (EWC) Kirkpatrick et al. [2017], incorporate constraints within the loss function to limit forgetting. Replay methods Lopez-Paz and Ranzato [2017], Shah et al. [2018], Sun et al. [2022], Liu et al. [2023], such as Gradient Episodic Memory (GEM) Lopez-Paz and Ranzato [2017], retain old memories by preserving old data or creating surrogate data. Architecture-based methods Rusu et al. [2016], Yoon et al. [2017] allocate distinct parameters to each task to prevent interference. Traditional CL typically focuses on sequentially learning tasks, which may lead to significant delays in integrating new information for multi-source tasks. The PCL task addresses this issue. Lyu et al. Lyu et al. [2023] proposed measuring asymmetric distances between gradients to obtain optimal update. However, the gradients obtained by this method neglect training stability during both forward and backward propagation.\nWeight Orthogonal Regularization. Incorporating orthogonality constraints in CNNs has proven to enhance their performance. In practice, orthogonal regularization improves CNN performance in tasks like image recognition, object detection, and semantic segmentation Vu et al. [2019]. Early studies indicates Saxe et al. [2013] that applying orthogonal initialization at the start of training significantly boosts efficiency and overall network performance. Existing research shows that applying orthogonality constraints during neural network training can significantly enhance model performance and stability Xie et al. [2017], Huang et al. [2018], Bansal et al. [2018], Lezcano-Casado and Martinez-Rubio [2019], Wang et al. [2020]. Two kinds of orthogonality constraints Achour et al. [2022] are well-studied. Hard orthogonality Jia et al. [2017], Huang et al. [2018], which keeps weight matrices strictly orthogonal throughout training using techniques like Stiefel manifold representation. Soft orthogonality Balestriero et al. [2018], Bansal et al. [2018], Balestriero and Baraniuk [2020], which adds a regularization term $\\||WW^T \u2013 I\\||^2$ to the loss function. Kernel orthogonality is also important in CNNs Brock et al. [2018], which ensures that each convolutional kernel, treated as a matrix, has linearly independent rows or columns, while convolutional layer orthogonality views the entire layer as a composite operation that maps input to output feature space, maintaining orthogonality between the two."}, {"title": "3 Stable Training in PCL", "content": "3.1 Preliminary: Parallel Continual Learning\nIn PCL, T tasks are represented by parallel data streams $D_1,\\cdots, D_T$, each consisting of data tuples (x, y) with x as the input and y as a one-hot encoded label. Unlike traditional CL, which processes tasks sequentially, PCL allows for simultaneous access to all tasks with no fixed training endpoint. This requires rapid model adaptation while preserving knowledge in diverse tasks. The PCL framework employs a task-neutral backbone with parameters $\\theta$, along with a growing set of task-specific classifiers $\\Theta_i$ for each task. Training in PCL is a dynamic Multi-Objective Optimization (MOO) challenge. At any time, the number of objectives depends on the number of intraining tasks, and PCL aims at concurrently minimizing empirical risks across tasks by optimizing both shared $\\theta$ and task-specific ${\\Theta_i | i \\in T_t}$ parameters through task-specific loss functions $l_i$,\n$\\min_{\\theta,\\{\\Theta_i|i \\in T_t\\}} \\{l_i(D_i) | i \\in T_t\\},$   (1)\nwhere $T_t$ is the set of tasks that are active at a given time t. The update for the shared parameters $\\theta$ is influenced by gradients from all currently active tasks. Thus, at any time of PCL training, we have multiple gradients {$g_1,\\ldots,g_{|T_t|}$} for multiple tasks. PCL navigates a complex optimization landscape filled with local minima and saddle points, which can destabilize updates to $\\theta$. Additionally, managing conflicts and dominance among multiple task gradients further complicates gradient stability across tasks Yu et al. [2020]. This complexity, combined with the continuous task shifts, poses significant challenges in balancing multiple learning objectives. Following Lyu et al. [2023], our method employs replay based techniques in continual learning, periodically reintroducing old data to prevent catastrophic forgetting while accommodating new information.\n3.2 Stable Training for PCL in perspective of orthogonality\nIn PCL, tasks have unique data distributions, causing training instabilities from gradient interference and conflicting parameter updates. This may worsen gradient vanishing or exploding if layer outputs are too small or large, disrupting training and hindering robust feature learning. To address this challenges, it is crucial to maintain consistent input-output relationships, ensuring smooth transmission of feature signals and preserving the key representations. Moreover,"}, {"title": "4 Method", "content": "4.1 Overview\nIn this paper, we explore the orthogonalization in PCL forward and backward propagation. During forward propagation, overlapping or entangled features may cause conflicting activations. In the context of backward propagation, the gradient updates from different tasks may conflict. On top of this, we present a dual-strategy approach termed SPCL for PCL. First, we apply Doubly-block Toeplit (DBT) matrix based orthogonality constraints to network parameters to ensure stable and consistent forward propagation. Second, we employ orthogonal decomposition for gradient management stabilizes backpropagation and mitigates gradient conflicts across tasks. Both strategies use soft orthogonal regularization under the Frobenius norm, adding minimal computational overhead while enhancing training stability in PCL environments. These methods collectively enhance PCL model performance by maintaining effective learning across tasks, leading to improved accuracy trends for both tasks, as demonstrated in Fig. 1(d).\n4.2 Doubly-block Toeplit Matrix based Orthogonal Regularization Method\nFirst, we propose to improve the training stability in the forward propagation. As mentioned in Sec. 3.2, the linear transformation between layers in CNN is computed as Y = K * X, where X and Y represent the input and output of each layer, respectively, and K denotes the kernel. To enhance computational efficiency, we utilize a DBT matrix representation of the kernel (see more details in Appendix A.2). From the insights provided in Wang et al. [2020], we define the conditions for orthogonality, which is crucial for the orthogonal properties of convolution layers. These conditions are derived from the results of self-convolution computations:\n$Conv (K, K, padding = P, stride = S) = I_o, $  (3)\nwhere $I_o \\in R^{N \\times N \\times (2P/S+1) \\times (2P/S+1)}$ is the identity matrix at the centre and zeros entries elsewhere and k is the kernel size. These conditions ensure that the filters within a layer and across layers remain orthogonal, reducing redundancy and improving feature diversity."}, {"title": "4.3 Structured Orthogonal Regularization Optimization", "content": "In Sec. 3.2, we discuss that the condition number of G ($\\kappa \\geq 1$) in the linear system g = GW influences the training stability of the gradient descent process in multi-task learning. The condition number, a measure of sensitivity to changes in input, directly affects training stability. Specifically, a high condition number in the gradient matrix G indicates potential instability due to disproportionate scaling of gradient contributions during optimization Senushkin et al. [2023]. The condition number is defined as:\nDefinition 1 (Condition number of gradient system). The condition number of the gradient matrix, formed from the gradients of the shared parameters:\n$\\kappa(G) = \\frac{\\sigma_{max}(G_e)}{\\sigma_{min}(G_e)},$ where $G_e=[g_{new,1},\\ldots, g_{new,n}, g_{old,1}, \\ldots, g_{old,m}].$   (5)\nThe condition number describes how the relative scale of the largest and smallest singular values (represented by $\\sigma_{max}$ and $\\sigma_{min}$, respectively) can influence the effectiveness of gradient updates. When the condition number of the gradients is constrained to 1 Senushkin et al. [2023], we have\n$\\|g_i\\|||g_j\\| = 0$ and $\\|g_i\\|||g_i\\| = \\|g_j\\|||g_j\\| = I.$   (6)\nThis means any two gradients are orthogonal.\nIn PCL scenario, differing gradients across tasks may cause dominance and conflict issues. Constraining the condition number of the gradient system G = [$g_{new,1},\\ldots, g_{new,n},g_{old,1},\\ldots,g_{old,m}$] to 1 risks normalizing all gradients to the same scale, which may stifle learning by reducing natural variations. Our approach maintains mutual orthogonality and a low condition number while preserving the diversity of gradient directions. This strategy ensures stability and accommodates the unique contributions of each task, effectively reducing conflicts and dominance within the gradient system.\nThus, we focus on the following optimization problem, where $\\lambda$ is the hyperparameter constraining the condition number of G:\n$\\min_{\\hat{G},\\kappa(\\hat{G})} \\|G - \\hat{G}\\| + \\lambda \\kappa(\\hat{G}).$   (7)\nThe scenario in Eq. (6) exemplifies this optimization target, where $\\kappa(\\hat{G}) = 1$ means achieving the minimal condition number for the gradient system and indicative of an optimal state. This situation corresponds to a Procrustes problem Sch\u00f6nemann [1966], offering a closed-form solution discussed in detail in Appendix A.4. The solution can be derived using Singular Value Decomposition (SVD):\n$\\hat{G} = UV^T$, where $U,V : G = U\\Sigma V^T.$   (8)"}, {"title": "4.4 Algorithm", "content": "The SPCL approach outlined in Alg. 2 utilizes advanced gradient and kernel orthogonalization methods to improve learning stability in neural networks. For each task and at each timestep, the network computes gradients for both task-specific loss and orthogonal regularization loss. These gradients are then adjusted for orthogonality using the method from Sec. 4.3 before updating the model parameters. In the process of computing $\\hat{G}$ in Alg. 2, the original constrained optimization problem Eq. (7), is relaxed into an unconstrained optimization problem Eq. (9) using the penalty function method. This method introduces a penalty term $\\|\\hat{G}^T\\hat{G} \u2013 diag(d)\\|$ to the objective function to handle constraints effectively.\n$\\min_{\\hat{G},d} \\|G \u2013 \\hat{G}\\| + \\alpha ||\\hat{G}^T\\hat{G} \u2013 diag(d)||^2 + \\beta ||d \u2013 1||^2.$   (9)"}, {"title": "5 Experiment", "content": "5.1 Dataset\nFollowing Lyu et al. [2023], we evaluate on three PCL datasets in both task-incremental and class-incremental learning scenarios.\nParallel Split EMNIST (PS-EMNIST), which includes 62 classes and is divided into 5 tasks. Each task is assigned three random label sets, with three timelines per label set, resulting in 9 different configs, which is same as the other two datasets.\nParallel Split CIFAR-100 (PS-CIFAR-100), which includes 100 classes, is divided into 20 tasks.\nParallel Split ImageNet-TINY (PS-ImageNet-TINY) includes 200 classes, is divided into 20 tasks.\n5.2 Experiment details\nWe set the batch size uniformly at 128 across all datasets, employing a 2-layer MLP for PS-EMNIST and a Resnet-18 He et al. [2016] for PS-CIFAR-100 and PS-ImageNet-Tiny. Our rehearsal strategy maintains varying samples per class: 30 for PS-CIFAR-100, 5 for PS-EMNIST, and 15 for PS-ImageNet-Tiny. For evaluating PCL, we measure average accuracy and forgetting, following methodologies from previous studies such as Lopez-Paz and Ranzato [2017], Chaudhry et al. [2018a]. With $e_t = max(e_1,e_2,\\ldots, e_T)$, we calculate:\n$A_e = \\frac{1}{T}\\sum_{t=1}^{T}a_t,$ and $F_e = \\frac{1}{T}\\sum_{t=1}^{T} (a_{e_t} - a_{e_t}^{wt}).$   (10)\nHere, $A_e$ signifies the overall average accuracy across tasks, while $F_e$, or backward transfer, indicates the drop in performance relative to the initial training of each task. All models are deployed with tensorflow and the experiments are conducted on an RTX 4090 GPU.\n5.3 Main Results\nIn our study, we benchmark SPCL against various MTL approaches such as MGDA Sener and Koltun [2018], DWA Liu et al. [2019], GradDrop Chen et al. [2020], PCGrad Yu et al. [2020], and RLW Lin et al. [2021] within a PCL framework. We treat each moment as a distinct MTL segment to facilitate PCL training, implementing each MTL method according to specifications from their respective publications. Additionally, we draw comparisons with rehearsal-based SCL methods like AGEM Chaudhry et al. [2018a], GMED Jin et al. [2021], and ER Chaudhry et al. [2019b]. To align with SCL strategies, we combine batches from all active tasks, simulating a straightforward sequential learning setup where only batches from the current task and a memory buffer are used. This methodological adaptation allows us to assess the efficacy of SCL methods in a PCL context effectively. We show the main comparisons with the proposed methods in Tables 1 and 2 on the three datasets. We have several major observations. Our methods shows significant improvements over others in both task-incremental and class-incremental PCL scenarios, especially on the PS-CIFAR-100 and PS-ImageNet-TINY dataset. Compared to Structured Orthogonal Regularization Optimization(SORO), which only orthogonalizes gradients during PCL model training, incorporating orthogonality constraints on CNN filters further enhances model performance. The combination of these two methods provides more stable training."}, {"title": "5.4 SPCL learning process", "content": "Thge ablation study on the CIFAR-10 dataset is shown in Fig. 3, which validates the efficacy of orthogonal regularization in managing task interference in PCL models. By applying orthogonal constraints either on the gradients, the filter in CNN, or both, the model shows varied capability in preserving learning stability and improving task-specific accuracy over training epochs. The introduction of Task 2 is a critical moment, as indicated by the dashed line, where the model's ability to adapt to new tasks while retaining performance on ongoing tasks is evaluated. The results show the orthogonalization enhances the adaptiveness and stability of continual learning systems."}, {"title": "5.5 Numerical analysis", "content": "We also compare the effects of introducing orthogonality constraints via a penalty function in the convolutional layers of CNNs. We contrast the condition number distribution, norm distribution, and filter redundancy of convolutional layers with and without the DBT-based orthogonalization method. In Fig. 5, experimental results indicate that employing DBT-based orthogonalization improves the stability of convolutional layer parameters. Feature redundancy presents a"}, {"title": "6 Conclusion", "content": "In conclusion, this paper systematically investigates the benefits of orthogonalization in both forward and backward propagation within the context of PCL. Through our studies, we have identified that entangled features during forward propagation can result in conflicting activations, whereas gradient updates from concurrent tasks often lead to conflicts in backward propagation. To address these challenges, we introduce a dual-strategy approach named SPCL, which incorporates orthogonality at two stages of the learning process. The first strategy in our approach applies DBT Matrix-based orthogonality constraints directly to the network's filters, ensuring stable and consistent forward propagation. The second strategy employs orthogonal decomposition to manage gradients, effectively stabilizing backward propagation and reducing conflicts among task-specific gradients. Both strategies utilize soft orthogonal regularization to optimize the balance between computational efficiency and robust training dynamics, enhancing the overall effectiveness of the learning process. However, a limitation of our approach is the need for precise tuning of the orthogonality regularization parameter, which is sensitive to task characteristics. Incorrect settings may lead to excessive or insufficient orthogonalization, impacting model adaptability and increasing computational time due to convergence difficulties. Future researches should explore adaptive methods for dynamically adjusting this parameter to improve model flexibility and efficacy in response to task complexities and data variations."}, {"title": "A Appendix", "content": "A.1 Overview of Learning Paradigms: MTL, SCL, and PCL\nAs shown in Fig. 6, Multi-Task Learning (MTL), Serial Continual Learning (SCL, or traditional CL), and Parallel Continual Learning (PCL) are three paradigms addressing the challenges of learning from multiple tasks. MTL simultaneously trains on multiple tasks to leverage shared knowledge but faces task conflicts. SCL sequentially trains tasks to avoid simultaneous complexity, but struggles with catastrophic forgetting, where new learning can erase previous knowledge. PCL combines these approaches, allowing for simultaneous training of new tasks as they arise, aiming to dynamically manage both task conflict and catastrophic forgetting, thus fitting more realistic and varied learning scenarios.\nA.2 Doubly-block Toeplitz Matrix\nThe linear transformation between layers in CNN is computed as Y = K * X. Alternatively, the kernel can be expanded using a faster doubly-block Toeplitz (DBT) matrix Wang et al. [2020]. DBT regularization enhances convolutional neural networks by structuring convolution as a DBT matrix-vector multiplication, which reduces storage requirements from $O(n^2)$ to O(n) and computational complexity from $O(n^2)$ to O(nlogn) Osahor and Nasrabadi [2022].\nA Toeplitz matrix is characterized by constant elements along each descending diagonal from left to right. A DBT matrix extends this concept by being a blocked Toeplitz matrix where each block is itself a Toeplitz matrix. Let vec(X) denote as flatten X:\n$vec(X) = [X_0;\\ldots;X_{H-1}]^T$   (12)\nthen, the convolution operation can be rewritten as:\n$vec(Y) = M \\cdot vec(X)$   (13)\nwhere M is a DBT matrix is formatted as $R^{O \\times C \\times H'W' \\times HW}$.\n$M = \\begin{bmatrix}\nM_{1,1} & M_{1,2} & \\ldots & M_{1,C} \\\\\nM_{2,1} & M_{2,2} & \\ldots & M_{2,C} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\nM_{N,1} & M_{N,2} & \\ldots & M_{N,C}\n\\end{bmatrix}$   (14)\nwhere each $M_{i,j} \\in R^{H'W' \\times HW}$ is the i-th row and j-th column block matrix and we denote $Tope(w_n)$ as a Toeplitz matrix generated by n-th row vector of convolution kernel w:\n$M_{i,j} = \\begin{bmatrix}\nTope(w_0) & & & Tope(w_{k-1}) \\\\\n& Tope(w_0) & & \\\\\n& & \\ldots & \\\\\nTope(w_0) & \\ldots & Tope(w_{k-1}) \\\\\n\\end{bmatrix}$   (15)\nA.3 Preservation of Orthogonality in Linear Transformations.\nThe preservation of orthogonality in linear transformations is first discussed in Huang et al. [2020]:\nTheorem 2. (Preservation of Orthogonality in Linear Transformations.) Let $\\hat{h} = Wx$, where $W^TW = I$ and $W \\in R^{n \\times d}$. Assume: (1) $E_x(x) = 0$, $cov(x) = \\sigma I$, and (2) $E_o(\\frac{\\partial l}{\\partial \\hat{h}}) = 0$, $cov(\\frac{\\partial l}{\\partial \\hat{h}}) = \\sigma \\frac{2}{n}I$. If n = d, we have the following properties: (1) $\\|\\hat{h}\\| = \\|x\\|$; (2) $E_x(\\hat{h}) = 0$, $cov(\\hat{h}) = \\sigma I$; (3) $\\|\\frac{\\partial l}{\\partial x}\\| = \\|\\frac{\\partial l}{\\partial \\hat{h}}\\|$ ; (4) $E_o(\\frac{\\partial^2 l}{\\partial x^2}) = 0$, $cov(\\frac{\\partial^2 l}{\\partial x^2}) = \\sigma \\frac{2}{n}I$. In particular, if n < d, property (2) and (3) hold; if n > d, property (1) and (4) hold.\nA.4 Optimization Goal\nGiven a matrix $G \\in R^{n \\times t}$ representing the gradients of different tasks in a multi-task learning framework, where t is the number of tasks and $t<n$. We expect to solve the following optimization problem:\n$\\min_{G} \\|G - \\hat{G}\\|^2, s.t. \\hat{G}^T\\hat{G} = I$   (16)"}]}