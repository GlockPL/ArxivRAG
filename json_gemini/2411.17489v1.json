{"title": "Puzzle Similarity: A Perceptually-guided No-Reference Metric for Artifact\nDetection in 3D Scene Reconstructions", "authors": ["Nicolai Hermann", "Jorge Condor", "Piotr Didyk"], "abstract": "Modern reconstruction techniques can effectively model\ncomplex 3D scenes from sparse 2D views. However, au-\ntomatically assessing the quality of novel views and identi-\nfying artifacts is challenging due to the lack of ground truth\nimages and the limitations of no-reference image metrics in\npredicting detailed artifact maps. The absence of such qual-\nity metrics hinders accurate predictions of the quality of\ngenerated views and limits the adoption of post-processing\ntechniques, such as inpainting, to enhance reconstruction\nquality. In this work, we propose a new no-reference met-\nric, Puzzle Similarity, which is designed to localize artifacts\nin novel views. Our approach utilizes image patch statistics\nfrom the input views to establish a scene-specific distribu-\ntion that is later used to identify poorly reconstructed re-\ngions in the novel views. We test and evaluate our method in\nthe context of 3D reconstruction; to this end, we collected a\nnovel dataset of human quality assessment in unseen recon-\nstructed views. Through this dataset, we demonstrate that\nour method can not only successfully localize artifacts in\nnovel views, correlating with human assessment, but do so\nwithout direct references. Surprisingly, our metric outper-\nforms both no-reference metrics and popular full-reference\nimage metrics. We can leverage our new metric to enhance\napplications like automatic image restoration, guided ac-\nquisition, or 3D reconstruction from sparse inputs.", "sections": [{"title": "1. Introduction", "content": "Image-based rendering and 3D reconstruction from a sparse\nset of 2D views has received ample attention in recent years,\nboth for pure geometry reconstruction and radiance-field\nmodeling. Classical approaches using simple triangulation\nand epipolar geometry through methods such as structure\nfrom motion (SfM) to produce sparse point-clouds of dif-\nfuse color [28]. Densifying these representations can be\ndone explicitly [13]. Alternatively, one can learn continu-\nous, implicit representations [3, 22, 25], normally modeled\nthrough some kind of multi-layer perceptron (MLP). A tan-\ngential problem to these efforts is the collection of 2D data,\nand the handling of corrupted, distorted, or simply incom-\nplete sets of images from an object or scene we would like\nto model. Learning representations from very sparse inputs\nhas been a widely studied topic as well [4, 5, 38, 44]; they\nnormally leverage learned priors on large datasets, help-\ning to fill in the gaps by enforcing 3D consistency and\nthat resulting reconstructions follow natural image statis-\ntics. However, quantifying the quality of novel views from\nthese reconstructions is still problematic. These views can\ncontain artifacts due to the sparsity of the dataset their\nmodel was constructed from, and automatically identify-\ning them helps with restoration (e.g. masking for image-\nbased inpainters [32]) or simply to guide future data acqui-\nsition to fill the gaps [15]. Recent works have followed a\nbayesian approach to quantification of the uncertainty of an\narea belonging or not to a reconstructed model or scene [7],\nwhich could potentially be leveraged for simple artifact de-\ntection; however, they require implicit representations of the\nscene, with fundamental changes to the scene model, and\nare not practical for more general applications that require\nvisual artifact identification outside of scene reconstruction,\nas well as incapable of detecting artifacts not arising from\nlack of coverage.\nTo tackle this, we propose a novel approach for artifact\ndetection that can be leveraged on any set of images without\nan encoded explicit or implicit model of the scene or ob-\nject they depict. As opposed to visual difference predictors\n(VDPs) [18] (which require references) and no-reference\nquality metrics [23, 24] (which typically do not provide\nmaps, but rather produce single values of overall quality)\nour approach provides visual artifact maps with no direct\nreferences. We leverage learned perceptual patch statistics\nfrom small clean datasets and compare them to the embded-"}, {"title": "2. Related Work", "content": "3D reconstruction and Image-based rendering Recon-\nstructing 3D objects or scenes from sparse sets of 2D ob-\nservations is a fundamental problem in vision [18]. Partic-\nularly, in the context of novel view synthesis, the objective\nis to approximate the radiance field (i.e. 5D function en-\ncoding spatially varying radiance emission) of specific ob-\njects or scenes. Most methods however, differ either on the\nmodel used to encode the function, or the rendering pro-\ncedure. Implicit approaches model the radiance field as a\ncontinuous function, approximated by a multi-layered per-\nceptron (MLP) [22, 31]. Rendering is usually done via sam-\npling the implicit volume using ray-marching [36], which\nprovides spatially varying values of density and anisotropic\ncolor emission modeled through Spherical Harmonics. Im-\nprovements over this formula have tackled performance\nlimitations, either by using more efficient sampling tech-"}, {"title": "3. Our Method", "content": "Let us establish an analogy for our method: pretend each\nreference image is a puzzle with many puzzle pieces. To test\nif a new image is similar to our references, we would simply\nshuffle all pieces from all puzzles from our references and\ntry to reassemble the test image only using those pieces. If\nthe new image is very similar to the references, we should\nhave enough puzzle pieces to compose the other image con-\nfidently. However, if the image holds regions very different\nfrom what we saw in the reference images, we would lack\npuzzle pieces to assemble this area, effectively leaving holes\nin the newly assembled puzzle (image). An overview of our\napproach through this analogy can be seen in Figure 1.\nIn our work, the puzzle pieces correspond to embedded\nimage patches. In order to assess patch similarity, an obvi-\nous approach would involve computing the dot product be-\ntween all patches; best-matching pieces would be recorded\nto create a similarity map. This simplistic approach, how-\never, would hardly align with human assessment. Inspired\nby the close correlation between human quality judgment\nand latent CNN feature maps [33, 49], we employ a pre-\ntrained convolutional network [10, 16, 30] to embed all the\nreferences, computing similarity in the latent feature space.\nNote that comparing feature map \"pixels\" in a convolutional\nnetwork is similar to comparing individual patches in the in-\nput domain; this is due to the locality of the sliding kernels\nwhen convolving. The patch size is dependent on the recep-\ntive field (showcased in Figure 2).\nChoosing the right layers for embedding\nis essential to maximize the quality of the predicted spa-\ntial maps. While early layers feature small receptive fields\nand capture fine details, deeper layers have larger receptive\nfields and capture coarser features. This can be observed\nin Figure 3, where we showcase different VGG layers. It\nis essentially a trade-off between prediction granularity, ac-\ncuracy and speed. We identified that combining multiple\nlayers into our metric computation incorporates the various\nlevels of abstraction and scales in a robust manner. We thus\ncompute the weighted average of the three layers; we em-\npirically found that halving the image resolution more than\nthree times did not significantly improve our results as the\nscale becomes too small and the pool of reference vectors\ntoo little and specific to find good correspondences among"}, {"title": "4. Results", "content": "We will now analyze how our method stacks against com-"}, {"title": "5. Application: Progressive Automatic Artifact\nInpainting", "content": "Finally, we will showcase a possible application of our met-\nric in automatic restoration of novel images from a recon-\nstructed scene.\nWhenever it is possible to establish a visual distribution\n(e.g. we have a training dataset available), we can recur-\nsively use our metric to automatically identify visual out-\nliers in novel views and remove them through inpainting.\nWe can take a new image I and employ\nour PuzzleSim metric to obtain the quality map Q.\nQ = PuzzleSim(1) \u2208 RH1\u00d7W1\n(6)\nTo apply neural inpainting, we first need to create a bi-\nnary mask from the quality map Q, indicating the areas to\nbe inpainted. This involves finding an optimal threshold\nT that clearly distinguishes artifact regions. The effective-\nness of inpainting depends on setting this mask carefully. If\nthe mask is too large, the inpainting may inadvertently re-\nmove clean parts of the scene. If the mask is too small, arti-\nfacts might be left untouched. In order to automatically find\na balanced threshold, we use a conservative, iterative ap-\nproach to refine the test image based on the assumption that\nartifacts have below-average quality scores. For an initial\nthreshold, we select N = 50 candidate values, uniformly\nspaced between the lowest and mean quality scores that we\nuse to threshold the quality map.\n\nTi = min(Q) +\nmin(Q)) (7)\nwith i, h, and w representing indices where i = 0, . . ., N \u2013\n1, h = 1,..., H\u2081, and w = 1,...,W1. We generate in-\npaintings using all N masks and recompute PuzzleSim for\neach option. The quality of each inpainted candidate is eval-\nuated by calculating the average quality difference within\nthe inpainted region, denoted as \u03b4\u03b9. To discourage overly\nlarge masks, we add a regularization term that penalizes\nthem. Further details on the mathematical definitions of \u03b4;\nand the regularization term are provided in the Supplemen-\ntary material.\nWe then select the candidate that maximizes d. After de-\ntermining the initial threshold, we iteratively refine the in-\npainted image by drawing new thresholds close to the pre-\nvious one. For each new threshold, we repeat the selection"}, {"title": "6. Limitations and Future Work", "content": "While our method demonstrates promising results, there are\nseveral limitations to consider. Finding the maximum sim-\nilarity with many other vectors becomes expensive as the\nnumber of reference images and image resolution rises. Per-\nforming approximate maximum search or fitting Gaussian\nmixture models in the embedding space can improve com-\nputational performance [11, 48]. Furthermore, our metric\nis empirically calibrated, but choosing the weights to com-\nbine layers and weighting in the channel dimension in a\ndata-driven manner could advance the metric further. The\nresolution at which our metric can be utilized is currently\nlimited by the CNN backbone's generalizability to higher\nresolutions. Although it is differentiable, it is unlikely to\nproduce valuable gradients due to the max operation across\nmany vectors. The next step would be to explore softmax\nalternatives to make the metric more suitable for gradient-\nbased optimization."}, {"title": "7. Conclusion", "content": "In this work, we have introduced Puzzle Similarity, a no-\nreference visual quality metric designed to detect and local-\nize artifacts in novel views generated by 3D scene recon-\nstruction methods. By leveraging learned patch statistics\nfrom input views, our method can generate spatial artifact\nmaps without needing ground-truth references, which is a\nsignificant advantage for evaluating reconstructed scenes.\nFurthermore, we have provided a novel dataset of human-\nassessed quality and artifact detection specifically tailored\nfor 3D scene reconstruction approaches.\nOur evaluation demonstrates that Puzzle Similarity out-\nperforms traditional full-reference metrics, such as SSIM\nand L2 norms, as well as spatial quality no-reference met-\nrics, like PIQE, in capturing artifacts that align with hu-\nman perception. Compared to sophisticated visual differ-\nence predictors like FovVideoVDP, which leverage explicit\nfitted models of low-level human vision, Puzzle Similarity\nachieves comparable or superior performance in complex\ntexture-rich scenes, proving its robustness across a range of\nartifact types and scenarios, while relinquishing the neces-\nsity of direct references.\nAdditionally, We demonstrate our metric applied to the\nproblem of automatic artifact inpainting, highlighting its\npotential for enhancing quality in scene reconstructions.\nOverall, Puzzle Similarity offers an effective solution for\nartifact localization in 3D reconstruction, paving the way\nfor more perceptually aligned, reference-free quality assess-"}]}