{"title": "ONEGEN: EFFICIENT ONE-PASS UNIFIED GENERATION AND RETRIEVAL FOR LLMS", "authors": ["Jintian Zhang", "Cheng Peng", "Mengshu Sun", "Xiang Chen", "Lei Liang", "Zhiqiang Zhang", "Jun Zhou", "Huajun Chen", "Ningyu Zhang"], "abstract": "Despite the recent advancements in Large Language Models (LLMs), which have significantly enhanced the generative capabilities for various NLP tasks, LLMS still face limitations in directly handling retrieval tasks. However, many practical applications demand the seamless integration of both retrieval and generation. This paper introduces a novel and efficient One-pass Generation and retrieval framework (OneGen), designed to improve LLMs' performance on tasks that require both generation and retrieval. The proposed framework bridges the traditionally separate training approaches for generation and retrieval by incorporating retrieval tokens generated autoregressively. This enables a single LLM to handle both tasks simultaneously in a unified forward pass. We conduct experiments on two distinct types of composite tasks, RAG and Entity Linking, to validate the pluggability, effectiveness, and efficiency of OneGen in training and inference. Furthermore, our results show that integrating generation and retrieval within the same context preserves the generative capabilities of LLMs while improving retrieval performance. To the best of our knowledge, OneGen is the first to enable LLMs to conduct vector retrieval during the generation.", "sections": [{"title": "1 INTRODUCTION", "content": "In the era of Large Language Models (LLMs), many Natural Language Processing (NLP) tasks can be reduced to generation, allowing them to be addressed by a single LLM (Zhao et al., 2023; Qin et al., 2023; OpenAI, 2023; Zeng et al., 2024). While LLMs excel in language generation, they still suffer from hallucinations (e.g., factual inaccuracies), stemming from their exclusive reliance on the parametric knowledge they contain (Zhang et al., 2023b; Yao et al., 2023; Tonmoy et al., 2024).\nOne promising approach is Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Jiang et al., 2023d; Asai et al., 2024; Mao et al., 2024; Gao et al., 2023), which augments the input by retrieving relevant passages based on the query either before or during generation. Other methods (Ding et al., 2024a; Luo et al., 2023a) anchor LLM generation to an external knowledge base through Entity Linking (EL) during or after generation. These systems typically rely on a retriever at various stages of generation. However, due to the separate training paradigms for generation and retrieval, most prior work Muennighoff et al. (2024) employs a separate model for text embedding. However, this pipeline approach has several drawbacks: i) Deploying and maintaining two separate models introduces additional hardware overhead and increases maintenance costs. ii) The separation of models creates two distinct representational spaces, limiting interaction between the retriever and generator (e.g., LLM) to text (i.e., query). As a result, whether the query is generated by the LLM or input directly by the user, it requires an additional forward pass through the retriever, increasing inference computational costs. iii) In multi-turn dialogues, as illustrated in Figure 1(a), query rewriting is required for follow-up questions like \u201cWho is his wife?\". This rewriting adds inference overhead and risks error propagation if inaccurate. iv) Additionally, the pipeline approach is difficult to optimize end-to-end and requires large amounts of training data, while end-to-end optimization has been shown to yield significant benefits (Lin et al., 2024).\nOur work introduces an efficient One-pass unified Generation and retrieval (OneGen) framework to enable an arbitrary LLM to generate and retrieve in one single forward pass. Inspired by the latest success in LLM for text embedding (Wang et al., 2024), we expand the original vocabulary by adding special tokens (i.e. retrieval tokens) and allocate the retrieval task to retrieval tokens generated in an autoregressive manner. During training, retrieval tokens only participate in representation fine-tuning through contrastive learning (van den Oord et al., 2018; Rendle et al., 2009), whereas other output tokens are trained using language model objectives. At inference time, we use retrieval tokens for efficient retrieving on demand.\nUnlike previous pipeline approaches, which require at least two models for retrieval and generation (as shown in Figure 1(b)), OneGen unifies both tasks into a single model, eliminating the need for a separate retriever. Muennighoff et al. (2024) presents Generative Representational Instruction Tuning (GRIT), which aligns with this approach by training one LLM to handle both generative and embedding tasks through different prompts and attention mechanisms, as depicted by the \"switch\u201d in Figure 1(c). However, GRIT still necessitates independent forward passes for generation and retrieval tasks, reducing efficiency for tasks that intertwine generation and retrieval.\nWe evaluate the effectiveness of our method on two main tasks that require both generation and retrieval: RAG (including single-hop QA which needs single-retrieval and multi-hop QA which needs multi-retrieval) and Entity Linking (EL). Empirical results show OneGen outperforms the previous pipeline solutions as well as GRIT where applicable. Specifically, OneGen achieves +1.5pt improvement on average with four Single-hop QA datasets on top of Self-RAG (Asai et al., 2024), +3.3pt F1 on average with two Multi-hop QA datasets under three different 7B-based LLMs, and +3.2pt accuracy on average with 6 out-of-domain entity linking datasets, with less training data. Moreover, further analysis demonstrates OneGen can enhance retrieval capability when jointly trained, with no sacrifice in generation capability. In addition, we demonstrate superior inference speed and memory consumption of OneGen compared with other LLM alternatives, particularly as retrieval frequency increases. In summary, our work makes the following contributions:\ni) We propose a training-efficiency, inference-efficiency, and pluggable framework OneGen that is particularly suitable for tasks interleaved with generation and retrieval. ii) Our model, fine-tuned on less training data, demonstrates superior performance on six RAG datasets and six entity linking datasets on average. iii) We demonstrate the efficiency of OneGen at inference, highlighting a significant speed improvement as the length of query increases or retrieval frequency increases, compared to other LLM alternatives. iv) From the perspective of methodology, OneGen is an extension of Generative Instruction Tuning (GIT) and Representative Instruction Tuning (RIT) (as shown in Figure 1(b)). v) We contribute to communities by releasing our dataset as well as code.\""}, {"title": "2 PRELIMINARIES AND RELATED WORKS", "content": "Most text-based tasks can be reduced to generation, retrieval, or combination of the two. We first introduce several hybrid tasks and their common solutions in \u00a7 2.1. Then, we introduce the three roles of tokens in LLMs in \u00a7 2.2. Finally, we further explain the motivation of our method in \u00a7 2.3."}, {"title": "2.1 GENERATION & RETRIEVAL", "content": "For NLP problem related to generation or retrieval, a user input or a query $u = {u_1, ..., u_n}$ and optionally document corpus $K = {d_i}$ are given (e.g., wiki articles), the end goal of the task is to generate sequence output $y = {Y_1, ..., Y_m}$ or the most relevant documents $\\epsilon$ from $K$ with respect to u or both. We also assume that each $d_i \\in \\epsilon$ is aligned to a subsequence or a whole sequence of tokens in u. We summarize the steps and typical input, output for generation, retrieval, and two hybrid tasks in Table 1.\nR\u2192 G Task leverages retrieval results to drive generation. In the simplest format, a dense retrieval model (e.g., a dense passage retriever, DPR) is used to retrieve a collection of relevant documents $\\epsilon$ given user input $u$ at t=1; $\\epsilon$ are then used as additional context when generating the target sequence using a generator (e.g. LLM) at t=2. Retrieval Argumented Generation (RAG) is a classic example of R \u2192 G task. Though there are some efforts in training the two model end-to-end predate the LLM era (Lewis et al., 2020), most recent work use an off-the-shelf-retriever such as Contriever (Izacard et al., 2022), BM25, or search engine (Jiang et al., 2023d). Furthermore, this task can involve multiple iterations of retrieval and generation, such as in multi-hop reasoning datasets like 2WIKI (Ho et al., 2020) and HotpotQA (Yang et al., 2018).\nG\u2192 R Task outputs retrieved documents relevant to user query in addition to generated content and are widely encountered in Information Retrieval (IR). A prominent example task is Entity Linking (EL), which involves locating mentions and disambiguating these surface forms into entities in some Knowledge Base (KB). Early EL methods (Hoffmann et al., 2011) treat EL as decomposed subtasks, such as Mention Detection (MD) and Entity Disambiguation (ED), and solve them in sequence. More recent works manage to frame EL as an end-to-end task, such as sequence generation (Cao et al., 2021b), question answering (Zhang et al., 2022), retrieve augmented generation (Xiao et al., 2023), and sequence tagging problem (Broscheit, 2019; Ayoola et al., 2022), which outperform the early pipeline approach. For the generative EL paradigm, MD can be modeled as a generation task where entities in the original sentences are generated; ED is a typical retrieval task of retrieving the most relevant entity from the KB given a mention span."}, {"title": "2.2 ROLES OF TOKENS IN LLMS", "content": "A token $x_i$ is the basic unit processed by an LLM. Token in the input of an LLM serves three different roles: 1) generating the next token, noted as $role(x_i) = GEN$; 2) providing context information, noted as $role(x_i) = CTX$; and 3) representing a sentence, noted as $role(x_i) = RET$. Recent works (Wang et al., 2024; Muennighoff et al., 2024) use the hidden state of the last token as the sentence representation."}, {"title": "2.3 MOTIVATION", "content": "Recent years have seen a rise in using LLMs to handle complex hybrid tasks, replacing traditional NLP model pipelines. Before LLMs, end-to-end approaches offered advantages for combining generation and retrieval tasks, reducing error propagation compared to pipelines and potentially improving efficiency with single-pass inference. However, earlier solutions are often task-specific and lack generalization across hybrid tasks. For instance, in generative EL, methods like constrained decoding (Cao et al., 2021b) are used to retrieve entities efficiently. Our work addresses the absence of a unified LLM framework for hybrid tasks, stemming from separate training approaches for generation and retrieval tasks, which typically use distinct objectives and datasets."}, {"title": "3 ONEGEN: ONE-PASS GENERATION AND RETRIEVAL FOR LLMS", "content": "We introduce a One-pass Generation and retrieval framework (OneGen) for fine-tuning LLMs on generation, retrieval, or hybrid tasks, as shown in Figure 2. Our core idea is to integrate generation and retrieval to the same context by allocating the retrieval task to retrieval tokens generated in an autoregressive manner, thus enabling LLM to perform both tasks in a single forward pass."}, {"title": "3.1 OVERVIEW", "content": "Notation. To ensure clarity and precision in our subsequent discussions, we standardize the notation used in Table 1. Define the dataset $\\mathcal{D} = {s_i}_{i=1}^{|D|}$, which consists of |D| sentences s of varying lengths, with each sentence $s = {x_{i,j}}_{j=1}^{|s|}$ comprising |s| tokens x. Let $x_{i,j}$ denote the j-th token of the i-th sentence in the dataset D, and define $x_{i,<j}$ as ${x_{i,1}, x_{i,2},..., x_{i,j}}$. We can distinguish the symbols u, y, and d defined in Table 1 based on the role of tokens x within the sentence s. Specifically, y corresponds to the segment of the s where $role(x_i) = GEN$, u corresponds to the segment where $role(x_i) = CTX$, and if all tokens x in a sentence s have $role(x_i) = CTX$, then it corresponds to d. Given the instruction dataset $\\mathcal{I}$, where $s = {u, y} \\in I$, we have $\\mathcal{D} = I \\cup K$.\nDesign. Retrieval requires encoding both the query and the document within the same representational space. Our core idea is to incorporate query encoding into the generation process. Thus we use the same LLM for encoding both the query and the document, without altering the model structure, such as the attention mechanism, unlike the approach taken by GritLM. Specifically, for query encoding, we introduce a special token $x_i = [RQ]$, where $role(x_i) = RET$. This token is generated by the LLM and used as input to represent the query. However, assigning $role(x_i) = RET$ prevents the generation of the next token $x_{i+1}$ if $role(x_{i+1}) = GEN$. To address this, we also introduce a <CON> token during data reconstruction, ensuring the continuation of the generation process.\nInference. At inference time, the documents to be retrieved are encoded offline by the trained LLM using the template \u201c{document}[RD]\", where $role([RD]) = RET$. Then the trained LLM autoregressively generates tokens based on the user's input until it encounters the [RQ] token. The logits corresponding to the [RQ] token are then used for retrieval. Depending on the task requirements, the retrieved content may be concatenated with the context, potentially along with the <CON> token, before continuing with the inference until the generation is complete.\""}, {"title": "3.2 TRAIN", "content": "Data Reconstruction. We augment the standard generation output with retrieval tokens wherever retrieval is needed. This makes our framework easily pluggable to existing methods. Generally, we insert [RQ] to sentence s for query representation. In particular, if the query span is explicit, we add optional tokens <LOC> and </LOC> to assist in locating the position of the query. The augmented sequence is $s = {x_{<i}, <LOC>,x_{i+1},...,x_j,</LOC>,[RQ],<CON>,x_{(>j)}}$. The token <CON> enables LLMs to generate continuously and it must be included if and only if $role(x_{j+1}) = GEN$, i.e., generation is required after retrieval. For each document x \u2208 K, [RD] is usually appended to the end of the document to represent the document. In particular, we can add [RD] for each end of the sentence in a document x \u2208 K to get the fine-grained representation. Figure 8 and Figure 11 in appendix show two different examples for reconstructed document."}, {"title": "Training Objective", "content": "The optimization is only performed for tokens $x_i \\in s$ where $role(x_i) \\in {GEN, RET}$. A simple application of OneGen in the RAG task is illustrated in Figure 2. Note that, $role(x_i) = RET$ iff $x_i \\in {[RQ], [RD]}$ (highlight in purple in Figure 2). For tokens where $role(x_i) = GEN$ (highlight in orange), optimization employs $\\mathcal{L}_g$:\n$\\mathcal{L}_g = \\frac{1}{|D|} \\sum_{i=1}^{D} \\frac{1}{|S_i|} \\sum_{j=1}^{S_i} \\mathcal{l}g(f_\\theta(x_{(i,j)}, \\text{Head})) \\cdot \\mathcal{I}g(i,j)$.\nHere, \u03b8 is the LLM parameter. $T_\\text{Head} \\in \\mathbb{R}^{N \\times d}$ denotes the expanded vocabulary (i.e., LM Head), consists of N d-dimensional vectors. $f_\\theta(x_{(i, j)}) \\in \\mathbb{R}^d$ denotes a d-dimensional vector generated by the LLM without LM Head from processing the first to the j-th token. $l_g$ typically represents the cross-entropy loss function, and $\\mathcal{I}_g(x_{i,j})$ is an indicator function, where $\\mathcal{I}_g(x_{i,j}) = 1$ iff $role(x_{i,j}) = GEN$; otherwise, it is 0. For tokens where $role(x_i) = RET$, optimization employs $\\mathcal{L}_r$:\n$\\mathcal{L}_r = \\frac{1}{|D|} \\sum_{i=1}^{D} \\frac{1}{|S_i|} \\sum_{j=1}^{S_i} \\mathcal{l}r (f_\\theta(x_{(i,j)}), f_\\theta(x_{(i,\\leq j)^+}), f_\\theta(x_{(i,\\leq j)^-})) \\cdot \\mathcal{I}r(i,j)$.\nHere, $\\mathcal{l}_r$ is the contrastive loss (e.g., InfoNCE (van den Oord et al., 2018)), with $(i, \\leq j)^-$ and $(i, \\leq j)^+$ representing the sets of indices for negative and positive samples about sequence $x_{i,<j}$, respectively. The example in Figure 2 illustrates this concept clearly and effectively. $\\mathcal{I}(x_{i,j})$ is an indicator function, where $\\mathcal{I}_r(x_{i,j}) = 1$ iff $role(x_{i,j}) = RET$; otherwise, it is 0. Finally, combining the two parts of loss by weighted addition gives the final optimization goal: $\\mathcal{L} = \\lambda_g\\mathcal{L}_g + \\lambda_r\\mathcal{L}_r$, where $\\lambda_g$ and $\\lambda_r$ are hyperparameters. For a comprehensive overview of the detailed training procedures employed in other tasks, kindly refer to Figures 6 in Appendix."}, {"title": "Optimization", "content": "We use the standard Cross Entropy to optimize the loss function $l_g$. For the loss function $l_r$, prior work (Muennighoff et al., 2024) often utilizes the InfoNCE, which requires Grad-Cache (Gao et al., 2021) to handle large batch sizes on low-memory GPUs, adding overhead and limiting to one positive sample per batch with a carefully chosen temperature hyperparameter. In contrast, we employ the hyperparameter-free BPR (Rendle et al., 2009), a pair-wise loss function.\nThe $l_r$ loss is defined as: $l = -log\\sigma (||f(x_{(i,j)})|| - (||f(x_{(i,\\leq j)^+})|| - || f(x_{(i,\\leq j)^-})||))$, where $\\sigma(\\cdot)$ is the sigmoid function, $||\\cdot||$ is normalization, and $(i, \\leq j)^+$ and $(i, \\leq j)^-$ are randomly selected from $(i, \\leq j)^+$ and $(i, \\leq j)^-$ respectively. Using BPR allows for Gradient Accumulation to support larger batch sizes and multiple positive samples per batch. Experimental results in Table 4 show that BPR reduces negative impacts on generative tasks and significantly enhances retrieval tasks."}, {"title": "3.3 INFERENCE", "content": "For the standalone tasks of Generation and Retrieval, OneGen aligns with prior work. Here, we exclusively address the hybrid task of Generation and Retrieval. The inference process includes two primary steps: 1) Cache document embedding. To facilitate efficient retrieval, we cache our document embedding after training is done, similar with prior work. Specifically, we append [RD] at the end of each document x \u2208 K and use $f_\\theta$ to encoding them. We process all the documents in batch and the collection of these representations is stored in $Emb_\\text{doc}$ where $Emb_\\text{doc} \\in \\mathbb{R}^{|K| \\times d}$. 2) Task generation. Given an instruction, 2.1) the LLM begins autoregressively generate the next token until $role(x_i) = RET$ or $x_i \\in Terminator$ (e.g., </s> in Llama2). If $role(x_i) = RET$, then $f_\\theta(x_i)$ is used to retrieve from $Emb_\\text{doc}$ to obtain the set of most relevant documents $d_r \\subset K$. Here we use cosine similarity. 2.2) How to precede to generate the next token $x_{i+1}$? Since $role(x_i) \\neq GEN$, the probability distribution $P(x_{i+1}|x_{<i})$ is not intrinsically modeled by LLM, thus, should be provided by user. For the multi-turn dialogue system, the user provides the token that initiates a new round of dialogue rather than the LLM. Specifically, for the R \u2192 G task, $P(x_{i+1} x_{<i}) = d_r$ (i.e., directly concatenating the retrieved document). For R \u2192 G task, we specify $P(x_{i+1} x_{<i}) = [CON]$. 2.3) Finally, the LLMs continue autoregressive prediction, repeating step 2.1. Detailed pseudocode and description can be found in Appendix F.3 and Appendix H.2.\nDifference from Related Works. GritLM, GENRE (Cao et al., 2021b), and RICHES (Jain et al., 2024) can all perform specific hybrid retrieval and generation tasks using a single model. GritLM with causal attention first generates a query, then re-encodes it with bidirectional attention for retrieval in continuous space (i.e., using vector). GENRE and RICHES generate a query explicitly and use it to constrain decoding, enabling retrieval during generation in discrete space (i.e., using tokens) with just one forward pass. In contrast, OneGen performs retrieval in continuous space during"}, {"title": "4 EXPERIMENTS", "content": "We conduct extensive experiments across three different settings to validate OneGen's effectiveness, efficiency in training and inference, and pluggability. We train and evaluate a model for each setting independently. Specifically, the Single-hop QA involves one round of R \u2192 G, Multi-hop QA entails multiple R \u2192 G executions, and Entity Linking involves multiple G \u2192 R executions. Due to page constraints, we provide a concise overview of the workflow, baseline, training data, training backbones, evaluation datasets, and evaluation metrics for each setting in order. Detailed information for each following settings can be found in the Appendix F.5, G.3, and H.4, respectively."}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "R \u2192 G Task: RAG for Single-hop QA. We apply OneGen directly to the Self-RAG (Asai et al., 2024) method (i.e., using Self-RAG as workflow), which incorporates adaptive retrieval and self-assessment. OneGen enhances it by enabling self-retrieval. The baselines we used are listed in Table 2. Llama2-7B-chat serves as the backbone of ours and Self-RAG. The training data is derived from the Self-RAG training dataset, modified according to \u00a7 3.2, comprising 150k instances. We construct positive and negative samples using heuristic rules for instances containing the [RQ] token. These samples are sourced from wiki, comprising 60k instances. We evaluate four datasets: PubHealth (Zhang et al., 2023a), ARC-Challenge (Clark et al., 2018), PopQA (Mallen et al., 2023), and TriviaQA (Joshi et al., 2017). For the first two, accuracy serves as the evaluation metric. For the others, evaluation is based on whether the model's output contained the ground truth. All evaluations are consistent with Self-RAG to ensure fair comparison.\nR \u2192 G Task: RAG for Multi-hop QA. The following text sequence effectively demonstrates the workflow:\" {Instruction} Are Alan Turing and Newton from the same country? What is Alan Turing's country? [RQ] {document 1} England. What is Newton's country? [RQ] {document 2} England. Therefore, yes.\". This workflow combines CoT and RAG, where the gray tokens represent the role of CTX, the purple tokens represent RET, and the orange tokens represent GEN. The baseline uses the Contriever for retrieval, and the generator in the baseline is trained on the constructed data, with the optimization objective solely being Lg. We sample 10% of the training dataset from HotpotQA and 2WIKI, using Qwen2-72B (Bai et al., 2023) for data construction. Heuristics rules are also employed to label the positive and negative samples. Evaluation is conducted using the validation set of HotpotQA and 2WIKI, as the ground truth of test sets is unavailable. We use EM and F1 to evaluate the model's generative capability and Recall@1 for its retrieval capability.\nG\u2192 R Task: Entity Linking. The following text sequence effectively shows the workflow:\u201c {Instruction} Steve Jobs founded Apple Inc. <LOC>Steve Jobs</LOC> [RQ] <CON> founded <LOC>Apple Inc</LOC> [RQ] <CON>.\". The baselines we used are listed in Table 5. We employ the Wikipedia (totaling 6M documents, yet randomly sampling only 60K without careful selection) and AIDA (Hoffart et al., 2011) datasets, applying data augmentation to each sample in Wikipedia following established methods from previous studies (Cao et al., 2021a). We adopt heuristic rules to"}, {"title": "4.2 MAIN RESULTS", "content": "In Table 2, Table 5, and Table 3, we report the performance of OneGen on three types of settings respectively, demonstrating that our method is both effective, pluggable and training-efficient.\nR \u2192 G Task for Single-hop QA. From Table 2, we draw the following conclusions: (1) One-Gen demonstrates efficacy in R \u2192 G task, and joint training of retrieval and generation yields performance gains on the RAG task. The Self-RAG endows LLMs with self-assessment and adaptive retrieval, while OneGen adds self-retrieval. Our method outperforms the original Self-RAG across all datasets, especially achieving improvements of 3.1pt on Pub dataset and 2.8pt on ARC dataset, validating the benefits of joint training, consistent with findings from RA-DIT (Lin et al., 2024) and GRIT (Muennighoff et al., 2024). However, ours on PopQA and TQA datasets remains inferior to GritLM-7B. We attribute this to using a larger retrieval dataset, E5S, which is twice the size of MS MARCO and 33 times larger than OneGen. Additionally, E5S includes TQA training data and higher-quality data. (2) OneGen is highly efficient in training, with instruction-finetuned LLMs showing strong retrieval capabilities with minimal additional tuning. It requires less and lower-quality retrieval data, achieving comparable performance with just 60K noisy samples and incomplete documents, without synthetic data. This efficiency aligns with findings from previous works like PromptEoL (Jiang et al., 2023c) and EchoEmbedding (Springer et al., 2024), which demonstrate excellent performance using prompt-based methods without further training.\nR \u2192 G Task for Multi-hop QA. From Table 3, we additionally find that OneGen remains effective across multiple R \u2192 G settings and works well with various models and scales. It consistently outperforms the baseline on most datasets, backbones, and metrics. Notably, on the 2WIKI, One-Gen achieves an average improvement of 5.2pt in EM and 4.6pt in F1. Additionally, our evaluation of end-to-end retrieval performance indicates that OneGen's performance on retrieval surpasses the baseline across all datasets and backbones."}, {"title": "4.3 ANALYSIS", "content": "4.3.1 EFFICIENCY AT INFERENCE TIME\nTo assess the efficiency of OneGen during inference, we evaluate the inference time for various scenarios and tasks, as illustrated in Figure 3. For a fair comparison, all LLMs deployed are Mistral-v0.1-7B, operating with vLLM (Kwon et al., 2023) as the inference backend. Following the optimum-benchmark, all tokens are randomly generated and consistently numbered across base-lines. Notably, the \"Pipeline\" in Figure 3(a-b) employ Contriever as the retriever, while the Figure 3(c), following the EntGPT, the retriever in Pipeline is Mistral-v0.1-7B, converting ED task into a QA task. The \"Lower Bound\" in Figure 3(a-b) denotes results obtained without retriever. Detailed settings are provided in the Appendix F.5.1 and H.4.1.\nR \u2192 G Task. Figure 3(a-b) depict inference latency comparisons, with the default instruction in each round serving as the query. Figure 3(a) examines the influence of query length on inference speed in five rounds of R \u2192 G. Figure 3(b) assesses how the retrieval frequency and context length affect inference speed with a fixed query length of 100 for each round. Key observations include:\n1) Figure 3(a) shows that OneGen's inference process is efficient, with a notably greater increase in speed as query length extends, compared to Grit. The efficiency stems from OneGen's use of extra retrieval tokens, maintaining overall time close to that of smaller models (e.g. Contriever) used as retrievers in the Pipeline. The improvement in inference speed with increasing query length varies from 4% to 20%, mainly due to OneGen's elimination of a second query forward pass compared with the alternatives (pipeline & GRIT). 2) Figure 3(b) reveals that OneGen maintains stable inference times, under extreme scenarios, defined as one retrieval per dialogue round for 100 rounds with each round using 140 tokens and no semantic relevancy between rounds. Even when dialogue rounds reach 80, the context length consequently extends to 11K tokens, the increase in inference\nG\u2192 R Task. From Table 5, we can draw the following conclusions: (1) OneGen demonstrates effectiveness and strong generalization in the G \u2192 R task. It outperforms ReFinED in the F1 score by 3.2pt on average and surpasses the most competitive baselines on the OKE16, REU, and K50 datasets by 3.1pt, 8.1pt, and 10.1pt, respectively. The lower performance on other datasets is attributed to insufficient training data, while the baselines utilized 6M data and we used only 1% of this amount. As INSGENEL (Xiao et al., 2023) has shown, increased training data can improve MD performance. In \u00a7 4.3.2, we analyze the bottlenecks in datasets with poorer outcomes, which lie in MD. Additionally, only 24% of entities in our candidate set participated in the training process. (2) OneGen is highly efficient in training, requiring merely 1% of data used in baselines."}, {"title": "4.3.2 IMPACTS ON GENERATION AND RETRIEVAL", "content": "Here we examine whether situating retrieval and generation within the same context impacts the generative capacities of LLMs and assess the effectiveness of the retrieval. Given that \u00a7 4.2 evaluates end-to-end performance, where OneGen differs from other baselines in both the generation and retrieval modules, our evaluation principle here is to fix the retriever to assess generative capabilities and fix the generator to evaluate retrieval capabilities. More details are shown in Appendix F.5.2\nG\u2192 R Task. Same Retriever but different Generator: Evaluation is performed through the MD task. Additionally, we train the LLM using the same data and hyperparameters with SFT (Supervised Fine-Tuning). Table 7 reports the average performance across seven datasets. Comparing the last two columns of Table 7, we find that OneGen does not impair the LLM's generative capabilities.\nSame Generator but different Retriever: We employ the Entity Disambiguation (ED) task to evaluate retrieval performance. Table 6 summarizes the average results over nine datasets, revealing that OneGen significantly enhances the retrieval capacity of LLMs."}, {"title": "4.3.3 ABLATION STUDY", "content": "The more ablation studies, such as \u03bbr, implicit query, are shown in App. F.5.3, G.3.1, and H.4.2.\nLoss Function. We examine the impact of Lr, comparing BPR and InfoNCE, on EL, MD, and ED task. These tasks are assessed using seven, seven, and nine datasets respectively. Our results, presented in Table 4, indicate the BPR consistently surpasses InfoNCE in performance. This may be due to the overly restrictive of InfoNCE, which potentially limits the LLM's generative capabilities."}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In this paper, we utilize mathematical notation to formally unify generative tasks, retrieval task, and their composites such as RAG and EL. For composite tasks, we integrate retrieval and generation within the same context. Building upon this unified approach, we propose the OneGen training framework, which harmonizes and expands both generative and representative instruction tuning. We conduct extensive experiments on two distinct types of composite tasks, RAG and EL, to validate the pluggability, effectiveness, and efficiency of OneGen in training and inference. Furthermore, our results confirm that integrating generation and retrieval within the same context does not negatively impact the generative capabilities of LLMs, while also providing significant enhancements in re-trieval capabilities. Future research directions include: 1) Extending OneGen to the multimodal domain for tasks such as multimodal RAG and multimodal EL. 2) Enhancing OneGen's training with diverse datasets to improve LLMs' capability for complex retrieval and generation tasks."}, {"title": "LIMITATIONS", "content": "Despite our comprehensive efforts, the study presents several limitations:\n1) It remains unknown whether parameter-efficient fine-tuning methods such as LoRA (Hu et al., 2022) and QLoRA (Dettmers et al., 2023) could bring benefits for OneGen training. In this study, we utilized full-parameter fine-tuning. OneGen could potentially benefit from parameter-efficient fine-tuning, as recent work (Wang et al., 2024) has used LoRA to equip LLMs with retrieval capabilities.\n2) The absence of performance evaluations in more diverse and extensive data scenarios. Although we achieved gains with limited data, a more diverse set of tasks and data, such as jointly training with Entity Linking, RAG, retrieval data, and generation data, might produce a model with enhanced capabilities.\n3) The efficacy of OneGen within Mixture of Experts (MoE) models has not been tested. It is possible that MoE architectures could significantly influence the routing of retrieval and generation tasks, potentially enhancing inference efficiency if integrated effectively with OneGen.\n4) The underlying mechanisms by which LLMs trained using OneGen achieve simultaneous retrieval and generation in a single forward pass, without mutual interference, remain unclear."}]}