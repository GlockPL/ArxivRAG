{"title": "Relieving Universal Label Noise for Unsupervised Visible-Infrared Person\nRe-Identification by Inferring from Neighbors", "authors": ["Xiao Teng", "Long Lan", "Dingyao Chen", "Kele Xu", "Nan Yin"], "abstract": "Unsupervised visible-infrared person re-identification (USL-\nVI-ReID) is of great research and practical significance yet\nremains challenging due to the absence of annotations. Ex-\nisting approaches aim to learn modality-invariant represen-\ntations in an unsupervised setting. However, these meth-\nods often encounter label noise within and across modal-\nities due to suboptimal clustering results and considerable\nmodality discrepancies, which impedes effective training. To\naddress these challenges, we propose a straightforward yet\neffective solution for USL-VI-ReID by mitigating univer-\nsal label noise using neighbor information. Specifically, we\nintroduce the Neighbor-guided Universal Label Calibration\n(N-ULC) module, which replaces explicit hard pseudo la-\nbels in both homogeneous and heterogeneous spaces with\nsoft labels derived from neighboring samples to reduce la-\nbel noise. Additionally, we present the Neighbor-guided Dy-\nnamic Weighting (N-DW) module to enhance training stabil-\nity by minimizing the influence of unreliable samples. Exten-\nsive experiments on the RegDB and SYSU-MM01 datasets\ndemonstrate that our method outperforms existing USL-VI-\nReID approaches, despite its simplicity. The source code is\navailable at: https://github.com/tengxiao14/Neighbor-guided-\nUSL-VI-ReID.", "sections": [{"title": "Introduction", "content": "Visible-infrared person re-identification (VI-ReID) focuses\non identifying the same person from the visible or infrared\ncamera when a query image is provided from the other\nmodality. It has at-tracted widespread attention due to its potential in night vi-\nsion surveillance applications, where traditional visible cam-\neras cannot work well. Benefiting from accurate manual\nannotations, recent works have achieved notable improve-\nment on the VI-ReID task. However, the reliance on an-\nnotations within and across modalities heavily limits their\napplications. To address the issue, the task of unsupervised\nvisible-infrared person re-identification (USL-VI-ReID) has\nemerged and garnered substantial interest."}, {"title": "Related Work", "content": ""}, {"title": "Supervised Visible-Infrared Person ReID", "content": "Supervised visible-infrared person ReID (SVI-ReID) aims\nto match the same person across different modalities. The\nkey of SVI-ReID is to obtain discriminative feature repre-\nsentations by leveraging accurate manual annotations . Among them, Wu et al. introduce the task of SVI-ReID and proposes\na zero-padding approach to boost the performance of one-\nstream model. To regularize the model from overfitting, the\nPartMix augmentation technique is intro-\nduced by mixing local descriptions of images from differ-\nent modalities for part-level data augmentation. To enhance\nthe generalization of graph-based models, Li et al. rethink the SVI-ReID problem from the perspective of\ncounterfactual intervention and propose a novel framework\nby transferring features and stressing the effect of topol-\nogy structure. Regarding that convolution neural network is\nlimited in extracting discriminative feature representations,\nZhao et al. introduce the transformer into the task of SVI-ReID and propose a novel framework by enhancing spatial\nand channel information.\nWhile these methodologies have demonstrated substantial\nefficacy in learning modality-shareable feature representa-\ntions, they all require expensive annotations that are often\nchallenging to acquire, thereby constraining their real-world\napplicability."}, {"title": "Unsupervised Visible-Infrared Person ReID", "content": "Unsupervised visible-infrared person re-identification\n(USL-VI-ReID) seeks to learn modality-shareable fea-\nture representations without requiring annotations. This\ntask is inherently more challenging than unsupervised\nsingle-modality person ReID, as the discrepancies between\nmodalities often surpass the intra-class variance within a\nsingle modality . To effectively utilize unlabeled training data\nfrom diverse modalities, current approaches typically es-\ntablish cross-modality correspondences through clustering\nwithin each modality . For instance, Yang et al. introduce a novel contrastive learning framework\nthat aggregates cross-modality memories based on priority\ncounts. Wu et al. frame cross-modality\ncorrespondence mining as a graph matching problem and\npropose a method featuring progressive graph matching and\nalternative cross-modality learning modules.\nDespite these advancements, the methods still face chal-\nlenges due to suboptimal clustering results and significant\nmodality discrepancies, which can introduce label noise\nboth within and across modalities, thereby limiting model\nperformance."}, {"title": "Neighbor-Related Person ReID", "content": "Since a single image contains limited information, recent\nstudies have sought to extract more valuable information\nfrom neighboring images in both supervised and unsuper-\nvised settings. For instance, Wang et al. introduce explicit relationships between input images to mitigate\noutlier effects and enhance the robustness of learned fea-\nture representations. Zhong et al. utilize\nneighbor information to refine retrieval results derived from\ndirect similarity calculations of feature vectors. To address\nocclusion issues in person ReID, Yu et al. propose a neighbor-guided feature reconstruction method to\nrecover missing information using reliable neighbor data\nSimilar to our approach, several studies also incorpo-\nrate neighbor information in USL-VI-ReID tasks as a sub-\nmodule to refine cross-modality correspondences or to ex-\ntract relationships among samples. However, our method is distinct in that it is ex-\nclusively based on neighbor information and employs it\nin a more comprehensive and universal manner. This ap-\nproach addresses pseudo label noise in both homogeneous"}, {"title": "Method", "content": "The framework of our proposed method is depicted in Fig.\n2. To provide a comprehensive understanding, we first revisit\nthe Progressive Graph Matching (PGM) framework , which serves as the baseline for our approach.\nBuilding upon PGM, we will then introduce two key compo-\nnents of our method: the Neighbor-guided Universal Label\nCalibration (N-ULC) module and the Neighbor-guided Dy-\nnamic Weighting (N-DW) module, which will be detailed in\nthe subsequent sections."}, {"title": "Progressive Graph Matching", "content": "Given the unlabeled training sets for visible and infrared\nmodalities, denoted as $\\mathcal{X} = {\\mathcal{X}_v, \\mathcal{X}_i}$, where $\\mathcal{X}_v =$\n{$x_1,x_2,...,x_N$} represents the visible training set with N\nimages, while $\\mathcal{X}_i = {x_1, x_2,...,x_M}$ denotes the infrared\ntraining set containing M images. To extract features from\nthe unlabeled training sets, the two-stream encoders $f_v$ and\n$f_i$ are utilized, which share the same convolution backbone\nbut with modality-specific classifiers. This process yields\nmodality-specific feature sets $\\mathcal{U}_v = {u_1, u_2, ...,u_N}$ and\n$\\mathcal{U}_i = {u_1, u_2, ..., u_M}$ for visible and infrared modali-\nties, respectively. To generate pseudo labels for these fea-\nture sets, we apply the DBSCAN clustering algorithm independently on $\\mathcal{U}_v$ and $\\mathcal{U}_i$. This re-\nsults in cluster sets $\\mathcal{H}_v = {C_1,C_2,...,C_K}$ and $\\mathcal{H}_i =$\n{$C_1,C_2,...,C_L$}, where K and L denote the number of\nclusters in the visible and infrared modalities, respectively.\nThen the modalities-specific prototypes $\\Phi_v \\in \\mathbb{R}^{K \\times d}$ and\n$\\Phi_i \\in \\mathbb{R}^{L \\times d}$ can be obtained by applying averaging opera-\ntion on feature vectors within each cluster, where d denotes\nthe dimension of the feature vector.\nTo establish associations between different modalities, the\ngraph matching algorithm can be formulated as follows:\n$$\nG(M) = \\arg \\min_{M} D^T M\n$$\ns.t.$\\forall i \\in [K], j\\in [L] : M_{ij} \\in {0,1}$,\n$$\\forall i \\in [K] : \\sum_{j \\in [L]} M_{ij} \\leq 1,$$\n$$\\forall j \\in [L]: \\sum_{i \\in [K]} M_{ij} \\leq 1,$$\nwhere $D \\in \\mathbb{R}^{K \\times L}$ denotes the cost matrix representing the\ndissimilarity measure between the visible prototype $\\Phi_v \\in$\n$\\mathbb{R}^{K \\times d}$ and infrared prototype $\\Phi_i \\in \\mathbb{R}^{L \\times d}$. Using the pro-\ngressive matching strategy detailed in ,\nit is possible to associate all clusters between the visible\nand infrared modalities. Due to space constraints, additional\ndetails on the matching process are omitted here. Conse-\nquently, the cross-modality label transformers $T_{V\\rightarrow I}$ and\n$T_{I\\rightarrow V}$ are derived, facilitating the transformation of pseudo"}, {"title": "Neighbor-Guided Universal Label Calibration", "content": "Due to suboptimal clustering results and significant modal-\nity discrepancies, learning with a progressive graph match-\ning framework can be adversely affected by substantial label\nnoise both within and across modalities. Since images of the\nsame individual may be assigned to different clusters during\nthe clustering process, relying solely on hard pseudo labels\nto establish connections between images and a single clus-\nter, either within or across modalities, proves inadequate.\nTo address this challenge, we propose the N-ULC module.\nThis module aims to enhance the accuracy of sample iden-\ntity representation by deriving soft labels from information\nprovided by neighboring samples.\nSpecifically, given the training sets from different modal-\nities, we obtain feature sets {$\\mathcal{U}_v, \\mathcal{U}_i$} and cluster sets\n{$H_v, H_i$} as described in the previous section. For each\nsample $q_v$ from the visible training set, which is assumed\nto belong to the $l'$-th cluster, we identify its $k$-nearest neigh-\nbors from the visible training set $\\mathcal{U}_v$. The resulting list of\nneighbors is denoted as $N(q_v, \\mathcal{U}_v, k)$. Intuitively, the iden-\ntity statistics of this ranking list can provide insights into the\ntrue identity of $q_v$. Consequently, the correlation between $q_v$\nand the visible clusters can be expressed as:\n$$[P^{intra}_v]_l = \\frac{\\vert N(q_v, \\mathcal{U}_v, k) \\cap C'_l \\vert}{\\vert N(q_v, \\mathcal{U}_v, k) \\cup C'_l \\vert},$$\nwhere $P^{intra}_v$ represents the correlation between $q_v$ and\nthe visible clusters, and $[P^{intra}_v]_l$ denotes the $l$-th entry of\n$P^{intra}_v$. Since the query sample $q_v$ belongs to the $l'$-th clus-\nter in the visible modality, its original one-hot pseudo label\nis denoted as $T^{intra}_v$, where the $l'$-th entry is set to 1 and all\nother entries are set to 0. The calibrated soft label for $q_v$ can\nthen be expressed as:\n$$T^{intra}_v = \\mu T^{intra}_v + (1 - \\mu)P^{intra}_v,$$\nwhere $P^{intra}_v$ denotes the $l_1$-norm of $P^{intra}_v$ and $\\mu$ is\nthe hyper-parameter. Drawing inspiration from the label\nsmoothing technique, the calibrated homogeneous learning\nprocess for the query sample $q_v$ can be expressed as follows:\n$$L^{intra}(q_v) = - \\sum_{k=1}^{K} [T^{intra}_v]_k \\log \\frac{\\exp(q_v \\cdot \\phi^v_k / \\tau)}{\\sum_{k'=1}^{K} \\exp(q_v \\cdot \\phi^v_{k'} / \\tau)},$$\nSimilarly, label noise across modalities can also be mit-\nigated using this approach. For a given query sample $q_v$,\nits cross-modality $k$-nearest neighbors can be identified as\n$N(q_v, \\mathcal{U}_i, k)$. Subsequently, the correlation between $q_v$ and\nclusters from the infrared modality can be represented as:\n$$[P^{inter}_v]_l = \\frac{\\vert N(q_v, \\mathcal{U}_i, k) \\cap C^i_l \\vert}{\\vert N(q_v, \\mathcal{U}_i, k) \\cup C^i_l \\vert},$$\nwhere $\\mathcal{U}_i$ denotes the feature set extracted from the infrared\ntraining data. $C^i_l$ represents the cluster set containing sam-\nples from the $l$-th cluster in the infrared modality. Since $q_v$\nbelongs to the $l'$-th cluster in the visible modality, its cor-\nresponding cluster in the infrared modality can be identi-\nfied using the cross-modality label transformer $T_{V\\rightarrow I}$, de-\nnoted as $T_{V\\rightarrow I}[l']$. Consequently, the cross-modality one-\nhot pseudo label for $q_v$ is represented as $T^{inter}_v$, where the\n$T_{V\\rightarrow I}[l']$-th entry is set to 1 and all other entries are set to\n0. Thus, the calibrated cross-modality soft label for $q_v$ is ex-\npressed as:\n$$T^{inter}_v = \\mu T^{inter}_v + (1 - \\mu)P^{inter}_v,$$ \nwhere $P^{inter}_v$ is the $l_1$-norm of $P^{inter}_v$. The calibrated het-\nerogeneous learning process for $q_v$ can then be described as:\n$$L^{inter}(q_v) = - \\sum_{k=1}^{L} [T^{inter}_v]_k \\log \\frac{\\exp(q_v \\cdot \\phi^i_k / \\tau)}{\\sum_{k'=1}^{L} \\exp(q_v \\cdot \\phi^i_{k'} / \\tau)},$$ \nThe same procedure can be applied to samples from the\ninfrared modality for label calibration both within and across\nmodalities. For simplicity, the details of this process are\nomitted."}, {"title": "Neighbor-Guided Dynamic Weighting", "content": "Our proposed neighbor-guided universal label calibration al-\nleviates label noise by more accurately reflecting the true\nidentity of images both within and across modalities . To enhance the stability of the learning pro-\ncess, we introduce the N-DW module, which reduces the im-\npact of unreliable samples during training. Specifically, for"}, {"title": "Theoretical Analysis", "content": "In this study, we derive soft labels from neighbor infor-\nmation to optimize model parameters both in the homo-\ngeneous learning and heterogeneous learning processes. In\nthis part, we aim to investigate the Rademacher generaliza-\ntion bound of our method. For simplicity, we merely con-\nsider the binary classification setting, i.e., each sample x is\npaired with a label y \u2208 {0,1}. Our method relies on soft\nlabels derived from a probabilistic distribution, leading to\nthe following loss function formulation: $(SOFT(h(x), y) =$\n$(1 \u2212 \u03b2)\u00b7l(h(x), y) + \u03b2\u00b7l(h(x),1 \u2212 y)$, where l represents a\ncommonly used loss function such as cross-entropy loss and\n\u1e9e denotes the probability of the irrelevant class. Our method\naims to obtain the classifier h by minimizing the following\nexpected risk:\n$$R(h) := E_{(x,y)\\sim D} [l^{SOFT}(h(x), y)],$$\nwhere D is the distribution of the dataset and y is the corre-\nsponding generated pseudo label. l is the loss function. Due\nto the limited scale of our dataset, our actual learning objec-\ntive becomes the empirical risk as follows:\n$$\\hat{R}(h) := \\frac{1}{N} \\sum_{i=1}^{N} [l^{SOFT}(h(x_i), y_i)],$$\nwhere N is the size of the training set.\nTheorem 1 With probability at least 1 \u2212\n\u03b4, for all h\u2208 H, we have:\n$$R(h) \\leq \\hat{R}(h)+2\\cdot \\Gamma \\cdot \\Re(H)+(1-2\\beta)\\sqrt{\\frac{\\log(1/\\delta)}{2N}},$$\nwhere $\\Gamma$ and $\\underline{\\Gamma}$ are the upper bound and lower bound of l and\nRe is the Rademacher complexity.\nRemark 1. Theorem 1 offers an upper bound for the gap\nbetween expected risk R(h) and empirical risk $\\hat{R}(h)$. Since\n\u1e9e is usually set to a small value to ensure the pivot can be\nclassified, it can be regarded as a constant here. When the\nsize of the dataset N is large enough and the Rademacher\ncomplexity of the hypothesis space $\\Re(H)$ is limited, R(h)\ncan approach $\\hat{R}(h)$ well with soft labels derived from prob-\nabilistic distributions."}, {"title": "Datasets and Evaluation Protocols", "content": "Our experiments are conducted on two public datasets:\nRegDB and SYSU-MM01 . To ensure fair comparison, we use mean average pre-\ncision (mAP), Cumulative Matching Characteristics (CMC),\nand mean Inverse Negative Penalty (mINP) as evaluation metrics, which are commonly employed in ex-\nisting research . SYSU-MM01 is a large-scale VI-ReID dataset collected\nfrom 4 visible and 2 infrared cameras. The training set in-\ncludes 395 identities with 22,258 visible images and 11,909\ninfrared images, while the testing set includes 96 identities.\nRegDB is another dataset acquired from one visible and\none infrared camera. It contains 412 identities, each identity\nincludes 10 visible and 10 infrared images. In line with pre-\nvious studies, we per-\nform 10 experiments on this dataset and report the average\nperformance as the final results."}, {"title": "Implementation Details", "content": "Our method is implemented in the PyTorch platform. Fol-\nlowing existing works, we utilize the\nImageNet-pretrained ResNet50  as the back-\nbone. In each mini-batch, we select 16 identities per modal-\nity, with each identity comprising 16 instances. Input images\nare resized to 288 \u00d7 144 pixels. Standard data augmenta-\ntion techniques, including random flipping, random crop-\nping, and random erasing, are applied. Pseudo labels are\ngenerated using the DBSCAN clustering algorithm, with a\nmaximum distance of 0.2 for the RegDB dataset and 0.6 for\nthe SYSU-MM01 dataset. Hyper-parameters \u03bc and \u03bb are set\nto 0.7 and 3, respectively, while the number of neighbors k is\nset to 20 and 30 for the RegDB and SYSU-MM01 datasets,\nrespectively. All other experimental settings follow those of\nprevious works."}, {"title": "Comparison with SOTA Methods", "content": "To assess the effectiveness of our method, we compare it\nagainst state-of-the-art approaches across three ReID set-\ntings: supervised visible-infrared person ReID (SVI-ReID),\nunsupervised single-modality person ReID (USL-ReID),\nand unsupervised visible-infrared person ReID (USL-VI-ReID).\nThe results are presented in Table 1.\nComparison with SVI-ReID methods. We compare our\nmethod with several recent SVI-ReID approaches, including\nAGW , CA , MAUM"}, {"title": "Ablation Study", "content": "In Table 2, we present experiments conducted on the SYSU-\nMM01 and RegDB datasets to evaluate the components of\nour method, specifically the Neighbor-Guided Label Uni-\nversal Calibration (N-ULC) and Neighbor-Guided Dynamic\nWeighting (N-DW) modules. The results indicate that ap-\nplying the N-ULC and N-DW modules individually yields\nsubstantial improvements on both datasets, highlighting the\neffectiveness of these components. When both modules are\ncombined, our method achieves optimal performance on\nthe SYSU-MM01 dataset. However, improvements on the\nRegDB dataset are more limited. This limitation is likely\ndue to the already high performance of the N-DW module\non RegDB and the partial overlap in functionality between\nthe modules, which restricts further gains in performance."}, {"title": "Hyper-parameter Analysis", "content": "Our method relies on information from nearest neighbors,\nmaking the nearest neighbor number k in Eq. (5) a crucial\nfactor in its performance. Figure 3 presents experiments con-\nducted on various datasets to assess the impact of this hyper-\nparameter. The results indicate that the performance of our\nmethod remains relatively stable across different values of\nk. For optimal results, k is set to 30 for the SYSU-MM01\ndataset and 20 for the RegDB dataset."}, {"title": "Visualization", "content": "To further evaluate the effectiveness of our method in learn-\ning modality-shareable feature representations, we employ\nt-SNE  to visualize the\nfeatures learned by both our method and PGM on the SYSU-\nMM01 and RegDB datasets, as shown in Fig. 4. The results\nreveal that PGM often separates images of the same person\nacross different modalities into distinct clusters, whereas our\nmethod generates more compact feature representations for\nthe same individual. In contrast, PGM tends to confuse more\nidentities and blend features from different individuals com-\npared with our method."}, {"title": "Conclusion", "content": "In this paper, we present a simple yet effective frame-\nwork for USL-VI-ReID that addresses universal label noise\nthrough the use of neighbor information. To mitigate label\nnoise, we introduce the neighbor-guided universal label cal-\nibration module, which refines explicit hard pseudo labels\nby leveraging information from neighbors in both homoge-\nneous and heterogeneous spaces. Additionally, to enhance\ntraining stability, we propose the neighbor-guided dynamic\nweighting module, which reduces the impact of unreliable\nsamples within and across modalities. Extensive experi-\nments conducted on the SYSU-MM01 and RegDB datasets\ndemonstrate the effectiveness of our proposed method, de-\nspite its inherent simplicity."}, {"title": "More Experimental Results", "content": ""}, {"title": "Influence of the hyper-parameter \u03bb", "content": "Our method involves both homogeneous learning and het-\nerogeneous learning processes. In Eq. (13), the hyper-\nparameter \u03bb is a crucial factor which balances the weights of\nhomogeneous learning and heterogeneous learning. When \u03bb\nincreases, the model will focus more on the association be-\ntween different modalities while neglecting the intrinsic pat-\nterns within the modality, and vice versa. To obtain the op-\ntimal value of \u03bb, we conduct experiments on SYSU-MM01\ndataset with different values of this hyper-parameter and re-\nsults are shown in Fig. 5(a). From the results, we can find\nthat the performance of the model is relatively robust against\ndifferent values of \u03bb. For simplicity, we set \u03bb to 3.0 in both\nSYSU-MM01 and RegDB datasets."}, {"title": "Influence of the hyper-parameter \u03bc", "content": "The hyperparameter \u03bc in the N-ULC module controls the\ncalibration strength of pseudo labels and is crucial for the\nmodule's performance. If \u03bc is set too small, the calibra-\ntion of the original pseudo labels will be insufficient. On\nthe other hand, if \u03bc is set too large, the calibration may\noverride the original pseudo label information, hindering the\nmodel's ability to learn discriminative feature representa-\ntions. Fig. 5(b) illustrates the model's performance on the\nSYSU-MM01 dataset for different values of \u03bc. The results\nsupport these observations: when \u03bc is set to 0.6 or 1.0, the\nmodel's performance drops significantly. However, with \u03bc\nset to 0.7, the model achieves the best performance. For con-\nsistency, we set \u03bc to 0.7 across all datasets."}, {"title": "Influence of the hyper-parameter w", "content": "The hyperparameter w in the N-DW module controls the\npenalty applied to unreliable samples. If w is set too small,\nthe weight differences between samples become negligi-\nble, making it hard to distinguish between them effectively.\nOn the other hand, if w is set too large, the module sup-\npresses too many samples, making it insufficient for model\ntraining. Table 3 shows the model's performance on the\nSYSU-MM01 dataset with different values of w. The model\nachieves the best performance when w is set to 10. For con-\nsistency, w is set to 10 across all datasets in the paper."}]}