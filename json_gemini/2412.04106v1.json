{"title": "MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities", "authors": ["Haoning Wu", "Ziheng Zhao", "Ya Zhang", "Weidi Xie", "Yanfeng Wang"], "abstract": "Medical image segmentation has recently demonstrated impressive progress with deep neural networks, yet the heterogeneous modalities and scarcity of mask annotations limit the development of segmentation models on unannotated modalities. This paper investigates a new paradigm for leveraging generative models in medical applications: controllably synthesizing data for unannotated modalities, without requiring registered data pairs. Specifically, we make the following contributions in this paper: (i) we collect and curate a large-scale radiology image-text dataset, MedGen-1M, comprising modality labels, attributes, region, and organ information, along with a subset of organ mask annotations, to support research in controllable medical image generation; (ii) we propose a diffusion-based data engine, termed MRGen, which enables generation conditioned on text prompts and masks, synthesizing MR images for diverse modalities lacking mask annotations, to train segmentation models on unannotated modalities; (iii) we conduct extensive experiments across various modalities, illustrating that our data engine can effectively synthesize training samples and extend MRI segmentation towards unannotated modalities. The code, model, and data will be publicly available at https://haoningwu3639.github.io/MRGen/.", "sections": [{"title": "1. Introduction", "content": "Medical image segmentation [21, 33, 43, 55] has achieved remarkable success, becoming a cornerstone of intelligent healthcare. However, challenges such as data privacy, modality complexity, and the high cost of segmentation annotations significantly restrict its development towards unannotated modalities. As depicted in Figure 1 (left), these limitations are especially acute in MRI, which exhibits significant discrepancies across modalities.\nMagnetic Resonance Imaging (MRI) is a non-invasive imaging technique widely recognized for its high-quality and radiation-free characteristics. However, MRI scans are often expensive, time-consuming, and sensitive to motion. Additionally, there is significant variability across different MRI modalities, and even within the same modality, differences in machines and scanning parameters can lead to substantial discrepancies in images. The complexity and variability of MRI, coupled with high annotation costs, lead to a scarcity of registered data pairs and segmentation annotations, posing significant challenges to the development of robust segmentation models. In this paper, we aim to leverage the progress in generative models, specifically, diffusion models [18], to facilitate the training of segmentation models on unannotated MRI modalities.\nGenerally speaking, there are three challenges when applying the generative models to MRI data synthesis: (i) paradigm: as presented in Figure 1 (right), previous models [3, 13] primarily focus on generating data for modalities with annotations. This approach limits their scalability to under-annotated or unannotated MRI modalities; (ii) data: the scarcity of MR images and fine-grained annotations makes it challenging to develop general and controllable generative models; (iii) controllability: to better benefit downstream tasks with synthetic data, generative models must enable controllable generation, guided by conditions such as text prompts or segmentation masks, to produce training samples for downstream tasks.\nTo tackle the above challenges, we collect and curate a large-scale radiology image-text dataset, MedGen-1M, featuring CT and MRI of various modalities sourced from the Internet and open-source data. Our dataset comprises 1.2 million 2D slices with modality labels, attributes, region, and organ information, with a subset also containing organ mask annotations. The extensive image-text pairs across diverse modalities provide a foundation for training a general MRI generation model, and the available mask annotations further enable more controllable generation.\nBuilding on this foundation, this paper introduces MRGen, a diffusion-based data engine for controllable MRI synthesis, particularly, for modalities with no segmentation masks ever available. At training time, we employ a two-stage strategy: (i) text-guided pretraining on diverse, large-scale radiology image-text pair data, enabling the model to generate images across various modalities based on text descriptions; and (ii) mask-conditioned finetuning on data with mask annotations, facilitating controllable generation based on organ masks. Consequently, such a strategy even allows MRGen to extend its controllable generation abilities towards modalities that do not have segmentation annotations available. During inference, MRGen takes text prompts and organ masks as inputs, producing MR images aligned with the given modality, region, and organs. With such synthetic samples, we can thus facilitate MRI segmentation tools towards modalities without manual annotations.\nOur contributions in this paper can be summarized as follows: (i) we establish a new paradigm for generative model applications in medical analysis, i.e., synthesizing data of unannotated modalities to serve as training samples for downstream segmentation models; (ii) we curate a large-scale radiology image-text dataset, MedGen-1M, which features detailed modality labels, attributes, regions, organ information, and a subset with organ mask annotations, providing a foundation for building general and controllable image generation models in the radiology domain; (iii) we develop MRGen, a diffusion-based data engine capable of generation conditioned on text prompts and masks, synthesizing MR images of various annotation-scarce modalities; (iv) we conduct extensive experiments across diverse modalities, demonstrating that MRGen can controllably synthesize high-quality MR images, boosting segmentation performance on unannotated modalities."}, {"title": "2. Related works", "content": "Generative models have been a research focus in computer vision for years, with GANs [12] and diffusion models [18, 45] leading the advancements. These models have found extensive applications across various tasks, including text-to-image generation [24, 38, 42, 48], image-to-image translation [4, 22, 58], artistic creation [30, 44, 51], and even challenging video generation [10, 19]. Notably, CycleGAN [58] employs cycle-consistency loss to facilitate image translation with unpaired data, while the Stable Diffusion series [11, 39, 42] efficiently produces high-resolution images in latent space, earning broad recognition.\nMedical image synthesis aims to leverage generative models to improve medical image analysis models or tackle challenges such as data scarcity and privacy concerns [28]. Prior works train generative models on X-ray [3], CT [13], and brain MRI [8, 36], while methods like DiffTumor [5] and FreeTumor [49] focus specifically on generating tumor images to improve tumor segmentation.\nExisting works [3, 13] primarily focus on in-domain augmentation within mask-annotated training modalities, and often struggle to generalize to unseen and unannotated modalities. To this end, this paper explores a novel paradigm for medical image generation, producing samples"}, {"title": "3. Data Curation", "content": "To train our proposed data engine, we first collect and curate a large-scale radiology dataset, termed MedGen-1M, containing CT and MR images with rich modality information, clinical attributes, and mask annotations. In the following, we elaborate on our data processing pipeline and present the statistics of the curated dataset.\nData collection. We first collect a large amount of abdominal CT and MR images from the Radiopaedia website, licensed under CC BY-NC-SA 3.0. This portion of data comprises a wide variety of modalities, serving as the large-scale image-text pairs ({(I,T)}), with each sample containing an image and its modality label. As listed in Table 1, we also integrate CT and MR data with modality labels and abdominal organ mask annotations from several open-source datasets, to construct data triplets ({(I,T,M)}), which will be further explained in Section 4.\nAutomatic annotations. Given the wide variability in abdominal imaging, such as differences between the upper abdominal region and the pelvic region, relying solely on modality labels may not sufficiently distinguish these images. Thus, we divide the abdomen into six regions, including: Upper Thoracic Region, Middle Thoracic Region, Lower Thoracic Region, Upper Abdominal Region, Lower Abdominal Region, and Pelvic Region. We then use off-the-shelf BiomedCLIP [52] to categorize all 2D slices into these six categories to serve as region information."}, {"title": "4. Method", "content": "We first formulate the problem in Section 4.1, followed by a detailed description of the proposed MRGen architecture in Section 4.2; then we elaborate on the training details of our model in Section 4.3; lastly, we present the procedure of synthesizing and filtering samples with our data engine for downstream segmentation tasks in Section 4.4."}, {"title": "4.1. Problem Formulation", "content": "In this paper, we focus on developing a diffusion-based controllable data engine for medical imaging, particularly for synthesizing data across various modalities with no mask annotations available. Specifically, our proposed MRGen (\u03a6_{MRGen}) generates the radiology image (I), conditioned on a text prompt (T) and the organs mask (M), i.e.,"}, {"title": "4.2. Architecture", "content": "For our generation paradigm, we expect the model to leverage the large number of image-text data pairs, and the limited data with segmentation masks, to achieve controllable generation for segmentation-scarce modalities. Our proposed model (MRGen) consists of three components: (i) latent encoding; (ii) text-guided generation; and (iii) mask-conditioned generation, as detailed below.\nLatent encoding. Given the high resolution of medical images, we adopt the idea of latent diffusion [42], for efficient generation within a compressed, low-dimensional latent space. As shown in Figure 2 (a), we utilize an autoencoder architecture, where a 2D MRI slice (I \u2208 R^{H\u00d7W\u00d71}) is encoded into a latent code (z \u2208 R^{h\u00d7w\u00d7d}) for diffusion training. The decoder then reconstructs the image (\u00ce) from the latent code, as expressed by the following:\n\u00ce = \u03a6_{Dec}(z) = Dec(\u03a6_{Enc}(I))\nText-guided generation. This part follows the diffusion model paradigm, consisting of a forward diffusion process and a denoising process. Concretely, the forward process progressively adds noise to the latent features (z\u2080) over T steps towards white Gaussian noise z_{T} \u223c N(0,1). At any intermediate timestep t \u2208 [1,T], the noisy features (z\u209c) can be expressed as: z_{t} = \u221a{\u03b1\u0304_{t}}z\u2080 + \u221a{1 \u2212 \u03b1\u0304_{t}}\u03f5, where \u03f5 \u223c N(0, 1) and \u03b1\u0304_{t} represent predefined hyperparameters, which will be detailed in Section A of the Appendix.\nAs depicted in Figure 2 (b), the learnable denoising process typically reconstructs images from noise by estimating the noise term \u03f5\u0302. To model this process in the compressed latent space, we adopt a UNet [43]. Specifically, to generate images guided by text prompts, we design templated text prompt (T), that consists of diverse modality labels, modality attributes, regions, and organs, for example:\n\"T1 MRI; fat high signal, muscle intermediate signal,\nwater low signal, fat bright, water dark; upper abdomen; liver, spleen, and kidney\".\nWe employ an off-the-shelf text encoder (\u03a6_{text}) to encode the text prompt into embeddings, expressed as C_{T} = \u03a6_{text}(T). These text embeddings are then fed into our model via cross-attention, serving as the key and value, with visual latent codes as the query.\nMask-conditioned generation. Next, we incorporate mask conditions to enable more controllable generation. As shown in Figure 2 (c), we adopt the idea similar to ControlNet [51], i.e., initializing the mask encoder (\u03a6_{mask}) with the weights of the diffusion UNet encoder, coupled with a learnable downsampling module (\u03a6_{down}) to align the dimensions of the mask with the latent code. In the input mask (M\u2208 R^{H\u00d7W\u00d71}), we use distinct intensity values to indicate the different organs, and encode it as a residual into the corresponding layers of the UNet decoder, guiding the generation process. In addition, zero convolution initialization is employed to stabilize the training process, as observed in [51]. For each layer F of the diffusion UNet decoder, the output O can now be formulated as follows:\nO = F(z_{t}) + \u03a6_{mask}(z_{t}, \u03a6_{down}(M), \u03a6_{text}(T))"}, {"title": "4.3. Model Training", "content": "Here, we present the training procedure for our generative model, including (i) autoencoder reconstruction, (ii) text-guided pretraining, and (iii) mask-conditioned finetuning.\nAutoencoder reconstruction. The autoencoder for compression is trained solely on raw images from D_{u}, using a combination of MSE loss and KL divergence loss as follows: L_{vae} = ||I \u2212 \u00ce||_{2}^{2} + \u03b3L_{KL}, where L_{KL} imposes a KL-penalty towards a standard normal on the learned latent, similar to a VAE [26] and \u03b3 represents a predefined weight.\nText-guided pretraining. The diffusion-based generative model, parameterized by \u0398, is trained on a large number of image-text pairs in D_{u}, covering diverse modalities. The objective function can be formulated as the MSE loss between the added Gaussian noise (\u03f5) and the prediction (\u03f5\u0302):\nL = E_{D_{u},t\u223c[1,T],\u03f5\u223cN(0,1)} [||\u03f5 \u2212 \u03f5\u0302(z_{t}, t, T)||2]\nThis pretraining phase enables MRGen to generate radiology images across diverse modalities based on text prompts.\nMask-conditioned finetuning. The mask condition controller, consisting of the mask encoder (\u03a6_{mask}) and the learnable downsampling module (\u03a6_{down}), can be trained jointly on the source-domain triplet dataset, D_{s} = {(I_{s}, T_{s}, M_{s})}, and target-domain dataset comprising only image-text pairs, D_{t} = {(I_{t}, T_{t}, \u2205)}, with all other parameters frozen. The training objective L_{c} can be expressed as:\nL_{c} = E_{D_{s},D_{t},t\u223c[1,T],\u03f5\u223cN(0,1)} [||\u03f5 \u2212 \u03f5\u0302_{c}(z_{t}, t, T, M)||2]\nHere, incorporating target-domain data without mask annotations prevents overfitting to the source domain."}, {"title": "4.4. Synthetic Data for Segmentation Training", "content": "With our data engine, we aim to generate target-domain data conditioned on the segmentation masks, to facilitate training segmentation models towards unannotated modalities.\nData synethesis. At inference time, we feed the text prompt (T't) and organ mask (M't) as conditions into our MRGen model (\u03a6_{MRGen}) to generate the corresponding target-domain image (I't). For simplicity, when constructing the input conditions, we directly utilize region, organ information, and mask annotations from source-domain data.\nData autofilter. To ensure the fidelity of generated images to the mask conditions, we design an automatic filtering pipeline using the off-the-shelf SAM2 [41] model, as depicted in Figure 3. Specifically, we feed the mask (M't) and the generated image (I't) into SAM2 to predict a segmentation confidence score (Sconf), and a pseudo mask annotation used to calculate the IoU score (SIoU) with M't. A sample is considered high-quality and aligned with the mask condition if both its IoU score and confidence score exceed the predefined thresholds; otherwise, it is discarded."}, {"title": "5. Experiments", "content": "Here, we start by describing the experimental settings in Section 5.1; then, in Section 5.2 and 5.3, we present a comprehensive evaluation of our method from both quantitative and qualitative perspectives; lastly, we provide results for ablation experiments in Section 5.4 to prove the effectiveness of our proposed methods."}, {"title": "5.1. Experimental Settings", "content": "We aim to evaluate our proposed data engine from two aspects: (i) image generation, and (ii) segmentation. Concretely, to simulate MRI segmentation towards unannotated modalities, we construct 8 cross-modal dataset pairs within our MedGen-1M. Each pair comprises a mask-annotated source-domain dataset and an unannotated target-domain dataset. The generative model is trained on each dataset pair to synthesize target-domain samples, which are then used to train segmentation models. We assess the quality of generated images, and the performance of the segmentation models on the target-domain test set."}, {"title": "6. Conclusion", "content": "This paper explores a novel paradigm of applying generative models in medical imaging: controllably synthesizing data for modalities lacking mask annotations, to train MRI segmentation models towards these challenging scenarios. To support this, we curate a large-scale radiology image-text dataset, MedGen-1M, featuring detailed modality labels, attributes, regions, and organ information, along with a subset of organ mask annotations. Built upon the dataset, our diffusion-based controllable data engine, MRGen, generates MR images conditioned on text prompts and masks, enabling data synthesis across unannotated modalities for training segmentation models. Comprehensive quantitative and qualitative evaluations across diverse modalities demonstrate that MRGen can effectively synthesize data, advancing MRI segmentation for unannotated modalities."}, {"title": "A. Preliminaries on Diffusion Models", "content": "Diffusion Models [18] are a class of deep generative models that convert Gaussian noise into structured data samples through an iterative denoising process. These models typically comprise a forward diffusion process and a reverse denoising process. Specifically, the forward diffusion process progressively introduces Gaussian noise into an image (x\u2080) via a Markov process over T steps. Let x\u209c represent the noisy image at step t. The transition from x\u209c\u208b\u2081 to x\u209c can be formulated as:\nq(x_{t}|x_{t-1}) = N(x_{t}; \u221a{1-\u03b2_{t}}x_{t-1}, \u03b2_{t}I)\nHere, \u03b2_{t} \u2208 (0,1) represents pre-determined hyperparameters that control the variance at each step. By defining \u03b1_{t} = 1 - \u03b2_{t} and \u03b1\u0304_{t} = \u03a0_{i=1}^{t} \u03b1_{i}, the properties of Gaussian distributions and the reparameterization trick allow for a refined expression:\nq(x_{t}|x_{o}) = N(x_{t}; \u221a{\u03b1\u0304_{t}}x_{o}, (1 \u2013 \u03b1\u0304_{t})I)\nThis insight provides a concise expression for the forward process with Gaussian noise \u03f5 as: x_{t} = \u221a{\u03b1\u0304_{t}}x_{o} + \u221a{1 \u2013 \u03b1\u0304_{t}}\u03f5.\nDiffusion models also encompass a reverse denoising process that reconstructs images from noise. A UNet-based model [43] is typically utilized to learn the reverse diffusion process p_{\u03b8}, represented as:\np_{\u03b8}(x_{t-1}|x_{t}) = N(x_{t}; \u03bc_{\u03b8}(x_{t}, t), \u03a3_{\u03b8}(x_{t}, t))\nHere, \u03bc_{\u03b8} represents the predicted mean of Gaussian distribution, derived from the estimated noise \u03f5\u0302 as:\n\u03bc_{\u03b8}(x_{t}, t) = \\frac{1}{\u221a{\u03b1_{t}}}(x_{t} - \\frac{1- \u03b1_{t}}{\u221a{1-\u03b1\u0304_{t}}}\u03f5\u0302(x_{t}, t))\nBuilding on this foundation, Latent Diffusion Models [42] adopt a Variational Autoencoder (VAE [26]) to project images into a learned, compressed, low-dimensional latent space. The forward diffusion and reverse denoising processes are then performed on the latent codes (z) within this latent space, significantly reducing computational cost and improving efficiency."}, {"title": "B. Details of MedGen-1M", "content": "In this section, we provide additional details about our collected and curated MedGen-1M dataset. In Section B.1, we elaborate on the implementation details of the automatic annotation pipeline; and in Section B.2, we present more comprehensive statistics about the dataset."}, {"title": "B.1. Automatic Annotations", "content": "We employ an automated annotation pipeline to annotate the data in our MedGen-1M, ensuring that the samples contain comprehensive clinically relevant annotation information. This process primarily includes two components: human body region classification and modality explanation, which will be detailed as follows.\nRegion classification. Considering the wide range and variability of abdominal imaging, we adopt the off-the-shelf Biomed-CLIP [52] image encoder to encode all 2D slices and the BiomedCLIP text encoder to encode predefined text descriptions of six abdominal regions. Based on the cosine similarity between the image and text embeddings, the 2D slices are classified into these six categories, including Upper Thoracic Region, Middle Thoracic Region, Lower Thoracic Region, Upper Abdominal Region, Lower Abdominal Region, and Pelvic Region. For text encoding, we use a templated text prompt as input:\nThis is a radiology image that shows $region$ of a human body, and probably contains $organ$.\nHere, $region$ and $organ$ represent the items in the following list:\n(region, organ) = [ (\u2018Upper Thoracic Region', 'lung, ribs and clavicles'), ('Middle Thoracic Region', 'lung, ribs and heart'), ('Lower Thoracic Region', 'lung, ribs and liver'), ('Upper Abdominal Region', 'liver, spleen, pancreas, kidney and stomach'), (\u2018Lower Abdominal Region', \u2018kidney, small intestine, colon, cecum and appendix'), (\u2018Pelvic Region', \u2018rectum, bladder, prostate/uterus and pelvic bones') ]\nModality explanation. To capture the correlations and distinctions among various modality labels, we leverage GPT-4 [1] to generate free-text descriptions detailing the signal intensities of fat, muscle, and water for each modality label. This helps the model better understand the imaging characteristics of distinct modalities. The prompt we use is as follows:"}, {"title": "4.  Limitations", "content": "Our proposed data engine, MRGen, is not without its limitations. Specifically, MRGen encounters difficulties when generating conditioned on extremely small organ masks and occasionally produces false-negative samples.\nExtremely small organ masks. The morphology of the same organ, such as the liver or spleen, can vary significantly across different slices of a 3D volume, resulting in significant variability in their corresponding masks. Furthermore, the distribution of these masks is often imbalanced, with extremely small masks being relatively rare. When generating in the latent space, these masks are further downsampled, leading to unstable generation quality, as depicted in Figure 8 (a). A feasible solution to mitigate this issue is to increase the amount of data with mask annotations, thereby improving the model's robustness.\nFalse-negative samples. Another challenge arises from the varying number of organs to be segmented on each slice. For instance, one slice may contain the liver, kidneys, and spleen, while another may include only the liver and spleen. This variability causes MRGen to occasionally generate additional segmentation targets not specified in the mask condition. For example, as illustrated in Figure 8 (b), kidneys are unexpectedly synthesized by MRGen, despite not being included in the mask conditions, leading to false negatives during the training of downstream segmentation networks. A feasible solution is to design a more comprehensive and robust data filtering pipeline to filter these false-negative samples. Alternatively, simple manual selection can serve as a quick and effective method to remove samples that do not meet the requirements."}, {"title": "2.  Future Works", "content": "To address the aforementioned limitations of MRGen, we propose several directions for future improvement: (i) Constructing more comprehensive and richly annotated datasets, such as incorporating more annotated CT data or collecting more MRI data, to enhance the model's ability to effectively utilize mask conditions; (ii) Designing finer-grained, accurate, and efficient generative model architectures to improve generation efficiency and accuracy, particularly for small-volume organs; and (iii) Developing a more robust and comprehensive data filtering pipeline to reliably select high-quality samples that meet the requirements of downstream tasks."}]}