{"title": "ECRTime: Ensemble Integration of Classification and Retrieval for Time Series\nClassification", "authors": ["Fan Zhao", "You Chen"], "abstract": "Deep learning-based methods for Time Series Classification (TSC) typically utilize deep networks to extract features, which are\nthen processed through a combination of a Fully Connected (FC) layer and a SoftMax function. However, we have observed\nthe phenomenon of inter-class similarity and intra-class inconsistency in the datasets from the UCR archive and further analyzed\nhow this phenomenon adversely affects the \"FC+SoftMax\u201d paradigm. To address the issue, we introduce ECR, which, for the\nfirst time to our knowledge, applies deep learning-based retrieval algorithm to the TSC problem and integrates classification and\nretrieval models. Experimental results on 112 UCR datasets demonstrate that ECR is state-of-the-art(sota) compared to existing\ndeep learning-based methods. Furthermore, we have developed a more precise classifier, ECRTime, which is an ensemble of ECR.\nECRTime surpasses the currently most accurate deep learning classifier, InceptionTime, in terms of accuracy, achieving this with\nreduced training time and comparable scalability.", "sections": [{"title": "1. Introduction", "content": "Time series data is extensively applied in various domains,\nincluding weather modeling, retail operations, financial fore-\ncasting, and many other sectors. This paper specifically focuses\non the classification of univariate time series data. In academia,\nthis field primarily consists of two methodological categories:\ndistance-based and feature-based approaches. Distance-based\nmethods, which serve as the foundational baseline in the field,\nclassify data by computing similarities in the original raw time\nseries, using pre-established distance metrics like Dynamic\nTime Warping (DTW) or Euclidean distance. DTW is no-\ntably effective in handling translational variances compared to\nthe Euclidean distance. On the other hand, feature-based ap-\nproaches extract feature vectors from the raw data and then em-\nploy classifiers such as Support Vector Machines (SVM), lo-\ngistic regression, and decision trees to determine classification\nresults.\nRecent years have seen a growing body of research utiliz-\ning deep learning, especially deep convolutional network tech-\nniques, to address TSC challenges. Typically, these methods\nbegin by extracting features using a deep network, and then\nproceed to classification through a FC layer combined with a\nSoftMax function. In the training phase, the FC layer funda-\nmentally learns a weight matrix $W_{dc}$ where d denotes the di-\nmension of the feature vector, and c indicates the number of\nclasses. This process is tantamount to learning a proxy(d *1\ndimension) for each class, leading to the convergence of fea-\ntures from a particular class near their respective proxy. During\nthe testing phase, the classification is determined by applying\nthe SoftMax to the distances between the test sequence features\nand the proxies for all classes."}, {"title": "2. Related work", "content": "The time series classification (TSC) problem represents a\nfoundational challenge within the domain, with the academic\ncommunity having proposed a multitude of effective algo-\nrithms to contend with this intricacy. Backoff-2023[4] sum-\nmarizes the current eight state-of-the-art (SOTA) Time Se-\nries Classification (TSC) classifiers, which are HIVE-COTE\n2.0[5], Hydra-MR[6], InceptionT[3], RDST[7], WEASEL-\nD[8], RSTSF[9], FreshPRINCE[10] and PF[11]. We categorize\nthem into ensemble-based and feature-based methods. Among\nthese, the most accurate ensemble method is HIVE-COTE 2.0,\nwhich is also currently the best time series classifier. It employs\nmore advanced ensemble techniques compared to its predeces-\nsor, Hive-COTE 1.0[12]. It includes the following: STC, Tem-\nporal Dictionary Ensemble (TDE)[13], Diverse Representation\nCanonical Interval Forest (DrCIF), and Arsenal. Among these\nmethods, DrCIF, developed by the authors of this paper, ex-\ntends the Canonical Interval Forest (CIF) [14]. Meanwhile, Ar-\nsenal, an ensemble of compact ROCKET classifiers, generates\nvaluable probability values for each class during predictions us-\ning CAWPE. Presently, these ensemble methods represent the\nstate-of-the-art in time series classification. Nevertheless, they\ntypically exhibit high time complexity, posing substantial chal-\nlenges for practical application. In our research, we employed\nan ensemble approach that balances high accuracy with com-\nparatively lower time complexity.\nHydra-MR stands as the most accurate feature-based method\nto date, achieving an excellent balance between accuracy and\ntime consumption. It amalgamates the Hydra algorithm[6] with\nthe MR (MultiRocket) model[15]. Building on the founda-\ntional work of Rocket[16] and MiniRocket[17], MultiRocket\nintroduces a variety of pooling operations and transformations,\nenhancing the diversity of feature distributions. This advance-\nment not only boosts classification accuracy but also maintains\ncomputational efficiency. HYDRA, a dictionary-based method,\ntransforms input time series data using a collection of randomly\nselected convolutional kernels grouped together. It quantifies\nthe frequency of kernels that most closely match the input time\nseries at each point in time. These quantifications are then uti-\nlized to train a linear classifier."}, {"title": "2.1. State-Of-The-Art Time Series Classifiers", "content": "The current academic emphasis on resolving TSC challenges\npredominantly resides in the domain beyond deep learning,\nwhere non-deep learning methods prevail in terms of both\nprevalence and performance. DL-review[18], as an influential\nwork in the field of Time Series Classification (TSC) within\nthe deep learning domain, summarizes nine advanced deep\nlearning-based time series classifiers, including: Resnet[2],\nFCN[2], Encoder[19], MLP[2], Time-CNN[20], TWISEN[21],\nMCDCNN[22], MCNN[23], and t-LeNeT[24]. Among these,\nResnet and FCN exhibit relatively optimal performance. Resnet\nconsists of three residual blocks, each followed by a Global Av-\nerage Pooling (GAP) layer and a softmax classifier at the end.\nThe number of neurons in the classifier corresponds to the num-\nber of classes in the dataset. Within each residual block, three\nconvolutions are initially performed, the output of which is\nadded to the block's input before being passed to the subsequent\nlayer. FCN comprises three convolutional blocks, each contain-\n      ing three sequential operations: a convolution, batch normal-\nization, and a ReLU activation function. The output of the third\nconvolutional block undergoes averaging across the entire time\ndimension, forming the Global Average Pooling (GAP) layer.\nSubsequently, a conventional softmax classifier is connected to\nthe output of the GAP layer. Subsequently, it was found in the\nthat directly ensembling these deep learning models could\nfurther enhance the algorithm's performance. Based on this dis-\ncovery, InceptionT[3] was initially inspired by the Inception-\n      v4[26] network from computer vision tasks and designed the\n\"AlexNet\" of TSC - the Inception net. It then ensembled these\nfive models, ultimately achieving an accuracy on UCR85 com-\nparable to HIVE-COTE 1.0. Moreover, Backoff-2023[4] com-\nprehensively reviews and summarizes numerous deep learning\nmodels in current TSC tasks, ultimately concluding that Incep-\ntionT is currently the best deep learning-based time series clas-\nsifier."}, {"title": "3. Method", "content": "In this section, we present the overall network framework of\nthe ECR module, which serves as the ensemble component of\nECRTime. Subsequently, detailed descriptions of each module\nwithin the network are provided. Ultimately, the final result\nis generated through a straightforward but effective ensemble\nstrategy."}, {"title": "3.1. ECR Framework", "content": "Fig. 5 illustrates the overall structure of the ECR, which in-\ncludes two stages: training and inference, as shown in Fig. 5(a)\nand Fig. 5(b) respectively.\nWe use $x \\in R^{1xL}$ to represent a single time series, 1 to in-\ndicate that it is univariate, and L to represent the length of the\nseries. During the training phase, each mini-batch input is de-\nnoted as X, the input label set as Y, and the batch size as B. Each\nmini-batch includes C different categories, and each category\ncontains m samples, thus resulting in $B = C \\times m, X \\in R^{B \\times 1 \\times L}$,\n$Y \\in \\{1, 2, ..., C\\}^B$.\nAs shown in Fig. 5(a), the training framework consists of two\nindependent forward branches: one trained based on classifica-\ntion method and the other based on retrieval method, both shar-\ning the same input X. During training, X passes through the\nclassification backbone and retrieval backbone to extract fea-\nture vectors $F_{cls} \\in R^{B \\times d \\times L}$ and $F_{Ret} \\in R^{B \\times d \\times L}$, respectively.\nNote that the two backbones have the same structure but do not\nshare weights. d represents the number of channels in the fea-\nture vector. Subsequently, $F_{cls}$ and $F_{Ret}$ are both dimensionally\nreduced to $F'_{CLS} \\in R^{Bxd}$ and $F'_{Ret} \\in R^{Bxd}$ through Global Av-\nerage Pooling(GAP) on the L dimension. In the classification\nbranch, $F'_{cls}$ is followed by a Fully Connected Layer, and the\noutput is then fed into the classification loss for learning. In\nthe retrieval branch, $F'_{Ret}$, after undergoing L2 norm operation, is\ninput into the retrieval loss.\nIn the inference phase shown in Fig. 5(b), for the test se-\nquence, features are extracted based on the backbone networks\nin the two branches, and then both are reduced in dimension\nand normalized through Global Average Pooling and L2 Nor-\nmalization. Additionally, we will pre-extract features of all se-\nquences in the training set in this manner and construct both a\nclassification feature library and a retrieval feature library. Sub-\nsequently, the classification and retrieval features of the test se-\nquence are compared with corresponding library features in the\nDistance module. If there are N features in the library, 1 \u00d7 N\nclassification distance vectors and retrieval distance vectors are\noutputted respectively. Finally, the predicted category is out-\nputted by ensembling these two types of distances. In the fol-\nlowing sections, we will specifically introduce the key modules\nin the training and inference, including: Backbone, Loss, Dis-\ntance and Ensemble."}, {"title": "3.2. Backbone network structure", "content": "During these years of rapid development in deep learning,\nmany backbone networks for feature extraction have emerged,\nyet ResNet[27], based on residual connections, remains the pre-\nferred choice in numerous application scenarios. The residual\nstructure, without introducing additional parameters and com-\nputational load, can effectively mitigate gradient vanishing on\none hand, ensuring the continuity of parameter learning; on the\nother hand, deeper networks can be constructed based on resid-\nual connections, and generally speaking, the deeper the net-\nwork, the stronger its feature extraction capability. Existing\nworks such as [2, 28, 29] have already applied ResNet in the\nTSC field. Based on the structures validated in these methods,\nwe designed the backbone module of this paper, as shown in\nFig. 6."}, {"title": "3.3. Loss", "content": "As shown in Fig. 5(a), the training phase employs two\ntypes of loss functions: the classification branch utilizes cross-\nentropy loss, denoted as $L_{cls}$, and the retrieval branch employs\nthe hard version of triplet loss, denoted as $L_{Ret}$. Based on the\nsymbol definitions in Section 3.1, $L_{cls}$ is defined in the follow-\ning form:\n$L_{cls} = (-\\frac{1}{B})\\sum_{i=1}^{B}\\sum_{j=1}^{C} 1(y=j)log [\\frac{e^{z_j}}{\\sum_{k=1}^{C} e^{z_k}}]$ (1)\nIn Eq. (1), z represents the vector obtained after $F'_{Cls}$ passes\nthrough the FC layer, and the indicator function $1(y = j)$ is\ndefined as follow:\n$1(y = j) = \\begin{cases}\n1, y = j\\\\\n0, y \\neq j\n\\end{cases}$ (2)\nBased on Eq. (2), Eq. (1) can be further simplified to the\nfollowing form:\n$L_{cls} = (-\\frac{1}{B})\\sum_{i=1}^{B}log [\\frac{e^{z_j}}{\\sum_{k=1}^{C} e^{z_k}}]$ (3)\nFor retrieval tasks, triplet loss[30] is generally used. It origi-\nnally works on an anchor series A, a positive sample P from the\nsame class and a negative sample N from a different class. The\nobjective is to minimize the distance between A \u2013 P, while push\naway the N. The formula of triplet loss is as follow:\n$L_{Triplet} = \\frac{1}{H_P}\\sum_{i=1}^{H_P}[||x_i - x_p||_2^2 - \\frac{1}{H_N}\\sum_{j=1}^{H_N}||x_i - x_N||_2^2 + \\alpha]_+$ (4)\nEq. (4) iterates over and calculates each sample in the mini-\nbatch input, taking the average as the final loss. In this context,\ng denotes the vector after L2 norm, H indicates the count of\npositive or negative samples. The term a represents the margin\nbetween positive and negative samples. Additionally, the sub-\nscripts P and N correspond to positive and negative samples,\nrespectively. The original triplet loss introduces many easily\nsatisfied triplets, which lack contribution to the training, lead-\ning to slower and less efficient convergence. Therefore, this\npaper follows the approach in [30] and uses hard triplet loss\nfor training. This loss narrows the distance between the anchor\nsample and the farthest positive, while increasing the distance\nbetween the anchor and the closest negative, defined as follow:\n$L_{Ret} = [ max_{i \\in [0, H_P)} (||g_A - g_P||_2^2) - min_{j \\in [0, H_N)} (||g_A - g_N||_2^2) + \\alpha]_+$ (5)"}, {"title": "3.4. Distance", "content": "In the training phase, two pivotal modules, the Backbone\nand the Loss, have been delineated earlier. Subsequently, in\nthe testing phase, we elucidate the Distance module, conceived\non the 1-NN classifier paradigm. As depicted in the testing\nprocedure (Fig. 5(b)), each time series input $x^1$ undergoes fea-\nture extraction via the classification and retrieval backbone,\nfollowed by processing through GAP and L2Norm, yielding\noutput vectors $f_{cls}^1$ and $f_{ret}^1$. Concurrently, features are ex-\ntracted from every sequence in the training set in a similar fash-\nion, leading to the formation of two libraries: the classifica-\ntion library $F_{cls_lib} = \\{ j \\in [0,N)|f_{cls}^j \\}$ and the retrieval library"}, {"title": "3.5. Ensemble", "content": "Following the Distance module is the Ensemble module,\nand this paper involves two stages of ensemble operations,\nas illustrated in Fig. 7. The first stage ensembles classifi-\ncation and retrieval models to obtain ECR, and the second\nstage ensembles multiple ECRs to derive the final ECRTime\nmodel. In the first ensemble, as shown in Fig. 5(b), we av-\nerage all corresponding elements in sets $D(f^1_{cls}, F_{cls_lib})$ and\n$D(f^1_{ret}, F_{ret_lib})$ to obtain the final distance set $D(f^1, F_{lib}) =\n\\{(D^1_{Cls} + D^1_{ret})/2, (D^2_{Cls} + D^2_{ret})/2,..., (D^N_{Cls} + D^N_{ret})/2\\}.\nFinally, the category of the corresponding sequence in the li-\nbrary, predicted for the test sequence $x^1$, is identified by taking\narg min(*) of $D(f^1, F_{lib})$.\nIt is important to note that L2 normalization is applied to the\nclassification features during testing to ensure uniformity in the\nvalue ranges of $D(f_{cls}, F_{cls_lib})$ and $D(f_{ret}, F_{ret_lib})$. This unifor-\nmity is crucial as it prevents the averaging of prediction results\nfrom being skewed by differing value ranges. Additionally, for\ntwo vectors with an L2 norm of 1, their Euclidean distance can\nbe reformulated as $\\sqrt{2(1 - cos\\theta)}$ where $\\theta$ is the angle between\nthe vectors. This ensures that both parties being averaged have\nthe same value range, which is [0, 2].\n$\\overline{y_{i,c}} = \\frac{1}{N}\\sum_{j=1}^{N} \\sigma(x_i, \\theta_j) , [\\forall_{i} \\in [1, C]]$ (6)\nFurthermore, in the second ensemble, based on Eq. (6), mul-\ntiple ECRs are integrated to obtain the final ECRTime model\npresented in this paper. In the formula, $y_{i,c}$ represents the en-\nsemble's output probability that the input time series, $x_i$ belongs\nto class c, This is equivalent to the average logistic output $\\sigma$\nacross n randomly initialized ECRs."}, {"title": "4. Experiments and results", "content": "In this section, we evaluate the ECR and ECRTime on 112\ndatasets in the UCR univariate time series archive. ECRTime\nrefers to an ensemble of three ECR modules, while the \"ECR-\nTime(n)\" notation is used to denote an ensemble of n ECR mod-\nules. \"UCR112\" is used to denote the 112-version of the UCR\narchive in the following text. The experimental results are avail-\nable on the website\u00b9. Initially, the experimental setup is intro-\nduced.\nDatasets and SOTAS: To compare with numerous advanced\nalgorithms while avoiding excessively time-consuming experi-\nments, we followed the approach used in [5, 4], conducting ex-\nperimentation with the 112 equal length problems in the 2019\nversion of the UCR archive. The comparison includes classi-\nfiers based on deep learning summarized in DL-review[18] and\nstate-of-the-art classifiers in the field of Time Series Classifi-\ncation(TSC) compiled in Backoff-2023[4]. The corresponding\ncomparison results are respectively sourced from 2 and 3.\nConfiguring ECRTime: Since the datasets in the UCR only\nconsist of training and test sets, lacking a validation set, it is\nnot possible to tune hyperparameters such as epochs. There-\nfore, we refer to the ResNet classifier in DL-review[18] for hy-\nperparameter settings. During the training phase, we set each\nmini-batch input to contain 4 categories, with 4 samples per cat-\negory, resulting in a batch size of 16. The margin in the hard\ntriplet loss is set to 0.1, and the optimizer used is Adam. For\nthe classification branch, the learning rate is set at 1e-3, and for\nthe retrieval branch, the learning rate is set at 1e-4. The sched-\nuler used is ReduceLROnPlateau from PyTorch, the number of\ntraining epochs is set to 1500. Experimental environment con-\nfiguration: PyTorch 2.0, Python 3.9."}, {"title": "4.1. Experiment setup", "content": "To facilitate a comparison with a range of deep learning\nclassifiers, we evaluated ECR against the eight methods de-\ntailed in DL-review[18]. For result validation, we adhered to\nDL-review's methodology, training ECR for five iterations on\nUCR112. During these iterations, only the random seed var-\nied, while the model's structure and training hyperparameters\nremained unchanged. The final reported accuracy represents\nthe mean of these iterations. Fig. 8 features a critical differ-\nence diagram that illustrates the accuracy comparisons between\nECR and various deep learning models. The horizontal thick\nline across different models indicates no significant difference\nbetween them (p-value>0.05). It is noted that ECR significantly\noutperforms all methods depicted in the figure (p-value<0.05),\nincluding ResNet and FCN, which were identified as the most\nprecise deep learning classifiers in DL-review at that time. In\nthe subsequent Section 4.5.5, the discussion will focus on how\nECRTime, an ensemble of ECR, markedly surpasses ECR, in-\ndicating that ECRTime also surpasses the aforementioned deep\nlearning-based methods. Consequently, to reduce the redun-\ndancy of the experiments, we refrained from conducting a par-\nallel comparative analysis for ECRTime as in Fig. 8."}, {"title": "4.2. Comparing with deep learning-based methods", "content": "In the preceding section, we discussed ECR's significant\noutperformance of most deep learning classifiers on UCR112.\nThis section broadens the comparative analysis to include all\nTime Series Classification (TSC) methods. We evaluated ECR\nagainst the eight leading state-of-the-art (SOTA) classifiers\nlisted in Backoff-2023[4], namely: (1) HIVE-COTE 2.0[5], (2)\nHydra-MR[6], (3) InceptionT[3], (4) RDST[7], (5) WEASEL-\nD[8], (6) RSTSF[9], (7) FreshPRINCE[10], and (8) PF[11]. Of\nthese, HIVE-COTE 2.0 is recognized as the most accurate al-\ngorithm for TSC issues, albeit with considerable computational\ndemands. InceptionT, on the other hand, is currently the most\naccurate deep learning-based TSC classifier. Detailed analysis\nof these SOTA classifiers can be found on website 4. Adhering\nto the methodology outlined in Backoff-2023, we conducted\ntraining and testing on the original UCR112 train/test set, as\ndepicted in Fig. 9. The results indicate that ECR outperforms\nPF (the leading distance-based method), FreshPRINCE (the\ntop feature-based method), and RSTSF (the foremost interval-\nbased method). However, it is surpassed by the other five\nmethodologies.\nTo extend our comparative analysis, we utilized the more ef-\nficacious ECRTime, an ensemble of three ECR models, against"}, {"title": "4.3. Comparing with SOTAs", "content": "Upon synthesizing the comparison results, it becomes clear\nthat the ECRTime model introduced in this study matches the\ncurrent top-performing deep learning classifier, InceptionT, in\nterms of overall effectiveness. To conduct a more detailed\nanalysis of their strengths and weaknesses, we compared them\nbased on the dataset types present in UCR112, namely the se-\nquence source types. As depicted in Fig. 12(a), UCR112 en-\ncompasses 13 dataset types[4]. The DEVICE, IMAGE, MO-\nTION, SENSOR, SIMULATED, and SPECTRO categories,\nwhich constitute 85% of the datasets, are the most significant,\nwith DEVICE and SIMULATED being the smallest (9 datasets\neach) and IMAGE being the largest (32 datasets). The \"oth-\ners\" category comprises 7 dataset types, each containing only\n1-3 datasets. Due to the dominance of the first six categories in\nUCR112, we utilized boxplots to illustrate the accuracy vari-\nances between ECRTime and InceptionT across these cate-\ngories. As shown in Fig. 12(b), ECRTime exceeds InceptionT\nin the DEVICE and SENSOR categories, equals InceptionT in\nthe IMAGE and SPECTRO categories, and is marginally less\neffective than InceptionT in the MOTION and SIMULATED\ncategories. These insights provide valuable guidance for re-\nsearchers in choosing the most suitable approaches for diverse\npractical applications."}, {"title": "4.4. Runtime analysis", "content": "To enable a comparison of time efficiency with ECRTime,\nwe extracted the average training time of various state-of-the-\nart methods on UCR142 from Backoff-2023 as an approximate\nfor the time spent on UCR112. These comparative findings are\npresented in Table 2. ECRTime exhibits a reduced training du-\nration compared to InceptionT. While it is more time-intensive\nthan other CPU-based methods like RDST and Hydra-MR,\nleveraging parallel training on multiple GPUs can decrease its\ntraining time, as ECRTime is GPU-based. It is important to note\nthat for this study, ECRTime was trained using an RTX3060\nGPU, a less powerful consumer-grade graphics card. Utilizing\nmore advanced graphics cards could lead to a further decrease\nin training duration."}, {"title": "4.5. Sensitivity study", "content": "We explore the effect of key parameter choices on accuracy\nover UCR112 for ECR and ECRTime:\n1-NN classifier versus SoftMax classifier.\nhard triplet loss versus triplet loss.\nensemble in ECR.\nensemble in ECRTime.\nbatch size."}, {"title": "4.5.1. 1-NN classifier versus SoftMax classifier", "content": "In Section 1, the phenomenon of \"inter-class similarity and\nintra-class inconsistency\" within UCR datasets was explored,\nand its adverse effect on the SoftMax classifier was analyzed.\nConsequently, the implementation of a 1-NN classifier was\nsuggested as a potential mitigation strategy. This subsection\ndetails comparative experiments conducted on UCR112, con-\ntrasting the 1-NN classifier with the SoftMax classifier, based\non ECR's classification sub-model. As depicted in Fig. 14(a),\nthe integration of a 1-NN classifier with the classification net-\nwork backbone shows a marginally better performance than the\n\"FC+SoftMax\" approach, though the difference is not statisti-\ncally significant (p-value>0.05). Specifically, the average ac-\ncuracies of these two methods on UCR112 were calculated to\nbe 84.04% and 83.52%, respectively, signifying a modest im-\nprovement of 0.5% with the 1-NN classifier. Remarkably, for\nthe HEMODYNAMICS-type PigCVP dataset, accuracy using\nthe SoftMax classifier was a mere 31.25%, which notably in-\ncreased to 87.98% when employing the 1-NN classifier, offer-\ning insightful implications for practical applications."}, {"title": "4.5.2. hard triplet loss versus triplet loss", "content": "To assess the impact of hard triplet loss versus triplet loss\non the model, we separately trained the retrieval sub-model of\nECR on UCR112 using each loss type. The findings, as illus-\ntrated in Fig. 14(b), indicate that hard triplet loss significantly\nenhances performance compared to standard triplet loss. No-\ntably, the model employing hard triplet loss surpasses the lat-\nter in 82 of the 112 datasets, often by margins exceeding 5%,\nand with the greatest improvement approaching 50%. Further-\nmore, while the model underperforms relative to the triplet loss\nin 23 datasets, the performance decrease generally remains be-\nlow 5%. These outcomes establish hard triplet loss as a consid-\nerably more effective option than traditional triplet loss."}, {"title": "4.5.3. batch size", "content": "The critical difference diagram presented in Fig. 17 eluci-\ndates the effect of batch size on ECR's performance. A horizon-\ntal line across different models in the diagram suggests no sub-\nstantial difference in their performance across the 112 datasets,\nwith a slight advantage for ECR(batch size equal to 16). Addi-\ntionally, the diagram indicates that at batch sizes of 64 and 128,\nECR does not demonstrate a significant benefit over ResNet (p-\nvalue>0.05). ECR's performance markedly surpasses that of\nother deep learning methods only at batch sizes of 32 and 16\n(p-value<0.05). In this study, the chosen default batch size for\nECR is 16."}, {"title": "4.5.4. ensemble in ECR", "content": "Fig. 15 shows the pairwise comparison results of the re-\ntrieval module vs. classification module, ECR vs. classifica-\ntion module, and ECR vs. retrieval module. As depicted in\nFig. 15(a), a parity in performance is observed in only 17 of the\n112 datasets, while each module exhibits strengths in the re-\nmaining 95 datasets. Remarkably, the retrieval module secures\na performance edge exceeding 20% in datasets such as PigAir-\n      wayPressure and Wine. To harness the benefits of both mod-\nules, we integrated the classification and retrieval submodules\nto form the ECR model. This integration's efficacy, as demon-\nstrated in Fig. 15(b), Fig. 15(c), and Fig. 18, reveals that the\ncomposite ECR model surpasses the performance of each indi-\nvidual submodule, thereby confirming the ensemble strategy's\neffectiveness. Additionally, an evaluation of ECR in contrast to\nclassification(2) and retrieval(2), detailed in Fig. 16, establishes\nthat ensembling two classification submodules or two retrieval\nsubmodules is less effective compared to an ensemble compris-\ning one of each. Classification(2) refers to the approach of en-\nsembling two classification models, following the method out-\nlined in Eq. (6). Similarly, retrieval(2) follows a comparable\napproach. This finding underscores the complementary nature\nof the submodules within ECR, effectively balancing their re-\nspective strengths and limitations."}, {"title": "4.5.5. ensemble in ECRTime", "content": "The final ECRTime model presented in this study, which at-\n      tains enhanced performance through the integration of multiple\n      ECRs, was subjected to a comparative analysis focusing on the\n      number of modules in the ensemble as a key hyperparameter.\n      Fig. 19 illustrates that the performance of ECRTime signifi-\n      cantly increases when the number of ECRs increases from 1\n      to 3. However, further expansion to 4 and 5 does not yield a no-\n      table improvement in performance, while concurrently increas-\n      ing training duration. To strike an optimal balance between ac-\n      curacy and computational efficiency, thereby boosting practical\n      usability, this study finalizes the ensemble at three modules.\n      A detailed examination of the enhancements achieved by en-\n      sembling three ECR models is conducted through a pairwise\n      comparison between ECRTime and ECR on UCR112, employ-\n      ing scatter charts as presented in Fig. 16(c). This figure re-\n      veals that the ensembled ECRTime model exhibits improve-\n      ments in 62 of the 112 datasets, though these improvements\n      predominantly fall within a 5% range. A marginal decrease\n      in performance is observed in 16 datasets, while the remain-\n      ing 34 datasets exhibit no variation. Furthermore, a post-hoc\n      statistical analysis confirms a significant distinction between\n      ECRTime and ECR (p-value<0.05). In conclusion, ECRTime\n      demonstrates better performance than ECR in time series clas-\n      sification tasks."}, {"title": "5. Conclusion", "content": "In the domain of deep learning-based time series classifi-\n      cation employing the \"FC+SoftMax\" paradigm, replacing the\n      SoftMax classifier with a 1-NN classifier has resulted in en-\n      hanced performance. Furthermore, to explicitly adapt to the\n      classification objectives of the 1-NN classifier, we innovatively\n      introduce a deep learning-based retrieval method for TSC is-\n      sues. By combining this with the classification model in an\n      ensemble, we present the ECRTime framework in this paper.\n      ECRTime exhibits a highly competitive and advanced stan-\n      dard in terms of accuracy and time complexity, matching or\n      surpassing current state-of-the-art (SOTA) methods in Time Se-\n      ries Classification (TSC) tasks. In future research, we aim to\n      delve deeper into the potential applications of retrieval methods\n      within time series classification and to expand our exploration\n      into the realm of multi-dimensional time series classification."}]}