{"title": "Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition", "authors": ["Hao Fei", "Shengqiong Wu", "Wei Ji", "Hanwang Zhang", "Meishan Zhang", "Mong Li Lee", "Wynne Hsu"], "abstract": "Existing research of video understanding still struggles to achieve in-depth comprehension and reasoning in complex videos, primarily due to the under-exploration of two key bottlenecks: fine-grained spatial-temporal perceptive understanding and cognitive-level video scene comprehension. This paper bridges the gap by presenting a novel solution. We first introduce a novel video Multimodal Large Language Model (MLLM), MotionEpic, which achieves fine-grained pixel-level spatial-temporal video grounding by integrating video spatial-temporal scene graph (STSG) representation. Building upon MotionEpic, we then develop a Video-of-Thought (VoT) reasoning framework. VoT inherits the Chain-of-Thought (CoT) core, breaking down a complex task into simpler and manageable sub-problems, and addressing them step-by-step from a low-level pixel perception to high-level cognitive interpretation. Extensive experiments across various complex video QA benchmarks demonstrate that our overall framework strikingly boosts existing state-of-the-art. To our knowledge, this is the first attempt at successfully implementing the CoT technique for achieving human-level video reasoning, where we show great potential in extending it to a wider range of video understanding scenarios.", "sections": [{"title": "1. Introduction", "content": "Enabling learning models to accurately interpret video data is one of the most paramount goals in the relevant community. In the current research, while there has been extensive exploration into building models for video action and dynamics recognition"}, {"title": "2. Related Work", "content": "A key objective in the intelligence community is the understanding of various modalities of data. Currently, with the advent of LLMs such as ChatGPT"}, {"title": "3. MotionEpic: Fine-grained Spatial-temporal Grounded Video MLLM", "content": "In this section, we describe the MotionEpic video MLLM, and elaborate on how the STSGs are integrated, as well as the fine-grained spatial-temporal grounding-aware tuning."}, {"title": "3.1. Architecture Briefing", "content": "Fig. 2 presents a schematic overview of MotionEpic, where MotionEpic takes as input three sources: text prompt, video, and STSG representation of video. We follow the most common practice, and employ the Vicuna-7B (v1.5) as the backbone LLM. To perceive video input, we adopt the ViT-L/14 encoder and Q-Former projector . We also design MotionEpic to support the STSG signal, where we retrofit the Graph Transformer with recurrent propagation to encode the multi-frame STSG information."}, {"title": "3.2. Integrating STSG Representation", "content": "By definition , an STSG consists of a sequence of single SGs corresponding to all video frames, with each SG comprising triplets in the video frame, i.e., 'subject'-'predicate'-'object', where 'subject' and 'object' refer to two visual proposals (RoIs) that are connected with the 'predicate' relationship. STSG intuitively depicts the underlying core semantics representations of videos while filtering the less-informative background information, aiding the perceptive understanding of videos. Also, such fine-grained structural feature helps effectively model the compositional spatiotemporal semantics.\nIn our practice, we slightly retrofit the vanilla STSG definition to meet the demand in our reasoning framework. Since a video has redundant temporal contents across frames, we first evenly sample the frames (with a proper sampling rate), which can effectively reduce computation costs. We denote each single SG at k-th frame as $G_k=(V_k; E_k)$, where $V_k$ is a list of nodes, i.e., object proposal, and $E_k$ is a list of predicate edges. For each object proposal $v_{k,i}$ we record the category label $c_i$, the proposal's neural representation $f_i$, and the bounding box (bbox) annotation $b_i=(x, y, w, h)$, i.e., the 2D coordinate in the image. Thus, each $v_{k,i}=(C_i, f_i, b_i)_k$. All nodes (i.e., $U_{k,i}$ and $U_{k,j}$) are connected with edges $e_{k,i,j}$.\nTo enhance the connectivity of STSG, we further create a type of temporal coreference edges across each single-frame SG, where the same objects are linked together with time-persistent edges, $e_{k\u22121\u2192k}$, mimicking the 'tracking' process.\nMotionEpic achieves fine-grained spatial-temporal video grounding by simultaneously understanding and generating STSGs. After full tuning, MotionEpic can directly output (partial) STSG based on input video (with text prompts), essentially grounding the specific portions of the video content as indicated in the input prompts. In Fig. 3 we illustrate how the generated STSG expression corresponds to the structural STSG. Further, the output STSG serving as the rationale will be recycled in the system, i.e., repurposed as the input for the subsequent round."}, {"title": "3.3. Fine-grained Video-Scene Grounding-aware Tuning", "content": "Intuitively, we expect our system to perform video reasoning for downstream tasks without relying on any external STSG annotations, i.e., STSG-free inference. This requires an accurate spatial-temporal grounding between videos and STSGs. To this end, we carry out tuning for MotionEpic such that it is learned to autonomously parse STSG according to input instructions. The grounding-aware tuning is performed based on video-STSG pairs. We design various training objectives, which can be further divided into coarse-grained and fine-grained levels:\n1) Enhancing coarse-grained correspondence:\n\u2022 $L_1$: predicting if the overall input video and STSG are paired.\n\u2022 $L_2$: given a video, generating the whole STSG (expression) of the video.\n2) Enhancing fine-grained correspondence:\n\u2022 $L_3$: given a video and action description(s), outputting the corresponding object tracklet(s), i.e., a partial STSG.\n\u2022 $L_4$: given a video and key object(s), describing the corresponding temporal action(s) in textual response, and outputting the corresponding object tracklet(s).\n\u2022 $L_5$: given a video and a bbox of a certain frame's object, outputting the object label, as well as the corresponding tracklet.\nFor each learning objective, we wrap up the inputs with instruction-tuning style question-answer pairs, being consistent with the following downstream inference. Overall, except for the STSG encoder and video projector, the video encoder and the backbone LLM are kept frozen throughout all the learning stages. To tune the LLM, we leverage LoRA to enable a small subset of parameters to be updated.\nBefore conducting the above grounding-level tuning, we perform conventional video pre-training on Webvid, which serves as the important warming starting for the following video understanding tuning, Despite aligning the encoding modules with LLM, there remains a gap towards the goal of enabling the overall system to faithfully follow and understand users' instructions and generate the desired outputs. To address this, further instruction tuning is necessary. After the grounding-level tuning, we utilized existing video instruction tuning data for instruction tuning of the model, which includes the dataset from VideoChat and Video-ChatGPT."}, {"title": "4. Video-of-Thought Reasoning Framework", "content": "Based on MotionEpic, we now perform video reasoning with VoT. Different from the vanilla CoT with one straightforward prompt, i.e., \"Let's think step by step\", VoT divides the raw problem into much smaller and finer-grained sub-problems. We consider an exact paradigm of task decomposition, which encompasses five chained steps, following a process from low-level perceptive pixel grounding to high-level cognitive semantic comprehension. In Fig. 4 we illustrate the overall VoT framework."}, {"title": "Step-1: Task Definition and Target Identification", "content": "First, MotionEpic is fed with the raw video along with the text prompt of the task definition, format, and raw question, all of which serve as the background foundation information of the reasoning. As the initial phase, we expect to identify the target within the video that requires analysis, which is a crucial prerequisite for determining the subsequent in-depth reasoning. It is noteworthy that sometimes the question may explicitly include targets visible in the video, or implicitly involve related targets. Therefore, we proceed to prompt the model, to infer from the question what the target object(s) involved or related to in the video might be:\nGiven the question [Question], what are the possible targets of the mainly mentioned or involved?\nAfter this step, all the possible [Target] involved in the question will be confirmed."}, {"title": "Step-2: Object Tracking", "content": "In the second step, we aim to further ground the object's full spatial-temporal characteristics, i.e., to track the target's trajectory. We note that grounding the targets' temporal tracking is pivotal for pursuing fine-grained video understanding, as only accurately perceiving the behaviors in the video can ensure that the subsequent cognitive-level understanding is meaningful. In this work, we leverage the STSG for the temporal grounding, rather than directly tracking the original video frames. Such semantic representation carried by STSG is highly concise, ensuring that the tracking of the video's target is more accurate and reliable. Also notably, object tracking and pixel grounding based on the STSG can effectively mitigate the hallucination issues inherent in existing MLLMs.\nHaving performed grounding-aware tuning, MotionEpic develops the full capability to ground from object to (partial) STSG. Therefore, we directly prompt the model with:\nProvide the tracklet of involved [Target] by outputting the corresponding partial expression.\nThe yielded grounded [Target Tracklet] of STSG will serve as low-level evidence (i.e., supporting rationale) for the next step of behavior analysis."}, {"title": "Step-3: Action Analyzing", "content": "In this step, VoT further analyze the corresponding actions and behaviors by integrating the target tracking in STSG. For an accurate understanding of the target object's motion, merely observing the target itself is insufficient. This process should also reference the higher-order neighbor nodes within the STSG representation, interacting targets with their neighboring scenes. On the other hand, directly inferring actions from video pixels alone is still inadequate, as interpretations based solely on pixel information often remain superficial. Therefore, we further prompt the model to consider more potentially relevant commonsense knowledge, allowing the model to connect video pixel observations with the factual world, achieving a more in-depth understanding of the video. Given that MLLMs possess the necessary repository of commonsense knowledge via extensive pre-training, all that is required is to properly prompt the model:\nCombining all possible related commonsense, analyze the motion behavior based on the [Target Tracklet] and the neighbor scenes within. Describing the action observations and implications.\nThis step yields the target action's [Observation and Implication]."}, {"title": "Step-4: Question Answering via Ranking", "content": "Having established an in-depth understanding of the target actions in the video, we can now consider answering the original question. We contemplate a multiple-choice QA format, where multiple candidate answers are provided. Inspired by the human pattern of answering multi-choice questions, we also consider a ranking mechanism to determine the final answer. Specifically, for each candidate answer, we prompt the model to score its likelihood (from 1 to 10) in conjunction with commonsense knowledge, and provide a corresponding rationale:\nFor the question [Question], given a candidate answer [Answer], please based on the action's [Observation and Implication] combined with commonsense, score the rationality of this answer with a 1-10 scale, and also output the rationale.\nWe then rank the scores of all options and select the most optimal answer [Answer]."}, {"title": "Step-5: Answer Verification", "content": "Given that complex video task often involves intricate questions and answers, and the entire reasoning process encompasses lengthy chained steps, it is essential to verify the answer provided in the previous step. Our basic idea to verification is that, assuming that answer A is correct, we retrospectively evaluate whether the answer results in contradictions with the input question and video in two aspects: 1) First, check the pixel grounding information if it aligns with the facts presented in the video from a perception standpoint. 2) On the other hand, prompt the model again from a cognition perspective to determine if the commonsense implications inherent in the answer contradict any of the main observations inferred in the 3-rd reasoning step.\nGiven the , and the raw question [Question], now you need to verify the previous answer by\n1) checking the pixel grounding information if the answer [Answer] aligns with the facts presented in the video from a perception standpoint;\n2) determining from a cognition perspective if the commonsense implications inherent in the answer contradict any of the main [Observations]inferred in the 3-rd reasoning step.\nOutput the verification result with rationale."}, {"title": "5. Experiments", "content": "5.1. Settings\nTask and Data. While in theory all video understanding tasks could benefit from our reasoning framework, we mainly focus on the most representative task, video QA. For fine-tuning setting, we adopt 6 benchmarks characterizing complex video QA where advanced video abilities, e.g., explanation, causality, foresight and imagination are required: VLEP , STAR , IntentQA , Social-IQ , Causal-VidQA and NExT-QA . For zero-shot setting, we further consider using MSR-VTT and ActivityNet datasets. All datasets come with their own splitting, and we follow the prior practice without modification.\nGrounding-aware Tuning Corpus. To construct the video-STSG pairs, we leverage the Action Genome data , which contains 10K high-quality manual annotated STSGs of videos. To enrich the data amount, we also use part of WebVid-10M videos , where we select 350K videos with clear actions, and parse the STSGs via SoTA parser .\nImplementations. MotionEpic uses the Vicuna-7B (v1.5) as the backbone LLM. We adopt the ViT-L/14 as the video encoder, and use the Q-Former as the projector. All the modules take the default configurations without much modification. For our recurrent Graph Transformer encoding STSGs, we take a 6-layer architecture with 768-d hidden sizes. The object neural representation $f_i$ is obtained via CLIP, which will be used as node embedding initiation. The text tokenizer is sourced from LLaMA, with approximately 32,000 classes. For each video, we uniformly sample certain frames with a sampling rate of 8 fps for fine-grained reasoning. We note that too large sampling rate introduces noises (i.e., redundant frames) and huge computation costs, while too small one will cause important information loss. Here we use the 8 fps, as in our preliminary study we verified that it helps achieve the best trade-off. For the fine-tuning setting of end tasks, we will tune the MotionEpic based on the training set using the setting as prior baselines, i.e., data split and evaluation methods. For the zero-shot setting, we will directly perform video QA without using the in-domain training set."}, {"title": "5.2. Main Performance on Video QA Reasoning", "content": "In Table 1, 2 and 3 we present the main results of different systems. Overall, our MotionEpic under the VoT reasoning framework has boosted all the SoTA baselines by very large margins consistently. Beyond enhanced performance, we further gain some key observations. First, by observing Video-LLaVA without/with CoT prompting, we see that the improvement from CoT for video reasoning could be quite limited. Further, by comparing Video-LLaVA without/with STSG integration, we notice that the structural fine-grained STSG features play a positive role in understanding videos. Third, by comparing Video-LLaVA+STSG with our MotionEpic under the same CoT, it is clear that the implicit integration of the scene graph features is quite superior to the explicit integration. Also, even our MotionEpic with vanilla CoT we beat the SoTA methods on certain datasets. Lastly, observing the MotionEpic under CoT and our proposed VoT, we see there are huge performance gaps in between consistently on all reasoning scenarios and tasks, indicating the great potential of our proposed video reasoning framework."}, {"title": "5.3. Zero-shot Performance", "content": "We then examine the performance in zero-shot setting. Table 4 presents the comparisons. In general, we can notice that CoT exhibits stronger improvements than direct prompting methods under the zero-shot scenario, compared with the scenario of the above fine-tuning. Notedly, the improvements by VoT over the CoT become larger and clearer under the zero-shot setting. The enhancements on these two complex video QA tasks are clearer than those on the comparatively simpler tasks, i.e., MSR-VTT and ActivityNet. This is largely because the latter datasets more tend to perceptive understanding (e.g., describing what's in video), rather than cognitive understanding (e.g., explanation, foresight or imagination). Further, we cancel the verification mechanism (at 6-th) of either the pixel grounding perspective or the commonsense perspective. We see that on MSR-VTT and ActivityNet, the perceptive-level pixel grounding verification is more crucial than the commonsense cognitive verification. For those complex videos, both types of verifications are pivotal."}, {"title": "5.4. Analyses on MotionEpic Video MLLM", "content": "Probing Video Grounding Ability. To evaluate how well our MotionEpic is capable of video grounding, we here perform the probing test. Specifically, we evaluate the performance of MotionEpic on STSG parsing on the Action Genome test set, by comparing with SoTA DSG parsers: GPS-Net , STTran and AP-Net . We measure three aspects: 1) the object grounding (bbox detection), 2) SG triplet classification (categories of entities, and relation predicates between entities), and 3) temporal action grounding (the start and end times of actions). Fig. 5 illustrates the results, where we see that MotionEpic achieves very competitive performance on par with SoTA parser, even with human-level performance. This reveals that MotionEpic shows reliable capability in providing video grounding information to support the subsequent in-depth video reasoning."}, {"title": "Influence of Various Grounding-aware Tuning Strategies", "content": "We further study the impacts/contributions of different grounding-aware tuning objectives introduced in \u00a73.3. We design five groups of ablations where each tuning goes without one item, and the resulting model performs zero-shot end task. The results on two datasets are shown in Fig. 6, where different items come with varied impacts, indicating the importance of video-STSG grounding fine-tuning. Notably, the lack of L2 and L4 result in the greatest degradation. This is intuitive, as these two objectives are directly associated with the subsequent reasoning process, i.e., understanding STSG from video, and generating (partial) STSG given objects."}, {"title": "5.5. Analyses on VoT Reasoning Framework", "content": "Reasoning Ability Breaking-down. Previously, we validated the overall stronger performance of the VoT reasoning system through extensive experimentation. Here, we aim to provide a more in-depth analysis of VoT. First, we select 200 hard instances each from the Causal-VidQA and Social-IQ test sets, and then compare the performance of Video-LLaVA and MotionEpic under CoT and VoT frameworks, respectively. Also we conduct human evaluation on this subset to gauge its difficulty level. The results, in the above table of Fig. 7, show that MotionEpic with VoT reasoning framework achieves quite exceptional results, comparable even to human performance. We further summarize the error cases and analyze differences in the 6 most frequent categories of errors. As seen in the below part of the figure, MotionEpic (with VoT) significantly reduces the error rate of Video-LLaVA (with CoT), especially in terms of action semantics and commonsense understanding.\nVideo Reasoning Visualization. Finally, we present a case study to aid an intuitive understanding of the superiority of our system. We randomly select an instance where our model gives the correct answer. As shown in Fig. 8, the video displays a complex scene, and the given question is abstract and complex, not directly answerable through the mere perception of the video itself. However, our MotionEpic provides the correct answer, while the other two baselines err. At the content perception level, VoT ensures accurate and robust understanding through STSG-based video grounding, preventing hallucination, i.e., it correctly interprets that the animal is a dog, then infers from commonsense that the scene involves a trainer training the dog. Then, at the cognitive level, it analyzes each option to determine the best answer. Through further verification, the result aligns with both the video content and factual commonsense understanding. Overall, the entire reasoning greatly improves the accuracy at each step through problem decomposition, while ensuring an explainable process decision rationale."}, {"title": "6. Conclusion", "content": "In this work, we for the first time introduce an innovative solution for complex video reasoning, the Video-of-Thought (VoT) framework. To accomplish the reasoning framework, also a novel video MLLM, MotionEpic, is proposed. MotionEpic achieves fine-grained pixel-level spatial-temporal video grounding by adeptly integrating video STSG representation. With MotionEpic, the VoT framework resolves the intricate video task by skillfully dissecting it into manageable sub-problems, tackling them sequentially from low-level pixel perception to advanced cognitive interpretation. Our experiments across various complex video QA benchmarks have not only proven the efficacy of our approach but have also boosted the existing state-of-the-art standards. Overall, this work marks a substantial contribution to the video modeling community, paving the way for more nuanced, human-level analysis in the relevant community."}, {"title": "Statement of Potential Broader Impact", "content": "This paper aims to construct a robust, human-level video understanding and reasoning framework. The system must be built upon existing LLM to realize its full potential. Potential implications include substantial energy consumption during LLM system training, leading to environmental degradation, and the necessity for more extensive data corpora for training. Moreover, due to the powerful video reasoning and comprehension capabilities, there exists the potential for malicious actors to exploit this framework for nefarious intents, posing a societal threat. Consequently, the release of this framework necessitates the establishment of specific licensing mechanisms to ensure responsible deployment and mitigate potential misuse."}, {"title": "A. More Configuration Details", "content": "A.1. Detailed Prompt Construction and System I/O\nHere, we provide detailed prompts as well as their inputs and outputs, for each step of the VoT reasoning framework."}, {"title": "A.2. Implementations", "content": "MotionEpic uses the Vicuna-7B (v1.5) as the backbone LLM. We adopt the ViT-L/14 as the video encoder, and use the Q-Former as the projector. All the modules take the default configurations without much modification. For our Recurrent Graph Transformer, we take a 6-layer architecture with 768-d hidden sizes. The text tokenizer is sourced from LLaMA, with approximately 32,000 classes. For each video, we uniformly sample certain frames with a sampling rate of 8 fps for fine-grained reasoning. We note that too large sampling rate introduces noises (i.e., redundant frames) and huge computation cost, while too small one will cause important information loss. Here we use the 8 fps, as in our preliminary study we verified that it helps achieve the best trade-off. For the fine-tuning setting of end tasks, we will tune the MotionEpic based on the training set using the setting as prior baselines, i.e., data split and evaluation methods. For the zero-shot setting, we will directly perform video QA without using the in-domain training set."}]}