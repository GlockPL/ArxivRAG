{"title": "Enhancing Multi-Text Long Video Generation Consistency without Tuning: Time-Frequency Analysis, Prompt Alignment, and Theory", "authors": ["Xingyao Li", "Fengzhuo Zhang", "Jiachun Pan", "Yunlong Hou", "Vincent Y. F. Tan", "Zhuoran Yang"], "abstract": "Despite the considerable progress achieved in the long video generation problem, there is still significant room to improve the consistency of the videos, particularly in terms of smoothness and transitions between scenes. We address these issues to enhance the consistency and coherence of videos generated with either single or multiple prompts. We propose the Time-frequency based temporal Attention Reweighting Algorithm (TiARA), which meticulously edits the attention score matrix based on the Discrete Short-Time Fourier Transform. Our method is supported by a theoretical guarantee, the first-of-its-kind for frequency-based methods in diffusion models. For videos generated by multiple prompts, we further investigate key factors affecting prompt interpolation quality and propose PromptBlend, an advanced prompt interpolation pipeline. The efficacy of our proposed method is validated via extensive experimental results, exhibiting consistent and impressive improvements over baseline methods. The code will be released upon acceptance.", "sections": [{"title": "1 Introduction", "content": "Video diffusion models have achieved broad successes in many fields, including text-to-video generation [Chen et al., 2024, Guo et al., 2023], image-to-video generation [Hu, 2024], and objects animation [Blattmann et al., 2023a]. Among them, long video generation has attracted increased attention recently. Practitioners have been training more powerful video diffusion models to generate ten-second level videos with high resolutions [Zheng et al., 2024, PKU-Yuan Lab and Tuzhan AI etc., 2024].\nDespite the substantial progress made in long video generation, some critical problems still persist. These problems concern the (in)consistency of long videos and, indeed, video generation with multiple prompts is poorly explored in the community. In the long videos generated by the existing methods, the number of subjects, the shape and color of the background and each single subject, and the motion pattern of the objects change abruptly and unnaturally, as shown in Fig. 1. In addition, multiple prompts are often assigned to different parts of the video. Thus, the interpolation or transition between different prompts is a critical part of the generated videos. However, existing works have not adequately investigated how to handle multiple prompt inputs to achieve better generation results.\nWe improve the consistency of generated long videos from two perspectives\u2014the video hidden states and the prompt hidden states in the denoising networks as shown in Fig. 2. First, we introduce Time-frequency based temporal Attention Reweighting Algorithm (TiARA) to improve consistency by enhancing the video hidden states in the denoising network. We examine"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Video Diffusion Models", "content": "In video diffusion models, capturing the temporal correlation across frames is crucial for generating consistent video sequences. To address this, recent works [Blattmann et al., 2023b, Guo et al., 2023, Chen et al., 2024, Wang et al., 2023b,c] introduce additional temporal attention modules to the U-Net architecture in pretrained text-to-image models, which are often referred to as 2D+1D U-Net. The input to U-Net is a latent $Z \\in \\mathbb{R}^{C\\times N \\times H \\times W}$, where C is the channel dimension, N is the number of frames, and H and W are respectively the height and width of each frame. The newly introduced temporal attention module is implemented along the temporal axis, i.e., for any $h \\in [H]$, $w \\in [W]$, the input to the attention module is $Q_{h,w} = [Z_{:,1,h,w}, \\cdots, Z_{:,N,h,w}]W_Q = Z_{:,h,w}W_Q \\in \\mathbb{R}^{N\\times d_k}$, $K_{h,w} = Z_{h,w}W_K \\in \\mathbb{R}^{N\\times d_k}$, $V_{h,w} = Z_{:,h,w}W_v \\in \\mathbb{R}^{N\\times d_v}$, and the output of the temporal attention is\n\n$Att(Q_{h,w}, K_{h,w}, V_{h,w}) = sm \\left(\\frac{1}{\\sqrt{d_k}}Q_{h,w}K_{h,w}^T\\right)V_{h,w} \\tag{2.1}$\n\nwhere $sm(\\cdot)$ is the row-wise softmax function, $d_k$ and $d_v$ are the embedding dimension of keys and values. There are also some models that implement a 3D spatial-temporal attention, such as Open-Sora-Plan 1.3 [PKU-Yuan Lab and Tuzhan AI etc., 2024]. In this paper, we mainly focus on the 2D+1D U-Net, and leave the 3D structure as future work."}, {"title": "2.2 Long Video Generation Methods", "content": "FIFO-Diffusion and Rolling Diffusion Model Traditionally, the video frames are generated as a whole vector, where all frames share the same magnitude of noise at each denoising step. FIFO-Diffusion [Kim et al., 2024a] proposes to increase the noise magnitude of the frames according to their frame indices. In this method, all noisy frames constitute a queue. After each denoising step, the first frame in the queue is fully denoised and is popped out; and another Gaussian noise frame is appended to the queue, prepared for the next step of denoising. This method is suitable for long video generation since the first-in-first-out scheme can generate an infinite-length sequence in an autoregressive manner. A similar design is also discussed in Rolling Diffusion Model [Ruhe et al., 2024].\nStreamingT2V StreamingT2V Henschel et al. [2024] extends short video generation models to long video generation using an autoregressive approach. It trains a conditional attention module that uses selected frames from the previous chunk as conditions for the current video chunk, ensuring the smoothness across chunks. An appearance preservation module is trained to incorporate anchor frame conditioning, which helps maintain consistent scenes and object features throughout generated videos.\nFreeNoise FreeNoise [Qiu et al., 2023] is a training-free method for long video generation. By applying window-based attention fusion and local noise shuffling, it extends the number of frames generated at the same time and enhances temporal consistency in the output video. However, this method can only generate videos with motions of a limited scale, which is restrictive for varying contexts.\nExisting problems in long video generation Although existing works propose algorithms to generate long videos, the generated videos contain inconsistent parts. As shown in Fig. 1, the baseline methods result in the number, color, and shape of objects being inconsistent across frames. We mitigate these inconsistencies."}, {"title": "2.3 Fourier Analysis", "content": "The Discrete Fourier Transform (DFT) and DSTFT are classical transformation methods in the frequency analysis of signals. These methods decompose a signal into a linear combination"}, {"title": "3 Methods", "content": "In Section 3.1, we examine the temporal attention scores of inconsistent frames in generated long videos and propose a temporal attention reweighting method based on our findings. In Section 3.2, we analyze the limitations of this method and enhance it with time-frequency analysis. In Section 3.3, we propose a new effective prompt interpolation method."}, {"title": "3.1 Temporal Attention Reweighting", "content": "Observations from Temporal Self Attention As mentioned in Section 2.2, generated long videos often exhibit frame-to-frame inconsistencies. Here, we examine the frequency character-istics of these inconsistencies and delve into the denoising networks to explore intermediate variables correlated with them. In the 2D+1D structure introduced in Section 2.1, the temporal attention plays a critical role in capturing the correlation across frames. Thus, we analyze the attention scores of the temporal attention modules in consistent and inconsistent part of the generated videos.\nWe adopt a 310-frame video clip generated using FIFO-Diffusion [Kim et al., 2024a] as a representative example. As shown in Fig. 3, the video displays sudden appearances of objects and abrupt scene changes in some frames. We apply DSTFT to the video to analyze the signal in the frequency domain, and examine the temporal attention scores during generation. We make the following observations: (1) The inconsistent part has substantially more power at the highest few frequencies bands in its DSTFT spectrum. In contrast, the power of the consistent part concentrates on relatively low-frequency part of the spectrum. (2) The temporal attention scores disproportionately concentrate along the diagonal for the inconsistent generated frames, unlike that in the consistent frames. This suggests that an excessive focus on the diagonal in the temporal attention can be a contributing factor to these inconsistencies.\nThe first observation motivates us to define the power of the inconsistent part of the video as a function of the spectrum that at frequencies higher than a certain threshold $k_t$ in our theoretical analysis (Section 4). The second observation dovetails with our intuition. Namely, when the temporal attention scores along the diagonal are higher, frames gather less information from other frames, which further results in increased inconsistency and reduced smoothness in the generated video.\nTemporal Attention Reweighting Based on the above observations, we propose a temporal attention reweighting method to enhance the consistency of the generated videos.\nWe define the reweighting matrix as $A = -x\\cdot I_{N\\times N} \\in \\mathbb{R}^{N\\times N}$, where $a \\geq 0$. This matrix is applied to the correlation matrix before the softmax operation in (2.1), adjusting the attention distribution to reduce over-concentration on the diagonal and improve temporal consistency, i.e., $Att(Q, K, V) = sm(QK^T +A)V = \\bar{A}V$. By subtracting a positive value from these diagonal entries, we encourage each frame to draw more information from other frames and maintain stronger correlations with them, thereby achieving greater consistency and smoothness between frames. Additionally, the reweighting value can be further optimized to balance between content consistency and dynamic intensity of the video.\nIt is also worth noting that in video generation, particularly for long videos, it is desirable for the frame to have more correlation with its neighbouring frames rather than distant frames. To achieve this, we also apply a reweighting procedure on the lower-left and upper-right corner of temporal attention maps."}, {"title": "3.2 TIARA", "content": "While temporal attention reweighting can effectively improve the consistency of generated videos, this technique is less suitable for video clips containing intense movements. In these cases, the weight matrix A will \u201cover-smooth\u201d the frames along the time axis, leading to blurring artifacts. As illustrated in Fig. 4, this original video generated from FIFO-Diffusion"}, {"title": "3.3 Multi-Prompts Alignment and Interpolation", "content": "In real-world applications, generating a complete long video often requires multiple prompts. The interpolation or transition between different prompts strongly influences the consistency of the generated videos. Some works have conducted preliminary explorations on this topic, e.g., Gen-L-Video [Wang et al., 2023a], FIFO-Diffusion [Kim et al., 2024a], and MVTG [Oh et al., 2023] propose switching directly from one prompt to the next during video generation, resulting in noticeable inconsistencies between scenes. An example of inconsistency is shown in Fig. 1, where new mountains appear and grow unrealistically during the transition between prompts. In FreeNoise Qiu et al. [2023], a motion injection method is introduced to control the inconsistency between two prompts. However, this method is restrictive because it can only deal with changes in motion verbs and it can only work on a limited motion scale.\nWe propose PROMPTBLEND, an effective pipeline for multi-prompt transitions. The pipeline is shown in Fig. 2, and the pseudo code is provided in Appendix A. First, the components of each input prompt are organized in a prescribed order, i.e., [The subject] [is doing something] [at some time] [in some place]. This operation can be carried out by the in-context learning of large language models [Dong et al., 2022]. The prompt template is provided in Appendix H. Then, to align the prompts in token space, we equalize component lengths in each prompt by padding extra tokens for shorter component instances. Instead of using blank tokens for padding, we repeat tokens of the shorter instance until it shares the same length as the longest instance across prompts. These two procedures derive the aligned prompt embeddings $\\{P_i\\}_{i=1}^{2}$, and we then clarify how to use these embeddings as text conditioning during video generation.\nAssume the total generated frames are divided into segments, marked with the starting and ending frame of prompt i: $\\{(n_i, n_{i+1})\\}_{i=1}^{q}$, where indices of the transition frames are between $n_i$ and $n_{i+1}$. For the non-transition frames between $n_i$ and $n_i$, we adopt the i-th prompt $E(P_i)$ as the text conditioning for the U-Net, where $E$ represents the text encoder. For frame n in the transition window $[n_i, n_{i+1}]$, we obtain the text conditioning $E(P_c)$ for the cross attention blocks at denoising time step t and U-Net layer d as follow:\n\n$P_c(n, t, d) = \\begin{cases}\n\\frac{(1-a_n)P_i+a_n P_{i+1}}{\\mathbb{I}(t \\in [t_1, t_2] \\lor d \\geq D)} & t \\in [t_1, t_2] or d \\geq D \\\\\nP_i & otherwise\n\\end{cases}$\n\nwhere $a_n = \\frac{(n - n_i)}{(n_{i+1} - n_i)} \\in [0,1]$. In practice, $[t_1, t_2]$ is set to be an interval located at the later phase of the denoising process, and $D$ is a prescribed layer index threshold, with $d > D$ indicating the decoder part in the U-Net. The design of this interpolation scheme is built upon two previous findings: (1) Prompt instruction at later denoising steps (i.e., $t \\in [t_1, t_2]$) primarily influence the generation of object shapes [Qiu et al., 2023, Balaji et al., 2022, Cao et al., 2023, Liew et al., 2022]. (2) The decoder part of the denoising U-Net (i.e., $d > D$) mainly influence the semantic details [Qiu et al., 2023] while preserving the scene layout [Cao et al., 2023]. Thus, our interpolation scheme can gradually introduce elements of the next prompt to the video, and is less likely to result in inconsistencies.\nComparison to Motion Injection in FreeNoise FreeNoise [Qiu et al., 2023] introduces Motion Injection that incorporates an interpolated prompt at later denoising time steps and U-Net decoders within the transition window. After the transition window, Motion Injection continues to apply the first prompt for the non-decoder components and across most time steps. In contrast, our interpolation scheme transitions completely to the subsequent prompt after the transition window $[n_i, n_{i+1}]$.\nMotion Injection facilitates a smoother, motion-focused transition within a single scene, as the overall structure generation remains conditioned on the initial prompt throughout. However, this approach is restricted to transitions between motions and has limited applicability for diverse scene changes. By introducing prompt alignment method, our design maintains component consistency during interpolation without depending on the first prompt. Consequently, PROMPTBLEND adapts seamlessly to multiple prompts, allowing for consistent video generation across multiple consecutive scenes. Experimental comparison results are shown in Fig. 5."}, {"title": "4 Theoretical Analysis", "content": "For the theoretical analysis, we consider the case where the value V is the concatenation of bounded scalars, i.e., $d_1 = 1$, and $|v_i| \\leq B_v$ for all $i \\in [N]$. The results for $d_v > 1$ can be easily derived by considering each dimension of V. To highlight the main intuitions behind TIARA, we consider a simplification of it as follows:\n\n$x = sm(QK^T)V = A_X \\cdot v, \\tag{4.1}$\n\n$y = sm(QK^T - a \\cdot I_{N\\times N})V = A_Y \\cdot v, \\tag{4.2}$\n\nwhere $A_X$ and $A_Y$ are the attention scores of the original and improved temporal attention, respectively. The weights for all the time steps share the same value. We also simplify the weight matrix A to be diagonal.\nPerformance metric Inconsistency in videos usually manifests as unnatural high-frequency changes in the frames across time, as shown in Fig. 3 and Section 3.1. For a pixel that varies across time, also called a signal x,we define the inconsistency error of a signal x in a local neighborhood of time $\\tau$ as $E(x, \\tau) := \\sum_{k=k_t}^{\\lfloor N/2 \\rfloor} | DSTFT(x, \\psi, \\tau, k) |$, where $|z|$ represents the magnitude of the complex number z, and $0 < k_t \\leq \\lfloor N/2 \\rfloor$ is the threshold of the frequency index to distinguish the inconsistent part of the video. $\\frac{1}{2}$ in TIARA is an estimate of this value. For example, this can be set to $5 \\cdot 2\\pi/N$ in Fig. 3. The metric E(x, t) serves as a quantification of the signal that originates from rapid and unnatural motion in the video.\nTo facilitate our theoretical analyses, we make three assumptions.\nAssumption 4.1 (Existence of inconsistency). The original signal $x = x^{(N)}$ of length N contains non-negligible inconsistency error. More precisely, there exists a constant $C > 0$ (not dependent on $\\tau$) such that $\\liminf_{x\\rightarrow \\infty} E(x^{(N)}, \\tau) > C$ for all $\\tau \\in \\mathbb{Z}$.\nThis assumption states that there is a non-negligible inconsistency error in the original output of the temporal attention. Under this case, we will show that TIARA decreases E(y, $\\tau$). We assume that such inconsistency appears in each time step $\\tau\\in [N]$ for the ease of analysis. Our results can be easily generalized to the situation where the inconsistency only appears in a strict subset of timesteps $N_{inconsistent}\\subseteq [N]$.\nAssumption 4.2 (Approximate time-homogeneity). The attention scores are approximately time-homogeneous, i.e., there exists a constant $\\gamma > 4$ such that\n\n$| A_{i,k}^X - A_{j+k}^X | = O(N^{-\\gamma})$ for all i, j, k $\\in \\mathbb{Z}$.\n\nThis assumption states that the attention scores at different time steps are approximately homogeneous. This can be empirically observed as the similar pattern of attention scores in each row in Figure. 3. We note that the indexes i, j, i + k, and j + k do not have to belong to the discrete interval [N]. If they do not lie in [N], we periodically extend of A which originates from DSTFT and is defined in Section 2.3. This assumption states that the attention scores at different time steps are approximately homogeneous. For example, this assumption is satisfied when the queries and keys satisfy $q_i^T k_i = exp(-|i - j|)$ for all i, j $\\in \\mathbb{Z}$, where $|i - j|$ denotes the difference between i and j modulo N. This type of homogeneity assumption has also been adopted in the existing works to analyze the properties of transformers [Dong et al., 2021].\nAs discussed in Section 3.1, the large values in the diagonal of the attention score $A_X$ contribute to the inconsistency of the video. This motivates us to define the dynamic component of $A_X$ by setting the diagonal components to zeros, i.e., $A_{i,j}^{X,dyn} = A_{i,j}^X$ for $i \\neq j$, and $A_{i,i}^{X,dyn} = 0$. The corresponding output is then denoted as $x^{dyn} = A^{X,dyn} \\cdot v$. We respectively term $A^{X,dyn}$ and $x^{dyn}$ as the dynamic components of $A_X$ and x, since $A^{X,dyn}$ and $x^{dyn}$ represent how all the other frames influence the value of the current frame. This corresponds to the dynamics of the video."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Setting", "content": "Implementation details We implement our methods on three long video generation models: FIFO-Diffusion [Kim et al., 2024a], (referred to as FIFO for simplicity) FreeNoise [Qiu et al.,"}, {"title": "5.2 Experimental Results", "content": "Qualitative results for single-prompt and multi-prompt generation are illustrated in Fig. 1, with additional results provided in Appendix J. Quantitative results are presented in Table 1. For single prompt video generation, the result indicates that TIARA significantly enhances the content consistency and temporal quality of the base models. Specifically, the improved SC, BC, and CTS imply that videos generated with TIARA enjoy greater visual and semantic consistency. Moreover, the reduced TF, WE and the increased MS underscore the effectiveness of TIARA in improving the temporal quality of generated videos. For multi-prompt generation, the integrated method, TIARA+PROMPTBLEND, achieves even greater gains in SC and BC compared to the base model, further showcasing its effectiveness in complex generation scenarios. We also conducted a user study to evaluate the generated single-prompt and multi-prompt videos based on human preferences. The results, shown in Fig. 6, indicate a strong preference for videos generated with our method over those generated without it, illustrating its impact on user-perceived quality. The details of user study setup are postponed to Appendix F.\nWe also compare the improvement of PROMPTBLEND over the Motion Injection method in FreeNoise [Qiu et al., 2023] in Fig. 5. Each generated videos have 310 frames, with text conditioning transitioning from the first prompt to the second prompt between frame 50 and 150, which gives sufficient length for generating the second prompt. In both cases, Motion Injection fails to generate the second prompt, whereas PROMPTBLEND successfully generates both prompts while maintaining content consistency."}, {"title": "5.3 Ablation Study", "content": "In this section, we present several quantitative and qualitative experiments to evaluate the effectiveness of TIARA and PROMPTBLEND. First, we conduct an ablation study on each component of TIARA in the context of single-prompt video generation. Four cases are tested: (1) the original FIFO; (2) reweighting applied only to the lower-left and upper-right corners of the temporal attention map; (3) reweighting applied only to the diagonal entries of the temporal attention map; and (4) TIARA with both reweightings. The results, shown in Table 2, indicate that applying reweighting to the corners alone provides a modest improvement, while reweighting along the diagonals yields better performance. The best results are achieved with combined reweightings, highlighting the importance of each component in TIARA. We also qualitatively show the efficacy of TIARA compared to the reweighting without time-frequency analysis in Fig. 4.\nIn Table 3, we evaluate the improvements achieved by our prompt alignment and inter-polation methods individually in the multi-prompt video generation task. The comparison is conducted on multi-prompt FIFO across four types of text conditioning schemes: (1) original prompt inputs; (2) prompts aligned using our method; (3) aligned and interpolated prompts applied as text conditioning for all layers and time steps; and (4) PROMPTBLEND. The performance gap between (1) and (2) demonstrates the effectiveness of our prompt alignment strategy, with significant improvements in SC, BC, and MS for videos generated using aligned prompts. Comparing (2) and (3) confirms that our interpolation scheme further enhances performance beyond simply using interpolated prompts as conditioning for all layers and time steps, with both approaches outperforming the no-interpolation schemes. An ablation study on the influence of prompt alignment on prompt interpolation can be found in Appendix G. The related works and limitations are included in Appendix B and Appendix C."}, {"title": "6 Conclusion", "content": "In this paper, we uncover the relationship between temporal attention scores and the consistency and smoothness of generated videos. Building on this insight, we introduce TIARA, which enhances consistency in long video generation. We also theoretically demonstrate its effectiveness under reasonable assumptions. Additionally, we propose PROMPTBLEND, a technique for smoothly interpolating between prompts to improve consistency in multi-prompt video genera-tion. Extensive quantitative and qualitative experiments across various long video generation pipelines and text prompts confirm the effectiveness of both TIARA and PROMPTBLEND in enhancing video consistency."}]}