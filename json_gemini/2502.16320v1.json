{"title": "Direct Alignment with Heterogeneous Preferences", "authors": ["Ali Shirali", "Arash Nasr-Esfahany", "Abdullah Alomar", "Parsa Mirtaheri", "Rediet Abebe", "Ariel Procaccia"], "abstract": "Alignment with human preferences is commonly framed using a universal reward function, even though human preferences are inherently heterogeneous. We formalize this heterogeneity by introducing user types and examine the limits of the homogeneity assumption. We show that aligning to heterogeneous preferences with a single policy is best achieved using the average reward across user types. However, this requires additional information about annotators. We examine improvements under different information settings, focusing on direct alignment methods. We find that minimal information can yield first-order improvements, while full feedback from each user type leads to consistent learning of the optimal policy. Surprisingly, however, no sample-efficient consistent direct loss exists in this latter setting. These results reveal a fundamental tension between consistency and sample efficiency in direct policy alignment.", "sections": [{"title": "Introduction", "content": "Human rewards and preferences are heterogeneous [1, 2, 3, 4, 5]. Despite this, learning from preference data often bypasses this insight, relying on what we dub the preference homogeneity assumption. This tension in assumptions is readily apparent in standard human-AI alignment methods such as reinforcement learning from human feedback (RLHF) [6, 7, 8] and direct preference optimization (DPO) [9] which assume a single reward function captures the interests of the entire population.\nWe examine the limits of the preference homogeneity assumption when individuals belong to user types, each characterized by a specific reward function. Recent work has shown that in this setting, the homogeneity assumption can lead to unexpected behavior [10, 11, 12]. One challenge is that, under this assumption, learning from human preferences becomes unrealizable, as a single reward function cannot capture the complexity of population preferences with multiple reward functions [13, 14]. Both RLHF and DPO rely on maximum likelihood estimation (MLE) to optimize the reward or policy. Unrealizability implies their likelihood functions cannot fully represent the underlying preference data distribution, resulting in a nontrivial optimal MLE solution. From another perspective, learning a universal reward or policy from a heterogeneous population inherently involves an aggregation of diverse interests, and this aggregation is nontrivial.\nIn the quest for a single policy that accommodates a heterogeneous population with multiple user types, we show that the only universal reward yielding a well-defined alignment problem is an affine"}, {"title": "Preliminaries", "content": "In the alignment problem, we consider a setting where a reward function \\(r^*\\) evaluates responses to queries. Formally, \\(r^*([x, y])\\) is the reward value of responding \\(y\\) to a query \\(x\\).\nThe alignment problem involves designing a policy, which chooses high-reward responses. Let \\(\\pi\\) denote a policy, defining a probability distribution over possible responses: i.e., given a query \\(x\\), \\(\\pi\\) returns \\(y\\) with probability \\(\\pi(y | x)\\). Commonly, we start with a reference policy \\(\\pi_{\\text{ref}}\\), which serves as a prior over \\(y\\) [16]. The goal is then to find a new policy \\(\\pi\\) that, for every \\(x\\), maximizes\n\\[\\mathbb{E}_{y \\sim \\pi(\\cdot | x)}[r^*([x, y])] - \\beta D_{KL}(\\pi(\\cdot | x); \\pi_{\\text{ref}}(\\cdot | x)).\\]\nWe denote the optimal policy by \\(\\pi^*\\). In practice, \\(\\pi_{\\text{ref}}\\) is often a pretrained language model, and the regularization parameter \\(\\beta\\) controls deviation from it. Eq. (1) often includes \\(\\mathbb{E}_x\\), which is important in practice but does not affect \\(\\pi^*\\) in theory.\nWhen \\(r^*\\) is explicitly known, we can directly apply RL to maximize Eq. (1). In many real-world settings, however, we do not know \\(r^*\\) and must estimate it. In such cases, we can collect human feedback to infer the reward function, after which we can use RL to optimize the policy, commonly known as RLHF. While RLHF is widely used, tuning this approach can be challenging due to the inherent complexities of RL. Recently, direct alignment with preferences has gained popularity as an alternative approach [17, 9, 18]. Unlike RLHF, direct alignment methods bypass explicit reward modeling to instead train a policy directly from human feedback.\nPreference Model. Both direct alignment and RLHF rely on a model of human preference to relate reward values with observed preference data. Consider the case where responses \\(y_1, y_2\\) are generated for a given query \\(x\\). We express the probability that \\(y_2\\) is preferred to \\(y_1\\) as\n\\[\\text{Pr}(y_2 \\succ y_1 | x; r^*) = \\sigma(r^*([x, y_2]) - r^*([x, y_1])),\\]"}, {"title": "Problem Formulation", "content": "The alignment problem is traditionally framed under a preference homogeneity assumption, where a single reward is presumed to capture all individual interests. In practice, people's preferences can differ significantly. To better capture real-world settings, we formalize preference heterogeneity by allowing reward functions to vary across user types.\nHeterogeneous Preferences. The influential study of \u201cindividual choice behavior\u201d by Luce [20] and other foundational works on human decision-making in mathematical psychology such as Shepard [24] focus on individual preference models. Luce [20] uses an axiomatic approach to establish the existence of a value function for each individual that, once normalized, explains the individual's choice probabilities. The widely used BT is one such example.\nIn practice, we often cannot observe individuals' identities. Therefore, standard approaches in preference modeling use a single reward function across the entire population. This homogeneity"}, {"title": "Implications of Homogeneity Assumption", "content": "With heterogeneous preferences, standard RLHF or DPO cannot yield the optimal policy \\(\\pi^*\\) that maximizes Eq. (9). If they did, it would also be possible to learn the user-weighted average reward as the induced reward of \\(\\pi^*\\). However, as we show in Proposition 5.1 and was previously observed by Siththaranjan, Laidlaw, and Hadfield-Menell [15] and Procaccia, Schiffer, and Zhang [29], learning the expected reward from anonymous preferences is impossible.\nTo explain DPO's failure in finding \\(\\pi^*\\), we extend its derivation to the heterogeneous setting in Section 4.1. This analysis lays the foundations to account for heterogeneity in DPO later on. In Section 4.2, we show that DPO's policy aligns with Borda count and, in Section 4.3, highlight its limitations. While our analysis focuses on DPO, similar insights extend to RLHF by substituting the policy with its induced reward."}, {"title": "Objective is Not the Expected Reward", "content": "We follow DPO's derivation from Section 2 but under heterogeneity. We show the closed-form connection between \\(\\pi^*\\) and \\(r^*\\) is no longer sufficient to express the likelihood function. Beginning with Eq. (9), the optimal policy is\n\\[\\pi^*(y) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y) \\exp \\left( \\mathbb{E}_{u} [r^*(y; u)] \\right).\\]\nDefine \\(\\Delta r^* (y_1, y_2; u) := r^*(y_2; u) - r^*(y_1; u)\\). The policy ratios of \\(\\pi^*\\) are related to the expected difference in rewards:\n\\[\\mathbb{E}_{u} [\\Delta r^* (y_1, y_2; u)] = \\beta \\log \\frac{\\pi^*(y_2)}{\\pi_{\\text{ref}}(y_2)} - \\beta \\log \\frac{\\pi^*(y_1)}{\\pi_{\\text{ref}}(y_1)}.\\]\nIn the homogeneous case, \\(\\Delta r^*\\) was sufficient to describe the likelihood of \\(y_2 \\succ y_1\\). However, with heterogeneous preferences, \\(\\mathbb{E}_{u} [\\Delta r^*]\\) alone does not suffice to write the likelihood function in Eq. (8). It is only under the approximation\n\\[\\mathbb{E}_{u}[\\sigma (\\Delta r^* (y_1, y_2; u))] \\approx \\sigma(\\mathbb{E}_{u} [\\Delta r^* (y_1, y_2; u)])\\]\nthat we can write \\(\\text{Pr}(y_2 \\succ y_1 | r^*)\\) in terms of policy ratios as in Eq. (5), and minimize DPO's loss to find \\(\\pi^*\\)."}, {"title": "Ordinal Consistency with Borda Count", "content": "If DPO were the answer, what would the question be? We partially answer this question by an adaptation of a result from Siththaranjan, Laidlaw, and Hadfield-Menell [15] which we restate for completeness. First, define Borda count as follows.\nDefinition 4.1 (Normalized Borda count). For a prompt \\(x\\), let \\(D(\\cdot | x)\\) denote the distribution of alternative responses sampled for \\(x\\). The Normalized Borda Count (NBC) of \\(y\\) at \\(x\\) is the probability that an annotator with a random type prefers \\(y\\) over an alternative response \\(y' \\sim D(\\cdot | x)\\):\n\\[\\text{NBC}(y | x) := \\mathbb{E}_{y' \\sim D(\\cdot | x)} [\\text{Pr}(y \\succ y' | x; r^*)].\\]\nWe next show that DPO's policy ratios are ordinally consistent with the normalized Borda count.\nProposition 4.2. Suppose responses to \\(x\\) in the preference dataset are drawn from \\(D(\\cdot | x)\\). In the limit of many data points, DPO's induced reward, or equivalently \\(\\frac{\\pi_{\\text{DPO}}(x)}{\\pi_{\\text{ref}}(x)}\\), has the same ordering over responses as NBC(\\. | x)."}, {"title": "Practical Drawbacks", "content": "In case of heterogeneous preferences, Borda count can significantly diverge from the user-weighted expected reward. This is studied under distortion in social choice problems [31]. Notably, NBC in Eq. (13) depends on \\(D\\). Therefore, although data collection is irrelevant in defining the optimal policy, it does affect \\(\\pi_{\\text{DPO}}\\).\nNext, we illustrate two key differences between \\(\\pi^*\\) and \\(\\pi_{\\text{DPO}}\\) using examples. Unless otherwise stated, we assume \\(D(\\cdot | x)\\) and \\(\\pi_{\\text{ref}}(\\cdot | x)\\) are uniform, and annotators follow BT. Refer to Appendix A for further drawbacks of DPO (minority suppression and IIA violation).\nSensitivity to Preference Dataset Distribution. Suppose \\(U = \\{A, B\\}\\) and types are equally represented. Given three possible responses, type A prefers \\(y_1\\) but type B prefers \\(y_2\\):\n\\[\\begin{aligned}\nr^*(y_1; A) &= 6, \\quad r^*(y_2; A) = 1, \\quad r^*(y_3; A) = 4, \\\nr^*(y_1; B) &= 3, \\quad r^*(y_2; B) = 9, \\quad r^*(y_3; B) = 4.\n\\end{aligned}\\]\nOne can verify when \\(D(y_1) = D(y_2)\\), increasing \\(D(y_3)\\) from 0.02 to 0.04 changes \\(\\pi_{\\text{DPO}}\\)'s preference from \\(y_2\\) to \\(y_1\\).\nDPO's policy is also sensitive to the preference model. Consider a variation of BT with a tem- perature of 2: \\(\\sigma_2(z) := (1 + \\exp(-z/2))^{-1}\\). For the same users and uniform sampling of alternatives, increasing the temperature from 1 to 2 flips \\(\\pi_{\\text{DPO}}\\)'s ranking over \\(y_1\\) and \\(y_2\\) while the preference model has no effect on \\(\\pi^*\\).\nWe have to emphasize that the dependence of NBC, and consequently \\(\\pi_{\\text{DPO}}\\, on the dataset sampling distribution \\(D\\) is not due to finite-sample limitations or insufficient offline dataset support. This issue persists even with complete data coverage and in the limit of infinite data.\nMediocrity Promotion. Consider the task of summarization. Suppose \\(U = \\{A, B, C\\}\\) and types are equally represented. Type A (B) strongly favors longer (shorter) summaries while type C slightly prefers medium-length ones:\n\\[\\begin{aligned}\nr^*(\\text{short}; A) &= 0, \\quad r^*(\\text{med}; A) = 1, \\quad r^*(\\text{long}; A) = 4, \\\nr^*(\\text{short}; B) &= 4, \\quad r^*(\\text{med}; B) = 1, \\quad r^*(\\text{long}; B) = 0, \\\nr^*(\\text{short}; C) &= 0, \\quad r^*(\\text{med}; C) = 1, \\quad r^*(\\text{long}; C) = 0.\n\\end{aligned}\\]\nIn this case, \\(\\pi^*(\\text{short}) = \\pi^*(\\text{long}) > \\pi^*(\\text{med})\\), however, NBC(\\text{short}) = NBC(\\text{long}) < NBC(\\text{med}). DPO prefers medium-length summaries not strongly favored by any type.\nReal-World Examples. The examples above are not contrived; in real-world cases, NBC can produce rankings different from \\(\\pi^*\\) and is sensitive to dataset distribution as extensively studied under distortion of social choice rules. To show this with a real example, we use Pew Research Center surveys and analyze a question to 5101 participants: \"The next time you purchase a vehicle, how likely are you to consider purchasing an electric vehicle?\u201d (options from A: very likely to D: not at all likely). Responses come from groups of different political leanings: Republican (45%), Democratic (48%), and Neither/refused (7%).\nAssuming the Luce-Shepard model [24] (see Eq. (27)), we estimate the reward for each group to calculate NBC and a user-weighted average reward. To find NBC, we use two distributions for alternatives: a uniform distribution \\(D_u\\) and a slightly altered distribution \\(D_a\\) with 0.2 total variation distance (TV) from \\(D_u\\). As shown in Fig. 1, NBC (with \\(D_u\\)) ranks option C first despite its mediocrity: it is the second or third preference for the three groups (see Fig. 8). In contrast, the user-weighted average reward favors D: the first and second preference for Republicans and the no-lean groups, respectively. Notably, altering \\(D_u\\) to \\(D_a\\) flips NBC's top ranking, highlighting NBC's sensitivity to dataset distribution. Similar discrepancies appear in other Pew surveys (see Appendix E)."}, {"title": "Approximate Direct Alignment with Minimal Annotator Information", "content": "The failure of standard alignment methods to find the optimal policy \\(\\pi^*\\) raises the question of whether it is even possible to design a method that identifies \\(\\pi^*\\). The answer is no without annotator information. To show this, it suffices to prove that the ranking based on the user-weighted average reward is not learnable. This implies that \\(\\pi^*\\) is also not learnable since its induced reward corresponds to this average reward by definition. We defer the formal definition of learnability to Definition C.2 in the appendix. Based on this definition, we prove the following impossibility:\nProposition 5.1. If there are at least two alternatives and two user types with a continuous preference model, the ranking based on the user-weighted expected reward is not learnable without annotator information.\nTo circumvent the impossibility in Proposition 5.1, we must either relax the requirement of exactly identifying \\(\\pi^*\\) or collect some information from the annotators. This section focuses on the former, and the latter is the subject of Section 6. Next, we introduce an approximate alignment objective, along with the required information and algorithms to solve it."}, {"title": "First-Order Approximation", "content": "The approximation in Eq. (12) is equivalent to using a zeroth-order Taylor expansion of \\(\\sigma(\\cdot)\\) around the average reward to calculate the likelihood function. To improve it, we extend DPO by incorporating an additional non-zero term from the expansion, which we call first-order corrected DPO. The derivation is as follows. Expanding \\(\\sigma(\\Delta r^* (y_1, y_2; u))\\) around \\(\\Delta r^* (y_1, y_2) := \\mathbb{E}_{u} [\\Delta r^* (y_1, y_2; u)]\\) up to the second order gives the below approximation for the likelihood:\n\\[\\mathbb{E}_{u} \\left[\\sigma(\\Delta r^* (y_1, y_2; u))\\right] \\approx \\sigma(\\Delta r^* (y_1, y_2)) + \\frac{1}{2} \\sigma''(\\Delta r^* (y_1, y_2)) \\cdot \\text{Var}_{u} [\\Delta r^* (y_1, y_2; u)] .\\]"}, {"title": "Impossibility without Annotator Information", "content": "If we limit our algorithms to M-estimators, which encompass most practical learning methods, con- sistent estimation of the variance term is impossible with anonymous data:\nProposition 5.2. There is no M-estimator that can estimate \\(V(x, y_1, y_2) := \\text{Var}_{u} [\\Delta r^*(x, y_1, y_2; u)]\\) consistently without annotator information.\nWhile Proposition 5.1 already implies that we cannot learn \\(\\pi^*\\) without annotation information, Proposition 5.2 goes further, showing that even improving DPO with a first- order approximation is practically impossible. Next, we show how minimal annotation information can overcome this impossibility."}, {"title": "Using Paired Preferences", "content": "We can get around Proposition 5.2 by collecting additional information on annotators. Specifically, it suffices to have a dataset \\(D\\) of pairs of preferences in the form \\(\\{(x, y_1, y_2, o), (x', y'_1, y'_2, o')\\}\\) where \\(o = 1\\{y_2 \\succ y_1\\}, o' = 1\\{y'_2 \\succ y'_1\\}\\) are labeled by the same person. Using \\(D\\), we can train a joint likelihood model \\(J(x, y_1, y_2, x', y'_1, y'_2)\\) by minimizing cross-entropy between \\(J\\) and \\((o \\cdot o')\\) as the label. The joint likelihood model consistently estimates\n\\[\\mathbb{E}_{u} [\\sigma (\\Delta r^* (x, y_1, y_2; u)) \\cdot \\sigma (\\Delta r^* (x', y'_1, y'_2; u))].\\]\nThis is in fact sufficient to estimate the variance term:\n\\[V(y_1, y_2) = \\frac{J_1 - (J_1 + J_2)^2}{\\sigma'(\\Delta r^* (y_1, y_2))^2}.\\]\nwhere \\(J_1\\) and \\(J_2\\)  are shorthands for \\(J(x, y_1, y_2, x, y_1, y_2)\\) and \\(J(x, y_1, y_2, x, y_2, y_1)\\)"}, {"title": "Direct Alignment with Maximum Annotator Information", "content": "Recall that learning the optimal policy from anonymous data is impossible, and an approximate improvement to DPO requires only minimal information about the annotations. But what if we collect richer data? Can we design a direct alignment method that consistently learns the optimal policy \\(\\pi^*\\)? To explore this, we consider a dataset where every sample is labeled by representatives of all user types. We show that consistent direct alignment is possible using this dataset.\nSuppose user types are in a finite set \\(U\\) and equally represented. This assumption makes our negative results stronger. Consider a rich data collection: for every context and candidate pairs \\((x, y_1, y_2)\\), we collect one preference data point from each user type. Let \\(o \\in \\{0, 1\\}^U\\) be the vector that indicates preferences where \\(o_u = 1\\) if user of type \\(u \\in U\\) has preferred \\(y_2\\), and 0 otherwise. Given such a dataset \\(D\\) with context, candidates, and preferences represented as \\((x, y_1, y_2, o)\\), our goal is to design a loss function\n\\[L(D; \\pi) = \\frac{1}{|D|} \\sum_{(x, y_1, y_2, o) \\in D} l(x, y_1, y_2, o; \\pi)\\]\nsuch that arg min \\(\\pi\\) \\(L(D; \\pi)\\) is a consistent estimator of \\(\\pi^*\\). Designing such a loss is, in fact, pos- sible. For instance, suppose we only look into the agreement cases in \\(D\\) where \\(o\\) is either all one or zero. Conditioned on agreement, we will show that the probability of \\(y_2 \\succ y_1\\) is proportional to \\(\\exp (\\sum_{u} \\Delta r^* (y_1, y_2; u))\\). We can write this likelihood in terms \\(\\pi^*\\) directly as we have a correspon- dence between \\(\\pi^*\\) and the difference in user-weighted average rewards (see Eq. (11)). We formally show this possibility through a temperature-adjusted DPO:\nProposition 6.1. Defining \\(l\\) in Eq. (17) as follows results in a consistent estimation of the optimal policy when preferences follow the BT model:\n\\[l(y_1, y_2, o; \\pi) = \\begin{cases}\n-\\log (|U| \\cdot h(y_1, y_2; \\pi)), & o = \\mathbf{1}, \\\\\n-\\log (|U| \\cdot h(y_2, y_1; \\pi)), & o = \\mathbf{0}, \\\\\n0 & o.w.\\\n\\end{cases}\\]\nHere, \\(h\\) is the difference of \\(\\pi\\)'s induced rewards (Eq. (15)).\nConsistent loss function is not unique. We give another example in Proposition C.3, and a systematic way to find such losses in Lemma D.2. In both examples, loss functions reduce to the standard DPO loss when \\(|U| = 1\\).\nWhile the loss function in Proposition 6.1 benefits from consistency, it only uses samples where all user types have agreed. In other words, it discards a sample with any disagreement. A natural question arises: Can we design a loss function that uses all data, including those with disagreement, while maintaining consistency? Surprisingly, the answer is no:\nTheorem 6.2. Suppose \\(l\\) in Eq. (17) only depends on \\((x, y_1, y_2)\\) through \\(\\pi\\) and \\(\\pi_{\\text{ref}}\\). If there are more than three types of user and the preferences follow BT, any loss \\(L\\) that allows a consistent estimation of the optimal policy discards samples with disagreement, i.e., those with \\(o \\notin \\{0, \\mathbf{1}\\}\\).\nThis theorem highlights a tension: To improve efficiency, one must compro- mise either consistency or direct optimization. The approximate direct alignment method proposed in Section 5 exemplifies forgoing consistency. Next, we discuss an alternative that favors consistency.\nAn Indirect Practical Solution: Averaging Personalized Rewards. The tradeoff between sample efficiency and consistency arises from the requirement for direct optimization. To regain sample efficiency, we may relax the requirement for direct alignment by training reward models while still avoiding RL. Specifically, we can learn personalized reward models \\(r(\\cdot; u)\\) for different user types \\(u \\in U\\), calculate a user-weighted expected reward, and use it to relabel a preference dataset. A dataset labeled"}, {"title": "Experiments", "content": "We provide empirical evidence for our claims throughout the paper. Section 7.1 extends our sensitivity example in Section 4.3 to a real-world preference dataset. In Section 7.2, we simulate DPO and our proposed improvements in a synthetic, small-scale environment where we can visualize and compare the resulting policies. Finally, we scale this experiment in Section 7.3 by fine-tuning large language models, illustrating the extent of improvement over DPO. Our code for reproducing the experimental results is publicly available at https://github.com/arashne/dahp."}, {"title": "NBC Sensitivity to Sampling Distribution", "content": "Recall from Section 4.2 that the common practice of alignment assuming homogeneity results in ordinal consistency with NBC. Here, we analyze the sensitivity of NBC to the distribution of pairwise preference datasets in real-world cases by using Pew surveys [34], the same dataset used in Section 4.3. Specifically, we address two questions: (i) Across the questions in the Pew surveys, how often would NBC rankings change when the sampling distribution of alternatives varies (while retaining support over all alternatives)? (ii) How much must the sampling distribution deviate from uniform to alter NBC rankings?\nTo answer these questions, we first estimate the reward of each option in each question (see Section 4.3 and Appendix E for further details). Given the rewards, we can calculate NBC under any sampling distribution using Eq. (13).\nFor question (i), among 1519 questions from 19 Pew surveys, NBC rankings change due to changing the sampling distribution from uniform in 20% of cases (306 questions), with the preferred choice changing in 136 cases. For question (ii), we find that a modest change in the sampling distribution suffices; in half the cases, a total variation (TV) distance of less than 0.23 from uniform alters the rankings. The cumulative distribution function (CDF) of the minimum TV distances required to change NBC rankings is shown in Fig. 2. Further experimental details are in Appendix E."}, {"title": "Synthetic Experiments", "content": "We generalize the discrete environment of Xu et al. [35] with a heterogeneous population. This environment enables us to visualize the differences between DPO's policy and the optimal policy, as well as to evaluate the effectiveness of applying a first-order correction (Section 5) and using a consistent loss function (Section 6)."}]}