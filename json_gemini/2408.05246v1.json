{"title": "Differentially Private Data Release on Graphs: Inefficiencies and Unfairness", "authors": ["Ferdinando Fioretto", "Diptangshu Sen", "Juba Ziani"], "abstract": "Networks are crucial components of many sectors, including telecommunications, healthcare, finance, energy, and transportation. The information carried in many such networks often contains sensitive user data, such as location data for commuters and packet data for online users. Therefore, when considering data release for networks, one must ensure that data release mechanisms do not leak excessive information about individual users, quantified in a precise mathematical sense. Differential Privacy (DP) is the widely accepted, formal, state-of-the-art technique, which has found use in a variety of real-life settings including the 2020 U.S. Census, Apple users' device data, or Google's location data, to name a few.\nYet, the use of differential privacy comes with new challenges, as the noise added for privacy introduces inaccuracies or biases. Such biases are unavoidable for any \"reasonable\" privacy technique; the issue, however, is that DP techniques can also distribute these biases disproportionately across different populations, inducing fairness issues. The goal of this paper is to characterize the impact of differential privacy on bias and unfairness in the context of releasing information about networks, taking a departure from previous work which has studied these effects in the context of private population counts release (such as in the U.S. Census). To this end, we consider a network release problem where the network structure is known to all, but the weights on edges must be released privately. We consider the impact of this private release on a simple downstream decision-making task run by a third-party, which is to find the shortest path between any two pairs of nodes and recommend the best route to users. This setting is of highly practical relevance, mirroring scenarios in transportation networks, where preserving privacy while providing accurate routing information is crucial. Our work provides theoretical foundations and empirical evidence into the bias and unfairness arising due to privacy in these networked decision problems.", "sections": [{"title": "Introduction", "content": "Networks underlie many crucial application domains, such as telecommunications, social networks, energy grids, infrastructure, and transportation. Understanding their properties is, therefore, crucial, and there is often a need for publishing network information to serve a multitude of purposes including but not limited to navigation and routing (transportation and computer networks), predictive network maintenance (computer and infrastructure networks), understanding (mis-)information propagation (social networks), for research and development purposes (e.g., energy grids), or to inform public policy.\nHowever the release of network data poses a key challenge since it often contains sensitive information and needs to be used and released carefully. For example, releasing data about energy and infrastructure can provide malicious entities insights into system vulnerabilities; data from social network and telecommunication can expose personal information about individuals' preferences, social interactions, and activities; transportation data can inadvertently reveal sensitive personal details like home addresses, healthcare-related visits, and other personal information allowing targeting of workers in high-security and confidential sectors NYT [2018, 2019].\nTherefore, when releasing network data, protecting potentially sensitive information is crucial. To this end, Differential Privacy Dwork et al. [2006] has emerged as the leading paradigm for preserving individual-level privacy in aggregate-level data release. Notably, this privacy framework has been adopted in various deployments, including the 2020 U.S. Census Bureau [2023], Apple's device data collection and federated learning frameworks Apple [2017], and Google's location data and maps services Google [2024].\nIn a nutshell, differential privacy relies on noise addition on the outputs of a computation to provide strong privacy guarantees. However, while this process ensures that the amount of sensitive information that can be \u201cleaked\u201d remain bounded, the added noise can introduce biases and inaccuracies, potentially impacting the reliability of the data. While these biases are a natural consequence of any privacy-preserving method, a concerning issue with DP is that it can distribute errors and biases unevenly across different groups, leading to concerns about fairness.\nOur work investigates the implications of DP on bias and fairness in network data release, focusing on routing recommendations. This constitutes a departure from previous research that primarily centered on the release of population histograms (e.g., in the U.S. Census) absent such network structure. Specifically, we examine the common scenario where the network structure is known but the edge weights need to be released privately. Our analysis shows how these perturbations influence tasks such as computing the shortest path and recommending optimal routes. \nOur work offers both theoretical insights and practical evidence on how differential privacy can introduce bias and unfairness in network-related decisions. Identifying these impacts is a first step towards developing more equitable and effective privacy-preserving techniques for network data. Beyond our characterizations of unfairness and biases due to DP, the paper also provides some understanding of how the network's structure affects unfairness and how different network structures are more or less robust to this unfairness, providing initial guidance toward network-design-oriented mitigation techniques.\nSummary of contributions. The main contributions of our work are as follows:\n1. We propose a model for differentially private network data release, assuming common knowledge of graph topology but requiring protection of sensitive edge weights through calibrated noise addition. This setup is detailed in Section 4.\n2. We investigate the bias and unfairness effects of using private (noisy) graph data to solve downstream optimization problems - particularly, the problem of computing shortest paths on the graph and recommending best routes to users. To the best of our knowledge, we are the first who seek to understand the tradeoff between privacy and fairness in the context of private graph data release.\n3. In Section 5, we develop a theoretical framework that explains how DP-induced biases could disproportionately affect certain groups, particularly through the mechanics of noise accumulation over different path lengths and the availability of alternate routes.\n4. Finally, in Section 6, details extensive simulations conducted on diverse network topologies to demonstrate how privacy-related disruptions can vary by network type. This analysis also identifies network structures that are inherently more resilient to privacy-induced biases."}, {"title": "Related Work", "content": "Observations that algorithms can mimic and amplify biases in data have resulted in a whole new research area that has focused on defining, analyzing, and mitigating unfairness (for surveys and summaries of the fairness literature, please refer to [Barocas et al., 2023; Mehrabi et al., 2021b; Pessach and Shmueli, 2022]). The source of the observed unfairness has often been attributed to either i) data properties or ii) different aspects of the model's properties. For example, imbalance in groups' size is commonly argued to create disparities in the task's performance Mehrabi et al. [2021a]. It has also been shown that constraining the model's hypothesis space to satisfy privacy Bagdasaryan et al. [2019]; Tran et al. [2021a], sparsity Hooker et al. [2019, 2020]; Tran et al. [2022a], or robustness Nanda et al. [2021]; Tran et al. [2024]; Xu et al. [2021] can result in disparate outcomes.\nParticularly relevant to our work is the study of the disparate impacts caused by privacy-preserving algorithms, which has recently seen several important developments Fioretto et al. [2022]. Much of this line of research, similarly to our work, focuses on differential privacy Dwork et al. [2006, 2014] as the formal notion of privacy leading to unfairness. In particular, in the context of learning tasks, Ekstrand et al. [2018] raise questions about the trade-offs involved between privacy and fairness. Subsequently, Cummings et al. [2019] study the trade-offs arising between differential privacy and equal opportunity, a fairness notion requiring a classifier to produce equal true positive rates across different groups. They show that there exists no classifier that simultaneously achieves $(\\epsilon, \\delta)$-DP, satisfies equal opportunity, and has accuracy better than a constant classifier. This development has risen the question of whether one can practically build fair models while retaining sensitive information private, which culminated in a variety of proposals, including [Jagielski et al., 2019; Mozannar et al., 2020; Tran and Fioretto, 2023; Tran et al., 2021a,b,c, 2022b, 2023].\nIn the context of private data release (which involves revealing a full, privatized version of a dataset as opposed to simply releasing targeted statistics), Pujol et al. [2020] were the first to show, empirically, that decision tasks made using DP datasets may disproportionately affect some groups of individuals over others. They noticed that the use of DP census data to allocate funds to school district produces unbalanced allocation errors, with some school districts systematically receiving more (or less) than what warranted. These observations were then attributed theoretically to two main factors: (1) the \u201cshape\" of the decision problem Tran et al. [2021d] and (2) the presence of non-negativity constraints in post-processing steps Zhu et al. [2021, 2022].\nTo the best of the authors knowledge, no other work has observed nor studied the tension between privacy and fairness in downstream tasks performed on differentially-privately released network data. Directly related to our work, Sealfon [2016] and Chen et al. [2023] do study the problem of differentially privately computing shortest paths, but i) they do not study the problem of releasing a private version of the entire network, just shortest path statistics, and ii) do not concern themselves with bias and fairness. Our paper thus builds on the body of work at the intersection of privacy and fairness and provides an analysis for the unfairness in a new context involving potentially complex network structures."}, {"title": "Preliminaries: Differential Privacy", "content": "Differential privacy (DP) Dwork et al. [2006, 2014] represents the forefront of techniques designed to safeguard individual data privacy. DP introduces a conceptual framework wherein two hypothetical scenarios are considered for each individual; these scenarios differ solely in the presence or absence of an individual's data. The principle of differential privacy mandates that an adversary should not be able to reliably discern between these two scenarios based solely on the output distributions of a computation. In essence, the precise data value of an individual exerts a minimal influence on the computation's result, thereby obscuring any single data point from being inferred with significant certainty.\nFormal definition. Formally, consider a mechanism M that operates on a dataset $x$ to derive a specific property $M(x)$. For a dataset comprising n individuals, we represent $x$ as a vector $(x_1,...,x_n)$, where $x_i$ corresponds to the data associated with the $i$-th individual. We begin by defining the concept of neighboring datasets, which is fundamental in the context of DP:\nDefinition 3.1 (Neighboring datasets). Two datasets $x$ and $x'$ are said to be neighboring if they differ solely by the data of a single individual. That is, there exists an index $j \\in [n]$ such that $x_j \\neq x'_j$, while $x_i = x'_i$ for all $i \\neq j$.\nDifferential privacy, as informally described above, requires that the outputs of mechanism M exhibit minimal variability when applied to any two neighboring datasets, $x$ and $x'$. The formal criterion for this requirement is articulated as follows:\nDefinition 3.2 $((\\epsilon, \\delta)$-differential privacy). Let $\\epsilon, \\delta > 0$. A randomized algorithm M satisfies $(\\epsilon, \\delta)$-differential privacy if, for any set of outcomes O in the range of M, the following inequality holds, for all neighboring databases $x,x'$:\n$Pr [M(x) \\in O] \\leq exp(\\epsilon) Pr [M(x') \\in O] + \\delta$.\nThe parameter $\\epsilon$ plays a key role in controlling the level of privacy provided by the mechanism on each individual. As $\\epsilon$ decreases, the privacy constraint becomes increasingly stringent, enhancing individual privacy protection. Specifically, as $\\epsilon \\rightarrow 0$, differential privacy requires that $Pr [M(x) = O]$ approaches $Pr [M(x') = O]$; i.e., the outcome of the mechanism becomes independent of the input data and thus perfectly preserves privacy (but, most likely, also provides no utility). Conversely, as $\\epsilon$ approaches $\\infty$, the privacy constraint is trivially satisfied, effectively offering no privacy safeguard.\nThe underlying mechanism adopted by algorithms satisfying differential privacy involves the addition of noise to computations that interact with the original data. This noise injection is designed around the concept of the sensitivity of a function, which is formally defined as follows:\n$\\Delta f = max \\| f(x) - f(x')\\|$\ns.t. $x, x'$ are neighbors.\nHere, $f$ represents a query or computation applied to the data, and $\\Delta f$ quantifies the maximum potential change in the function's output across two neighboring databases. The sensitivity of $f$ is a key concept; a lower sensitivity indicates minor changes between outputs for neighboring databases, simplifying the task of masking these differences with noise. Consequently, lesser noise is required to achieve privacy. Notably, if the sensitivity is zero, $f$ effectively behaves as a constant function, and no noise is necessary to preserve privacy. Numerical queries, which output a real number, can be made differentially private by adding calibrated noise to their true output values. For a given function $f$:\nLemma (The Gaussian mechanism). The Gaussian mechanism, defined as $M(f,x,\\epsilon) = f(x) + Z$ where $Z \\sim N (0, \\sqrt{2ln(1.25/\\delta)} \\cdot \\Delta f/\\epsilon)$ is $(\\epsilon, \\delta)$-differentially private.\nIn practice, the magnitude of noise introduced to preserve privacy is inversely related to the $\\epsilon$ parameter. Lower $\\epsilon$ values are associated to the addition of more noise, which in turn enhances the privacy guarantees. This inverse relationship underscores a fundamental trade-off in differential privacy: increasing privacy strength typically results in a reduction of the utility of the output due to the greater noise level. This paper will focus on understanding how this reduction in utility may be disproportional distributed among different individuals.\nPost-processing invariance. Differential privacy satisfies several important properties Dwork et al. [2014]. In particular DP is resistant to post-processing manipulations. Informally, this property states that any data-independent post-processing step applied solely to the output of a differentially private mechanism does not compromise its privacy guarantees. More formally:\nTheorem (Dwork et al. [2014]). Let $M$ be a randomized algorithm that is $(\\epsilon, \\delta)$-differentially private. Consider $f$, an arbitrary randomized function from the range of M to R. The composite function $f \\circ M$ retains the $(\\epsilon,\\delta)$-differential privacy properties of M."}, {"title": "Model: Settings and Goals", "content": "We consider the problem of differentially private graph data release. Formally, let $G = (V, E, w)$ be a weighted graph with vertex set $V$, edge set $E$, and weights $w : E \\rightarrow \\mathbb{R}_{>0}$. For each edge $e \\in E$, $w(e)$ is used to denote the its weight, here used to represent the \"time\" or \u201ccost\u201d it takes to traverse it. Without loss of generality, we consider connected graphs $G$ in which any two nodes are reachable from each other. Importantly, in this work we consider weights $w$ that are functions of sensitive user data and whose values must be protected. For instance, the weights might represent traffic congestion based on commuter locations or the strength of private social relationships in a network. We write $w(e) = f_e(x_1,...,x_n)$ where $(x_1,...,x_n)$ denotes sensitive information, such as geographic locations of users 1 through $n$.\nDifferential privacy graph release model. Consider a network administrator who wishes to release information about a weighted graph $G = (V, E, w)$ to a third party. To preserve the privacy of underlying data, the administrator generates a graph $\\tilde{G} = (V, E, \\tilde{w})$ where the structure of nodes and edges remains unchanged, but the edge weights $\\tilde{w}$ are altered to ensure differential privacy. This modified graph, termed the privatized or publicized graph, retains the publicly available network topology of $G$ while safeguarding sensitive weight information through differential privacy techniques.\nThe administrator uses the Gaussian mechanism, described in Section 3, to release weights $w$; formally, for each $e \\in E$,\n$\\tilde{w}(e) = \\max (0, w(e) + Z(e))$, (1)\nwhere $Z(e) \\sim N(0, \\sigma^2)$ is a centered Gaussian random variable. The application of the max function ensures that all reported weights remain non-negative, adhering to the post-processing immunity of differential privacy, as outlined in the Preliminaries\u00b9. When the sensitivity of function $f_e(\\cdot)$ in users' data is bounded by $\\Delta f$ for all $e \\in E$, the released graph guarantees the $(\\epsilon, \\delta)$-differential privacy of the edge weights for any $(\\epsilon, \\delta)$ satisfying $\\sigma = \\sqrt{2 \\ln(1.25/\\delta)} \\cdot \\Delta f/\\epsilon$. The higher the value of $\\sigma$, the better the privacy guarantee. In this paper, we will focus on $\\sigma$ as our main parameter controlling the level of noise and privacy, and refer the reader to this model section to relate the choice of $\\sigma$ to a formal differential privacy guarantee.\nRemark 4.1 (Motivating Example). The study of the shortest path problem provides a compelling context for our study. A notable real-world application is the private release of traffic data on road networks. Services like Google Maps leverage crowd-sourcing to gather live location data from thousands of users, enabling the system to assess traffic conditions, predict commute times, and suggest optimal routes in real-time [Google, 2009]. Numerous other organizations also collect and disseminate extensive user data to third parties, aiming to enhance understanding of traffic patterns and congestion levels. However, the use of such sensitive data raises significant privacy concerns [Vice, 2020], necessitating robust privacy-preserving mechanisms. Differential privacy is, in this setting, a widely adopted tool for private graph data release.\nImpact of differential privacy on bias and fairness. As the introduction of noise for privacy and the subsequent post-processing step in $G$, which ensures non-negative edge weights, can introduce inaccuracies and biases in statistical and optimization tasks performed on the publicized, privatized graph. In this paper we aim to (1) characterize such bias both theoretically and experimentally, and (2) to understand unfairness in how different segments of the network may be disparately affected by this bias. Our analysis focuses primarily on the disparities in how users, experiencing varying commute times through the network, are impacted by these modifications.\nIn most of the paper, we fix the task of interest to be a shortest-path computation task. Let $P_{ij}$ be the set of paths between any two vertices $i, j \\in V$. The length of a path $P \\in P_{ij}$ is given by $w_G(P) = \\sum_{e \\in P} w(e)$. The shortest path between nodes $x$ and $y$ is given by\n$P^*_{ij} = arg \\min_{P \\in P_{ij}} W_G(P) = arg \\min_{P \\in P_{ij}} \\sum_{e \\in P} w(e)$.\nOur goal is to evaluate the extent to which differential privacy mechanisms, when applied to graph $G$ to produce graph $G$, impact this computation. In the privatized graph $G$, the perceived shortest path is computed as:\n$\\tilde{P}_{ij} = arg \\min_{P \\in P_{ij}} \\tilde{w}(P) = arg \\min_{P \\in P_{ij}} \\sum_{e \\in P} \\tilde{w}(e)$.\nWe note that $G$ serves as a basis for the shortest path computations and route recommendation, the actual cost incurred by a user that decides to take path $\\tilde{P}_{ij}$ corresponds to the weights from the original graph $G$. Therefore, our evaluation metric is based on $w_G(\\tilde{P}_{ij}) = \\sum_{e \\in \\tilde{P}_{ij}} w(e)$, as highlighted in Figure 2, and the realized bias\u00b2 or error of the shortest path computation is given by\n$B_{ij} (\\tilde{P}_{ij}) = \\sum_{e \\in \\tilde{P}_{ij}} \\tilde{w}(e) - \\sum_{e \\in P^*_{ij}} w(e)$.\nGiven the stochastic nature of $w$, the perceived shortest path $\\tilde{P}_{ij}$ is subject to variability. Therefore, it is useful to also define the (expected) bias of the shortest path computation as follows:\n$E[B_{ij}] = E \\bigg[ \\sum_{e \\in \\tilde{P}_{ij}} w(e) - \\sum_{e \\in P^*_{ij}} w(e) \\bigg]$. (2)\nIn the numerical section, we will often work with relative errors or bias, defined as\n$R_{ij} = \\frac{E[B_{ij}]}{\\sum_{e \\in P^*_{ij}} w(e)}$, (3)\nand representing the percentage change in the length of the recommended path (in expectation) compared to the true shortest path.\nSummary of model and interactions. We conclude this section with an overview of the interactions in our model referring back to . A network administrator with access to the true graph $G$ computes a differentially private version $G$ of said graph though addition of noise to the edge weights. The network administrator then shares the privatized graph $G$ with a downstream user, that runs an optimization task on $G$ which, in this case, is a shortest path computation."}, {"title": "Bias: A Theoretical Perspective", "content": "This section presents the main theoretical insights of our work. Our primary contribution is characterizing the bias of the shortest path computation due to privacy noise and understanding how it drives unfair outcomes across different types of source-destination pairs on graphs. We introduce our first result in Claim 5.1 which provides insights about the sign or direction of the bias.\nClaim 5.1. The realized bias of the shortest path computation due to privacy noise is always greater than or equal to zero.\nProof. Suppose, some path $P \\in P_{ij}$ is the new perceived shortest path on privatized graph $G$ instead of the true shortest path $P^*_i$ on G. In this case, the realized bias $B_{ij} (P)$ is given by:\n$B_{ij} (P) = \\sum_{e \\in P} w(e) - \\sum_{e \\in P^*_i} w(e) = w_G(P) - w_G(P^*_i)$.\nNow, since $P^*_i$ is the true shortest path on G, by definition, it must be that:\n$w_G(P) \\geq w_G(P^*_i)  \\forall P \\in P_{ij}$,\nwhich directly implies that $B_{ij}(P) \\geq 0$. Since the above holds for any general path $P \\in P_{ij}$, this concludes the proof of the claim.\nA direct consequence of the above claim is that the expected bias and expected relative bias are non-negative. Note that all our numerical results in Section 6 plot empirical probabilities for incurring different levels of expected relative bias.\nWhen it comes to fairness impacts of privacy, there are two main competing effects that drive which groups of node pairs will unfairly face more disruptions (on average) due to privacy:\n1. The first of those is the effective relative noise effect which is explored in Section 5.1: when the number of path alternatives is fixed, we show that node pairs which are farther apart have a lower likelihood of being affected by privacy noise.\n2. On the other hand, we also demonstrate the path cardinality effect in Section 5.2, i.e., the higher the number of different paths available to travel between the source and destination, the higher is the likelihood of shifting to a worse path due to privacy noise and incurring a large bias. This effect favours node pairs which are closer because they usually have a smaller number of alternate path options.\nThe trade-off between these two effects explains most of our observations in the numerical experiments section. We also provide a dual interpretation of our main theorem in Section 5.2 which helps us to derive high probability bounds on the realized bias of any shortest path computation.\nBefore we present our main results, we need to introduce some additional notation for ease of exposition. From now on, we drop the subscript \u201cij\u201d whenever it is clear from context to simplify notations."}, {"title": "Effective Relative Noise Effect", "content": "In this segment, we are interested in understanding the disparate impacts that privacy noise has on node pairs which are close by versus node pairs which are far apart, when the number of alternate path options is kept fixed for each pair. We measure the impact of noise by estimating the probability that for any two given paths, the worse one is perceived to be better when computations are done using privatized graph $G$. Higher the value of this probability, higher is the impact of noise. We make the following conjecture:\nConjecture 5.2. Node pairs which are closer incur, on average, larger levels of relative noise and hence are more impacted by privacy as opposed to node pairs which are farther apart.\nIn order to gain intuition about why the above conjecture may be true, we will start by presenting the following technical result. Let $P^*$ be the true shortest path between nodes $i$ and $j$ and $P' \\neq P^*$ be any other alternate path. Define the gap $a_{P',P^*}$ as $a_{P',P^*} = w_G(P') - w_G(P^*)$. We assume that $a_{P',P^*} > 0$ which means that $P^*$ is strictly better than $P'$. Then,\nLemma 5.3. The probability that path $P'$ is perceived to be shorter than the true best path $P^*$ on a privatized graph $G$, i.e., $P [\\tilde{w}(P') < \\tilde{w}(P^*)]$, is given by:\n$q = \\Phi \\bigg(\\frac{a_{P', P^*}}{\\sigma \\sqrt{|S_{P',P^*}|}} \\bigg)$,\nwhere $\\Phi(\u00b7)$ is the complementary CDF of a standard normal random variable. We call \u201cq\u201d the path deviation probability.\nProof Sketch. Recall that $Z(e)$ is the amount of noise added to edge $e \\in E$. We know that $Z(e)$'s are i.i.d. normal mean-zero random variables with variance $\\sigma^2$. The proof idea is to express the event of choosing the wrong shortest path equivalently as an event when a certain linear inequality condition on $Z(e)$'s is satisfied. Then we can exploit the normality and independence properties of $Z(e)$'s to reason about the probability."}, {"title": "Path Cardinality Effect", "content": "In this segment, we are interested in understanding the disparate impacts that privacy noise has on node pairs which have many alternate path choices as opposed to node pairs which have fewer paths. We call this effect the path cardinality effect. In this case, we measure the impact of noise by estimating the probability of realizing bias at least as large as $\\beta$, given that the standard deviation of the privacy noise $\\sigma$ depends on the privacy parameter $\\epsilon$ and the sensitivity of the weight function $\\Delta f$. The dependence is of the following form: $\\sigma \\propto \\frac{\\Delta f}{\\epsilon}$. This implies that at higher levels of privacy (smaller $\\epsilon$), the probability q would be larger. This is intuitive: stronger privacy requires more perturbation to the edge weights and therefore there is a higher chance that the order is flipped, i.e., a previously longer path is perceived to be shorter. We can argue similarly for the case where the sensitivity of $f(\\cdot)$ is high. Higher sensitivity of $f(\\cdot)$ implies we need more noise to achieve the same level of privacy. This leads to higher q. We plot these dependencies in Figure 3.\nWe have already explored at depth how q depends in average on the effective relative noise (Conjecture 5.2). q also depends on the local network topology of paths $P'$ and $P^*$ as we illustrate with the following example. Let there be two users traveling between two different node pairs, each of them has two path choices, one which is the true best and another which is strictly worse. For ease of comparison, we assume that for both node pairs, the worse path is off the respective true best by the same amount $a$. Now, suppose that user 1 faces a scenario where both of her paths have a large degree of overlap, leading to a smaller $|S|$, while for user 2, the paths are largely distinct. In this case, user 2 has a higher chance of deviating to the worse path, simply because noise on shared edges affects both paths equally. This example demonstrates that despite the path gap being identical, unfairness can also arise due to network topology wherein privacy has a much more adverse effect on some users compared to others.\nWe consider a special case where none of the paths in $P_{ij}$ have overlapping edges. In this case, we will show that it is possible to derive an exact expression for $q_{\\beta}$.\nCorollary B.1. When paths in $P_{ij}$ have no overlapping edges, the probability $q_{\\beta}$ can be computed exactly and is given by:\n$q_{\\beta} = \\sum_{P \\in P^{> \\beta}_{ij}} \\bigg(\\int_{-\\infty}^{\\infty} \\Phi\\bigg(\\frac{t - w_G(R)}{\\sigma_{w_R}}\\bigg)  \\Phi\\bigg(\\frac{t - w_G(P)}{\\sigma_{w_P}}\\bigg) dt\\bigg)$.\nProof. The proof is similar to Theorem 5.5, the only difference being that we can compute the probability of the intersection event in closed form this time. We have already shown that:\n$q_{\\beta} = \\sum_{P \\in P^{> \\beta}_{ij}} \\bigg[ \\bigcap_{\\forall R \\in P_{ij} \\setminus P} \\{w_G(P) < w_G(R)\\} \\bigg]$"}, {"title": "Experimental Characterization of Bias and Unfairness", "content": "In this section", "generation": "The experiments investigate three different classes of graphs: i) 2-dimensional grid graphs", "class": "We use the following sets of parameters to generate synthetic networks for each graph class:\n\u2022 2-D grid graphs: A grid graph of size $N$ has $N^2$ nodes and $2N^2 + N$ edges. An illustration is provided in Figure 6 (top left).\n\u2022 Wheel graphs: These graphs are described by the number of nodes $N$ and the ratio $r$ of the spoke edge weights to the circumference edge weights ($r \\geq 1$). The central node is by default", "graphs": "These graphs have a degree distribution following a power law and are parametrized by their size (number of nodes $N$) and the exponent of the power law $(\\gamma)$. A higher $\\gamma$ indicates very few high-degree nodes", "model": "Given a ground truth graph", "Metrics": "Given a graph $G$", "bias": "i) 0% (indicating the shortest path remains unchanged)", "Uniform[0,1": "distribution. The two main parameters of interest here are i) size of grid N and ii) the variance (or standard deviation) of the privacy noise added. We generate results for 3 different grid sizes N = 10 (with 100 nodes)", "weight)": 20, "5.3": "for any node pair (i, j) and any path P, a higher noise level leads to a higher probability"}]}