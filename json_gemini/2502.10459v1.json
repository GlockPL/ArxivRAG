{"title": "LLM4GNAS: A Large Language Model Based Toolkit for Graph Neural Architecture Search", "authors": ["Yang Gao", "Hong Yang", "Yizhi Chen", "Junxian Wu", "Peng Zhang", "Haishuai Wang"], "abstract": "Graph Neural Architecture Search (GNAS) facilitates the automatic design of Graph Neural Networks (GNNs) tailored to specific downstream graph learning tasks. However, existing GNAS approaches often require manual adaptation to new graph search spaces, necessitating substantial code optimization and domain-specific knowledge. To address this challenge, we present LLM4GNAS, a toolkit for GNAS that leverages the generative capabilities of Large Language Models (LLMs). LLM4GNAS includes an algorithm library for graph neural architecture search algorithms based on LLMs, enabling the adaptation of GNAS methods to new search spaces through the modification of LLM prompts. This approach reduces the need for manual intervention in algorithm adaptation and code modification. The LLM4GNAS toolkit is extensible and robust, incorporating LLM-enhanced graph feature engineering, LLM-enhanced graph neural architecture search, and LLM-enhanced hyperparameter optimization. Experimental results indicate that LLM4GNAS outperforms existing GNAS methods on tasks involving both homogeneous and heterogeneous graphs.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Architecture Search (GNAS) [13, 16] has garnered significant attention as a promising method for automating the design of Graph Neural Networks (GNNs) tailored to downstream graph learning tasks. GNAS aims to alleviate the manual effort required for designing GNN architectures by leveraging reinforcement learning [16], differential gradient [24], and evolutionary algorithm [34]. However, current GNAS algorithms often face the challenges of designing new search spaces when meeting new downstream graph learning tasks, which calls for heavy manual adjustments to achieve optimal performance. For example, AutoHeG [50] aims to integrate additional operations of Heterophilic GNNs [3, 5] into traditional search space, which entails not only modifying the search space but also adapting the search algorithm to facilitate the selection of both legacy and novel operators. This limitation hampers the scalability and generalizability of GNAS.\nRecently, Large Language Models (LLMs), such as GPT-4 [31], have exhibited remarkable language understanding and generation capabilities. In particular, LLMs have been successfully used in designing novel optimizers [41] and CNN architectures [49], where a new set of prompts are designed to guide LLMs towards generating better CNN architectures by learning from prior attempts. Motivated by the success of LLMs in model design and optimization, we aim to integrate LLMs into GNAS by leveraging the generative capabilities of LLMs. Our goal is to develop a new class of GNAS library based on LLMs that are easily transferable across different graph tasks and search spaces.\nIn this paper, we present LLM4GNAS, a new toolkit for GNAS based on LLMs. LLM4GNAS leverages the language understanding and generation capabilities of LLMs to enhance the GNAS process. LLM4GNAS consist of three modules, i.e., LLM-enhanced node augmentation, LLM-enhanced graph neural search algorithms, and LLM-enhanced hyperparameter optimization. First, LLM4GNAS employs LLMs to generate rich features for graph data, which can augment the node features based on LLMs. Second, LLM4GNAS integrates graph neural search algorithms with LLM prompts, which enables diverse exploration of the search space. Third, LLM4GNAS incorporates hyperparameter optimization driven by LLMs. This approach ensures that the optimal set of hyperparameters can be identified effectively. Our contributions are outlined as follows:"}, {"title": "2 LLM4GNAS Framework", "content": "In this section, we introduce the framework of LLM4GNAS, as shown in Figure 1. Our framework is built on PyTorch, Hugging-Face Transformers, and PYG [10], which are popular deep learn-ing libraries for graphs and LLMs. We first demonstrate how to use LLM as the controller to perform graph neural architecture search, and then introduce three main modules of LLM4GNAS, i.e., LLM-enhanced Node Augmentation, LLM-enhanced GNAS, and LLM-enhanced Hyperparameter Optimization."}, {"title": "2.1 LLM as Controller", "content": "In LLM4GNAS, LLMs act as controllers of the graph neural architecture search algorithm. The framework begins with the LLM-enhanced Node Augmentation module, which enhances the features of the input graph data by employing LLMs to generate meaning-ful embeddings for nodes. Following this, LLM4GNAS integrates LLM-based Graph Neural Architecture Search and Hyperparameter Optimization. The Graph Neural Architecture Search utilizes LLMs to explore the search space of possible GNN architectures, aiming to identify architectures and hyperparameters that yield the best performance on the given graph data. The Hyperparameter Optimization module further refines the model's hyperparameters based on performance feedback.\nThe iterative nature of LLM4GNAS enables continuous refinement of the GNN architecture and its hyperparameters. The performance metrics of the designed model serve as feedback signals, guiding LLM4GNAS in adjusting the architecture and hyperparameters accordingly. This iterative process ensures the creation of highly effective and tailored GNN models capable of addressing diverse graph-related tasks. By incorporating LLMs into the design process, LLM4GNAS enhances the adaptability and performance of GNN models, making it a valuable tool for graph data analysis and processing."}, {"title": "2.2 LLM-enhanced Node Augmentation", "content": "In LLM4GNAS, we employ LLM-enhanced node augmentation to generate new node features for downstream tasks. This process incorporates both training-based node augmentation methods, such as TAPE [21], and training-free methods that utilize LLMs to generate node embeddings directly.\nFor labeled graph data, LLM4GNAS adopts explanation-based enhancement methods, such as TAPE [21] that uses LLMs to generate explanations and pseudo-labels to augment textual attributes. Subsequently, relatively small language models are fine-tuned on"}, {"title": "2.3 LLM-enhanced GNAS", "content": "We define the problem of LLM-based graph architecture search as follows. Given an LLM model 2, a dataset G, a GNN search space M, and an evaluation metric A, we aim to find the best architecture m* e Mon a given graph G, i.e.,\n $$m^* = \\underset{m \\in M(\\Omega(P))}{\\operatorname{argmax}} A(m(G)),$$ (1)\nwhere \u039c(\u03a9(P)) denotes the search space generated by the LLM; P is the GNAS prompt that guides LLM to perform graph neural architecture search; and the metric A, for example, can be either accuracy or AUC for node classification tasks.\nThe process of LLM-based GNAS involves iteratively designing GNNs and optimizing their architectures based on feedback from their validation performance. This process is analogous to code generation using LLM-based chatbots that adapt according to user feedback. Specifically, we first employ an LLM to design a set of GNN architectures, denoted as M(\u03a9(P)). These GNNs are then derived from the search space, constructed, and evaluated on the given graph data. Performance metrics, such as accuracy on the validation set A, serve as feedback signals, guiding the LLM to generate improved GNN architectures. By continuously integrating the generated GNNs M(\u03a9(P)) and their evaluation results A in the reward prompt PF, the LLM converges fast. In the last step, GNAS-LLM obtains the best GNN m* generated by the LLM.\nWe design the GNAS Prompts P to guide LLMs to generate new candidate GNN architectures, i.e., M(\u03a9(P)). The design of GNAS prompts needs to be aware of the diverse search space and search strategy in GNAS. Specifically, GNAS Prompts P contains candidate GNN operations and their connections to describe the search space. Besides, GNAS Prompts P also contains the description of the search strategy to guide the generation of new GNN architectures, such as reinforcement learning-like search strategies used in GNAS-LLM and GHGNAS.\nThe advantages of LLM4GNAS are summarized as follows. Firstly, LLM4GNAS enables adaptation to different search spaces by the modification of the GNAS prompts. This capability allows researchers to adjust the architecture search process to suit the characteristics of the target graph data, ensuring that the resulting GNN architecture is well-suited to the specific task. Secondly, LLM4GNAS enables to adjustment of the architecture search process through prompts and enables researchers to iteratively refine the search process, potentially leading to the discovery of more effective architectures."}, {"title": "2.4 LLM-enhanced Hyperparameter Optimization", "content": "LLM4GNAS also incorporates LLM-enhanced hyperparameter optimization tools. Empirical evaluations [43] indicate that, in settings with constrained search budgets, LLMs can perform comparably to, or even surpass, traditional hyperparameter optimization methods such as random search and Bayesian optimization on standard benchmarks. By leveraging the advanced capabilities of LLMs, LLM4GNAS streamlines the hyperparameter optimization process, thereby reducing both the computational burden and the time required for this essential task."}, {"title": "3 Interfaces of LLM4GNAS", "content": "In this part, we introduce the interfaces of LLM4GNAS by using different examples."}, {"title": "3.1 GNAS Example with LLM4GNAS", "content": "LLM4GNAS provides a user-friendly interface for designing new GNN models on new graphs. Users can run the program with a single command, as illustrated in the following example:\npython -m 11m4gnas.main -search_space AutoGEL -input /path/data -task NodeClassification -output /path/result\nBy specifying the graph data path, search space (e.g., AutoGEL), task type (e.g., Node Classification), and output directory, users can seamlessly configure LLM4GNAS to automatically load the graphs and conduct graph neural architecture search. This streamlined process empowers users to efficiently explore and design GNN models tailored to their specific graph learning tasks.\nIn addition to supporting command-line training, LLM4GNAS offers Python APIs for user programming, enabling manual datasets loading and automatic GNN design. The following example demonstrates the usage of LLM4GNAS APIs:"}, {"title": "3.2 GNAS Example with New Search Space", "content": "LLM4GNAS introduces a new feature that can adapt to new search spaces, making it easy to extend to new graph learning tasks. This feature leverages prompt engineering to adapt, verify, and equip LLM4GNAS for new search spaces, enabling users to integrate their expertise and domain knowledge into the search process. By simplifying the adaptation process through prompts, LLM4GNAS facilitates the exploration of novel search spaces, leading to the development of more effective and specialized GNN architectures for diverse graph tasks. The following example demonstrates how to add a new Search Space to LLM4GNAS:"}, {"title": "4 Experiments", "content": "In this section, we present a series of experiments designed to validate the performance of LLM4GNAS. Initially, we evaluate LLM4GNAS on homogeneous graphs. Subsequently, we extend our evaluation to heterogeneous graphs. Finally, we conduct an ab-lation study to assess the efficiency of LLM4GNAS, complemented by a test case exploring its adaptability to new search spaces."}, {"title": "4.1 Experiment Setup", "content": "4.1.1 Baselines. We compare GNNs designed by LLM4GNAS with the most popular GNNs, including GCN [25], GAT [36], GCNII, and GATv2 [4]. For experiments on heterogeneous graphs, we take RGCN [33], HAN [38], HGT [23], MAGNN [11] as additional baselines. We also compare the proposed model with other GNAS methods, including Random Search, GraphNAS [13], HGNAS [14], and AutoGEL [39]. We use the same experimental setting used in AutoGEL for homogeneous graphs and HGNAS++ [17] for heterogeneous graphs.\n4.1.2 Hyperparameters. LLM4GNAS runs 15 search iterations. The number of iterations is constrained by the length of prompts that the LLMs allow. During each iteration, LLMs generate 10 new architectures. For each architecture search, we only use one candidate operation connections and nine candidate operations. In the following experiments, we repeat our method three times and show the best results w.r.t. accuracy on a validation dataset.\nIf not otherwise specified, we use GPT-4 with version V20230314 as the default LLM in the experiments. For all models, we set temperature \u03c4 = 0 for reproducibility. We adopt accuracy as the metric for all tasks. Additionally, we use all the GNAS baselines to generate N = 10 architectures at each iteration. Specifically, dor GraphNAS, the ADAM optimizer is used, with a learning rate of 0.00035. In the experiments involving AutoGEL, we set the layer number to 2, the ADAM optimizer with a learning rate of 5e-4, a minibatch size of"}, {"title": "4.2 Results on Homogeneous Graphs", "content": "We evaluated the performance of LLM4GNAS on homogeneous graphs across node classification, link prediction, and graph classification tasks, adhering to the same experimental settings as used in AutoGEL.\nTable 2 presents a comparison of LLM4GNAS with several baseline models across various tasks on homogeneous graphs. Specifically, in link prediction tasks, LLM4GNAS attains accuracies of 99.88% on the NS dataset and 99.33% on the Router dataset, outperforming the other methods. Our method performs worse than AutoGEL on the Power dataset but higher than the other baseline models. For node classification tasks, LLM4GNAS yields accuracies of 89.20% on Cora and 90.30% on PubMed, ranking second on both datasets; AutoGEL and GraphNAS achieve the highest accuracies on Cora (89.89%) and PubMed (91.20%), respectively. On Citeseer, LLM4GNAS obtains an accuracy of 76.20%, which is comparable to other methods but slightly lower than the best-performing models. In graph classification tasks, LLM4GNAS achieves an accuracy of 94.73% on MUTAG, closely matching AutoGEL's result of 94.74%. On the PROTEINS dataset, it attains 76.23%, representing the second-highest performance among the compared methods. Although LLM4GNAS does not achieve the highest accuracy on IMDB-B, it maintains competitive performance.\nTo further illustrate the effectiveness of LLM4GNAS during the architecture search process, we present Figure 2 and Figure 3. Figure 2 depicts the variation in test accuracy of the best GNN architectures identified at each iteration by GNAS methods, where LLM4GNAS demonstrates a rapid improvement over the initial iterations, consistently surpassing the performance of the other methods. Figure 3 shows the iterations of the best GNN architectures generated by LLM4GNAS on homogeneous graphs, highlighting the modifications made at each step. These figures indicate that the integration of LLMs enables LLM4GNAS to more effectively explore the search space, leading to the identification of superior architectures in fewer iterations. Consequently, LLM4GNAS not"}, {"title": "4.3 Results on Heterogeneous Graphs", "content": "We evaluated LLM4GNAS on heterogeneous graphs for node classification and link prediction tasks, following the experimental setup used in HGNAS++. The node classification experiments were conducted on the ACM, DBLP, and IMDB datasets, while the link prediction tasks were performed on the Amazon, Yelp, and Movie-Lens datasets. The results are summarized in Table 3.\nIn both node classification and link prediction tasks, LLM4GNAS demonstrated strong performance. For node classification, it achieved the highest accuracies on the ACM dataset (92.97%) and the IMDB dataset (62.66%). On the DBLP dataset, LLM4GNAS attained an accuracy of 94.41%, which is slightly below the highest score obtained by DiffMG (94.45%) but higher than all other compared methods.\nCompared to baseline models such as GCN, GAT, GCNII, and others, LLM4GNAS consistently showed improved results. In the link prediction tasks, LLM4GNAS outperformed all other methods on the Amazon, Yelp, and MovieLens datasets, achieving accuracies of 79.04%, 92.92%, and 92.76%, respectively. These results surpass those of HGNAS, which previously held the highest accuracies among the compared models for these datasets.\nThe comparative analysis indicates that LLM4GNAS effectively leverages LLMs to explore the architecture search space in heterogeneous graphs, resulting in models that perform better on both node classification and link prediction tasks. The consistent performance gains across multiple datasets suggest that LLM4GNAS is robust and generalizes well to various heterogeneous graph settings."}, {"title": "4.4 Ablation Study", "content": "We design ablation experiments to verify that the design of LLM4GNAS are effective. Firstly, we design ablation experiments on modules of LLM4GNAS to verify that LLM-based Node Augmentation and HPO are effective. Furthermore, we design ablation experiments on LLMs to show our toolkit can adapt to different LLMs. Lastly, we design a case study on a new search space to show our toolkit can adapt to a new search space.\n4.4.1 On Modules. We investigated the impact of incorporating LLM-enhanced node augmentation into various graph neural networks (GNNs) to generate more informative node embeddings for downstream tasks. The node augmentation process enriches the feature space of graph nodes by integrating external data, specifically utilizing large language models (LLMs) to supplement the textual information associated with nodes. By introducing additional contextual features, this approach allows models to capture more nuanced node representations, potentially leading to improved performance in tasks such as node classification. We integrated this augmentation technique with several GNN architectures, including GCN, GAT, and GCNII, which traditionally rely solely on the structural and inherent feature information present in the dataset. As shown in Table 4, incorporating node augmentation resulted in performance improvements across multiple datasets, particularly those rich in features. For example, on the PubMed dataset, GCN with node augmentation (GCN+NA) improved accuracy from 83.62% to 93.53%. Similarly, on the Arxiv dataset, GCNII+NA achieved an accuracy of 81.57%, compared to 69.85% with GCNII alone, demonstrating the benefit of the added node augmentation. However, the performance gains were not uniform across all datasets and models. In the Cora dataset, the addition of node augmentation to the GCN model did not significantly enhance results. Conversely, in more challenging datasets like Citeseer, GAT+NA outperformed the baseline GAT model, increasing accuracy from 65.9% to 75.0%. Additionally, GCNII+NA showed consistent improvements across all datasets, achieving the highest performance of 75.80% in Citeseer, surpassing both GCN and GAT variants with node augmentation. These findings suggest that the effectiveness of node augmentation depends on the dataset characteristics and that combining a robust model architecture with enriched node features can lead to substantial performance enhancements, especially in tasks where the structural complexity of the graph requires advanced feature extraction.\n4.4.2 On Different LLMs. We evaluated the impact of using different large language models (LLMs) within our LLM4GNAS framework by employing GPT-4, GPT-3, LLAMA-2, and GLM-3. The results of this ablation study are presented in Table 5. LLM4GNAS utilizing GPT-4 achieved the highest accuracy across all datasets. On the Cora dataset, for instance, LLM4GNAS with GPT-4 attained an accuracy of 89.22%, compared to 88.90% with GPT-3, 87.88% with GLM-3, and 85.45% with LLAMA-2. Similar patterns were observed on the Citeseer and Pubmed datasets. These results indicate that the choice of LLM has a significant effect on the performance of our method, with GPT-4 providing the most effective enhancements.\n4.4.3 On New Search Space. We evaluated LLM4GNAS within the search space defined by NAS-Bench-Graph, using the same experimental settings outlined in that benchmark. This approach ensures fair comparisons and consistency across all evaluated models. The primary objective was to assess the architecture search capabilities of LLM4GNAS relative to other established methods such as Random search, GraphNAS, and Genetic-GNN, all of which have been previously tested within the same search space.\nAs shown in Table 6, LLM4GNAS outperformed its counterparts across various datasets, achieving the highest accuracy in all tasks. For instance, on the Cora dataset, LLM4GNAS attained an accuracy of 80.93%, surpassing the results of Genetic-GNN and GraphNAS. Similarly, on the Citeseer dataset, it achieved an accuracy of 70.22%, slightly exceeding other models. In the Pubmed dataset, LLM4GNAS achieved 79.87% accuracy, outperforming both GraphNAS and Random methods. Additionally, on the Arxiv dataset, it matched the top performance of Genetic-GNN with an accuracy of 73.41%. These results indicate that LLM4GNAS is effective in navigating the architecture search space, consistently achieving high accuracy across diverse graph datasets. This suggests its potential utility as a tool for neural architecture search in graph-based learning tasks.\n4.4.4 On New Graphs. We conduct experiment on a large graph dataset, ogbn-products [22], which contains 2,449,029 nodes and 61,859,140 edges. Each node has 100 features, and a total of 47 different node classes. We use the functions provided by the SGL library [45]. For the search process, we use GNAS-LLM to search 10 architectures in each of the 15 rounds. We also use random search and Pasca [45] as baselines, exploring a total of 150 architectures."}, {"title": "5 Related Work", "content": "5.1 Graph Neural Architecture Search (GNAS)\nGraphNAS [13] is among the earliest methods employing reinforcement learning to design GNN architectures. Building upon GraphNAS, AutoGNN [52] introduces an entropy-driven candidate model sampling method and a weight-sharing strategy to select GNN components more efficiently. GraphNAS++ [16] accelerates GraphNAS by utilizing distributed architecture evaluation. GM2NAS [12] also employs reinforcement learning to design GNNs for multitask multiview graph learning, while MVGNAS [1] is tailored for biomedical entity and relation extraction. Additionally, HGNAS [15] and HGNAS++ [17] utilize reinforcement learning to discover heterogeneous graph neural networks.\nIn contrast to reinforcement learning-based GNAS methods that explore a discrete GNN search space, differentiable gradient-based GNAS methods have emerged, exploring a relaxed, continuous GNN search space. These methods include DSS [27], SANE [24], GAUSS [18], GRACES [32], AutoGT [48], and Auto-HEG [50]. SANE focuses on discovering data-specific neighborhood aggregation architectures, while DSS is designed for GNN architectures with a dynamic search space. GAUSS addresses large-scale graphs by devising a lightweight supernet and employing joint architecture-graph sampling for efficient handling. GRACES aims to generalize under distribution shifts by tailoring GNN architectures to each graph instance with an unknown distribution. AutoGT extends GNAS to Graph Transformers, and Auto-HEG enables automated graph neural architecture search for heterophilic graphs. Furthermore, methods such as AutoGEL [39], DiffMG [8], MR-GNAS [51], and DHGAS [46] focus on heterogeneous graphs.\nEvolutionary algorithms have also been employed in GNAS by methods such as AutoGraph [28] and Genetic-GNN [34], which aim to identify optimal GNN architectures. G-RNA [40] introduces a unique search space and defines a robustness metric to guide the search process for defensive GNNs. Surveys by Zhang et al. [47] and Oloulade et al. [30] provide comprehensive overviews of automated machine learning methods on graphs.\nGNAS-LLM [37] and GHGNAS [9] explore the application of LLMs to enhance the GNAS search process. Building upon these works, we introduce an LLM-based GNAS toolkit that integrates feature augmentation, graph neural architecture search, and hyperparameter optimization driven by LLMs. This approach allows our toolkit to adapt to complex search spaces and design GNN architectures for new tasks, thereby advancing the flexibility of GNAS methodologies."}, {"title": "5.2 Large Language Models", "content": "GPT-4 has emerged as an AI model capable of providing responses to inquiries involving multi-modal data [31]. Studies indicate that GPT-4 exhibits proficiency in comprehending graph data [19], demonstrating strong performance across various graph learning tasks. Additionally, research has integrated large language models with graph learning models, utilizing GPT-4 for reasoning over graph data [42]. In contrast, BERT adopts a pre-training approach on extensive unlabeled data, followed by fine-tuning on specific downstream tasks [7]. Building upon BERT, several language models have been proposed [20, 26, 29]. For instance, PaLM is constructed based on the decoder of Transformers [35], with PaLM 2 achieving improved results through a larger dataset and a more intricate architecture [2, 6].\nRecent developments have seen the use of large language models for neural architecture search. GENIUS, for example, leverages GPT-4 to design neural architectures for CNNs, exploring the generative capacity of LLMs [49]. The fundamental concept involves enabling GPT-4 to learn from feedback on generated neural architectures, thereby iteratively improving the design. Experimental results on various benchmarks demonstrate GPT-4's ability to discover top-ranked architectures after several prompt iterations. AutoML-GPT introduces a series of prompts for LLMs to autonomously undertake tasks such as data processing, model architecture design, and hyperparameter tuning [44]. Notably, GPT-4 has been introduced into graph neural architecture search by GNAS-LLM [37] and GHG-NAS [9], aiming to search for the optimal GNN or heterogeneous GNN within a designated search space.\nIn this paper, we present a toolkit that extends the application of LLMs to the generation of new GNN architectures. The toolkit is designed to be easily extendable to new search spaces and graph tasks, offering a user-friendly interface for practical applications and academic research. Our aim is to enhance the performance of existing GNAS methods by providing a versatile and efficient solution for GNN design."}, {"title": "6 Conclusions", "content": "In this work, we introduced LLM4GNAS, a toolkit designed to automatically generate GNN models tailored to a variety of graph-related tasks. The toolkit integrates LLM-based node augmentation methods, GNAS strategies, and hyperparameter optimization techniques. By leveraging the capabilities of LLMs, LLM4GNAS effectively extends the search space and accommodates different types of graphs, demonstrating adaptability and efficacy in enhancing GNN performance. LLM4GNAS contributes to the field of GNN architecture search by providing a versatile solution capable of addressing a range of graph-related challenges. The integration of LLMs within the toolkit automates and optimizes the design process, aiming to achieve high performance and generalization across multiple tasks.\nFuture work will focus on enhancing the toolkit by automating the design of search strategy prompts and developing transferable LLM-based GNAS methods specifically for graph foundation models. By concentrating on the architecture design and fine-tuning of these graph foundation models, we aim to streamline the neural architecture search process and improve the adaptability of LLM4GNAS to a broader spectrum of graph-related problems. These efforts are intended to further optimize the toolkit's effectiveness and extend its applicability across diverse graph learning tasks."}]}