{"title": "Multi-Agent Risks from Advanced AI", "authors": ["Lewis Hammond", "Alan Chan", "Jesse Clifton", "Jason Hoelscher-Obermaier", "Akbir Khan", "Euan McLean", "Chandler Smith", "Wolfram Barfuss", "Jakob Foerster", "Tom\u00e1\u0161 Gaven\u010diak", "The Anh Han", "Edward Hughes", "Vojt\u011bch Kova\u0159\u00edk", "Jan Kulveit", "Joel Z. Leibo", "Caspar Oesterheld", "Christian Schroeder de Witt", "Nisarg Shah", "Michael Wellman", "Paolo Bova", "Theodor Cimpeanu", "Carson Ezell", "Quentin Feuillade-Montixi", "Matija Franklin", "Esben Kran", "Igor Krawczuk", "Max Lamparth", "Niklas Lauffer", "Alexander Meinke", "Sumeet Motwani", "Anka Reuel", "Vincent Conitzer", "Michael Dennis", "Iason Gabriel", "Adam Gleave", "Gillian Hadfield", "Nika Haghtalab", "Atoosa Kasirzadeh", "S\u00e9bastien Krier", "Kate Larson", "Joel Lehman", "David C. Parkes", "Georgios Piliouras", "Iyad Rahwan"], "abstract": "The rapid development of advanced AI agents and the imminent deployment\nof many instances of these agents will give rise to multi-agent systems of un-\nprecedented complexity. These systems pose novel and under-explored risks.\nIn this report, we provide a structured taxonomy of these risks by identify-\ning three key failure modes (miscoordination, conflict, and collusion) based on\nagents' incentives, as well as seven key risk factors (information asymmetries,\nnetwork effects, selection pressures, destabilising dynamics, commitment prob-\nlems, emergent agency, and multi-agent security) that can underpin them. We\nhighlight several important instances of each risk, as well as promising direc-\ntions to help mitigate them. By anchoring our analysis in a range of real-world\nexamples and experimental evidence, we illustrate the distinct challenges posed\nby multi-agent systems and their implications for the safety, governance, and\nethics of advanced A\u0399.", "sections": [{"title": "1 Introduction", "content": "The proliferation of increasingly advanced AI not only promises widespread benefits, but also presents\nnew risks (Bengio et al., 2024; Chan et al., 2023). In the future, AI systems will commonly interact and\nadapt in response to one another, forming multi-agent systems.\u00b9 This trend will be driven by several\nfactors. First, recent technical progress and publicity will continue to drive adoption, including in high-\nstakes areas such as financial trading (AmplifyETFs, 2025; Ferreira et al., 2021; Sun et al., 2023a) and\nmilitary strategy (Black et al., 2024; Manson, 2024; Palantir, 2025). Second, AI systems that can act\nautonomously and adapt while deployed as agents will have competitive advantages compared to non-\nadaptive systems or those with humans in the loop. Third, the more widely such agents are deployed,\nthe more they will come to interact with one another.\nThe emergence of these advanced multi-agent systems presents a number of risks which have thus far\nbeen systematically underappreciated and understudied. In part, this lack of attention is because the\ndeployment of such systems is currently rare, or constrained to highly controlled settings (such as auto-\nmated warehouses) that do not suffer from the most severe risks. In part, it is because even the simpler\nproblem of ensuring the safe and ethical behaviour of a single advanced AI system is far from solved\n(Amodei et al., 2016; Anwar et al., 2024; Hendrycks et al., 2021), and multi-agent settings are strictly\nmore complex. Indeed, many multi-agent risks are inherently sociotechnical and require attention from\nmany stakeholders and researchers across many disciplines (Curtis et al., 2024; Lazar & Nelson, 2023).\nImportantly, these risks are distinct from those posed by single agents or less advanced technologies, and\nwill not necessarily be addressed by efforts to mitigate the latter. For example: the alignment of AI agents\nwith different actors is insufficient to prevent conflict if those actors have diverging interests (Critch &\nKrueger, 2020; Dafoe et al., 2020; Jagadeesan et al., 2023a; Manheim, 2019; Sourbut et al., 2024); errors\nthat may be acceptable in isolation could compound in complex, dynamic networks of agents (Buldyrev\net al., 2010; Kirilenko et al., 2017; Lee & Tiwari, 2024; Maas, 2018; Sanders et al., 2018); and groups\nof agents could combine or collude to develop dangerous capabilities or goals that cannot be ascribed\nto any individual (Calvano et al., 2020; Drexler, 2022; Jones et al., 2024; Mogul, 2006; Motwani et al.,\n2024). Advanced AI also introduces phenomena that differ fundamentally from previous generations of\nAI or other technologies, requiring new approaches to mitigating these risks (Bengio et al., 2024).\nWith the current rate of progress, we therefore urgently need to evaluate (and prepare to mitigate)\nmulti-agent risks from advanced AI. In this report we take a first step in this direction by providing a\ntaxonomy of risks that either: emerge, are much more challenging, or are qualitatively different in the\nmulti-agent setting (see Table 1). We identify three key high-level failure modes (Section 2), and seven\nkey risk factors that can lead to these failures (Section 3), before discussing the implications for AI\nsafety, AI governance, and AI ethics (Section 4). Throughout the report we illustrate these risks with\nconcrete examples, either from real-world events, previous research, or novel experiments (see Table 3)."}, {"title": "1.1 Overview", "content": "We begin by identifying different failure modes in multi-agent systems based on the nature of the agents'\ngoals and the intended behaviour of the system. In most multi-agent systems, we are interested in AI\nagents working together to achieve their respective goals or the goals of those who deployed them. In this\ncase, we categorise failures into miscoordination (Section 2.1), where agents fail to cooperate despite\nhaving the same goal, and conflict (Section 2.2), where agents with different goals fail to cooperate. A\nthird and final kind of failure \u2013 collusion (Section 2.3) can arise in competitive settings where we do\nnot want agents cooperating (such as markets).\nWe next introduce a number of risk factors by which these failure modes can arise, and which are largely\nindependent of the agents' precise incentives.\u00b2 For example, information asymmetries could lead to\nmiscoordination between agents with the same goal, or to conflict among agents with competing goals.\nThese factors are not specific to AI systems, but the differences between AI systems and other kinds\nof intelligent agents (such as humans or corporations) leads to different risk instances and potential\nsolutions. Finally, note that the following factors are not necessarily exhaustive or mutually exclusive."}, {"title": "1.2 Scope", "content": "Concerns about the risks posed by AI systems range from biased hiring decisions (Raghavan et al., 2020)\nto existential catastrophes (Bostrom, 2014), and are represented by a vast literature. Before giving a\nbrief overview of the most closely related works, it is therefore worth us pausing to clarify the scope of\nthis report, which is as follows.\n\u2022 Risks and failure modes: we seek to identify specific mechanisms via which risks could emerge,\nrather than just just the open research problems that these risks present.\n\u2022 Multiple agents: if the risk could arise in essentially the same way in the context of a single AI\nsystem, then we deem it to be out of scope for this report (while not diminishing its importance).\u00b3\n\u2022 Advanced AI: while many of the risks we identify also apply to simpler systems, their effects\nare most severe in the context of increasingly autonomous and powerful AI agents, and so this is\nwhere our primary focus lies.\n\u2022 Real-world examples: wherever possible, we make sure to ground these risks in real-world events,\nprevious research, or novel experiments - not merely hypothetical speculation (see Table 3).\n\u2022 Technical perspectives: due to the authors' expertise (and to keep the scope of the report\nmanageable), we primarily discuss risks from a technical perspective, while acknowledging that\nthis perspective is limited.\n\u2022 Concrete paths forwards: where possible, we aim to specify relatively narrow proposals for\nfuture research, in the hope that this makes it easier for others to contribute.\nNeedless to say, multi-agent risks from advanced AI are by no means the only risks posed by AI, and\nthe perspective we take in this report is by no means the only approach to understanding these risks.\nMoreover, we almost entirely neglect the potential upsides of advanced multi-agent systems: greater\ndecentralisation and democratisation of AI technologies; assistance in cooperating and coordinating\nwith others; increased robustness, flexibility, and efficiency; novel approaches to solving alignment and\nsafety issues in single-agent settings; and - perhaps most importantly more widespread and evenly\ndistributed benefits from AI. We hope that this report serves to complement earlier and adjacent research\non understanding these challenges and opportunities."}, {"title": "1.3 Related Work", "content": "The most similar report to ours is that of Manheim (2019), who introduces a range of technical multi-\nagent failure modes through the lens of model over-optimization. This over-optimisation can result in\nthe intended and actual behaviour of the model coming apart when faced with low-probability inputs, a\nregime change, measurement errors, or inaccuracies in the model's internal representations. While this\nlens is helpful for understanding some multi-agent risk factors, not all factors can neatly be captured\nthrough it. Altmann et al. (2024) and Mogul (2006) take an alternative perspective and focus on\n'emergent' failures that occur specifically in multi-agent settings, though their focus is not on advanced\nAI agents. Also highly relevant is Clifton (2020)'s agenda on cooperation and conflict in the context of\ntransformative AI, though the priority of that work is to describe a set of promising research directions,\nrather than to explicate the underlying risks.\nMore broadly, the topics of this report are closely related to the emerging subfield of cooperative AI\n(Bertino et al., 2020; Conitzer & Oesterheld, 2023; Dafoe et al., 2021, 2020), which chiefly studies how\nto engineer AI systems in order to help solve cooperation problems between humans, AI agents, or\ncombinations thereof. In contrast to these previous agendas, we also discuss failures from undesirable\ncooperation (i.e., collusion) and focus more on the concrete mechanisms via which failures can occur,\nrather than the capabilities needed for addressing them. We also incorporate additional perspectives\nbeyond traditional game-theoretic paradigms - such as complex systems and security - and highlight\nimplications for work in AI governance and AI ethics in addition to AI safety.\nOther surveys of AI risks focus primarily on the case of individual (often present-day) AI systems. For\nexample, Amodei et al. (2016) survey a range of concrete problems in AI safety (side effects, reward\nhacking, scalable oversight, safe exploration, and robustness to distributional shifts), while Hendrycks\net al. (2021) provide a classification of problems in ML safety (robustness, monitoring, alignment, and\nsystemic safety). Anwar et al. (2024), Bird et al. (2023), Bommasani et al. (2021), and Weidinger et al.\n(2022) focus on the risks from foundation models specifically, while Chan et al. (2023) and Gabriel et al.\n(2024) consider the harms posed by increasingly \u2018agentic' systems and AI assistants. Other taxonomies\nseek to adopt an explicitly sociotechnical lens (Abercrombie et al., 2024; Shelby et al., 2023; Weidinger\net al., 2023b), often focusing primarily on present-day risks. Uuk et al. (2025) and Zeng et al. (2024)\nprovide meta-reviews of AI risks derived from different research papers, as well as government and"}, {"title": "2 Failure Modes", "content": "Multi-agent systems can fail in various ways, depending on the intended behaviour of the system and the\nobjectives of the agents. First, we can distinguish between cases where we want the agents to be cooper-\nating (as in collective action problems or team games) or competing (such as in markets or adversarial\ntraining). Second, we can further divide the space of failure modes depending on whether the agents\nhave exactly the same objectives, different but overlapping objectives, or completely opposed objectives. While different authors have used different terms to describe these cases, we use the terminology shown\nin Figure 1. Finally, there are many potential risks from advanced multi-agent systems that do not\nnecessarily arise through agents competently pursuing their objectives, but due to their incompetencies\nor vulnerabilities. We consider these latter failures as part of our discussion on different risk factors in\nSection 3."}, {"title": "2.1 Miscoordination", "content": "The simplest kind of cooperation failures are those in which all agents have (approximately) the same\nobjectives. Even in such common-interest settings, however, miscoordination abounds. While it is\nreasonable to expect that these problems will tend to be addressed as the general capabilities of AI\nsystems (such as communication and reasoning about others) improve, they may still present risks in\nthe near-term."}, {"title": "2.1.1 Definition", "content": "Miscoordination arises when agents, despite a mutual and clear objective, cannot align their behaviours\nto achieve this objective. Unlike the case of differing objectives, in common-interest settings there is a\nmore easily well-defined notion of 'optimal' behaviour and we describe agents as miscoordinating to the\nextent that they fall short of this optimum. Note that for common-interest settings it is not sufficient\nfor agents' objectives to be the same in the sense of being symmetric (e.g., when two agents both want\nthe same prize, but only one can win). Rather, agents must have identical preferences over outcomes\n(e.g., when two agents are on the same team and win a prize as a team or not at all).\nIt is rare that two humans will share exactly the same objectives in this sense. For example, two\nsportspeople on the same team may be primarily aiming to win their match but will also have individual\npreferences, such as who scores the winning point. In the case of AI systems, however, different agents can\nmore easily be given precisely the same goal, and indeed much work on cooperation in AI focuses solely\non the common-interest setting (Boutilier, 1996; Omidshafiei et al., 2017; Oroojlooy & Hajinezhad, 2022;\nPeshkin et al., 2000; Rashid et al., 2018; Stone et al., 2010). Such approaches are typically motivated by"}, {"title": "2.1.2 Instances", "content": "Perhaps the most likely way that common-interest settings may arise in practice is where a single prin-\ncipal deploys multiple AI agents on their behalf in order to jointly solve a task. This choice might be\nmotivated by: physical constraints (if the task comprises sub-tasks that must be completed separately\nand simultaneously); efficiency considerations (if having a single agent in charge of all aspects of the task\nwould lead to an intractably complex problem); or a desire for robustness (if an individual agent might\nfail but others could still succeed in their place). Alternatively, we might see multi-principal multi-agent\nsettings in which the agents' goals are sufficiently aligned to be viewed as (approximately) identical.\nFor example, if two autonomous vehicles are driving along the same road, then the mutual harms from\npotential miscoordination (such as a collision) are far greater than any small individual benefits from\ncompetition (such as attempting a risky overtaking manoeuvre to get slightly ahead).\nIncompatible Strategies. Even if all agents can perform well in isolation, miscoordination can still\noccur due to the agents choosing incompatible strategies (Cooper et al., 1990). Competitive (i.e., two-\nplayer zero-sum) settings allow designers to produce agents that are maximally capable without taking\nother players into account. Crucially, this is possible because playing a strategy at equilibrium in the\nzero-sum setting guarantees a certain payoff, even if other players deviate from the equilibrium (Nash,\n1951). On the other hand, common-interest (and mixed-motive) settings often allow a vast number of\nmutually incompatible solutions (Schelling, 1980), which is worsened in partially observable environments\n(Bernstein et al., 2002; Reif, 1984). As a simple example, everyone driving on the left side or the right\nside of the road are both perfectly valid ways of keeping drivers safe, but these two conventions are\ninherently incompatible with one another (see Case Study 1)."}, {"title": "2.1.3 Directions", "content": "Decentralised control and coordination in multi-agent systems have been well-studied problems for\ndecades (Boutilier, 1996; Omidshafiei et al., 2017; Oroojlooy & Hajinezhad, 2022; Peshkin et al., 2000;\nRashid et al., 2018; Stone et al., 2010). At one level of abstraction, the key challenge of coordination\nis that of sharing information, i.e., communication. If agents have the same preferences and are able to\ncommunicate, they can coordinate by (say) having a single agent announce their intended action and\neveryone else follow suit, since there are no incentives for the leader to lie or the followers to deviate\n(e.g., Farrell & Rabin, 1996b). Given the superhuman capabilities of advanced AI to transmit and pro-\ncess vast swathes of information, the most important research directions in this area will therefore be\nthose in which it is not possible to exercise these capabilities (e.g., due to complexity, latency, or privacy\nconstraints).\nCommunication. As noted above, the advanced communication abilities of LLMs promise to simplify\nmany coordination challenges. In order to successfully integrate these advances into real-world systems,\nhowever, agents need to know when and what needs to be coordinated on something that may not\nalways be obvious in novel or out-of-distribution domains. In safety-critical domains, it may therefore\nbe necessary to introduce, or have the agents invent, protocols (i.e., rules and specifications) for commu-\nnication between advanced AI agents (Marro et al., 2024). Moreover, agents need to agree on how the\ncommunication channel is grounded (Clark & Brennan, 1991) to actions or strategies in the environment.\nGrounding LLMs is a problem that is not unique to coordination (Bender & Koller, 2020; Bisk et al.,\n2020; Mahowald et al., 2023), but it is exacerbated by the fact that agents attempting to coordinate\nthrough natural language need to be grounded in the same way. For instance, if they are designed with\ndifferent interfaces to tools in a domain, they must be able to coordinate despite these differences in\ninterfaces.\nNorms and Conventions. For settings in which inter-agent communication is infeasible or insufficient,\nnorms and conventions may be necessary in order to avoid miscoordination (Leibo et al., 2024). For\nexample Hadfield-Menell et al. (2019) show that even the adoption of so-called 'silly rules' (those that do\nnot have direct bearing on the agents' payoffs) can help groups adapt and be more robust to uncertainty\nby enriching the information environment. Moving beyond more arbitrary conventions, we may choose to\ndesign particular norms and conventions (Bicchieri, 2016; Nyborg et al., 2016; Shoham & Tennenholtz,\n1992). In this setting, the challenge is to select norms that are both legible and enforceable, as well as\nleading to jointly beneficial outcomes. On the other hand, if the agents can adapt their behaviour, it\nmay be that norms and conventions emerge over time (McElreath et al., 2003). For example, K\u00f6ster\net al. (2020) show that multi-agent reinforcement learning (MARL) agents can establish and switch\nbetween conventions, even compromising on their own objective when doing so is necessary for effective\ncoordination. More generally, we may be interested in studying how norms and conventions emerge"}, {"title": "2.2 Conflict", "content": "In the vast majority of real-world strategic interactions, agents' objectives are neither identical nor\ncompletely opposed. Indeed, if AI agents are sufficiently aligned to their users or deployers, we should\nexpect some degree of both cooperation and competition, mirroring human society. These mixed-motive\nsettings include the possibility of mutual gains, but also the risk of conflict due to selfish incentives. In\nwhat follows, we examine the extent to which advanced AI might precipitate or exacerbate such risks."}, {"title": "2.2.1 Definition", "content": "In this work, we use the word conflict in a relatively broad sense to refer to any outcome in a mixed-\nmotive setting that does not lie on the Pareto frontier. This includes classic examples of conflict such as\nlegal disputes and warfare, but also encompasses cooperation failures in collective action problems, such\nas the depletion of a common natural resource or a race to the bottom on legislation (Dawes & Messick,\n2000; Snyder, 1971).\nIt is worth noting first that AI systems could help to solve conflicts, for example, by searching over\na larger space of potential solutions to disagreements, monitoring agreements, or acting as mediators\n(Bakker et al., 2022; Dafoe et al., 2020; McKee et al., 2023; Small et al., 2023). At the same time, the\nselfish incentives that drive said conflict may also incentivise actors to adopt AI systems in order to gain\nan advantage over their competitors. In such cases, delegation to increasingly advanced AI agents is far\nfrom guaranteed to lead to more cooperative outcomes, and could in some circumstances increase both\nthe speed and the scale at which conflict might emerge. Indeed, even if advanced AI systems are able to\novercome human cooperation problems, they may introduce even more complex cooperation problems\n(compare to how adults may be able to prevent children from fighting, but aren't immune from conflict\nthemselves)."}, {"title": "2.2.2 Instances", "content": "As we noted above, virtually all real-world strategic interactions of interest are mixed-motive, and as\nsuch the potential for conflict (even if in low-stakes scenarios) abounds. The introduction of advanced\nAI agents could both worsen existing risks of conflict (such as increasing the degree of competition in\ncommon-resource problems, or escalating military tensions) as well as well as introducing new forms of\nconflict (such as via sophisticated methods of coercion and extortion).\nSocial Dilemmas. As noted in our definition, conflict can arise in any situation in which selfish\nincentives diverge from the collective good, known as a social dilemma (Dawes & Messick, 2000; Hardin,\n1968; Kollock, 1998; Ostrom, 1990). While this is by no means a modern problem, advances in AI could\nfurther enable actors to pursue their selfish incentives by overcoming the technical, legal, or social barriers\nthat standardly help to prevent this. To take a plausible, near-term (if very low-stakes) example, an\nautomated AI assistant could easily reserve a table at every restaurant in town in minutes, enabling the\nuser to decide later and cancel all other reservations. Alternatively, the ability of AI assistants to search\nand switch between different consumer products and services could lead to 'hyper-switching' (Van Loo,"}, {"title": "2.2.3 Directions", "content": "The majority of work in multi-agent systems (and especially in multi-agent learning) has, until recently,\ntended to focus on either pure cooperation (e.g., Boutilier, 1996; Omidshafiei et al., 2017; Oroojlooy\n& Hajinezhad, 2022; Peshkin et al., 2000; Rashid et al., 2018; Stone et al., 2010) or pure competition\n(e.g., Bakhtin et al., 2022; Brown & Sandholm, 2019; Daskalakis et al., 2011, 2020; Silver et al., 2016;\nZhang et al., 2020). As there are not yet large numbers of mixed-motive interactions involving AI\nsystems, part of the challenge is to identify interventions that encourage cooperation in such settings\nwhile making realistic assumptions about the computational and strategic nature of agents in the real\nworld. For example, an intervention that relies on ensuring all agents use the same learning algorithm\nor on modifying the objectives of the agents will be unlikely to help if the agents act freely and are\ndeveloped independently by private, self-interested actors.\nLearning Peer and Pool Incentivisation. One major direction for avoiding conflict is building the\ncapabilities and infrastructure required for AI agents to (learn to) incentivise each other towards more\ncooperative outcomes. Such approaches can broadly be classified as 'top down' (where there is a system\ndesigner seeking to encourage cooperation among a population) or 'bottom up' (where agents attempt\nto incentivise each other directly). In adaptive mechanism design (Baumann et al., 2020; Gerstgrasser\n& Parkes, 2023; Pardoe et al., 2006; Yang et al., 2022; Zhang & Parkes, 2008; Zheng et al., 2022)\nor peer incentivisation methods (Lupu & Precup, 2020; Wang et al., 2021b; Yang et al., 2020), the\nsystem designer or agent typically learns to incentivise other agents by a direct utility transfer. Related\napproaches focus on the establishment of norms (K\u00f6ster et al., 2020; Oldenburg & Zhi-Xuan, 2024;\nVinitsky et al., 2023) to either encourage or sanction certain behaviour, also often via utility transfers."}, {"title": "2.3 Collusion", "content": "While some of the most important risks from advanced AI are due to cooperation failure, there are some\nsettings where cooperation between AI systems is undesirable. We refer to the problem of unwanted\ncooperation between AI systems as AI collusion."}, {"title": "2.3.1 Definition", "content": "Collusion has long been a topic of intense study in economics, law, and politics, among other disciplines.\nWhile there is no universal definition of collusion, it generally refers to secretive cooperation between\ntwo or more parties at the expense of one or more other parties. Most classic examples of collusion\nsuch as firms working together to set supra-competitive prices at the expense of consumers also tend\nto be not only secretive but in violation of some law, rule, or ethical standard. Distinctions are also\ncommonly made between explicit and tacit collusion (Rees, 1993), depending on whether the colluding\nparties communicate with each other.\nAI collusion could differ from classic definitions of collusion in a number of ways. First, for more basic AI\nsystems (such as algorithmic trading agents) it may be hard to ascribe any notion of intent to collude.\nRelatedly, there may be forms of AI collusion that are not currently ruled unlawful, because existing\nlegislation may not (yet) apply to the case of AI collusion (Beneke & Mackenrodt, 2019; Harrington,\n2019). Second, the distinction between explicit and tacit collusion may break down when it comes to\nagents whose communication can take very different forms to our own. Third, typical definitions of\ncollusion focus on mixed-motive settings where, while selfish agents are incentivised to compete, they also\nstand to gain (at the expense of some third party) if they can overcome these competitive pressures. AI"}, {"title": "2.3.2 Instances", "content": "The possibility of collusion between advanced AI systems raises several important concerns (Drexler,\n2022). First, collusion between AI systems could lead to qualitatively new capabilities or goals (see\nSection 3.6), exacerbating risks such as the manipulation or deception of humans by AI (Evans et al.,\n2021; Park et al., 2023b) or the ability to bypass security checks and other safeguards (Jones et al.,\n2024; OpenAI, 2023a). Second, many of the promising approaches to building safe AI rely on a lack of\ncooperation, such as adversarial training (Huang et al., 2011; Perez et al., 2022a; Ziegler et al., 2022) or\nscalable oversight (Christiano et al., 2018, 2021; Greenblatt et al., 2023; Irving et al., 2018; Leike et al.,\n2018). If advanced AI systems can learn to collude without our knowledge, these approaches may be\ninsufficient to ensure their safety (Goel et al., 2025, see also Section 4.1).\nMarkets. The quintessential case of collusion in mixed-motive settings is markets, in which efficiency\nresults from competition, not cooperation. While this is not a new problem, collusion between AI\nsystems is especially concerning since they may operate inscrutably due to the speed, scale, complexity,\nor subtlety of their actions. Warnings of this possibility have come from technologists, economists, and\nlegal scholars (Beneke & Mackenrodt, 2019; Brown & MacKay, 2023; Ezrachi & Stucke, 2017; Harrington,\n2019; Mehra, 2016). Importantly, AI systems can collude even when collusion is not intended by their\ndevelopers, since they might learn that colluding is a profitable strategy. Currently, most pricing and\ntrading algorithms are relatively unsophisticated compared to today's state-of-the-art AI systems, though\nthere is already a growing body of both theoretical (Brown & MacKay, 2023) and empirical (Abada &\nLambin, 2023; Assad et al., 2020; Calvano et al., 2020; Klein, 2021; Wieting & Sapi, 2021) evidence that\nsuch algorithms may sometimes learn to collude. Unfortunately, due to the huge\nfinancial incentives to deploy more advanced, adaptive AI systems in such settings, this risk is likely\nto increase despite the potential for catastrophic outcomes (see Case Study 10). As well as economic\nharm to consumers, firms in less well-resourced regions may be forced out (widening geographic economic\ninequality) and increasingly oligopolistic markets may emerge.\nSteganography. In the near future we will likely see LLMs communicating with each other to jointly\naccomplish tasks. To try to prevent collusion, we could monitor and constrain their communication\n(e.g., to be in natural language). However, models might secretly learn to communicate by concealing\nmessages within other, non-secret text. Recent work on steganography using ML has demonstrated that\nthis concern is well-founded (Hu et al., 2018; Mathew et al., 2024; Roger & Greenblatt, 2023; Schroeder\nde Witt et al., 2023b; Yang et al., 2019, see also Case Study 5). Secret communication could also occur\nvia text compression (OpenAI, 2023c), or via the emergence of communication between agents where the\nsymbols used by agents lack any predefined meanings or usage guidelines or are otherwise uninterpretable\nto humans (Foerster et al., 2016; Lazaridou & Baroni, 2020; Sukhbaatar et al., 2016)."}, {"title": "2.3.3 Directions", "content": "There is a growing interdisciplinary body of work studying AI collusion. Thus far, however, the vast\nmajority of this work is either speculative or shows that collusive behaviour between learning agents\ncan emerge in simple settings. More work is therefore required in order to detect AI collusion in more\ncomplex settings, and to develop robust methods for mitigating AI collusion.\nDetecting AI Collusion. Brown & MacKay (2023) show that collusive behaviour can emerge under\nequilibrium in theoretical models, while Calvano et al. (2020) and Klein (2021) show that in simple\nsettings, independent RL algorithms can learn to collude. Leisten (2021), Normann & Sternberg (2023),\nand Werner (2021) consider collusion in synthetic settings involving both machines and humans. There\nhave also been documented instances of algorithmic collusion not only in the German gasoline market\n(Assad et al., 2020), but also in \u2018buy box' placement on the largest online marketplace in Belgium and the\nNetherlands (Wieting & Sapi, 2021). Only a small number of works, however, have considered collusion\nin the context of more advanced AI agents, such as those powered by LLMs (Fish et al., 2024; Mathew\net al., 2024; Motwani et al., 2024; OpenAI, 2023c), and this remains a highly under-explored area.\nAlongside context-specific evaluations, more effort ought to be devoted to developing general methods\nfor detecting collusion. Some progress in this direction was recently made by Bonjour et al. (2022), who\ndefine an information-theoretic measure of collusion applicable to arbitrary models. Future work should\ninvestigate additional general methods to detect collusion, including by leveraging interpretability tools\nto understand systems' goals (Colognese & Jose, 2023; Marks et al."}]}