{"title": "Reusing Embeddings: Reproducible Reward Model Research in Large Language Model Alignment without GPUs", "authors": ["Hao Sun", "Yunyi Shen", "Jean-Fran\u00e7ois Ton", "Mihaela van der Schaar"], "abstract": "Large Language Models (LLMs) have made substantial strides in structured tasks through Reinforcement Learning (RL), demonstrating proficiency in mathematical reasoning and code generation. However, applying RL in broader domains like chat bots and content generation \u2013 through the process known as Reinforcement Learning from Human Feedback (RLHF) \u2013 presents unique challenges. Reward models in RLHF are critical, acting as proxies that evaluate the alignment of LLM outputs with human intent. Despite advancements, the development of reward models is hindered by challenges such as computational heavy training, costly evaluation, and therefore poor reproducibility. We advocate for using embedding-based input in reward model research as an accelerated solution to those challenges. By leveraging embeddings for reward modeling, we can enhance reproducibility, reduce computational demands on hardware, improve training stability, and significantly reduce training and evaluation costs, hence facilitating fair and efficient comparisons in this active research area. We then show a case study of reproducing existing reward model ensemble research using embedding-based reward models. We discussed future avenues for research, aiming to contribute to safer and more effective LLM deployments.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have achieved great success on structured tasks like mathematical reasoning and code generation (Guo et al., 2025; Jaech et al., 2024; Trinh et al., 2024) using the technique of Reinforcement Learning (RL). In such a process, rule-based reward functions can be explicitly defined to guide optimization.\nUniversity of Cambridge Massachusetts Institute of Technology ByteDance Research. Correspondence to: Hao Sun <hs789@cam.ac.uk>.\nPreprint. Code is available at GitHub."}, {"title": "2. Reward Models with Embedding Inputs", "content": "In this paper, we argue that building reward models using embedded input can greatly accelerate research in this field. Specifically, it enhances reproducibility by reducing hardware and computational resource requirements, cutting the cost of training and evaluation, improving training stability, and minimizing the cost of reproduction, hence accelerating the pace of reward model research. Additionally, it opens new avenues for further exploration such as research using the statistical lenses.\nThis position paper is structured as follows: In Sec. 2, we present and compare embedding-based reward models with conventional LLM-based reward models, where general-purpose LLMs with value heads are optimized to serve as value predictors. In Sec. 3, we elaborate on the motivations of training reward models using embeddings as inputs, and demonstrating its advantages in practice - high reproducibility with low cost associated with training (Sec. 3.1), evaluation (Sec. 3.2), and inference (Sec. 3.3). In Sec. 4, we demonstrate our positions through an efficient reproduction of existing reward modeling research. Sec. 5 explores open questions and future research opportunities in this domain. Lastly, Sec. 6 provides alternative perspectives to enhance the comprehensiveness of this position paper."}, {"title": "2.1. Alternatives to LLM-based Reward Models", "content": "In this section, we explore the use of embeddings as inputs for reward modeling and contrast this approach with traditional methods employing natural language inputs. Figure 1 illustrates the key differences: green boxes represent trainable parameter groups, while gray boxes denote intermediate variables. The left panel depicts the processing of natural language inputs by LLMs for generation tasks, whereas the right panel shows their use in LLM-based reward models for quality evaluation.\nWhen LLMs equipped with replaced value heads are utilized for reward modeling, only a minimal number of parameters are removed. Consequently, these models retain a substantial degree of parameter freedom, making them large and computationally demanding. For example, training a 2B-parameter model using LoRA (Hu et al., 2021) on a Tesla-V100 GPU with a typical alignment dataset of 10,000 samples approximately requires two hours. Additionally, the training process involves numerous hyperparameters that can significantly influence the outcomes.\nConversely, given the aim to evaluate natural language content effectively, and recognizing that the embedding space encapsulates a rich representation of the input\u2014both before and during the LLM era\u2014as evidenced in tasks ranging from classification to more complex applications (Mikolov, 2013; Pennington et al., 2014; Devlin, 2018; Kiros et al.,"}, {"title": "2.2. Empirical Comparisons", "content": "Reward Model Sizes In the extant literature on reward models, LLMs typically range from 3M to 3B parameters, with specific instances such as Coste et al. (2023) employing"}, {"title": "Data Generation Processes", "content": "We use the Anthropic-HH dataset, which includes the Helpful and Harmless alignment tasks to assess the efficacy of various reward model approaches (Bai et al., 2022). The dataset contains 40000 prompts for each task. To ensure reproducible and reliable comparisons, we use golden reward models as proxy annotators following established workflows in the literature (Xiong et al., 2023; Dong et al., 2024; 2023; Gao et al., 2023; Yang et al., 2024b). We consider three LLMs Gemma-2B and -7B (Team et al., 2024), and LLaMA3-8B (Touvron et al., 2023) to generate responses. For each prompt, we generate 10 responses and randomly select N pairs for preference annotation using the golden reward models. We use Gemma2B to generate embeddings for the embedding-based reward models. This approach ensures that our evaluation accurately reflects the preferences of the golden reward model, thereby minimizing bias."}, {"title": "Annotation Quality and Quantity Control", "content": "In preference generation, the quality of annotations is often limited by the capabilities of the annotators (Sanderson et al., 2010; Stewart et al., 2005; Guest et al., 2016; Wang et al., 2024a). We apply the location-scale function class to describe annotation noise (Sun et al., 2024b), positing that closer values"}, {"title": "3. Motivations of Using Embeddings as Reward Model Inputs", "content": ""}, {"title": "3.1. Reproducibility: Foundation of Research", "content": "Reproducibility is the foundation of scientific research. In the study of reward modeling, the ability to replicate results across different studies is essential for evaluating theoretical and practical contributions. Nonetheless, the reproduction of LLM-based reward model research often faces considerable obstacles, such as vulnerability to many sensitive hyperparameters, the necessity for large memory GPUs, large training instability, and extensive computational demands associated with slow training processes. These challenges can make the replication of existing works extremely challenging \u2013 if not unfeasible \u2013 for many of the research communities.\nAs a consequence, in new research, if a method lacks systematic comparisons with established methods due to the above challenges, or its efficacy can not be verified through repeated and statistically significant trials, the results may be unfounded.\nThe utilization of embedding-based reward models offers several advantages:"}, {"title": "Lower Computational Requirements for Statistical Significance", "content": "In embedding-based reward model research, the computational overhead is lower not only because of the much cheaper model training process but also for the more consistent results across multiple runs. And there are much less vulnerable hyperparameters that may drastically affect the results. This efficiency enables researchers to rapidly prototype, validate ideas, and innovate based on reliable conclusive empirical observations, maximally isolating the source of gains from complex LLM-based reward modeling systems, thereby accelerating the cycle of scientific discovery and validation in the field."}, {"title": "Data Standardization and Scalability", "content": "In embedding-based reward model research, it is possible to create and share a standardized, publicly accessible dataset that includes multiple language models' generations (generality among models), contains a large number of samples (sufficient data for training), flexibly simulate annotation strategies (to stress test methods), and cost-efficient evaluation process."}, {"title": "3.2. Scalable Evaluation with Embedding-based Reward Models", "content": "In addition to the high computational costs of training, LLM-based reward modeling faces significant challenges in evaluation time and expense. Specifically, reward models are tasked with evaluating test-time generations to differentiate superior responses from inferior ones. Previous research has primarily utilized two metrics for this purpose: LLM-as-a-Judge and evaluation using open-sourced golden reward models (Dong et al., 2023; 2024).\nThe LLM-as-a-Judge evaluation, which often involves calling commercial APIs, can be prohibitively expensive for even medium-sized datasets. For example, in a study involving 3 different language models and 2 datasets, evaluating a proposed method using 2000 test samples each comparison truncated to 1024 tokens through the GPT-3.5 API incurs a cost of 20 US dollars per experiment, and this cost will be amplified by the number of individual run of the experiment.\nWhile the Golden Reward Model Evaluation avoids the use of commercial APIs, making it more accessible and economical for researchers, it still imposes substantial computational demands. For example, evaluating the aforementioned test case necessitates the LLM-based RM to process 12000 pairs of sequences. In the more computationally intensive best-of-N evaluations, a typical study with N = 500 (KL divergence approximately 5 Nats, Gao et al., 2023) requires 6 million forward passes.\nCheap and Fast Evaluation with Embedding-based Reward Models. In contrast, embedding-based reward models leverage fixed embeddings, allowing for the preparation of a standardized test dataset that is reusable across various methods. For instance, in the scenario described above, we only need to generate the embeddings and golden rewards for the 500 test responses on each prompt once. These embeddings and rewards are then reusable for any embedding-based reward model evaluation.\nWe have implemented such a preprocessing step, resulting in a dataset asset that includes 500 responses for each test prompt. To provide a clearer understanding of the dataset prepared for embedding-based reward modeling, we provide"}, {"title": "3.3. Scalable Inference-Time Optimization", "content": "In this section, we elucidate an additional advantage of embedding-based reward models enhancing the inference-time optimization efficiency. With embedding-based reward models, language generation and evaluation require only a single LLM forward pass. Although this may appear to reduce computation time by less than half, it significantly lowers the computational burden in evaluation by shifting from hosting an LLM (reward model) to a much simpler and smaller model. This is particularly beneficial for API-based service providers who previously could not perform inference-time optimization due to the high computational demands of running LLM-based reward models"}, {"title": "4. Case Study: Efficient Reproduction of Reward Model Ensemble Papers", "content": "In this section, we replicate the findings from prior research on mitigating overoptimization in reward models through ensemble methods, as discussed in (Coste et al., 2023; Ahmed et al., 2024), using our proposed embedding-based reward modeling framework.\nTo validate the principal finding in those works that ensembles can alleviate reward model overoptimization, we train 10 LightGBM models (Ke et al., 2017) using default hyperparameter settings, alongside an MLP-based implementation with 256 hidden units. We assess the performance of these ensemble reward models by averaging predictions across the 10 models. Experiments are repeated with 5 independent runs to draw statistically significant conclusions.\nIn total, we train and evaluate 12000 models. Using the CPU server, training and evaluating the 6000 LightGBM models takes 4.9 hours, while the 6000 MLP models require 17.3 hours. In total, these 12000 experimental configurations are completed within 1 single CPU day.\nFinally, unlike prior research, our investigation into reward model ensembles using embeddings as inputs clarifies that the observed enhancements stem from conservative modeling approaches, rather than from the scaling laws typical of LLM-based reward models (Gao et al., 2023). These distinctions are visually demonstrated in the case study illustrated"}, {"title": "5. Call for Contributions", "content": ""}, {"title": "5.1. Contributing to Public Embedding Assets", "content": "In this position paper, we have demonstrated the advantages of embedding-based reward models. We successfully reproduced the findings of a reward model ensemble study with 12,000 experiment runs in just one day using only CPU resources, highlighting the efficiency of our approach. However, it's important to note that this workflow is feasible only when embeddings from LLM generations are available"}, {"title": "5.2. Representation Learning: Searching for General Purpose Reward Embeddings", "content": "Current language model embeddings are primarily designed and optimized for text generation. While they can be repurposed as inputs for reward models, as demonstrated"}, {"title": "5.3. Flash Back of Classic Statistics", "content": "Back in the early days of statistical natural language processing, circa the 1990s to early 2000s (for even earlier history, we refer to Jones, 1994), researchers had quite limited options for features even for simple classification tasks. Simple models (e.g., classification trees) were often accompanied by handcrafted, ad hoc features like bags of words, n-grams, and tf-idf (Chowdhury, 2010), which are seen as insufficient today. With neural networks, representation learning and model development occurred simultaneously; one can even argue that the success of deep models lies in the success of representation learning (Bengio et al., 2013).\nIn future works, can we get the best of both worlds by combining powerful embeddings from an LLM, together with a solid understanding of classic methods to better advance reward modeling with a gray box approach? Can we develop theories building upon the knowledge of classic methods? for instance, under the linear assumption with embeddings, what theoretical properties can we establish, and how can we conduct active learning? There are vast research opportunities lying at the interface between statistics and embedding-based reward modeling."}, {"title": "6. Alternative Views", "content": ""}, {"title": "Success of End to End Training", "content": "The remarkable success of deep learning is largely attributed to the end-to-end learning capability of deep neural networks (LeCun et al., 2015; Goodfellow et al., 2016), which has proven effective across diverse domains, including image processing (Krizhevsky et al., 2012; He et al., 2016), natural language processing (Vaswani, 2017; Devlin, 2018), tabular data analysis (Arik & Pfister, 2021), and time series data (Van Den Oord et al., 2016; Ismail Fawaz et al., 2019; Ding et al., 2020).\nComputational Costs are Decreasing Over Time As computational costs continue to decrease, future research on reward models may efficiently leverage LLMs or even more powerful foundation models. This could eliminate the need for embedding-based reward modeling approaches, further supporting the case for end-to-end learning."}, {"title": "A. More Results", "content": "Performance Comparison: Embedding-based reward models v.s. LLM-based reward models. In our main text, we presented the results with the Gemma 2B model when comparing the performance of different reward modeling approaches. We now provide the results using the Gemma 7B and LLaMA3 8B models as complementary empirical supports. The observations concluded in our main test still hold true on those experiment setups."}]}