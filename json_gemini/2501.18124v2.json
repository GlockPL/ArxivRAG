{"title": "REMOTE: Real-time Ego-motion Tracking for Various Endoscopes via Multimodal Visual Feature Learning", "authors": ["Liangjing Shao", "Benshuang Chen", "Shuting Zhao", "Xinrong Chen"], "abstract": "Real-time ego-motion tracking for endoscope is a significant task for efficient navigation and robotic automation of endoscopy. In this paper, a novel framework is proposed to perform real-time ego-motion tracking for endoscope. Firstly, a multi-modal visual feature learning network is proposed to perform relative pose prediction, in which the motion feature from the optical flow, the scene features and the joint feature from two adjacent observations are all extracted for prediction. Due to more correlation information in the channel dimension of the concatenated image, a novel feature extractor is designed based on an attention mechanism to integrate multi-dimensional information from the concatenation of two continuous frames. To extract more complete feature representation from the fused features, a novel pose decoder is proposed to predict the pose transformation from the concatenated feature map at the end of the framework. At last, the absolute pose of endoscope is calculated based on relative poses. The experiment is conducted on three datasets of various endoscopic scenes and the results demonstrate that the proposed method outperforms state-of-the-art methods. Besides, the inference speed of the proposed method is over 30 frames per second, which meets the real-time requirement. The project page is here: remote-bmxs.netlify.app", "sections": [{"title": "I. INTRODUCTION", "content": "Endoscopy has become the primary medical method for the surgery and the examination of the body cavities. Due to the limited visual field and the narrow pathway in the body cavities, the navigation is critical for the safety, accuracy and efficiency of endoscopic treatment, especially automatic robot-assisted endoscopy [1].\nThe common methods for navigation of endoscopy include optical tracking system [2] or magnetic tracking system [3]. However, the optical tracking system are easily influenced by the occlusion by human, while the magnetic localization could be interfered by the electromagnetic field from medical instruments. Meanwhile, such tracking systems are expen-sive and complex to implement. Therefore, the vision-based tracking method has become significant for the navigation of endoscopy.\nThe vision-based tracking methods can be categorized into two terms, including pose estimation and ego-motion estimation. The target of pose estimation is to estimate the pose of the object from the image of the scene comprising the object, which is impossible for the enclosed narrow body cavities. Differently, the task of ego-motion estimation is to predict the pose of the camera from the observation captured by itself. Therefore, real-time ego-motion tracking is the main method of vision-based tracking for endoscopes.\nThe traditional methods for ego-motion estimation are based on feature matching and iterative optimization [4], [5], which is time-consuming and not suitable for real-time tracking. Therefore, learning-based methods are proposed to estimate relative ego-motion between two continuous images and then more efficient ego-motion tracking is performed. The primary pipeline of the relative pose estimation con-sists of feature extraction and pose decoder [6]. However, currently, most of the existing methods for ego-motion es-timation just extract feature representation from two images separately [7], [8] or from the concatenation of two images [9],[10]. Moreover, the structures of the existing pose de-coders are mainly based on a fully-connected layer [11] or a simple network consisting of four convolution layers [10], which are not enough for extracting more complete feature representation from the feature maps. In some researches, recurrent neural networks (RNNs) are applied to extract temporal feature from the sequence of endoscopic images [12],[13],[8],[14]. However, the training of RNNs needs a large quantity of the whole endoscopic videos, which is expensive for establishment of the dataset.\nTo this end, a novel deep learning-based framework is proposed to perform real-time ego-motion tracking for endo-scope. In the proposed network, multi-modal visual features are extracted for relative pose estimation, as Fig. 1 shows."}, {"title": "II. RELATED WORK", "content": "Most of the existing methods for ego-motion tracking are based on the structure of PoseNet[17], which consists of a feature extractor and a pose decoder. Some works utilize different feature extractors and pose decoders in the similar structure. Based on PoseNet, Nasser et al. [18] utilize VGG instead of GoogLeNet and add two fully-connected layers into the pose decoder to extract more complete feature ex-traction. Due to the excellent performance of ResNet, Wang et al. [19] and Bui et al. [20] respectively use ResNet-18 and ResNet-34 as feature extractors in their frameworks. The similar structure is also used in the most of previous methods for ego-motion estimation of endoscope [7],[15],[9],[8],[10]. Differently, multi-modal visual features are extracted in the proposed work. Moreover, a novel feature extractor for multi-dimensional feature integration is proposed and a novel pose decoder to extract more complete feature representation is designed.\nRecently, attention mechanisms are widely applied in the ego-motion estimation of endoscope. For instance, Zhou et al. [21] apply the attention mechanism into the pose decoder to extract geometrically robust feature from the feature map. To extract more contextual feature, Shavit et al. [22] directly replace CNN networks with a Transformer-based model as the feature extractor. Based on the ego-motion estimation network of Monodepth2 [23], Ozyoruk et al. [9] apply a spatial attention block into the feature extractor to perform ego-motion estimation for endoscope in the gastrointestinal tract. Similarly, Liu et al. [10] propose a dual-attention module in the feature encoder, in which spatial attention and channel attention are performed to extract contextual feature in both of the spatial dimension and the channel dimension. Yang et al. [11] combine CNN layers of ResNet with multi-head attention layers to perform feature extraction based on laparoscopic images. In the proposed joint feature extractor, a novel attention-based module is designed to extract and integrate both of local and global feature from multiple dimensions.\nMoreover, in the most of the previous researches, the ego-motion estimation is performed on one certain dataset, such as the simulated dataset of the bronchoscopy [7],[8], the dataset of the laparoscopy [10],[15],[9],[11]. The proposed framework is applied on three different datasets including an in-vivo dataset of the nasal endoscopy collected by human experts, a simulated dataset of colonoscopy [15] and an ex-vivo dataset of intestine endoscopy collected by a robot arm [9]."}, {"title": "III. PROPOSED METHOD", "content": "A. Overview\nThe real-time ego-motion tracking of the proposed method can be divided into two stage, relative pose estimation and absolute pose calculation, which is shown as Fig. 2. At the first stage of relative pose estimation, given the current ob-servation $I_i$ and the adjacent previous frame $I_{i-k}$, the relative pose transformation $p_{-k}$ between two frames can be pre-dicted by the proposed deep learning-based network. Based on a series of relative pose transformations ${p_{-1k}}_{i=1,2,...,N}$ and the known initial pose $P_0$, the absolute pose $P_{Nk}$ of the endoscope in the current frame can be calculated.\nB. Relative Pose Estimation\n1) Optical Flow Prediction: To achieve real-time track-ing, the previous efficient network, FastFlowNet [24], is uti-lized to predict the optical flow between two adjacent obser-vations. However, the pretrained FastFlowNets are trained on the natural dataset, which may be influenced by the domain gap between the in-the-wild scene and the endoscopic scene. Therefore, the FastFlowNet in our framework is finetuned based on more accurate results calculated by a traditional but slow method. Given two adjacent frames $I_i$ and $I_{i-k}$, the optical flow $O_k$ as the ground truth is calculated by Dual TVL1 algorithm. Following [24], the multi-scale robust loss between $O_k$ and the optical flow $\\hat{O}_k$ predicted by the pretrained network is utilized for finetuning, formulated as Eq. 1.\n$L = \\theta_1\\sum_{l=2}^{6}(\\rho(O_k(x; l) - \\hat{O}_k(x; l)|) + \\epsilon )^{q}$ (1)\nwhere $l$ denotes the optical flow with different scales ($O_k(:,l) \\in \\mathbb{R}^{H/2^{l-1}\\times W/2^{l-1}\\times 2}$), $x$ denotes each pixel in the optical flow, $\\epsilon = 0.01$ is a small constant of noise, $q < 1$ is a penalty parameter which makes the model more robust for large magnitude outliers, and $\\theta_l$ are set following [24].\n2) Feature Extraction: Firstly, the feature of the scenes $f_i$ and $f_{i-k}$ in two adjacent observations are extracted from single images respectively. Meanwhile, the feature of the endoscope motion $f_m$ is extracted from the optical flow of two adjacent observations by the same feature extractor. The network based on [25] is utilized as the feature extractor, which is pretrained on the Imagenet-1k [26].\nTo extract the contextual and corresponding feature of two adjacent observations, the joint feature $f_j$ is extracted from the concatenation of two frames. Due to more information in the channel dimension of the concatenated image, a unique feature extractor is proposed for joint feature extraction. Specifically, based on the first two layers of ResNet-34, a attention-based module is inserted after each layer. Given the feature map $F_0$, the feature map $F$ is provided by integrating multi-dimensional information. Firstly, the $F_0$ is permuted into three feature maps with different dimensions, $F_0\\in \\mathbb{R}^{H\\times W\\times C}$, $F_1\\in \\mathbb{R}^{C\\times H\\times W}$ and $F_2 \\in \\mathbb{R}^{W\\times C\\times H}$. The attention weight of each feature map is calculated by Eq. 2. To consider both of the critical information and global information, adaptive integration of max pooling $F_0^{maxpooling}$ and average pooling $F_0^{avgpooling}$ is utilized to calculate attention weights, followed by a 1\u00d73 convolution layer $F_{1\\times 3}^{conv}$ and a sigmoid process. In the end, multiplications of the feature maps and corresponding attention weights are summed with equal weights as Eq. 3.\n$A^{'} = sigmoid(F_{1\\times 3}^{conv}(\\alpha F_0^{maxpooling} + \\beta F_0^{avgpooling}))$ (2)\n$F = \\sum_{i=1}^{3} A_i^{'}F_i$ (3)\nwhile $\\alpha$ and $\\beta$ are two learnable weights, which are respec-tively set to a random value in the range of [0,1). At the end of the feature extraction, all normalized feature maps are concatenated and passed through the proposed pose decoder as Eq. 4 shows.\n$f_{i-k} = f_i \\oplus f_{i-k} \\oplus f_m \\oplus f_j$ (4)\n3) Pose Decoder: Based on the concatenated feature map $f_{i-k}$, the relative pose transformation vector $\\hat{p}_{-k}$ is predicted by a pose decoder, which can extract more complete repre-sentation from the feature map. At first, the feature map is squeezed by a 1\u00d71 convolution layer followed by a ReLU activation. After this, the feature map is downsampled by layer normalization and a 2\u00d72 convolution layer. For more complete extraction of feature representation, two blocks based on the depthwise separable convolution are used in our pose decoder. In each block, depthwise convolution is firstly performed by a 7\u00d77 convolution layer, in which the input feature map is divided into 3 groups for computation. Pointwise convolution is performed by two 1 \u00d7 1 convolution layer to integrate the information in the feature maps from from different groups of the input. At the end of each block, the feature map is scaled by a learnable parameter $\\gamma$ which is initialized as $10^{-6}$, followed by the residual connection with the input feature map.\nC. Loss Function\nThe predicted relative pose vector can be represented as $\\hat{p}_{i-k} = [\\hat{t}_{i-k},\\hat{q}_{i-k}]$, where $\\hat{t}_{i-k} \\in \\mathbb{R}^3$ is the vector of translation and $\\hat{q}_{i-k} \\in \\mathbb{R}^4$ is the quaternion of the endoscope. The ground truths of endoscope pose in two adjacent frames are $P_{i-k}$ and $P_i$. The relative pose transformation $P = P_iP_{i-k}^{-1}$ can be calculated, which can be transformed into $p_k = [t_{i-k},q_{i-k}]$ based on the relation between the homogeneous matrix and the quaternion.\nRefer to [28], geometric loss function is utilized as our loss function. To calculate the loss function, the quaternion $q= [q^0,q^x,q^y,q^z]$ is represented as $q = [q^0,\\upsilon]$. Based on this, the logarithm of the quaternion $log q$ is defined as:\n$log q =\\begin{cases} \\frac{\\upsilon cos^{-1}q^0}{\\|\\upsilon\\|}, & \\|\\upsilon\\| \\neq 0 \\\\\\ 0, & \\|\\upsilon\\| = 0 \\end{cases}$ (5)\nGiven the predicted relative pose vector $\\hat{p}_{i-k}$ and the ground truth $p_{i-k}$ with $\\lambda_1$ and $\\lambda_2$ as two learnable parameters, the loss function $L(P_{i,i+k}, P_{i,i+k})$ is described as below:\n$L(P_{\\hat{i-k}}, P_{i-k}) = \\|\\hat{t}_{i-k}-t_{i-k}\\|_{1}e^{-\\lambda_1} + \\lambda_1 + \\|log \\hat{q}_{i-k}-log q_{i-k}\\|_{1}e^{-\\lambda_2} + \\lambda_2$ (6)\nwhere $\\|\\cdot\\|_1$ represents the $L_1$ norm, $\\lambda_1$ and $\\lambda_2$ is initially set to 0 and -3, respectively.\nD. Absolute Pose Calculation\nThe predicted relative pose transformation $\\hat{p}_{i-k}$ can be transformed into the corresponding homogeneous matrix $\\hat{P}_{i-k}$. Given the know initial pose of the endoscope $P_0 = \\hat{P}_0$, the absolute pose of the endoscope can be calculated by the recursive formula Eq. 7 based on a series of relative pose transformation.\n$P_{i} = \\hat{P}_{i-k}\\hat{P}_k, i = k, 2k, ...$ (7)"}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "A. Datasets\n1) NEPose: NEPose is a dataset collected by our own. the complete process of nasal endoscopy is recorded by an experienced surgeon using a XION binocular 4K endoscope with an optical tracking system. The dataset consists of 50 endoscopic videos of 16 subjects consisting of over 30k frames. In the experiment, 4328 frames from 30 different subjects are randomly chosen as the training set and the validation set.\n2) SimCol: SimCol [15] is a virtual dataset generated based on a computer tomography scan of a real human colon in a Unity simulation environment. The dataset consists of 18k frames from 15 different subjects. In the experiment, 7200 frames from 24 different trajectories are randomly selected as the training set and the validation set.\n3) EndoSLAM: EndoSLAM [9] is a large scale dataset for endoscopic perception. In our work, to perform ego-motion tracking, the ex-vivo part of EndoSLAM are utilized in the experiments. The chosen part of dataset consists of about 40k frames collected by a robot arm from ex-vivo colon, intestine and stomach. In the experiment, 3482 frames from 18 different trajectories are chosen from the dataset as the training set.\nB. Experiment Implementation\nIn the experiment, the proposed framework is implemented on Ubuntu 22.04 using Pytorch 1.13.1 with CUDA 11.7. Images are sampled every 4 frames from the video sequence, which means k mentioned in III-A is set to 4. Our model is trained for 500 epochs with a batch size of 32 on one NVIDIA RTX 4090 GPU. Adam optimizer is used for optimization. The learning rate is set to 0.001 initially and decayed with a factor of 0.98 after each epoch.\nC. Metrics\nAbsolute Translation Error (ATE) [15] is used to evaluate the accuracy of absolute position tracking. Cosinus Error (CE) and Direction Error (DE) [8] are used to evaluate the accuracy of absolute direction tracking. Moreover, Relative Translation Error (RTE) and ROTation error(ROT) [15] are utilized to separately test the performance of relative trans-lation prediction and relative rotation prediction.\n$ATE = \\|Trans(P_{ik}) - Trans(\\hat{P}_{ik}) \\|$ (8)\n$CE = \\frac{1}{3}\\sum_{(r,f) \\in \\{(r_x,\\hat{r}_x),(r_y,\\hat{r}_y),(r_z,\\hat{r}_z)\\} (1 - cos(0 - \\hat{0}))$ (9)\n$DE = cos^{-1}[(\\begin{matrix}1 & 1 & 1\\end{matrix}) (Rot P_{ik})\\cdot(\\begin{matrix}1 & 1 & 1\\end{matrix}) (Rot \\hat{P}_{ik})]$ (10)\n$RTE = \\frac{\\|Trans(P^{-1}_{i(i-1)k}) - T(P^{-1}_{i(i-1)k}) \\||}{t_{i-1}}$ (11)\n$ROT = \\frac{Tr(Rot(P^{-1}_{i(i-1)k})P_{(i-1)k}) - 1}{2} \\frac{180}{\\pi}$ (12)\nwhere Trans(P) represents the translation vector T in the homogeneous matrix P, Rot(P) represents the rotation matrix R in the homogeneous matrix P, and Tr(\u00b7) represents the trace of the matrix."}, {"title": "D. Comparison with Previous Methods", "content": "The proposed method is compared with state-of-the-art methods including OffsetNet[7], PoseResNet[16], Attention PoseNet[9], the method from [27], the method from [15], Dual-attention PoseNet [10] and the most recent method from [11]. For comparison, 1308 frames from 12 different subjects in NEPose dataset, 1800 frames from 6 different tra-jectories in SimCol dataset and 1482 frames from 6 different trajectories in EndoSLAM dataset are randomly selected in the experiments. The same implementation described in IV-B is used in the experiments of all methods. The experimental results on three datasets are respectively shown as Table I. The results demonstrate that the proposed method can perform the most accurate relative pose estimation and ego-motion tracking for various endoscopic scenes, compared with previous works. Fig. 3 and Fig. 4 visualize the results of ego-motion tracking, which displays the best qualitative results are from the proposed method."}, {"title": "E. Ablation Studies", "content": "In the ablation studies, we set several experiments to prove the effect of each visual modal and the effect of novel modules, which includes feature extractors and the pose decoder. In the experiments, 1297 frames from NEPose, 1800 frames from SimCol and 1050 frames from EndoSLAM are randomly selected again for the comparison. The results in Table II demonstrate that each visual modal plays an important role in the ego-motion tracking by our framework. To evaluate the positive effects of the proposed modules, the feature extractor is compared with pretrained ResNet-18, which is usually used in the previous work. Meanwhile, the pose decoder is replaced with the advanced pose decoder and a fully-connect layer, which are usually applied in the previous framework. In the experiments, 1279 frames from NEPose, 1800 frames from SimCol and 1145 frames from EndoSLAM are randomly selected again for the comparison. The results in Table III prove the effects of the novel feature extractors and the proposed pose decoder."}, {"title": "F. Inference Speed Test", "content": "To evaluate the inference speed of the proposed frame-work, 18 samples are randomly selected from all three datasets for experiments. As Table IV shows, the average inference speed of our framework can be over 30 frames per second (fps), which meets real-time requirement for application."}, {"title": "V. CONCLUSION", "content": "In this paper, a deep learning-based framework, REMOTE, is proposed to perform real-time ego-motion tracking for endoscope in various scenes, in which different aspects of features from multi-modal visual inputs are learned. The experimental results show that the proposed method outper-forms the advanced methods on three different endoscopic datasets, providing the most accurate real-time ego-motion estimation. This work could contribute to the automation and navigation of robot-assisted endoscopy."}]}