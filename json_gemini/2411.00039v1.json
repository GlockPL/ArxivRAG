{"title": "LINEAR CHAIN TRANSFORMATION: EXPANDING OPTIMIZATION\n DYNAMICS FOR FINE-TUNING LARGE LANGUAGE MODELS", "authors": ["Yulong Wang", "Chang Zuo", "Yin Xuan", "Hong Li", "Ni Wei"], "abstract": "Fine-tuning large language models (LLMs) has become essential for adapting pretrained models to\n specific downstream tasks. In this paper, we propose Linear Chain Transformation (LinChain), a novel\n approach that introduces a sequence of linear transformations during fine-tuning to enrich optimization\n dynamics. By incorporating multiple linear transformations into the parameter update process,\n LinChain expands the effective rank of updates and enhances the model's ability to learn complex\n task-specific representations. We demonstrate that this method significantly improves the performance\n of LLM fine-tuning over state-of-the-art methods by providing more flexible optimization paths\n during training, while maintaining the inference efficiency of the resulting model. Our experiments\n on various benchmark tasks show that LinChain leads to better generalization, fewer learnable\n parameters, and improved task adaptation, making it a compelling strategy for LLM fine-tuning.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs), such as ChatGPT, Claude, and LLaMA, have achieved remarkable success across\n a wide variety of natural language processing (NLP) tasks, ranging from text generation to question answering\n and summarization. Their ability to learn rich representations from vast amounts of data has enabled significant\n advancements in language understanding and generation. However, as LLMs continue to grow in size, the computational\n cost associated with fine-tuning these models for specific tasks becomes increasingly prohibitive. The challenge lies\n in balancing the power of large models with the need for efficient adaptation, especially when deploying them in\n real-world scenarios where computational resources may be limited.\n\nAs LLMs continue to grow in scale, with billions of parameters, fine-tuning these models for specific tasks has become\n computationally expensive. To address this, several Parameter-Efficient Fine-Tuning (PEFT) methods have been\n introduced, focusing on updating only a small subset of parameters while maintaining task-specific performance. A\n prominent method in this area is Low-Rank Adaptation (LoRA) [1], which updates a frozen pre-trained model by\n introducing a low-rank decomposition to the parameter updates. LoRA decomposes the weight update AW into\n two smaller matrices A \u2208 Rdior and B\u2208 Rrxd2, reducing the number of trainable parameters from d\u2081 \u00d7 d2 to\n (d1 + d2)r [1]. This has been effective in reducing memory usage and training costs, while preserving task performance.\n\nHowever, LoRA's low-rank updates can limit the model's expressiveness, particularly for tasks requiring more complex\n feature interactions. This has led to several LoRA variants, each attempting to balance efficiency and flexibility. For"}, {"title": "2 Preliminaries and Motivation", "content": "LORA [1] is widely recognized for its effectiveness in fine-tuning LLMs while minimizing computational overhead. In\n LORA, the weight update matrix AW is decomposed into two low-rank matrices:\n$\\displaystyle \\Delta W = AB^T, A \\in \\mathbb{R}^{d_1 \\times r}, B \\in \\mathbb{R}^{r \\times d_2},$\nwhere A and B are learned during fine-tuning, significantly reducing the number of trainable parameters. This structure\nallows for efficient adaptation to specific tasks by focusing on low-rank subspace updates.\n\nHowever, LoRA's fixed low-rank structure may not fully capture the complexity of tasks that require higher-dimensional\nor multi-modal representations. To address this, MoSLORA [2] introduces an extension by adding a learnable mixer\nmatrix between the LoRA matrices A and B. The update equation in MoSLORA is modified as:\n$\\displaystyle AW = AW_mB^T,$\nwhere Wm \u2208 R\u2033\u00d7r serves as a mixer matrix. MoSLORA interprets this mixer in terms of the Mixture of Experts (MoE)\nframework [3], where the mixer plays the role of a gating network. It helps match subspaces between the matrices A\nand B. This addition of the mixer matrix allows MOSLORA to capture more complex interactions between subspaces,\nresulting in improved task adaptation and performance."}, {"title": "3 Linear Chain Transformation (LinChain)", "content": "The main idea behind the proposed LinChain finetuning method is to enhance the expressiveness of low-rank fine-\ntuning methods, such as LoRA, by introducing a series of linear transformations between the projection matrices.\nInstead of limiting the parameter update to a single low-rank transformation, LinChain employs multiple intermediate\ntransformations to capture more complex relationships in the model's parameter space."}, {"title": "3.1 The Core Idea", "content": "The main idea behind the proposed LinChain finetuning method is to enhance the expressiveness of low-rank fine-\ntuning methods, such as LoRA, by introducing a series of linear transformations between the projection matrices.\nInstead of limiting the parameter update to a single low-rank transformation, LinChain employs multiple intermediate\ntransformations to capture more complex relationships in the model's parameter space."}, {"title": "3.2 Analysis of Optimization Traces", "content": "In LinChain, the chain of matrices introduces additional linear transformations between A and B. Each matrix W\u2081 in\n the chain adds an intermediate transformation, increasing the depth of the computational graph. This depth affects the\n optimization dynamics by introducing more paths through which gradients can propagate. In this analysis, we will\n quantify how the chain of matrices affects the number of optimization traces during gradient descent.\n\nDefinition of Optimization Traces An optimization trace refers to the sequence of parameter updates during training,\n influenced by the gradients flowing through the computational graph. The number of optimization traces can be\n associated with the number of unique paths through which gradients can propagate from the loss function to each\n parameter.\n\nModel Setup In LinChain, the adapted weight matrix is defined as:\n$\\displaystyle W_{merge} = W_0 + AW_1W_2...W_nB,$\nwhere Wo \u2208 Rdin\u00d7dout is the frozen base weight matrix. A \u2208 Rdin\u00d7r and B \u2208 Rr\u00d7dout are low-rank adaptation matrices.\nWi for i = 1, 2, ..., n are the mixer matrices forming the chain.\n\nForward Pass For an input x \u2208 Rdin, the output is:\n$\\displaystyle y = xW_{merge} = xW_0 + xAW_{chain}B,$\nwhere Wchain = W1W2... Wn.\n\nBackward Pass and Gradient Computation To analyze the optimization traces, we compute the gradients of the\nloss L with respect to each parameter.\n\nLet A = $\\frac{\\partial L}{\\partial y}$ \u2208 Rdout. The gradients are:\n1. Gradient with respect to A:\n$\\displaystyle \\frac{\\partial L}{\\partial A} = (x\\Delta)^T BW_{chain}^T$\n2. Gradient with respect to B:\n$\\displaystyle \\frac{\\partial L}{\\partial B} = W_{chain}^T A^T x^T \\Delta.$\n3. Gradient with respect to each Wi:\n$\\displaystyle \\frac{\\partial L}{\\partial W_i} = (A^T x^T \\Delta B^T).$"}, {"title": "4 Experiments", "content": "In this section, we evaluate the performance of LinChain through a series of experiments on standard NLP benchmarks.\nWe compare LinChain to LoRA and its variants in terms of task performance, convergence rates, memory usage,\ntraining time, and computational cost."}, {"title": "4.1 Benchmarking on NLP Tasks", "content": "To assess the performance of LinChain, we conduct experiments on several widely-used NLP tasks, including com-\nmonsense reasoning, arithmetic reasoning, and natural language understanding. We compare LinChain to LoRA and\nMOSLORA across these benchmarks.\n\nDatasets We evaluate the methods on the following datasets:\n\nCommonsense Reasoning: Our model is fine-tuned on COMMONSENSE170K, a combined training dataset\nconsisting of 170,420 questions from multiple commonsense reasoning tasks. We evaluate the model on the\nfollowing datasets:\nPIQA: Tests physical commonsense reasoning, where the model chooses one of the provided actions\nbased on a hypothetical scenario. The test set includes 1,830 questions.\nSIQA: Focuses on reasoning about people's actions and their social consequences. The test set includes\n1,954 questions.\nHellaSwag: Requires the model to select the most appropriate sentence completion given a context. The\ntest set contains 10,042 questions.\nWinoGrande: Inspired by the Winograd Schema Challenge, this dataset tests commonsense reasoning\nby asking the model to fill in a blank with binary options. The test set has 1,267 questions.\nARC Easy (ARC-e): A set of grade-school level multiple-choice science questions. The test set includes\n1,172 questions.\nARC Challenge (ARC-c): A more difficult version of ARC-e, designed to challenge co-occurrence-based\nmethods. The test set contains 2,376 questions.\nOBQA: A knowledge-intensive, open-book QA dataset requiring multi-hop reasoning. The test set\nincludes 500 questions.\nArithmetic Reasoning: We fine-tune our model on MATH10K, a combined training dataset containing 9,919\nproblems from four tasks (GSM8K, MAWPS, MAWPS-single, and AQuA). Evaluation is performed on the\nfollowing datasets:"}, {"title": "4.2 Effectiveness of Chain-of-Linear-Transformations", "content": "One of the key contributions of LinChain is the introduction of multiple linear transformations, which provide additional\nflexibility in representing complex feature interactions. In this subsection, we analyze how this chain-of-transformations\nimpacts task performance and convergence rates.\n\nPerformance Improvement The introduction of intermediate transformations significantly enhances the model's\nability to capture high-dimensional dependencies. As shown in the fourth column of Table 1, LinChain achieves a\nnotable performance boost over both LoRA and MoSLORA on commonsense reasoning tasks. With a comparable\nnumber of learnable parameters (28.43M for LinChain vs. 28.31M for LoRA and 28.35M for MoSLORA), LinChain"}, {"title": "4.3 Efficiency Evaluation", "content": "While LinChain introduces additional parameters compared to LoRA, it remains computationally efficient. In this\nsubsection, we compare LinChain to LoRA and MoSLORA in terms of memory usage, training time, and computational\ncost.\n\nMemory Usage The second column of Table 4 presents the memory usage of the different methods during training.\nAlthough LinChain introduces multiple intermediate matrices, the overall increase in memory usage is minimal with\nthe same rank of 16. This is because the transformations are still low-rank and the number of parameters added by the\nchain (see the fourth column of Table 4) is relatively small compared to full fine-tuning.\n\nLinChain uses slightly more memory than LoRA and MoSLoRA, but it remains far more efficient than full fine-tuning.\n\nTraining Time In terms of training time, LinChain performs comparably to LoRA and MoSLORA (see the third\ncolumn of Table 4). The additional matrix multiplications introduced by the chain result in a small increase in\ncomputation time per epoch, but this is offset by the faster convergence seen in Section 4.2. Overall, LinChain achieves\nbetter performance in a similar number of training epochs.\n\nOverall Efficiency Despite the introduction of additional parameters, LinChain offers a favorable trade-off between\nexpressiveness and efficiency. The small increase in memory and training time is justified by the significant gains in task"}, {"title": "5 Conclusion", "content": "In this paper, we present LinChain, a novel parameter-efficient fine-tuning method for LLMs that extends the low-rank\nadaptation framework by introducing a chain of linear transformations. Our proposed method addresses the limitations\nof existing techniques such as LoRA and MoSLORA, which, despite their efficiency, struggle to capture complex\ntask-specific feature interactions. By decomposing the weight update into a sequence of transformations, LinChain\nsignificantly improves the expressiveness and flexibility of the model without compromising computational efficiency.\n\nOur experimental results demonstrate that LinChain outperforms both LoRA and its variants across a variety of tasks,\nincluding commonsense reasoning, arithmetic reasoning, and various NLP tasks. LinChain's ability to model high-order\ndependencies between features is particularly evident in tasks that require complex token-level interactions, where it\nachieves superior performance in terms of accuracy and correlation coefficient. Furthermore, LinChain shows faster\nconvergence during training, thanks to its richer optimization paths facilitated by the chain of transformations.\n\nIn terms of efficiency, LinChain introduces only a minimal increase in memory usage and computational cost compared\nto LoRA and MoSLORA. The flexibility offered by the additional transformation matrices leads to better task adaptation\nwith only a slight trade-off in parameter efficiency. The overall balance between expressiveness and computational cost\nmakes LinChain a highly effective solution for fine-tuning large models on complex tasks. We will conduct further\nexperiments to validate the effectiveness of the proposed LinChain fine-tuning method."}]}