{"title": "LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset", "authors": ["Zhaojie Fang", "Xiao Yu", "Guanyu Zhou", "Ke Zhuang", "Yifei Chen", "Ruiquan Ge", "Changmiao Wang", "Gangyong Jia", "Qing Wu", "Juan Ye", "Maimaiti Nuliqiman", "Peifang Xu", "Ahmed Elazab"], "abstract": "Ultra-Wide-Field Fluorescein Angiography (UWF-FA) enables precise identification of ocular diseases using sodium fluorescein, which can be potentially harmful. Existing research has developed methods to generate UWF-FA from Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) to reduce the adverse reactions associated with injections. However, these methods have been less effective in producing high-quality late-phase UWF-FA, particularly in lesion areas and fine details. Two primary challenges hinder the generation of high-quality late-phase UWF-FA: the scarcity of paired UWF-SLO and early/late-phase UWF-FA datasets, and the need for realistic generation at lesion sites and potential blood leakage regions. This study introduces an improved latent diffusion model framework to generate high-quality late-phase UWF-FA from limited paired UWF images. To address the challenges as mentioned earlier, our approach employs a module utilizing Cross-temporal Regional Difference Loss, which encourages the model to focus on the differences between early and late phases. Additionally, we introduce a low-frequency enhanced noise strategy in the diffusion forward process to improve the realism of medical images. To further enhance the mapping capability of the variational autoencoder module, especially with limited datasets, we implement a Gated Convolutional Encoder to extract additional information from conditional images. Our Latent Diffusion Model for Ultra-Wide-Field Late-Phase Fluorescein Angiography (LPUWF-LDM) effectively reconstructs fine details in late-phase UWF-FA and achieves state-of-the-art results compared to other existing methods when working with limited datasets. Our source code is available at: https://github.com/Tinysqua/LPUWF-LDM.", "sections": [{"title": "1. Introduction", "content": "Ultrawide field fluorescein angiography (UWF-FA) is a dynamic imaging technique for diagnosing and treating fundus-related diseases (Ashraf et al., 2020; Ehlers et al., 2019; Wang et al., 2021). This procedure involves injecting a dye into a patient's vein, which travels to the eye's fundus, potentially causing nausea or vomiting. It can also pose risks for patients with serious heart conditions. In contrast, Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) rapidly scans the retina using laser imaging and does not adversely affect the patient. However, UWF-SLO produces less detailed vascular images. Previous studies, such as VTGAN (Kamran et al., 2021) and Reg-GAN (Rezagholiradeh and Haidar, 2018), have attempted to address this issue by transforming UWF-SLO images into UWF-FA images using cross-modal generation methods. These approaches, however, only supervise learning with a single UWF-SLO and an early-phase UWF-FA, neglecting the dynamic nature of UWF-FA. Issues related to retinal structure, like central serous chorioretinopathy (Chen et al., 2021), are often assessed in the late phase. The scarcity of paired UWF-SLO, early-phase, and late-phase UWF-FA datasets makes generating high-quality UWF-FA from UWF-SLO a significant challenge. Two primary technical difficulties arise: (1) producing high-quality UWF-FA with detailed lesion information to aid diagnosis, and (2) maintaining the quality of generated late-phase UWF-FA despite limited paired datasets.\nTo address these challenges, we propose an enhanced latent diffusion framework. This framework utilizes a diffusion network and a Variational Autoencoder (VAE) (Kingma and Welling, 2013) to generate late-phase UWF-FA images with high fidelity and stability. Besides, unlike traditional diffusion models that generally require extensive training on large datasets and are typically optimized for natural images, our approach enhances the traditional latent diffusion framework by improving its performance with smaller datasets and increasing its sensitivity to the differences between early and late-phase UWF-FA images. This makes our model more suitable for training on medical retina data, ensuring high-quality image generation even with limited data availability.\nIn this paper, we begin by training our model on a multicenter dataset of UWF-SLO and early-phase UWF-FA images to capture structural information. Following this, we train the model on a smaller dataset of paired UWF-SLO and late-phase UWF-FA images. To enhance the model's focus on temporal discrepancies, we incorporate a Cross-temporal Regional Difference Loss (CTRD Loss) into the loss function. Given that ophthalmic images are rich in low-frequency components and traditional noise addition methods often vary in their handling of high and low frequencies, we employ a low-frequency enhanced noise strategy. This approach improves the model's ability to infer medical images that are abundant in low-frequency information. For the VAE component, we introduce a Gated Convolutional Encoder. This encoder extracts additional information from UWF-SLO images, assisting in pixel-space reconstruction by filtering useful information through the gated module. Additionally, we propose a preprocessing method for UWF fundus photographs, which includes sharpening UWF-SLO images and aligning early and late-phase UWF-FA images.\nTo demonstrate the effectiveness of our LPUWF-LDM framework, we benchmarked it against leading contemporary image generation models using our proprietary UWF image datasets. We employed both qualitative and quantitative metrics, including the Fr\u00e9chet Inception Distance (FID) (Binkowski et al., 2018), Inception Score (IS) (Chong and Forsyth, 2020), Peak Signal-to-Noise Ratio (PSNR) (Sara et al., 2019), and Multi-Scale Structural Similarity Index (MS-SSIM) (Wang et al., 2004).\nOur contributions can be summarized as follows:\n\u2022 To the best of our knowledge, this is the first study to train and evaluate diffusion models for generating late-phase UWF-FA images from UWF-SLO, eliminating the need for dye injections. We provide pairs of early-phase and late-phase UWF-FA images that have undergone registration and noise reduction."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Cross-modal Generation", "content": "In medical imaging, various modalities such as magnetic resonance imaging, positron emission tomography, UWF-SLO, and UWF-FA are commonly used for patient examinations. Establishing nonlinear mappings between these modalities using deep learning can provide multimodal information that aids in diagnosis while avoiding the side effects associated with certain imaging procedures. Cross-modal image generation, particularly generating late-phase UWF-FA from UWF-SLO, requires high-quality output characterized by accurate structural and pathological information. The U-Net architecture, which utilizes convolutional neural networks for upsampling and downsampling and incorporates skip connections, was initially employed for cross-modal generation (Li et al., 2019; Dovletov et al., 2022). Generative adversarial networks (GANs) (Goodfellow et al., 2020) and their variant, conditional GAN (Vaidya et al., 2022; Uzunova et al., 2020), built on the U-Net architecture, use a discriminator to judge the authenticity of generated content. This adversarial approach improves the quality of cross-modal generated images by reducing local blurriness.\nIn ophthalmology, Kamran et al. (Kamran et al., 2021) and Fang et al. (Fang et al., 2023) have successfully achieved realistic cross-modal generation on standard and UWF fundus images. However, GAN-based methods often face issues such as local distortions and mode collapse, making them unstable for practical use. Recently, denoising diffusion probabilistic models have emerged as a more stable alternative. These models gradually introduce noise to the images, converting them into pure noise, and then train the model to map them back to the original state, generating high-fidelity images. Due to these advantages, diffusion models have been applied to medical image generation (Pan et al., 2023; Song et al., 2023).\nDespite their promise, diffusion models still struggle with poor performance on small datasets and a lack of focus on lesion content. To address this, our approach enhances the learning of the diffusion model in lesion areas through CTRD Loss, allowing it to generate finer lesion details even on smaller datasets."}, {"title": "2.2. Diffusion Models combined with VAE", "content": "Traditional diffusion models, while stable and capable of producing high-quality images, require substantial computational resources. Recent models, such as Latent Diffusion Model (LDM) and Stable Diffusion, use a VAE to downsample images into a latent space, making them effective for generating high-resolution ophthalmic images (Jang et al., 2023; Zhu et al., 2023). However, LDMs are not directly suitable for cross-modality generation due to their primary reliance on text as a conditioning input.\nControlNet (Zhang et al., 2023) introduced the potential for high-resolution and high-quality generation in late-phase UWF-FA tasks by using the source image as a condition. Because of these advantages, methods based on controlled and LDM have begun to be practiced on medical images (Go et al., 2024; Kim et al., 2024). Despite this advancement, LDMs still face challenges with poor performance on small datasets, and the incorporation of VAES introduces additional issues. LDMs typically employ a VQGAN, which combines vector quantization (Rombach et al., 2022) with a GAN discriminator, or an AutoencoderKL that includes a traditional VAE with an added discriminator. VAEs need to be trained on a wide range of datasets; otherwise, even if the diffusion part is well-fitted, overall image quality can degrade due to a decline in VAE-generated quality. Moreover, the effectiveness of converting pathologies in cross-modality generation remains limited.\nTo address these issues, we have enhanced the original VAE with a Gated Convolutional Encoder that leverages transfer learning. This enhancement enables the selective use of UWF-SLO condition information during the reconstruction of late-phase UWF-FA images. Additionally, sharpening the UWF-SLO images allows for more effective conversion of pathologies, thereby improving the quality and accuracy of cross-modality generation."}, {"title": "3. Methods", "content": "The overall architecture of our method, as illustrated in Fig. 1, comprises four principal components: a VAE module for upsampling and downsampling, a diffusion forward noise addition module, a backbone module using a U-Net architecture for noise prediction, and a Control Encoder module for spatial conditioning.\nGiven a dataset $(x^l, y, y^l) \\in D$, where $x^l$ denotes UWF-SLO, $y$ represents early-phase UWF-FA, and $y^l$ signifies late-phase UWF-FA, the training process is as follows: Initially, the model is trained on pairs of $x$ and $y$ to learn structural information. Subsequently, it is trained on pairs of $x^l$ and $y^l$. During inference, the model generates the corresponding $y$ based on the input image $x^l$.\nFor the diffusion component, $y^l$ is first compressed into the latent space $y_L$ via the VAE's Encoder. The forward noise addition module then incorporates our proposed Low-Frequency Enhanced Noise, transforming $y_L$ into a noise-augmented image at step $t$, denoted as $y_t$. Both $x^l$ and $y_t$ are concurrently input into the Control Encoder, which extracts features and feeds them into the backbone network to predict the noise added to $y_L$, represented as $\\tilde{\\epsilon}$. Training involves augmenting the original diffusion MAE loss with a CTRD Loss. During inference, what was originally input into the backbone and Control Encoder as $y_L$ turns into pure noise $\\epsilon$. This noise is then iteratively refined back to $y$ through a recursive process. Finally, the VAE's Decoder reverts $\\tilde{y}$ to $y^l$. During the VAE Decoder's operation, the Gated Convolutional Encoder takes $x^l$ as input, and the intermediate feature maps it produces, after passing through the gating mechanism, are input into the Decoder to aid in the restoration process of $y^l$."}, {"title": "3.1. Gated Convolutional Encoder For VAE", "content": "Our cross-modal tasks use images as conditions, which will provide rich spatial information, such as the details of the blood vessels. Notably, the addition of this supplementary information has been observed to improve the generation quality and generalization performance of VAEs, especially when applied to smaller datasets. However, UWF-SLO images, which serve as additional information, often contain significant noise, such as orbital areas around the eyes. To address this, we propose a gating mechanism to filter out noise in the conditional images.\nThe detailed architecture is presented in Fig. 2, consisting of VAE backbone and Gated Convolutional Encoder. The VAE backbone contains an Encoder $(E_{FA})$ and a Decoder $(D_{FA})$, along with a discriminator to enhance the quality of the generated images. Initially, the late-phase UWF-FA image $(y^l)$ is fed into $E_{FA}$. It first passes through a convolutional layer with a kernel size of 3, a stride of 1, and padding of 1 (Conv, k=3, s=1,p=1). Next, it moves through three Down blocks, each comprising two Residual blocks and one Down-sample block. The output of $E_{FA}$ provides the mean and variance in the latent space, from which the latent vector $(\\tilde{y})$ is sampled. The latent vector $(\\tilde{y})$ is then fed into $D_{FA}$, which consists of three Up blocks. Since the Decoder also receives additional spatial information, each Up block checks for this extra information and incorporates it into the current layer's vector if available. The vector then passes through two Residual blocks and a self-attention block. Finally, it moves through a Group Normalization layer and another convolutional layer (Conv, k=3, s=1, p=1) to reconstruct $\\tilde{y}_i$ into the final output.\nThe Gated Convolutional Encoder consists of three Gated Down blocks, each incorporating a convolutional layer with a kernel size of 3, stride of 2, and padding of 1 to compress the input image $x^l$. Additionally, each Down block includes a Gate Module, which features a convolutional layer (Conv, k=3, s=1,p=1) followed by a non-linear Sigmoid layer. The relationship between the Down blocks is depicted in Fig. 2. For an input $y_n^l$ at layer n, the next layer's input $y_{n+1}^l$ is computed as:\n$Y_{n+1}^l = \\sigma \\ (gate\\_module(Conv(y_n^l)) \\bullet Conv(\\tilde{y}))$,\nwhere $\\sigma$ denotes the non-linear computation, Conv represents the convolution operation and $\\bullet$ represents element-wise multiplication.\nTraining the entire VAE includes a two-step process. First, we perform transfer learning from the AutoencoderKL of Stable Diffusion and then fine-tune it on our dataset. The loss function used is:\n$L = ||y_i - D_{FA} (E_{FA} (y_i))||^2 + log \\left(1 - D \\left(D_{FA} (E_{FA} (y))\\right)\\right) + Perc \\left[y^l, D_{FA} (E_{FA})\\right] + KL \\left(E_{FA} (y^l)\\right)$,\nwhere D denotes the discriminator, Perc represents the Perception loss, and KL denotes the Kullback-Leibler loss. The loss function for the discriminator D is:\n$L (D) = log(1 - D(y^l)) + log(D(D_{FA} (E_{FA} (y^l)))).$\nWhen training the Gated Convolutional Encoder, we set only its parameters to be trainable and remove the KL loss from the loss function in Eq. 2, while keeping the other components unchanged. Training continues until the model is well-fitted."}, {"title": "3.2. Cross-temporal Reginal Difference Loss", "content": "The early-phase UWF-FA is very clear in terms of structural information, such as the position of arteries, veins and optic disc. Therefore, it is better for the model to learn the mapping from UWF-SLO to early-phase UWF-FA efficiently. However, while the structural information can guide the realism of image generation, it is disadvantageous in generating detailed information related to lesions, as more lesion information is actually reflected in the late-phase UWF-FA. Therefore, the module that is designed aims to solve two problems: 1) How to locate the lesion areas without the need for manual annotation. 2) How to make the diffusion model focus more on these areas. Therefore, we propose the Cross-temporal Regional Difference Loss. Specifically, we first register the early-phase and late-phase UWF-FA images, then compute the pixel-wise absolute difference, and normalize the result to the range (0, 1) to obtain a heatmap of the same shape and size as the UWF-FA images. Regions with higher values indicate greater differences between the early and late phases, while lower values correspond to smaller differences. From Fig. 3, we can observe that the uninformative black background has lower values, while the vascular regions and the leakage area on the left exhibit higher values. Statistically, the average difference induced by fluorescence leakage is 0.87, while that of neovascularization is 0.71. The higher-valued locations signify that the model should focus more on generating these regions, forming an unsupervised attention to the lesion areas. Since the diffusion is trained in the latent space, this heatmap is resized to match the same size as the latent space. Therefore, the overall learning objective based on diffusion model is:\n$\\mathcal{L} = \\underset{E_{FA}(y),t,x^l,\\epsilon \\sim N(0,1)}{\\text{lim }} \\mathbb{E} \\left[ \\alpha \\cdot w(y,t,x^l) \\cdot ||\\epsilon - \\epsilon_{\\theta} (y,t,x^l)||^2 + ||\\epsilon - \\epsilon_{\\theta} (y,t,x^l)||^2 \\right]$,\nthe $\\alpha$ is a hyperparameter that represents the weight of this loss term. In the experiments, $\\alpha$ gradually increases from 0.25 to 1 during the first half of the training epochs, and remains at 1 for the second half of the training epochs. The $w$ denotes the aforementioned heatmap, which is related to the early-phase and late-phase UWF-FA. $y$ represents the noisy image, where $t$ indicates the current step of adding noise, and $x^l$ is the conditional input UWF-SLO."}, {"title": "3.3. Low Frequency Enhanced Noise", "content": "The traditional strategy of injecting noise into images by adding identically distributed (i.i.d.) Gaussian noise affects high-frequency and low-frequency information differently. When a noisy UWF-FA image is decomposed into these components, a noticeable disparity in noise injection between high-frequency and low-frequency regions is observed. Specifically, low-frequency regions experience less noise interference compared to high-frequency regions. This discrepancy can impair the model's ability to restore low-frequency details, which are abundant in medical images.\nTo address this issue, we aim to increase the noise interference on low-frequency information. We achieve this by adding a low-frequency noise component to the original Gaussian noise, which follows a N(0, 1) distribution. To generate this low-frequency noise, we first sample a value a from N(0, 1) and a scale factor $\\beta$ from N(0, 0.5) to control the degree of low-frequency noise. We then multiply a and $\\beta$, and replicate the resultant product $\\alpha\\beta$ across the height and width dimensions of the image, creating a low-frequency noise term $e_L$ with no numerical variation. Fig. 4 shows the separated images of high and low frequencies after increasing the low frequency. Under the ordinary noise addition strategy, the low frequency still maintains relatively clear textures, while our strategy makes the damage of high- and low-frequency noise more balanced.\nBy adding this low-frequency noise term to the original noise, the new noise vector $y_t$ maintains the i.i.d. assumption. Therefore, the new $y_t$ can be represented as follows:\n$y_t = \\sqrt{a} \\cdot y_t + \\sqrt{1-a} \\cdot (\\epsilon + e_L)$,\nwhere $\\epsilon$ and $e_L$ denote pure noise and low-frequency noise, respectively."}, {"title": "3.4. Preprocessing of UWF photos", "content": "The raw UWF fundus images do not perform well in direct training due to two significant sources of noise: 1) the blood vessels in the UWF-SLO are mixed with the laser background, and 2) there is misalignment between the early-phase and late-phase UWF-FA. To address these issues, we have incorporated image sharpening for UWF-SLO and registration between early-phase and late-phase UWF-FA into our pipeline.\nTo enhance the UWF-SLO images, we apply histogram equalization techniques separately to the RGB channels before merging them back into the UWF-SLO image. This process makes the vessel colors more distinguishable from the background, as illustrated in Fig. 5. For registering the early-phase and late-phase UWF-FA images, we use the SIFT algorithm to detect keypoints and compute descriptors. The best matching points are then used to calculate the homography matrix and perform a perspective transformation, aligning the late-phase UWF-FA to the early-phase.\nHowever, due to significant choroidal background fluorescence noise, the registration algorithm struggles to detect enough keypoints for matching. Therefore, it is crucial to minimize the noise in the input images. To achieve this, we use a multi-scale linear filter for vessel segmentation. The filter formula is as follows:\n$I_{\\alpha} = \\frac{\\sum \\alpha \\cdot (I_m - \\alpha \\cdot (\\widehat{G}_x + \\widehat{G}_y) - I) + I}{N+1}$,\nwhere I is the grayscale value of the window center, $I_m$ is the maximum average grayscale value of the fan-shaped region in the window, $\\widehat{G}_x$ and $\\widehat{G}_y$ are the average gradient values in the horizontal and vertical directions of the fan-shaped region, N is the number of fan-shaped regions in the window, and $\\alpha$ is an adjustable parameter that controls the influence of gradient compensation. In this filtering process, a window is defined, and N sectors with variable radii are drawn from the center. The average grayscale value for each sector is computed, incorporating compensation based on local average gradient values. The data obtained at different scales are linearly combined to calculate the multi-scale filtered grayscale value $I_a$ at the center of the window.\nAfter this filtering process, the vessels are effectively segmented from the background, significantly enhancing the image registration effect. Fig. 5 depicts the UWF-FA before registration, the segmented blood vessel image during registration, and the final result after registration."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Dataset", "content": "We utilized datasets from two collaborating hospitals. The first hospital's dataset primarily includes paired UWF-SLO images and early-phase UWF-FA images. These data underwent rigorous screening to exclude images that could compromise clinical diagnosis, such as those with capture intervals exceeding three months, visible fresh bleeding, severe eyelash occlusion, or poor focus. Ultimately, 304 pairs of high-quality images were selected from this hospital. After data augmentation, these images were fully utilized for extensive pre-training of the model. The second hospital's dataset contains paired UWF-SLO images, early-phase UWF-FA images, and late-phase UWF-FA images. After filtering, we identified 387 image pairs for our dataset, with each set containing all three image types. We then divided these images into 309 pairs for the training set and 78 pairs for the test set. The classification was based on physician definitions, with early-phase designated as 0 to 15 seconds and late-phase as after 10 minutes.\nTo ensure data consistency, we adjusted the image resolution to a uniform 853x682 pixels across both datasets and applied various enhancement measures. These included random rotation within [\u22125\u00b0, 5\u00b0], cropping to 512x512 pixels, and random flipping. These enhancements effectively expanded the original training dataset from 304 pairs to 19,456 pairs and the 309 sets to 6,192 sets, totaling 25,648 UWF-SLO images, 25,648 early-phase UWF-FA images, and 6,192 late-phase UWF-FA images."}, {"title": "4.2. Evaluation Metrics", "content": "We adopted four evaluation metrics: FID (\u2193), IS (\u2191), PSNR (\u2191), and MS-SSIM (\u2191), which together provided a comprehensive framework for assessing the quality of generated images. FID is a crucial metric for evaluating the fidelity and realism of generated images, as it captures perceptual differences beyond mere pixel-level disparities. IS measures the diversity and quality of generated images through the softmax probability distribution within the Inception model, with a higher IS value indicating clearer and richer image content. PSNR quantifies distortion by calculating the mean squared error between images, directly reflecting the degree of quality loss or degradation in generated images compared to their real counterparts. MS-SSIM, on the other hand, offers a detailed assessment of structural similarities between images from a multi-scale perspective, encompassing dimensions such as luminance, contrast, and structural information. All evaluations based on these metrics were conducted on the test set of the second dataset, which comprised 309 training sets and 78 test sets."}, {"title": "4.3. Implementation Details", "content": "In this study, we employed the Pytorch 2.0 cuda 11.8 framework to construct the LPUWF-LDM network and trained it using two NVIDIA A100 GPUs. The VAE and diffusion components were each trained for 1000 epochs. The VAE component, trained with a batch size of 8, required 52 hours. While the diffusion component, with a batch size of 32, took 33 hours. The training process began with the VAE itself, followed by freezing the VAE to train the Gated Convolutional Encoder. Subsequently, both the VAE and the Gated Convolutional Encoder were frozen. The diffusion model was then initially trained on paired UWF-SLO and early-phase UWF-FA data, and later on paired UWF-SLO and late-phase UWF-FA data. Each component inherited parameters from Stable Diffusion and underwent transfer learning. Both models were optimized using the Adam algorithm, with a learning rate and betas set to (0.9, 0.999)."}, {"title": "4.4. Comparison", "content": "In this experiment, we compared our method against various other generative models, selecting three methods each from GAN networks and diffusion networks for comparison. For the GAN network methods, we chose Pix2pixHD Wang et al. (2018), Reg-GAN and UWAFA-GAN. Pix2pixHD can generate high-quality images at a resolution of 2048x1024 through its multi-scale generator design and the incorporation of perceptual loss. Reg-GAN combines CycleGAN with a registration module, enabling it to generate high-quality images even with slightly misaligned ophthalmic image datasets. UWAFA-GAN, on the other hand, is the first method to use GANs to create early-phase UWF-FA from UWF-SLO, achieving promising results."}, {"title": "4.5. Ablation Studies", "content": "In this section, we examine the effectiveness of the proposed modules in our approach. We conduct an ablation study by comparing the FID, IS, PSNR, and MS-SSIM metrics with and without the proposed modules and preprocessing strategy. The results of this analysis are presented in Table 2.\nGated Convolutional Encoder: To assess the effectiveness of our proposed module, we conducted two experiments. First, we tested the Gated Module, which extracted useful information from the encoder through a gating mechanism. For this evaluation, we removed the Gated Module (denoted as w/o GM), allowing the convolutional information from the encoder to enter the VAE decoder directly without filtering. This removal led to an increase in FID to 82.386, while IS, PSNR, and MS-SSIM decreased from 1.7578, 30.6727, and 0.7104 to 1.6787, 30.0977, and 0.6554, respectively. The second experiment further confirmed the impact of the Gated Convolutional Encoder on generative performance with a small dataset. Without the Gated Convolutional Encoder (w/o GCE), FID increased to 84.5027, and IS, PSNR, and MS-SSIM further declined from 1.7578, 30.6727, and 0.7104 to 1.5496, 28.7481, and 0.6595, respectively. In addition, the reconstruction effects with or without GCE are shown in Fig. 7. During reconstruction, lacking GCE results in the loss of small blood vessels and details of lesions. The results show that increasing information improves the quality of generated images, but directly incorporating UWF-SLO spatial information without filtering actually decreases the quality.\nCross-temporal Reginal Difference Loss: We conducted ablation studies to investigate the impact of CTRD Loss and its hyperparameter a on the model's performance. Initially, we removed the CTRD Loss (denoted as w/o CTRDL), which resulted in an increase in FID from 77.6596 to 80.3336, and a decrease in IS, PSNR, and MS-SSIM from 1.7578, 30.6727, and 0.7104 to 1.6676, 30.1324, and 0.4278, respectively.\nNext, we examined the influence of the hyperparameter a by setting it to either 0.25 or 1 throughout the entire training process. When a was consistently set to 0.25, FID slightly increased to 79.9259, with IS, PSNR, and MS-SSIM decreasing from 1.7578, 30.6727, and 0.7104 to 1.6883, 30.3782, and 0.6666, respectively. This indicates that maintaining a constant a value of 0.25 has a minimal impact on generation quality. Conversely, when a was kept at 1, FID rose to 82.5635, and IS, PSNR, and MS-SSIM decreased from 1.7578, 30.6727, and 0.7104 to 1.6260, 28.7534, and 0.5621, respectively. These results suggest that maintaining a high value of a throughout the training process may diminish the model's performance.\nLow-Frequency Enhanced Noise: We conducted an ablation study to evaluate the importance of Low-Frequency Enhanced Noise (LFEN). When the low-frequency noise was removed (denoted as w/o LFEN), we observed a decline in performance. Specifically, the FID increased to 88.4215, while the IS, PSNR, and MS-SSIM decreased from 1.7578, 30.6727, and 0.7104 to 1.6170, 28.6761, and 0.4184, respectively. The results indicate that the new noise addition strategy could significantly improve the generation quality. The visual difference can be viewed in Fig. 4.\nPreprocessing Strategy: In Section 3.4, we implement image sharpening for UWF-SLO and perform registration on the early-phase and late-phase UWF-FA. To evaluate the impact of these preprocessing steps, we conducted two experiments, with one using only raw UWF-SLO without image sharpening (designated as w/o ImgS) and another using early-phase and late-phase UWF-FA without registering (designated as w/o Reg). Without image sharpening, the model's performance declined due to some blood vessels blending with the background. The FID increased to 85.9619, and IS, PSNR, and MS-SSIM decreased to 1.6641, 29.3813, and 0.5835, respectively. The sharpening of UWF-SLO makes the difference between blood vessels and background clearer, allowing the model to generate better. Second experiment resulted in a marked increase in FID to 104.4215, with IS, PSNR, and MS-SSIM decreasing to 1.5167, 25.3922, and 0.3601, respectively. The results indicate that CTRD Loss has a negative impact on model training on misaligned early and late-phase UWF-FA, resulting in decreased model performance."}, {"title": "5. Conclusion", "content": "This paper introduces an advanced latent diffusion framework named LPUWF-LDM, designed to generate high-resolution, detail-rich UWF-FA images from UWF-SLO, specifically targeting the late-phase generation of UWF-FA images. The framework tackles the challenge of producing high-quality UWF-FA images from a small sample dataset by incorporating a CTRD Loss and a low-frequency enhanced noise strategy. These enhancements significantly improve the depiction of blood vessels and pathological details in the generated images. A significant feature of our framework is a Gated Convolutional Encoder is employed in the VAE component. This encoder effectively extracts crucial information from UWF-SLO images and regulates the incorporation of noise information, thereby enhancing the model's expressive power and stability. Experimental results demonstrate that LPUWF-LDM achieves state-of-the-art performance on a proprietary UWF image dataset, offering robust support for non-invasive retinal disease diagnosis.\nLooking ahead, we plan to expand the dataset by including a more diverse and complex array of retinal disease cases. Besides, in addition to self-supervised attention to lesions, we will explicitly incorporate some lesion attention mechanisms. Additionally, we aim to integrate clinical information and collaborate closely with ophthalmology experts to develop detailed operating procedures and evaluation standards. These efforts will promote the stable clinical application of our technology and improve the efficiency of retinal disease diagnosis."}]}