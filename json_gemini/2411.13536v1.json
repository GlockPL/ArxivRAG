{"title": "Identity Preserving 3D Head Stylization with Multiview Score Distillation", "authors": ["Bahri Batuhan Bilecen", "Ahmet Berke Gokmen", "Furkan Guzelant", "Aysegul Dundar"], "abstract": "3D head stylization transforms realistic facial features into artistic representations, enhancing user engagement across applications such as gaming and virtual reality. While 3D-aware generators have made significant advancements, many 3D stylization methods primarily provide near-frontal views and struggle to preserve the unique identities of original subjects, often resulting in outputs that lack diversity and individuality. This paper addresses these challenges by leveraging the PanoHead model, synthesizing images from a comprehensive 360-degree perspective. We propose a novel framework that employs negative log-likelihood distillation (LD) to enhance identity preservation and improve stylization quality. By integrating multi-view grid score and mirror gradients within the 3D GAN architecture and introducing a score rank weighing technique, our approach achieves substantial qualitative and quantitative improvements. Our findings not only advance the state of 3D head stylization but also provide valuable insights into effective distillation processes between diffusion models and GANs, focusing on the critical issue of identity preservation. Please visit the project page.", "sections": [{"title": "1. Introduction", "content": "3D head stylization refers to transforming realistic facial and head features into artistic representations that can be rendered from multiple viewpoints, providing a dynamic and immersive experience. 3D head stylization has diverse applications in entertainment, advertising, education, enhancing user engagement, and creating relatable characters, and it is also a popular research topic due to its potential for innovation and impact across various fields.\nPreviously, face stylization was extensively explored in 2D image domains [13, 40, 43, 56], particularly through generative models like StyleGAN [24], which allowed for the manipulation of stylistic elements to create diverse, high-quality facial representations. With recent advancements in realistic 3D-aware face generators [4, 8], research in stylization has now shifted to the 3D domain [1, 5, 25, 26, 29, 50, 51, 61], as these models enable the creation of lifelike representations that can be viewed from multiple angles, significantly enhancing user experiences in applications such as gaming, virtual reality, and animation.\nMost of these methods proposed for 3D stylization are built on the EG3D generator [8], which primarily synthesizes near-frontal views. This limitation restricts the generation of comprehensive 3D scenes from diverse viewpoints. In this work, we focus on PanoHead [4], which excels in synthesizing images from a 360-degree perspective. We establish applicable baselines using PanoHead, highlighting the challenges of stylizing 360-degree heads. Our findings demonstrate that most of the current methods fall short of effectively achieving comprehensive and cohesive stylization across all angles. This limitation arises because many of these methods rely on stylized images that are primarily available in near-frontal views that fine-tuned StyleGAN models generate [1, 50, 61].\nThere are other works that leverage text-based image diffusion or CLIP models to either generate datasets or guide training [5, 25, 26, 29, 51] providing more flexibility, especially for 3D generators. Especially, the Score Distillation Sampling (SDS) algorithm [45] has demonstrated its effectiveness in providing sufficient training feedback for the stylization of 3D portraits [29, 51]. It leverages the insights from text-based diffusion models to propagate signals through backpropagation. However, it has been noted that the SDS technique can result in a loss of diversity in the generated outputs [29]. For example, when the generator is fine-tuned for a Joker or Pixar style, the outputs from random samplings tend to exhibit significant similarity. Even though DiffusionGAN3D [29] introduces a relative distance loss to mitigate diversity loss, we find that its outputs, along with those from other competing methods, tend to be similar for different inputs. This results in challenges with identity preservation, as illustrated in Fig. 1. The outputs tend to be similar across different inputs, resulting in a loss of the unique identity of the original images.\nBased on this observation, we propose a framework that achieves 3D head stylization with identity preservation in this work. Specifically,\n\u2022 We propose using distillation with negative log-likelihood distillation (LD) [20] for domain adaptation of 3D-aware image generators, yielding sharper and more ID-preserving results compared to SDS [45].\n\u2022 We propose rank weighing for score tensors on latent channels to regularize the LD gradients, which we show achieve better input color and ID preservation in the distillation process.\n\u2022 In compliance with the 3D GAN architecture [4], we extend LD with multi-view grid and mirror score gradients for improved stylization quality. Noting the importance of super-resolution (SR) networks in style-based GANs in domain adaptation, we avoid distilling grid scores to SR layers to further improve stylization.\nOur method has shown significant qualitative and quantitative improvements over the relevant head stylization methods and provides important insights into the distillation from diffusion to GAN backbones."}, {"title": "2. Related Work", "content": "3D-aware generators. Generative Adversarial Networks (GANs), when integrated with differentiable renderers, have made significant advancements in generating 3D-aware images consistent across multiple views. While earlier methods focus on mesh representations [11, 12, 15, 41], the latest innovations leverage implicit representations [9, 14, 36, 39]. Among implicit representations, triplane representations have become particularly popular due to their computational efficiency and the high quality of the generated outputs [4, 8]. The architectures of models like EG3D [8] and PanoHead [4] are reminiscent of the StyleGAN2 structure [24]. These frameworks consist of mapping and synthesis networks that create triplanes, which are then projected into 2D images through volumetric rendering processes similar to those used in NeRF [34]. While EG3D is trained on the FFHQ dataset [23] with limited angle diversity, PanoHead allows for a 360-degree perspective in face generation, attributed to its dataset selection and model enhancements. In our work, we investigate PanoHead to achieve 3D portrait stylization. We are interested in 3D portrait stylization of real input images, which requires embedding images into PanoHead's latent space. This task is referred to as image inversion and extensively studied for StyleGAN generators [2, 3, 42, 46, 58, 59] and recently for 3D-aware GAN models [6, 7, 30, 54, 60].\nAdopting image generators to new domains. A popular method for stylizing 2D and 3D portraits involves fine-tuning generators initially trained to produce realistic faces. This process, known as domain adaptation of generators, utilizes the generators' expertise in synthesizing real images. It can be accomplished using either a small number of samples for each style [31, 37] or by employing text-guided models to direct the training process [5, 25, 26, 29, 51]. Using text-based models to guide stylization offers the advantage of generating styles based on various prompts, which can be challenging to sample directly. These models, trained on extensive datasets, effectively acquire knowledge in a disentangled manner, enabling them to produce meaningful results for intriguing prompts that may not have corresponding data available [38, 44, 48]. Moreover, this guidance can be provided from multiple poses for 3D-aware generators, allowing for flexibility beyond the poses in the collected datasets. While previously, CLIP loss has been a popular choice for the guidance [19, 35, 49], more recently, the Score Distillation Sampling (SDS) algorithm has demonstrated impressive performance [29, 45, 51]. However, previous research indicates that when generators are fine-tuned solely using SDS losses, they may collapse into producing a fixed image pattern, irrespective of the input noise [29, 51]. To promote diversity, both StyleGANFusion [51] and DiffusionGAN3D [29] introduce regularizers aimed at enhancing variety. However, we observe that this diversity remains constrained, leading to issues with identity preservation, where different inputs yield similar outputs. In this work, we propose a framework that enables 3D portrait stylization guided by text prompts while maintaining identity preservation."}, {"title": "3. Method", "content": "Our method involves fine-tuning the pre-trained parameters of PanoHead to adapt it for generating images from different domains. This section will go through the development of the method. Specifically, we will explain the adoption of LD to 3D-aware image generators, mention the differences between SDS, extend LD with cross-pose dependencies and grid denoising, and use rank reduction on score tensors.\n3.1. Preliminaries\nDenoising diffusion probabilistic models (DDPM). In the forward process of DDPM [17], Gaussian noise \u20ac is gradually added to the initial data point xo over time steps t, reaching a pure Gaussian N as t \u2192 T (Eq. (1)). Since we use a latent diffusion model [47], we denote image latent vectors with x for simplicity. In the reverse process, the denoising UNet \u00ea is trained to predict the noise \u20ac added at each time step t with the MSE objective (Eq. (2)).\n$\\sqrt{a_t}x_0 + \\sqrt{1 - \\bar{a}_t}\\epsilon = x_t, \\epsilon \\sim N(0, I)$ (1)\n$\\mathbb{E}_{x_0,\\epsilon,t}\\{\\|\\epsilon - \\epsilon(x_t,t)\\|_2^2\\}$ (2)\nScore-based stochastic differential equations (SDE). In the score-based SDE framework [52], the forward process (Eq. (3)) adds Gaussian noise to x over time, where g(t) = $\\sqrt{1 \u2013 a_t}$ is the noise scale and w is Wiener process. The reverse process (Eq. (4)) is modeled as a reverse-time SDE and denoises x by moving in the direction of higher probability density, where \u2207xt log p(x|y) is the score function. We keep the drift terms f = 0 for simpler notation.\ndxt = g(t)dW (3)\ndxt = -g(t)^2x + \u2207xtlogp(x|y)dt + g(t)dW (4)\nThe relation between the score function and the noise prediction can be described in Eq. (5):\n\u2207xt log p(x|y) \u2248 $-\\frac{\\hat{\\epsilon}(x_t, t, y)}{g(t)}$ (5)\nPanoHead. PanoHead [4] generates hybrid 3D representations called triplanes with a StyleGAN-like architecture which takes 512-dimensional style vectors and outputs 256 \u00d7 256 resolution triplanes. These triplanes are rendered from a specified camera pose using a neural volumetric renderer to synthesize a 2D head image with size 64 \u00d7 64. Finally, a convolution-based super-resolution (SR) network performs upscaling from 64 \u00d7 64 images to 512 \u00d7 512 final images to refine details. Since the SR network upscales significantly, it plays a critical role in maintaining geometric consistency while prioritizing a rich texture and color.\n3.2. ID-Preserving Stylization with Distillation\nLikelihood distillation (LD) objective. We adopt a part of the distillation procedure in [20] explained in this section briefly. A detailed derivation is provided in Supplementary for completeness.\nAssume that distribution (q) of the 3D representation (\u03b8) conditioned on text prompt (y) is proportional to the prompt-conditioned distribution (p) of independent 2D renders (xi) on different poses (i):\nq(\u03b8|y) \u221d p(x1, x2,..., xN|y) = \u220fp(xi|y) (6)\nWe optimize negative log-likelihood of Eq. (6) to find \u03b8:\n$\\text{-log }q(\\theta |y) = \\text{-log }\\prod_i p(x_i | y) = \\sum_i \\text{-log }p(x_i | y)$ (7)\nDefine the loss LLD and find gradient \u2207\u03b8 to update \u03b8 via gradient descent:\n$\\nabla_{\\theta} L_{LD} = -E\\{\\nabla_{\\theta}\\text{log }p(x_i |y)\\}$ (8)\nCombining Eqs. (1) and (8) results in:\n$\\nabla_{\\theta}\\text{log }p(x_i |y) = \\sqrt{a_t}\\nabla_{x_t} \\text{log }p(x_t|y) \\frac{\\partial x_t}{\\partial x_0} \\frac{\\partial x_0}{\\partial \\theta}$ (9)\nwhere \u2207xt log p(xt|y) is the score function estimation. Plugging Eq. (9) into Eq. (8) yields the update direction:\n$\\nabla_{\\theta} L_{LD} = -E_{\\pi,x_t}\\{\\nabla_{x_t} \\text{log }p(x_t |y) \\frac{\\partial x_t}{\\partial x_0} \\frac{\\partial x_0}{\\partial \\theta}\\}$ (10)\nwhere $\\frac{\\partial x_t}{\\partial x_0} = \\sqrt{a_t}$ from Eq. (1). Notice that to update \u03b8, we do not need to back-propagate through the denoiser and can acknowledge the output as a part of the gradient.\nRank weighted score tensors. While LD reduces the smoothness issues, we come across some artifacted results, especially around the hair, ears, and neck on some prompts (Fig. 3, full-rank). Notice that the artifacts are more focused on incorrect color distribution rather than the style itself. Inspired by previous work utilizing feature decomposition techniques [21, 27] and expecting a disentanglement, we investigate the SVD of the score tensors along the VQ-VAE latent channel (4) dimension. We notice that the first rank contains most of the stylization and conclude that surpassing the contributions of the lower three ranks can mitigate undesired tints. Hence, we re-weigh the diagonal singular value matrix S with linearly decaying coefficients W from the largest singular value to the smallest. Specifically,\n$UEV^T = SVD(\\nabla_{x_t} \\text{log }p(x_t |y))$ (11)\n$\\nabla_{x_t} \\text{log }p(x_t |y) = U\\Sigma W V^T$\np is the rank-weighted score distribution, \u03a34\u00d74 diag(\u03c31, \u03c32,..., \u03c34), U4\u00d74 = [u1, u2,..., u4], V4096\u00d74 = [v1, v2,..., v4], and W = diag(1, 0.75, 0.5, 0.25).\nAs seen in Fig. 3, rank-weighted score tensors eliminate the problematic tints and set a good baseline for the following improvements. We set the weight scores based on our empirical analysis, and the same weights are used for the training of all prompts.\nExtending LD via cross-dependencies with mirror poses. The expression in Eq. (10) does not consider the correlation between different views. To add cross-dependencies, we account for another poses where \u03c0' \u2260 \u03c0 in Eq. (12):\n$\\nabla_{\\theta} L_{LD} = -E_{\\pi,x_t}\\{\\nabla_{x_t} \\text{log }p(x_t |y) \\frac{\\partial x_t}{\\partial x_0} \\frac{\\partial x_0}{\\partial \\theta} + \\sum_{\\pi\\neq \\pi'} \\frac{\\partial \\text{log }p(x_{\\pi'}|y)}{\\partial x_{\\pi'}}\\frac{\\partial x_{\\pi'}}{\\partial x_0} \\frac{\\partial x_0}{\\partial \\theta} \\}$ (12)\nNotice that $\\frac{\\partial \\text{log }p(x_{\\pi'}|y)}{\\partial x_{\\pi'}} = \\nabla_{x_{\\pi'}} \\text{log }p(x_{\\pi'}|y)$ in Eq. (12):\n$\\nabla_{\\theta} L_{LD} = -E_{\\pi,x_t}\\{\\nabla_{x_t} \\text{log }p(x_t |y) \\frac{\\partial x_t}{\\partial x_0} \\frac{\\partial x_0}{\\partial \\theta} + \\sum_{\\pi\\neq \\pi'} \\nabla_{x_{\\pi'}} \\text{log }p(x_{\\pi'}|y) \\frac{\\partial x_{\\pi'}}{\\partial x_0} \\frac{\\partial x_0}{\\partial \\theta} \\}$ (13)\nBy utilizing the symmetry prior for human heads, we assume that if \u03c0 and \u03c0' are yaw-symmetric camera matrices, x\u03c0\u2032 = M(x\u03c0) where M is the vertical mirror (flip) operator. Then, by Eq. (1),  simply becomes M\u221aat. Further simplifying the expression Eq. (13) yields:\n$\\nabla_{\\theta} L_{LD} = -E_{\\pi,x_t}\\{\\nabla_{x_t} \\text{log }p(x_t |y) \\frac{\\partial x_t}{\\partial x_0} \\frac{\\partial x_0}{\\partial \\theta} + \\nabla_{x_{\\pi'}} \\text{log }p(x_{\\pi'}|y) \\frac{\\partial x_{\\pi'}}{\\partial x_0} \\frac{\\partial x_0}{\\partial \\theta} \\}$ (14)\nAs shown in Fig. 2 (a), this intuitively means that we utilize the same score estimation for mirror poses but must also mirror the gradients while back-propagating to the generator. Since 3D-consistent face domain editing with 2D-diffusion models is challenging, keeping the same score estimation for different renders of the same ID under logical constraints avoids deviating from the convergence path. This is because there is no guarantee that \u2207xt log p(xt|y) and \u2207x\u03c0\u2032log p(x\u03c0\u2032|y) will indicate the same direction for the same y.\nNotice the mirror approach can be extended to any render pose tuple (\u03c0, \u03c0\u2032) as long as we can find a tractable gradient chain for \u03c0\u2032. Our experiments show that mirror gradients further improve stylization while accentuating 3D-aware features like glasses (Fig. 8).\nMulti-view distillation via grid denoising. Mirror gradients only correlate yaw-symmetric poses. Now, let us consider the joint probability distribution p and not assume explicit independence among poses, yielding Eq. (15):\nq(\u03b8|y) \u221d p(x0, x1,..., xN|y) = p({xi}|y)\n$\\nabla_{\\theta} L_{LD_g} = -E_{\\pi,\\{x_t\\}}\\{\\nabla_{\\{x_t\\}}\\text{log }p(\\{x_t\\}|y) \\frac{\\partial \\{x_t\\}}{\\partial \\theta}\\}$ (15)\nwhere {xi} is defined to have all poses {0, 1,..., N}. However, this is not computationally feasible. Instead, as shown in Fig. 2 (b), we approximate {xi} with a 2\u00d72 grid, where each element is a xi with different render pose \u03c0. This way, denoising UNet can correlate between different renders of \u03b8, improving stylization consistency across views. We further employ a depth-conditioned ControlNet [62] as a regularizer to ensure the estimated score does not collapse to a single fused image.\nOur approach does not fine-tune any diffusion model to accommodate multiple inputs [32, 33, 57] for multi-view consistency. Instead, it can use any pre-trained diffusion model and re-purposes grid structures for distillation task, first proposed for video generation [28] and editing [22] with spatiotemporal consistency.\nEffect of super-resolution network in style-based 3D GANs. The PanoHead model generates images at a resolution of 512 \u00d7 512 [4], and the diffusion model similarly processes 512\u00d7512 images when using LD-losses [47]. For grid construction, we arrange four images in a 2 \u00d7 2 layout but reduce the individual image size to 256 \u00d7 256 to avoid memory issues during forward passes. This way, the final grid image has the size of 512 \u00d7 512. Although this maintains a good correlation between poses and consistent stylization, it creates a resolution mismatch when gradients are directly propagated from the PanoHead output. To resolve this, we experiment with feeding the gradients before the SR network, where the renderer outputs images at a lower resolution of 64 \u00d7 64. This adjustment is only necessary for multi-view distillation, where we backpropagate \u2207\u03b8LLDg before the SR layers. For mirror poses, we do not need to skip the super-resolution layers since they match the correct resolution, and so we backpropagate \u2207\u03b8LLD after the SR, as visualized in Fig. 2. This strategy improves the stylization quality and reduces unwanted artifacts, such as blur, over-saturation, and color tint."}, {"title": "4. Experiments", "content": "Baselines. We include various domain adaptation methods for our comparisons and adopt all of them for the PanoHead generator. StyleCLIP [40] trains a W+ mapper network with the CLIP loss. StyleGAN-NADA [13] employs the CLIP objective to train adaptively-selected generator layers rather than modifying W+. StyleGANFusion [51] utilizes SDS for domain adaptation and a directional regularizer via the frozen generator for more stable distillation. DiffusionGAN3D [29] further improves the regularization of [51] by employing a relative distance loss. After tuning the generators, for inference, we invert images by optimizing W+ to reconstruct the input image with the original PanoHead generator. The optimized W+ is then fed to these different domain-adapted generators, and rendering from various views is outputted. Details of the baseline implementations are detailed in Supplementary.\nTraining setup. We train the generator with synthetic \u223c1\u00d7512 z \u223c N(0, I) data for 10k iterations with batch size 1, where the truncation parameter of the generator's mapping network is \u03c8 = 0.8. The optimizer is Adam with a learning rate 1e-4. The CFG weight and ControlNet guidance weight are set to 7.5 and 1.0, respectively. Depth ground truths are extracted from [55]. More details of the training recipe are provided in Supplementary.\nMetrics. For stylization performance, we measure Fr\u00e9chet Inception Distance (FID) [16] and CLIP embedding similarity; for ID preservation, we measure ArcFace-based [10] ID similarity and multi-view render depth L2 difference \u2206D. To construct our test set, we perform W+ inversion to randomly chosen \u223c100 FFHQ images. These W+'s are input\nin Fig. 8 to provide a clearer explanation of the method, so we omit that discussion here. Next, we add the grid-based denoising, which provides multi-view consistent distillation. We observe that the multi-view grid gradients improve the results even when we feed them after the super-resolution layers, but if we feed the gradients to the generator before the super-resolution network, the results improve further both quantitatively and qualitatively, as shown in Tab. 2 and Fig. 8, respectively. Finally, we incorporate cross-dependencies with mirror poses, enhancing intricate details, as evidenced by the zoomed-in views.\nResults with other prompts. We present additional results with various prompts in Fig. 9. These prompts include local stylization, such as adding a mustache or editing hair to pink. For these experiments, we do not employ attribute-based masking for score gradients (for example, we do not mask the face when editing the hair). Despite this, our method successfully achieves both localized edits and more global transformations, such as modifying the character to resemble an elf under a single framework."}, {"title": "5. Conclusion", "content": "We introduced a novel approach to 3D head stylization that improves identity preservation and stylization quality. By utilizing the PanoHead model for 360-degree image synthesis and incorporating negative log-likelihood distillation (LD), mirror and grid score gradients, and score rank weighing, our method overcomes the limitations of previous approaches that struggled with identity retention. Experimental results show significant qualitative and quantitative improvements, advancing the state of 3D head stylization. Our work also provides valuable insights into effective distillation processes between diffusion models and GANs, emphasizing the importance of identity preservation in stylization tasks."}, {"title": "A. LD objective", "content": "This section will go through the detailed derivation.\nRecall the DDPM forward process:\n$\\sqrt{a_t}x_0 + \\sqrt{1 - \\bar{a}_t}n = x_t, n \\sim N(0,I)$ (16)\nAssume that distribution (q) of the 3D representation (\u03b8) conditioned on generation prompt (y) is proportional to the prompt-conditioned distribution (p) of independent 2D renders (xi) on different poses (i). In our setup, \u03b8 is the style-based 3D GAN layers.\nq(\u03b8|y) \u221d p(x1, x2,..., xN|y) = \u220fp(xi|y) (17)\nWe optimize negative log-likelihood of Eq. (17) to find \u03b8:\n$\\text{-log }q(\\theta |y) = \\text{-log }\\prod_i p(x_i | y) = \\sum_i \\text{-log }p(x_i | y)$ (18)\nDefine the loss LLD as the average of infinitely many N render poses and find gradient \u2207\u03b8 to update \u03b8 via gradient descent, where \u03c0 is any given pose:\n$L_{LD} = - \\frac{1}{N}\\text{ lim }\\sum_{N \\rightarrow \\infty} \\text{log }q(\\theta | y) = -E\\{\\text{log }p(x_i | y)\\}$ (19)\n$\\nabla_{\\theta} L_{LD} = -E\\{\\nabla_{\\theta}\\text{log }p(x_i |y)\\}$\nUsing Eqs. (16) and (19) and change of variables in probability:\np(xi|y) = p(xt|y) $\\frac{\\partial x_t}{\\partial x_0} \\frac{\\partial x_0}{\\partial \\theta}$ (20)\nTake the log of both sides, the partial derivative with respect to x\u03b8, and decompose the right-hand side with chain rule using the relation in Eq. (16):\n$\\text{log }p(x_i |y) = \\text{log }p(x_t |y) - \\text{log }\\sqrt{a_t}$ (21)\n$\\frac{\\partial \\text{log }p(x_i |y)}{\\partial x_0} = \\frac{\\partial p(x_t |y)}{\\partial x_t} \\frac{\\partial x_t}{\\partial x_0}$\n$\\nabla_{x_0} \\text{log }p(x_i |y) = \\nabla_{x_t} \\text{log }p(x_t |y) \\frac{\\partial x_t}{\\partial x_0}$\nExtend the partial gradient chain in Eq. (21) to \u03b8 from x\u03b8:\n$\\frac{\\partial x_0}{\\partial \\theta}$ (22)\n$\\nabla_{\\theta} \\text{log }p(x_i |y) = \\nabla_{x_t} \\text{log }p(x_t |y)$\nwhere \u2207xt log p(xt|y) is the score function estimation. Plugging Eq. (22) into Eq. (19) yields the update direction:\n$\\nabla_{\\theta} L_{LD} = -E_{\\pi,x_t}\\{\\nabla_{x_t} \\text{log }p(x_t |y) \\frac{\\partial x_t}{\\partial x_0} \\frac{\\partial x_0}{\\partial \\theta} \\}$ (23)"}]}