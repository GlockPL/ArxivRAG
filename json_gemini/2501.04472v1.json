{"title": "Hybrid Artificial Intelligence Strategies for Drone Navigation", "authors": ["Rub\u00e9n San-Segundo", "Luc\u00eda Angulo", "Manuel Gil-Mart\u00edn", "David Carrami\u00f1ana", "Ana M. Bernardos"], "abstract": "Objective: This paper describes the development of hybrid artificial intelligence strategies for drone navigation. Methods: The navigation module combines a deep learning model with a rule-based engine depending on the agent state. The deep learning model has been trained using reinforcement learning. The rule-based engine uses expert knowledge to deal with specific situations. The navigation module incorporates several strategies to explain the drone decision based on its observation space, and different mechanisms for including human decisions in the navigation process. Finally, this paper proposes an evaluation methodology based on defining several scenarios and analyzing the performance of the different strategies according to metrics adapted to each scenario. Results: Two main navigation problems have been studied. For the first scenario (reaching known targets), it has been possible to obtain a 90% task completion rate, reducing significantly the number of collisions thanks to the rule-based engine. For the second scenario, it has been possible to reduce 20% of the time required to locate all the targets using the reinforcement learning model. Conclusions: Reinforcement learning is a very good strategy to learn policies for drone navigation, but in critical situations, it is necessary to complement it with a rule-based module to increase task success rate.", "sections": [{"title": "1. Introduction", "content": "Nowadays, the use of Unmanned Aerial Vehicles (UAVs) or drones has increased significantly in many domains, including military and civil applications [1,2]. Civil ap-plications include aerial surveillance, parcel delivery, precision agriculture, intelligent transportation, search and rescue operations, post-disaster operations, wildfire manage-ment, remote sensing, and traffic monitoring [3]. These developments have been possible thanks to a fast deployment of radio communication interfaces, sensors, device minia-turization, global positioning systems (GPSs), and artificial intelligence (AI) techniques. Using drones has important advantages such as cost effectiveness, fast mobility, and easy deployment [4], but also must face relevant challenges like scalability or computer power consumption [5].\nArtificial intelligence and machine learning (ML) algorithms are having an important development [6] and a significant impact across a great variability of sectors ranging from healthcare [7], smart cities [8], natural language processing and human-computer interac-tion [9], to transportation and logistics [10]. Similarly, these algorithms have an important role in drone applications because ML enhances drones' capabilities in navigation [11], object detection [12], or mission planning [13].\nThis paper is focused on the development and evaluation of hybrid artificial intelli-gence strategies for drone navigation in simulated environments. The navigation problem"}, {"title": "2. System Description and Navigation Tasks", "content": "The next figure (Figure 1) represents a diagram of the multiagent system, including the main modules.\nThe global system is composed of two main modules:\n\u2022\tThe agent (drone) module generates the next drone action using the local observation of the agent. This process selects the drone's action proposed from the rule-based module or the deep learning model, depending on the agent state. The input of this module is the local observation space (a 20\u00d720 submatrix, explained in Section 3.1).\n\u2022\tThe output is the next drone's action. The agent can perform six possible actions in a 3D environment (forward, backward, left, right, up, down) or four in a 2D environment (forward, backward, left, right). The agents/drones perform their action in sequence considering a specific order. When all the agents have executed a new action, the system has completed one cycle.\n\u2022\tThe simulated environment is a global representation of the scene including drones, targets, and obstacles. The environment includes two agents (drones) that must complete different drone navigation tasks considering a configurable number of targets (in green circles) and avoiding several obstacles (represented with bombs in black). An example is shown in Figure 2. Every object includes a Z coordinate in black, indicating its height. This way, it is possible to simulate 2D and 3D navigation scenarios. The inputs to this module are the drones' actions, and the outputs are the new environment state and the reward associated with every action. This reward is used only during training.\nAdditionally, the system integrates several explainability mechanisms and human interaction strategies (green module). The explainability mechanisms are more focused on helping with the algorithm development while the human interaction strategies"}, {"title": "2.1. Reaching Located Targets", "content": "In this task, the drones must navigate through an environment to reach all the targets. The simulated environment is a global representation of the scene including drones, targets, and obstacles The drones know the target coordinates, but they do not know the whole environment; drones only see a local observation of the environment. The scenarios evaluated in this task are as follows:\n\u2022\tConsidering static or moving targets without obstacles. In the case of moving targets, they try to reach the bottom of the screen to escape from the drones.\n\u2022\tStatic targets including obstacles. Several obstacles are included, and the drones must avoid them to reach the targets.\n\u2022\tConsidering a 3D navigation problem with static targets and a different number of obstacles. A third dimension increases the flexibility of the drone's movements."}, {"title": "2.2. Searching for Targets", "content": "In this task, the drones must search the environment looking for targets. The drones do not know the targets' coordinates and they only see a local observation space. The objective is to find all the targets as soon as possible, with the lowest number of movements or cycles.\nThe targets are distributed randomly in the game scene but using specific patterns. Considering these patterns, it is possible to train specific search strategies to reduce the time to discover all the targets. In this work, the targets are organized in groups randomly distributed in the scene. Every group occupies a small zone, and the targets are also randomly distributed inside the group zone. These small areas are defined as squares with a size of 20% of the total width and height. The analyzed scenarios are as follows:\n\u2022\tConsidering static or moving targets. In the case of moving targets, when one target is detected, the rest try to reach the bottom of the screen to escape from the drones."}, {"title": "3. Materials and Methods", "content": "This section describes in detail the algorithms implemented in the module of the drone navigation system. Additionally, a description of the data used for training and evaluating the different proposals is included."}, {"title": "3.1. Simulated Environment", "content": "The simulated environment has been developed based on the PettingZoo environment definition [28]. PettingZoo is a simple interface, developed in Python, able to represent gen-eral multi-agent reinforcement learning problems. PettingZoo includes many environments as examples, and tools for designing custom environments.\nThe 2D environment state space is represented as a 200200 matrix with different values per cell: 4.0 values are assigned to forbidden areas (obstacles and game margins to avoid collisions) (Figure 3). In the case of a 3D environment, the environment state space is a 3D matrix."}, {"title": "Reward strategy", "content": "The reward strategy includes the following two main aspects:\n\u2022\tTarget reached by an agent or drone: when a target is reached by an agent, the corre-sponding agent's reward is increased by 1, emphasizing the importance of reaching a target.\n\u2022\tPartial rewards depending on the distance to the target: An additional reward based on the agent's proximity to the target has been added. This reward is associated with the distance reduction between the drone and the target. Once the drone executes a movement, its reward increases according to the distance (to target) reduction. It is important to remark that these partial rewards are only applicable when the drone knows the target coordinates. These partial rewards introduce a reward-shaping mechanism that encourages agents to reduce the distance to their target. Including these partial rewards allows significantly reducing the training time."}, {"title": "Distance to the target", "content": "The following two alternatives have been analyzed to compute the distance to the target:\n\u2022\tThe first one consists of computing the minimum Euclidean distance between two points in the environment matrix. This distance generates frequent problems when dealing with obstacles. During training, the agent learns to take the shortest path to"}, {"title": "3.2. Agent Module", "content": "The main target of this module is to define the next action according to the drone navigation policy. The next drone action is selected from the rule-based module or from the deep learning model depending on the drone state and the local observation of this drone. The local observation of a drone is represented by a 220 submatrix of the state space (the number of 2D or 3D dimensions depends on the scenario), centered in the agent (Figure 5). Every cell in this matrix includes the reward that the agent would obtain if moving to this point: -1.0 values are assigned to forbidden areas (margins and obstacles).\nIn this figure, the center is marked with a -1.0 (only for representation)."}, {"title": "3.2.1. Task 1: Reaching Located Targets", "content": "In this task, the drones must navigate to the different targets until reaching all of them. Every drone is assigned a specific target that it must reach (the closest target). When a target is reached, it is marked as seen and the drone is reassigned a new target (the next closest one). It is important to comment that when there are more unseen targets than agents (drone), every drone has different assigned targets to parallelize the task resolution.\nFor drone navigation in this task, there are two main agent states: the normal naviga-tion state (first state) and obstacle detected in the path to the target (second state)."}, {"title": "First state: normal navigation", "content": "In the normal navigation state, the policy is defined by a deep learning module trained using a reinforcement learning (RL) algorithm. Stable-Baselines3 [29] is the RL toolkit used in this work because of the large amount of available RL algorithms and its flexibility in the integrations process. After an initial analysis of the different RL algorithms, PPO [30] was chosen because it provides a very good compromise between training time and performance and offers a good stability during the training process. PPO iteratively improves the agent's policy to maximize cumulative rewards. Its main advantage lies in balancing exploration and exploitation by judiciously constraining policy updates.\nIn this paper, the PPO-Clip version has been used from the Stable-Baselines3 [29] toolkit. This library implements PPO-Clip with a clip range parameter equal to 0.3. The clipping parameter is a function of the current progress remaining (a value between 0 and 1). This parameter was set to 0.3 following the suggestion of the Stable-Baseline3 library. This value is a good compromise for a wide range of applications. Additionally, several prelaminar experiments were conducted while modifying this parameter, and the best results were obtained with this proposed value. PPO-Clip performs better than PPO-Penalty because in this implementation, the new policy would not update so far away from the old policy, avoiding oscillations during the learning process.\nThe policy is implemented based on a deep learning architecture where the input is the local observation space (representation of the context observed by the agent) and the output is an array of probabilities for all the possible actions or movements of the agent: forward, backward, left, right, up, and down (6 possible actions).\nThis deep learning architecture is composed of two main parts (Figure 6):\n\u2022\tA feature extractor for extracting features from high-dimensional observations. This module includes two CNN layers with 32 kernels and ReLU functions.\n\u2022\tA fully connected network that maps the features to actions, including two fully connected layers with 128 neurons each and a final layer with P outputs and the Softmax function (classification). The number of outputs is 6 (forward, backward, left, right, up, down) or 4 for 2D scenarios (forward, backward, left, right)."}, {"title": "Second state: avoiding an obstacle in the path to the target", "content": "Although a new distance was defined to provide certain flexibility to the drone path, there are situations where the drone cannot find an alternative path without obstacles. These cases are automatically detected by the drone when the drone repeats several movements in a cycle without decreasing the distance to the target. In this circumstance, the drone understands that it was not able to find an alternative path without obstacles and starts executing the avoiding strategy based on rules. In this state, the agent is forced to go around the obstacle, describing a circumference as shown in Figure 7. This behavior is provoked by changing the agent target; the system generates a sequence of fictitious targets that go around the obstacle."}, {"title": "3.2.2. Task 2: Searching for Targets", "content": "In this scenario, the drones must search the environment looking for targets. The drones do not know the targets' coordinates and they only see a local observation space. The objective is to find all the targets as soon as possible, with the lowest number of movements or cycles.\nThe targets are distributed randomly in the game scene but follow specific patterns. Considering these patterns, it is possible to train specific search strategies to reduce the time to discover all the targets. In this work, the targets are organized in groups randomly distributed in the scene. Every group occupies a small zone, and the targets are also randomly distributed inside the group zone.\nFor drone navigation in this task, the system considers three main agent states: ex-haustive search (state 0), local search around the last target detected (state 1), and obstacle detected during local search (state 1.1)."}, {"title": "First state: exhaustive search", "content": "All the agents (drones) start doing an exhaustive search. The navigation is guided by a rule-based module. During the exhaustive search, the drone movements are defined by a set of expert rules (Figure 9), trying to cover the whole environment in vertical paths like in Figure 10."}, {"title": "Second state: local search around the last target detected", "content": "When one target is detected, the drone stops the exhaustive search, and a reinforcement learning model guides the drone movements to search in the local area, close to the first discovered target. This model has been trained according to the pattern commented above: the targets are organized in several groups. The main aspects while training the model are the following:\n\u2022\tFor the drone observation space, the same observation space used for the previous scenario (reaching targets) has been considered: a 20\u00d720 submatrix of the state space, centered in the agent, including for each cell the reward that the agent would obtain moving to this point (1.0 values are assigned to forbidden areas, scene, and object margins). In this case, the first detected target is the reference to compute the agent_target distances. In the case of having several detected targets in the same zone, the reference for computing the distance is the average target position.\n\u2022\tFor the reward strategy, the system computes a distance based on the vertical and horizontal movements between the current position of the drone and the reference, defined as the average position along the already discovered target.\nWhen one target is detected by a drone, only this drone focuses on this local zone while the rest maintain an exhaustive search for dealing with other groups of targets. When a drone spends some time looking in a local zone without success, it changes its behavior to an exhaustive search again (state 0). This change is necessary to provide the possibility to search for several groups with the same drone."}, {"title": "Third state: avoiding an obstacle in the path to the target during the local search", "content": "Finally, it is important to comment that the same rule-based strategies for avoiding obstacles during the local search are also integrated in this case."}, {"title": "3.3. Combination of Deep Learning and Rule-Based Policies", "content": "This section summarizes the combination of deep learning and rule-based policies based on the navigation task. This combination is based on states depending on the following tasks:\n\u2022\tTask 1: reaching located targets:\nFirst state: normal navigation using the deep learning policy learnt using reinforcement learning.\nSecond state: Avoiding an obstacle in the path to the target. The rule-based engine is used to define fictious targets that allow avoiding the obstacle. The drone is maintained in the second state while it cannot find a direct path to the target. This situation is detected when the drone performs repetitive movements in sequence.\n\u2022\tTask 2: searching for targets:\nFirst state: Exhaustive search. Using the rule-based engine for covering the whole area.\nSecond state: Local search around the last target detected. When the first target is found, the drone changes to the second state and the deep learning architecture is used for navigation. The target is to find close targets in the local area of the first detected target. After a certain time without finding new targets, the drone returns to the first state.\nThird state: Avoiding an obstacle in the path to the target during the local search. Like task 1, the rule-based engine for obstacle avoidance is used when the drone cannot find a direct path to the target."}, {"title": "3.4. Explainability Mechanisms", "content": "In the system, two main explainability mechanisms have been integrated to help with the algorithm development: LIME (Local Interpretable Model-Agnostic Explanations) [31] and SHAP (SHapley Additive exPlanations) [32]."}, {"title": "3.4.1. Local Interpretable Model-Agnostic Explanations (LIME)", "content": "LIME is a technique that approximates any black box machine learning model with a local interpretable model to explain each individual prediction, providing an explanation of the decisions made by an agent in certain situations.\nFigure 11 shows the local observation, a matrix where the agent's observation space is represented. This figure shows two obstacles corresponding to the black blocks in the next image (bottom and right parts)."}, {"title": "3.4.2. SHapley Additive exPlanations (SHAP)", "content": "Additionally, the study also includes an analysis using SHAP to obtain more details about the explainability of the decisions made by the agents/drones. This method is based on game theory and uses Shapley values to compute contributions to each feature in the prediction of a model. These features are pixels when analyzing an image.\nSHAP explanations are often represented graphically using bar charts or scatter plots, where each bar or point represents the contribution of a specific feature to a particular prediction. In SHAP, the areas appearing in pink and blue on the graphs indicate the impact of the contribution of each pixel to the model prediction value. The interpretation of the colors in SHAP graphs is as follows:\n\u2022\tBlue: negative contribution (low): The blue areas represent features that contribute negatively to the model's prediction value. In a classification context, this could be interpreted as features that decrease the probability of the predicted class.\n\u2022\tPink: positive contribution (high): The pink areas represent features that contribute positively to the model's prediction value. In a classification context, this could be interpreted as features that increase the probability of the predicted class.\n\u2022\tColor intensity: magnitude of contribution. The color intensity (whether blue or pink) indicates the magnitude of the contribution of that specific feature. The darker the color, the greater the magnitude of the contribution.\nFigure 13 shows an example of a SHAP analysis with the actions sorted from more to less probability (left to right). The action with the highest positive contribution is action number 1 (move up), and the action with the highest negative contribution is action number 2 (move down).\nLIME and SHAP mechanisms have been used to evaluate the relevance of each point or pixel in the local observation space when selecting the drone action by the deep learning model. These mechanisms have provided information about the contribution of each pixel (positive or negative) and the intensity for each possible drone's action. For example, when detecting an obstacle ahead, the pixels showing the obstacle have a negative contribution for going in the same direction, but a positive contribution for the opposite action. Thanks"}, {"title": "3.5. Human Interaction Strategies", "content": "This section describes the functionalities incorporated to allow the human supervisor to interact with the agent, modifying its behavior. Additionally, some functionalities that help to understand the behavior of the agents (commented in the previous section) have also been incorporated. Most of the functionalities are only available when the supervisor pauses the navigation. These features are the following:\n\u2022\tSave the current state of the game: The current state of the game (agents, targets, and obstacles) can be saved at any moment. This current state also contains information about various aspects of the game, such as the score, the number of agents of each type, agent selection, dead agents, and rewards, among others. Several states can be stored in sequence.\n\u2022\tMove forward and backward in the stored state sequence; in this case, there is the possibility to go backward or forward along the state sequence.\n\u2022\tAdvance step by step the movement of the agents.\n\u2022\tPerform explainability analyses for the observations of the agents: LIME and SHAP.\n\u2022\tMove the agents manually: change the position (Figure 14) and the target (right side) of the agents (Figure 15).\n\u2022\tInformation window. The following image shows the visualization of the information window that shows the coordinates of each agent. These coordinates can be modified manually. Figure 16, at the bottom, shows the observation space of each agent and the explainability associated with each action (by pressing the LIME and SHAP buttons)."}, {"title": "3.6. Dataset", "content": "All reinforcement learning models developed in this work have been trained using specific data. The training dataset is composed of 900,000 episodes (or games) generated automatically, including more than 18 million drone movements or cycles. The system can generate different games considering the different initial positions of the drones, targets, and enemies. These positions are generated randomly, covering a wide range of possibilities.\nFor evaluating all the strategies in different scenarios, new games have been generated. The system has evaluated the hybrid strategies in every new scenario with 200 new games."}, {"title": "4. Evaluation Results and Discussions", "content": "This section describes the general evaluation methodology, and the analyses carried out for both tasks: reaching several located targets and searching for targets. It is important to remark that human interaction strategies (described in the previous section) have not been used during the system evaluation and testing. All the experiments have been carried out using the system automatically, and no human intervention has been considered."}, {"title": "4.1. Evaluation Methodology", "content": "The evaluation methodology is composed of two main phases. In the first phase, the reinforcement learning model is trained simulating many episodes in every task, considering several scenarios (Section 2). Every episode is defined by a setup of drones, targets, and obstacles organized in the scene.\nIn the second phase, the proposed system is evaluated by analyzing its behavior when dealing with new episodes generated automatically (different from those seen during the training phase). These episodes are generated for both tasks and different scenarios. The objective of considering several scenarios per task is to have a more complete analysis covering a wider range of possible applications. For each task, different evaluation metrics reporting results in different situations have been used to evaluate the navigation strategies.\nIn both scenarios, there are quality and time-processing metrics. The quality metrics provide information about the task success (percentage of episodes completely solved by the drones) and the incidence of several possible problems like obstacle collisions (times a drone hit an obstacle, for example). To complete the analysis, time-processing metrics like the number of cycles or steps carried out by the drones for solving the task have been considered. The reason for this analysis is because in real application, drones have a limited autonomy and it is necessary to optimize their movements; it is important to solve the task, but while using as few movements aspossible."}, {"title": "4.2. Task 1: Reaching Several Located Targets", "content": "In this task, the following metrics have been considered:\n\u2022\tQuality metrics: to analyze the task success and the incidence of possible problems.\nPercentage of episodes where all the targets were reached (task completion rate).\nPercentage of episodes where at least one agent hit an obstacle.\nPercentage of episodes where all agents hit an obstacle.\n\u2022\tTime-processing metrics: to analyze the drone movements required to complete the task.\nNumber of cycles used for training. One cycle is completed when all the agents/drones have executed a new action. These metrics allow evaluating the amount of training required to train a goodmodel.\nMaximum number of cycles per episode during testing. This limit simulates the situation of having a limited drone autonomy."}, {"title": "4.2.1. Static or Moving Targets Without Obstacles", "content": "This section includes the results associated with the initial situation where the agents (drones) must reach the targets without obstacles. The baseline system consists of consider-ing a simple reward strategy: the agents get a reward of 1.0, only when they reach a target. The rest of the movements do not provide any positive reward.\nThe next experiment includes partial rewards for each agent's movement (based on the distance reduction, as commented before). In this case, two situations have been evaluated: static targets and moving targets trying to reach the bottom of the screen. Table 1 includes the main results obtained when reaching targets without obstacles. The main parameters of the experimentation setup are as follows:\n\u2022\tReinforcement learning algorithm: PPO with a learning rate adjusted in preliminary experiments: 0.0003."}, {"title": "4.2.2. Static Targets Including Obstacles in Task 1", "content": "When an agent hits an obstacle, this agent dies and disappears from the scene. The baseline for these experiments is the best system developed without obstacles, evaluated in the previous section but including the obstacles. After that, the following several modifications were considered:\n\u2022\tModifying the reward strategy: in the next experiments, the reward strategy includes a negative reward when an agent hits an obstacle.\n\u2022\tModifying the agent-target distance to have alternative paths with the same distance.\n\u2022\tModifying the reward strategy (training) and target correction (testing): the rule-based engine controls that the agent does not hit any obstacle introducing a correc-tion strategy based on intermediate fictitious targets to surround the obstacle, as commented previously.\nTable 2 includes the main results obtained when reaching targets with obstacles in the game. The main parameters of the experimentation setup are the following:\n\u2022\tReinforcement learning algorithm: PPO with a learning rate adjusted in preliminary experiments: 0.0003.\n\u2022\t2D environment.\n\u2022\tTwo agents (drones).\n\u2022\tReward = 1 \u00d7 T (number of targets reached) \u2013 1 \u00d7 O (number of obstacles hit) AD (distance to target variation).\n\u2022\tFour static targets are randomly distributed.\n\u2022\tFour obstacles randomly distributed.\n\u2022\tTotal cycles used in training: 8 \u00d7 106.\nThe main conclusions from these experiments are the following:\n\u2022\tWhen including the obstacles, it is necessary to increase the number of cycles in training to better learn the agent policy (compared to Table 1).\n\u2022\tIncluding a negative reward when hitting an obstacle is crucial to reduce the number of situations where one drone hits an obstacle (second row).\n\u2022\tWhen considering alternative paths with the same distance, it is possible to increase the task success (percentage of episodes where all the targets are reached) from 35% to 57%.\n\u2022\tThe only way to guarantee that the agents do not hit any obstacle is by including expert rules to detect stuck situations and surround the obstacle. In this case, the task"}, {"title": "4.2.3. 3D Scenario with Static Targets and Different Number of Obstacles", "content": "The next experiments consider a 3D scenario. As shown, when including another dimension, the results improve for the same number of obstacles (four obstacles in the first two rows) because the drones have more possibilities to reach the targets in a 3D environ-ment. Figure 17 includes the principal results. The main parameters of the experimentation setup are the following:\n\u2022\tReinforcement learning algorithm: PPO with a learning rate adjusted in preliminary experiments: 0.0003.\n\u2022\t3D environment, including the Z component.\n\u2022\tTwo agents (drones).\n\u2022\tReward = 1 \u00d7 T (number of targets reached) 1 \u00d7 O (number of obstacles hit) - AD (distance to target variation).\n\u2022\tFour static targets randomly distributed.\n\u2022\tThe number of obstacles is variable, and they are randomly situated.\nThe main conclusions of these experiments are the following:\n\u2022\tAs shown, when adding the third dimension (Z coordinate) with only four obstacles, the results on the percentage of episodes reaching all targets improves, reaching a very good value. This value (around 92%) is difficult to improve because, as targets and obstacles are situated randomly, there is always the possibility to have a target situated in the security zone of an obstacle and it cannot be reached.\n\u2022\tAs predicted, when increasing the number of obstacles, the % of episodes reaching all targets decreases.\n\u2022\tAn interesting result is that when increasing the number of obstacles, the drones do not hit obstacles, only one case when considering 20 obstacles."}, {"title": "4.3. Task 2: Searching for Targets", "content": "This section presents the results obtained after evaluating the different strategies implemented for the second task: the agents (drones) must go over all the space searching for targets. This scenario includes several situations including or not including obstacles in the searching space and considering static or moving targets.\nThe main evaluation metrics considered in this study are the following:\n\u2022\tQuality metrics: to analyze the task success and the incidence of possible problems (like obstacles hits).\nPercentage of episodes where all the targets were reached.\nPercentage of episodes where at least one agent hit anobstacle.\nPercentage of episodes where all agents hit an obstacle.\n\u2022\tTime-processing metrics: to analyze the drone movements required to complete the task.\nNumber of cycles used for training. To evaluate the amount of training required to train a good model.\nAverage number of cycles required to complete an episode. This number is divided into several numbers depending on the system state: average number of cycles per episode during testing, average number of cycles per episode during an exhaustive search, and average number of cycles per episode while searching based on the learnt RL model.\nIt is important to remark that not all metrics have been used in all the experiments. In these experiments, considering an exhaustive search, it is possible to discover all tar-gets, so the most interesting performance metric is the reduction in the number of cy-cles/movements required to discover all the targets."}, {"title": "4.3.1. Static and Moving Targets Without Obstacles", "content": "The first scenario considers a searching problem with static targets and without obstacles. The targets are distributed randomly in the scene organized in groups. The groups are also randomly situated in the scene. Every group occupies a small local area, and the targets are also randomly distributed inside the group area. Table 3 includes the"}, {"title": "4.3.2. Static Targets Including Obstacles in Task2", "content": "Finally, this section describes the results when including obstacles in the searching mission with static obstacles (Table 5). The main parameters of the experimentation setup are the following:\n\u2022\tReinforcement learning algorithm: PPO.\n\u2022\tLearning rate: 0.0003.\n\u2022\t2D environment.\n\u2022\tTwo agents (drones).\n\u2022\tReward when using RL = 1 \u00d7 T (number offound targets) 1 \u00d7 O (number of obstacles hit) \u2013 AD (distance to the first found target in the local zone).\n\u2022\tFour static targets randomly distributed in several groups situated in zones with a size of 20% of the total width and height.\n\u2022\tFive obstacles randomly distributed in the scene."}, {"title": "5. Conclusions", "content": "This paper describes the development and evaluation of hybrid artificial intelligence strategies for drone navigation in simulated environments. The hybrid Al combines deep learning models with rule-based strategies to generate the agent action based on the agent state. The system has a high level of configurability to adjust the scenario difficulty, including a different number of targets or obstacles. This tool incorporates explainable strategies for analyzing agent decisions and human interaction facilities for correcting or modifying agents' behaviors by a humanoperator.\nFrom the experiments, the main conclusion is that hybrid AI, combining machine learning and rule-based engines, allows obtaining a very good compromise between per-formance and robustness. In the reaching scenario, the rule-based engine allows avoiding obstacles in a better way. For the searching scenarios, the exhaustive search based on expert rules has permitted the integration of RL models when the targets are located based on patterns that can be learnt using reinforcement learning."}]}