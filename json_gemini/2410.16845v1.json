{"title": "Fast Graph Sharpness-Aware Minimization for Enhancing and Accelerating Few-Shot Node Classification", "authors": ["Yihong Luo", "Yuhan Chen", "Siya Qiu", "Yiwei Wang", "Chen Zhang", "Yan Zhou", "Xiaochun Cao", "Jing Tang"], "abstract": "Graph Neural Networks (GNNs) have shown superior performance in node classification. However, GNNs perform poorly in the Few-Shot Node Classification (FSNC) task that requires robust generalization to make accurate predictions for unseen classes with limited labels. To tackle the challenge, we propose the integration of Sharpness-Aware Minimization (SAM)\u2014a technique designed to enhance model generalization by finding a flat minimum of the loss landscape\u2014into GNN training. The standard SAM approach, however, consists of two forward-backward steps in each training iteration, doubling the computational cost compared to the base optimizer (e.g., Adam). To mitigate this drawback, we introduce a novel algorithm, Fast Graph Sharpness-Aware Minimization (FGSAM), that integrates the rapid training of Multi-Layer Perceptrons (MLPs) with the superior performance of GNNs. Specifically, we utilize GNNs for parameter perturbation while employing MLPs to minimize the perturbed loss so that we can find a flat minimum with good generalization more efficiently. Moreover, our method reutilizes the gradient from the perturbation phase to incorporate graph topology into the minimization process at almost zero additional cost. To further enhance training efficiency, we develop FGSAM+ that executes exact perturbations periodically. Extensive experiments demonstrate that our proposed algorithm outperforms the standard SAM with lower computational costs in FSNC tasks. In particular, our FGSAM+ as a SAM variant offers a faster optimization than the base optimizer in most cases. In addition to FSNC, our proposed methods also demonstrate competitive performance in the standard node classification task for heterophilic graphs, highlighting the broad applicability. The code is available at https://github.com/draym28/FGSAM_NeurIPS24.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) have received significant interest in recent years due to their powerful ability in various graph learning tasks, e.g., node classification. Numerous GNNs have been developed accordingly [14, 19, 34]. Despite their successes, GNNs, like traditional neural networks, tend to be over-parameterized, often requiring extensive labeled data for training to ensure generalization. However, in real-world networks, many node classes have few labeled instances, which can lead to GNNs overfitting, resulting in poor generalization in these limited labeled classes. Recently, an increasing amount of research is focusing on developing superior GNNs, e.g., Meta-GCN [41], AMM-GNN [36], GPN [7] and TENT [37], for Few-Shot Node Classification (FSNC) which aims to classify nodes from new classes with limited labelled instances.\nIntuitively, training GNNs for FSNC requires robust model generalization ability for recognizing unseen classes from a small number of labelled examples. Motivated by the success of the recently proposed Sharpness-Aware Minimization (SAM) for improving models' generalization in the vision domain [11], we suggest incorporating SAM into training GNNs for addressing FSNC tasks. The core idea of SAM is to perturb the model parameters to find flat minima of the loss landscape, thereby making the model more generalizable. However, a key drawback of SAM is that it requires executing two forward-backward steps to complete one optimization step, resulting in twice the time consumption compared to general optimizers like Adam. Some works [8, 9, 22] have been proposed to accelerate SAM, but none of them are crafted for graphs, i.e., not leveraging the graph properties for accelerating SAM.\nThis paper mainly focuses on efficient GNN training in FSNC scenarios by leveraging SAM for improving the generalization of GNNs on unseen classes. To tackle the high training cost issue of SAM, we utilize the connection between GNNs and MLPS-GNNs discarding Message-Passing (MP) are equivalent to MLPs with faster training and worse performance in general-to accelerate training. Specifically, we propose Fast Graph Sharpness-Aware Minimization (FGSAM) that uses GNNs for perturbing parameters and employs MLPs (i.e., GNNs discarding MP) to minimize perturbed training loss. This speeds up training at the cost of dropping graph topology information during minimizing the perturbed loss. Interestingly, we find that the gradient computed in parameter perturbation can be reused when minimizing loss to explicitly reintroduce topology information with negligible extra cost. Moreover, we can add back MP during inference to improve performance. To further reduce the computational cost, we propose FGSAM+ which conducts an exact FGSAM-update at every k steps. As shown in Fig. 1, empirical results in FSNC tasks show that our proposed FGSAM and FGSAM+ methods outperform both Adam and SAM, and meanwhile FGSAM+ is even faster than Adam. In addition, we evaluate the proposed methods in node classification, showing strong results, especially in heterophilic graphs which are known to be challenging for GNNs [6, 29]. This indicates that our proposed methods can effectively improve the GNN's generalization capability for better performance.\nThe contributions of this paper can be summarized as follows.\n\u2022 We study the application of SAM in FSNC tasks.\n\u2022 We propose FGSAM that improves generalization in an efficient way by leveraging GNNs for sharpness-aware perturbation parameters and employing MLPs to expedite training.\n\u2022 We further propose an enhanced version named FGSAM+, which conducts the actual FGSAM at every k steps and approximates it in the intermediate steps.\n\u2022 We demonstrate strong empirical results of the proposed methods across tasks."}, {"title": "2 Preliminary", "content": "Graph Neural Networks. Let G = (V, E) denotes an undirected graph, V = {vi}i=1n is the node set and E \u2286 V \u00d7 V is the edge set. A \u2208 Rn\u00d7n is the adjacency matrix. Let X = {x{i}i=1n \u2208 Rnxdo be the initial node feature matrix, where do is the initial dimension, and Y = {yi}i=1n \u2208 RnxC denotes the ground-truth node label matrix, where C denotes the number of classes and yi is the one-hot encoding of node vi\u2019s label yi. Let H(L) be the output of the last layer of an L-layer GCN, the prediction probability matrix \u00dd = softmax (H(L)) is the final output of node classification.\nFew-Shot Node Classification. In the FSNC task, the entire set of node classes C can be divided into two disjoint subsets: base classes set Cbase and novel classes set Cnovel, such that C = Cbase UCnovel and Cbase \u2229 Cnovel = \u00d8. There are sufficient labeled nodes in Cbase, while there are only a limited number of labeled nodes in Cnovel. FSNC task aims to learn a model using the sufficient labeled nodes from"}, {"title": "3 Methodology", "content": "In this section, we propose Fast Graph Sharpness-Aware Minimization (FGSAM), an efficient version of SAM for GNNs, aiming to reduce the training time when using SAM in FSNC tasks while improving model's generalization."}, {"title": "3.1 Motivating Analysis", "content": "SAMs are a series of new general training scheme used to improve the model's generalization, thus it is intuitive to use SAM in FSNC tasks. However, there is no work studying how to apply SAM to FSNC tasks. So our first question is: Q1: Can SAM benefit few-shot node classification tasks?"}, {"title": "3.2 FGSAM", "content": "We elaborate our proposed method Fast Graph Sharpness-Aware Minimization (FGSAM). For the ease of reference, Fig. 3a visualizes the framework of FGSAM, so does to its enhanced version FGSAM+. There are two forward-backward steps in the FGSAM-update.\nStep 1: Graph sharpness-aware perturbation. The first forward-backward step is served for computing the maximum perturbation \u00ea (Eq. (2)), where we propose to perturb parameters with MP (GNN), i.e.,\n$$\u20ac = p \\frac{\u2207wLG(W; fgnn)}{|| \u2207wLG(W; fgnn)||} = p \\frac{\u2207wL(fgnn(G;w), Y)}{||\u2207wL(fgnn(G; w), Y)||}$$\nStep 2: Minimizing perturbed loss. We propose to minimize the perturbed loss by removing the MP (PeerMLP) to speed up training, i.e.,\nw* = arg min Lx (w + \u00ea; fmlp) = arg min L(fmlp (X; w + \u20ac), Y)\nw = arg min L(fgnn(G = {X, I}; w + \u00ea), Y).\nw\nIt is clear that minimizing the loss on PeerMLPs is equivalent to minimizing the loss on GNNs ignoring the topology information. As demonstrated in Sec. 3.1, intuitively the proposed approach can make model convergence near the local minima easily due to the connection between MLPs and GNNs, and perturbing parameters with MP can find the good flat minima of GNNs (see Fig. 2a).\nReintroducing Graph Topology in Minimization with Free Lunch. While reintroducing the MP in evaluation can improve performance, its absence during the minimization process may result in"}, {"title": "3.3 FGSAM+", "content": "Although the training time of FGSAM can be largely faster than na\u00efve SAM by ignoring the MP in minimizing perturbed loss, it still requires a full forward-backward step of GNN, which makes our approach need an extra computation cost for a forward-backward step of PeerMLP, compared to the base optimizer.\nFortunately, the forward-backward step of GNN is mainly for perturbing parameters in FGSAM, thus we can further reduce the training time while maintaining performance, by employing FGSAM-update at every k step (i.e., perturb parameters at every k step) and reusing the preserved gradients from parameters perturbation into the intermediate steps [22]. Eq. (3) can be rewritten as:\n$$\u2207w\u00a3Dtr(W)|w+\u00ea \u2248 \u2207w\u00a3Dtr (W + \u20ac) \u2248 \u2207w [LDtr (W) + p||\u2207wLDtr(W)||].$$\nIn this way, SAM-gradient gs is composed by the vanilla gradient \u2207wLDtr (w) and the gradient of the l2-norm of vanilla gradient \u2207w||\u2207wLDtr(W)||.\nThis suggests that SAM-gradient gs = \u2207wLDtr (W)|w+\u00ea can be divided into two orthogonal parts [22]: gh (in the direction of vanilla gradient g = \u2207wLDtr (W) ) is used to minimize the loss value, and flatness-gradient gv is used to adjust the updates towards a flat region. So gh and gv can be easily obtained if gs and g are given:\ngh = \\frac{gs.g}{||gs|| ||g||}. g = \\frac{||gs||}{||g||}.g,      gv = gs-gh,\nwhere @ is the angle between gs and gh. As illustrated in [22], gv changes much slower than gs and gh, thus we can compute and preserve gu at every k steps, and reuse it to approximate gs in intermediate steps."}, {"title": "4 Analysis of Toy Case", "content": "In this section, we employ the Contextual Stochastic Block Model (CSBM) to analyze why minimizing perturbed training loss without MP can work to some extent, which is the underlying mechanism of FGSAM. The CSBM has been widely used to analyze of the properties of GNN [25, 26].\nSpecifically, we focus on a CSBM model that contains K distinct classes C1, C2,...,CK. The nodes within the resulting graphs are grouped into n non-overlapping sets C1, C2, ..., \u0421\u043a, each set representing one of the K classes. The generation of edges is governed by a probability p within the same class and a probability q between different classes. For any given node i, we sample its initial features xi \u2208 R\u00b9 from a Gaussian distribution denoted by xi ~ N(\u03bc, I), where the mean \u03bc = \u03bc\u03b5 \u2208 R\u00b9 corresponds to node i belonging to set Ck, and k is an element of {1, 2, ..., K}. Furthermore, the condition ||\u03bc\u03ae - \u03bcj||2 = D holds true for all i, j belonging to {1, 2, . . ., K}, with D being a positive constant. Graphs that arise from this specified CSBM model are referred to as K-classes CSBM. After applying a MP operation, the resultant features for node i are denoted by hi.\nThe neighborhood label distribution Di of node i is a K-dimensions vector, where Di[j] = I(i \u2208 Cj)p + (1 \u2212 I(i \u2208 Cj))q. Based on the neighborhood label distribution, consider the MP operation as hi =  \\frac{1}{deg(i)} \\sum_{Dj\u2208N(i)} xi, we have: hi ~ N ((pq)+K, (p-q)\u03bck+qK\u016b deg(i) I \u53ef),\nwhere i \u2208 Ck and \u00b5 =  \\frac{1}{K}\\sum^{K}_{j=1}\u03bcj . Based on the distribution of hi and \u00e6i, we can obtain following theorem:\nTheorem 4.1 (The effectiveness of removing MP in minimization). Consider a K-classes CSBM, the optimal linear classifiers for both original features xi and filtered features hi are the same.\nDetailed proof is in Appendix C. The theorem tells us that under the linear case, whether the MP layer is used or not, the optimal decision bound is the same. Hence, this encourages us to learn the weight of transformation layers without MP to speed up training. However, the real graph is more complex and we do not use a linear classifier, thus we propose to perform the graph sharpness-aware perturbation which implicitly involves the information of neighbors."}, {"title": "5 Experiments", "content": "We verify the effectiveness of our proposed FGSAM and FGSAM+ in this section. We first conduct experiments to demonstrate that our proposed algorithms achieve better performance compared to SAM which requires twice the training time. Then we show that our proposed algorithms can achieve faster training speed compared to base optimizers (e.g., Adam). Next, we also conduct extra studies and an extra task to show the robustness and potential applications of our proposed algorithms."}, {"title": "5.1 Experiment Settings", "content": "Baseline. We evaluate our proposed FGSAM and FGSAM+ on SOTA models. The existing models can be divided into two main categories: MAML and non-MAML methods. Two representative models are selected from each category, respectively, as baselines for evaluation (Meta-GCN [41] and AMM-GNN [36] for MAML models, and GPN [7] and TENT [37] for non-MAML models).\nDatasets. We conduct evaluations on three widely used real-world benchmark node classification datasets: CoraFull [5], DBLP and ogbn-arXiv [17], and we use the train/val/test split as in [33] and [23]. The comprehensive statistics of datasets are shown in Tab. 5 in Appendix D.1.\nImplementation Details. We implement our model by PyTorch [28] and conduct experiments on an RTX-3090Ti. We use Optuna [2] to search the hyper-parameters for each setting. See Appendix D.2 for detailed FSNC learning protocol."}, {"title": "5.2 Evaluation on Real-World Datasets", "content": "The results of different models across datasets are summarized in Tab. 2. All the models share a 2-layers architecture with 16 hidden channels. It can be seen that our proposed algorithms FGSAM and FGSAM+ provide better performance than Adam in most cases, and provide comparable performance with SAM. These results support our claim that FGSAM and FGSAM+ can find local minima with better generalization properties. Note that message-passing is only used in perturbing parameters, not involved in parameters update (i.e., MLPs). The results further indicate that implicitly involving graph topology in training can make PeerMLPs outperform GNNs. See Appendix D.3 for details."}, {"title": "5.3 Time Consumption", "content": "To demonstrate the training speed advantage of our proposed algorithm, we summarize the training time for different models using various optimization methods across three datasets (Tab. 2). The results indicate that our proposed algorithm FGSAM demonstrates only a slight increase in training cost compared to Adam in most cases. Furthermore, our enhanced version FGSAM+ outperforms Adam in terms of speed in the majority of scenarios. It is worth mentioning that our proposed algorithms achieve superior or comparable performance when compared to both Adam and SAM. See Appendix D.4 for detailed results.\nLimitation. For models composed of many non-GNN components (e.g., TENT), the training time on FGSAM+ may be still longer than that on Adam, since it is hardly further reduced."}, {"title": "5.4 Comparison of the Variants of SAM", "content": "Training with Different Optimizer. We compare the performance of Meta-GNN and GPN training with different variants of SAM, including original SAM, ESAM [8], LookSAM [22], AE-SAM [18], our proposed FGSAM and FGSAM+ (Tab. 3). We observe an anomalous phenomenon where ESAM, as an efficient variant of SAM, actually trains slower than SAM. This is because ESAM sorts the sample losses and selects a suitable subset at each iteration, an operation that is negligible for image tasks; however, for graph tasks, since GNNs are relatively smaller, the proportion of time consumed by the sorting step is significant, leading to an increase in training time. As shown in Tab. 3, our proposed method greatly reduces the training time, based on the relationship between GNN and MLP, while maintaining and even achieving superior performance, compared to other optimizers, indicating ours' high efficiency and effectiveness.\nThe Impact of Perturbing Parameters with Message-Passing. A key point of our work is that we perform parameter perturbation using GNNs, while PeerMLPs (i.e., without message-passing) are used to minimize the perturbed loss. This is significantly distinct from previous SAM methods which shared the same model for both parameter perturbation and loss minimization. So a natural question arises: to what extent does our approach benefit from performing parameter perturbation using GNNs? We thus compare our approach to PeerMLPs training with Adam and vanilla SAM. Note that message-passing would be reintroduced during validation and test. From Tab. 3, although the training time of PeerMLPs is shorter than that of GNNs, GNNs outperform their PeerMLPs in most cases. Despite that using PeerMLPs can accelerate the training of GNNs, the topology information is still very important for learning node representations. Thus our proposed FGSAM+ is a better solution, achieving a better trade-off between efficiency and performance."}, {"title": "5.5 Ablation Studies", "content": "We further verify the consistent effectiveness of our method compared to Adam across different settings regarding model implementation and graph property. Due to the computational resource restriction, all experiments here were conducted using GPN on the CoraFull with the 5-way 3-shot setting. We provide additional experiments (e.g., the effect of update interval k) in the Appendix E.\nThe Impact of Network Structure. Here we investigate the effect of hidden dimension and the number of layers on the performance (on the left of Fig. 4). GPN with Adam requires a higher hidden dimension (128) to achieve relatively high accuracy, whereas GPN with FGSAM+ can attain SOTA even with a small hidden dimension (16). With respect to the number of layers, GPN with FGSAM+ consistently performs better within the range of 1~8 compared to GPN with Adam, demonstrating the effectiveness of our proposed method (middle left of Fig. 4).\nThe Impact of Noisy Features and Edges. Here we investigate the effect of randomly adding Gaussian noise to features and randomly adding edges during testing (on the middle right and the right of Fig. 4). Specifically, for noisy features, we randomly add Gaussian noise with varying standard deviations to the node features. Meanwhile, for noisy edges, we uniformly and randomly introduce additional edges into the original structure. The results show that GPN with FGSAM+ method can still achieve relatively high performance, compared to GPN with Adam. These results effectively verify the robustness of our proposed method."}, {"title": "5.6 Additional Task on Conventional Node Classfication", "content": "Our proposed FGSAM+ also has the potential to be extended to other domains. To demonstrate this, we evaluate the performance of the FGSAM+ on the standard node classification task on both homophilic and heterophilic graphs. For homophilic graphs, we utilize three well-established citation networks: Cora, Citeseer, and Pubmed [12, 31]. For heterophilic graphs, we include page-page"}, {"title": "5.7 Additional Study", "content": "We observe that both FGSAM and FGSAM+ generally outperform the standard SAM across tasks (FSNC and standard node classification). This is an interesting finding, as our FGSAM and FGSAM+ algorithm remove message-passing during the minimization of the perturbed loss, which is expected to hurt performance. We attribute these counter-intuitive results to the mitigation of the imbalance adversarial game. The training process of SAM-like algorithms entails an adversarial game similar to that in Generative Adversarial Nets (GANs) [13]. Prior studies [3, 4, 27] have demonstrated that imbalanced adversarial games in GANs can give rise to worse results. Both FGSAM and FGSAM+ employ distinct models for perturbation and minimization, which can help alleviate the extent of imbalance. These factors may explain the observed performance discrepancies among the compared algorithms. To verify the explanation, we conduct experiments varying the hyper-parameter \u03c1. Specifically, we graphically illustrate the comparative training loss of SAM and FGSAM+ over a range of \u03c1 values in Fig. 5, which reveals that while SAM struggles to converge with higher \u03c1 values, FGSAM+ consistently achieves convergence. Moreover, it is established that a higher \u03c1 value is conducive to a tighter generalization bound, suggesting that a larger \u03c1 could potentially enhance performance. Consequently, FGSAM+ is capable of mitigating the imbalanced games issue and tolerating a larger \u03c1, which contributes to its enhanced performance."}, {"title": "6 Conclusion", "content": "In this work, we study the application of Sharpness-Aware Minimization (SAM) in FSNC to improve model's generalization, since the key for FSNC is to generalize the model to unseen samples. In order to alleviate the heavy computation cost of SAM, we utilize the connection between MLPs and GNNs and use MLPs to accelerate the training of GNNs. However, the low generalization and lack of using graph topology of MLPs also limit its performance. Hence we propose to apply GNNs to perturb parameters for generalization and use MLPs to minimize the perturbed training loss for conducting the proposed FGSAM. Moreover, we reuse the GNN gradient in perturbation in minimization for better including topology information. We further reduce the training time by conducting exact FGSAM update at every k steps and approximate FGSAM's gradient with reusing information in the intermediate steps. Finally, the extensive experiments demonstrate the effectiveness and efficiency of our proposed methods."}, {"title": "A Potential Broader Impact", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "B Algorithm", "content": "Algorithm 1 Training with FGSAM and FGSAM+.\nRequire: G, Cbase, learning rate \u03b7, radius \u03c1, FGSAM update interval k, adaptive ratio \u03b1.\nEnsure: A flat minimum solution w. rang\nInitialize weights wo;\nfor t \u2190 0 to T - 1 do\nSample training task Tt from G and Cbase;\n### only for FGSAM\nVanilla grad ggnn = \u2207wtLTt(wt; fgnn);\nPerturbed weights $\u20ac = p \\frac{ggnn}{||ggnn||};$\nFGSAM-grad gFGSAM = \u2207\\_w+LTt(wt; fmlp)|w+\\\u0454;\n### only for FGSAM+\nif t%k == 0 then\n# the actual FGSAM-update\nVanilla grad ggnn = \u2207wtLTt(wt; fgnn);\nVanilla grad gmlp = \u2207wtLTt(wt; fmlp);\nPerturbed weights $\u20ac = p \\frac{ggenn}{||ggen||};$\nTopology-grad $gg = ggnn - \\frac{ggnn.gmlp}{|| ggnn |||| gmlp ||} \\frac{gmlp}{|| gmlp||};$\nSAM-grad gs = \u2207wtLTt(wt; fmlp)|w+\\\u0454;\nFlatness-grad $gv = gs - \\frac{gmlp.gs}{||gmlp ||} \\frac{gmlp}{||gmlp||};$\nFGSAM-grad GFGSAM = Aggnn + gs;\nelse\n# approximate FGSAM-gradient\nVanilla grad gmlp = \u2207wtLTt(wt; fmlp);\nApprox gnn-grad $\\hat{g}gnn = gmlp + \\frac{gv}{||gv||} \\frac{gg}{||gg||};$\nApprox FGSAM-grad gFGSAM = gmlp + \u03b1gv + \u03bb\u011dgnn;\nend if\nUpdate weights: wt+1 \u2190 wt - \u03b7. 9FGSAM;\nend for\n\u0175 \u2190 \u03c9\u03c4."}, {"title": "C Proof", "content": "The linear classifier for K-classification problems can be formulated as  \\frac{K(K-1)}{2} binary classification problems.\nHence we study the classification between class Co and Cp without loss of generality.\nThe distribution of original features from different classes follows:\nxi ~ \u039d (\u03bc\u03bf, \u0399), i \u2208 Co\nxi ~ \u039d (\u03bc\u03c1, \u0399), i \u2208 Cp"}]}