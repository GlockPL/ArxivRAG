{"title": "Less is More:\nSelective Reduction of CT Data for Self-Supervised Pre-Training of Deep\nLearning Models with Contrastive Learning Improves Downstream Classification\nPerformance", "authors": ["Daniel Wolf", "Tristan Payer", "Catharina Silvia Lisson", "Christoph Gerhard Lisson", "Meinrad Beer", "Michael G\u00f6tz", "Timo Ropinski"], "abstract": "Background: Self-supervised pre-training of deep learning models with contrastive learning is a widely used tech-\nnique in image analysis. Current findings indicate a strong potential for contrastive pre-training on medical images.\nHowever, further research is necessary to incorporate the particular characteristics of these images.\nMethod: We hypothesize that the similarity of medical images hinders the success of contrastive learning in the\nmedical imaging domain. To this end, we investigate different strategies based on deep embedding, information\ntheory, and hashing in order to identify and reduce redundancy in medical pre-training datasets. The effect of these\ndifferent reduction strategies on contrastive learning is evaluated on two pre-training datasets and several downstream\nclassification tasks.\nResults: In all of our experiments, dataset reduction leads to a considerable performance gain in downstream tasks,\ne.g., an AUC score improvement from 0.78 to 0.83 for the COVID CT Classification Grand Challenge, 0.97 to 0.98 for\nthe OrganSMNIST Classification Challenge and 0.73 to 0.83 for a brain hemorrhage classification task. Furthermore,\npre-training is up to nine times faster due to the dataset reduction.\nConclusions: In conclusion, the proposed approach highlights the importance of dataset quality and provides a trans-\nferable approach to improve contrastive pre-training for classification downstream tasks on medical images.", "sections": [{"title": "1. Introduction", "content": "Supervised training of a deep learning model requires\nlarge and accurate datasets. Annotations are necessary for\nall training samples. In the medical imaging domain, an-\nnotated datasets for specific tasks are often limited due\nto factors such as the rarity of diseases, limited access,\nor the high complexity of annotations (Kiryati and Lan-\ndau, 2021; Maier-Hein et al., 2018). To overcome this\nchallenge, deep learning models can be pre-trained on\nlarge medical image datasets without annotations, using\nself-supervised learning techniques (Huang et al., 2023).\nThese techniques train the models to create meaningful\nrepresentations from unlabeled datasets, allowing them to\nlearn general high-level features of the images. To fine-\ntune the models for specific tasks, the so-called \"down-\nstream tasks\", small annotated datasets are sufficient af-\nter pre-training. Contrastive learning is a state-of-the-art\napproach for self-supervised pre-training on unannotated\nimages (Balestriero et al., 2023) and according to Huang\net al.'s study (Huang et al., 2023), currently the most\npopular approach in the medical imaging domain. Sev-\neral works show remarkable performance gains on med-\nical downstream tasks when the models are pre-trained\nwith contrastive learning on large unannotated medical\nimage datasets compared to training the models from\nscratch (Ghesu et al., 2022; Tang et al., 2022; Chen et al.,\n2021; Azizi et al., 2021; Ewen and Khan, 2021; Dufumier\net al., 2021). Despite the great potential of contrastive\npre-training techniques in the medical domain, the special\ncharacteristics of volumetric radiological images, consist-\ning of many consecutive slices, such as CT, MRI or PET,\nhave not been sufficiently exploited. In our work, we eval-\nuate the composition of the pre-training datasets for con-\ntrastive learning on CT scans.\nWhen it comes to deep learning on CT scans in general,\nthere are two widely used approaches, both of which show\nexcellent results on clinically relevant imaging tasks. The\nfirst approach is to train on the whole CT volumes using a\n3D model (Lisson et al., 2022; Andrearczyk et al., 2021),\nand the second approach is to train on individual slices of\nthe volumes using a 2D model (Jiang et al., 2022; Xing\net al., 2022; Baghdadi et al., 2022; Wang et al., 2021).\nEach approach has its own advantages. While training a\n3D model on volumes enables the model to better cap-\nture the 3D properties of the images (Avesta et al., 2023),\ntraining a 2D model by using each slice of a volume sep-\narately can improve performance on small datasets due to\nthe increased sample size (Zettler and Mastmeyer, 2021;\nKern et al., 2021; Bhattacharjee et al., 2021), and re-\nduces the computational cost of training and inference\nas smaller GPUs are sufficient (Zettler and Mastmeyer,\n2021; Kern et al., 2021; Yu et al., 2020; Nemoto et al.,\n2020; Avesta et al., 2023). For both approaches, there\nare several publications that investigate self-supervised\npre-training with contrastive learning, achieving signifi-\ncant performance gains on several CT image downstream\ntasks. Tang et al. (Tang et al., 2022) and Dufumier et\nal. (Dufumier et al., 2021) pre-train 3D models on CT\nvolumes, while Wolf et al. (Wolf et al., 2023), Ghesu et\nal. (Ghesu et al., 2022) and Chen et al. (Chen et al., 2021)\npre-train 2D models on CT slices. In this study, we chose\nto conduct our experiments with 2D models because we\nsee that it is critical for deep learning to be globally ac-\ncessible without the need for powerful GPUs, and the\nadvantages of 2D models for sparse data remain signifi-\ncant even when using pre-trained models, as small anno-\ntated datasets are a major challenge in the field of medical\nimaging.\nContrastive learning involves the following steps: A\ndataset of unlabeled images is used as a starting point.\nRandom augmentations are applied to generate multiple\nrandomly varied samples of each original image. These\nare fed into a deep learning model to obtain latent-space\nrepresentations for each sample. The model always com-\npares two representations and is trained to discriminate\nwhether these are derived from the same original im-\nage (referred to as positive pairs) or derived from differ-\nent original images (referred to as negative pairs). Pre-\nvious works on contrastive pre-training with CT slices\nhave included as many slices as possible, following\nthe traditional approach of maximizing the pre-training\ndataset (Ghesu et al., 2022; Chen et al., 2021; Wolf et al.,\n2023). In this paper, we hypothesize that using all slices\nof each CT volume in a dataset for contrastive pre-training\nmay lead to performance degradation. We derive our hy-\npothesis from the fact that CT datasets have very low vari-\nance compared to natural image datasets due to the high\nsimilarity of the CT slices. This may result in the model\nbeing unable to discriminate between positive and nega-\ntive pairs since the similarity between two augmented ver-\nsions of a slice might be lower than the similarity between\ntwo different slices. Our hypothesis is supported by recent\nwork that provides preliminary evidence that this may be\na challenge in contrastive learning. Using ImageNet data,\nJing et al. (Jing et al., 2022) show that a lower variance\nof the data distribution than the variance caused by the\ndata augmentation of contrastive learning leads to per-\nformance degradation in downstream tasks. Conrad and\nNarayan (Conrad and Narayan, 2021) show on electron\nmicroscopy images that low variance in the pre-training\ndataset affects downstream task results.\nTo investigate our hypothesis that using all CT slices\nfor contrastive pre-training may lead to performance\ndegradation, we explore various strategies based on deep\nembedding, information theory, and hashing to identify\nand reduce redundancy in pre-training datasets. Figure 1\nillustrates our general approach. Starting with a dataset\nof unannotated CT slices, we perform different reduc-\ntion strategies and pre-train the models with contrastive\nlearning on the reduced datasets. The pre-trainings are\nevaluated on downstream tasks by fine-tuning the pre-\ntrained models with supervised learning. We choose two\npre-training datasets and three downstream classification\ntasks, the benchmark task for evaluating self-supervised\npre-training (Huang et al., 2023). The outcomes sup-\nport our hypothesis, as the downstream results improve\nwith our dataset reduction strategies. Furthermore, we in-\nvestigate which reduction strategy is best suited for CT\ndatasets and what is the optimal threshold that repre-\nsents the best trade-off between high variation but also\na sufficiently large number of samples in the pre-training\ndataset to achieve the best downstream results. Finally,\nour work provides a ready-to-use model for improving\nself-supervised pre-training on CT datasets for classifi-\ncation downstream tasks. These findings have the po-\ntential to improve the handling of small annotated CT\ndatasets while maintaining low computational costs. The\npre-trained models, as well as the ready-to-use code,\nare available on GitHub: https://github.com/Wolfda95/\nLess_is_More"}, {"title": "2. Materials and Methods", "content": "In this section, we explain in detail the methods for\ninvestigating our hypothesis that using all slices of each\nCT volume for contrastive pre-training may lead to per-\nformance degradation due to the high similarity of the\nslices. We first present strategies for selecting slices of\nCT volumes to obtain a reduced pre-training dataset with\nincreased variation. This is followed by describing the\ncontrastive pre-training methods and datasets. Finally, we\nintroduce the downstream tasks to evaluate the impact of\nthe reduction strategies on contrastive pre-training."}, {"title": "2.1. Dataset Reduction", "content": "We investigate our hypothesis by comparing six ap-\nproaches for slice selection: two baseline approaches and\nfour similarity-based approaches. The similarity-based\napproaches perform a pairwise comparison between all\nslices in a volume. A similarity score is calculated for\neach slice pair. The pairs are sorted from most similar to\nmost dissimilar. Starting from the most similar pair, one\nslice is removed from the pairs until either all pairs have\nsimilarities below a given threshold or until a given num-\nber of slices is left. We incorporate commonly used sim-\nilarity computation methods from different fields, such as\ninformation theory, deep embedding, and hashing, with-\nout claiming completeness. The methods we selected are\nwell-established for image comparison and computation-\nally fast, which is necessary due to the large number of\npairwise comparisons.\nALL: The first baseline approach follows the current\nstate of the art (Chen et al., 2021; Ghesu et al., 2022). All\nslices are included in the training.\nEveryN: The second baseline approach is our baseline\nreduction method. Here, CT datasets are reduced by using\nevery nth slice of a volume.\nSSIM: As our first similarity-based approach, we use\nthe Structural Similarity Index (SSIM) (Wang et al., 2004)\nfrom information theory, which is a common similarity\nmeasure for images (Wang and Bovik, 2009). It compares\nthe luminance, contrast, and structure of two given images\nx and y by the equation\n$d(x, y) = \\frac{(2\\mu_x\\mu_y + (K_1L)^2)(2\\sigma_{xy} + (K_2L)^2)}{(\\mu_x^2 + \\mu_y^2 + (K_1L)^2) (\\sigma_x^2 + \\sigma_y^2 + (K_2L)^2)}$\nwhere $\\mu_x$, $\\mu_y$ and $\\sigma_x$, $\\sigma_y$ are the mean and standard de-\nviation and $\\sigma_{xy}$ the covariance of all pixel values of two\nimages. To avoid instability, $(K_1L)^2$ and $(K_2L)^2$ are added,\nwhere $L$ is the dynamic range of the pixel values and\n$K_1 = 0.01$ and $K_2 = 0.03$ are small constants. SSIM is\ncomputed as the average result of a moving 11\u00d711 kernel\nwith a Gaussian weighting function. The parameters were\nchosen as suggested by Wang et al. Wang et al. (2004).\nMI: We use Mutual Information (MI) as the second\nsimilarity-based approach from information theory. MI\nis a widely used technique for similarity calculation and\nregistration of medical images (Pluim et al., 2003; Rus-\nsakoff et al., 2004) and measures the dependence between\ntwo images x and y by calculating the Kullback\u2013Leibler\ndivergence\n$KL(XIY) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) log(\\frac{p(x, y)}{p(x)p(y)})$ \nbetween the joint distribution p(x, y) and the independent\ndistributions p(x)p(y) of the pixel values. We use the nor-\nmalized Mutual Information as introduced by Studholme\net al. (Studholme et al., 1998).\nDeepNet: Motivated by the success of the Perceptual\nSimilarity Metric (Zhang et al., 2018) for image compar-\nison, which is computationally expensive, we introduce\nDeepNet similarity, which reduces the complexity so that\npairwise comparisons can be performed in a reasonable\namount of time. Like perceptual similarity, DeepNet sim-\nilarity compares two images by running them through a\npre-trained deep learning model. Instead of computing\nthe cosine similarity in the channel dimension, DeepNet\nsimilarity computes the cosine similarity between the out-\nput vectors. Using PyTorch's ResNet50 (He et al., 2016)\npre-trained on ImageNet (Deng et al., 2009) to compute\nthe output vectors, we get the following equation\n$d(x, y) = \\frac{ResNet(x) \\cdot ResNet(y)}{||ResNet(x)||^2 ||ResNet(y)||^2}$\nto compare two images x and y.\nHASH: The HASH similarity is based on the compar-\nison of hash values derived from each image. It is mo-\ntivated by Conrad and Narayan (Conrad and Narayan,\n2021), who used it to extract dissimilar images from an\nelectron microscopy dataset for contrastive pre-training.\nThe procedure is as follows: Each image is compressed\nto the size of 9 \u00d7 8 pixel and encoded into a 64-bit hash.\nThe compression is performed with the Antialias function\nfrom Pillow (Clark, 2015). The hash is computed by loop-\ning through each row of the compressed image, compar-\ning each pixel with its right neighbor, and selecting one if\nthe neighbor is larger and zero if the neighbor is smaller.\nFor each row of nine pixels, this results in a hash of eight\nbits, leading to a 64-bit hash in total. The Hamming dis-\ntance\n$d(x, y) = \\frac{1}{n} \\sum_{i=1}^{n} |Hash(x)_i - Hash(y)_i|$\nbetween the two hashes of images x and y measures the\nsimilarity, where n = 64 is the length of the hash. All pa-\nrameters are chosen following Conrad and Narayan (Con-\nrad and Narayan, 2021)."}, {"title": "2.2. Pre-Training", "content": "Following Huang et al.'s. (Huang et al., 2023) study,\npopular contrastive learning methods from natural im-\nage processing that are widely utilized for medical pre-\ntraining are SimCLR (Chen et al., 2020a), MoCo (He\net al., 2020), BYOL (Grill et al., 2020), and SwAV (Caron\net al., 2020). SimCLR follows the basic contrastive learn-\ning strategy, where the model is trained to distinguish\nbetween positive and negative pairs within a mini-batch.\nThis requires a large batch size in order to obtain a suffi-\ncient number of negative samples within one mini-batch.\nMoCo adds a queue for storing negative samples, which\nreduces batch size requirements but increases storage de-\nmands. BYOL introduces two competing models to de-\ncrease batch size demands. SwAV adds online feature\nclustering to the latent space representations, which low-\ners the batch-size constraints.\nFor our evaluations, we chose the method\nSwAV (Caron et al., 2020), since it outperformed\nthe other state-of-the-art contrastive learning methods\nwith convolutional models on several natural imaging"}, {"title": "2.3. Downstream Evaluation", "content": "As Huang et al. (Huang et al., 2023) shows, classifica-\ntion tasks are commonly used as a benchmark for eval-\nuating self-supervised pre-training. Usually, only a sin-\ngle linear layer is added to the pre-trained encoder to ad-\njust the model to the correct output size, resulting in only\nthe weights of one layer not being pre-trained. In con-\ntrast, segmentation tasks require the addition of a large de-\ncoder to the pre-trained encoder, such as in a U-Net (Ron-\nneberger et al., 2015), resulting in a more significant pro-\nportion of untrained model weights. This increases the\ndependency on the dataset of the downstream task. There-\nfore, we focus on classification downstream tasks to eval-\nuate pre-training performance, although our results are\nexpected to apply to other tasks as well.\nWe selected three classification tasks on CT slices, en-\nsuring that the images do not overlap with those in the\npre-training datasets. These tasks include two public chal-\nlenges and an internal task as part of a clinical study. For\nthe two publicly available challenges, we perform five\ndownstream runs with the given train/validation/test split\nof the challenge, to ensure the comparability with other\nchallenge participants. For the internal task, a five-fold\nstratified cross-validation is performed. For each fold,\nfour parts of the data are used for training and valida-\ntion (90% training, 10% validation), and the remaining\npart that has not been used for training and validation is\nused for testing. This ensures, that the model works on\ndifferent data splits. The mean and standard deviation of\naccuracy, AUC score, and F1-sore over the five runs are\nreported for all three tasks. The tasks include CT scans\nfrom different hospitals, scanners, and body parts to prove\nthe generalizability of our findings. The three tasks, sum-\nmarized in Table 2, are the following:\nCOVID-19: The COVID-19 CT Classification Grand\nChallenge (Zhao et al., 2020) dataset consists of 349 CT\nslices (216 patients) and 397 CT slices (171 patients) with\nand without clinical findings of COVID-19, respectively.\nThe task is to classify between COVID-19 findings and\nno COVID-19 findings.\nOrgMNIST: The OrganSMNIST Challenge from\nMedMNIST (Yang et al., 2023) consists of 25,221 im-\nage patches of the size 28 \u00d7 28, cropped around organs\nfrom abdominal CT scans of 201 patients. The challenge\nis a multi-class classification of 11 body organs.\nBrain: An internal dataset with CT slices from 100\npatients with and 100 patients without brain hemorrhage\nis used for the third downstream task. All CT examina-\ntions were part of the routine clinical practice at the Uni-\nversity Hospital of Ulm. Representative slices were se-\nlected by Dr. Ch. G. Lisson and Dr. Ca. S. Lisson,\ntwo well-trained senior radiologists. This study aims to\ndetermine whether brain hemorrhages can be detected au-\ntomatically on CT scans, which could help physicians in\ntheir diagnosis. All patients provided written consent for"}, {"title": "3. Experiments and Results", "content": "Our experiments are designed to investigate our hy-\npothesis by answering whether dataset reduction leads to\nperformance gains, which of our selected reduction meth-\nods performs best, what is the optimal similarity thresh-\nold, and how much performance gain can be achieved.\nAll experiments are performed separately on the two pre-\ntraining datasets PET-CT and LIDC. In total, we con-\nducted 24 different pre-trainings, resulting in over 2000\npre-training hours. The pre-trainings are evaluated on\nthe three downstream tasks COVID-19, OrgMNIST, and\nBrain. For all results, we report the mean and standard\ndeviation of AUC and F1 scores over five fine-tuning runs\non the downstream tasks. Table 3 shows the downstream\ntask results without any pre-training as a reference."}, {"title": "3.1. Evaluation A: Does reduction lead to performance\ngains?", "content": "The first experiment evaluates whether reducing CT\ndatasets for contrastive pre-training leads to performance\ngains in downstream tasks. To answer this question,\nwe compare the baseline method ALL with the baseline\nreduction method EveryN. Pre-training is performed on\nboth pre-training datasets with all slices (ALL), with ev-\nery tenth slice, and with every fifth slice (EveryN). The re-\nduction numbers are chosen randomly. Table 4 shows the\ndownstream task results. Performance gains are achieved\nin all three downstream tasks by reducing the pre-training\ndataset to 20%, and 10% with the EveryN method. The\nperformance gains are slightly higher for the 10% reduc-\ntion."}, {"title": "3.2. Evaluation B: Which reduction method performs\nbest?", "content": "Having found that CT data reduction for contrastive\npre-training leads to considerable performance gains in\ndownstream tasks, the second experiment investigates\nwhich of our selected reduction methods is the best op-\ntion. We compare the baseline reduction method EveryN\nwith the similarity-based approaches SSIM, MI, Deep-\nNet, and HASH. For an accurate comparison, the reduced\ndatasets should contain the same number of slices for each\nmethod. We chose to reduce the pre-training datasets to\n10%, since we found a considerable performance gain for\nreducing the datasets to 10% with the baseline method."}, {"title": "3.3. Evaluation C: What is the optimal threshold?", "content": "When comparing five approaches for reducing CT\ndatasets to 10%, we found that the HASH approach per-\nforms best. However, the percentage of a CT dataset vol-\nume that leads to the best results can vary from dataset\nto dataset, depending on the variation of the datasets.\nDatasets with high variation, for example, due to higher\nslice thickness, may require less reduction than datasets\nwith lower variation. Therefore, in the third experiment,"}, {"title": "3.4. Evaluation D: How much performance gain can be\nachieved?", "content": "Through several experiments, we found that the HASH\napproach with a Hamming distance threshold of six\n(HASH-6) performs best. In the last step, we compare the\ndownstream task results of the best-performing approach\nwith the baseline method ALL, the current state of the\nart, using all slices of the dataset for pre-training. Fig-\nure 7 shows the pre-training duration and the AUC scores\nof the downstream task results. On the PET-CT pre-\ntraining dataset, we achieve performance gains in AUC\nvalues from 0.775 to 0.830, 0.968 to 0.978, and 0.727 to\n0.831 for the COVID-19, OrgMNIST, and Brain down-\nstream tasks, respectively. Performance gains from 0.807\nto 0.823, 0.972 to 0.982, and 0.734 to 0.840 are achieved\non the LIDC pre-training dataset. The pre-training time is\nreduced from 538 h to 62 h and from 280 h to 27 h on the\nPET-CT and LIDC datasets, respectively, with a slice se-\nlection time of less than 30 minutes. For a better interpre-\ntation of our results, in the following we further analyze\nthe difference between pre-training with ALL data and\npre-training with the best performing method HASH-6 by\ncalculating the Centered Kernel Alignment CKA (Korn-"}, {"title": "3.5. Evaluation E: Other Self-Supervised Pre-Training\nApproaches", "content": "To show the generalizability of our results, we tested\nfurther self-supervised pre-training approaches with the\nbest-performing reduction approach HASH. For this eval-\nuation, we only use the LIDC dataset which is smaller\nand thus needs less pre-training time and has less com-\nputational effort. According to Huang et al.'s. (Huang\net al., 2023) study, another popular contrastive learning\napproach on convolutional neural networks from natu-\nral image processing that is widely utilized for medical\npre-training is MoCo (Momentum Contrast) (He et al.,\n2020). MoCo has been slightly updated in MoCo ver-\nsion 2 (Chen et al., 2020b) and has also been successfully\napplied to pre-training on CT slices (Wolf et al., 2023;\nChen et al., 2021). A completely different approach for\nself-supervised pre-training is masked image modeling,\nwhich has gained much popularity in the imaging field\nin the last few years (He et al., 2022). In a recent study,\nTian et al. (Tian et al., 2023) show on ImageNet (Deng\net al., 2009) data that masked autoencoders (He et al.,\n2022), that have been mainly used for self-supervised\npre-training of transformers (Huang et al., 2023), can be\nadapted to convolutional models. Masked autoencoders\ndivide the images into patches, mask part of the patches,\nand train the model to reconstruct the original images.\nDue to the moderate success of this method for convo-\nlutional models so far (Huang et al., 2023), they adapted\nit by using sparse convolutions instead of normal convolu-\ntions for the pre-training, where they achieved compara-\nble results to contrastive learning. In (Wolf et al., 2023),\ntheir method, called SparK, was applied to CT slices and\nshows similar performance to SwAV and MoCo for self-\nsupervised pre-training and is particularly robust for small\ndownstream datasets.\nWe compare the best-performing reduction method\nHASH with threshold six against the baseline method\nALL on the contrastive learning approach MoCo Version\n2 and the masked autoencoder approach SparK. To prove\nthat our results are generalizable to other contrastive pre-\ntraining methods, we expect to see performance gains\nwith our slice reduction method for MoCo, since, as in\nSwAV, less similar images should improve the model's\nability to distinguish in latent space. On the other hand,\nsince there is no distinguishing involved in masked au-\ntoencoder pre-training approaches, similar images should\nnot be a problem there. Thus, we expect no performance\ngain or slightly reduced performance for the masked au-\ntoencoder method SparK, since less similar images should\nnot bring any advantage and the model just has less train-\ning data. Detailed explanations of MoCo and Spark can be\nfound in Appendix B and Appendix C. Table 7 shows the\ndownstream task results. The contrastive learning method\nMoCo performs better with the reduced dataset, analo-\ngous to the contrastive learning method SwAV discussed\nearlier. For the masked autoencoder method SparK, we\ndo not achieve any improvements with the reduction, us-\ning all slices achieves superior results."}, {"title": "4. Discussion", "content": "Self-supervised pre-training of deep learning models\nwith contrastive learning on large unannotated datasets\nis a common and successful approach in medical imag-\ning to cope with small annotated datasets (Huang et al.,\n2023). The most popular contrastive learning methods\nwere initially developed for natural image processing and\ntransferred to the medical domain (Huang et al., 2023).\nMany methods can be directly applied to the medical do-\nmain without adaptation; however, not all methods show\nthe same behavior because medical images have differ-\nent structures and color schemes (Raghu et al., 2019).\nIn this work, we investigate the composition of the pre-\ntraining datasets for contrastive learning on CT slices."}, {"title": "5. Limitations", "content": "As our work shows the great potential of CT dataset\nreduction for contrastive pre-training, it would be inter-\nesting to further investigate these findings in future re-\nsearch. A limitation of our work is that we only chose\nwell-established, computationally fast dataset reduction\nmethods that are based on similarity calculations. For\nfuture work, it would be interesting to see if there are\nother dataset reduction techniques that could lead to even\nbetter results. For example, core-set selection such as\nSVP (Selection via Proxy) (Coleman et al., 2020) or\nCRAIG (Coresets for Accelerating Incremental Gradient\ndescent) (Mirzasoleiman et al., 2020) might be a promis-\ning idea. However, these methods require significantly\nmore time and computational effort for the dataset reduc-\ntion.\nAnother limitation of our work is that we did not in-\nvestigate different augmentation strategies for the con-\ntrastive pre-training. The ability to distinguish between\nlaten"}]}