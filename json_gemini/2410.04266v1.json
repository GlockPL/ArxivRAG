{"title": "Constructing Cloze Questions Generatively", "authors": ["Yicheng Sun", "Jie Wang"], "abstract": "We present a generative method called CQG for constructing cloze questions from a given article using neural networks and WordNet, with an emphasis on generating multigram distractors. Built on sense disambiguation, text-to-text transformation, WordNet's synset taxonomies and lexical labels, CQG selects an answer key for a given sentence, segments it into a sequence of instances, generates instance-level distractor candidates (IDCs) using a transformer and sibling synsets. It then removes inappropriate IDCs, ranks the remaining IDCs based on contextual embedding similarities, as well as synset and lexical relatedness, forms distractor candidates by combinatorially replacing instances with the corresponding top-ranked IDCs, and checks if they are legitimate phrases. Finally, it selects top-ranked distractor candidates based on contextual semantic similarities to the answer key. Experiments show that this method significantly outperforms SOTA results. Human judges also confirm the high qualities of the generated distractors.", "sections": [{"title": "INTRODUCTION", "content": "Cloze questions are a common mean for assessing reading comprehension. A standard cloze question consists of a stem - a sentence with one or more blanks - and four answer choices for each blank, with one choice being the correct answer (aka. the answer key) and the rest being the distractors. Appropriate distractors must be reliable and plausible. A distractor is reliable if filling in the blank with it yields a contextually fit and grammatically correct yet logically wrong sentence. A distractor is plausible if it is not manifestly incorrect, namely, if it presents sufficient confusion as to which is the correct answer.\nMining WordNet [1] is a common approach to finding distractors that share the same hypernym of the answer key. WordNet offers exquisite taxonomies of English nouns, verbs, adjectives, and adverbs, as well as noun and verb phrases. This approach, however, could easily lead to inappropriate distractors. For example, suppose that the word dog is the answer key selected from a sentence about domestic animals. In WordNet, the word dog in the sense of animal has canine as a hypernym, with hyponyms wolf, fox, wild dog, and others. Selecting any of these hyponyms as distractors is inappropriate for the context.\nWe devise a generative method called CQG (Cloze Question Generator) to generate cloze questions using text-to-text transformer neural nets and WordNet. On a given article, CQG carries out the following tasks: select declarative sentences as stems according to their relative importance, select answer keys, segment an answer key into instances, determine each instance's lexical label with respect to the underlying synset after sense disambiguation, generate instance-level distractor candidates (IDCs) using a transformer and the hypernym-hyponym hierarchical structure, filter inappropriate IDCs using a number of syntactic and semantic indicators, rank the remaining IDCs according to similarities of contextual embeddings and synsets with the same lexical label, and combinatorially substitute top-ranked IDCs for the corresponding instance in the answer key to form new phrases. We filter these phrases to produce a pool of distractor candidates that conform to human writings. Finally, we select from the pool a fixed number of distractors that are contextually and semantically closest to the answer key.\nExperiments show that CQG significantly outperforms SOTA results [2] on unigram answer keys. For multigram answer keys and distractors, we note that the standard token-frequency-based measures are ill-suited to measuring multigram distractors, and there are no existing results to compare with. To overcome this obstacle, we construct a dataset with multigram answer keys in the same way as the dataset with unigram answer keys is constructed, and show that CQG generates distractors with over 81% contextual semantic similarity with the ground truth distractors. Human judges also confirm the high qualities of both unigram and multigram distractors generated by CQG.\nThe major contribution of this paper is the construction of the Cloze Question Generator (CQG), built on sentence ranking, text-to-text transformers, sense disambiguation, and WordNet's vocabulary synsets and lexical labels. CQG enhances contextual understanding, generates distractors of high quality, manages effectively and multigram answer keys, and produces more appropriate cloze questions.\nThe rest of the paper is organized as follows: We summarize in Section II related works. We describe in Section III the architecture of CQG, the selections of stems and answer keys, and the segmentation of multigram answer keys into a sequence of instances. We present in Section IV how to generate instance-level distractors, and describe in Section V-D how to generate distractors. We then present in Section VI datasets, experiments, and evaluation results. Section VII concludes the paper with final remarks."}, {"title": "RELATED WORKS", "content": "Research on finding distractors is along the following line:\nFind candidates based on answer keys and the underlying sentences, and select distractors from candidates according to certain metrics. Finding reasonable distractor candidates is challenging. A common approach extracts words and phrases that can serve as answer keys from (possibly domain-specific) vocabularies, thesauri, and taxonomies [2]\u2013[6], and for each answer key ranks the rest of the words and phrases according to certain criteria. These are extractive methods that search for distractor candidates from a text corpus or a knowledge base.\nWordNet is a large and growing knowledge base of English nouns, verbs, adjectives, and adverbs, as well as phrases, where words and phrases are grouped into sets of cognitive synonyms called synsets, with each synset expressing a distinct concept. Synsets are indexed and interconnected. A synset's parent node is called a hypernym synset (where there is no confusion, the word synset may be omitted) and its siblings are hyponyms. The relations of hypernyms and hyponyms of synsets are particularly useful for finding distractors. Probase [7] that supports semantic search is another knowledge base useful for finding distractors for a given answer key.\nIntuitively, an entry item in WordNet could be used as an answer key as well as a distractor for a different answer key, and entry items in a synset with the same hypernym of the answer key's synset are distractor candidates. This straightforward application of hypernym-hyponym structure may lead to distractors of wrong senses, and the following remedies have been attempted: (1) Use topic distributions such as those generated by the LDA topic model [8] to determine the sense of the answer key [2] and then use context search to select candidates with the corresponding sense [9]. WordNet CSG+DS and Probase CSG+DS [2], for example, are such attempts, where the former uses hypernyms and the latter uses is_a_relation to find candidates. (2) Identify distractors directly from the underlying text through visual and informal examination [10].\nDistractors are selected from candidates according to certain metrics of similarities to the answer key, including embedding- vector similarities [11] difficulty levels [12], WordNet-based metrics [13], and syntactic features [14]. Researchers also considered semantic relatedness of distractors with the whole stem and domain restrictions [15], [16], and explored machine-learning ranking models to select distractors and quantitatively evaluate the top generated distractors [2], [17], [18].\nDespite extensive efforts, the qualities of the distractors produced by existing methods are still below satisfactory. We present a generative method to improve satisfactions."}, {"title": "SELECTIONS OF STEMS AND ANSWER KEYS", "content": "CQG consists of three components as shown in Fig. 1: (1) Stem and Answer-key Selector (SAS). (2) Instance-level Distractor Generator (IDG). (3) Answer-key Distractor Generator (ADG)."}, {"title": "INSTANCE-LEVEL DISTRACTOR GENERATOR", "content": "IDG consists of three sub-components (see Fig. 3): (1) Transformer. (2) Sense Disambiguation. (3) Instance-level Distractor Candidate & Instance Info Extraction.\na) Transformer: We use a transformer's token-level prediction tasks (e.g., BERT [21]) to predict words that may serve as a substitute for the given instance. Let $S = w_1w_2...w_m$ be a sentence of n words with $w_i$ being the i-th word (a.k.a. token) and $S[i..j]$ the substring $w_i \u00b7\u00b7\u00b7 w_j$. Let $A = S[i..j]$ be an answer key and $U = S[p..q]$ with $i < p \u2264 q \u2264 j$ an instance of A. We mask U and denote the masked sentence by\n$S_U = S[1..p - 1][MASK]S[q + 1..n]$,\nand predict words in the place of U according to the surrounding context, with conditional probabilities in descending order.\nPredicted words and sibling entries of U obtained from WordNet are initial instance-level distractor candidates (IDCs) for U."}, {"title": "ANSWER-KEY DISTRACTOR GENERATOR", "content": "ADG consists of four sub-components (see Fig. 4): (1) Feature Filter. (2) IDC Ranker. (3) Ngram Filter. (4) Final Distractor Selector.\nA. Feature filter\nThe feature filter consists of a sequence of five checkers: POS, NER, Lexical, Synonym, and Inherited Hypernym & Hyponym Synset (IHHS).\na) POS and NER checkers: Remove Y if the POS tag or NER tag of Y differs from the corresponding tag of U. The requirement that Y and U should have the same part of speech follows the guidelines for writing up English vocabulary questions [28]. We extend this requirement to named entities.\nb) Lexical checker: Analyzing unigram distractors in the SciQ database [29], we find that in most cases, human-written distractors and the corresponding answer keys have the same lexical labels in WordNet, which happens about 94% of the time. Thus, we use WordNet's lexical groupings of synsets to reduce the range for finding IDCs. Synsets in WordNet are grouped into 45 lexicographer files, which are divided into 26 noun files with lexical labels from noun.Tops (unique beginner for nouns) to noun.time (nouns denoting time and temporal relations), 15 verb files with lexical labels from verb.body (verbs of grooming, dressing and bodily care) to verb.weather (verbs of raining, snowing, thawing, thundering), three adjective files, and one adverb file. Under this lexical grouping, each synset belongs to exactly one lexical group. For example, the eight synsets of dog are divided into five lexicographer files with the following lexical labels: noun.animal, noun.artifact, noun.food, noun.person, and verb.motion.\nDenote by $l_U$ the lexical label of U. Let $L_Y$ be the set of lexical labels for the lexicographer files containing the synsets of Y. Furthermore, let $l_Y$ be the lexical label of Y obtained by replacing U with Y in sentence S. The lexical checker removes Y if $l_U \u2260 l_Y$.\nWe may also use a relaxed version of lexical checker, for human-written distractors may have different lexical labels from that of the instance. For example, in the sentence \"The average human body contains 5,830 g of blood\" with the answer key blood whose lexical label is noun.body, the following distractors are provided by human writers: (1) muscle (noun.body), (2) bacteria (noun.animal), and (3) water (noun.substance), where lexical labels with respect to the sentence are parenthesized. The relaxed lexical checker checks if $l_U \u2208 L_Y$. If no, remove Y. Nevertheless, in our experiments, we will use the stronger version.\nc) Synonym checker: Remove Y if Y is a synonym of U, i.e, if the synset of Y is the same as that of U.\nd) IHHS checker: Finally, let $H_U$ be the set of U's inherited hypernym and hyponym synsets and $S_Y$ the set of Y's synsets. Remove Y if $H_U \u2229 S_Y = \u2205$."}, {"title": "IDC Ranker", "content": "Let C be an L-IDC. We use the contextual embedding similarity of U and C as the baseline ranking of C, denoted by $E_S(U,C)$. We define WordNet synset similarity reward score $W_S(U, C)$ and prediction reward score $P_S(C)$ to adjust the baseline ranking score. The ranking of C, denoted by $R_S(U,C)$, is defined to be the product of these metrics:\n$R_S(C) = E_S (U, C) \u00b7 W_S(U, C) \u00b7 P_S(C)$.\nLet $e_S(U)$ denote the contextual embedding of U with respect to S obtained from a text-to-text transformer such as BERT, and define $e_S(C)$ similarly. Denote by $E_S(U,C)$ the cosine similarity of $e_S(U)$ and $e_S(C)$ as follows, with \u2022 representing the dot product and $||x||_2$ the Euclidean norm of x:\n$E_S (U, C) = \\frac{e_S(U) \\cdot e_S(C))}{||e_S(U)||_2 \\cdot ||e_S (C)||_2}$        (1)"}, {"title": "Ngram filter", "content": "For each instance $U_i$ we consider top m C's according to $R_S(C)$ (e.g., let m = 10). Let the answer key be a sequence of instances $U_1\u00b7\u00b7\u00b7U_k$. To form a distractor candidate, select an instance $U_i$ with $i \u2208 [1,k]$. Let $P_i = {C_{i_1},..., C_{i_m}}$ be the set of L-IDCs for $U_i$. We replace $U_i$ with a $C_{i_j} \u2208 P_i$ for one or more instances combinatorially to produce a set of new phrases. This means that we look at all possible ways of replacing instances with L-IDCs. For example, suppose that [A, B] is an answer key with two instances A and B. Suppose that C and D are IDCs for A and B, respectively, then [A, D], [C, B], and [C, D] are all combinations. To determine if a new phrase conforms to human writing, we use Google's Ngram Viewer [30] to check if the new phrase returns a result. If yes, then it confirms that the new phrase has been written by a human writer and so we keep it as a distractor candidate. If no, we remove it."}, {"title": "Final distractor selector", "content": "Let D be a distractor candidate that survives the Ngram filter. We define the ranking of D with respect to A under S, denoted by $R_S(A, D)$, to be the cosine similarity of the contextual embeddings $e_S(A)$ and $e_S(D)$. FDS outputs the top n distractor candidates as distractors for A with n = 3 by default."}, {"title": "Remark on distractor length", "content": "Existing methods typically require that the length of a distractor be the same as that of the answer key to ensure that both have the same sequence of POS tags of words. We note that, however, an appropriate distractor may or may not be of the same length of the answer key. Our method can generate distractors of different lengths, as the length of a sibling entry of an instance may differ. If the underlying text-to-text transformer can predict phrases this can be done, e.g., by retraining a BERT model on datasets segmented according to phrases, we may also obtain distractor candidates of different lengths. Moreover, we may remove some or all premodifying instances at random to reduce the length of the answer key."}, {"title": "EVALUATIONS", "content": "We implement CQG, run numerical experiments, and carry out ablation results on different ranking methods. We use BERT as text-to-text transformer and WordNet version 3.0 to train CQG.\nA. Datasets\nWe use two datasets for evaluating CQG: U-SciQ and M-SciQ, where U-SciQ consists of cloze questions with only unigram answer keys and M-SciQ consists of cloze questions with only multigram answer keys. Both datasets are separated from the same dataset provided by Ren et al [2], denoted by SciQ+, which was compiled from SciQ [29], MCQL [18], AI2 Science Questions [31], and vocabulary MCQs crawled from the Internet, with unigram and multigram answer keys."}, {"title": "Model training", "content": "We train four variants of CQG models: CQG-E is CQG using the baseline ranking method for L-IDCs, CQG-E/W, CQG-E/P, and CQG-E/W/P are CQG using E and W, E and P, and all ranking metrics, respectively. Namely, CQG is CQG-E/W/P. In particular, we divide U-SciQ with a 3-1 split and apply grid search with cross validation to find the best values of the parameters that achieve the highest F1 scores, with an increment of 0.1. The trained values are as follows: \u03b1 = 5.2 for CQG-E/W, \u03b2 = 1.1 for CQG-E/P, and \u03b1 = 20.5, \u03b2 = 1.1 for CQG-E/W/P.\nRemark. Our evaluations use the stronger version of lexical checker, which removes candidates with lexical labels different from that of the answer key. However, good distractors may have different lexical labels. For example, in stem \"A bee will sometimes do a dance to tell other bees in the hive where to find **blank**.\" The answer key is food and the ground- truth distractors are honey, enemies, and water. The distractor enemies has a different lexical label from food. The relaxed version allows us to keep such distractors. If we use the relaxed lexical checker, we would need to add a penalty score as follows by multiplying a lexical penalty score $L_S(U,C)$ to the rank $R_S (U, C)$, with $L_S(U, C)$: \n$L_S (U,C) = \\begin{cases} 1, & \\text{if } l_S(C) \\neq l_S(U), \\\\ \\gamma, & \\text{otherwise,} \\end{cases}$   (4)\nwhere \u03b3\u2208 [0, 1] is to be determined via grid search."}, {"title": "Intrinsic evaluation", "content": "a) Over U-SciQ: We apply the same evaluation metrics used by Ren et al [2] to carry out intrinsic evaluations on U-SciQ, averaging over top 3 generated distractors with the 3 ground-truth distractors pairwise unless otherwise stated. These metrics include F1 score (F1), precision (P@1, P), recall (R), mean reciprocal rank (MRR), normalized discounted cumulative gain (NDCG@10), and Word2Vec Semantic Similarity (WSS), where @k means averaging over top k distractors against the three ground truth distractors pairwise, and WSS is the average cosine similarity of the top three generated distractors and ground truth distractors with Word2Vec embeddings trained on a Wikipedia dump. However, WSS does not represent words of multiple senses, and so we also use Contextual Semantic Similarity (CSS), which is the average cosine similarity of the top three generated distractors and ground truth distractors with embeddings generated by BERT for the underlying sentences."}, {"title": "Extrinsic evaluation", "content": "Three human judges evaluate distractors' reliability and plausibility for cloze questions generated over U-SciQ and M- SciQ with the following guidelines: A distractor receives a reliability score of 1 if placing the distractor in the blank space of the stem produces a contextually fit and grammatically correct yet logically wrong sentence, and 0 otherwise. For example, if the stem is about biology but a distractor is in the domain of physics, then the the reliability score of this distractor should be 0 even if the new sentence is grammatically correct, because it is contextually unfit. The judges assess the plausibility of a distractor on a 3-point scale: a distractor receives 0 points if it is obviously wrong, 2 points if it is sufficiently confusing as to which is the absolute correct answer, and 1 point if it is somewhat confusing.\nThe judges are presented with the evaluation dataset of U- SciQ and M-SciQ, where each data point consists of a stem, the corresponding answer keys and 3 ground-truth distractors, mixing with 3 CQG-generated distractors, but the judges do not know which distractors are ground truth and which are CQG generated. In so doing we hope to achieve unbiased judging. All judges have written exam questions with at least three years of teaching experience. We compare judges' scores on CQG-generated distractors against ground-truth distractors."}, {"title": "CONCLUSIONS AND FINAL REMARKS", "content": "CQG generates cloze questions of high quality and surpasses the state-of-the-art results. Techniques used in CQG can also be used to generate distractors for multiple-choice questions by forming interrogative sentences using transformers based on an answer key and surrounding declarative statements [32]. To facilitate further research, we have published the CQG API, datasets, and assessment results by judges on https://github.com/clozeQ/A-Generative- Method-for-Producing-Distractors-for-Cloze-Questions.\nThe qualities of distractors generated by CQG depend on a number of factors, including sense disambiguation and Word- Net taxonomies. As better sense disambiguation algorithms are developed and more words and phrases are added to WordNet, CQG is expected to generate distractors with higher qualities.\nThe dataset SciQ+ consists of stems converted from interrogative sentences, where a number of conversions are erroneous. For example, the stem **blank** of devices scientists use to determine wind speed is likely converted from what kind of device do scientists use to determine wind speed, with anemometer being the answer key. For this interrogative- sentence-like stem, our algorithm generates the following distractors: type, tool, mechanism, which are clearly not reliable. Using the corrected stem scientists use **blank** to determine wind speed, our algorithm generates the following appropriate distractors: GPS, vacuum gage, binoculars. We have posted on the aforementioned github link a cleaned version of the dataset to ensure that each stem is in the appropriate declarative form for future studies."}]}