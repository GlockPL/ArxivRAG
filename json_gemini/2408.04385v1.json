{"title": "Non-maximizing policies that fulfill\nmulti-criterion aspirations in expectation", "authors": ["Simon Dima", "Simon Fischer", "Jobst Heitzig", "Joss Oliver"], "abstract": "In dynamic programming and reinforcement learning, the\npolicy for the sequential decision making of an agent in a stochastic\nenvironment is usually determined by expressing the goal as a scalar\nreward function and seeking a policy that maximizes the expected total\nreward. However, many goals that humans care about naturally concern\nmultiple aspects of the world, and it may not be obvious how to condense\nthose into a single reward function. Furthermore, maximization suffers\nfrom specification gaming, where the obtained policy achieves a high\nexpected total reward in an unintended way, often taking extreme or\nnonsensical actions.\nHere we consider finite acyclic Markov Decision Processes with multiple\ndistinct evaluation metrics, which do not necessarily represent quanti-\nties that the user wants to be maximized. We assume the task of the\nagent is to ensure that the vector of expected totals of the evaluation\nmetrics falls into some given convex set, called the aspiration set. Our\nalgorithm guarantees that this task is fulfilled by using simplices to ap-\nproximate feasibility sets and propagate aspirations forward while en-\nsuring they remain feasible. It has complexity linear in the number of\npossible state-action-successor triples and polynomial in the number of\nevaluation metrics. Moreover, the explicitly non-maximizing nature of\nthe chosen policy and goals yields additional degrees of freedom, which\ncan be used to apply heuristic safety criteria to the choice of actions. We\ndiscuss several such safety criteria that aim to steer the agent towards\nmore conservative behavior.", "sections": [{"title": "1 Introduction", "content": "In typical reinforcement learning (RL) and dynamic programming problems an\nagent is trained or programmed to solve tasks encoded by a single real-valued\nreward function that it shall maximize. However, many tasks are not easily\nexpressed by such a function [12], human preferences are hard to learn and may\nnot be easy to aggregate across stakeholders [5], and maximizing a misspecified\nobjective may fall prey to reward hacking [1] and Goodhart's law [11], leading\nto unintended side-effects and potentially harmful consequences.\nIn this work, we study a particular aspiration-based approach to agent design.\nWe assume an existing task-specific world model in the form of a fully observed\nMarkov Decision Process (MDP), where the task is not encoded by a reward\nfunction but instead a multi-criterion evaluation function and a bounded, con-\nvex subset of its range, called an aspiration set, that can be thought of as an\n\"instruction\" [4] to the agent. Aspiration-type goals can also naturally arise from\nsubtasks in complex environments even if the overall goal is to maximize some\nobjective, when the complexity requires a hierarchical decision-making approach\nwhose highest level selects subtasks that turn into aspiration sets for lower hier-\narchal levels.\nIn our version of aspiration-based agents, the goal is to make the expected\nvalue of the total with respect to this evaluation function fall within the aspi-\nration set, and select from this set according to certain performance and safety\ncriteria. They do so step-wise, exploiting recursion equations similar to the Bell-\nman equation. Thus our approach is like multi-objective reinforcement learning\n(MORL), with a primary aspiration-based objective and at least one secondary\nobjective incorporated via action-selection criteria [16]. Unlike MORL, the com-\nponents of the evaluation function (called evaluation metrics) are not objectives\nin the sense of targets for maximization. Rather, an aspiration formulated w.r.t.\nseveral evaluation metrics might correspond to a single objective (e.g., \"make\na cup of tea\"). Also, at no point does an aspiration-based agent aggregate the\nevaluation metrics into a single value. Instead, any trade-offs are built into the\naspiration set itself, similar to what [6] call a \"safety specification\". For example,\naspiring to buy a total of 10 oranges and/or apples for at most EUR 1 per item\ncould be encoded with the aspiration set {(o, a, c) : o, a > 0; c < o + a = 10}.\nA similar set-up to ours has been used in [9] which has used reinforcement\nlearning to find a policy whose expected discounted reward vector lies inside\na convex set. Instead of a reinforcement learning perspective, we use a model-\nbased planning perspective and design an algorithm that explicitly calculates a\npolicy for solving the task, based on a model of the environment. 5 Also, the\napproach in [9] is concerned with guaranteed bounds for the distance between\nreceived rewards and the convex constraints in terms of the number of iterations,\nwhereas we focus on guaranteeing aspiration satisfaction in a fixed number of\ncomputational steps, providing a verifiable guarantee in the sense of [6].\nOther agent designs that follow a non-maximization-goal-based approach in-\nclude quantilizers, decision transformers and active inference. Quantilizers are\nagents that use random actions in the top n% of a \"base distribution\" over ac-\ntions, sorted by expected return [13]. The goal for decision transformers is to\nmake the expected return equal a particular value $R_{\\text{target}}$ [3]. The goal for active\ninference agents is to produce a particular probability distribution of observa-\ntions [14]. While the goal space for quantilizers and decision transformers, being\nbased on a single real-valued function, is often too restricted for many applica-\ntions, that of active inference agents (all probability distributions) appears too\nwide for the formal study of many aspects of aspiration-based decision-making.\nOur approach is of intermediary complexity.\nAn important consideration in this work, to ensure tractability in large envi-\nronments, is also the computational complexity in the number of actions, states,\nand evaluation metrics. We will see that for our algorithm, the preparation of an\nepisode has linear complexity in the number of possible state-action-successor\ntransitions and (conjectured and numerically confirmed) linear average complex-\nity in the number of evaluation metrics, and then the additional per-time-step\ncomplexity of the policy is linear in the number of actions, constant in the num-\nber of states, and polynomial in the number of evaluation metrics.\nOur work also affects the emerging AI safety/alignment field, which views un-\nintended consequences from maximization, e.g., reward hacking and Goodhart's\nlaw, as a major source of risk once agentic AI systems become very capable [1]."}, {"title": "2 Preliminaries", "content": "Environment. An environment $E = (S, s_0, S_T, A, T)$ is a finite Markov Decision\nProcess without a reward function, consisting of a finite state space $S$, an initial\nstate $s_0 \\in S$, a nonempty subset $S_T \\subseteq S$ of terminal states, a nonempty finite\naction space $A$, and a function $T : (S \\backslash S_T) \\times A \\rightarrow \\Delta(S \\backslash \\{s_0\\})$ specifying\ntransition probabilities: $T(s,a)(s')$ is the probability that taking action $a$ from\nstate $s$ leads to state $s'$. We assume that the environment is acyclic, i.e., that it is\nimpossible to reach a given state again after leaving it. We fix some environment\n$E$ and write $s' \\sim s, a$ to denote that $s'$ is distributed according to $T(s, a)$.\nPolicy. A (memory-based) policy is given by some nonempty finite set $M$ of\nmemory states internal to the agent, an initial memory state $m_0 \\in M$ and a\nfunction $\\pi: M \\times (S\\backslash S_T) \\rightarrow \\Delta(A \\times M)$ that maps each possible combination\nof memory state $m \\in M$ and (environment) state $s \\in S \\backslash S_T$ to a probability\ndistribution over combinations of actions $a \\in A$ and successor memory states\n$m' \\in M$. Let $\\Pi_M$ be the set of all policies with memory space $M$. The special\nclass of Markovian or memoryless policies is obtained when $M$ is a singleton.\nPolicies which are both Markovian and deterministic are called pure Markov\npolicies, and amount to a function $(S \\backslash S_T) \\rightarrow A$. We denote by $\\Pi^0$ the set of\nMarkovian policies and by $\\Pi^P$ the set of all pure Markov policies.\nEvaluation, Delta, Total. A (multi-criterion) evaluation function for the envi-\nronment is a function $f: (S\\backslash S_T) \\times A \\times (S \\backslash \\{s_0\\}) \\rightarrow \\mathbb{R}^d$ where $d > 1$. The\nquantity $f(s, a, s')$ is called the Delta received under transition $(s, a) \\rightarrow s'$. It\nrepresents by how much certain evaluation metrics change when the agent takes\naction $a$ in state $s$ and the successor state is $s'$. Let us fix $f$ for the rest of the pa-\nper. The (received) Total of a trajectory $h = (m_0, s_0, a_1, m_1, s_1,..., a_T, m_T, s_T)$"}, {"title": "3 Fulfilling aspirations", "content": "Aspirations, feasibility. An (initial) aspiration is a convex polytope $E_0 \\subset \\mathbb{R}^d$,\nrepresenting the values of the expected total $E\\tau$ which are considered acceptable.\nWe say that a policy $\\pi$ fulfills the aspiration when it satisfies $V^{\\pi}(m_0, s_0) \\in E_0$.\nTo answer the question of whether it is possible to fulfill a given aspiration, we\nintroduce feasibility sets. The state-feasibility set of $s \\in S$ is the set of possible\nvalues for the expected future Total from $s$, under any memory-based policy:\n$V(s) = \\{V^{\\pi}(m, s) | M \\text{ finite set; } m_0, m \\in M; \\pi \\in \\Pi_M\\}$; likewise, we define the\naction-feasibility set $Q(s, a)$. It is straightforward to verify that $V(s \\in S_T) = \\{0\\}$,\nand that the following recursive equations hold for $s \\neq S_T$:\n$Q(s, a) = E_{s'\\sim s, a}(f(s, a, s') + V(s')) = \\sum_{s'} T(s, a)(s')\\cdot(f(s, a, s') + V(s')),(4)$\n$V(s) = \\bigcup_{p \\in \\Delta(A)} E_{a \\sim p} Q(s, a) = \\bigcup_{p \\in \\Delta(A)} \\sum_{a \\in A}p(a)Q(s, a). (5)$\nIn this, we use set arithmetic: $rX + r'X' = \\{rx + r'x' | x \\in X, x' \\in X'\\}$ for\n$r, r' \\in \\mathbb{R}$ and $X, X' \\subset \\mathbb{R}^d$. It is clear that feasibility sets are convex polytopes.\nAspiration propagation. Algorithm scheme 1 shows a general manner of fulfilling\na feasible initial aspiration, starting from a given state or state-action pair. It\nmemorizes and updates aspirations $\\mathcal{E}$ and $\\mathcal{E}_a$, initially equalling $E_0$. The agent"}, {"title": "4 Propagating aspirations", "content": "4.1\nPropagating action-aspirations to state-aspirations\nTo implement algorithm schema 1, we first focus on line 5 in procedure FUL-\nFILLACTIONASPIRATION, which is the easier part. Given a state-action pair\n$s, a$ and an action-aspiration set $\\mathcal{E}_a \\subseteq Q^\\mathcal{R}(s, a)$, we must construct nonempty\nstate-aspiration sets $\\mathcal{E}_{s'} \\subseteq V^\\mathcal{R}(s')$ for all possible successor states $s'$, such that\n$E_{s'\\sim s, a}(f(s, a, s') + \\mathcal{E}_{s'}) \\subseteq \\mathcal{E}_a$. We assume that all reference simplices are nonde-\ngenerate, i.e. have full dimension $d$. This is almost surely the case if there are\nsufficiently many actions and these have enough possible consequences.\nTracing maps. Under this assumption, we define tracing maps $\\rho_{s, a, s'}$ from the\nreference simplex $Q^\\mathcal{R}(s, a)$ to $V^\\mathcal{R}(s')$. Since domain and codomain are simplices,\nwe can choose $\\rho_{s, a, s'}$ to be the unique affine linear map that maps vertices to\nvertices, $\\rho_{s, a, s'}(Q_i(s,a)) = V_i(s')$. For any point $e \\in Q^\\mathcal{R}(s, a)$, it follows from\nequation (2) that $E_{s'\\sim s, a}(f(s, a, s') + \\rho_{s, a, s'}(e)) = e$. Accordingly, to propagate\naspirations of the form $\\mathcal{E}_a = \\{e\\}$, it is sufficient to just set $\\mathcal{E}_{s'} = \\{\\rho_{s, a, s'}(e)\\}$. How-\never, for general subsets $\\mathcal{E}_a \\subseteq Q^\\mathcal{R}(s, a)$, the set $E_{s'\\sim s, a}(f(s, a, s') +\\rho_{s, a, s'}(\\mathcal{E}_a))$ is\nin general strictly larger than $\\mathcal{E}_a$, and hence we map $\\mathcal{E}_a$ in a different way.\nFor this, choose an arbitrary \"anchor\" point $e \\in \\mathcal{E}_a$; here we let $e$ be the\naverage of the vertices, $C(\\mathcal{E}_a)$, but any other standard type of center would also\nwork (e.g. analytic center, center of mass/centroid, Chebyshev center). Now, let\n$\\mathcal{X}_{s'} = \\mathcal{E}_a - e + \\rho_{s, a, s'}(e)$ be shifted copies of the action-aspiration. We would like\nto use the $\\mathcal{X}_{s'}$ as state-aspirations, and indeed they have the property that\n$E_{s'\\sim s, a}(f(s, a, s') + \\mathcal{X}_{s'}) = E_{s'\\sim s, a}(f(s, a, s') + \\rho_{s,a,s'}(e) - e + \\mathcal{E}_a)\n= e = e + E_{s'\\sim s, a}(\\mathcal{E}_a) = \\mathcal{E}_a \\text{ as } \\mathcal{E}_a \\text{ is convex. (9)}$"}, {"title": "5 Determining appropriate reference policies", "content": "First, find some feasible aspiration point $x \\in E_0 \\cap V(s_0)$ (e.g., using binary\nsearch). We now aim to find policies whose values are likely to contain $x$ in their\nconvex hull, by using backwards induction and greedily minimizing the angle\nbetween the vector $\\mathcal{E}\\tau - x$ and a suitable direction in evaluation space.\nMore precisely: Pick an arbitrary direction (unit vector) $y_1 \\in \\mathbb{R}^d$, e.g., uni-\nformly at random. Then, for $k = 1,2,...$ until stopping:\n1. Let $\\pi_k$ be the pure Markovian policy defined by backwards induction as\n$\\pi(s) = \\text{argmax}_a \\frac{y\\cdot (Q^{\\pi_k}(s, a) - x)}{||Q^{\\pi_k}(s, a) - x||^2}. (11)$\nLet $v_k = V^{\\pi_k}(s_0)$ be the resulting candidate vertex for our reference simplex.\nIf $k > d + 1$, run a primal-sparse linear program solver [18] to determine if\n$x \\in \\text{conv}\\{v_1,..., v_k\\}$. If so, the solver will return a basic feasible solution,\ni.e. $d + 1$ many vertices that contain $x$ in their convex hull. Let the policies\ncorresponding to the vertices that are part of the basic feasible solution be\nour reference policies and stop. Otherwise, continue to step 2 below:\n2. Let $e_k = (x - v_k)/||x - v_k||^2$ be the unit vector in the direction from $v_k$ to $x$.\n3. Let $y_{k+1} = \\sum_{i=1}^k e_i/k$ be the average of all those directions. Assuming that\n$x \\in V(s_0)$ and because of the hyperplane separation theorem, choosing di-\nrections like this ensures that the algorithm doesn't loop with $v_{i+l} = v_i$ for\nsome $l$. Also, note that to check $x \\in \\text{conv}\\{v_1,..., v_k, v_{k+1}\\}$ it is sufficient\nto check $v_{k+1} - x \\in \\text{conv}\\{x - v_1,..., x - v_k\\}$, the cone generated by the\nnegative of the vertices centred at $x$. So hunting for a policy whose value lies\napproximately in the direction $y_{k+1}$ from $x$ gives us a good chance of finding\na vertex in the aforementioned cone."}, {"title": "6 Selection of candidate actions", "content": "As we have seen in section 4.2 (ii), when choosing actions, we still have many\nremaining degrees of freedom. Thus, we can use additional criteria to choose\nactions while still fulfilling the aspirations. We discuss a few candidate criteria\nhere which are related either to gaining information, improving performance, or\nreducing potential safety-related impacts of implementing the policy.\nFor many of the criteria, there are myopic versions, which only rely on quan-\ntities that are already available at each step in the algorithms presented so far,\nor farsighted versions which depend on the continuation policy and thus have to\nbe specifically computed recursively via Bellman-style equations.\nInformation-related criteria. If the used world model is imperfect, one might\nwant the agent to aim to gain knowledge by exploration, e.g. by considering\nsome measure of expected information gain such as the evidence lower bound.\n6.1\nPerformance-related criteria\nFor now, the task of the agent in this paper has been given by specifying aspira-\ntion sets for the expected total of the evaluation function. It is natural to consider\nextensions of this approach to further properties of the trajectory distribution,\ne.g. by specifying that the variance of the total should be small.\nA simple, myopic approach to reducing variance is preferring actions and\naction-aspirations that are somehow close to the state aspiration $\\mathcal{E}$, e.g. by choos-\ning action-aspirations where the Hausdorff distance $d_H(\\mathcal{E}_a, \\mathcal{E})$ is small. A more\nprincipled, farsighted approach would be choosing actions and action-aspirations\nsuch that the variance of the resulting total is small. Based on equation (2), the\nvariance can be computed from the total raw second moment $M^{\\pi}$ as\n$M^{\\pi}(s, a, \\mathcal{E}_a) = E_{s', \\mathcal{E}_{s'}\\sim s, a, \\mathcal{E}_a}[||f(s, a, s')||^2 + 2f(s, a, s') \\cdot V^{\\pi} (s', \\mathcal{E}_{s'})\n+ E_{a', \\mathcal{E}_{a'}\\sim \\pi(s',\\mathcal{E}_{s'})} M^{\\pi} (s', a', \\mathcal{E}_{a'})], (12)$\n$\\text{Var}(s, a, \\mathcal{E}_a) = M^{\\pi}(s, a, \\mathcal{E}_a) - ||Q^{\\pi} (s, a, \\mathcal{E}_a)||^2. (13)$\nNote that computing this farsighted metric requires knowing the continuation\npolicy $\\pi$, for which algorithm 2 does not suffice in its current form as it only\nsamples actions. It is however easy to convert it to an algorithm for computing\nthe whole local policy $\\pi(s,\\mathcal{E})$, which is described in the Supplement.\n6.2\nSafety-related criteria\nAs mentioned in the introduction, unintended consequences of optimization can\nbe a source of safety problems, thus we suggest to not use any of the criteria\nintroduced in this section as maximization/minimization goals to completely\ndetermine the chosen actions; instead, they can be combined into a loss for a\n$\\text{softmax}$ action selection policy $\\pi_i(a \\in A_i) \\propto \\text{exp} \\left( - \\beta \\sum_j a_j g_j(a) \\right)$, where $g_j(a)$\nare the individual criteria. Indeed, in analogy to quantilizers, choosing among\nadequate actions at random can by itself be considered a useful safety measure,\nas a random action is very unlikely to be special in a harmful way."}, {"title": "7 Discussion and conclusion", "content": "7.1 Special cases\nA single evaluation metric. It is natural to ask what our algorithm reduces\nto in the single-criterion case $d = 1$. The reference simplices can then sim-\nply be taken to be the intervals $V^\\mathcal{R}(s) = [V_{min}(s), V_{max}(s)]$ and $Q^\\mathcal{R}(s, a) ="}]}