{"title": "Non-maximizing policies that fulfill\nmulti-criterion aspirations in expectation", "authors": ["Simon Dima", "Simon Fischer", "Jobst Heitzig", "Joss Oliver"], "abstract": "In dynamic programming and reinforcement learning, the\npolicy for the sequential decision making of an agent in a stochastic\nenvironment is usually determined by expressing the goal as a scalar\nreward function and seeking a policy that maximizes the expected total\nreward. However, many goals that humans care about naturally concern\nmultiple aspects of the world, and it may not be obvious how to condense\nthose into a single reward function. Furthermore, maximization suffers\nfrom specification gaming, where the obtained policy achieves a high\nexpected total reward in an unintended way, often taking extreme or\nnonsensical actions.\nHere we consider finite acyclic Markov Decision Processes with multiple\ndistinct evaluation metrics, which do not necessarily represent quanti-\nties that the user wants to be maximized. We assume the task of the\nagent is to ensure that the vector of expected totals of the evaluation\nmetrics falls into some given convex set, called the aspiration set. Our\nalgorithm guarantees that this task is fulfilled by using simplices to ap-\nproximate feasibility sets and propagate aspirations forward while en-\nsuring they remain feasible. It has complexity linear in the number of\npossible state-action-successor triples and polynomial in the number of\nevaluation metrics. Moreover, the explicitly non-maximizing nature of\nthe chosen policy and goals yields additional degrees of freedom, which\ncan be used to apply heuristic safety criteria to the choice of actions. We\ndiscuss several such safety criteria that aim to steer the agent towards\nmore conservative behavior.", "sections": [{"title": "1 Introduction", "content": "In typical reinforcement learning (RL) and dynamic programming problems an\nagent is trained or programmed to solve tasks encoded by a single real-valued\nreward function that it shall maximize. However, many tasks are not easily"}, {"title": "2 Preliminaries", "content": "Environment. An environment $E = (S, s_0, S_T, A, T)$ is a finite Markov Decision\nProcess without a reward function, consisting of a finite state space $S$, an initial\nstate $s_0 \\in S$, a nonempty subset $S_T \\subseteq S$ of terminal states, a nonempty finite\naction space $A$, and a function $T : (S \\setminus S_T) \\times A \\rightarrow \\Delta(S \\setminus \\{s_0\\})$ specifying\ntransition probabilities: $T(s,a)(s')$ is the probability that taking action $a$ from\nstate $s$ leads to state $s'$. We assume that the environment is acyclic, i.e., that it is\nimpossible to reach a given state again after leaving it. We fix some environment\n$E$ and write $s' \\sim s, a$ to denote that $s'$ is distributed according to $T(s, a)$.\nPolicy. A (memory-based) policy is given by some nonempty finite set $M$ of\nmemory states internal to the agent, an initial memory state $m_0 \\in M$ and a\nfunction $\\pi: M \\times (S\\setminus S_T) \\rightarrow \\Delta(A \\times M)$ that maps each possible combination\nof memory state $m \\in M$ and (environment) states $s \\in S \\setminus S_T$ to a probability\ndistribution over combinations of actions $a \\in A$ and successor memory states\n$m' \\in M$. Let $\\Pi_M$ be the set of all policies with memory space $M$. The special\nclass of Markovian or memoryless policies is obtained when $M$ is a singleton.\nPolicies which are both Markovian and deterministic are called pure Markov\npolicies, and amount to a function $(S \\setminus S_T) \\rightarrow A$. We denote by $\\Pi^0$ the set of\nMarkovian policies and by $\\Pi^P$ the set of all pure policies.\nEvaluation, Delta, Total. A (multi-criterion) evaluation function for the envi-\nronment is a function $f: (S\\setminus S_T) \\times A \\times (S \\setminus \\{s_0\\}) \\rightarrow \\mathbb{R}^d$ where $d > 1$. The\nquantity $f(s, a, s')$ is called the Delta received under transition $(s, a) \\rightarrow s'$. It\nrepresents by how much certain evaluation metrics change when the agent takes\naction $a$ in state $s$ and the successor state is $s'$. Let us fix $f$ for the rest of the pa-\nper. The (received) Total of a trajectory $h = (m_0, s_0, a_1, m_1, s_1, ..., a_T, m_T, s_T)$"}, {"title": "3 Fulfilling aspirations", "content": "Aspirations, feasibility. An (initial) aspiration is a convex polytope $E_0 \\subset \\mathbb{R}^d$,\nrepresenting the values of the expected total $E\\tau$ which are considered acceptable.\nWe say that a policy $\\pi$ fulfills the aspiration when it satisfies $V^{\\pi}(m_0, s_0) \\in E_0$.\nTo answer the question of whether it is possible to fulfill a given aspiration, we\nintroduce feasibility sets. The state-feasibility set of $s \\in S$ is the set of possible\nvalues for the expected future Total from $s$, under any memory-based policy:\n$V(s) = \\{V^{\\pi}(m, s) \\vert M \\text{ finite set}; m_0, m \\in M; \\pi \\in \\Pi_M\\}$; likewise, we define the\naction-feasibility set $Q(s, a)$. It is straightforward to verify that $V(s \\in S_T) = \\{0\\}$,\nand that the following recursive equations hold for $s \\neq S_T$:\n$Q(s, a) = \\mathbb{E}_{s' \\sim s, a}(f(s, a, s') + V(s')) = \\sum_{s'} T(s, a)(s') \\cdot (f(s, a, s') + V(s')),$   (4)\n$V(s) = \\bigcup_{p \\in \\Delta(A)} \\mathbb{E}_{a \\sim p} Q(s, a) = \\bigcup_{p \\in \\Delta(A)} \\sum_{a \\in A} p(a) Q(s, a).$   (5)\nIn this, we use set arithmetic: $rX + r'X' = \\{rx + r'x' \\vert x \\in X, x' \\in X'\\}$ for\n$r, r' \\in \\mathbb{R}$ and $X, X' \\subset \\mathbb{R}^d$. It is clear that feasibility sets are convex polytopes.\nAspiration propagation. Algorithm scheme 1 shows a general manner of fulfilling\na feasible initial aspiration, starting from a given state or state-action pair. It\nmemorizes and updates aspirations $\\mathcal{E}$ and $\\mathcal{E}_a$, initially equalling $E_0$. The agent"}, {"title": "4 Propagating aspirations", "content": "To implement algorithm schema 1, we first focus on line 5 in procedure FUL-\nFILLACTIONASPIRATION, which is the easier part. Given a state-action pair\n$s, a$ and an action-aspiration set $\\mathcal{E}_a \\subseteq Q_R(s, a)$, we must construct nonempty\nstate-aspiration sets $\\mathcal{E}_{s'} \\subseteq V_R(s')$ for all possible successor states $s'$, such that\n$\\mathbb{E}_{s'\\sim s, a}(f(s, a, s') + \\mathcal{E}_{s'}) \\subseteq \\mathcal{E}_a$. We assume that all reference simplices are nonde-\ngenerate, i.e. have full dimension $d$. This is almost surely the case if there are\nsufficiently many actions and these have enough possible consequences.\nTracing maps. Under this assumption, we define tracing maps $\\rho_{s,a,s'}$ from the\nreference simplex $Q_R(s, a)$ to $V_R(s')$. Since domain and codomain are simplices,\nwe can choose $\\rho_{s,a,s'}$ to be the unique affine linear map that maps vertices to\nvertices, $\\rho_{s,a,s'}(Q_i(s,a)) = V_i(s')$. For any point $e \\in Q_R(s, a)$, it follows from\nequation (2) that $\\mathbb{E}_{s'\\sim s, a}(f(s, a, s') + \\rho_{s,a,s'}(e)) = e$. Accordingly, to propagate\naspirations of the form $\\mathcal{E}_a = \\{e\\}$, it is sufficient to just set $\\mathcal{E}_{s'} = \\{\\rho_{s,a,s'}(e)\\}$. How-\never, for general subsets $\\mathcal{E}_a \\subseteq Q_R(s, a)$, the set $\\mathbb{E}_{s'\\sim s, a}(f(s, a, s') + \\rho_{s,a,s'}(\\mathcal{E}_a))$ is\nin general strictly larger than $\\mathcal{E}_a$, and hence we map $\\mathcal{E}_a$ in a different way.\nFor this, choose an arbitrary \"anchor\u201d point $e \\in \\mathcal{E}_a$; here we let $e$ be the\naverage of the vertices, $C(\\mathcal{E}_a)$, but any other standard type of center would also\nwork (e.g. analytic center, center of mass/centroid, Chebyshev center). Now, let\n$\\mathcal{X}_{s'} = \\mathcal{E}_a - e + \\rho_{s,a,s'}(e)$ be shifted copies of the action-aspiration. We would like\nto use the $\\mathcal{X}_{s'}$ as state-aspirations, and indeed they have the property that\n$\\mathbb{E}_{s'\\sim s, a}(f(s, a, s') + \\mathcal{X}_{s'}) = \\mathbb{E}_{s'\\sim s, a}(f(s, a, s') + \\rho_{s,a,s'}(e) - e + \\mathcal{E}_a)$\n$= e = e + \\mathbb{E}_{s'\\sim s, a}(\\mathcal{E}_a) = \\mathcal{E}_a$ as $\\mathcal{E}_a$ is convex.\t\t(8)\t\t(9)"}, {"title": "5 Determining appropriate reference policies", "content": "First, find some feasible aspiration point $x \\in E_0 \\cap V(s_0)$ (e.g., using binary\nsearch). We now aim to find policies whose values are likely to contain $x$ in their\nconvex hull, by using backwards induction and greedily minimizing the angle\nbetween the vector $E\\tau - x$ and a suitable direction in evaluation space.\nMore precisely: Pick an arbitrary direction (unit vector) $y_1 \\in \\mathbb{R}^d$, e.g., uni-\nformly at random. Then, for $k = 1,2,...$ until stopping:\n1. Let $\\pi_k$ be the pure Markovian policy defined by backwards induction as\n$\\pi(s) = argmax_a  \\frac{y_k \\cdot (Q^{\\pi}(s, a) - x)}{\n||Q^{\\pi}(s, a) - x||_2}$\t(11)\nLet $v_k = V_k(s_0)$ be the resulting candidate vertex for our reference simplex.\nIf $k > d + 1$, run a primal-sparse linear program solver [18] to determine if\n$x \\in conv\\{v_1,..., v_k\\}$. If so, the solver will return a basic feasible solution,\ni.e. $d + 1$ many vertices that contain $x$ in their convex hull. Let the policies\ncorresponding to the vertices that are part of the basic feasible solution be\nour reference policies and stop. Otherwise, continue to step 2 below:\n2. Let $e_k = (x - V_k)/||x - V_k||_2$ be the unit vector in the direction from $v_k$ to $x$.\n3. Let $y_{k+1} = \\sum_{i=1}^k e_i/k$ be the average of all those directions. Assuming that\n$x \\in V(s_0)$ and because of the hyperplane separation theorem, choosing di-\nrections like this ensures that the algorithm doesn't loop with $v_{i+1} = v_i$ for\nsome $l$. Also, note that to check $x \\in conv\\{v_1,..., v_k, v_{k+1}\\}$ it is sufficient\nto check $v_{k+1} - x \\in conex \\{v_1,..., x - v_k\\}$, the cone generated by the\nnegative of the vertices centred at $x$. So hunting for a policy whose value lies\napproximately in the direction $y_{k+1}$ from $x$ gives us a good chance of finding\na vertex in the aforementioned cone."}, {"title": "6 Selection of candidate actions", "content": "As we have seen in section 4.2 (ii), when choosing actions, we still have many\nremaining degrees of freedom. Thus, we can use additional criteria to choose\nactions while still fulfilling the aspirations. We discuss a few candidate criteria\nhere which are related either to gaining information, improving performance, or\nreducing potential safety-related impacts of implementing the policy.\nFor many of the criteria, there are myopic versions, which only rely on quan-\ntities that are already available at each step in the algorithms presented so far,\nor farsighted versions which depend on the continuation policy and thus have to\nbe specifically computed recursively via Bellman-style equations.\nInformation-related criteria. If the used world model is imperfect, one might\nwant the agent to aim to gain knowledge by exploration, e.g. by considering\nsome measure of expected information gain such as the evidence lower bound."}, {"title": "6.1 Performance-related criteria", "content": "For now, the task of the agent in this paper has been given by specifying aspira-\ntion sets for the expected total of the evaluation function. It is natural to consider\nextensions of this approach to further properties of the trajectory distribution,\ne.g. by specifying that the variance of the total should be small.\nA simple, myopic approach to reducing variance is preferring actions and\naction-aspirations that are somehow close to the state aspiration $\\mathcal{E}$, e.g. by choos-\ning action-aspirations where the Hausdorff distance $d_H(\\mathcal{E}_a, \\mathcal{E})$ is small. A more\nprincipled, farsighted approach would be choosing actions and action-aspirations\nsuch that the variance of the resulting total is small. Based on equation (2), the\nvariance can be computed from the total raw second moment $M^{\\pi}$ as\n$M^{\\pi}(s, a, \\mathcal{E}_a) = \\mathbb{E}_{s', \\mathcal{E}_{a'} \\sim s, a, \\mathcal{E}_a} [||f(s, a, s')||^2 + 2f(s, a, s') \\cdot V^{\\pi}(s', \\mathcal{E}_{s'})\n+ \\mathbb{E}_{a', \\mathcal{E}_{a'} \\sim \\pi(s', \\mathcal{E}_{s'})} M^{\\pi}(s', a', \\mathcal{E}_{a'})],$   (12)\n$Var(s, a, \\mathcal{E}_a) = M^{\\pi}(s, a, \\mathcal{E}_a) - ||Q^{\\pi}(s, a, \\mathcal{E}_a)||^2.$   (13)\nNote that computing this farsighted metric requires knowing the continuation\npolicy $\\pi$, for which algorithm 2 does not suffice in its current form as it only\nsamples actions. It is however easy to convert it to an algorithm for computing\nthe whole local policy $\\pi(s,\\mathcal{E})$, which is described in the Supplement."}, {"title": "6.2 Safety-related criteria", "content": "As mentioned in the introduction, unintended consequences of optimization can\nbe a source of safety problems, thus we suggest to not use any of the criteria\nintroduced in this section as maximization/minimization goals to completely\ndetermine the chosen actions; instead, they can be combined into a loss for a\nsoftmin action selection policy $\\pi_i(a \\in A_i) \\propto exp\\left( - \\beta \\sum_j a_j g_j(a) \\right)$, where $g_j(a)$\nare the individual criteria. Indeed, in analogy to quantilizers, choosing among\nadequate actions at random can by itself be considered a useful safety measure,\nas a random action is very unlikely to be special in a harmful way."}, {"title": "7 Discussion and conclusion", "content": ""}, {"title": "7.1 Special cases", "content": "A single evaluation metric. It is natural to ask what our algorithm reduces\nto in the single-criterion case $d = 1$. The reference simplices can then sim-\nply be taken to be the intervals $V_R(s) = [V_{min}(s), V_{max}(s)]$ and $Q_R(s,a) =$"}, {"title": "7.2 Relationship to reinforcement learning", "content": "Even though we formulated our approach in a planning framework where the\nenvironment's transition probabilities are known and simple enough to admit\ndynamic programming, it is clear from Eq. (11) that the required reference poli-\ncies $\\pi$ and corresponding reference vertices $V^{\\pi}(s), Q^{\\pi}(s, a)$ can in principle also\nbe approximated by reinforcement learning techniques such as (deep) expected"}, {"title": "7.3 Invariance under reparameterization", "content": "For many applications there will be several possible parameterizations of the $d$-\ndimensional evaluation space into $d$ different evaluation metrics, so the question\narises which parts of our approach are invariant under which types of reparam-\neterizations of evaluation space. It is easy to see that all parts are invariant\nunder affine transformations, except for the algorithm for finding reference poli-\ncies which is only invariant under orthogonal transformations since it makes use\nof angles, and except for certain safety criteria such as total variance."}]}