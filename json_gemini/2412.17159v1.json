{"title": "Semantic Web: Past, Present, and Future", "authors": ["Ansgar Scherp", "Gerd Groener", "Petr \u0160koda", "Katja Hose", "Maria-Esther Vidal"], "abstract": "Ever since the vision was formulated, the Semantic Web has inspired many generations of innovations. Semantic technologies have been used to share vast amounts of information on the Web, enhance them with semantics to give them meaning, and enable inference and reasoning on them. Throughout the years, semantic technologies, and in particular knowledge graphs, have been used in search engines, data integration, enterprise settings, and machine learning.\nIn this paper, we recap the classical concepts and foundations of the Semantic Web as well as modern and recent concepts and applications, building upon these foundations. The classical topics we cover include knowledge representation, creating and validating knowledge on the Web, reasoning and linking, and distributed querying. We enhance this classical view of the so-called \"Semantic Web Layer Cake\" with an update of recent concepts that include provenance, security and trust, as well as a discussion of practical impacts from industry-led contributions. We conclude with an outlook on the future directions of the Semantic Web.", "sections": [{"title": "1 Introduction", "content": "The vision of the Semantic Web as coined by Tim Berners-Lee, James Hendler, and Orla Lassila [22] in 2001 is to develop intelligent agents that can automatically gather semantic information from distributed sources accessible over the Web, integrate that knowledge, use automated reasoning [69], and solve complex tasks as such as schedule appointments in negotiation of the preferences of the involved parties. We have come a long way since then. In this paper, we reflect on the past, i. e., the ideas and components developed in the early days of the Semantic Web. Since the beginning, the Semantic Web has tremendously developed and undergone multiple waves of innovation. The Linked Data movement has especially seen uptake by industries, governments, and non-profit organizations, alike. We discuss those present components and concepts that have been added over the years and shown to be very useful. Although many concepts of the initial idea of the Semantic Web have been implemented and put into practice, still further research is needed to reach the full vision. Thus, this paper concludes with an outlook to future directions and steps that may be taken.\nFor the novice reader of the Semantic Web, we provide a brief historical overview of the developments and innovation waves of the Semantic Web: At the beginning of the Semantic Web, we were mainly talking about publishing Linked Data on the Web [78], i. e., semantic data typically structured using the Resource Description Framework (RDF)\u00b9 that is accessible on the Web using URIs/IRIS to identify entities, classes, predicates, etc. By referencing entities from other websites and Web-accessible sources, i. e., dereferencable via HTTP, the data becomes naturally linked. By using standardized vocabularies and ontologies the information then becomes more aligned and easier to use across sources. These principles have allowed non-profit organizations, companies, governments, and individuals to publish and share large amounts of interlinked data, which has led to the success of the Linked Open Data cloud\u00b2 since 2007. Since many of the large interconnected semantic sources are accessible via interfaces understanding structured query languages (SPARQL endpoints), federated query processing methods were developed that allow exploiting the strengths of structured query languages to precisely formulate an information need and optimize the query for efficient execution in a distributed setting.\nWhen Google launched its Knowledge Graph in 2012\u00b3, semantic technologies experienced another wave of new applications in the context of searching information. Whereas search engines before mainly relied on keyword search and string-based matches of the keywords in the websites' text, the knowledge graph enabled including semantics to capture the user's information need as well as the meaning of potentially relevant documents. To achieve this purpose, Google's knowledge graph integrates large amounts of machine-processable data available on the Web and uses this information not only to improve search results but also to display infoboxes for entities identified in the user's keywords. It is only since 2012 that we have widely used the term \"knowledge graph\" to refer to semantic data, where entities are connected via relationships and form large graphs of interconnected information, typically with RDF as a common standard language. In recent years though (labeled) property graphs (LPG) have been used to manage knowledge graphs. We refer to the literature for a detailed comparison of RDF graphs and LPGs [85] and also like to point out that they can be converted into each other [26]. In this article, we consider knowledge graphs from the perspective of the Semantic Web, i. e., we consider RDF graphs."}, {"title": "2 Motivating Example", "content": "On the Semantic Web, knowledge components from different sources can be intelligently integrated with each other. As a result, complex questions can be answered, questions like \"What types of music are played on British radio stations? At which time and day of the week?\" or \"Which radio station plays songs by Swedish artists?\" In this section, we provide an overview of how the Semantic Web can be employed to answer those questions. We provide details of the components of the Semantic Web in the following sections.\nWe consider the example of the BBC program ontology with links to various other ontologies such as for music, events, and social networks as shown in Figure 1. We start with the BBC playlists of its radio stations. The playlists are published online in Semantic Web formats. We can leverage the playlist to get unique identifiers of played artists and bands. For example, the music group \"ABBA\u201d has a unique identifier in the form of a URI (https://www.bbc.co.uk/ programmes/b03lyzpr). This URI can be used to link the music group to information from the MusicBrainz music portal. MusicBrainz knows the members of the band, such as Benny Andersson, as well as the genre and songs. In addition, MusicBrainz is linked to Wikipedia10 (not shown in the figure), e.g., to provide information about artists, such as biographies on DBpedia [12]. Information about British radio stations can be found in the form of lists on Web pages such as Radio UK\u00b9\u00b9, which can also be converted into a representation in the Semantic Web.\nWe can see that the required information is distributed across multiple knowledge components, e. g., BBC Program, MusicBrainz, and others. Each knowledge component can in principle provide different access to the data and utilize various ways to describe the data. Consequently, to answer the questions the data must be integrated. On the Semantic Web, data integration relies on ontologies describing data and the meaning of relations in data.\nColloquially, an ontology is a description of concepts and their relationships. Ontologies are used to formally represent knowledge on the Semantic Web. 12 For example, Dublin Core13 provides a metadata schema for describing common properties of objects, such as the creator of the information, type, date, title, usage rights, and so on. Figure 1 presents ontologies used to describe data in our example. For example, the Playcount ontology14 of the BBC is used to model which artist was played and how many times in the programs. Ontologies can be interconnected in the Semantic Web. For example, the MusicBrainz ontology is connected to the BBC ontology using the Playcount ontology. Different ontologies with varying degrees of formality and different relationships to each other are used by the BBC to describe their data (see also [133]).\nAs all this data is available and interconnected by ontologies, a user of the Semantic Web can directly ask for answers to questions in this and other scenarios. To make this possible, the"}, {"title": "3 Architecture of the Semantic Web", "content": "The example in Section 2 describes what the Semantic Web is as an infrastructure, but not how this is achieved. In fact, the capabilities of the Semantic Web in a small scale have already been implemented by some knowledge-based systems originating from artificial intelligence research, e. g., Heinsohn et al [82]. However, for the implementation of the vision on a large scale, i. e, the Web, these knowledge-based systems lacked flexibility, robustness, and scalability. In part, this was due to the complexity of the algorithms used. For example, knowledge bases in description logic in the 1990s, which serve as the basis of Web ontologies, were limited regarding their size such that they could handle at the most some hundred concepts [82].\nIn the meantime, enormous improvements have been achieved. Greatly increased computational power and optimized algorithms allow a practical handling of large ontologies like Simple Knowledge"}, {"title": "4 Representation of Graph Data", "content": "The Linked Open Data principles are notably the most successful and widely adopted choice for representing RDF graph data on the web. Thus, we first introduce the reader to how to represent graph data as Linked Data. Subsequently, we introduce the notion of ontologies. This is followed by a more detailed analysis of the different types of ontologies. We give examples of ontologies throughout the sections. With this background in mind, we reconsider our running example"}, {"title": "4.1 Linked Graph Data on the Web", "content": "The Linked Data principles 40 define the methods for representing, publishing, and using data on the Semantic Web. They can be summarized as follows:\n1. URIs are used as names for entities.\n2. The HTTP protocol's GET method is used to retrieve descriptions for a URI.\n3. Data providers shall return relevant information in response to HTTP GET requests on URIs using standards, e. g., in RDF.\n4. Links to other URIs shall be used to facilitate knowledge discovery and use of additional information.\nPublishing data using Linked Data principles allows easy access to data via HTTP. This allows exploration of resources and navigation across resources on the Semantic Web. URIs (see 1.) are dereferenced using HTTP requests (2.) to obtain additional information about a given resource. In particular, using standardized syntax (3.), this information may also contain links to other resources (4.).\nFigure 3 represents an example of Linked Data about the pop group ABBA. The example describes several relationships linking entities to ABBA's URI, such as foaf:member and rdf:type. In the figure \"ABBA\", or more precisely the URI of ABBA, is the subject, \"Property\" refers to relationships, and \"Value\" represents objects of the RDF triples. The relation owl:sameAs will be explained in more in Section 6. The prefixes foaf, rdf, and owl refer to vocabularies of the FOAF ontology41, and the W3C language specifications of RDF and OWL, respectively."}, {"title": "4.2 Ontologies", "content": "An ontology is commonly defined as a formal, machine-readable representation of key concepts and relationships within a specific domain [122, 120]. In essence, ontologies capture a shared perspective [122] that is, the formal conceptualization of ontologies expresses a consensus view among different stakeholders. Visualizing ontologies is akin to viewing a spectrum, with a specificity of concepts, their relationships, and the granularity of meaning representation varying along this continuum [109, 160, 159]. A controlled vocabulary corresponds to the less expressive form of ontology, comprising a restrictive list of words or terms used for labeling, indexing, or categorization. The Clinical Data Interchange Standards Consortium (CDISC) Terminology is an exemplary vocabulary that harmonizes definitions in clinical research. 42 A thesaurus is located next in the spectrum; they enhance controlled vocabularies with information about terms and their synonyms and broader/narrower relationships. The Unified Medical Language System (UMLS) integrates medical terms and their synonyms.43 Next, taxonomies are built over controlled vocabularies to provide a hierarchical structure, e.g., parent/child relationship. SNOMED-CT44 (Systematized Nomenclature of Medicine Clinical Terms) provides a terminology and coding system used in healthcare and medical fields; medical concepts organized in a hierarchical structure enabling a granular representation of clinical information. The Simple Knowledge Organization"}, {"title": "4.3 Types and Examples of Ontologies", "content": "A network of ontologies, such as the example shown in Figure 1, may consist of a variety of ontologies created by different actors and communities. Ontologies may be the result of a transformation or reengineering activity of a legacy system, such as a relational database or existing taxonomy such as the Dewey Decimal Classification47 or Dublin Core. Other ontologies are created from scratch. This involves applying existing methods and tools for ontology engineering and choosing an appropriate representation language for the ontology (see Section 6).\nOntology engineering deals with the methods for creating ontologies [70] and has its origins in software engineering in the creation of domain models and in database design in the creation of conceptual models. A good overview of ontology engineering can be found in several reference books [70]. Ontologies vary greatly in their structure, size, development methods applied, and"}, {"title": "4.4 Distributed Network of Ontologies and Ontology Patterns", "content": "A network of ontologies must be flexible with respect to the functional requirements imposed on it. This is because systems are modified, extended, combined, or integrated over time. In addition, the networked ontologies must lead to a common understanding of the modeled domain. This common understanding can be achieved through a sufficient level of formalization and axiomatization, and through the use of ontology patterns. An ontology pattern, similar to a design pattern in software engineering, represents a generic solution to a recurring modeling problem [142]. Ontology patterns allow to select parts from the original ontology. Either all or only certain patterns of an ontology can be reused in the network. Thus, to create a network of ontologies, e. g., existing ontologies and ontology patterns can be merged on the Web. The ontology engineer can drive or explicitly provide for the modularization of ontologies using ontology patterns. Core ontologies represent one approach to designing a network of ontologies (see in detail [142]). They allow to capture and exchange structured knowledge in complex domains. Well-defined core ontologies fulfill the properties mentioned in the previous section and allow easy integration and smooth interaction of ontologies (see also [142]). The networked ontologies approach leads to a flat structure, as shown"}, {"title": "5 Creation and Validation of Graph Data", "content": "In this section, we describe the creation of graph data from legacy data. Many tools are available for this task, which support various mappings and transformations. Subsequently, we discuss data quality and the validation of knowledge graphs, including the recent approaches on shapes. We also reflect on the role of the open-world versus closed-world assumption with respect to validating data."}, {"title": "5.1 Graph Data Creation", "content": "Graph data can be created by transforming legacy data via a data integration system [106], which consists of a unified schema, data sources, and mapping rules. These mapping rules define the concepts within the schema and establish links to the data sources. By employing declarative definitions, knowledge graph creation promotes modularity and reusability. This approach allows users to trace the entire graph creation process, leading to improved transparency and ease of maintenance.\nTo enable comprehensive and extensive graph specification, mappings and transformations have been developed to convert data from various storage models into Semantic Web data models like RDF. These mappings and transformations facilitate the mapping of data into RDF, thereby supporting the integration of diverse data sources into the Semantic Web.\nThe mapping language R2RML [41] defines mapping rules from relational databases (relational data models) to RDF graphs. These mappings themselves are also RDF triples [18]. Because of its compact representation, Turtle is considered a user-friendly notation of RDF graphs. The structure of R2RML is illustrated in Figure 5; essentially, table contents are mapped to triples by the classes SubjectMap, PredicateMap, and ObjectMap. If the object is a reference to another table, this reference is called RefObjectMap. Here, SubjectMap contains primary key attributes of the corresponding table. Thus, there exists a mapping rule representable in RDF graphs by means of which tables of relational databases can be represented as RDF graphs.\nThe RDF Mapping Language (RML) [46] extends R2RML to encompass the definition of logical sources in various formats, including CSV, JSON, XML, and HTML. This enhancement enables RML to introduce new operators that facilitate the integration of data from diverse sources into the Semantic Web. Thus, instead of LogicalTable, RML includes the tag LogicalSource, to allow for the retrieval of data in several formats. Additionally, RML resorts to W3C-standardized vocabularies and enables the definition of retrieval procedures to collect data from Web APIs or databases. R2RML and RDF mapping rules are expressed in RDF, and their graphs document how classes and properties in one or various ontologies that are part of an RDF graph are populated from data collected from potentially heterogeneous data sources.\nOver time, the Semantic Web community has actively contributed to addressing the challenge of integrating heterogeneous datasets, resulting in the development of several frameworks for executing declarative mapping rules [37, 31, 127]. A rich spectrum of tools (e. g., RMLMapper [46], RocketRML [147], CARML48, SDM-RDFizer [94], Morph-KGC [9], and RMLStreamer [123]) offers the possibility of executing R2RML and RML rules and efficiently materializing the transformed data into RDF graphs. Van Assche et al. [11] provided an extensive survey detailing the main characteristics of these engines. Despite significant efforts in developing these solutions, certain parameters can impact the performance of the graph creation process [34]. Existing engines may face challenges when handling complex mapping rules or large data sources. Nonetheless, the"}, {"title": "5.2 Quality and Validation of Graph Data", "content": "Quality and validation of the graph data are crucial to maintaining the integrity of the Semantic Web [48, 42, 173]. The evaluation of integrity constraints allows for the identification of inconsistencies, inaccuracies, or contradictions within the data. They also help maintain consistency by ensuring related data elements remain coherent. Constraints are logical statements - expressed in a particular language - that impose restrictions on the values taken for target nodes in a given property.\nConstraints can be expressed using OWL [155], SPARQL queries [105], or using shapes. However, the interpretation of the results depends on the semantics followed to interpret the failure of an integrity constraint. For example, constraints expressed in OWL are validated using an Open-World Assumption (OWA) (i. e., a statement cannot be inferred to be false based on failures to prove it) and under the absence of the Unique Name Assumption (UNA) (i. e., two"}, {"title": "6 Reasoning over and Linking of Graph Data", "content": "Section 3 introduced several formal languages for knowledge representation on the Semantic Web. RDF allows the description of simple facts (statements with subject, predicate, and object, so-called RDF triples), e. g., \"Anni-Frid Lyngstad\" \"is a member of\" \"ABBA\". RDFS allows the definition of types of entities (classes), relationships between classes, and a subclass and superclass hierarchy between types (analogously for relations). OWL is even more expressive than RDF and RDFS. For example, OWL allows the definition of disjoint classes or the description of classes in terms of intersection, union, and complement of other classes.\nBelow, we first introduce the reasoning over RDFS and OWL at the example of our BBC scenario from Section 2. Subsequently, we discuss works on linking data objects and concepts."}, {"title": "6.1 Reasoning over Graph Data", "content": "Based on formal languages representing graph data and their semantics, further (implicit) facts can be derived from the knowledge base by deductive inference. In the following, we exemplify the derivation of implicit facts from a set of explicitly given facts using the RDFS construct rdfs:subClassOf and the OWL construct owl:sameAs. The property rdfs: subClassOf describes hierarchical relationships between classes and with owl:sameAs two resources can be defined as identical.\nAs a first example, we consider the class foaf:Person, which is defined in the FOAF ontology, and the classes mo: Musician and mo: Group, which are defined in the music ontology. In the music ontology, there is an additional axiom that defines mo: Musician as a subclass of foaf: Person using rdfs:subClass0f. Given this axiom, it can be deduced by deductive inference that instances of mo: Musician are also instances of foaf:Person. Now if there is such a hierarchy of classes and in addition a statement that Anni-Frid Lyngstad is of type mo: Musician, then it can be inferred by inference that Anni-Frid Lyngstad is also of type foaf:Person. This means that all queries asking for entities of type foaf: Person will also include Anni-Frid Lyngstad in the query result,"}, {"title": "6.2 Linking of Objects and Concepts", "content": "In the Semantic Web, it cannot be assumed that two URIs refer to two different real-world objects (cf. unique name assumption in Section 5.2). A URI by itself, or in itself, has no identity [75]. Rather, the identity or interpretation of a URI is revealed by the context in which it is used on the Semantic Web. Determining whether or not two URIs refer to the same entity is not a simple task and has been studied extensively in data mining and language understanding in the past. For example, to identify whether or not the author names of research papers refer to the same person, it is often not sufficient to resolve the name, venue, title, and co-authors [97]. The process of determining the identity of a resource is often referred to as entity resolution [97], coreference"}, {"title": "7 Querying of Linked Data", "content": "Queries over Linked Data can be processed using link traversal [79], i. e., the query processor would use one of those IRIs given directly in the query as starting point and query the respective"}, {"title": "7.1 Basic Query Processing", "content": "In principle, a SPARQL query is evaluated by comparing the graph pattern defined in the query to the RDF graph and reporting all matches as results. The set of results can be restricted by additional criteria, such as filters, i. e., conditions on variables and triple patterns that additionally need to be fulfilled.\nAs an example, let us consider the query illustrated in Figures 8 and 9 that we want to execute over our example MusicBrainz graph from Section 2. We are now interested in the musicians of ABBA who are also members of other bands. If we follow the Linked Data principles and evaluate the query using link traversal [79], this would mean first querying for triples including the IRI that represents ABBA, then navigating to the individual band members, and then following the links to all of the members' bands and query more relevant triples."}, {"title": "7.2 Entailment Regimes and Query Processing", "content": "In addition to explicitly querying existing facts, SPARQL provides inferencing support through so-called entailment regimes. They correspond to logical consequences describing the relationship"}, {"title": "7.3 Federated Query Processing", "content": "Federations provide another perspective on querying linked data over multiple sources. A federation of knowledge graphs shares common entities while potentially providing different perspectives on those entities. Each knowledge graph within the federation operates autonomously and can be accessed through various Web interfaces, such as SPARQL endpoints or Linked Data Fragments (LDFs) [162]. SPARQL endpoints offer users the ability to execute any SPARQL query against multiple SPARQL endpoints. In contrast, LDFs enable access to specific graph patterns, such as"}, {"title": "8 Trustworthiness and Provenance of Graph Data", "content": "Trustworthiness of web pages and data on the web can be detected by various indicators, e. g., by certificates, by the placement of search engine results, and by links (forward and backward links) to other pages. However, on the Semantic Web, there are few ways for users to assess the trustworthiness of individual data. Rules can be utilized to define policies and business logic over the web of data, and transparently used to infer data that validate or do not validate these policies.\nThe trustworthiness of inferred data can be assessed through its provenance, which encompasses metadata detailing how the data was acquired and verified [102].\nThe trustworthiness of data on the web can be inferred from the trustworthiness of other users (\"Who said that?\"), the temporal validity of facts (\"When was a fact described?\"), or in terms of uncertainty of statements (\u201cTo what degree is the statement true?\"). Artz and Gil [10] summarize trustworthiness as follows: \"Trust is not a new research topic in computer science, spanning areas as diverse as security and access control in computer networks, reliability in distributed systems, game theory and agent systems, and policies for decision-making under uncertainty. The concept of trust in these different communities varies in how it is represented, computed, and used.\" Although trustworthiness has long been considered in these areas, the provision and publication of data by many users to multiple sources on the Semantic Web introduces new and unique challenges."}, {"title": "9 Applications", "content": "With the increasing spread and use of semantic and linked data on the Web, the requirements for Semantic Web applications have increased at the same time as their application possibilities. The general requirements for applications based on semantic data on the Web are given by their flexible and diverse representation and descriptions. Applications that use data from relational databases or XML documents can start from a fixed schema. However, this cannot be assumed for data on the Web. Often, neither the data sources nor the type and amount of data in a source are fully known. The dynamics of semantic data on the Web must be taken into account by applications accordingly, both when querying and aggregating data, and when visualizing data. Thus, the real challenge of Semantic Web applications is to guarantee the best possible flexibility"}, {"title": "9.1 Vocabularies and Schemas: Schema.org", "content": "In HTML documents, the structure and composition of pages can be described with tags, but not the meaning of the information. Vocabulary, schemas, and microdata can be used as mark-up in HTML documents to describe information about page content and its meaning in a way that search engines can process this information.\nSchema.org62 is a collection of vocabularies and schemas to enrich HTML pages with additional information. The vocabulary of Schema.org includes a set of classes and their properties. A universal class \"thing\" is the most general, which is a kind of umbrella term for all classes. Other common classes are Organization, Person, Event, and Place. Properties are used to describe classes in more detail. For example, a person has the properties such as name, address, and date of birth.\nIn addition to vocabularies, Schema.org also specifies the use of HTML microdata, with the goal of representing data in HTML documents in as unambiguous a form as possible so that search engines can interpret it correctly. An example of this is formats for unique dates and times, which can also describe intervals to indicate the duration of events.\nSchema.org is supported by the search engines Bing, Google, and Yandex, among others. There are extensions and libraries for various programming languages, including PHP, JavaScript, Ruby, and Python, to create web pages and web applications using vocabularies and microdata from Schema.org. Likewise, there are mappings from Schema.org vocabularies and microdata to RDFS."}, {"title": "9.2 Semantic Search", "content": "A classic web browser enables the display of web pages. A semantic web browser goes one step further by additionally allowing the user to visualize the underlying information of individual pages, for example in the form of RDF metadata. Semantic Web browsers are also referred to as hyperdata browsers because they allow navigation between data while also allowing one to explore the connection to information about that data. Thus, ordinary users can use and exploit Semantic Web data for their information search.\nSig.ma [158] was an application for (browsing) Semantic Web data, which may come from multiple distributed data sources. Sig.ma provided an API for automatically integrating multiple data sources on the Web. The requested data sources describe information in RDF. A search in Sig.ma was initiated by a textual query from the user. Entities such as people, places, or products"}, {"title": "9.3 Knowledge Graphs and Wikidata", "content": "There is an increasing number of knowledge bases and representations of structured data. For example, the secondary database Wikidata64 [165]. A secondary database includes, in addition to the (actual) statements, relationships to their sources and other databases (called secondary information). Wikidata is a shared database between Wikipedia and Wikimedia. Wikidata mainly contains a collection of objects, which are represented as triples over the objects' properties and the corresponding values. Semantic MediaWiki65 is an extension of MediaWiki. It serves as a flexible knowledge base and knowledge management system. Semantic MediaWiki extends a classic wiki with the ability to enrich content in a machine-readable way using semantic annotations.\nAnother knowledge base was Freebase66, also an open and collaborative platform initiated in 2007 and acquired by Google in 2010. The content from Freebase was taken from various sources, including parts from the MusicBrainz ontology mentioned earlier. The success and widespread use of Wikidata prompted Google to migrate Freebase to Wikidata [154]. This strengthened the goal to develop a comprehensive, collaborative basis of structured data.\nGoogle offers a semantic search function with Google Knowledge Graph 67,68. A knowledge graph, like an RDF graphs, is a set of triples representing links between entities. This forms a semantic database. Possible entity types are described on schema.org, among others. If a search term occurs in a query, the corresponding entity is searched for in the knowledge graph. Starting from this entity, it is then possible to navigate to further entities by means of the links."}, {"title": "9.4 API-Access to Social Networks", "content": "A social network is essentially a graph in which connections are formed from users to other users, e. g., in the form of a friendship relationship or to events and groups. Facebook's Graph API describes a programming interface to the Facebook Graph (called Open Graph). Within the graph, people, events, pages, and photos are represented as objects, with each object having a"}, {"title": "10 Impact for Practitioners", "content": "Linking and using graph data on the Web has become a widespread practice. Today, there is a large amount of open data in various formats and domains, such as bibliographic information management, bioinformatics, and e-government. DBpedia is the central hub in this context, around which different datasets and domains are grouped (cf. [24]). This is illustrated, e. g., by the tremendous growth of the Linked Open Data Cloud69 since 2007. Two of the latest notable supporters of graph-based data are online auctioneer eBay with their graph database 70 and the U.S. space agency NASA with the unification of internal distributed case databases as knowledge graphs71. These and other success stories of the Semantic Web in industries and industry-scale knowledge graphs are described by Noy et al. [118]. Further analyses and surveys arguing about the importance but also challenges of using graph data can be found in the literature like the 2020 survey of Sahu et al. [138] and the 2021 reflection about the future of graphs by Sakr et al. [139]. The usefulness of knowledge graphs and semantic-based data modeling for complex systems is also discussed in the 2024 book by Abonyi et al. [2]. The importance of graph databases is also reflected by the Forbes business magazine, which predicted in 2019 that graph databases would be the next mainstream database technology72.\nRegarding lightweight open graph data, Schema.org defines schemas for modeling data on web pages to provide information about the underlying data structures and meaning of the data. Search engines can use this additional information to better analyze the content of web pages. As mentioned above, Schema.org is supported by search engines such as Bing, Google, and Yandex. Studies on selected sources have shown that web pages among the top 10 results have up to 15% higher click-through rate73. Other companies like BestBuy.com even report up to 30% higher click-through rates since adding semantic data to their websites (cf. Section 9) in 2009. BestBuy.com uses the GoodRelations vocabulary74 to describe online offers. Similarly, Google uses semantic data from online commerce portals that use the GoodRelations vocabulary and takes it into account when searching75.\nAnother success is the publication of government data. For example, the U.S. government makes government data publicly available with data.gov76, and U.S. Census77 publishes statistical data about the United States. In the UK, data.gov.uk78 is a key part of a program to increase"}, {"title": "11 Summary and Outlook", "content": "The Semantic Web consists of a variety of techniques that have been heavily influenced by long-term artificial intelligence research and its results. The current state is also driven by an industry uptake under the umbrella term of Knowledge Graphs and reflected in various activities as described. In summary, therefore, it can be observed that semantic data on the Web is having a real impact on commercial providers of products and services, as well as on governments and public administrations.\nDespite all the research and industrial developments, the full potential of the Semantic Web has not yet been exploited. Some important components of the Semantic Web architecture are still being explored, such as data provenance and trustworthiness. Below, we describe three example directions for future work.\nNeuro-symbolic systems: As mentioned in the introduction, we see as an important direction of future work the combination of symbolic AI and subsymbolic AI. By combining the strength of Large Language Models (LLM), i. e., generative AI, in processing and generating natural language text and accessing structured data and logical reasoning capabilities of the Semantic Web, a next step towards the vision of automated agents that perform complex planning tasks may be reached. An example is performing A* search with an LLM [174]. Specifically, LLMs might comprehensively capture and acquire human knowledge [45], but current LLMs lack responding to simple questions of non-existing facts in their training data [45], may not contain all facts [152], and thus return less accurate answers [91]. To leverage the distinct capabilities of both LLMs and the Semantic Web, the integration of neuro-symbolic systems appears to offer a viable solution [124]. Neuro-symbolic systems could also address the problem that LLMs' output is based on the most probable answer, which sometimes leads to wrong answers often referred to as \u201challucinations\" [17, 88, 152].\nNatural interfaces between machine and users: A key to successful applications of the Semantic Web is intuitive user interfaces. Users must be offered applications that are intuitive and easy to use. This includes improving interfaces based on natural language for formulating\"\n    }"}]}