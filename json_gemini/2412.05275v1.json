{"title": "MotionFlow: Attention-Driven Motion Transfer in Video Diffusion Models", "authors": ["Tuna Han Salih Meral", "Hidir Yesiltepe", "Connor Dunlop", "Pinar Yanardag"], "abstract": "Text-to-video models have demonstrated impressive capabilities in producing diverse and captivating video content, showcasing a notable advancement in generative AI. However, these models generally lack fine-grained control over motion patterns, limiting their practical applicability. We introduce MotionFlow, a novel framework designed for motion transfer in video diffusion models. Our method utilizes cross-attention maps to accurately capture and manipulate spatial and temporal dynamics, enabling seamless motion transfers across various contexts. Our approach does not require training and works on test-time by leveraging the inherent capabilities of pre-trained video diffusion models. In contrast to traditional approaches, which struggle with comprehensive scene changes while maintaining consistent motion, MotionFlow successfully handles such complex transformations through its attention-based mechanism. Our qualitative and quantitative experiments demonstrate that MotionFlow significantly outperforms existing models in both fidelity and versatility even during drastic scene alterations.", "sections": [{"title": "1. Introduction", "content": "Recent advances in diffusion models demonstrated significant capabilities in generating high-quality images and videos. The emergence of text-to-video (T2V) generation models has opened new possibilities in creative content creation, enabling the synthesis of complex video sequences from user-provided text prompts. These models have shown a remarkable ability to generate diverse and visually compelling video content, marking a significant milestone in generative AI.\nDespite their success, current T2V models offer limited controllability, particularly in manipulating motion patterns. The ability to control motion in video generation is an important task for various creative applications. Imagine a filmmaker in the early stages of planning a new movie scene, eager to explore various motion styles before committing to the labor-intensive process of shooting or animating the actual footage. Using MotionFlow, this filmmaker can repurpose video clips\u2014such as a scene of a dog jumping into the lake (see Fig. 1)\u2014and transfer these motions directly into the settings they envision. This capability allows the filmmaker to quickly prototype various motion effects and see how they might look with different characters or within different narrative contexts. For example, using the same motion from the dog video, they could experiment with a rabbit jumping into the river surrounded by blooming flowers (see Fig. 1). By enabling rapid experimentation with different motion dynamics, MotionFlow helps the filmmaker brainstorm, iterate on creative ideas, and overcome traditional time and resource constraints.\nHowever, existing motion transfer methods face several limitations. They often struggle to balance motion fidelity and diversity, leading to issues like unwanted transfer of appearance and scene layout from the source video. Additionally, many approaches require extensive training or fine-tuning on specific motion patterns, making them impractical for real-world applications where flexibility and efficiency are important. These limitations highlight the need for more sophisticated and practical solutions to motion transfer in video generation.\nTo address these challenges, we propose MotionFlow, a novel test-time approach that leverages the inherent capabilities of pre-trained video diffusion models without requiring additional training. While other approaches primarily rely on temporal attention features our method primarily leverages cross-attention features from existing videos to guide motion transfer. This approach enables the effective capture and transfer of motion information while remaining independent of the source video's appearance and scene composition. By visualizing cross-attention maps during both inversion and generation, we illustrate how linguistic elements influence object generation and motion (see Fig. 2). These visualizations demonstrate how MotionFlow transfers motion dynamics by aligning the attention maps of the generated subject with those of the original, preserving motion patterns while adhering to the new edit prompt. Our contributions include:\n\u2022 We introduce the first test-time motion transfer method that leverages cross-attention maps from pre-trained video diffusion models, eliminating the need for additional training, fine-tuning, or extra conditions.\n\u2022 We provide comprehensive experimental results showing the effectiveness of our method across various scenarios and motion types. Our approach achieves a balance between motion fidelity and diversity, generating the intended appearance and scene layout of the target video while accurately transferring motion patterns.\n\u2022 We make our source code publicly available to enable further research and applications in this domain."}, {"title": "2. Related Work", "content": "2.1. Text-to-Video Diffusion Models\nBuilding on the success of diffusion-based Text-to-Image (T2I) models, Text-to-Video (T2V) generation models have made remarkable progress. These models extend 2D diffusion frameworks by incorporating a temporal dimension, enabling the modeling of cross-frame dependencies and thus facilitating coherent video generation from text prompts.\nCommonly, T2V models augment 2D diffusion architectures with temporal layers to explicitly model the relationships between video frames. For instance, AnimateDiff and ModelScope enhance pre-trained diffusion models by adding temporal layers and fine-tuning them for video generation. Instruct Video takes a different approach by leveraging human feedback to refine video quality. Additionally, several works introduce conditioning inputs\u2014such as depth maps, bounding boxes, and motion trajectories\u2014to allow for more precise control over object shapes and movements within generated videos.\n2.2. Attention-Based Guidance\nAttention-based guidance has emerged as a key technique for improving the quality and controllability of T2I generation. Methods such as Attend-and-Excite , and CONFORM apply attention constraints to optimize latent features during inference, addressing issues like subject omission and incorrect attribute binding, while methods like CLORA and Bounded Attention further enhance multi-subject generation by managing cross-attention in complex compositions. These advances in attention-based guidance for T2I generation provide a foundation for more sophisticated control in T2V models, where temporal consistency is crucial.\n2.3. Video Motion Editing\nWhile Text-to-Video (T2V) models are designed to control motion through text prompts, they often struggle with complex or nuanced motions. To address this, recent methods have introduced bounding boxes for more precise control, either during training or at inference time.\nAnother line of work focuses on transferring motion from a reference video. Fine-tuning approaches store motion in the model's weights, while inversion-based methods store motion in model features. For example, Tune-a-Video adapts text-to-image models by adding spatiotemporal attention layers, training only the motion-specific components. Similarly, MotionDirector separates motion and appearance using a dual-path LoRA architecture. Other methods, such as Dream Video and Customize-A-Video , use separate branches for appearance and motion learning. Wang et al. learn motion embeddings by training over the original video using temporal attention layers.\nInversion-based editing methods, initially developed for image editing, have also been adapted for video. DDIM inversion enables reconstruction through backward diffusion, as used in methods like DMT , UniEdit and VMC . DMT uses a space-time feature loss that leverages DDIM inversion and UNet activations. UniEdit and VMC blend fine-tuning with inversion to adjust temporal layers while maintaining content fidelity. However, a common limitation is the assumption that the features of the reference and target videos are identical, which can pose challenges when generating videos with different geometries.\nIn contrast, our proposed method, MotionFlow, introduces a novel test-time approach that leverages cross-attention features from pre-trained video diffusion models. This allows for effective motion transfer without additional training or fine-tuning, overcoming the limitations of existing methods. By capturing and transferring motion independently of the source video's appearance and scene composition, MotionFlow achieves a balance between motion fidelity and diversity, offering enhanced flexibility and control in video motion transfer tasks."}, {"title": "3. Methodology", "content": "3.1. Diffusion Models\nDiffusion models iteratively transform input noise $\\mathbf{x}_T \\sim \\mathcal{N}(0, I)$ into a meaningful sample $\\mathbf{x}_0$ through a structured denoising process. Denoising Diffusion Implicit Models (DDIM) enable this process deterministically, converting $\\mathbf{x}_T$ into a clear sample $\\mathbf{x}_0$. The reverse process, known as inversion, reconstructs the initial noise $\\mathbf{x}_T$ responsible for generating $\\mathbf{x}_0$ and the entire sequence of noisy latents $\\{\\mathbf{x}_t\\}_{t=1}^T$.\nLatent diffusion models, like Stable Diffusion , operate within the latent space of an autoencoder. The input image $\\mathbf{x}$ is compressed into a lower-dimensional latent representation $\\mathbf{z} = \\mathcal{E}(\\mathbf{x})$ via an encoder $\\mathcal{E}$, and then reconstructed by a decoder $\\mathcal{D}$. For instance, Stable Diffusion encodes an image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ into a latent space $\\mathbf{z} \\in \\mathbb{R}^{h \\times w \\times d}$, where $H \\gg h$, $W \\gg w$, and $d$ is the latent dimension, typically $d = 4$.\nExtending this approach to video data, latent video diffusion models encode video input $\\mathbf{x} \\in \\mathbb{R}^{F \\times H \\times W \\times C}$ into a latent space $\\mathbf{z} \\in \\mathbb{R}^{f \\times h \\times w \\times d}$, where $f < F$. These models enhance existing T2I frameworks by incorporating temporal layers, which integrate convolution and attention mechanisms, and are fine-tuned on video datasets. In our study, we use the publicly available ZeroScope T2V model , which extends Stable Diffusion by integrating temporal convolution and attention layers.\n3.2. Attention Mechanisms\nThe backbone of our method, ZeroScope, built on a UNet architecture, integrates two key components for influencing video motion: temporal attention layers in the temporal module and cross-attention layers in the spatial module.\nCross-attention layers incorporate text-based information, allowing the text prompt to guide the content and structure of the generated video. This ensures that linguistic elements, such as nouns and verbs, are accurately translated into visual features, guiding object generation and their associated motions. Temporal attention layers establish inter-frame connections, ensuring smooth and coherent motion across frames, while self-attention within spatial blocks preserves spatial consistency.\nThe query features, $\\mathbf{Q} \\in \\mathbb{R}^{F \\times N \\times D}$, represent the latent features of the model, where $N = h \\times w$ (with $h \\ll H$ and $w \\ll W$) is the spatial resolution in the latent space, and $D$ is the feature dimension. The key features, $\\mathbf{K} \\in \\mathbb{R}^{L \\times D}$, are derived from the text encoder (e.g. CLIP ), where $L$ represents the number of tokens in the input text prompt. Consequently, the cross-attention map has dimensions $F \\times N \\times L$, where $F$ is the number of frames, $N = h \\times w$ denotes the spatial resolution, and $L$ is the number of tokens from the text prompt. The attention map at time $t$ is calculated as $\\mathbf{A}_t = \\text{Softmax}(\\mathbf{Q}\\mathbf{K}^T/\\sqrt{d})$ where $t$ is the timestep and $d$ is the dimension of the keys.\n3.3. Our Method\nWe propose a framework for generating a new video $\\mathcal{V}$ by transferring the motion dynamics of a specific subject from an original video $\\mathcal{V}_o$, while ensuring compliance with a target text prompt $P$. Our method leverages ZeroScope, a pre-trained latent text-to-video (T2V) diffusion model, and involves two key steps: (1) DDIM inversion of the original video to extract latent representations and cross-attention-based extraction of subject-specific motion, and (2) guided video generation using the extracted motion and the target text prompt."}, {"title": "4. Experiments", "content": "Experimental Setup. For the inversion of the original video, we use DDIM inversion to obtain the initial noise latent. During the generation process, we set the threshold parameter $\\tau = 0.4$ in Eq. 1 to determine the significance of attention values. For optimization, we utilize a learning rate $\\alpha = 5.0$ in Eq. 4, and assign equal weights to the loss functions, with $\\lambda_{\\text{cross}} = \\lambda_{\\text{self}} = \\lambda_{\\text{temporal}} = 1.0$.\nThe generation process involves performing latent updates for 20 steps out of a total of 50 backward diffusion steps, similar to , with 20 iterations per update. To mitigate the introduction of unwanted artifacts in the output, we halt optimization after the 20th step, allowing the remaining steps to proceed without further updates.\nFor our experiments, we selected a range of videos from the DAVIS dataset, which is widely used in video editing and motion transfer research. This dataset provides a robust foundation for evaluating the effectiveness of MotionFlow in diverse motion transfer scenarios.\nBaselines and Metrics. We evaluate MotionFlow against state-of-the-art video motion transfer methods: DMT , VMC , Motion Director , and Motion Inversion , using 100 randomly selected video-prompt pairs for each method. To assess performance, we employ three metrics: Motion Fidelity Score , which measures how well the generated video preserves the motion patterns from the source video, evaluating the accuracy of motion dynamics such as speed, direction, and style; Temporal Consistency, which assesses the smoothness and coherence of motion across frames by calculating the average cosine similarity between CLIP image features of consecutive frames, ensuring stable transitions and natural motion flow; and Text Similarity, which evaluates how accurately the generated video aligns with the input text prompt by computing the cosine similarity between CLIP embeddings of the video frames and the prompt, ensuring that the visual content reflects the intended modifications.\n4.1. Qualitative Experiments\nAs shown in Fig. 4, MotionFlow demonstrates remarkable flexibility in motion transfer across diverse scenarios while offering control over scene composition. In Fig. 4 (a), our method successfully transfers motion between significantly different animals and objects, mapping a bear's movement to both a robot and a tiger. This highlights MotionFlow's ability to generate entirely new scene layouts with different subjects, overcoming the common limitation of being constrained by the source video's background. Conversely, our method can also preserve the original scene layout, as seen in Fig. 4 (e), where we transfer motion from an elephant to a moose and a gorilla while maintaining the original background composition. Moreover, our method excels at transferring motion between fundamentally different object categories, as shown in Fig. 4 (f) where we successfully map the motion of a train to a motorbike while preserving the distinctive movement patterns. These results underscore MotionFlow's versatility in handling diverse motion transfer scenarios and its ability to either maintain or alter scene layouts as needed, offering high flexibility in video motion transfer applications. Full videos are available in the Supplementary Material.\nQualitative Comparison. In Fig. 5, MotionFlow's motion transfer capabilities are compared across different videos against benchmark methods. In Fig. 5 (a), MotionFlow successfully transfers motion from the subject of the original video and generates a background aligned with the prompt, unlike DMT and Motion Inversion. While both MotionDirector and VMC generate videos aligned with the edit prompt, MotionDirector's output suffers from poor quality, and VMC produces a video with low motion fidelity to the original. In Fig. 5 (c), all methods except VMC, which generally exhibits low motion fidelity, were able to transfer motion. However, only our method successfully generates a scene aligned with the edit 'river of lava'."}, {"title": "4.2. Quantitative Experiments", "content": "Table 1 and Fig. 6 present the performance of our approach across key metrics, highlighting its advantages over existing methods. Our method consistently outperforms baselines in text similarity, demonstrating better alignment with target prompts while retaining strong motion fidelity and temporal consistency.\nThis improved performance can be attributed to three key factors: (1) By leveraging cross-attention maps, our framework achieves precise motion transfer, preserving the integrity of original motion patterns without requiring fine-tuning. This sets it apart from training-intensive methods like DMT and MotionDirector. (2) As shown in Fig. 6, our method balances text similarity and motion fidelity, enabling accurate motion adaptation to prompt specifications. High motion fidelity alone can sometimes penalize necessary edits, but our method strikes an effective compromise, maintaining fidelity to the original while allowing flexibility for editing tasks. (3) Our approach closely aligns with the target prompt, resulting in higher text similarity scores and improved semantic coherence in generated videos. This alignment surpasses methods such as VMC, which struggles with motion fidelity, and Motion Inversion, although it uses an alternative diffusion model backbone (Motion-CraftV2 ).\nIn summary, our framework achieves a strong balance across metrics. It provides competitive motion fidelity and temporal consistency while maintaining superior adherence to user-defined prompts, as evidenced by its higher text similarity scores.\nUser Study. To evaluate the perceptual quality of our motion transfer results, we conducted a user study on Amazon Mechanical Turk with 50 participants. Each participant viewed 30 sets of videos, each set containing five generated videos: one from our method, four from baseline methods (Motion Inversion, DMT, VMC, and Motion Director), along with the original video and the corresponding edit prompt. Participants were asked to select the best video based on three key criteria: Motion Fidelity (how well the original motion was preserved), Visual Quality (overall appearance and coherence), and Prompt Alignment (how accurately the video matched the edit prompt). As shown in Table 1, our method consistently ranked higher across all criteria according to user preferences.\nProcessing Times. We compared the processing times for each method using Nvidia L40 GPUs. Each method involves an initial setup phase (training, fine-tuning, or inversion) followed by video generation, requiring 49 seconds for inversion and 376 seconds for generation, totaling 425 seconds. DMT takes 258 seconds for inversion and 332 seconds for generation (590 seconds total), Motion Director requires 410 seconds for fine-tuning and 67 seconds for generation (477 seconds total), VMC takes 227 seconds for training and 503 seconds for generation (730 seconds total), and Motion Inversion requires 195 seconds for training and 30 seconds for generation, totaling 225 seconds.\nAblation Study. To evaluate the importance of latent updates in motion transfer, we conducted an ablation study (Fig. 7) by omitting the latent update step and using only the initial noise latent from DDIM inversion. The results show that while DDIM inversion provides a high-level structure and preserves camera motion, it often fails to capture detailed subject motion. For example, in the prompt 'A dragon walking across a bridge made of clouds in a fantasy realm', The results show that while DDIM inversion provides a high-level structure and preserves camera motion, it often fails to capture detailed subject motion. For example, in the prompt 'A robot walking across ancient stone ruins', the model struggles to generate the intended subject. In contrast, our method, which incorporates cross-attention-guided latent updates, accurately captures both the subject and its motion, ensuring correct spatial placement. This ablation study demonstrates the critical role of latent updates in achieving precise motion transfer and subject generation."}, {"title": "5. Limitation and Societal Impact", "content": "While MotionFlow demonstrates strong performance in motion transfer tasks, we acknowledge some limitations that present opportunities for future research. Our method's fundamental reliance on attention maps from pre-trained video diffusion models makes it sensitive to the quality of these underlying models. When attention maps are noisy or improperly focused due to the pre-trained model's limitations, our motion transfer quality may degrade accordingly.\nWhile MotionFlow has the potential to significantly impact creative industries, it is essential to consider ethical implications, such as the misuse of the technology for creating deceptive content. Ensuring responsible use through clear guidelines and safeguards will help maximize its positive societal contributions while minimizing potential risks."}, {"title": "6. Conclusion", "content": "In this paper, we introduced MotionFlow, a novel approach to video motion transfer that leverages cross-attention maps from pre-trained video diffusion models without requiring additional training. Our method addresses a significant challenge in video generation by enabling precise control over motion patterns while maintaining the flexibility to either preserve or modify scene compositions as desired. Through comprehensive experiments, we demonstrated that MotionFlow achieves state-of-the-art performance across various metrics, successfully handling diverse scenarios from simple object transformations to complex cross-category motion transfers. The method's ability to work with drastically different objects (e.g., train to motorbike) and animals (e.g., bear to elephant) while offering control over scene preservation demonstrates its versatility and practical utility. By making our code public, we hope to facilitate further research in this direction and enable practical applications in content creation, animation, and video editing."}]}