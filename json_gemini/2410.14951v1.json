{"title": "LSS-SKAN: Efficient Kolmogorov-Arnold Networks based on Single-Parameterized Function", "authors": ["Zhijie Chen", "Xinglin Zhang"], "abstract": "The recently proposed Kolmogorov-Arnold Networks (KAN) networks have attracted increasing attention due to their advantage of high visualizability compared to MLP. In this paper, based on a series of small-scale experiments, we proposed the Efficient KAN Expansion Principle (EKE Principle): allocating parameters to expand network scale, rather than employing more complex basis functions, leads to more efficient performance improvements in KANs. Based on this principle, we proposed a superior KAN termed SKAN, where the basis function utilizes only a single learnable parameter. We then evaluated various single-parameterized functions for constructing SKANs, with LShifted Softplus-based SKANS (LSS-SKANs) demonstrating superior accuracy. Subsequently, extensive experiments were performed, comparing LSS-SKAN with other KAN variants on the MNIST dataset. In the final accuracy tests, LSS-SKAN exhibited superior performance on the MNIST dataset compared to all tested pure KAN variants. Regarding execution speed, LSS-SKAN outperformed all compared popular KAN variants. Our experimental codes are available at https://github.com/chikkkit/LSS-SKAN and SKAN's Python library (for quick construction of SKAN in python) codes are available at https://github.com/chikkkit/SKAN.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid development of artificial intelligence (AI) is reshaping our world. Among numerous Al technologies, neural networks have garnered significant attention due to their exceptional learning capabilities and broad application prospects. The Multilayer Perceptron (MLP), as a fundamental form of neural network, has demonstrated outstanding performance in various tasks. However, as neural networks are increasingly applied in critical decision-making areas, the issue of their interpretability has become a growing concern.\nTo enhance the interpretability of neural networks, Kolmogorov-Arnold Networks (KANs) were proposed as an innovative neural network architecture [1]. The design of KAN is inspired by the Kolmogorov-Arnold representation theorem, achieving a parameterized representation of functions on network weights by replacing the fixed activation functions and learnable linear parameters in traditional MLPs with learnable activation functions. This design not only enhances the model's interpretability but also demonstrates superior performance in multiple application domains.\nSince the introduction of KAN, researchers have proposed various KAN variants to further improve its performance and applicability. Wav-KAN, which uses wavelet functions as basis functions, has achieved significant improvements in both accuracy and training speed [2]. Fast-KAN, by employing Gaussian radial basis functions, significantly increased the forward propagation speed [3]. FourierKAN, which proposes a novel neural network layer that replaces traditional Linear + non-linear activation combinations with Fourier series approximations, offered potential benefits in optimization and numerical stability [4]. fKAN and rKAN, utilizing trainable adaptive fractional orthogonal Jacobi functions and Pad\u00e9 approximations with rational Jacobi functions as basis functions respectively, further optimized KAN's performance [5] [6].\nInitially, KAN referred specifically to KAN-type networks using B-spline functions as basis functions. However, as research progressed, the definition of KAN has expanded to include various variants. To avoid ambiguity, this paper refers to KAN-type networks collectively as KAN, while the original KAN using B-spline functions is termed Spl-KAN.\nThis study proposes a new KAN design principle (Efficient KAN Expansion Principle, EKE Principle), namely, allocating parameters in expanding network scale, rather than adopting more complex basis functions, can achieve more efficient performance"}, {"title": null, "content": "improvements in KAN. This principle aims to enable KAN to have a larger network scale under the same parameter configuration, thereby enhancing the network's ability to represent complex interactions between input parameters. KAN using nonlinear functions with only a single learnable parameter as basis functions is named SKAN (KAN based on function with Single learnable parameter, Single-Parameterized KAN).\nGuided by this principle, a series of nonlinear basis functions with a single learnable parameter were designed, and through experiments, the shifted softplus function with a learnable parameter (LSS function, learnable shifted softplus function) was selected as the best performing among them. The SKAN network using the LSS function is called LSS-SKAN.\nExtensive experiments demonstrate that LSS-SKAN outperforms several pure KAN variants in terms of accuracy and computational speed, including Wav-KAN, Spl-KAN, FastKAN, MLP+rKAN, and MLP+fKAN. In the final accuracy tests, LSS-SKAN demonstrated superior performance on the MNIST dataset compared to all pure KAN variants in experiments. Specifically, its accuracy surpassed FourierKAN by 0.58%, Spl-KAN by 1.65%, FastKAN by 2.57%, and WavKAN by 0.22%. Furthermore, in terms of execution speed, LSS-SKAN outperformed all compared popular KAN variants, operating 8.03% faster than FourierKAN, 502.89% faster than MLP+rKAN, 41.78% faster than MLP+fKAN, 38.56% faster than Spl-KAN, 9.27% faster than FastKAN, and 9.75% faster than WavKAN.\nIn brief, the innovation of this study lies in proposing a new KAN design principle (EKE Principle) and proposing Single-Parameterized KAN as SKAN. Furthermore, under this principle, an excellent SKAN based on the learnable shifted softplus function (LSS-SKAN) was found, and its effectiveness was demonstrated through experiments."}, {"title": "II. RELATED WORK", "content": null}, {"title": "A. The proposal of KAN", "content": "Kolmogorov-Arnold Networks (KANs) are an innovative neural network architecture inspired by the Kolmogorov-Arnold representation theorem [1]. This theorem posits that any multivariate continuous function can be represented through a combination of univariate functions and addition operations. KANs achieve a parameterized representation of functions on network weights by replacing fixed activation functions and learnable linear parameters in traditional MLPs with learnable activation functions."}, {"title": null, "content": "The core architecture of KANs consists of multiple layers, each composed of a set of univariate functions defined on the edges rather than the nodes of the network. This innovative design allows KANs to capture complex patterns in data by adjusting the function, facilitating the visualization of the function, and thereby providing a clear understanding of the network's functional formula.\nEach KAN layer comprises a set of univariate functions \u00d3p,q, where p denotes the input dimension and q represents the output dimension. These functions \u00d3p,q are all learnable. In the original paper proposing KAN, the basis functions are designed as B-spline curves with trainable coefficients.\nThe general structure of KAN can be represented by the following formula:\n$\\text{KAN} (x) = (\\Phi_{L-1} \\circ \\Phi_{L-1} \\circ \\cdots \\circ \\Phi_{1} \\circ \\Phi_{0}) (x)$  (1)\nwhere \u03a6\u03b9 denotes the function matrix of the 1-th layer; x is the input vector; and o is the relational composition operator, indicating the concatenation of function calculations for each layer.\nThe advantages of KANs lie in their ability to improve model accuracy and interpretability by learning the inherent structure of the data. By decomposing complex multivariate functions into combinations of univariate functions, KANs not only capture complex patterns in data but also provide intuitive understanding of the model through visualization of the functions within the network.\nFurthermore, the original literature of KAN also proposes techniques including, but not limited to, Grid Extension techniques (specific to Spl-KAN) to improve KAN and Simplification techniques to train KAN better. Although these are important components of KAN, they are not discussed in depth in this paper.\nIt was pointed out that the original implementation code for Spl-KAN (hereafter referred to as pykan) lacked engineering refinement, resulting in suboptimal processing speeds. To address this issue, the efficient-kan library has been widely adopted as an alternative solution. This library, hosted on GitHub, represents a reimplementation of the Spl-KAN algorithm. Through optimizations in key areas, the usability of the algorithm has been significantly enhanced. Due to these improvements, the efficient-kan library has gained widespread recognition and has become the preferred tool for comparative experiments. In this paper, this library will also be utilized for subsequent experimental comparative analysis."}, {"title": "B. Development of KAN", "content": "Since its introduction, KAN has rapidly attracted widespread attention. Currently, research in the KAN field primarily focuses on two aspects: optimization of general structures and applications in specific scenarios.\nIn terms of optimizing the general structure of KAN, research efforts mainly concentrate on improving model accuracy and computational efficiency. These efforts have significantly enhanced KAN's performance, making it more competitive in various application scenarios.\nWav-KAN, proposed by Bozorgas and Chen, is one of the most notable variants in the KAN field at present. This method employs wavelet functions as basis functions to construct KAN, achieving higher accuracy and faster training speed compared to Spl-KAN [2]. Fast-KAN, proposed by Li, uses Gaussian radial basis functions to replace B-spline functions, achieving 3.33 times the forward propagation speed of Spl-KAN [3]. FourierKAN, proposed by Gist Noesis(a company), is a novel neural network layer that replaces traditional Linear + non-linear activation combinations with Fourier series approximations, offering potential benefits in optimization and numerical stability [4]. fKAN and rKAN, proposed by Aghaei et al., adopt trainable adaptive fractional orthogonal Jacobi functions and Pad\u00e9 approximation with rational Jacobi functions as basis functions, respectively, further improving KAN's training speed and performance [5] [6]. MultKAN, proposed by Liu et al., is an extension of KAN that introduces multiplication nodes [7].\nKAN has demonstrated enormous potential in applications for specific scenarios, making unique contributions in fields ranging from time series prediction to graph neural networks, complex networks, and scientific computing. In time series prediction, Genet and Inzirillo, inspired by RNN and LSTM, proposed TKAN [8]; T-KAN and MT-KAN, proposed by Xu et al., outperform traditional methods in time series prediction tasks [9].\nIn the field of graph neural networks, Graph Kolmogorov-Arnold Network (GKAN) proposed by Kiamari et al. surpasses traditional GCN in accuracy [10]. GKAN, constructed by De Carlo et al. based on KAN, outperforms state-of-the-art GNN models in node classification, link prediction, and graph classification tasks, while offering good interpretability [11]. KAGNNs, constructed by Bresson et al., also perform excellently in graph neural network tasks [12]."}, {"title": null, "content": "In the field of human activity recognition, the iKAN framework proposed by Liu et al. significantly outperforms existing methods in incremental learning tasks for wearable sensor-based human activity recognition (HAR) [13].\nKAN has also shown great potential as a substitute for MLP in composite networks. Cheon's research indicates that replacing MLP with KAN in CNN networks can accelerate convergence, with the combination with ConvNeXT being particularly effective [14]. FourierKAN-GCF, proposed by Xu et al., uses Fourier KAN to replace MLP in GCN, surpassing most state-of-the-art methods [15]. U-KAN, constructed by Li et al., achieves higher accuracy at lower computational cost in medical image segmentation tasks [16].\nIn the field of AI for Science, KAN has also demonstrated enormous potential. Liu et al., the original authors of KANs, further explored a framework for integrating KAN with scientific knowledge, focusing mainly on small-scale, physics-related problems [7]. KINN, proposed by Wang et al., performs excellently in solving partial differential equation (PDE) problems [17]. Liu et al. developed a KAN-based domain decomposition method, providing solutions for multi-scale problems [18]. Abueidda et al. created a KAN-based neural operator for the discovery of physical laws [19]."}, {"title": "III. PRELIMINARY EXPERIMENTS", "content": "In this section, experiments are carried out to demonstrate the Efficient KAN Expansion Principle (EKE Principle): Allocating parameters in expanding network scale, rather than adopting more complex basis functions, can achieve more efficient performance improvements in KAN."}, {"title": "A. Experimental Setup", "content": "Experimental scenario: In this experiment, neural networks are tasked with MNIST handwritten digit recognition, utilizing the MNIST data set from the torchvision data set module. The batch size is set to 64 to facilitate efficient model training. The Adam optimizer is selected due to its superior performance across various tasks. To determine the optimal learning rate, a linear search strategy is employed, selecting different values at equal intervals from three distinct ranges: 9 points equally spaced in the interval [0.001, 0.009], 9 points in [0.01, 0.09], and 10 points in [0.1, 1]. This aims to identify the learning rate that enables the model to achieve optimal performance.\nSpl-KAN networks with grid sizes of 1, 2, 3, 4, and 5 are tested, where larger grid sizes indicate more complex basis functions but at the cost of reduced network scale."}, {"title": null, "content": "All networks consist of two layers, with an input size of 784 (corresponding to MNIST image size 28 x 28) and an output size of 10 (representing the 10 handwritten digits). It is noteworthy that a modified version of Spl-KAN from the efficient-kan GitHub repository is utilized in this study, as the original KAN library code has a lower level of engineering and slower speed. The modified version of KAN operates at a faster rate. The parameter count for Spl-KAN can be given by the following formula:\n$N_{\\text{param, Spl-KAN}} = \\sum_{l=1}^{N_{\\text{layer}}} n_{\\text{in}}^{l} n_{\\text{out}}^{l} (n_{\\text{gridSize}} + N_{\\text{splineOrder}} + 2)$ (2)\nwhere $N_{\\text{param,Spl-KAN}}$ represents the total number of parameters in Spl-KAN, $N_{\\text{layer}}$ is the number of layers, which is 2 in this experiment; $n_{\\text{in}}^{l}$ and $n_{\\text{out}}^{l}$ are the input and output sizes of layer l; $n_{\\text{gridSize}}$ is the grid size of Spl-KAN basis functions; serving as the independent variable in the experiment, ranging from 1 to 5; and $N_{\\text{splineOrder}}$ is the order of the spline basis functions, which is set to 3 in this experiment.\nBased on the above parameter count formula, the following equation is provided to calculate the hidden layer size for a two-layer network:\n$N_{\\text{hidden Size}} = \\sqrt{\\frac{N_{\\text{param, Spl - KAN}}}{\\frac{1}{2} (n_{\\text{in}}^{1} + n_{\\text{out}}^{2}) (N_{\\text{gridSize}} + N_{\\text{splineOrder}} + 2)}}$ (3)\nwhere $N_{\\text{hiddenSize}}$ is the hidden layer size; $N_{\\text{param,Spl-KAN}} = 80000$; $n_{\\text{in}}^{1} = 784$ is the input size of the first layer; $n_{\\text{out}}^{2} = 10$ is the output size of the second layer. The reason why $N_{\\text{param,Spl-KAN}}$ is set to 80000 is that [784, 100, 10] is a common network size for MLPs in MNIST tasks, with approximately 80,000 parameters.\nThe experiments are conducted on GPU, with the host CPU being \u201cIntel(R) Core(TM) i7-6850K CPU @ 3.60GHz\u201d and the GPU being \u201cGeForce RTX 3090\u201d. The operating system is Ubuntu 22.04.1, and we use Conda 24.7.1 and Python 3.12.4."}, {"title": "B. Experimental Results", "content": "Fig. 1 illustrates the performance of Spl-KAN networks with different grid sizes on the test set at the 10th epoch (note: epoch counting starts from 0) as the learning rate varies. Evaluation metrics include train loss (Fig. 1(a)), test loss (Fig. 1(b)), train accuracy (Fig. 1(c)), test accuracy (Fig. 1(d)), and F1 score (Fig. 1(e)). Observations indicate that all neural networks perform optimally when the learning rate is between 0.001 and 0.01. Therefore, focus is placed on the detailed performance within this learning rate range, and a separate plot is generated for this portion of the experimental results:"}, {"title": null, "content": "Fig. 2 demonstrates the experimental results within the learning rate range of [0.001, 0.01]. It is observed that as the grid size decreases, the values of the loss curves decrease, while the values of Accuracy and F1 score increase. This indicates that networks with smaller grid sizes demonstrate superior performance compared to those with larger grid sizes under various learning rate conditions.\nMoreover, the extreme values of each curve (minimum for loss curves, maximum for accuracy and F1 score) reflect the best performance of the network at that grid size. The data shows that as the grid size decreases, the minimum of the loss curves"}, {"title": null, "content": "decreases, while the maximum of accuracy and F1 score increases, further confirming the advantage of networks with smaller grid sizes in terms of optimal performance.\nIt is worth noting that due to the need for integer network sizes, there are slight differences in the parameters of different networks. Specifically, the network with a grid size of 1 has the least number of parameters, further emphasizing the superiority of networks with smaller grid sizes. A smaller grid size implies fewer learnable parameters in the basis functions, while allowing for a larger network scale given similar parameter counts. Therefore, the experimental results suggest that in KAN design, investing parameters in expanding network scale, rather than in more complex basis functions, is a more effective strategy.\nIt is speculated that this phenomenon may be attributed to the fact that when processing large-scale input data, the interactions between input parameters are more important than the transformations of the input parameters themselves. Consequently, the network should focus more on capturing the expressions of these interactions."}, {"title": "IV. SKAN: SINGLE-PARAMETERIZED KAN", "content": "Based on the principle obtained from the aforementioned preliminary experiments, the network scale is considered crucial for parameter utilization efficiency when constructing KAN. The most ideal structure is composed of basis function contains only one learnable parameter, enabling efficient network construction. A KAN with basis function containing only one learnable parameter is termed SKAN (Single-Parameterized KAN).\nFig. 3 illustrates the architecture of a two-layer SKAN model, which is similar to the KAN model. Each edge represents a parameterized function, while the intermediate and final nodes perform only addition operations. The key difference lies in the functions on the edges, which contain only one learnable parameter."}, {"title": "A. Definition of SKAN", "content": "The definition of a SKAN is given as follows:\n$\\text{SKAN} (x) = (\\Phi_{l}^{-1} \\circ \\Phi_{l}^{-2} \\circ \\cdots \\circ \\Phi_{l}^{0} \\circ \\Phi_{l}^{0}) (x)$ (4)"}, {"title": "B. Design of Single-Parameterized basis function", "content": "In designing single-parameterized basis functions for the SKAN model, various activation functions from MLP were referenced. Here follows description of the referenced activation functions:"}, {"title": null, "content": "1.ReLU (Rectified Linear Unit), Fig. 4(a):\nThe ReLU activation function is widely used in deep learning models due to its computational simplicity and effectiveness in mitigating the vanishing gradient problem. It is defined as:\n$f(x) = \\max(0, x)$ (6)\nwhere the output is 0 for negative inputs and x for positive inputs.\n2. Leaky ReLU, Fig. 4(b):\nLeaky ReLU is an improvement on the ReLU activation function, designed to address the suppression of negative inputs during training. It is defined as:\n$f(x) = \\max(\\alpha x, x)$ (7)\nwhere a is a small positive number, typically set to 0.01.\n3. Swish, Fig. 4(c):\nSwish is a self-gated activation function defined as:\n$f (x) = \\frac{x}{1+ e^{-x}}$ (8)"}, {"title": null, "content": "This function controls the degree of activation through a transformation of its own input, demonstrating good performance.\n4. Mish, Fig. 4(d):\nMish is a non-monotonic self-gated activation function defined as:\n$f (x) = x \\cdot \\text{tanh} (\\ln (1 + e^{x}))$ (9)\nMish sometimes outperforms ReLU and Swish by more effectively utilizing input information.\n5. Softplus, Fig. 4(e):\nSoftplus is a smooth version of the ReLU function, defined as:\n$f (x) = \\ln (1 + e^{x})$ (10)\nThis function is differentiable over the entire real domain and introduces smooth non-linearity to the model.\n6. Hard Sigmoid, Fig. 4(f):\nHard Sigmoid is a simplified version of the Sigmoid function, defined as:\n$f (x) = \\frac{x}{1+ e^{-x}}$ (11)\nThis function improves efficiency through simplified computation but may sacrifice some model expressiveness.\n7. ELU (Exponential Linear Unit), Fig. 4(g):\nELU improves upon ReLU by introducing an exponential transformation for negative inputs. It is defined as:\n$f (x) = \\begin{cases} e^{x}-1 & \\text{if } x < 0\\\\ x & \\text{otherwise} \\end{cases}$ (12)\nELU provides non-zero output for negative inputs, aiding model learning.\n8. Shifted Softplus, Fig. 4(h):\nShifted Softplus is a variant of the Softplus function, adjusting the output range by subtracting a constant. It is defined as:\n$f (x) = \\ln (1 + e^{x}) \u2013 \\ln (2)$ (13)\nThis adjustment helps control the output range of the activation function.\n9. GELU (Gaussian Error Linear Unit), Fig. 4(i):"}, {"title": null, "content": "GELU is an activation function based on the Gaussian distribution, defined as:\n$f(x) = 0.5x \\left(1 + \\text{tanh} \\left( \\sqrt{\\frac{2}{\\pi}} \\left(x + 0.044715x^{3} \\right) \\right) \\right)$ (14)\nGELU performs excellently in natural language processing tasks due to its ability to model input data uncertainty.\nBased on these activation functions, the single-parameter learnable non-linear functions were designed and shown in Table I. In these functions, the initial \u201cL\u201d in the function name indicates that the function is learnable; the name following \u201cL\u201d is the name of the prototype activation function. The variable k in each formula represents the sole learnable parameter."}, {"title": "V. EXPERIMENT", "content": null}, {"title": "A. Comparison of single-parameterized functions", "content": "1) Experimental Configuration\nExperimental scenario: This experiment aims to accomplish the MNIST handwritten digit recognition task using the MNIST dataset from torchvision's datasets module. To"}, {"title": null, "content": "enhance model training efficiency, the batch size is set to 64. The Adam optimization algorithm is selected due to its superior performance across various tasks. The training duration is set to 10 epochs. All SKANs maintain consistent network dimensions with the default configuration [784, 100, 10], and due to the single-parameter nature of SKAN basis functions, the parameter counts remain uniform across all networks. To determine the optimal learning rate, a linear search strategy is employed across three distinct ranges: [0.0001, 0.0009], [0.001, 0.009], and [0.01, 0.09], with 9 equidistant learning rate values selected within each interval. This method aims to identify the learning rate that enables optimal model performance. The experimental results, including loss, accuracy, and run time of each epoch, are recorded for SKANs utilizing single-parameter nonlinear functions as basis functions from Table I.\nExperiments are conducted on GPU, with the host CPU being \"Intel(R) Core(TM) i7-14700F@2.10 GHz\u201d and GPU being \"GeForce RTX 4060Ti\u201d.The operating System is Windows 11. The software configuration is Conda 24.7.1 and Python 3.12.3.\n2) Result\nFig. 5 presents the comparative results of various SKANs. To exclude meaningless data resulting from NaN values during training, accuracy values below 0.2 are omitted from the plots. SKANs constructed with function X are denoted as X-SKAN. LShifted Softplus-SKAN demonstrates superior performance across all learning rate values, surpassing other functions. LELU-SKAN, LLeaky ReLU-SKAN, and LHard Sigmoid-SKAN failed to generate data, indicating these SKANs were unable to train within the"}, {"title": "B. Comparison of SKAN and other KANs in 10 epochs", "content": "1) Experimental Configuration\nExperimental scenario: This experiment similarly aims to complete the MNIST handwritten digit classification task, utilizing the same learning rate settings as the previous experiment (V-A). The performance of LSS-SKAN, Spl-KAN (EfficientKAN implementation version), WavKAN, FastKAN, rKAN, fKAN, and FourierKAN is evaluated over 10 epochs of training, with assessment metrics including training set loss, test set loss, training set accuracy, test set accuracy, and F1 score. While fKAN"}, {"title": null, "content": "and rKAN are typically embedded within other networks, their integration with Spl-KAN gets error when training. Therefore, fKAN and rKAN are embedded within MLP for this study, with fKAN+MLP_and_rKAN+MLP deriving primary parameters from MLP and minimal parameters from KAN. To ensure result reliability, experiments are repeated 10 times, with the optimal results at each learning rate for each network selected as final data.\nThe hardware configuration is the same as V-A.\n2) Result\nFig. 7 illustrates the performance of various networks under different learning rates over 10 epochs of training. To maintain visual clarity, accuracy values below 0.9 are omitted. Significant performance variations are observed across different learning rates, with networks showing instability in the 0.01-0.1 range, while superior performance is noted in the 0.0001-0.01 range. Consequently, this study focuses on presenting data within this optimal range. Fig. 8 demonstrates the performance of different KANs within the learning rate interval [0.0001, 0.01]. LSS-SKAN exhibits optimal performance at the ideal learning rate (approximately 0.004), achieving superior test set accuracy and F1 scores compared to other pure KAN variants. Although rKAN+MLP and fKAN+MLP demonstrate higher accuracy, it should be noted that these are not pure KAN implementations. Furthermore, LSS-SKAN's superior performance is achieved with fewer parameters than WavKAN, FastKAN, and Spl-KAN, highlighting its enhanced efficiency. Fig. 9 provides a detailed comparison of network performance"}, {"title": "C. Comparison of SKAN and other KANs in 30 epochs", "content": "1) Experimental Configuration\nExperimental scenario: The experimental setup remains largely consistent with the previous configuration, with the key distinction being an extended training duration of 30 epochs to investigate network performance under prolonged learning conditions. The learning rate search space is refined to include 9 equidistant points within [0.0001, 0.0009] and 10 equidistant points within [0.001, 0.01], based on the superior performance observed in these ranges during previous experiments. Each experiment is repeated 10 times, with optimal results selected for each network at each learning rate.\nThe hardware configuration is the same as V-A.\n2) Result"}, {"title": null, "content": "Fig. 11 illustrates network performance across various learning rates over 30 epochs of training. Similar to the 10-epoch results, LSS-SKAN demonstrates superior performance at its optimal setting (lr=0.004) compared to other pure KAN implementations. While MLP+fKAN and MLP+rKAN maintain higher accuracy compared to pure KAN implementations, these networks lack the visualization advantages inherent to pure KAN architectures.\nFig. 12 provides a detailed comparison of optimal performance metrics across networks after 30 epochs of training. Consistent with previous observations, LSS-SKAN achieves the highest accuracy and F1 scores among pure KAN implementations, signif-"}, {"title": null, "content": "icantly outperforming Spl-KAN and FastKAN, moderately exceeding FourierKAN, and marginally surpassing WavKAN. A notable distinction is the observation of LSS-SKAN outperforming WavKAN, with more pronounced accuracy advantages, indicating enhanced potential of LSS-SKAN during extended training periods. Also, compared to 10-epoch testing, the performance gap between LSS-SKAN and MLP+fKAN/MLP+rKAN narrows during extended training. After 30 epochs, LSS-SKAN demonstrates test set accuracy improvements of 0.58%, 1.65%, 2.57%, and 0.22%, compared to FourierKAN, Spl-KAN, FastKAN, and WavKAN, respectively.\nFig. 13 presents epoch-wise training progression for networks under optimal conditions over 30 epochs. WavKAN, MLP+fKAN, and MLP+rKAN exhibit rapid initial learning rates, with LSS-SKAN showing slightly slower initial progress, while other networks demonstrate more gradual early-stage learning. Despite FastKAN's early convergence, its accuracy remains comparatively low.\nDuring the intermediate training phase, LSS-SKAN gains momentum, gradually approaching WavKAN's performance, while WavKAN, MLP+fKAN, and MLP+rKAN exhibit oscillatory behavior with slower learning progression. FourierKAN also shows improvement but maintains a significant gap relative to WavKAN. Spl-SKAN's accuracy stabilizes around 0.96, while FastKAN experiences substantial fluctuations between 0.93-0.96.\nIn the later training stages, LSS-SKAN progressively surpasses WavKAN, achieving"}, {"title": null, "content": "superior accuracy among all pure KAN implementations. LSS-SKAN, WavKAN, MLP+fKAN, and MLP+rKAN stabilize within the 0.97-0.98 accuracy range. FourierKAN demonstrates continued improvement but remains below WavKAN's performance level. FastKAN continues to exhibit accuracy fluctuations, while other networks stabilize at slightly lower accuracy levels.\nFig. 14 presents average per-epoch training times over 30 epochs. In contrast to 10-epoch results, LSS-SKAN demonstrates more pronounced computational efficiency advantages compared to other networks, surpassing even FourierKAN. LSS-SKAN achieves training time reductions of 8.03%, 502.89%, 41.78%, 38.56%, 9.27%, and 9.75%, compared to FourierKAN, MLP+rKAN, MLP+fKAN, Spl-KAN, FastKAN, and WavKAN, respectively."}, {"title": "D. Summary of Experimental Results", "content": "Three experiments were conducted in this section. In the first experiment, a linear learning rate search strategy was employed to train on the MNIST handwritten digit dataset for 10 epochs, evaluating the effectiveness of different single-parameter functions as basis functions in constructing SKAN. The results indicated that the LShifted Softplus function demonstrated optimal performance in SKAN construction, leading to its selection for building LSS-SKAN.\nIn the second experiment, a short-term comparative analysis (10 training epochs) was performed between LSS-SKAN and six popular KAN variants (FourierKAN,"}, {"title": null, "content": "MLP+rKAN, MLP+fKAN, Spl-KAN, FastKAN, and WavKAN) on the MNIST handwritten digit classification task, examining both accuracy and execution time. It should be noted that rKAN and fKAN are typically combined with MLP, as their composite training with Spl-KAN proved unsuccessful. The test results demonstrated that LSS-SKAN surpassed all pure KAN variants in accuracy, outperforming FourierKAN, Spl-KAN, FastKAN, and WavKAN by 0.53%, 1.51%, 2.15%, and 0.07% respectively. Additionally, in terms of average training time, LSS-SKAN exhibited superior performance compared to most tested KAN variants, achieving faster execution times than MLP+rKAN, MLP+fKAN, Spl-KAN, FastKAN, and WavKAN by 490.63%, 36.77%, 37.61%, 2.65%, and 9.53% respectively.\nThe third experiment consisted of long-term testing, extending the training to 30 epochs to evaluate the performance of LSS-SKAN against the six KAN variants in terms of accuracy and execution time. The results revealed that after 30 epochs of training, LSS-SKAN maintained its superiority over all pure KAN variants, achieving higher accuracy than FourierKAN, Spl-KAN, FastKAN, and WavKAN by 0.58%, 1.65%, 2.57%, and 0.22% respectively. Regarding average training time, LSS-SKAN demonstrated better efficiency than all tested KAN variants, performing faster than FourierKAN, MLP+rKAN, MLP+fKAN, Spl-KAN, FastKAN, and WavKAN by 8.03%, 502.89%, 41.78%, 38.56%, 9.27%, and 9.75% respectively. Although LSS-SKAN showed slightly lower accuracy compared to MLP+rKAN and MLP+fKAN in long-term testing, it is worth noting that MLP+rKAN and MLP+fKAN are primarily composed of MLP structures and therefore lack the high visualization characteristics inherent to pure KAN architectures."}, {"title": "VI. DISSCUSION", "content": null}, {"title": "A. Building Your Own SKAN", "content": "In this study", "pip install single-kan": "The source code of SKAN has been made publicly available on the GitHub platform to facilitate further development and contributions from the community (https://github.com/chikkkit/SKAN).\nThe experimental code has also been made public in the GitHub repository (https://github.com/chikkkit/LSS-SKAN). It should be noted that this repository is distinct from the SKAN library. The SKAN library is designed to"}]}