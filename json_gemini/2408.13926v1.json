{"title": "FedGlu: A personalized federated learning-based glucose forecasting algorithm for improved performance in glycemic excursion regions", "authors": ["Darpit, Dave", "Kathan Vyas", "Jagadish Kumaran Jayagopal", "Alfredo Garcia", "Madhav Erraguntla", "Mark Lawley"], "abstract": "Continuous glucose monitoring (CGM) devices provide real-time glucose monitoring and timely alerts for glycemic excursions, improving glycemic control among patients with diabetes. However, identifying rare events like hypoglycemia and hyperglycemia remain challenging due to their infrequency. Moreover, limited access to sensitive patient data hampers the development of robust machine learning models. Our objective is to accurately predict glycemic excursions while addressing data privacy concerns. To tackle excursion prediction, we propose a novel Hypo-Hyper (HH) loss function, which significantly improves performance in the glycemic excursion regions. The HH loss function demonstrates a 46% improvement over mean-squared error (MSE) loss across 125 patients. To address privacy concerns, we propose FedGlu, a machine learning model trained in a federated learning (FL) framework. FL allows collaborative learning without sharing sensitive data by training models locally and sharing only model parameters across other patients. FedGlu achieves a 35% superior glycemic excursion detection rate compared to local models. This improvement translates to enhanced performance in predicting both, hypoglycemia and hyperglycemia, for 105 out of 125 patients. These results underscore the effectiveness of the proposed HH loss function in augmenting the predictive capabilities of glucose predictions. Moreover, implementing models within a federated learning framework not only ensures better predictive capabilities but also safeguards sensitive data concurrently.", "sections": [{"title": "1 INTRODUCTION", "content": "A peptide hormone known as insulin is produced by the beta cells in the pancreases. Insulin plays a crucial role in regulating the transport of glucose and facilitating its absorption by cells from the bloodstream for energy production. Any disruption in this process leads to a medical condition called Diabetes mellitus (DM), more commonly referred as diabetes. Globally, this chronic disease affects nearly 537 million lives with an additional 374 million classified as prediabetic, putting them at similar risks of developing complications [1]. Diabetes can lead to both, immediate and long-term health issues, including blindness, kidney failures, amputations, heart-related diseases, seizures and in severe cases, even death [2-4]. Diabetes is broadly categorized into Type 1 and Type 2. In type 2 diabetes, cells become resistant to insulin, hindering their ability to absorb glucose. Type 2 diabetes constitutes 95% of the diabetes population and is typically prevalent in older age-groups. On the other hand, Type 1 diabetes arises when the pancreas fails to produce sufficient insulin. Type 1 diabetes accounts for about 5% of the diabetes population and is mainly found in age groups 0-20 years of age. However, it requires a much more rigorous diabetes management to avoid the imminent consequences. The overarching goal of diabetes management is to maintain euglycemia (normal glucose levels) or increase time spent within the optimal range (time-in-range, TIR)[2]. Insulin therapy is the mainstay of diabetes management, requiring a delicate balance between short-term and long-term objectives [5, 6]. While diabetes cannot be cured, proper insulin management allows patients with diabetes to keep optimal blood glucose levels and minimize the risk of further complications [7].\nThe most widely used method for glucose monitoring involves the use of fingerstick technique, where a small blood sample is drawn by pricking the finger and analyzed using the glucometer [8, 9]. This was revolutionized by Continuous Glucose Monitoring (CGM) devices, that offer real-time glucose monitoring with automated readings. This has evidently led to increasing time spent by patients in TIR, a crucial metric in diabetes care [10, 11]. CGM devices have proven to enhance glycemic control by reducing both hypoglycemia and hyperglycemic excursions through real-time predictions [12, 13]. While previous research for Type 1 diabetes has primarily focused on predicting hypoglycemic events [10, 14-19], achieving glycemic goals requires maximizing glucose readings in TIR by avoiding both hypoglycemia and hyperglycemia.\nEfforts to develop machine learning (ML) models for glucose predicting using historical CGM readings have shown reasonable efficacy. These models typically rely on extensive training data, often gathered through large-scale clinical research studies or healthcare providers. The data is often stored in cloud-based servers and poses significant privacy risks, with concerns about data theft and user privacy [20]. Given the escalating concerns of data theft, user security and privacy, recently many countries have enforced regulations aiming to address these concerns and protection regulations to protect the interests of the people [21-23]. An alternative strategy is to develop individual models with personal data to enhance privacy, but this comes with the drawback of reduced prediction performance due to limited data. ML and deep learning models require substantial data for optimal performance.\nThus, to overcome the challenges in glycemic excursion prediction and privacy concerns, we propose a collaborative learning framework that addresses these concerns simultaneously. The major contributions of this paper are as follows:\n\u2022 A novel, HH loss function, for improved prediction performance in the glycemic excursion regions\n\u2022 FedGlu, a machine learning model trained in a federated learning framework for improving model performance and data privacy preservation simultaneously.\nThe remainder of the paper is organized as follows: Section II reviews the current literature and points out relevant papers. Section III describes the dataset in detail. It also explains the methods like HH loss function,"}, {"title": "2 STATE OF THE ART", "content": "In this section, we provide a brief overview of the existing literature on glucose prediction. While a comprehensive review may be out of the scope of this paper, we emphasize works that have focused on predicting glucose values with deep learning approaches, use of federated learning for healthcare applications and extending the global federated model through fine tuning for personalization.\n2.1 CGM-based glucose prediction\nThe first attempt to predict future glucose levels using past values dates back to 1999 by Bremer and Gough [24]. Since then, researchers have continually advanced the literature to develop powerful and highly accurate models. Machine learning models for glucose prediction fall into two categories: (a) Classification tasks, (b) Regression task. Consistent with the theme of our paper, we focus on the regression tasks. Earlier approaches utilized conventional machine learning methods such as linear regression (LR), support vector regression (SVR), random forests (RF), boosting algorithms [25-28] and time-series forecasting methods like autoregressive integrated moving average (ARIMA) [29, 30] for predicting glucose levels. In recent years, there has been a shift towards employing deep learning (DL) models, leveraging automated feature learning, robust pattern recognition, and abstraction through multiple layers.\nVarious DL architectures, including multi-layer perceptron (MLP) [31, 32], convolutional neural networks (CNN) and convolutional recurrent neural networks (CRNN) [33, 34], recurrent neural networks (RNN) [35, 36], short long-term memory networks (LSTM) [37-39], dilated RNNs [40, 41] and bi-directional LSTMs [42-44], have been proposed for blood glucose prediction.\nShuvo et al. [45] introduced a deep multi-task learning approach using stacked LSTMs to predict personalized blood glucose concentration. The proposed approach includes a combination of stacked LSTMs to learn generalized features across patients, clustered hidden layers for phenotypical variability in the data and subject-specific hidden layers for optimally fine-tuning models for individual patient improvement. The authors demonstrate superior results compared to state-of-the-art ML and DL approaches on the OhioT1DM dataset.\nA transformer based on an attention mechanism was recently proposed to forecast glucose levels and hypoglycemia and hyperglycemia events [46]. The proposed transformer network includes an encoder network to perform the regression and classification tasks under a unified framework, and a data augmentation step using a generative adversarial network (GAN) to compensate for the rare events of hypoglycemia and hyperglycemia. Results were demonstrated on two datasets, one including Type 1 diabetes patients and the other on Type 2 diabetes patients.\nIn pursuit of enhanced prediction performance, researchers have often grappled with the constraint of limited data for individual subjects or patients. A recently proposed approach addresses this challenge through multitask learning for advancing personalized blood glucose prediction [47]. This approach was evaluated against sequential transfer learning, revealing two key findings: (a) individual patient data alone may not suffice for training DL models and (b) a thoughtful strategy is crucial to leverage population data for improved individual models. The dataset that was used in this study was the OhioT1DM dataset."}, {"title": "2.2 Data imbalance", "content": "There is a significant imbalance in glucose data distribution across the hypoglycemic, hyperglycemia, and normal ranges. Typically, only 2-10% of glucose readings fall into the hypoglycemic range, while about 30-40% fall in the hyperglycemic range. From a statistical perspective, this presents a classic case of an imbalanced regression problem. While numerous approaches have been developed to tackle the imbalanced data in the classification setting, very few works have been proposed to address the imbalance regression problem, like ours, in the literature [49]. Existing approaches for imbalanced regression can be broadly categorized into two types:\n2.2.1 Sampling-based approaches\nThese methods attempt to either undersample the high-frequency values or oversample the low frequency (rare) values. However, determining the 'notion or rarity' in a regression problem is challenging compared to a classification task. Oversampling may lead to overfitting, whereas under sampling may result in sub-optimal performance because of loss of key information. Chawla et. al [50] proposed an approach that generates synthetic samples by combining oversampling and under sampling of the training data for classification task.\n2.2.2 Cost-sensitive approaches\nCost-sensitive approaches: These approaches introduce a penalizing scheme during training to enable the model to handle outlier values (low-frequency or rare values) enhancing its effectiveness in predicting within those ranges. The recent success of this approach [51-53] motivates us to explore this approach further through our customized loss function."}, {"title": "2.3 Federated learning for healthcare", "content": "Federated learning (FL) has substantial disruptive potential in healthcare, a domain constrained by sensitive data and strict regulations such as the Health Insurance Portability and Accountability Act (HIPAA) in the US. The reluctance of healthcare entities to share sensitive data has fueled the adoption of FL in various healthcare applications, like medical image processing [54-56], IoT-based smart healthcare applications [57], managing electronic health records (EHR) [58], disease prediction [59-61], predict hospitalizations and mortality [62-64], natural language processing from clinical notes [65-67], etc. A few researchers comprehensively reviewed federated learning in healthcare [68-71].\nIn the diabetes literature, FL has recently gained traction. A recent study proposed a deep learning approach in the Diabetes Management Control System (DMCS) [72], using 30 virtual subjects from the FDA approved UVA/Padova Type 1 diabetes simulator. Features such as past glucose values, carbohydrate intake, and insulin-on-board were used as input for the model for the diagnosis of diabetes. The findings clearly demonstrate the superior performance of the federated model over the local models."}, {"title": "2.4 Personalized federated learning", "content": "Federated learning has its many unique challenges [75-77], with one prominent issue being the variation in data distribution across clients in the network. This is characteristic of non-i.i.d. and imbalanced data [78]. This is particularly significant in healthcare applications where individual patients possess diverse demographics and health histories, necessitating adaptive solutions tailored to each participant [79].\nOne of the pioneering works in federated learning with wearable healthcare data is FedHealth [80], which introduces personalization through transfer learning. This is achieved by first training a conventional global model in a federated learning framework and later fine-tuning two fully connected layers (through transfer learning) to learn activities and tasks for specific users. The study utilizes publicly available human activity recognition data from accelerometry and gyroscope data across 30 users for multiple activity class prediction. The authors employ a CNN based deep learning model, comparing its performance against traditional machine learning methods like RF, SVM, and KNN. The results demonstrate a 4% average improvement in performance for personalized models compared to the global model.\nAnother noteworthy application of personalized federated learning was used for in-home health monitoring [81]. The authors introduce FedHome, a cloud-edge based federated learning framework, where a shared global model is initially learned from multiple network participants. Individual personalization is achieved using a generative convolutional autoencoder (GCAE), aiming to generate a class-balanced dataset tailored to individual client's data. FedHome exhibits a notable improvement of over 10% in accuracy compared to a conventional global federated learning model.\nA recent study on remote patient monitoring (RPM) introduced FedStack architecture, a personalized federated learning approach [79]. The study was based on the MHEALTH dataset with 10 patients [82, 83]. A total of 21 features are extracted from three sensor data types (accelerometry, gyroscope and magnetometer) to classify 12 different natural activities. Three different model architectures (ANN, CNN, Bi-LSTM) are used in the study. FedStack architecture achieves personalization by aggregating heterogenous architectural models at the client level and demonstrates that the FedStack approach consistently outperforms both local and global models.\nA recent work in personalized federated learning focused on in-hospital mortality prediction [62]. The study utilized a publicly available electronic health records (HER) database, comprising over 200,000 patients across 208 hospitals in the US. Features were extracted from patients' HER and employed for a binary classification problem, with a multi-layer perceptron (MLP) adopted for modeling. The proposed POLA method involves the initial training of a global federated learning model, referred to as the teacher model. In the subsequent step, local adaptation is accomplished through a Genetic Algorithm (GA) approach. Comparative results highlight the superior performance of the POLA approach against the traditional FedAvg [84] and two other state-of-the-art personalized federated learning architectures [85, 86]."}, {"title": "3 METHODS AND MATERIALS", "content": "This section describes the datasets used in this study, data processing steps and the experimental setup. Further, we describe the prediction model and the different frameworks used in this study.\n3.1 Clinical datasets\nThere are two clinical datasets used in this study:\n3.1.1 Ohio T1DM\nThe Ohio T1DM [87] dataset was publicly released in two batches (2016 and 2018) with a total of 12 participants. During the 8-week study period, all 12 participants wore a Medtronic Enlite CGM for collecting glucose readings, a Medtronic 530G/630G insulin pump and an Empatica/ Basis sensor for collecting physiological data collection. For our analysis, we only focus on the CGM data. The patient data was collected in free-living conditions.\n3.1.2 TCH study\nThis is a proprietary data collected at Texas Children's Hospital, Houston, TX. from a total of 113 T1D patients using Dexcom CGM devices. The data for each patient spans a period of 30-90 days. Additional information about this dataset is available in our previous publications. [25].Similar to the OhioT1DM dataset, the data for all patients was collected under free-living conditions.\nComprehensive details about these two datasets are provided in Table I. Figure. 1 illustrates glycemic excursions across all patients, specifically hypoglycemia and hyperglycemia profiles. The dotted grey lines on the x-axis and y-axis represent the median hypoglycemia and hyperglycemia percentages respectively. It is evident that the prevalence of hyperglycemia ($x = 41.7%$and $x = 22.7%$) is considerably higher compared to hypoglycemia ($x = 2.3%$and $x = 1.7%$).\n3.2 HH loss\nThe genesis of our custom loss function is based on two needs: (a) enhancing penalties for errors in glycemic excursion regions, and (b) simultaneously balancing these penalties considering the uneven distribution of samples in the hypoglycemia, normal and hyperglycemia glucose ranges. For the first goal, we add a polynomial increasing penalty for glucose readings further they deviate from the normal range. To achieve the second objective i.e., to account for the disproportionate sample distribution in the hypoglycemia, hyperglycemia, and normal glucose ranges, we introduce a tuning parameter \u03b1. This parameter ensures that while reducing the errors in the glycemic excursion regions, the predictive performance for overall glucose readings is not compromised."}, {"title": "3.3 Federated learning", "content": "Federated Learning is a distributed learning framework that enables the training of machine learning models without transmitting sensitive user data to a central server. It relies on collaborative learning between participating users, where each user trains a shared model locally on its own training data through multiple rounds of optimization. Only the model characteristics such as model parameters, weights, gradients etc. are shared among users in the network. The sharing process can occur directly among the participating users or through a central server depending on the FL network topology. After receiving and aggregating the shared\n3.4 Problem formulation\nIn general, input for a data-driven algorithm for predicting future glucose levels primarily consists of historical glucose readings (g) and other features (physiological data, insulin, and carbohydrates intake etc.) if available. In our case where we only rely on glucose readings previously observed and the input data $X_t$ is denoted as:\n$X_t = [x_{t-W_L+1}, x_{t\u2212W_L+1}, \u2026, x_t]\u2208 R^{1 \u00d7 W_L}$\nwhere $x_t \u2208 R^{1\u00d71}$ is the glucose reading at timestep t, and $W_L$ is the length of previous glucose readings that are considered as input features for prediction. The final set of input features X passed to the model is a transformation $f$ of $X_t$ such that $X = f(X_t)$ where $f_n$ is the min-max normalization function to scale the range of input features between the range [0,1].\nFor a given prediction horizon (PH) and a series of glucose readings at time t as $g_t$, predicting a future glucose value can be defined as\n$\\hat{g}_{t+PH} = f_n^{-1}(M(X))$\nwhere $\\hat{g}_{t+PH}$ is an output of the deep learning model M and is de-normalized to get the final prediction. Based on the available literature [88], data preprocessing involves two main steps: first, replacing 'Low' and 'High' glucose readings with 40 mg/dL and 400 mg/dL, respectively. Second, imputing missing glucose readings through linear interpolation when less than six consecutive readings are missing. This threshold is chosen based on the literature available on autocorrelation between glucose readings [27, 89]. After interpolating missing values, consecutive sequences of glucose readings in the last 2 hours (24 readings) are taken as individual samples. This time-window is selected based on our previous works for glucose prediction [12]. Any samples with missing values at this stage are excluded from our analysis.\nFor our analysis, we compare three different model types (Figure. 2), frameworks based on the data used for training and the training process:\n3.4.1 Local model (LM or $M_{Local}$)\nData is stored locally with the patient and not shared across other entities, such as a central server or other patients in the network. The model is trained individually for each patient, and the training data is limited to individual patient. Local models are fully privacy-preserving with no data or parameter sharing, but they risk lower prediction performance due to limited training data.\n3.4.2 Central model (CM or $M_{central}$)\nPatient data resides at the central server, and model training occurs on this server. A single model is trained commonly across all available patients. While the central model benefits from a large data pool of data for training, it has minimal privacy preservation for individual patients, as the entire patient data corpus is shared with a central server.\n3.4.3 Federated model (FM or $M_{Fed}$)\nData storage and model training occur locally at the patient's device. During training, model weights/parameters are frequently shared with a central server for aggregation. These aggregated weights are then returned to individual patients and serve as initializers for further training. Actual data is never shared with other patients in the network or with a global server, maintaining privacy. This is known as model development in a federated-learning framework [84]. Models in this framework achieve better prediction performance through shared learning with other patients while preserving privacy. However, global models may not always be the most suitable for every entity (here, patient) in the network. To address this, we extend the global model via a fine-tuning step, by further optimizing the global model with the custom HH loss function to achieve personalization for individual patients in the network."}, {"title": "3.5 FedGlu: The algorithm", "content": "The holistic mechanism of the FedGlu algorithm is presented in Algorithm 1 where each patient trains a globally shared model for predicting future glucose values under the coordination of an aggregating server by leveraging federated learning. In each communication round, each client (here, patient) receives a parameterized model from the server, which it optimized using local data (Algorithm 2). After a fixed number of epochs, these individual locally trained model parameters are shared with the server for aggregation. The server aggregates these parameters and shares the updated global model with each individual client. This process repeats until the global model converges. After that, each patient performs an asynchronous 'local fine-tuning' step (Algorithm 3) on the local user data. For this, each client (here, patient) in the networking receives the final global model, which the local clients optimize based on its local data. The loss function is the key difference between training the global model and the local fine-tuning step. The global federated model is trained using the standard MSE loss whereas the local fine-tuning step is performed using the custom HH-loss function."}, {"title": "3.6 Model architecture", "content": "A multilayer perceptron model is used for analysis in this study. The input layer consists of glucose readings observed in the last 2 hours. This input with dimensions 24 x 1 is fed into a dense layer of 512 neurons with a rectified linear unit (ReLU) activation function. The output feature map is passed through two hidden layers with 256 neurons and outputs a 64-neuron layer which, both activated with a ReLU activation. The final output layers get their input from the 3rd dense layer in the network and provide a single number prediction of the future glucose reading."}, {"title": "3.7 Validation approach", "content": "In line with our prior research [12, 14], we implement a 5-fold validation strategy with temporally partitioned splits. This strategy guarantees that training and testing splits are derived from non-overlapping time-windows, mitigating potential biases from temporal correlations that could lead to overly optimistic results. This methodology aligns with the BGLP Challenge, where the initial few days of data are utilized for training and the subsequent days constitute the hold-out test set. In our work, we extend the same across multiple splits to make it more robust."}, {"title": "3.8 Evaluation metrics", "content": "We illustrate the efficiency of our approach using standard metrics in glucose prediction: root mean squared error (RMSE) along with Clark's Error Grid (Figure. 3) [90] to evaluate clinical significance of the predictions. RMSE is defined as:\n$RMSE = \\sqrt{\\frac{\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2}{N}}$\n$N$ = number of data points\n$y_i$ = true value\n$\\hat{y}_i$ = predicted value\nClarke's Error Grid Analysis (CEGA) is described in Figure. 3 and the definitions of each zone are provided in Table 2."}, {"title": "3.9 Model training/ Hyperparameter tuning", "content": "To provide robust estimates of our methodology and support the enlisted contributions with results, we consider the following experiments:\n1. Setup 1 - Advantage of HH loss function: The performance with the HH loss function is compared against the standard mean squared error (MSE) loss. We demonstrate the advantage in terms of standard regression metrics and the clinical significance for glucose predictions. We compare the HH loss function results across two separate datasets namely TCH study data and the OhioT1DM dataset. We use a central (population-level) model to obtain the results.\n2. Setup 2 - Usefulness of Personalized Federated Models: We compare the performance across the different model types: central model, local model, global federated model, and the personalized federated model. The comparison is made independently across both datasets. This setup will prove the robustness of ML models trained in a federated learning work against the central and local models."}, {"title": "3.10 Evaluation metrics", "content": "All models at central, local, and federated (global and personal) levels shared identical architecture and parameters. The learning rate was fixed at 0.001 with a constant batch-size of 500. The training was set to a maximum of 50 epochs, with early stopping for a patience of 10 epochs to avoid prevent redundant training and thereby reduce computational time. The global federated model is trained with TensorFlow-federated. The client-optimizer was set to \u2018Adam' with a learning rate of 0.001 and the serve-optimizer to \u2018SGD' with a learning rate of 1 to mimic the baseline federated model [84]. The number of communication rounds was set to 50, and a simple weighted-average proportional to the number of samples with each patient (node) in the network was used as the aggregating function. The global federated model is saved after observing the train loss (MSE) convergence. Individual patients (nodes) then fine-tune this saved global federated model on their individual local data with an 'Adam' optimizer and a learning rate of 0.001. This fine-tuning step is however done the custom HH loss function. For comparing results, the local-level and (personalized) federated-level models, with a specific \u03b1, is selected based on the training data where the combined RMSE (across hypoglycemia and hyperglycemia) is minimum. At the central level, a single \u03b1 which showed the maximum combined reduction across all patients, was considered."}, {"title": "4 RESULTS", "content": "This section describes the datasets used in this study, data processing steps and the experimental setup. Further, we describe the prediction model and the different frameworks used in this study.\n4.1 Advantage of HH loss function\nIn the first analysis, we assess how the HH loss functions enhances prediction performance in the glycemic excursion regions while maintaining clinical significance for overall predictions based on two independent datasets. For this we consider a centralized (population-level) model development setting. Since a common model is trained across all the patients for the dataset, we choose a single'a' value (\u03b1 = 1) which showed the combined maximum improvement (reduction in RMSE) across hypoglycemia and hyperglycemia ranges on the training dataset.\n4.1.1 TCH study dataset"}, {"title": "4.2 Model training in a federated learning framework", "content": "The predictive capabilities of models incorporating the HH loss function were extensively evaluated at the local, central, and federated levels. In Figure. 5, the magnitude of difference (measured in RMSE) in various glycemic excursion regions is presented. Table 4 compares different model types through Clarke's Error grid analysis, while Table 5 outlines improvements in terms of the number of patients for RMSE and CEGA. Compared to local models, federated models exhibit an improvement of 16.67% (in RMSE) for predicting hypoglycemia and 18.91% (in RMSE) for predicting hyperglycemia simultaneous both statistically significant using the paired t-test (p < 0.01). For the TCH study data, federated models demonstrated improvements for 96 (out of 113) patients in hypoglycemia region and 110 (out of 111) patients in hyperglycemia regions over local models simultaneously. Regarding the OhioT1DM dataset, federated models improved by 9% (in RMSE) for predicting hypoglycemia and 33.29% (in RMSE) for predicting hyperglycemia. Although the improvement in hyperglycemia is statistically significant (p < 0.01) but hypoglycemia (p = 0.18) is not. For the OhioT1DM dataset, federated models improve for 9 (out of 12) patients in hypoglycemia region and 12/12 patients in hyperglycemia region, over the local models. Across the two datasets, there is an improvement of 12.37% (in RMSE) for predicting hypoglycemia and 29.05% (in RMSE) for prediction hyperglycemia \u2013 which are both statistically significant with (p < 0.01).\nWhen evaluated for clinical significance through Clark's Error Grid, it is evident that HH loss function improves glycemic excursions (Region: D+E) prediction at all levels without compromising the clinical significance of overall predictions (Region: A+B) across both datasets. Comparing local and federated models, for the TCH study data, we see a 37% reduction (p < 0.01) and for the OhioT1DM dataset, 31% reduction (p < 0.01) in points falling in the Region: D+E (compared to MSE) which signifies an increase in the detection capability of glycemic excursions. When comparing local and federated models for predictions falling in the Region: C of CEGA, for the TCH study data, there is a 50% reduction whereas for the OhioT1DM dataset, there is a 20% reduction in which signifies fewer false predictions with the federated models. Table 4 also provides evidence that federated models when compared to local models through CEGA, are able to improve over local models for all regions of CEGA for almost all patients across the two datasets.\nOn the other hand, central models distinctly outperform federated models for both hypoglycemia and hyperglycemia due to their advantage of a significantly larger training pool. This is evident in the evaluation with RMSE and CEGA. However, compared to the local models, federated models achieve a much closer performance than central models. Table 5 outlines the improvement with federated models against central and local models for each of TCH and OhioT1DM datasets."}, {"title": "5 DISCUSSION", "content": "Based on the results, the main contributions of our work can be summarized as:\n1. Introduction of a novel HH loss function aimed at improving glycemic predictions in the excursion regions while ensuring clinically significance.\n2. Model implementation with the HH loss function within a federated learning framework, balancing performance in the excursion region and privacy.\n5.1 Insights and observations\nModels with HH loss function can predict more accurately and clinically significantly in glycemic excursion regions at local, central, and federated levels. This performance may result in sub-optimal RMSE values in the overall glucose ranges. However, when compared for clinical significance, there is no reduction and on the contrary, it improves overall predictions in regions: A+B. A detailed evaluation with CEGA with different model types, including local, central, and federated models is presented in Table 6. A further expansion of CEGA zones for Table 6 is provided in Appendix 1."}, {"title": "5.2 Impact of 'a' parameter", "content": "The parameter 'a' can be tuned to achieve a well-balanced optimal prediction performance for hypoglycemia and hyperglycemia excursions regions simultaneously. A higher 'a' value prioritizes performance in the hypoglycemia regions whereas a lower 'a' emphasizes performance in the hyperglycemia region. Figure. 6 illustrates the average improvement (in RMSE) across patients in both glycemic excursion regions occurring co-occurring for various 'a' values. The red curve, representing improvement in hypoglycemia values, shows an exponential increase compared to MSE as 'a' increases. In contrast, the blue curve indicating improvement in hyperglycemia values, exhibits a downward trend with increasing 'a'. However, the slope for hypoglycemia values is significantly steeper than hyperglycemia values. This is because of the high data imbalance that exists, especially for hypoglycemia values (median: 1.64%) as compared to hyperglycemia values (median: 43%). This also highlights the greater of accurately predicting hypoglycemia values compared to hyperglycemic values. The parameter 'a' can be customized for local and federated models based on individual preferences and glycemic profiles to yield optimal results. Table 7 provides performance metrics (for central models) in terms of Clark's EGA for different 'a' values."}, {"title": "5.3 Federated models: Who benefits the most?", "content": "We compare the performance improvements achieved with federated models against central and local models (with HH loss) across varying glycemic profiles of both hypoglycemia and hyperglycemic regions. To categorize patients effectively, they are initially binned into groups of ten based on the percentage of hypoglycemic or hyperglycemic values in their profile. For hypoglycemia, these intervals include (0, 0.22]%, (0.22, 0.5]%, and so forth, while for hyperglycemia, intervals are defined as (0, 7.81]%, (7.81, 19.31]%, and so on.\nIn the context of hypoglycemia prediction (Figure. 7 - left), a notable trend emerges: as the percentage of hypoglycemic values increases, the prediction performance of federated models, global models and local models converges. Patients with higher hypoglycemia values exhibit similar prediction performance across local, central, and federated models. However, for patients with extremely low instances of hypoglycemia, central models far outperform local and federated models. This is because training data for central models has multiple instances of hypoglycemia for the model to train whereas local and federated models do not have that advantage. The Spearman correlation coefficient (\u03c1) between mean improvement (in RMSE) with federated models and increasing hypoglycemic (interval) values, is statistically significant against both local (\u03c1 = 0.01) and central models (\u03c1 = 0.02). When compared for variance, it shows similar trend and is statistically significant across local (\u03c1 = 0.01) and central (\u03c1 = 0.01) models. It is also observed that, predictions performance with federated models is closer to the central models and also maintain a clear advantage over the local models for close to 50% of the patients.\nIn contrast, for hyperglycemia prediction (Figure. 7 - right), no discernible patterns emerge across the different model types (federated model, global model, and local model) when compared to the hyperglycemia profile of patients. The Spearman correlation coefficient (\u03c1) for improvements with federated models over local (\u03c1 = 0.7) and central models (\u03c1 = 0.1) is not statistically significant. Furthermore, the performance disparity across these three model types (relative to hypoglycemia) is minimal for hyperglycemia prediction. A major contributing factor is a substantial difference in the number of hypoglycemic glucose values compared to hyperglycemic glucose values (the lowest bin for hyperglycemia is (0, 7.81]% and a median of 1.64%. In contrast, the highest bin for hypoglycemia is (4,63, 12.13]% and a median of 43%)."}, {"title": "5.4 Comparison with literature", "content": "We comprehensively compared between the HH loss function and a recent state-of-the-art NMSE (normalized mean squared error) that showed significant performance improvement in the hypoglycemic range. The comparison is made on standard evaluation metrics of RMSE and Clark's EGA. Analyzing the TCH study data", "Regions": "A+B with the HH loss function compared to using either of MSE or NMSE (Table 8).\nThe OhioT1DM dataset, used in the BGLP challenge, is a standard dataset for comparing glucose prediction performance. While the BGLP challenge and other literature aim to show accurate predictions of overall glucose values, our work focuses explicitly on improved performance in the glycemic excursion"}]}