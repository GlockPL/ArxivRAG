{"title": "A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models", "authors": ["Zihao Lin", "Samyadeep Basu", "Mohammad Beigi", "Varun Manjunatha", "Ryan A. Rossi", "Zichao Wang", "Yufan Zhou", "Sriram Balasubramanian", "Arman Zarei", "Keivan Rezaei", "Ying Shen", "Barry Menglong Yao", "Zhiyang Xu", "Qin Liu", "Yuxiang Zhang", "Yan Sun", "Shilong Liu", "Li Shen", "Hongxuan Li", "Soheil Feizi", "Lifu Huang"], "abstract": "The rise of foundation models has transformed machine learning research, prompting efforts to uncover their inner workings and develop more efficient and reliable applications for better control. While significant progress has been made in interpreting Large Language Models (LLMs), multimodal foundation models (MMFMs)\u2014such as contrastive vision-language models, generative vision-language models, and text-to-image models-pose unique interpretability challenges beyond unimodal frameworks. Despite initial studies, a substantial gap remains between the interpretability of LLMs and MMFMs. This survey explores two key aspects: (1) the adaptation of LLM interpretability methods to multimodal models and (2) understanding the mechanistic differences between unimodal language models and cross-modal systems. By systematically reviewing current MMFM analysis techniques, we propose a structured taxonomy of interpretability methods, compare insights across unimodal and multimodal architectures, and highlight critical research gaps.", "sections": [{"title": "1 Introduction", "content": "The rapid development and adoption of multimodal foundation models (MMFMs)\u2014particularly those integrating image and text modalities-have enabled a wide range of real-world applications. For example, text-to-image models (Rombach et al., 2022; Ramesh et al., 2022; Podell et al., 2023) facilitate image generation and editing, generative vision-language models (VLMs) (Zhu et al., 2023; Agrawal et al., 2024) support tasks like visual question answering (VQA) or image captioning tasks, and contrastive (i.e., non-generative) VLMs such as CLIP (Radford et al., 2021) are widely used for image retrieval. As multimodal models advance, there is a growing need to understand their internal mechanisms and decision-making processes (Basu et al., 2024a). Mechanistic interpretability is crucial not only for explaining model behavior but also for enabling downstream applications such as model editing (Basu et al., 2024a), mitigating spurious correlations (Balasubramanian et al., 2024), and improving compositional generalization (Zarei et al., 2024).\nInterpretability in machine learning, LLMs, and multimodal models is a broad and context-dependent concept, varying by task, objective, and stakeholder needs. In this survey, we adopt the definition proposed by Murdoch et al. (2019): \u201cThe process of extracting and elucidating the relevant knowledge, mechanisms, features, and relationships a model has learned, whether encoded in its parameters or emerging from input patterns, to explain how and why it produces outputs.\u201d This definition emphasizes the extraction and understanding of model knowledge, but what constitutes relevant knowledge\" depends on the application context. For instance, in memory editing applications, interpretability enables precise modifications to internal representations without disrupting other model functions, while in security contexts, it helps highlight input features and activations that signal adversarial inputs. Through this lens, this survey examines interpretability methods, exploring how they uncover model mechanisms, facilitate practical applications, and reveal key research challenges.\nWhile interpretability research has made significant progress in unimodal large language models (LLMs) (Meng et al., 2022a; Marks et al., 2024), the study of MMFMs remains comparatively underexplored. Given that most multimodal models are transformer-based, several key questions arise: Can LLM interpretability methods be adapted to multimodal models? If so, do they yield similar insights? Do multimodal models exhibit fundamental mechanistic differences from unimodal language models? Additionally, to analyze multimodal-specific processes like cross-modal interactions, are entirely new methods required? Finally, we also examine the practical impact of interpretability by asking\u2014How can multimodal interpretability methods enhance downstream applications?\nTo address these questions, we conduct a comprehensive survey and introduce a three-dimensional taxonomy for mechanistic interpretability in multimodal models: (1) Model Family \u2013 covering text-to-image diffusion models, generative VLMs, and non-generative VLMs; (2) Interpretability Techniques \u2013 distinguishing between methods adapted from unimodal LLM research and those originally designed for multimodal models; and (3) Applications \u2013 categorizing real-world tasks enhanced by mechanistic insights. Our survey synthesizes existing research and uncovers the following insights: (i) LLM-based interpretability methods can be extended to MMFMS with moderate adjustments, particularly when treating visual and textual inputs similarly. (ii) Novel multimodal challenges arise such as interpreting visual embeddings in human-understandable terms, necessitating new dedicated analysis methods. (iii) While interpretability aids downstream tasks, applications like hallucination mitigation and model editing remain underdeveloped in multimodal models compared to language models. These findings can guide future research in multimodal mechanistic interpretability.\nRecently, Dang et al. (2024) provides a broad overview of interpretability methods for MMFMS across data, model architecture, and training paradigms. Another concurrent work (Sun et al., 2024) reviews the multimodal interpretability methods from a historical view, covering works from 2000 to 2025. While insightful, our work differs from theirs in both focus and scope. To be specific, our work examines how established LLM interpretability techniques adapt to various multimodal models, analyzing key differences between unimodal and multimodal systems in techniques, applications, and findings.\nThe summary of our contributions are:\n\u2022 We offer a comprehensive survey of mechanistic interpretability for multimodal foundation models spanning generative VLMs, contrastive VLMs, and text-to-image diffusion models.\n\u2022 We introduce a simple and intuitive taxonomy which helps to distinguish the mechanistic methods, findings, and applications across unimodal and multimodal foundation models, highlighting critical research gaps.\n\u2022 Based on the mechanistic differences between LLMs and multimodal foundation models, we identify fundamental open challenges and limitations in multimodal interpretability, providing directions for future research"}, {"title": "2 Taxonomy", "content": "In our survey, we present an easy-to-read taxonomy that categorizes mechanistic interpretability techniques along three dimensions: (i) Dimension 1 provides a view of the mechanistic insights across various multimodal model families including non-generative VLMs (e.g., CLIP), text-to-image models (e.g., Stable-Diffusion), and multimodal language models (e.g., LLaVa). We describe the architectures studied in our paper in Sec.(3); (ii) Dimension 2 categorizes whether the technique has been used for language models (Sec.4) or is specifically designed for multimodal models (Sec.5); (iii) Dimension 3 links insights from these mechanistic methods to downstream practical applications (Sec.6). The taxonomy is visualized in Figure 1. In particular, the distribution of insights and applications are in-line in Sec. (4, 5, 6).\nWe believe this simple categorization will help readers (i) understand the gaps between unimodal language models and multimodal models in terms of mechanistic insights and applications, and (ii) identify the multimodal models where mechanistic interpretability (and their applications) is underexplored."}, {"title": "3 Details on Model Architectures", "content": "In this section, we introduce three main categories of multimodal models covered by our survey, including (i) Contrastive (i.e., Non-Generative) Vision-Language Models, Generative Vision-Language Models, and Text-to-image Diffusion Models. We choose these three families as they encompass the majority of the state-of-the-art architectures used by the community currently."}, {"title": "3.1 Non-Generative Vision-Language Models", "content": "One non-generative vision-language model (e.g., CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), FILIP (Yao et al., 2021), SigCLIP (Zhai et al., 2023), DeCLIP (Li et al., 2022) and LLIP (Lavoie et al., 2024)) usually contains one language-model-based text encoder and one vision-model-based vision encoder. These models are particularly suited for real-world applications such as text-guided image retrieval, image-guided text retrieval and zero-shot image classification."}, {"title": "3.2 Text-to-Image Diffusion Models", "content": "State-of-the-art text-guided image generation models are primarily based on the diffusion objective (Rombach et al., 2022; Ho et al., 2020), which predicts the noise that was added during the forward diffusion process, allowing it to learn how to gradually denoise random Gaussian noise back into a clean image during the reverse diffusion process. One diffusion model often contains a text encoder (e.g., CLIP) and a CNN-based U-Net (Ronneberger et al., 2015) for denoising to generate images. Early variants of text-to-image generative models with this objective include Stable-Diffusion-1 (Rombach et al., 2022) (which perform the diffusion process in a compressed latent space) and Dalle-2 (Ramesh et al., 2022) (which perform the diffusion process in the image space instead of a compressed latent space). In recent times, SD-XL (Podell et al., 2023) improves on the early Stable-Diffusion variants by using a larger denoising UNet and an improved conditioning (e.g., text or image) mechanism. More recent models such as Stable-Diffusion-3 (Esser et al., 2024) obtain stronger image generation results than previous Stable-Diffusion variants by (i) using a rectified flow formulation, (ii) scalable transformer architecture as the diffusion backbone and (iii) using an ensemble of strong text-encoders (e.g., T5 (Raffel et al., 2020; Chung et al., 2022)). Beyond image generation, in terms of downstream applications, text-to-image models can also be applied for image editing (Hertz et al., 2022), and style transfer (Zhang et al., 2023)."}, {"title": "3.3 Generative Vision-Language Models", "content": "In our paper, we investigate the most common generative VLMs which are developed by connecting a vision encoder (e.g., CLIP) to a large language model through a bridge module. This bridge module (e.g., a few MLP layers (Liu et al., 2023a) or a Q-former (Li et al., 2023a)) is then trained on large-scale image-text pairs. Frozen (Tsimpoukelli et al., 2021) is one of the first works to take advantage of a large language model in image understanding tasks (e.g., few-shot learning). Follow-up works such as MiniGpt (Zhu et al., 2023), BLIP variants (Li et al., 2023b) and LLava (Liu et al., 2023a) improved on Frozen by modifying the scale and type of the training data, as well as the underlying architecture. In recent times, much focus has been geared toward curating high-quality image-text pairs encompassing various vision-language tasks. Qwen (Yang et al., 2024a), Pixtral (Agrawal et al., 2024) and Molmo (Deitke et al., 2024) are some of the recent multimodal language models focusing on high-quality image-text curated data. Multimodal language models have various real-world applications, such as VQA, and image captioning.\nNote. We acknowledge the emergence of unified transformer-based multimodal models capable of both image generation and multimodal understanding, such as (Xie et al., 2024a; Team, 2024; Dong et al., 2024). However, we exclude these from our discussion due to the absence of mechanistic interpretability studies on them. Besides, another variant of model architecture, which is designed to generate interleaved images and text, such as GILL (Koh et al., 2024), combines an MLLM and a diffusion model into one system. We will classify such a model based on its analyzed components."}, {"title": "4 LLM Interpretability Methods for Multimodal Models", "content": "We first examine mechanistic interpretability methods originally developed for large language models and their adaptability to multimodal models with minimal to moderate modifications. Our focus is on how existing LLM interpretability techniques can provide valuable mechanistic insights into multimodal models.\nSpecifically, we begin discussing diagnostic tools (Linear Probing (Sec. 4.1), Logit Lens (Sec. 4.2)), which passively map what knowledge is encoded in model representations and where it resides across layers. We then introduce causal intervention methods (Causal Tracing and Circuit Analysis (Sec. 4.3)), which actively perturb model states to uncover where the knowledge is stored and how specific predictions emerge in multimodal models. These insights then enable representation-centric approaches (Representation Decomposition (Sec. 4.4)) to mathematically disentangle activations into interpretable components, exposing the building blocks of model knowledge. This structural understanding directly informs behavioral control paradigms: General Task Vectors (Sec. 4.5) leverage explicit task-driven arithmetic to edit model outputs, while Sparse Autoencoders (as their unsupervised counterpart, (Sec. 4.6)) provide machine-discovered feature bases for granular manipulation, bridging analysis to application. Finally, Neuron-level descriptions (Sec. 4.7) anchor these interpretations in empirical reality, validating macroscopic hypotheses through microscopic activation patterns (e.g., concept-specific neurons) and ensuring mechanistic fidelity."}, {"title": "4.1 Linear Probing", "content": "Probing trains lightweight classifiers on supervised\u00b9 probing datasets, typically linear probes, on frozen LLM representations to assess whether they encode linguistic properties such as syntax, semantics, and factual knowledge (Hao et al., 2021; Liu et al., 2024e; Zhang et al., 2024b; Liu et al., 2023b; Beigi et al., 2024). The illustration of Linear Probing is shown in Figure 2 (a). This approach has been extended to multimodal models, introducing new challenges such as disentangling the relative contributions of each modality (i.e., visual or textual). To tackle these challenges, Salin et al. (2022) developed probing methods to specifically assess how Vision-Language models synthesize and merge visual inputs with textual data to enhance comprehension, while Dahlgren Lindstr\u00f6m et al. (2020) investigated the processing of linguistic features within image-caption pairings in visual-semantic embeddings. Unlike in LLMs, where upper layers predominantly encode abstract semantics (Jawahar et al., 2019; Tenney et al., 2019), multimodal probing studies (Tao et al., 2024; Salin et al., 2022) suggest that intermediate layers in multimodal models are more effective at capturing global cross-modal interactions, whereas upper layers often emphasize local details or textual biases. Furthermore, despite the fact that probing applications in LLMs are centered on specific linguistic analyses, the scope of probing in multimodal models extends to more varied aspects. For instance, Dai et al. (2023) investigated object hallucination in vision-language models, analyzing how image encodings affect text generation accuracy and token alignment.\nMain Findings and Gap. The main drawback of linear probing is the requirement of supervised probing data and training a separate classifier for understanding concept encoding in layers. Therefore, scaling it via multimodal probing data curation and training separate classifiers across diverse multimodal models is a challenge."}, {"title": "4.2 Logit Lens", "content": "The Logit Lens is an supervised interpretability method used to understand the inner workings of LLMs by examining the logits value of the output. As is shown in Figure 2 (b), this method conducts a layer-by-layer analysis, tracking logits at each layer (by projecting to the vocabulary space using the unembedding projection matrix) to observe how predictions evolve across the network. By decoding intermediate representations into a distribution over the output vocabulary, it reveals what the network \"thinks\u201d at each stage (Belrose et al., 2023). In the context of multimodal models, studies show that predictions from earlier layers often exhibit greater robustness to misleading inputs compared to final layers (Halawi et al., 2024). Studies also demonstrate that anomalous inputs alter prediction trajectories, making this method a useful tool for anomaly detection (Halawi et al., 2024; Belrose et al., 2023). Additionally, for easy examples\u2014situations where the model can confidently predict outcomes from initial layers\u2014correct answers often emerge in early layers, enabling computational efficiency through adaptive early exiting (Schuster et al., 2022; Xin et al., 2020). Furthermore, the Logit Lens has been extended to analyze multiple inputs. Huo et al. (2024) adapted it to study neuron activations in feedforward network (FFN) layers, identifying neurons specialized for different domains to enhance model training. Further research has integrated contextual embeddings to improve hallucination detection (Phukan et al., 2024; Zhao et al., 2024a). Additionally, the \u201cattention lens\" introduced in (Jiang et al., 2024c) examines how visual information is processed, revealing that hallucinated tokens exhibit weaker attention patterns in critical layers.\nMain Findings and Gap. Beyond multimodal language models, logit-lens can be potentially utilised to mechanistically understand modern models such as unified understanding and generation models such as (Xie et al., 2024a; Team, 2024).\""}, {"title": "4.3 Causal Tracing", "content": "Unlike passive diagnostic tools, Causal Tracing Analysis (Pearl, 2014) is rooted in causal inference that studies the change in a response variable following an active intervention on intermediate variables of interest (mediators). An example of causal tracing applied to transformer-based generative VLM is illustrated in Figure 2 (c). The approach has been widely applied to language models to pinpoint the network components\u2014such as FFN layers\u2014that are responsible for specific tasks (Meng et al., 2022a,b; Pearl, 2001). For instance, Meng et al. (2022a) demonstrated that mid-layer MLPs in LLMs are crucial for factual recall, while Stolfo et al. (2023) identified the important layers for mathematical reasoning. Building on this technique and using a supervised probing dataset, Basu et al. (2023) found that, unlike LLMs, visual concepts (e.g., style, copyrighted objects) are distributed across layers in the noise model for diffusion models, but can be localized within the conditioning text-encoder. Further, Basu et al. (2024b) identified critical cross-attention layers that encode concepts like artistic style and general facts. Recent works have also extended causal tracing to mechanistically understand generative VLMs for VQA tasks (Basu et al., 2024a; Palit et al., 2023; Yu and Ananiadou, 2024c), revealing key layers that guide model decisions in VQA tasks.\nExtending to Circuit Analysis While causal tracing helps to identify individual \u201ccausal\u201d components for a particular task, it does not automatically lead to the extraction of a sub-graph of the underlying computational graph of a model which is \u201ccausal\u201d for a task. In this regard, there has been a range of works in language modeling to extract task-specific circuits (Syed et al., 2023; Wang et al., 2022b; Conmy et al., 2023b). However, extending these methods to obtain task-specific circuits is still an open problem for MMFMs.\nMain Findings and Gap. While causal tracing has been extensively used to analyze factuality and reasoning in LLMs, its application in multimodal models remains relatively limited. Expanding this method to newer, more complex multimodal architectures and diverse tasks remains an important challenge to address."}, {"title": "4.4 Representation Decomposition", "content": "In transformer-based LLMs, as illustrated in Figure 3, the concept of representation decomposition pertains to the analysis of the model's internal mechanisms, specifically dissecting individual transformer layers to core meaningful components, which aims at understanding the inner process of transformers. In unimodal LLMs, research has mainly decomposed the architecture and representation of a model's layer into two principal components: the attention mechanism and the multi-layer perceptron (MLP) layer. Intensive research efforts have focused on analyzing these components to understand their individual contributions to the model's decision-making process. Studies find that while attention should not be directly equated with explanation (Pruthi et al., 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), it provides significant insights into the model's operational behavior and helps in error diagnosis and hypothesis development (Park et al., 2019; Voita et al., 2019; Vig, 2019; Hoover et al., 2020; Vashishth et al., 2019). Furthermore, concurrently, research has shown that Feed-Forward Networks (FFNs) within the Transformer MLP layer, functioning as key-value memories, encode and retrieve factual and semantic knowledge (Geva et al., 2021). Experimental studies have established a direct correlation between modifications in FFN output distributions and subsequent token probabilities, suggesting that the model's output is crafted through cumulative updates from each layer (Geva et al., 2022b). This core property serves as the foundation for identifying language model circuits associated with specific tasks in (Syed et al., 2023; Wang et al., 2022c; Conmy et al., 2023a).\nIn multimodal models, representation decomposition has been instrumental in analyzing modality processing and layer-specific properties. Studies such as (Gandelsman et al., 2024a; Balasubramanian et al., 2024) leverage supervised probing datasets and propose a hierarchical decomposition approach\u2014spanning layers, attention heads, and tokens\u2014to provide granular insights into model behavior.\nLayer-wise decomposition reveals that shallow layers primarily integrate modality-specific inputs into a unified representation, while deeper layers refine task-specific details through denoising (Yin et al., 2024). Tao et al. (2024) further demonstrated that intermediate layers capture broader semantic information, balancing modality-specific details with holistic understanding\u2014crucial for tasks such as visual-language entailment. In diffusion models like Stable Diffusion, Prasad et al. (2023) found that lower U-Net layers drive semantic shifts, while higher layers focus on denoising, progressively refining the latent representations into high-quality outputs. Quantmeyer et al. (2024) utilized causal tracing with representation decomposition to identify CLIP text encoder heads responsible for processing negation and semantic nuances, thereby improving cross-modal alignment. Similarly, Cao et al. (2020) identified attention heads specialized for cross-modal interactions, integrating linguistic and visual cues for high-quality multimodal synthesis. Notably, it shares similarities with causal tracing, which can be applied once a layer has been broken down into distinct components using Representation Decomposition.\nMain Findings and Gap. While CLIP and diffusion models are a great starting point for a case-study using representation decomposition, leveraging the inherent decomposability of transformers can be extended to understanding multimodal language models, and text-to-video models\u2014an important gap that needs to be addressed."}, {"title": "4.5 General Task Vectors", "content": "General Task (or steering) vectors in language models are directional embeddings that, when added to specific layers, enhance model capabilities such as in-context learning and instruction following. To obtain these task vectors, one requires a well-annotated supervised probing dataset. Hendel et al. (2023a) discovered a task vector for compressing task demonstrations, while Zhang et al. (2024a) and Jiang et al. (2024a) leveraged instruction vectors to improve model adherence to user instructions and mitigate catastrophic forgetting. In multimodal models, task vectors facilitate controlled image generation and editing. Baumann et al. (2024) mapped text-embedding vectors to visual concepts for adjustable intensity, while Gandikota et al. (2025) fine-tuned low-rank matrices in UNet to create controllable concept vectors. Cohen et al. (2025) explored multiple task vectors in diffusion models, proposing a prompt-conditioned adaptation method to minimize interference.\nMain Findings and Gap. While language models support both fine-tuning and zero-shot steering, multimodal models largely rely on fine-tuning. Advancing zero-shot steering for multimodal models remains a crucial research direction."}, {"title": "4.6 Sparse Autoencoders: A Special Class of Unsupervised Task Vectors", "content": "Sparse Autoencoders (SAEs, Yun et al. (2021)) offer an unsupervised approach to discovering conceptual representations in neural networks post-training. SAEs learn a dictionary of concepts such that any representation can be expressed as a linear combination of a sparse subset of these concepts. As illustrated in Figure 3 (b), an SAE is typically a two-layer MLP of the form $SAE(x) = Dec(Act(Enc(x)))$ where x is the input feature. The encoder $(Enc)$ and the decoder $(Dec)$ layers are simple linear layers and the activation function $(Act)$ is a design choice and can be a simple ReLU (Agarap, 2019), Top K (Gao et al., 2024), JumpReLU (Rajamanoharan et al., 2024), and so on. The SAE is trained to reconstruct its own input, with the constraint that the activations should be sparse. Once trained, the neurons in the activation layer are assigned interpretations based on the highest activating input samples for the specific neuron in question. This results in a concept dictionary where concepts are mapped to directions (i.e., vectors) in representation space. These vectors can be added to the residual stream of the model to potentially control various facets such as the safety and intensity of various attributes in image generation models. The SAE with an autoencoder architecture is trained to reconstruct its input while enforcing sparse activations. Once trained, neurons are interpreted based on their highest-activating inputs, forming a concept dictionary that maps concepts to vectors in representation space. These vectors can then be added to the model's residual stream to control attributes like safety and intensity in image generation. Due to their unsupervised nature, which minimizes the need for annotated examples for probing, SAEs have been applied extensively to LLMs to identify human interpretable directions for various concepts (e.g., refusal) in representation space (Cunningham et al., 2023). These directions can then be used to steer the language model (Marks et al., 2024) without the need of fine-tuning it. More recently, SAEs have been extended to vision-language models like CLIP (Daujotas, 2024; Rao et al., 2024; Lim et al., 2024) and audio transcription models like Whisper (Sadov, 2024). Despite their promise, SAEs face challenges such as feature absorption and splitting (Chanin et al., 2024), lack of robust evaluation metrics (Makelov et al., 2024) and underperformance compared to supervised methods for model control.\nMain Findings and Gap. The effectiveness of SAEs as a control mechanism for multimodal models is still in its early stages and requires validation across a range of multimodal models, including the latest diffusion models and MLLMs."}, {"title": "4.7 Neuron-Level Descriptions", "content": "Neuron-level analysis methods aim to identify specific neurons that contribute to model predictions (Sajjad et al., 2022). The illustration is shown in Figure 3 (c). In this section, we divide these methods into two main categories: gradient-based attribution, and activation-based analysis.\nThere are different definitions of neurons in deep neural networks. We define x as the input embeddings, and $h_i$ as the hidden states of the i-th layer's output. A model layer multiplies the hidden states with parameter $M_i$ followed by an activation function a = $f(xM_i)$. Some studies define the activation $a_j$, which is the j-th element of a as the neuron (Dai et al., 2021). While other works (Dalvi et al., 2019; Durrani et al., 2020; Antverg and Belinkov, 2021) define the dimensions in output representation as a neuron. For consistency, in our survey, we follow the most widely used definition to define an element $m_j$ of a layer's parameter M as the neuron.\nGradient-based attribution methods analyze how neuron values influence model outputs by perturbing neuron activations and accumulating weight contributions based on corresponding gradients (Dai et al., 2021). In unimodal settings, Dai et al. (2021) detected fact-related neurons concentrated in the top layers of a pretrained language model, such as BERT (Devlin, 2018), while Wang et al. (2022a) identified neurons for encoding hierarchical concepts in a CNN-based vision model, such as VGG19 (Simonyan and Zisserman, 2015). Extending this approach to multimodal settings, Schwettmann et al. (2023) identified \u201cmultimodal neurons", "dead": "eurons that are never activated, revealing the sparsity of LLMs. In multimodal contexts, Goh et al. (2021) detected neurons encoding distinct visual features in non-generative models, while in generative VLMs, researchers have identified domain-specific neurons (Huo et al., 2024) and modality-specific neurons (Huang et al., 2024c).\nIn diffusion models, Hintersdorf et al. (2024) identified memorization neurons by analyzing their out-of-distribution activations.\nPrediction Probability Changes methods usually change the neuron output value, and analyze its influence on the final prediction. Yu and Ananiadou (2024b) quantifies the importance level of a neuron by calculating the difference of the log of the probabilities by giving and without giving the neuron value. In this way, this paper finds that both attention and FFN layer store knowledge. Besides, all important neurons directly contributing to knowledge prediction are in deep layers. Yu and Ananiadou (2024a) utilizes the same method to find that features are enhanced in shallow FFN layers and neurons in deep layers are used to enhance prediction. Following a similar strategy, Yu and Ananiadou (2024d) finds important attention heads for handling VQA tasks.\nAttribution Method is to project the internal hidden representation into output space to analyze each neuron's contribution to the final prediction (Geva et al., 2022a). In the multimodal domain, Pan et al. (2023) projects the activation of one neuron into output space to quantify the importance of one neuron to the final prediction and identify multimodal neurons. Fang et al. (2025) utilizes this method to find the semantic knowledge neurons and some interesting properties such as cross-modal invariance and semantic sensitivity.\nOther Method covers many different types of neuron-level analysis methods. For example, instead of directly analyzing the first-order effect, which is the logits of each neuron, Gandelsman et al. (2024b) analyzes the accumulation of information of a neuron after the attention head. A new method to analyze information flow. Focus on the contribution of neurons to the output representation.\nMain Findings and Gap. Neuron-level analysis adapts well to multimodal settings, but deeper neuron interactions remain underexplored, such as activation shifts in generative VLMs when adding visual input to identical text."}, {"title": "4.8 Summary", "content": "Overall, we find that the core principles of popular LLM-based mechanistic interpretability methods can be extended to multimodal models without complex modification. However, extracting meaningful mechanistic insights from these models often requires carefully tailored adaptations.\nMain Findings and Gap. The effectiveness of SAEs as a control mechanism for multimodal models is still in its early stages and requires validation across a range of multimodal models, including the latest diffusion models and MLLMs."}, {"title": "5 Interpretability Methods Specific to Multimodal Models", "content": "Many recent research studies also propose multimodal-specific inner mechanism interpretation analysis methods. Different from LLM-based methods introduced in Sec. 4, those methods are designed and applied only for multimodal foundation models. These methods include techniques for annotating embeddings or neurons in human-understandable language (Sec. 5.1 and Sec. 5.2, leveraging unique multimodal architectural components like cross-attention layers for deeper insights (Sec. 5.3), developing novel data attribution methods tailored to multimodal models, such as text-to-image diffusion models (Sec. 5.4) and specific visualization methods (Sec. 5.5)."}, {"title": "5.1 Text-Explanations of Internal Embeddings", "content": "In Sec. 4.4, we leverage the representation decomposition property of transformers to identify key components in token representations. However, interpreting these components in human-understandable terms remains a challenge. For CLIP models, Gandelsman et al. (2024a) proposed TextSpan, which assigns textual descriptions to model components (e.g., attention heads) by identifying a text embedding that explains most of the variance in their outputs. The dataset for this task is supervised in nature. Expanding on this, Balasubramanian et al. (2024) introduced a scoring function to rank relevant textual descriptions across components. Concurrently, SpLiCE (Bhalla et al., 2024) mapped CLIP visual embeddings to sparse, interpretable concept combinations. Additionally, Parekh et al. (2024) employed dictionary learning to show that predefined concepts are semantically grounded in both vision and language. Together, these methods enhance the interpretability of internal embeddings in multimodal models by providing textual explanations. All the text-explanations of internal embedding papers aim to interpret where knowledge is stored in the model.\nMain Findings and Gap. Current text-based explanations of internal embeddings primarily focus on simple concepts (e.g., color, location). It remains unclear whether these methods can effectively map visual embeddings to more abstract concepts, such as physical laws. Moreover, their applicability beyond CLIP, particularly in text-to-image and video generation models, remains largely underexplored."}, {"title": "5.2 Network Dissection", "content": "Network Dissection (ND) (Bau et al., 2017), pioneered automated neuron interpretability in multimodal networks by establishing connections between individual neurons and human-understandable concepts. Different from the internal embedding methods (Sec. 5.1), ND compares neuron activations with groud-truth concept annotations in images. When a neuron's activation pattern consistently matches with a specific concept over a certain threshold, that concept is assigned as the neuron's interpretation (Oikarinen and Weng, 2023; Kalibhat et al., 2023). Moving beyond simple concept matching, MILAN (Hernandez et al., 2021) introduced a generative approach that produces natural language descriptions of neuron behavior based on highly activating images. DnD (Bai et al., 2024) then extend this work by first leveraging a generative VLM to describe highly activating images for each neuron and semantically combine these descriptions using an LLM.\nMain Findings and Gap. The generalization of this method are constrained by their underlying multimodal architectures, e.g., CLIP. Moreover, while ND has proven effective for CNN"}]}