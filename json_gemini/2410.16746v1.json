{"title": "SpikMamba: When SNN meets Mamba in Event-based Human Action Recognition", "authors": ["Jiaqi Chen", "Yan Yang", "Shizhuo Deng", "Da Teng", "Liyuan Pan"], "abstract": "Human action recognition (HAR) plays a key role in various applications such as video analysis, surveillance, autonomous driving, robotics, and healthcare. Most HAR algorithms are developed from RGB images, which capture detailed visual information. However, these algorithms raise concerns in privacy-sensitive environments due to the recording of identifiable features. Event cameras offer a promising solution by capturing scene brightness changes sparsely at the pixel level, without capturing full images. Moreover, event cameras have high dynamic ranges that can effectively handle scenarios with complex lighting conditions, such as low light or high contrast environments. However, using event cameras introduces challenges in modeling the spatially sparse and high temporal resolution event data for HAR. To address these issues, we propose the SpikMamba framework, which combines the energy efficiency of spiking neural networks and the long sequence modeling capability of Mamba to efficiently capture global features from spatially sparse and high a temporal resolution event data. Additionally, to improve the locality of modeling, a spiking window-based linear attention mechanism is used. Extensive experiments show that SpikMamba achieves remarkable recognition performance, surpassing the previous state-of-the-art by 1.45%, 7.22%, 0.15%, and 3.92% on the PAF, HARDVS, DVS128, and E-FAction datasets, respectively. The code is available at https://github.com/Typistchen/SpikMamba.", "sections": [{"title": "1 Introduction", "content": "Human action recognition (HAR) aims to classify human activities and movements [24, 28]. It has been applied to various domains such as robot navigation [35, 51], healthcare [5], and abnormal human behaviour recognition [26, 41]. Most HAR methods are developed for RGB images. Although they achieve high performance, human privacy information is inevitably recorded, e.g., facial features, which presents challenges and concerns for deploying under privacy-sensitive environments [45, 48]. Therefore, we pose a question: can we design a framework that effectively protects user privacy and accurately recognizes human actions?\nEvent cameras are novel sensors inspired by the working mechanism of the human retina [1, 11, 29]. Unlike traditional RGB cameras that record all pixel intensities, event cameras asynchronously and sparsely detect changes in light intensity with microsecond-level temporal resolution and a high dynamic range [39, 40, 63, 64]. This means that privacy-related features are usually discarded, e.g., facial textures. While using event camera data for human action recognition (HAR) can address user privacy concerns, it introduces new challenges for HAR frameworks: 1) The event stream is spatially sparse, requiring the model to associate events from different times to capture meaningful features. 2) The event stream has a high temporal resolution, resulting in an excessive number of events that require efficient processing. In this paper, we aim to design an event-based HAR framework that addresses these challenges for HAR with high performance.\nExisting event-based HAR methods are developed based on artificial neural networks (ANN) [3, 11, 42, 61] or spiking neural networks (SNN) [2, 4, 7, 18, 37, 56]. To handle the spatial sparsity of event camera data, ANN-based methods (Fig. 1(a)) often use attention mechanisms, convolutional neural network, or graph convolutional network to enhance feature extraction from the sparse event data. For computational efficiency, these methods downsample the event data over the temporal dimension, e.g. [3] uses event data of 48ms duration for every 0.35 seconds. However, the event data downsampling loses fine-grained information about human actions, which could enhance model performance.\nBy design, SNNs (Fig. 1(b)) effectively handle the spatial sparsity of event camera data through event-driven computation on temporal dynamics, integrating event features over time to form a coherent understanding of the scene. However, the computations of existing SNN-based methods are usually restricted to local temporal contexts for computational efficiency, and they lose the global temporal dependency of event data for accurately recognizing human actions. While methods such as attention mechanisms can be applied to dynamically capture global temporal dependencies, doing so often reduces the efficiency of SNNs.\nLuckily, recent advancements in state space models [10, 14, 36, 49], such as Mamba [13], suggests an efficient solution for dynamically modeling data with a high temporal resolution, offering an alternative to attention mechanisms. Motivated by the success of Mamba and SNNs, we propose combining these approaches to efficiently and accurately recognize human actions using event data. To this end, we are the first to introduce the SpikMamba which has the two key designs for event-based HAR.\nTo address the spatial sparsity and high temporal resolution of event camera data, we model the global and local temporal dependencies of the event data (Fig. 1(c)). First, we construct a Mamba block in spike form (only 0s and 1s) to globally model the interdependencies among event data. Second, to enhance the locality of spike features, we apply a spike-based linear attention mechanism to the event data across different temporal windows. To validate the effectiveness of our framework, we experiment with common event-based HAR datasets, demonstrating that our method surpasses previous state-of-the-art approaches.\nIn summary, our main contributions are:\n\u2022 We propose a SpikMamba framework to effectively and accurately recognize human actions using event data.\n\u2022 We explore Mamba and window-based linear attention spike-based mechanisms for modeling global and local temporal dependencies of the event data.\n\u2022 We experiment with common event-based HAR data to demonstrate our superior performance compared to existing state-of-the-art algorithms."}, {"title": "2 Related Work", "content": "In this section, we briefly introduce ANN for Event-based HAR, SNN for Event-based HAR, and the state space model.\nANN for Event-based HAR. ANN-based methods typically use CNNs [11, 42], ViTs [46, 62], and GCNs [3, 61] to extract sparse event data feature. EV-ACT [11] employs a CNN with spatial-temporal attention for action recognition, while [42] adapts event data to CNNS using event memory surfaces. ViTs [3, 61] employ patch-based and voxel transformer encoders for efficient spatio-temporal feature extraction, and GCNs [62] manage the sparse, asynchronous structure. However, most ANN-based HAR methods overlook spatial sparsity and high temporal resolution. Our SpikMamba network tackles both issues effectively.\nSNN for Event-based HAR. SNNs for Event-based HAR. Spiking Neural Networks (SNNs) [2, 4, 7, 18, 37, 56] differ from traditional deep learning models, which use continuous decimal values, while SNNs utilize discrete spike sequences. This makes SNNs well-suited for processing temporal data, leading to their use in event-based HAR [1, 12, 25, 29, 50]. However, SNN-based HAR methods often lose fine-grained action details due to event data downsampling. In contrast, our SpikMamba effectively combines Mamba and window-based linear attention mechanisms in spike form to model global and local temporal dependencies.\nState Space Model. The state-space model [14] (S4) serves as an alternative to CNNs and Transformers for long-range dependency modeling. Mamba has been applied to event data [43, 54], with [43] integrating a spiking front-end for temporal processing and [54] using a linear complexity state-space model for tracking. Our research combines Mamba's time-series strengths with SNNs' efficiency in sparse event data, proposing the SpikMamba network."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Preliminaries", "content": "SNN. The core of SNN is spiking neurons that receive the input X[t] and accumulate membrane potentials H[t] at each time step t.\nWhen the membrane potential X[t] exceeds a threshold $V_{th}$, a spike S[t] is triggered, and the membrane potential is reset. In our work, we employ the Leaky Integrate-and-Fire (LIF) spiking neurons [59]. The mathematical representations of LIF are summarized as follows:\n$\nH[t] = V[t - 1] + \\frac{1}{\\tau}(X[t] \u2013 (V[t - 1] - V_{reset}),\n$\n$S[t] = \\Theta(H[t] - V_{th}),\n$\n$V[t] = H[t](1-S[t]) + V_{reset}S[t],\n$\nwhere $\\tau$ is the membrane time constant, $\\Theta(\u00b7)$ is the Heaviside step function that fires a spike (outputs 1) if $H[t] \u2013 V_{th} \u2265 0$, and V[t] is the membrane potential which resets to $V_{reset}$ if a spike is fired.\nMamba. Mamba is inspired by continuous system that maps a sequence X to Y using a hidden time state Z. At each time step t, the mapping is calculated using the state-space equations:\n$Z' [t] = AZ[t] + BX[t],\nY[t] = CZ[t],\n$\nwhere Z' [t] is a placeholder variable, A is the system evolution matrix, and B and C are the projection matrices. In Mamba, the state-space equations are discretized, by using the zero-order hold (ZOH) method to transform the parameters continuous system A and B into their discrete versions $\\overline{A}$ and $\\overline{B}$ with a timescale parameter \u2206 to control the step size of the discretization process,\n$\\overline{A} = exp(\\Delta A),\n$\n$\\overline{B} = (\\Delta A)^{-1}(exp(\\Delta A) \u2013 I) \u00b7 \\Delta B,\n$\nwhere I is an identity matrix, and $\u25e6$ represents elementwise multiplication. Mamba also makes the parameters B, C, and \u2206 dependent on input X[t] to calculate A and B. Additionally, it uses a global convolution shared across different time steps to compute the output Y[t]. We refer the reader to [13] for more details."}, {"title": "3.2 SpikMamba", "content": "We use the representation from [66] that transforms the event data into three channel event images $X \\in R^{3\u00d7T\u00d7H\u00d7W}$, where T, H, and W are the temporal dimension, height, and width of the event images. We predict the action class of the event images X with our SpikMamba (Fig. 2), which has two main modules: i) Spiking 3D patch embedding. It splits event frames X into patches to calculate patch embeddings P with SNN. ii) SpikMamba block. It encapsulates window-based linear attention and Mamba into SNN to model local and global temporal dependency of event data for HAR from the patch embeddings. Last, the embeddings produced by the SpikMamba blocks are pooled and then projected to the action class with the use of a final linear layer for classification.\nSpiking 3D Patch Embedding. As shown in Fig. 2, we first divide the event frames X into patches and then project them into spike-form features. Similar to ViT, we use a convolution layer with shared parameters across patches to calculate the patch embedding P,\n$P = SL_{patch} (BN(Conv3d(X))) + PE,\n$\nwhere $SL_{patch}(\u00b7)$, BN(\u00b7), Conv3d(\u00b7) are the spike layer, batch normalization layer, and convolution 3D layer with a stride of 1 \u00d78\u00d78 and kernel size of 1 \u00d7 8 \u00d7 8, and PE is the positional embedding that introduces inductive bias on the spatial and temporal dimensions to the patch embeddings. Though more than one 3D convolution layer can be used to progressively compute the patch embedding, our experiments show that a single Conv3D layer is sufficient to accurately recognize human actions and is the most efficient option.\nSpikMamba Block. The patch embeddings P of event frames X are sent to N SpikMamba blocks. In a SpikMamba block, it includes a window-based spike linear attention layer SpikeSLA(\u00b7), a spike Mamba layer SpikMamba(.), and a feedforward network FFN(\u00b7). For simplicity, we describe the calculations of a single Mamba block without differentiating between the Mamba block indices:\n$P_{local} = SpikeSLA(P),\nP_{global} = SpikMamba(P_{local}) + P_{local},\nP_{out} = FFN(P_{global}) + P_{global},\n$\nwhere $P_{local}, P_{global}$, and $P_{out}$ are the output patch embeddings generated from the respective layers.\nIn the window-based spike linear attention layer, we reshape the patch embedding P to divide it into different windows, and project the patch embedding into spike form query Q, key K, and value V using linear layers. We use spike-form query and key, while the continuous value is used to improve feature representation,\n$Q = SL_q (Linear_q (Reshape_{window} (P))),\nK = SL_k (Linear_k (Reshape_{window} (P))),\nV = Linear_v (Reshape_{window} (P)),\n$\nwhere $SL_q(\u00b7)$ and $SL_k(\u00b7)$ are spike layers responsible for processing query and key, $Linear_q(\u00b7), Linear_k(\u00b7)$, and $Linear_v(\u00b7)$ are linear layers, and $Reshape_{window}()$ is the window reshape layer. Then, we calculate the embedding Patt using a linear attention layer $Linear_{Att}(\u00b7, \u00b7, \u00b7)$ from [16] and a spike layer $SL_{att}(\u00b7)$,\n$P_{att} = SL_{att} (Linear_{Att}(Q, K, V)) .\n$\nThe $P_{att}$ is projected to the patch embedding $P_{local}$ with a linear output layer $Linear_{out}()$, reshaped back with $Reverse_{window}()$, and undergoes a Hadamard product with P,\n$P_{local} Reverse_{window} (Linear_{out} (P_{att})) \u25e6 P.\n$\nTo model the temporal global dependency in $P_{local}$, our spike Mamba layer SpikMamba() uses a linear layer $Linear_m()$ and a 1D convolution layer $Conv1D_m()$ with spike layers $SL_{m1}()$ and $SL_{m2}()$ to expand the dimension of $P_{local}$,\n$P_{global} = SL_{m2}(Conv1D_m (SL_{m1} (Linear_m (P_{local})))) ,$\nand predict the evolution matrices $\\overline{A}$, B, and the timescale parameter \u2206 of the state-space equations in Mamba with $Linear_B(\u00b7)$, $Linear_C (\u00b7)$, and $Linear_{\\Delta} (\u00b7)$,\n$B = Linear_B (P_{global}),\nC = Linear_C (P_{global}),\n$\\Delta = log(1 + exp(Linear_{\\Delta} (P_{global}) + bias_{\\Delta}),\n$\nwhere $bias_{\\Delta}$ is a trainable bias. With trainable system evolution parameter A, A and B, $\\overline{A}$ and $\\overline{B}$ are discretized into their respective forms $\\overline{A}$ and $\\overline{B}$ [13], the state space equation $SSM(\u00b7, \u00b7, \u00b7, \u00b7)$ is calculated with a spike layer $SL_{ssm} (\u00b7)$, and undergoes a Hadamard product with $P_{local}$,\n$P_{global} = SL_{ssm}(SSM(\\overline{A}, \\overline{B}, C, P_{global}))\u25e6P_{local},\n$\nand the output $P_{global}$ is then sent to the feedforward layer FFN(\u00b7).\nPrediction. We pool the patch embeddings $P_{out}$ from the Spik-Mamba block with a global average pooling GAP(.), and predict the human action y using a linear layer $Linear_{predict}(\u00b7)$,\n$y = Linear_{predict} (GAP(P_{out})) .\n$\nThe prediction y is optimized with the ground truth human action class and cross-entropy loss during training."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset and Implementation details", "content": "Dataset. We use four datasets to evaluate the performance of our model, SpikMamba. These datasets include PAF [38], HARDVS [55], DVSGesture [1], and E-FAction [65]. Specifically, 1) PAF [38] is a human action dataset collected using the DVSIS346 event camera, containing 10 categories of actions with 45 samples per category. 2) HARDVS [55], a recently released dataset, boasts the largest number of action categories and samples, totaling 300 categories and 107,646 recordings. 3) DVSGesture [1] captures hand and arm movements, containing 11 action categories with a resolution of 128\u00d7128. 4) E-FAction [65] dataset has 128 human action classes, totaling 1024 recordings with a resolution of 346\u00d7260.\nImplementation details. We employ the AFE representation [66] to compress the event stream into event frames. Our SpikMamba model has a layer of spiking 3D patch embedding and two layers of SpikMamba blocks for feature extraction. We use a hidden state dimension of 256, and expand the state dimension for state space equations to 256 with the $Linear_m()$. The state-space equation operates in a dimension of 2048. The hidden dimension of the feedforward networks is 1024. In training, we use the Adam optimizer with a weight decay of 2e-4. The learning rate is initialized to 1e\u22125, and we adopt the CosineAnnealingLR, [33] with a minimum learning rate of 1e-6. Our model was trained on two NVIDIA 4090 GPUs for 100 epochs with batch size 32. Our code will be made available online for future studies and comparisons."}, {"title": "4.2 Comparison with SOTA Methods", "content": "Tab. 1 shows the performance of our proposed SpikMamba on the PAF, HARDVS, DVSGesture, and E-FAction datasets for event-based action recognition tasks. We compare it with state-of-the-art methods on all these datasets. The method with the highest accuracy is highlighted in bold in the table.\nOur findings are as follows: 1) Our method has the best accuracy of 96.28%, 97.32%, 99.01%, and 71.02% across the four datasets. 2) Compared to ANN-based ExACT, which has the second highest accuracy, our method shows accuracy improvements of 1.45%, 7.22%, 0.33%, and 3.09%. 3) On the HARDVS dataset, which has the largest number of complex and diverse human actions, our SpikMamba and ExACT show significant improvements over other methods, showing accuracy increases of more than 35%. Additionally, our SpikMamba further improves ExACT's accuracy by 7.22%. 4) On the DVSGesture dataset, the highest accuracy of the state-of-the-art method is already 98.86%, but our SpikMamba increases it to 99.01%. 5) Compared to the SNNs with the second highest accuracy across the four datasets, our SpikMamba improves the accuracy by 6.14% and 2.81% on PAF and DVSGesture dataset, which is the first SNN method that is better than the ANN method."}, {"title": "4.3 Ablation Studies", "content": "We ablate SpikeSLA and SpikMamba layers in Tab. 2. We found: 1) When using only the SpikeSLA layers of our model, the network significantly loses the ability to capture long-term/global information from high temporal resolution event data, and has 97.12%, 95.33%, 98.17%, and 70.66% accuracy. 2) When removing the SpikeSLA layers from our model, we observe a significant accuracy drop. The average decrease is 19.73%. Given that the action durations recorded in the four datasets primarily range from 5 to 7 seconds, key frames of the action are likely short-term that constitute the main features of the action. Consequently, when the SpikeSLA is removed from our model, the network cannot effectively enhance the feature locality for HAR. 3) The model with SpikeSLA and SpikMamba layers efficiently and accurately models the global and local temporal dependencies of the event data, and has the best performance."}, {"title": "4.4 Discussion", "content": "Attention Map. In Fig. 4, we illustrate attention maps from the final SpikMamba block at the last time step. For clarity, we provide attention maps on RGB images generated by SpikMamba. High attention regions are marked in white, while low attention regions are marked in black. Our SpikMamba effectively captures image regions with human actions.\nComputational Efficiency. We compare SpikMamba with the state-of-the-art ANN and SNN methods that are ExACT and EvT on computational efficiency in Tab. 3. Our method has 0.18M parameters, which is 1.95M and 0.30M less than ExACT and EvT. The FLOPs of SpikMamba, ExACT, and EvT are 0.12, 1.1, and 0.2 GFLOPs. Our SpikMamba combines SNN and Mamba to efficiently capture global dependencies in event data and uses a spiking window-based linear attention mechanism to model the event data local dependency, striking a balance between computational efficiency and performance in HAR. Our method has the fewest parameters and FLOPs, while also achieving better HAR performance than the best state-of-the-art ANN and SNN methods.\nANN and SNN. To explore the performance of SpikMamba, we removed the SNN layers module. It creates an ANN model based on window-based linear attention and Mamba. The results for four datasets are 94.53%, 92.47%, 98.01%, and 67.77%. Compared to SpikMamba, the ANN method shows a decrease in performance across all four datasets, and the average is 2.71%. It is evident that SNN-based Mamba and linear attention are more suitable for event data. We believe this is because of the alignment between the sparsity of SNNs and the sparsity of event data, enabling SNN-based Mamba and linear attention to effectively and accurately model the global and local dependencies of the event data for HAR."}, {"title": "5 Conclusion", "content": "In this paper, we propose SpikMamba for event data-based Human Activity Recognition (HAR). Using event data for HAR presents challenges in effectively capturing meaningful features from spatially sparse and high temporal resolution event data. By leveraging the energy efficiency of Spiking Neural Networks (SNN) and the long sequence modeling capabilities of Mamba, SpikMamba effectively captures global dependencies from sparse and high temporal resolution event streams. Additionally, a spiking window-based linear attention mechanism is proposed to enhance the locality of event data modeling for HAR. Experiments on common event-based HAR datasets demonstrate our superior performance compared to existing state-of-the-art ANN and SNN methods."}]}