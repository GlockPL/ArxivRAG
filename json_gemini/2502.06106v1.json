{"title": "Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks", "authors": ["Yueyan Li", "Caixia Yuan", "Xiaojie Wang"], "abstract": "The study of mechanistic interpretability aims to reverse-engineer a model to explain its behaviors. While recent studies have focused on the static mechanism of a certain behavior, the training dynamics inside a model remain to be explored. In this work, we develop an interpretable method for fine-tuning and reveal the mechanism behind learning. We first propose the concept of node redundancy as an extension of intrinsic dimension and explain the idea behind circuit discovery from a fresh view. Based on the theory, we propose circuit-tuning, a two-stage algorithm that iteratively performs circuit discovery to mask out irrelevant edges and updates the remaining parameters responsible for a specific task. Experiments show that our method not only improves performance on a wide range of tasks but is also scalable while preserving general capabilities. We visualize and analyze the circuits before, during, and after fine-tuning, providing new insights into the self-organization mechanism of a neural network in the learning process.", "sections": [{"title": "1. Introduction", "content": "Benefiting from the high efficiency in computation and the scalable architecture of Transformer (Vaswani, 2017), large language models (LLMs) have demonstrated outstanding performance in a wide range of tasks. For real-world applications, fine-tuning aims to adapt a model to the downstream tasks. With the concept of intrinsic dimension (Li et al., 2018; Aghajanyan et al., 2020) that the parameters in a model are often redundant for solving a specific problem, parameter-efficient fine-tuning is proposed and has reduced computation to a great extent (Han et al., 2024). However, these techniques lack a mechanism fully clear to humans, and thus are less stable and may lead to catastrophic forgetting when applied to a new task (Wang et al., 2024). Besides, with the rapid development of LLMs, AI alignment has become an important issue in machine learning, highlighting the need for interpretability as well (Ji et al., 2023).\nTo discover the underlying mechanisms inside a model, (Elhage et al., 2021) provided a mathematical framework for transformer circuits, showing the potential of reverse-engineering a model under the guidance of mechanistic interpretability (Olah et al., 2020). Recent studies in this field have tried to isolate the circuit responsible for a single behavior (Wang et al., 2022), extract features using sparse autoencoders (Bricken et al., 2023), or apply a steering vector (Turner et al., 2023) to modify the model behaviors. While the mechanism of an existing model behavior is explored, few studies have paid attention to the mechanism in learning dynamics. Though several efforts have been made from various aspects (Olsson et al., 2022; Nanda et al., 2023), many issues are yet to be further explored in fine-tuning, e.g., how a new capability is acquired, how the model components interact and reorganize themselves, or how to intervene in the training process.\nIn this work, we develop an interpretable method for fine-tuning and reveal the mechanism behind learning. Our contributions are as follows:\n\u2022 We formulate different kinds of redundancy and propose the concept of node redundancy as an extension of the intrinsic dimension to the node level. Based on the theory, we provide an explanation for circuit discovery from the view of redundancy detection.\n\u2022 We propose circuit-tuning, an algorithm that iteratively finds out the components necessary for a task and performs fine-tuning in a mechanistic way. Our method not only performs well but is also scalable to various models and tasks while preserving general capabilities.\n\u2022 We provide a comprehensive analysis on training dynamics based on circuit-tuning and present some amazing findings for the self-organization inside a model, providing new insights and improvements for both fine-tuning and interpretability."}, {"title": "2. Background and Preliminaries", "content": ""}, {"title": "2.1. Features and Representations", "content": "Definition 2.1. (Feature Vector) Given a representation space VD with a set of bases E = {e1, e2, ..., ep} where each basis ei corresponds to a neuron, the feature vector vf of feature f is a direction in VD, which is:\nvf = $\\sum_{i=1}^{m} c_i e_i$ = $c_1 e_1 + c_2 e_2 + ... + c_m e_m$\nwhere the coefficient ci satisfies |ci| \u2264 1 and $\\sum_{i=1}^{m} c_i^2 = 1$.\nRemark 2.2. The idea of Definition 2.1 follows the linear representation hypothesis that features are represented as vectors in a linear space, the evidence of which has been widely discussed in (Mikolov et al., 2013; Bricken et al., 2023; Park et al., 2024).\nAssumption 2.3. (Superposition) Given a neural network and a representation space VD. If all features represented in o is F, then we have D < |F|, where |F| is the number of elements in F.\nRemark 2.4. The idea comes from the superposition hypothesis discussed in (Olah et al., 2020; Elhage et al., 2022) which explains the phenomenon of polysemanticity that one neuron in a neural network often responds to multiple unrelated inputs. Since there are much more features than the dimensions in VD, a feature f may not be aligned with a single basis. In fact, it is akin to the strategy of population coding (Pouget et al., 2000) in human brains that information is encoded by clusters of cells.\nConsidering the settings of Definition 2.1 and Assumption 2.3, we can have the conclusion: Given a task T and an input x with its features Fx = {f1, f2, ..., ft} \u2286 Fr. Suppose the feature vectors corresponding to the features in x are Vx = {Vf1, Vf2, ..., Vft } and the activations of Fx are Ax = {af\u0131, a f2, ..., a f\u2081 }, then the magnitude ai of e\u00a1 in the representation space VD can be written as:\na = $(\\sum_{i=1}^{t} a_{f_i} V_{f_i}) e_i $      (1)\nThis means the magnitude of dimension i in VD often originates from more than one features."}, {"title": "2.2. Computational Graph", "content": "If we view a model as a directed acyclic graph (DAG), then the nodes are terms in its forward pass (neurons, attention heads, etc.) and the edges are the interactions between the nodes. Note that the definitions of the node and edge do not necessarily follow the structure of the model. The shape of a node depends on the level of granularity when we inspect a model, and an edge can be a virtual connection between nodes far apart from each other."}, {"title": "2.3. Circuit Discovery", "content": "A circuit is a subgraph in a computational graph that is responsible for a certain behavior. Recent studies generally use causal intervention for circuit discovery. (Meng et al., 2023) proposed activation patching to identify activations in a model relevant to the output, while (Nanda, 2023) proposed attribution patching to accelerate it. (Wang et al., 2022; Conmy et al., 2023) focused on edges and proposed path patching. Others optimized this technique from various aspects (Syed et al., 2023; Kram\u00e1r et al., 2024)."}, {"title": "2.4. Intrinsic Dimension", "content": "(Li et al., 2018) first proposed the concept of intrinsic dimension to describe the minimum number of dimensions needed for solving a specific problem. (Aghajanyan et al., 2020) provided an analysis into fine-tuning with this concept and proved the feasibility of effective fine-tuning. Inspired by previous works, (Hu et al., 2021) proposed LoRA as a parameter-efficient fine-tuning method, which is of great significance for fine-tuning large language models."}, {"title": "3. The Theory of Node Redundancy", "content": "In this section, we introduce the concept of node redundancy as an extension of the intrinsic dimension, together with a method for detecting it, which can also be used to explain the idea behind circuit discovery from a new perspective.\nSince the intrinsic dimension describes the redundancy in parameters, our goal is also to find out the redundant parameters in a model. If we inspect a representation H = (h1,h2, ...,hm)T \u2208 VD(D = m), then we can denote the parameter matrix to examine as Wpre \u2208 Rmxn, which maps an input from Rn to Rm."}, {"title": "3.1. Representation Redundancy", "content": "Assumption 3.1. (Feature Redundancy) Suppose all features represented in a neural network is F. Given a task T with X = {X1,X2, ..., xt} ~ Dr which consists of t samples that follows a specific data distribution Dr, the features for representing X is Fr \u2286 F.\nRemark 3.2. Assumption 3.1 means that when conditioned on a task T, the number of features needed is smaller than that of the elements in the universal set F. The assumption follows the concept of feature sparsity from (Elhage et al., 2022) that many features do not frequently appear.\nDefinition 3.3. (Semantic Preservation) Consider a feature f with its feature vector vf = $\\sum_{i=1}^{m} c_i e_i$. If we set the coefficients in vf which correspond to a set of bases Er to zero, then we can get a new vector V'f. If the cosine similarity cos(Vf, V'f) >= \u03b7 (\u03b7 \u2208 (0,1]), then we say vf has preserved the semantic information in f."}, {"title": "Remark 3.4.", "content": "It is a nice property akin to the robustness of population encoding to noise. Since information is encoded across many cells, a perturbation on a few cells will not destroy the representation (Pouget et al., 2000)."}, {"title": "Definition 3.5.", "content": "(Dimension Redundancy) Given a task T with a number of features Fr. Let's consider a single basis ei in the representation space Vm. For any feature f in Fr, we set the coefficient ci in vf to zero and get the deformed feature vector vf. If e\u00a1 satisfies:\nmin[cos(Vf\u2081 , V\u2081\u2081), ..., cos(Vf|Fr| , V'f|Fr|)] > \u03b7\nthen e\u00a1 is a redundant dimension for representing features in task T. Note that \u03b7 \u2208 (0, 1] serves as the lower bound for judging whether V'f preserves the information in f."}, {"title": "Proposition 3.6.", "content": "Consider the setup of Definition 3.5. If ei satisfies:\nmax(|Vf\u2081 ei|, |Vf2 ei, ..., Vf |Fr|ei) < \u03be\nwhere \u03be = $\\sqrt{1 - \u03b7^2}$, then we say e\u00a1 is redundant for representing features in task T."}, {"title": "Remark 3.7.", "content": "This is obvious from Definition 3.5. Note that |Vf; ei| is the projection of vf; on direction ei. Higher | vf; ei| means higher dependency of vf; on ei. The redundancy probability Pr of a single basis e\u2081 can be written as:\nPr = $\\prod_{j=1}^{FT} P(|V_{f_j} e_i < \u03be|)$\nThe equation tells us that the probability a basis e\u00a1 is redundant will decrease with the increasing number of features. On the contrary, the fewer features related to a specific task, the higher the likelihood that a certain basis is redundant, leading to more redundant dimensions in the representation."}, {"title": "Definition 3.8.", "content": "(Representation Redundancy) Given a specific task T with features Fr, we choose a set of bases Er \u2286 E. For each feature f \u2208 Fr, we set the coefficients in its feature vector vf which correspond to the bases in Er to zero and get the deformed feature vector V'f. Given a threshold Tr as the tolerance of deformation. If Er satisfies:\n$\\sum_{j=1}^{|FT|}max[(\u03b7 - cos(v_f, V'_f)), 0] < T_r$\nthen Er is a set of redundant bases for representing features FT, and the representation redundancy in Rm is defined under the choice of Er. If |Er| = r, then the value of representation redundancy is equal to r."}, {"title": "Remark 3.9.", "content": "The definition of representation redundancy is actually an extension of Definition 3.5 from one-dimension to multi-dimension. The difference \u03b7 - cos(Vf, V'f) is used to measure the degree of deformation of feature vectors. If the total deformation in feature vectors is accepted under the threshold Tr, then the influence from the removal of Er on the semantic representation of Fr can be ignored, and the number of useful dimensions for representing Fr is m - r."}, {"title": "3.2. Forward Redundancy", "content": "If the representation H = (h1, h2, ..., hm)T is followed by another weight matrix Wpost \u2208 Rl\u00d7m, then the following representation is H' = WpostH \u2208 VD' (D' = l). To investigate into the influence of H on H', we split H' into the sum of influences from the dimensions in H:\nW post H = $\\sum_{i=1}^m W_{:i}^{post} h_i $    (2)\nFrom (2), we find that the influence from hi to H' is subject to $W_{:i}^{post}$. If the value of $W_{:i}^{post}$ is small, then the influence from hi will be weakened, leading to a reduced impact on the final result in the follow-up calculations."}, {"title": "Definition 3.10.", "content": "(Forward Redundancy) Given a representation H and the follow-up parameters Wpost, we inspect the i-th dimension in H. Given a lower bound Tf < 0 for the influence of H in the forward propagation.If\n$|| W_{:i}^{post} h_i || < T_f$\nthen we say the dimension i in Rm is redundant in the forward propagation. If the number of redundant dimensions under this condition is r, then the value of forward redundancy is equal to r."}, {"title": "3.3. Node Redundancy", "content": ""}, {"title": "3.3.1. THE DEFINITION OF NODE REDUNDANCY", "content": "The discussions in previous sections focus on the real computations inside a model. In this section, we regard the whole model as a DAG composed of nodes and edges. The node could be a single neuron or an attention head in Transformer, based on the level of granularity.\nTo determine whether a node is redundant or not, a method is needed for measuring the importance of that node. Inspired by the idea of causal intervention in the area of causal inference, we provide the following definition:\nDefinition 3.11. (Node Redundancy) Given a model M and its computational graph G = (V, E) in which V and E represent the nodes and edges in G respectively, consider a node n \u2208 V. For task T, a batch of inputs X = {xi}=1 ~ DT is fed into the model and the activation of n is saved.\nFor each input xi, if we replace n(xi) which is the value of node n with another value n(x') corresponding to the corrupted input x' while keeping other activations in the forward propagation unchanged, we can get another output M(x'). Given a metric f for measuring the difference between two output distributions caused by n and a threshold T, we can calculate the contribution c(n) of the node n as:\nci(n) = f(n; M(xi), M(x))"}, {"title": "If the contribution of n satisfies:", "content": "Exix [[ci(n)|] <T\nthen n is a redundant node in G. If the redundant nodes in terms of task T in model M is N, then the value of the intrinsic dimension at the node level is |V \u2013 N|."}, {"title": "3.3.2. THE DETECTION OF NODE REDUNDANCY", "content": "With the idea of intervention from causal inference, we adopt the concept of indirect effect (Pearl, 2001) to estimate the contribution of a node n to the output as:\nIE(n; x) = Lm [M(x | do(n \u2190 n(x')))] \u2013 Lm [M(x)]\nin which IE is the indirect effect from node n to the final output of the model, and Lm is a metric for measuring the final output. We use the do-calculus notation (Neuberg, 2003) to express the intervention behavior, which is also called patching. The metric IE measures the effect of an intermediate variable in a causal graph to the final behavior of the model, which is widely used in causal inference. If Lm is viewed as a function of n, we can apply a first-order Taylor expansion to IE at n = n(x') for approximation. Thus IE(n; x) can be simplified as\nLm[M(x)]+[n(x')-n(x)]\u00af\u2207n\u00a3m[M(x)]-Lm[M(x)]\n=[n(x')-n(x)] \u2207nLm[M(x)]|n(x)\nIf zero ablation is used which means n(x') is set to zero, then the contribution of node n can be written as:\nc(n) \u2248 Ex\u2081~x[n\u2207nLm[M(x)]]        (3)\nThe IE in equation (3) consists of two parts: (i) n the value of node n on the clean input x and (ii) \u2207nLm[M(x)] the derivatives of the patching metric Lm with respect to node n. The two parts correspond to the representation redundancy and the forward redundancy respectively.\nTo gain an intuitive understanding of this, let's consider a representation in Vm which consists of a set of nodes N = {n1, n2, ..., nm} \u2282V at the neuron level and then focus on the node ni:\n\u2022 The first part of (3) directly serves as an indicator for representation redundancy since the magnitude of a neuron is a good indicator for dimension redundancy. According to (1), the smaller the value of a neuron is, the higher the likelihood that the neuron corresponds to a redundant dimension, since the activation of a feature f on a basis e\u00a1 is directly proportional to the projection of its feature vector vf on ei.\nOne thing to note is the case where two features f1 and f2 are almost in opposite directions and the activations in the shared direction may cancel out each other, which results in a situation that the value of a neuron seems to be small though both of the two features fire on it. While this case may be tricky, (Elhage et al., 2022) show that models prefer to represent anti-correlated features in opposite directions, which means f1 and f2 may never co-occur at the same time, so the probability of the above case occurring is very small.\n\u2022 As for the second part, according to the chain rule during the back propagation stage, it can be written as:\n\u2207ni Lm = $\\sum_{j=1}^{n} \u2207_{nj} Lm \\cdot \u2207_{n_i} n_j$\nwhere the node nj in the follow-up vector space Vn is\nnj = $\\sum_{i=1}^{m} w_{ji}^{post} n_i$\nThus\n\u2207ni Lm = $\\sum_{j=1}^{n} \u2207_{nj} Lm w_{ji}^{post}$    (4)\nFrom equation (4), it can be seen that the second part in (3) is closely related to the follow-up calculations as discussed before in Section 3.2, hence serving as an indicator for forward redundancy.\nAs discussed above, we can see that the metric for judging the redundancy of a node is a combination of the representation redundancy and the forward redundancy. Therefore the essence of the intrinsic dimension at the node level can be decomposed and understood from the above two aspects.\nNote that Equation (3) is also adopted in circuit discovery, e.g., attribution patching (Nanda, 2023). Thus the process of circuit discovery can be viewed as a process of identifying the two types of redundancy and finding out the intrinsic dimensions in terms of a task. Each of the intrinsic dimensions corresponds to a node in the computational graph, and the number of the nodes in the circuit is expected to be equal to the dimensionality of the solution space. Given a specific task, the co-occurrence of the representation redundancy and the forward redundancy is the necessary and sufficient condition of the node redundancy in the graph."}, {"title": "4. Main Method", "content": "In this section, we introduce circuit-tuning, an algorithm based on the theory of node redundancy for both fine-tuning neural networks and analyzing training dynamics."}, {"title": "4.1. Circuit-tuning", "content": ""}, {"title": "4.1.1. FROM NODES TO EDGES", "content": "The definition of node redundancy as well as the detection of it is provided in Section 3.3. Since the nodes inside a graph are connected by edges, we can also study the node redundancy from the view of edges."}, {"title": "4.1.2. THE CIRCUIT-TUNING ALGORITHM", "content": "Given a model M for fine-tuning, the model is initialized into a computational graph G = (V, E) in which V and E represent the nodes and edges in G respectively. A metric Lm and a loss function L are used for circuit discovery and parameter optimization respectively. Given a dataset X ~ DT for the fine-tuning task T, circuit-tuning alternately performs the following two procedures:\nCircuit Discovery For a batch of data XT \u2208 X, we calculate the contribution of each edge e \u2208 E according to (5). Then all the edges are sorted in descending order based on their contributions, and the edges with top N contributions are selected. Then the graph G is pruned into a circuit C = (VT, ET) with only the selected edges Er together with the nodes Vr at both ends of each edge inside.\nCircuit-tuning For a batch of data XT \u2208 X, all the parameters outside C are frozen, and only the parameters corresponding to the nodes Vr inside C are updated through a forward pass and a backward pass as usual. After K steps of optimization, all the frozen parameters are freed and the graph G is reset to its original state.\nThe full process is shown in Algorithm 1. The parameter to optimize corresponds to the parameter matrix that maps an input to the activation of a node in the circuit. For example, the parameter corresponding to the node at the attention output refers to Wo, which is the output projection matrix."}, {"title": "4.2. Characteristics of Our Method", "content": "Capability Acquisition Different from recent studies that focus on the circuits of certain model behaviors, circuit-tuning aims to automatically find the relevant components responsible for a specific task and guide the model to develop an ability through parameter optimization. The dynamics of this process will be verified in Section 5.1.\nPrecise Fine-tuning Compared with full fine-tuning and LoRA-based methods, our method only update a few parameters instead of taking all of the parameters into account. Since our method tries to leave out the irrelevant structures in terms of a target task, the general abilities are expected not to be affected, which will be verified in Section 5."}, {"title": "4.3. Convergence Analysis", "content": "To demonstrate the stability of circuit-tuning, we provide an analysis of the training convergence. Specifically, let's consider the model as a continuously differentiable function f(0). Let 0 \u2208 RD = (01, 02, ...,0D), and suppose that f satisfies the conditions for Lipschitz gradient continuity:\n||\u2207of(0') \u2013 \u2207of(0)|| \u2264 L||0\u2032 \u2013 0||\nwhere L is a Lipschitz constant."}, {"title": "If we mask out a set of parameters Omask \u2282 0, then we have", "content": "||\u22070\\0mask f (0\u2032) \u2013 \u22070\\0maskf (0)|| <||\u2207of(0') \u2013 Vof(0)|| <L||0' - 0||\nThis is because for each masked parameter \u03b8i \u2208 Omask, the gradient of it equals to 0, which leads to a smaller norm for gradient change. Thus a smaller Lipschitz constant Lmask < L serves as the upper bound for the change rate of gradients, which means the training process will be more stable compared with full fine-tuning. The evidence for this conclusion will be discussed in Section 5.1.3."}, {"title": "5. Experiments", "content": "In this section, we test our method across a variety of models and tasks. To comprehensively investigate the performance as well as the training dynamics during fine-tuning, we arrange the experiments into two parts. The first part serves as the verification and analysis of our method, while the second part aims to show that our method is scalable to various model sizes and tasks."}, {"title": "5.1. The Subject-verb Disagreement Task", "content": ""}, {"title": "5.1.1. TASK DESCRIPTION AND DATA PREPARATION", "content": "The goal of the subject-verb disagreement task is to match a verb with a subject in an abnormal way. For example, \u201cI is\u201d, \u201che are\u201d and \u201cthe cows eats\u201d are all expected results for this task. In each sentence, the word before the verb is called the END token. The automatic evaluation metric for this task is the logit difference between the flipped verb and the original verb at the END token. This is because the logit at the END token is directly used for predicting the verb.\nThe reason why we create this task as the start of our experiments is that we expect the model to acquire a new capability from scratch. Besides, the task is simple and interesting for analyzing the training dynamics during fine-tuning.\nFor training data, we use the first 10k samples in the Pile (Gao et al., 2020) corpus. We extract 30k sentences in the present tense in English and flip the forms of the verbs in each sentence. For details, please refer to Appendix C.1."}, {"title": "5.1.2. IMPLEMENTATION DETAILS", "content": "We use GPT2-small (Radford et al., 2019) in this task. We set the output of the attention and the MLP in each layer as upstream nodes, and the input of the query, key, value and the MLP in each layer as downstream nodes. During training, we use the logit difference discussed before as the metric Lm and follow (Syed et al., 2023) to calculate the edge contributions. We sweep over a range of N which is the number of edges to be saved during circuit discovery. Mini-batch SGD is used for optimization. For details of settings, please refer to Appendix C.2."}, {"title": "5.1.3. MAIN RESULTS", "content": "From Figure 1(a), we observe a flip in logit difference from negative to positive, which means the model adjusts its grammar to fit the data distribution of subject-verb disagreement. One noteworthy finding is that the trained model can generate abnormal texts in the past tense, such as \u201cto be or not to be, that were a ...\u201d, which implies that the model really learns the new grammar and applies it smartly, since there is no sentence in the present tense in our training data at all.\nWe find that with the increasing number of top N edges, the logit difference gets bigger until N reaches around 1000. We check and find that the number of tunable parameters for each N is also saturated at this point. We believe when N = 1000, almost all the necessary parameters for this task are included, and the performance cannot be further improved, suggesting the existence of intrinsic dimension. For verification of this, please refer to Appendix C.3.\nIn Figure 1(b), we find that the perplexity (PPL) of full fine-tuning is high and fluctuates wildly during training, though the logit difference of it is higher. This observation implies better training stability as well as better preservation of general capabilities of circuit-tuning over full fine-tuning."}, {"title": "5.1.4. THE QUALITY OF THE DISCOVERED CIRCUIT", "content": "To demonstrate that our method is able to find the required parameters accurately, we (i) calculate the faithfulness and completeness (Marks et al., 2024) of the circuits and (ii) provide an ablation study in which we randomly unfreeze the nodes outside the circuit to check their influence on the performance of this task. Results show from various aspects that the parameters required are found accurately. For details, please refer to Appendix C.3."}, {"title": "5.1.5. ANALYSES ON TRAINING DYNAMICS", "content": "We analyze the circuits before, during, and after training, and report some amazing findings that are worth studying."}, {"title": "Interpretation of the Circuits", "content": "To interpret the circuit into human-understandable structures, we follow (Wang et al., 2022) and decompose the circuit into heads of different functions. Firstly we find out the attention head that directly affects the output, which is called the Subject Attribute Head. Then we find out the head responsible for the localization of the subject, which is called the Subject Identification Head. Finally, we find out the Collaborative Heads that could affect the behaviors of the Subject Attribute Heads. For details of the processes above, please refer to Appendix C.4.1."}, {"title": "The Flip of the Subject Attribute Heads", "content": "The Subject Attribute Heads are responsible for matching the subject and the verb. We visualize the heads before and after fine-tuning in Figure 2. Through comparison, we observe an obvious flip at head.10.9, which implies the reversal in its function from subject-verb agreement to disagreement. Other heads (3.0, 6.0, 4.4, 11.8, etc) also see flips with varying degrees, which serves as strong evidence for the self-adjustment inside the nodes."}, {"title": "The Sharing of the Subject Identification Heads", "content": "The Subject Identification Heads attend heavily to the subject. One type of these heads attends to the END token (0.1, 0.3, etc), which is helpful to the cases like \"they are\"; the other type of heads (8.5, 10.5, etc) attends to the subject several tokens before, which is helpful to the cases like \"the girl wearing a dress is\". Both types of heads perform the same before and after fine-tuning, which means their functions are preserved and shared all the way down."}, {"title": "The Evidence of Hebbian Learning", "content": "We visualize the circuits during the fine-tuning process in Figure 9, and observe a phenomenon akin to Hebbian learning (Do, 1949). We find that several edges are strengthened during training, just as the synapses between neurons can be strengthened after continuous stimulation. We regard this finding as evidence of the self-organization (Kohonen, 2012) inside the model to a new environment. See Appendix C.4.3 for details."}, {"title": "5.1.6. IMPROVEMENT ON ATTRIBUTION PATCHING", "content": "During the analyses, we notice that some heads fail to appear in the circuit. We assume that the attribution score in EAP fails to measure the importance of a node. This is because an important node na may be connected with many nodes through edges Ea with low contributions, while another less important node n\u1eef may connect with only one node through an edge that is stronger than anyone in Ea. As a result, the edges in Ea are discarded, and thus the importance of na is underestimated. To solve this problem, we revise the contribution c(e) of an edge e : n \u2192 ng as:\nc(e)' = c(e). $\\sum_{k=1}^{N_{down}} c(n_n)  \\sum_{k=1}^{N_{up}} c(n \\rightarrow n_i^u)$\nThe revision takes into account all the edges connected to the upstream node and the downstream node. The results in Figure 1(c) show that our improvement is effective without introducing too much computation. Details of the results can be found in Appendix C.4.5."}, {"title": "5.2. Application to Complex Tasks", "content": ""}, {"title": "5.2.1. TASK DESCRIPTION", "content": "We prepare two types of tasks based on whether reasoning is involved, each of which contains two specific tasks.\nFor reasoning-based tasks, chain-of-thought (CoT) style responses are involved to solve a problem. We prepare a mathematical task and a logical reasoning task since (Sprague et al., 2024) demonstrated that CoT is effective only on tasks requiring mathematical, logical, or algorithmic reasoning.\nFor reasoning-free tasks, only the final answer or a signal token is required. We prepare a gender de-biasing task which requires the language model to develop an unbiased perspective on genders, and a reading comprehension task which requires only the keywords as the answer."}, {"title": "6. Conclusion", "content": "In conclusion, we propose the theory of node redundancy from the mechanistic view as an extension of the intrinsic dimension. With the guidance of the theory, we propose the algorithm of circuit-tuning, which not only shows great potential in fine-tuning but also serves as a useful tool for analyzing the training dynamics inside a model."}, {"title": "A. The Derivation of Edge Contribution", "content": "According to the definition of attribution score in (Syed et al.", "e": "n\u2081 \u2192 n\u2082 with n\u2081 as the upstream node and n\u2082 as the downstream node. We use the indirect effect IE to measure the change in the output caused by the patching of the edge. Thus given a dataset X for patching", "follows": "nc(e) = Exix [[ci(e)|", "M(x))|": "Exi~x [[IE(e; xi)|"}]}