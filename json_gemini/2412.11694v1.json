{"title": "From Specific-MLLM to Omni-MLLM:\nA Survey about the MLLMs alligned with Multi-Modality", "authors": ["Shixin Jiang", "Jiafeng Liang", "Ming Liu", "Bing Qin"], "abstract": "From the Specific-MLLM, which excels in\nsingle-modal tasks, to the Omni-MLLM, which\nextends the range of general modalities, this\nevolution aims to achieve understanding and\ngeneration of multimodal information. Omni-\nMLLM treats the features of different modali-\nties as different \"foreign languages,\" enabling\ncross-modal interaction and understanding\nwithin a unified space. To promote the advance-\nment of related research, we have compiled 47\nrelevant papers to provide the community with\na comprehensive introduction to Omni-MLLM.\nWe first explain the four core components of\nOmni-MLLM for unified modeling and inter-\naction of multiple modalities. Next, we intro-\nduce the effective integration achieved through\n\"alignment pretraining\" and \"instruction fine-\ntuning,\" and discuss open-source datasets and\ntesting of interaction capabilities. Finally, we\nsummarize the main challenges facing current\nOmni-MLLM and outline future directions.", "sections": [{"title": "1 Introduction", "content": "The remarkable performance of continuously evolv-\ning large language models (LLMs) has pointed to\na possible direction for achieving general artifi-\ncial intelligence (Brown, 2020; Chung et al., 2024;\nAchiam et al., 2023; Bubeck et al., 2023). Be-\nyond the domain of natural language, researchers\nhave initially created Multi-Modal Large Language\nModels (MLLMs) by combining LLMs with dif-\nferent modality-specific pretrained models. Dif-\nferent MLLMs are capable of understanding vari-\nous modalities, such as Vision-MLLMs (Alayrac\net al., 2022; Li et al., 2023; Liu et al., 2024; Wang\net al., 2024a), Audio-MLLMs (Zhang et al., 2023a;\nChu et al., 2023), and 3D-MLLMs (Xu et al.,\n2025). MLLMs that link LLMs with single non-\nlinguistic modalities are referred to as Specific-\nMLLMs, which have demonstrated superior perfor-\nmance in uni-modal tasks compared to traditional\npretrained models (Lin et al., 2024; Chen et al.,\n2024d). However, just as humans rely on auditory,\nvisual, tactile, and other senses to interact with the\nreal world, achieving general artificial intelligence\nalso requires understanding and even generating\nmore non-linguistic modalities. Therefore, in ad-\ndition to improving the performance of Specific-\nMLLMS, another developmental path for MLLMS\nis to expand the range of modalities they can under-\nstand and generate, referred to as Omni-MLLM.\nOmni-MLLM leverages the emergent abilities of\nLLMs to treat features from different non-linguistic\nmodalities as distinct \"foreign languages\", enabling\ninteraction and understanding between these modal-\nities in the same space (Chen et al., 2023a). By\nutilizing generative models pre-trained on differ-\nent modalities, Omni-MLLM ultimately achieves\nthe execution of both uni-modal and cross-modal\ntasks. The key challenges in realizing this include:\n(1) how to unify the encoding of different modal-\nities into a common space, (2) how to facilitate\ninteraction between different modalities, and (3)\nhow to generate content across modalities. We will\nprovide a detailed explanation of these issues.\nA review of the development of Omni-MLLM"}, {"title": "2 Omni-MLLM", "content": "Omni-MLLM is an extension of Modal-Specific\nMLLM. It retains the overall structure of \"encoding\nstage - alignment stage - interaction stage - genera-\ntion stage\" and expands the encoding, alignment,\nand generation stages to handle a broader range of\nmodalities. This section introduces the implemen-\ntation and functions of these four components in\nOmni-MLLM."}, {"title": "2.1 Multi-Modalities Encoding", "content": "Compared to the single encoders used in Specific-\nMLLM, Omni-MLLM requires multiple encoders\nto process different modalities. Considering the\nperformance and ease of use of the encoders, Omni-\nMLLM adopts various encoding strategies.\nContinuous Encoding Continuous encoding\nrefers to the use of modality-specific encoders to\nencode modality X into continuous features Fx for\nsubsequent processing, as shown in Equation 2.1.\nMost Omni-MLLMs integrate various pre-trained\nuni-modal encoders for continuous encoding, in-\ncluding EVA CLIP (Sun et al., 2023b) and In-\nternVit (Chen et al., 2024d) for visual modalities,\nBeats (Chen et al., 2023b) and CLAP (Elizalde\net al., 2023) for auditory modalities, and ULIP-\n2 (Xue et al., 2024) for 3D modalities.\n$F_x = SpecificEncoder (X)$\n$F_x = PreAlignedEncoder (X)$"}, {"title": null, "content": "Excecpt this approach of using multiple hetero-\ngeneous encoders, some Omni-MLLMs, such\nas One-LLM (Han et al., 2024) and Image-\nBind_LLM(Han et al., 2023), utilize unified pre-\naligned encoders(Girdhar et al., 2023; Zhu et al.)\nto encode multiple modalities into a unified feature\nspace, facilitating training. The continuous encod-\ning approach helps preserve the semantic informa-\ntion of different input modalities, enabling easier\ninteraction and processing in subsequent stages(Su\net al., 2023).\nDiscrete Encoding Discrete encoding refers to\nencoding modality X into discrete tokens Tx, al-\nlowing the interaction stage to engage with differ-\nent modalities in the form of hard prompts (Zhan\net al., 2024). Examples include the SEED tok-\nenizer (Ge et al.) and Jukebox tokenizer (Dhari-\nwal et al., 2020) based on VQ, the Speech Tok-\nenizer (Zhang et al., 2023c) based on RVQ, and\nTeal's Audio Tokenizer (Yang et al., 2023) based\non clustering. Omni-MLLMs can leverage unified\ndiscrete representations to achieve joint generation\nacross multiple modalities in a next-token man-\nner (Zhan et al., 2024).\n$T_x = Specific Tokenizer$"}, {"title": "2.2 Connector", "content": "Omni-MLLM uses a unified transformer for inter-\naction and reasoning. Regardless of the encoding\nmethod for multiple modalities, the resulting multi-\nmodal features need to be aligned into a common\nfeature space, e.g. the input space of an LLM. The\ndifferent encoding strategies in Omni-MLLM's en-\ncoding stage have also led to the adoption of vari-\nous alignment methods and structures.\nContinuous Base The continuous encoded fea-\ntures Fx of different modalities are transformed\ninto aligned features Fproj through the connec-\ntor structure, regardless of whether they are in the\nsame pre-aligned feature space or in different fea-\nture spaces without pre-alignment. For the former,\nOmni-MLLM typically employs modality-specific\nalignment layers to address issues of dimen-\nsional mismatch and feature misalignment between\nmodalities (Chen et al., 2023a; Panagopoulou et al.,\n2023; Ye et al., 2024a). For pre-aligned features\nfrom different modalities, Omni-MLLM reduces\nthe training parameters by using a shared alignment\nlayer for unified alignment (Han et al., 2023; Su\net al., 2023; Han et al., 2024). These approaches\nare referred to as \"multi-branch\" and \"uni-branch\"\nmethods, as illustrated in Figure 3.\n$F_{proj} = SpecificConnector (F_x)$\n$F_{proj} = UnifiedConnector (F_x)$"}, {"title": null, "content": "The specific design of the alignment stage can be\ncategorized into two types based on the underly-\ning principles: linear-based and attention-based.\nLinear-based methods use MLPs or a single lin-\near layer to project non-linguistic modality fea-\ntures into the LLM's embedding space, preserving"}, {"title": "2.3 Interactive Backbone", "content": "Omni-MLLM enables multi-modal interaction\nthrough transformer-based LLMs, avoiding the\ninductive bias of CNNs and the long-sequence\nmodeling issues of LSTMs and RNNs (Dosovit-\nskiy, 2020; Zhang et al., 2023d). Commonly used\nLLMs include pre-trained models such as the T5 se-\nries (Raffel et al., 2020), LLaMA series (Touvron\net al., 2023), and Qwen series (Bai et al., 2023).\nIn terms of interaction methods, Omni-MLLM ei-\nther concatenates the aligned modality features\nwith text features at the input stage (Panagopoulou\net al., 2023) or inserts modality information into the\ndeeper structures of the LLM (Han et al., 2023) to\nfacilitate interaction between different modalities.\nInteraction Modalities In terms of interaction\nmodalities, compared to Specific-MLLM, which\ncan only perform uni-modal interactions with\na single non-linguistic modality, Omni-MLLM\ncan handle uni-modal interactions with multi-\nple non-linguistic modalities as well as cross-\nmodal interactions across different non-linguistic\nmodalities. For example, ImageBind-LLM (Han"}, {"title": "2.4 Generator", "content": "Leveraging the results of feature interactions be-\ntween different modalities, Omni-MLLM can not\nonly generate textual outputs using the LLM's head\nbut also generate outputs of multiple modalities,\nincluding audio, image, and video, through genera-\ntive models of other modalities. The modal exten-\nsion of input and output enables Omni-MLLM to\nperform uni-modal understanding tasks such as\nQA and captioning, uni-modal generation tasks\nsuch as Text2X and X-Edit, as well as cross-modal\nunderstanding and generation tasks such as X-Y-\nQA and X2Y.\nThe specific methods for modality generation\ninclude: (1) Embedding-based: Omni-MLLM\nleverages MLP or transformers to map the last\nlayer's embedding or last hidden state of the spe-\ncial token output from the interaction module to\nthe latent space of pre-trained diffusion models.\nThese models, such as StableDiffusion and Audi-\nOLDM, are then used for generation (Wu et al.;"}, {"title": "3 Omni-MLLM Training", "content": "Omni-MLLM adopts a two-stage training strategy\nsimilar to Specific-MLLM to align different modal-\nities and improve its ability to respond to instruc-\ntions in the target modality, e.g. the \"Alignment\nStage\" and the \"Instruction Fine-tuning Stage.\" The\nexpansion of modalities makes the two stages of\nOmni-MLLM somewhat more complex."}, {"title": "3.1 Alignment Pre-training", "content": "The Alignment Pre-training phase of Omni-MLLM\nprimarily aims to achieve alignment between the\nencoding space of different modalities in the en-\ncoding segment and the input of the interaction\nsegment, as well as alignment between the textual\nfeature space of the interaction segment and the\nlatent space of the decoding model of different\nmodalities. Input alignment and output alignment\ncan be performed separately (Wu et al.) or simulta-\nneously (Ye et al., 2024a).\nInput Alignment Input alignment mainly uses\npaired text data from different modalities to com-\npute the generation loss of the Eq 5 and optimize.\n$L_{CE} = CE_{LOSS} (LLM (F_{proj}, F_{ins}), T)$"}, {"title": null, "content": "where Fproj is the feature of the instruction text\nthat guides the LLM's output, T is the targe descrip-\ntive text, LLM(*) is the LLM's prediction text.\nOmni-MLLMs with differently designed align-\nment layers adopt varying input alignment strate-\ngies. For the multi-branch Omni-MLLM described\nin Section 2.2, the alignment layers of different\nmodalities do not interfere with each other, so\nseparate alignment training can be conducted for\neach modality (Panagopoulou et al., 2023; Chen\net al., 2023a; Li et al., 2024a; OpenGV, 2024).\nIn contrast, for the uni-branch Omni-MLLM, dif-\nferent modalities share the same alignment layer,"}, {"title": null, "content": "so the alignment effects between modalities will\ninfluence each other. Such Omni-MLLMs often\nalign multiple modalities directly using a progres-\nsive alignment strategy (Han et al., 2024; Chen\net al., 2024b; Yu et al., 2024a), or leverage pre-\naligned feature spaces where the data-rich visual\nmodality is used to indirectly align other modali-\nties (Han et al., 2023; Zhang et al., 2023b). For\nOmni-MLLMs with discrete encoding, fine-tuning\nthe corresponding expanded LLM's input embed-\nding is sufficient (Zhan et al., 2024; Luo et al.,\n2024).\nOutput Alignment Output alignment is trained\nusing the same data as input alignment. However,\nfor the different output configurations described\nin Section 2.4, the training losses are different.\nFor token-based and text-based Omni-MLLMs, the\nloss of the special tokens or corresponding text is\nthe same as input alignment, and adjustments are\nmade to the extended LLM's output head (Zhan\net al., 2024; Yang et al., 2023; Luo et al., 2024).\nIn contrast, for embedding-based output configu-\nrations, the output projector is adjusted using the\nloss comprising three components (Wu et al.; Ye\net al., 2024a): 1) the text generation loss in Eq 5; 2)\nthe L2 distance between the output embedding and\nthe corresponding decoder's condition vector, e.g.\nMSE loss; and 3) the conditional latent denoising\nloss (Rombach et al., 2022)."}, {"title": "3.2 Instruction Fine-tuning", "content": "The instruction fine-tuning phase aims to enhance\nOmni-MLLM's ability to respond to instructions\nfor the four interaction forms described in Sec-\ntion 2.4, improving the model's generalization.\nTherefore, compared to Specific-MLLM, Omni-\nMLLM not only uses Uni-Modal instruction data\nbut also trains with Cross-Modal understanding in-\nstruction fine-tuning datasets and modality genera-\ntion instruction fine-tuning datasets, employing the\nsame loss function as the alignment phase (Chen\net al., 2023a; Lyu et al., 2023; Sun et al., 2023a). In\nthis stage, in addition to adjusting the parameters\nof the interaction segment, some Omni-MLLMS\nfurther perform full or partial fine-tuning of the\nLLM (Zhang et al., 2023b; Panagopoulou et al.,\n2023).\nOther Training Recipes In addition to the gen-\neral training paradigms mentioned above, we in-\ntroduce some other training recipes used by cer-\ntain Omni-MLLMs: (1) Prior knowledge from"}, {"title": "4 Data and benchmark", "content": "This section further summarizes the sources of\nalignment data and instruction fine-tuning data\nused during the training process of Omni-MLLM,\nas well as the test benchmarks for different interac-\ntion scenarios."}, {"title": "4.1 Training Data", "content": "Alignment Data Omni-MLLM differs from\nSpecific-MLLM in requiring text-paired data for\nalignment across different modalities. For data-rich\nmodalities such as vision and audio, Omni-MLLM\nbenefits from abundant text-paired data for align-\nment. However, for data-scarce modalities like\ndepth maps, normal maps, and thermal maps, large-\nscale text-paired data is lacking. To address this,\nsynthetic methods that use translation models (Ran-\nftl et al., 2021; Lee et al., 2023; Goel et al., 2024) to\nconvert image-text pairs into corresponding modal-\nity text-paired data are widely employed (Han et al.,\n2024; Jiang et al., 2024).\nIt is also worth noting that interleaved datasets,\nsuch as Multimodal C4, OBELICS, and MIMICIT,\nare extensively used to enhance Omni-MLLM's\ncontextual understanding capabilities. For more\ndetails, please refer to the appendix."}, {"title": null, "content": "Instruction Data Omni-MLLM's diverse appli-\ncation capabilities are achieved through uni-modal"}, {"title": null, "content": "instruction tuning dataset from different modalities\nas well as cross-modal instruction tuning dataset.\nFor the former, Omni-MLLM extensively utilizes\nthe open-source datasets of Specific-MLLM.\nAs for cross-modal instruction data, Omni-\nMLLM employs various methods for expansion:\n(1) instructization method: Instruction data is\nconstructed using cross-modal downstream task\ndata combined with templates. For example, FA-\nVOR (Sun et al., 2023a) designs corresponding\ninstruction templates based on the How2 (Sanabria\net al., 2018) and VGGSS (Chen et al., 2020)\ndatasets to get cross-modal instruction tuning\ndata; (2) GPT's generation method: Some omni-\nMLLMs (Lyu et al., 2023; Zhao et al., 2023b) fol-\nlow the LLaVA's generation paradigm, using pre-\ntrained models like SAM and language models like\nGPT-4 to generate cross-modal instruction tuning\ndata based on meta-information such as captions\nand object categories in different modalities; (3)\nText-to-Multimodality conversion method: Based\non Uni-Modal instruction data, Text2X models like\nTTS models or Diffusion models are used to con-\nvert the text into non-language modalities for con-\nstruction. For instance, Uni-Moe (Li et al., 2024c)\nuses TTS (Microsoft) tools to convert text from\nLLaVA-Instruct (Liu et al., 2024) data into audio,\nwhile AnyGPT (Zhan et al., 2024) further utilizes\ntools like DALL-E-3 (Betker et al., 2023) and Mu-\nsicGen (Copet et al., 2024) to convert pure text\ninstructions into cross-modal instruction data."}, {"title": "4.2 Benchmark", "content": "We categorize the commonly used evaluation\nbenchmarks in Omni-MLLM according to the in-\nteraction tasks defined in Section 2.4, selecting\nfrequently used benchmarks and showcasing the\nperformance of some Omni-MLLM models. Addi-\ntionally, comparisons with corresponding Specific-\nMLLM models are provided. Due to space limita-\ntions, further details can be found in the appendix.\nUni-Modal Understanding & Generation The\nevaluation of Uni-Modal performance includes not\nonly specific downstream task test sets such as\nX-QA (Goyal et al., 2017; Xu et al., 2017), X-\nCaption (Plummer et al., 2015; Xu et al., 2016),\nand Classified (Deitke et al., 2023), for each\nmodality but also comprehensive benchmarks like\nMMB (Liu et al., 2025). As shown in Table 1,\nmost Omni-MLLMs still exhibit significant perfor-"}, {"title": "5 Challenges and Future Directions", "content": "As more Omni-MLLMs emerge, this section will\nelaborate on some key issues in Omni-MLLMs and"}, {"title": null, "content": "potential future development directions.\nExpansion of modalities Most Omni-MLLMs\ncan only handle 2-3 types of non-linguistic modal\ninformation, and expanding to additional modali-\nties often requires addressing the following aspects:\n(1) Improving efficiency and reducing training\ncosts: Most existing Omni-MLLMs require new\nalignment training and instruction fine-tuning when\nextending to new modalities, which often leads\nto significant computational costs. Approaches\nsuch as leveraging prior knowledge from Specific-\nMLLMS (Panagopoulou et al., 2023; Chen et al.,\n2024a) or indirect alignment based on pre-aligned\nencoders (Han et al., 2023) can reduce the train-\ning process, but may impact performance. Further\nexploration is needed to balance performance and\ncost in expansion methods. (2) Avoiding perfor-\nmance degradation in trained modalities: When\nexpanding to new modalities involves adjusting pa-\nrameters shared by the same structure, catastrophic\nforgetting of previously trained modalities may oc-\ncur (Yu et al., 2024a). Mixing previously trained\nmodality data during training or fine-tuning only\nthe newly added modality-specific parameters can\npartially alleviate this issue (Yu et al., 2024b; Han\net al., 2024). Continuous learning methods can also\nbe explored further for better solutions. (3) Ex-"}, {"title": null, "content": "ties with limited data, translating and synthesizing\npaired and instructional data from text-paired data\nof other modalities can, to some extent, alleviate\nthe issue of data scarcity (Han et al., 2024; Jiang\net al., 2024). However, the lack of real paired data\nfor the target modality may limit the model's perfor-\nmance on that modality. Methods such as the visual\nfeature cache retrieval mechanism in ImageBind-\nLLM (Han et al., 2023) could be referenced to\nenrich the inference process and alleviate perfor-\nmance limitations in low-resource modalities.\nCross-modal capabilities As shown in Section 4,\nexisting Omni-MLLMs have achieved promising\nresults in cross-modal reasoning and understanding\ntasks, but there remain areas for optimization: (1)\nLong-context capability: When input sequences\ninclude multiple sequential modalities, the token\nsequence length may exceed the model's context\nwindow. Methods such as compressing inputs us-\ning attention or pooling (Liang et al., 2024; Zhang\net al., 2023b) or partially sampling the original\ndata (Zhan et al., 2024) can reduce the number of in-\nput tokens but often lead to a decline in cross-modal\nperformance. Exploring alternative approaches to\nenhance long-context capabilities is necessary. (2)\nEnhancement of components: Due to resource\nconstraints, current Omni-MLLMs often use base\nor small versions of modality encoders and base\nversions of LLMs (Lu et al., 2024; Panagopoulou\net al., 2023). Increasing the size of the encoding\nand interaction components could significantly im-\nprove performance. (3) Temporal alignment be-\ntween different modal sequential data: Retaining\ntemporal alignment information at the input stage\nis crucial for subsequent cross-modal understand-\ning, especially when processing modalities with\ntime dependencies. Current Omni-MLLMs employ\nmethods such as interleaved inputs across modal-\nities (Lu et al., 2024; Tang et al., 2024b) and the\nintroduction of time-related special tokens (Goel\net al., 2024; Tang et al., 2024a) to align audio and\nvideo temporally. However, further exploration\nis needed to develop time-alignment methods for\nmore modalities."}, {"title": null, "content": "Data and benchmark High-quality uni-modal\ninstruction data and benchmarks have driven the\ndevelopment of Specific-MLLMs. However, Omni-\nMLLMs still lack large-scale cross-modal instruc-\ntion data and benchmarks. (1) High-quality cross-\nmodal instruction data: Section 4 highlights\nthat different Omni-MLLMs have used various"}, {"title": null, "content": "methods to synthesize cross-modal instruction data\nin order to alleviate the issue of insufficient in-\nstruction data. However, there is still signifi-\ncant room for improvement and expansion, in-\ncluding enhancing instruction diversity, enabling\nlonger context dialogues, and incorporating more\nmodal interaction paradigms. (2) Comprehensive\nbenchmarks: Omni Benchmarks, including Om-\nniBench (Li et al., 2024b), OmniXR (Chen et al.,\n2024c), and Curse (Leng et al., 2024), still fall short\ncompared to Uni-Modal benchmarks like MM-\nBench (Liu et al., 2025) in terms of task richness\nand instruction diversity. Moreover, most cross-\nmodal interactions in these benchmarks focus on\nvisual, audio, and text modalities, lacking other\nmodalities such as 3D and IMU (Han et al., 2024).\nFurther expansion of current Omni Benchmarks is\nnecessary to provide a more comprehensive evalua-\ntion of Omni-MLLMs.\nApplication scenarios The emergence of Omni-\nMLLMs brings new opportunities and possibilities\nfor tasks in various scenarios. (1) Smarter real-\ntime voice interaction: Models like VITA and\nMini-Omni2 (Xie and Wu, 2024; Fu et al., 2024)\nimplement end-to-end understanding and real-time\nresponse to human voice commands, interpreting\ncorresponding text and image inputs and generat-\ning spoken replies in real-time. Compared to non-\nend-to-end voice assistants that use ASR and TTS\ncombined with Specific-MLLMs, these systems are\nmore intelligent in recognizing and understanding\nhuman commands. (2) More comprehensive plan-\nning capabilities: In contrast to other Specific Au-\nodrive MLLMs like DriveGPT4 (Xu et al., 2024),\nwhich uses only visual modality information for\naction, DriveMLLM (Wang et al., 2023) leverages\nthe complementarity between multiple modalities\nto achieve better path planning capabilities. At\nthe same time, the Grounding Action (Szot et al.,\n2024), which understands both action vectors and\nvisual information, not only enables vision-based\naction planning but also performs autoregressive\ngeneration of subsequent actions, enabling con-\ntinuous action planning. (3) \"Any2Any\" world\nsimulator: WordGPT (Ge et al., 2024), through its\nunderstanding and generation of audio and visual\ndata, enables state transitions across any modality\ncombinations and simulation of the world."}, {"title": "6 Conclusion", "content": "In this paper, we provide a comprehensive survey\non Omni-MLLM. We break down Omni-MLLM\ninto four key components and classify it based on\nthe unified encoding of different modalities. At the\nsame time, we elaborate on the training process\nof Omni-MLLM and the relevant resources used.\nFinally, we summarize the current limitations of\nOmni-MLLM and discuss future directions for its\ndevelopment. We hope our work contributes to the\nadvancement of Omni-MLLM."}, {"title": "7 Limitations", "content": ""}]}