{"title": "Neuro-Symbolic AI: Explainability, Challenges, and Future Trends", "authors": ["XIN ZHANG", "VICTOR S.SHENG"], "abstract": "Explainability is an essential reason limiting the application of neural networks in many vital fields. Although neuro-symbolic AI hopes to enhance the overall explainability by leveraging the transparency of symbolic learning, the results are less evident than imagined. This article proposes a classification for explainability by considering both model design and behavior of 191 studies from 2013, focusing on neuro-symbolic AI, hoping to inspire scholars who want to understand the explainability of neuro-symbolic AI. Precisely, we classify them into five categories by considering whether the form of bridging the representation differences is readable as their design factor, if there are representation differences between neural networks and symbolic logic learning, and whether a model decision or prediction process is understandable as their behavior factor: implicit intermediate representations and implicit prediction, partially explicit intermediate representations and partially explicit prediction, explicit intermediate representations or explicit prediction, explicit intermediate representation and explicit prediction, unified representation and explicit prediction. We also analyzed the research trends and three significant challenges: unified representations, explainability and transparency, and sufficient cooperation from neural networks and symbolic learning. Finally, we put forward suggestions for future research in three aspects: unified representations, enhancing model explainability, ethical considerations, and social impact.", "sections": [{"title": "1 INTRODUCTION", "content": "The difficulties of explaining neural networks come from its feature extraction and reasoning process based on features. It could be more intuitive to associate float numbers with our macroscopically perceived object features. Although many attempts have been made to explore this aspect, it still takes much effort to solve this problem fundamentally. Explainability is less evident than other metrics for measuring model performance, such as accuracy, recall or precision. Facchin [69] proposed that researchers have yet to directly observe or identify a clear neural structure that fully meets or represents a specific NSR (neural structure representation) standard in neuroscience. In other words, although significant progress has been made in understanding how the brain processes and stores information, we still need to be able to specify an exact neural structure or network of neurons that precisely corresponds to a specific encoding of"}, {"title": "2 EXPLAINABILITY ANALYSIS", "content": "Calegari et al. [36] divided models' transparency into explainability by design and post-hoc. The former designs the models to be easily understood by humans or embeds logical rules and constraints in the model construction process to make the neuro-symbolic AI system transparent and explainable by design, essentially avoiding the impact of black box models from the beginning. The latter refers to analyzing the model's behavior to explain its decision-making process"}, {"title": "3 EXPLAINABILITY-BASED CLASSIFICATION OF NEURO-SYMBOLIC METHODS", "content": "Combining these two ideas, we proposed an explainability classification method that considers both the design and behavior of the models. As Table 1 shows, we utilize this method to classify 191 research into five categories: low, medium-low, medium, medium-high, and high.\nFirst, we consider the difference between features extracted by neural networks and the form of information processed by symbolic logic. This perspective focuses on the compatibility and conversion mechanism between these two diagrams. In addition to the explainability of neural network itself, the format of this conversion also affects the explainability of integrated models, contributing to the first standard for categorization, which measures the readability of intermediate representation that bridges the neural representation and the logical symbolic representation. Second, we consider the explainability of decision-making or prediction logic in neuro-symbolic AI models. Specifically, even considering the inevitable impact of the black-box processing of neural networks, we still can understand the essence of knowledge-processing methods to various degrees, which will help us to explain the decision-making or prediction to a certain degree. Based on the above conditions, our specific classification method is shown in Figure 1."}, {"title": "3.1 Implicit Intermediate Representations and Implicit Decision Making(prediction)", "content": "This category includes 74 neuro-symbolic AI studies with three common characteristics, as Table 2 shows. First, they all use neural networks to extract features from data. However, the representations of these features cannot be directly processed by symbolic logic, so they need an intermediate representation to fill the gap. Secondly, the intermediate representation bridging the two is often latent vector embedding or combined with structured representation but is only partially explicit and directly human-readable. Third, most of the overall decision-making logic or prediction method is implicitly expressed through the weights and activation functions of the neural network. Some methods directly integrate symbolic logic into the decision-making process or provide an indirect understanding of decision-making logic by designing interpretable interfaces such as attention mechanisms and logic rule generators. However, decision-making logic still needs to be explained overall."}, {"title": "3.2 Partially Explicit Intermediate Representations and Partially Explicit Decision Making(prediction)", "content": "As Table 3 shows, this category involves 110 neuro-symbolic AI studies, and they have three common characteristics. First, they all use neural networks to extract features from data. However, the representations of these embeddings cannot be directly processed by symbolic logic, so they need an intermediate representation to fill the gap. Secondly, most intermediate representations are symbolic logic expressions, mathematical expressions, structured programs, logic circuits, probability distributions, virtual circuits, and virtual machine instructions. These representations are partially explicit and human-readable. Third, decision logic combines implicit representations from neural networks and explicit representations of symbolic logic, so it is partially explicit and readable.\nPetersen et al. [143] proposed a DSR(Deep Symbolic Regression) method to recover mathematical expressions from data. This method first represents each expression as a node sequence through symbolic expression trees, including mathematical operators such as addition, subtraction, multiplication, and division and operands such as x and constants. A RNN(Recurrent Neural Network) is then used to predict the next operator or operand based on the existing sequence. In this method, RNN predicts new nodes based on existing expression trees, and logical symbols exist as expression trees of mathematical expressions. The tree structure represents the logical relationship between operators and operands, such as which two numbers are added together or which number is divided by another number. Each time the RNN generates a complete expression, the expression is used to calculate the degree of fit on a specific data set, such as by calculating the difference between the expression and the actual data, and the fitting is feedback to the RNN to guide the adjustment of the subsequent expression generation process. In DSR, the weights, activation functions, and currently processed sequences inside the RNN act as an implicit intermediate representation, although this representation is not directly oriented to the end user, nor is it in any explicit or structured way. The symbolic form exists, but it connects the input sequence and the output prediction and is finally output explicitly in the form of a specific symbol in a"}, {"title": "3.3 Explicit Intermediate Representations or Explicit Decision Making(prediction)", "content": "There are 3 neuro-symbolic AI studies in this category, as Table 4 shows, and all of which have three characteristics. First, neural networks are used to extract features from data. However, the representations of these features cannot be directly processed by symbolic logic, so they must use intermediate representations to fill the gap. Second, either intermediate representations or overall decision logic is entirely explicit.\nJiang et al. [102] proposed an entity link prediction method based on LNN, LNN-EL(Logical Neural Network-Entity linking) to solve the entity linking problem in short texts, detailed in the multimodal non-heterogeneous Neuro-Symbolic Al section in Part I of this article. This method converts a set of logical rules into the network structure of LNN in LNN. It accurately matches the mentioned entities by processing features extracted from text or knowledge graph data. As mentioned in Part I 3.4, LNN maps logical operations directly into neural networks. Its neural network and symbolic logic operate the same data type in the same representation space without an intermediate representation to bridge the two. At the same time, the activation state of each neuron or neuron group can directly correspond to the truth value state of a logical proposition, so the decision-making process of logical operations is more accessible to explain. However, for LNN-EL, using deep models in the feature extraction stage results in the intermediate representation between its output and the LNN input being only partially explicit. Therefore, even if the LNN network structure is used in the inference stage, it is only moderately interpretable."}, {"title": "3.4 Explicit Intermediate Representations and Explicit Decision Making(prediction)", "content": "There is 1 neuro-symbolic AI study in this category, which has three characteristics. The most significant difference between this classification and the previous one is that the intermediate representation and the overall decision logic are explicit. However, an intermediate representation is still needed to fill the gap between extracted features and symbolic processing.\nKimura et al. [108] et al. (2021) proposed a neural symbolic framework to solve reinforcement learning problems in text-based games. This method first extracts basic propositional logic from text observations obtained in the environment through a semantic parser, converting natural language text into a logical expression form. Then, external knowledge bases such as ConceptNet are used to understand the semantic categories of words in the text and refine the extracted propositional logic. Finally, the extracted propositional logic and the lexical category information obtained from ConceptNet are combined through the FOL(First Order Logic) converter and converted into first-order logical facts that reflect the specific characteristics and conditions of the game state. These logical facts are subsequently used as"}, {"title": "3.5 Unified Representation and Explicit Decision Making(prediction)", "content": "There are 3 Neuro-Symbolic Al studies in this category, with three commonalities. First, although they use neural networks to obtain features, the neural network's output maintains the exact representation that can be processed by symbolic logic. Second, the overall decision logic is fully explicit and interpretable.\nRiegel et al. [149] proposed a LNN(logical neural network). As we mentioned in Section 3.4 of Part I, LNN directly interprets and operates logical operations by mapping each of its neurons to elements in the logical formula, which means that when the model processes information or makes decisions, its calculation process can be equivalent to performing a series of logical judgments, in which the output of each neuron not only represents the truth value of a logical proposition, and can also reflect how this truth value is derived from the input through logical operations, which is also the source of its high interpretability. In addition, each logical proposition in LNN is assigned a truth value range, and the uncertainty of the truth value of the proposition is captured and expressed through upper and lower bounds. For example, when faced with logical contradictions and incomplete knowledge, LNN will increase the uncertainty of the authenticity of certain propositions by expanding the truth value range of the proposition and finding a truth range adjustment solution that minimizes the contradiction. Similarly, when there is not enough information to determine the truth value of a proposition, LNN can reflect the uncertainty of the proposition by giving it a broader truth value range and dynamically adjust the true range as information is obtained to adapt to complex and dynamic changing information environment. Finally, unlike traditional single-task neural networks, LNN can simultaneously perform various logical reasoning tasks, such as theorem proving and fact derivation, demonstrating its high flexibility in logical processing and future development potential.\nSen et al. [160] proposed an LNN-based inductive logic programming method that learns interpretive rules from noisy, real-world data, which can generate interpretable logic rules as output based on structured input data. This method first uses a knowledge base containing facts and relations, as well as rules describing the form or structure of the target as input. Then, an LNN network is built based on the template to simulate logical connectives, where each node or neuron represents an expression or a logical rule combined with logical connectives. Because LNN allows the behavior of logical operations to be adjusted through the learning process, the facts in the knowledge base can be used as training data, and the parameters in the LNN can be correctly reflected through optimization algorithms such as back propagation and gradient descent, so that it can correctly reflect the logic relation between facts. Finally, the trained LNN can be converted into a set of logical rules that directly reflect the logical relationships in the input data. It can be used to reason, predict, or explain patterns in the data and is interpretable. This method applies LNN in the ILP direction, allowing it to handle the complexity and uncertainty in real-world data while improving learning efficiency and the quality of rules."}, {"title": "4 TRENDS", "content": "This study used the keywords 'neuro-symbolic', 'neuro symbolic', and 'neuro symbolic learning' to survey relevant research from 2014 to the present on Google Scholar and Research Gate. Although the search was conducted to the greatest extent, it is possible that some literature could not be included due to limitations. Therefore, the trend comparison below mainly aims to illustrate the evolution of research trends rather than provide an accurate total literature volume. According to the scope of this study, several apparent trends can be seen from 2014 to Feb of 2024: First, the number of papers has increased significantly, especially between 2020 and 2023; the number of papers published is on an upward trend, and its publication in 2023 has the most significant number of papers, 55 in total based on presenting data. These numbers reflect the primary growth in research interest and activity in neural symbolic systems. Secondly, among current neuro-symbolic methods, images and text are the most common input data types, reflecting their ubiquity and importance in studying neuro-symbolic systems.\nThere are relatively few explorations into numerical and mathematical expression processing, structured data processing, environment and state awareness, and multimodal data types. However, as Figure 1 shows, the number of neuro-symbolic studies on these four input types has continued growing since 2016.\nIn addition, the vast majority of research used unimodal and non-heterogeneous representation space, which shows that in neuro-symbolic system research, a single type of data, such as text, image, or structured data, is still more common in research objects. Only a few papers explore the representation space of unimodal heterogeneous, multimodal non-heterogeneous, and multimodal heterogeneous, showing that these directions are relatively new and may be future growth points for research.\nFinally, the most significant papers had medium-low explainability and low explainability. This figure indicates that most research results in this area still need to be higher in explainability. Only a few research have achieved medium, medium-high, and high explainability, indicating that there are still significant challenges in improving the explainability of neural symbolic systems in the future."}, {"title": "5 MAJOR RESEARCH CHALLENGES", "content": "Despite the benefits, designing an ideal unified representation still faces many obstacles. An ideal representation could capture the structural properties of symbolic logic while maintaining the essential patterns of the data. Such designs require a solid understanding of the data distribution and its latent relationship with the logical entities. For"}, {"title": "5.1 Unified Representations in Neural Networks and Symbolic Logic", "content": "The conversion of representation between neural networks and symbolic logic has constantly challenged neuro-symbolic learning. In the two most common situations of neuro-symbolic AI models, neural networks are utilized to augment the extraction of symbolic logical features by interpreting complex patterns or data structures, which enhances the effectiveness of traditional symbolic methods, or symbolic knowledge embedding is applied to provide rule constraints or reasoning logic for neural networks. However, these cooperations are usually optimized for specific tasks, which are difficult to adapt or transfer to new tasks or data sets. Therefore, retraining the neural network model or adjusting the symbolic logic rules are necessary when requirements change, which hints that specific combinations constrain the model's overall generalization ability. Alternatively, both diagrams can directly utilize the extracted features or learned knowledge when neural networks and symbolic logic modules use unified representation, bringing additional benefits, such as improving training and inference efficiency.\nIn addition, the two methods of cooperation between neural networks and symbolic logic mentioned above are synchronization processes of knowledge, except that they are inefficient and offline. However, applying the unified representation will make this synchronization more efficient. Specifically, this consistency avoids additional knowledge transformation steps and corresponding information loss, which can make the system more flexible and efficient, reducing reliance on large amounts of offline training data while decreasing the complexity of model update and maintenance in handling complex practical application problems such as multi-step reasoning tasks in natural language or image recognition."}, {"title": "5.2 Explainability and Transparency", "content": "Neural networks introduce inescapable black-box features and inferences in cooperation with symbolic learning. For the case where they are loosely coupled, the explainability of the neural network cannot get any improvement because even if logical symbols provide rules or constraints for the neural network as embedded vectors, the embedding process itself is not intuitive and requires the addition of complex logical symbol reasoning, and the increased complexity brought about by the interaction between the two. Alternatively, their semantic overlap can form at least partly complementary in explainability when neural networks and symbolic logic utilize the unified representation."}, {"title": "5.3 Sufficient Cooperation", "content": "The current integration of neural networks and symbolic logic makes it difficult to avoid the intrinsic problems in both diagrams. For instance, the inexplicable inference, the training cost of neural networks, or the expression limitations and generalization problems of symbolic logic could be introduced to the integrated neural-symbolic model. System complexity and knowledge synchronization may all become new issues. One promising avenue for addressing these challenges is to develop a new model architecture. This architecture would apply an integration layer for the outputs of the neural network component and the symbolic logic component, potentially overcoming the limitations of the current integration. An elastic two-way learning mechanism could be utilized to synchronize their knowledge. However, it is crucial to consider explainability from the outset of the design process."}, {"title": "6 FUTURE RESEARCH DIRECTIONS", "content": "In Part II of the review, we classify recent neuro-symbolic AI studies into five categories based on information diversity in processing and integrating, as well as the characteristics and capabilities of the representation space in expressing neural network and symbolic logic: unimodal non-heterogeneous, multi-modal non-heterogeneous, single-modal heterogeneous, multi-modal heterogeneous and dynamic adaptive neuro-symbolic AI. This classification method is a good starting point in understanding and characterizing neuro-symbolic Al's performance from the representation"}, {"title": "6.1 Unified Representations", "content": "perspective. Therefore, unified representation may be one of the key directions in future breakthroughs, as it minimizes information loss and maximizes knowledge that represents efficiency for both diagrams.\nIn the meantime, representation space is another promising research direction. Most current neuro-symbolic methods use Euclidean space as the representation space. However, dealing with non-linear problems such as complex relation-ships, graph-structured data, and timing dependencies is difficult and may not be as efficient as non-Euclidean space. Euclidean space is just a case of non-Euclidean space. Although the latter still needs much exploration regarding specific design methods and combination with European space, it still provides valuable options for future neuro-symbolic AI research."}, {"title": "6.2 Enhancing Model Explainability", "content": "The explainability of models has become an unavoidable challenge in artificial intelligence research. Although the neuro-symbolic Al method provides more substantial transparency than traditional AI to a certain extent, it still cannot meet the requirements for its application in critical fields. Explainability should be considered more during the design phase rather than an afterthought. However, considering the complexity of explainability, we must first establish the basis for explainability. For example, mathematics and physical laws can be regarded as correct standards to a certain extent, while human common-sense logic may be full of contradictions and logical fallacies. Similarly, explainability must be based on a relatively stable concept in neuro-symbolic AI to be more convincible. Therefore, verifying and updating knowledge in LLMs is also an open topic.\nThe explainability requirements for Neuro-Symbolic AI are essentially divided into two parts: process and result transparency. The former may be based on rigorous logic or formulaic arguments, which means that even if a neural network is used to generate symbols for logical reasoning, this process should be transparent and interpretable enough to verify correctness. The latter shows that some unique thinking habits should also be considered, such as common sense in providing contextual evidence for reasoning results.\nAlthough it is unclear and has obstacles to understanding what is happening in the model, advances in computational neuroscience can still provide some inspiration for feature directions. For example, Casta\u00f1eda et al. [38] found that human deductive reasoning and probabilistic reasoning processes rely on different neurocognitive mechanisms in the brain, and people can suppress prior knowledge for deductive reasoning according to task requirements. Not all reasoning processes can be attributed to probability. These insights may imply that our prior knowledge of deductive reasoning should be tailored to the specific situation. Trumpp et al. [180] found that activity in sensory and motor areas during conceptual processing can also occur unconsciously. Popp et al. [146] also found that words related to actions and sounds activate sensory and motor areas of the brain during conceptual processing. This result shows that the acquisition of conceptual knowledge relies on reproducing sensory and motor brain networks, which supports the view of ground cognitive theory. Designing neuro-symbolic AI with a stable working or knowledge memory structure may inspire brain function. Belekou et al. [26] found significant differences in brain activation patterns when processing paradox reasoning and deductive reasoning tasks. Coetzee et al. [51] found that logical reasoning in the adult brain may be separated from language processing. This research may inspire us to consider more flexible reasoning paths in neuro-symbolic AI or even dynamically configurable task reasoning methods."}, {"title": "6.3 Ethical Considerations and Social Impact", "content": "Ethical and social considerations are another inescapable problem. If a high portion of our content will come from generative Al in the future, then the significance of this content will be far beyond the scope of being measured by"}]}