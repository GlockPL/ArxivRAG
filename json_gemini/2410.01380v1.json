{"title": "KNOWLEDGE ENTROPY DECAY DURING LANGUAGE MODEL\nPRETRAINING HINDERS NEW KNOWLEDGE ACQUISITION", "authors": ["Jiyeon Kim", "Hyunji Lee", "Hyowon Cho", "Joel Jang", "Hyeonbin Hwang", "Seungpil Won", "Youbin Ahn", "Dohaeng Lee", "Minjoon Seo"], "abstract": "In this work, we investigate how a model's tendency to broadly integrate its parametric\nknowledge evolves throughout pretraining, and how this behavior affects overall perfor-\nmance, particularly in terms of knowledge acquisition and forgetting. We introduce the\nconcept of knowledge entropy, which quantifies the range of memory sources the model\nengages with; high knowledge entropy indicates that the model utilizes a wide range of\nmemory sources, while low knowledge entropy suggests reliance on specific sources with\ngreater certainty. Our analysis reveals a consistent decline in knowledge entropy as pre-\ntraining advances. We also find that the decline is closely associated with a reduction\nin the model's ability to acquire and retain knowledge, leading us to conclude that di-\nminishing knowledge entropy (smaller number of active memory sources) impairs the\nmodel's knowledge acquisition and retention capabilities. We find further support for this\nby demonstrating that increasing the activity of inactive memory sources enhances the\nmodel's capacity for knowledge acquisition and retention.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent works have analyzed how language models store world knowledge in their parameters and utilize\nthis knowledge to generate responses during inference time (Geva et al., 2021; Dai et al., 2022a;b; Meng\net al., 2022; Yao et al., 2024). However, little is known about how their behavior of integrating various\nfactual knowledge embedded in their parameters changes throughout the pretraining stage. In this work,\nwe perform deep analysis into how a model's property of broadly integrating diverse parametric knowledge\nevolves throughout pretraining and how these shifts affect overall performance, particularly in terms of\nknowledge acquisition and forgetting in a continual learning setup. We hypothesize that this varying level of\nintegration may explain why models in the later stage of pretraining encounter challenges in acquiring new\nknowledge (Dohare et al., 2024; Jang et al., 2022; Chang et al., 2024).\nWe introduce knowledge entropy, which reflects how a language model integrates various knowledge sources,\nto investigate how this behavior evolves throughout pretraining. Recent studies have shown that feed-forward\nlayers (FFNs) serve as a key-value memory (Geva et al., 2022; 2021; Dai et al., 2022a). Building on this\nresearch, as shown in Figure 1, we view the second projection matrix V as a memory, composed of memory"}, {"title": "2 RELATED WORKS", "content": "Dynamics of Knowledge in Language Models Recent studies have shown that language models embed\nworld knowledge within their parameters and integrate this knowledge to generate responses (Yang, 2024;\nPetroni et al., 2019; Wang et al., 2021). Thereby, various research aims to understand these dynamics\nof knowledge in language models (how they learn, store, and engage their parametric knowledge) during\ninference and training phases. Several works have focused on investigating the inference process: Geva et al.\n(2023) analyzes the role of different layers in language models. Allen-Zhu & Li (2024b) demonstrates that\nmodel parameters have a limited knowledge capacity. Some studies suggest key-value memory (Geva et al.,\n2021; Meng et al., 2022; Dai et al., 2022a). Other research focuses on the pretraining phase. Chang et al.\n(2024) explores how language models acquire knowledge. Liu et al. (2021) studies the sequence that language\nmodels learn various types of knowledge. Allen-Zhu & Li (2024a) examines strategies to enhance knowledge\nstorage and extraction. Teehan et al. analyzes internal structural changes. Sun & Dredze (2024) investigates\nthe interaction between pretraining and finetuning. Our work is the first to explore how the behavior of\nlanguage models in integrating their knowledge evolves throughout the pretraining phase, and to analyze\nhow these changes affect model performance in terms of knowledge acquisition and forgetting in continual\nknowledge learning.\nEntropy in Natural Language Processing In information theory, entropy quantifies the value of infor-\nmation, where predictable (certain) events have low entropy and unpredictable (uncertain) events have high\nentropy (Lairez, 2022; Majenz, 2018). In natural language processing (NLP), entropy is used in various\nways to measure the certainty of language models. Yang (2024) analyzes the entropy of model outputs based\non input prompts. Araujo et al. (2022) calculates the entropy of outputs at each layer to determine weight\nadjustments in a continual learning setup. Other works focus on token probability entropy to understand the\ninformation required to predict the next word in a sequence (Vazhentsev et al.; Geng et al., 2024; Malinin &\nGales, 2021). Lower entropy in a model's predictions may indicate that the model has become more certain\nabout its predictions based on training data. Additionally, Kumar & Sarawagi (2019) measures entropy over\nthe cross-attention layer to assess the uncertainty in the attention layer of encoder-decoder models. The\nentropy proposed in our paper, knowledge entropy, differs from previous works in that it focuses on the\nentropy of a model's parametric knowledge, assessing the uncertainty or variability in utilizing the knowledge\nencoded within the language model."}, {"title": "3 KNOWLEDGE ENTROPY", "content": "In this section, we introduce knowledge entropy (Section 3.1) to examine how broadly the model integrates its\nparametric knowledge and an experiment setup to measure it (Section 3.2). Next, in Section 3.3, we measure\nknowledge entropy at various pretraining stages to analyze how the model's knowledge integration behavior\nevolves over pretraining. In Section 3.4, we extend our investigation by exploring alternative definitions of\nentropy.\n3.1 DEFINITION\nIn this work, we introduce a new concept, knowledge entropy, to analyze the scope of a model's access\npatterns to its parametric knowledge. Low knowledge entropy suggests that the model relies more on a\nnarrower set of specific knowledge sources with high certainty whereas high knowledge entropy indicates\nthat the model integrates with a diverse range of knowledge sources. Inspired by prior works that consider\nfeed-forward layers (FFNs) as key-value memory containing model's parametric memory (Geva et al., 2021;\nDai et al., 2022a; Meng et al., 2022; Dong et al., 2022), we consider the knowledge source to be the memory\nvectors, which is the second projection matrix of FFN, and measurement of how broadly the model integrates\nas memory coefficients, calculated by the first projection matrix.\nGeva et al. (2021) propose the concept of key-value memory, demonstrating that FFNs function similarly to\nthe key-value neural memories (Sukhbaatar et al., 2015). The feed-forward layer consists of two projection\nlayers and activation in the middle:\n$FFN(x) = f(x \\cdot K^T) \\cdot V$ \nwhere x \u2208 Rd. The first projection matrix (K \u2208 Rm\u00d7d) corresponds to the keys, and the second projection\nmatrix (V \u2208 Rm\u00d7d) represents the values, or the memories comprised of memory vectors. The output,\nFFN(x), is a linear combination of the memory vectors vi=1,\u2026,m \u2208 Rd which are the rows of V, where\nthe coefficients C are determined by f (x \u00b7 KT), with f being a non-linear activation function such as ReLU.\nPrevious studies have shown that various types of factual and linguistic knowledge are encoded within these\nmemories (Dai et al., 2022a; Geva et al., 2022; Meng et al., 2022; Dong et al., 2022). Thus, the final output is\ngenerated by combining the contributions of these memory vectors, where the memory coefficients determine\nthe combination.\nThereby knowledge entropy, H(\u03b8), is calculated by the sum of layer-wise entropy H(\u03b8\u00b9), which is based on\nthe average coefficient Cl \u2208 Rm averaged across all tokens in dataset D, as described in Equation 2.\n$C^l = \\frac{1}{|D|} \\sum_{n=1}^{|D|} (\\frac{1}{T_n} \\sum_{j=1}^{T_n} C_{n,j}^l)$  ,  $prob(c_i^l) = \\frac{\\sum_{k=1}^{m} c_k^l}{\\sum_{i=1}^{m} c_i^l}$ for i = 1, 2, ..., m\n$H(\u03b8^l) = - \\sum_{i=1}^{m} prob(c_i^l) \\cdot log(prob(c_i^l));$   $H(\u03b8) = \\sum_{l=1}^{L} H(\u03b8^l)$ \nCh represents the coefficient of j-th token position of n-th instance at layer 1, \u00bf indicates an i-th element\nfrom Cl, Tn is the sequence length of the n-th instance in the dataset D, m is the inner dimension of\nfeed-forward layer, and L denotes the number of layers in the model."}, {"title": "3.2 EXPERIMENT SETUP", "content": "To conduct the experiment, we use the OLMO (Groeneveld et al., 2024) models (1B and 7B), which are\nopen-source large language models with intermediate pretraining checkpoints released, trained on the Dolma\ndataset (Soldaini et al., 2024). To measure knowledge entropy, we use a subset of Dolma, 2k instances\nthat appear in the first batch within the official pretraining data order to ensure that all models we are using\nhave seen the corpus during pretraining step. Please note that the trend persists across other corpora as\nwell (Figure 6 in Appendix A.1); however, since we are analyzing the model's behavior throughout training,\nwe define knowledge entropy based on calculations using the training dataset.\nn.j\nIn the case of OLMo, the memory coefficient C is calculated as C) = abs(SwiGLU(x)) where xj is the\nj-th token of input x and SwiGLU (Shazeer, 2020) is the activation function. We apply the absolute value\nsince the SwiGLU allows negative values and the magnitude determines the contribution of the corresponding\nmemory vector in the linear combination. Then, the absolute values are converted into probability distribution.\nWe also experiment that the trend persists with different choices of activation functions. Further details\nregarding knowledge entropy can be found in Appendix A.1."}, {"title": "3.3 FINAL MODELS TEND TO EXHIBIT LOWER KNOWLEDGE ENTROPY", "content": "Figure 2 illustrates how knowledge entropy (y-axis) changes across different stages of pretraining (x-axis).\nThe results show a consistent decrease in knowledge entropy as pretraining progresses in both 1B and\n7B models. This trend suggests that models in the later stage of pretraining tends to engage with a narrower\nrange of memories, relying more heavily on specific memory vectors rather than accessing and integrating\nknowledge from a broader range of memories. Consistent reduction in knowledge entropy is observed across\nall layers, with the most significant reduction occuring in the last layer, which closely resembles the output\ndistribution right before the token prediction (Figure 8 in Appendix)."}, {"title": "3.4 SIMILAR TRENDS ARE OBSERVED BY DIFFERENT DEFINITIONS OF ENTROPY", "content": "While our work defines knowledge entropy focusing on the feed-forward layer, previous studies have exam-\nined entropy in different contexts, such as the entropy of attention (Kumar & Sarawagi, 2019) and the entropy"}, {"title": "4 KNOWLEDGE ACQUISITION AND FORGETTING", "content": "We hypothesize that the reduction of knowledge entropy as pretraining progresses impacts the model's\nknowledge acquisition and forgetting as low knowledge entropy indicates sparse activation of memory\nvectors, thus the vectors are likely to be consistently overwritten when new knowledge is introduced. To test\nthis hypothesis, we measure knowledge acquisition and forgetting using checkpoints from different stages of\npretraining in a continual knowledge learning setup (Jang et al., 2022; Wu et al., 2023), where further training\nis performed on new-domain corpora by next token prediction to inject new knowledge to the pretrained\nmodels. Section 4.1 details the experimental setup and the metrics used. In Section 4.2, we present the\nresults of knowledge acquisition and forgetting across various pretraining stages. Section 4.3 further explores\nwhether a relationship exists between the two behaviors: activating the inactive memory vectors increases the\nknowledge acquisition ability."}, {"title": "4.1 EXPERIMENT SETUP", "content": "Model & Hyperparameters We experiment using intermediate checkpoints from OLMo. Hyperparame-\nters are chosen following previous research on continual knowledge learning (Jang et al., 2022; Kim et al.,\n2023) and we test various combinations to assess their generalizability. For batch size, we test 128 and 2048;\nfor learning rate, we experiment with 1e-4, 4e-4, and le-3. We also investigate the effect of training duration\nby comparing a single epoch to three epochs. Among these configurations, we focus primarily on batch size\n128, learning rate 4e-4, and single epoch training as this setup most closely aligns with continual knowledge\nlearning.\nDataset We experiment on a subset of two datasets: PubMed , a corpus of bio-medical and life science\ntopics with abstracts, and C4 (Raffel et al., 2020), a large-scale corpus comprising diverse text data gathered\nfrom web pages. We use PubMed as the primary dataset as it contains more new knowledge, making it a"}, {"title": "4.2 KNOWLEDGE ACQUISITION AND RETENTION DECREASES ACROSS PRETRAINING STAGE", "content": "Figure 4a shows the performance of OLMo 1B and 7B models 10 corpus from various stages of pretraining as\nan initial state. We observe that models in the final stage of pretraining struggle more with acquiring\nnew knowledge A(0) and exhibit greater forgetting F(0). As shown in Figure 4b, continually training\nthe models at the mid-point of the pretraining as the initial checkpoint tends to yield the best performance\nin knowledge probing and downstream tasks compared to both models from the initial and final stage of\npretraining. While early-stage models demonstrate high knowledge acquisition with minimal forgetting, their\noverall performance is limited by weaker language modeling capabilities. Conversely, later-stage models,\ndespite being trained on larger datasets, exhibit lower knowledge acquisition and higher rates of forgetting,\nresulting in lower overall performance compared to the mid-stage models. This aligns with previous research\nsuggesting that a model in the final stage of pretraining tends to struggle when learning new knowledge,\nshowing a tradeoff between plasticity and stability (Dohare et al., 2024; Biesialska et al., 2020; Jang et al.,\n2022). Therefore, we suggest that using a mid-stage checkpoint strikes a good balance, making it a practical\nchoice as an initial starting point for further training to inject new knowledge.\nWe consistently observe this pattern of later-stage models underperforming compared to earlier-stage models\nacross various hyperparameter settings, including batch size, learning rate, training corpus, and the number of\nepochs. A detailed analysis of these results is provided in Appendix B.4."}, {"title": "4.3 RESUSCITATING INACTIVE MEMORY VECTORS INCREASES KNOWLEDGE ACQUISITION", "content": "We observe a strong correlation between the trend of knowledge entropy (Figure 2) and the model's ability\nto acquire and retain knowledge (Figure 4a). We assume that the model's increasing reliance on a limited set\nof memory vectors (decrease in knowledge entropy) leads to more frequent updates to these vectors, making\nit difficult to acquire new knowledge and resulting in a higher rate of forgetting. To test this assumption, we\nconduct experiments where we artificially increase the activity or resuscitate previously inactive memory\nvectors."}, {"title": "Algorithm 1 Resuscitating Low Memory Coefficients", "content": "Require: C (average coefficients), p (resuscitation ratio), q (amplifying factor), K (up-projection matrix)\nEnsure: Scaled up-projection matrix K using computed multiplier m\n1: for each layer l in K do\n2:\nExtract average activations for layer l:\n$C^l = C^l[l]$\n3:\nCompute the threshold t for the lowest p% activations:\n$t = quant(C, p)$\n4:\nIdentify positions of values below the threshold t:\n$idx = (C < t).nonzero()$\n5:\nCompute the scaling factor multiplier m for coefficients in layer l:\n$m = \\frac{mean(C)}{C} \\times q$\n6:\nApply scaling to the up-projection weights K\u012b at the identified positions:\n$K_l[idx,:] \\times= m[idx]$\n7: end for\nTo resuscitate inactive memory vectors, we modify the up-projection matrix K which engages with producing\nmemory coefficients C (notations from Equation 2). Specifically, as shown in Algorithm 1, we identify the\nlowest p% (resuscitation ratio) of memory coefficients and apply a multiplier m to parameters in K that\nare associated with these p%. Multiplier m can be any numbers; in this experiment, we divide the mean\ncoefficient value of each layer by the respective coefficient valued at each identified position i at layer l,\nand then multiply the result by an amplifying factor q. By varying the value of q, we control the degree of\nresuscitation applied to the p% low-activation coefficients, thereby influencing the magnitude of the average\ncoefficient and corresponding size of the parameter updates.\nFigure 5a shows the knowledge acquisition and forgetting rates and Figure 5b presents the knowledge probe\nand downstream task performance after continual learning with various resuscitation configurations. For the\nexperiment, we fix p to 50 with varying q and use the OLMo checkpoint at the last step of pretraining. Results\nshow that when q is set to 1 or greater, it generally yields better performance in both knowledge acquisition\nand retention compared to the original model. In contrast, when q is set to 0.5, which further reduces\nalready inactive memory coefficients, both acquisition and retention declines suggesting that concentrating\nparameter updates more heavily on already active locations led to sparser updates, ultimately impairing overall\nperformance. These results suggest that having a narrower active memory vector (low knowledge entropy)\ntends to reduce the model's capacity to acquire new knowledge and increases knowledge forgetting.\nFurther experiments with fixed q and varying p shows that increasing p to activate a larger portion of inactive\nparameters generally led to improved performance. Detailed result of this configuration is in Appendix B.5.\nWe also analyze how the result changes when using models from different stages of pretraining as the original\nmodel. We could see that the trend persists over different checkpoints from the final-stage of pretraining\n(554k). However, the effect of the resuscitation becomes more pronounced as the original model progresses\nto later stages of pretraining. Detailed results are in Appendix B.6.\nOur result indicates that resuscitating inactive memory vectors of final-stage models tends to enhance\nknowledge acquisition and overall performance than the final-stage model alone. However, we observe that the"}, {"title": "5 CONCLUSION", "content": "In this work, we examine how large language models' ability to broadly integrate their parametric knowledge\n(measured by knowledge entropy) changes throughout pretraining and how these changes affect knowledge\nacquisition and forgetting in a continual learning setup. Our findings reveal a strong correlation between\nknowledge entropy and the model's capacity to acquire and retain knowledge. Models in the final stages\nof pretraining tend to exhibit narrower integration of memory vectors, leading to lower knowledge entropy,\nwhich negatively impacts both knowledge acquisition and retention. Interestingly, we could see that artificially\nincreasing knowledge entropy by modifying the parameters of final-stage models tends to improve these\ncapabilities. Based on our analysis, we suggest that models from the mid-stage of pretraining offer a good\nbalance between knowledge acquisition, retention, and overall performance, making them a good choice for\nfurther training to introduce new knowledge."}, {"title": "6 LIMITATION & FUTURE WORK", "content": "Due to computational constraints, our study measures knowledge acquisition and forgetting in a continual\nlearning setup. Future work could explore whether these behaviors also occur during the pretraining phase.\nWe focused on OLMo 1B and 7B models, as they are the only models that publicly provide intermediate\npretraining checkpoints and demonstrate strong performance (Sun & Dredze, 2024; Chang et al., 2024).\nExtending this investigation to other models would be a valuable direction for further research. Our re-\nsuscitation method, which arbitrarily modifies model parameters to test our hypothesis, showed promising\nresults in improving knowledge acquisition and retention. However, performance tended to decline when\nresuscitating models in their initial or mid-stages. This suggests that more refined methods for resuscitating\nmodel parameters\u2014ones that avoid random modification and preserve language modeling capabilities-could\nyield better outcomes. Additionally, while we observed that models in the mid-stage of pretraining strike\na good balance for further training on tasks that involve acquiring new knowledge, defining the mid-point\nprecisely remains an open question. In this study, we approximated the mid-point as 50% of the learning rate\nschedule."}, {"title": "A KNOWLEDGE ENTROPY", "content": "A.1\nKNOWLEDGE ENTROPY\nDoes the choice of dataset change the trend? As expressed in Equation 2, knowledge entropy is dependent\non the dataset D. We define D as the dataset used during pretraining, as knowledge entropy reflects how the\nmodel integrates the knowledge stored in its memory vectors, learned during pretraining. However, to further\nexplore whether the choice of dataset influences the trend of knowledge entropy, we measure it using PubMed\nand C4. Figure 6 shows that the trend remains consistent regardless of the dataset used when calculating\nknowledge entropy.\nDoes the choice of activation function change the trend? We also explored an alternative where we do\nnot take the absolute value of the SwiGLU output. Instead, following the ReLU function (Agarap, 2018),\nanother widely used activation function, we replaced all negative values with 0. Figure 7 shows that the trend\nremains consistent even under this modification.\n$C_{n,j}^{l} = C_{n,j}^{l} = ReLU(gate(x_j)) \\& up(x_j)$,\nLayer-wise Knowledge Entropy Figure 8 shows how knowledge entropy changes during pretraining by\nlayer. Knowledge entropy consistently decreases in every layer, with the most significant reduction occuring\nin the last layer, which closely resembles the output distribution right before the token prediction. OLMo-7B\nalso shows similar trend with 1B model."}, {"title": "A.2 ENTROPY OF ATTENTION LAYERS", "content": "Inspired by previous research that emphasizes attention layer's role of attribute extraction (Geva et al., 2023),\nwe also measure attention entropy H(\u03b8att) similarly to Kumar & Sarawagi (2019)12. Attention weights,\noutput of softmax normalization after key-query-value operation, can be interpreted as weight assigned\nto the previous tokens. As attention weight for each token position in each attention head is probability"}, {"title": "A.3 ENTROPY OF NEXT TOKEN PREDICTION", "content": "The entropy of next token prediction (Vazhentsev et al.; Geng et al., 2024; Malinin & Gales, 2021) is defined\nas (0) = - \u03a31 Pi.log(pi), where pi represents the probability of the i-th token. This is then averaged\nover the sequence length (Tm) and the dataset size (D)."}, {"title": "B KNOWLEDGE ACQUISITION AND FORGETTING", "content": "B.1 DATASETS\nTraining Dataset for Continual Knowledge Learning In this section, we share a brief description over\nthe datasets we used. For continual knowledge learning, we experiment over PubMed and C4. The PubMed\ndataset consists of biomedical literature abstracts from the PubMed database, containing articles across a\nwide range of topics in medicine and biology. The C4 (Colossal Clean Crawled Corpus) dataset (Raffel"}, {"title": "Evaluation Dataset to measure Knowledge Acquisition", "content": "To measure knowledge acquisition of language\nmodel, we use the fictional knowledge dataset (Chang et al., 2024), which is designed to assess how well\nLLMs acquire factual knowledge during pretraining. This dataset includes 130 paragraphs 13 with fictional\nyet realistic entities (injected knowledge) and 1,950 probes, with three different acquisition depths across five\ndifferent questions for each knowledge instance, resulting in a total of 15 different probes for each sequence\nin the corpus. The injected knowledge is incorporated into the training corpus during continual learning, with\nupdates occurring every 160 steps.\nFollowing Chang et al. (2024), we divide the 130 corpora into two settings: paraphrase and once. In the\nparaphrase setting, 70 instances are each paraphrased 10 times. For every 160 steps, one paraphrased version\nof an instance is added to the training corpus, repeating this process 10 times14. In the once setting, each\ninstance is presented only once throughout the entire continual learning process. The 60 instances are divided\ninto 10 groups, with 6 instances added every 160 steps."}, {"title": "Evaluation Dataset to measure Knowledge Forgetting", "content": "To measure forgetting rate, we evaluate over 6\ndownstream datasets."}, {"title": "B.4 KNOWLEDGE ACQUISITION & FORGETTING", "content": "B.4.1 BASELINE SETUP\nOur base experiements are reported with hyperparameter configuration most closely aligned with continual\nknowledge learning studies, specifically with batch size 128, learning rate 4e-4, while training single-epoch"}, {"title": "RESUSCITATION EXPERIMENT: VARYING p WHILE FIXING q", "content": "Figure 13 shows the overall performance when q was fixed at 1, meaning that the lowest p% of coefficients\nwere multiplied to converge toward the layer's mean. It is shown that increasing p to activate a larger portion\nof inactive parameters generally led to improved performance. However, interestingly, when p was set too"}, {"title": "B.6 RESUSCITATION EXPERIMENT ACROSS PRETRAINING STEPS", "content": "Figure 14a illustrates the overall performance when q was fixed to 2 and p to 50, across different pretraining\nstages of the original model. The effect of resuscitation method becomes more pronounced as the original\nmodel progresses to later stages of pretraining, as indicated by the transition from the dotted line to the\nsolid line. We hypothesize that this is because late-stage models tend to rely on a smaller subset of memory\nsources and thus benefit from a broader scope of activation enabled by the resuscitation method. As shown in\nFigure 14b, end performance deteriorates when the beginning model is initial (118k) and mid (369k) stage\nmodel, indicating that resuscitation may impair performance when the model's knowledge entropy is not low\nenough. This trend of the resuscitation showing a more positive effect for models in later stage of pretraining\ncan also be seen in Figure 15, which shows the result when varying q while fixing p as 50: performance"}]}