{"title": "Psychometric Alignment: Capturing Human Knowledge Distributions via Language Models", "authors": ["Joy He-Yueya", "Benjamin W. Domingue", "Wanjing Anya Ma", "Emma Brunskill", "Kanishk Gandhi", "Noah D. Goodman"], "abstract": "Language models (LMs) are increasingly used to simulate human-like responses in\nscenarios where accurately mimicking a population's behavior can guide decision-\nmaking, such as in developing educational materials and designing public policies.\nThe objective of these simulations is for LMs to capture the variations in human\nresponses, rather than merely providing the expected correct answers. Prior work\nhas shown that LMs often generate unrealistically accurate responses, but there are\nno established metrics to quantify how closely the knowledge distribution of LMs\naligns with that of humans. To address this, we introduce \"psychometric alignment,\"\na metric that measures the extent to which LMs reflect human knowledge distribu-\ntion. Assessing this alignment involves collecting responses from both LMs and\nhumans to the same set of test items and using Item Response Theory to analyze\nthe differences in item functioning between the groups. We demonstrate that our\nmetric can capture important variations in populations that traditional metrics, like\ndifferences in accuracy, fail to capture. We apply this metric to assess existing LMs\nfor their alignment with human knowledge distributions across three real-world\ndomains. We find significant misalignment between LMs and human populations,\nthough using persona-based prompts can improve alignment. Interestingly, smaller\nLMs tend to achieve greater psychometric alignment than larger LMs. Further,\ntraining LMs on human response data from the target distribution enhances their\npsychometric alignment on unseen test items, but the effectiveness of such training\nvaries across domains.", "sections": [{"title": "1 Introduction", "content": "The ability of language models (LMs) to mimic human behaviors has been used to replicate results\nfrom social science experiments and public opinion surveys [Argyle et al., 2023, Aher et al., 2023,\nHorton, 2023], and opens up exciting possibilities in areas such as education [Markel et al., 2023,\nHe-Yueya et al., 2024], marketing [Brand et al., 2023], and product design [Park et al., 2022, 2023].\nIn these applications, LMs have been used to represent human populations and respond to questions in\nvarious domains. Unlike typical benchmarks that ta standard ideal or common response, the objective\nin these settings is for LMs to reflect the distribution of responses and outcomes observed in human\npopulations. For instance, when simulating interactions between a novice student and a teacher, we\nexpect that students with different levels of knowledge will have varying probabilities of producing\ncorrect answers. However, prior work has demonstrated that LM-generated responses can sometimes\nbe unrealistically advanced [Aher et al., 2023, Chuang et al., 2023]. It is therefore important to assess\nthe extent to which LMs capture the human population distribution of knowledge or capabilities and\ndevelop methods to align LMs with human distributions. If LMs could effectively mimic human"}, {"title": "2 Related work", "content": "Using LMs to simulate humans Our work is closely related to recent work on using LMs to\nsimulate human behaviors. For instance, researchers have used LMs to replicate results from social\nscience experiments and public opinion surveys [Argyle et al., 2023, Aher et al., 2023, Horton, 2023].\nThe ability of LMs to mimic human behaviors offers exciting opportunities in areas such as education\n[Markel et al., 2023, He-Yueya et al., 2024, Jin et al., 2024, Zelikman et al., 2023, Lu and Wang, 2024,\nShaikh et al., 2023, Liu et al., 2023, Xu and Zhang, 2023], marketing [Brand et al., 2023, Li et al.,\n2023], and product design [Park et al., 2022, 2023]. Evaluations of such simulations have typically\nbeen limited to replicating well-established results from prior studies involing real humans, asking\nexperts to assess believability, or comparing summary statistics such as accuracies on various tests.\nThese methods often overlook or fail to assess the alignment between the knowledge distributions\nof LMs and the target human populations (see Section 5). Notably, several studies have proposed\nmetrics to measure the alignment between two population distributions. Santurkar et al. [2023] have\ndeveloped a metric to measure the alignment of LM opinions with different demographic groups over\ncommon topics in public opinion surveys. Safdari et al. [2023], Pellert et al. [2023] have explored\nwhether LMs can simulate non-cognitive human traits such as personalities.\nOur work is also related to research on whether LMs can learn representations of concepts that\nare aligned with humans, as explored in the field of representational alignment (see Sucholutsky\net al. [2023] for a survey). The capabilities of LMs may be fundamentally different from human\ncapabilities even though they may achieve similar overall accuracy scores on certain benchmarks\n[Anwar et al., 2024]. For instance, GPT-4's accuracy in a counting task drops significantly when the"}, {"title": "3 Measuring psychometric alignment", "content": "We briefly review Item Response Theory and introduce our metric for quantifying how well LMs\nalign with a human population distribution of knowledge across a set of test items."}, {"title": "3.1 Item Response Theory", "content": "Consider a scenario where a group of individuals answers a series of test items. Each response from a\nperson reflects an interaction between their \u201cability\" (knowledge or capabilities) and various attributes\nof the test item such as its difficulty. To assess the abilities of individuals, a simple approach is to\ncount the number of correct answers. However, this method fails to account for variations in item\ndifficulty; some items might test more complex concepts. To address these subtleties, Item Response\nTheory (IRT) [Lord, 2012] offers a psychometric framework widely used in educational assessments\nand psychological measurements to analyze both the abilities of the individuals and the characteristics\nof the test items simultaneously. Among various IRT models, we review the simplest one-parameter\nlogistic model (1PL), also called the Rasch model [Rasch, 1960]. The 1PL model assumes that the\nprobability of a correct response to an item is determined by the difference between the person's\nability \u03b8\u2081 and the item's difficulty bj, shown in Eq. 1.\n\n$p(X_{i,j} = 1|\\theta_i, b_j) = \\frac{1}{1+e^{-(\\theta_i-b_j)}}$ (1)\n\nOne of the key features of IRT models is related to the assumption of parameter invariance [Rupp and\nZumbo, 2006], meaning that item and person parameters remain stable even when different groups\ngenerate responses under varying measurement conditions. This is potentially a strong assumption.\nFor example, consider administering a math test to two different groups: native English speakers\n(Group 1) and English language learners (Group 2). There are scenarios where responses may\ndepend on group membership in a way not captured by Eq. 1. However, by comparing the item\ndifficulty parameters between these groups, we can evaluate whether parameter invariance holds. If\nthe parameters are highly correlated and the differences in item difficulties between the groups are\nminor, we know that the parameters are invariant across groups. This indicates that the test items\nfunction similarly for both groups, ensuring that the test is not biased against any group [Camilli, 2006,\nMa et al., 2023]. Conversely, significant differences in item parameters indicate a lack of invariance,\nprompting further analysis through psychometric methods to identify items with differential item\nfunctioning (DIF) and exclude those items to enhance the test validity [Magis et al., 2010]."}, {"title": "3.2 Psychometric alignment metric", "content": "Assuming parameter invariance across cognitively equivalent populations allows us to measure\npopulation-level (mis-)alignment by analyzing the correlation among their parameters. Inspired by\nthe concept of parameter invariance in IRT, we develop a metric for quantifying how well LMs align\nwith a human population distribution of knowledge on a set of test items.\nConsider a group of N people {h1, h2, ..., hv } and a test with a set of M items {q1, q2, ..., qM } with\ntrue answers {Y1, Y2, ..., \u0443\u043c }. We observe their responses and record them in a matrix $R_h \\in R^{N \\times M}$,\nwhere $(R_h)_{ij}$ represents the response of the i-th person to the j-th item. To compare this with LMs,"}, {"title": "4 Datasets", "content": "To assess how well LMs capture the human population distribution using psychometric alignment,\nwe need datasets of human responses to a set of test items. It is important that we have the full text\ncontent of the items in order to enable LM evaluation; this content is missing from most available\neducational datasets. We now describe three real-world datasets with the required information.\nEEDI: Math diagnostic assessments The EEDI dataset is built on the NeurIPS 2020 Education\nChallenge dataset [Wang et al., 2020], provided by the Eedi online educational platform. It contains\nstudent responses to math multiple-choice questions (see Figure 1) collected between September 2018\nand May 2020. The NeurIPS 2020 Education Challenge dataset provided question content in image\nformat (e.g., Figure 1) without accompanying texts. With permission from Eedi, we have extracted\nthe text from these question images and released this modified dataset. We excluded questions with\ngraphs or diagrams since most current language models do not support visual inputs. The modified\ndataset contains 573 unique questions and 443, 433 responses to these questions from 2, 287 students,"}, {"title": "5 The importance of psychometric alignment", "content": "We first illustrate the importance of our psychometric alignment metric (Eq. 2) using the EEDI dataset\nas an example. While it might seem straightforward to compare populations based on summary\nstatistics such as person accuracies or test scores [Xu and Zhang, 2023, Zelikman et al., 2023, Chen\net al., 2024], these metrics do not capture the distribution of knowledge across individual test items\nand can be misleading when assessing the similarity of two populations and the significance of"}, {"title": "6 Prompting-based ensemble", "content": "We use our psychometric alignment metric to evaluate existing LMs on three datasets: EEDI,\nWORDBANK, and DUOLINGO. We start by assessing the default psychometric alignment of an\nensemble of LMs without prompting them to mimic any specific group. Then, we explore the impact\nof various group-specific prompting strategies on psychometric alignment."}, {"title": "6.1 Control conditions", "content": "In the ensuing evaluations we compare to two control conditions:\nHuman (positive control): For the EEDI and WORDBANK datasets, we construct 20 datasets\nwhere each dataset consists of 150 students randomly selected from the corresponding data but not in\nthe test set. For the DUOLINGO dataset, we similarly construct 20 datasets, each consisting of 500\nstudents. In all domains, we only consider students who have complete responses to all 50 items that\nare in the test set. We then calculate the psychometric alignment metric by comparing the difficulty\nparameters derived from the test set against those from each of the 20 human datasets. This allows us"}, {"title": "6.2 Ensembling different LMs", "content": "To simulate a human population, we need to create a population of LMs. We start by exploring\nwhether an ensemble of different LMs can capture the response variations in a human population. To\ndo so, we evaluate 10 open-source LMs of varying capabilities (see details in Appendix A.1) on the\nEEDI dataset and mix responses from these LMs to create an LM-ensemble response matrix. We\nprompt each LM to answer each of the 50 questions from the held-out test set (see Section 4) 15\ntimes. We use a zero-shot or a few-shot prompt depending on the LM's capability (see Appendix\nA.1). We also vary the temperature settings (0, 0.7, and 1) to diversify responses, resulting in a total\nof 150 sets of responses to the 50 questions. We then fit the 1PL IRT model on this response data\nto estimate the item difficulty parameters. We selected the EEDI dataset because, unlike vocabulary\nor language acquisition tasks, the mathematical capabilities of LMs continue to show significant\nvariations across LMs and datasets, which is crucial for both simulating human variations and fitting\nIRT models effectively."}, {"title": "6.3 Persona-based prompting", "content": "Recent papers have demonstrated that LMs can more accurately capture certain behaviors of a human\ngroup (e.g., voting preferences) when prompted with group-specific demographic information in\ntheir context [Argyle et al., 2023, Santurkar et al., 2023]. We refer to this approach as persona-based\nprompting and explore whether asking an LM to pretend to be individuals with different personas\ncan steer the model to better represent the human population. For example, before asking the LM to\nrespond to a problem we can create personas such as \"Pretend that you are an 11-year-old student.\nYour gender is female. You are eligible for free school meals due to being financially disadvantaged.\"\nWe explore three prompting strategies that use persona descriptions like the above:\n1. PERSONA: We ask the LM to solve the problem given the persona."}, {"title": "7 Fine-tuning LMs on student response data", "content": "In educational contexts, researchers have considered fine-tuning an LM on student response data\nto create student simulators for generating or evaluating test items [Srivastava and Goodman, 2021,\nZelikman et al., 2023]. Therefore, we explore if fine-tuning LMs on student response data can\nenhance the psychometric alignment between LMs and humans on unseen test items. We train three\ndifferent LMs (Mistral-7b, Llama-8b, and Deepseek-7b) to predict student responses from their\nattributes (persona) and historical data. Each training example consists of a sampled student's persona\nand a randomly-selected subset of that student's item-response pairs (see examples in Figure 5). For\nDUOLINGO and WORDBANK, since we only have binary labels, responses are classified as either\n\"Correct\" or \"Incorrect.\" For EEDI, which collects actual student responses (e.g., selected letters), we\ninclude both the student's chosen answer and the true answer. To fine-tune LMs, we use Low-Rank\nAdaptation (LoRA) [Hu et al., 2021] with an adaptor rank of r = 32 and lora_alpha = 64. We train\nthe LMs using different amounts of student data, by varying the number of unique students included.\nThe training data do not include any students or items that are in the test set. Further details on the\ntraining data and hyperparameters are available in Appendix A.4.\nFor evaluation, we ask the LM to simulate a student's response to each item in the test set based on\nthe student's persona, prior items, and LM-predicted responses. For EEDI, we also include the true\nanswer to each prior item. The evaluation prompts follow the same template used for training (see\nFigure 5).\nWe find that fine-tuning these base models on student response data does not improve over the\nbest prompting baseline (see Table 2) for EEDI and DUOLINGO (see Figure 6a and Figure 6c). On\nEEDI, Llama-8b outperforms the other LMs, but there is no significant difference across the LMs\nfor DUOLINGO. However, fine-tuned LMs outperform the best prompting baseline for WORDBANK\n(Figure 6b), using historical data from just a few students. This improvement could be due to higher\nsimilarities between the training and test set items. We also compare all LMs to the human baseline\nthat uses real human data on the test set items (referred to as \"human subset\"); however, we do not\nexpect any of the other methods to match the \u201chuman subset\u201d performance as they do not have access\nto the response data on the test set items. The \u201chuman subset\u201d baseline helps gauge how much real\ndata could potentially be saved by training LMs on historical data. For instance, the top-performing"}, {"title": "8 Limitations", "content": "There are several limitations. First, we focus on the 1PL IRT model because it is widely used and\nfits the EEDI dataset best, but further insights might be gained by examining more sophisticated IRT\nmodels such as those considering multiple latent ability dimensions. Second, we acknowledge that\nno dataset can fully represent the entire human population. For example, our EEDI dataset is limited\nto students in England who choose to use the platform. Third, the datasets we use were not collected\nin typical assessment settings and may violate certain IRT assumptions (e.g., no learning between\nindividual responses)."}, {"title": "9 Conclusion", "content": "We propose an evaluation metric to assess the extent to which LMs capture the distribution of human\nknowledge. We demonstrate that our metric is more robust than traditional ones. We view our metric\nas a tool to enable people to better understand LM behaviors and identify potential representation\nfailures when using LMs to simulate humans."}, {"title": "A.1 LM ensembling implementation details", "content": "To create the LM-ensemble, we consider Mistral-7B-v0.1, llemma_7b, llemma_34b, deepseek-math-7b-base, deepseek-math-7b-instruct, deepseek-math-7b-rl, Meta-Llama-3-8B, Meta-Llama-3-8B-Instruct, Meta-Llama-3-70B, and Meta-Llama-3-70B-Instruct.\nFor the base LMs (Mistral-7B-v0.1, llemma_7b, llemma_34b, deepseek-math-7b-base, Meta-Llama-3-8B, and Meta-Llama-3-70B), we need to use a few-shot prompt (see Figure 7) to ensure their responses to test items are in a consistent format. For the other instruction-tuned LMs, using a zero-shot prompt is sufficient."}, {"title": "A.2 Persona-based prompting examples", "content": "no persona:\nCan you solve the following problem?\nProblem: If 9 is a factor of a number, then another factor of that number must be...\nA) 3\nB) 4 and 5\nC) 18\nD) Impossible to say\nPut the final answer choice (a single letter) in double square brackets.\npersona:\nPretend that you are an 11-year-old student. Your gender is female. You are eligible for free school meals or pupil premium due to being financially disadvantaged.\nGiven your characteristics, would you be able to solve the following problem?\nProblem: If 9 is a factor of a number, then another factor of that number must be...\nA) 3\nB) 4 and 5\nC) 18\nD) Impossible to say\nPut the final answer choice (a single letter) in double square brackets.\npersona + CoT:\nPretend that you are an 11-year-old student. Your gender is female. You are eligible for free school meals or pupil premium due to being financially disadvantaged.\nGiven your characteristics, would you be able to solve the following problem?\nProblem: If 9 is a factor of a number, then another factor of that number must be...\nA) 3\nB) 4 and 5\nC) 18\nD) Impossible to say\nExplain whether you think you can solve this problem and put the final answer choice (a single letter) in double square brackets.\npersona + CoT + structure:\nPretend that you are an 11-year-old student. Your gender is female. You are eligible for free school meals or pupil premium due to being financially disadvantaged.\nGiven your characteristics, would you be able to solve the following problem?\nProblem: If 9 is a factor of a number, then another factor of that number must be...\nA) 3\nB) 4 and 5\nC) 18\nD) Impossible to say\nIf yes, explain your reasoning and put the final answer choice (a single letter) in double square brackets. If you are likely to struggle with this problem, give a plausible incorrect solution and put the final incorrect answer choice (a single letter) in double square brackets."}, {"title": "A.3 Prompting ablations", "content": "The EEDI dataset contains three attributes (see Section 4). We generate 5 additional math-relevant features: numerical proficiency, working memory, math anxiety, math importance, parental involvement."}, {"title": "A.4 Fine-tune on student data", "content": "Training data For each dataset, we randomly split the dataset into training and validation set by the user. Specifically, we use 10% of the users as the validation set. We remove all questions in the test set from the training and validation data. To create each data point for the EEDI domain, we randomly sample 4-11 question-response pairs from each user's quiz sequence and repeat 20 times for each user's quiz. For DUOLINGO and WORDBANK, since the items are shorter, we randomly sample up to 50 question-response pairs from each user's data and repeat 200 times for each user in DUOLINGO and 100 times for each user in WORDBANK. All question-response pairs are arranged in random order in each data point.\nLMs We use the base models of three LM classes: Mistral-7b, Llama-8b, and Deepseek-7b. Specifically, we use Mistral-7B-v0.1, Meta-Llama-3-8B, and deepseek-math-7b-base.\nHyperparameters To fine-tune LMs, we use Low-Rank Adaptation (LoRA) [Hu et al., 2021] with an adaptor rank of r = 32 and lora_alpha = 64. All models are run in 4-bit quantization. We use the 8-bit AdamW with a learning rate fixed at 2.5e-5, and the models are trained with a batch size of 32. We set gradient_accumulation_steps = 1. We selected hyperparameters based on early experiments with Mistral-7B-v0.1. We compared r = 16 with r = 32 and did not find a significant difference, so we consistently used r = 32 and lora_alpha = 64 for all LMs. Each model is trained until there is no reduction in evaluation loss on the validation set across three successive iterations."}]}