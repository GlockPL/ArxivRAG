{"title": "Psychometric Alignment: Capturing Human Knowledge Distributions via Language Models", "authors": ["Joy He-Yueya", "Benjamin W. Domingue", "Wanjing Anya Ma", "Emma Brunskill", "Kanishk Gandhi", "Noah D. Goodman"], "abstract": "Language models (LMs) are increasingly used to simulate human-like responses in scenarios where accurately mimicking a population's behavior can guide decision-making, such as in developing educational materials and designing public policies. The objective of these simulations is for LMs to capture the variations in human responses, rather than merely providing the expected correct answers. Prior work has shown that LMs often generate unrealistically accurate responses, but there are no established metrics to quantify how closely the knowledge distribution of LMs aligns with that of humans. To address this, we introduce \"psychometric alignment,\" a metric that measures the extent to which LMs reflect human knowledge distribution. Assessing this alignment involves collecting responses from both LMs and humans to the same set of test items and using Item Response Theory to analyze the differences in item functioning between the groups. We demonstrate that our metric can capture important variations in populations that traditional metrics, like differences in accuracy, fail to capture. We apply this metric to assess existing LMs for their alignment with human knowledge distributions across three real-world domains. We find significant misalignment between LMs and human populations, though using persona-based prompts can improve alignment. Interestingly, smaller LMs tend to achieve greater psychometric alignment than larger LMs. Further, training LMs on human response data from the target distribution enhances their psychometric alignment on unseen test items, but the effectiveness of such training varies across domains.", "sections": [{"title": "Introduction", "content": "The ability of language models (LMs) to mimic human behaviors has been used to replicate results from social science experiments and public opinion surveys [Argyle et al., 2023, Aher et al., 2023, Horton, 2023], and opens up exciting possibilities in areas such as education [Markel et al., 2023, He-Yueya et al., 2024], marketing [Brand et al., 2023], and product design [Park et al., 2022, 2023]. In these applications, LMs have been used to represent human populations and respond to questions in various domains. Unlike typical benchmarks that ta standard ideal or common response, the objective in these settings is for LMs to reflect the distribution of responses and outcomes observed in human populations. For instance, when simulating interactions between a novice student and a teacher, we expect that students with different levels of knowledge will have varying probabilities of producing correct answers. However, prior work has demonstrated that LM-generated responses can sometimes be unrealistically advanced [Aher et al., 2023, Chuang et al., 2023]. It is therefore important to assess the extent to which LMs capture the human population distribution of knowledge or capabilities and develop methods to align LMs with human distributions. If LMs could effectively mimic human distributions of knowledge, it opens up exciting opportunities for LMs to understand and support human learning.\nMeasuring the alignment between the knowledge distribution of LMs and that of a human population poses a challenge, as we cannot directly observe the cognitive processes of either group. One intuitive approach is to assess the knowledge of both LMs and humans on a set of test items (questions) and compare the accuracy/score distributions of the two groups. However, simply comparing scores can be misleading for evaluating the similarity between two populations and understanding the impact of specific test items because it fails to capture the distribution of knowledge across individual test items (see Section 5). To address this, we introduce psychometric alignment, an evaluation metric that measures the extent to which LMs capture the knowledge distribution of a human population. Assessing this alignment involves collecting responses from both LMs and humans to a set of test items and analyzing the differences in item functioning across groups. In particular, we use Item Response Theory [Lord, 2012] to estimate the item parameters, such as difficulty, for each group and compute the Pearson correlation between these parameters to quantify the extent to which LMs reflect human knowledge distributions. Through psychometric simulations, we demonstrate that this metric is robust, sensitive, and stable in identifying (mis)alignment.\nUsing this metric, we benchmark the ability of existing LMs to capture human knowledge distributions across three real-world domains: first-language acquisition (WORDBANK [Frank et al., 2017]), second-language learning (DUOLINGO [Settles et al., 2018]), and mathematics (EEDI). EEDI is a new dataset that is built on the NeurIPS 2020 Education Challenge dataset [Wang et al., 2020] and contains responses from 2287 students aged 11-12 to 573 math multiple-choice questions. We find a substantial misalignment between the knowledge distribution of an ensemble of LMs that vary in size and capability and that of humans in the mathematics domain.\nWe then explore prompting and training methods aimed at enhancing the psychometric alignment between LMs and humans in various domains. In particular, we show that creating a set of LM instances using persona-based prompting leads to stronger psychometric alignment. However, this method falls far short of ceiling performance (i.e., the alignment between human subgroups from the same population), and its effectiveness varies significantly across domains and LMs. Interestingly, smaller LMs tend to achieve better psychometric alignment than larger LMs. This suggests that increasing model size/training data, that is intended to make LMs more capable at instruction following and various tasks, may reduce the capacity of LMs to simulate human behaviors accurately. Moreover, fine-tuning LMs on data from the target human distribution leads to further improvement in psychometric alignment on unseen test items, but its effectiveness varies across domains."}, {"title": "Related work", "content": "Using LMs to simulate humans Our work is closely related to recent work on using LMs to simulate human behaviors. For instance, researchers have used LMs to replicate results from social science experiments and public opinion surveys [Argyle et al., 2023, Aher et al., 2023, Horton, 2023]. The ability of LMs to mimic human behaviors offers exciting opportunities in areas such as education [Markel et al., 2023, He-Yueya et al., 2024, Jin et al., 2024, Zelikman et al., 2023, Lu and Wang, 2024, Shaikh et al., 2023, Liu et al., 2023, Xu and Zhang, 2023], marketing [Brand et al., 2023, Li et al., 2023], and product design [Park et al., 2022, 2023]. Evaluations of such simulations have typically been limited to replicating well-established results from prior studies involing real humans, asking experts to assess believability, or comparing summary statistics such as accuracies on various tests. These methods often overlook or fail to assess the alignment between the knowledge distributions of LMs and the target human populations (see Section 5). Notably, several studies have proposed metrics to measure the alignment between two population distributions. Santurkar et al. [2023] have developed a metric to measure the alignment of LM opinions with different demographic groups over common topics in public opinion surveys. Safdari et al. [2023], Pellert et al. [2023] have explored whether LMs can simulate non-cognitive human traits such as personalities.\nOur work is also related to research on whether LMs can learn representations of concepts that are aligned with humans, as explored in the field of representational alignment (see Sucholutsky et al. [2023] for a survey). The capabilities of LMs may be fundamentally different from human capabilities even though they may achieve similar overall accuracy scores on certain benchmarks [Anwar et al., 2024]. For instance, GPT-4's accuracy in a counting task drops significantly when the"}, {"title": "Measuring psychometric alignment", "content": "We briefly review Item Response Theory and introduce our metric for quantifying how well LMs align with a human population distribution of knowledge across a set of test items."}, {"title": "Item Response Theory", "content": "Consider a scenario where a group of individuals answers a series of test items. Each response from a person reflects an interaction between their \u201cability\" (knowledge or capabilities) and various attributes of the test item such as its difficulty. To assess the abilities of individuals, a simple approach is to count the number of correct answers. However, this method fails to account for variations in item difficulty; some items might test more complex concepts. To address these subtleties, Item Response Theory (IRT) [Lord, 2012] offers a psychometric framework widely used in educational assessments and psychological measurements to analyze both the abilities of the individuals and the characteristics of the test items simultaneously. Among various IRT models, we review the simplest one-parameter logistic model (1PL), also called the Rasch model [Rasch, 1960]. The 1PL model assumes that the probability of a correct response to an item is determined by the difference between the person's ability \\(\\theta_i\\) and the item's difficulty \\(b_j\\), shown in Eq. 1.\n\\[p(X_{i,j} = 1|\\theta_i, b_j) = \\frac{1}{1+e^{-(\\theta_i-b_j)}}\\]\nOne of the key features of IRT models is related to the assumption of parameter invariance [Rupp and Zumbo, 2006], meaning that item and person parameters remain stable even when different groups generate responses under varying measurement conditions. This is potentially a strong assumption. For example, consider administering a math test to two different groups: native English speakers (Group 1) and English language learners (Group 2). There are scenarios where responses may depend on group membership in a way not captured by Eq. 1. However, by comparing the item difficulty parameters between these groups, we can evaluate whether parameter invariance holds. If the parameters are highly correlated and the differences in item difficulties between the groups are minor, we know that the parameters are invariant across groups. This indicates that the test items function similarly for both groups, ensuring that the test is not biased against any group [Camilli, 2006, Ma et al., 2023]. Conversely, significant differences in item parameters indicate a lack of invariance, prompting further analysis through psychometric methods to identify items with differential item functioning (DIF) and exclude those items to enhance the test validity [Magis et al., 2010]."}, {"title": "Psychometric alignment metric", "content": "Assuming parameter invariance across cognitively equivalent populations allows us to measure population-level (mis-)alignment by analyzing the correlation among their parameters. Inspired by the concept of parameter invariance in IRT, we develop a metric for quantifying how well LMs align with a human population distribution of knowledge on a set of test items.\nConsider a group of N people \\({h_1, h_2, ..., h_N }\\) and a test with a set of M items \\({q_1, q_2, ..., q_M }\\) with true answers \\({Y_1, Y_2, ..., Y_M }\\). We observe their responses and record them in a matrix \\(R_h \\in \\mathbb{R}^{N \\times M}\\), where \\((R_h)_{ij}\\) represents the response of the i-th person to the j-th item. To compare this with LMs,"}, {"title": "Datasets", "content": "To assess how well LMs capture the human population distribution using psychometric alignment, we need datasets of human responses to a set of test items. It is important that we have the full text content of the items in order to enable LM evaluation; this content is missing from most available educational datasets. We now describe three real-world datasets with the required information.\nEEDI: Math diagnostic assessments The EEDI dataset is built on the NeurIPS 2020 Education Challenge dataset [Wang et al., 2020], provided by the Eedi online educational platform. It contains student responses to math multiple-choice questions (see Figure 1) collected between September 2018 and May 2020. The NeurIPS 2020 Education Challenge dataset provided question content in image format (e.g., Figure 1) without accompanying texts. With permission from Eedi, we have extracted the text from these question images and released this modified dataset. We excluded questions with graphs or diagrams since most current language models do not support visual inputs. The modified dataset contains 573 unique questions and 443, 433 responses to these questions from 2, 287 students,\nWORDBANK: Vocabulary development The WORDBANK dataset is from the WordBank database [Frank et al., 2017]. We focus on the English (American) subset, which includes responses from 5, 520 children aged between 16 and 30 months. Each child responded to 680 vocabulary items. We only consider items that are words. The responses, reported by parents, are binary and indicate whether the child can produce each word. The dataset also contains demographic details for each child such as age, gender, ethnicity, and the education level of the mother. We randomly selected a test set of 50 words and 150 children to represent the human population.\nDUOLINGO: App-based language learning The DUOLINGO dataset is from the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM) [Settles et al., 2018]. This dataset contains anonymized data from users of the educational application Duolingo. We focus on the subset of English speakers learning Spanish through lesson sessions. Each user's data consists of a series of binary responses to vocabulary words, with each word presented multiple times. Following the approach described in Wu et al. [2020], we adapted this dataset for Item Response Theory modeling by averaging responses to each vocabulary item, rounding the average score to a binary outcome (0 or 1). For instance, if a user was shown the word \u201chola\" 10 times and correctly translated the word 5 times, the average score would be 0.5 and rounded to 1. After processing, the dataset includes 2, 783 vocabulary words and 573, 321 responses from 2, 640 users, with missing data due to user dropout. The dataset also includes additional user information such as country and device type. We randomly selected a test set of 50 words and 500 users who have responded to these words. In this domain, we needed a larger sample to get stable IRT parameters compared to the other domains because the dataset is more sparse (with more positive labels than negative labels). The percentage of correct responses is over 87%."}, {"title": "The importance of psychometric alignment", "content": "We first illustrate the importance of our psychometric alignment metric (Eq. 2) using the EEDI dataset as an example. While it might seem straightforward to compare populations based on summary statistics such as person accuracies or test scores [Xu and Zhang, 2023, Zelikman et al., 2023, Chen et al., 2024], these metrics do not capture the distribution of knowledge across individual test items and can be misleading when assessing the similarity of two populations and the significance of specific items. To illustrate this, we modified the EEDI dataset to create a synthetic population by randomly shuffling responses."}, {"title": "Prompting-based ensemble", "content": "We use our psychometric alignment metric to evaluate existing LMs on three datasets: EEDI, WORDBANK, and DUOLINGO. We start by assessing the default psychometric alignment of an ensemble of LMs without prompting them to mimic any specific group. Then, we explore the impact of various group-specific prompting strategies on psychometric alignment."}, {"title": "Control conditions", "content": "In the ensuing evaluations we compare to two control conditions:\nHuman (positive control): For the EEDI and WORDBANK datasets, we construct 20 datasets where each dataset consists of 150 students randomly selected from the corresponding data but not in the test set. For the DUOLINGO dataset, we similarly construct 20 datasets, each consisting of 500 students. In all domains, we only consider students who have complete responses to all 50 items that are in the test set. We then calculate the psychometric alignment metric by comparing the difficulty parameters derived from the test set against those from each of the 20 human datasets. This allows us to assess whether the item parameters are consistent within the human population and represents an estimate of ceiling psychometric alignment.\nRandom (negative control): For EEDI, we construct a synthetic response matrix where each user has a 25% chance of answering each question correctly, aligning with the probability of guessing correctly in a four-option multiple-choice format. For WORDBANK and DUOLINGO, where we only have binary labels, we generate a response matrix where each user has a 50% chance of getting each item correct. We calculate the psychometric alignment metric by comparing the difficulty parameters obtained from the random response matrix with those from each human sample. This represents floor psychometric alignment."}, {"title": "Ensembling different LMs", "content": "To simulate a human population, we need to create a population of LMs. We start by exploring whether an ensemble of different LMs can capture the response variations in a human population. To do so, we evaluate 10 open-source LMs of varying capabilities (see details in Appendix A.1) on the EEDI dataset and mix responses from these LMs to create an LM-ensemble response matrix. We prompt each LM to answer each of the 50 questions from the held-out test set (see Section 4) 15 times. We use a zero-shot or a few-shot prompt depending on the LM's capability (see Appendix A.1). We also vary the temperature settings (0, 0.7, and 1) to diversify responses, resulting in a total of 150 sets of responses to the 50 questions. We then fit the 1PL IRT model on this response data to estimate the item difficulty parameters. We selected the EEDI dataset because, unlike vocabulary or language acquisition tasks, the mathematical capabilities of LMs continue to show significant variations across LMs and datasets, which is crucial for both simulating human variations and fitting IRT models effectively."}, {"title": "Persona-based prompting", "content": "Recent papers have demonstrated that LMs can more accurately capture certain behaviors of a human group (e.g., voting preferences) when prompted with group-specific demographic information in their context [Argyle et al., 2023, Santurkar et al., 2023]. We refer to this approach as persona-based prompting and explore whether asking an LM to pretend to be individuals with different personas can steer the model to better represent the human population. For example, before asking the LM to respond to a problem we can create personas such as \"Pretend that you are an 11-year-old student. Your gender is female. You are eligible for free school meals due to being financially disadvantaged.\"\nWe explore three prompting strategies that use persona descriptions like the above:\n1. PERSONA: We ask the LM to solve the problem given the persona.\n2. PERSONA-COT (persona + CoT): We ask the LM to reason about its ability to solve the problem given the persona before providing an answer. This is inspired by the Chain-of-Thought prompting method [Wei et al., 2022] that asks an LM to present explicit intermediate reasoning steps to further enhance its own reasoning capability.\n3. PERSONA-COT-S (persona + CoT + structure): We ask the LM to reason about its ability to solve the problem given the persona and explicitly structure its response based on this assessment."}, {"title": "Fine-tuning LMs on student response data", "content": "In educational contexts, researchers have considered fine-tuning an LM on student response data to create student simulators for generating or evaluating test items [Srivastava and Goodman, 2021, Zelikman et al., 2023]. Therefore, we explore if fine-tuning LMs on student response data can enhance the psychometric alignment between LMs and humans on unseen test items. We train three different LMs (Mistral-7b, Llama-8b, and Deepseek-7b) to predict student responses from their attributes (persona) and historical data. Each training example consists of a sampled student's persona and a randomly-selected subset of that student's item-response pairs (see examples in Figure 5). For DUOLINGO and WORDBANK, since we only have binary labels, responses are classified as either \"Correct\" or \"Incorrect.\" For EEDI, which collects actual student responses (e.g., selected letters), we include both the student's chosen answer and the true answer. To fine-tune LMs, we use Low-Rank Adaptation (LoRA) [Hu et al., 2021] with an adaptor rank of r = 32 and lora_alpha = 64. We train the LMs using different amounts of student data, by varying the number of unique students included. The training data do not include any students or items that are in the test set. Further details on the training data and hyperparameters are available in Appendix A.4.\nFor evaluation, we ask the LM to simulate a student's response to each item in the test set based on the student's persona, prior items, and LM-predicted responses. For EEDI, we also include the true answer to each prior item. The evaluation prompts follow the same template used for training (see Figure 5).\nWe find that fine-tuning these base models on student response data does not improve over the best prompting baseline (see Table 2) for EEDI and DUOLINGO (see Figure 6a and Figure 6c). On EEDI, Llama-8b outperforms the other LMs, but there is no significant difference across the LMs for DUOLINGO. However, fine-tuned LMs outperform the best prompting baseline for WORDBANK (Figure 6b), using historical data from just a few students. This improvement could be due to higher similarities between the training and test set items. We also compare all LMs to the human baseline that uses real human data on the test set items (referred to as \"human subset\"); however, we do not expect any of the other methods to match the \u201chuman subset\u201d performance as they do not have access to the response data on the test set items. The \u201chuman subset\u201d baseline helps gauge how much real data could potentially be saved by training LMs on historical data. For instance, the top-performing"}, {"title": "Limitations", "content": "There are several limitations. First, we focus on the 1PL IRT model because it is widely used and fits the EEDI dataset best, but further insights might be gained by examining more sophisticated IRT models such as those considering multiple latent ability dimensions. Second, we acknowledge that no dataset can fully represent the entire human population. For example, our EEDI dataset is limited to students in England who choose to use the platform. Third, the datasets we use were not collected in typical assessment settings and may violate certain IRT assumptions (e.g., no learning between individual responses)."}, {"title": "Conclusion", "content": "We propose an evaluation metric to assess the extent to which LMs capture the distribution of human knowledge. We demonstrate that our metric is more robust than traditional ones. We view our metric as a tool to enable people to better understand LM behaviors and identify potential representation failures when using LMs to simulate humans."}, {"title": "Appendix / supplemental material", "content": null}, {"title": "LM ensembling implementation details", "content": "To create the LM-ensemble, we consider Mistral-7B-v0.1, llemma_7b, llemma_34b, deepseek-math-7b-base, deepseek-math-7b-instruct, deepseek-math-7b-rl, Meta-Llama-3-8B, Meta-Llama-3-8B-Instruct, Meta-Llama-3-70B, and Meta-Llama-3-70B-Instruct.\nFor the base LMs (Mistral-7B-v0.1, llemma_7b, llemma_34b, deepseek-math-7b-base, Meta-Llama-3-8B, and Meta-Llama-3-70B), we need to use a few-shot prompt (see Figure 7) to ensure their responses to test items are in a consistent format. For the other instruction-tuned LMs, using a zero-shot prompt is sufficient."}, {"title": "Persona-based prompting examples", "content": null}, {"title": "Prompting ablations", "content": "The EEDI dataset contains three attributes (see Section 4). We generate 5 additional math-relevant features: numerical proficiency, working memory, math anxiety, math importance, parental involvement."}, {"title": "Fine-tune on student data", "content": "Training data For each dataset, we randomly split the dataset into training and validation set by the user. Specifically, we use 10% of the users as the validation set. We remove all questions in the test set from the training and validation data. To create each data point for the EEDI domain, we randomly sample 4-11 question-response pairs from each user's quiz sequence and repeat 20 times for each user's quiz. For DUOLINGO and WORDBANK, since the items are shorter, we randomly sample up to 50 question-response pairs from each user's data and repeat 200 times for each user in DUOLINGO and 100 times for each user in WORDBANK. All question-response pairs are arranged in random order in each data point.\nLMs We use the base models of three LM classes: Mistral-7b, Llama-8b, and Deepseek-7b. Specifically, we use Mistral-7B-v0.1, Meta-Llama-3-8B, and deepseek-math-7b-base.\nHyperparameters To fine-tune LMs, we use Low-Rank Adaptation (LoRA) [Hu et al., 2021] with an adaptor rank of r = 32 and lora_alpha = 64. All models are run in 4-bit quantization. We use the 8-bit AdamW with a learning rate fixed at 2.5e-5, and the models are trained with a batch size of 32. We set gradient_accumulation_steps = 1. We selected hyperparameters based on early experiments with Mistral-7B-v0.1. We compared r = 16 with r = 32 and did not find a significant difference, so we consistently used r = 32 and lora_alpha = 64 for all LMs. Each model is trained until there is no reduction in evaluation loss on the validation set across three successive iterations."}]}