{"title": "VLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models", "authors": ["Jingtao Cao", "Zheng Zhang", "Hongru Wang", "Kam-Fai Wong"], "abstract": "Progress in Text-to-Image (T2I) models has significantly advanced the generation of images from textual descriptions. However, there is a lack of metrics to evaluate the models' ability to handle a diverse and extensive range of textual prompts a key aspect of generalizability. To fill this gap, we propose the VLEU (Visual Language Evaluation Understudy) metric. VLEU leverages the power of the Large Language Model (LLM) to sample from the visual text domain, which we define as the entire set of potential inputs for the T2I task, to generate a wide variety of visual text. The images generated by T2I models from these prompts are then assessed for their alignment with the input text using the CLIP model. VLEU quantitatively measures a model's generalizability by computing the Kullback-Leibler (KL) divergence between the visual text marginal distribution and the conditional distribution over the images generated by the model. This measure serves as a quantitative metric for comparing the generalizability of T2I models, and also provides a valuable metric for assessing improvements during the finetuning process. Our experimental results demonstrate VLEU's effectiveness in evaluating the generalizability of various T2I models, positioning it as an essential metric for future research and development in image synthesis from text prompts. Our code and data will be publicly available at https://github.com.", "sections": [{"title": "1 Introduction", "content": "The emergence of latent diffusion models (LDMs) marked a significant advancement in generative models, addressing a crucial limitation that was prevalent during the era dominated by Generative Adversarial Networks (GANs). Unlike GANs, which were often constrained by limited expressive and computational capabilities and focused on specific tasks or datasets, LDMs, trained on extensive datasets like LAION-5B , introduced an enhanced capacity for conditional generation across diverse scenarios. This pivotal development laid the groundwork for major strides in the field of text-to-image (T2I) generation. Notable examples include Stable Diffusion , SDXL , Imagen and DALL-E 3 , all of which have demonstrated impressive capabilities in generating detailed and contextually relevant images from textual descriptions.\nWhen assessing the performance of T2I models, a variety of metrics are employed, including Inception Score (IS) , Fr\u00e9chet Inception Distance (FID) , which primarily gauge the quality and diversity of the generated images. In contrast, metrics like CLIP score , DINO"}, {"title": null, "content": "and ImageReward are designed to measure the semantic alignment between the generated images and the input text. Despite the effectiveness of these metrics in their respective domains, they do not fully capture a model's generalizability, which refers to its ability to produce accurate and diverse images across a wide range of textual prompts. This aspect of T2I model performance is often evaluated through subjective human judgment, highlighting the need for a standardized measure of generalizability.\nSuch issue of evaluating generalizability also extends to the methodologies employed for finetuning T2I models. Techniques such as text inversion , LoRA , DreamBooth , and HiFi Tuner have been instrumental in adapting pre-trained T2I models to specific subject or style. However, during the evaluation phase of these fine-tuned models, there is a lack of robust metrics to effectively measure the loss of generalization, which is shown in Figure 1. Many studies, including DreamBooth among others, have encountered this issue and presented it visually, ultimately relying on subjective human interpretation for assessment.\nTo bridge this gap, we introduce VLEU (Visual Language Evaluation Understudy) metric. Firstly we define the input text prompt for the T2I task as visual text, and also the sets of potential inputs for the T2I task as visual text domain. This delineated definition is essential, as not all text is appropriate as input for T2I models. For example, the chat text from the dialogue system, when used as input for T2I models, is nonsensical. VLEU seeks to quantify the generalizability of T2I models by measuring the alignment between the visual text domain and the images generated by the T2I model conditioned on the visual text domain.\nVLEU operates by calculating the Kullback-Leibler (KL) divergence between the distribution of the visual text domain and the distribution of the images conditionally generated by the model. This divergence serves as a metric for the alignment between the intended text prompts and the generated images. To facilitate this measurement, Large Language Models (LLMs) such as ChatGPT , GPT-4 , and LLaMA are utilized to sample from the visual text domain. These descriptions are then paired with images produced by the T2I model, and the CLIP model is used to evaluate the semantic congruence between each text-image pair.\nThe principal contributions of this work are as follows:\n\u2022 We proposed the VLEU metric, an automatic evaluation designed to assess the generalizability of T2I models.\n\u2022 We detailed the implementation of VLEU, which involves applying LLMs to sample visual text from the visual text domain and utilizing the CLIP model to evaluate the semantic alignment of generated images with the input visual text.\n\u2022 We conducted comprehensive experiments to analyze the effectiveness of VLEU and the impact of different components in the evaluation pipeline, validating its effectiveness in quantifying T2I models' generalizability.\n\u2022 We presented two real-world case studies showcasing the practical utility of VLEU in evaluating T2I models, positioning it as a vital metric for T2I model development."}, {"title": "2 Background", "content": "Text-to-Image Generation: The field of image generation was once dominated by GANs , which operate on a framework of competing networks, one generating images and the other evaluating them. However, at this stage, models based on GANs had limited expressive and computational capabilities, which constrained their generalizability across diverse tasks and datasets. For instance, StyleGAN focused on generating high-quality face images but was not suitable for broader text-to-image generation. GAN research was often focused on specific domains or datasets such as CIFAR-10 , CelebA and ImageNet , rather than being capable of conditional image generation from unconstrained natural language inputs. While there were some promising works exploring conditional generation with GANs model , significant challenges remained in bridging the gap between research prototypes and practical applications of text-to-image"}, {"title": null, "content": "generation. The pivotal breakthrough came with the introduction of diffusion models, exemplified by DDPM and DDIM . Unlike GANs, diffusion models can synthesize high-fidelity images by gradually denoising random noise through reverse diffusion sampling. This approach provides stronger generative capabilities by leveraging the modeling power of deep neural networks. Furthermore, latent diffusion models like DALL-E and DALL-E 2 significantly reduced the computational costs of sampling high-resolution images. The release of the LAION-5B dataset , containing billions of image-text pairs, provided immense amounts of training data to empower text-to-image generation. On top of the latent diffusion framework, the adoption of Transformer architectures was another vital innovation. Models like Imagen and Stable Diffusion incorporated cross-attention layers that align textual prompts with generated image features. This mechanism enabled explicit conditioning on text descriptions and proved crucial for advancing text-to-image capabilities. The breakthrough in scaling up diffusion models to billions of parameters, combined with the effective technique of using Transformer architectures for text conditioning, has led to recent T2I models demonstrating impressive capabilities in synthesizing high-fidelity, controllable images from a wide range of textual descriptions.\nMetrics for T2I Models: In evaluating T2I models, traditional metrics like Inception Score (IS) , Fr\u00e9chet Inception Distance (FID) focus on image quality. IS assesses the clarity and diversity of images using a pre-trained Inception network which has fixed classes trained on Imagenet , while FID measures the distance between feature vectors of real and generated images to gauge realism. Meanwhile, newer metrics such as DINO , CLIP similarity , and ImageReward offer a more nuanced assessment. DINO focuses on discerning the similarity between generated and actual images by emphasizing distinctive features. CLIP similarity metric examines the congruence between images and their textual descriptions. ImageReward gauges the aesthetic and creative attributes of generated images,"}, {"title": null, "content": "aligning them with human aesthetic preferences. However, all these metrics primarily concentrate on either image quality or semantic alignment, revealing a gap in evaluating a model's generalizability across varied textual prompts.\nLarge Language Models: The landscape of natural language processing has been revolutionized by the advent of LLMs like ChatGPT , GPT-4 , and LLaMA . These models exhibit remarkable abilities in understanding context, generating coherent and contextually relevant text, and mimicking human conversational styles. These LLMs are not only adept in producing context-aware and coherent text but are also effectively utilized to generate prompts that guide text-to-image models. For example, recent work Visual ChatGPT demonstrates using ChatGPT to automatically generate prompts for text-to-image models. The conversational nature of ChatGPT allows generating prompts with greater contextual awareness and abstraction compared to human-written prompts."}, {"title": "3 VLEU", "content": "How can we quantify the generalizability of a T2I model? This seems like an intractable problem at first glance. Our intuition stems from observing the phenomenon of T2I models losing generalizability during finetuning.\nAs shown in Figure 2, through comparison, we"}, {"title": null, "content": "can find that if the T2I model can generate good images for all prompts, then the generated images should be aligned with the given prompts overall. If the model loses the ability to generate some prompts, it will cause an overall misalignment. We quantify this alignment as the KL divergence between the visual text marginal distribution and the conditional distribution over the images generated by the model, which VLEU aims to measure.\nVLEU employs a three-step automatic process to measure the above KL divergence. First, text prompts are sampled from the visual text domain and used to generate corresponding images (detailed in Section 3.1). Second, the CLIP model evaluates the semantic alignment between each generated image and its original textual prompt (explained in Section 3.2). Finally, these text-image alignments are utilized for probability modeling to obtain the visual text marginal distribution and the conditional distribution over generated images. These distributions are then used to compute the final VLEU score (formulation provided in Section 3.3)."}, {"title": "3.1 Visual Text Sampling", "content": "To initiate the evaluation process, we sample from the visual text domain - the space of potential textual inputs for the T2I task. To effectively sample from the expansive visual text domain without requiring extensive manual effort, we leverage the generalizability of LLMs to automatically generate diverse T2I prompts that closely approximate the broad sampling from the visual text domain. In this approach, we utilize LLMs to sample two types of prompts: unconstrained subject and constrained subject.\nTable 1 demonstrates the templates used for sampling T2I prompts. The prompt template for constrained subjects is tailored to ensure that the generated prompts contain the same class word. This"}, {"title": null, "content": "consistency is vital when evaluating the loss of generalization in relation to a specific word. For instance, if \"dog\" is the class word, but the LLM replaces it with synonyms like \u201cpooch\u201d, \u201chound\", or \"pup\" in the T2I prompts, it could obscure the true extent to which the model's generalizability to the word \"dog\" has been affected.\nAdditionally, despite increasing the diversity of output by adjusting parameters like the temperature in the ChatGPT API, the responses can still exhibit a degree of convergence. To counteract this and further diversify the T2I prompts, a multi-turn dialogue approach is adopted. After the initial use of the above-mentioned templates, subsequent interactions simply use the prompt \u201cAgain\u201d to stimulate the generation of new T2I prompts. This dialogue can span up to 50 rounds, excluding the more convergent prompts typically produced in the first round."}, {"title": "3.2 Text-Image Scoring", "content": "Using the T2I prompts sampled from the visual text domain, corresponding images are generated by the test model under evaluation. To assess the semantic alignment between each input prompt and output image, we leverage CLIP (Contrastive Language-Image Pre-training), which has demonstrated strong capabilities in matching textual descriptions to images.\nSpecifically, we obtain embedded representations of the text prompts and generated images from CLIP. The similarity between the text and image embeddings for each pair is then quantified using cosine similarity. Cosine similarity measures the angle between two vectors in high-dimensional space, providing a bounded similarity score that is robust to distortions from large vector magnitudes. This enables an effective assessment of how well the generated image aligns with the semantic concepts expressed in the original textual prompt."}, {"title": "3.3 VLEU Calculation", "content": "The Visual Text Sampling and Text-Image Scoring modules together provide the foundation for the VLEU metric. The VLEU metric quantitatively measures the generalizability of a T2I model by computing the divergence between two probability distributions - the marginal visual text distribution P(x), and the conditional distribution P(x|y) over text prompts given a generated image.\nLet G denote the T2I model being evaluated. Given a corpus X = x_1, x_2, ..., x_N of N textual prompts sampled from the visual text domain, the model generates corresponding image y_i = G(x_i). The similarity between each text-image pair (x_i, y_i) is scored using CLIP as S_{ij}. These similarity scores are transformed into a conditional distribution over text prompts associated with each generated image via a softmax function:\nP(x|y_i) = \\text{softmax}(S_{:,i}/t) \nwhere t is a temperature parameter. The marginal distribution is obtained by averaging over the conditionals:\nP(x_i) = \\frac{1}{N} \\sum_y P(x|y_i)\nFinally, the VLEU score is computed as the exponentiated expected KL divergence between the conditional and marginal distributions:\nVLEU = \\text{exp} \\left( E_x [KL(P(x|y) | P(x))] \\right)\nTaking the exponentiation is to scale the scores into a more convenient range for comparison. This provides an interpretable measure of the model's ability to generate diverse images aligned with the visual text domain."}, {"title": "4 Experiments And Analysis", "content": "This section investigates the effectiveness of VLEU in evaluating T2I models and analyzes the impact of different components within the evaluation pipeline. Specifically, we conducted two experiments: (1) Analyze VLEU's effectiveness in capturing model generalizability changes during finetuning and across different T2I models, which is presented in section 4.1. (2) Assess the impact of key components like Visual Text Sampler and Text-Image Scorer on VLEU scores, which is detailed in section 4.2.\nIn line with our objectives, we primarily evaluated four open-source T2I models (SD 1.5, SD 2.0, SD 2.1, SDXL) under various conditions. For finetuning, we utilized the dataset provided by DreamBooth , which comprises several subsets, each containing a series of images related to a specific subject. Besides, within all VLEU calculations, the value of temperature in Equation (1) was set to 0.01 to scale the computed results into a visually convenient range for analysis purposes."}, {"title": "4.1 Effectiveness Analysis", "content": "Analysis on Finetuning. We selected the finetuning process of T2I models on specific datasets, which is often considered detrimental to the models' generalizability, and tested the changes of IS , FID , CLIP score and VLEU during finetuning. Specifically, we finetuned SD 1.5 on the DreamBooth dataset and sampled 25 prompts related to each subset using GPT 3.5. For FID calculation, we treated the images in the dataset as the real image distribution. For CLIP score, we simply calculate the average clip similarity of all prompts and corresponding image."}, {"title": null, "content": "Taking the finetuning on teddy bear images as an example, as shown in Figure 4, the generated images during the finetuning process tend to increasingly resemble images from the training set as the number of finetuning steps increases, indicating a decrease in model generalizability. Figure 3 illustrates the trends of IS, FID, CLIP score and VLEU throughout the finetuning process. Among these, VLEU consistently decreases, aligning more closely with the diminishing generalizability during finetuning compared to the CLIP score. However, IS and FID, which are commonly used metrics reflecting image clarity and diversity, exhibited significant fluctuations. This discrepancy can be attributed to the fact that while diversity consistently decreases during finetuning, changes in image clarity may not follow a consistent pattern.\nAnalysis across T2I models. We applied our VLEU metrics to comprehensively evaluate four open-source T2I models across four major visual text domains, including unconstrained, scene-focused, person-focused, and style-focused visual text. We sampled 1000 prompts in each domain using GPT 3.5. Additionally, we calculated CLIP scores, which are obtained by simply averaging CLIP similarity scores without using our VLEU calculation."}, {"title": null, "content": "As a comparison, we also conducted a human evaluation study, where human evaluators were asked to create a variety of T2I prompts, either freely or focused on a specific subject, similar to how LLMs generate prompts. These prompts were then used to generate images with the four T2I models. The evaluators compared pairs of images generated by randomly selected pairs of models, without knowing which model produced which image, and determine which image better adhered to the prompts. We used these pairwise comparisons to compute an Elo rating for each model, which is a method commonly used to calculate the relative levels of players in win-loss games.\nAs shown in Table 2, the Elo ratings derived from human evaluations were consistent with the VLEU scores, with SD 2.0 and SD 2.1 outperforming SD 1.5 and SDXL across most domains. This alignment between human evaluation and VLEU scores demonstrates the effectiveness of our VLEU metric in assessing the generalizability of T2I models, and proves that our VLEU calculation reflects the generalizability better than simply averaging CLIP similarity scores.\nFor a detailed explanation of the Elo rating calculation and the human evaluation process, please"}, {"title": "4.2 Component Impact Analysis", "content": "In the VLEU pipeline, we focused on two key components: the Visual Text Sampler and the Text-Image Scorer. These components play pivotal roles in shaping the effectiveness and outcomes of the VLEU system. In this analysis, we investigated the impact of utilizing different models for these components on the efficacy of the VLEU pipeline.\nVisual Text Sampler. We experimented with four widely used LLMs as the Visual Text Sampler within the VLEU pipeline and assessed their effectiveness. For each sampler, We sampled 1000 subject-unconstrained prompts and evaluated them on different T2I models. It's worth noting that because the two LLaMA models have a weaker ability to follow instructions, they couldn't directly output T2I prompts in the expected format under zero-shot conditions. Hence, we manually crafted initial dialogues for the first two rounds in a few-shot manner to guide the model in generating T2I prompts in the desired format. As depicted in Table 3, while different samplers exert varying influences on VLEU scores, the overall ranking of several T2I models remains largely consistent. Additionally, we observed that utilizing LLMs deemed to have better generalizability, such as GPT-4, as the Visual Text Sampler resulted in higher VLEU scores compared to those obtained with LLMs with poorer generalizability, such as LLaMA-2-7B. We hypothesize that this phenomenon stems from the weaker descriptive capability of prompts generated by LLMs with poor generalizability, leading to substantially low similarity between the generated images and these prompts. Consequently, this increases the KL divergence of each image relative to the marginal"}, {"title": null, "content": "distribution for text embeddings, thereby yielding higher VLEU scores.\nText-Image Scorer. We explored four different Text-Image Scorers for computing VLEU scores. We retained 1000 subject-unconstrained prompts from GPT 3.5 along with corresponding images generated by each T2I model, only changing the Text-Image Scorer in the pipeline to compute the final VLEU scores. As shown in Table 4, it can be observed that the higher-performing scorers yield higher VLEU scores. We attribute this to the superior matching capability of the higher-performing scorers in aligning images with textual prompts. Consequently, each image exhibits greater discrepancies in scores between its own prompt and other prompts, resulting in larger KL divergences computed and thus higher final scores."}, {"title": "5 Case Studies Using VLEU", "content": "In this section, we present two case studies to illustrate the practical application of VLEU.\nRacial Bias in T21 Models. We tested four T21 models to evaluate their VLEU scores across African, Asian, and Caucasian people. For each ethnicity, we sampled 1000 prompts using GPT 3.5. The results, displayed in Table 5, indicate that all tested models achieved higher scores on Caucasians compared to Africans and Asians. This suggests that these models exhibit higher generalizability performance on Caucasians, due to significant disparities in the representation of different racial groups within the training data."}, {"title": null, "content": "Finetuning Methods Comparison. We compared the performance of two finetuning methods, naive finetuning and Dreambooth, on the SD 1.5 model using the DreamBooth dataset. Specifically, we select a subset of 5 teddy bear images as an example. We sampled 25 prompts about teddy bears and calculated the VLEU scores during finetuning. As expected and shown in Figure 5, Dreambooth showed a slower decline in VLEU compared to naive finetuning, aligning with its goal of preserving model generalizability during specialized training. The discernible gap in VLEU curves validates its sensitivity in capturing different rates of generalization loss. This demonstrates VLEU's efficacy in evaluating finetuning methods' ability to balance specificity and generalizability, a valuable asset for model development."}, {"title": "6 Conclusion", "content": "We introduced VLEU, a novel automatic metric to evaluate T2I models' generalizability. VLEU quantifies alignment between sampled visual text prompts and generated images using LLMs and CLIP. Through experiments and case studies, we demonstrated VLEU's efficacy in capturing declining generalizability during finetuning, discerning differences across models, and comparing finetuning techniques. VLEU provides an automated, standardized metric accounting for a broad space of textual prompts. Our results validate VLEU as an effective metric for quantifying T2I models' generalizability."}, {"title": "7 Limitations", "content": "The efficacy of VLEU is constrained by the expressive capabilities of LLMs as the visual text sampler. Even state-of-the-art models struggle to achieve full coverage of the expansive visual text domain when sampling prompts, limiting prompt diversity for evaluation. As language models advance further in natural language understanding and generation, the accuracy and robustness of the VLEU metric will also improve.\nDetermining sufficient prompt quantities for robust evaluation also presents a challenge. Our experiments indicate this depends on the application scenario. For tracking declining generalizability during finetuning, relatively small samples (around 25 prompts) suffice. However, comparing generalizability across models necessitates larger samples (e.g. 1000 prompts) to ensure evaluation rigor. Further research could systematically investigate optimal prompt quantities for varying contexts to enhance VLEU stability.\nWhile limitations exist, VLEU remains a promising metric providing a standardized framework for evaluating and improving T2I models' generalizability. Future work can explore sampling strategies and evaluation configurations to enhance VLEU robustness and utility."}, {"title": "8 Ethical Considerations", "content": "Our research adheres to stringent ethical standards. We utilized publicly available datasets that have been ethically vetted to avoid offensive or biased content. Participants in our human evaluation were fairly compensated, ensuring ethical treatment. Consequently, our study presents no ethical concerns, as the data is ethically sourced, the analysis unbiased, and all procedures comply with established ethical guidelines."}, {"title": "A Implementation Details", "content": "In this section, we provide a detailed description of the implementation process for our proposed VLEU metric. The implementation consists of two main components: sampling text prompts and calculating the VLEU score. We provide the pseudocode for each component to facilitate understanding and reproducibility."}, {"title": "A.1 Sampling Text Prompts", "content": "The first step in our process is to sample text prompts from the visual text domain. We use LLMs to generate these prompts. The prompts can either be random or contain a specific keyword. The pseudocode described in Algorithm 1 outlines the process of generating text prompts."}, {"title": "A.2 Calculating the VLEU Score", "content": "The second step involves calculating the VLEU score using the CLIP model to evaluate the semantic alignment between generated images and their corresponding text prompts. The pseudocode described in Algorithm 2 outlines the process of calculating the VLEU score."}, {"title": "B Human Evaluation", "content": "To demonstrate the effectiveness our VLEU metric evaluation, we conducted a comprehensive human evaluation study involving 10 human evaluators. These evaluators were tasked with creating a variety of T2I prompts, either freely or focused on specific subjects, similar to the prompt generation process of LLMs. The prompts generated by the evaluators were then used to produce images using the four T2I models under investigation.\nTo facilitate the evaluation process, we developed an interactive web interface using Gradio, which allowed evaluators to compare pairs of images generated by randomly selected pairs of models. Figure 6 shows a screenshot of the Gradio interface used in the study. Evaluators were asked to determine which image better adhered to the given prompts, without knowing which model produced which image.\nThe pairwise comparisons made by the evaluators were used to compute an Elo rating for each model. The Elo rating system, originally developed for ranking chess players, is a method for calculating the relative skill levels of players in win-loss games. Each model's initial rating was set to 1000. The Elo rating for a model is updated based on"}, {"title": null, "content": "the outcome of each pairwise comparison, with the winning model gaining points and the losing model losing points. The amount of points exchanged depends on the difference in the ratings of the two models, with larger differences resulting in smaller point exchanges.\nThe Elo rating R for a model is updated using the following formula:\nR_{new} = R_{old} + K \\times (S \u2013 E)\nwhere: - R_{new} is the new Elo rating. - R_{old} is the old Elo rating. - K is a constant that determines the sensitivity of the rating system (commonly set to 32). - S is the actual score of the match (1 for a win, 0.5 for a draw, and 0 for a loss). - E is the expected score, calculated using the formula:\nE = \\frac{1}{1 + 10^{(R_{opponent}-R_{old})/400}}\nwhere R_{opponent} is the Elo rating of the opposing model."}]}