{"title": "FROM REWARD SHAPING TO Q-SHAPING: ACHIEVING\nUNBIASED LEARNING WITH LLM-GUIDED KNOWL-\nEDGE", "authors": ["Xiefeng Wu"], "abstract": "Q-shaping is an extension of Q-value initialization and serves as an alternative to\nreward shaping for incorporating domain knowledge to accelerate agent training,\nthereby improving sample efficiency by directly shaping Q-values. This approach\nis both general and robust across diverse tasks, allowing for immediate impact\nassessment while guaranteeing optimality. We evaluated Q-shaping across 20 dif-\nferent environments using a large language model (LLM) as the heuristic provider.\nThe results demonstrate that Q-shaping significantly enhances sample efficiency,\nachieving a 16.87% improvement over the best baseline in each environment and a\n253.80% improvement compared to LLM-based reward shaping methods. These\nfindings establish Q-shaping as a superior and unbiased alternative to conventional\nreward shaping in reinforcement learning.", "sections": [{"title": "INTRODUCTION", "content": "Reinforcement learning (RL) can solve complex tasks but often faces sample inefficiency. For\nexample, AlphaGo (Silver et al., 2016) required approximately 4 weeks of training on 50 GPUs,\nlearning from 30 million expert Go game positions to reach a 57% accuracy. Similarly, training a real\nbipedal soccer robot required 9.0 \u00d7 108 environment steps, amounting to 68 hours of wall-clock time\nfor the full 1v1 agent (Haarnoja et al., 2024). These cases demonstrate the significant computational\ndemands of RL.\nTo improve efficiency, popular methods include (1) imitation learning, (2) residual reinforcement\nlearning, (3) reward shaping, and (4) Q-value initialization. Yet, each has limitations: imitation\nlearning requires expert data, residual RL needs a well-designed controller, and Q-value initialization\ndemands precise estimates. Therefore, reward shaping is the most practical approach, as it avoids the\nneed for expert trajectories or predefined controllers."}, {"title": "RELATED WORK", "content": ""}, {"title": "HEURISTIC REINFORCEMENT LEARNING", "content": "There are four common approaches to incorporating domain knowledge into reinforcement learning\nto enhance sample efficiency: (1) Imitation Learning, (2) Residual Policy, (3) Reward Shaping, and\n(4) Q-value Initialization.\nImitation Learning requires access to expert trajectories, as demonstrated by works such as GAIL (Ho\n& Ermon, 2016), where agents learn by mimicking expert behavior. However, the reliance on high-\nquality expert data limits its applicability in complex tasks. Residual Policy (Johannink et al., 2019)\nmethods involve designing a controller to guide agent actions, but this manual design process restricts\ntheir scalability and generality.\nQ-value initialization, although promising, often requires precise Q-value estimates to derive an\neffective policy. For instance, Cal-QL (Nakamoto et al., 2024) employs calibrated Q-values to\nenhance agent exploration, but these calibrated values still rely on expert knowledge, making Q-value\ndesign more challenging than reward shaping. Consequently, few studies have pursued this direction\ndue to the inherent difficulty in obtaining accurate Q-values compared to reward shaping.\nReward shaping directly modifies the reward function to influence agent behavior, improving training\nefficiency without requiring expert trajectories or manual controller design. This approach has\nbeen refined to address diverse learning scenarios, such as in Inverse Reinforcement Learning (IRL)\n(Ziebart et al., 2008; Wulfmeier et al., 2015; Finn et al., 2016) and Preference-based RL (Christiano\net al., 2017; Ibarz et al., 2018; Lee et al., 2021; Park et al., 2022). Additionally, various heuristic\ntechniques have been introduced, including unsupervised auxiliary task rewards (Jaderberg et al.,\n2016), count-based reward heuristics (Bellemare et al., 2016; Ostrovski et al., 2017), and self-"}, {"title": "LLM\\VLM AGENT", "content": "LLMs/VLMs can achieve few-shot or even zero-shot learning in various contexts, as demonstrated\nby works such as Voyager (Wang et al., 2023), ReAct (Yao et al., 2022), and SwiftSage (Lin et al.,\n2024).In the field of robotics, VIMA Jiang et al. (2022) employs multimodal learning to enhance\nagents' comprehension capabilities. Additionally, the use of LLMs for high-level control is becoming\na trend in control tasks Shi et al. (2024); Liu et al. (2023); Ouyang et al. (2024).In web search,\ninteractive agents Gur et al. (2023); Shaw et al. (2024); Zhou et al. (2023) can be constructed using\nLLMs/VLMs. Moreover, frameworks have been developed to reduce the impact of hallucinations,\nsuch as decision reconsideration (Yao et al., 2024; Long, 2023), self-correction (Shinn et al., 2023;\nKim et al., 2024), and observation summarization (Sridhar et al., 2023)."}, {"title": "LLM-ENHANCED RL", "content": "Relying on the understanding and generation capabilities of large models, LLM-enhanced RL has\nbecome a popular fieldDu et al. (2023); Carta et al. (2023). Researchers have investigated the diverse\nroles of large models within reinforcement learning (RL) architectures, including their application\nin reward design Kwon et al. (2023); Wu et al. (2024); Carta et al. (2023); Chu et al. (2023); Yu\net al. (2023); Ma et al. (2023), information processing Paischer et al. (2022; 2024); Radford et al.\n(2021), as a policy generator, and as a generator within large language models (LLMs)Chen et al.\n(2021); Micheli et al. (2022); Robine et al. (2023); Chen et al. (2022). While LLM-assisted reward\ndesign has improved task success rates (Ma et al., 2023; Xie et al.), it often introduces bias into the\noriginal Markov Decision Process (MDP) or fails to provide sufficient guidance for complex tasks.\nAdditionally, the verification process is time-consuming, which slows down the pace of iterative\nimprovements."}, {"title": "NOTATION", "content": "Markov Decision Processes. We represent the environment as a Markov Decision Process (MDP)\nin the standard form: M := (S, A, R, P, \u03b3, \u03c1). Here, S and A denote the discrete state and action\nspaces, respectively. We use Z := S \u00d7 A as shorthand for the joint state-action space. The reward\nfunction R: Z \u2192 Dist([0,1]) maps state-action pairs to distributions over the unit interval, while\nthe transition function P: Z \u2192 Dist(S) maps state-action pairs to distributions over subsequent\nstates. Lastly, p\u2208 Dist(S) represents the distribution over initial states. We denote rm and PM as\nthe true reward and transition functions of the environment.\nFor policy definition, the space of all possible policies is denoted as \u03a0. A policy \u03c0 : S \u2192 \u2206(\u0391)\ndefines a conditional distribution over actions given states. A deterministic policy \u03bc : S \u2192 A is a\nspecial case of \u03c0, where one action is selected per state with a probability of 1. We define the value\nfunction as v: \u03a0 \u2192 S \u2192 Ror q: \u03a0 \u2192 S \u00d7 A \u2192 R, both with bounded outputs. The terms q and v\nrepresent discrete matrix representations, where v(s) and q(s, a) specifically denote the outputs of\nan arbitrary value function for a given policy at a particular state or state-action pair.\nM\nAn optimal policy for an MDP M, denoted by \u03c0\u039c, is one that maximizes the expected return\nunder the initial state distribution: \u03c0M:= arg max\u03c0 E\u03c1 [VM]. The state-wise expected returns\nof this optimal policy are represented by vM. The Bellman consistency equation for the MDP\nMat x is given by BM(x) := r + \u03b3Px. Notably, (vM)* is the unique vector that satisfies\n(vM)* = A*BM((vM)*). We abbreviate q* as (qM)* and qD as (qD)* for some MDP g.\nDatasets We define fundamental concepts essential for fixed-dataset policy optimization. Let\nD := {(s, a, r, s')}d represent a dataset of d transitions. From this dataset, we can construct a local\nMDP D and derive a local optimal Q-value function, denoted as qD."}, {"title": "Q-SHAPING FRAMEWORK", "content": "In the Q-learning framework, an experience buffer D is used to store transitions from the Markov\nDecision Process (MDP), supporting both online and offline training. The TD-update method utilizes\nthis experience buffer to estimate the Q-values for (s, a) pairs. The policy is then derived from the\ntrained Q-function, which maximizes q(s,\u00b7). Thus, accurate Q-value estimation is crucial, as it\ndetermines policy quality and guides exploration. To facilitate better exploration, Q-shaping leverages\nboth the experience buffer and a heuristic function provided by a large language model to estimate\nthe Q-function. The general form of Q-shaping is given by:\nqk+1(s, a) = qk (s, a) + \u03b1\u011dD(s, a) +h(s,a), (s,a,h(s, a)) \u2208 DLM,\nwhere QD(s, a) represents the temporal-difference (TD) update estimation of q(s, a) at step k,\nexpressed as: QD(s,a) = r(s,a, s') + \u03b3\u00f4k (s, a). Here, DELM denotes the set of (s, a, Q) pairs\nprovided by the LLM at iteration k.\nIn the early stages of training, the convergence of the Q-function does not yield optimal performance,\nas the agent has yet to gather high-quality trajectories. Previous works, such as MCTS (Browne\net al., 2012) and SAC (Haarnoja et al., 2018), have employed action-bonus heuristics to bias Q-\nvalues, thereby facilitating better exploration. While these methods may compromise the accuracy of\nQ-value estimation, they significantly enhance the agent's trajectory exploration in the short term.\nOur approach aligns with these action-bonus methods but leverages the LLM's understanding and\nthinking abilities to provide heuristic bonuses, resulting in a more informed exploration strategy."}, {"title": "UNBIASED OPTIMALITY", "content": "The Q-value represents a high-level abstraction of both the environment and the agent's policy. It\nencapsulates key elements such as rewards r, transition probabilities P, states s, actions a, and the\npolicy \u03c0, thereby integrating the environmental dynamics and the policy under evaluation. Changes in\nany of these components directly influence the Q values associated with different actions. Specifically,\nthe term h can take various forms, such as the entropy term used in SAC or the UCT heuristic term\nemployed in MCTS and is utilized to shape the Q-values at each step. Compared to these algorithms,\nthe LLM-guided Q-shaping method provides heuristic guidance only at specific steps, ensuring that\nthe final optimality of the Q-function remains unaffected. The converged shaped Q-function is thus\nequivalent to the locally optimal Q-function q*:\nTheorem 1 (Contraction and Equivalence of q). Let q be a contraction mapping defined in the\nmetrics space (X, ||\u00b7||\u221e), i.e,\n||BD(q) \u2013 BD(q')||\u221e\u2264 \u03b3||q \u2013 q'||\u221e,\nwhere BD is the Bellman operator for the sampled MDP D and \u03b3 is the discount factor.\nSince both q and q* are updated on the same MDP, we have the following equation:\nq* = q^*"}, {"title": "UTILIZING IMPRECISE Q VALUE ESTIMATION", "content": "At the early training stage, the Q-values for different actions are nearly identical, leading the policy\nto execute actions randomly.To address this, we leverage the LLM's domain knowledge to provide\npositive Q-values for actions that contribute to task success and negative Q-values for actions that do\nnot. The imprecise Q-values provided by the LLM can be categorized into two types: overestimations\nand underestimations\nUnderestimation of Non-Optimal Actions An agent does not need to fully traverse the entire\nstate-action space to identify the optimal trajectory that leads to task success. Therefore, imprecise\nQ-value estimation can be effectively utilized to guide the agent's exploration.\nFor instance, consider a scenario where the agent is required to control a robot arm to operate on\na drawer located in front of it. In this case, actions such as moving the arm backward or upward\nare evidently unhelpful in finding the optimal trajectory. Assigning very low Q-values to these\nnon-contributory actions discourages the agent from exploring them, thereby enhancing sample\nefficiency.\nOverestimation of Near-Optimal Ac-\ntions At the initial training phase (it-\neration step k = 0), let action a\nbe assumed to have the highest es-\ntimated Q-value for a given state s,\nwhile a* denotes the true optimal action.\nThis assumption leads to the inequal-\nity q(s, a*) < q(s,a) < q*(s, a*).\nConsequently, the agent is predisposed\nto explore actions around the subop-\ntimal a in its search for states, given\nthat \u03bc(s) = maxa q(s,\u00b7) + \u03b5, where\n\u03b5 ~ \u039d(0, \u03b4\u00b2).\nHowever, the number of steps required\nto discover the optimal action a* is in-\nherently constrained by the environment\nand the distance between a and a*. To\nexpedite this exploration process, we in-\ntroduce an action aLLM suggested by\nthe LLM, replacing a via Q-shaping guided by the loss function in Equation 1 to enhance sample\nefficiency. Given the assumption |aLLM - a*| < |a \u2013 a* | < \u03b4, we can express \u03bc(s) = aLLM + \u03b5.\nConsequently, the agent has a higher chance of selecting a*, significantly improving the likelihood of\nidentifying the optimal trajectory.\nIn conclusion, by letting the LLM provide the goodQ set and badQ set, the agent is guided to\nprioritize exploring actions suggested by the LLM, thereby enhancing sample efficiency. Over time,\nas indicated by Hasselt (2010); Fujimoto et al. (2018) and Theorem 1, \u00f4q converges towards the locally\noptimal Q-function. We now present the theoretical upper bound on the sample complexity required\nfor q to converge to q* for any given MDP D:\nTheorem 2 (Convergence Sample Complexity). The sample complexity n required for q to converge\nto the local optimal fixed-point q* with probability 1 \u2013 \u03b4 is:\nn > O\n(\n|S||A|\n\u03b5\u00b2\nln\n|S||A|\n\u03b4\n)"}, {"title": "ALGORITHM IMPLEMENTATION", "content": "For the implementation of Q-shaping, we employ TD3 (Fujimoto et al., 2018) as the RL solver\n(backbone) and GPT-4o as the heuristic provider, introducing three additional training phases: (1)\nQ-Network Shaping (2) Policy-Network Shaping, and (3) High-performance agent selection. Pseudo-\ncode 1 outlines the detailed steps of the Q-shaping framework.\nQ-Network Shaping In the Q-shaping framework, the LLM is tasked with providing a set of\n(s, a, Q) pairs to guide exploration. This approach is particularly crucial during the early training\nstage when it is challenging for the agent to independently discover expert trajectories. Traditional\nRL solvers often require a substantial number of steps to identify the correct path to success, leading\nto sample inefficiency. The goal of the Q-shaping framework is to leverage the provided (s, a, Q)\npairs to accelerate exploration and help the agent quickly identify the optimal path."}, {"title": "ALGORITHM IMPLEMENTATION", "content": "For the implementation of Q-shaping, we employ TD3 (Fujimoto et al., 2018) as the RL solver\n(backbone) and GPT-4o as the heuristic provider, introducing three additional training phases: (1)\nQ-Network Shaping (2) Policy-Network Shaping, and (3) High-performance agent selection. Pseudo-\ncode 1 outlines the detailed steps of the Q-shaping framework.\nQ-Network Shaping In the Q-shaping framework, the LLM is tasked with providing a set of\n(s, a, Q) pairs to guide exploration. This approach is particularly crucial during the early training\nstage when it is challenging for the agent to independently discover expert trajectories. Traditional\nRL solvers often require a substantial number of steps to identify the correct path to success, leading\nto sample inefficiency. The goal of the Q-shaping framework is to leverage the provided (s, a, Q)\npairs to accelerate exploration and help the agent quickly identify the optimal path."}, {"title": "Q-Network Shaping", "content": "In the Q-shaping framework, the LLM is tasked with providing a set of\n(s, a, Q) pairs to guide exploration. This approach is particularly crucial during the early training\nstage when it is challenging for the agent to independently discover expert trajectories. Traditional\nRL solvers often require a substantial number of steps to identify the correct path to success, leading\nto sample inefficiency. The goal of the Q-shaping framework is to leverage the provided (s, a, Q)\npairs to accelerate exploration and help the agent quickly identify the optimal path."}, {"title": "", "content": "To obtain DLM, we construct a general code template as the prompt as illustrated in Figure\n2, supplemented by task-specific environment configuration files and a detailed definition of the\nobservation and action spaces within the simulator. Subsequently, we apply the loss function\nLq-shaping to update the Q-function:\nLq-shaping (\u03b8) = E(si,ai,Qi)\u223cDg (Qi \u2013 \u011d\u03b8 (si, ai))2"}, {"title": "Policy-Network Shaping", "content": "In most reinforcement learning (RL) algorithms, the policy is derived\nfrom the Q-function, where the policy is optimized to execute actions that maximize the Q-value\ngiven a state. The policy update is expressed as:\u00b5(s) = arg maxa q(s,\u00b7)\nWhile introducing a learning rate and target policy can help stabilize the training process and prevent\nfluctuations in the policy network, this approach often slows down the convergence speed. To\naccelerate this adaptation, we introduce a \"Policy-Network Shaping\" phase designed to allow the\npolicy to quickly align with the good actions and avoid the bad actions provided by the LLM.\nThe shaping loss function is defined as:\nLpolicy-shaping = \u03bb\u2081E(s,a)\u223cGLLM [||\u03bc(s) \u2013 a||2] \u2013 \u03bb2E(s,a)\u223cBLLM [||\u03bc(s) \u2013 a||2]\nwhere (s, a) ~ GLLM and (s,a) ~ BLLM represent state-action pairs sampled from the LLM-\nprovided goodQ and badQ sets, respectively, and \u5165\u2081 and 12 are hyperparameters controlling the\ninfluence of the LLM-guided shaping.\nWith this \"Policy-Network Shaping\" phase, researchers can quickly observe the impact of heuristic\nvalues, facilitating the rapid evolution of heuristic quality, ultimately leading to a more efficient\nexploration process and faster convergence to optimal behavior."}, {"title": "High-Performance Agent Selection", "content": "With Q-network shaping and policy-network shaping, the\nQ-shaping framework enables a more rapid verification of the quality of provided heuristic values\ncompared to traditional reward shaping. This allows the experimenter to selectively retain high-\nperforming agents for further training while discarding those that underperform. As outlined in\nAlgorithm 1, following the shaping of the policy and Q-values, each agent is allowed 10,000 steps to\nexplore. After this exploration phase, weaker agents are removed, and only the top-performing agent\ncontinues with the training process."}, {"title": "EXPERIMENT SETTINGS", "content": "We investigate the following hypotheses through a series of four experiments:\n1. Can Q-shaping enhance sample efficiency in reinforcement learning?\n2. Can Q-shaping adapt to incorrect or hallucinated heuristics while maintaining optimality?\n3. Does Q-shaping outperform LLM-based reward shaping methods?\n4. Can LLM design heuristic functions that provide s,a,Q altogether?"}, {"title": "Environments", "content": "We evaluate Q-shaping across 20 distinct environments, including 8 from Gym-\nnasium Classic Control and MuJoCo (Todorov et al., 2012), 9 from MetaWorld (Yu et al., 2020),\nand 3 from PyFlyt (Tai et al., 2023). The environments span a range of robot types, from simple\npendulum systems to humanoid control. Notably, the robot arm and drone environments used are less\ncommonly studied, making it unlikely that the LLM was pretrained on these specific environments."}, {"title": "Baselines", "content": "For the sample efficiency experiments, we compared Q-shaping against several baseline\nalgorithms, including CleanRL-PPO, CleanRL-SAC (Huang et al., 2022), DDPG (Lillicrap et al.,\n2015), and TD3 (Fujimoto et al., 2018). When evaluating Q-shaping against other reward shaping\nmethods, we selected Text2Reward and Eureka as baselines. In the LLM-type ablation study, we\nassessed the performance of different LLMs: O1-Preview, GPT-40-Mini, Gemini-1.5-Flash (Team\net al., 2023), DeepSeek-V2 (DeepSeek-AI et al., 2024), and Yi-Large (Young et al., 2024).\nFor the reward shaping method comparison, we implemented Eureka and Text2Reward (Xie et al.).\nSpecifically, for the MetaWorld tasks using Eureka, we set K = 16 and limited the evolution round\nto 1 due to the long verification cycle of Eureka."}, {"title": "Metrics", "content": "To evaluate sample efficiency, we measure the number of steps required to reach 80% of\npeak performance, where peak performance is defined as the highest performance achieved by any\nbaseline agent. For clarity in visualization, improvements exceeding 150% are truncated to 150%.\nEach algorithm is tested 10 times, and the average evaluation performance is reported. Evaluations\nare conducted at intervals of 5,000 steps. During each evaluation, the agent will be tested over 10\nepisodes, and the average episodic return will be plotted to form the learning curve."}, {"title": "RESULTS AND ANALYSIS", "content": "Q-Shaping Outperforms Best Baseline by an Average of 16.87% Across 20 Tasks As shown in\nFigure 5 and Figure 4, Q-shaping demonstrated a notable improvement over both the best baseline\nand TD3 across 20 tasks. On average, Q-shaping improves performance by 16.87% compared to the\nbest baseline and by 55.39% compared to TD3, highlighting its effectiveness in enhancing sample\nefficiency and task performance. This supports H1.\nQ-Shaping Outperforms LLM-Based Reward Shaping Methods by 253.80% We evaluated\nQ-shaping and baseline methods on four Meta-World environments: door-close, drawer-open,\nwindow-close, and sweep-into. Using peak performance as the basis for comparison, Q-shaping\nachieved substantial improvements over both the Eureka and T2R baselines according to Figure 6."}, {"title": "", "content": "Compared to the best baseline, LLM-TD3 improved by 38.68% in the door-close task, 406.04% in\ndrawer-open, 389.77% in window-close, and 180.70% in sweep-into, resulting in an average peak\nperformance improvement of 253.80%.\nThough LLM-based reward shaping methods can improve task success rates (Ma et al., 2023; Xie\net al.), they often sacrifice optimality by modifying the original MDP. In contrast, Q-shaping achieves\nsuperior performance, retaining both success and optimality, with a 253.80% improvement over the\nbest LLM-based reward shaping methods. This supports H2 and H3."}, {"title": "Most LLMs Can Provide Correct\nHeuristic Functions", "content": "We evaluated\nthe quality of LLM-generated heuris-\ntic functions from five perspectives:\n(1) adherence to the required code\ntemplate, (2) correctness of the as-\nsigned Q-values, (3) accuracy of the\nstate-action dimension, (4) complete-\nness of the generated code, and (5)\npresence of bugs in the generated\ncode. Each LLM was prompted 10 times with the same request, and we quantified their performance\nusing a correctness rate across these metrics."}, {"title": "Most LLMs Can Provide Correct\nHeuristic Functions", "content": "We evaluated\nthe quality of LLM-generated heuris-\ntic functions from five perspectives:\n(1) adherence to the required code\ntemplate, (2) correctness of the as-\nsigned Q-values, (3) accuracy of the\nstate-action dimension, (4) complete-\nness of the generated code, and (5)\npresence of bugs in the generated\ncode. Each LLM was prompted 10 times with the same request, and we quantified their performance\nusing a correctness rate across these metrics.\nThe results, as shown in Table 1, indicate that most LLMs, including 01-Preview, GPT-40, DeepSeek-\nV2.5, and yi-large, provided correct heuristic functions with a 100% success rate across all evaluation\nmetrics. However, Gemini exhibited poorer performance, achieving only 44% on average. This\nsupports H4."}, {"title": "Ablation Study on Additional Training Phases", "content": "We conducted an ablation study to evaluate the\nimpact of three key training phases: (1) Q-Network Shaping, (2) Policy-Network shaping, and (3)\nagent selection, across four Meta-World environments: door-close, drawer-open, window-close, and\nsweep-into. The effectiveness of each phase was measured by convergence steps, with algorithms\nmarked as \"Failed\" if they did not reach the convergence threshold within 106 steps. The study aimed\nto assess how each phase contributes to improving sample efficiency.\nAs shown in Table 2, each training phase significantly enhances sample efficiency. Q-Network shaping\nand policy-network shaping together result in substantial performance gains for TD3. Additionally,\nthe agent selection phase helps by eliminating agents that fail to explore effective trajectories in the\nearly stages of training, providing a slight improvement in average sample efficiency."}, {"title": "CONCLUSION", "content": "We propose Q-shaping, an alternative framework that integrates domain knowledge to enhance sample\nefficiency in reinforcement learning. In contrast to traditional reward shaping, Q-shaping offers two\nkey advantages: (1) it preserves optimality, and (2) it allows for rapid verification of the agent's\nbehavior. These features enable experimenters or LLMs to iteratively refine the quality of heuristic\nvalues without concern for the potential negative impact of poorly designed heuristics. Experimental\nresults demonstrate that Q-shaping significantly improves sample efficiency and outperforms LLM-\nguided reward shaping methods across various tasks.\nWe hope this work encourages further research into advanced techniques that leverage LLM outputs\nto guide and enhance the search process in reinforcement learning."}]}