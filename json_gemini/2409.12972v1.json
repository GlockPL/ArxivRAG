{"title": "TRACE: Transformer-based user Representations from\nAttributed Clickstream Event sequences", "authors": ["William Black", "Alexander Manlove", "Jack Pennington", "Andrea Marchini", "Ercument Ilhan", "Vilda Markeviciute"], "abstract": "For users navigating travel e-commerce websites, the process of researching products and making a purchase\noften results in intricate browsing patterns that span numerous sessions over an extended period of time. The\nresulting clickstream data chronicle these user journeys and present valuable opportunities to derive insights\nthat can significantly enhance personalized recommendations. We introduce TRACE, a novel transformer-\nbased approach tailored to generate rich user embeddings from live multi-session clickstreams for real-time\nrecommendation applications. Prior works largely focus on single-session product sequences, whereas TRACE\nleverages site-wide page view sequences spanning multiple user sessions to model long-term engagement.\nEmploying a multi-task learning framework, TRACE captures comprehensive user preferences and intents\ndistilled into low-dimensional representations. We demonstrate TRACE's superior performance over vanilla\ntransformer and LLM-style architectures through extensive experiments on a large-scale travel e-commerce\ndataset of real user journeys, where the challenges of long page-histories and sparse targets are particularly\nprevalent. Visualizations of the learned embeddings reveal meaningful clusters corresponding to latent user\nstates and behaviors, highlighting TRACE's potential to enhance recommendation systems by capturing nuanced\nuser interactions and preferences.", "sections": [{"title": "1. Introduction", "content": "On tourism e-commerce websites users often exhibit complex navigation patterns whilst they browse\ntravel and accommodation options before making a purchase. A typical user could land on the homepage,\nsearch for a flight then bounce, only to return a few days later to browse hotels and then purchase a\npackage holiday. The resulting clickstream data captures these intricate journeys and offers valuable\ninsights into users' behaviour and intentions. By harnessing this data and better understanding users'\nlatent psychological states and preferences, we can significantly enhance personalized experiences by\nmatching them with more relevant content [1, 2, 3, 4, 5] and adapting the experience to better suit their\ncontext [4]. For instance, users earlier in their search can be presented with more exploratory content,\nas compared to users nearer the end of the purchase funnel.\nHowever, achieving this level of personalization can be challenging as user journeys often span\nmultiple sessions over an extended period of time, and specific goals, such as completing a purchase,\noccur infrequently within this window. This is a particularly pertinent challenge within the tourism\nindustry as users will often only make one booking a year, which can takes weeks of searching and\nplanning before purchasing it months in advance.\nIn this work, we present TRACE (Transformer-based Representations of Attributed Clickstream Event\nsequences), a novel approach for generating rich user embeddings from live multi-session clickstream\ndata with sparse targets. TRACE employs a multi-task learning (MTL) framework, where a lightweight\ntransformer encoder is trained to predict multiple user engagement targets based on sequences of"}, {"title": "2. Methodology", "content": "Each time a user visits a new page it is logged in a clickstream as a page view event P \u2208 P, characterized\nby a small set of contextual features including the page name and timestamp tp. These events collectively\nform user sessions S, representing ordered sequences of the pages visited within defined time intervals.\nFormally, a session S = {P0, P1, ..., PN }, where Pj denotes the jth page the user visited in this session,\nsubject to the condition that\ntp; - P3-1\u2264T, \u2200j\u2208 [1,N].\nHere T is a fixed constant, often in the order of magnitude of a few hours. If the difference in\ntimestamps of two sequential page view events is greater than T, the latter is considered to be in a new\nsession.\nThen for each user, we define their journey J as the chronological sequence of their sessions, where\nJ = {S0, S1, ..., Sk}, with Si representing their ith session. In this way a journey J is the sequence of\npages a user has visited across multiple sessions. We use a corpus of user journeys I = {J0, J1, ...}\ncaptured on a large-scale travel e-commerce site over a few months, where |J| > 50M, and the\nvocabulary exceeds 1000 page names.\nOur objective is to predict future engagement of users using their past navigation patterns on the\nwebsite. Formally, we want to learn a model f : I \u2192 Rd for some positive integer d, which summarises\nthese journeys in rich low dimensional representations that can then be used for downstream machine\nlearning applications, such as content personalisation and product recommendations. As such the\nmodel f must satisfy three main requirements:\n1. Effectively capture the intricate page navigation patterns in users' journeys which span multiple\nsessions.\n2. Meaningfully distill user journeys into embeddings that can predict engagement across diverse\ntasks and contexts.\n3. Scale efficiently to accommodate high-traffic real-time production environments."}, {"title": "2.2. Feature and Position Encoding", "content": "We first crop each input journey, taking up to the L most recent page view events, where L is chosen\nin a way to capture most users' entire recent page view history.\nEach page view event P has a set of categorical attributes, such as a page name and the user's device\ntype, which are passed through their own learnable embedding layer to produce a dense representation\nin R32. We engineer two features from the event timestamp; the time interval between consecutive\nevents and the time elapsed until the most recent event, both logged and standard scaled. Additionally\nwe encode session ID where events in the nth most recent session are given value n. These time-based\nfeatures aim to capture planning phases and session gaps common in extended travel user journeys. All\nfeatures are standard scaled and concatenated s.t. each P is now represented by a vector in RD, where\nD is approximately a few hundred.\nWe also enumerate the event position, where the mth most recent event in the entire journey is given\nvalue m. Then both the event and session position indexes are independently embedded in RD via\ntheir own learnable layers, and added onto the final feature vector, acting as an event-session position\nencoding. This was designed to allow the model to learn representations specific to session and position\ncombinations, enabling it to capture dynamics both within and across multiple sessions more effectively."}, {"title": "2.3. Model Architecture", "content": "In TRACE, we use a transformer encoder architecture to process the input sequences of pages, and train\nit in a multi-task regime across five different targets, representing a variety of future user engagement\nsignals.\nAn encoded journey M\u1ef9 \u2208 RL\u00d7D is passed through a single transformer encoder block, constisting of\na multi-head self-attention layer with 8 heads followed by a position-wise fully connected feed-forward\nnetwork (FFN) with an intermediate dimension of 128. We employ dropout and a residual connection\naround each of the two sub-layers, followed by layer normalization. Global max pooling is applied to\nthe output of this encoder block, before being forward passed through a FFN. For an input journey J\nthe output of this shared backbone is some ej \u2208 Rd.\nThis tensor ej is then passed through five separate task-specific dense layers, each compressing\ndown to a scalar value so the final output of the model is some five logits \u0177 \u2208 R5. After training\nwe then remove the five task-specific heads, and take the output of the shared backbone ej as the\njourney embedding. We deliberately restrict the heads to be simple logistic regression layers. This\napproach encourages the shared backbone to capture most of the nuance, ensuring the embeddings are\ninformation-rich and generalizable, as opposed to relying too heavily on the task-specific layers.\nThroughout the architecture we use ReLU activations, except for the final shared dense layer where\nsigmoid is used for its desirable bounding property. This ensures normalization of the output embedding,\nwith our experiments demonstrating no performance loss. We set dimension d = 32 for the embedding,\nwhich is well suited for downstream applications."}, {"title": "2.4. Multi Task Training Regime and Objective", "content": "The motivation behind the MTL approach is that by jointly predicting a diverse set of user engagement\nsignals, the model is encouraged to learn comprehensive and generalizable representations that can be\neffectively utilized across a variety of downstream applications, extending beyond just the tasks during\ntraining. Furthermore, by mixing the infrequent targets such as purchases, with more common events\nlike product searches, the model learns from a stronger signal and as our results show perform better\non those sparse tasks. This is especially advantageous in the travel domain for events such as bookings,\nas demonstrated in our experiments.\nThe model is trained on five binary classification tasks which represent potential future actions of\na user: (PW2) Make any purchase within two weeks; (BN5) Bounce within next five pages, and the\nfollowing which relate to actions within rest of session; (SRP) Make a search for a product; (PDP) View\na product details page; and (VUO) View an upcoming order. Each task head has its own class-weighted\nbinary cross-entropy loss function. The overall objective is expressed as a linear combination of these\ntask-specific losses. For a journey J with model prediction \u0177 and true labels y, the loss is defined as:\nL(J, y) = \u03a3[WkYk log(yk) + (1 \u2212 yk) \u00b7 log(1 \u2212 \u0177k)].\nClass weights wk are computed as the reciprocal of the proportion of positive samples for each task k,\nin order to account for task-specific class imbalance. We weights tasks equally to encourage the model\nto develop features which generalize across each task."}, {"title": "3. Experimental Results", "content": "Supervised probing techniques have previously been developed to assess linguistic embeddings [19,\n20, 21], although are not directly suited to this scenario. We instead propose a downstream strategy\nfor evaluating the richness of information contained within a set of embeddings. After training, we\ncompute ground truth targets on an unseen test set of historical user journeys. These targets seek to\nencapsulate users' latent psychological states and future intentions. For this, we use the same five tasks\nfrom the TRACE objective in eqn. 2, and introduce three more evaluation tasks that were not previously\nseen. These include: (HOM) whether a user returns to the homepage; (PWS) converts within the current\nsession; and (RE7) whether they return to the site within seven days. This captures a broad scope of\nuser outcomes, allowing us to characterize how well the embeddings generalize. We pass the unseen\ntest journeys through the model to obtain a corresponding set of embeddings. Next, we train XGBoost\nmodels [22] on these test set embeddings. We fit one XGBoost model independently to each unique\nevaluation task and optimize hyperparameters, such as max_depth and learning_rate, using K-fold\ncross validation. The trained XGBoosts then undergo evaluation and we compute performance metrics\non the model predictions. These metrics serve as proxies for assessing the richness of embeddings\nand exemplify downstream model performance across various use cases. Throughout this section, we\nevaluate each upstream embedding model by using the same procedure on the same unseen test set."}, {"title": "3.2. Comparable Models vs TRACE", "content": "We evaluate the quality of TRACE embeddings against several comparable approaches. We express our\ncomparisons as the mean uplift taken over all evaluation tasks. Results are shown in Table 1.\nMyopic Baseline. Our baseline predicts targets using explicit attributes from only the most recent\nevent. We report all results as percentage uplifts from this. TRACE significantly outperforms this,\nhighlighting the benefits of mining a user's full navigation history.\nSingle Task Cohort. To demonstrate the effectiveness of TRACE's MTL approach, we trained a\ndedicated single-task transformer for each of the evaluation tasks. These models each produce an\nembedding. In Table 2, the TRACE score on a given task is compared to the corresponding dedicated\nST model embedding's score. Overall results show that the TRACE embeddings outperform every\ntask-specific equivalent on the 5 tasks TRACE was trained on, and even wins on all but one of the\nunseen targets, demonstrating the advantages of the MTL approach.\nSingle Task Aggregated. Here we combine the task-specific models' embeddings into a single\nembedding of the same length by taking the mean along each dimension.\nMulti-Task LSTM. We note the demonstrated efficacy of LSTMs in related works [14, 17, 23, 24].\nWe train a comparable LSTM minimizing the same multi-task objective function shown in (2).\nMini-GPT. We train a small GPT-style model [25] on the page name sequences, with a single\ntransformer block and causal masking in the attention layer for next event prediction. Embeddings are\ncomputed from the mean of the transformer block outputs."}, {"title": "3.3. Visualisation of Learned Embeddings", "content": "In Fig. 2, we present a visualization of the 32-dimensional embeddings learned by TRACE, reduced to 2\ndimensions using t-SNE [26]. This subset of observations was uniformly sampled with respect to users'\nnext visited page, ensuring equal representation from seven common pages. We note the emergence of\nclusters corresponding to the next page visited by users, despite TRACE never being explicitly exposed\nto this information during training. Qualitatively, the clusters appear to loosely align with how a user\ntraverses a website, going from homepage at the bottom progressing through to search and product\npages, before reaching checkout and order confirmation. This underscores TRACE's ability to identify\nand encode patterns in user journeys, showcasing the effectiveness of our approach for generating\ninformation-rich embeddings."}, {"title": "3.4. Ablation Experiments", "content": "In Table 3 we list the results of our ablation studies."}, {"title": "3.4.1. Position Encodings", "content": "In section 2.2, we discussed our approach to position encoding which is designed to tackle event\nsequences over multiple sessions. Static trigonometric position encodings are also widely popular"}, {"title": "4. Conclusion", "content": "In this work we have presented TRACE, a novel approach for generating user embeddings from\nmulti-session page view sequences through a multi-task learning (MTL) framework, which employs a\nlightweight encoder-only transformer to process real-time cross-session clickstream data. Our experi-\nments on a large scale real world travel e-commerce dataset, demonstrate the superior performance of\nTRACE embeddings compared to traditional single-task and LSTM-based models, and highlights its\npotential for enhancing tourism recommender systems. The learned embeddings exhibit strong results\non a diverse set of targets and demonstrate the ability to generalize well to unseen tasks, underscoring\ntheir utility for applications like content personalization and user modeling. Visualizations reveal that\nTRACE can effectively capture meaningful clusters corresponding to latent user intents and behaviours.\nIn future, we plan to explore the integration of LLMs, as in [28, 29], and investigate hierarchical\nmodels to further improve the model's representational capacity."}]}