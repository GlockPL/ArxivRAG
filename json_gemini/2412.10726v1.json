{"title": "NoisyEQA: Benchmarking Embodied Question Answering Against Noisy Queries", "authors": ["Tao Wu", "Chuhao Zhou", "Yen Heng Wong", "Lin Gu", "Jianfei Yang"], "abstract": "The rapid advancement of Vision-Language Models (VLMs) has significantly advanced the development of Embodied Question Answering (EQA), enhancing agents' abilities in language understanding and reasoning within complex and realistic scenarios. However, EQA in real-world scenarios remains challenging, as human-posed questions often contain noise that can interfere with an agent's exploration and response, bringing challenges especially for language beginners and non-expert users. To address this, we introduce a NoisyEQA benchmark designed to evaluate an agent's ability to recognize and correct noisy questions. This benchmark introduces four common types of noise found in real-world applications\u2014Latent Hallucination Noise, Memory Noise, Perception Noise, and Semantic Noise-generated through an automated dataset creation framework. Additionally, we also propose a 'Self-Correction' prompting mechanism and a new evaluation metric to enhance and measure both noise detection capability and answer quality. Our comprehensive evaluation reveals that current EQA agents often struggle to detect noise in questions, leading to responses that frequently contain erroneous information. Through our Self-Correct Prompting mechanism, we can effectively improve the accuracy of agent answers.", "sections": [{"title": "1. Introduction", "content": "Embodied Question Answering (EQA) is a task where an agent actively explores a scene, builds a comprehensive understanding of the environment, and answers questions about it using natural language [3, 19, 21, 22]. Unlike Visual Question Answering (VQA), which relies on a single static image [2, 16, 27, 33, 38], EQA requires responses based on multiple images collected during the agent's exploration.\nExisting QA systems often assume that the questions posed to them are clear and accurate, leading to challenges when erroneous human presumptions or biases are present. This issue is concerning for EQA, as it must interact with diverse clients and conditions in the real world, where incorrect responses can frustrate or even harm users. The situation is more severe in critical or high-stakes environments. For instance, when assisting an Alzheimer's patient, the agent might be asked to locate a commonly misplaced object, such as glasses. If the agent repeatedly searches this area based on the patient's erroneous presumptions, it not only becomes inefficient but may also collide with obstacles or even the patient, posing serious safety risks. Unlike ideal scenarios with accurate questions, real-world queries often contain ambiguities and inaccuracies. It highlights the need for EQA systems to flexibly handle these noisy questions, thereby enhancing practicality for real-world applications."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Embodied Question Answering", "content": "The Embodied Question Answering (EQA) task aims to navigate a robot agent to an appropriate location in real-world scenarios and enable question-answering through physical interaction with humans. Existing methods for EQA can be divided into two categories: Traditional Neural Networks and those utilizing Large Language Models (LLMs)/Vision-Language Models (VLMs) [19].\nTraditional Neural Network. Das et al. [3] first defined the EQA as a task requiring agents to navigate and answer questions in a real environment. To address this task, most early works employ traditional neural networks with relatively small scales. For example, Das et al. [4] proposed a modular, hierarchical strategy for learning long-range navigation from language input. Yu et al. [35] further extended EQA by addressing questions involving multiple targets.\nLLMs and VLMs. Leveraging the strong reasoning and generalization capabilities of large-scale LLMs and VLMs, EQA has witnessed significant advancements, particularly in complex reasoning, memory integration, and adaptive exploration capabilities [5, 22, 26]. Based on LLMs and VLMs, more complex EQA tasks that are similar to real application can be achieved. For instance, OPEN-EQA [22] focus on EQA tasks with episodic memory (EM-EQA) and active exploration (A-EQA). Based on Open-EQA, agents can memorize history observations and exploration by LLMs equipped with scene graphs, enabling adaptively recall them to answer queries. To explore the real environment more efficiently, Ren et al. [26] leveraged an external semantic map to highlight regions worth investigating, optimizing agents' exploration process by minimizing unnecessary actions."}, {"title": "2.2. Learning with Noisy Labels", "content": "Facilitated by availability of large datasets, deep learning has achieved remarkable success across numerous domains. However, the scarcity of high-quality data remains a significant concern in many real-world applications. To develop robust models that can effectively learn from noisy training data, many methods has been developed for robust learning with noisy labels [8, 23, 25, 28, 32, 37, 39]. For example, Quinonero et al. [25] alleviated the distribution shifts between training and testing data, enhancing model adaptability in real-world scenarios. In the field of EQA, Luo et al. [20] first proposed a noise-robust model that employs a hierarchical approach in the navigation module to filter unreliable labels, along with dual-branch training in the VQA module to enhance the performance in noisy environments. Instead of only considering the noise in training data, Northcutt et al. [24] highlighted that noise in testing data is equally critical and should not be overlooked. Inspire by this, our approach takes the first step in addressing the noise in testing data specifically for EQA tasks, and focuses on alleviating noise that will be encountered in real-world EQA tasks."}, {"title": "2.3. Information Inconsistency", "content": "Information inconsistency is a tricky problem faced by existing Vision-Language Models (VLM). Hallucinations, where the generated language output does not align with the visual input [9, 12, 18, 34], is one of the typical forms of information inconsistency. Oceans of works are proposed to alleviate the hallucinations in VLMs [10, 11, 13, 15, 36]. For example, Li et al. [17] formed hallucination detection as a binary classification problem, aiming at eliminating the misalignment of objects between agent responses and visual cues. However, existing works predominantly concentrate on hallucinations generated by agents and overlook one of the most common types of information inconsistency: misalignment between user instructions and the visual cues. Some researchers have tried to formulate this issue. For instance, Taioli et al. [29] introduced a novel benchmark for Vision-Language Navigation in Continuous Environments (VLN-CE) that simulates realistic instruction errors. Followed by a cross-modal transformer to detect these errors, robustness against inaccurate human instructions is enhanced. Gao et al. [7] also introduced the Self-Contradictory Instructions benchmark to evaluate agents in recognizing conflicting commands, and they proposed the Cognitive Awakening Prompting enhance the detection of such noisy instructions. These methods address specific inconsistency issues, however, a framework for systematically analyzing noise types in human inputs is absent. Our work fills this gap by defining a thorough taxonomy on noise types within human questions from a human-agent interaction perspective, comprehensively modeling and solving the information inconsistency in EQA tasks."}, {"title": "3. Dataset", "content": ""}, {"title": "3.1. Overview", "content": "When querying about agents, humans often unconsciously introduce various presumptions and implicit biases, which can hinder the agents from generating accurate responses. To model these presumptions and implicit biases in practical scenarios, we define them as different types of \"Noise\" and taxonomize them into two main categories: Latent Hallucination Noise and Active Noise. The primary difference between these two types of noise lies in the presence of factual errors in human questions. Active Noise arises from the factual errors caused by humans' misperception or misunderstanding of the environment during interacting with agents. Latent Hallucination Noise occurs when questions refer to non-existing objects or entities within the scene.\nIn this work, we aim to model such mistaken presumptions from humans with finer granularity. As shown in Figure 3, we subdivide the Active Noise into three subtypes: Memory Noise, Perception Noise, and Semantic Noise. Besides, Memory Noise is further classified into five categories based on the forgetting of different object attributes: Position, Counting, Shape, Color and Material.\nIn total, we generate 500 noisy questions corresponding to 4 types of noise. The distribution of \u201cnoise types\" and the \"attributes of Memory Noise\u201d is illustrated in Figure 4 (a) and (b), respectively. Please refer to supplementary materials for more details about the noise types distributions."}, {"title": "3.2. Definition of Noise Types", "content": "In this subsection, we elaborate our definition of different types of noises in the NoisyEQA dataset.\nLatent Hallucination Noise refers that questions from human contain non-existent objects in the scene. Such non-existent objects could trigger the hallucination of the agent, causing it to incorrectly generate fabricated responses that incorporate non-existent items. This kind of noise is common in the tasks of scene understanding. For example, when asked about the location of a non-existent object, the agent tends to fabricate a position rather than indicate that the object does not exist in the scene. This not only compromises EQA accuracy but also severely misleads users in practical applications.\nMemory Noise arises from memory errors, where the human inaccurately recalls an attribute of an object, results in a mismatch between memory and reality. Imagine a scene with a white vase in the living room. If a human incorrectly recalls the vase as being in the bedroom, an example of memory noise about location attribute could be \"Is the vase in the bedroom white?\". For other attributes, similar noise could be introduced into the questions.\nPerceptual Noise originates from inaccuracies in humans' perception of the environment, simulating the visual ambiguities that they may encounter when observing objects in a scene. For instance, a nearsighted person without glasses may misidentify the object A as another incorrect object, B, in the scene. In this case, a question containing perceptual noise could be \"What's the shape of object B?\".\nSemantic Noise is caused by the incorrect substitutions of objects in a scene when humans attempt to describe the environment. Unlike perceptual noise, which involves visual misidentification of objects, semantic noise contains logically feasible substitutions with semantically related but incorrect objects. Intuitively, in a living room, substituting \"fireplace\" with a semantically related object (e.g., \"stove\") could introduce such noise."}, {"title": "3.3. Paradigms of Question Generation", "content": "Based on our definitions of different types of noise, we present a systematic framework powered by structured templates and LLMs to automatically generate noisy questions, as shown in Figure 6. Inspired by OpenEQA [22], we focus on generating open-ended question-answer pairs to better simulate the challenges of real-world environments.\nFor Latent Hallucination Noise, we use both Wh-questions and yes/no-questions templates, such as \"Where is the <OBJ>?\" and \"Is the <OBJ> {Attribute}?\u201d. The non-existent objects <OBJ> and the {Attribute} aim to induce hallucinated responses and simulate the erroneous human presumptions, respectively. Given a scene, we first establish a list of all present objects, then introduce a non-existent object with a random attribute to form a noisy question. For example, if a living room contains a table, chair, and book, one of the generated questions with latent hallucination noise could be \"Where is the vase in the living room?\" or \"Is the vase in the living room white?\" Although this question seems to be reasonable, it introduces a phantom object (vase), increasing the likelihood of hallucinated responses.\nAs for Active Noise, the facts (red boxes in Figure 6) are pre-defined by assigning each object known attributes. We then introduce different types of Active Noise by modifying or substituting either the object or its attribute. In this way, factual errors between the pre-defined facts (or scenes) and the noisy questions are introduced. Except for Wh-questions, a yes-no question template is also utilized to introduce additional attributes ({Attribute2}), simulating the erroneous presumptions from humans on object attributes. By this way, noisy questions that contain both factual errors and human erroneous presumptions can be generated.\nConcretely, for Memory Noise, we first select an object with its original attributes from a scene, then replace the attribute with an incorrect one to introduce noise. For instance, to create memory noise related to the location attribute, we select a vase that located in the 'living room' and change its location to the 'bedroom', then ask \"What color is the vase located in the bed room?\" or \"Is the vase located in the bed room white?\". This method can similarly be applied to other attributes, allowing us to comprehensively evaluate the robustness against various recall errors.\nTo generate Perceptual Noise, we add Gaussian noise to the original image of the scene to simulate perceptual confusion. Subsequently, a Vision-Language Model (VLM) [1] is utilized to generate a description of the disturbed image, as shown in Figure 5, some objects are incorrectly described due to the distortion (marked by red). We then construct questions with perceptual noise based on these misidentified objects. According to Figure 5, the sofa near the television is recognized as a bed, and a noisy question is formulated as \"What's the color of the bed near the television?\" or \"Is the bed near the television made of wood?\".\nFinally, we generate Semantic Noise by substituting the object in the scene with semantic-related but incorrect object. Particularly, objects from both OpenEQA [22] and HMEQA [26] datasets are gathered, and BERT is adopted to embed all candidate objects. We then create a pairwise similarity matrix to indicate the semantic correlations among objects. For an original object in each question, we first select the objects with top-10 similarities to form a subset of candidates. Then, the logically related object is further chosen from the subset by humans according to two principles: category overlap or functional association. Specifically, category overlap is defined by widely accepted taxonomy, for example, 'sofa' and 'armchair' are both belonged to 'furniture'. Functional association considers objects with similar roles in the same scene, for example, 'fireplace' and 'stove' both support for heat function. Consequently, we replace the original object with a logically related one to introduce the semantic noise. Take the scene of living room as an example, an question with semantics noise could be \"What material is the stove in the living room?\" or \"Is the stove in the living room silver?\" while the \u201cstove\u201d is a \u201cfireplace\u201d in reality.\nAfter generating the dataset, we recruit two dedicated volunteers to conduct a thorough review and validation. The volunteers meticulously examine each entry for inconsistencies, errors, and potential biases. Additionally, they also verify that the generated data accurately contains the intended noise types across various scenarios."}, {"title": "4. Self-Correction Mechanism", "content": "Once the presumptions and implicit biases are incorporated into the human questions (noisy questions), the accuracy of responses generated by agents will significantly degrade. To address this issue, we propose a Self-Correction mechanism to enhance the robustness of agents against noisy questions.\nAs illustrated in Figure 7 (a), for a scene with multiple views, the noisy question is marked by blue. Instead of directly answering the question, we design an additional \"confidence question\" in yes/no format (which is marked by green) for the agent to answer first. By taking the probability regards to \"yes\" token as confidence score, the agent could identify the most reliable view from the scene. Then, the agent equipped with our Self-Correction mechanism answers the noisy question based on the selected view. The Self-Correction mechanism has two formats: Noise Aware Prompt (NAP) and Noise Aware Chain of Thought (NA-CoT), which are elaborated in the following."}, {"title": "4.1. Noise Aware Prompt (NAP)", "content": "The Noise Aware Prompt (NAP) adopts an extra cognition prompt to enhance agents' sensitivity towards potential noise in human questions. As shown in Figure 7 (b), a simple yet effective prompt is appended to the input of the agent, which reminds the agent to carefully review the question-related information before generating a response. Instead of altering the reasoning process of the agent, NAP encourages the agent to aware the mismatches between human questions and visual clues. By this way, the agent achieves to identify the noisy question more accurately, resulting in more robust and reliable responses."}, {"title": "4.2. Noise Aware Chain of Thought (NACoT)", "content": "Instead of simply prompting the agent to recognize the noise in human questions, we explore a more fine-grained mechanism, called Noise Aware Chain of Thought (NACoT), to guide the agent in detecting and addressing noise step by step. Particularly, NACoT deconstructs a specific question into key components, and systematically prompts the agent to verify whether each component is consistent with the visual content in the scene. As shown in Figure 7 (b), this processes starts with object identification, ensuring that all objects mentioned in the questions are presented in the scene. Followed by attributes checking, the agent makes sure that the attribute of objects are consistent between questions and real scene. Finally, the function and semantics verification utilized to detect any semantic noise. Once any discrepancies are detected, the agent will provide a clear explanation of the detected noise, then the expected responses that not contaminated by noises can be adaptively generated."}, {"title": "5. Evaluation Criteria", "content": "As discussed in section 1, existing metrics for question-answering systems (e.g., accuracy) are insufficient for evaluating the agents' performances in the presence of noisy questions. Therefore, we propose a novel evaluation framework that simultaneously considers both the quality of responses and the robustness towards noises."}, {"title": "6. Experiment", "content": "In this section, we thoroughly evaluate the performance of different agents in addressing noisy questions."}, {"title": "6.1. Implementation details", "content": "Baseline Agents. Two common Vision-Language Models (VLMs) are selected as baseline agents, which show strong question answering and spatial reasoning capabilities based on clean questions. Following Prismatic-VLMs [14], we adopt the mixed DINO+SigLIP as our vision encoder, and replace the LLM with Llama-2 and GPT-4o to formulate our agents, which are denote as \"Llama2-EQA\" and \"GPT-4o-EQA\", respectively. Notably, the probability of the predicted tokens is necessary in our method to select the most reliable view from the scene. Therefore, LLMs whose probabilities are inaccessible are not included in our experiments, such as Gemini [30], Claude, and others.\nDataset and Evaluation Metric. All experiments are conducted using our generated NoisyEQA Dataset, which consists of 500 noisy questions in total. For evaluation, the three metrics: Accuracy, Detection Rate and Correction Rate, are adopted as described in section 5. Additionally, two volunteers are recruited to provide human responses to the noisy questions, creating a benchmark for comparing the capability of agents versus real human."}, {"title": "6.2. Evaluation results", "content": "The evaluation results for different types of noisy questions for Llama2-EQA and GPT-4o-EQA are listed in Table 1 and Table 2, respectively. As shown in the tables, the original Llama2-EQA and GPT-4o-EQA agents exhibit low performance when noise is present the questions. This suggests that the presumptions or implicit biases in human questions pose challenges even for agents with strong question-answering capabilities. It highlights that these noises do hinder the application of agents on EQA tasks in the real-world, forming an issue that must be addressed. By incorporating NAP or NACoT for self-correction, both agents show significant improvements across various types of noisy questions. It fully demonstrates the effectiveness of our proposed self-correction mechanism. However, the best results of our agents (i.e., GPT4o-EQA+NACoT) still fall short of real humans performance, especially for the Active Noise (i.e., Memory, Perception and Semantics Noise). This indicates that noisy questions containing factual errors are extremely challenging to correct. In contrast to identifying of non-existent objects, we conjecture that the agent requires more fine-grained and complex reasoning to detect and correct the factual errors related to the objects.\nFurthermore, we show how NAP and NACoT enhance agents' robustness to noises. As shown in Table 3, our NAP and NACoT achieve to improve agents by significantly boosting the detection and correction rates of the noisy questions. (Please refer to our supplementary materials for more visualization results.) Besides, although both agents achieve relatively high detection rate, the correction rate remains much lower compared with humans. Correcting noisy questions requires to trigger agents' reasoning capability at more fine-grained level, we leave it as an open question for future works."}, {"title": "7. Conclusion", "content": "In this paper, we introduce the NoisyEQA benchmark, a dataset specially designed to evaluate agents' capability towards noisy human questions. To fully simulate human-posed questions in the real-world scenario, we define four types of noise including Latent Hallucination, Memory, Perception and Semantic noise. Based on these noise types, we systematically generated 500 noisy questions using a novel LLM-based framework, which can be easily scalable. By evaluating various baseline agents on our NoisyEQA benchmark, we reveal that our dataset essentially model the issues existed in real-world EQA tasks, forming a problem must be solved for practical application. To this end, we propose a simple yet effective self-correction mechanism with two formats to help agents detect and correct noise before answering. significantly boosting the robustness of agents. Finally, two novel evaluation metrics that separately focus on noise detection and correction are proposed, providing more thorough evaluation for agents in the real-scene. This work paves the way for building reliable agents for real-word application and a more comprehensive benchmark exploring the noise at finer-grained level is underway."}, {"title": "1. Transparency and Accountability", "content": "In this supplementary materials, we first illustrate that NoisyEQA achieves AI transparency and accountability in section 1. Then, more details of existing datasets and evaluation metrics are elaborated in section 2 and section 3, respectively. Eventually, we provide more experiment results and comprehensive analysis regarding to noise impacts and the effectiveness of our methods in section 4."}, {"title": "1.1. AI Transparency in NoisyEQA", "content": "Transparency in AI systems is essential for reliable [6] and interpretable decision-making, especially in noisy scenarios addressed in NoisyEQA. In this work, we propose the Self-Correction mechanism to tackle the transparency gap in Embodied Question Answering (EQA) agents. This mechanism enables agents to identify noise in user queries before generating corrected responses. By separating the detection and correction phases, the reasoning process of the system becomes explicit, allowing users to understand both the identification of noises and the rationale behind corrections. This enhances the transparency in agents and makes the responses more trustworthy and interpretable. In contrast, correcting without an explicit detection of noise makes the decision-making process obscure and harder for users to trust the corrected responses. Furthermore, our detailed evaluation framework, incorporating Detection Rate (DR) and Correction Rate (CR) metrics, offers a more transparent and fine-grained understanding of the agents' performance. By separating noise detection and correction, our approach aligns with transparency principles. It enables users and developers to trace the reasoning process, identify potential vulnerabilities, and correct agents' decisions in time."}, {"title": "1.2. AI Accountability in NoisyEQA", "content": "Accountability in AI systems is crucial to ensure that every phase of development, deployment, and operation are traceable to responsible actions [31]. In the context of NoisyEQA, the Self-Correction mechanism assures accountability by compelling agents to both detect noise and suggest actionable corrections. It guarantees that agents' decisions are validated and can be scrutinized at every step, minimizing the risk of error propagation. Additionally, the Self-Correction mechanism can incorporate a human-in-the-loop processes, which allows humans to review and confirm critical decisions. In this way, the agents completely meet regulatory standards and ethical requirements, ensuring the robust and trustworthy responses. The stepwise accountability, achieved through the combination of detection, explanation, and correction, promotes the development of responsible agents capable of effectively managing real-world noise."}, {"title": "2. More Details of Existing Datasets", "content": "As shown in Figure 9, we observed that in existing OpenEQA [22] and ExploreEQA [26] datasets, \"Position\" constitutes the largest proportion of all attributes in the questions. This aligns with the natural tendency of humans that we usually rely on positional information to identify different objects in a scene, especially when multiple similar objects are presented. Followed the observation, our dataset also treats \"Position\" as a primary attribute. By this way, our dataset can better simulate realistic interactions, where users frequently depend on location for precise objects identification, enhancing its applicability and usability in real-world scenario."}, {"title": "3. Evaluation Details", "content": ""}, {"title": "3.1. Evaluation Scale", "content": "To comprehensively evaluate the quality of responses from question-answering agents in the presence of noisy questions, we elaborately design a scoring scale ranging from 1 to 5. The evluation scale assesses three key aspects: noise detection, noise correction, and accuracy of the generated response. The detailed criteria for each score are outlined as follows:"}, {"title": "4. More Experiment Results", "content": ""}, {"title": "4.1. Impact of Noise on Response Confidence", "content": "Figure 2 in the manuscript shows that the noise in the question will significantly decrease the generation accuracy. Except for it, we further show that the impact of noise can deteriorate the response confidence as well. Recalling figure 7(a), the response confidence helps the agents to select the most reliable view to answer human questions. Therefore, the deterioration of response confidence will hinder the agents to choose the optimal view from the scene and indirectly make the responses worse. As presented in Figure 10, the agent consistently shows lower response confidence across all types of noisy question compared with the clean ones. Notably, more remarkable confidence drop is observed in categories of Latent Hallucination Noise and Memory Material Noise. We conjecture that it can be attributed to the essential of these tow noise types: they either introduce non-existent object or objects with totally incorrect attribute, which incorporates a greater impact on view selection. Conversely, Perception Noise or Semantic Noise separately involves visually similar or semantically related objects, which introduces milder impacts on view selection but needs more complex reasoning ability from agents to detect and correct. The facts revealed by Figure 2 and Figure 10 underline the dual impact of noise on both accuracy and response confidence, underscoring the necessity of our work to systematically investigate these noises and to develop noise-robust agents in real-world."}, {"title": "4.2. More Analysis on Noise Detection and Correction", "content": "We visualize the Detection Rate (DR) and Correction Rate (CR) regarding to various types of noise in Figure 11 to intuitively show the effectiveness of our method. Besides, we conduct a more comprehensive analysis on the difference in effectiveness of our method when dealing with different noises."}]}