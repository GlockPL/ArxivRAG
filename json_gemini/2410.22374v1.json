{"title": "Machine Unlearning using Forgetting Neural Networks", "authors": ["Amartya Hatua", "Trung T. Nguyen", "Filip Cano", "Andrew H. Sung"], "abstract": "Modern computer systems store vast amounts of personal data, enabling advances in AI and ML but risking user privacy and trust. For privacy reasons, it is desired sometimes for an ML model to forget part of the data it was trained on. This paper presents a new approach to machine unlearning using forgetting neural networks (FNN). FNNS are neural networks with specific forgetting layers, that take inspiration from the processes involved when a human brain forgets. While FNNs had been proposed as a theoretical construct, they have not been previously used as a machine unlearning method. We describe four different types of forgetting layers and study their properties. In our experimental evaluation, we report our results on the MNIST handwritten digit recognition and fashion datasets. The effectiveness of the unlearned models was tested using Membership Inference Attacks (MIA). Successful experimental results demonstrate the great potential of our proposed method for dealing with the machine unlearning problem.", "sections": [{"title": "1 Introduction", "content": "Machine learning (ML) has become pivotal in various daily applications. At the same time, concerns have been raised regarding its ethical and legal consider-ations. Governments across the globe are taking steps to regulate the use of artificial intelligence (AI), ensuring that algorithmic decisions and data usage practices are both ethical and safe [29,14,32,3].\nOne of the main concerns across the public and regulating bodies is that of privacy [1,4,2]. The abundance of data has allowed machine learning models to achieve impressive performance, sometimes at the cost of using personal data, which can lead to privacy vulnerabilities if the personal data can be retrieved from an ML model. This motivates the need for solutions to make an ML model forget certain part of the data it has been trained on."}, {"title": "Machine Unlearning", "content": "Machine unlearning [34] systematically removes the influence of specific data points from a trained machine learning model. This procedure ensures that the model behaves as if the data points in question had never been part of the training set, effectively forgetting them. Machine unlearn-ing has gained traction in the research community because of its implications in terms of responsible AI usage [28]. For example, let $D = \\{x_i, y_i\\}_1^n$ be a dataset for a machine learning problem, divided into the train (DTrain) and test (DTest) dataset. For machine unlearning, the training dataset (DTrain) is further divided into two parts forget set (DF) and retain set (DR) such that, DTrain = DRUDF while {DR\u2229 DF = 0}.The forget set is a subset of the train-ing dataset that contains sensitive information and is already used during the training phase of the model. The objective of the unlearning phase is to remove the effect of the sensitive information from the trained model."}, {"title": "Forgetting Neural Network", "content": "Introduced in [9], A forgetting neural network (FNN) is a type of neural network that incorporates forgetting layers, inspired by the Ebbinghaus forgetting curve [15] (Figure 1), which describes the information learning and forgetting pattern in human brains. Ebbinghaus's forgetting curve shows that memory retention decreases sharply after learning and can be im-proved through repetition and active recall. Regular review is essential for ef-fective learning. While FNNs have been previously studied as a theoretical con-struct [9,8], they have not been previously used as a method for machine un-learning. Our research focuses on implementing machine unlearning by enabling the forgetting layers in deep learning models."}, {"title": "Membership Inference Attacks (MIA)", "content": "[30], [11] are widely used to test the effectiveness of the machine unlearning algorithm. In an ideal scenario, the score of the MIA should be 0.5, which suggests the unlearned model cannot distinguish between the distribution of the test and the forget dataset. We can represent it using the MIA using a likelihood test as represented in formula 1. In an ideal machine unlearning scenario, the value of formula 1 would be 0.5. F(x; 0) denotes"}, {"title": "Machine Unlearning using Forgetting Neural Networks", "content": "a function parameterized by trainable parameters (0) and L is the loss function.\n$P(L((F(x; 0), y)|DRUDF)$\n(1)\nThe primary objective of machine unlearning algorithms is to enhance the trustworthiness of machine learning systems while safeguarding sensitive infor-mation. However, it is crucial to acknowledge that this process may reduce the available training data, potentially impacting the model's performance. There-fore, machine unlearning algorithms aim to strike a delicate balance between learning and unlearning data. This balance is essential for achieving an optimal position that ensures both the model's accuracy and the forget datasets' removal. Therefore, the objectives of the machine unlearning task can be represented as Equation 2 and 3.\n$f_1(0) = min(\\frac{1}{N}\\sum_{i=1}^{N} L(F(x; 0), y_i))$\n(2)\n$f_2(0) = max P\\{[L(F(x;0), y)|(DRU DF) \u2013 0.5] = 0\\}$\n$= min(1 \u2013 P\\{[L(F(x; 0), y)|(DRU DF) \u2013 0.5] = 0\\})$\n(3)\nCombining Equation 2 and 3, we can have a single objective function in Equation 4. In our research, we have employed heuristic methods to determine the optimal set of hyperparameters conducive to minimizing loss (or maximizing accuracy) and achieving an MIA score close to 0.5.\n$min(f(0)) = min(\\frac{1}{N}\\sum_{i=1}^{N} L(F(x; 0), Y_i)+$\n(1 \u2013 P\\{[L(F(x; 0), y)|(DRUDF) - 0.5] = 0\\}))$\n(4)\nIn the present study, we have implemented the FNN algorithm to train mod-els for the widely used MNIST handwritten digit recognization (HDR) [12] and fashion datasets [33]. The main contributions of this paper are:\n1. Provide the first implementation of forgetting neural networks, providing an empirical demonstration of their theoretical properties.\n2. Introduce a method for machine unlearning with forgetting neural networks.\n3. Empirical evaluation of our framework on the MNIST digit and fashion datasets.\nThe following sections of the paper present the related work, preliminaries, proposed methodology, implementation details, results, and discussion."}, {"title": "2 Related Work", "content": "This section will summarize some related machine unlearning techniques and Membership Inference Attack (MIA) models."}, {"title": "2.1 Machine Unlearning Techniques", "content": "According to related surveys of Machine Unlearning [6,24,26,35], there were two general unlearning paradigms: exact unlearning and approximate unlearning."}, {"title": "Exact unlearning", "content": "For a given dataset D, the forget set Df and a machine learning algorithm A, an exact unlearning algorithm U needs to find the model after unlearning M' = U(D, Df, A(D)) so that M' is in the same distribution of the model M* = A(D \\ Df) [24]. M* is the perfect model if we retrain from scratch on the original dataset D after removing the samples of the forget set Df. Retraining requires high computational costs and cannot handle multiple training configurations, e.g., batch settings."}, {"title": "Approximate unlearning", "content": "Compared to exact unlearning, approximate un-learning algorithms try to find the unlearning model M' that can approximate the perfect unlearning model M* with a small deviation e, satisfying the follow-ing condition: $\\epsilon = \\frac{Pr(U(D,Df,A(D))}{Pr(A(D\\Df))} < e$ in which Pr(M) defines the prob-ability distribution of the trained model M. Approximate unlearning has the advantage of smaller retraining costs compared to perfect unlearning, which in-volves retraining the model from scratch (after removing the forget samples from the original dataset). Approaches for approximate unlearning can be classified based on the target traditional machine learning or neural network-based models (including deep learning ones)."}, {"title": "Unlearning for traditional machine learning models", "content": "Some early machine un-learning techniques were built for Support Vector Machines (SVM) models, such as [13,20]. In 2015, Cao et al. proposed a general machine unlearning approach, which involved transforming the machine learning algorithms into a summation form [10]. Forgetting samples was done by updating several summations to re-move the related residual linage from the target model. However, this method has limitations regarding applicable learning models and granularity levels. In 2021, Brophy and Lowd proposed a machine unlearning algorithm for Random Forest [7]. Their proposed Data-Removal-Enabled (DARE) forests require the caching of statistics of each node to update the pre-trained model for unlearning forgetting samples. Later, attempts were made to develop unlearning algorithms based on the Bayesian model [23] or K-means clustering [16]."}, {"title": "Unlearning for neural network-based models (deep learning also)", "content": "As neural network models, especially deep learning ones, can efficiently learn the target data distribution and are widely used in multiple disciplines, there has been a lot of research to develop unlearning algorithms for these models. Kim et al. employed a bias prediction network to unlearn the biased information from the previous feature embedding network [21]. Golatkar et al. proposed a method to estimate the information of forgetting samples in the weights of the learned deep neural network (DNN) [18]. Later, they improved their approach with the linear approximation of standard deep network to transform the unlearning problem into quadratic form [17]. Chundawat et al. proposed a student-teacher framework"}, {"title": "Machine Unlearning using Forgetting Neural Networks", "content": "that uses a Zero Retrain Forgetting (ZRF) metric to selectively transfer the existing model to obtain a new model that does not contain the forget samples. In [19], the authors proposed the Projected Gradient Unlearning (PGU) method, in which the knowledge of the retaining dataset is preserved by following the orthogonal direction of their gradient subspaces during the stochastic gradient descent process."}, {"title": "3 Preliminaries", "content": null}, {"title": "3.1 Forgetting Neural Networks", "content": "Forgetting Neural Networks (FNN) is a modification of the classical multilayer perception introduced in [9], that adds a multiplicative forgetting function to some of the trainable parameters of the network."}, {"title": "Single-layer perceptron", "content": "Let \u2286 R\" be the domain and \u03c3: R \u2192 R is the activation function of a neural network trained on parameters (a's, w's, and b's) as in equation 5. The \u201ctrained\" values are denoted with an asterisk in the superscript (for example, w*).\n$\u03c3((x, w(t)) + b(t)) = y(t)\u03c3((x, w*) + b*)$\n(5)\nFor the rest of the discussion, we can consider a shallow neural network of N units. A shallow neural network is a function for \u03a3: \u03a9 \u2192 R, which is represented in equation 6, where ak \u2208 R.\n$\u03a3(x) = \u03a3\u03b1\u03ba\u03c3((x, wk) + bk$\n(6)\nFor the time (t) parameter, a shallow network as equation 6 can be repre-sented as equation 7.\n$\u03a3(x;t) = \u03a3\u03b1\u03ba(t)((x, wk(t)) + bk(t))$\n$\\sum_{k=1}^{N}\u03b1\u03c6^{2}(t)((x, w)+b)$\n$\\varphi^{2}(t)\u2211(x; t = 0)$\n(7)"}, {"title": "Deep Networks", "content": "Suppose we have a deep learning model with d layers and the forgetting function (t) is present in the i-th layer As shown in equation 7, the output of that layer will be multiplied by \u03c62(t). This is the input of the following layer, so in the (i + 1)st layer, the input will be multiplied by \u03c62(t), and the bias will also be multiplied by \u03c62(t). Using the symmetric property of"}, {"title": "Machine Unlearning Using Forgetting Neural Networks", "content": "the scalar product and analogous reasoning as in equation 7, we can conclude that the output of the (i + 1)st layer is multiplied by \u03c62(t). This reasoning can be repeated with the layers above, and we can conclude that this \u03c62(t). directly affects the result of the entire network. In general, if we can implement different forgetting functions for each layer, ok(t) for k = 1, ..., d, then the whole network \u2211 behaves as:\n$\u03a3(x; t) = \u03a0(t) (x; t = 0)$\n(8)"}, {"title": "4 Proposed Method: Machine Unlearning Using FNN", "content": "This research proposes a novel algorithm for machine unlearning that draws in-spiration from the 'Theory of Intelligence with Forgetting'. The proposed algo-rithm is implemented using FNN and exhibits a similar pattern to Ebbinghaus's [31] hypothesis of learning and forgetting."}, {"title": "4.1 Fixed Forgetting Rate vs Varying Forgetting Rate FNN", "content": "As the parameters of the network are time-dependent, we can model the forget-ting of the network with a forgetting function \u03c6(t) such as a\u2081(t) = a\u2081.\u03c6(t). The formula in Ebbinghaus's hypothesis [31] can be used as the forgetting function: \u03c6(t) = e-t/\u03c4 (where \u03c4 is the stability of memory or forgetting rate). The for-getting network is called a Fixed Forgetting Rate (FFR) model if the forgetting"}, {"title": "Machine Unlearning using Forgetting Neural Networks", "content": "rate is fixed for all neurons; otherwise, it is Varying Forgetting Rate (VFR). The VFR FNN can be further classified into four different types:\n1. Rank forget rate: Neurons in the fully connected layer are assigned a forgetting rate (\u03c4) based on their activation level. The most highly activated neuron has the highest rank.\n2. Ordered forget rate: Neurons in the fully connected layer are assigned a forgetting rate (\u03c4) based on their position in the fully connected layer.\n3. Top 30 forget rate: Neurons in the fully connected layer are assigned a forgetting rate (\u03c4) for the top 30 activated neurons based on their activation level, and the rest of the neurons are assigned a constant value as forgetting rate.\n4. Random forget rate: Randomly assigned forgetting rate (\u03c4) for all neurons in the fully connected layers."}, {"title": "5 Experimental Evaluation", "content": "In the previous section 4, we have explained the theoretical underpinnings of FNN and its various types. In this section, we aim to provide a comprehensive account of the implementation details of different FNN types using two distinct datasets and their respective results. Furthermore, we also provide the intricacies of calculating the MIA for the forget dataset and other pertinent implementation details of the controllable machine unlearning model. Since there is a lower bound on the size of the training samples for any machine learning model to achieve a performance level exceeding a given threshold [27], it is assumed that the size of the retain set is always sufficient during machine unlearning."}, {"title": "5.1 Data", "content": "In our research, we utilize two datasets: the MNIST handwritten digit recognition dataset [12] and the MNIST fashion dataset [33]. The MNIST handwritten digit recognition dataset includes 60,000 training and 10,000 testing images. We divide the training dataset into two parts: the retain dataset, which contains 50,000 data points, and the forget dataset, which comprises 10,000 data points. Similarly, we create classes with equal data points for the MNIST fashion dataset."}, {"title": "5.2 Forgetting Neural Network Setup", "content": "Figure 2 depicts the schematic diagram of the implemented FNN model. We have implemented two forget layers present after two fully connected layers. Using this model, we have performed two sets of experiments to test the effect of the forget layers. Only the last forget layer (FL2) is present in one set of experiments (1-L), while both are in the second set of experiments (2-L). The forgetting rates (T) are selected by trial and error method. The FNNs with the chosen forgetting rates display behavior similar to the Ebbinghaus forgetting curve and yield the best results."}, {"title": "Fixed forgetting rate FNN", "content": "In Figure 3, the FFR FNN learning-unlearning graph (one-layer forgetting (1-L) and two-layer (2-L) forgetting) is plotted for both datasets. The graph displays the accuracy over time, with markers indi-cating the epochs (time). We have executed five learning-unlearning turns, each consisting of two epochs for learning and four for unlearning."}, {"title": "Varying forgetting rates FNN", "content": "Varying forgetting rates FNN employs four distinct types of forgetting rates for both One-layer and Two-layer forgetting variants. Figures 4a and 4b illustrate the learning-unlearning curves for the one-layered and two-layered FNN models, respectively, using the MNIST handwrit-ten digit recognition dataset. The figures feature four subplots, each displaying the learning-unlearning curves for four types of learning rates, i.e., rank, ordered, random, and top 30 forget rate.\nThe rank forget rate closely follows the Ebbinghaus forgetting curve. The ordered forget rate exhibits a similar pattern to the Ebbinghaus forgetting curve, but its consistency is not always guaranteed. The random forget rate shows a random change in accuracy in the learning-unlearning curves. The top 30 forget rate does not exhibit any gradual improvement in accuracy for the unlearning phase, and over time, it shows little change in learning and unlearning patterns. Similar experiments are also conducted on the MNIST fashion dataset, and the results were plotted in Figure 5a and 5b. Results showed that similar to the MNIST HDR, the rank forgetting rate also produced the best results for the MNIST fashion dataset.\nUpon analyzing the various variants of FNN, we have observed that the FNN variant with two-layer forgetting using a rank forgetting rate function produces a learning-unlearning curve similar to the Ebbinghaus forgetting curve. Thus, this FNN variant is deemed the most optimal model for FFR and VFR variants. Subsequently, further experiments are conducted to calculate the MIA only on these models."}, {"title": "5.3 \u039c\u0399\u0391 Calculation", "content": "In Section 5.1, we discussed dividing the original dataset into retain and for-get datasets. The forget dataset determines the MIA scores during the learning-unlearning phase. One of the objectives of this research is to identify the optimal"}, {"title": "A. Hatua et al.", "content": "point that maximizes accuracy and has the best MIA score. To achieve this ob-jective, we calculate the MIA score at each epoch during the learning-unlearning phase. Using predefined acceptance criteria for the optimal point, we can stop the training process, similar to any early stopping criteria, and save the model for inference. To implement MIA, we have used the Adversarial Robustness Toolbox (ART) [25]."}, {"title": "6 Conclusion", "content": "In this paper, we have presented a novel approach to machine unlearning using FNN. This is the first time that FNN has been employed for machine unlearning. The FNN results corroborate the theory and can generate a learning-unlearning curve similar to the Ebbinghaus forgetting curve. We have implemented different variations of FNN with multiple positions and types of forget functions. Our results suggest that i) If the forget function is present at least in the last fully connected layer of the deep neural network, then the network can behave like an FNN; ii) 'rank forget rate' is the most efficient type of forgetting function in the current context. Furthermore, we have been able to strike a balance between the accuracy of the machine learning model and achieving an optimal MIA score; iii) Multiple phases of the learning-unlearning lead to less overfitting of the training data and better generalization of the testing data. Moreover, we compared the results with a baseline method, and the experiment results showed that FNN-based machine unlearning performed more efficiently than the baseline model. This confirms that our implementation is a successful attempt to implement machine unlearning using FNN. We plan to extend this work to improve the fairness of models and eliminate bias."}]}