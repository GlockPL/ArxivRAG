{"title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models \u2014 The Story Goes On", "authors": ["Liang Zeng", "Liangjun Zhong", "Liang Zhao", "Tianwen Wei", "Liu Yang", "Jujie He", "Cheng Cheng", "Rui Hu", "Yang Liu", "Shuicheng Yan", "Han Fang", "Yahui Zhou"], "abstract": "In this paper, we investigate the underlying factors that potentially enhance the mathematical reasoning capabilities of large language models (LLMs). We argue that the data scaling law for math reasoning capabilities in modern LLMs is far from being saturated, highlighting how the model's quality improves with increases in data quantity. To support this claim, we introduce the Skywork-Math model series, supervised fine-tuned (SFT) on common 7B LLMs using our proposed 2.5M-instance Skywork-MathQA dataset. Skywork-Math 7B has achieved impressive accuracies of 51.2% on the competition-level MATH benchmark and 83.9% on the GSM8K benchmark using only SFT data, outperforming an early version of GPT-4 on MATH. The superior performance of Skywork-Math models contributes to our novel two-stage data synthesis and model SFT pipelines, which include three different augmentation methods and a diverse seed problem set, ensuring both the quantity and quality of Skywork-MathQA dataset across varying difficulty levels. Most importantly, we provide several practical takeaways to enhance math reasoning abilities in LLMs for both research and industry applications.", "sections": [{"title": "1. Introduction", "content": "Reasoning ability is a hallmark of human intelligence (Gendron et al., 2024; Huang and Chang, 2022; Wei et al., 2022b). Although Large Language Models (LLMs) have recently demonstrated significant capabilities in various tasks such as conversation (Achiam et al., 2023; Anthropic, 2024; Peng et al., 2023) and summarization (Almazrouei et al., 2023; Scao et al., 2022; Wei et al., 2023b; Yang et al., 2023), they often struggle with complex reasoning tasks (Gendron et al., 2024; Lu et al., 2023; Wu et al., 2023). One particularly challenging area is mathematical reasoning (Arora et al., 2023; Cobbe et al., 2021; He et al., 2024; Hendrycks et al., 2021; Zhong et al., 2023), which requires the ability to solve mathematical problems and derive logical conclusions in a step by step manner (Saxton et al., 2019; Shao et al., 2024; Toshniwal et al., 2024; Wei et al., 2022b; Yu et al., 2024).\nTwo prevailing beliefs guide researchers and practitioners in enhancing mathematical reasoning abilities of LLMs. The first belief posits that complex reasoning abilities, especially mathematical reasoning, are emergent abilities that exist in large language models but not in small models (Wei et al., 2022a,b). Typically, models with more than 30 billion parameters exhibit the strong mathematical reasoning ability (Brown et al., 2020). The second belief is the seminal \"superficial alignment\" hypothesis (Zhou et al., 2023), which asserts that \"A model's knowledge and capabilities are learnt almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used when interacting with users.\". According to this hypothesis, the alignment process, primarily through supervised fine-tuning (SFT), does not inject new knowledge or improve inherent abilities but rather adjusts the output response format. This implies that the strong mathematical reasoning ability may not be significantly improved by a large amount of synthetic SFT data.\nIn this paper, we re-examine these two common beliefs mentioned above regarding mathematical reasoning abilities of LLMs. For the first belief, we introduce the Skywork-Math model series, which are supervised fine-tuned (SFT) on common 7B pre-trained LLM models without employing other complex alignment techniques such as RLHF (Bai et al., 2022; Casper et al., 2023) and DPO (Rafailov et al., 2024). Skywork-Math 7B models have achieved impressive accuracies of 51.2% on the competition-level MATH (Hendrycks et al., 2021) benchmark and 83.9% on the GSM8K (Cobbe et al., 2021) benchmark, notably outperforming an early version of GPT-4 on MATH. Our empirical findings, consistent with the conclusions in Li et al. (2024), suggest that strong mathematical reasoning ability can indeed exist in common 7B language models. Moreover, scaling up synthetic SFT data can further enhance the mathematical reasoning ability of Skywork-Math 7B models.\nFor the second belief, we propose Skywork-MathQA high-quality SFT dataset containing 2.5 million instances, which is much larger than open-sourced dataset of its kind to date, such as MetaMathQA (Yu et al., 2024) containing 395K samples. We empirically observe that the scaling law curve on the SFT alignment for mathematical reasoning in modern LLMs is far from being saturated (ref. Figure 5). We have carefully scaled the Skywork-MathQA SFT dataset with diverse and high-quality samples specifically within the mathematical domain to enhance the model's capability in understanding and solving mathematical problems.\nDue to the scarcity of high-quality and challenging mathematical data, various pipelines"}, {"title": "2. Related Work", "content": "Alignment in LLMs. Large Language Models (LLMs) have recently transformed Natural Language Processing (NLP) (Achiam et al., 2023; Anil et al., 2023; Anthropic, 2024; Touvron et al., 2023), excelling in tasks such as automated summarization (Scao et al., 2022) and machine translation (Almazrouei et al., 2023). Alignment in LLMs refers to the process of ensuring that the model's outputs adhere to user preferences (Shen et al., 2023). Various techniques contribute to achieving alignment, including supervised fine-tuning (SFT) (Taori et al., 2023), reinforcement learning from human feedback (RLHF) (Bai et al., 2022), and direct policy optimization (DPO) (Rafailov et al., 2024). Among these techniques, SFT is typically an indispensable method for aligning LLMs and has achieved highly competitive performance across various tasks (Chiang et al., 2023), particularly in mathematical reasoning (Li et al., 2024). SFT involves fine-tuning a pre-trained large model using annotated data, making the model's performance more accurate for downstream tasks. Our work aims to deeply explore the performance boundaries of common 7B pre-trained LLMs using only the SFT alignment technique.\nQuantity and Quality of SFT Data. Data is the fuel that powers the performance of LLMs. This ongoing discussion about whether the quantity or quality of SFT data is more important highlights their significance in enhancing the SFT performance of LLMs. (1) Quantity. Many recent research demonstrates the scaling properties in LLM fine-tuning (Kaplan et al., 2020; Li et al., 2024). The size of the fine-tuning dataset is a crucial factor affecting the LLMs' performance. However, the optimal fine-tuning data size is highly task-dependent (Zhang et al., 2024). (2) Quality. Several studies (Cao et al., 2023; Gunasekar et al., 2023; Li et al., 2023; Zhou et al., 2023) argue that the quality of fine-tuning data is equally critical. The renowned \"less is more\" work (Zhou et al., 2023) suggests that substantial knowledge acquisition occurs during the pre-training stage, minimizing the need for extensive fine-tuning data. Additionally, the Instruction-Following Difficulty (IFD) metric introduced by (Li et al., 2023) and the QaDS strategy proposed in (Ni et al., 2024) aim to select diverse and high-quality instruction-following data to enhance LLM fine-tuning efficiency. Collecting a huge number of high-quality mathematical reasoning data is often time-consuming and labor-intensive. In this work, we generate a substantial amount of SFT synthetic data to investigate how the quantity of data impacts the performance of LLM models in mathematical reasoning.\nMathematical Reasoning in LLMs. LLMs have recently achieved significant progress in the area of mathematical reasoning (Shao et al., 2024). Initial benchmarks, such as simple math problems (Lan et al., 2022; Saxton et al., 2019), were readily solved by recent LLM models. This success prompts the introduction of more challenging benchmarks, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). Many recent works have proposed continual pre-training on massive math corpora to improve their math reasoning capabilities (Azerbayev et al., 2023; Jiang et al., 2023; Paster et al., 2024; Shao et al., 2024). Furthermore, significant progress has been made in alignment for solving mathematical problems (Li et al., 2024; Luo et al., 2023; Ni et al., 2024; Shao et al., 2024; Xu et al., 2024; Yu et al., 2024; Yue et al., 2023). These studies focus on generating high-quality synthetic data or collecting human-labeled data for model fine-tuning and alignment in the domain of math problem-solving. Additionally, reasoning frameworks aim at improving math capacity, such as the chain-of-thought (COT) prompting technique (Wang et al., 2022; Wei et al., 2022b), which enable LLMs to break down the reasoning process into manageable steps, resulting in more accurate outputs. Moreover, some complex math problems need the ability to conduct accurate arithmetic operations, a"}, {"title": "3. Method", "content": "In this section, we present the detailed methodology of Skywork-Math 7B models, as illustrated in Figure 2. Skywork-Math models aim to enhance math reasoning abilities during the model alignment process, particularly in the SFT stage, using common and publicly available 7B pre-trained models. We employ a two-stage SFT approach, in conjunction with two data synthesis pipelines to produce high-quality data. In stage 1, we feed base pre-trained models with our generated normal synthetic problems to produce an intermediate model. In stage 2, to mitigate the diminishing returns in LLMs' performance as the quantity of data increases, we generate hard synthetic problems and develop our Skywork-Math models. To ensure the quality of data, we primarily utilize GPT-4 1 (Achiam et al., 2023) to generate 2.5M-instance synthetic Skywork-MathQA dataset.\nSupervised Fine-Tuning (SFT). SFT is an important and widely-used alignment technique in LLMs to enhance pre-trained models for excelling at specific tasks (Shen et al., 2023). We denote the token space of an input query and output response as X and Y, respectively. Typically,"}, {"title": "3.1. Stage 1: Normal Synthetic Problems", "content": "In this stage, we examine how the quality of Skywork-Math models improves as the quantity of SFT data increases. We generate 2.1 million high-quality and diverse SFT data within math reasoning domains by GPT-4. Our primary goal is to equip the model with a comprehensive understanding of mathematical reasoning problems by exposing it to a diverse range of math questions. Our empirical findings indicate that diversity is crucial for generating and scaling SFT data (ref. Section 4.3.2). We investigate this issue from two perspectives: data augmentation methods and diversity selection of seed problems.\nData Augmentation Methods. To ensure diversity in our synthetic data, we employ three distinct methods to augment our Skywork-MathQA dataset. We notice that the differences among these augmentation methods are subtle, however, combining these methods to improve diversity indeed influences the model's performance. Three data augmentation methods have distinct approaches. By combining them, we can leverage the advantages of all three unique approaches in our data synthesis pipeline. Figure 4 demonstrates three prompt snippets used in our paper to highlight the characteristics of these distinct approaches. Detailed examples of the same query with different responses using these three methods can be found in Appendix A.\nThe first data augmentation method we adopt is MetaMathQA (Yu et al., 2024), which comprises four specific approaches: three for query bootstrapping and one for response augmentation. For query augmentation, we leave the corresponding query unchanged and employ GPT-4 to refine its response. For query bootstrapping, the rephrasing method utilizes pre-defined prompts to generate more questions, followed by the few-shot Chain-of-Thought (COT) (Wei et al., 2022b) prompting to generate answers. Additionally, the FOBAR (Jiang et al., 2024b) and self-verification (Weng et al., 2022) methods deterministically convert the problem into a backward format to mimic backward reasoning, i.e., given the result and think backward to determine the unknown variable in the question. After transforming the questions, we then generate corresponding answers with COT techniques using GPT-4. We also strive to balance the quantity of SFT data produced by these four augmentation approaches.\nThe second data augmentation method is the Evol-Instruct approach, as implemented in WizardLM (Xu et al., 2023). Starting from the initial set of mathematical problems, Evol-Instruct iteratively rewrites them step by step into more complex queries. We set the maximum length of the evolutionary trajectory to five steps and employ the following five augmentation strategies:\n\u2022 Rewrite the original problem to create a completely new problem of similar length and difficulty.\n\u2022 Add constraints and requirements to the original problem.\n\u2022 Increase the complexity of the original problem in both depth and breadth.\n\u2022 Replace general concepts with more specific ones.\n\u2022 Explicitly request additional steps in the reasoning process of the original question."}, {"title": "Diversity Selection of Seed Problems", "content": "Initially, we simply use the training dataset of MATH along with additional mathematical data from other sources as the seed problem to generate queries and responses. To improve the diversity of seed problems, we employ the core-set approach (Sener and Savarese, 2017), which selects a representative subset of data that maximizes diversity while maintaining coverage of the original dataset's key features. As shown in"}, {"title": "3.2. Stage 2: Hard Synthetic Problems", "content": "As the quantity of data increased, we empirically observe that the relationship between performance and data quantity begins to plateau (ref. Section 4.3.1). Motivated by the concept of curriculum learning (Bengio et al., 2009; Soviany et al., 2022), we recognize that models can learn much better when data are organized in a meaningful order rather than presented randomly, introducing more complex concepts and problems gradually. In the domain of math problem-solving, it is natural to first learn the basic math operations and then progressively tackle more difficult problems. Therefore, we employ this strategy to guide the SFT data synthetic process. The stage 2 in the data synthesis pipeline is specifically designed for models to focus on mastering the more challenging problems. In this stage, we utilize the challenging problems, i.e., those categorized as Level 4 or Level 5 in the MATH dataset (Hendrycks et al., 2021) to generate additional 0.4 million query-response pairs. Finally, combined with 2.1M normal synthetic problems in stage 1, we obtain the 2.5M-instance Skywork-MathQA dataset. The rationale behind using these two stages and the experimental analysis of their impacts are discussed in Section 4.3.1. We further fine-tune the intermediate models on these hard synthetic problems to obtain the Skywork-Math model series, which exhibit strong mathematical reasoning abilities.\nRemark It is worth noting that the accuracy of our utilized GPT-4 version on the MATH benchmark is approximately 50%, indicating that about half of our synthetic data in Skywork-MathQA dataset may contain minor errors in their results and intermediate reasoning process. However, scaling these SFT synthetic data reveals a clear positive trend in the performance of LLMs (ref. Figure 5). An interesting experimental phenomenon is that before reaching the upper bound performance of Skywork-Math 7B model series, data quantity seems to play a more important role than data quality."}, {"title": "4. Experiment", "content": "We primarily conduct our experiments on two benchmarks widely recognized for assessing mathematical reasoning capabilities. (1) GSM8K (Cobbe et al., 2021) comprises a collection of high-quality math word problems at the grade school level. It contains 1,319 test questions. Typically, the reasoning steps in GSM8K vary between two and eight, ultimately yielding an integer as the answer. (2) MATH (Hendrycks et al., 2021) contains 5,000 test questions, featuring math competition-level problems. The answers in GSM8K are integer, making it relatively easy for the regular expression matching program in evaluation frameworks to extract and verify answers. However the answers in MATH may contain complex mathematical formulas (e.g., $\\frac{2+\\sqrt{2}}{3}$, $(\\sqrt{2}, \\sqrt{3})$). We have explored several evaluation benchmarks to assess the results on"}, {"title": "4.1.2. Pre-Trained Models", "content": "We utilize three publicly available top-performing 7B pre-trained LLMs in the Skywork-MathQA models to push the limit of mathematical reasoning abilities in small-scale LLMs. Our empirical results indicate that Skywork-MathQA 7B models even outperform the recently released 70B LLaMA-3 Instruct Model (AI@Meta, 2024) on the MATH benchmark.\n\u2022 LLaMA2-7B (Touvron et al., 2023) is a general-purpose LLM model that has demonstrated significant performance across various benchmarks. However, it exhibits limited mathematical reasoning abilities.\n\u2022 Mistral-7B (Jiang et al., 2023) is another general-purpose LLM model that exhibits strong reasoning abilities in math problem-solving and code generation.\n\u2022 DeepSeekMath-Base-7B (Shao et al., 2024) is a specialized LLM model tailored for mathematics reasoning. It stems from DeepSeek-Coder-Base-v1.5-7B (Guo et al., 2024) and has been further pre-trained on a mathematical corpus with 120 billion tokens. Due to this extended pre-training on massive math corpus, we observe a notable performance divergence between the specialized model and general-purpose LLM model (ref. Section 4.2.2)."}, {"title": "4.1.3. Implementation Details", "content": "We utilize the GPT-4 API with a temperature of 0.7 to generate query-response pairs in Skywork-MathQA dataset. To prevent data leakage, we evaluate the Skywork-Math models on the test examples of GSM8K and MATH with a 30-gram hit, as suggested by (Azerbayev et al., 2023). For all experiments, including ablations, Skywork-MathQA models are trained for 3 epochs. A global batch size of 32 is used along with the AdamW optimizer without weight decay. Following the original configurations of 7B pre-trained models, the learning rate is set to 2e \u2013 5 for LLAMA2-7B and 2e \u2013 6 for both Mistral-7B and DeekSeekMath-Base-7B. The learning rate warm-up ratio is 0.03. All experiments are conducted on 8 Nvidia A800 GPUs with 80G memory. For evaluation, we use the vLLM (Kwon et al., 2023) library to generate inference responses, using the same prompt as in the SFT stage described in Section 3. Unless otherwise noted, we set the maximum length of models to 2048 in both the model SFT stage and the evaluation stage. We employ a stringent criterion similar to that used in Metamath (Yu et al., 2024), achieving nearly 100% precision but at the cost of a relatively low recall rate. This approach results in several instances where correct responses from the model are mistakenly labeled as incorrect according to our criteria. Specific examples can be found in Appendix B."}, {"title": "4.2. Main Results", "content": "Comprehensive Performance Comparison with State-of-the-art Models\nTable 1 presents the comparison of Skywork-Math model series with the state-of-the-art closed- and open-source models on the test set of GSM8K and MATH benchmark to evaluate their math reasoning abilities. Because GPT-4-Turbo is a commercially closed-source model and cannot be fine-tuned to adhere to specific output formats, its responses are evaluated using a"}, {"title": "4.2.2. Scaling Laws in SFT on Mathematical Reasoning", "content": "In Figure 5, we illustrate the relationship between synthetic SFT dataset size and model performance on GSM8K and MATH. The curve clearly exhibits a scaling law relationship between the size of SFT data and model's performance. Here are some in-depth observations:\nQuantity Breeds Quality. To enhance the mathematical reasoning abilities in LLMs, increasing the quantity of synthetic data can significantly improve the quality of model performance. This scaling trend implies that, while SFT with a small amount of data could achieve decent results Zhou et al. (2023), utilizing a larger scale of synthetic SFT data can further improve math reasoning performance.\nDiminishing Returns from Continual Pre-Training. The DeepSeekMath-Base (Shao et al., 2024) 7B model, which has been continually pre-trained with 120B math-related tokens sourced from the web, initially demonstrates superior performance. However, as we increase the synthetic dataset size in the Skywork-MathQA dataset, this advantage diminishes and is eventually surpassed by the Mistral (Jiang et al., 2023) 7B base model. As the amount of SFT data increases, Skywork-Math-Mistral-7B and Skywork-Math-LLaMA2-7B catch up in performance to the Skywork-Math-DeepSeekMath-7B. This suggests that while specialized pre-training provides a strong initial boost, its benefits are not consistently scalable and can be matched by increasing the quantity of synthetic SFT data.\nEffect of Problem Difficulty. The accuracy performance for Skywork-Math 7B model series significantly increases as the synthetic data size expands from 2.1M to 2.5M, corresponding to the stage 2 in our data synthesis pipeline. This performance improvement in the final stage of data scaling indicates that incorporating more complex problems\u2014 ranging from Level 3 to Level 5 in the MATH dataset\u2014has a substantial positive impact on model performance. This finding underscores the importance of not only generating a large quantity of data but also including more challenging problems to push the limits of math reasoning abilities of LLM models. We will discuss this in more detail in Section 4.3.1."}, {"title": "4.3. Experimental Analysis", "content": "4.3.1. Fine-Grained Analysis across Different Difficulty Levels\nWe explore model's performance across various difficulty levels to analyze the internal rela- tionship between data difficulty and LLM model's capability. The difficulty level distribution"}, {"title": "4.3.2. Effect of Data Diversity", "content": "Diversity on Data Augmentation Methods. One dimension of diversity is the data augmenta- tion methods. We select 60K synthetic data in the Skywork-Math dataset to study this problem. As shown in Figure 7, the \"Mix\" approach, a combination of synthetic data generated by three augmentation methods, achieves the highest performance. Therefore, we utilize the \"mix\" method to generate our Skywork-MathQA dataset. Moreover, the Xwin-style (Li et al., 2024) approach and the MetaMathQA-style (Yu et al., 2024) approach require extensive time for answer verification and two steps for data generation, respectively. For the consideration of efficiency, we utilize the Evol-style (Luo et al., 2023) approach as a major component of the synthetic data due to requiring fewer input and output tokens within LLM models. We also observe that the impact of the mix rate of augmentation methods is not significant on the GSM8K and MATH benchmarks. However, combining these data augmentation methods is crucial for enhancing the data diversity of the Sykwork-MathQA dataset. Detailed exploration of data mixtures with different data augmentation methods is left for future work.\nDiversity of Seed Problems. Another dimension of diversity is the selection of seed problems. We construct two SFT datasets, each comprising 360K entries. The first dataset uses only the training set of MATH as the seed problems. The second dataset employs the diversity selection method introduced in Section 3.1, which includes a wide range of non-proving problems from multiple academic data sources and uses the diversity selection method to further ensure the diversity. As illustrated in Table 3, the improved diversity of seed problems in SFT data substantially enhances the math reasoning abilities in Skywork-Math models across three 7B base LLM models."}, {"title": "4.3.3. Data Selection with a Verifier", "content": "Since the accuracy of GPT-4 on MATH is around 50%, we can infer that approximately half of the data samples in the Skywork-MathQA dataset may not have the right solving processes and answers. To ensure the collection of high-quality data, it is a natural way to perform data selection with a verifier to filter out wrong responses. We first eliminate synthetic data entries that fail to align with the ground truth final answers. However, most data samples either lack the ground truth final answers or contain errors in intermediate reasoning steps. Therefore, we should design a more precise approach to ensure the entire solution is consistent with the ground truth. We fine-tune a Mistral-7B (Jiang et al., 2023) base model with few-shot prompting to verify if the reasoning paths and final answers are correct. Finally, we obtain approximately 1 million samples deemed correct by this fine-tuned verifier. With human verification of the results judged by the trained Mistral-7B verifier, it achieves an accuracy of approximately 80%. After implementing our filtering process, the fraction of correct data (80%) increases significantly compared to its original fraction (50%). As shown in Table 4, we present the results selected using the trained verifier in contrast to a random selection in the Skywork-Math dataset. We initially anticipated that, after filtering for correctness to obtain the 1M filtered dataset, the accuracies on GSM8K and MATH would range between 1M to 1.5M samples with random selection due to their quantitative relationship. However, the actual performance on the LLaMA2-7B and Mistral-7B models showed that the 1M filtered dataset performed even worse than the 1M dataset with random selection."}, {"title": "4.3.4. Can Math Reasoning Abilities Transfer Between Bilingual Language?", "content": "The common view holds that mathematical problems mainly consist of symbols and expressions, and the textual language used to state them is not crucial for understanding. To explore whether math reasoning abilities can transfer between bilingual languages, we translate the GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) benchmarks from English to"}, {"title": "4.3.5. Can Math Reasoning Abilities Be Maintained in Robustness Tests?", "content": "As suggested in CMATH (Wei et al., 2023a), several open-sourced LLM models, except GPT-4- Turbo, are vulnerable to robustness tests of math reasoning abilities influenced by distractors. To ascertain if models effectively comprehend the fundamental elements of mathematical word problems and their solutions, we inject each problem in GSM8K with 1-5 distractors as implemented in CMATH (Wei et al., 2023a). An example of two distractors is shown in Figure 8. As listed in Table 7, open-sourced fine-tuned LLM models are sensitive to the distractors injected into math word problems. Compared to the MetaMathQA SFT dataset, our proposed Skywork-MathQA dataset significantly improves robustness performance in GSM8K based on common pre-trained models, such as Mistral-7B and DeepSeekMath-7B. We hypothesize that the reason lies in the significantly larger size of the Skywork-MathQA dataset compared to the MetaMathQA dataset. The improved diversity of the Skywork-MathQA dataset can help the LLM models STF on it to better withstand robustness tests. However, GPT-4-Turbo consistently excludes interference information and focuses on the relevant information, thereby producing correct responses with even 5 distractors in GSM8K. These results suggest that most of open-source SFT models cannot truly understand the semantic information of math world problems but rather mechanically extract numbers from the sentence and calculate them. Effectively improving math reasoning abilities while maintaining robustness like GPT-4-Turbo is an important area for future exploration."}, {"title": "4.3.6. Ablation Studies Between Sparse MOE and Dense Models", "content": "Recent advancements have witnessed the rapid development of sparse MOE models (DeepSeek- AI, 2024). To evaluate the generalization capability of our Skywork-MathQA dataset across both sparse MOE and dense models, we select commonly used dense (Skywork-Math-Mistral- 7B (Jiang et al., 2023)) and sparse MOE (Mixtral-8\u00d77B (Jiang et al., 2024a)) models as the pre-trained LLM base models. We conduct experiments using the Skywork-MathQA dataset in both stage 1 and stage 2. As shown in Table 8, the results confirm strong generalization across different types of LLM models. However, the Mixtral-8\u00d77B fine-tuned on the Skywork- MathQA dataset does not show superior performance compared with its dense counterpart. The Mixtral-8\u00d77B and Skywork-Math-Mistral-7B almost exhibit almost identical performance on GSM8K and MATH. We posit the reason is that the sparse MoE model, due to its mixture- of-expert architecture, may not significantly improve the performance on the specific task (i.e., the math reasoning task), but can better handle task-specific knowledge without compromising performance on other tasks (Wei et al., 2024; Xue et al., 2024)."}, {"title": "4.3.7. Effect of Data Leakage", "content": "Though we never use the test data from MATH (Hendrycks et al., 2021) or GSM8K (Cobbe et al., 2021) for fine-tuning LLM models, we utilize GPT-4 (Achiam et al., 2023) to synthesize"}, {"title": "4.3.8. Effect of Model Maximum Length", "content": "As the difficulty level of problems increases, the length of reasoning steps typically becomes longer, especially with those generated by LLMs. If the model's maximum length is too small, the response may be truncated. In our synthetic Skywork-MathQA SFT dataset, around 130K problems exceed 512 tokens. Therefore, we set the maximum length of models to 2048 tokens in both the SFT stage and the evaluation stage. As shown in Table 10, increasing the model's maximum length leads to improved performance, indicating that 7B models can comprehend and execute long reasoning processes."}, {"title": "5. Closing Remarks and Future Directions", "content": "We study how to empower mathematical reasoning abilities for common 7B pre-trained LLM models. We propose the Skywork-MathQA dataset, consisting of 2.5 million diverse and high- quality SFT instances, implemented through our novel two-stage data synthesis pipeline. We introduce Skywork-Math model series, demonstrating that common small-scale 7B language models can stimulate strong mathematical reasoning ability using only synthetic SFT data. Skywork-Math models achieve state-of-the-art accuracy among models smaller than 10B pa- rameters using only synthetic SFT data, surpassing 70B LLM models and an early version of GPT-4 on MATH. These results suggest that the data scaling law for mathematical reasoning in LLM models remains significant and promising. Notably, this research provides several valuable insights and practical takeaways to advance our understanding of the capabilities and limitations of LLMs in mathematical reasoning.\nFinally, we present two promising future directions for this work:\nCode-Integrated Math Reasoning. Complex scientific calculations are essential for tackling difficult mathematical problems. By embedding executable code, LLMs can dynamically gener- ate and execute code to solve intricate mathematical problems, ensuring higher accuracy and robustness. Some recent works have already been proposed to translate mathematical problems into executable code (Gou et al., 2023; Toshniwal et al., 2024). However, code cannot always be generated correctly on the first attempt. Therefore, iteratively utilizing code to solve challenging math problems is a promising direction for future research.\nMore General Reasoning Tasks. Reasoning is a crucial ability for complex problem-solving. Beyond mathematical reasoning, there are many other important reasoning tasks, such as logical reasoning, causal reasoning, and commonsense reasoning (Sun et al., 2023). It is intriguing to explore how our proposed method can be applied to these more general reasoning tasks."}, {"title": "A. Illustrations of Three Different Data Augmentation Methods", "content": "We present three specific examples using the corresponding augmentation styles introduced in Section 3.1. We use the same query to investigate the differences in the response. Overall, the differences among these three methods are nuanced, but combining them is crucial to enhance the diversity of the Skywork-MathQA dataset (ref. Section 4.3.2). In Figure 10, the MetaMathQA- style data is answer-focused and maintains a coherent solving process. Figure 11 illustrates the Evol-style data, which provides a more detailed solution and includes extensive text to describe the problem-solving process. Figure 12 presents the Xwin-style response with a more detailed calculation process."}, {"title": "B. Case Studies with Correct Answers Presented in Incorrect Formats", "content": "\u2022 Different formats of the final answer but with the same value.\nGround Truth: 0.24\nResponse: The answer is 24%\nGround Truth: \u221a2, \u221a3\nResponse: The answer is \u221a3, \u221a2\nGround Truth: $\\frac{2+\\sqrt{2}}{4}$\nResponse: The answer is $\\frac{1}{2} + \\frac{\\sqrt{2}}{4}$\nGround Truth: \\text{odd}\nResponse: The answer is \\\"odd\\\"."}, {"title": "C. Performance Analysis in Stage 2 of the Data Synthesis pipeline", "content": "Table 11 illustrates the relationship between data size in stage 2 of the data synthesis pipeline and the model performance. As we generate more hard synthetic problems in stage 2 of our data synthesis pipeline, the fine-tuned LLM models show gradual improvement in handling hard problems (Level 3-5) on the MATH benchmark."}]}