{"title": "CONTROLLABLE CONTEXT SENSITIVITY AND THE KNOB BEHIND IT", "authors": ["Julian Minder", "Kevin Du", "Niklas Stoehr", "Giovanni Monea", "Chris Wendler", "Robert West", "Ryan Cotterell"], "abstract": "When making predictions, a language model must trade off how much it relies on its context vs. its prior knowledge. Choosing how sensitive the model is to its context is a fundamental functionality, as it enables the model to excel at tasks like retrieval-augmented generation and question-answering. In this paper, we search for a knob which controls this sensitivity, determining whether language models answer from the context or their prior knowledge. To guide this search, we design a task for controllable context sensitivity. In this task, we first feed the model a context (Paris is in England) and a question (Where is Paris?); we then instruct the model to either use its prior or contextual knowledge and evaluate whether it generates the correct answer for both intents (either France or England). When fine-tuned on this task, instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it with high accuracy (85-95%). Analyzing these high-performing models, we narrow down which layers may be important to context sensitivity using a novel linear time algorithm. Then, in each model, we identify a 1-D subspace in a single layer that encodes whether the model follows context or prior knowledge. Interestingly, while we identify this subspace in a fine-tuned model, we find that the exact same subspace serves as an effective knob in not only that model but also non-fine-tuned instruct and base models of that model family. Finally, we show a strong correlation between a model's performance and how distinctly it separates context-agreeing from context-ignoring answers in this subspace. These results suggest a single subspace facilitates how the model chooses between context and prior knowledge, hinting at a simple fundamental mechanism that controls this behavior.", "sections": [{"title": "INTRODUCTION", "content": "Language models are often prompted with a query and preceding context, e.g., in settings of in-context learning, retrieval-augmented generation, or document analysis. In such scenarios, the language model needs to integrate information from both the context and its prior knowledge stored in its parameters. In some cases, we may prefer the model to rely more on the context, e.g., to avoid hallucinating responses that may be based on outdated prior knowledge (Zhang et al., 2023); however, in other cases, we may prefer the model to rely more on its prior knowledge, e.g., to avoid being misled by misinformation provided in the context (Hong et al., 2024). As a motivating example, consider a document analysis setting in which a language model is asked to help understand an opinion article in a newspaper. It might first be asked to summarize, e.g., What is the main argument of this article?. In this case, the model should rely heavily on the context, i.e., the text of the article. Then, one might ask: What are some criticisms of this argument?. To answer this, the model ought to be skeptical; an opinion article may be written very authoritatively as if its arguments are established fact, or it may make some misleading claims to support its argument. To answer this critically, the language model must draw more upon its prior knowledge of the issue and related opinions than blindly following the context. More broadly, because the degree of context sensitivity depends highly on the use case, it would be desirable to be able to specify how much and whether the model should be influenced by the context versus its prior knowledge."}, {"title": "CONTEXT AND PRIOR KNOWLEDGE", "content": "Prior Knowledge in Language Models. Many prior studies have noted that LMs exhibit remarkable capabilities at answering questions depending on prior knowledge, such as factual recall (e.g., What's the capital of France?). When queried, language models tend to produce plausible answers suggesting they may encode some level of knowledge about these entities (Brown et al., 2020; Petroni et al., 2019; Roberts et al., 2020; Geva et al., 2021). This knowledge is encoded within the model's weights during training as the model is exposed to mentions of these entities in its pretraining data (Xu et al., 2022; Zhou et al., 2023). Pretraining can lead to not only learning facts, but also memorizing specific strings (Carlini et al., 2023; Stoehr et al., 2024b).\nInfluence of Context on Language Models: Promises and Perils. Models might also be prompted with context in addition to the query. For many use cases, the information provided in the context is critical to the model solving the task effectively, such as in: (a) In-context learning (Brown et al., 2020), in which a user provides several demonstrations of a task in context, which the model uses to answer the user's query; (b) Retrieval-augmented generation (Lewis et al., 2020) and open-book question-answering (Mihaylov et al., 2018; Kasai et al., 2023), in which potentially relevant information from documents is included in context to help the model answer a user's query; (c) Interactive dialogue/chat (Vinyals & Le, 2015; OpenAI, 2023), in which a user converses with a language model, often with several turns of dialogue; and (d) Text annotation (Ziems et al., 2024), in which a model is given a passage of text (e.g., a tweet) and asked to annotate the passage's sentiment, toxicity, coherence, inter alia\u2014possibly in relation to a specific entity mentioned in the passage.\nHowever, other use cases may be better served by ignoring the context to some degree, i.e., in: (a) combating jailbreaking (Yu et al., 2024), e.g., ignoring attempts by the user to override a model's built-in behaviors; and (b) resilience to misinformation (Hong et al., 2024), e.g., avoiding integrating potentially incorrect information provided in context when answering a user's query. In all of the above settings, the model has two sources from which it can draw upon when formulating a response: the context, and its knowledge about the query encoded during training. Being able to control a model's sensitivity to context in an application-dependent manner is key to their robust use.\nControlling Model Sensitivity to Context. Several studies have proposed interventions to reduce dependency on prior knowledge and favor in-context information. These approaches include prompting (Zhou et al., 2023; Onoe et al., 2023), modifying training data (Wang et al., 2023a), fine-tuning (Li et al., 2023a), and activation-level interventions (Li et al., 2023b; Stoehr et al., 2024a) at inference time. While Li et al. (2023a) further aims for some level of controllable context sensitivity, i.e., by attempting to ignore irrelevant context, they do not allow for any form of explicit controllability by the user. Neeman et al. (2023) train a model to predict both an answer based on context and one on prior knowledge."}, {"title": "IDENTIFYING MECHANISMS IN NEURAL NETWORKS", "content": "According to the linear subspace hypothesis (Bolukbasi et al., 2016; Vargas & Cotterell, 2020; Wang et al., 2023b), model representations can encode concepts as low-dimensional linear subspaces. Based on this hypothesis, much prior work has explored how various concepts including truthfulness (Marks & Tegmark, 2024; Li et al., 2023b), humor (von R\u00fctte et al., 2024), sentiment (Tigges et al., 2023), and refusal (Arditi et al., 2024) are encoded within model representations. Beyond identifying the subspace representations of these concepts, researchers have further intervened on those subspaces to control the model behavior, by using the subspace to additively steer (i.e., adding a vector to a model representation) (Rimsky et al., 2024; Turner et al., 2023; Zou et al., 2023; Ravfogel et al., 2022). Several methods can be used to identify a subspace representing a concept, including distributed alignment search (Geiger et al., 2024), LEACE (Belrose et al., 2023b), and difference in means (Marks & Tegmark, 2024)."}, {"title": "HOW TO FIND THE KNOB BEHIND CONTEXT SENSITIVITY", "content": "4.1 DESIGNING THE TASK\nFirst, we define the task of controllable context sensitivity, which should include minimally contrastive example pairs. Each pair has the same context c and query q, differing only in whether the model should follow the context or prior knowledge. These pairs allow us to compare the model's internal states when it follows context versus prior knowledge, with all else equal.\nWe formalize our setup for this task as follows. Consider a language model p over an alphabet \u03a3, i.e., p is a distribution over the Kleene closure \u03a3*. Further, consider a distinguished subset 2 \u2282 \u03a3* corresponding to licit queries and a distinguished subset C \u2282 \u03a3* corresponding to licit contexts. Let \u03b5 be the empty string. For a given query q \u2208 Q, e.g., What is the capital of France?, and context c \u2208 C, e.g., The capital of France is London., let a(q, \u03b5) \u2208 \u03a3* be the answer to q when ignoring the context (Paris) and a(q, c) \u2208 \u03a3* be the answer to q that agrees with context c (London), respectively. Let w \u2208 {ctx, pri} denote an intent, i.e., a signal for whether to agree with the context (ctx) or"}, {"title": "IDENTIFYING MODEL BEHAVIOR", "content": "Adapting a Model to this Task. To inspect the mechanism behind the model behavior, we first need a model capable of controllably following the context or prior knowledge. As such, we adapt a language model to solve the task defined in \u00a74.1 with two methods: first, we consider fine-tuning p using a standard next-token prediction objective of negative log-likelihood on the training set Dtrn, and second, we consider including samples from the training set as few-shot demonstrations for in-context learning.\nEvaluating Controllable Context Sensitivity. We evaluate a model's ability to controllably choose between context and prior knowledge using pair-accuracy. An example is correct only if the model outputs the correct answer to a given query q and context c for both intents (ctx and pri). That is, given a model p and dataset S,\nPairAcc(p, S)\n= \n\\frac{1}{|S|} \\sum_{(q,c) \\in S} \\mathbb{1} \\{\\text{greedy } p(a \\mid F(q, c, \\text{ctx})) = a(q, c)\\} \\mathbb{1} \\{\\text{greedy } p(a \\mid F(q, c, \\text{pri})) = a(q, \\epsilon)\\}\n\nwhere greedya\u2208\u03a3\u2217 denotes the output of greedy decoding."}, {"content": "Adapting a Model to this Task. To inspect the mechanism behind the model behavior, we first need a model capable of controllably following the context or prior knowledge. As such, we adapt a language model to solve the task defined in \u00a74.1 with two methods: first, we consider fine-tuning p using a standard next-token prediction objective of negative log-likelihood on the training set Dtrn, and second, we consider including samples from the training set as few-shot demonstrations for in-context learning.\nEvaluating Controllable Context Sensitivity. We evaluate a model's ability to controllably choose between context and prior knowledge using pair-accuracy. An example is correct only if the model outputs the correct answer to a given query q and context c for both intents (ctx and pri). That is, given a model p and dataset S,\nPairAcc(p, S) = \\frac{1}{|S|} \\sum_{(q,c) \\in S} \\mathbb{1} \\{\\text{greedy } p(a \\mid F(q, c, \\text{ctx})) = a(q, c)\\} \\mathbb{1} \\{\\text{greedy } p(a \\mid F(q, c, \\text{pri})) = a(q, \\epsilon)\\}\nwhere greedya\u2208\u03a3\u2217 denotes the output of greedy decoding."}, {"title": "IDENTIFYING IMPORTANT LAYERS", "content": "Next, we need to identify layers in the model where the target behavior appears to emerge. Consistent with prior work (Jin et al., 2024), we posit that for a model to succeed at this task, it must be able to execute at least three steps (not necessarily in this order): (i) extract the answer from the model's prior knowledge; (ii) extract the answer from the context; and (iii) decide whether to answer according to the context or the prior knowledge according to the given intent. Note that, under the framing of Geiger et al. (2024), these would be considered causal variables in a high-level model. Without specifying an exact causal graph, we posit these must be components in any reasonable one. We use tools from mechanistic interpretability to identify the layers at which the model appears to implement each of these steps.\nIntervention-based Interpretability. Intervention-based interpretability techniques like activation patching are commonly used to identify which activations in a model are critical for a given task (Meng et al., 2022). Intuitively, the idea is that if intervening at some set of intermediate states can change a model's output behavior for a given task, then those intermediate states likely play some critical role in the model's ability in that task. Activation patching, specifically using interchange interventions (Geiger et al., 2023), involves using two strings that differ only with respect to the task of interest. For example, to identify intermediate activations that may encode the intent of a prompt, we can consider two input strings that share the same query and context but differ in their intent. For a given model, p, we define a source string, s \u2208 \u03a3*, and a target string, t \u2208 \u03a3*. During the forward pass of p (t), we replace a subset of intermediate activations with the corresponding activations from p (s) and observe the resulting effect on model internals and the output distribution of the patched p (t). We only patch at the last token because this has been shown in previous work to be most informative to determining the next token (Yu et al., 2023; Jin et al., 2024; Stoehr et al., 2024a; Monea et al., 2024).\nWe also only patch the outputs of the multi-head attention (MHA) components in a transformer block; the intuition behind this choice is that this component ought to integrate information from the context into the residual stream of the last token. Interchanging these output activations allows us to analyze what kind of information is written on the residual stream and whether it has a causal effect on the model internals and the output distribution. By searching over different subsets of intermediate activations, we can identify those with the greatest impact on task performance and thus facilitate critical functionality."}, {"title": "IDENTIFYING THE CONTEXT-CONTROLLABILITY SUBSPACE FEATURE", "content": "Learning the Context-versus-Prior Subspace. Once we determined a subset of model components that might contain the mechanism which facilitates the functionality of deciding between answering from the context or prior knowledge, we can zoom further in and search for whether this functionality\nTIP can interpret the information in a model's residual stream at intermediate layers by using the model to map from the residual stream at a given layer and token index to a distribution over tokens that best represents the information stored in that intermediate state. This approach can also be viewed as a variant of the Selfie method Chen et al. (2024). While other alternatives for interpreting intermediate states could also be used here (e.g., probing (Tenney et al., 2019), LogitLens (nostalgebraist, 2020), and TunedLens (Belrose et al., 2023a)), we prefer TIP because it performs better than these other methods."}, {"title": "CASE STUDY: LLAMA-3.1 8B", "content": "We now describe detailed empirical results in executing the recipe to identify the mechanism behind controllable context sensitivity as described in \u00a74. Results for other models can found in \u00a76 and App. E.\n2P is redundant in the second term of Eq. (4) since Puc(w) = uuTuc(w) = uc(w), but is included for consistency."}, {"title": "TASK SETUP", "content": "Datasets. Following the task formulation in \u00a74.1, we construct our intent-augmented datasets, CCS-BASEFAKEPEDIA, CCS-MULTIHOPFAKEPEDIA, and CCS-ARITHMETIC, based on the query-context pairs in BASEFAKEPEDIA, MULTIHOPFAKEPEDIA, and ARITHMETIC, respectively (Monea et al., 2024). BASEFAKEPEDIA is a knowledge-conflict dataset from Wikipedia with queries across 23 relation types (e.g., Norway's capital city or Mac Pro, a product created by) and paragraphs generated by a large language model that directly provide counterfactual answers. MULTIHOPFAKEPEDIA is similar to BASEFAKEPEDIA, but instead of the counterfactual answer being directly stated in the paragraph, it is implied via an extra hop of reasoning (e.g., London is the capital of France. Tunis is in the same country as London. What country is Tunis in?). ARITHMETIC is a synthetically-generated dataset whose queries are simple arithmetic expressions using the operators {+, -, \u00d7, \u00f7, exp} and contexts are reassignments of subexpressions to another value which results in a counterfactual answer. For example, given the query (5 + 1)/2 = and the context 5 = 9, the prior-agreeing answer would be 3, while the context-agreeing answer would be 5. In our experiments, we limit expressions to a depth of 2, i.e., two operators, with input and output numbers between 0 and 9.\nIntent Format. We also format the intent w \u2208 {ctx, pri} in two different ways in order to probe the model's robustness to different formulations of the same query, context, and intent. First, the instruction intent format () expresses the intent as a string instruction, e.g., Ignore the context in answering the query. or Only consider the context in answering the query. Second, the weight intent format (1) expresses the intent as a context weight, e.g., Context weight: 0. or Context weight: 1."}, {"title": "ADAPTING MODELS TO THE TASK", "content": "Training. We investigate the degree to which instruction-tuned Llama-3.1 8B (Dubey et al., 2024) can solve this task after adapting it in two ways. First, we fine-tune (FT) models on the CCS-BASEFAKEPEDIA train set for 2048 examples using QLoRA on the attention components of the full network. Second, we also use in-context learning (ICL) with 10 examples from CCS-BASEFAKEPEDIA prepended to the prompt. See App. B for more details on training hyperparameters.\nEvaluation. We are interested in two forms of generalization: first, robustness to different datasets, and second, robustness to different intent formats. For the former, we evaluate whether a model trained on CCS-BASEFAKEPEDIA can perform well on test splits from CCS-BASEFAKEPEDIA, CCS-MULTIHOPFAKEPEDIA, and CCS-ARITHMETIC. For the latter, we assess whether a model trained with one intent format, e.g., , performs well with prompts in another format, e.g., 1.\nResults. In Fig. 1, we present the results of our generalization studies for Llama-3.1-8B-Instruct. From Fig. 1a, the model achieves high pair accuracy on its in-domain test set with FT (\u2248 90%) and ICL (\u2248 88%). However, performance drops significantly for ICL and mildly for FT on CCS-MULTIHOPFAKEPEDIA, which requires additional reasoning. On CCS-ARITHMETIC, both models show significant performance degradation, as the task is out-of-domain and requires reasoning beyond context extraction. Fig. 1b shows that, for intent formats, the model: (a) performs well when fine-tuned on either intent format, (b) generalizes well from the 1 to the format, and (c) struggles more when trained on the format but evaluated on 1. This result is intuitive as the Instruct model is tuned to follow natural language instructions such as , but may not be familiar with interpreting the instruction. Overall, Llama-3.1-8B-Instruct: (a) learns the task in-domain with high accuracy, (b) generalizes moderately well to other datasets, depending on the degree of difference, and (c) adapts reasonably well to other intent formats, especially if they are in natural language."}, {"title": "IDENTIFYING IMPORTANT COMPONENTS", "content": "Focusing on Llama-3.1-8B-Instruct fine-tuned using the instruction () intent format, we apply the algorithm presented in \u00a74.3 (using TIP and activation patching) to identify important layers which appear to facilitate the model's sensitivity to context. First, we investigate where the intent w is computed by using tuples from D\u2192C and De\u2192P (described in Tab. 1). Second, we investigate which layers compute the prior answer a(q, \u03b5) and context answer a(q, c). If these layers are later than the ones identified in the first step then that could suggest that w is encoded in the residual stream and depending on its value, either a(q, c) or a(q, \u03b5) is retrieved."}, {"title": "IDENTIFYING THE CONTEXT-CONTROLLABILITY SUBSPACE", "content": "Following the method in Section 4.4, we learn a rank-1 orthogonal projection matrix P to identify a subspace Fw that encodes intent. We search for this subspace in layer 16, as this is the last layer in the base range of influential layers found in \u00a75.3.1 using the algorithm described in \u00a74.3. We train on\n3In the appendix, we show that without patching at layer 24, the probability of the SRC PRI significantly decreases after layer 24 (Fig. 6a)."}, {"title": "A FUNDAMENTAL SUBSPACE FOR CONTROLLABLE CONTEXT SENSITIVITY", "content": "Due to the strong evidence for a high alignment of Fw to the causal intent variable, we propose two hypotheses: (i) This subspace is fundamental to the model and different learning methods learn to set the value of this subspace. (ii) As a fundamental subspace to language models, a similar rank-1 subspace to encode choosing context or prior knowledge can be found in other language models too."}, {"title": "SEARCHING FOR IMPORTANT LAYERS", "content": "A.1 ALGORITHM\nWe describe the algorithm in Python-esque pseudocode. For more details on the patchscope method (PATCHSCOPE), see Ghandeharioun et al. (2024). For more details on activation patching (INTERCHANGE), see Meng et al. (2022).\ndef search(m, s, t, s_ans, t_ans, thres=0.85, margin=0.3, eps=0.05):\n\"\"\"\nLet m be a model with L layers, hidden size HS, and vocab size VS.\nLet s and t be the tokenized source & target inputs.\nLet s_ans & t_ans be the answer indices corresponding to the source & target inputs.\n\"\"\"\n# 1. Find early layers which induce high probability of s_ans in some model layer.\n# Let interchange (model, s, t, layers) return the last-token forward pass\n# of a model on target input t when interchanging the multihead attention\n# activations from s at given layers.\n# Output shape: (L, HS)\n# Let patchscope (activations) return the model's next token probabilities\n# based on each layer's activations.\n# Output shape: (L, VS)\nL = len(m.layers)\nstart_1 = 0\nend_1 = 0\nwhile max(patchscope (interchange (m, s, t, range(0, end_1)))[:, s_ans]) < thres:\n end_1 += 1\nwhile max(patchscope (interchange (m, s, t, range(start_l, end_l)))[:, s_ans]) < thres:\n start_1 += 1\n# 2. Find layers which counter late-layer suppression\nlayers = range (start_l, end_l)\nwhile (\n softmax(interchange (m, s, t, layers) [-1])[s_ans] <\n margin + softmax(interchange (m, s, t, layers) [-1])[t_ans]\n ):\n for l in range(max (layers) + 1, L):\n if abs(\n patchscope (interchange (m, s, t, layers)) [l, s_ans] -\n patchscope (interchange (m, s, t, layers)) [l-1, s_ans]\n ) > eps:\n layers.append(l)\n break\nreturn layers"}, {"content": "A.1 ALGORITHM\nWe describe the algorithm in Python-esque pseudocode. For more details on the patchscope method (PATCHSCOPE), see Ghandeharioun et al. (2024). For more details on activation patching (INTERCHANGE), see Meng et al. (2022).\ndef search(m, s, t, s_ans, t_ans, thres=0.85, margin=0.3, eps=0.05):\n\"\"\"\nLet m be a model with L layers, hidden size HS, and vocab size VS.\nLet s and t be the tokenized source & target inputs.\nLet s_ans & t_ans be the answer indices corresponding to the source & target inputs.\n\"\"\"\n# 1. Find early layers which induce high probability of s_ans in some model layer.\n# Let interchange (model, s, t, layers) return the last-token forward pass\n# of a model on target input t when interchanging the multihead attention\n# activations from s at given layers.\n# Output shape: (L, HS)\n# Let patchscope (activations) return the model's next token probabilities\n# based on each layer's activations.\n# Output shape: (L, VS)\nL = len(m.layers)\nstart_1 = 0\nend_1 = 0\nwhile max(patchscope (interchange (m, s, t, range(0, end_1)))[:, s_ans]) < thres:\n end_1 += 1\nwhile max(patchscope (interchange (m, s, t, range(start_l, end_l)))[:, s_ans]) < thres:\n start_1 += 1\n# 2. Find layers which counter late-layer suppression\nlayers = range (start_l, end_l)\nwhile (\n softmax(interchange (m, s, t, layers) [-1])[s_ans] <\n margin + softmax(interchange (m, s, t, layers) [-1])[t_ans]\n ):\n for l in range(max (layers) + 1, L):\n if abs(\n patchscope (interchange (m, s, t, layers)) [l, s_ans] -\n patchscope (interchange (m, s, t, layers)) [l-1, s_ans]\n ) > eps:\n layers.append(l)\n break\nreturn layers"}, {"title": "TRAINING PARAMETERS", "content": "To fine-tune models in the CCS-BASEFAKEPEDIA task, we use QLORA with the following hyperparameters:\n\u2022 Effective batch size (after gradient accumulation): 16.\n\u2022 Optimizer: AdamW (8-bit).\n\u2022 Learning rate: 2e - 4.\n\u2022 QLORA hyperparameters: attention head projection matrices in all layers.\n\u2022 Training set size: 2048 examples."}, {"title": "ADAPTING MODELS TO THE TASK (ADDITIONAL MODELS)", "content": "We repeat the experiments from $5.2 for the Mistral-v0.3 7B and Gemma-2 9B instruction tuned models and report the results in Fig. 7a and Fig. 8, respectively. These results tell a similar story as those for the Llama-3.108B-Instruct. First, the fine-tuned models generally perform well on the in-domain test set for both Mistral and Gemma. However, Mistral appears to be worse at the out-of-domain generalization, as performance drops significantly for both CCS-MULTIHOPFAKEPEDIA and CCS-ARITHMETIC. This is also evident in the experiment testing generalization to intent formats, as Mistral is much worse when trained on the instruction format and evaluated on the context weight format; this could suggest that Mistral is has little understanding on how to interpret an instruction in the context weight format. Meanwhile, Gemma appears to generalize to out-of-domain test sets comparatively well, with the fine-tuned model performance at CCS-MULTIHOPFAKEPEDIA not significantly worse than that of CCS-BASEFAKEPEDIA, and the performance on CCS-ARITHMETIC being relatively high (similar to that of Llama-3.1). While training on the instruction format and evaluating on the context weight format also results in worse performance for the model, the drop is signifcantly less."}, {"title": "PARAMETRIZATION OF THE ORTHOGONAL PROJECTION MATRIX", "content": "Parametrizing a rank-k orthogonal projection matrix P \u2208 RD\u00d7D is a non-trivial task. To address this, we utilize the fact that if u\u2081, . . ., uk is an orthonormal basis for a subspace, and A = [u\u2081, \u2026\u2026\u2026, uk] \u0404 IRD\u00d7k, then the projection matrix P = AAT is an orthogonal projection onto the subspace spanned by the basis vectors u1,..., uk (Meyer, 2000, p.430, Eq. 5.13.4). Rather than learning P directly,"}, {"title": "SUBSPACE INTERVENTION FOR ADDITIONAL MODELS", "content": "We repeat the methods in \u00a74 for Mistral-v0.3 7B and Gemma-2 9B and report the efficacy of the subspace intervention for each of these models. Figure Fig. 10 and Fig. 12 show that for both of these models, we see high correlation (> 0.87) between the subspace value mean difference and the PairAcc. Fig. 11 and Fig. 9 indicate that for both of these models, the process successfully identifies a subspace which can be used to induce controllable context sensitivity capabilities in the model that is on par with or beyond those of baseline models on examples with an explicit intent instruction. Further, in Fig. 13b and Fig. 13a we can observe that this generalizes similarly to other datasets. For these Mistral-v0.3 7B, we choose c (pri) = 5 and c (ctx) = -5 and for Gemma-2 9B c (pri) = -100 and c (ctx) = 150."}]}