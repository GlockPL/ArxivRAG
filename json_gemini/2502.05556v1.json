{"title": "Knowledge is Power: Harnessing Large Language Models\nfor Enhanced Cognitive Diagnosis", "authors": ["Zhiang Dong", "Jingyuan Chen", "Fei Wu"], "abstract": "Cognitive Diagnosis Models (CDMs) are designed to as-\nsess students' cognitive states by analyzing their performance\nacross a series of exercises. However, existing CDMs of-\nten struggle with diagnosing infrequent students and exer-\ncises due to a lack of rich prior knowledge. With the ad-\nvancement in large language models (LLMs), which pos-\nsess extensive domain knowledge, their integration into cog-\nnitive diagnosis presents a promising opportunity. Despite\nthis potential, integrating LLMs with CDMs poses significant\nchallenges. LLMs are not well-suited for capturing the fine-\ngrained collaborative interactions between students and exer-\ncises, and the disparity between the semantic space of LLMs\nand the behavioral space of CDMs hinders effective integra-\ntion. To address these issues, we propose a novel Knowledge-\nenhanced Cognitive Diagnosis (KCD) framework, which is a\nmodel-agnostic framework utilizing LLMs to enhance CDMs\nand compatible with various CDM architectures. The KCD\nframework operates in two stages: LLM Diagnosis and Cog-\nnitive Level Alignment. In the LLM Diagnosis stage, both\nstudents and exercises are diagnosed to achieve comprehen-\nsive and detailed modeling. In the Cognitive Level Align-\nment stage, we bridge the gap between the CDMs' behav-\nioral space and the LLMs' semantic space using contrastive\nlearning and mask-reconstruction approaches. Experiments\non several real-world datasets demonstrate the effectiveness\nof our proposed framework.", "sections": [{"title": "Introduction", "content": "Cognitive diagnosis evaluates a student's learning profi-\nciency through his responses to a series of exercises, as\nshown in Figure 1 (a), which plays a fundamental role in\nintelligent education systems. The outcomes of cognitive\ndiagnosis are crucial for various educational applications,\nsuch as educational recommendation (Huang et al. 2019)\nand computerized adaptive testing (Bi et al. 2020; Zhuang\net al. 2022). Consequently, the accuracy and reliability of\ncognitive diagnosis are essential for enhancing the effective-\nness of these educational technologies.\nTraditional cognitive diagnosis models (CDMs) are pri-\nmarily grounded in psychometric theories, employing man-\nually designed interaction functions inspired by principles\nof"}, {"title": "Related Work", "content": "Cognitive diagnosis, which originated from educational psy-\nchology, is a fundamental task in the field of intelligent\neducation. It characterizes students' learning status and\nknowledge proficiency based on their responses to various\nquestions (Liu 2021). Existing cognitive diagnosis methods\nare mainly divided into two main categories: psychometric\ntheory-based methods (Lord 1952; De La Torre 2009; Reck-\nase 2009) and neural network-based methods (Wang et al.\n2020; Gao et al. 2021; Bi et al. 2023; Liu et al. 2024a; Wang\net al. 2023). Psychometric theory-based methods, such as\nItem Response Theory (IRT) (Lord 1952), Multidimen-\nsional IRT (MIRT) (Reckase 2009), and Deterministic In-\nputs, Noisy And gate model (DINA) (De La Torre 2009), are\ndesigned to evaluate students' proficiency through latent fac-\ntors utilizing psychological theories. Neural network-based\nmethods use deep neural networks to profile students' learn-\ning status. NCD (Wang et al. 2020) first incorporates neu-\nral networks into cognitive diagnosis to effectively capture\nthe fine-grained student-exercise relationships. RCD (Gao\net al. 2021) and RDGT (Yu et al. 2024) employ graph ar-\nchitectures to explore the relationships among exercises,\nknowledge concepts, and students. Recently, BETA-CD (Bi\net al. 2023) developed a reliable and rapidly adaptable cog-\nnitive diagnosis framework for new students through meta-\nlearning. ACD (Wang et al. 2024b) considered the connec-\ntion between students' affective states and cognitive states in\nlearning. However, few existing cognitive diagnosis meth-\nods take into account prior knowledge, which makes it chal-\nlenging for them to generate accurate diagnoses.\nWith the rise of Transformer (Vaswani et al. 2017), large\nlanguage models (LLMs) with extensive parameters and\nvast training data have gradually become mainstream. LLMs\nusually follow a pre-training and fine-tuning approach to\naccommodate various downstream tasks. They have sig-\nnificantly improved performance in numerous NLP appli-\ncations, including text summarization (Laskar, Hoque, and\nHuang 2022; Zhang, Liu, and Zhang 2023), sentiment anal-\nysis (Hoang, Bihorac, and Rouces 2019; Deng et al. 2023),\ntranslation (Zhang, Haddow, and Birch 2023; Moslem,\nHaque, and Way 2023), and multimodal understanding (Wu\net al. 2024; Huang et al. 2024).\nThe advanced comprehension and reasoning capabilities,\nalong with the extensive knowledge repository of LLMs,\nnaturally lead to potential applications in the realm of ed-\nucation. LLMs can provide researchers with new perspec-\ntives by simulating the roles of teachers or students (Wang\net al. 2024a; Li et al. 2023; Xu, Zhang, and Qin 2024; Liu\net al. 2024b; Lin et al. 2024c), or generating educational re-\nsources (Lin et al. 2024b,a; Dai et al. 2024). However, less\nexploration has been made to utilize LLMs for cognitive di-\nagnosis. The demonstrated success of LLMs in text sum-\nmarization tasks and educational contexts indicates LLMs'\ncapability to undertake cognitive diagnostic tasks."}, {"title": "Methodology", "content": "In this section, we first present the task definition and the\ngeneral framework. Then, we show the detailed strategies\nemployed within our framework.\nFormally, suppose $S = {$s_1,\u00b7\u00b7\u00b7, s_{|s|}$}, E = {e_1,\u2026, e_{|e|}}\nand $K = {k_1,\u2026\u2026,k_{|k|}$} be the sets of students, exercises\nand knowledge concepts. The response logs R of students\nare represented as triplets $(s_i, e_j, K_j, r_{ij}) \\in R$, where $r_{ij}$\nindicates whether student $s_i$ correctly answered the exercise\n$e_j$ and $K_j$ denotes the knowledge concepts related to $e_j$. In\nsome datasets, exercise e also includes the text content t as\nits attributes. The goal of cognitive diagnosis is to evaluate\nstudents' proficiency levels across various knowledge con-\ncepts by predicting their performance based on the response\nlogs R.\nThe proposed Knowledge-enhanced Cognitive Diagnosis\n(KCD) framework consists of two main modules: LLM di-\nagnosis and cognitive level alignment, as illustrated in Fig-\nure 2. This framework is designed to integrate collaborative\ninformation while leveraging the rich prior knowledge of\nLLMs. Additionally, it aligns the semantic space of LLMS"}, {"title": "LLM Diagnosis", "content": "LLMs can be guided more effectively through carefully\ncrafted natural language instructions, resulting in higher-\nquality outputs. In this section, we distinguish between two\ntypes of input instructions for LLMs: system prompts $M$\nand input prompts $P$. The system prompt $M$ defines the\ntasks that LLMs need to perform and specifies the input and"}, {"title": "Cognitive Level Alignment", "content": "By leveraging LLMs, we can generate textual diagnoses of\nstudents' cognitive status and exercises' attributes. How-\never, LLMs cannot fully comprehend response logs due to\nconstraints on input length, which restrict the inclusion of\nstudent-exercise interactions. Therefore, it is necessary to\nalign these LLM-generated diagnoses with those produced\nby CDMs at the cognitive level. Since LLMs operate within\na semantic space while CDMs work within a behavioral\nspace, both need to be mapped to a common space for ef-\nfective alignment. To achieve this, we propose two align-\nment methods: behavioral space alignment (KCD-Beh) and\nsemantic space alignment (KCD-Sem).\nBefore implementing the alignment approach, we obtain\nthe semantic representation of LLMs by encoding their tex-\ntual diagnoses. Specifically, we utilize the text embedding\nmodel (Su et al. 2023), which has demonstrated significant\nstrength in textual representation, to encode the diagnoses\nas follows: $L = E(T)$, where E(\u00b7) denotes the text embed-\nding models and 1 \u2208 L denotes the modeling of students\nand exercises generated by LLMs in semantic space. Mean-\nwhile, we denote the representation embeddings of students\nand exercises by CDMs as c \u2208 C in behavioral space, such\nas Neural Cognitive Diagnosis (NCD) (Wang et al. 2020), as\ndescribed in Figure 2.\nBehavioral space alignment involves mapping the LLM-generated models of stu-\ndents and exercises to the behavioral space of CDMs. We\nemploy contrastive learning, a widely-used technique for\nbidirectionally aligning different views (Khosla et al. 2020;\nCui et al. 2023), to align the representations of LLMs and\nCDMs within the behavioral space. The intuition behind us-\ning contrastive learning is that $c_i$ and $l_i$ are most similar to\neach other within L since they represent the same student or\nexercise. We apply a multi-layer perceptron (MLP) to map\n1 from the LLMs' semantic space to the CDMs' behavioral\nspace, denoted as l\u2032 = MLP(1).\nSpecifically, we conduct contrastive learning from both\nglobal and local perspectives. Global contrast involves us-\ning the entire set L, while local contrast selects a subset\nL'CL, composed of the k most similar students and ex-\nercises for each student and exercise. This subset is obtained\nby calculating the cosine similarity between each student\nand exercise with others and selecting the top k most sim-\nilar instances (k = 20 in our experiments). Global contrast\ncaptures general features, while local contrast captures fine-\ngrained differences between similar students and exercises.\nDuring training, we use the InfoNCE (Oord, Li, and\nVinyals 2018) loss function to calculate both global and lo-\ncal contrast loss values, aiming to maximize the mutual in-\nformation between c and I within the behavioral space, de-\nnoted as:\n$f = -\\frac{1}{N}\\sum_{i=1}^{N}log(\\frac{exp(\\frac{x_i^T y_{i+}}{T})}{\\sum_{j=1}^{N} exp(\\frac{x_i^T y_{j-}}{T})}),$\nwhere $x_i$ and $y_{i+}$ are positive samples, $y_{j-}$ represents the\nnegative sample, $T$ denotes the temperature parameter, N\ndenotes the number of samples. For CDMs, the loss func-\ntion is denoted as $L_{cdm}$. For example, the loss function of\nNCD is cross entropy between output y and ground truth r:\n$L_{cdm} = -\\sum_{i}(r_i log y_i + (1 \u2212 r_i) log (1 \u2013 y_i)).$\nThe complete loss function is formulated as:\n$\\begin{cases}\nL_{global} = f(c_i, l'_i, L'),\\\\\nL_{local} = f(c_i, l'_i, L'),\\\\\nL = L_{cdm} + \\alpha L_{global} + \\beta L_{local},\n\\end{cases}$\nwhere f(xi, xj, Xk) denotes the InfoNCE loss function, xi\nand xj are positive samples, Xk represents the set of neg-\native samples. The term $L_{cdm}$ denotes the loss function of"}, {"title": "Experiments", "content": "In this section, we conduct experiments to answer the fol-\nlowing research questions:\n\u2022 RQ1: Can the proposed model effectively improve the\nperformance of the original CDMs?\n\u2022 RQ2: What is the impact of each component within the\nproposed method?\n\u2022 RQ3: How does the proposed model perform on cold-\nstart scenarios?\n\u2022 RQ4: How effective is the alignment of semantic and\nbehavioral space embeddings during the cognitive level\nalignment process.\nIn our experiments, we utilize four courses,\nPython Programming (Python), Linux System (Linux),\nDatabase Technology and Application (Database), and Lit-\nerature and History (Literature), from a publicly available\ndataset PTADisc (Hu et al. 2023), which comes from real-\nworld students' responses in the educational website PTA\u00b9\nand contains textual information of exercises and knowledge\nconcepts. The statistics of the datasets are presented in Ta-\nble 1. The datasets are divided into training, validation, and\ntesting sets, with a ratio of 8:1:1.\nFollowing previous works, we eval-\nuate the students' cognitive status by predicting the perfor-\nmance of students on the testing set, as the cognitive status\ncan not be directly observed. We adopt commonly used met-\nrics, namely the Area Under a ROC Curve (AUC), the Pre-\ndiction Accuracy (ACC), and the Root Mean Square Error\n(RMSE), to validate the effectiveness of the CDMs. For all\nthe metrics, \u2191 represents that a greater value is better, while\n\u2193 represents the opposite.\nTo validate the effectiveness of the pro-\nposed method, we conduct experiments on several repre-\nsentative CDMs, including IRT (Lord 1952), MIRT (Reck-\nase 2009), DINA (De La Torre 2009), NCD (Wang et al.\n2020), RCD (Gao et al. 2021), SCD (Wang et al. 2023) and\nACD (Wang et al. 2024b).\nWe utilize PyTorch to imple-\nment both the baseline methods and our proposed KCD\nframework. For the baseline models, We use the default\nhyper-parameters as stated in their papers and for KCD,\nwe use the same hyper-parameter settings, such as training\nepoch, learning rate, and batch size. We employ ChatGPT to\nrepresent LLMs (specifically, gpt-3.5-turbo-16k) and text-\nembedding-ada002 as the text embedding model. All the ex-\nperiments are conducted on a GeForce RTX 3090 GPU. We\ntrain the model on train set and at the end of each epoch,\nwe evaluate the model on the validation set. The hyper-\nparameter \u03b1, \u03b2, and A was set to 0.04, 0.015, and 0.2. Since\nour dataset does not include affect labels, we utilize the\nunsupervised contrastive ACD model and employ NCD as\nthe basic cognitive diagnosis module. The behavioral space\nalignment approach is denoted as '-Beh' and the semantic\nspace alignment approach is denoted as '-Sem'."}, {"title": "Performance Comparison (RQ1)", "content": "To demonstrate the effectiveness of our proposed method\nin improving cognitive diagnosis, we implement the frame-\nwork on seven cognitive diagnosis models, and the results\nare shown in Table 2. Additionally, we compared the perfor-\nmance of NCD in warm and cold scenarios, with the results\nillustrated in Figure 3. Here we define the cold scenario as\nless than 3 interactions in the training set for exercises and\ndefine the warm scenario as more than 10 interactions in the\ntraining set for exercises. Following this definition, we di-\nvide the testing set into cold and warm subsets. We have the\nfollowing observations from the results:\n1) Both KCD-Beh and KCD-Sem achieve significant im-\nprovements compared to the basic CDMs. This indicates\nthat our proposed framework is widely applicable to var-\nious CDMs, and both alignment methods can effectively\nalign the behavioral space of CDMs and the semantic\nspace of LLMs. In most models, the behavioral space\nalignment approach performs better, indicating that align-\ning in the behavioral space of CDMs can better align in-"}, {"title": "Ablation Study (RQ2)", "content": "To validate the effectiveness of different components of our\nproposed method, we conduct ablation experiments to ver-\nify several components utilized in LLM Diagnosis and Cog-\nnitive Level alignment, including the usage of collabora-\ntive information (denoted as 'Coll. Info'), the local contrast\nand global contrast (denoted as 'Local Con.' and 'Global\nCon.'), and the dynamic masking strategy (denoted as 'Dym.\nMask')."}, {"title": "Performance on Cold-Start Scenarios (RQ3)", "content": "we conduct additional experiments on sub-datasets with\nvarying degrees of sparsity. Specifically, we apply random\ndropout to the training sets of the Python and Linux datasets\nat ratios of 10%, 20%, 30%, 40%, and 50%.\nFigure 4 shows the results of the experiments on differ-\nent dropout ratios. It is obvious that as the dropout ratio in-\ncreases, both AUC and ACC decrease. This is because the\ntraining set becomes more sparse, approaching a cold-start\nscenario. Additionally, compared to ACC, AUC experiences\na greater decline, which might be due to the different calcu-\nlation methods of the two metrics."}, {"title": "Visualization of Semantic and Behavioral\nEmbeddings (RQ4)", "content": "To validate the effectiveness of the two alignment processes,\nwe utilize t-SNE (Van der Maaten and Hinton 2008) to visu-\nalize the distribution of features in LLMs semantic space and\nCDMs behavioral space. We randomly select 200 example\nstudents and map their behavioral embeddings and semantic"}, {"title": "Case Study", "content": "To more intuitively demonstrate the improvements our pro-\nposed methods bring to CDMs, we selected a diagnosis for a\nspecific student in the Linux dataset and compared the pre-\ndiction results of NCD with the diagnosis results of NCD-\nBeh. As illustrated in Figure 6, we randomly choose a stu-\ndent, and list his mastery of some knowledge concepts pre-\ndicted by NCD and our proposed NCD-Beh. This student\ncorrectly answered the exercises related to 'numerical en-\ncoding' and 'process communication', showing mastery of\nthese concepts. He answered other exercises incorrectly, in-\ndicating a lack of familiarity with the remaining knowledge\nconcepts. From the LLM's diagnostic results, it can be ob-\nserved that the LLM captured similar question-answer in-\nformation from the training set and made corresponding in-\nferences. This played an important role in NCD-Beh's more\naccurate prediction of the student's mastery level."}, {"title": "Conclusion", "content": "In this work, we propose a model-agnostic framework KCD\nthat can efficiently employ LLMs to enhance the knowl-\nedge of conventional CDMs. By utilizing LLM diagnosis\nand cognitive level alignment, the framework can leverage\nthe rich knowledge of LLMs and align the semantic space of\nLLMs and the behavioral space of CDMs to achieve optimal\ndiagnostic results. Several experiments on four real-world\ndatasets for cognitive diagnosis demonstrate the superior-\nity of our proposed framework, surpassing all the baseline\nCDMs."}]}