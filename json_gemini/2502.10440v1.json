{"title": "Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Ownership Verification with Reasoning", "authors": ["Junfeng Guo", "Yiming Li", "Ruibo Chen", "Yihan Wu", "Chenxi Liu", "Tong Zheng", "Heng Huang"], "abstract": "Large language models (LLMs) are increasingly integrated into real-world applications through retrieval-augmented generation (RAG) mechanisms to supplement their responses with up-to-date and domain-specific knowledge. However, the valuable and often proprietary nature of the knowledge bases used in RAG introduces the risk of unauthorized usage by adversaries. Existing methods that can be generalized as watermarking techniques to protect these knowledge bases typically involve poisoning attacks. However, these methods require to alter the results of verification samples (e.g., generating incorrect outputs), inevitably making them susceptible to anomaly detection and even introduce new security risks. To address these challenges, we propose RAG\u00a9 for 'harmless' copyright protection of knowledge bases. Instead of manipulating LLM's final output, RAG\u00a9 implants distinct verification behaviors in the space of chain-of-thought (CoT) reasoning, maintaining the correctness of the final answer. Our method has three main stages: (1) Generating CoTs: For each verification question, we generate two CoTs, including a target CoT for building watermark behaviors; (2) Optimizing Watermark Phrases and Target CoTs: We optimize them to minimize retrieval errors under the black-box setting of suspicious LLM, ensuring that the watermarked verification queries activate the target CoTs without being activated in non-watermarked ones; (3) Ownership Verification: We exploit a pairwise Wilcoxon test to statistically verify whether a suspicious LLM is augmented with the protected knowledge base by comparing its responses to watermarked and benign verification queries. Our experiments on diverse benchmarks demonstrate that RAG\u00a9 effectively protects knowledge bases against unauthorized usage while preserving the integrity and performance of the RAG.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs), such as GPT (Achiam et al., 2023) and LLaMA (Touvron et al., 2023), have been widely deployed in many real-world applications (Zheng et al., 2023; Dong et al., 2023; Dowling & Lucey, 2023). Despite their success in exceptional generative capabilities, they also suffer from lacking up-to-date knowledge as they are pre-trained on past data (Wu et al., 2024); they could also lack knowledge on specific domains (e.g., medical domain), restricting the real-world deployment of LLMs in applications like healthcare (Zakka et al., 2024).\nTo address the above limitations, retrieval-augmented generation (RAG) is proposed to augment an LLM with external knowledge retrieved from given knowledge databases. Its main idea is to combine the strengths of retrieval-based and generative models to produce more accurate and contextually relevant outputs. In general, RAG contains three main modules: LLMs, retriever, and knowledge base. Specifically, LLMs and the retriever are both machine learning models pre-trained with existing data for generating answers and knowledge retrieval. Knowledge bases contain a large number of texts collected from various domains or the Internet to provide domain-specific expertise and up-to-date information for LLMs. In particular, these knowledge bases, especially those from mission-critical domains (e.g., finance (Zhang et al., 2023) and healthcare (Zakka et al., 2024)), usually contain a large amount of valuable or even exclusive data. It leads to great incentives for adversaries to 'steal' or 'misuse' these knowledge bases for enhancing their deployed LLMs service without authorization. In this paper, we explore the copyright protection of knowledge bases used for RAG by detecting potential misuse.\nTo the best of our knowledge, no existing work has been proposed to protect the copyright for RAG's knowledge bases. Arguably, one of the most straightforward or even the only feasible solutions is to formulate this copyright protection problem as an ownership verification: defenders evaluate whether a third-party suspicious LLM is augmented with"}, {"title": "2. Background and Related Work", "content": "Retrieval-Augmented Generation (RAG). RAG is a technique designed to enhance the capabilities of LLMs by integrating external knowledge sources (i.e., knowledge bases) (Lewis et al., 2020). Unlike traditional LLMs, which generate responses solely based on the knowledge encoded during pre-training, RAG combines both retrieval and generation mechanisms to produce more accurate, contextually relevant, and up-to-date outputs. Existing RAG systems implemented dual encoders to map queries and texts within the knowledge base into the embedding space and retrieve candidate texts that produce high similarity values with the given query. Recent works were proposed to improve the effectiveness of retrieval models by implementing different encoder architectures (Nogueira & Cho, 2019; Humeau et al., 2020; Khattab et al., 2021), searching algorithms (Xiong et al., 2021), embedding capacity (G\u00fcnther et al., 2023), max tokens (Muennighoff et al., 2022), etc. In general, the knowledge base plays a critical role in the effectiveness of the RAG, containing valuable and often proprietary content. They are valuable intellectual property of their owners and their copyright deserves to be protected.\nPoisoning and Backdoor Attacks against RAG Systems. Recently, there are also a few pioneering works exploring data-centric threats in RAG systems (Zou et al., 2024; Xiang et al., 2024; Chen et al., 2024; Cheng et al., 2024). Specifically, PoisonedRAG (Zou et al., 2024) proposed the first data poisoning attack against RAG by injecting several malicious and wrong answers into the knowledge base for each predefined query. The adversaries could lead the compromised RAG to generate targeted wrong answers with these predefined queries. TrojanAgent (Cheng et al., 2024) proposed a backdoor attack by compromising its retriever; thus, leveraging queries attaching with adversary-specified optimized trigger patterns could activate the malicious behavior embedded in its compromised retriever. Most recently, AgentPoison (Chen et al., 2024) proposed the backdoor attack against RAG by injecting optimized malicious target texts (decisions) into the external knowledge base. AgentPoison also proposed an optimization framework to optimize a stealthy and effective trigger pattern for increasing the probability of retriever retrieving the hidden malicious target texts. These methods all seriously undermine the integrity of RAG systems, making them susceptible to anomaly detection and"}, {"title": "3. Methodology", "content": "3.1. Preliminaries and Threat Model\nThe Main Pipeline of Retrieval-augmented LLMs. In this paper, we discuss LLMs built with retrieval-augmented generation (RAG) mechanism under a knowledge base D based on the prompt corpus. Specifically, the knowledge base D contains a set of in-context query-solution examples {xi, Yi}ND, where x and y represent the query and its corresponding solution within the retrial knowledge base D, respectively. In RAG, for each given query \u00e6, the retrieval model uses an encoder Eq(; 0\u2084) parameterized by Oq to map it into the embedding space via Eq(x;0q) and seeks the most relevant samples within D based on their similarity (i.e., cosine similarity). Technically, RAG finds k nearest examples within D of x (dubbed \u025bk (x, D)) in the embedding space through KNN search (Cover & Hart, 1967). After retrieving \u03b5\u03ba(x, D), RAG arranges these instances and x into an augmented input text \u00e6r using a specifically designed template. Finally, the (pre-trained) LLM f(\u00b7; \u03b8\u03b9) takes \u00e6r as input to perform in-context learning and output the generated text f (xr; \u03b8\u03b9).\nThreat Model. Following previous works in data copyright protection, we consider two main parties, including the de-"}, {"title": "3.2. The Overview of RAG", "content": "Before we illustrate the technical details of our method, we first provide the definition of the degree of harmfulness.\nDefinition 3.1 (Harmfulness Degree of Ownership Verification for Knowledge Base). Let D = {(xi, Yi)}\\\u2081 indicates the pairs of questions and results for ownership verification of a RAG system with the LLM f, where is the verification question with y\u2081 as its solution. H\uc2a5 \ub274 \u03a311{Y & f(x)} where I{\u00b7} is the indicator function.\nAccording to Definition 3.1, it is obvious that existing poisoning-based or backdoor-based methods can not achieve harmless verification. To address this problem, we propose to implant verification-required distinctive behaviors in the space of chain-of-thought instead of in the final results.\nSpecifically, as shown in Figure 2, our RAG\u00a9 consists of three main stages, including (1) generating CoTs, (2) optimizing watermark phrases and target CoTs, and (3) ownership verification. In the first stage, RAG\u00a9 generates two CoTs for each defender-specified verification question. In the second stage, RAG\u00a9 optimizes watermark phrases and target CoTs to minimize retrieval errors under the black-box setting of suspicious LLM, ensuring that the watermarked verification questions activate the target CoTs without being activated in non-watermarked ones. In the last stage, RAG\u00a9 exploits the pairwise Wilcoxon test to statistically verify whether a suspicious LLM is augmented with the protected knowledge base by comparing its responses to watermarked and benign verification questions. Their technical details are described in the following parts."}, {"title": "3.3. Generating Watermark Phrases and Target CoTs", "content": "We hereby introduce the first two stages of our RAG\u00a9.\nGenerating CoTs. Let {x}_1 denote n defender-specified verification questions. For each question xi, RAG\u00a9 requires human experts or an advanced LLM (e.g., GPT-4) to generate the correct answer y\u017c and its corresponding two distinctive chain-of-thoughts (CoTs) (i.e., c(1) and c2)). Without loss of generality, let c c1) and c(2) denote"}, {"title": "3.4. Ownership Verification in RAGO", "content": "RAGO hereby identifies whether a given suspicious LLM is augmented with our protected knowledge base by querying it with the original and watermarked verification questions.\nSpecifically, we query the suspicious LLM f with any verification question & and its watermarked version x d to determine whether their answers contain the information of their corresponding target CoT (i.e., t \u2208 f(x + d) and t & f(x)). Given the complexity and diversity of natural languages, we leverage the power of advanced LLMs (i.e., GPT-4) to judge it. We put the designed template used by GPT-4 in Appendix B. In particular, to reduce the side effects of randomness in selecting verification questions and the VSR threshold, we design a hypothesis-test-guided method for ownership verification, as follows.\nProposition 3.3. Let X, X', T denote the variable of verification question, its watermarked version, and its target CoT, respectively. For a suspicious large language model f, suppose C is the judgment function, i.e., C(X') \u2261 2 \u00b7 I{T \u2208 f(X')} \u2013 1 and C(X) \u2261 2\u00b7 I{T \u2208 f(X)} \u2013 1. Given the null hypothesis Ho: C(X') + C(X) = 0 (H\u2081: C(X') + C(X) > 0), we claim that it is built with the protected knowledge base if and only if Ho is rejected.\nIn practice, we randomly select m (i.e., 100) verification questions (as well as their watermarked versions and target CoTs) for the ownership verification. Specifically, we hereby use the pairwise Wilcoxon test (Schmetterer, 2012) since the results of the judgment function C are discrete (i.\u0435., \u2208 {\u22121,1}) instead of following the Gaussian distribution. The null hypothesis Ho is rejected if and only if the p-value is smaller than the significance level a (e.g., 0.01)."}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nBenchmarks. Consistent with previous work (Zou et al., 2024), we use three benchmarks for evaluation, including: Natural Questions (NQ) (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), and MS-MARCO (Bajaj et al., 2016). Each evaluated benchmark contains a knowledge base and a set of questions. The details description for evaluated benchmarks are included in Appendix B.4.\nRAG Configurations. Consistent with previous work (Zou et al., 2024), we consider three retrievers, including Contriever (Izacard et al., 2022), Contriever-ms (fine-tuned on MS-MARCO) (Izacard et al., 2022), and ANCE (Xiong et al., 2021), in our evaluation. We here use Contriever-ms as the surrogate retriever for the optimization-based approach (i.e., RAG\u00a9-O). Following previous works (Zou et al., 2024; Chen et al., 2024), we exploit the dot product between the embedding space for pairs of questions and text"}, {"title": "4.2. Performance of Knowledge Base Watermarking", "content": "Evaluation Metrics. We adopt two metrics to evaluate each approach: (1) Verification Success Rate (dubbed as 'VSR') is defined as the percentage that the suspicious RAG system can generate the target CoTs for verification questions as the defender expected. (2) Harmful Degree H \u2208 [0, 1] is defined as Definition 3.1 to measure the watermark harmfulness for evaluate watermark techniques. In general, the larger VSR while the smaller H, the better the watermark.\nResults. As shown in Table 1, both existing backdoor-/poisoned-based watermarks and our RAG\u00a9-O and RAG\u00a9-L can lead a sufficient watermark effectiveness using Contriever (Izacard et al., 2022) as the target retriever. For example, all methods can lead to a high ASR greater than 0.7 in all cases (mostly > 0.8). Besides, as we expected, the optimization-based approach (i.e., RAG\u00a9-O) typically performs better than the LLM-based one (i.e., RAG\u00a9-L). As we will demonstrate in the next part, these marginal differences do not affect the accuracy of ownership verification. Beyond that, We also investigate the false positive rate of"}, {"title": "4.3. Performance of Ownership Verification", "content": "Settings. Following previous works (Li et al., 2022a; 2023; Guo et al., 2023), we evaluate the verification effectiveness of RAG under three practical scenarios: (1) independent CoT (dubbed 'Ind.-C'), (2) independent RAG (dubbed 'Ind.-R'), and (3) unauthorized knowledge base usage (dubbed 'Malicious'). In the first case, we used watermarked verification questions to query the LLMs augmented by a knowledge base embedded with different watermarked texts; In the second case, we query the innocent LLMs with our verification questions; In the last case, we query the LLMs augmented with the protected knowledge base using the corresponding watermarked verification questions. Only the last case should be treated as having unauthorized usuage.\nEvaluation Metrics. Following the settings in (Li et al., 2022a; 2023; Guo et al., 2023), we use p-value \u2208 [0, 1] for evaluation. For independent scenarios, a large p-value is expected. In contrast, for the malicious one, the smaller the"}, {"title": "4.4. Ablation Study", "content": "We hereby discuss the effects of several factors involved in our method (e.g., the number of verification questions and the length of watermark phrases). Please find more experiments regarding other parameters and details in Appendix F.\nEffects of the Length of Watermark Phrases. We hereby study the effects of the length of watermark phrases on RAGO's verification effectiveness. We conduct experiments on RAG\u00a9-L since RAG\u00a9-O cannot explicitly control the length of generated watermark phrases. Specifically, we perform RAGO-L with different lengths by adjusting the constraints for watermark phrases' length in the designed template for LLM. As shown in Figure 3(a), the VSR increases with the increase in length. However, increasing the length will also reduce the stealthiness of the watermark phrases. The owners of knowledge bases should adjust this hyper-parameter based on their specific requirements."}, {"title": "4.5. The Resistance to Potential Adaptive Attacks", "content": "Following previous work (Chen et al., 2024), we here evaluate the robustness of RAG against two potential adaptive attacks: Perplexity Filter (Alon & Kamfonas, 2023) and Query Rephrasing (Kumar et al., 2023). As shown in Table 4, both RAGO-L and RAG\u00a9-O can still perform effectively against two potential attacks, resulting in \u2265 50% and \u2265 35% VSR (\u226b 0% of the cases without RAG\u00a9) for RAGO-O and RAG\u00a9-L, respectively. In particular, RAG\u00a9-O can lead to more robust watermarking results. These results verify the resistance of RAG\u00a9 to adaptive attacks."}, {"title": "5. Conclusion", "content": "In this paper, we introduced RAG\u00a9to protect the copyright of knowledge bases used in retrieval-augmented generation (RAG) of large language models (LLMs). By leveraging chain-of-thought reasoning instead of manipulating final outputs, RAG\u00a9offers a \u2018harmless' watermarking method for ownership verification that maintains the correctness of the generated answers of LLMs augmented with the protected knowledge base. RAG\u00a9 leveraged optimized watermark phrases and verification questions to detect potential misuse through hypothesis-test-guided ownership verification. We also provided the theoretical foundations of our RAGO.\nExtensive experiments on benchmark datasets verified the effectiveness of RAG\u00a9 and its resistance to potential adaptive attacks. Our work highlights the urgency of protecting copyright in RAG's knowledge bases and provides its solutions, to facilitate their trustworthy circulation and deployment."}, {"title": "Impact Statement", "content": "Unauthorized knowledge base misuse and stealing have posed a serious threat to the intellectual property rights (IPRs) of the knowledge base's owners. Arguably, ownership verification via watermarking knowledge bases is a promising solution to detect whether a suspicious LLM is augmented by the protected knowledge base. In this paper, we propose a new paradigm of harmless knowledge base ownership verification, named RAG\u00a9. Our RAG\u00a9 is purely defensive and harmless, which does not introduce new threats. Moreover, our work only exploits the open-source benchmark and does not infringe on the privacy of any individual. As such, this work does not any raise ethical issue and negative societal impact in general."}, {"title": "Appendix", "content": "A. Proof for Theorem 1\nTheorem A.1 (Retrieval Error Bound for the Watermarked Target CoT). Let r and ro be the portion of questions with type c in the set of verification questions D and knowledge base D, respectively. Let s\u00f8 (x + d, D\u00af(t \u2295 \u03b4)) is the cosine similarity measurement given by a retrieval model Eq(\u00b7; 0q) and D\u00af(t + d) denotes data in D other than the watermarked target CoT (i.e., t\u2295 d), where x is the verification question, t is the target CoT, denotes concatenation, and 8 is the watermark phase. Let Z be the retrieval result given by the retriever Eq, we have the following inequality:\n$P[t \u00a9 8 \u00a2 Z(x + 8, D)] \u2264 \u2211r% \u2022 (1 - r) - |D|- P[so, (x + 8,t + d) < so, (x + 8, D\u00af(t \u2212 8))]\\D\\-r%,$\nwhere |D| is the size of knowledge base D.\nproof. We upper bound the probability that the watermarked target text t\u2295 d can not be retrieved given its corresponding watermark query x d as following:\n$P[td & Z(x + \u03b4, D)] = P$\n$Px8~D t$\n$= P\u00e6\u22958~D max se(t, xd) \u2265 max$\n$t-ED-(t+d)$\n$t+ED+(t+d)$\n$= Px+8~D max seo (t, x) \u2265 max$\n$t+ED+(t+d)$", "subsections": [{"title": "B. Templates used by RAGO", "content": "B.1. Templates and Demonstrations for Generating CoTs\nTemplate.\nPrompt: You are a helpful assistant, below is a query from a user.\nQuestion: [verification question] and Why? Could you provide two distinct reasons using the Chain-of-Thought approach? Additionally, ensure that each reason is independent in contents and fully self-contained."}]}]}