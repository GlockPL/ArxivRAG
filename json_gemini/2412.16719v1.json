{"title": "Large Language Models Compression via Low-Rank Feature Distillation", "authors": ["Yaya Sy", "Christophe Cerisara", "Irina Illina"], "abstract": "Current LLM structured pruning methods involve two steps: (1) compressing with calibration data and (2) continued pretraining on billions of tokens to recover the lost performance. This costly second step is needed as the first step significantly impacts performance. Previous studies have found that pretrained Transformer weights aren't inherently low-rank, unlike their activations, which may explain this performance drop. Based on this observation, we introduce a one-shot compression method that locally distills low-rank weights. We accelerate convergence by initializing the low-rank weights with SVD and using a joint loss that combines teacher and student activations. We reduce memory requirements by applying local gradient updates only. Our approach can compress Mixtral-8x7B within minutes on a single A100 GPU, removing 10 billion parameters while maintaining over 95% of the original performance. Phi-2 3B can be compressed by 40% using only 13 million calibration tokens into a small model that competes with recent models of similar size. We show our method generalizes well to non-transformer architectures: Mamba-3B can be compressed by 20% while maintaining 99% of its performance\u00b9.", "sections": [{"title": "Introduction", "content": "Compressing Large Language Models is a crucial challenge that can be achieved with several complementary approaches: quantization, pruning, compression, and distillation. We propose a new one-shot compression method that locally distills low-rank weights and satisfies the three objectives:\n1. The compression algorithm must be compute-efficient, e.g., it should require minimal computational resources and run in a reasonable amount of time. For instance, the iterative"}, {"title": "Related Works", "content": "We discuss next previous compression methods, highlight their limitations, and explain how our approach addresses them.\nPruning methods remove unimportant weights in the pre-trained model (LeCun et al., 1989; Han et al., 2015). Structured Pruning removes entire groups of parameters, which results in a smaller and faster model (Xia et al., 2024; Ma et al., 2023).\nLow-Rank Decomposition compresses a model by approximating its pre-trained matrices with lower-dimension ones. Based on the observation by Li et al. (2018) that Neural Networks have lower intrinsic dimensions, Aghajanyan et al. (2020) show that Transformer language models also require lower intrinsic dimensions depending on the language task."}, {"title": "Background: Low-Rank Approximation of Transformer Models", "content": null}, {"title": "Low-Rank Approximation from Weights.", "content": "Given $W \\in \\mathbb{R}^{d_1\\times d_2}$, a pretrained weight matrix, and an example $x \\in \\mathbb{R}^{d_2}$, the activations $y \\in \\mathbb{R}^{d_1}$ are computed as $y = Wx$. It is possible to reduce the computations in this operation by using a low-rank approximation of $W$: $y \\approx \\Delta Wx = ABx$ with $A_W = AB$ the low-rank decomposition of $W$, composed of $A \\in \\mathbb{R}^{d_1\\times r}$ and $B\\in \\mathbb{R}^{r\\times d_2}$. When the rank $r$ is small enough, the number of parameters $r(d_1 + d_2)$ in the low-rank equation is smaller than $d_1d_2$ in the full-rank equation. Estimating the low-rank matrix $A_W = AB$ can be formulated as a minimization problem to find the low-rank $A_W$ that best approximates $W$:\n$\\qquad A_W = \\underset{A_W}{\\operatorname{argmin}} ||W - A_W||_F \\qquad$(1)\nIt is well-known that this minimization problem can be approached using SVD, a low-rank approximation method that offers the optimal r-rank approximation of a given matrix with regard to the Frobenius norm $|| . ||_F$. SVD approximates a ma-"}, {"title": "Low-Rank Approximation from Feature", "content": "Previous studies (Chen et al., 2021; Yu and Wu, 2023) have shown that the activations (i.e., features) of pretrained transformers, are more low-rank than the weights. Recently, Liu et al. (2024) show that transformer activations can be sparsified up to 60% without too much drop in performance.\nNow, let's examine how previous works (Kaushal et al., 2023; Chen et al., 2021; Yu and Wu, 2023) have approached this.\nLet $D = {x_i \\in \\mathbb{R}^{d_2}}_{1<i<N}$ be a calibration dataset, and $W \\in \\mathbb{R}^{d_1\\times d_2}$ the parameters of a linear layer. Finding the low-rank matrix $A_W$ that best reproduces the activations ${y = Wx} \\forall x \\in D$ involves solving:\n$\\qquad A_W = \\underset{A_W}{\\operatorname{argmin}} \\frac{1}{N} \\sum_{x \\in D}|Wx - A_Wx||_F \\qquad$(3)\nAn analytic solution to this minimization problem is the eigendecomposition of the covariance matrix of the activations. We first collect all activations $Y = {y = Wx}\\{x\\in D$, then the covariance matrix of the activations can be estimated as $\\Sigma = E_Y[yy^T] - E[y]E[y]^T$ with $\\Sigma \\in \\mathbb{R}^{d_1\\times d_1}$. Since $\\Sigma$ is a diagonalizable matrix, we can apply its eigendecomposition: $\\Sigma = USU^T$, where $U \\in \\mathbb{R}^{d_1\\times d_1}$ contains the eigenvectors of $\\Sigma$ and $S\\in \\mathbb{R}^{d_1\\times d_1}$ is the diagonal matrix containing the eigenvalues sorted in decreasing order. As for Eq-2, we can only keep the eigenvectors corresponding to the r largest eigenvalues, which gives us $A = U_{:,:r}$ and $B = U^T W$. Compared to the low-rank matrices in Eq-2, A and B weights here learned to reproduce the activations of the base weight W, thanks to the minimization objective in Eq-3. In the next section, we highlight the limitations of this approach and"}, {"title": "Proposed Approach", "content": "We identify and handle next three potential limitations of the approach presented in Section 3.2.\nNon-linear Feature Approximation. The previous analytical solution for Eq-3 is limited to activations from linear layers. To generalize to non-linear modules, we first observe that Eq-3 can be seen as a feature distillation objective that may be optimized numerically by gradient descent rather than analytically by eigendecomposition. Given the input batch $X \\in \\mathbb{R}^{d\\times b}$ of b examples, we denote the output activations of the ith Teacher module $T^{(i)}$ as\n$\\qquad Y^{(i)} = T^{(i)} (X; \\Theta^{(i)}) \\qquad$(4)\nwhere $\\Theta^{(i)}$ are the original pretrained matrices of the ith Teacher, and $Y^{(i)} \\in \\mathbb{R}^{d\\times b}$ its output activations. Similarly, we note the output activations of the ith Student module $S^{(i)}$ as :\n$\\qquad \\hat{Y}^{(i)} = S^{(i)} (X; \\Delta\\Theta^{(i)}) \\qquad$(5)\nwhere the Student module $S^{(i)}$ is parametrized with the low-rank matrices $\\Delta\\Theta^{(i)}$ and $\\hat{Y}^{(i)} \\in \\mathbb{R}^{d\\times b}$ are the output activations. We can express the distillation objective of the ith Student module as:\n$\\qquad \\Delta\\Theta^{(i)} = \\underset{\\Delta\\Theta^{(i)}}{\\operatorname{argmin}} L^{(i)} (Y^{(i)}, \\hat{Y}^{(i)}) \\qquad$(6)\nwhere $\\Delta\\Theta^{(i)}$ are the estimated low-rank matrices of the ith Student module $S^{(i)}$ and $L^{(i)} (Y^{(i)}, \\hat{Y}^{(i)})$ is the loss measuring the distance between the activations of the Teacher and the Student. We opted for the same loss as Chang et al. (2022) because we observed that the $l_1$ loss yields instabilities:\n$\\qquad L^{(i)} = L_c^{(i)} + L_e^{(i)}$\n$\\qquad = \\frac{1}{b} \\sum_{t=1}^b [- log \\sigma(\\frac{{\\hat{Y}_{t}^{(i)}}^T Y_{t}^{(i)}}{||\\hat{Y}_{t}^{(i)}|| ||Y_{t}^{(i)}||} + L_e^{(i)})] \\qquad$(7)\nwhere D is the hidden vectors dimension, $\\sigma$ is the sigmoid activation and cos(\u00b7,\u00b7) is the cosine similarity. We propose to approximate the low-rank weights of the Students through Gradient Descent.\nBeyond Teacher-only Activations. The second potential limitation of the approach described in Section 3.2 and equations 4 and 5 is that the Student"}, {"title": "Proposed Approach", "content": "the output activations of the Student module when fed $Y^{(i-1)}_T$, the output activations of the Teacher $T$ at the previous layer $i - 1$. The loss can then be written as:\n$\\qquad L = L_c^{(i)} (Y_T^{(i)}, \\hat{Y}_T^{(i)}) \\qquad$(8)\nwhere $Y_T^{(i)}$ are the gold activations of the current Teacher.\n$\\qquad L = L_c^{(i)} (Y^{(i)}, \\hat{Y}^{(i)}) \\qquad$(9)\nwhere $\\hat{Y}^{(i)} = S^{(i)} (\\hat{Y}^{(i-1)}; \\Delta\\Theta^{(i)})$ is the output activations of the current Student module S when taking as input the output activations $\\hat{Y}^{(i-1)}$ of the Student module of the previous layer, and $Y^{(i)}$ are the gold activations of the corresponding Teacher module.\n$\\qquad L_{T+S}^{(i)} = L_T^{(i)} + L_S^{(i)} \\qquad$(10)"}, {"title": "Fast and Memory-Efficient Compression", "content": "Computing the optimal compression rank of each matrix individually may be costly: while search algorithms can be a solution for smaller models, they can be difficult to scale to LLMs with billions of parameters. Assuming a target compression rate N, we propose to consider three simple strategies: uniform, top, and bottom first compression strategies, which are illustrated in Appendix A.9.\nUniform. This approach removes N% parameters to each weight matrix in the model. This strategy involves training all LLM weights.\nBottom. This approach removes N% parameters from the model by prioritizing lower layers as detailed in Algorithm 1.\nTop. This approach removes N% parameters from the model by prioritizing top layers. This consists roughly of reversing the order of layers in the line 7 of Algorithm 1."}, {"title": "Experiments on Transformers", "content": null}, {"title": "Setup", "content": "Models. We first evaluate our method on various transformer-based LLMs: a 47B mixture-of experts language model (Mixtral-v0.1 8x7B (Jiang et al., 2024)), medium-sized LLMS (Mistral-v0.1 7B (Jiang et al., 2023) and Phi-3 14B \u00b2), and a small language model (Phi-2 3B\u00b3).\nData. For calibration data, we use 13 million tokens randomly sampled from Slim-Orca (Lian"}, {"title": "Experiments on Mamba Architecture", "content": "In this section, we show the generalizability of our approach by evaluating it on the Mamba architecture for text (Gu and Dao, 2024). The attention mechanism in the Transformer is limited by its increasing complexity as a function of the input sequence length.\nMethod. We tested our approach on two Mamba architectures: Mamba 3B (Gu and Dao, 2024) and Falcon-Mamba 7B."}, {"title": "Analysis", "content": null}, {"title": "Conclusion", "content": "We propose a new efficient compression method that requires far less calibration data than most state-of-the-art approaches and provides competitive performance for various models (Dense and MoE Transformers, Mamba), modalities (Text, Speech)."}, {"title": "Limitations", "content": "Compatibility with quantization: Although we propose in this work a pruning/compression method, the LLM size reduction approach that is the most used nowadays is quantization.\nCompromise between specificity and genericity: When designing an efficient LLM compression method like the one proposed in this work and when trying to minimize the computational cost of this method, we inevitably have to make choices with regard to important compromises,"}, {"title": "On bottom, top, and uniform compression strategies", "content": "All the experiments presented here use the same hyperparameters as in Section 6.\nDifferences in running time and memory. The bottom-first compression strategy allows to compress all models\nDifferences in performances. The bottom-first compression strategy performs well and is also more memory efficient."}, {"title": "Compression at 20%, 25%, and 30%", "content": "To show how performance is affected by different compression ratios, we compressed the models Phi-2 3B, Phi-3 14B, and Mistral-v0.1 7B at 20%, 25%, and 30%."}, {"title": "Convergence when using Teacher, Student or Teacher+Student loss", "content": "As shown in Figure 3, the Teacher+Student loss combines the best of both worlds: fast convergence due to the Teacher activations and high performance at inference thanks to the Student activations."}, {"title": "Speed up for Mamba architectures", "content": "Table 13 shows that compressed Mamba models are also lighter and faster than their original model."}, {"title": "Activations are more low-rank than weights", "content": "We use stable rank (srank) as a proxy measure for rank:\n$\\qquad srank(X) = \\frac{(\\sum_{i=1}^r \\sigma_i)^2}{\\sum_{i=1}^r \\sigma_i^2} \\qquad$\nwith $X \\in \\mathbb{R}^{d_1\\times d_2}$ being the input matrix (activations or weights), $r = min(d_1,d_2)$, and $\\sigma_1 \\ge ... \\ge \\sigma_r$ the singular values of X."}, {"title": "Ease of Implementation of our approach", "content": "Our proposed pruning approach gives good performance, is cost-efficient, supports various model architectures and modalities, and is further straightforward to implement."}, {"title": "General hyper-parameters", "content": "The following general hyper-parameters are used in all our experiments, except when stated otherwise.\nImplementation. We implement our approach using version 4.44.2 of the Huggingface Transformers library (Wolf et al., 2020) and PyTorch version 2.4.0+cu121.\nHyperparameters. When compressing the models using Algorithm 1, we set a minimum rank of"}, {"title": "Metrics used when evaluating with lm-evaluation-harness", "content": "We provide in Table 17 the metrics we used to evaluate the models in Section 6."}, {"title": "A Tiny Model for a Low-Resource Language", "content": "For most human languages, there is not enough data (generally less than 1B tokens) to pretrain large LLMs. In such cases, data-efficient approaches are preferred.\n$\\qquad \\Delta \\hat{W} = \\underset{\\Delta W}{\\operatorname{argmin}} L(Wx, \\Delta \\hat{W} x) \\qquad$\nwhere $\\Delta \\hat{W}$ are the low-rank embedding matrices or prediction head, and x is an input example."}]}