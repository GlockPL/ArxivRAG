{"title": "SELD-MAMBA: SELECTIVE STATE-SPACE MODEL FOR SOUND EVENT LOCALIZATION\nAND DETECTION WITH SOURCE DISTANCE ESTIMATION", "authors": ["Da Mu", "Zhicheng Zhang", "Haobo Yue", "Zehao Wang", "Jin Tang", "Jianqin Yin"], "abstract": "In the Sound Event Localization and Detection (SELD) task,\nTransformer-based models have demonstrated impressive ca-\npabilities. However, the quadratic complexity of the Trans-\nformer's self-attention mechanism results in computational\ninefficiencies. In this paper, we propose a network architec-\nture for SELD called SELD-Mamba, which utilizes Mamba, a\nselective state-space model. We adopt the Event-Independent\nNetwork V2 (EINV2) as the foundational framework and re-\nplace its Conformer blocks with bidirectional Mamba blocks\nto capture a broader range of contextual information while\nmaintaining computational efficiency. Additionally, we im-\nplement a two-stage training method, with the first stage fo-\ncusing on Sound Event Detection (SED) and Direction of\nArrival (DoA) estimation losses, and the second stage rein-\ntroducing the Source Distance Estimation (SDE) loss. Our\nexperimental results on the 2024 DCASE Challenge Task3\ndataset demonstrate the effectiveness of the selective state-\nspace model in SELD and highlight the benefits of the two-\nstage training approach in enhancing SELD performance.", "sections": [{"title": "1. INTRODUCTION", "content": "Sound Event Localization and Detection (SELD) is a multi-\ntask that includes Sound Event Detection (SED) and Direc-\ntion of Arrival (DoA) estimation. Since its introduction as\nTask3 of the Detection and Classification of Acoustic Scenes\nand Events (DCASE) challenge [1], SELD has been sig-\nnificantly developed with the use of deep neural network\n(DNN) models [2-5], especially those based on Transformer\narchitectures, such as the Event-Independent Network V2\n(EINV2) [3] and CST-former [5]. EINV2 employs the Con-\nformer [6], which integrates convolutional layers and multi-\nhead self-attention (MHSA) mechanisms [7] to extract both\nlocal and global features. CST-former independently ap-\nplies attention mechanisms to channel, spectral, and temporal\ndomains. Although Transformer-based models have shown\npromising results, their quadratic complexity in self-attention\nrenders them computationally inefficient. Furthermore, the\n2024 DCASE Challenge Task3 introduces Source Distance\nEstimation (SDE) for the detected events, which makes the\ntask significantly more challenging.\nUtilizing State Space Models (SSMs), which establish\nlong-range context dependencies with linear computational\ncomplexity, is expected to overcome the aforementioned\nlimitation. Recently, SSMs, exemplified by Mamba [8],\nhave demonstrated their effectiveness across various do-\nmains, including natural language processing [9], computer\nvision [10, 11], and speech processing [12-14]. However,\nthe design of effective and efficient models using SSMs for\nSELD has yet to be explored.\nIn this paper, we introduce Mamba to SELD, proposing\na novel architecture named SELD-Mamba. SELD-Mamba is\nbuilt upon the robust framework of EINV2, which leverages\nthe Conv-Conformer architecture. Specifically, by replacing\nthe Conformer blocks of EINV2 with bidirectional Mamba\n(BMamba) blocks, SELD-Mamba aims to enhance the mod-\neling of audio sequence contexts while maintaining linear\ncomplexity with sequence length. Furthermore, recognizing\nthe greater challenge of SED and DoA estimation compared\nto SDE, we employ a two-stage training approach. In the first\nstage, we focus on the losses for SED and DoA estimation\ntasks, and in the second stage, we reintroduce the SDE task\nloss. Our comprehensive experiments on the 2024 DCASE\nChallenge Task3 dataset highlight the exceptional perfor-\nmance of SELD-Mamba and the effectiveness of the two-\nstage training method. Compared with EINV2, we achieve\nsuperior results by utilizing fewer parameters and reduced\ncomputational complexity. In addition to directly improving\nperformance, this work also pioneers the application of SSMs\nin the field of SELD."}, {"title": "2. RELATED WORK: MAMBA", "content": "SSM performs a sequence-to-sequence transformation, map-\nping input $x(t) \\in R$ to output $y(t) \\in R$ through an implicit\nlatent state $h(t) \\in R^N$, where N is the dimension of the hid-\ndden state, as illustrated in the equation below:\n$h'(t) = Ah(t) + Bx(t), y(t) = Ch(t)$\nwhere $A \\in R^{N \\times N}$, $B \\in R^{N \\times 1}$, and $C \\in R^{1 \\times N}$ represent\nthe state transition matrix, the input projection matrix, and"}, {"title": "3. METHOD", "content": "In this section, we will first explain SELD-Mamba, as illus-\ntrated in Fig.1.(a), with a focus on the BMamba block. Then,\nwe will introduce the loss function design and outline our\ntwo-stage training method."}, {"title": "3.1. SELD-Mamba", "content": "The SELD-Mamba model utilizes EINV2 as its backbone, a\nmulti-task learning network with two branches dedicated to\nthe SED and DoA estimation tasks. We expand it to three\nbranches by incorporating the SDE task. Additionally, we\nreplace the Conformer blocks with BMamba blocks."}, {"title": "3.1.1. Encoder", "content": "The encoder processes input features extracted from the FOA\narray signals. Specifically, we extract log-mel spectrogram\nand Intensity Vectors (IVs), which are then concatenated\nalong the channel dimension, resulting in audio features with\na shape of 7 \u00d7 T \u00d7 F, where 7 represents channels, T rep-\nresents the temporal bins, and F represents the frequency\nbins. The three branches receive different audio features: the\nSED and SDE branches receive log-mel spectrograms, while\nthe DoA branch receives both log-mel spectrograms and IVs.\nEach branch contains four Dual Convolutional (Dual Conv)\nlayers. Time-Frequency (T-F) pooling layers are applied af-\nter the first three Dual Conv layers, while only F pooling\nis applied after the final Dual Conv layer. This results in a\ntensor with a shape of 512 \u00d7 T/8 \u00d7 F/16. This tensor is then\nreshaped and applied frequency average pooling, producing\nT/8 \u00d7 512 dimensional feature embedding. The T/8 dimen-\nsion ensures alignment with the temporal resolution of the\ntarget label.\nIn addition, we employ cross-stitch [15] as soft connec-\ntions to facilitate the exchange of useful information between\neach branch, represented as follows:\n$[\\hat{x}_{SED}, \\hat{x}_{DOA}, \\hat{x}_{SDE}] = \\alpha [x_{SED}, x_{DOA}, x_{SDE}]$\nwhere $\\hat{x}_{SED}, \\hat{x}_{DOA}, and \\hat{x}_{SDE}$ are the new features, and\n$x_{SED}, x_{DOA}, and x_{SDE}$ are the original features. $\\alpha$ is a 3 \u00d73\nmatrix that denotes learnable parameters."}, {"title": "3.1.2. Decoder", "content": "As the decoder, we replace the Conformer with BMamba.\nEach branch utilizes three parallel BMamba blocks, cor-\nresponding to the three tracks of the output. The Mamba\narchitecture is limited to capturing only historical infor-\nmation about the input due to its causal processing. To\nleverage future context, we borrow the BMamba design\nfrom [12]. This design processes the original and flipped\ninput sequences through two separate Mamba components,\nas shown in Fig.1.(b). A Mamba component is composed of\ntwo Mamba layers, with the structure of one Mamba layer\nillustrated in Fig.2.\nTaking forward audio sequence as an example, we begin\nwith an input $u \\in R^{L \\times D}$, where L is the number of frames\nand D matches the encoder dimension. A linear layer projects\nu to $\\hat{u} \\in R^{L \\times E}$, where $E = 2D$, representing the dimension\nexpanded by a factor of 2. Another linear layer projects u to\n$z \\in R^{L \\times E}$, which will be used to gate the outputs of SSM:\n$\\hat{u} = Linear_{input}(u), z = Linear_{gated}(u)$\nNext, $\\hat{u}$ is processed through convolution and SiLU activa-\ntion, resulting in x:\n$x = \\sigma(Conv1D(\\hat{u}))$\nwhere $\\sigma$ represents the SiLU function. Then, $\\\u00e6$ serves as the\ninput to the SSM, as described in Section 2. The outputs of\nthe SSM are gated by $\\sigma(z)$:\n$y = \\sigma(z) \\otimes SSM(x)$\nA linear projection is then applied to obtain the final output:\n$\\hat{y} = Linear_{output}(y)$\n$\\hat{y}$ is used as the input for the next Mamba layer.\nWe employ RMSNorm [16] to normalize the outputs of\nthe Mamba layers. The outputs obtained from the backward\nMamba are then reversed to the forward direction and fused\nwith the outputs from the forward Mamba through element-\nwise addition."}, {"title": "3.2. Loss Function", "content": "For the loss function, we utilize frame-level Permutation In-\nvariant Training (PIT) [2] to compute the total loss:\n$L_{PIT}(\\Theta) =$\n$\\underset{\\alpha \\in P(\\Theta)}{min} {\\lambda_1 L_{SED} (\\sigma) + \\lambda_2 L_{DOA}(\\sigma) + \\lambda_3 L_{SDE}(\\sigma)}$\nWhere $\\alpha \\in P(\\Theta)$ denotes one of the possible permutations.\n$L_{SED}$ is binary cross entropy loss for SED, $L_{DOA}$ is mean\nsquared error loss for DoA, and $L_{SDE}$ is L1 loss for SDE. $\\lambda_1$,\n$\\lambda_2$, and $\\lambda_3$ are weights for the SED, DoA, and SDE losses,\nrespectively. The permutation yielding the minimum loss is\nselected for optimization.\nIn comparison to the SDE task, the SED and DoA esti-\nmation tasks are considerably more challenging. Therefore,\nwe introduce a two-stage training method for SELD-Mamba.\nInitially, we focus on optimizing the SED and DoA losses by\nsetting $\\lambda_3$ to 0 and assigning weights of $\\lambda_1$ = 25 and $\\lambda_2$ = 5.\nIn the second stage, we reintroduce the SDE loss by adjust-\ning $\\lambda_3$ to 3. This two-stage training approach is essential for\nachieving balanced performance across the three tasks."}, {"title": "4. EXPERIMENTS", "content": "The proposed method was evaluated using the official devel-\nopment [18] and synthetic dataset [19] of the 2024 DCASE\nChallenge Task3, without employing data augmentation. The\nmodel was only trained on FoA array signals. Audio clips\nwere segmented into non-overlapping 5-second fixed seg-\nments, with a sampling rate of 24 kHz. A Short Time Fourier\nTransform (STFT) was applied using a 1024-point Hanning\nwindow and a hop size of 300. Subsequently, log-mel spec-\ntrograms and IVs were generated in the log-mel space with\n128 frequency bins. The corresponding audio features were\nfed into their respective branches. The output includes three\ntracks, enabling the detection of up to three overlapping\nsound events. The AdamW [20] optimizer was employed\nfor training over 80 epochs. The initial learning rate was\nset at 0.0003 and halved after 65 epochs. We employed two"}, {"title": "4.3. Ablations", "content": "To find the best input features for the SDE branch, we tested\ntwo types of features, with the results shown in Table 2. Com-"}, {"title": "4.3.1. Input features of SDE branch", "content": "pared to using log-mel spectrograms alone, adding IVs only\nimproved the DOAE. This might be due to IVs providing\nmore source direction information, but not necessarily offer-\ning additional benefits for sound class perception and source\ndistance estimation. Therefore, we chose to use only log-mel\nspectrograms as the input for the SDE branch."}, {"title": "4.3.2. Loss weight of SDE task in the second stage", "content": "The loss weight of the SDE task in the second stage affects\nthe performance balance across different tasks. We adjusted\nthe value of $\\lambda_3$, and the results are presented in Table 3. The\nresults indicate that $\\lambda_3$ = 3 achieves a balanced performance\nand results in the best SELDscore."}, {"title": "5. CONCLUSION", "content": "In this paper, we introduce SELD-Mamba, a novel SELD ar-\nchitecture. By integrating the BMamba module into EINV2,\nSELD-Mamba is able to capture long-range contextual in-\nformation while maintaining computational efficiency. Addi-\ntionally, we employ a two-stage training approach to balance\nperformance across different tasks. Our experimental results\ndemonstrate the superior performance of SELD-Mamba and\nvalidate the effectiveness of the selective state-space model in\nthe SELD task."}]}