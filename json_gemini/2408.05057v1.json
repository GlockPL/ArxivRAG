{"title": "SELD-MAMBA: SELECTIVE STATE-SPACE MODEL FOR SOUND EVENT LOCALIZATION\nAND DETECTION WITH SOURCE DISTANCE ESTIMATION", "authors": ["Da Mu", "Zhicheng Zhang", "Haobo Yue", "Zehao Wang", "Jin Tang", "Jianqin Yin"], "abstract": "In the Sound Event Localization and Detection (SELD) task,\nTransformer-based models have demonstrated impressive ca-\npabilities. However, the quadratic complexity of the Trans-\nformer's self-attention mechanism results in computational\ninefficiencies. In this paper, we propose a network architec-\nture for SELD called SELD-Mamba, which utilizes Mamba, a\nselective state-space model. We adopt the Event-Independent\nNetwork V2 (EINV2) as the foundational framework and re-\nplace its Conformer blocks with bidirectional Mamba blocks\nto capture a broader range of contextual information while\nmaintaining computational efficiency. Additionally, we im-\nplement a two-stage training method, with the first stage fo-\ncusing on Sound Event Detection (SED) and Direction of\nArrival (DoA) estimation losses, and the second stage rein-\ntroducing the Source Distance Estimation (SDE) loss. Our\nexperimental results on the 2024 DCASE Challenge Task3\ndataset demonstrate the effectiveness of the selective state-\nspace model in SELD and highlight the benefits of the two-\nstage training approach in enhancing SELD performance.", "sections": [{"title": "1. INTRODUCTION", "content": "Sound Event Localization and Detection (SELD) is a multi-\ntask that includes Sound Event Detection (SED) and Direc-\ntion of Arrival (DoA) estimation. Since its introduction as\nTask3 of the Detection and Classification of Acoustic Scenes\nand Events (DCASE) challenge [1], SELD has been sig-\nnificantly developed with the use of deep neural network\n(DNN) models [2-5], especially those based on Transformer\narchitectures, such as the Event-Independent Network V2\n(EINV2) [3] and CST-former [5]. EINV2 employs the Con-\nformer [6], which integrates convolutional layers and multi-\nhead self-attention (MHSA) mechanisms [7] to extract both\nlocal and global features. CST-former independently ap-\nplies attention mechanisms to channel, spectral, and temporal\ndomains. Although Transformer-based models have shown\npromising results, their quadratic complexity in self-attention\nrenders them computationally inefficient. Furthermore, the\n2024 DCASE Challenge Task3 introduces Source Distance\nEstimation (SDE) for the detected events, which makes the\ntask significantly more challenging.\nUtilizing State Space Models (SSMs), which establish\nlong-range context dependencies with linear computational\ncomplexity, is expected to overcome the aforementioned\nlimitation. Recently, SSMs, exemplified by Mamba [8],\nhave demonstrated their effectiveness across various do-\nmains, including natural language processing [9], computer\nvision [10, 11], and speech processing [12-14]. However,\nthe design of effective and efficient models using SSMs for\nSELD has yet to be explored.\nIn this paper, we introduce Mamba to SELD, proposing\na novel architecture named SELD-Mamba. SELD-Mamba is\nbuilt upon the robust framework of EINV2, which leverages\nthe Conv-Conformer architecture. Specifically, by replacing\nthe Conformer blocks of EINV2 with bidirectional Mamba\n(BMamba) blocks, SELD-Mamba aims to enhance the mod-\neling of audio sequence contexts while maintaining linear\ncomplexity with sequence length. Furthermore, recognizing\nthe greater challenge of SED and DoA estimation compared\nto SDE, we employ a two-stage training approach. In the first\nstage, we focus on the losses for SED and DoA estimation\ntasks, and in the second stage, we reintroduce the SDE task\nloss. Our comprehensive experiments on the 2024 DCASE\nChallenge Task3 dataset highlight the exceptional perfor-\nmance of SELD-Mamba and the effectiveness of the two-\nstage training method. Compared with EINV2, we achieve\nsuperior results by utilizing fewer parameters and reduced\ncomputational complexity. In addition to directly improving\nperformance, this work also pioneers the application of SSMs\nin the field of SELD."}, {"title": "2. RELATED WORK: MAMBA", "content": "SSM performs a sequence-to-sequence transformation, map-\nping input $x(t) \\in R$ to output $y(t) \\in R$ through an implicit\nlatent state $h(t) \\in R^N$, where N is the dimension of the hid-\ndden state, as illustrated in the equation below:\n$h'(t) = Ah(t) + Bx(t), y(t) = Ch(t)$\nwhere $A \\in R^{N\\times N}$, $B \\in R^{N\\times 1}$, and $C \\in R^{1\\times N}$ represent\nthe state transition matrix, the input projection matrix, and"}, {"title": "3. METHOD", "content": "In this section, we will first explain SELD-Mamba, as illus-\ntrated in Fig.1.(a), with a focus on the BMamba block. Then,\nwe will introduce the loss function design and outline our\ntwo-stage training method."}, {"title": "3.1. SELD-Mamba", "content": "The SELD-Mamba model utilizes EINV2 as its backbone, a\nmulti-task learning network with two branches dedicated to\nthe SED and DoA estimation tasks. We expand it to three\nbranches by incorporating the SDE task. Additionally, we\nreplace the Conformer blocks with BMamba blocks.\nFig.1.(a) illustrates an overview of SELD-Mamba. The\nmodel employs CNNs as the encoder and BMamba blocks\nas the decoder. The final output is produced by fully con-\nnected (FC) layers in a track-wise output format, consisting\nof three tracks. Soft connections are established between the\nthree branches, allowing each to exchange useful information\nselectively."}, {"title": "3.1.1. Encoder", "content": "The encoder processes input features extracted from the FOA\narray signals. Specifically, we extract log-mel spectrogram\nand Intensity Vectors (IVs), which are then concatenated\nalong the channel dimension, resulting in audio features with\na shape of $7 \\times T \\times F$, where 7 represents channels, T rep-\nresents the temporal bins, and F represents the frequency\nbins. The three branches receive different audio features: the\nSED and SDE branches receive log-mel spectrograms, while\nthe DoA branch receives both log-mel spectrograms and IVs.\nEach branch contains four Dual Convolutional (Dual Conv)\nlayers. Time-Frequency (T-F) pooling layers are applied af-\nter the first three Dual Conv layers, while only F pooling\nis applied after the final Dual Conv layer. This results in a\ntensor with a shape of 512 \u00d7 T/8 \u00d7 F/16. This tensor is then\nreshaped and applied frequency average pooling, producing\nT/8 \u00d7 512 dimensional feature embedding. The T/8 dimen-\nsion ensures alignment with the temporal resolution of the\ntarget label.\nIn addition, we employ cross-stitch [15] as soft connec-\ntions to facilitate the exchange of useful information between\neach branch, represented as follows:\n$[\\hat{x}_{SED}, \\hat{x}_{DOA}, \\hat{x}_{SDE}] = \\alpha [x_{SED}, x_{DOA}, x_{SDE}]$"}, {"title": "3.1.2. Decoder", "content": "As the decoder, we replace the Conformer with BMamba.\nEach branch utilizes three parallel BMamba blocks, cor-\nresponding to the three tracks of the output. The Mamba\narchitecture is limited to capturing only historical infor-\nmation about the input due to its causal processing. To\nleverage future context, we borrow the BMamba design\nfrom [12]. This design processes the original and flipped\ninput sequences through two separate Mamba components,\nas shown in Fig.1.(b). A Mamba component is composed of\ntwo Mamba layers, with the structure of one Mamba layer\nillustrated in Fig.2.\nTaking forward audio sequence as an example, we begin\nwith an input $u \\in R^{L\\times D}$, where L is the number of frames\nand D matches the encoder dimension. A linear layer projects\nu to $\\hat{u} \\in R^{L\\times E}$, where E = 2D, representing the dimension\nexpanded by a factor of 2. Another linear layer projects u to\nz\u2208 $R^{L\\times E}$, which will be used to gate the outputs of SSM:\n$\\hat{u} = Linear_{input}(u), z = Linear_{gated}(u)$\nNext, $\\hat{u}$ is processed through convolution and SiLU activa-\ntion, resulting in x:\n$x = \\sigma(Conv1D(\\hat{u}))$\nwhere $\\sigma$ represents the SiLU function. Then, x serves as the\ninput to the SSM, as described in Section 2. The outputs of\nthe SSM are gated by $\\sigma(z)$:\n$y = \\sigma(z) \\otimes SSM(x)$\nA linear projection is then applied to obtain the final output:\n$\\hat{y} = Linear_{output}(y)$\n$\\hat{y}$ is used as the input for the next Mamba layer.\nWe employ RMSNorm [16] to normalize the outputs of\nthe Mamba layers. The outputs obtained from the backward\nMamba are then reversed to the forward direction and fused\nwith the outputs from the forward Mamba through element-\nwise addition."}, {"title": "3.2. Loss Function", "content": "For the loss function, we utilize frame-level Permutation In-\nvariant Training (PIT) [2] to compute the total loss:\n$L_{PIT}(O) =$\n$min_{\\alpha \\in P(O)} {\\lambda_1L_{SED}(O) + \\lambda_2L_{DOA}(O) + \\lambda_3L_{SDE}(O)}$\nWhere $a \\in P(0)$ denotes one of the possible permutations.\n$L_{SED}$ is binary cross entropy loss for SED, $L_{DOA}$ is mean\nsquared error loss for DoA, and $L_{SDE}$ is L1 loss for SDE. $\u51651$,\n\u51652, and 3 are weights for the SED, DoA, and SDE losses,\nrespectively. The permutation yielding the minimum loss is\nselected for optimization.\nIn comparison to the SDE task, the SED and DoA esti-\nmation tasks are considerably more challenging. Therefore,\nwe introduce a two-stage training method for SELD-Mamba.\nInitially, we focus on optimizing the SED and DoA losses by\nsetting 13 to 0 and assigning weights of \u5165\u2081 = 25 and 12 = 5.\nIn the second stage, we reintroduce the SDE loss by adjust-\ning 13 to 3. This two-stage training approach is essential for\nachieving balanced performance across the three tasks."}, {"title": "4. EXPERIMENTS", "content": "The proposed method was evaluated using the official devel-\nopment [18] and synthetic dataset [19] of the 2024 DCASE\nChallenge Task3, without employing data augmentation. The\nmodel was only trained on FoA array signals. Audio clips\nwere segmented into non-overlapping 5-second fixed seg-\nments, with a sampling rate of 24 kHz. A Short Time Fourier\nTransform (STFT) was applied using a 1024-point Hanning\nwindow and a hop size of 300. Subsequently, log-mel spec-\ntrograms and IVs were generated in the log-mel space with\n128 frequency bins. The corresponding audio features were\nfed into their respective branches. The output includes three\ntracks, enabling the detection of up to three overlapping\nsound events. The AdamW [20] optimizer was employed\nfor training over 80 epochs. The initial learning rate was\nset at 0.0003 and halved after 65 epochs. We employed two"}, {"title": "4.2. Performance Comparison", "content": "To validate the proposed model, we compare SELD-Mamba\nwith the 2024 Baseline [17] and EINV2 [3] models. 2024\nBaseline employs a convolutional recurrent neural network\n(CRNN) with two additional MHSA layers. EINV2 uses\nConv-Conformer architecture. The comparison of model\nperformance is presented in Table 1.\nUsing the unified-training approach, SELD-Mamba out-\nperforms the 2024 Baseline across all metrics. Compared\nto EINV2, our $F_{20^\\circ}$ slightly lags behind, but our $DOAE$,\n$RDE$, and $SELD_{score}$ are superior. Notably, SELD-Mamba\nachieves these results with significantly fewer parameters and\nlower computational complexity. This underscores the ef-\nfectiveness and efficiency of SELD-Mamba in handling the\nSELD task.\nWhen utilizing the two-stage training approach, our\nmodel attains the best $F_{20^\\circ}$ and $DOAE$ in the first stage.\nInterestingly, even with the SDE loss weight set to 0, $RDE$\nachieved 62.6. This may be attributed to the model learning\ndistance information from the DoA estimation task. Upon\nincorporating the SDE loss in the second stage, $SELD_{score}$\nachieves 0.381. This demonstrates the effectiveness of the\ntwo-stage training approach in balancing results across dif-\nferent tasks and enhancing performance."}, {"title": "4.3. Ablations", "content": "To find the best input features for the SDE branch, we tested\ntwo types of features, with the results shown in Table 2. Com-"}, {"title": "5. CONCLUSION", "content": "In this paper, we introduce SELD-Mamba, a novel SELD ar-\nchitecture. By integrating the BMamba module into EINV2,\nSELD-Mamba is able to capture long-range contextual in-\nformation while maintaining computational efficiency. Addi-\ntionally, we employ a two-stage training approach to balance\nperformance across different tasks. Our experimental results\ndemonstrate the superior performance of SELD-Mamba and\nvalidate the effectiveness of the selective state-space model in\nthe SELD task."}]}