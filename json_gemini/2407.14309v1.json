{"title": "How to Engage your Readers?\nGenerating Guiding Questions to Promote Active Reading", "authors": ["Peng Cui", "Vil\u00e9m Zouhar", "Xiaoyu Zhang", "Mrinmaya Sachan"], "abstract": "Using questions in written text is an effective\nstrategy to enhance readability. However, what\nmakes an active reading question good, what\nthe linguistic role of these questions is, and\nwhat is their impact on human reading remains\nunderstudied. We introduce GUIDINGQ, a\ndataset of 10K in-text questions from textbooks\nand scientific articles. By analyzing the dataset,\nwe present a comprehensive understanding of\nthe use, distribution, and linguistic character-\nistics of these questions. Then, we explore\nvarious approaches to generate such questions\nusing language models. Our results highlight\nthe importance of capturing inter-question\nrelationships and the challenge of question\nposition identification in generating these\nquestions. Finally, we conduct a human study\nto understand the implication of such questions\non reading comprehension. We find that the\ngenerated questions are of high quality and are\nalmost as effective as human-written questions\nin terms of improving readers' memorization\nand comprehension.", "sections": [{"title": "Introduction", "content": "Questions play an important role in reading com-\nprehension. Through actively raising questions and\nseeking answers from the content during reading,\nreaders can deeply engage with the text and achieve\nbetter comprehension (Bharuthram, 2017; Syam-\nsiah et al., 2018). However, asking good questions\nis challenging and requires complex skills.\nHow can we facilitate readers' active thinking\nand questioning during reading? An effective\napproach could be presenting valuable questions\nexplicitly in the text, which is a recognized strategy\nto enhance readability and engage readers (Hag-\ngan, 2004). An example is shown in Figure 1. The"}, {"title": "Taxonomy of Guiding Questions", "content": "This section describes our taxonomy of guiding\nquestions. The taxonomy is built upon the discus-\nsion in Hyland (2002), which we adapt based on\nour dataset (Section 4). In particular, we classify\nquestions into five different roles based on their\ndifferent discourse or interactional effects. Their\ndefinitions and examples are provided below.\n\u2022 Arouse Interest. The first category refers to\nquestions that appear in titles. Since the title is\ngenerally the reader's first encounter with a text,\nformulating it as an intuitive question can grab\nthe reader's attention.\nQuestions in titles:\nHow do Philosophers arrive at truth?\nIs there no quantum form of Einstein Gravity?\nWhy do house-hunting ants recruit in both directions?\n\u2022 Frame Purpose. This type of question often\nsurfaces and clusters in the beginning section to\nforeground the central topics to be explored.\nThis motivates a whole set of questions in the context\nof email. How does the volume of incoming email affect\nuser behavior? How do people compensate for the\nincreased load? We address these questions with ...\nWriters pose these questions to provide an agenda\nfor the article and then pick them up again in later\nsections to direct readers through the reading.\n\u2022 Organize Discourse. Questions can also serve\nas subheadings to structure the text, guiding read-\ners by explicitly introducing shifts in information\nand identifying what will be discussed in the en-\nsuing section.\nWhat are the advances of telecommuting? The term\ntelecommuting emerged in the 1970s to What are the\ndrawbacks of telecommuting? In 2013, Yahoo's then-\nCEO, Marissa Mayer, ended\nWhat are the ethical\nchallenges of telecommuting?\nNoticeably, such questions usually appear multi-\nple times throughout an article, collectively cre-\nating a sense of progression toward a greater un-\nderstanding of the topic.\n\u2022 Establish Claim. Another use of questions is to\nintroduce and emphasize the writer's arguments\nrather than to seek the reader's interaction or\nviewpoint.\nWhat contributes to a corporation's positive image over\nthe long term? Many factors contribute, including a\nreputation for treating customers and employees fairly\nand for engaging in business honestly.\nA distinct feature of such questions is that the\nwriter often provides a clear answer (i.e., the\nargument), usually close to the question, thereby\nlimiting the reader's alternative interpretations to\nthe preferred one.\n\u2022 Provoke Thought. Finally, there are some\n\"genuine\" questions that do not anticipate spe-\ncific responses within the text. Therefore, they\ncan facilitate the reader's active thinking to the\ngreatest extent.\nIf the technological resources of today's governments\nhad been available to the East Germany Stasi and the\nRomanian Securitate, would those repressive regimes\nhave fallen? How much privacy and freedom should\ncitizens sacrifice to feel safe? [END]\nIt's worth noting that different roles are not mutu-\nally exclusive. For instance, an AROUSE INTEREST\nquestion may also provoke thoughts and vice versa.\nNevertheless, we focus on understanding the main\nrole of a question."}, {"title": "GUIDINGQ DATASET", "content": "In this section, we first discuss the choice and ra-\ntionale of source texts (\u00a7 4.1), followed by the\nconstruction pipeline (Figure 2) of the GUIDINGQ\ndataset (\u00a7 4.2). Then, we present a series of distri-\nbutional features of guiding questions (\u00a7 4.3).\n4.1 Source Texts\nWe select scientific articles and textbooks as the\nsource texts to build the dataset. Our choice is\nbased on two considerations. First, their writer-\nreader discourses have a clear communicative\nintent, either peer-to-peer or teacher-to-student,\nwhich can motivate the use of questions. Second,\nthey are formal texts written by experts, ensuring\nthat questions presented in them are strategically"}, {"title": "Construction Pipeline", "content": "We describe the main steps of collecting and anno-\ntating GUIDINGQ below.\nS1: Question Extraction. We start by extracting\nquestions from source texts by detecting interroga-\ntive marks. We only keep documents with at least\nthree questions, indicating the writer actively used\nquestions in the writing.\nS2: Question Completion. Since the extracted\nquestions are a part of the source texts, they are not\nalways semantically complete due to omissions or\nunclear pronouns, e.g., What central point might\nconstitute such a code? Therefore, we first identify\nand complete such questions based on the context.\nWe do this because it is the first step to understand-ing the meaning of such questions.\nS3: Question Answering. Next, we generate the\nanswer to each question. In particular, the answer\nshould be detailed enough and solely based on the\narticle. Therefore, we use the article's words as\nthe answer whenever possible. If a question is not\ndiscussed in the article (e.g., PROVOKE THOUGHTS\nquestion), we label it as \"NO ANSWER.\"\nS4: Evidence Extraction. Given a produced\nanswer, we automatically extract supporting sen-\ntences from the article as evidence. We do this\nby greedily searching a set of sentences that has\nthe maximum Rouge score with the answer, which\nis the standard way to find Oracle sentences for\nextractive summarization systems (Nallapati et al.,\n2017). For questions without answers as per S3,\nwe directly label them as \"NO EVIDENCE.\"\nS5: Question Role Identification. Finally, we\nidentify the role of each question. We make this the\nlast step as the information collected in previous"}, {"title": "GUIDINGQ Analysis", "content": "The statistics of the GUIDINGQ dataset is summa-\nrized in Table 1. In general, textbook chapters use\nquestions slightly more frequently than research ar-\nticles. This is possibly because teacher-to-student\ninteractions are more inclined to involve questions\nthan peer-to-peer ones.\nWhat is the distribution of question roles? In\nFigure 3, we compare the distributions of differ-\nent questions on the two subsets. As can be seen,\nresearch articles contain more FRAME PURPOSE\nand ORGANIZE DISCOURSE questions, while text-\nbooks favor PROVOKE THOUGHT questions possi-\nbly due to their educational purpose. Besides,\nAROUSE INTEREST (title) questions are more com-\nmon in articles than textbooks, although both pro-\nportions are small. A possible reason is that re-\nsearch articles exist in a competitive environment\nwhere potential readers are confronted with a large\nnumber of papers, under which circumstances inter-\nrogative titles could help attract readers (Haggan,\n2004; Jamali and Nikzad, 2011)."}, {"title": "Guiding Question Generation", "content": "In this section, we first describe our methods to\nmodel guiding questions and then report experi-\nmental results.\n5.1 Task Formulation\nGiven a document D = {$1,..., $n} of n sen-\ntences, our goal is to learn a sequence of ques-\ntions Q = {q1, \u2026, qm}, their positions in the ar-\nticle P = {p1, \u2026, pm}, and (optionally) their an-\nswer information A = {a1, ..., am}. In particular,\n1 \u2264 pi\u2264 n is the index of the sentence after which\nqi should be asked. We call these sentences as\nanchor sentences {$p1, \u2026\u2026\u2026, Spm}.\n5.2 Data Preparation\nTo construct training examples, we remove ques-\ntions from articles and reconstruct them based on\nthe corrupted articles such that the model can learn\nhow to use guiding questions as human writers. In\nconcrete, given an article D, we extract its ques-\ntions and locate their positions to obtain Q and P.\nFor each question qi, its answer information is a set\nof keywords a\u2081 = {W1, ..., W|az|} extracted from its\nevidence sentences obtained during the annotation\n(Section 4.2). We take this form to exclude re-\ndundant information in the full answer and reduce\ninput (output) length for efficiency consideration.\nSince deleting sentences would create incoher-\nence and reduce learning P to identify where sen-\ntences are removed, we use gpt-3.5-turbo-1106\nto assess the coherence of missing positions and,\nif necessary, eliminate the incoherence by making\nsmall edits around the missing positions. Since\nthere could still be nuanced differences in edited\npositions, we perform the same \u201cdelete and smooth\"\noperation on 1% randomly selected non-question\nsentences as noise, detailed in Appendix B.1.\n5.3 Modeling\nIn what follows, we describe three popular QG\nparadigms considered for our task: Pipeline, Mul-\ntitask, and Joint Generation (Ushio et al., 2023).\nAll approaches are unified as a text generation task\nand use Flan-T5-base (Chung et al., 2022) as the\nbackbone model, which we finetune on our dataset.\nPipeline. We decompose the task into three sub-\ntasks to learn {P, A, Q} with independent models.\nFirst, a Position Predictor (PP) identifies the po-\nsitions of questions. A naive way is to directly gen-\nerate the position indices {p1, ..., pm} conditioned\non D. However, this requires learning a mapping\nbetween sentences and numerical symbols. Instead,\nwe opt to identify the anchor sentences by training\nPP to copy them from D:\n$\\check{C} = argmax_{C} P_{opp} (C|D)$.\nwhere the output C = [sp\u2081|...|Spm] is a concatena-\ntion of anchor sentences separated by \"|\". P can\nbe obtained by relocating copied sentences in D.\nWhen there is no exact match, we use BM25 to get\nthe most similar sentence.\nThen, we highlight the target position in D by\ninserting a special mark [Question] after the an-\nchor sentence and use an Answer Extractor (AP)\nto generate answer keywords aj.:\nD(i) = [81, ..., Sp\u2081, [Question], ..., sn],\n$\\bar{a}_{i}= argmax_{a} P_{oe} (a|D^{(i)})$.\nwhere D(i) is the document marked at position pi.\nFinally, a Question Generator (QG) generates\nquestions based on predicted positions and ex-\ntracted answers:\n$\\hat{q}_{i}= argmax_{q} P_{oos} (q|D^{(i)}, a_{i})$.\nNote that the PP model predicts all positions in one\npass while the other two generate output one by\none.\nMultiTask. The multitask model still consists of\nthe three components described above. However,\ninstead of independently training three models, we\ntrain a unified model for all tasks in a multitask\nlearning manner. In practice, we mix the training\nexamples of the three tasks and distinguish them\nby adding different task prefixes before the inputs.\nJoint Model. As observed in Figure 1, guid-\ning questions tend to be related to each other.\nTherefore, we consider jointly generating all\nquestions at the same time. To be spe-\ncific, we use a template function to convert\n{P, A, Q} into a flattened sequence T(P, A, Q) =\n{t(P1, a1,91)|t(p2, a2, q2)...} where t(p,a,q) =\n\"Position:sp#Answer:a#Question:q\". The"}, {"title": "Automatic Evaluation", "content": "The main results are presented in Table 3,\nwhere we also include zero-shot prompted GPT-4\n(gpt-4-1104-preview) with the prompt in Table\n13. We report the average number of generated\nquestions # Q, Rouge (L) (Lin, 2004), Meteor\n(Banerjee and Lavie, 2005), and BertScore (Zhang\net al., 2020). Besides, we use Dist-N (1/2) (Li\net al., 2016), the percentage of distinct n-grams\nin generated questions, to measure the balance be-\ntween diversity and relevance of guiding questions.\nSince there is no one-to-one map between gener-\nated and ground-truth questions, we concatenate\nall the questions as a whole sequence to compute\nreference-based metrics.\nOverall, fine-tuned models generate fewer ques-\ntions than references, while zero-shot GPT-4 gener-\nates more. We found that Flan-T5 tends to output\nshort sequences; therefore, we attribute this to their\ndifferent pre-training paradigms. Jointly generat-\ning question roles can boost performance, which\nis expected as they are indicative of a series of dis-\ntributional features. The best fine-tuned models\nachieve remarkable BertScores. This suggests that\nthe generated questions successfully replicate the\nmain information of human questions.\nFor the textbook dataset, the joint model gen-\nerally performs the best among fine-tuned ap-\nproaches. In particular, it best resembles the Dist-\n1/2 results of human questions, while others tend to\ngenerate more independent questions (higher Dist-\n1/2). This suggests that the success of this model is\npossibly because it better learns the inter-question\nrelationships. As for the scientific set, the multi-\ntask model achieves competitive results with the\njoint model. We conjecture the more complex con-\ntent and sparser questions increase the difficulty\nof learning the question relationship, which could\ndiminish the advantage of joint generation.\nFinally, we scale up the parameter size of the\nbest-performing model Joint up to 11B. We can\nsee that scaling the model size results in significant\nperformance gain on the textbook set but little on\nthe scientific set. This demonstrates the challenge\nof understanding scientific language with LLMs.\nQuestion Position. Since the generated ques-\ntions are different from references, naively match-\ning their positions (e.g., recall, precision) is not a\nsuitable way. Instead, we evaluate a question's"}, {"title": "Human Study", "content": "Finally, we conduct a between-group human study\nto investigate the impact of guiding questions on\nreading comprehension. Participants are asked\nto read articles with (or without) questions, after\nwhich we gather their feedback and analyze their\ninformation retention and understanding to gain a\nholistic view of the effect of guiding questions.\n6.1 Experiment Design\nProcedure. The human study runs on a crowd-\nsourcing platform Prolific and consists of 4 stages.\n1. Demographic Questionnaire: We collect par-\nticipants' demographic information (Appendix\nC.1) and consent before the experiment.\n2. Article Reading: Participants read an assigned\narticle in 20 minutes with or without questions\nbeside the article. Details of the reading inter-\nface are in Appendix C.2.\n3. Comprehension Test: After reading, partici-\npants are asked to write a summary of at least\n100 words without access to the article.\n4. Evaluation: Finally, we ask participants to rate\nthe usefulness and quality of questions (If any).\nParticipants. We select 45 participants with the cri-\nteria being at least C1 command of English. Each\nparticipant received 10 GBP/hour on average. We\nrandomly assign them to one of three groups:\n\u2022 Control: reading w/o questions.\n\u2022 Reference: reading w/ expert-written questions.\n\u2022 Generated: reading w/ generated questions.\nTest Articles. We use the Joint model to generate\nquestions for five textbook chapters selected from\ndifferent domains spanning Business, Philosophy,\nSociology, Political Science, and Psychology. A\nfew small adaptations are made to make them better\nfit the study (e.g., length reduction). To focus on\nquestion quality, we generate the same number of\nquestions as reference. Each article is assigned to\n3 participants to average-out the effect of individual\narticles."}, {"title": "Effect on Reading Comprehension", "content": "User Perceived Usefulness. Results in the sec-\nond part of Table 4 prove that the generated ques-\ntions are as good as human questions in perceived\nusefulness. See the questionnaire in Table 14.\nImproved Memorization and Comprehension.\nWe use summary quality as a proxy to measure par-\nticipants' memorization and comprehension. The\nevaluation consists of three dimensions: Coher-\nence, Consistency, and Informativeness, and is\nbased on GPT-4, which has shown a superior cor-\nrelation with human annotations on summary eval-\nuation (Liu et al., 2023), detailed in Appendix C.3.\nAs shown in Table 5, the reference and generated\ngroups achieve comparable scores, and both out-\nperform the control group by a large margin. An\nadditional between-group summary analysis, in-\ncluding summary time, length, and n-gram overlap,\nis shown in Table 15.\nAn intuitive explanation for the improved quality\nis that users memorize question-related information\nand incorporate them into the summary. To verify\nthis, we measured the entailment score between\nsummaries and answers to guiding questions using\nBertScore recall (BS):\n$\\text{ENTSCORE} = \\frac{1}{|S|} \\sum_{s \\in S} \\frac{1}{|Q_s|} \\sum_{q \\in Q_s} BS_r(s, a_q)$,\nwhere s is a summary, Q, is the set of guiding\nquestions of the source article of s, and aq is the\nanswer to q."}, {"title": "Conclusion", "content": "This paper studies the discourse and interactional\nrole of guiding questions in textbooks and scien-\ntific articles. We explore various approaches for\nmodeling these questions, providing insights into\nhow to model this task and highlighting challenges\nto be solved. We validate our results with human\nstudies, which demonstrate reading with guiding\nquestions can improve the high-level memorization\nand understanding of human readers."}, {"title": "Broader Impacts", "content": "In this study, we analyzed the use of questions in\nacademic and educational articles, demonstrating\ntheir benefits for reading comprehension. While\nquestions can enhance engagement, they can also\nincrease readers' cognitive load, as evidenced by\nlonger reading times (Table 7). Additionally, ques-\ntions may introduce unintended nuances for com-\nmunication, such as creating unequal social rela-\ntionships (Hyland, 2002). Therefore, it is important\nto be aware of these mixed effects when using guid-\ning questions in writing."}, {"title": "Limitations", "content": "We summarize the limitations of this study into the\nfollowing open questions.\nIs our question role taxonomy generalizable to\nother domains?\u2605 Our investigation of the role\nof guiding questions is initially focused on text-\nbooks and scientific articles. However, different\ndomains might use questions differently. Neverthe-\nless, our analysis (Section 4.3) uncovers distribu-\ntional features that are indicative of question func-\ntions, such as their positions and question-answer\nrelationships. These findings offer insights that\ncould be generalized to understand the roles of\nquestions in broader contexts.\nHow to align guiding questions with individ-\nual preferences?\u2605 Our model aims to replicate\nguiding questions crafted by human writers. How-\never, these questions may not always resonate with\nindividual readers, given their different reading\ngoals and prior knowledge. We expect that person-\nalized generation (Cui and Sachan, 2023), which\ntakes into account user profiles, would yield more\nhelpful questions.\nHow has the role of questions evolved?\u2605 It is\nimportant to note that the use of questions could\nchange over time. For instance, Ball (2009); Jiang\nand Hyland (2022) have analyzed the distribution\nshift of questions in titles over the past decades.\nIn this study, we did not take the temporal dimen-\nsion into account, and the conclusions are based on\ncontemporary texts. Therefore, the findings of this\npaper may not remain consistent in the future."}, {"title": "Article Processing", "content": "To process an article into the training format, we\ndelete questions from the article and eliminate inco-\nherence by prompting gpt-3.5-turbo-1104 with\nthe instruction in Table 12. To reduce the cost, we\nbuild the paragraph using the 5 sentences before\nand after the deleted question, which is enough to\nassess or restore the coherence of the local context\naccording to our qualitative inspection. When per-\nforming this operation on random sentences, we\ndisallow sentences around (distance<10) any al-\nready deleted or question sentences to be selected\nin order to avoid severe incoherence."}, {"title": "Training setup", "content": "We split the dataset into 90% training and 10% test\nsets. Our implementations are based on the Trans-\nformers Library (Wolf et al., 2020). In concrete,\nfor all approaches, we fine-tune the Flan-T5 for\nup to 10 epochs with a learning rate of 5e - 5 and\nbatch size of 32. Following Raffel et al. (2020), we\nemploy the AdaFactor (Shazeer and Stern, 2018)\noptimizer and do not use warm-up. An early stop\nstrategy is applied when the loss on the validation\nset does not decrease in three continuous epochs.\nWe use 4 Nvidia Tesla A100 cards with 40 GB\nGPU memory for training. One epoch takes around\nhalf an hour. At inference, we use beam search"}, {"title": "Demographic Information of Participants", "content": "The average age was 29 years, with 25 female and\n20 male participants. The simplified ethnicity dis-\ntribution is: 23 white, 15 black, and 7 Asian. All\ninformation is on a self-identification basis."}]}