{"title": "Zero-shot Load Forecasting for Integrated Energy Systems: A Large Language Model-based Framework with Multi-task Learning", "authors": ["Jiaheng Li", "Donghe Li", "Ye Yang", "Huan Xi", "Yu Xiao", "Li Sun", "Dou An", "Qingyu Yang"], "abstract": "The growing penetration of renewable energy sources in power systems has increased the complexity and uncertainty of load forecasting, especially for integrated energy systems with multiple energy carriers. Traditional forecasting methods heavily rely on historical data and exhibit limited transferability across different scenarios, posing significant challenges for emerging applications in smart grids and energy internet. This paper proposes the TSLLM-Load Forecasting Mechanism, a novel zero-shot load forecasting framework based on large language models (LLMs) to address these challenges. The framework consists of three key components: a data preprocessing module that handles multi-source energy load data, a time series prompt generation module that bridges the semantic gap between energy data and LLMs through multi-task learning and similarity alignment, and a prediction module that leverages pre-trained LLMs for accurate forecasting. The framework's effectiveness was validated on a real-world dataset comprising load profiles from 20 Australian solar-powered households, demonstrating superior performance in both conventional and zero-shot scenarios. In conventional testing, our method achieved a Mean Squared Error (MSE) of 0.4163 and a Mean Absolute Error (MAE) of 0.3760, outperforming existing approaches by at least 8%. In zero-shot prediction experiments across 19 households, the framework maintained consistent accuracy with a total MSE of 11.2712 and MAE of 7.6709, showing at least 12% improvement over current methods. The results validate the framework's potential for accurate and transferable load forecasting in integrated energy systems, particularly beneficial for renewable energy integration and smart grid applications.", "sections": [{"title": "1. Introduction", "content": "The growing penetration of renewable energy generation has led to significant challenges for power systems, particularly in terms of system dispatch and balance. The inherent variability, intermittency, and unpredictability of renewable energy sources make traditional power system management techniques increasingly inadequate. In the development of integrated energy systems that incorporate multiple energy forms, the effective management of multi-energy flexible loads\u2014such as electricity, heating/cooling, and natural gas at the consumer end\u2014has emerged as a key strategy for mitigating renewable energy instability at the production end[1, 2, 3, 4].\nLoad forecasting plays a crucial role in energy management systems, facilitating power system planning and operational management. It helps predict future electricity demand, ensuring a balance between power supply and demand. This balance is essential for the efficient distribution and utilization of energy within the energy internet framework. As global energy structures evolve and integrated energy system dispatching advances, load forecasting technologies have gained significant attention. These technologies have reached a mature stage, encompassing various forecasting models and techniques, such as regression models, time series models[5, 6, 7], deep learning models[8, 9, 10, 11, 12, 13, 14], and artificial intelligence (AI) models[15, 16, 17, 18, 19]. Accurate load forecasting is critical for ensuring grid stability, optimizing resource allocation, promoting energy complementarity, and advancing the construction of the energy internet and integrated"}, {"title": "2. System Model of Load Forecasting", "content": "This section establishes a comprehensive mathematical framework for energy load forecasting in integrated energy systems. We first formalize the load forecasting problem for systems with multiple energy sources and then outline the key challenges in developing effective prediction models.\nIn modern integrated energy systems, multiple energy sources contribute to meeting diverse user demands. The Energy Management System (EMS) is responsible for forecasting energy requirements to optimize resource utilization, minimize waste, and guide rational electricity consumption. Consider a system with N users, denoted as $U = {u_1, u_2, ..., u_N}$. For each user $u_i$, their energy consumption data over a specific time period can be represented by a multi-dimensional time series $X_i^f$, where f indicates different energy features, and t represents the temporal dimension. For a given time interval \u2206t, the data for user ui can be expressed as:\n$X_i(\\Delta t) = \\{X_i^{hp}, X_i^{ap}, X_i^{sp}\\} \\in \\mathbb{R}^{T \\times 3}$\nwhere:\n\u2022 $X_i^{hp} \\in \\mathbb{R}^T$: Power consumption of high-power appliances\n\u2022 $X_i^{ap} \\in \\mathbb{R}^T$: Power consumption of other appliances\n\u2022 $X_i^{sp} \\in \\mathbb{R}^T$: Power generation from solar installations\nThe net energy consumption can be formulated as a temporal sequence:\n$E_i(\\Delta t) = X_i^{hp} + X_i^{ap} - X_i^{sp}$\nFor a time interval \u2206t containing T time steps, the temporal evolution of energy consumption is represented as:\n$E_i(\\Delta t) = \\{e_i^1, e_i^2, ..., e_i^T\\} \\in \\mathbb{R}^T$\nThe complete energy consumption profile for all users during this period forms a multi-dimensional tensor:\n$\\mathbb{E}(\\Delta t) = \\{E_1(\\Delta t), E_2(\\Delta t), ..., E_N(\\Delta t)\\} \\in \\mathbb{R}^{N \\times T}$\n$\\mathbb{E}(\\Delta t) = \\{e_i^t | i \\in [1, N], t \\in [1, T]\\}$\nThe objective of load forecasting is to predict energy consumption $\\hat{\\mathbb{E}}(\\Delta t+k)$ for the next k time intervals:"}, {"title": "3. TSLLM-Load Forecasting Mechanism", "content": "This section presents the proposed TSLLM-Load Forecasting Mechanism, encompassing a comprehensive framework that integrates data preprocessing, prompt generation, and prediction capabilities. The following subsections detail the design rationale, operational workflow, and mathematical foundations of each functional module.\nBuilding upon the load prediction analysis presented in Section 2, the implementation of Large Language Models (LLMs) for solar household load prediction presents three fundamental technical challenges beyond the inherent stochasticity common to all prediction methods."}, {"title": "3.1. Design Rationale", "content": "The primary challenge stems from the heterogeneity in household characteristics, as solar households exhibit diverse electricity consumption patterns influenced by geographical locations, climatic conditions, and solar panel installations. Traditional data-driven methodologies, which heavily rely on historical data for feature extraction, demonstrate limited efficacy in cold-start scenarios and cross-user transfers under few-shot conditions. Developing a model capable of accurate predictions across diverse solar households under few-shot conditions remains a fundamental challenge.\nThe second challenge involves the dimensional transformation between time series data and the textual domain. The system input comprises time series electricity load data for individual households over specified periods, denoted as $X_t \\in \\mathbb{R}^{n \\times d}$, where n represents the sequence length of the time series (i.e., the number of time steps), and d denotes the dimensionality of features at each time step. However, LLMs require textual inputs, processing them through Transformer architectures that encode relationships between text segments as high-dimensional vectors. Given that different pre-trained LLMs accommodate varying vector dimensions - with BERT and GPT-2 accepting 768-dimensional vectors and advanced models like LLAMA3 handling 1024-dimensional vectors we propose a mapping function $f : \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{d'}$, where d' represents the target embedding dimension of the specific LLM, that generates LLM-compatible input $E_t$.\nThe third challenge arises from the inherent interdependencies in multivariate time series data. For a system input containing multiple interrelated features, we define:\n$X_t = [x_t^{(1)}, x_t^{(2)}, x_t^{(3)}]$\nwhere $x_t^{(1)} \\in \\mathbb{R}^n$ represents the solar power generation sequence, $x_t^{(2)} \\in \\mathbb{R}^n$ denotes the major appliance power consumption sequence, and $x_t^{(3)} \\in \\mathbb{R}^n$ indicates the regular appliance power consumption sequence. These features exhibit complex interdependencies characterized by the functional relationship:\n$f(x_t^{(1)}, x_t^{(2)}, x_t^{(3)}) \\rightarrow \\mathbb{R}$"}, {"title": "3.2. Overview", "content": "As illustrated in Fig. 2, the TSLLM-Load Forecasting Mechanism implements a systematic workflow through three primary functional modules. Each module performs specific transformations in the sequential process of converting raw load data into accurate forecasting results.\nIn Step 1, the Data Preprocessing Module establishes the foundation of"}, {"title": "3.3. Data Preprocessing Implementation", "content": "The TSLLM-Load Forecasting Mechanism utilizes comprehensive load data from 20 selected solar households within the Ausgrid supply area, spanning from July 1, 2010, to June 30, 2013. The dataset encompasses households equipped with gross metering solar systems, with rigorous data quality assurance performed by the provider to exclude extreme cases of annual household electricity consumption and solar power generation efficiency during the initial year. The temporal resolution maintains consistency at 30-minute intervals, where each timestamp (e.g., 12:00 AM) represents the aggregated load data for the preceding 30-minute period (e.g., 11:30 PM to 12:00 AM).\nThe preprocessing pipeline comprises several essential stages to ensure data quality and consistency. For missing data points, we employ linear interpolation to maintain temporal continuity. To address the non-stationarity inherent in real-world data, we implement reversible instance normalization on the input load data. For a given load input sequence Xt over a time window of length T, the normalized sequence $\\hat{X}_t$ is computed through the transformation:\n$\\hat{X}_t = \\gamma \\frac{X_t - \\mu_t}{\\sqrt{\\sigma_t^2 + \\epsilon}} + \\beta$\nwhere:\n\u2022 $\u00b5_t$ and $\u03c3_t^2$ represent the mean and variance computed over the temporal dimension of the specific instance respectively\n\u2022 \u03b3 and \u03b2 are learnable parameters that enable adaptive scaling of the normalized data\n\u2022 e denotes a small constant (typically $10^{-5}$) introduced to ensure numerical stability\nThis normalization approach ensures consistent data distribution across different time periods while preserving the relative patterns and relationships within the load sequences. The trainable parameters \u03b3 and \u03b2 enable the model to learn optimal scaling factors during the training process, enhancing its ability to adapt to varying load patterns.\nThe implementation of this preprocessing module adheres to strict quality control measures. The linear interpolation for missing values considers both temporal proximity and pattern consistency to maintain data integrity. The normalization procedure is applied independently to each feature dimension, ensuring that the distinct characteristics of different load components are preserved while achieving consistent scale across the dataset.\nFurthermore, the module incorporates robust error handling mechanisms and validation checks to ensure the reliability of the preprocessed data. These measures include:\n\u2022 Boundary condition verification for interpolated values\n\u2022 Statistical consistency checks for normalized sequences\n\u2022 Temporal alignment validation for synchronized data streams\nThe preprocessed data provides a standardized and reliable foundation for subsequent feature extraction and model training processes. The effectiveness of these preprocessing steps is critical for the overall performance of the TSLLM-Load Forecasting Mechanism, particularly in maintaining prediction accuracy across diverse household load patterns."}, {"title": "3.4. Time Series Prompt Generation Module", "content": "The Time Series Prompt Generation Module incorporates a multi-task learning framework to effectively capture and process the intricate correlations between various features in time series data. This section presents a detailed exposition of the data decomposition, segmentation, and feature embedding methodologies."}, {"title": "3.4.1. Data Decomposition and Segmentation", "content": "We implement additive seasonal-trend decomposition to systematically partition the normalized time series into its fundamental constituent components. The decomposition process is mathematically formulated as:\n$X_t = T_t + S_t + R_t$\nwhere $T_t \\in \\mathbb{R}^n$, $S_t \\in \\mathbb{R}^n$, and $R_t \\in \\mathbb{R}^n$ represent the long-term trend, seasonal, and residual components, respectively. The decomposition follows a classical seasonal-trend additive methodology. Initially, the long-term trend component is extracted utilizing moving average methods to capture gradual variations in the time series. Subsequently, the detrended time series undergoes analysis with predefined seasonal parameters to estimate the periodic component. Finally, the residual component, which captures stochastic fluctuations, is derived by subtracting the estimated trend and seasonal components from the normalized time series.\nTo effectively capture the temporal encoding and local context within the input time series, we aggregate consecutive time steps into overlapping patch tokens. For the trend component sequence Tt, the patch token representation Pt is generated as follows:\n$P_t = \\{p_i\\}_{i=1}^{n_p}, p_i \\in \\mathbb{R}^{l \\times d}$\nwhere I denotes the patch length determining the temporal granularity of analysis, $n_p$ represents the total number of patches dictating the resolution of representation, and sh indicates the horizontal sliding stride controlling the overlap between adjacent patches."}, {"title": "3.4.2. Time Series Feature Embedding", "content": "Multi-Task Learning Framework. The module employs a sophisticated multi-task learning approach to process solar household data characterized by three distinct features: solar power generation, major appliance power consumption, and regular appliance power consumption. For a time series dataset with these heterogeneous features, we define:\n$X_t = [x_t^{(1)}, x_t^{(2)}, x_t^{(3)}]$\nwhere $x_t^{(1)}, x_t^{(2)}, x_t^{(3)} \\in \\mathbb{R}^n$ represent the respective feature sequences. The initial transformation through the input model and subsequent transformation of LLM outputs are conceptualized as tasks with inherent feature similarity. The embeddings of different features are processed as distinct but interconnected sub-tasks:\n$E_t = [e_t^{(1)}, e_t^{(2)}, e_t^{(3)}]$\nFeature Projection and Alignment. To facilitate comprehensive feature representation, we concatenate the component tokens into meta-tokens:\n$M_t = concat[T_t, S_t, R_t]$\nThese meta-tokens undergo transformation through a shared linear projection layer:\n$E_t = W_sM_t + b_s$\nwhere $W_s \\in \\mathbb{R}^{d' \\times (3d)}$ and $b_s \\in \\mathbb{R}^{d'}$ are learnable parameters that facilitate dimensionality transformation and feature extraction. The resulting embeddings are then separated by features to maintain their distinct identities while preserving inter-feature relationships.\nThe token embeddings space of the pre-trained LLM is denoted as $\\mathbb{R}^{V \\times d}$, where V represents the vocabulary size (typically in the order of tens of thousands) and d is the embedding dimension. To optimize computational efficiency and enhance model performance, we employ a task-independent linear mapping layer $W_t \\in \\mathbb{R}^{k \\times V}$ to derive a subset $\\mathbb{R}^{k \\times d}$ for alignment with time series data, where $k \\ll V$ represents a significantly reduced vocabulary size.\nThe alignment between semantic space and time series data utilizes cosine similarity, a metric particularly effective for high-dimensional vector spaces:\n$s(e, v) = \\frac{e \\cdot v}{\\|e\\| \\|v\\|}$\nwhere e \u2208 Rd represents time series embeddings and v \u2208 Rd denotes token embeddings in the reduced vocabulary space. Based on computed similarity scores, the most semantically relevant tokens are selected to enhance the input embeddings:\n$\\hat{E}_t = concat[\\{v_i\\}_{i=1}^{k}, E_t]$"}, {"title": "3.5. Time Series LLM Prediction Module", "content": "This module leverages the time series prediction prompt to generate forecasts through the pre-trained large language model. It integrates prediction generation, output reconstruction, and parameter optimization in a cohesive framework to achieve accurate load forecasting."}, {"title": "3.5.1. Reconstruction Output Layer and Pre-trained LLM", "content": "After processing through the pre-trained LLM, we strategically discard portions of the model's output $H_t \\in \\mathbb{R}^{L \\times d}$ from the m-th layer corresponding to the prefix prompt, where L represents the sequence length and d denotes the embedding dimension. The remaining output undergoes flattening and transformation through a linear mapping layer to produce the model's final output:\n$Y_t = W_r H_t + b_r$\nwhere $W_r \\in \\mathbb{R}^{(n \\times d) \\times d}$ and $b_r \\in \\mathbb{R}^{n \\times d}$ are learnable parameters of the reconstruction layer that facilitate the transformation from the LLM's embedding space to the target time series space. Owing to the initial decomposition step, the overall prediction combines individual component predictions in an additive manner. The output Yt is further decomposed into its constituent components:\n$Y_t = [y_t^{(T)}, y_t^{(S)}, y_t^{(R)}]$\nwhere $y_t^{(T)}, y_t^{(S)}$, and $y_t^{(R)}$ represent the predicted trend, seasonal, and residual components, respectively. The final prediction is obtained through component recomposition following the inverse operation of the initial decomposition:\n$X_{t+1} = y_t^{(T)} + y_t^{(S)} + y_t^{(R)}$\nThis additive recombination preserves the distinct characteristics of each component while yielding a comprehensive forecasting result."}, {"title": "3.5.2. Training and Optimization Strategy", "content": "As illustrated in Fig. 3, the training process adopts a selective optimization approach, focusing on specific model components while preserving the general capabilities of the pre-trained LLM. Empirical evidence suggests that maintaining most parameters in a non-trainable state yields superior generalization performance compared to comprehensive model retraining. Therefore, we selectively adjust the parameters of the position embedding layer and the layer normalization layer within the large language model, which are crucial for adapting the model to time series data while retaining its linguistic knowledge.\nThe optimization process targets five key components: the shared input layer in the time series prompt generation module, independent token extraction layers for each textual domain, position embedding layer, layer"}, {"title": "4. Results and Discussion", "content": "To validate the effectiveness of the proposed algorithm, we conducted comprehensive experiments using the Australian solar household dataset [41]. This dataset comprises load data collected from 300 solar-equipped households within the Australian electricity grid jurisdiction from July 1, 2010, to June 30, 2013. The data encompasses three key measurements sampled at 30-minute intervals: solar panel generation load, major appliance load, and other appliance load.\nGiven experimental constraints, we randomly selected 20 households for zero-shot load forecasting evaluation using the TSLLM-Load Forecasting Mechanism. From these households, we designated one household's load data as the primary training target, partitioning its data into training, validation, and test sets with a ratio of 7:1:2. For the remaining 19 households, we extracted load data segments of equivalent length to the first household's test set as prediction targets. The implementation utilizes GPT-2 as the backbone pre-trained large language model. All experiments were conducted using PyTorch on a computing platform equipped with an NVIDIA GeForce GTX A100 80GB GPU."}, {"title": "4.2. Experimental Framework", "content": "The TSLLM-Load Forecasting Mechanism is designed to predict household electricity load for a 48-hour horizon. Fig. 4 illustrates the experimental framework of the algorithm.\nThe model accepts three-dimensional input data covering the 10 days preceding the prediction time point: solar panel generation load, major appliance load, and other appliance load. These data streams are consolidated into a vector with dimensions (512,3) for model processing. The system transforms this input into a TS-Prompt comprising Npatch data patch prompts, where each patch prompt consists of a prefix prompt Pprefix and patch embedding Pembed\u00b7\nAs shown in Fig. 5, the model generates load forecasts for the subsequent 48 hours without requiring training on historical data from the target household. The output maintains the three-dimensional structure, producing a (96, 3) vector representing the predicted loads across all three measurement categories.\nTo evaluate prediction performance, we employ two standard metrics: Mean Squared Error (MSE) and Mean Absolute Error (MAE). These metrics quantify the deviation between predicted values and actual measurements, defined as:\n$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\nwhere yi represents the actual values, \u0177i denotes the predicted values, and n indicates the number of samples. These metrics provide complementary perspectives on prediction accuracy, with MSE being more sensitive to large errors and MAE offering a direct interpretation of average prediction deviation."}, {"title": "4.3. Comparative Analysis of Prediction Capability and Transferability", "content": "To systematically evaluate the TSLLM-Load Forecasting Mechanism's effectiveness, we conducted a comprehensive comparative analysis against state-of-the-art baseline models. Our evaluation framework encompasses two key dimensions: prediction accuracy on the training dataset and transferability performance across multiple households.\nFirst, we established a diverse baseline comprising advanced forecasting models: Informer [42], Autoformer [43], CNN_LSTM [44], CNN_LSTM_Attention [45], and TCN_LSTM_Attention [46]. Additionally, we included a single-task variant of our TSLLM-Load Forecasting mechanism to isolate the impact of the multi-task learning framework. Each baseline model represents a distinct approach to time series forecasting:\nThe CNN_LSTM architecture combines convolutional feature extraction with sequential modeling through LSTM networks. The CNN_LSTM_Attention model enhances this architecture with an attention mechanism, enabling dynamic focus on significant sequence components. The TCN_LSTM_Attention model further incorporates temporal convolutional networks, leveraging causal and dilated convolutions for improved temporal pattern capture.\nIn the Transformer-based category, Informer introduces a probabilistic sparse attention mechanism optimized for time-series forecasting, while Autoformer extends this approach with adaptive frequency decomposition for separate modeling of seasonal and trend components.\nOur experimental methodology ensured statistical robustness through three independent evaluation runs for each model, with results averaged"}, {"title": "4.4. Ablation Analysis", "content": "To understand the contribution of individual components and validate our design choices, we conducted a systematic ablation study focusing on two critical aspects: the Multi-Task Learning (MTL) framework and the"}, {"title": "4.5. Sensitivity Analysis", "content": "To investigate the influence of hyperparameter A on model performance, we conducted a comprehensive sensitivity analysis using identical training and testing data while varying A values. This analysis evaluates how different weightings between prediction and alignment losses affect the model's forecasting accuracy.\nThe experimental results, as shown in Table 1 and Fig. 9, demonstrate that increasing A initially improves time series prediction accuracy up to a certain threshold, beyond which prediction accuracy decreases significantly. The analysis reveals that while optimal \u5165 values differ slightly between MSE and MAE metrics, \u03bb = 0.1 achieves consistently strong performance across both evaluation criteria. Figure 9 illustrates this relationship, plotting the inverse of SUM_MSE and SUM_MAE against varying A values to provide a clear visualization of the model's sensitivity to this hyperparameter.\nThis sensitivity analysis provides crucial insights into the model's behavior and guides the selection of appropriate hyperparameter values for optimal performance. The results indicate that careful tuning of A is essential for balancing the contributions of prediction and alignment losses in the overall optimization objective."}, {"title": "5. Conclusions", "content": "Our paper introduces a method for load forecasting in power systems under zero-shot scenarios, termed the TSLLM-Load Forecasting Mechanism. This approach leverages large language models (LLMs) to address the limitations of existing forecasting methods, including poor transferability, inability to handle zero-shot scenarios without historical data, and lack of adaptability across diverse data sources.\nTo overcome the limitations of traditional methods in zero-shot forecasting scenarios, our approach combines large language models with load forecasting. To address the challenge of enabling LLMs to understand time series data, we designed a Time Series Prompt Generation Module. This module preprocesses power load data by decomposing and segmenting it into time series format. It employs a shared linear input layer and a task-independent text extraction layer to extract features from both the time series and text domains. Finally, a similarity alignment technique is applied to generate time series prompts, enabling the LLM to comprehend the time series data and perform load forecasting.\nIn a migration experiment involving load data from 20 Australian solar-powered households, the proposed method demonstrated superior performance compared to existing approaches in terms of Mean Squared Error (MSE) and Mean Absolute Error (MAE). It excelled both in testing on the training sample derived from one household's load data and in zero-shot migration experiments on the remaining 19 households' data. The method achieved a total MSE of 11.2712 and a total MAE of 7.6710 across the entire dataset, delivering at least a 12% performance improvement over existing methods, thereby validating the accuracy and transferability of the proposed zero-shot forecasting method."}]}