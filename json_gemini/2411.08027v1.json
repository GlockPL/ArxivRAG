{"title": "LLMPHY: COMPLEX PHYSICAL REASONING USING LARGE LANGUAGE MODELS AND WORLD MODELS", "authors": ["Anoop Cherian", "Radu Corcodel", "Siddarth Jain", "Diego Romeres"], "abstract": "Physical reasoning is an important skill needed for robotic agents when operating in the real world. However, solving such reasoning problems often involves hypothesizing and reflecting over complex multi-body interactions under the effect of a multitude of physical forces and thus learning all such interactions poses a significant hurdle for state-of-the-art machine learning frameworks, including large language models (LLMs). To study this problem, we propose a new physical reasoning task and a dataset, dubbed TraySim. Our task involves predicting the dynamics of several objects on a tray that is given an external impact \u2013 the domino effect of the ensued object interactions and their dynamics thus offering a challenging yet controlled setup, with the goal of reasoning being to infer the stability of the objects after the impact. To solve this complex physical reasoning task, we present LLMPhy, a zero-shot black-box optimization framework that leverages the physics knowledge and program synthesis abilities of LLMs, and synergizes these abilities with the world models built into modern physics engines. Specifically, LLMPhy uses an LLM to generate code to iteratively estimate the physical hyperparameters of the system (friction, damping, layout, etc.) via an implicit analysis-by-synthesis approach using a (non-differentiable) simulator in the loop and uses the inferred parameters to imagine the dynamics of the scene towards solving the reasoning task. To show the effectiveness of LLMPhy, we present experiments on our TraySim dataset to predict the steady-state poses of the objects. Our results show that the combination of the LLM and the physics engine leads to state-of-the-art zero-shot physical reasoning performance, while demonstrating superior convergence against standard black-box optimization methods and better estimation of the physical parameters. Further, we show that LLMPhy is capable of solving both continuous and discrete black-box optimization problems.", "sections": [{"title": "1 INTRODUCTION", "content": "Many recent Large Language models (LLMs) appear to demonstrate the capacity to effectively capture knowledge from vast amounts of multimodal training data and their generative capabilities allow humans to naturally interact with them towards extracting this knowledge for solving challenging real-world problems. This powerful paradigm of LLM-powered problem solving has manifested in a dramatic shift in the manner of scientific pursuit towards modeling research problems attuned to a form that can leverage this condensed knowledge of the LLMs. A few notable such efforts include, but not limited to the use of LLMs for robotic planning (Song et al., 2023; Kim et al., 2024), complex code generation (Tang et al., 2024; Jin et al., 2023), solving optimization problems (Yang et al., 2024; Hao et al., 2024), conduct sophisticated mathematical reasoning (Trinh et al., 2024; Cherian et al., 2024), or even making scientific discoveries (Romera-Paredes et al., 2024).\nWhile current LLMs seem to possess the knowledge of the physical world and may be able to provide a plan for solving a physical reasoning task (Singh et al., 2023; Kim et al., 2024) when crafted in a suitable multimodal format (prompt), their inability to interact with the real world or measure unobservable attributes of the world model, hinders their capability in solving complex physical reasoning problems (Wang et al., 2023; Bakhtin et al., 2019; Riochet et al., 2021; Harter et al., 2020; Xue et al., 2021). Consider for example the scene in Figure 1. Suppose a reasoning model is provided as input only the first image (left-most) and is asked to answer the question:"}, {"title": "2 RELATED WORKS", "content": "Large language models (LLMs) demonstrate remarkable reasoning skills across a variety of domains, highlighting their versatility and adaptability. They have shown proficiency in managing complex conversations (Glaese et al., 2022; Thoppilan et al., 2022), engaging in methodical reasoning processes (Wei et al., 2022; Kojima et al., 2022), planning (Huang et al., 2022), tackling mathematical challenges (Lewkowycz et al., 2022; Polu et al., 2022), and even generating code to solve problems (Chen et al., 2021). As we start to incorporate LLMs into physically embodied systems, it's crucial to thoroughly assess their ability for physical reasoning. However, there has been limited investigation into the physical reasoning capabilities of LLMs.\nIn the field of language-based physical reasoning, previous research has mainly concentrated on grasping physical concepts and the attributes of different objects. (Zellers et al., 2018) introduced grounded commonsense inference, merging natural language inference with commonsense reasoning. Meanwhile, (Bisk et al., 2020) developed the task of physical commonsense reasoning and a corresponding benchmark dataset, discovering that pretrained models often lack an understanding of fundamental physical properties. (Aroca-Ouellette et al., 2021) introduced a probing dataset that evaluates physical reasoning through multiple-choice questions. This dataset tests both causal and masked language models in a zero-shot context. However, many leading pretrained models struggle with reasoning about physical interactions, particularly when answer choices are reordered or questions are rephrased. (Tian et al., 2023) explored creative problem-solving capabilities of modern LLMs in constrained setting. They automatically a generate dataset consisting of real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. (Wang et al., 2023) presented a benchmark designed to assess the physics reasoning skills of large language models (LLMs). It features a range of object-attribute pairs and questions aimed at evaluating the physical reasoning capabilities of various mainstream language models across foundational, explicit, and implicit reasoning tasks. The results indicate that while models like GPT-4 demonstrate strong reasoning abilities in scenario-based tasks, they are less consistent in object-attribute reasoning compared to human performance.\nIn addition to harnessing LLMs for physical reasoning, recent works have used LLMs for optimization. The main focus has been on targeted optimization for employing LLMs to produce prompts that"}, {"title": "3 PROPOSED METHOD", "content": "The purpose of this work is to enable LLMs to perform physics-based reasoning in a zero-shot manner. Although LLMs may possess knowledge of physical principles that are learned from their training data, state-of-the-art models struggle to effectively apply this knowledge when solving specific problems. This limitation, we believe, is due to the inability of the model to interact with the scene to estimate its physical parameters, which are essential and needs to be used in the physics models for reasoning, apart from the stochastic attributes implicit in any such system. While, an LLM may be trained to implicitly model the physics given a visual scene e.g., generative models such as SoRA\u00b9, Emu-video Girdhar et al. (2023), etc., may be considered as world model simulators training such models for given scenes may demand exorbitant training data and compute cycles. Instead, in this paper, we seek an alternative approach by leveraging the recent advancements in realistic physics simulation engines, and use such simulators as a tool accessible to the LLM for solving its given physical reasoning task. Specifically, we attempt to solve the reasoning task as that of equipping the LLM to model and solve the problem using the simulator, and for which we leverage on the LLM's code generation ability as a bridge. In the following sections, we exposit the technical details involved in achieving this LLM-physics engine synergy."}, {"title": "3.1 PROBLEM SETUP", "content": "Suppose $X = (x_1, x_2,...,x_T)$ denote a video sequence with T frames capturing the dynamics of a system from a camera viewpoint v. We will omit the superscript v when referring to all the views jointly. In our setup, we assume the scene consists of a circular disk (let us call it a tray) of a given radius, friction, and mass. Further, let C denote a set of object types, e.g., in Figure 1, there are three types of objects: a bottle, a martini glass, and a wine glass. The tray is assumed to hold a maximum of K object instances, the k-th instance is denoted $o_k$; K being a perfect square. To simplify our"}, {"title": "3.2 PROBLEM FORMULATION", "content": "With the notation above, we are now ready to formally state our problem. In our setup, we define an input task instance as: $T = (\\{{x_1^v\\}}_{v=1}^{|V|}, p_s, Q, O, I, X_T, C_T)$, where $x_1^v$ is the first frame of a video sequence X with V views, $p_s$ is the initial velocity of the pusher p, Q is a question text describing the task, and O is a set of answer candidates for the question. The goal of our reasoning agent is to select the correct answer set $A \\subseteq O$. The notation $C_T \\subseteq C$ denotes the subset of object classes that are used in the given task example T. In this paper, we assume the question is the same for all task examples, i.e., which of the object instances on the tray will remain steady when impacted by the pusher with a velocity of ps? We also assume to have been given a few in-context examples I that familiarises the LLM on the structure of the programs it should generate. We found that such examples embedded in the prompt are essential for the LLM to restrict its generative skills to the problem at hand, while we emphasize that the knowledge of these in-context examples will not by themselves help the LLM to correctly solve a given test example.\nAs it is physically unrealistic to solve the above setup using only a single image (or multiple views of the same time-step), especially when different task examples have distinct dynamical physics parameters \u03a6 for CT, we also assume to have access to an additional video sequence $X'_T$ associated with the given task example T containing the same set of objects as in $x_1^v$ but in a different layout and potentially containing a smaller number of object instances. The purpose of having $X'_T$ is to estimate the physics parameters of the objects in $x_1^v$, so that these parameters can then be used to conduct physical reasoning for solving T. Note that this setup closely mirrors how humans would solve such a reasoning task. Indeed, humans may pick up and interact with some object instances in the scene to understand their physical properties, before applying sophisticated reasoning on a complex setup. Without any loss of generality, we assume the pusher velocity in $X'_T$ is fixed across all such auxiliary sequences and is different from ps, which varies across examples."}, {"title": "3.3 \u0421\u043e\u043cBINING LLMS AND PHYSICS ENGINES FOR PHYSICAL REASONING", "content": "In this section, our proposed LLMPhy method for our solving physical reasoning task is outlined. Figure 2 illustrates our setup. Since LLMs on their own may be incapable of performing physical reasoning over a given task example, we propose combining the LLM with a physics engine. The physics engine provides the constraints of the world model and evaluates the feasibility of the reasoning hypothesis generated by the LLMs. This setup provides feedback to the LLM that enables it reflect on and improve its reasoning. Effectively solving our proposed task demands inferring two key entities: i) the physical parameters of the setup, and ii) layout of the task scene for simulation using physics to solve the task. We solve for each of these sub-tasks in two distinct phases as detailed below. Figure 3 illustrates our detailed architecture, depicting the two phases and their interactions."}, {"title": "3.3.1 LLMPhy PHASE 1: INFERRING PHYSICAL PARAMETERS", "content": "As described above, given the task example T, LLMPhy uses the task auxiliary video $X'_T$ to infer the physical attributes \u03a6 of the object classes in C. Note that these physical attributes are specific to each task example. Supposer: $X \\rightarrow [\\mathbb{R}^{3\\times T\\times |C_T|}]$ be a function that extracts the 3D physical trajectories of length T of each of the objects in the given video $X'_T \\in X$, where X denotes the set of all videos. Recall that CT denotes the subset of object types that appear in the given task example.\nSuppose LLM\u2081 denotes the LLM used in Phase 1, which takes as input the in-context examples $I_1 \\in CI$ and the object trajectories from $X'_T$, and is tasked to produce a program \u03c0(\u03a6) \u2208 \u03a0, where \u03a0 denotes the set of all programs. Further, let SIM : \u03a0 $\\rightarrow [\\mathbb{R}^{3\\times T\\times |C_T|}]$ be a physics-based simulator that takes as input a program \u03c0(\u03a6) \u2208 \u03a0, executes it, and produce the trajectories of objects described by the program using the physics attributes. Then, the objective of Phase 1 of LLMPhy can be described as:\n$\\Phi^* = arg\\min_{\\Phi} ||LLMPhy_1(\\pi(\\Phi) | \\tau(X'_T), I_1) - \\tau(X'_T)||_2$,\t\t\t(1)"}, {"title": "3.3.2 LLMPhy PHASE 2: SIMULATING TASK EXAMPLE", "content": "The second phase of LLMPhy involves applying the inferred physical parameters \u03a6 for the object classes in C to solve the task problem described in $x_1^v$, i.e., the original multi-view task images (see Figure 3). This involves two steps: i) understanding the scene layout (i.e., where the various object instances are located on the tray, their classes, and attributes (e.g., color); this is important as we assume that different type of objects have distinct physical attributes, and ii) using the physical attributes and the object layout to produce code that can be executed in the physics engine to simulate the full dynamics of the system to infer the outcome; i.e., our idea is to use the simulator to synthesize a dynamical task video from the given input task images, and use the ending frames of this synthesized video to infer the outcome (see Figure 2).\nSuppose LLM2 denotes the LLM used in Phase 2, which takes as input the multi-view task images $x_1^v$, the physical attributes \u03a6*, and Phase 2 in-context examples $I_2 \\in CI$ to produce a program \u03c0(\u03a8) \u2208 \u03a0 that reproduces the scene layout parameters, i.e., the triplet \u03a8 = $\\{(class, location, color)\\}k$ for each instance. The objective for estimating the layout parameters \u03a8 can be written as:\n$\\Psi^* = arg\\min_{\\Psi} || LLMPhy_2(\\pi(\\Psi) | x_1^v, I_2) - x_1^v||_2$,\t\t\t(2)\nwhere $LLMPhy_2$ = SIM\u25e6LLM2. Once the correct layout parameters \u03a8* are estimated, we can produce a video sequence $X|\\Psi^*, \\Phi^*$ using the simulator, and which can then be used for solving the problem by selecting an answer subset A from the answer options O. We may use an LLM or extract the state of the instances within the simulator to solve the question-answering task."}, {"title": "3.4 \u039f\u03a1\u03a4\u0399MIZING LLM-SIMULATOR COMBINATION", "content": "In Alg. 1, we detail the steps for optimizing LLMPhy. Given that we assume the simulator might be non-differentiable, we frame this as a black-box optimization problem. Here, the optimization variables are sampled based on the inductive bias and the knowledge of physics learned by the LLM from its large corpora of training data. The LLM generates samples over multiple trials, which are then validated using the simulator. The resulting error is used to refine the LLM's hyperparameter search. A key insight of our approach is that, since the hyperparameters in our setup have physical interpretations in the real world, a knowledgeable LLM should be capable of selecting them appropriately by considering the error produced by its previous choices. In order for the LLM to know the history of its previous choices and the corresponding error produced, we augment the LLM prompt with this optimization trace from the simulator at each step."}, {"title": "4 EXPERIMENTS AND RESULTS", "content": "In this section, we detail our simulation setup used to build our TraySim dataset, followed by details of other parts of our framework, before presenting our results."}, {"title": "4.1 SIMULATION SETUP", "content": "As discussed in the previous section, we are determining the physical characteristics of our simulation using a physics engine. MuJoCo Todorov et al. (2012) was used to setup the simulation and compute the rigid body interactions within the scene. It is important to note that any physics engine capable of computing the forward dynamics of a multi-body system can be integrated within our framework. This is because LLMPhy implicitly estimates the outcome of a scene based on the specific physical laws the engine is computing. To be clear, LLMPhy does not assume any physical model of the world and operates entirely as a black-box optimizer. The world model is entirely captured by the physics engine that executes the program LLMPhy produces.\nThe simulation environment is build upon a template of the World, W, which contains the initial parametrization of our model of Newtonian physics. This includes the gravity vector g, time step, and contact formulation, but also graphical and rendering parameters later invoked by the LLM when executing the synthesized program. MuJoCo uses internally a soft contact model to compute for instance complementarity constraints; in our implementation we use a non-linear sigmoid function that allows a very small inter-body penetration and increases the simulation stability during abrupt accelerations. We use elliptic friction cones to replicate natural contacts more closely. We further take advantage of the model architecture of MuJoCo by programatically inserting arbitrary objects $O_k$ from the classes in C into the scene, as described in Section 3.1. For each parametric object class in C, we generate an arbitrary appearance and physical attributes such as static friction, stiffness, damping, and armature. An arbitrary number of object instances are created from each class (up to a provided limit on their total number) and placed at randomly chosen positions on a regular grid (scene layout). The graphical renderer is used to record the frame sequences X corresponding to five orthogonally placed cameras around the World origin, including a top-down camera. In addition, we support panoptic segmentation of all objects in the scene and store the corresponding masks for arbitrarily chosen keyframes. The simulated data also contains privileged information such the pusher-tray contact information (i.e. force, location, velocity, time stamp), and the stability information for each object, $S_k = \\{1| arccos(g, O_{zk}) < \\alpha, 0|otherwise\\}$, where g is the gravity vector, $O_{zk}$ is the upright direction of object k and \u03b1 is an arbitrarily chosen allowable tilt. In our experiments, we use \u03b1 = 45\u00b0.\nOther than the physics parametrization of each object class C and the scene layout $U_{o_k}$, the outcome of the simulation for sequence X is given by the initial conditions of the pusher object p, namely its initial velocity $p_s$ and position p. The usual torque representation is used:\n$\\tau = I\\dot{\\omega} + \\omega \\times I_s\\omega,$\t\t\t(3)\nwhich relates the angular acceleration $\\dot{\\omega}$ and angular velocity \u03c9 to the objects torque \u03c4. The simulator computes in the end the motion of each object based on the contact dynamics model given by:\n$M(q)\\ddot{q} + C(q, \\dot{q}) = S_a\\tau + S_u X_u + J^T(q)A_c,$\t\t\t(4)\nwhere M(q) \u2208 $ \\mathbb{R}^{(n_a+n_u)\\times(n_a+n_u)}$ is the mass matrix; q [q, $\\dot{q}$]$^T$ \u2208 $ \\mathbb{R}^{n_a+n_u}$ are generalized coordinates; and C(q, $\\dot{q}$) \u2208 $ \\mathbb{R}^{n_a+n_u}$ represents the gravitational, centrifugal, and the Coriolis term. The selector matrices $S_a = [I_{naxna} O_{naxnu}]$ and $S_u = [O_{nuxna} I_{nuxnu}]$ select the vector of generalized joint forces \u03c4 \u2208 $ \\mathbb{R}^{na}$ for the actuated joints na, or du \u2208 $ \\mathbb{R}^{nu}$ which are the generalized contact forces of the unactuated DOF created by the dynamics model, respectively. $J_c(q) \\in \\mathbb{R}^{6ncx(na+nu)}$ is the Jacobian matrix and $\u03bb_c \\in \\mathbb{R}^{6nc}$ are the generalized contact forces at nc contact points. In our simulated environment, only the pusher object p has actuated joints which sets its initial velocity and heading, while the rest of the joints are either unactuated or created by contacts. The state of the system is represented by s \uc2ab [qTqT]T."}, {"title": "4.2 TRAYSIM DATASET", "content": "Using the above setup, we created 100 task sequences using object classes C = {wine glass, martini glass, bottle} with object instances from these classes arranged roughly in a"}, {"title": "4.3 LARGE LANGUAGE MODEL", "content": "We use the OpenAI 01-mini 2 text-based LLM for our Phase 1 experiments and GPT-403 vision-and-language model in Phase 2. Note that our Phase 1 is an entirely text-based LLM as we pre-extract the object trajectories. At the time of this writing, OpenAI 01-mini model does not support visual inputs, and thus we use the GPT-40 model in Phase 2.\nPhase 1: In this phase, we provide as input to the LLM four items: i) a prompt describing the problem setup, the qualitative parameters of the objects (such as mass, height, size of tray, etc.) and the task description, ii) in-context examples consisting of sample trajectories of the object instances from its example auxiliary sequence, iii) an in-context program example that, for the given example auxiliary sequence trajectories, shows their physical parameters and the output structure, and iv) auxiliary task sequence trajectories (from the sequence for which the physical parameters have to be estimated) and a prompt describing what the LLM should do. The in-context example is meant to guide the LLM to understand the setup we have, the program structure we expect the LLM to synthesize, and our specific APIs that need to be called from the synthesized program to reconstruct the scene in our simulator. Please see our Appendix for an example of the full prompt that we use. When iterating over the LLM predictions, we augment the above prompt with the history of all the estimations of the physical parameters that the LLM produced in the previous iterations (extracted from the then generated code) and the l2 norm between the generated and ground truth object trajectories for each object instance in the auxiliary sequence, with an additional prompt to the LLM as follows: \u201cWe ran your code in our simulator using the physical parameters you provided below... The error in the prediction of the trajectories using these physical parameters is given below. Can you refine your code to make the trajectories look more similar to the ones in given in ...? Your written code should strictly follow the same code structure as provided in ...\u201d."}, {"title": "3.4 \u039f\u03a1\u03a4\u0399MIZING LLM-SIMULATOR COMBINATION", "content": "In Alg. 1, we detail the steps for optimizing LLMPhy. Given that we assume the simulator might be non-differentiable, we frame this as a black-box optimization problem. Here, the optimization variables are sampled based on the inductive bias and the knowledge of physics learned by the LLM from its large corpora of training data. The LLM generates samples over multiple trials, which are then validated using the simulator. The resulting error is used to refine the LLM's hyperparameter search. A key insight of our approach is that, since the hyperparameters in our setup have physical interpretations in the real world, a knowledgeable LLM should be capable of selecting them appropriately by considering the error produced by its previous choices. In order for the LLM to know the history of its previous choices and the corresponding error produced, we augment the LLM prompt with this optimization trace from the simulator at each step."}, {"title": "4.4 EXPERIMENTAL RESULTS", "content": ""}, {"title": "4.4.1 \u0421\u043eMPARISON WITH PRIOR METHODS", "content": "In Table 1, we compare the performance of Phase 1 and Phase 2 of LLMPhy to various alternatives and prior black-box optimization methods. Specifically, we see that random choice (Experiment #1) for the two Phases \u2013 that is, randomly choosing the physics parameters from the respective range in Phase 1, and random location, type, and objects in the synthesized program in Phase 2 \u2013 lead to only 20% accuracy. Next, if we use the Phase 2 multiview images (no Phase 1 or any sequence level information) and directly ask the LLM (GPT-40 in this case) to predict the outcome of the interaction (using the ground truth physics parameters provided), this leads to 32% accuracy as depicted in Phase 2, suggesting the LLM may be able to provide an educated guess based on the provided task images. To clarify, in this experiment, we are measuring the ability of the LLM"}, {"title": "4.5 DETAILED ANALYSIS", "content": "In this section, we make a deeper look into the performances reported in Table 1.\nConvergence: In Figure 8(a), we plot the mean convergence (over a subset of the dataset) when using GPT-40, 01-mini, Bayesian Optimization, and CMA-ES. We also compare the results to the more recent, powerful, expensive, and text-only OpenAI 01-preview LLM model, on a subset of our dataset; these experiments used a maximum of 20 optimization iterations. The convergence trajectories in this figure show that o1-mini and o1-preview perform significantly better than GPT-40 in Phase 1 optimization. Specifically, the o1 model being explicitly trained for solving scientific reasoning appears to be beneficial in our task. Interestingly, we see that ol's initial convergence is fast, however with longer iterations CMA-ES appears to outperform in minimizing the trajectory error."}, {"title": "4.6 QUALITATIVE RESULTS", "content": "In Figure 12, we show several qualitative results from our TraySim dataset and comparisons of LLMPhy predictions to those of BO and CMA-ES. In general, we find that when the velocity of the pusher is lower, and the sliding friction is high, objects tend to stay stable if they are heavier (e.g., a bottle), albeit other physics parameters also playing into the outcome. In Figure 10, we show example iterations from Phase 1 that explicitly shows how the adjustment of the physical parameters by LLMPhy is causing the predicted object trajectories to align with the ground truth. In Figure 11, we show qualitative outputs from the optimization steps in Phase 2, demonstrating how the error feedback to the LLM corrects its previous mistakes to improve the layout estimation."}, {"title": "5 CONCLUSIONS", "content": "In this paper, we introduced a novel task of predicting the outcome of complex physical interactions using an LLM with the help of a physics engine; a combination we refer to as LLMPhy. Our model systematically synergizes the capabilities of each underlying components, towards estimating the underlying physics of the scene, and experiments on our proposed dataset TraySim, demonstrate its"}, {"title": "6 LIMITATIONS", "content": "While, our simulation setup has been designed to be as general as possible, we note that we are only experimenting with four physical attributes of the physics engine to study the plausibility of implicitly estimating a world model. In real-world settings, there are many more important physics attributes that need to be accounted for. Further, we considered the use of GPT-40 in our evaluations, as it is popular for its excellent performance. We note that as it is a closed-source model and its responses may be stochastic, making it difficult to provide any guarantees on the performance. However, we would like to note that our key intention is to show the usefulness of an LLM for solving this task, and we hope future open-source LLMs would be applicable as well to our setting."}]}