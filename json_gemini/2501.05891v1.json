{"title": "Affordably Fine-tuned LLMs Provide Better Answers to Course-specific MCQs", "authors": ["Bianca Raimondi", "Saverio Giallorenzo", "Maurizio Gabbrielli"], "abstract": "In education, the capability of generating human-like text of Large Language Models (LLMs) inspired work on how they can increase the efficiency of learning and teaching. We study the affordability of these models for educators and students by investigating how LLMs answer multiple-choice questions (MCQs) with respect to hardware constraints and refinement techniques. We explore this space by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of LLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming Languages (PL)-the MCQ dataset is a contribu-tion of this work, which we make publicly available. Specifically, we dissect how different factors, such as using readily-available material-(parts of) the course's textbook-for fine-tuning and quantisation (to decrease resource usage) can change the accuracy of the responses. The main takeaway is that smaller textbook-based fine-tuned models outperform generic larger ones (whose pre-training requires conspicuous resources), making the usage of LLMs for answering MCQs resource- and material-wise affordable.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) [26] have recently attracted atten-tion for their capability to handle complex tasks [24]. In education, LLMs can help students and teachers reduce the effort involved in learning and teaching [4, 5, 18]. In this paper, we focus on studying how LLMs answer multiple-choice questions (MCQs) regarding a specific academic topic (bachelor-level programming languages) with respect to hardware constraints and refinement techniques."}, {"title": "1.1 Related Work", "content": "Recent evidence shows how ascertaining MCQs is challenging for LLMs and requires ad-hoc prompting to obtain good results [19]. Here, we look at work close to ours, focussed on the domain of question-answering for education. Nori et al. [15] evaluate GPT-4 on medical competency examinations including both QA and MCQs and Silva et al. [21] conduct a study on MCQs in the field of agriculture. In both works, the authors show that using specific prompt engineering techniques increases the number of correct answers. Other works aim to test LLMs such as LLaMA and GPT to answer MCQs. In particular, some works [14, 20, 22] show that GPT performs poorly in correctly answering Computer Science MCQs. These results align with the accuracy profiles we see in ours. Several other investigations [17, 25, 27] have assessed the ability of LLMs like LLaMA and GPT to handle MCQs. In particular, Zhang et al. [25] show a higher proficiency of LLMs in STEM than social sciences and humanities, highlighting the importance of optimising their knowledge structures. Unfortunately, Zhang et al. [25] conducted their experiments on three closed-source LLMs, which did not allow them to refine their models and further study their optimisation. In order to improve the performance of these models, Bucher and Martini [10] tried to refine some open source models in the field of text classification. The authors showed how this type of refinement, carried out on smaller models, can be used to obtain a resource with a higher precision than the larger ones. In our contribution, we provide further insight on and address some limitations of the cited work by 1) experimenting with open-source LLMs, also comparing different versions thereof (in terms of the number of parameters) and 2) investigating how one can refine the knowledge structure of LLMs, via fine-tuning."}, {"title": "1.2 Proposed Research", "content": "Given the recent, spreading success of LLMs in many assistive tasks, researchers have been exploring their potential in educa-tion. Specifically, LLMs offer opportunities such as helping students generate ideas, summarise literature, and create personalised learn-ing experiences. Moreover, they can assist lecturers by designing quizzes, providing feedback on materials and exams, and generat-ing educational content tailored to student needs. However, this new technology comes with challenges, such as the acceptance of AI feedback and the need for lecturers to integrate these tools into their teaching. For instance, to address concerns about accu-racy and trust, researchers advocate [5] that universities coordinate AI-generated and human feedback to ensure reliable information. We appraise our work in this context, aiming to find ways to improve the accuracy of the responses of such models, so that they could be used in real educational environments, thereby reducing the problem of hallucinations.\nSpecifically, we investigate the affordability of using LLMs to an-swer MCQs on a common item of undergraduate Computer Science curricula: Programming Languages (PL). We study the accuracy of a publicly available, generic model, LLaMA-2, in responding 162 MCQs on different subjects of a PL course-we make the dataset of questions publicly available. In the case of PL, LLMs need to manipulate linguistic structures that, while different from natural language, are analogous to the latter and should theoretically be within their capabilities. However, the domain of PL has technical specificities, similar to other STEM disciplines, requiring dedicated reasoning skills that the current LLMs may lack. Hence, broadening our view, we see this work contributing to the general body of knowledge focused on improving task-specific LLM accuracy. To build our study, the first question we ask is to what extent one can use the considered, generic models 'as-is' to accomplish the task of ascertaining our MCQ questionnaire. In answering this question, we dissect how different factors, such as quantisation (to decrease resource usage), can change the accuracy of the re-sponses. Following these results, we investigate the affordability of increasing the accuracy of the model. Given the work of Bucher and Martini [10], we wonder whether fine-tuning could not only improve performance in tasks such as classification (where models can be tested using classic metrics such as accuracy, precision and recall), but also be able to acquire more specific knowledge (or at least refine their handling of the lexicon) in a given subject area. This aspect can be particularly helpful in the field of Education, where the use of smaller models, and thus reduced resources, can encourage schools to adopt such technology. Namely, we refine the model by fine-tuning it with material readily available to the educator-the textbook used to teach the course-and report on the hardware requirements and accuracy increases. To further shed light on how the different parameters of the fine-tuning routine and how the material from the textbook influences the answers of the refined model, we conduct a correlation study that clarifies the relationship between these variables and the increases in ac-curacy. The LLaMA-2 model comes in four sizes-defined by their number of parameters; empirically, ceteris paribus, the greater the number of parameters afforded by a model the more accurate are its responses [7]. We also explore this dimension by fine-tuning the two smaller variants 7B and 13B and comparing their accuracy with respect to the generic largest version 70B. We measure the hardware requirements for refining 7B and 13B, and show how the fine-tuned smaller models achieve equal or greater accuracy than the largest pre-trained variant.\nThe paper is structured in the following way: in Section 2, we present the dataset, the experimental settings to test the models and the initial accuracy of the models before fine-tuning, in Section 3, we present the general results and those related to the parameters of fine-tuning, in Section 4, we comment on the results of our work and discuss its limitations."}, {"title": "2 Methodology", "content": "MCQ Dataset. We first present the MCQ dataset.\nBoth dataset \u00b9 and code 2 are publicly available. Data include 162 questions where 15% are from an undergraduate-level university course on Programming Languages of the degree in Computer Science, 25% from a book on undergraduate-level questions on PL by Williams et al. [23], and 60% come from publicly available MCQs from the web (we confirmed the appropriateness of this set of questions with 2 undergraduate PL course teachers).\nWe format the dataset in JSON, of which we report an excerpt in Fig. 1. Technically, the dataset is an array of items that include 5 fields: question, which contains the text of the MCQ question; options, which is the list of at least 2 possible answers to the MCQ; answers, which contains the character identifiers (a, b, c, ...) of the correct answers; explanation, which provides an explanation for the correct answer(s); topic, which specifies the specific domain of the question with respect to the partition of subjects as found in the chapters of the PL university textbook (we use this datum for fine-tuning and our correlation study). Since 19 of all 162 questions have multiple correct answer identifiers, the answer field is an array of character identifiers. To check the LLM's answer, it must provide at least one of these correct answers.\nWe represent the list of subjects with the related number of questions in the MCQ dataset in Fig. 2.\nAll subjects except the Programming Languages MCQs cover different chapters of the book used for fine-tuning. The latter par-tition, containing 80 MCQs, covers topics that are still related to the topic of PL but do not appear in the textbook. We include these questions in the dataset as a litmus test to distinguish between ques-tions that a fine-tuned model can answer correctly thanks to the\nhttps://zenodo.org/records/14284346\n2https://github.com/biancaraimondi/LLaMA2_for_MCQs"}, {"title": "Performance of Generic LLaMA-2 Variants.", "content": "We take the 16-bit 7B, 13B, and 70B variants of LLaMA-2 [11] and test their accuracy against our dataset-technically, we use them for inference, i.e., to generate a response to the prompts described above.\nWe start our analysis of affordability by studying how lighter alternatives of the variants compare against their base versions. Specifically, we include in the comparison the quantised alterna-tives 7Bq, 13Bq, and 70Bq, which decrease the hardware resources to run inference. To obtain the quantised alternatives, we use the bits-and-bytes 4-bit NormalFloat quantisation technique [3], which substantially decreases the amount of needed memory while keep-ing accuracy high (with respect to same-memory-occupation al-ternatives like 4-bit Floating Point) and avoid the more expensive 8-bit alternatives [2]\nWe run our tests on a machine with an Intel Xeon Platinum 8480+ CPU, and an NVIDIA A100-SXM4, with 80GB of GPU memory-which can host all variants except the largest model of 70B.\nWe report the results of our experiments in Table 1, where the best-performing variant is 70Bq, with an accuracy of 59%, followed by 13B, 13Bq, 7B, and 7Bq-as expected, quantisation reduces the accuracy of the models, but the loss is reasonable compared to the decreased memory occupation.\nThis result motivates us to look at fine-tuning to increase the accuracy of the different LLaMA-2 variants on the PL domain. We want to test whether giving affordable, readily-available material"}, {"title": "On Fine-Tuning: Technique and Dataset.", "content": "To fine-tune the selected variants, we use the efficient technique of Lora [8]-for the quan-tised alternatives, we use the related qLora technique [3]. (q)Lora minimises the total number of parameters updated during fine-tuning thereby saving memory resources. Concretely, we customise lit-gpt [1], a framework that includes Lora as a fine-tuning tech-nique, to automate the execution of our tests\nThe dataset we use for fine-tuning draws its contents from the widely adopted undergraduate-level textbook on Programming Language by Gabbrielli and Martini [6], who provided us with private versions of the sources of each chapter, which we adapted for the lit-gpt framework to run fine-tuning.\nIn particular, we converted the content of the book into a JSON structure consisting of a sample for each paragraph of the book, made of three different fields as in the following sample:\ninstruction - Explain data types\ninput - \" \"\noutput - Data types are present in programming languages for at least three reasons [...]\nThe instruction field indicates the request from the user. We leave the input field empty (as suggested by lit-gpt's authors) and we fill the output with the full paragraph text of the book. To further reduce memory consumption, we split the output field, providing multiple samples for each paragraph of the book. We split the value of the output field every 1000 tokens and for each sample generated we changed the instruction field (e.g. from \"Explain data types\" to \"Explain data types - part 2\").\nOn Hardware Requirements. Running our experiments, we tracked the GPU memory usage of the different LLaMA-2 variants, both considering inference (which requires fewer resources) and fine-tuning (more resource-consuming). We use this data to frame the affordability of the different variants. We report the values of GPU memory occupancy obtained from our experiments in Table 2.\nThe values in Table 2 allow us to indicate the target models for this part of the study. Indeed, currently, mid-to-high customer market video cards sport ca. 24GB. This amount of memory can only fit the 7Bq and 13Bq versions for fine-tuning and, hence, we deem only these two variants affordable for educational institutes (e.g., schools and universities). Another approach is to split the models across multiple cards or to use a cloud-based service such"}, {"title": "3 Results and Discussion", "content": "We now present the results of our experiments. Our main goal is to investigate whether the fine-tuned alternatives have higher accuracy compared to the pre-trained variants. In Table 5, we show the percentage of fine-tuned models that perform better than the pre-trained version. From the table, the best-performing fine-tuned variants are those of the 13B quantised version. While interesting, this datum is quite raw, since we do not know how these \"better\" fine-tuned variants distribute. Therefore, in Fig. 3, we report the comparison over all 162 MCQs present in the dataset to plot the accuracy of these variants against their density. We illustrate the comparison by graphing the accuracy of the pre-trained variants as horizontal lines and depicting the distribution of accuracies for all 720 fine-tuned alternatives using violin plots, which are a hybrid"}, {"title": "3.1 Topic-wise Accuracy", "content": "We start by looking at three partitions of the MCQs, namely, the set of questions not covered by the book used for fine-tuning (PL), the dual set of questions covered by the book (Others), and the largest set of questions covered by the book included in Others (SD \"Structuring Data\"). We resp. report the accuracy performance of all the 720 fine-tuned alternatives in Figs. 4 to 6, using the same visualisation template of Fig. 3. First, we notice that the accuracy of the pre-trained versions is higher for Others (Fig. 5) than for PL (Fig. 4) MCQs. The results show that for the PL partition, the fine-tuned models can achieve higher accuracy than the pre-trained variants, both for 7B and 13B. The increase for Others is smaller since the pre-trained variants start from a higher accuracy with respect to PL. We can notice, e.g., that the pre-trained accuracy of the 13B version in PL and Others differs of ca. 15%. The explanation of the increase in accuracy for PL MCQs-whose content is not covered by the material used for fine-tuning-is that the fine-tuned alternatives \"learn\" to deal with the vocabulary of these MCQs since they share the domain of the content for fine-tuning (on Programming Languages).\nWith the SD partition (Fig. 6), we investigate a subset of the Oth-ers partition. The results show that, considering only SD questions, we have a higher accuracy for the fine-tuned alternatives-reaching ca. 70%. The comparison between Others and SD allows us to infer that some characteristics of the questions or the related content in the fine-tuning material lent itself better than the whole set of Others MCQs to fine-tuning. We start this analysis by looking into"}, {"title": "3.2 Hyperparameters of the SD partition", "content": "We now investigate which parameters influence the models' accu-racy, identifying which factors enable fine-tuned variants to achieve better accuracy than their pre-trained base versions.\n3.2.1 Fine-tuning dataset. The first hyperparameter we investigate is the influence of the fine-tuning dataset on accuracy. In general, we want to measure how much knowledge from the book the fine-tuned variants need to \"see\" to increase their accuracy.\nAs mentioned, the fine-tuning process we followed using three sets of chapters from the book supports this type of analysis. Recall-ing the partitioning, the first set of chapters includes only the one directly related to the SD MCQs partition, i.e., \u201cStructuring Data\", the second set includes \"Structuring Data\" plus the neighbouring chapters of \"Abstracting Data\" and \"Object-Oriented Paradigm\", and the third set includes all chapters of the book.\nWe report in Table 6 the percentage of fine-tuned variants that outperform their pre-trained version on the SD MCQs, divided according to the partitions of the book used for their fine-tuning. From the results, only one chapter of the book can generally lead to more stable results (i.e., the accuracy of all alternatives falls within a narrower range than using the other two sets) without losing higher accuracy results. This is particularly evident for the 13B variants, which have a higher number of fine-tuned alternatives that reach an accuracy that is higher than the one of pre-trained models."}, {"title": "3.2.2 Quantised fine-tuning.", "content": "Since we test variants fine-tuned with or without quantisation, we explore this dimension by reporting in Table 7 the results. In the table, the 7B fine-tuned quantised variants are more likely to improve the accuracy of the results. We can only make this observation for the 7B variants since our machine could"}, {"title": "3.2.3 Learning Rate.", "content": "Similarly, we explore the influence of learning rate on fine-tuning, divided between 0.001 and 0.0001. The results, in Table 8, show that for the 7B variants a higher learning rate favours quantisation while a lower learning rate favours the base variants. Moreover, the 7B variants show catastrophic forgetting and a learning rate inversely proportional to accuracy, while the 13B variants improve in direct proportion to the learning rate."}, {"title": "3.2.4 Batch Size & Epochs.", "content": "We investigate the influence on fine-tuning of batch sizes and numbers of epochs together. We plot our results in Fig. 7 dividing between the 7B and the 13B fine-tuned variants. In the plots, we show the relationship between batch size and accuracy depending on the number of epochs. What emerges from the plots is that we have a dual behaviour for the two versions of the models. The 7B variants achieve high accuracy with high batch sizes and a low number of epochs. Contrarily, the 13B variants achieve high accuracy with smaller batch sizes and a higher number of epochs. The latter result shows that the 7B variants, given the lower number of parameters, are less prone to increase in accuracy with a higher number of iterations, probably due to their reduced parameter space. This observation leads us to notice that the fine-tuning process is more beneficial for larger models that afford a larger parameter space to store new, domain-specific knowledge."}, {"title": "3.3 Correlations", "content": "The last part of our experiments regards studying the correlation between the hyperparameters and the accuracy of the MCQ dataset partitions, reported in Fig. 8. The results confirm the dual behaviour observed earlier: hyperparameters that positively correlate with"}, {"title": "4 Discussion and Conclusion", "content": "We investigated the affordability of using LLMs in education, focus-ing on LLaMA-2, MCQs, and the domain of PL. Our most gen-eral result is that one can obtain good accuracy in answering domain-specific MCQs with small fine-tuned models (7B) that run on consumer-grade machines, making it affordable for education purposes. These results are particularly interesting because the smaller fine-tuned models achieve an accuracy that is similar to the larger pre-trained ones. More in detail, as observed in Section 3.2, we characterised how fine-tuning hyperparameters influence accuracy, finding that the choice of the dataset (in our case, which chapters of the textbook) has the highest influence, along with quantisation performed during the fine-tuning. Our study has confirmed that some smaller fine-tuned LLMs are prone to catastrophic forgetting. To address this issue, we conducted an extensive analysis of the hyperparameters that affect accuracy during fine-tuning. These results help future research in avoiding repeating less adequate ap-proaches and focus on more efficient fine-tuning methods. Another interesting perspective regarding education relates to understand-ing how LLMs generate responses, offering valuable insights into their decision-making processes and potentially improving their accuracy and reliability as tutoring systems."}, {"title": "4.1 Limitations and Future Work", "content": "Scope and Experiments. In this study, we focussed on the sub-ject of Programming Languages, which limits the coverage of our results. To be able to extend our results to other areas (and gen-eralise them), we would need to perform the same study on both neighbouring subjects (e.g., from mathematics, physics, biology, and engineering) but also more distant ones (e.g., from literature, history, philosophy, and arts). Broadening our focus on tutoring systems, an interesting evolution of this work regards the usage of multimodal models-e.g., such that MCQs and fine-tuning datasets can integrate visual elements-to both cover more MCQs types and enhance the accuracy of LLMs using textbooks, which frequently use figures and schemas to integrate explanations.\nDatasets and Models. While our study sheds light on the afford-ability and effectiveness of using LLMs in educational contexts, there are notable limitations worth acknowledging. Firstly, regard-ing the fine-tuning dataset, our research primarily focused on spe-cific chapters of the textbook, which may limit the generalisability of our findings to broader educational content that falls outside the information covered in the book. The selection of these chapters can influence the performance of the fine-tuned models. Exploring a more diverse array of educational materials can provide a more comprehensive understanding of LLMs' efficacy across various sub-jects and topics. Additionally, our investigation into multiple-choice questions (MCQs) datasets is limited in scope. Further research can delve deeper into the nuances of MCQs from different disciplines, considering factors such as question complexity and diversity. Ad-dressing these limitations can enhance the applicability and robust-ness of our findings in real-world educational settings. Regarding the models used, the limitations concern the availability of the lat-ter as open-source resources for use and testing (e.g., GPT is not open-source).\nAffordability for Institutions. Looking at affordability for edu-cational institutions with limited resources, it is important to ac-knowledge that, while fine-tuning smaller models like the 7B and 13B variants is feasible on mid-range GPUs, larger models are cur-rently impractical due to their significant/costly hardware demands. However, resource-saving strategies, such as LoRA and parameter-efficient fine-tuning (PEFT) [13], can mitigate these challenges by reducing memory and computational requirements. Additionally, institutions can explore cloud-based GPU solutions, including AWS and Google Colab, which offer scalable access to high-performance hardware at competitive rates. These alternatives broaden access to fine-tuning capabilities, making advanced model training more affordable and inclusive for academic purposes."}]}