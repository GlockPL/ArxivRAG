{"title": "AnomalyControl: Learning Cross-modal Semantic Features for Controllable Anomaly Synthesis", "authors": ["Shidan He", "Lei Liu", "Shen Zhao"], "abstract": "Anomaly synthesis is a crucial approach to augment abnormal data for advancing anomaly inspection. Based on the knowledge from the large-scale pre-training, existing text-to-image anomaly synthesis methods predominantly focus on textual information or coarse-aligned visual features to guide the entire generation process. However, these methods often lack sufficient descriptors to capture the complicated characteristics of realistic anomalies (e.g., the fine-grained visual pattern of anomalies), limiting the realism and generalization of the generation process. To this end, we propose a novel anomaly synthesis framework called AnomalyControl to learn cross-modal semantic features as guidance signals, which could encode the generalized anomaly cues from text-image reference prompts and improve the realism of synthesized abnormal samples. Specifically, AnomalyControl adopts a flexible and non-matching prompt pair (i.e., a text-image reference prompt and a targeted text prompt) \u00b9, where a Cross-modal Semantic Modeling (CSM) module is designed to extract cross-modal semantic features from the textual and visual descriptors. Then, an Anomaly-Semantic Enhanced Attention (ASEA) mechanism is formulated to allow CSM to focus on the specific visual patterns of the anomaly, thus enhancing the realism and contextual relevance of the generated anomaly features. Treating cross-modal semantic features as the prior, a Semantic Guided Adapter (SGA) is designed to encode effective guidance signals for the adequate and controllable synthesis process. Extensive experiments indicate that AnomalyControl can achieve state-of-the-art results in anomaly synthesis compared with existing methods while exhibiting superior performance for downstream tasks.", "sections": [{"title": "1. Introduction", "content": "Anomaly inspection plays a crucial role in various fields, from industrial anomaly detection [17, 38] to medical imaging [14]. One of the most challenging issues is the scarcity of diverse abnormal data due to the resource-intensive procedure for acquiring large and varied datasets. Therefore, anomaly synthesis task [7, 8, 12, 13, 21, 22, 34, 35, 39] gradually emerged as an advancing technique to improve the data scale of the available abnormal sample.\nMany downstream tasks, such as anomaly detection and localization, could benefit from enriching the training dataset using synthetic anomaly samples. Recently, text-to-image synthesis techniques [26, 31] have received more research attention for their outperforming performance, which can generate visually realistic natural images according to the input text prompt. However, for the anomaly synthesis task, they still suffer from insufficient descriptors of the textual prompt, resulting in unsatisfactory realism for the generated anomalies. This limitation hampers the model's ability to recognize rare or previously unseen defect types.\nMotivated by the success of publicly available large text-to-image diffusion models [2, 20, 27, 29, 32], many approaches proposed to incorporate extra control signals to assist the diffusion process, which could enrich the targeted text prompt via injecting more prior information. For example, ControlNet [37], Uni-ControlNet [40], and IP-Adapter [33] proposed to apply adapters on stable diffusion models [27] to provide extra control for generation. The main idea is to learn stylistic information from a reference image (called image prompt). However, image prompts without a clear focal point generally offer weak alignment signals, limiting the effectiveness of generating high-fidelity and customized images. This limitation is especially pronounced in anomaly synthesis tasks, where anomalies typically occupy only a small region of the image and can easily be overlooked during generation. As a result, the synthesized anomalies often lack sufficient details to match real samples, failing to meet the high precision required for effective anomaly inspection and resulting in a lack of realism in the synthesized samples.\nTo this end, this work aims to enhance the controllability of anomaly synthesis, improving the realism of synthesized samples and supporting more flexible and generalizable prompts. The resulting framework is called AnomalyControl (as shown in Figure 2). Specifically, the controllability signals come from a non-matching pair, i.e., a text-image reference prompt and a targeted text prompt. The text-image reference prompt includes a textual anomaly descriptor and visual anomaly descriptor, which are used to offer semantic and contextual cues for guiding the synthesis process. Importantly, the anomaly described by the text-image reference prompt is aligned with that indicated in the targeted text prompt, while the background and other properties are left unrestricted, allowing for greater flexibility in anomaly placement. To improve realism, we introduce Cross-modal Semantic Modeling (CSM) to capture cross-modal semantic features via multimodal interactions, where the Anomaly-Semantic Enhanced Attention (ASEA) mechanism could extract fine-grained information related to the anomaly region. By taking cross-modal semantic features as the prior, the Semantic Guided Adapter (SGA) could insert effective semantic signals to support an adequate and controllable synthesis process."}, {"title": "2. Related Work", "content": "Anomaly synthesis has become an essential technique to support anomaly detection systems, especially in scenarios where real defect data is scarce. Existing methods can be broadly divided into two main categories. First, crop-and-paste anomaly augmentation methods [7, 34] combine normal images with abnormal patterns sourced from small abnormal samples or external textures. While these methods are computationally efficient and straightforward to implement, they often lack realism and generalization, limiting their ability to represent the complex characteristics of anomalies. Second, generative model-based methods [8, 12, 13, 21, 22, 35, 39], such as GANs and diffusion models, generate anomalies from scratch or by fine-tuning pre-trained models. Although these methods typically perform well for in-distribution anomalies, they require sufficient sample generalization and frequently struggle to produce out-of-distribution anomalies, which limits their ability to generalize to unseen defect types."}, {"title": "3. Methods", "content": "Our method is built upon Stable Diffusion [27], which efficiently performs the diffusion process in a low-dimensional latent space instead of pixel space by using an auto-encoder [15]. Specifically, given an input image xi \u2208 RH\u00d7W\u00d73, the encoder maps it to a latent representation zo = \u00a7(xi), where zo \u2208 Rh\u00d7w\u00d7c with H/h = W/w as the downsampling factor and c as the number of latent dimensions. The diffusion process utilizes a denoising UNet [28] to gradually refine a noisy latent zt, conditioned on the current timestep T and a textual prompt embedding C. Here, C represents the embedding of the targeted text prompt, generated by a pre-trained CLIP [25] text encoder. The training objective is defined as:\nL = Ezt,t,C,\u20ac~N(0,1) [||\u20ac \u2013 \u20aco (zt, t, C) ||2],\nwhere the goal is to predict and remove the noise e added to zt at each timestep."}, {"title": "3.2.1. Cross-modal Semantic Modeling", "content": "Leveraging the text-image reference prompt, the CSM module targets to encode sufficient cross-modal reference information as control signals for the diffusion process, i.e., consistent anomaly semantics from the text-image prompt.\nSpecifically, the text-image reference prompt provides a visual anomaly descriptor Ia and a textual anomaly descriptor Ta. Note that such text-image reference prompt only focuses on the contextual and descriptive information of the anomaly region, excluding the anomaly-unrelated information, such as background and materials. Such a pattern would allow greater flexibility in anomaly placement. In detail, Ia defines a specific anomaly pattern to provide a direct visual context reference, ensuring accurate visual control over the generated anomaly. Ta is created by combining an anomaly-specific keyword K with a template, providing semantic guidance to help the model generate samples that accurately reflect the designated anomaly type and attributes. The anomaly described in the Ta aligns with the anomaly region in the Ia, while the background and other elements of Ia remain unconstrained.\nTo capture effective cross-modal information from the text-image reference, a frozen VLM with cross-attention layers is utilized for its powerful multimodal integration ability. Thus, VLM-based CSM could integrate the reference image and text to obtain a unified semantic feature for the anomaly, called a cross-modal semantic feature, which serves as a prior for the following diffusion process. Natural needs arise from the anomaly-specific regions without losing fine-grained detail to generate realistic and contextually accurate anomalies, which motivates the introduction of the ASEA mechanism in the following section."}, {"title": "3.2.2. Anomaly-Semantic Enhanced Attention", "content": "ASEA forces the CSM's attention to focus on designated anomaly regions within the image reference, enabling precise extraction of anomaly-specific features for high-fidelity synthesis. The main pathway is to isolate the anomaly region in the attention map and optimize a trainable attention guidance variable, eg, which assists the frozen VLM in prioritizing relevant anomaly areas without retraining steps.\nThe textual anomaly descriptor Ta is structured as the template of \"This is an image with [anomaly]\", where the variable [anomaly] specifies the anomaly type. Lprefix represents the length of the fixed prefix \"This is an image with\". L denotes the total length of Ta. The anomaly descriptor [anomaly] begins at position Lprefix + 1 and extends to L.\nFirstly, an attention map A is generated by the VLM's cross-attention layer, focusing on the feature span Aanomaly associated with the [anomaly] part of the text prompt. Aanomaly captures the anomaly-specific attention values: Aanomaly = A[Lprefix + 1 : L, :]. By averaging these values across the anomaly section, we compute a mean attention map, Aanomaly, which helps stabilize the focus by reducing noise from individual tokens:\n\u0100anomaly =\n1\nL - Lprefix\nL\ni=Lprefix +1\nA[i, :],\nTo further refine the attention on anomaly regions, we apply an anomaly region mask MA to Aanomaly. This mask constrains attention to the spatial area of interest in the image, which is enhanced by introducing the trainable attention guidance variable eg. The optimization of eg is guided by an energy function [6], defined as:\nE(Aanomaly, MA) =\n(1\n2\n\u03a3\u00a1\u2208MA Aanomaly, i)\n\u03a3\u00a1 Aanomaly, i\nwhere i \u2208 MA refers to indices within the masked region. This function encourages concentration within MA, prioritizing relevant features in the anomaly area.\nFinally, eg is optimized iteratively over Tg steps using gradient descent to minimize the energy function:\neg \u2190 eg \u2013 a\u2207egE(\u0100anomaly, MA),\nwhere a is the learning rate. Through this iterative refinement, ASEA aligns the model's attention precisely with MA, effectively capturing anomaly-specific details. The challenge here lies in maintaining focused attention on the anomaly regions without disrupting the model's overall comprehension, a task ASEA addresses by carefully guiding attention with eg."}, {"title": "3.2.3. Semantic Guided Adapter", "content": "Motivated by the decoupled cross-attention [33], the SGA module introduces an adaptive mechanism to incorporate an additional targeted text prompt Tt as input, indicating the specific anomaly to be generated. Unlike previous approaches using solely image features as the prior [33, 40], which often miss critical details in subtle anomaly regions with low signal-to-noise ratios, SGA relies on a richer, semantically focused representation, i.e., cross-modal semantic features produced by CSM, which captures fine-grained anomaly details essential for accurate synthesis.\nTo facilitate classifier-free guidance [11], similar to text conditioning, SGA employs random dropout during training. This process involves occasionally setting cross-modal semantic features to zero, allowing the model to jointly learn both conditional and unconditional prompts. The enhanced cross-attention mechanism for integrating text and cross-modal semantic features is defined as:\nZnew = S(\nQ(K)T\n\u221ad\nV+S(\nQ(K')T\n\u221ad\nv',\nwhere y is a weight to balance these two terms. S is the function of Softmax. Q, K, and V are the query, key, and value matrices for the attention operation applied to text cross-attention, while K' and V' correspond to cross-modal attention. Given the query features Z and cross-modal semantic features C', the query matrix Q is defined as Q = ZWq, with K' = C'Wk and V\u2032 = C'Wv. Notably, only Wk and Wv are trainable parameters, focusing the adaptation on cross-modal information. During training, only the parameters of the SGA and eg are optimized, while the pre-trained diffusion model and VLM parameters remain frozen. The entire AnomalyControl pipeline is trained on image-text pairs of anomaly images, with a training objective based on the original Stable Diffusion work:\nL = Ezt,t,C,C',\u20ac~N(0,1) [||\u20ac \u2013 \u20aco (zt, t, C, C')||2].\nThis setup allows the SGA to integrate anomaly-specific semantic signals effectively, ensuring high-quality and controllable anomaly synthesis."}, {"title": "4. Experiments", "content": "We conducted our experiments on the widely used MVTec AD dataset [3, 4], which comprises 5,354 images across 15 categories for industrial anomaly detection tasks, including 10 object categories and 5 texture categories. Each category is represented by a set of defect-free training images and a test set containing images with various defects as well as defect-free images. Pixel-level annotations are provided for all existing anomalies in this dataset. We utilized the captions from MVTec AD Caption [12] for training and testing, which were carefully designed to ensure cross-modal complementarity. For data partitioning, we use the lowest one-third of ID numbers from the anomaly dataset as the training set, with the remaining two-thirds reserved for testing. For each anomaly type, 1,000 pairs of anomaly images and corresponding masks are generated using the mask generation method from AnoDiff [13]. These pairs are employed for downstream anomaly detection and localization tasks.\nFollowing previous methods [12, 13], to evaluate the quality and diversity of synthesized anomalies, we used the Inception Score (IS) [30], which measures overall quality and diversity in generated images, and the Intracluster Pairwise Learned Perceptual Image Patch Similarity (IC-LPIPS) [23] to evaluate perceptual similarity and the generalization of synthetic anomalies. For evaluating accuracy in downstream tasks such as anomaly detection and localization, we employed measurements through pixel-level and image-level Area Under the Receiver Operating Characteristic Curve (AUROC), Average Precision (AP), and F1-max scores. Consistent with DRAEM[34]'s procedure, we generated 1,000 images per anomaly category and trained a U-Net alongside normal samples.\nAnomalyControl is implemented with the HuggingFace Diffusers library [24] and built on the Stable Diffusion 1.5 (SD1.5) model. The VLM utilized in CSM is based on BLIP-2 [16], which provides robust semantic feature extraction for cross-modal inputs. We incorporate a new image cross-attention layer into each of SD1.5's 16 cross-attention layers, setting the cross-modal guidance strength parameter y = 1 for balanced guidance. The number of guidance steps Tg in ASEA is set to 3. We use the AdamW optimizer [18] with a fixed learning rate of 0.0001 and weight decay of 0.01. For classifier-free guidance, text or cross-modal semantic features are independently dropped with a probability of 0.05, and both are dropped simultaneously with a probability of 0.05. Training is conducted on a single NVIDIA 4090 GPU, requiring approximately three days for 30K iterations. During inference, a 30-step DDIM sampler is used, with a guidance scale of 7.5, to achieve a balance between fidelity and generalization in the generated anomalies.\nFor anomaly synthesis comparisons, we selected several recent high-performing methods as baselines, including DiffAug [39], CDC [22], SDGAN [21], DefGAN [35], DFMGAN [8], AnoDiff [13], and AnoXFusion [12]. To evaluate performance in downstream anomaly detection and localization tasks, we compared our approach with existing outperforming techniques such as DRAEM [34], PRN [36], DFMGAN [8], AnoDiff [13], and AnoXFusion [12]."}, {"title": "4.3. Comparison in Downstream Tasks", "content": "As shown in Table 3, our model consistently outperformed other anomaly synthesis methods across most evaluation metrics, demonstrating superior efficacy in both anomaly detection and localization. By enabling fine-grained controls over the appearance of anomalies, our approach can ensure that the generated samples closely resemble real-world defects. This capability is crucial in providing diverse and realistic training data, allowing downstream models to generalize better to real-world anomalies and improve both detection sensitivity and localization accuracy."}, {"title": "4.4. Ablation Study", "content": "From the experimental results in Table 4, it is observed that the stepwise introduction of each module significantly improves model performance. First, with only the SGA module, IS and IL increase to 1.66 and 0.23, respectively, while AUC-P and AUC-I reach 92.7 and 95.4. At this stage, the model only uses the visual anomaly descriptor, functioning as an adapter based solely on the image prompt. The addition of the SGA module shows that SGA can strengthen the focus on the anomaly regions in the image. When the CSM module is further introduced, IS rises from 1.66 to 1.69, IL from 0.23 to 0.25, and AUC-P and AUC-I reach 93.1 and 95.8. This improvement indicates that the CSM module incorporates the textual anomaly descriptor, integrating it with the visual anomaly descriptor for cross-modal semantic fusion, thereby improving the quality and coherence of anomaly generation. As described in the methods section, the CSM module captures cross-modal semantic features of the anomaly region through multimodal interactions, resulting in anomaly features that align better with the intended descriptions. Finally, for the ASEA module, different guidance steps T, demonstrate its impact on model performance. When Tg = 3, all metrics reach optimal values, providing the best balance between computational efficiency and performance. Increasing Tg to 4 or 5 shows marginal improvements, indicating that performance gains plateau while computational costs continue to rise. As shown in Figure 4, ASEA progressively refines VLM's focus on specified anomaly areas, leading to generated images with higher anomaly realism. This indicates ASEA's effectiveness in concentrating attention, resulting in visually realistic and detailed anomalies. By enhancing focus on anomaly-specific regions, ASEA effectively helps to generate high-quality, detailed anomaly images."}, {"title": "5. Conclusion", "content": "In this paper, we propose an AnomalyControl framework for controllable anomaly synthesis, which enables the flexible generation of realistic and generalized anomaly samples. AnomalyControl introduces two three modules, i.e., Cross-modal Semantic Modeling (CSM), Anomaly-Semantic Enhanced Attention (ASEA), and Semantic Guided Adapter (SGA). To enhance the realism of the synthesized anomalies, CSM aims to extract detailed cross-modal semantic features based on the textual and visual anomaly descriptors, where ASEA could capture fine-grained visual details through multimodal interactions. Then, prompted by precise and controllable instructions, SGA could leverage these enriched semantic signals to guide the diffusion process for high-quality anomaly synthesis. Extensive experiments indicate that AnomalyControl can achieve state-of-the-art results in anomaly synthesis by generating realistic and generalized anomalies based on different control signals and outperforming existing methods for downstream tasks."}]}