{"title": "Derivation of Closed Form of Expected Improvement for\nGaussian Process Trained on Log-Transformed Objective", "authors": ["Shuhei Watanabe"], "abstract": "Expected Improvement (EI) is arguably the most widely used acquisition function in\nBayesian optimization. However, it is often challenging to enhance the performance with\nEl due to its sensitivity to numerical precision. Previously, Hutter et al. (2009) tackled this\nproblem by using Gaussian process trained on the log-transformed objective function and\nit was reported that this trick improves the predictive accuracy of GP, leading to substan-\ntially better performance. Although Hutter et al. (2009) offered the closed form of their EI,\nits intermediate derivation has not been provided so far. In this paper, we give a friendly\nderivation of their proposition.", "sections": [{"title": "Preliminaries", "content": "In this paper, we consistently consider the maximization problem and use the following notations:\n\u2022 x \u2208 X, an input vector defined on the search space X \u2286 R\u1d30,\n\u2022 y \u2208 R, an objective value, which is better when it is larger,\n\u2022 D := {(x\u2099, y\u2099)}\u2099=\u2081\u1d3a, a set of observations where y\u2099 is the objective value given an input vector x\u2099,\n\u2022 [N] := {1, . . ., N}, a set of integers from 1 to N,\n\u2022 \u03a6: R\u2192 [0, 1], the standard normal distribution function,\n\u2022 \u03d5 : R \u2192 R+, the probability density function of \u03a6, i.e. \\frac{d\u03a6}{du} = \u03d5(u) = \\frac{1}{\\sqrt{2\u03c0}} exp(-\\frac{u\u00b2}{2}),\n\u2022 \u03bc(x), the mean at x predicted by Gaussian process (GP) trained on D, and\n\u2022 \u03c3(x), the standard deviation at x predicted by GP trained on D.\nTraining GP means that we can predict the following:\ny ~ \u039d (\u03bc(x), \u03c3(x)\u00b2),\nand we use the probability density function of this distribution as the posterior p(y|x, D). Note\nthat since this paper focuses only on the derivation of the closed form provided by Hutter et al.\n(2009, 2011), we defer the details of Bayesian optimization to other works such as Brochu et al.\n(2010); Shahriari et al. (2016); Garnett (2022)."}, {"title": "Derivation of Closed Form Solutions", "content": "In this section, we show how to derive the closed-form solution provided by Hutter et al. (2009).\nTo do so, we first begin with deriving the closed-form solution of expected improvement\n(EI) (Jones et al., 1998)."}, {"title": "Expected Improvement", "content": "First of all, EI is the following acquisition function:\n\u03b1_{EI} (x|D) = \\int_{-\u221e}^{y*} (y* - y)p(y|x, D)dy\nwhere y* \u2208 R is usually max\u2099\u2208[N] y\u2099. With GP trained on D, we can transform EI as follows:\n\u03b1_{EI} (x|D) = \\int_{-\u221e}^{y*} (y* - y)p(y|x, D)dy\n= \\int_{-\u221e}^{y*} (y* - y) \\frac{1}{\\sqrt{2\u03c0\u03c3\u00b2}} exp(-\\frac{(y - \u03bc)\u00b2}{2\u03c3\u00b2}) dy (Def. \u03bc := \u03bc(x), \u03c3 := \u03c3(x))\n= \\int_{-\u221e}^{y*} (y* - \u03bc - u\u03c3) \\frac{1}{\\sqrt{2\u03c0\u03c3\u00b2}} exp(-\\frac{u\u00b2}{2}) (\u03c3 du) (Def. u := \\frac{y - \u03bc}{\u03c3}, dy = \u03c3 du, z := \\frac{y* - \u03bc}{\u03c3})\n= \\int_{-\u221e}^{z} (y* - \u03bc - u\u03c3)\u03d5(u)du\nSince \u03a6(z) = \\int_{-\u221e}^{z} \u03d5(u)du by definition, and y* and \u03bc does not depend on u, the first and second\nterms can be computed as \\int_{-\u221e}^{z} (y* \u2013 \u03bc)\u03d5(u)du = (y* \u2013 \u03bc)\u03a6(z) = z\u03c3\u03a6(z). The last term can be\ncomputed as follows:\n\\int_{-\u221e}^{z} u\u03d5(u)du = \\int_{-\u221e}^{z} u \\frac{1}{\\sqrt{2\u03c0}} exp(-\\frac{u\u00b2}{2}) du\n= \\int_{-\u221e}^{z} \\frac{1}{\\sqrt{2\u03c0}} exp(-\\frac{u\u00b2}{2}) du (Def. v := -\\frac{u\u00b2}{2}, \\frac{dv}{du} = -u)\n= -\\frac{1}{\\sqrt{2\u03c0}} [exp(v)]_{-\u221e}^{z} = - \\frac{1}{\\sqrt{2\u03c0}} exp(-\\frac{z\u00b2}{2}) = -\u03d5(z).\nTherefore, the closed-form solution of EI is:\n\u03b1_{EI}(x|D) = z\u03c3\u03a6(z) \u2013 \u03c3(\u2212\u03d5(z)) = z\u03c3\u03a6(z) + \u03c3\u03d5(z)."}, {"title": "Expected Improvement Proposed by Hutter et al. (2009)", "content": "We now consider EI for the log-transformed objective, i.e. Eq. (5) by Hutter et al. (2009), where\nl = log y is the log-transformed objective and GP is trained using l\u2099 := log y\u2099 instead of y\u2099, meaning\nthat l ~ N (\u03bc(x), \u03c3(x)\u00b2):\n\u03b1_{log EI} (x|D) = \\int_{-\u221e}^{log y*} (y* \u2013 exp l)p(l|x, D)dl.\nBy defining z = \\frac{log y* - \u03bc}{\u03c3} and u = \\frac{l - \u03bc}{\u03c3}, we obtain the following in a similar way as Eq. (3):\n\u03b1_{log EI} (x|D) = \\int_{-\u221e}^{z} (y* \u2013 exp(\u03bc + u\u03c3))\u03d5(u)du = y*\u03a6(z) - exp \u03bc \\int_{-\u221e}^{z} exp(u\u03c3)\u03d5(u)du."}, {"title": null, "content": "The second term can be transformed by completing the square as follows:\n\\int_{-\u221e}^{z} exp(u\u03c3)\u03d5(u) du = \\int_{-\u221e}^{z} \\frac{1}{\\sqrt{2\u03c0}} exp(-\\frac{u\u00b2}{2} + u\u03c3) du\n= \\int_{-\u221e}^{z} \\frac{1}{\\sqrt{2\u03c0}} exp(-\\frac{(u - \u03c3)\u00b2}{2} + \\frac{\u03c3\u00b2}{2}) du\n= exp(\\frac{\u03c3\u00b2}{2}) \\int_{-\u221e}^{z} \\frac{1}{\\sqrt{2\u03c0}} exp(-\\frac{v\u00b2}{2}) dv (Def. v := u - \u03c3, \\frac{dv}{du} = 1)\n= exp(\\frac{\u03c3\u00b2}{2}) \u03a6(z \u2013 \u03c3).\nCombining Eq. (7) and Eq. (8), we yield:\n\u03b1_{log EI} (x|D) = y*\u03a6(z) \u2013 exp \u03bc \\frac{\u03c3\u00b2}{2} \u03a6(z \u2013 \u03c3) = y*\u03a6(z) \u2013 exp (\u03bc + \\frac{\u03c3\u00b2}{2}) \u03a6(z \u2013 \u03c3)\nwhere z = \\frac{log y* - \u03bc}{\u03c3}. The final result is identical to the closed-form solution, i.e. Eq. (6) provided by\nHutter et al. (2009)."}, {"title": "Related Work", "content": "The acquisition function discussed in this paper is primarily used by SMAC3 (Lindauer et al., 2022).\nAs mentioned by Hutter et al. (2009), the logarithmic transformation increases the predictive accu-\nracy of GP, leading to better performance. As this acquisition function involves logarithmic trans-\nformation, this function is conventionally called logEI. However, we need to emphasize that this\nacquisition function is very different from the acquisition function discussed by Ament et al. (2024)\nwhere the authors literally take the logarithm of EI, i.e. log \u03b1_{EI}(x|D). For example, logEI defined in\nBoTorch (Balandat et al., 2020) and GPSampler of Optuna (Akiba et al., 2019) is log \u03b1_{EI} (x|D). Note\nthat TPESampler, the main Bayesian optimization algorithm used in Optuna, also uses log \u03b1_{EI} (x|D)\nfor the numerical stability. We defer the details of the TPE algorithm to Bergstra et al. (2011);\nWatanabe (2023)."}, {"title": "Conclusion", "content": "In this paper, we showed the detailed derivation of the closed-form solution of EI proposed by\nHutter et al. (2009). We hope that our derivation helps practitioners verify their implementations\nand works as a future reference if anyone would like to further enhance this acquisition function."}]}