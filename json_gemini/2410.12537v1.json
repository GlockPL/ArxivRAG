{"title": "IS COMPLEX QUERY ANSWERING REALLY COMPLEX?", "authors": ["Cosimo Gregucci", "Bo Xiong", "Daniel Hern\u00e1ndez", "Lorenzo Loconte", "Pasquale Minervini", "Steffen Staab", "Antonio Vergari"], "abstract": "Complex query answering (CQA) on knowledge graphs (KGs) is gaining mo- mentum as a challenging reasoning task. In this paper, we show that the current benchmarks for CQA are not really complex, and the way they are built distorts our perception of progress in this field. For example, we find that in these bench- marks most queries (up to 98% for some query types) can be reduced to simpler problems, e.g. link prediction, where only one link needs to be predicted. The performance of state-of-the-art CQA models drops significantly when such mod- els are evaluated on queries that cannot be reduced to easier types. Thus, we propose a set of more challenging benchmarks, composed of queries that require models to reason over multiple hops and better reflect the construction of real- world KGs. In a systematic empirical investigation, the new benchmarks show that current methods leave much to be desired from current CQA methods.", "sections": [{"title": "INTRODUCTION", "content": "A crucial challenge in AI and ML is learning to perform complex reasoning, i.e., solving tasks that involve a number of intermediate steps and sub-goals to be completed. Complex query answer- ing (CQA; Hamilton et al., 2018; Zhang et al., 2021; Arakelyan et al., 2021; Zhu et al., 2022) has emerged as one of the most prominent ways to measure complex reasoning over external knowledge bases, encoded as knowledge graphs (KGs; Hogan et al., 2021). For instance, to answer the query:\nover a KG such as FreeBase (Bollacker et al., 2008), one would need to first intersect the set of movies found on Blue Ray and the ones shot in New York City, and then link these intermediate candidate answers to another entity, i.e., an actor participating in it. However, the answers computed in this way may not include entities that are unreachable if missing links are present.\nTo deal with the unavoidable incompleteness of real-world KGs, ML methods were developed to solve CQA in the presence of missing links. Neural query answering models, constituting the current state-of-the-art (SoTA) for CQA (Arakelyan et al., 2021; Zhu et al., 2022; Arakelyan et al., 2023; Ren et al., 2023; Galkin et al., 2024b), map queries and KGs (i.e., entities and relation names) into a unified latent space that supports reasoning. Performance measurements on de-facto-standard benchmarks such as FB15k237 (Toutanova & Chen, 2015) and NELL995 (Xiong et al., 2017) suggest that in recent years such models achieved impressive progress on CQA on queries having different structures, and hence posing apparently different levels of difficulty to be answered.\nThe difficulty of a benchmark relates to the size and structural complexity\u00b9 of its queries, and several query \"types\" have been proposed (Ren et al., 2020), each involving a different combination of logical operators-conjunctions, disjunctions, and requiring to traverse a number of missing links"}, {"title": "KGS AND COMPLEX QUERY ANSWERING", "content": "Knowledge graphs. A KG is a graph-structured knowledge base where knowledge about the world is encoded as relationships between entities. More formally, a KG can be represented as a multi-relational graph G = (E, R, T), where & is a set of entities, R is a set of relation names, and T\u2286 E\u00d7R\u00d7 E is a set of links or triples, where each triple (s, p, o) \u2208 T represents a relation- ship of type p \u2208 R between the subject s \u2208 E and the object o \u2208 E of the triple. For instance, in a KG such as FreeBase (Bollacker et al., 2008), the fact that a movie such as \"Spiderman 2\" is distributed in Blue Ray can be stored into the triple (Spiderman2, distributedVia, BlueRay) or equivalently (BlueRay, distributedVia\u00af\u00b9, Spiderman2) where distributedVia\u00af\u00b9 denotes the inverse relation of distributed Via. Fig. 1 shows examples of fragments from the KG FB15k237.\nComplex Query Answering. The aim of CQA is to retrieve a set of possible answers to a logical query q that poses conditions over entities and relation types in a KG. Following the literature on CQA (Arakelyan et al., 2021), we consider the problem of answering logical queries with a single target variable (T), a set of constants including anchor entities (a1, a2,..., ak \u2208 E), given relation names (r1,2,...,rn \u2208 R), and first-order logical operations that include conjunction A, disjunc- tion V and existential quantification \u2203. In this work, we do not deal with queries involving negation; however, all of our considerations about the inflated performance of current benchmarks will also transfer to versions of these benchmarks that include them (Ren et al., 2023; Galkin et al., 2020).\nDifferent queries are categorized into different types based on the structure of their corresponding logical sentence (Xiong et al., 2017). The idea behind this taxonomy is that queries of the same type share the same \u201chardness\", i.e., the level of difficulty to be answered and different query types correspond to tasks that map to more or less complex reasoning tasks. The simplest CQA task is link prediction (Bordes et al., 2013), i.e., answering a query of the form:\n?T: (a1, r1,T),\nthat is, given an entity a\u2081 (e.g., NYC) and a relation name r\u2081 (e.g., locatedIn\u00af\u00b9), find the entity that when substituted to T correctly matches the link in the KG (e.g., Spiderman2, see Fig. 1). Instead of matching a single link, more complex queries involve matching sub-graphs in a KG (see Fig. 2). Xiong et al. (2017) extend 1p queries, and ask questions that involve traversing sequential paths made of two or three links, i.e.,\n?T: V\u2081.(a1, 1, V\u2081) ^ (V1, r2, T),\n?T: V1, V2.(a1, r1, V\u2081) ^ (V1, r2, V2) \u2227 (V2, 13, T),\nwhere V1, V2 denote variables that need to be grounded into entities associated to nodes in the path. Moreover, multiple ground entities can directly participate in a conjunction, e.g., in queries such as:\n?T: (a1, r1, T) \u2227 (a2, r2, T),\n?T: (a1, r1,T) \u2227 (a2, r2, T) \u2227 (\u0430\u0437, r3, T),\nwhich represent the intersection of the target entity sets defined over two (2i) or three (31) links. Path and intersection structures can be combined into more complex queries: for example, the natural language expression for query q1 can be formalized as the formula\n?T: V1.(a1, 1, V\u2081) \u2227 (a2, r2, V\u2081) ^ (V1, r3, T),"}, {"title": "WHAT IS THE REAL \u201cHARDNESS\u201d OF CQA IN INCOMPLETE KGS?", "content": "involving one intersection followed by a one-length path. See also the example in Fig. 1. By inverting the order of operations, we obtain the query type \"1p2i\":\n?T: \u2203V1.(a1, r1, V\u2081) ^ (V1, r2, T) \u2227 (a2, r3, T).\nSimilarly to the introduction of conjunctions, we can consider disjunctions in queries, realizing the union query types which can be answered by matching one link or the other, such as\n?T: (a1, r1, T) V (a2, r2,T),\nor by combining the union with the previous query types, e.g., obtained by combining 2u and 1p:\n?T: V\u2081.((a1, 1, V\u2081) V (a2, r2, V\u2081)) ^ (V1, r2, T).\nNote that despite their dissimilar syntaxes and the different sub-graphs defining possible solutions (Fig. 2), the query type 2u should be as difficult as 1p, as to answer the first it suffices to match a single link correctly. Similarly, 2u1p is as complex as 2p. The fact that 2u and 2u1p are reported to be harder to solve in practice than 1p and 2p (Ren et al., 2020) is due to the way standard benchmarks are built, which we discuss in Sec. 4, and the way in which CQA is evaluated, discussed next.\nStandard evaluation. Given a KG G and a logical query q from one of the types described above, answering q boils down to a graph matching problem (Hogan, 2020) if we assume that all the mean- ingful links are already in G. Instead, if G is incomplete, we will need to predict missing links while answering q. Many ML approaches to CQA, reviewed in Sec. 5, therefore assume a distribution over possible links (Loconte et al., 2023), requiring probabilistic reasoning. To evaluate them, standard benchmarks such as FB15k237 and NELL995 artificially divide G into Gtrain and Gtest, treating the triples in the latter as missing links. This splitting process is done uniformly at random, a procedural choice that can alter the measured performance, as we discuss next. CQA is generally treated as a ranking task, counting how many true candidate answers to a query are ranked higher than non- answer ones. Denoting the rank of a query answer pair (q,t) by rank(q, t), the performance for each query type is calculated as the mean reciprocal rank (MRR), i.e.,\n$\\2-1 \\sum_{q \\in Q, t \\in E_q} |Eq|\u00af\u00b9 rank(q, t)$,\nwhere Q denotes the set of test queries of the considered type, and Eq is the set of candidate answer entities for each query q \u2208 Q. This average, across queries and answers, assumes that every query answer pair having the same query type is equivalently hard, which is not the case. In fact, we show that certain query answer pairs can be easier to retrieve if links from the training data leak into the model (Fig. 1), and that the distribution of the query answer pairs in the existing benchmarks is very skewed towards those involving a single missing link (Table 1). Thus, computing Eq. (1) without understanding what is the benchmark distribution, distorts the perception of performance gains.\nAs discussed in the previous section, the perceived complexity of a query is related to the graph structure associated to its query type (Fig. 2): queries containing more hops/existentially-quantified variables are more challenging, e.g., a 3p query is harder than a 2p query. In this section, we give an alternative perspective on the difficulty of answering queries that take into account the information coming from the training data, and that might have leaked into a learned model. We argue that predicting links that are truly missing, i.e., not accessible to a learned model, is actually what makes a query \"hard\". To do so, we formalize the notion of a reasoning tree for a query answer pair, and then define how we determine the practical hardness of a query answer pair.\nGiven a query q and every answer set of candidate answers t \u2208 Eq in its candidate set, we define the reasoning tree of each query answer pair (q, t), as the directed acyclic graph starting from the anchor entities of q to the target entity t, whose relational structure matches the query graph. Fig. 1 provides examples of different reasoning trees for four different answers to the same query. There, we highlight whether a link belongs to Gtrain or not, i.e., it is a missing link. We assess the hardness"}, {"title": "HOW MANY \u201cCOMPLEX\u201d QUERIES IN CURRENT CQA BENCHMARKS?", "content": "In this section, we systematically analyze the practical hardness of queries from very popular CQA benchmarks and answer the following research question: (RQ1) What is the proportion of query answer pairs that can be classified as full-inference rather than the easier partial-inference?\nFor this purpose, we consider the CQA benchmarks generated from the KG FB15k237 (based on Freebase) and NELL995 (based on NELL systems (Carlson et al., 2010)) as they are the most used to evaluate SoTA methods for CQA (Ren et al., 2020; Ren & Leskovec, 2020; Arakelyan et al., 2021; Zhang et al., 2021; Zhu et al., 2022; Arakelyan et al., 2023).\nComplex queries can be reduced to much simpler types. We group the testing query answer pairs (q, t) into query types (Sec. 2), and we further split them based on whether they can be re- duced to simpler types after observing the training links in Gtrain (Sec. 3). Table 1 shows that for both FB15k237 and NELL995 the vast majority of (q, t) pairs can be reduced to simpler types. For FB15k237, 86.8% to 98.3% of (q, t) pairs can be reduced to 1p queries, while only 0.1% to 4% require full inference. Similarly, for NELL995, 49.5% to 98.5% of (q, t) pairs map to 1p queries, and only 0.1% to 6% to full inference. For instance, 96.7% of 2i1p (q, t) pairs in FB15k237 can be reduced to 1p queries, 1.8% to 2p, and 1.4% to 2i. However, only 0.1% of these pairs cannot be reduced to any other (q', t) pair, i.e., they require full inference in order to be predicted. The only exceptions to this trend are (q, t) pairs where q has a 1p or 2u structure which, by definition, only require the prediction of a single link and therefore cannot be reduced by any other query type.\nNon-existing links for union queries. Additionally, we discovered that there are non-existing links, i.e., links that are not in the original KG G and hence neither in Gtrain or Gtest, in both FB15k237 and NELL995 in the reasoning trees of queries involving unions, i.e., the 2u and 2u1p types. Fig. A.1 shows some examples. These links violate our definitions of inference (q, t) pairs, hence, we filter them out, and report only filtered (q, t) pairs in Table 1. More crucially, these non-existing links can alter the performance of solvers for 2u and 2u1p types, as we discuss in the next section."}, {"title": "HOW DO SOTA MODELS PERFORM ON FULL-INFERENCE QUERIES?", "content": "In this section, we re-evaluate a number of SoTA methods for CQA and answer to the following question: (RQ2) How do these approaches perform on partial-inference and full-inference query answer pairs? Furthermore, we leverage our query hardness distinction (Sec. 3) to devise a model class that explicitly leverages the links stored in the training KG to solve CQA tasks.\nSOTA CQA methods.\nOver the years, a significant number of neural models have been proposed for solving the CQA task. Guu et al. (2015) propose compositional training for embedding methods to predict answers for path queries. GQE (Hamilton et al., 2018) learns a geometric intersection operator to answer conjunctive queries in embedding space; this approach was later extended by Query2Box (Ren et al., 2020), Be- taE (Ren & Leskovec, 2020), and GNN-QE (Zhu et al., 2022). FuzzQE (Chen et al., 2022) improves embedding methods with t-norm fuzzy logic, which satisfies the axiomatic system of classical logic. Some recent works such as HypE (Choudhary et al., 2021) and ConE (Zhang et al., 2021) use geo- metric interpretations of entity and relation embeddings to achieve desired properties for the logical operators. Other solutions to CQA combine neural methods with symbolic algorithms. For example, EmQL (Sun et al., 2020) ensembles an embedding model and a count-min sketch, and is able to find logically entailed answers, while CQD (Arakelyan et al., 2021; 2023) extends a pretrained knowl- edge graph embedding model to infer answers for complex queries. In this work, we consider four representative approaches that significantly differ in their methodological designs and yield SoTA results compared to other models in their class. We consider the following models: (1) Cone Embed- dings (ConE; Zhang et al., 2021) is a geometry-based complex query answering model where logical conjunctions and disjunctions are represented as intersections and union of cones. (2) Graph Neu- ral Network Query Executor (GNN-QE; Zhu et al., 2022) decomposes a complex first-order logic query into projections over fuzzy sets. (3) Continuous Query Decomposition (CQD; Arakelyan et al., 2021; 2023), reduces the CQA task to the problem of finding the most likely variable assign- ment, where the likelihood of each link (1p query) is assessed by a neural link predictor, and logical connectives are relaxed via fuzzy logic operators. (4) UltraQuery (ULTRAQ; Galkin et al., 2024b) is a foundation model for CQA inspired by Galkin et al. (2024a) where links and logical operations are represented by vocabulary-independent functions which can generalize to new entities and relation types in any KG."}, {"title": "BUILDING AN HYBRID REASONER TO EXPLOIT TRAINING LINKS", "content": "So far, we have assessed that current benchmarks are skewed towards easier query types (Table 1) and thus the perceived progress of current SoTA CQA methods boils down to their performance on 1p queries (Table 2). To have an undistorted view of this progress, in the next section, we will devise a benchmark that allows to precisely measure model performance on full-inference queries only. Nevertheless, we argue that in a real-world scenario one has to perform reasoning over both existing links and in the presence of missing ones. Therefore, it is worth to evaluate CQA over partial-inference queries, however in a stratified analysis and accounting for the disproportion of 1p links (see Takeaway 1). Next, we discuss how to build an hybrid solver that exploits both link types.\nCurrent SoTA CQA methods might implicitly exploit this aspect at test time if they are able to mem- orize well the entire training KG. However, there is no guarantee that this is the case, especially for less parameterized models. If one is interested in hybrid inference on real-world KGs, discarding the information in Gtrain is wasteful. To this end, we build an hybrid CQA method that explicitly re- trieves existing links from Gtrain when answering a query at test time. To the best of our knowledge, this is the first time such an hybrid solver is proposed in the CQA literature.\nOur hybrid solver, named CQD-Hybrid, is based on the variant of CQD that greedily answers a query by incremental search (Arakelyan et al., 2021). As in CQD, a pre-trained link predictor is used to compute scores for the answer entities of 1p queries only, denoting the unnormalized probability of that single link to exist (Loconte et al., 2023). Then, assignments to existentially quantified variables in a complex query are greedily obtained by maximizing the combined score of the links, computed by replacing logical operators in the query with fuzzy ones (van Krieken et al., 2022). A complete assignments to the variables (and hence to the target variable T), gives us an answer to the query. In our CQD-Hybrid, we assign the maximum score to those links that are observed in Gtrain. That is, training links will have a higher score than the one that the link predictor would output, hence effectively steering the mentioned greedy procedure at test time. We report hyperparameters and implementation details of CQD-Hybrid in App. \u0421.1.\nThis simple heuristics allows training links to be considered at test time. By doing so we might add additional noise to the greedy search process in CQD-Hybrid. In our experiments, however, we found CQD-Hybrid was consistently boosting performance in terms of MRR, as reported in Table 4, which shows aggregated performance for space reasons. A stratified comparison is reported in the appendix in Tables A.2 and A.3."}, {"title": "NEW BENCHMARKS FOR \u201cTRULY COMPLEX\u201d QUERY ANSWERING", "content": "In this section, we answer to the following question: (RQ3) How can we construct a set of CQA benchmarks that let us measure truly challenging queries? To do so, we present a new set of CQA benchmarks based on the previously used FB15k237 and NELL995, as well as a novel one we design out of temporal knowledge graph ICEWS18 to question the current way to split the original KG G into Gtrain and Gtest. Then, we evaluate current SoTA methods on these new benchmarks as to establish a strong baseline for future works.\nBuilding new CQA benchmarks. We generate our benchmarks to comprise only full-inference question answer pairs, filtering out all partial-inference (and trivial) ones from the evaluation. To this end, we modify the algorithm of Ren et al. (2020) to ensure that no training links are present in the reasoning trees of any (q, t) pair we generate. To raise the bar of \u201ccomplexity\u201d in CQA, we introduce two query types that, in their full-inference versions, are harder than simpler types by design. Specifically, we design \"4p\u201d queries, for a sequential path made of four links, i.e.,\n?T: V1, V2, V3.(a1, r1, V\u2081) ^ (V1, r2, V2) \u2227 (V2, 3, V3) \u2227 (V3, r4, T),\nand \"4i\u201d, which represents the intersection of the target entity sets defined over four links, i.e.\n?T: (a1, r1,T) \u2227 (a2, r2, T) \u2227 (\u0430\u0437, r3, T) \u2227 (a4, r4,T).\nWe name the harder versions of FB15k237 and NELL995, FB15k237+H and NELL995+H, each com- prising 50.000 full-inference test and validation (q, t) pairs for all query types apart from 1p queries, for which we keep the same set of the old benchmark (this being already full-inference), and for the new types 4p and 4i, for which we have 10.000 of them. More details in App. \u0412.\nNon-uniform at random splits. For FB15k237+H and NELL995+H, we keep the existing data splits of Gtrain and Gtest. However, to evaluate the impact of this artificial splitting process, we adopt a more realistic one for ICEWS18+H, where Gtrain contains present links and we might want to predict the future links contained in Gtest. To this end, we leverage the temporal information in ICEWS18 by (1) ordering the links based on their timestamp; (2) removing the temporal information, thus obtaining normal triples; and (3) selecting the train set to be the first temporally-ordered 80% of triples, the valid the next 10%, and the remaining to be the test split. If the same fact appears with multiple temporal information, we retain only the link with the earliest timestamp. For detailed statistics about the splits please refer to App. D.\nHow do SoTA methods fare on our new benchmarks? For FB15k237+H and NELL995+H we re-used the pre-trained models used for the experiments in Sec. 4, while for the newly created ICEWS18+H we trained GNN-QE, CQD and ConE from scratch. As this is not necessary for UL- TRAQ, being a zero-shot neural link prediction applicable to any KG, we re-used the checkpoint provided by the authors. Hyperparameters of all models are in App. C.2."}, {"title": "CONCLUSION", "content": "In this paper, we revisit CQA on KGs and reveal that the \u201cgood\u201d performance of SoTA approaches predominantly comes from answers that can be reduced to easier types (Table 2), the vast major- ity of which boiling down to single link prediction (Table 1). We also propose an hybrid solver, CQD-Hybrid, that by combining classical graph matching approaches with neural query answer- ing explicitly reduces query answer pairs to easier query types when existing links are available and therefore can surpass SoTA on old benchmarks. We then created a set of new benchmarks FB15k237+H,NELL995+H and ICEWS18+H that only consider full-inference queries and are much more challenging for current SoTA approaches. We consider them to be a stepping stone towards benchmarking truly complex reasoning with ML models.\nHowever, both old and our new benchmarks only consider queries with bindings of single target variables. While this reflects an important class of queries on real-world KGs, many real-world queries require bindings to multiple target variables (i.e. answer tuples). We plan to extend our current study of the \"real hardness\" of CQA benchmarks to queries involving negation (Zhang et al., 2021), and to other popular settings in neural query answering, such as inductive scenarios (Galkin et al., 2022) where some entities and/or relations are unseen during the test stage."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We report the hyperparameter settings of all compared models in App. C. Our code and new bench- marks will be made available upon acceptance."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "AV, CG, and LL conceived the initial idea for which existing benchmarks were not hard enough for complex query answering. AV, CG, DH, and LL had the intuition that a simple hybrid solver could obtain SoTA results on the old benchmarks. BX, and CG found the issue with union queries. CG developed the necessary code, ran all the experiments, and plotted the figures, with the following exceptions: BX ran all the experiments involving ConE, LL contributed to the generation of the new benchmarks, specifically for the 4p and 4i queries, PM wrote the script for generating the KG splits of ICEWS18, AV drew Fig.1 and 2. BX, and CG wrote the initial draft of the paper, while AV shaped the story line of the paper and helped with the writing. All authors critically revised the paper. AV, PM, and SS supervised all the phases of the project and gave feedback."}, {"title": "HYPERPARAMETERS", "content": "In this section we detail the hyperparameters used for each dataset and model.\nOLD BENCHMARKS\nGNN-QE We did not tune hyperparameters, but re-used the ones provided in the official repo.\nULTRA We did not tune hyperparameters, but re-used the ones provided in the official repo.\nCQD As mentioned in Sec. 5, we re-used the pre-trained link predictor provided by the authors. However, we tuned CQD-specific hyperparameters, namely the CQD beam \"k\", ranging from [2,512] and the t-norm type being \u201cprod\" or \"min\" In Table C.1 we provide the hyperparameter selection for the old benchmarks FB15k237 and NELL995. Also note that we normalize scores with min-max normalization.\nConE We did not tune hyperparameters but re-used the ones provided in the official repo.\nCQD-Hybrid For CQD-Hybrid, to make the comparison with CQD fair, we re-used the hyperpa- rameters found for CQD and fixed an upper bound value for the CQD beam \u201ck\u201d to 512, even when there are more existing entities matching the existentially quantified variables, to match the upper bound of the \"k\u201d used for CQD. Additionally, the scores of the pre-trained link predictor are nor- malized between [0, 0.9] using min-max normalization, and a score of 1 is assigned to the existing triples.\nNEW BENCHMARKS\nFor FB15k237+H and NELL995+H, we re-used the same models trained for the old benchmarks, and using the same hyperparameters presented in App. C.1. Instead, for the new benchmark ICEWS18+H we trained every model. The used hyperparameters for each model are presented in the following:\nGNN-QE For GNN-QE, we tuned the following hyperparameters: (1) batchsize, with values 8 or 48, and concat hidden being True or False, while the rest are the same used for the old benchmarks and do not change across benchmarks. For ICEWS18+H the best hyperparameters are \u201cbatchsize=48\" and \"concat hidden=True\".\nULTRAQ Being a zero-shot neural link predictor, we re-used the same checkpoint provided in the official repo, as for C.1.\nCQD For NELL995+H and FB15k237+H we re-used the pre-trained link-predictor and the same hyperparameters found for the old benchmarks C.1. Instead, for the newly created ICEWS18+H, we train ComplEx link predictor with hyperparameters \u201cregweight\u201d 0.1 or 0.01, and batch size 1000 or 2000, with the best being, respectively 0.1 and 1000. Moreover, in C.2 are shown the hyperparameters for CQD on the new benchmark ICEWS18+H."}, {"title": "KNOWLEDGE GRAPHS STATISTICS", "content": "The statistics, i.e., number of entities, relation names, training/validation/test links, of the knowledge graphs used in this paper are shown in Table D.1."}]}