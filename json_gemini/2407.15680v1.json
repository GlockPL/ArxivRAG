{"title": "HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning*", "authors": ["Zhecan Wang", "Garrett Bingham", "Adams Wei Yu", "Quoc V. Le", "Thang Luong", "Golnaz Ghiasi"], "abstract": "Hallucination has been a major problem for large language models and remains a critical challenge when it comes to multimodality in which vision-language models (VLMs) have to deal with not just textual but also visual inputs. Despite rapid progress in VLMs, resources for evaluating and addressing multimodal hallucination are limited and mostly focused on evaluation. This work introduces HaloQuest, a novel visual question answering dataset that captures various aspects of multimodal hallucination such as false premises, insufficient contexts, and visual challenges. A novel idea from HaloQuest is to leverage synthetic images, apart from real ones, to enable dataset creation at scale. With over 7.7K examples spanning across a wide variety of categories, HaloQuest was designed to be both a challenging benchmark for VLMs and a fine-tuning dataset for advancing multimodal reasoning. Our experiments reveal that current models struggle with HaloQuest, with all open-source VLMs achieving below 36% accuracy. On the other hand, fine-tuning on HaloQuest significantly reduces hallucination rates while preserving performance on standard reasoning tasks. Our results discover that benchmarking with generated images is highly correlated (r = 0.97) with real images. Last but not least, we propose a novel Auto-Eval mechanism that is highly correlated with human raters (r = 0.99) for evaluating VLMs. In sum, this work makes concrete strides towards understanding, evaluating, and mitigating hallucination in VLMs, serving as an important step towards more reliable multimodal Al systems in the future.", "sections": [{"title": "1 Introduction", "content": "Hallucination, the generation of factually incorrect or inconsistent information, poses a critical challenge for the reliability of vision-language models (VLMs) [7, 13, 29, 40]. Hallucination in these systems can result from visual misinterpretations [24,32, 49], misaligned language understanding [12], or the generation of"}, {"title": "2 Related Work", "content": "Hallucination, the generation of factually incorrect or inconsistent information, is a well-documented issue in large language models (LLMs) [8,17,23]. Within the domain of vision and language understanding, hallucinations can manifest in several ways, including misinterpretation of visual elements, misaligned language understanding, or responses unsupported by either modality. While still a developing area of study, recent works have begun to explore these vision-specific hallucination phenomena [16,18,24,32,39,57,63]. Consequently, research efforts have focused on understanding, evaluating, and mitigating hallucination in VLMs.\nThere are a number of mechanisms that may cause a VLM to hallucinate. An over-reliance on language priors [42] is one such mechanism. For example, models often learn pairs of objects that co-occur together, and the presence of \"keyboard\" may bias a model towards outputting \"mouse\" or \"monitor,\" even if one is not present in the image [63]. Certain statistics can also be predictive of hallucination. An output token with low probability may indicate a model is hallucinating due to low confidence, while tokens towards the end of a long response may be hallucinatory if the model is running out of meaningful things to say [63]. It is also possible to understand hallucination in isolated instances by directly inspecting the attention weights to see what the model is attending to when it outputs hallucinatory text [51]. Despite these advancements, hallucination in VLMs is still not completely understood, in part because evaluating hallucination is not trivial.\nExisting approaches for evaluating hallucinations in VLMs have limitations. Methods that use binary yes/no questions [24], are constrained to short-word answers [27], rely on caption evaluation metrics [13,42], and require manual as-"}, {"title": "3 HaloQuest", "content": "This section describes the HaloQuest dataset. It details the image collection methodology, the design of questions to trigger hallucinations, the filtering and refinement process, and the LLM-based Auto-Eval mechanism. Example Halo-Quest entries are shown in Figure 1."}, {"title": "3.1 Image Collection", "content": "First, to ensure a rich and varied dataset, HaloQuest leverages both real and synthetic images. The real images are a random sample from the Open Images dataset, and synthetic images are sourced from online Midjourney and Stable Diffusion galleries [2, 20, 43]. Images are selected based on high view counts and positive ratings in order to prioritize quality and relevance. Search queries in-corporating combinations of topic words from a carefully curated list inspired by PartiPrompts are used to retrieve a varied range of images [59, Table 1].\nHuman annotators filter this initial set of images according to two criteria. The images should be interesting or unusual, but they must also be compre-hensible. For example, images are deemed interesting if they depict scenarios outside of everyday experiences, contain unexpected juxtapositions of objects,"}, {"title": "3.2 Designing Questions to Elicit Hallucination", "content": "Once the images are collected, humans and LLMs craft questions and answers about the images, focusing on creativity, nuanced reasoning, and probing poten-tial model biases. Specifically, HaloQuest includes three categories of questions designed to elicit hallucinations.\nFirst, questions with a false premise contain statements or assumptions that directly contradict the visual content of the image. They are designed to test whether the model can correctly prioritize visual evidence over misleading linguistic cues.\nNext, questions that are visually challenging require a deep understanding of image details, such as counting objects, determining spatial relationships, or reasoning about occluded areas. They evaluate the model's ability to perform complex visual analysis."}, {"title": "3.3 Filtering and Refining the Data Examples", "content": "The quality of annotated question-answer pairs is next improved through fil-tering. First, high-performing VQA models generate preliminary responses for an initial question pool. Then, experienced human annotators review both the questions and model-generated responses. Questions judged to be too easy are discarded or revised to increase difficulty. Ambiguous or nonsensical answers are flagged, ensuring each question has a clear and well-defined solution. This process leads to a dataset composed of challenging, high-quality examples."}, {"title": "3.4 Automatic VQA Evaluation", "content": "In order to facilitate free-form and open-ended VLM hallucination evaluation at scale, an LLM-based automatic evaluation method is developed. While in principle any LLM can perform such evaluation with basic prompting, this work introduces a recipe that is more effective than this baseline strategy. Specifically, a Langfun schema is developed which helps Gemini to accurately extract the main point in the model response and ground truth, and then decide whether these points are in agreement [38,41]."}, {"title": "4 Experiments", "content": "This section includes experiments that demonstrate the usefulness of HaloQuest in understanding, measuring, and reducing hallucination tendencies in VLMs. The results show that current models perform poorly on HaloQuest in a zero-shot setting, showing that much work remains to be done to build models that are hallucination-free. Furthermore, current evaluation metrics do not accurately quantify hallucination, a missing capability that the Auto-Eval framework di-rectly addresses. HaloQuest is also useful for reducing hallucination rates, and this training does not hurt performance on related VQA tasks. Additional ex-periments contrast the models' performance on generated and real images, and similarly for different question types. These results facilitate a more fine-grained understanding of model capabilities, enabling future hallucination mitigation strategies to be more targeted. Together, these findings highlight the significant step HaloQuest provides towards building more reliable and trustworthy VLMs."}, {"title": "4.1 Zero-shot Evaluation on HaloQuest", "content": "Table 3 lists zero-shot evaluation of top-performing VLMs on HaloQuest and reveals two key insights. First, existing VLMs struggle with HaloQuest, exhibit-ing high hallucination rates. This result indicates substantial shortcomings in model capabilities and highlights the need for robust hallucination mitigation. Second, increased model size doesn't necessarily translate to better hallucina-tion resistance. Surprisingly, BEiT-3 [53], a smaller model, outperforms several larger models. These findings underscore the importance of developing data-driven hallucination mitigation strategies that are not solely reliant on model scaling."}, {"title": "4.2 Quantifying Hallucination with Auto-Eval", "content": "Before VLM hallucination can be addressed, it must be accurately measured. Figure 3 compares modern metrics like BLEU, CIDER, ROUGE, and METEOR with human evaluation on the HaloQuest evaluation set [6, 25, 36, 48]. None of the metrics correlate well with human evaluation, demonstrating they are insufficient for measuring hallucination. Fortunately, Auto-Eval (Section 3.4) correlates strongly with human evaluation. While all experiments in this paper include both human evaluation and Auto-Eval scores, this result suggests that Auto-Eval can be used in the future if human evaluation is unavailable or is too expensive.\nTable 4 shows an ablation comparing different Auto-Eval implementations. Text-only prompting or simple schemas that do not prompt the model to reason"}, {"title": "4.3 Mitigating Hallucination with HaloQuest", "content": "In addition to identifying hallucination tendencies in VLMs, HaloQuest is also useful for mitigating them. In this experiment, four VLMs were fine-tuned with"}, {"title": "4.4 Understanding Hallucination in Synthetic Images", "content": "This work extends previous research on hallucination with real images in VLMs to include synthetically generated images as well. Table 7 shows model per-formance separated according to whether the images are real or synthetically generated. Although most models tend to hallucinate more with real images in this set, hallucination rates are quite high with synthetic images as well. In fact, performance on generated images is highly correlated with performance on real images, with r = 0.97 for both human evaluation and Auto-Eval, suggesting that synthetic images can provide an accurate measure of model capability, despite small discrepancies in overall performance.\nAlthough real images are more challenging in HaloQuest, there remain many reasons to continue to utilize synthetic images. These synthetically generated images offer a cost-effective and scalable solution for expanding datasets, and experimental results indicate that incorporating these images helps reduce hal-lucination rates in models (Tables 5 and 7). Indeed, while the synthetic images in HaloQuest are not as difficult on average as the real images, advancements in image generation models will likely close this gap in the near future. Further-more, as image generation systems become more widely used around the world, it will become even more important for models to be robust to hallucination in synthetic images. This surprising finding opens up exciting avenues for future"}, {"title": "4.5 Understanding Hallucination Triggers", "content": "VLMs hallucinate for various reasons. This work explores triggering hallucina-tion with questions with false premises, visually challenging questions, and ques-tions with insufficient context. Table 8 shows model performance broken down according to these image categories. On average, open-source models struggle substantially with false premise and insufficient context questions, but perform slightly better with visually challenging ones. Interestingly, different models have different strengths and weaknesses in different question categories. GPT-4 is more adept at addressing false premise and insufficient context questions, but is not as performant in the visually challenging section. This finding demonstrates"}, {"title": "5 Discussion and Future Work", "content": "This section explores the impact of this work and its potential to shape future research directions in the field, including discussion on the semantic novelty of synthetic images, solving hallucination comprehensively, multimodal hallucina-tion, finding nuance in responses with Auto-Eval, and broader societal impacts."}, {"title": "5.1 Visualizing Semantic Novelty in Synthetic Images", "content": "Beyond cost-effectiveness and scalability, HaloQuest leverages prompt-based syn-thetic images to access a wider spectrum of visual scenarios, including unusual, complex, and abstract scenes, which are challenging or infeasible to obtain from real-world sources. This is particularly critical given the growing prevalence of"}, {"title": "5.2 Hallucination Remains an Unsolved Problem", "content": "Experiments using HaloQuest highlight the severity of hallucination in current models. While fine-tuning on HaloQuest demonstrates significant reduction in hallucination rates, the problem persists. This aligns with trends in related work, where techniques can identify and alleviate hallucination but fall short of a complete solution. Tackling hallucination comprehensively will likely re-quire a multi-pronged approach. Further exploration into integrating symbolic reasoning, scaling both model parameters and dataset size, and potentially even rethinking model architectures might hold the key. This work represents an im-"}, {"title": "5.3 Multimodal Hallucination", "content": "This paper focuses on visual hallucination in VLMs, a phenomenon related to but distinct from text-only hallucination in LLMs. As AI systems continue to op-erate within multimodal environments (code, video, audio, etc.), the necessity of addressing hallucination across these varied modalities will become increasingly important. The key question remains: are there techniques that are capable of reducing hallucination universally, or will modality-specific approaches be essen-tial? Exploiting inherent structural differences between modalities might reveal new insights, but developing techniques that are modality-agnostic may be a more efficient path forward. The development of datasets like HaloQuest serves as a good starting point, emphasizing the importance of designing challenging benchmarks as the field looks towards tackling hallucination in the broader land-scape of multimodal AI."}, {"title": "5.4 Unconvering Nuance with Auto-Eval", "content": "This paper uses human evaluation as the gold standard for measuring model per-formance, but also contributes a novel Auto-Eval mechanism that holds promise for efficient evaluation at scale in future work. Human evaluation is important for benchmarking the Auto-Eval system itself. Interestingly, the relationship is reciprocal: exploring instances where human and Auto-Eval judgments diverge was useful for finding nuanced and challenging cases that highlight the subtle nature of hallucination detection. In a limited number of scenarios, this analysis even led to refinements in the ground truth labels. This demonstrates the po-tential of human and automated evaluation systems to work in tandem, driving continuous improvement in detecting and understanding hallucination."}, {"title": "5.5 Societal Impact", "content": "While this work primarily centers on the creation of a novel dataset, the po-tential societal impacts are significant. HaloQuest aims to provide a crucial tool for mitigating hallucination in VLMs, thereby improving their robustness and reducing the likelihood of erroneous or misleading outputs. This has implications for real-world applications where safety and reliability are paramount, such as autonomous systems or medical image analysis. However, it is important to ac-knowledge that like any technology, datasets can be used for both beneficial and potentially harmful purposes. Bad actors could leverage datasets like HaloQuest to intentionally train models to generate misleading or deceptive content tailored to exploit model weaknesses. This fact underscores the importance of ongoing research into the detection and mitigation of such malicious use of AI systems."}, {"title": "6 Conclusion", "content": "This work has introduced HaloQuest, a novel VQA benchmark that leverages both real-world and synthetically generated images. HaloQuest's controlled im-age generation and questions designed to elicit specific hallucination types en-able a more targeted analysis of hallucination triggers in VLMs. Experiments demonstrate that current state-of-the-art models struggle with HaloQuest, re-vealing a crucial disconnect between their capabilities and real-world reliability requirements. Importantly, fine-tuning VLMs on HaloQuest demonstrably re-duces hallucination rates while maintaining performance on typical reasoning tasks.\nHaloQuest highlights the potential of synthetic images in the development of robust multimodal AI. It addresses limitations present in traditional datasets, enabling the creation of richer and more varied visual scenarios. The dataset, coupled with an innovative machine-human-in-the-loop generation process, fa-cilitates targeted investigation into VLM weaknesses.\nFurther, this work introduces an LLM-based Auto-Eval mechanism that fa-cilitates open-ended and nuanced evaluation of VLM responses. This approach is a marked improvement over existing methods that often limit the model's expressive ability or are impractical for evaluating complex hallucinations.\nHaloQuest stands as a valuable resource for the vision-and-language commu-nity. It provides both a challenging evaluation benchmark and a training dataset aimed at mitigating hallucination in VLMs. This work underscores the power of synthetic image generation and advanced evaluation techniques in driving the creation of more reliable and trustworthy multimodal AI systems."}, {"title": "A Instructions for Crowdworkers Writing Questions", "content": "Crowdworkers were given the following instructions when asked to draft ques-tions and answers for a given image:\nFor each input image, please write 2 challenging questions as described below and also 3 answers for each question. In case it is hard to write a challenging question, skip the writing and write \"skip\".\nFirst question\nThe first question should ask something about a visual element related to the image which is not possible to answer by look-ing at the image. (We've discovered that AI models often struggle to express uncertainty and instead generate answers for these types of ques-tions. Therefore, we wish to create a dataset specifically for evaluating AI models on these types of questions.) These are some example cases for writing these types of questions.\nThe question asks some details about a visual element that is not visibly present in the image, consequently, we either cannot answer the question, or the answer to the question implies that the subject is not present. Please provide questions about elements that, while not visible, are relevant to the scene depicted in the image. For instance, you could ask about something that is hidden or cropped in the current context. Alternatively, you might consider asking about a detail that would likely be found in a similar image. For example please see the first question about the cat image below.\nThe question asks about specific information regarding one object which is visible in the image. However it is not possible to know the answer by checking the image. For example the ques-tion asks about the name of a building, art, street, mountain, ect which is presented in the image. However, by checking the image it's impossible to answer. Because the image doesn't show a popular landmark/object and also the name is not visible in the image.\nWe want to create challenging questions about the input image. But in some cases it's hard to create challenging questions (for example the input image is too simple). In these cases please just write \"skip\" instead of writing a question.\nSecond question\nThe second question should ask about a subtle detail presented in the image which we are able to easily provide a clear an-swer and the answer does not vary upon personal preferences or opinions. Please concentrate on minor details within the image that would be challenging to answer. For instance, if the image features a"}, {"title": "B Auto-Eval Implementation Details", "content": "Table 4 of the main paper explored different implementations of Auto-Eval. Each of the three implementations of Auto-Eval are detailed below. The first implementation uses a simple text-only prompt (Figure 5). The second imple-mentation adds a basic Langfun schema that the model must populate (Figure 6). The final implementation used throughout the paper also includes additional schema attributes that prompt the model to reason deeply about the main points of the response and ground truth before making a final decision (Figure 7). This reasoning is demonstrated in Figure 8. The result is an Auto-Eval system that has higher agreement with human raters than text-only prompting or the basic schema. All implementations use Gemini Pro as the underlying LLM."}, {"title": "C Finetuning Experiment Implementation", "content": "The results in Table 5 were obtained in the following way. For BLIP2, tuning focuses solely on the Q-Former's parameters to enhance question-answering capa-bilities, while the image encoder and LLM remain unchanged [22,62]. MiniGPT4 employs a Vision Transformer for image encoding and Vicuna for text decoding, connected by a Q-Former [11,65]. Its tuning targets a learnable linear projection layer to align visual features with Vicuna's embeddings, improving visual-textual integration. In mPLUG-Owl, the tuning strategy freezes the pre-trained visual encoder and abstractor, concentrating on improving the text decoder (Vicuna) through low-rank adaptation [56]. This enhances the model's ability to process and interpret visual-text data. A language generation loss is used to effectively minimize hallucination while maintaining generalizability.\nTo align our fine-tuning process with established best practices, we adhere to the methodologies outlined by [31,65] for crafting fine-tuning instructions. While both VQA v2 and HaloQuest fall within the domain of Visual Question Answer-ing tasks, they differ significantly in their answer formats. VQA v2 adopts a \u201cclosed-book\u201d approach, limiting responses to a predefined list of short answers that include both single words and phrases. Conversely, HaloQuest permits free-form answers, embracing a more flexible response format. This divergence ne-cessitates the formulation of task-specific instructions to optimize model perfor-mance during fine-tuning.\nFor the VQA v2 task, the instruction template provided to BLIP2 is structured as follows:\n<Image> Question: (Question} Short Answer:\nThis template is designed to elicit concise, predefined responses, aligning with VQA v2's structured answer requirements.\nIn contrast, for the HaloQuest task, we modify the instruction template to ac-commodate open-ended responses:\n<Image> Question: (Question} Answer:\nThis adjustment signals the model to generate elaborated and unrestricted re-sponses, catering to the open-ended nature of HaloQuest.\nSimilarly, for MiniGPT4 and mPLUG-Owl, we customize the prompts to align with the task requirements of VQA v2 and HaloQuest. These tailored prompts are designed to guide the models towards generating the expected form of an-swers, whether they be concise answers for VQA v2 or more elaborate responses for HaloQuest. Similarly, for the VQA v2 task, the instruction for MiniGPT4 and mPLUG-Owl is as follows:"}, {"title": "<Image> Answer the question. Q: {Question}", "content": "Conversely, for the HaloQuest task, the prompt is adjusted to encourage re-sponses in either words or phrases:\n<Image> Answer the question in words or phrases. Q: {Question}\nBy tailoring the instructions to the specific needs of each task, we ensure that the fine-tuning process enhances the relevance and accuracy of the model's out-puts, effectively addressing the unique objectives and constraints of VQA v2 and HaloQuest."}]}