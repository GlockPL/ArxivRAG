{"title": "SAFLITE: Fuzzing Autonomous Systems via Large Language Models", "authors": ["Taohong Zhu", "Adrians Skapars", "Fardeen Mackenzie", "Declan Kehoe", "William Newton", "Suzanne Embury", "Youcheng Sun"], "abstract": "Fuzz testing effectively uncovers software vulnera-bilities; however, it faces challenges with Autonomous Systems (AS) due to their vast search spaces and complex state spaces, which reflect the unpredictability and complexity of real-world environments. This paper presents a universal framework aimed at improving the efficiency of fuzz testing for AS. At its core is SAFLITE, a predictive component that evaluates whether a test case meets predefined safety criteria. By leveraging the large language model (LLM) with information about the test objective and the AS state, SAFLITE assesses the relevance of each test case.\nWe evaluated SAFLITE by instantiating it with various LLMs, including GPT-3.5, Mistral-7B, and Llama2-7B, and integrating it into four fuzz testing tools: PGFuzz, DeepHyperion-UAV, CAMBA, and TUMB. These tools are designed specifically for testing autonomous drone control systems, such as ArduPilot, PX4, and PX4-Avoidance. The experimental results demonstrate that, compared to PGFuzz, SAFLITE increased the likelihood of selecting operations that triggered bug occurrences in each fuzzing iteration by an average of 93.1%. Additionally, after integrating _ SAFLITE, the ability of Deep Hyperion-UAV, CAMBA, and TUMB to generate test cases that caused system violations increased by 234.5%, 33.3%, and 17.8%, respectively. The benchmark for this evaluation was sourced from a UAV Testing Competition.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous Systems (AS) are increasingly used in sectors such as transportation, healthcare, and industrial automation [1]. Ensuring their reliability and safety is essential, as failures pose significant risks to human life and infrastructure [2], [3].\nWhile the testing approach proposed in this paper is applica-ble to general AS, we focus specifically on AS for Unmanned Aerial Vehicles (UAVs). Commands are generated based on whether the mission is complete or if certain conditions hinder further progress. If the mission continues, the system outputs commands according to a predefined safety policy for the UAV to execute. The UAV then carries out these commands and sends updated sensor data back. External factors, such as wireless remote control or changing wind conditions, can also influence the UAV, and the AS adjusts accordingly based on the sensor feedback."}, {"title": "II. APPROACH", "content": "We developed a framework called UNIVERSAL AUTONON-MOUS SYSTEM FuzzING WITH LLMS to leverage large lan-guage models' understanding of safety properties in fuzzing tests for autonomous systems. This framework allows all au-tonomous systems fuzzing tools to integrate with large language models, facilitating comprehensive fuzzing tests on these sys-tems."}, {"title": "A. The Universal AS Fuzzing with LLMs", "content": "To establish a universal fuzzing framework for AS that in-tegrates with large language models, we initiated our approach by analysing current AS fuzzing tools including PGFuzz [7], DeepHyperion-UAV [9], CAMBA [10], and TUMB [11]. These tools primarily utilise mutation-based fuzzing techniques. By abstracting and refining their processes, we designed a universal AS fuzzing framework that encapsulates the methodologies of these tools. The fuzzing process begins with randomly selected/generated test cases being stored as initial inputs in the seed pool within the seed manager module. The seed manager selects a test case, known as a seed, from the seed pool based on a predefined seed scheduling strategy. This seed is then transferred to the mutation module, where it is subjected to predefined mutation operations, such as randomly changing a value of parameter in the test case. This mutation creates a new test case that is executed by AS. Optionally, there could be a \"number of test case setting\" in the mutation module to control the number of new test cases generated at each step by the fuzzing tool. In the event of a system failure, i.e., the violation of some safety property, a report is generated for detailed analysis. Otherwise, if the new test case does not result in a system failure, it is used by the seed manager to update the seed pool, for example, replacing the original test case with the new one, and the fuzzing process continues.\nFollowing the formulation of the universal AS fuzzing frame-work as illustrated in Figure 2, we include the SAFLITE module prior to the AS executing the test cases, which are now first analysed through LLMs."}, {"title": "B. SAFLITE", "content": "In fuzz testing, fuzzing tools for autonomous systems often produce a high volume of ineffective test cases due to the expansive input space of autonomous systems. These tools traditionally mutate seeds using predefined mutation operations, such as randomly changing the value of a parameter, which leads to inefficiency in discovering system safety issues. To address this, we have introduced an automated tool SAFLITE\u00b9, designed to improve testing efficiency without the manual review burden. SAFLITE employs a large language model to assess the proximity of generated test cases to predefined conditions. SAFLITE adapts the LLM as its reasoning engine for fuzzing, featuring the following essential pieces:\na) Definition of Interestingness: Based on the safety specifications of an autonomous system, some test cases are deemed more \"interesting\" than others due to their potential to breach safety requirements. This interestingness is defined by the expected behavior of the autonomous system when it violates these safety properties. The scope of this definition can vary from broad objectives, such as causing a control system crash, to more specific scenarios.\nAs illustrated in Figure 30, the definition of interestingness stipulates that the UAV must maintain a minimum distance of 1.5 meters from obstacles during the navigation of the PX4-Avoidance system's mission. This criterion emphasizes the UAV's safe navigation, specifically designed to prevent colli-sions with obstacles, thus reflecting the safety requirements for\nb) Current System State: It refers to the condition of the autonomous system immediately before SAFLITE analyses the test cases. For example, when testing a UAV control system, the current system state might include details such as the current location of drone and additional parameters like GPS coordinates or pitch angle. The selection of specific information to include can be informed by the definition of interestingness, which interprets the safety specifications, aiming for improving the accuracy of the assessment regarding how closely the system aligns with the predefined test conditions.\nThe example in Figure 36 shows the coordinates of the UAV under testing, which represent the current state information essential for the navigation task.\nc) Mutants: These represent the outcomes of mutations performed by the fuzzing tool. It is helpful to include a more detailed description of these results when presenting them, as this will assist the LLM in accurately understanding each test case.\nd) Prompt Template: To enable SAFLITE to work ef-fectively with different LLMs as the prediction core while ensuring consistent output, we developed a prompt template that systematically incorporates all relevant information to be provided to the LLM and standardises its output. As shown in Figure 3, the template (d) is populated using the definition of interestingness (\u2466), the current system state (6), and the corresponding mutants (0).\nMoverover, SAFLITE employs the CoT (Chain of Thoughts) approach to customise the LLM output format. The LLM should interpret the meaning of each mutant, then evaluate how each mutant will impact the current system state, based on the information provided about the current state. The LLM should produce a brief explanation for each thought process and assign a score to each mutant. A score of 10 indicates that the test case is the most interesting, meaning it is highly likely to meet the test conditions outlined in the definition of interestingness after execution, while a score of 0 signifies the opposite.\nWhen SAFLITE is applied to the AS fuzzing tools (as shown in Figure 2), the default expected output is a selection of test cases from a ranked list of mutants.\nIn addition, the LLM provides insights for each test case in its output, upon which it generates a final ranked list of test case scores. As illustrated in Figure 30, the LLM assesses obstacle positions in the environment and evaluates how proximity to waypoints might impact the avoidance system of UAV."}, {"title": "III. EVALUATION", "content": "In this section, we evaluate the proposed SAFLITE approach by addressing three research questions (RQs): validating that LLMs can indeed identify interesting test cases (Section III-A), showing that a more specific definition of interestingness can further improve its effectiveness (Section III-B), and, utmost, confirming that SAFLITE significantly improves the perfor-mance of existing fuzzing tools (Section III-C)."}, {"title": "A. RQ1: How effective do LLMs predict the interestingness of test cases?", "content": "a) Experimental Setup: For RQ1, we provided SAFLITE with a labelled dataset for prediction and evaluation. The dataset comprises 117 log files representing both flight issues (interesting) and normal flights (non-interesting), sourced from a combination of real-world and synthesised flights [12], [13]. The interesting logs were drawn either from the PX4 GitHub issue tracker, where users had uploaded logs that were accepted as demonstrating buggy behaviour requiring code changes, or from simulated flights replicating similar issues. The non-interesting logs were sourced from two places: simulated flights conforming to a standardised test card used by PX4 developers to validate code changes, and flights completed during the development of a new navigation submodule. These non-interesting logs showed standard mission completions and adhered to the policy requirements of the additional submod-ule. We broadly define interestingness as the occurrence of uncommon, difficult-to-understand, or significantly unexpected behaviors in drones, which may potentially lead to high-risk flights.\nTo assess the effectiveness of SAFLITE across different LLMs, we selected three models: GPT-3.5 [14], Mistral-7B [15], and Llama-2-7B [16] such that Mistral-7B and Llama-2-7B are deployed locally. The evaluation was conducted using four metrics: Accuracy (Acc), Precision, (Prec) Recall, and F1-score (F1).\nb) Results: The results for the four metrics are sum-marised in Table I, showing that GPT-3.5 achieves the highest performance in three metrics, with an accuracy of 68.6% and an F1 score of 68.6%. Its precision of 70.6% underscores GPT-3.5's strong ability to identify interesting test cases. The overall accuracy metric indicates that Llama-2-7B, the local LLM, performs the least effectively. However, another local LLM, Mistral-7B, performs comparably to GPT-3.5 and even surpasses it in recall, suggesting a strong capability to distin-guish between interesting and non-interesting test cases. This demonstrates that smaller, locally deployed LLMs can achieve performance comparable to GPT-3.5. Given the advantages of local LLMs for secure, local deployments, task-specific fine-tuning of Mistral-7B could further enhance its potential as the reasoning engine for SAFLITE.\nAn analysis of the recall metric reveals that GPT-3.5's recall drops significantly when its temperature is set to 1, indicating confusion in distinguishing between interesting and non-interesting test cases. This suggests that a high temperature results in overly random outputs and fabricated responses without adequate reasoning. Therefore, we recommend avoiding high-temperature settings when using SAFLITE.\nAnswer to RQ1: LLMs, such as GPT-3.5 and Mistral-7B, demonstrate effectiveness in predicting the inter-estingness of the flight log benchmark, though their performance is not exceptional. The broad definition of interestingness likely plays a key role in limiting their effectiveness."}, {"title": "B. RQ2: How does a more specific definition of interestingness impact the effectiveness of SAFLITE?", "content": "a) Experimental Setup: To examine the impact of using a more specific definition of interestingness on the effective-ness of SAFLITE, we conducted experiments by integrating SAFLITE with the existing fuzzing tool PGFuzz [7].\nPGFuzz can perform fuzzing tests on ArduPilot [17] and PX4 [13] systems based on safety policies to detect policy violations. We obtained a list of bugs from the PGFuzz repository2. Since most bugs in the list lacked details about the system state or violated policies, we selected 8 bugs for our experiment. For each bug, we translated the associated policy violation into a natural language description, which served as a definition of interestingness for SAFLITE. We then provided SAFLITE with the control system's state at the time of the bug occurrence (details in APPENDIX A). Given that PGFuzz randomly selects a parameter from a list related to the target policy as a test case, we extracted all possible parameters from this list and provided them to SAFLITE for analysis. Each test case was scored: below 4 as non-interesting, 5 to 7 as mid-interesting, and above 7 as interesting. If the test case that caused the bug was rated mid-interesting or interesting, it indicates that SAFLITE improves the likelihood of selecting bug-causing test cases over PGFuzz's random selection, thereby enhancing efficiency and demonstrating the value of using more specific Definitions of Interestingness.\nb) Results: The results of this experiment are presented in Figure 4. Each pie chart represents the outcomes of using SAFLITE to analyse a specific violated safety property. For example, Figure 4a shows that, for the selected safety property and across all test cases generated by PGFuzz, SAFLITE classified 58.8% of test cases as non-interesting, 36.5% as mid-interesting, and only 4.9% as interesting. Although the proportion of interesting test cases is the smallest, the test case involving FLIGHT_MODE, which caused the bug, was\nAdditionally, we observed that GPT-3.5 accurately under-stands the function of each test case and its impact on drone control systems. This suggests that GPT-3.5's training data might include considerable information about ArduPilot and PX4 sys-tems. However, the test cases that caused bugs No.24 and No.25 were categorised as mid-interesting instead of Interesting. Upon review, this occurred because GPT-3.5 lacked knowledge of the specific values assigned to these test cases, making it difficult to predict their exact impact. As correct classification is challenging without specific values, this demonstrates that GPT-3.5 recognises that some value assignments may not cause bugs, while others might. This underscores the importance of retaining some randomness in fuzzing when testing automated systems and highlights the value of integrating SAFLITE with fuzzing tools like PGFuzz.\nc) New bugs: Both PGFuzz and PGFuzz+SAFLITE were given 24 hours to perform fuzzing tests on each policy. The results were compared based on the number of bugs discov-ered and the number of fuzzing rounds completed. Notably, when testing the \u201cPX.HOLD1\" policy, both PGFuzz and PG-Fuzz+SAFLITE identified the same policy violation. However, PGFuzz+SAFLITE achieved this in just 126 rounds of fuzzing, significantly fewer than the 269 rounds required by PGFuzz. Additionally, the bug identified by PGFuzz+SAFLITE is a new bug in the PX4 system that had not been previously identi-fied and was absent from the bug list presented by PGFuzz. The bug occurred as follows: 1 The drone was initially in Hold mode, hovering in place. \u25cf SAFLITE analysed the test cases, and the MAV_CMD_DO_REPOSITION test case was assigned a score of 7.3 When the PX4 system executed the MAV_CMD_DO_REPOSITION test case, the drone began moving northwest but remained in Hold mode.\nMoreover, during the study, we also discovered a new bug. The bug occurred as follows: 1 The PX4 system was preparing to switch to \u201cACRO\" flight mode during a mission. 2 SAFLITE analyzed the test cases and gave a score of 8 to the MAV_CMD_DO_PARACHUTE test case. 3 The PX4 system switched to \"ACRO\" flight mode and executed MAV_CMD_DO_PARACHUTE. Although the safety policy prohibits deploying the parachute in \"ACRO\u201d mode, the parachute was deployed, causing the drone to crash.\nAnswer to RQ2: Using SAFLITE with a specific defini-tion of interestingness as the test condition can enhance the effectiveness of existing fuzzing tools, particularly by improving their ability to understand and predict test cases."}, {"title": "C. RQ3: How effectively does SAFLITE improve the perfor-mance of existing AS fuzzing tools?", "content": "a) Experimental Setup: To assess whether SAFLITE can successfully integrate with fuzzing tools and improve their performance in a full fuzzing test process, we followed the structure of UNIVERSAL AUTONONMOUS SYSTEM FUZZING WITH LLMS and combined SAFLITE with fuzzing tools from the SBFT UAV Testing Competition [12]: DeepHyperion-UAV [9], TUMB [11], and CAMBA [10]. The competition tasked these fuzzing tools with testing the PX4-Avoidance system, where a UAV navigates missions in a virtual environment under the control of PX4-Avoidance. The objective was to generate as many valid test cases as possible. A test case defines the size and coordinates of obstacles in the environment and is considered valid if the UAV either collides with an obstacle or comes too close. We translated this valid test case rule into the definition of interestingness for SAFLITE, as in Figure 30.\nIn our experiments, we adhered strictly to the competition rules and ran all fuzzing tools in the designated environment. Since automated control systems may behave differently across various missions, we provided six distinct missions (Mission 2-7) to compare the performance of the fuzzing tools in different scenarios. Mission 1 was excluded, as it was intended for participants to test their tools during development. Each mission, defined by the competition organisers, varied only in specific waypoints, requiring the system to guide the drone through them in sequence. We compared the number of valid test cases generated by the modified tools (integrated with SAFLITE) to those produced by the original tools used in the competition.\nb) Results: The experimental results, presented in Table II, demonstrate that SAFLITE successfully integrates with various fuzzing tools, completing the full fuzzing process and improving their performance. Each integrated tool generated more valid test, with improvements observed in all the missions. Among the three tools, DeepHyperion-UAV exhibited the most significant improvement in the number of valid test cases. However, the gains for CAMBA and TUMB were more modest. Upon reviewing their code, we believe this is due to their approach of making only minor mutations to each test case when generating new ones. As a result, the test cases produced are often very similar, and the LLM assigns similar scores, reducing the ability of the tools to rank test cases effectively based on their relevance to the Definition of Interestingness.\nNotably, all three fuzzing tools integrated with SAFLITE showed significant improvements in the number of valid test cases generated in Mission 2. In the other missions, at least one tool either showed no increase or achieved only a single additional valid test case. This variability in performance across missions may be attributed to the complexity of the tasks. Mission 2, being the simplest, involves the drone navigating to a waypoint and returning, which could explain the more consistent performance. However, as the missions become more complex, the LLM may struggle to fully understand the mis-sion, leading to unstable test case predictions and inconsistent performance. To address this, fine-tuning the LLM for specific missions or providing more detailed descriptions of the mission in the Current System State within the prompt might help improve its understanding and improve the accuracy of test case predictions.\nc) Different LLMs: We further investigate the outcomes from DeepHyperion-UAV+SAFLITE, using different LLMs as the prediction core. As shown in Table III, overall, GPT-3.5 still delivers the most significant improvement in the capabilities of DeepHyperion-UAV, while Llama-2-7B offers relatively less improvement. Notably, Mistral-7B demonstrates performance improvements that are nearly on par with GPT-3.5 and even sur-passes GPT-3.5 in Mission 4 and Mission 6. The performance rankings of the three LLMs in fuzzing align with the results from RQ1, with GPT-3.5 leading, Mistral-7B close behind, and Llama-2-7B showing the weakest performance.\nGiven the results, Mistral-7B's strong performance is par-ticularly impressive, considering it is a local LLM that has not undergone fine-tuning. considering it is a local LLM that has not undergone fine-tuning. This highlights the suitability of locally deployed, smaller LLMs for AS fuzzing tasks, and further improvements could unlock even greater potential for such models in this domain.\nAnswer to RQ3: SAFLITE can successfully integrate with various fuzzing tools, execute a full fuzzing test, and significantly improve their performance."}, {"title": "IV. RELATED WORK", "content": "In fuzzing AS, most tools rely on mutation-based techniques. For example, Hu et al. mutated traffic scenarios by changing traffic light colors or vehicle starting positions for testing the autonomous driving systems [18]. Additionally, there is also a lot of research focused on fuzzing UAV AS. For example, Khatiri et al. organized a fuzzing competition for the PX4-avoidance system, aiming to generate test cases that cause policy violations [12]. This led to the development of several fuzzing tools for UAV systems. Zohdinasab et al. introduced a novel algorithm to rank seeds, selecting the best ones for mutation and test case generation [9]. Tang et al. developed TUMB, which uses Monte Carlo Tree Search (MCTS) to ex-plore the UAV environment and generate diverse test cases [11]. Similarly, Winsten et al. [19] applied Wasserstein generative ad-versarial networks [20] to generate UAV test cases, comparable to Zhong et al.'s use of neural networks in fuzzing autonomous driving systems to predict whether new seeds would trigger traffic violations [21]. Both approaches exploit neural networks' ability to manage complex relationships. However, training neural networks can pose significant stability and convergence issues [22]. In contrast, we leverage LLMs to predict test cases, offering a more adaptive approach. Current research on LLMs in AS focuses on environmental understanding. For instance, Jiahui et al. showed that LLMs can assess the realism of driving environments [23]. Similarly, PromptTrack, introduced by Dongming et al., enables LLMs to predict object trajectories [24], while HiLM-D predicts risk object locations from images [25]. Our research differs by focusing on using LLMs to predict interesting test cases based on their environmental understanding."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce SAFLITE, a predictor that uses an LLM to determine whether a test case aligns with a user-defined test condition. We designed a framework for SAFLITE that integrates with any fuzz testing tool for AS, using its predictive capabilities to filter out irrelevant test cases and enhance performance. We evaluated the prediction accuracy of SAFLITE using real flight logs and tested it with four fuzz testing tools on three automated systems. The results show that SAFLITE significantly improves fuzz testing efficiency."}]}