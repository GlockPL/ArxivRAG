{"title": "RNA-GPT: Multimodal Generative System for RNA Sequence Understanding", "authors": ["Yijia Xiao", "Edward Sun", "Yiqiao Jin", "Wei Wang"], "abstract": "RNAs are vital molecules that carry genetic information essential for life, with significant implications for drug development and biotechnology. However, RNA research is often slowed by the vast amount of literature. To address this, we introduce RNA-GPT, a multi-modal RNA chat model that simplifies RNA discovery by leveraging extensive RNA literature. RNA-GPT combines RNA sequence encoders with linear projection layers and state-of-the-art large language models (LLMs) for precise representation alignment. This enables it to process user-uploaded RNA sequences and provide concise, accurate responses. Our scalable training pipeline, powered by RNA-QA, automatically gathers RNA annotations from RNACentral using a divide-and-conquer approach with GPT-40 and latent Dirichlet allocation (LDA) to handle large datasets and generate instruction tuning samples. Experiments show RNA-GPT effectively handles complex RNA queries, streamlining RNA research. We also introduce RNA-QA, a 407,616 RNA dataset for modality alignment and instruction tuning.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) trained on internet-scale corpora have been shown to perform extraordinarily well on a large array of tasks from Olympiad-level mathematical and scientific reasoning to planning long-term tasks for robotic systems [1, 2, 3]. Recent advances in the biological and medical fields have enabled the adaptation of powerful models to accelerate research, significantly reducing reliance on traditional experiments. Since proteins, RNAs, and DNAs can be represented as character strings and a vast amount of sequenced data is readily available, this has created an ideal environment for training language models to predict and generate protein, DNA, and RNA structures and sequences. Protein language models like ESM have successfully encoded protein sequence and structure information, inspiring works such as ProteinGPT and ProtSt, which adapt protein representations into a language-based format, enabling natural language querying of protein data [4, 5, 6, 7, 8, 9, 10]. Similar to ESM-2, works like RiNALMo and RNA-FM have utilized the flexible capabilities of LLMs to learn and predict RNA structure and functions [11, 12].\nMuch like the motivation behind protein research, where proteins are represented as strings of characters, RNAs\u2014with their sequences of five unique nucleotides\u2014have also sparked interest in computational RNA and DNA research using large language models (LLMs).\nWhile models like ProteinGPT, ProtST, ProteinChat, and ProteinCLIP, have made significant progress in aligning protein sequences and structures with textual descriptions, advancements in the DNA and RNA domains are far less advanced [9, 13, 14, 10, 15, 16]. Previous efforts, such as RiNALMo"}, {"title": "Methodology", "content": "and RNA-FM have mainly focused on specific tasks like promoter or enhancer prediction, and structure and function analysis [12, 11, 17]. ChatNT is among the few models striving to bridge the gap between RNA comprehension and natural language [18]. However, its emphasis is more on performing biological tasks as a conversational agent rather than providing deep RNA understanding and comprehensive dialogue. As a result, there is a notable gap in RNA chat models that offer in-depth knowledge. However, applying multimodal LLMs to RNA modeling presents unique challenges, especially in integrating diverse modalities such as textual descriptions, RNA sequences, and structural data.\nTo overcome these challenges, we propose a two-step approach to RNA-GPT. First, we utilize the RNA-FM sequence encoder to embed RNA sequences, followed by aligning these sequence representations with natural language through a large, automatically curated QA dataset from RNA Central [12, 19]. Secondly, to ensure our model generates concise and accurate responses, we break down RNA-QA's abstract summaries into individual QA pairs for instruction tuning, enhancing the model's ability to deliver clear and relevant answers. We utilize Meta AI's flagship Llama-3 8B Instruction as our backbone LLM to provide solid general language understanding [20]. More specifically, our contributions are as follows:\n\u2022 Novel Framework. RNA-GPT is one of the first multi-modal RNA sequence chat models that enables deep, interactive RNA-focused conversations, significantly enhancing the understanding of RNAs for biological research.\n\u2022 Large-scale Dataset and Collection Pipeline. We introduce RNA-QA, a QA dataset derived from the RNA Central Dataset for modality alignment instruction tuning of RNA chat models [19]. We also present our highly scalable collection pipeline that automates the scraping and summarizing of relevant literature on RNA. Using a divide-and-conquer summarization strategy, we ensure that research details are preserved effectively. For over 407,616 RNA samples, we create QA pairs, each accompanied by a comprehensive research summary based on available literature, and between 5 and 14 QA pairs. The depth and diversity of these annotations make RNA-QA an excellent resource for instruction tuning."}, {"title": "Model Architecture", "content": "RNA-GPT uses the pre-trained RNA-FM sequence encoder (Figure 1 and Figure 2) to embed RNA sequences, which are then passed through a linear projection layer. This layer learns to map the RNA embeddings to a shared representation space with natural language, enabling alignment with a backbone LLM, for which we chose Meta's Llama-3 8B model. The training process is divided into two stages: 1) Sequence and Modality Alignment, where RNA and natural language representations are aligned, and 2) Instruction Tuning, where the model is fine-tuned for task-specific QA generation."}, {"title": "Modality Alignment Stage (Stage 1)", "content": "RNA sequences in the form of strings are first fed into the pre-trained sequence encoder, featuring 12 transformer layers trained with 23 million RNAs from the RNA Central database via self-supervised learning [19, 12]. We utilize a specialized token <RNAHere> for RNA-text modality alignment:\nQ: < RNA >< RNAHere></RNA >< QuestionPrompts >\nA: < Description >\nThe embedded sequence information is encoded into the soft prompts and prepended to the question prompt. In stage 1 training, the question Q is left empty to prioritize learning the abstract description from the RNA representation. The description tag A is replaced with the full annotation from RNA Central [19] to train the linear projection layer to align an RNA with its full abstract annotation."}, {"title": "Instruction Tuning Stage (Stage 2)", "content": "In stage 2, we instruction-tune the model using our curated RNA-QA dataset. Previous protein-related chat models rely on fully annotated abstracts, frequently resulting in excessively lengthy and irrelevant responses. We take a different approach to address this by breaking down the full annotations into targeted QA samples with concise answers to specific questions as prediction targets. This allows the chat model to provide more relevant and accurate responses.\nWe augment the full abstract annotation dataset from stage 1 using GPT-40-mini to generate explicit QA pairs for this stage. The prompts from stage 1 are adapted to the Llama-3 style (\u201c###Human : ...\" and\u201c###Assistant : . . .\u201d), with Q replaced by explicit questions from RNA-QA, such as \"What regulatory role does the RNA have along with other RNAs?\u201d The model then generates descriptive yet concise answers such as \u201cThe RNA is involved in transcript splicing regulation along with RNVU1-18 and CLK1\" as A.\""}, {"title": "RNA-QA Dataset", "content": "To achieve modality alignment, we constructed a large-scale dataset from the RNA Central database [19], comprising 407,616 RNA sequences paired with abstract descriptions."}, {"title": "Divide and Conquer RNA Literature Summarization", "content": "We begin by filtering RNA sequences from RNA Central [19], focusing on those indexed with \"Lit Scan,\" yielding around 420,000 RNAs with associated research papers. We refine this set to include only sequences up to 1024 nucleotides, the maximum input length for our sequence encoder. For the remaining 407,616 RNAs, we scrape and extract abstracts from all relevant literature. As shown in Figure 3, we apply LDA topic modeling to group papers by topic, summarizing each group individually. This ensures each summarization focuses on a narrower, cohesive subject area, minimizing information loss. We have found that summarizing broad topics often causes key details to be missed, as the model struggles to condense diverse information. Grouping similar topics allows for more precise, detailed summaries that retain essential context. The final annotation is created by combining these summaries in a final round of summarization. This divide-and-conquer approach improves accuracy and efficiently handles large datasets. Moreover, it overcomes the token limits of GPT models, allowing for detailed, information-dense annotations of large RNA research profiles."}, {"title": "Data Augmentation", "content": "Similar works in protein chat alignment often use the entire protein annotation for instruction tuning [10, 13, 21, 9], which often result in verbose and irrelevant responses. To address this, RNA-GPT decomposes the rich RNA annotations of RNA-QA into more specific QA-pairs for instruction tuning using GPT-40-mini so that user instructions can be concisely answered. Concretely, we prompt GPT-40-mini to generate both open-ended and close-ended QA pairs with the context of the RNA-QA annotation to decompose the abstract into atom-level QA pairs."}, {"title": "Experiments", "content": "We trained RNA-GPT using the flagship Llama-3 8B model architecture [20] using a smaller 5K RNA, 121K QA samples subset for our initial model. We are in the process of training the large RNA-GPT that uses all 407,616 RNAs of RNA-QA with millions of QA samples."}, {"title": "Conclusions", "content": "We present RNA-GPT, a multimodal chat model for RNA sequences that enhances LLM-based question-answering and accelerates RNA discovery by providing concise, accurate responses to complex queries. RNA-GPT aligns RNA embeddings from the RNA-FM encoder with natural language in LLMs like Llama-3 using a learnable projection layer. We optimize instruction tuning with GPT-40-mini to ensure high-quality, precise answers. We also introduce RNA-QA, a 407,616 RNA question-answering dataset derived from the extensive RNA research literature. Our scalable framework, which uses topic modeling and divide-and-conquer summarization, enables efficient RNA-to-language dataset curation.\nExperiments with our initial model trained on a 5K subset of RNA-QA show that RNA-GPT generates very promising responses already, achieving high semantic and lexical scores on unseen RNA data. The full RNA-QA provides even more value in the development of multimodal RNA LLMs. RNA-GPT with RNA-QA will inspire further innovations in LLM-based RNA research, driving rapid advancements in the field."}, {"title": "Training Details", "content": "We conducted initial training of RNA-GPT on a 5K subset of the RNA-QA dataset to generate the initial model checkpoints.\nDuring the modality alignment (MA) stage (Stage 1), we optimized the projection layer over 10 epochs using a batch size of 1, weight decay of 0.05, and 2,048 warm-up steps. The AdamW optimizer [22] was employed with \\(\\beta_1 = 0.9\\) and \\(\\beta_2 = 0.98\\). A learning rate scheduler with a linear warm-up followed by cosine annealing was applied. Specifically, the initial learning rate was set to 1 \u00d7 10\u22124, the minimum learning rate to 8 \u00d7 10\u22125, and the warm-up learning rate to 1 \u00d7 10\u22126.\nAutomatic Mixed Precision (AMP) [23] was employed to enhance training efficiency.\nFor the instruction tuning (IT) stage (Stage 2), the RNA-GPT architecture was further trained for an additional 10 epochs, maintaining the same batch size of 1, weight decay of 0.05, and reducing the warm-up steps to 200. The configuration of the AdamW optimizer and the use of AMP remained consistent with those used in Stage 1.\nThe training process was executed on two Nvidia A100 GPUs (40GB) and required a total of 1 week for both stages combined.\nWe will extend the training process by scaling RNA-GPT to a larger configuration using the full 407,616 dataset from RNA-QA."}, {"title": "Case Studies", "content": ""}]}