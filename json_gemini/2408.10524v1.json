{"title": "XCB: an effective contextual biasing approach to bias cross-lingual phrases in speech recognition", "authors": ["Xucheng Wan", "Naijun Zheng", "Kai Liu", "Huan Zhou"], "abstract": "Contextualized ASR models have been demonstrated to effectively improve the recognition accuracy of uncommon phrases when a predefined phrase list is available. However, these models often struggle with bilingual settings, which are prevalent in code-switching speech recognition. In this study, we make the initial attempt to address this challenge by introducing a Cross-lingual Contextual Biasing(XCB) module. Specifically, we augment a pre-trained ASR model for the dominant language by integrating an auxiliary language biasing module and a supplementary language-specific loss, aimed at enhancing the recognition of phrases in the secondary language. Experimental results conducted on our in-house code-switching dataset have validated the efficacy of our approach, demonstrating significant improvements in the recognition of biasing phrases in the secondary language, even without any additional inference overhead. Additionally, our proposed system exhibits both efficiency and generalization when is applied by the unseen ASRU-2019 test set.", "sections": [{"title": "1 Introduction", "content": "For the past decade, End-to-end (E2E) ASR models have demonstrated remarkable progresses on speech recognition. Driven by large scale dataset, typical E2E Models like Transformer[1,2], Transducer [3,4] and Conformer[5] have been reported to yield the best results to-date on variant speech recognition tasks. Nevertheless, these E2E ASR systems might encounter recognition errors with long-tailed rare words (such as jargon or unusual named-entities in unique target domain), presenting a challenge for real-world ASR implementations.\nTo address this issue, one popular way is to enhance an E2E ASR system by integrating contextual information (extracted from a predefined hot-word list of rare words), known as contextualized ASR[6,7,8,9,10]. For example, Paraformer[11], as a non-autoregressive (NAR) model, has recently attracted increasing attention due to its high accuracy and efficient NAR inference. To enable it with hotword customization ability, SeACo-Paraformer[12] was proposed with improved ASR accuracy and hotwords recall rate. Nevertheless, in practical"}, {"title": "2 Preliminary", "content": "2.1 Paraformer\nAs a fast and accuracy parallel transformer, Paraformer [11] is a powerful NAR(non-autoregressive) ASR model trained with a large amount (more than 20k hours) of data. To speed up inference, two key technologies are designed. One is a continuous integrate-and-fire-based (CIF) based predictor to accurately predict the number of output tokens, the other is a glancing language model (GLM) based sampler to enhance the NAR decoder with the ability to model token inter-dependence. Benefiting from the technologies, on many public Mandarin benchmarks, Paraformer delivers remarkable performance on par with state-of-the-art AR systems, with more than a 10-fold speedup.\n2.2 SeACo-Paraformer\nSeACo-Paraformer [12] integrates the ability of hotword customization to the Paraformer backbone. By appending a semantic-augmentation contextual mod-ule aside the encoder, SeACo extracts hotword embeddings from the hotword"}, {"title": "3 Proposed Methods", "content": "To bias cross-lingual phrases in context of SeACo-Parafromer (pre-trained on large-scale first language(L1st), we introduce an additional XCB module and a language specific loss component, with motivation to enhance the acoustic embeddings associated with L2nd.\n3.1 XCB Module\nThe proposed XCB module is sandwiched between Paraformer encoder and pre-dictor, as illustrated in Fig.1(a). It comprises two core components: Language Biasing Adapter(LB Adapter) and Biasing Merging Gate(BM Gate), which are elaborated in Fig. 1(b).\nLB Adapter LB adapter is piled up with up- and down-sampling layers, in-terleaved with layer normalization and ReLU activation. It transforms H (the hidden representations of acoustic features X) into a language-biased hidden representation Hiba. The LB Adapter serves to distinguish the frames that asso-ciated with Land and enhance the corresponding representations in feature space"}, {"title": "3.2 Language Specific Loss", "content": "To encourage the learning of the language-biased acoustic embeddings Elb, an additional loss component, Land, is introduced to be combined with the original end-to-end system training loss (joint ASR and biasing loss). For this purpose, a L2nd GT label set is firstly constructed. This involves masking the List context with <unk> tokens and retaining only the Land context in each GT label. Then, in parallel to normal ASR decoding process, another decoding branch is built (the grey dotted line presented in Fig.1 (a)) to explicitly predict the L2nd tokens, as illustrated in Fig.1(a). The language-biased representation Hiba is fed into the predictor to produce CIF-triggered L2nd tokens, then these tokens are directly fed into the decoder to produce posterior probabilities $P_{L_{2nd}}$, which is used to calculate loss $L_{L_{2nd}}^{CE}$. The cross-entropy (CE) loss calculated between $P_{L_{2nd}}$ and the L2nd GT labels is referred to as $L_{2nd}^{CE}$. Overall, the total loss function is structured as a linear combination of three loss components, formulated as:\n$L_{total} = L_{ASR} + L_{bias} + \\alpha L_{L_{2nd}}^{CE}$\nwhere \u03b1 is a hyper-parameter determining the contribution of proposed language-specific loss."}, {"title": "4 Experiments", "content": "4.1 Data Preparation\nOur proposed ASR model with XCB-module is trained and evaluated on an internal code-switching (Mandarin-English) data. This in-house dataset has 13 independently recorded industrial audio data, containing 14k utterances with duration of 20 hours. All these recordings are code-switching utterances, collected with diverse speech topics and recording environment.\nThe in-house data is randomly split, with 11 recordings for training and 2 for testing. Considering that most early works on code-switching ASR use the ASRU 2019 Mandarin-English code-switching challenge dataset, we also perform"}, {"title": "4.2 Experimental Setup", "content": "The official published model 1 of SeACo-Paraformer is used as our ASR baseline system, which was pre-trained using up to 50k hours Mandarin speech data. Initialized by the baseline, our proposed XCB-enhanced ASR model is trained using the in-house training dataset for 10 epochs, with learning rate equalling 0.0002 and batch size of 30. The weight \u03b1 is set as 0.3 and no averaging action is applied to model checkpoints.\nApart from the conventional mixed error rate (MER), three more evaluation metrics are adopted to measure the performance on biasing phrases. They are: biasd character error rate(BCER) for Mandarin, biasd word error rate(BWER) for English and biasd mixed error rate(BMER) representing overall biasing per-formance, defined as:\n$BMER = \\frac{n_{bc} * BCER + n_{bw} * BWER}{N_{bc} + N_{bw}}$\nwhere nbc denotes the number of biasd Mandarin characters and nbw is the number of biasd English words."}, {"title": "4.3 Experimental Results", "content": "The experimental results are presented in Table1. Besides the SeACo baseline, we also directly finetuned SeACo using the in-house training dataset, and add the supervised-finetuned version (termed SeACo:sft) as another baseline for per-formance comparison.\nFrom the experiment results, we obtain a few valuable insights: 1) as ex-pected, our proposed XCB system significantly outperforms both baselines in terms of BWER. On the in-house test dataset, our system shows an impressive 17.2% relative reduction comparing to the vanilla backbone, and 8% over the"}, {"title": "4.4 Active v.s. Inactive XCB", "content": "As proved above, our proposed XCB system, by adding auxiliary XCB module on the ASR backbone, outperforms its baseline in both BMER and BWER. Herein, we conduct one more experiment by keeping the auxiliary XCB module inactive during the inference, with expectation to compare our system with the baseline at the condition of same computational complexity. The results are presented in Table2, where XCB:nBM refers to the inference system that inactivates the XCB module and directly feeds the hidden representation H into the predictor. Interestingly, even bypassed the XCB module, the XCB:nBM system shows even better performance than the XCB, on both test datasets. It sounds appealing considering the XCB:nBM incurs no additional computational overhead. We speculate that our XCB training might encourage discrimination of L2nd in the lower features produced by the encoder. Further in-depth investigation is needed in our future work."}, {"title": "5 Conclusion", "content": "To enhance performance of a contextualized ASR, particularly in recognizing phrases in the secondary language within code-switching utterance, we proposed"}]}