{"title": "XCB: an effective contextual biasing approach to\nbias cross-lingual phrases in speech recognition", "authors": ["Xucheng Wan", "Naijun Zheng", "Kai Liu", "Huan Zhou"], "abstract": "Contextualized ASR models have been demonstrated to ef-\nfectively improve the recognition accuracy of uncommon phrases when\na predefined phrase list is available. However, these models often strug-\ngle with bilingual settings, which are prevalent in code-switching speech\nrecognition. In this study, we make the initial attempt to address this\nchallenge by introducing a Cross-lingual Contextual Biasing(XCB) mod-\nule. Specifically, we augment a pre-trained ASR model for the dominant\nlanguage by integrating an auxiliary language biasing module and a sup-\nplementary language-specific loss, aimed at enhancing the recognition of\nphrases in the secondary language. Experimental results conducted on\nour in-house code-switching dataset have validated the efficacy of our\napproach, demonstrating significant improvements in the recognition of\nbiasing phrases in the secondary language, even without any additional\ninference overhead. Additionally, our proposed system exhibits both effi-\nciency and generalization when is applied by the unseen ASRU-2019 test\nset.", "sections": [{"title": "1 Introduction", "content": "For the past decade, End-to-end (E2E) ASR models have demonstrated remark-\nable progresses on speech recognition. Driven by large scale dataset, typical\nE2E Models like Transformer[1,2], Transducer [3,4] and Conformer[5] have been\nreported to yield the best results to-date on variant speech recognition tasks.\nNevertheless, these E2E ASR systems might encounter recognition errors with\nlong-tailed rare words (such as jargon or unusual named-entities in unique target\ndomain), presenting a challenge for real-world ASR implementations.\nTo address this issue, one popular way is to enhance an E2E ASR sys-\ntem by integrating contextual information (extracted from a predefined hot-\nword list of rare words), known as contextualized ASR[6,7,8,9,10]. For example,\nParaformer[11], as a non-autoregressive (NAR) model, has recently attracted\nincreasing attention due to its high accuracy and efficient NAR inference. To en-\nable it with hotword customization ability, SeACo-Paraformer[12] was proposed\nwith improved ASR accuracy and hotwords recall rate. Nevertheless, in practical"}, {"title": "2 Preliminary", "content": "As a fast and accuracy parallel transformer, Paraformer [11] is a powerful NAR(non-\nautoregressive) ASR model trained with a large amount (more than 20k hours)\nof data. To speed up inference, two key technologies are designed. One is a\ncontinuous integrate-and-fire-based (CIF) based predictor to accurately predict\nthe number of output tokens, the other is a glancing language model (GLM)\nbased sampler to enhance the NAR decoder with the ability to model token\ninter-dependence. Benefiting from the technologies, on many public Mandarin\nbenchmarks, Paraformer delivers remarkable performance on par with state-of-\nthe-art AR systems, with more than a 10-fold speedup."}, {"title": "2.2 SeACo-Paraformer", "content": "SeACo-Paraformer [12] integrates the ability of hotword customization to the\nParaformer backbone. By appending a semantic-augmentation contextual mod-\nule aside the encoder, SeACo extracts hotword embeddings from the hotword"}, {"title": "3 Proposed Methods", "content": "To bias cross-lingual phrases in context of SeACo-Parafromer (pre-trained on\nlarge-scale first language(L1st), we introduce an additional XCB module and\na language specific loss component, with motivation to enhance the acoustic\nembeddings associated with L2nd."}, {"title": "3.1 XCB Module", "content": "The proposed XCB module is sandwiched between Paraformer encoder and pre-\ndictor, as illustrated in Fig.1(a). It comprises two core components: Language\nBiasing Adapter(LB Adapter) and Biasing Merging Gate(BM Gate), which are\nelaborated in Fig. 1(b).\nLB Adapter LB adapter is piled up with up- and down-sampling layers, in-\nterleaved with layer normalization and ReLU activation. It transforms H (the\nhidden representations of acoustic features X) into a language-biased hidden\nrepresentation Hiba. The LB Adapter serves to distinguish the frames that asso-\nciated with Land and enhance the corresponding representations in feature space"}, {"title": "BM Gate", "content": "The BM gate takes in hidden representation H and language biased\nrepresentation Hiba to generate language-biased acoustic embedding Elb. The\ndual path inputs are fed into linear projection separately to obtain the language-\nspecific weights, then scaled by these weights, respectively. Lastly, both scaled\nresults and raw input representations (with residual connections) are merged to\nyield embedding Elb. The whole process can be formulated as following:\n$E_{lb} = BMGate(H, LBAdapter(H))$"}, {"title": "3.2 Language Specific Loss", "content": "To encourage the learning of the language-biased acoustic embeddings Elb, an\nadditional loss component, Land, is introduced to be combined with the original\nend-to-end system training loss (joint ASR and biasing loss). For this purpose, a\nL2nd GT label set is firstly constructed. This involves masking the List context\nwith <unk> tokens and retaining only the Land context in each GT label. Then,\nin parallel to normal ASR decoding process, another decoding branch is built\n(the grey dotted line presented in Fig.1 (a)) to explicitly predict the L2nd tokens,\nas illustrated in Fig.1(a). The language-biased representation Hiba is fed into the\npredictor to produce CIF-triggered L2nd tokens, then these tokens are directly\nfed into the decoder to produce posterior probabilities $P_{L_{2nd}}$, which is used to\ncalculate loss $L_{2nd}^{CE}$. The cross-entropy (CE) loss calculated between $P_{L_{2nd}}$ and\nthe L2nd GT labels is referred to as $L_{2nd}^{CE}$. Overall, the total loss function is\nstructured as a linear combination of three loss components, formulated as:\n$L_{total} = L_{ASR} + L_{bias} + \\alpha L_{CE}^{2nd}$\nwhere \u03b1 is a hyper-parameter determining the contribution of proposed language-\nspecific loss."}, {"title": "4 Experiments", "content": "Our proposed ASR model with XCB-module is trained and evaluated on an\ninternal code-switching (Mandarin-English) data. This in-house dataset has 13\nindependently recorded industrial audio data, containing 14k utterances with\nduration of 20 hours. All these recordings are code-switching utterances, collected\nwith diverse speech topics and recording environment.\nThe in-house data is randomly split, with 11 recordings for training and 2\nfor testing. Considering that most early works on code-switching ASR use the\nASRU 2019 Mandarin-English code-switching challenge dataset, we also perform"}, {"title": "4.2 Experimental Setup", "content": "The official published model 1 of SeACo-Paraformer is used as our ASR baseline\nsystem, which was pre-trained using up to 50k hours Mandarin speech data.\nInitialized by the baseline, our proposed XCB-enhanced ASR model is trained\nusing the in-house training dataset for 10 epochs, with learning rate equalling\n0.0002 and batch size of 30. The weight \u03b1 is set as 0.3 and no averaging action\nis applied to model checkpoints.\nApart from the conventional mixed error rate (MER), three more evaluation\nmetrics are adopted to measure the performance on biasing phrases. They are:\nbiasd character error rate(BCER) for Mandarin, biasd word error rate(BWER)\nfor English and biasd mixed error rate(BMER) representing overall biasing per-\nformance, defined as:\n$BMER = \\frac{n_{bc} * BCER+ n_{bw} * BWER}{N_{bc} + N_{bw}}$\nwhere nbc denotes the number of biasd Mandarin characters and now is the\nnumber of biasd English words."}, {"title": "4.3 Experimental Results", "content": "The experimental results are presented in Table1. Besides the SeACo baseline,\nwe also directly finetuned SeACo using the in-house training dataset, and add\nthe supervised-finetuned version (termed SeACo:sft) as another baseline for per-\nformance comparison.\nFrom the experiment results, we obtain a few valuable insights: 1) as ex-\npected, our proposed XCB system significantly outperforms both baselines in\nterms of BWER. On the in-house test dataset, our system shows an impressive\n17.2% relative reduction comparing to the vanilla backbone, and 8% over the"}, {"title": "4.4 Active v.s. Inactive XCB", "content": "As proved above, our proposed XCB system, by adding auxiliary XCB module on\nthe ASR backbone, outperforms its baseline in both BMER and BWER. Herein,\nwe conduct one more experiment by keeping the auxiliary XCB module inactive\nduring the inference, with expectation to compare our system with the baseline\nat the condition of same computational complexity. The results are presented\nin Table2, where XCB:nBM refers to the inference system that inactivates the\nXCB module and directly feeds the hidden representation H into the predictor.\nInterestingly, even bypassed the XCB module, the XCB:nBM system shows even\nbetter performance than the XCB, on both test datasets. It sounds appealing\nconsidering the XCB:nBM incurs no additional computational overhead. We\nspeculate that our XCB training might encourage discrimination of L2nd in the\nlower features produced by the encoder. Further in-depth investigation is needed\nin our future work."}, {"title": "5 Conclusion", "content": "To enhance performance of a contextualized ASR, particularly in recognizing\nphrases in the secondary language within code-switching utterance, we proposed"}]}