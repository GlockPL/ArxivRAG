{"title": "DEEP VESSEL SEGMENTATION WITH JOINT MULTI-PRIOR ENCODING", "authors": ["A. Sadikine", "B. Badic", "E. Ferrante", "V. Noblet", "P. Ballet", "D. Visvikis", "P.-H. Conze"], "abstract": "The precise delineation of blood vessels in medical images\nis critical for many clinical applications, including pathology\ndetection and surgical planning. However, fully-automated\nvascular segmentation is challenging because of the variabil-\nity in shape, size, and topology. Manual segmentation re-\nmains the gold standard but is time-consuming, subjective,\nand impractical for large-scale studies. Hence, there is a need\nfor automatic and reliable segmentation methods that can ac-\ncurately detect blood vessels from medical images. The in-\ntegration of shape and topological priors into vessel segmen-\ntation models has been shown to improve segmentation accu-\nracy by offering contextual information about the shape of the\nblood vessels and their spatial relationships within the vascu-\nlar tree. To further improve anatomical consistency, we pro-\npose a new joint prior encoding mechanism which incorpo-\nrates both shape and topology in a single latent space. The\neffectiveness of our method is demonstrated on the publicly\navailable 3D-IRCADb dataset. More globally, the proposed\napproach holds promise in overcoming the challenges associ-\nated with automatic vessel delineation and has the potential to\nadvance the field of deep priors encoding.", "sections": [{"title": "1. INTRODUCTION", "content": "Vessel segmentation is a vital component of medical image\nanalysis, focusing on the precise identification and differ-\nentiation of blood vessels within medical images such as\nComputed Tomography (CT) scans. It holds great signifi-\ncance in various medical applications, including diagnosis,\nsurgical planning, and disease monitoring [1, 2]. However,\nthis task is riddled with challenges including low contrast\nwith surrounding tissues, intricated multi-scale geometry [3],\nand variability in vessel structure. Preserving anatomical\nfeatures is critical for accurate analysis and treatment plan-\nning, as vessel shape and topology provide valuable insight"}, {"title": "2. METHODS", "content": "Let us consider a as a grayscale volume and y its corre-\nsponding binary ground truth segmentation. Deep super-\nvised segmentation involves formulating a mapping function\n: x \u2192 y. This mapping function is optimized through\nthe optimization of a loss function $\\mathcal{L}(y,\\hat{y})$, which in our\ncase consists of a combination of both weighted binary cross-\nentropy and Dice loss components. However, such loss func-\ntions are defined at the pixel level and do not have the abiltiy\nto account for high-level features or topological characteris-\ntics of the target. In this context, our work focuses on the\nprocess of integrating multiple priors into the segmentation\npipeline, through a compact and non-linear encoding."}, {"title": "2.1. Shape and topology information", "content": "The geometric and spatial characteristics of tubular structures\nare crucial for discerning their shape and overall topology.\nCross-sectional radii and skeleton primarily characterize their\ngeometrical properties. Determining the shape of a given ves-\nsel tree is typically performed by medical experts according\nto their domain knowledge. This process leverages spatial\ncoordinates and prior knowledge of the anatomy to create a\nground truth segmentation mask y. In binary scenarios, this\nsegmentation mask is defined as:\n$Y_i(v) =\\begin{cases} 1 & \\text{if } v \\in S \\\\ 0 & \\text{otherwise} \\end{cases}$                                                                                                                                                                     (1)\nwhere v is the set of voxels belonging to $y_i$ and S the spatial\ndomain of the vascular structure of interest.\nIn addition, topological connectivity refers to the arrange-\nment and connection of components within the targeted vas-\ncular structure. It involves analyzing how different parts of\nthe structure are connected, including branching points, bifur-\ncations, and endpoints. This connectivity can be effectively\ncaptured by abstract representations through skeletonization.\nAlternatively, the Euclidean Distance Transform (EDT) [13]\nprovides another approach to encode this topological prop-\nerty within tubular structures into a distance map, noted as\n$T_i$, where ridge points [14] correspond to the skeleton of the\nEDT. This representation ensures a seamless continuity be-\ntween the ridge-based skeleton and its adjacent voxels. The\nEDT is a well-known technique for computing the minimum\nEuclidean distance between each voxel and the nearest back-\nground voxel surface \u03a9. This calculation is expressed as:\n$T_i(v)=\\begin{cases} \\min_{v' \\in \\Omega} ||v - v'||_2 & \\text{if } v \\in S\\\\ 0 & \\text{otherwise} \\end{cases}$                                                                                                                                            (2)\nTo extract high-level features for shape and topology from\nmask $y_i$ and distance map $T_i$, the process involves defining\nthe transformation that represents both shape and topological\nconnectivity into a compact non-linear representation."}, {"title": "2.2. Deep prior encoding", "content": "Towards deep prior incorporation, we are interested in learn-\ning a mapping function that captures high-level information\nfrom observations $a_i$. This mapping function, denoted as $E_\\theta$\nand parameterized by $\\theta$, transforms the input data $a$ into a\nhigh-level undercomplete summary represented as $z$, with a\nsmaller size compared to the input, as this design choice al-\nlows to capture global information in a compact form. On the\nother hand, the decoder function D maps the latent code $z$\nback to the original observation space, generating an approx-\nimate reconstruction $\\tilde{a}$. The entire process is characterized by\nthe pair of functions $E_\\theta: A \\rightarrow Z$ and $D: Z \\rightarrow A$. In\nessence, D is applied to the latent representation $z$ obtained\nfrom $E_\\theta$, resulting in the reconstructed data $\\tilde{a} = D \\circ E_\\theta(a)$.\nThe parameters of such convolutional auto-encoder architec-\nture are estimated by minimizing the following loss function:\n$\\mathcal{L}_{CAE}(a, \\tilde{a}) \\propto \\sum_i l_i(a_i, \\tilde{a}_i)$                                                                                                                                                                                                           (3)"}, {"title": "2.3. Proposed deep joint multi-prior encoding", "content": "The pursuit of learning multiple priors in a unified compact\nrepresentation $z$, which we refer to as Joint Multi-Prior En-\ncoding (JMPE), stands as a more efficient alternative than em-\nploying separate encodings $z_p$. This challenge is effectively\naddressed through a multi-task learning approach, facilitated\nby a single encoder $E_\\theta$ and multiple decoders $D_p$, all sharing\nthe same latent code representation $z$. This technique proves\nparticularly valuable in applications where various tasks ex-\nhibit inter-dependencies, offering a streamlined and holistic\napproach to jointly managing multiple priors representation\nin a single latent space. The formulation of the multi-task\nconvolutional auto-encoder $\\xi$ (Fig.2), is defined as:\n$\\xi(y) := \\{D_s(z) = \\hat{y}, D_t(z) = \\hat{T} | z = E_\\theta(y)\\}$                                                                                                                                                                                                                                               (9)\nwhere $D_s$ and $D_t$ are dedicated to the tasks of reconstruction\nand regression, respectively. The optimal model $\\xi$ is achieved\nby minimizing the following loss across all training tasks:\n$\\mathcal{L}_{JMPE}(y, \\hat{y}) \\propto \\alpha_s \\sum_i l_i(y_i, \\hat{y}_i) + \\alpha_t \\sum_i l_i(T_i, \\hat{T}_i)$                                                                                                                                                                                                                                    (10)\nwhere $\\alpha_p$ weighting factors aim at balancing both tasks during\ntraining. Once the network $\\xi$ has been trained, the anatomi-\ncal alignment $\\mathcal{L}_{reg}^{JMPE}$ can be computed analogously to that\nshown in Eq.6. This is achieved by quantifying the distance\nbetween the projections of $y$ and $\\hat{y}$. The encoder $E_\\theta$, fol-\nlowed by a conv1\u00d71\u00d71 operation with fixed weights, is used\nto reduce the number of latent code feature maps, thereby im-\nproving the capture of more subtle features. In this scenario,\nEq.8 is streamlined into a unified regularization term:\n$\\mathcal{L}_t = \\mathcal{L}_{\\phi}(y, \\hat{y}) + \\lambda \\mathcal{L}_{reg}^{JMPE}(y, \\hat{y})$                                                                                                                                                                                                                (11)"}, {"title": "3. EXPERIMENTS", "content": ""}, {"title": "3.1. Imaging datasets", "content": "The 3D-IRCADb [15] dataset includes contrast-enhanced CT\nscans from 20 patients, equally divided between 10 females\nand 10 males. In approximately 75% of cases, these scans\nshow the presence of liver tumors. Expert radiologists man-\nually annotated ground truth masks for the liver, liver ves-\nsels, and liver tumors. Pre-processing included resampling to\na median voxel spacing, cropping to focus on the liver area,\nand appropriate clipping ([-150, 250]) of CT intensities."}, {"title": "3.2. Implementation details", "content": "Throughout the encoding stage, we set the following param-\neters: the number of layers l to 5, the initial number of fea-\nture maps $f_0$ to 8 (Fig.2), $\\alpha_p$ in Eq.10 to 1, the number of\nlatent code feature maps to 32, the learning rate to $10^{-4}$,\nthe batch size to 2, and the number of epochs to 1000. In\ncontrast, for the segmentation experiments, the learning rate,\nbatch size, and number of epochs were set to $3 \u00d7 10^{-4}$, 2, and\n1500, respectively. Additionally, the distance function d(.)\nwas defined as the cosine distance (Eq.6). Hyper-parameter\noptimization was performed using Optuna [16] with 20 tri-\nals for each configuration, and optimal $\\lambda$ values were deter-\nmined empirically as shown in Tab.1. Random data augmen-\ntation techniques including rotation, translation, flipping, and\ngamma correction was applied. A 5-fold cross-validation ap-\nproach was used. Deep networks were implemented using Py-\nTorch. In practice, seeds were fixed for weight initialization,\ndata augmentation and shuffling to ensure reproducibility."}, {"title": "4. RESULTS AND DISCUSSION", "content": "Results in Tab.1 show the performance of different models\nfor liver vessel segmentation, assessed using various eval-\nuation metrics including DSC (Dice Similarity Coefficient),\nJacc (Jaccard score), clDSC coefficient [17] for connec-\ntivity assessment, HD (Hausdorff distance), AVD (Absolute\nVolume Difference), and ASSD (Average Symmetric Surface\nDistance). The models include ResUNet as baseline, Re-\nsUNet+shape, ResUNet+topo, ResUNet+shape+topo, and the\nproposed approch. Our method outperforms the other mod-\nels, achieving the highest DSC, Jacc, clDSC, and ASSD\nscores with 54.78%, 38.00%, 50.34%, and 4.77mm respec-\ntively. It delivers robust performance in clDSC assessment,\npositioning it as a promising topology-aware model. Further,\nFig.3 illustrates the connectivity improvement reached by our\napproach. In particular, fine branches remain less discon-\nnected from main vessels compared to ResUNet+topo or Re-\nsUNet+shape+topo. The performance of our approach could\nbe improved by calibrating the hyper-parameter $\\alpha_p$ (Eq.10),\nwhich indirectly affects the JMPE coding scheme. Further-\nmore, our method allows the use of a single encoder instead\nof multiple encoders, thus reducing memory consumption."}, {"title": "5. CONCLUSION", "content": "In this paper, we presented an innovative approach that ad-\ndresses the integration of multiple priors into a unified formu-\nlation for segmentation purposes. The liver vessel delineation\nresults obtained from our method highlight the importance of\nincorporating high-level and topological constraints in med-\nical image segmentation, and provide potential avenues for\nfuture research in this area. Furthermore, extending this ap-\nproach to other datasets could provide valuable insights into\nits generalizability and effectiveness in various clinical con-\ntexts. Additionally, integrating graph neural networks in our\npipeline could further enhance connectivity encoding."}, {"title": "6. COMPLIANCE WITH ETHICAL STANDARDS", "content": "This research study was conducted retrospectively using hu-\nman subject data made available in open access [15]. The\nauthors declare that they do not have any conflicts of interest."}]}