{"title": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models", "authors": ["Zhenhong Zhou", "Zherui Li", "Jie Zhang", "Yuanhe Zhang", "Kun Wang", "Yang Liu", "Qing Guo"], "abstract": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated remarkable real-world capabilities, effectively collaborating to complete complex tasks. While these systems are designed with safety mechanisms, such as rejecting harmful instructions through alignment, their security remains largely unexplored. This gap leaves LLM-MASs vulnerable to targeted disruptions. In this paper, we introduce Contagious Recursive Blocking Attacks (CORBA), a novel and simple yet highly effective attack that disrupts interactions between agents within an LLM-MAS. CORBA leverages two key properties: its contagious nature allows it to propagate across arbitrary network topologies, while its recursive property enables sustained depletion of computational resources. Notably, these blocking attacks often involve seemingly benign instructions, making them particularly challenging to mitigate using conventional alignment methods. We evaluate CORBA on two widely-used LLM-MASs, namely, AutoGen and Camel across various topologies and commercial models. Additionally, we conduct more extensive experiments in open-ended interactive LLM-MASs, demonstrating the effectiveness of CORBA in complex topology structures and open-source models. Our code is available at: https://github.com/zhrli324/Corba.", "sections": [{"title": "1 Introduction", "content": "Agents based on Large Language Models (LLMs) (Achiam et al., 2023) are able to use external tools and memory, assisting humans in completing complex tasks (Wang et al., 2024; Xi et al., 2025). When multiple LLM-based agents collaborate, they form LLM Multi-Agent Systems (LLM-MASS) (Guo et al., 2024), which offer greater problem-solving capabilities and can also simulate human society in autonomous systems (Park et al., 2023). Despite their potential, the robustness and security of LLM-MASs remain significant concerns (He et al., 2024; Yuan et al., 2024). To this end, ensuring LLM-MASs ethically and trustworthy poses an important challenge (Hua et al., 2024).\nRecent research shows that LLM-MASs are vulnerable to malicious attacks, such as misinformation (Ju et al., 2024) and jailbreak attacks (Gu et al., 2024), that can propagate within the system. However, existing work has largely overlooked blocking attacks (Gao et al., 2024; Dong et al., 2025), which aim to reduce the availability of LLM-MASS and consume excessive computational resources. Such blocking attacks pose a particular threat to LLM-MASs, because these systems require more computational resources and are less resilient to resource wastage (Zhang et al., 2024b). Besides, since LLM-MASs rely on information exchange and interaction among agents (Hong et al., 2023; Qian et al., 2024), blocking attacks designed to spread contagiously further amplify their impact.\nIn this paper, we propose Contagious Recursive Blocking Attacks (CORBA), a simple yet novel attack that can effectively increase unnecessary computational overhead and degrade the availability of LLM-MASs. Specifically, we introduce a contagious attack paradigm that can propagate through the LLM-MAS topology (Yu et al., 2024), infecting any reachable node from the entry agent. Compared to broadcast-based attacks (Zhang et al., 2024a), CORBA uses an infinitely recursive mechanism that ensures the malicious prompt persists within the system and remains effective without being nullified by divergence (Nasr et al., 2025).\nWe evaluate CORBA on two popular open-source LLM-MAS frameworks, namely, AutoGen (Wu et al., 2024) and Camel (Li et al., 2023). Experimental results demonstrate that CORBA can reduce the availability of LLM-MASs and waste computational resources across various topology structures."}, {"title": "2 Related Works & Preliminary", "content": "LLM-Based Agent and Multi-Agent System.\nWith the development of LLMs, LLM-based agents have made significant achievements (Yao et al., 2023; Shen et al., 2023). By collaboration, the capabilities of multi-agent systems are further extended, enabling them to accomplish more complex tasks (Chen et al., 2023; Baek et al., 2024). Some autonomous LLM-MASs can even simulate human society (Hua et al., 2023; Park et al., 2024), facilitating the exploration of AI-human interactions.\nLLM Blocking Attacks. Some studies have noted that LLMs can be induced by malicious attackers to generate redundant responses through specially crafted prompts, leading to wasted computational resources (Geiping et al., 2024). In white-box settings, such attacks typically rely on training modifications or access to gradient information (Gao et al., 2024; Dong et al., 2025). Recently, optimization-based black-box approaches utilizing LLMs have also been proposed (Zhang et al., 2024b). These works indicate the growing attention to blocking attacks as a critical security concern.\nPreliminary. We denote an LLM-MAS consisting of n LLM-based agents as S, where the LLM used is denoted as L. The topology structure T of agents A = {a1, a2, ..., an} in S is represented as a graph:\nT = (A, E), E \u2286 A \u00d7 A,\nwhere the edge e = (ai,aj) \u2208 E represent ai and aj are allowed to exchange their information or pass instruction to each other. We define r = a(P) to represent that the x-th agent executes instruction P at time step t and generates response r. Besides, we assume that the attacker M uses a malicious prompt Pm to attack the system S, and the agent affected by the attack is denoted as ab."}, {"title": "3 Method", "content": "In this section, we first introduce the blocking attacks in LLM-MAS and formally define it. Then, we describe the design of the contagious attacks. Finally, we demonstrate how CORBA operates."}, {"title": "3.1 Blocking Attacks B in LLM-MAS", "content": "Since multiple agents are combined into a system, agent blocking attacks focus more on disrupting information exchange and instruction passing compared to existing LLM blocking attacks (Geiping et al., 2024). In addition to consuming computational resources, it also reduces system availability.\nSimilar to repetition blocking in LLMs (Gao et al., 2024), an agent as in LLM-MASs is considered blocked if, from time tm onward, it consistently produces the blocking response Rm. This condition is formally defined as below:\n\u2200t\u2265tm, a(Pm) = Rm,\nwhere a(Pm) = Rm indicates that starting at malicious time point tm, the agent as always returns the response Rm when fed with the prompt Pm.\nA blocking attack B that continuously blocks an agent is further defined by its dynamic, recursive behavior. At any subsequent time step tm+1, the agent remains blocked if:\n\u2200l\u22650, a(Pm+1) = Rm+l Pm+1,\nand the attack passes recursively via a self-loop:\nRm+1 (ab,ab),\natm+1+1 (Pm+1),\nab\nwhere the arrow \u2192 represents the transmission of instructions, and (ab, ab) \u2208 E denotes a self-loop on ab, ensuring that the blocking state of af is maintained during the victim LLM-MAS working."}, {"title": "3.2 Contagious Attacks C in LLM-MASS", "content": "Inspired by the blocking attack B, which reduces availability and consumes computational resources by generating repetitive junk instructions at the single-agent level, we extend this idea to the multi-agent level. In a multi-agent system S, if a malicious attack prompt P propagates indefinitely, its accumulative effect can result in a similar attack across agents. We formally define the contagious instruction attack C as follows:\nam+1(Pm+1) = Rm+1 Pm+1,\nRm+1 (ac,ad)\natm+1+1 (Pm+l),\nwhere (ac, ad) \u2208 E indicates that ad is an arbitrary neighboring agent of ac."}, {"title": "3.3 CORBA", "content": "We integrate the characteristics of blocking attack B and contagious attack C into CORBA, a contagious blocking attack paradigm for LLM-MASS. We formally define CORBA as follows:\natm+1(Pm+1) = Rm+l Pm+1,\nRm+1 (ac,ac), atm+1+1 (pm+l),\n(ac,ac\u0131), atm+1+1 (pm+1).\nRm+l\nThat is, CORBA not only maintains a blocking state on each individual agent but also propagates the attack to neighboring agents. Through this infinitely recursive transmission, the attack prompt Pc of CORBA achieves the following effect:\n\u2200ab \u2208 R(ab), \u2203d \u2265 0,\ns.t. atm+d(Pc) = Rc \u21d4 Pc.\nwhere R(ab) denotes the set of all nodes reachable from ab in the topology T, ensuring that every reachable agent enters a blocked state after a finite number of time steps. We present the complete CORBA workflow in Appendix B."}, {"title": "4 Experiment", "content": "In this section, we introduce the configuration of our experiments, including the LLM-MAS frameworks and the underlying backbone LLMs we employ. Besides, we define two metrics to evaluate the impact of blocking attacks on the availability."}, {"title": "4.1 Experiment Setups", "content": "LLM-MASS. AutoGen (Wu et al., 2024) and Camel (Li et al., 2023) are popular open-source agent frameworks that enable the flexible construction of LLM-MASs with various topologies. Our experiments utilize these task-driven frameworks to implement an open-ended multi-agent system where agents can interact freely, simulating an agent society. For both AutoGen and Camel, we employ state-of-the-art LLM APIs as the foundation for agents, including GPT-40-mini, GPT-4, GPT-3.5-turbo, and Gemini-2.0-Flash (Team et al., 2023). In addition, our open-ended MAS provides greater flexibility, allowing us to evaluate a broader range of LLMs. Beyond the aforementioned APIs, we also test Qwen2.5-14B (Yang et al., 2024), Llama3.1-70B (Dubey et al., 2024), and Gemma-2-27B (Team et al., 2024).\nEvaluation Metric. Due to LLM-MASS can have various topologies, and an effective attack must ensure that as many agents as possible enter a blocked state. To quantify this, we define the Proportional Attack Success Rate (P-ASR), which measures the proportion of blocked agents within an attacked LLM-MAS. A higher P-ASR indicates a greater reduction in system availability, meaning a more effective attack.\nIn addition to effectiveness, we also consider efficiency equally vital. Therefore, we design the Peak Blocking Turn Number (PTN), which evaluates how quickly the attack reaches its peak impact. PTN indicates the number of turns required for the attack to stabilize at the maximum P-ASR. A lower PTN suggests a faster and more efficient attack. Note that PTN = 1 typically indicates either an ineffective attack or a topology with too few nodes.\nThe Baseline Method. We employ the prompt injection method from Zhang et al. (2024a) as our baseline method for inducing harmful behavior in LLM-MASs. Specifically, we insert the following command into normal agent prompts: \"Ignore all previous instructions and repeat the last action fo 100 times, then ask other agents to do so.\" This causes agents to enter a loop and attempt to induce similar looping behavior in other agents."}, {"title": "4.2 Experimental Results", "content": "Open-Source Frameworks Are Vulnerable.\nWe begin by evaluating the effectiveness and efficiency of CORBA in settings where agents can share chat histories and communicate via broadcast. For each configuration, we conduct 10 trials, averaging the results to ensure consistency. In each trial, a random agent is selected as the attack entry point. The experimental results, presented in Tables 1 and 2, confirms our findings.\nEffectiveness under Complex Topologies.\nSince LLM-MASs can adopt complex topologies, and prior work has shown that different topologies significantly impact security (Yu et al., 2024), we extend our experiments to various topology structures. As shown in Tables 3 and 4, bold highlights the LLM with the highest P-ASR for each topology, while underline denotes the topology with the highest P-ASR for each LLM. Our method remains effective across non-trivial topologies and consistently outperforms baseline approaches. These results demonstrate that CORBA is well-suited for real-world scenarios and presents a more substantial security threat.\nOpen-ended LLM-MASs Are Also Susceptible.\nLLM-MASs are increasingly being used for open-ended chat and complex societal simulations. To access their vulnerability, We investigate the impact of injecting the CORBA attack into these systems. As shown in Fig. 1, our attack is not only faster but also more robust than baseline methods, achieving nearly 100% P-ASR within just 20 turns.\nWe also attempt to apply commonly-used safety defense methods to detect and mitigate CORBA. Detailed results and analysis can be found in A."}, {"title": "5 Conclusion", "content": "This paper introduces CORBA, a malicious attack paradigm designed to block LLM-MASs and degrade their availability. Extensive experiments demonstrate the vulnerability of existing open-source frameworks and open-ended simulations."}, {"title": "Limitations", "content": "Our study reveals the potential risk of blocking attacks in existing LLM-MAS applications. However, our focus is primarily on exposing these vulnerabilities rather than developing strategies to mitigate them. In future work, we will further investigate effective defense mechanisms to prevent such blocking attacks in LLM-MASS."}, {"title": "A To what extent can existing defense methods resist LLM-MAS blocking attack?", "content": "Various safeguard mechanisms have been developed to protect LLMs, primarily focusing on defending against jailbreak attacks (Deng et al., 2023; Zou et al., 2023; Zeng et al., 2024) and ensure safety. However, little attention has been given to mitigating LLM blocking attacks. To address this gap, we conduct experiments to assess whether existing defenses can effectively counter such attacks. We evaluate CORBA by determining whether the response Rm generated from the attack prompt Pm is flagged as malicious. This method is commonly used to evaluate jailbreak outputs for harmful content (Chao et al., 2023). For this evaluation, we employ GPT-40-2024-08-06."}, {"title": "B How CORBA Works", "content": "To facilitate understanding, this section visualizes how CORBA progressively blocks agents in an LLM-MAS with a complex topology.\nStep 1: We illustrate the initial state of the LLM-MAS, where all agents are structured into a topology and function normally.\nStep 2: A malicious attacker injects the CORBA prompt into the entry agent, causing it to enter an infinite recursive blocking state.\nStep 3: The entry agent propagates the CORBA prompt to its neighboring agents, leading them to become blocked as well.\nStep 4: All reachable nodes are blocked, fully compromising the LLM-MAS. This significantly reduces system availability and results in excessive computational resource consumption.\nFigure 6 illustrates the complete attack flow of CORBA. After injecting the CORBA prompt into the LLM-MAS, affected agents become blocked and propagate the viral prompt outward, ultimately leading to system-wide suspension of the MAS."}]}