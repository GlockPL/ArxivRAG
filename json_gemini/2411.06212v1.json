{"title": "MULTISTAGE NON-DETERMINISTIC CLASSIFICATION USING SECONDARY CONCEPT GRAPHS AND GRAPH CONVOLUTIONAL NETWORKS FOR HIGH-LEVEL FEATURE EXTRACTION", "authors": ["Masoud Kargar", "Nasim Jelodari", "Alireza Assadzadeh"], "abstract": "Graphs, comprising nodes and edges, visually depict relationships and structures, posing challenges in extracting high-level features due to their intricate connections Multiple connections introduce complexities in discovering patterns, where node weights may affect some features more than others. In domains with diverse topics, graph representations illustrate interrelations among features.Pattern discovery within graphs is recognized as NPhard. Graph Convolutional Networks(GCNs)are a prominent deep learning approach for acquiring meaningful representations by leveraging node connectivity and characteristics.Despite achievements, predicting and assigning 9 deterministic classes often involve errors.To address this challenge, we present a multi-stage non-deterministic classification method based on a secondary conceptual graph and graph convolutional networks, which includes distinct steps:1)leveraging GCN for the extraction and generation of 12 high-level features; 2) employing incomplete, non-deterministic models for feature extraction, conducted before reaching a definitive prediction;3)formulating definitive forecasts grounded in conceptual (logical)graphs. The empirical findings indicate that our proposed approach outperforms contemporary methods in classification tasks.Across three datasets-Cora, Citeseer, and PubMed-the achieved accuracies are 96%, 93%, and 95%, respectively.", "sections": [{"title": "1 Introduction", "content": "In recent decades, with the significant expansion of networks and the vast amount of data generated by these networks, the demand for graphs to effectively display these data has increased dramatically [Goyal and Ferrara, 2018]. As an essential data structure, a graph includes a set of nodes and edges. This data structure effectively describes and visualizes the relationships between nodes and edges [Waikhom and Patgiri, 2021, Jiang et al., 2020]. Graphs have many applications in various fields, including social networks, sensor networks, and document networks [Pan et al., 2019]. Feature-based graphs are another type of graphs where the nodes and edges have information or, in other words, features. Due to the extensive and complex interactions of information within networks, graph analysis is of particular importance.\nClassification is considered a challenge in feature-based graphs due to their unique features. Node and graph classification are two particular challenges in graph learning that have been recently noticed. In machine learning,"}, {"title": "2 Previous Work", "content": "Classification has emerged as a prevalent task in machine learning, focusing on classification nodes or subgraphs within a graph into discrete classes or labels. This is particularly valu-able when there is a need to comprehend the connections"}, {"title": "2.1 Node classification", "content": "We will provide a brief overview of the three categories into which graph neural networks are classified based on the mechanism they use for node classification. FSGNN, a simple two-layer model for graph neural networks (GNN), was presented by Liu et al. FSGNN incorporates a \"soft selection\u201d mechanism that effectively learns the importance of features during the training process. The proposed approach performs better than the state-of-the-art method (SOTA) and achieves more than 51.1% accuracy in node classification in the given class Maurya et al. [2022]. Yang Hu et al. introduced a simple graph learning framework called graph-MLP, which relies on MLP (Multilayer Perceptron) with-out message-passing modules. Notably, this framework achieves graph node classification for the first time without using traditional message-passing methods. What distinguishes graph-MLP is its ability to maintain stable performance even when adjacency information is compromised or lost during the inference process Hu et al. [2021]. Yao et al. proposed a new framework for joint training that incorporates pairwise constraints between nodes and uses the AdaDW strategy to enhance the training of classification models in homogeneous graphs. However, it should be noted that this method does not apply to heterogeneous graphs such as HLN, making it ineffective in such scenarios [Wu et al., 2021].To address the limited information acquisition caused by the shallow structure of existing classification methods, a hierarchical graph attention network (HGAT) is introduced for semi-supervised classification. HGAT increases the accuracy by combining the attention mechanism in the input and prediction layers and assigning weights to different nodes. The simulation results show the improved performance of the proposed method on simplified datasets such as Cora, Citeseer, PubMed, and NELL. However, the limitation of this approach is its lack of application exclusively for directed graphs [Li et al., 2020]. Tang et al. introduce a new GNN-INCM model, including ECO and GRO modules for node embedding learning. This paper also uses HSKDM to train multiple GNN-INCM models simultaneously. GNN-INCM shows improved performance on the base dataset, and using HSKDM further increases the classification performance Huang et al. [2022]. Yu Wang et al. proposed a tree decomposition method to separate neighborhood features into distinct layers. They introduce a tree-decomposed graph neural network (TDGNN),which includes TDGNN-s and TDGNN-w variants for node classification. This method provides a flexible layer configuration for complex networks and reduces the feature smoothing problem [Wang and Derr, 2021]. Tang et al. introduce a new analytical framework called WCE-GNN that explores the relationship between graph neural networks (GNN) and hypergraph neural networks (HyperGNN) in the context of node classification. The framework enables the direct application of GNNs for hypergraph node classification. The results indicate that WCE-GNN has lower spatial and temporal complexity than modern HyperGNN methods [Tang et al., 2024]. Wang et al. have introduced a novel Graph Convolutional Network (GCL) encoder referred to as low graph adversarial learning (LR GCL) .The proposed methodology engages in semi-supervised or transitional node classification through a two-stage process, wherein the graph data undergoes corruption by noise in the feature labels of each node [Wang and Yang, 2024]. Sejan et al. have introduced a model based on Graph Convolutional Networks (GCN), utilizing weight, adjacency, and identity matrices to enhance node classification performance, specifically regarding classification accuracy [Sejan et al., 2023].\nA novel Community GCN method is introduced for community detection in social networks through node classification utilizing Graph Convolutional Networks (GCN). This approach adeptly leverages the benefits of transmitting messages via feedforward networks (FFN)and traversing connections via GCN. Furthermore, it incorporates spatial proximity ma-trices, weighted average feature extraction, and GCNN-based node classification, effectively contributing to identifying communities within social networks [Bhattacharya et al., 2023]."}, {"title": "2.2 Graph classification", "content": "Lara et al. introduced a simple algorithm that uses Laplacian graph spectral decomposition to achieve graph classification and provide an initial reference score for a given dataset [de Lara and Pineau, 2018]. The GAM framework, a new RNN-based approach for graph classification with attention, is presented. This framework uses external memory to integrate information from different parts of the graph and takes care of processing parts of the graph [Lee et al., 2018]. Similarly, Wang et al. propose an innovative edge feature graph attention network (EGAT)that combines edge and node data into a graph attention mechanism. This study is the first to consider edge features in the graph attention mechanism. EGAT is a versatile framework that integrates custom messaging functions to address specific domains. The simulation results in this paper show that the proposed method outperforms previous approaches in terms of accuracy when evaluated on Cora, Citeseer, PubMed, and AMLSim datasets [Wang et al., 2021]. To address the issue of over-smoothing resulting from an augmented number of convolutional layers in graph classification tasks, the authors advocate for implementing a Multi-Level Coarsening-based Graph Convolutional Network (MLC-GCN). This framework introduces an adaptive module for structural coarsening to produce coarse graphs,subsequently employed in"}, {"title": "2.3 Edge classification", "content": "Jiang et al. introduced CensNet, a convolution with node-edge switching graph neural network designed for learning tasks involving graph-structured data with node and edge features. CensNet is a versatile graph embedding framework that embeds nodes and edges in a hidden feature space. The proposed method performs strongly in various graph learning tasks such as semi-supervised node classification, graph classification, and unsupervised link prediction, outperforming existing approaches in various benchmarks and establishing itself as a state-of-the-art solution [Jiang et al., 2020]. A new unsupervised approach called AttE2vec is introduced, aiming to learn a low-dimensional vector representation for attributed edges. AttE2vec directly obtains feature information from edges and nodes, even with significant distances, to infer current and new node embeddings. The proposed method shows good edge classification and clustering results in Cora,Citeseer, and PubMed datasets Bielak et al. [2022]. Zhong et al. present a dynamic graph representation learning technique based on a time transformer graph (TGT). TGT uses a sequential interactive switching network to learn information from neighboring nodes at one-step and two-step intervals, ensuring accurate node representation. This article intro- duces three aggregation modules and one propagation module, which increases the accuracy of dynamic graph display while reducing execution time. This method works well in link prediction and edge classification Zhong and Huang [2023]."}, {"title": "3 Proposed method", "content": "An attributed graph is denoted by H = (V, E), where V is a set of nodes, and E is a set of edges. In H, each node v is associated with a feature vector xv. In the feature vector xv, m shows the number of node features. Furthermore, the structure of the graph is represented by the adjacency matrix A, where Aij = 1 if there is an edge between nodes vi and vj, and Aij = 0 otherwise. The feature matrix X is an n\u00d7m matrix, where n represents the number of samples and m represents the number of features. In the attributed graphs, there is a discussion called neighborhood. In other words, the neighborhood represents the common feature among the nodes. As shown in Figure 1, each node has a specific color, each color represents a group; for example, the orange color refers to one group, and the green color to another group."}, {"title": "3.1 Architecture", "content": "Figure 3 shows the general framework proposed in this study. Our approach consists of several steps.The first step is graph input, which includes nodes and edges that show the data structure and their relationships. The second step is extracting high-level features from nodes by GCN. The third step is the non-deterministic prediction step, which uses incomplete and non-deterministic models to extract intermediate features. The fourth step is a conceptual layer, creating a conceptual graph for the final classification. Finally, the fifth step is the final step for predicting classes."}, {"title": "3.2 Complexity Analysis", "content": "The computational complexity of our proposed method involves several key components. First, the adjacency matrix construction has a complexity of O(| E |), where | E | represents the number of edges in the graph. This step requires iterating through all edges to construct the matrix. Next, processing the feature matrix has a complexity of O(n \u00d7 m), where n is the number of nodes and m is the number of features, representing the cost of handling all node features. The neighborhood search, which identifies neighbors for each node, has a complexity of O(n \u00d7 k), with k being the average number of neighbors. Embedding the nodes into a new feature space has a complexity of O(n \u00d7 m \u00d7 d), where d represents the embedding dimension.\nThe transpose of the matrix, required in subsequent operations, has a complexity of O(| E |). Extracting attributes from the neighborhood data has a complexity of O(n \u00d7 k \u00d7 d), scaling with the node count, neighborhood size, and feature dimension. Encoding the input features results in a complexity of O(z\u00b2), where z is the size of the final encoded representation.\nThe graph convolutional network (GCN) layers have a complexity of O(L\u00d7 | E | \u00d7d), where L is the number of layers, | E | is the number of edges, and d is the feature dimension. The first GCN layer, responsible for high-level feature extraction, has a complexity of O(L\u2081\u00d7 | E | \u00d7d), while the second GCN layer, which performs the final prediction, has a similar complexity of O(L2\u00d7 | E | \u00d7d), both scaling with the number of layers and the graph size."}, {"title": "4 Experimental Results", "content": "This section evaluates and comprehensively reviews our proposed method by performing various tests. All tests were performed using a system with Windows 10, a Core i7 processor, 16GB of RAM, and an NVIDIA GPU."}, {"title": "4.1 Experimental Settings", "content": "We evaluate the performance of our proposed model to perform the fully supervised node classification task using three benchmark datasets, the details of which are shown in Table 2. Cora, Citeseer, and PubMed are three real-world datasets commonly used for node classification and introduced by [Sen et al., 2008]."}, {"title": "4.2 Node Classification Results", "content": "Table 3 shows each model's average node classification accuracy on three selected datasets.The results show that our method increases accuracy compared to methods such as GCNII, which have been presented recently and show better performance."}, {"title": "4.3 Evaluation Criteria", "content": "Accuracy: It refers to a measure used to evaluate the performance of a model in correctly predicting the class or label of a graph. It measures the proportion of correctly classified graphs from the total number of graphs in the evaluation dataset. Accuracy is calculated by dividing the number of correctly classified graphs by the total number of graphs in the data set. This shows how accurately the model can classify graphs. Which is calculated using Eqs (2):\nAccuracy = $\\frac{TP+TN}{TP+TN+FP+ FN}$"}, {"title": "4.4 Discussion", "content": "Figures 5-7 show graphical representations of precision and loss curves for the Cora, Citeseer, and PubMed datasets. Graphical representations in TSNE embedding format show categories of different classes (seven, six, and three classes for Cora, Citeseer, and PubMed,respectively). Accuracy and loss curves have been drawn for the training and validation stages, which show the x-axis of accuracy and the y-axis of loss. We obtain 96% accuracy with a loss of 0.008 for CORA, 93% accuracy with a loss of 0.005 for Citeseer, and 95% accuracy with a loss of 0.0009 for PubMed."}, {"title": "Baseline Methods:", "content": "To demonstrate the better performance of our model, we compare it with basic methods in graph neural networks (GNN), including GCN, GAT, APPNP, GraphSAGE, and GCNII.\n\u2022 GCN: It uses a layered propagation principle rooted in approximating the initial spectral complexity on graphs. Considering the characteristics of nodes and their neighboring connections enables the potential to obtain representations of nodes through learning [Kipf and Welling, 2016].\n\u2022 GAT: This method assigns different weights to neighboring nodes depending on their importance and is generally used as the baseline of GNN [Velickovic et al., 2017].\n\u2022 APPNP: This model is customized based on the relationship between GCN and PageRank and also based on the PageRank algorithm. It is designed to work with graph data and collect relational information between nodes. Its purpose is to predict labels for nodes in a graph [Gasteiger et al., 2018].\n\u2022 GraphSAGE: proposed for inductive node embedding that simultaneously learns the topological structure of each node's neighborhood and the distribution of node characteristics in the neighborhood. In addition to the characteristics of nodes, it also uses the structural characteristics of graphs [Hamilton et al., 2017].\n\u2022 GCNII: It is a deep GCN model capable of providing a K-order polynomial filter with arbitrary coefficients that avoid over-smoothing [Chen et al., 2020]."}, {"title": "Parameter Settings:", "content": "In this section, we briefly explain each of the parameters used. More details of the hyperparameters for all three datasets are listed in Table 4.\n\u2022 Momentum: In deep learning, it is a parameter used to improve the training of neural networks, especially when using gradient-based optimization algorithms. Momentum helps speed up the convergence of the training process and helps improve the performance of the Loss Function. It should be noted that learning rate and momentum have almost a complementary relationship.\n\u2022 Negative slope: It is used to prevent the decrease of the gradient in deep networks, and if the input is negative, a small non-zero coefficient is applied to it until there is a downward slope. In this way, the sudden decrease of the gradient Prevents the length of the operation.\n\u2022 Weight-decay: The learning process of deep models gradually reduces the learning rate. This reduction of the learning rate continues until it reaches zero. This method is used to reduce the side effects associated with large weights and control the degree of generalizability of the model and is obtained from the Eqs (1):\nWeight-decay = $\\frac{Learning Rate}{Epoch}$\n\u2022 Gamma coefficient (Y) or discount rate: It is a rate that controls how much a model benefits from new data changes at each stage of learning. This parameter determines how fast or slow the model is directed towards the optimal solution. We used the gamma coefficient in all three datasets.\n\u2022 Epoch: It means a stage or period in the training of a deep model, which passes through all its training data in each period of the model.\n\u2022 Batch Size: a meta-parameter in deep learning that determines the number of training samples used per iteration (or batch) during the training process. Due to the increase in the speed and quality of the model, it is included in the agenda of our paper."}, {"title": "5 Conclusion", "content": "In this paper, we introduced a novel approach for node classification. The proposed model comprises multiple stages. The initial stage involves utilizing GCN to derive high-level features from embedded and encoded inputs. The subsequent step employs Using incomplete (non-deterministic) models for non-deterministic forecasting. Creating a secondary conceptual graph is based on non-deterministic prediction and, finally, classification and prediction of classes. Experimental results demonstrate that our proposed method outperforms other methods by at least 5% on benchmark datasets. Thus, in addressing the research inquiries, it can be asserted that incomplete models function as Logical classifications, and their output (logical graphs) serves as input for the ensuing step. Logical graphs enable preliminary estimations before final classification, leading to a substantial enhancement in prediction results."}, {"title": "6 Future Work", "content": "In future endeavors, the proposed approach can be applied to extract protein subpatterns in the second stage. Additionally, in social networks, it can be employed to extract diverse subgroups based on events, extract subsystems and modules of software systems, and deduce utility modules for practical applications."}]}