{"title": "Swift Brush v2: Make Your One-step Diffusion Model Better Than Its Teacher", "authors": ["Trung Dao", "Thuan Hoang Nguyen", "Thanh Le", "Duc Vu", "Khoi Nguyen", "Cuong Pham", "Anh Tran"], "abstract": "In this paper, we aim to enhance the performance of Swift- Brush, a prominent one-step text-to-image diffusion model, to be com- petitive with its multi-step Stable Diffusion counterpart. Initially, we ex- plore the quality-diversity trade-off between SwiftBrush and SD Turbo: the former excels in image diversity, while the latter excels in image quality. This observation motivates our proposed modifications in the training methodology, including better weight initialization and efficient LORA training. Moreover, our introduction of a novel clamped CLIP loss enhances image-text alignment and results in improved image quality. Remarkably, by combining the weights of models trained with efficient LORA and full training, we achieve a new state-of-the-art one-step diffu- sion model, achieving an FID of 8.14 and surpassing all GAN-based and multi-step Stable Diffusion models. The evaluation code is available at: https://github.com/vinairesearch/swiftbrushv2", "sections": [{"title": "1 Introduction", "content": "Text-to-image generation has experienced tremendous growth in recent years, allowing users to create high-quality images from simple descriptions. State-of- the-art models [3, 33, 40, 42] could surpass humans in art competition [43] or produce synthetic images nearly indistinguishable from real ones [7]. Among popular text-to-image networks, Stable Diffusion (SD) models [41, 42] are the most widely used due to their open-source accessibility. However, most SD mod- els are designed as multi-step diffusion models, which require multiple forwarding steps to produce an output image. Such a slow and computationally expensive mechanism hinders the use of these models in real-time or on-device applications.\nRecently, many works have tried to reduce the denoising steps required in text-to-image diffusion models. Notably, few recent studies have successfully de- veloped one-step diffusion models, thus significantly speed up the image generation. While early attempts [13,32] produce blurry and malformed photos, subsequent methods produce sharp and high-quality outputs. These methods mainly distill knowledge from a pre-trained multi-step SD model (referred to as the teacher model) to a one-step student model. InstaFlow [30] employs Recti- fied Flows [29] in a multi-stage and computation-expensive training procedure. DMD [54] combines a reconstruction and a distribution matching loss as the training objectives, requiring massive pre-generated images from the teacher. SD Turbo [46] incorporates adversarial training alongside a score distillation loss, achieving photorealistic generation. However, it heavily relies on a large-scale image-text pair training dataset and, as later discussed, has a poor diversity. SwiftBrush [34] utilizes Variational Score Distillation (VSD) to transfer knowl- edge from the teacher network to the one-step student through a LoRA [17] intermediate teacher model. Notably, training SwiftBrush is simple, fast, and image-free, making it an intriguing method.\nDespite these promising achievements, one-step text-to-image diffusion mod- els still fall short of multi-step models in terms of the FID metric. On the stan- dard COCO 2014 benchmark [28], SDv2.1 can achieve the lowest FID-30K score of 9.64 with classifier-free guidance (cfg) scale of 2, while the best-reported score from the one-step models of equivalent parameters scale is 11.49 [54]. The gap is expected since directly predicting a clean image from noise in a single step is much more challenging than via a multi-step scheme. Hence, one may be- lieve that one-step text-to-image models could only approach or reach a similar performance as the teacher model but never exceeding it.\nIn this paper, we challenge this belief by seeking a one-step model that can surpass its multi-step teacher model quantitatively and qualitatively. Our so-"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Text-to-Image Generation", "content": "Text-to-image generation involves synthesizing high-quality images based on input text prompts. This task has evolved over decades, transitioning from constrained domains like CUB [48] and COCO [28] to general domains such as LAION-5B [47]. This evolution is driven by the emergence of large vision-language models (VLMs) like CLIP [39] and ALIGN [20]. Leveraging these"}, {"title": "2.2 Accelerating Text-to-Image Diffusion Models", "content": "Efforts to accelerate diffusion model sampling include faster samplers and distil- lation techniques. Early methods reduce sampling steps to as few as 4-8 steps by incorporating Latent Consistency Models to distill latent diffusion models [9,32]. Recent studies have achieved one-step text-to-image generation by training a student model distilled from a pretrained multi-step diffusion model, employing various techniques such as Rectified Flows [30], reconstruction and distribution matching losses [54], and adversarial objectives [27,46,52,57]. However, the out- put images often exhibit blurriness and artifacts, and one-step methods still underperform compared to multi-step models while requiring large-scale text- image pairs for training.\nDifferentiating itself from the rest, SwiftBrush [34] proposed a one-step dis- tillation technique that required only training on prompt inputs. The method gradually transfers knowledge from the teacher to the one-step student through an intermediate LoRA multi-step teacher. SwiftBrush's image-free training pro- cedure offers a simple way to scale training data and extend the student model capability via auxiliary losses, which are not constrained by limited-size imagery training data. Therefore, while Swift Brush also falls short in quality compared to the teacher model, we find its high potential for further development to produce a one-step student that even beat the multi-step teacher at its own game."}, {"title": "3 Background", "content": "Diffusion Models are generative models that transform a noise distribution into a target data distribution by simulating the diffusion process. This trans- formation involves gradually adding noise $e \\sim N(0, I)$ to clean image $x_o$ over a series of T steps (forward process) and then learning to reverse this process (reverse process). The forward process can be formulated as:\n$x_t = a_tX_o + \\sigma t \\epsilon, t \\in 0,T$ (1)\nwhere $x_t$ is the data at time step t and $\\{(a_t, \\sigma_t)\\}_{t=1}^{T}$ is the noise schedule such that $(a_T, \\sigma_T) = (0,1)$ and $(a_0,\\sigma_0) = (1,0)$. On the other hand, the reverse process aims to reconstruct the original data from noise. Training involves mini- mizing the difference between predicted output from model $e$ parameterized by"}, {"title": "4 Proposed Methods", "content": "In this section, we begin by conducting an in-depth analysis of the quality- diversity trade-off in representative diffusion-based text-to-image models (Sec. 4.1). Subsequently, we discuss our strategy for incorporating the strengths of Swift- Brush and SD Turbo (Sec. 4.2). Lastly, we explore various approaches to en- hance the distillation process and post-training procedures (Sec. 4.3 to 4.5). An overview of our methodologies is presented in Fig. 2."}, {"title": "4.1 Quality-Diversity Trade-off in Existing Models", "content": "We first analyze the properties of the teacher model, SDv2.1, and existing one- step diffusion-based text-to-image models. For the teacher model, we assess its performance across different guidance scales. Here, we select SwiftBrush and SD Turbo due to their quality and distinct training procedures. SwiftBrush relies solely on score distillation from the teacher in its image-free training, while SD Turbo trains on real images with adversarial and distillation loss. We conduct our analysis on the COCO 2014 benchmark and report relevant metrics in Tab. 1.\nWhen assessing the multi-step teacher's performance, the classifier-free guid- ance scale (cfg) plays a crucial role. A low cfg (e.g., cfg = 2) yields a low FID score of 9.64, driven by high output diversity (recall = 0.53). However, this set- ting results in weak alignment between images and prompts (CLIP score = 0.30) and lower image quality (low precision). Conversely, a large cfg (e.g., cfg = 7.5) markedly improves text-image alignment (CLIP score = 0.33) but restricts diver- sity (recall = 0.36), resulting in a poor FID score of 15.93. Moderate cfg values (e.g., cfg = 4.5) strike a better balance, offering the highest precision score.\nWhen evaluating the one-step students, we notice distinct behaviors. SD Turbo, benefiting from adversarial training on real images, yields highly natu- ralistic outputs with an exceptionally high precision score, surpassing even those of the multi-step teacher. However, this results in poor diversity, reflected in a low recall of 0.35. Conversely, SwiftBrush adopts an image-free training approach, allowing flexible combinations of random-noise latents and input prompts. Such a relaxed supervision enables the student model to generate more diverse outputs but at the expense of quality (Tab. 1). We further verify this finding by a quali- tative evaluation, illustrated in Fig. 1. When given identical input prompts, SD Turbo generates realistic yet similar outputs. In contrast, SwiftBrush produces a wider range of outcomes, albeit with greatly distorted artifacts. Regardless, both one-step models exhibit FID scores around 15-16, significantly higher than the teacher's best score. Observing a quality-diversity trade-off in existing one-step diffusion model such as SD Turbo and SwiftBrush, we aim to combine these two to leverage the strengths of both."}, {"title": "4.2 SwiftBrush and SD Turbo Integration", "content": "In this section, we explore strategies for effectively merging SwiftBrush and SD Turbo to enhance the quality-diversity trade-off. A direct approach is unify- ing their training procedure, i.e., combining adversarial training from SD Turbo and Variational Score Distillation from SwiftBrush. However, this simplistic ap- proach proves challenging due to computational demands and potential failure. While Swift Brush's image-free procedure is easier to implement, reproducing SD Turbo's training process is complex and resource-intensive. The presence of the discriminator complicates the training, necessitating significant VRAM and dataset requirements. Additionally, SD Turbo's intense supervision may con- strain SwiftBrush's loose guidance, limiting output diversity.\nBased on our discussions, we opt not to utilize SD Turbo's adversarial train- ing. Instead, we leverage its pretrained weights to initialize the student network within SwiftBrush's training framework. This straightforward approach proves highly effective. As can be seen in the comparison between the second and the third row in Tab. 2, the resulting model has improved FID and recall. By em- ploying SD Turbo's pretrained weights, we provide a solid foundation for the training model to maintain high-quality outputs, while SwiftBrush's image-free training process gradually enhances generation diversity."}, {"title": "4.3 In-training Improvements", "content": "Besides data efficiency and diversity promotion, Swift Brush's image-free training still has room for improvement. First, it allows an easy means to scale up training data by collecting more prompt inputs. This task is simple, given the abundance of textual datasets and the availability of large language models, unlike the costly and labor-intensive task of collecting image-text pair data commonly required. Second, by not forcing the output of the student model to be the same as that of the teacher, SwiftBrush allows the student to go even beyond the quality and capability of the teacher. We can advocate it to happen by adding extra auxiliary loss functions in SwiftBrush training. In this section, we will discuss the implementation of those ideas for improvement.\nImplications of Dataset Size. SwiftBrush's image-free approach allows for scalable training datasets without limitations. To explore the dataset's impact on SwiftBrush performance, we conducted supplementary experiments by augment- ing the dataset with an additional 2M prompts from the LAION dataset [47] to the original 1.5M deduplicated prompts from the JourneyDB dataset [36]. Analysis (Tab. 2) reveals improved performance with the expanded dataset. Specifically, this leads to a significant improvement in terms of FID and preci- sion, suggesting a positive correlation between dataset size and the quality of the generated outputs. However, a slight degradation in recall was observed, indicating a potential trade-off between image diversity and overall quality. Fur- thermore, despite an increase in CLIP score compared to the previous version, there remains room for improvements in terms of text alignment."}, {"title": "4.4 Resource-efficient training schemes", "content": "While our CLIP loss is highly beneficial, it comes with memory and computa- tion costs. Particularly, the CLIP image encoder can only work on image space, requiring decoding the predicted latent to image via the image decoder D as can be seen in Eq. (5). We find incorporating CLIP loss into SwiftBrush's full-model distillation significantly slows down training speed, particularly on GPUs with moderate VRAM. This urges us to design a resource-efficient training scheme to fully exploit the proposed CLIP loss in constrained setting.\nIt is possible to significantly reduce memory requirements during fine-tuning with the LORA framework [17], where only a set of small-rank parameters are trained. Also, to compute the CLIP loss, the predicted latent goes through a large VAE decoder, increasing training length and memory consumption. To address this, we integrate TinyVAE [4], a compact variant of Stable Diffusion's VAE. TinyVAE sacrifices some fine detail in images but preserves overall structure and object identity comparable to the original VAE. This approach maintains training efficiency close to those of the original fully fine-tuned model, as shown in Tab. 6 and Sec. 5.3."}, {"title": "4.5 Post-training improvements", "content": "Recent literature [25] has shown a growing interest in model fusion techniques, aiming to integrate models performing distinct subtasks into a unified multi- task model [21, 53] or combining fine-tuned iterations to create an enhanced version [10,19,50]. Our research focuses on the latter, particularly in the context of one-step text-to-image diffusion models. These models, although designed for the same task, differ in their training objectives, providing each with unique advantages. By merging these models, we aim to create a new model that cap- tures the strength of each model without increasing model size or inference costs.\nGiven two one-step diffusion models with weights $A$ and $B$ and an interpola- tion weight $\u03bb$, we merge them using a simple linear interpolation of the weights:\n$\\Theta_{merged} = \\lambda \\Theta_A + (1 - \\lambda)\\Theta_B$. (6)\nWe empirically demonstrate the benefit of such interpolation scheme with SD Turbo, known for its precision and strong text alignment, and the original Swift- Brush, which excels in diversity. In our empirical analysis (refer to Fig. 3), we observe that by interpolating from one model to the other, all evaluated metrics (except for the CLIP score) show improvement at some optimal point. This in- dicates that the fused model could potentially outperform the original models. These findings underscore the potential of model fusion techniques in enhancing model efficacy, as evidenced by the metric analysis.\nAs discussed in Sec. 4.4, our proposal suggests two training schemes. We can either train the student model with LoRA and TinyVAE utilizing VSD and CLIP losses or fully finetune the student model employing only VSD loss. These two training schemes lead to two resulting one-step models with different behaviors, making them ideal ingredients for merging. By merging these models, we obtain the final model output of our proposed Swift Brush v2 framework."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Setup", "content": "Evaluation metrics. Our text-to-image model is evaluated using the \"zero- shot\" setting, i.e., trained on some datasets and tested on another dataset, undergoing comprehensive evaluation across three key aspects: image quality, diversity, and textual fidelity. We use the Fr\u00e9chet Inception Distance (FID) [15] on resized 256 \u00d7 256 images as our primary metric for evaluating image quality, consistent with prior text-to-image research [23]. In addition, we employ preci- sion [24] as a complementary metric to FID. For evaluating diversity, we rely on the recall metric [24]. Textual alignment is measured using the CLIP [39] score and the Human Preference Score v2 [51] (HPSv2).\nDatasets. We utilize two training datasets: (1) 1.3M prompts from JourneyDB [36], and (2) an expanded dataset incorporating 2M prompts from LAI\u039f\u039d [47], totaling 3.3M prompts. Additionally, a human-feedback dataset, comprising 200K pairs from LAION-Aesthetic-6.25+ [47], can be optionally used for further image regularization [54], which is around 5% of the total training data. We use the MS COCO-2014 validation set as the standard zero-shot text-to-image benchmark, consistent with established practices in the field [13,23,30,32,45,46]. Samples are generated from the first 30k prompts, with the entire dataset serv- ing as the reference for obtaining metrics. For HPSv2, we adopt the evaluation protocol from [51].\nTraining details. Our method is built on top of SwiftBrush [34] with our proposed modifications. We conduct all our training on four NVIDIA A100 40GB GPUs, with training durations of one or three days depending on the dataset (JourneyDB alone or combined with LAION prompts). The batch size is 16 per GPU, and we use learning rates of 1e-6 and 1e-3 for the student and LoRA teacher, respectively, with the AdamW optimizer [31]. Our approach utilizes Stable Diffusion 2.1 [41] with cfg = 4.5 as the frozen teacher and LoRA teacher initialized with rank r = 64 and scaling \u03b3 = 128. As for the LoRA student, we set r = 256 and \u03b3 = 512 to enhance its learning capacity. In addition, we introduce the clamped CLIP loss with a margin of \u03c4 = 0.35, starting with a weight of 0.1 and gradually reducing to zero. We use ViT-B/32 [18] as the backbone for CLIP image and text feature extraction. Finally, we merge two final models with \u03bb = 0.5, further details are available in the Appendix."}, {"title": "5.2 Comparison with Prior Approaches", "content": "Quantitative results. Tab. 3 presents a comprehensive quantitative compar- ison between our approach and prior text-to-image models. This encompasses GAN-based models (group 1), multi-step diffusion models (group 2), and a va- riety of distillation techniques (group 3), both with and without image super- vision. Our approach outperforms all competitors, notably achieving superior results even without direct image regularization. Remarkably, our distilled stu- dent models exceed their teacher model, SDv2.1, in FID scores by a significant margin while maintaining equivalent model size and inference times comparable to SD Turbo or SwiftBrush. Our model effectively addresses previous text align- ment issues observed in SwiftBrush, being close to the CLIP scores of SD Turbo and multi-step models. Precision metrics show high-realism akin to the reference dataset, enhanced solely with a text-driven training dataset. Notably, ours ex- hibits significant recall improvements due to its image-free nature. Image-based regularization further improves student quality with a small reduction in recall.\nIn terms of HPSv2 (Tab. 4), our approach achieves competitive scores com- pared to the multi-step teacher model SDv2.1 and other distillation methods. Particularly, our model with image regularization achieves the highest HPSv2 scores for photos and remains close to the top performers in other categories. Qualitative results. We provide a qualitative comparison in Fig. 5. Our model produces higher quality compared to its teachers and one-step counterparts. In"}, {"title": "5.3 Ablation Studies", "content": "Effect of each proposed component is summarized in Tab. 5. We compare two student training schemes: (1) full model training and (2) efficient model training. Initializing the model from SD Turbo [46] significantly improves the FID in both schemes. Adding prompts from LAION [47] leads to notable FID"}, {"title": "6 Discussion and Conclusion", "content": "Limitations: Despite the promising results, our distilled model still inherits some limitations from the teacher model, such as compositional problems. To address these limitations, future work could explore the integration of auxiliary losses that focus on cross-attention mechanisms during the distillation process.\nSocietal Impact: Our advancements improve high-quality image synthesis speed and accessibility. However, misuse of our advancements could spread misinfor- mation and manipulate public perception. Thus, responsible use and safeguards are crucial to ensure that the benefits outweigh the risks.\nConclusion: This paper proposes a novel method to enhance Swift Brush, a one- step text-to-image diffusion models. We address the quality-diversity trade-off by initializing the SwiftBrush student model with SD Turbo's pretrained weights and incorporating efficient in-training techniques as well as margin CLIP loss and large-scale dataset training. Also, by weight merging and optional image regularization, we achieve an outstanding FID score of 8.14, surpassing exist- ing approaches in both GAN-based and one-step diffusion-based text-to-image generation while maintaining near real-time inference speed."}, {"title": "7 Additional details", "content": ""}, {"title": "7.1 Weight Interpolation", "content": "In this section, we first provide quantitative analyses on the model merging pro- cess conducted on the fully finetuned (Model A) and the resource-efficient trained model (Model B) to form our final model. We run a comprehensive evaluation by reporting essential metrics, including FID, CLIP score, Precision, and Re- call, upon the zero-shot MS-COCO 2014 across different interpolation weights, following the same protocol as in the main paper. We provide the plots in both scenarios when the regularization term is applied (Fig. 7.b) or not (Fig. 7.a). In either case, we observe that the CLIP and the precision scores change mono- tonically from one model to another, while both the FID and recall scores get enhanced when fusing the two models. This analysis provides a data-driven jus- tification for the selected interpolation weight used in the final model, ensuring it achieves the best combination of visual quality, semantic coherence, and di- versity. Specifically, for both cases, we pick the weight to optimize for the FID metrics, while not trading off too much with other metrics, hence the interpola- tion weight \u03bb = 0.5 serve well with our purposes.\nFurthermore, we delve into the visual analysis of model interpolation (Fig. 8), exploring the effects on the generated output as the interpolation weight is var- ied between two trained one-step text-to-image models. We provide qualitative figures for both the interpolation between SwiftBrush and SD Turbo (analyzed"}, {"title": "7.2 CLIP loss", "content": "Fig. 9 presents a qualitative comparison between the naive approach of inte- grating the CLIP loss and our proposed method. The output of the distilled model using the naive approach suffers from poor quality, with issues such as over-saturation, over-smoothing, and the appearance of textual artifacts on the image that reflect the conditioned prompt. These artifacts can be visually dis-"}, {"title": "7.3 Dependency on the existing one-step diffusion.", "content": "Our work aims to enhance the one-step diffusion models' performance. When no pretrained one-step model is available, we can still run the SwiftBrush (SB) training procedure on a small prompt dataset to build that initial model. To validate SBv2's effectiveness in that scenario, we re-train our method but using SB pretrained weights for initialization and report the results in Tab. 7. As shown, our final merged model obtains the FID score of 11.69, approaching the state-of-the-art score from DMD (11.49). Note that DMD achieves that score with regularization on real images. Due to the rebuttal's time limit, we could not produce our results with the regularization loss. However, we expect the regularization could further reduce our FID by at least 0.5 points, based on observations from SBv2 models in the main paper. This result demonstrates that SBv2 still can reach SoTA one-step performance w/o the help of existing one-step models, though the gain is not as significant. Even when counting SB training"}, {"title": "7.4 Compare to SD Turbo training with SB initialization.", "content": "Three reasons make this training scheme less favorable than our approach: (1) SD Turbo training needs a large text-image pair dataset, which is costly in stor- age and computation, while SB training is image-free, making it more efficient and scalable. (2) SD Turbo's training code isn't publicly available, making repro- duction difficult, whereas SB's training code, though also unavailable, is easier to reimplement using VSD loss. (3) SD Turbo's adversarial training is prone to mode collapse, requiring careful monitoring. Our attempts to reimplement SD Turbo faced mode collapse issues in early epochs (Fig. 10)."}, {"title": "7.5 Training Cost and Inference Speed", "content": "Setup. All of the self-measurements are taken on NVIDIA A100 40GB GPUs. However, most of the reported numbers about training time are taken directly from the corresponding papers, which did not specify whether A100 40GB or A100 80GB GPUs were used during training, except for SwiftBrush families. Even though both of these GPUs have equivalent computational speed, the 80GB version is capable of larger training batch size, which can drastically improve the training time. For inference time, we re-run every method except for GigaGAN due to its public model unavailability. We follow a standardized procedure to ensure fair comparisons for inference times. First, we warm up the model by running 5 inference passes. Then, we perform inference 50 times, repeating this process 10 times and taking the average of the results. The inference flow for distilled one-step diffusion models consists of three main steps: text encoding,"}, {"title": "8 Analysis and further applications", "content": ""}, {"title": "8.1 About the robustness of the training scheme", "content": "We further demonstrate the robustness and flexibility of our distillation scheme, as stated in the main paper, by incorporating additional loss functions. Observing that the student can still improve upon the human preference aspect in the HPS benchmark, we integrate an HPSv2 loss, similar to the integration of the CLIP"}, {"title": "8.2 Composition", "content": "In Fig. 12, similar to other diffusion models, our distilled model still demon- strates low compositional ability and text-image misalignment when tasked with prompts that require generating multiple objects associated with attributes. Our model can generate the purple frog; however, it fails to generate the ball as well as its color. There have been a number of works aimed at solving this issue by running attention guidance [12, 16] or enhancing attention masks [1,8,26]. In our experiments, we chose to apply the Divide-and-Bind [26] approach since this method enhances both the generation of the objects and their corresponding at- tributes. Since the model predicts the final image in only one step, we choose to iteratively update the latent 100 times while keeping the same scale size of 20 as the original paper. As illustrated in the final row of Fig. 12, our model demon- strates the ability to generate following the prompt accurately. The technique, initially developed for multi-step diffusion models, has been successfully adapted to enhance the output quality of our one-step model with minimal modifications. This achievement highlights the remarkable versatility and compatibility of our model with existing techniques commonly associated with text-to-image diffusion models. Although the original optimization process was designed for a multi-step approach, resulting in slower running times, our model demonstrates faster per- formance compared to the original multi-step teacher model. Moving forward, we believe that exploring novel latent optimization methods tailored explicitly for one-step diffusion models presents a promising new research direction, and we hope to draw attention to this area in the future."}, {"title": "8.3 Latent Interpolation", "content": "By interpolating between two random input noises via spherical linear interpo- lation (slerp) in Fig. 13, our model seamlessly captures the gradual transforma-"}, {"title": "8.4 Prompt Interpolation", "content": "We showcase our model's capability when interpolating two input prompt em- beddings in Fig. 14 using same template but with only one word different. Our model effectively captures and blends the characteristics of both prompts, creat- ing visually compelling and coherent intermediate representations. This feature demonstrates our model's capacity to understand and manipulate the semantic relationships between different textual inputs, providing a powerful tool for cre- ative exploration and generating diverse images that seamlessly bridge different concepts or styles."}, {"title": "8.5 Arbitrary size and aspect ratio generation with ScaleCrafter", "content": "Our models inherit the architecture of SDv2.1, which limits the ability to gen- erate images with varying sizes or aspect ratios, unlike other works such as SD-XL [27,37]. To address this limitation, we apply ScaleCrafter [14], a tech- nique that adjusts the convolution layers of the U-Net model during inference through re-dilation. Fig. 15 illustrates the synthesized images generated in vari- ous ratios and resolutions using this method. This once again demonstrates that our one-step model can effectively incorporate techniques from the multi-step diffusion model domain to enhance performance, similar to the application of Divide-and-Bind for improving composition, as discussed in Sec. 8.2."}, {"title": "9 More qualitative results", "content": "We provide additional comparisons of our model with SD Turbo, SDv2.1, and SwiftBrush in Fig. 16. Fig. 17 illustrates the diversity of our model when generat- ing images with the same input prompts. Finally, Fig. 18 shows more uncurated samples synthesized by our model."}]}