{"title": "Swift Brush v2: Make Your One-step Diffusion Model Better Than Its Teacher", "authors": ["Trung Dao", "Thuan Hoang Nguyen", "Thanh Le", "Duc Vu", "Khoi Nguyen", "Cuong Pham", "Anh Tran"], "abstract": "In this paper, we aim to enhance the performance of Swift- Brush, a prominent one-step text-to-image diffusion model, to be com- petitive with its multi-step Stable Diffusion counterpart. Initially, we ex- plore the quality-diversity trade-off between SwiftBrush and SD Turbo: the former excels in image diversity, while the latter excels in image quality. This observation motivates our proposed modifications in the training methodology, including better weight initialization and efficient LORA training. Moreover, our introduction of a novel clamped CLIP loss enhances image-text alignment and results in improved image quality. Remarkably, by combining the weights of models trained with efficient LORA and full training, we achieve a new state-of-the-art one-step diffu- sion model, achieving an FID of 8.14 and surpassing all GAN-based and multi-step Stable Diffusion models. The evaluation code is available at: https://github.com/vinairesearch/swiftbrushv2", "sections": [{"title": "1 Introduction", "content": "Text-to-image generation has experienced tremendous growth in recent years, allowing users to create high-quality images from simple descriptions. State-of- the-art models [3, 33, 40, 42] could surpass humans in art competition [43] or produce synthetic images nearly indistinguishable from real ones [7]. Among popular text-to-image networks, Stable Diffusion (SD) models [41, 42] are the most widely used due to their open-source accessibility. However, most SD mod- els are designed as multi-step diffusion models, which require multiple forwarding steps to produce an output image. Such a slow and computationally expensive mechanism hinders the use of these models in real-time or on-device applications. Recently, many works have tried to reduce the denoising steps required in text-to-image diffusion models. Notably, few recent studies have successfully de- veloped one-step diffusion models, thus significantly speed up the image generation. While early attempts [13, 32] produce blurry and malformed photos, subsequent methods produce sharp and high-quality outputs. These methods mainly distill knowledge from a pre-trained multi-step SD model (referred to as"}, {"title": "2 Related Work", "content": "Text-to-image generation involves synthesizing high-quality images based on input text prompts. This task has evolved over decades, transitioning from constrained domains like CUB [48] and COCO [28] to general domains such as LAION-5B [47]. This evolution is driven by the emergence of large vision- language models (VLMs) like CLIP [39] and ALIGN [20]. Leveraging these"}, {"title": "2.1 Text-to-Image Generation", "content": "Text-to-image generation involves synthesizing high-quality images based on input text prompts. This task has evolved over decades, transitioning from constrained domains like CUB [48] and COCO [28] to general domains such as LAION-5B [47]. This evolution is driven by the emergence of large vision- language models (VLMs) like CLIP [39] and ALIGN [20]. Leveraging these"}, {"title": "2.2 Accelerating Text-to-Image Diffusion Models", "content": "Efforts to accelerate diffusion model sampling include faster samplers and distil- lation techniques. Early methods reduce sampling steps to as few as 4-8 steps by incorporating Latent Consistency Models to distill latent diffusion models [9, 32]. Recent studies have achieved one-step text-to-image generation by training a student model distilled from a pretrained multi-step diffusion model, employing various techniques such as Rectified Flows [30], reconstruction and distribution matching losses [54], and adversarial objectives [27, 46, 52, 57]. However, the out- put images often exhibit blurriness and artifacts, and one-step methods still underperform compared to multi-step models while requiring large-scale text- image pairs for training. Differentiating itself from the rest, SwiftBrush [34] proposed a one-step dis- tillation technique that required only training on prompt inputs. The method gradually transfers knowledge from the teacher to the one-step student through an intermediate LoRA multi-step teacher. SwiftBrush's image-free training pro- cedure offers a simple way to scale training data and extend the student model capability via auxiliary losses, which are not constrained by limited-size imagery training data. Therefore, while Swift Brush also falls short in quality compared to the teacher model, we find its high potential for further development to produce a one-step student that even beat the multi-step teacher at its own game."}, {"title": "3 Background", "content": "Diffusion Models are generative models that transform a noise distribution into a target data distribution by simulating the diffusion process. This trans- formation involves gradually adding noise e~ N(0, I) to clean image xo over a series of T steps (forward process) and then learning to reverse this process (reverse process). The forward process can be formulated as:\nXt = at\u03a7\u03bf + \u03c3\u03c4\u03b5 t \u2208 0,T\nwhere xt is the data at time step t and {(at, \u03c3\u03c4)}{=1 is the noise schedule such that (\u03b1\u03c4, \u03c3\u03c4) = (0,1) and (\u03b1\u03bf,\u03c3\u03bf) = (1,0). On the other hand, the reverse process aims to reconstruct the original data from noise. Training involves mini- mizing the difference between predicted output from model e parameterized by"}, {"title": "4 Proposed Methods", "content": "In this section, we begin by conducting an in-depth analysis of the quality- diversity trade-off in representative diffusion-based text-to-image models (Sec. 4.1). Subsequently, we discuss our strategy for incorporating the strengths of Swift- Brush and SD Turbo (Sec. 4.2). Lastly, we explore various approaches to en- hance the distillation process and post-training procedures (Sec. 4.3 to 4.5). An overview of our methodologies is presented in Fig. 2."}, {"title": "4.1 Quality-Diversity Trade-off in Existing Models", "content": "We first analyze the properties of the teacher model, SDv2.1, and existing one- step diffusion-based text-to-image models. For the teacher model, we assess its performance across different guidance scales. Here, we select SwiftBrush and SD Turbo due to their quality and distinct training procedures. SwiftBrush relies solely on score distillation from the teacher in its image-free training, while SD Turbo trains on real images with adversarial and distillation loss. We conduct our analysis on the COCO 2014 benchmark and report relevant metrics in Tab. 1. When assessing the multi-step teacher's performance, the classifier-free guid- ance scale (cfg) plays a crucial role. A low cfg (e.g., cfg = 2) yields a low FID score of 9.64, driven by high output diversity (recall = 0.53). However, this set- ting results in weak alignment between images and prompts (CLIP score = 0.30) and lower image quality (low precision). Conversely, a large cfg (e.g., cfg = 7.5) markedly improves text-image alignment (CLIP score = 0.33) but restricts diver- sity (recall = 0.36), resulting in a poor FID score of 15.93. Moderate cfg values (e.g., cfg = 4.5) strike a better balance, offering the highest precision score. When evaluating the one-step students, we notice distinct behaviors. SD Turbo, benefiting from adversarial training on real images, yields highly natu- ralistic outputs with an exceptionally high precision score, surpassing even those of the multi-step teacher. However, this results in poor diversity, reflected in a low recall of 0.35. Conversely, SwiftBrush adopts an image-free training approach, allowing flexible combinations of random-noise latents and input prompts. Such a relaxed supervision enables the student model to generate more diverse outputs but at the expense of quality (Tab. 1). We further verify this finding by a quali- tative evaluation, illustrated in Fig. 1. When given identical input prompts, SD Turbo generates realistic yet similar outputs. In contrast, SwiftBrush produces a wider range of outcomes, albeit with greatly distorted artifacts. Regardless, both one-step models exhibit FID scores around 15-16, significantly higher than the teacher's best score. Observing a quality-diversity trade-off in existing one-step diffusion model such as SD Turbo and SwiftBrush, we aim to combine these two to leverage the strengths of both."}, {"title": "4.2 SwiftBrush and SD Turbo Integration", "content": "In this section, we explore strategies for effectively merging SwiftBrush and SD Turbo to enhance the quality-diversity trade-off. A direct approach is unify- ing their training procedure, i.e., combining adversarial training from SD Turbo and Variational Score Distillation from SwiftBrush. However, this simplistic ap- proach proves challenging due to computational demands and potential failure. While Swift Brush's image-free procedure is easier to implement, reproducing SD Turbo's training process is complex and resource-intensive. The presence of the discriminator complicates the training, necessitating significant VRAM and dataset requirements. Additionally, SD Turbo's intense supervision may con- strain SwiftBrush's loose guidance, limiting output diversity. Based on our discussions, we opt not to utilize SD Turbo's adversarial train- ing. Instead, we leverage its pretrained weights to initialize the student network within SwiftBrush's training framework. This straightforward approach proves highly effective. As can be seen in the comparison between the second and the third row in Tab. 2, the resulting model has improved FID and recall. By em- ploying SD Turbo's pretrained weights, we provide a solid foundation for the training model to maintain high-quality outputs, while SwiftBrush's image-free training process gradually enhances generation diversity."}, {"title": "4.3 In-training Improvements", "content": "Besides data efficiency and diversity promotion, Swift Brush's image-free training still has room for improvement. First, it allows an easy means to scale up training data by collecting more prompt inputs. This task is simple, given the abundance of textual datasets and the availability of large language models, unlike the costly and labor-intensive task of collecting image-text pair data commonly required. Second, by not forcing the output of the student model to be the same as that of the teacher, SwiftBrush allows the student to go even beyond the quality and capability of the teacher. We can advocate it to happen by adding extra auxiliary loss functions in SwiftBrush training. In this section, we will discuss the implementation of those ideas for improvement. Implications of Dataset Size. SwiftBrush's image-free approach allows for scalable training datasets without limitations. To explore the dataset's impact on SwiftBrush performance, we conducted supplementary experiments by augment- ing the dataset with an additional 2M prompts from the LAION dataset [47] to the original 1.5M deduplicated prompts from the JourneyDB dataset [36]. Analysis (Tab. 2) reveals improved performance with the expanded dataset. Specifically, this leads to a significant improvement in terms of FID and preci- sion, suggesting a positive correlation between dataset size and the quality of the generated outputs. However, a slight degradation in recall was observed, indicating a potential trade-off between image diversity and overall quality. Fur- thermore, despite an increase in CLIP score compared to the previous version, there remains room for improvements in terms of text alignment."}, {"title": "Tackling the text-alignment problem.", "content": "To refine the coherence between tex- tual prompts and visual outputs, we integrate an additional CLIP loss within the distillation process. However, naively employing such loss between the stu- dent model's predictions and the original textual prompts poses challenges, as over-optimizing for the CLIP score potentially degrade image quality. We ob- served issues such as blurriness, increased color saturation, and the emergence of textual artifacts within the generated images. To address this, we propose clamping the CLIP value during training with ReLU activation. This aims to balance text alignment with preserving image quality, ensuring the model maintains visual integrity. Additionally, we introduce dynamic scheduling to control the influence of CLIP loss, gradually reducing its weight to zero by the end of distillation. This balanced approach integrates visual-textual alignment and image fidelity effectively. Our clamped CLIP loss is formulated as:\nLCLIP = max (0, \u315c \u2013 (Eimage (D (fe(z, y))), Etext(y))),\nwhere Eimage and Etext represent the CLIP image and text encoders, respectively. D is the VAE decoder used to map the latent back to the image. The term 7 in- troduces a threshold on the desired cosine similarity (\u00b7,\u00b7) between the image and text embeddings, preventing the model from overemphasizing textual alignment at the expense of image quality."}, {"title": "4.4 Resource-efficient training schemes", "content": "While our CLIP loss is highly beneficial, it comes with memory and computa- tion costs. Particularly, the CLIP image encoder can only work on image space, requiring decoding the predicted latent to image via the image decoder D as can be seen in Eq. (5). We find incorporating CLIP loss into SwiftBrush's full-model distillation significantly slows down training speed, particularly on GPUs with moderate VRAM. This urges us to design a resource-efficient training scheme to fully exploit the proposed CLIP loss in constrained setting. It is possible to significantly reduce memory requirements during fine-tuning with the LORA framework [17], where only a set of small-rank parameters are trained. Also, to compute the CLIP loss, the predicted latent goes through a large VAE decoder, increasing training length and memory consumption. To address this, we integrate TinyVAE [4], a compact variant of Stable Diffusion's VAE. TinyVAE sacrifices some fine detail in images but preserves overall structure and object identity comparable to the original VAE. This approach maintains training efficiency close to those of the original fully fine-tuned model, as shown in Tab. 6 and Sec. 5.3."}, {"title": "4.5 Post-training improvements", "content": "Recent literature [25] has shown a growing interest in model fusion techniques, aiming to integrate models performing distinct subtasks into a unified multi- task model [21, 53] or combining fine-tuned iterations to create an enhanced version [10,19,50]. Our research focuses on the latter, particularly in the context of one-step text-to-image diffusion models. These models, although designed for the same task, differ in their training objectives, providing each with unique advantages. By merging these models, we aim to create a new model that cap- tures the strength of each model without increasing model size or inference costs. Given two one-step diffusion models with weights A and B and an interpola- tion weight \u5165, we merge them using a simple linear interpolation of the weights:\nOmerged = 10 + (1 \u2212 1)0\u0432.\nWe empirically demonstrate the benefit of such interpolation scheme with SD Turbo, known for its precision and strong text alignment, and the original Swift- Brush, which excels in diversity. In our empirical analysis (refer to Fig. 3), we observe that by interpolating from one model to the other, all evaluated metrics (except for the CLIP score) show improvement at some optimal point. This in- dicates that the fused model could potentially outperform the original models. These findings underscore the potential of model fusion techniques in enhancing model efficacy, as evidenced by the metric analysis. As discussed in Sec. 4.4, our proposal suggests two training schemes. We can either train the student model with LoRA and TinyVAE utilizing VSD and CLIP losses or fully finetune the student model employing only VSD loss. These two training schemes lead to two resulting one-step models with different behaviors, making them ideal ingredients for merging. By merging these models, we obtain the final model output of our proposed Swift Brush v2 framework."}, {"title": "5 Experiments", "content": "Evaluation metrics. Our text-to-image model is evaluated using the \"zero- shot\" setting, i.e., trained on some datasets and tested on another dataset, undergoing comprehensive evaluation across three key aspects: image quality, diversity, and textual fidelity. We use the Fr\u00e9chet Inception Distance (FID) [15] on resized 256 \u00d7 256 images as our primary metric for evaluating image quality, consistent with prior text-to-image research [23]. In addition, we employ preci- sion [24] as a complementary metric to FID. For evaluating diversity, we rely on the recall metric [24]. Textual alignment is measured using the CLIP [39] score and the Human Preference Score v2 [51] (HPSv2). Datasets. We utilize two training datasets: (1) 1.3M prompts from JourneyDB [36], and (2) an expanded dataset incorporating 2M prompts from LAI\u039f\u039d [47], totaling 3.3M prompts. Additionally, a human-feedback dataset, comprising 200K pairs from LAION-Aesthetic-6.25+ [47], can be optionally used for further image regularization [54], which is around 5% of the total training data. We use the MS COCO-2014 validation set as the standard zero-shot text-to-image benchmark, consistent with established practices in the field [13,23,30,32,45,46]. Samples are generated from the first 30k prompts, with the entire dataset serv- ing as the reference for obtaining metrics. For HPSv2, we adopt the evaluation protocol from [51]. Training details. Our method is built on top of SwiftBrush [34] with our proposed modifications. We conduct all our training on four NVIDIA A100 40GB GPUs, with training durations of one or three days depending on the dataset (JourneyDB alone or combined with LAION prompts). The batch size is 16 per GPU, and we use learning rates of le-6 and le-3 for the student and LoRA teacher, respectively, with the AdamW optimizer [31]. Our approach utilizes Stable Diffusion 2.1 [41] with cfg = 4.5 as the frozen teacher and LoRA teacher initialized with rank r = 64 and scaling y = 128. As for the LoRA student, we set r = 256 and y = 512 to enhance its learning capacity. In addition, we introduce the clamped CLIP loss with a margin of T = 0.35, starting with a weight of 0.1 and gradually reducing to zero. We use ViT-B/32 [18] as the backbone for CLIP image and text feature extraction. Finally, we merge two final models with X = 0.5, further details are available in the Appendix."}, {"title": "5.1 Experimental Setup", "content": "Evaluation metrics. Our text-to-image model is evaluated using the \"zero- shot\" setting, i.e., trained on some datasets and tested on another dataset, undergoing comprehensive evaluation across three key aspects: image quality, diversity, and textual fidelity. We use the Fr\u00e9chet Inception Distance (FID) [15] on resized 256 \u00d7 256 images as our primary metric for evaluating image quality, consistent with prior text-to-image research [23]. In addition, we employ preci- sion [24] as a complementary metric to FID. For evaluating diversity, we rely on the recall metric [24]. Textual alignment is measured using the CLIP [39] score and the Human Preference Score v2 [51] (HPSv2). Datasets. We utilize two training datasets: (1) 1.3M prompts from JourneyDB [36], and (2) an expanded dataset incorporating 2M prompts from LAI\u039f\u039d [47], totaling 3.3M prompts. Additionally, a human-feedback dataset, comprising 200K pairs from LAION-Aesthetic-6.25+ [47], can be optionally used for further image regularization [54], which is around 5% of the total training data. We use the MS COCO-2014 validation set as the standard zero-shot text-to-image benchmark, consistent with established practices in the field [13,23,30,32,45,46]. Samples are generated from the first 30k prompts, with the entire dataset serv- ing as the reference for obtaining metrics. For HPSv2, we adopt the evaluation protocol from [51]. Training details. Our method is built on top of SwiftBrush [34] with our proposed modifications. We conduct all our training on four NVIDIA A100 40GB GPUs, with training durations of one or three days depending on the dataset (JourneyDB alone or combined with LAION prompts). The batch size is 16 per GPU, and we use learning rates of le-6 and le-3 for the student and LoRA teacher, respectively, with the AdamW optimizer [31]. Our approach utilizes Stable Diffusion 2.1 [41] with cfg = 4.5 as the frozen teacher and LoRA teacher initialized with rank r = 64 and scaling y = 128. As for the LoRA student, we set r = 256 and y = 512 to enhance its learning capacity. In addition, we introduce the clamped CLIP loss with a margin of T = 0.35, starting with a weight of 0.1 and gradually reducing to zero. We use ViT-B/32 [18] as the backbone for CLIP image and text feature extraction. Finally, we merge two final models with X = 0.5, further details are available in the Appendix."}, {"title": "5.2 Comparison with Prior Approaches", "content": "Quantitative results. Tab. 3 presents a comprehensive quantitative compar- ison between our approach and prior text-to-image models. This encompasses GAN-based models (group 1), multi-step diffusion models (group 2), and a va- riety of distillation techniques (group 3), both with and without image super- vision. Our approach outperforms all competitors, notably achieving superior results even without direct image regularization. Remarkably, our distilled stu- dent models exceed their teacher model, SDv2.1, in FID scores by a significant margin while maintaining equivalent model size and inference times comparable to SD Turbo or SwiftBrush. Our model effectively addresses previous text align- ment issues observed in SwiftBrush, being close to the CLIP scores of SD Turbo and multi-step models. Precision metrics show high-realism akin to the reference dataset, enhanced solely with a text-driven training dataset. Notably, ours ex- hibits significant recall improvements due to its image-free nature. Image-based regularization further improves student quality with a small reduction in recall. In terms of HPSv2 (Tab. 4), our approach achieves competitive scores com- pared to the multi-step teacher model SDv2.1 and other distillation methods. Particularly, our model with image regularization achieves the highest HPSv2 scores for photos and remains close to the top performers in other categories. Qualitative results. We provide a qualitative comparison in Fig. 5. Our model produces higher quality compared to its teachers and one-step counterparts. In"}, {"title": "5.3 Ablation Studies", "content": "Effect of each proposed component is summarized in Tab. 5. We compare two student training schemes: (1) full model training and (2) efficient model training. Initializing the model from SD Turbo [46] significantly improves the FID in both schemes. Adding prompts from LAION [47] leads to notable FID"}, {"title": "6 Discussion and Conclusion", "content": "Limitations: Despite the promising results, our distilled model still inherits some limitations from the teacher model, such as compositional problems. To address these limitations, future work could explore the integration of auxiliary losses that focus on cross-attention mechanisms during the distillation process. Societal Impact: Our advancements improve high-quality image synthesis speed and accessibility. However, misuse of our advancements could spread misinfor- mation and manipulate public perception. Thus, responsible use and safeguards are crucial to ensure that the benefits outweigh the risks. Conclusion: This paper proposes a novel method to enhance Swift Brush, a one- step text-to-image diffusion models. We address the quality-diversity trade-off by initializing the SwiftBrush student model with SD Turbo's pretrained weights and incorporating efficient in-training techniques as well as margin CLIP loss and large-scale dataset training. Also, by weight merging and optional image regularization, we achieve an outstanding FID score of 8.14, surpassing exist- ing approaches in both GAN-based and one-step diffusion-based text-to-image generation while maintaining near real-time inference speed."}]}