{"title": "Personalisation via Dynamic Policy Fusion", "authors": ["Ajsal Shereef Palattuparambil", "Thommen George Karimpanal", "Santu Rana"], "abstract": "Deep reinforcement learning (RL) policies, although optimal in terms of task rewards, may not align with the personal preferences of human users. To ensure this alignment, a na\u00efve solution would be to re-train the agent using a reward function that encodes the user's specific preferences. However, such a reward function is typically not readily available, and as such, retraining the agent from scratch can be prohibitively expensive. We propose a more practical approach to adapt the already trained policy to user-specific needs with the help of human feedback. To this end, we infer the user's intent through trajectory-level feedback and combine it with the trained task policy via a theoretically grounded dynamic policy fusion approach. As our approach collects human feedback on the very same trajectories used to learn the task policy, it does not require any additional interactions with the environment, making it a zero-shot approach. We empirically demonstrate in a number of environments that our proposed dynamic policy fusion approach consistently achieves the intended task while simultaneously adhering to user-specific needs.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has demonstrated its effectiveness in various real- world scenarios, including computer games [16], inventory management, au- tonomous driving, robotics [8], healthcare [30], recommendation systems [25] etc. In RL, the learning agent typically learns to maximise its reward by learn- ing an optimal task policy through interactions with the environment. However, in certain scenarios, a user may desire a policy that subtly deviates from this optimal policy to accommodate their personal preference or style. For example, in a navigation setting, users may want to avoid toll roads, or they may prefer to drive along a scenic coastal route, even if this translates to slightly longer travel times compared to the route determined to be the optimal one by an RL agent trained to minimise travel time.\n\nA simple solution to accommodate such preferences in a policy would be to retrain an RL agent from scratch, using a reward function that takes users'"}, {"title": "2 Related Work", "content": "Learning from human feedback is of particular interest in the RL community as it leverages human knowledge during the learning process, offering several benefits. Firstly, it improves the efficiency of the system in terms of sample requirements as well as overall performance [6]. Secondly, leveraging human feedback has been shown to enable RL agents to solve complex tasks that are otherwise challenging to manually specify through conventional reward functions [12,5]. Consequently, there exist a number of approaches aimed at leveraging human feedback for learning.\n\nA number of works in interactive reinforcement learning [28,14] examine how agents can learn from state-action level human feedback. However, such methods are limited, as providing state-action-level feedback is non-intuitive and cogni- tively taxing. Preference-based reinforcement learning [5,12], uses trajectory- level human preference feedback, using which a corresponding reward function is learned. However, following the learning of the reward function, this approach may require the agent to interact with the environment again to learn the corre- sponding policy. As our present work focuses on developing a zero-shot approach, to estimate human intent, we instead use RUDDER [1], an LSTM-based ap- proach for credit assignment by directly approximating the Q-values from agent trajectories. These Q-values are then translated into the required intent-specific policy. There are more recent work addressing credit assignment such as [19,31], but we resort to RUDDER for our work due to it's simplicity.\n\nIn the context of policy fusion, a closely related work by Sestini et al. [24] combines policies with different fusion methods. However, the described policy fusion methods are static and may result in the dominance of one of the con- stituent policies. Policy fusion has been explored previously in Haarnoja et al. [7] and Hunt et al. [11]. Such works study the concept of learning different poli- cies independently using soft Q-functions, each with its own reward function. These individual policies are later combined, leading to the emergence of new behaviours in robotic manipulations. Our work views policy fusion through a lens of personalisation, requiring the fusion to satisfy certain constraints. In ad- dition, unlike existing policy fusion methods, our dynamic policy fusion approach prevents the over-dominance of the component policies."}, {"title": "3 Preliminaries and Background", "content": "We briefly survey some of the related backgrounds that form the basis for our work. We refer to the policy trained to solve the task as the task-specific policy (\u03c0\u03c6) and the policy that captures human intent as the intent-specific policy(\u03c0\u03c8)."}, {"title": "3.1 Reinforcement Learning", "content": "The policies considered in this work are learned by solving tasks represented as individual MDPs [21] M = (S, A, P, R, \u03b3) where S, and A are the state and action spaces. P is the transition function that captures the transition dynam- ics of the environment. R is the reward function and y is the discount factor. In each task, R can vary, while the remaining components stay the same. At timestep t, the agent in state st \u2208 S takes an action at \u2208 A and obtains a reward R(st+1, St, at) and moves to state st+1 according to the transition func- tion P(st+1|St, at). A policy \u03c0(\u03b1t|st) outputs the probability of taking an action at from a given state st. The episodic discounted return is Gt = \u2211t=1 \u03b3tort+1, where y specifies how much the future reward is discounted and T is the total number of timesteps. The agent's objective is to maximise the future expected reward E[Gt] by learning a Q-function Q(st, at), which estimates the expected cumulative reward."}, {"title": "3.2 RUDDER", "content": "RUDDER [1] addresses the problem of credit assignment and learning from sparse rewards. In RUDDER, an LSTM network analyses the entire trajectory"}, {"title": "4 Methodology", "content": "We construct our proposed zero-shot approach for personalisation by first infer- ring the intent-specific policy via trajectory-level human feedback using RUD- DER, followed by a dynamic policy fusion mechanism that automatically main- tains a balance between the inferred policy and a trained task-specific policy.\n\nWe assume that the trajectory data used to train task-specific policy is ac- cessible, and reusing this data allows us to perform zero-shot personalisation i.e., without collecting new environment interactions. A subset of this data is sam- pled with personalised human feedback scores with more desirable trajectories assigned higher scores. We then infer the human intent (intent-specific policy) us- ing RUDDER (Section 3.2) and dynamically fuse it with the task-specific policy. The overview of our method is illustrated in Figure 1. The subsequent sections provide in-depth details of the components of our personalisation approach."}, {"title": "4.1 Learning Human Intent using LSTM", "content": "To learn the intent-specific policy, as previously described, we leverage the LSTM-based approach of RUDDER. We train this LSTM using human trajectory- level feedback on the same training data used to learn the task-specific policy. Hence, no additional interaction data is required. To get the human feedback, we simulated the human. The feedback score is the number of times the human preference is met within a trajectory (Refer to supplementary Section A for more details).\n\nAt each time step, the state-action vector is fed into the LSTM units. In the case of image inputs, we pre-train a Variational Autoencoder (VAE) to reduce the dimensionality of the state. We use FiLM [20] to modulate the state vector with the action vector. In [20], a FiLM network conditions the feature map of the neural network depending on another input signal. Here, we condition the state feature with the action vector and train the LSTM using the human feedback"}, {"title": "4.2 Policy Construction", "content": "We use a DQN parameterised by o to learn the task and it produces the Q-values (Q) for each state-action pair. Similarly, the LSTM parameterised by also produces the Q-values (Q') corresponding to the human intent. We choose the Boltzmann distribution to convert the Q-values into a policy. Therefore \u03c0\u03c6 is the task-specific policy and \u03c0\u03c8 is the intent-specific policy. This choice is motivated by the fact that if the intersection of the support of two policies is an empty set, their product would result in a random policy. By contrast, the Boltzmann distribution assigns at least a small probability to all actions, ensuring that the support of the distribution covers the entire action space.\n\n\u03c0\u03c6(a|st) = \\frac{exp(\\frac{Q(s,a)}{T_{\\phi}})}{\\Sigma_{a \\in A} exp (\\frac{Q(s,a)}{T_{\\phi}})} (2)\n\n\u03c0\u03c8(a|st) = \\frac{exp(\\frac{Q'(s,a)}{T_{\\psi}})}{\\Sigma_{a \\in A} exp (\\frac{Q'(s,a)}{T_{\\psi}})} (3)\n\nwhere T\u03c6 is the temperature corresponding to task-specific policy and T\u03c8 is the temperature corresponding to intent-specific policy. With policies constructed from Q-values, these constituent policies are fused together to produce the per- sonalised policy"}, {"title": "4.3 Policy Fusion", "content": "Policy fusion is a process of combining two or more policies to produce a new policy. Sestini et al. [24] discussed several approaches through which policy fu- sion can be performed. We also aim to obtain a personalised policy by fusing the intent-specific and task-specific policies. However, policy fusion in the context of personalisation should satisfy two constraints. Firstly, fused policy should be identical to the task-specific policy if the task-specific and intent-specific poli- cies are identical to each other - a property that we refer to as the invariability constraint. This constraint is crucial because it ensures that when the objectives of the task and the human are aligned, the personalised policy remains con- sistent and does not deviate during the fusion of identical constituent policies. Correspondingly, we define invariant policies as follows:\n\nDefinition 1 (Invariant policies). Two policies \u03c0\u2081(a|s) and \u03c0\u2082(a|s) are said to be invariant policies if KL (\u03c0\u2081(a|s)||\u03c0\u2082(a|s)) = 0 \u2200 s \u2208 S."}, {"title": "4.4 Pitfalls of Static Fusion", "content": "With the static policy fusion technique described in Section 4.3, a potential chal- lenge arises wherein one of the component policies over-dominates the other. Consequently, the agent may disproportionately exhibit the corresponding be- haviour, resulting in noncompliance with either the task-specific policy or the intent-specific policy.\n\nTo illustrate this phenomenon, we consider a 2D Navigation scenario where an agent is tasked with reaching a target location. However, a human may wish for some checkpoint state, different from the target state, to be visited before the agent reaches its target. In this case, the task-specific policy corresponds to the actions along the shortest path towards the target, and an intent-specific policy would correspond to one inferred from human feedback, which exclusively favours visiting the checkpoint state. A static policy fusion approach in this case could lead to over-dominance of one of the component policies. For example, over-dominance of the intent-specific policy would lead to the agent visiting the checkpoint state indefinitely. This motivates the need for fusing the policies in a dynamic fashion, such that the agent respects the human's preferences, while also simultaneously completing the task at hand. We develop such a dynamic fusion technique to control the relative dominance of the individual policies by modulating the temperature parameter T\u03c8 of the intent-specific policy. We now describe the details of our dynamic fusion strategy."}, {"title": "4.5 Dynamically Modulating Policy Dominance", "content": "To mitigate unintended policy dominance, we adopt the idea that when the temperature parameter T\u03c8 in Equation (3) is increased, the probability distri- bution of actions tends to become uniform, thereby reducing the influence of the intent-specific policy on the personalised policy. We therefore modulate T depending on whether the fused personalised policy exhibits over-adherence or under-adherence to the intent-specific policy \u03c0\u03c8.\n\nWhen to increase T\u03c8? T\u03c8 should increase if the intent-specific policy \u03c0\u03c8 is en- forced too strongly, which is characterised by high accumulated human-induced rewards. We design T\u03c8 to increase when the accumulated human-induced reward surpasses an accumulated reward threshold \u03b7.\n\nHowever, we note that human intent can be specified in various modes: pref- erence (where a state is preferred over others), avoidance (where the preference is to avoid a particular state) or mixed (where the preference is to avoid certain states and to prefer certain others). In avoidance cases, human feedback assigns"}, {"title": "6 Limitations and Future Work", "content": "Even though our proposed dynamic policy fusion approach dynamically opti- mises two objectives, in its current state, it is restricted to discrete action spaces. Extension to continuous action domains could be explored in future work. Addi- tionally, the choice of hyper-parameters is done using trial and error; reducing or learning the number of parameters will enhance the utility of the method. Our method does not capture the varying preferences of the human. To address this limitation, future research could explore adaptive methods, perhaps based on meta-RL [3] to adjust to changing human preferences with a few shot learning of the LSTM network. The experiments presented in this work also make use of a noise-free ground truth function to simulate human feedback. We acknowledge that in reality, human feedback may be subject to noise, which could affect the robustness of our presented approach. Further research is necessary to adapt our approach to be more robust to noise, the performance of which could then be evaluated with noisy feedback from real humans. Another practical aspect to consider is that the number of trajectories one could expect a human to label is limited. From an initial investigation, our approach does exhibit a drop in performance as fewer labelled trajectories become available. Although this prob- lem could partially be mitigated through the use of semi-supervised methods [29] to automatically generate trajectory score labels, we recognise that more noise-tolerant and feedback-efficient variants of our method need to be explored. Nevertheless, we believe the proposed dynamic policy fusion method presents a novel and an in-principle, practical approach for adapting and personalising already trained policies."}, {"title": "7 Conclusion", "content": "We proposed a novel approach to personalise a trained policy to respect human- specified preferences using trajectory-level human feedback, without any addi- tional interactions with the environment. Using an LSTM-based approach to infer human intent, we designed a theoretically grounded dynamic policy fusion method to ensure the resulting policy completes a given task while also respect- ing human preferences. We empirically evaluated our approach on the Highway, 2D Navigation and Pong environments and demonstrated that our approach is capable of handling various modes of intent while only minimally compromising the task performance. We believe our approach presents an elegant and scalable solution to the problem of personalising pretrained policies. With a growing fo- cus on personalisation in applications such as chatbots, robotic assistants, self- driving vehicles, etc., we believe our approach has the potential for imminent and widespread impact."}, {"title": "A Simulating human feedback", "content": "Our method involves collecting human feedback for trajectories used for train- ing the task-specific policy. This is done by simulating humans. We conducted experiments in three personalisation modes such as preference, Avoidance and Mixed. In each case, we counted the number of times the agent met the personal- isation criteria within a trajectory and this is regarded as the ground truth score for a trajectory. For instance, In 2D navigation, if the human wishes the agent to visit a preferred region, we count the number of times the agent visited that state and a positive score is given to that trajectory. Similarly, in the avoidance case, a negative score is given and in the mixed case, we sum both the positives and the negatives.\n\nApplying this method in a real-world context could involve several challenges. Using a real human for scoring trajectories, for example, may have limitations with regards to the number of trajectories to be labeled. Fewer labeled trajecto- ries may have an adverse effect on the performance of our approach. In addition to this, a real subject would be prone to fatigue and may provide more arbitrary score labels. For instance, their scores may be biased based on the quality of trajectories presented to them immediately prior to scoring. These, along with a number of other challenges may need to be overcome before applying this approach to real-world scenarios."}, {"title": "B Training of LSTM", "content": "As described in the main paper, we used the same trajectory training data of the task-specific policy to train the LSTM. We sampled 2000 trajectories along with the human feedback as the training data.\n\nThe input to the LSTM is the state and the action chosen at time t and it outputs the corresponding Q-values. We made a custom one-hot encoding of the action to nearly match the dimension of the state vector. For example, in the Highway environment, the state vector is 26 dimensional and there are 5 actions. We encode the selected action to a 25-dimensional vector with one filling in the corresponding 5 positions of the action and the rest being zero. We then condition the state vector with the action vector and train the LSTM. An overview of this dataflow is outlined in Figure 4.\n\nThe LSTM network architecture is single-layered with 64 units. The output gate and forgot gate of the LSTM are set to 1 as in [1] as we don't want to forget anything while performing the credit assignment. The optimiser details are provided in Table 7. We used the same loss functions as that of [1] to train the LSTM.\n\nLc = (1 \u2212 QH)2, (7)\n\nLm = \\frac{1}{H + 1} \\Sigma_{t=0}^{H} (1 - \\frac{e^{-(Q_{t} - \\beta)}}{e^{-\\delta}}) (8)"}, {"title": "C Training of VAE", "content": "VAE is pretrained and only used if the observation of the environment is pixel- based. Training data is the same as the training data of the task policy. We use the conventional VAE loss functions which consist of reconstruction loss and KL- divergence loss. The encoder and decoder architectures are tabulated in Tables 9, 10 respectively. A fully connected linear layer is used to map the output from the encoder to a dimension of 512 subsequently the output is mapped to a latent dimension (set as 16) using another linear layer. Optimiser details of the VAE is shown in Table 8. The loss curve is shown in Figure 8."}, {"title": "D Different choices of static fusion", "content": "Policy fusion is a process of combining two or more policies to produce a new policy. [24] studied various policy fusion methods which are as follows."}, {"title": "D.1 Product Policy", "content": "The fused policy is obtained by multiplying different policies. The action selec- tion process from the f is as follows:\n\narg max \u03c0\u03c6(a|st) \u00d7 \u03c0\u03c8(a|St) (11)\n\u03b1\u0395\u0391"}, {"title": "D.2 Mixture Policy", "content": "In this fusion method, the fused policy is obtained by taking the average of the policies. The action selection process is expressed as follows:\n\narg max \\frac{\u03c0\u03c6(a|st) + \u03c0\u03c8(a|st)}{2} (12)\n\u03b1\u0395\u0391"}, {"title": "D.3 Entropy Threshold", "content": "This method selects the action based on the entropy of each policy at state st. Let H\u03c6(st) represent the entropy of the task policy, and H\u03c8(st) represent the entropy of the Intent-specific policy at state st. The action is selected according to the following conditions:\n\n{\\begin{cases} arg max_{a \\in \u0391} \u03c0\u03c5(\u03b1|st) & \\text{If } H\u03c8(st) < H\u03c6(st) + \u0454 \\\\ arg max_{a \\in \u0391}\u03c0\u03c6 (a|st) & \\text{Otherwise} \\end{cases}} (13)\n\nHere, e is a small positive value."}, {"title": "D.4 Entropy Weighted", "content": "This fusion method computes a weighted average of the two policies, where the weights are determined by the minimum entropy among the two policies. Let H* = min (H\u03c8(st), H\u03c6(st)). The action is selected as follows:\n\narg max (H* \u00d7 \u03c0\u03c1(a|st) + (1 \u2212 H*) \u00d7 \u03c0\u03c5(a|st)) (14)\n\u03b1\u0395\u0391\n\nThe term H* \u00d7 \u03c0\u03c1(a|st) represents the weighted contribution of the task policy, and (1-H*)\u00d7\u03c0\u03c5(a|st) represents the weighted contribution of the Intent-specific policy."}, {"title": "E Analysis of other choices of policy fusion", "content": "In this section", "methods": "the weighted average and product fusion methods through the lens of personalisation. The fusion strategy should comply with two constraints as defined in the main paper. Firstly", "policies": "\u03c0\u0192(a|s) = \u03c0\u2081(a|s)\u00d7\u03c0\u2082(a|s). While this fusion operates on the intersection of the support of the input policies, ensuring that actions are selected to maximize both objectives simultaneously, it fails to satisfy the invariability property as shown in Lemma 2,\n\nHowever, the fused policies produced by the product can be bounded as captured by the following theorem,\n\nTheorem 2. Let Q and Q' be Q-values corresponding to the task policy and the intent specific policy respectively. Let \u03c0\u2084(a|s) and \u03c0\u2084(a|s) represent the respective policies, with corresponding temperatures T\u2084 and T. Let ||Q(s, a) \u2013 Q'(s,a)||2 < e \u2200s \u2208 S and a \u2208 A and ||T - T||2 < \u03b4. Then,\n\nKL (\u03c0\u03c6(\u03b1\u03c2)||\u03c0ff(a|s)) \u2264 log (Z) + (\\frac{Q*\u03b4 + \u20ac\u03a4}{\u03a4\u03a4}) + log (\u03b6) \u2200a \u2208 A, s E S,\n\nwhere Z is its normalising factor of \u03c0f(as), Q* = argmaxa\u2208AQ(s,a) and \u03b6 = \\frac{h(Q', T_{\\psi})}{h(Q,T)}, here h(Q,T) = \\Sigma_{a} exp \\frac{Q}{T}\n\nProof.\n\nKL (\u03c0\u03bf\u03c0\u03c2) = \u03a3\u03c0log (\\frac{\\pi_{o}Z}{\\pi_{\\xi} + \nLog(Z) + \\Sigma_{a} \u03c0\u03c6log (\\frac{Q}{\u03a4\\psi})\n\\sqrt{2} - ( \\frac{\\pi\t{e.g.}}{\\log(5)})"}]}