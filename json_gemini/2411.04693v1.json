{"title": "Reciprocal Point Learning Network with Large Electromagnetic Kernel for SAR Open-Set Recognition", "authors": ["Xiayang Xiao", "Zhuoxuan Li", "Ruyi Zhang", "Jiacheng Chen", "Haipeng Wang"], "abstract": "The limitations of existing Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) methods lie in their confinement by the closed-environment assumption, hindering their effective and robust handling of unknown target categories in open environments. Open Set Recognition (OSR), a pivotal facet for algorithmic practicality, intends to categorize known classes while denoting unknown ones as \"unknown.\" The chief challenge in OSR involves concurrently mitigating risks associated with generalizing features from a restricted set of known classes to numerous unknown samples and the open space exposure to potential unknown data. To enhance open-set SAR classification, a method called scattering kernel with reciprocal learning network is proposed. Initially, a feature learning framework is constructed based on reciprocal point learning (RPL), establishing a bounded space for potential unknown classes. This approach indirectly introduces unknown information into a learner confined to known classes, thereby acquiring more concise and discriminative representations. Subsequently, considering the variability in the imaging of targets at different angles and the discreteness of components in SAR images, a proposal is made to design convolutional kernels based on large-sized attribute scattering center models. This enhances the ability to extract intrinsic non-linear features and specific scattering characteristics in SAR images, thereby improving the discriminative features of the model and mitigating the impact of imaging variations on classification performance. Experiments on the MSTAR datasets substantiate the superior performance of the proposed approach called ASC-RPL over mainstream methods.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the introduction of deep convolutional neural network (CNN) technology has significantly -propelled the development of SAR remote sensing image analysis, particularly playing a pivotal role in target detection and recognition[1]-[5]. Inspired by the cognitive mechanisms of the human brain, CNNs construct a multi-layered feature extraction structure through multiple non-linear transformations, enabling the nonlinear mapping of raw data into a high-dimensional feature space conducive to accurately classifying targets[6]. Typically, a CNN model employs a softmax classifier [7] at its end to output the confidence in target classification, evaluated under closed-set conditions. Based on the closed-world assumption, using softmax operations during training demands that all test classes are known or visible during the training phase, thereby not preserving probabilities for unknown classes.\nHowever, the real-world environment is continually changing and open, imposing various objective limitations on classifiers, making it impractical during the training phase to exhaustively test all possible categories. During testing, unknown categories may be submitted to the algorithmic model, violating the closed-world assumption of recognition methods, rendering the original recognition system ineffective [8]. Typical CNNs use linear classification layers and softmax on embedded features to generate probability distributions for known categories, thereby assuming samples from unknown categories have a uniform distribution across known categories. This leads to softmax loss solely increasing feature separability without effectively distinguishing between known and unknown categories. Specifically, the model might yield high confidence scores for unknown categories but actually misclassify them. Therefore, outstanding performance under closed-set settings is illusory and does not meet the requirements of real-world applications. A robust recognition system should be capable of distinguishing test samples into known and unknown categories and correctly classifying known categories. This task is commonly referred to as Open Set Recognition (OSR) in more realistic scenarios [9].\nThis paper focuses on addressing the identification issues of SAR images in open environments, namely SAR Open Set Recognition (SOSR). The objective is to enhance the CNN's rejection performance of unknown categories while maintaining high-performance recognition of known categories. In the setting of Open Set Recognition, SAR images pose the following challenges for CNNs: 1) Detectors need to mitigate the classification risk of known data under strong supervision, minimizing the overlap between known data distribution in feature space and potential distribution of unknown data to reject unknown samples. 2) Existing deep learning-based SAR image classification networks primarily rely on fine-tuning optical networks, without taking into account the unique imaging mechanism and scattering characteristics of SAR images. Additionally, the underlying physical decision logic of the original CNN networks remains unclear. Unlike optical satellite imaging mechanisms, SAR is sensitive to imaging conditions and observation angles. Directly transferring optical networks to SAR images fails to achieve optimal results. Therefore, the key issue lies in how to fully leverage the scattering characteristics of SAR images to enhance the interpretability and decision transparency of the network, ultimately achieving a more robust performance.\nOne of the key issues in SOSR is how to reduce the risk of potentially labeling unknown data in an open space as any known class. Researchers [10]-[12] utilize prototype learning to represent known classes in the embedded space, aiming to bring the features of training samples closer to their corresponding prototypes. However, the learned prototype representation may converge to the space of unknown classes during training, resulting in an overlap between unknown and known classes in the feature space. This occurs because these methods only consider data from known classes, disregarding potential features of unknown data, which poses significant risk in open spaces. Drawing upon the aforementioned analysis, this study proposes the utilization of reciprocal points to model the latent unknown deep space, thereby mitigating the risks inherent in open spatial domains. Reciprocal points, construed as inverse prototypes of known categories, can be comprehended as instantiated representations of the latent out- of-class space[13]. During the training phase, all samples from known classes are projected to the periphery of the space by their corresponding reciprocal points and constrained by boundary limits. The unknown category, along with the reciprocal points, forms the outer space of categories, with the reciprocal point set restricting its range, and bounded constraints preventing the neural network from generating arbitrarily high confidence in unknown samples. By widening the distance between known and unknown categories in the high-dimensional feature space, unknown categories become more easily distinguishable. This method provides better distinctiveness for Open Set Recognition tasks, improving system performance when faced with unknown categories and offering new possibilities in Open Set Recognition tasks.\nFurthermore, a study [14] indicates a significant correlation between closed-set and open-set performance in open-set recognition tasks. The study confirms the crucial importance of enhancing recognition accuracy within the closed-set to effectively improve open-set recognition outcomes. It repositions the OSR problem as part of traditional recognition problems. Therefore, robust feature representation is crucial for enhancing a model's performance in open-set tasks. Current mainstream SAR classification methods primarily rely on networks like CNNs and Transformers. However, most studies utilize network structures based on optical images, disregarding SAR targets' intrinsic characteristics, limiting improvements in recognition accuracy and generalization capabilities. In contrast to deep learning approaches, traditional SAR target classification algorithms provide interpretable SAR target features. Common SAR target features encompass geometric features [15], transformation domain features[16], and electromagnetic scattering features (ESF)[17]. Target classification algorithms based on ESF are currently receiving considerable attention. In high-frequency regions, the radar backscatter of distributed targets can be approximated as the sum of responses from individual scattering centers. The Attribute Scattering Center (ASC) model [18] effectively describes the dependence of target backscatter response on frequency and azimuth angle. This model includes abundant physical and geometric attributes, providing a more accurate description of radar targets' electromagnetic scattering characteristics in high-frequency areas. Consequently, the ASC model effectively reflects the physical features of SAR targets. Considering the unique attributes of SAR images in the scattering domain[19], this study proposes the design of a large-sized convolutional kernel based on electromagnetic scattering centers, enabling the network to better extract scattering features from SAR images, thereby enhancing the accuracy and robustness of the model.\nBased on the aforementioned, this study introduces an open- set recognition network structure integrating ASC characteristics within convolutional kernels to enhance SAR classification performance under open-set conditions. As illustrated in Fig.1, this framework retains the representation capability of CNN while discarding the closed-world assumption of softmax, employing an reciprocal point-based model to classify known targets and discern unknown targets. Specifically, to enhance the network's representation of SAR target features, this work firstly constructs an ASC-based network architecture. Parameters in the convolutional kernels are modulated using ASC that reflects target scattering characteristics, allowing the network to better focus on the scattering features of SAR targets. Furthermore, for varying sample sizes in classification scenarios, a larger convolutional kernel structure was designed to further enhance recognition performance. Finally, within the classification head, reciprocal point learning is designed for SAR image object open-set recognition, addressing issues of modeling unknown data and mitigating risks within open spaces. Additionally, a bounded constraint is devised to prevent the neural network from generating arbitrarily high confidence levels for unknown samples, thereby improving the separability between known and unknown categories. Ultimately, extensive experiments conducted on the MSTAR dataset validated the effectiveness of the proposed methodology.\nThe contributions of our work can be summarized as follows:\n1) The designed large-sized ASC kernel, in comparison to conventional convolutional kernels, offers a larger receptive field. It demonstrates higher focus and efficiency in extracting certain intrinsic non-linear features and specific scattering characteristics in SAR images.\n2) Addressing the modeling of out-of-class space through known category-based risk reduction in open spatial domains, reciprocal point learning is proposed to address SAR image target open-set recognition. This approach tackles the challenge of unknown data modeling, thereby reducing the risks associated with open space. To the best of our knowledge, this represents the first application of reciprocal point learning in SAR image processing.\n3) The original CNN network lacks explicit physical decision logic. The designed model in this paper elucidates the physical logic behind the network's discriminative decisions, enhancing interpretability and decision transparency. This augmentation contributes to the algorithm model's robustness. Concurrently, visualization results substantiate the complementary relationship between deep learning network features and electromagnetic scattering characteristics."}, {"title": "II. PRELIMINARY KNOWLEDGE", "content": "Traditional CNNs are generally divided into two main parts [20]: Firstly, the feature extraction stage, where the feature extractor f(x, 0) maps from the input layer to the penultimate layer, with x and 0 representing the original input and CNN parameters, respectively. Secondly, the linear classification stage involves the linear classifier C(f(x, 0), 0c), executed on the final layer of the CNN using the deep feature f(x, 0), with 0c denoting the parameters of the last layer. The feature extractor f(x, 0) and classifier C(f(x, 0), 0c) jointly undergo end-to-end learning, achieving the traditional closed-set recognition task by learning the mapping function f: X \u2192 Y, mapping the input image domain X to the corresponding label domain Y. This mapping function model dictates the network's performance and efficiency in handling various image data. To better adapt the network to the relationship between samples and supervision information, optimization of trainable parameters is typically necessary. The core of this optimization process involves adjusting the model's parameters by minimizing a loss function to better fit the training data.\nFormula (1) illustrates that in the classifier C(f(x, 0), 0c), it's usually assumed during training that the sum of the posterior probabilities of known classes is 1, namely:\n$$p(y|x) = \\frac{e^{(W_y f(x;\\theta) + b_y)}}{\\sum_{i=1}^{C} e^{(W_i f(x;\\theta) + b_i)}}$$\n$$\\sum_{i=1}^{C}p(i|x) = 1$$\nHere, y represents the actual label of sample x. The wi and bi represent the weight vector and bias parameters for the i-th class, respectively. It's worth noting that this framework exclusively considers training for known classes and doesn't retain probability information for unknown classes, thus becoming ineffective when dealing with unknown categories.\nFormula (3) below represents the traditional training loss function:\n$$loss = \\min_{f \\in F} \\sum_{i=1}^{N} l(f(x_i; \\theta), y_i)$$\nwhere Xi and yi denote the i th sample and its corresponding label, N represents the total number of targets, and l denotes the network's objective loss function.\nConsidering the physical model g: X \u2192 Z providing additional prior information by mapping samples X to a physical representation Z, which incorporates a deep neural network of Z, learns to fit the joint distribution f(X, Z) of images and physical priors [21]. Therefore, its minimized objective function can be expressed as:\n$$loss = \\min_{f \\in F} \\sum_{i=1}^{N} l(f(x_i; \\theta; g(x)), y_i)$$\nwhere zi = g(xi) denotes the prior information of the ith sample xi. To effectively learn discriminative features of SAR targets, networks typically require training on samples from the same or similar distributions. However, when observation conditions change, generalization performance might be affected, making robust prior knowledge crucial for addressing automatic recognition problems caused by differences in image modalities."}, {"title": "B. Open-set Recognition for SAR Images", "content": "In the domain of SAR image analysis, the open-set problem on the MSTAR dataset was tackled by Scherreik et al. [22] using Weibull-calibrated SVM (W-SVM) [23] and Probabilistic Open Set SVM (POSSVM) [24]. Zeng et al. [25] employed the Kullback-Leibler Divergence (KLD) between test and training set features to roughly identify unknown SAR targets. Generative Adversarial Networks (GAN) [26] were utilized for SAR image open-set recognition, distinguishing between unknown and known targets based on a branch output score compared against a predefined threshold. Inkawhich et al. [27] proposed Adversarial Out-of-Distribution Exposure (AdvOE) to jointly design SAR automatic detection systems, ensuring accuracy and Out-of-Distribution (OOD) detection. Their approach introduced a large, diverse, and unlabeled auxiliary training dataset containing samples from the OOD set. AdvOE aimed to encourage robust feature learning from in-distribution training data while promoting maximum entropy predictions for adversarially perturbed versions of OOD data. Dang et al. [28] introduced a classifier for continuously detecting and learning new categories in an incremental learning scenario, leveraging the MSTAR dataset. Subsequently, in [29], they proposed an extended method to retain prior recognition capabilities when adding new tasks/categories. These studies addressed SAR's OSR problem through threshold-based decisions, reconstruction error-based judgments, and incremental learning. However, the intrinsic characteristics of SAR targets were overlooked in their processing, limiting the improvement of identification precision and weaknesses in generalization capabilities."}, {"title": "III. METHODOLOGY", "content": "The Attributed Scattering Center Model (ASC) model is developed from physical optics and the geometric theory of diffraction, with its parameters reflecting a variety of physical and geometric characteristics that describe the electromagnetic properties of typical ASCs.. In the high-frequency domain, as shown by the following equation, the radar echo from targets can be considered as the superposition of multiple ASCs:\n$$E(f, \u03c6; 0) = \\sum_{i=1}^{q} \u0395\u2081(\u03be, \u03c6; \u03b8\u2081) + n(f, \u03c6; \u03b8\u2081)$$\nWhere, f denotes the operating frequency in Hertz (Hz); \\\u03c6 represents azimuthal angle encompassing the parameter set of the scattering center. n(f, \u03c6; \u03b8\u2081) signifies noise, and the total number of ASCs within the target is defined as q.\nThe scattering field of distributed targets in the high- frequency domain can be represented as follows:\n$$\u0395\u2081(f, \u03c6; \u03b8\u2081) = A\u2081) exp [ -j \\frac{4\u03c0f}{fc} (xicos + yisin \u03c6)] \\ \\bullet \\sinc (\\frac{2\u03c0f}{c}Lisin (q \u2013 \u03c6\u2081)) \\bullet exp ( \u2212 \\frac{2\u03c0f}{C}y\u012fsin \u03c6)$$\nWhere sinc(x) = (sin (\u03c0x)/(\u03c0\u03c7)), fc is the radar center frequency and c means is the light propagation. (xi,yi) denote the cross-range and range coordinate of the scattering center i, respectively. A is the relative amplitude, Li and represent the length and pointing angle of the distributed of the scattering center i, respectively. The parameter a presents frequency-dependent factor, and Vi is the aspect dependence. \u03b8\u2081 = [\u0391, \u03b1, X, Yi, Li, Pis Vil represents the parameters of the i-th scattering center. For local scattering centers, Li = \u2081 = 0 . and for distributed scattering centers, Vi = 0.\nTaking into account the actual working conditions of a SAR system, certain approximations can be applied to the ASC model[30]: The angle observation is usually short, allowing the the angle dependence factor Yi to be approximately disregarded. Additionally, the ratio of center frequency to bandwidth is generally low,, meaning that the parameter ai has minimal impact. As a result, in this study, Yi and a\u2081 are set to zero. With these assumptions, the ASC can be simplified as follows:\n$$E\u2081(f, 4; 0\u2081) \u2248 A\u00a1sinc [\\frac{2\\pi f}{C}Lisin (4-4)] \\times exp [ \u2212 j\\frac{4n\\pi f}{C} (xicos q + y\u2081sin q)]$$\nAs shown in Fig. 2, the convolution process and its results are clearly depicted. In this procedure, the convolution operation emphasizes the primary structural features of the original image, predominantly encompassing shape and position, correlated with the characteristics of the convolution kernel. This phenomenon reveals the crucial role of convolution in feature extraction and its substantial impact on image analysis. Based on the fundamental principles of convolution kernels and the ASC model, we designed ASC kernels with diverse parameters. These ASC kernels exhibit higher efficiency compared to traditional convolutional kernels in extracting certain intrinsic nonlinear features and specific scattering characteristics in SAR images."}, {"title": "B. Large Kernel Design", "content": "The emergence of Vision Transformers (ViT)[31] has posed new challenges to Convolutional Neural Networks (CNNs), showcasing remarkable performance across various visual tasks and prompting a reevaluation of the effectiveness of CNNs. In Vision Transformers, the design of Multi-Head Self-Attention (MHSA) has proven crucial, whether global [32] or local. The larger kernel in MHSA allows each MHSA layer's output to gather information from a wide region, effectively enhancing the model's receptive field. In contrast to ViT, Convolutional Neural Networks address the receptive field issue differently, favoring the use of a series of small spatial convolutions [33], such as 3\u00d73, rather than widely adopting large kernels. Recent research [34] has attempted to maintain comparable results by introducing 7\u00d77 depth convolutions as an alternative to MHSA layers. While this approach shares similarities with ViT's design, its motivation differs. The study does not extensively explore the relationship between large kernels and performance nor elaborate on the application of large kernels in conventional CNNs. Instead, it attributes the outstanding performance of Vision Transformers to sparse connections, parameter sharing, and dynamic mechanisms, highlighting differences in understanding and emphasis on performance improvement among various research endeavors.\nThe design of large kernels can effectively increase receptive field, thus ameliorating shape distortions [35]. Although achieving a large receptive field can be realized by combining a series of small kernels, for instance, decomposing a 7 \u00d7 7 convolutional kernel into a stack of three 3\u00d73 kernels to ensure information preservation (albeit requiring additional channels to maintain model flexibility), its effectiveness may not always be as pronounced as using a large convolutional kernel directly in certain scenarios."}, {"title": "C. Reciprocal Point Learning", "content": "The reciprocal points can be considered as the inverse prototype of each known category[13], so they can be used to constrain the extra-category space. In the training phase, reciprocal point learning expands the distance between the embedded features of the known target class and the corresponding reciprocal points by pushing all known categories to the periphery of the space by the corresponding reciprocal points, thereby distinguishing known categories from unknown ones. category. RPL has proven to be an effective tool for minimizing feature overlap between known and unknown classes while maintaining closed-set performance.\nSpecifically, given a set of training sample D\u2081 = {(X1,Y1), ..., (X, Yn)}, it comprises Nknown classes denoted as Ck = {1, ..., N}. The test dataset: D\u2081 = {t\u2081, ..., tu}, ti \u2208 Ck U Cu, Cu = {N = N + 1, ..., N + u}, and u represents the number of unknown classes in the actual testing environment. The potential unknown data is represented as Du. Known classes within the m-dimensional feature space Rm are denoted as Sk, while the space of unknown classes can be expressed as Ok = Rm - Sk.\n#k\nTherefore, as the inverse prototype of the known category k, the reciprocal point pk of category k can be considered as the potential representation of the sub-dataset D\u2081 U Du. As shown in Fig.5(a), contrary to prototype learning, the reciprocal point Pk should be more similar to the samples in Ok. It is closer to Ok in the feature space and further away from Sk. It can be expressed as:\n\u2200d \u2208 \u03b6(D\u2081k, Pk), max(\u03b6(D\u2081\u2260k U Du, Pk)) \u2264 d)\nWhere (() represents the calculation of the distance set of all samples between the two groups; max() is the maximum value function. The goal is to separate unknown categories from known categories as much as possible in the feature space. Based on this, samples can be classified by maximizing the distance between the reciprocal point and the corresponding known category. The distance between the known category k and Pk is the largest. Specifically, the reciprocal points Pk of known categories are optimized through the learnable feature representation network fo(x). Since each known category is opposite to its corresponding reciprocal point in spatial position and angular direction, the Euclidean distance de and dot product da are used to evaluate the similarity:\n$$d_e(f_\\theta(x), P_k) = \\frac{1}{m} ||f_\\theta(x) \u2013 P_k ||^2$$\n$$d_a(f_\\theta(x), P_k) = f_\\theta(x) \\bullet P_k$$\n$$d(f_\\theta(x), P_k) = d_e(f_\\theta(x), P_k) \u2013 (f_\\theta(x), P_k)$$\nBased on distance measurement, the assignment of x to a specific class can be determined by assessing the discrepancy between the embedded feature f(x) and the reciprocal points of each class. In contrast to prototype learning, the characteristics of reciprocal points dictate that a larger d(f(x), Pk) value results in a higher probability of x being assigned to class k. The ultimate probability representation can be derived through softmax.\n$$p(y = k|x, f_\\theta, P) = \\frac{e^{d(f_\\theta(x),p_k)}}{\\sum_{i=1}^{C} e^{d(f_\\theta(x),p_i)}}$$\nThe loss function defines the negative log probability of category k as:\n$$L_c(x; \\theta, P) =\u2212 log p(y = k|x, f_\\theta, P)$$\nThe optimization of the learnable parameters in the feature representation network fo(x) is achieved by minimizing the loss function Lc. This optimization aims to enhance the embedding function's ability to classify known categories. Simultaneously, effective separation between known and unknown categories in the feature space is achieved by maximizing the distances between the mutual points and their corresponding samples. As illustrated in Fig. 5 (b), during the training phase, all known categories are pushed towards the outskirts of the space by their respective mutual points, while the mutual points outside these known categories are pushed away, creating separation from the known space.\nHowever, potential unknown categories are also encompassed within the space outside the class, so the risk in open space still exists. To mitigate the risk in open space, it is necessary to constrain unknown categories to a bounded unknown space through the utilization of the reciprocal points, as expressed by Equation (18):\n$$max(\\zeta(D_{1\\neq k} \\cup D_u, P_k)) \\leq R$$\nWhere the learnable boundary is denoted as R constraining potential unknown category boundaries. Since the spaces Sk and Ok are complementary, the distance between the samples in Sk and the reciprocal point Pk can be constrained within R by optimizing the margin loss function, thereby indirectly limiting the risk of open space. The boundary loss function can be expressed as:\n$$L_o(x; \\theta, P, R) = max(d_e(f_\\theta(x), P_k) \u2013 R, 0)$$\nThe overall loss function for reciprocal point learning can be descripted as follows:\n$$L(x; \\theta, P, R) = L_c(x; \\theta, P) + \\lambda L_o(x; \\theta, P, R)$$\nWhere is a hyperparameter that controls the weight of the open space risk reduction module. O, P, R represent the learnable parameters of the framework."}, {"title": "Algorithm 1: The Reciprocal Point Learning for Open-set SAR Recognition", "content": "The SAR image x\u2081with label y\u2081 for training\nThe convolution network f(x) with initial parameters 0; The reciprocal points P and the constrained boundary R Hyper-parameter: \u03bb, \u03b3\nThe Optimized Parameters, 0, P, R When f(x) is not converge do Calculate the loss by L = Lc + Lo\nUpdate the Parameters 0, P, R by the back propagation end while\nSAR image x for testing The predicted open-set label \u0177\nCalculate the distance d between x and P\nPredict the closet-set label \u0177 If d(y) < R then\n\u0177 = unknow end if\nreturn y"}, {"title": "IV. EXPERIMENTS AND ANALYSIS", "content": "To evaluate the proposed method, the MSTAR dataset collected by Sandia National Laboratories' SAR sensor platform is selected as the baseline for the study [37]. This SAR sensor operates in the X-band and uses HH polarization and the data set is widely used in SAR automatic target recognition research. Each data point is a complex image that can be decomposed into amplitude and phase. The image has a resolution of 0.3 meters \u00d7 0.3 meters and a size of 128 \u00d7 128 pixels, covering 360\u00b0 in all directions. The MSTAR dataset contains ten different military vehicle types, namely rocket launcher: 2S1, armored personnel carrier: BMP2, BRDM2, BTR70, BTR60, bulldozer: D7, tank: T62, T72, truck: ZIL131 and air defense unit: ZSU234. Optical images and corresponding SAR images of these targets can be observed in Fig.5.\nDifferent operating conditions (OC) result in varying image distributions. Standard Operating Conditions (SOC) refer to acquiring targets in similar or even identical radar imaging conditions for both training and testing SAR images. On the other hand, Extended Operating Conditions (EOC) signify significant differences between testing and training image acquisitions, including variations in elevation angle, noise interference, and configuration variables. This experiment is conducted under, with training images at a depression angle of 17\u00b0 and testing images at 15\u00b0. There are 1747 training images and 2425 testing images used. Specific details about the MSTAR dataset are provided in TABLE II.\nTo assess the performance of the proposed open-set recognition method, the experimental results are evaluated using the following metrics: Recall, Precision, F1, and Accuracy. Their formulas are as follows:\n$$Recall = \\frac{\\sum_{i=1}^{C} recall_i}{C}, recall_i = \\frac{TP_i}{TP_i + FN_i}$$\n$$Precision = \\frac{\\sum_{i=1}^{C} precision_i}{C}, precision_i = \\frac{TP_i}{TP_i + FP_i}$$\n$$F1 = \\frac{\\sum_{i=1}^{C} Fli}{C}, F1_i = \\frac{2 \\times precision_i \\times recall_i}{precision_i + recall_i}$$\n$$Openness = 1- \\sqrt{\\frac{2 \\times CTR}{CTR + CTE}}$$\nWhere CTR is the number of training classes and CTE is the number of testing classes. A higher openness value, closer to 1, indicates a more open environment, while an openness value of 0 represents a problem equivalent to closed-set classification."}, {"title": "B. Implementation Details", "content": "In the MSTAR dataset, there are not only complex images used for training and testing, but also detailed parameters of the imaging process: the carrier frequency fc is 9.6GHz, and the bandwidth B is 0.49GHz. The frequency f in the attribute scattering center model is set between 9.36 and 9.85GHz. The sampling frequency is 0.591GHz, resulting in approximately 189 samples for f, adding 19 zeros at both ends. \u0394\u03c6 is 2.8, so the range of @ is from -1.4 to 1.4. There are 227 sampling points, matching the size of the input SAR image. For a convolutional kernel size of 11 \u00d7 11, L\u2081 is set to {0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3}, and is set to {0\u00b0,10\u00b0,20\u00b0,30\u00b0,40\u00b0,50\u00b0,60\u00b0, 70\u00b0,80\u00b0,90\u00b0}. For kernel sizes 21 and 31, the range of Li is expanded, and is uniformly distributed between 0 and 90.\nFor feature extraction using deep CNN, training is conducted in cycles of 50 epochs. The cross-entropy function is utilized as the loss function, and stochastic gradient descent with an initial learning rate of 0.001 is employed for loss optimization with the batch size of 64. The weight for constraining the open-set risk using the counterpoints is set to 0.1."}, {"title": "C. Experiments for Closed-Set Recognition", "content": "The aim of this experiment is to analyze the proposed method's test accuracy under closed-set conditions and to more intuitively showcase its performance advantages. Due to the relatively standard characteristics of the MSTAR dataset and the attainment of up to 99% accuracy on some common deep learning networks, it's challenging to effectively demonstrate the advantages of the proposed method. Therefore, to demonstrate the effectiveness of this method, this chapter devised experiments focusing on limited training samples. As each target sample in the MSTAR dataset covers a full 360\u00b0 observation angle, reducing the proportion of training samples is akin to diminishing certain perspectives of the targets, allowing an analysis of the method's robustness against target rotation. The experiment is conducted with training sample sizes of 20, 40, 60, and 80. Here, 'Base' represents the original randomly initialized convolutional kernel, while ASC signifies the kernel designed based on ASC. As shown in TABLE IV, compared to the original convolutional kernel, it's evident from the training process that the ASC-designed kernel achieved significant improvements across all training sample sizes, with respective accuracy enhancements of 9.41%, 1.78%, 4.46%, and 0.66%."}, {"title": "E. Ablation Study", "content": "According to the results in TABLE VI, under the Base algorithm, as the convolutional kernel size increases from 11 to 31, the accuracy exhibits varying degrees of improvement: 4.53%, 2.59%, 2.31%, and 5.15% respectively. The potential rationale behind this trend could be attributed to the introduction of large kernels, which introduce more shape biases into the network. In essence, ImageNet images can be correctly classified based on texture or shape. However, human perception primarily relies on shape cues rather than texture for object recognition. Therefore, models with stronger shape biases may better generalize to the target task. In the context of ASC augmentation, the accuracy relative to the Base algorithm increases by 13.94%, 4.37%, 6.77%, and 5.81% respectively. The underlying cause is attributed to the designed large-sized ASC kernel, which, compared to traditional convolutional kernels, provides a larger receptive field. Moreover, it demonstrates higher focus in extracting certain intrinsic non-linear features and specific scattering characteristics in SAR images, as evident in the visualizations in Fig. 8 and Fig. 10."}, {"title": "F. Visual Analysis of Convolution Kernel Base ASC", "content": "Fig. 10 displays the amplitude images of convolutional kernels designed based on the ASC model. These kernels have a size of 11\u00d711, with a quantity of 100 and a single channel. The ASC-designed convolutional kernels exhibit good orthogonality in their kernel weights due to different combinations of {Lp,Cop}. Compared to regular convolutional kernels, networks modulated by ASC achieve higher average precision. This could be attributed to the difference between SAR images and optical images, as SAR responses originate from microwaves rather than visible light. The linear structures extracted from the first convolutional layer in optical images are continuous [19]. However, in SAR images, the complete linear structure is only visible at a zero azimuth angle (as seen in Fig. 10). When the scattering center points in other directions, the linear structure crosses distance bins and breaks. This might be the primary reason why ASC-designed convolutional kernels outperform regular ones in SAR target recognition. Fig. 11 presents visualizations of convolutional kernels with a size of 21x21, totaling 441 kernels. Compared to the 11x11 kernels, the 21x21 kernels can extract more abundant scattering feature."}, {"title": "G. The Heatmap Visualization", "content": "This present paper employed visualization techniques to analyze the decision-making process of the model. Specifically, we introduced the Grad-CAM (Gradient-weighted Class Activation Mapping) method for visualizing and interpreting the decision mechanism of CNNs. Grad-CAM generates heatmaps representing important regions for category determination by utilizing the feature maps of the last convolutional layer and the gradient backpropagation information for a specific target class. GradCAM++ enhances Grad-CAM by introducing regularization terms to improve the quality of visualization. XGradCAM extends GradCAM and GradCAM++ by incorporating additional regularization terms, further enhancing the visualization effects. EigenCAMs utilize eigenvalue decomposition to generate heatmaps based on the feature maps of convolutional layers, aiming to improve interpretability. These methods collectively contribute to explaining the decision-making process of CNNs, enhancing the understanding of model predictions, and providing robust support for the interpretability of deep learning models."}, {"title": "V. CONCLUSION", "content": "With the continuous evolution of deep neural networks, significant advancements have been made in SAR image recognition methods. However, the typical evaluation processes are limited to closed environments, failing to adequately meet the practical demands and variations of open environments. Common closed-set detection methods, constrained by limited training samples, often struggle to effectively handle unknown categories in open environments. Therefore, this paper introduces a novel approach, aimed at enhancing the performance of SAR image recognition in open sets.\nThe motivation behind this study encompasses two crucial aspects: Firstly, a feature learning framework based on reciprocal point learning is established to simulate extraneous spaces, thereby mitigating risks in open environments; Secondly, the learned feature space demonstrates the capability to augment differences between known and unknown categories, reducing overlap between their respective distributions; Thirdly, a large-scale convolutional kernel network integrating the attribute scattering centers is designed to better model SAR image scattering features, capturing richer spatial semantic information. This enables the backbone network to acquire robust feature representations for identifying both known and unknown categories within a refined feature space. Comprehensive experiments validate the effectiveness of the proposed framework.\nTraditional closed-set recognition primarily focuses on distributional changes between training and test data. However, for open-set recognition, models must extend known categories into a more extensive open space. In the future, we aim to expand this research into open-set detection, encompassing the identification of unknown samples as well as the task of detecting and classifying them as new categories, continuously enhancing the model's performance in both closed-set and open-set scenarios."}]}