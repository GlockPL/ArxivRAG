{"title": "Displacement-Sparse Neural Optimal Transport", "authors": ["Peter Chen", "Yue Xie", "Qingpeng Zhang"], "abstract": "Optimal Transport (OT) theory seeks to determine the map T: X \u2192 Y that transports a source measure P to a target measure Q, minimizing the cost c(x, T(x)) between x and its image T(x). Building upon the Input Convex Neural Network OT solver (Makkuva et al., 2020; Amos et al., 2017) and incorporating the concept of displacement-sparse maps (Cuturi et al., 2023), we introduce a sparsity penalty into the minimax Wasserstein formulation, promote sparsity in displacement vectors \u0394(x) := T(x) \u2212 x, and enhance the interpretability of the resulting map. However, increasing sparsity often reduces feasibility, causing T#(P) to deviate more significantly from the target measure. In low-dimensional settings, we propose a heuristic framework to balance the trade-off between sparsity and feasibility by dynamically adjusting the sparsity intensity parameter during training. For high-dimensional settings, we directly constrain the dimensionality of displacement vectors by enforcing dim(\u25b3(x)) \u2264 l, where l < d for X C Rd. Among maps satisfying this constraint, we aim to identify the most feasible one. This goal can be effectively achieved by adapting our low-dimensional heuristic framework without resorting to dimensionality reduction. We validate our method on both synthesized sc-RNA and real 4i cell perturbation datasets, demonstrating improvements over existing methods.", "sections": [{"title": "1. Introduction", "content": "Optimal transport (OT) aims to find a map T that efficiently transfers mass from one measure to another under the ground-truth cost. This technique has been applied in various machine-learning-based single-cell biology studies, as demonstrated in (Huizing et al., 2022; Zhang et al., 2021; Demetci et al., 2022; Schiebinger et al., 2019; Bunne et al., 2023b). Compared to traditional methods of solving OT, such as Sinkhorn's algorithm (Cuturi, 2013), recent studies in single-cell biology have increasingly utilized Neural Optimal Transport (Neural OT) solvers. These solvers efficiently scale OT to large-scale and high-dimensional problems using continuous methods (Korotin et al., 2021b). For instance, CellOT (Bunne et al., 2023b) addresses cell perturbation response by leveraging Neural OT, implemented via the Input Convex Neural Network (ICNN) framework.\nWhile Neural OT solvers can produce feasible maps, the lack of sparsity in the displacement vector reduces their interpretability, often resulting in overly complex OT maps that involve all dimensions (features). To address the similar issue in the Sinkhorn-solved map, Cuturi et al. (2023) introduced the concept of displacement-sparse maps. These maps are more interpretable with lower-dimensional displacement while preserving their applicability in the original space without relying on dimensionality reduction techniques. Inspired by this work, we extend similar sparsity-promoting features to Neural OT solvers, specifically the Neural OT solver implemented via ICNN (Makkuva et al., 2020) (referred to as \u201cICNN OT\u201d in the following discussion).\nBuilding on the minimax formulation of the Kantorovich dual problem used in ICNN OT, we incorporate sparsity penalties into the framework. This builds on the reparameterization of the dual potentials f and g as convex functions, which can be efficiently learned through ICNN training. The sparsity penalty is inspired by the Cuturi et al. (2023)'s sparsity-induced transportation cost function, which is defined as c(x, y) = ||x - y||2 + \u03bb\u00b7 \u03c4. Here, \u0442 serves as a sparsity regulator, and A controls the intensity of the sparsity-inducing.\nHowever, inducing sparsity in OT maps comes with a trade-off: as the sparsity level increases, the accuracy of the map's feasibility decreases, which means that the source measure cannot be effectively transported to the target measure. This effect is evident in Figure 2, where at higher sparsity intensity level, less points can be effectively transported to the target location, and displacement vectors are becoming shorter due to the stronger sparsity penalty. To tackle this issue, we create a dynamic framework to adjust the sparsity intensity, offering two models to solve OT tasks in both low-dimensional and high-dimensional spaces."}, {"title": "2. Related Works", "content": "Neural Optimal Transport. Neural OT solvers employ continuous methods to estimate transportation plans or dual potentials through neural networks, avoiding the discrete formulations typically used by traditional approaches. Popular methods (Taghvaei & Jalali, 2019; Makkuva et al., 2020; Fan et al., 2021; Korotin et al., 2021a; Mokrov et al., 2021; Bunne et al., 2022; Alvarez-Melis et al., 2022) often reparameterize the transportation plan or dual potentials and leverage ICNN (Amos et al., 2017) for approximation. In addition, Korotin et al. (2021b) provides a benchmark for evaluating the performance of current neural OT solvers, and Asadulaev et al. (2024) proposes a framework to solve neural OT problems with general cost functions.\nIn line with CellOT (Bunne et al., 2023b), we utilize the minimax formulation of dual potentials from Makkuva et al. (2020) to introduce displacement sparsity. Comparing to other neural OT solvers, this formulation was chosen because it provides a straightforward approach to recovering displacement vectors through dual potentials.\nUnlike Sinkhorn OT solvers, which relies on entropic reg-"}, {"title": "3. Preliminaries", "content": ""}, {"title": "3.1. Optimal Transport", "content": "Given two probability measure P and Q in Rd and quadratic transportation cost, the Monge (1781) problem seeks to find the transport map T : Rd \u2192 Rd that minimizes the transportation cost:\n\n$T^* = arg \\min_{T:T#P=Q} E_{X~P}||T(X) \u2013 X ||^2$. (1)\n\nHere, T* is the optimal transport map among all maps T that push forward P to Q. However, solving (1) is challenging because the set of feasible maps T may not be convex or might not exist at all. To address this, Kantorovich relaxed the problem by allowing mass splitting, replacing the direct map T with a coupling \u03c0 between P and Q:\n\n$W_2(P,Q) = \\inf_{\\Pi\\in\\Pi(P,Q)} \\frac{1}{2} E_{(X,Y)~\\pi}||X - Y ||^2$, (2)\n\nwhere \u03a0(P, Q) is the set of all couplings (or transport plans) between P and Q. This relaxation makes the problem con-"}, {"title": "3.2. Input Convex Neural Network", "content": "Amos et al. (2017) introduced Input Convex Neural Networks (ICNNs) to model convex functions by optimizing a set of parameters, {W(1), W(2)}, during each iteration.\nGiven an initial input y, the ICNN learns the convex function zk through the following formulation:\n$z_{i+1} = g_i (W_i^{(z)} z_i + W_i^{(y)} y + b_i), \\quad z_k = f(y; \\theta)$,\nwhere zi represents the layer activations, $\\theta = {W_{0:k-1}^{(z)}, W_{1:k-1}^{(y)}, b_{0:k-1} }$ denotes the model parameters"}, {"title": "3.3. Sparsity-inducing Penalties \u03c4", "content": "Cuturi et al. (2023) proposed the following cost function to induce displacement sparsity:\n\n$c(x,y) = \\frac{1}{2} ||x \u2212 y||^2 + \u03bb\u00b7 \u03c4(y \u2013 x)$. (5)\n\nUnder this regularized cost, the optimal transport mapping can be expressed as:\n\n$T(x) = x-prox_\u03c4(x - \\sum_{j=1}^m p^j (x) (y^j + \\nabla \u03c4(x - y^j)))$.\n\nwhere prox\u03c4 denotes the proximal operator of \u03c4, and pj (x) represents the x-dependent Gibbs distribution over the m- simplex. Cuturi et al. (2023) introduced three choices for the penalty function \u03c4: the l\u2081-norm, Vanishing Shrinkage STVS (Tstvs), and k-overlap norm (Argyriou et al., 2012). However, the k-overlap was ultimately abandoned due to its undesirable computational cost.\nl1-Norm. The l\u2081-norm directly penalizes the magnitude of the displacement vector, encouraging it to move towards the nearest target and inducing sparsity.\nVanishing Shrinkage STVS. Schreck et al. (2015) introduce the soft-thresholding operator with vanishing shrink-"}, {"title": "4. Sparsity-inducing Minimax Formulation", "content": "Our goal is to bring sparsity to the displacement vector \u0394(x) := T(x) \u2212 x from the map T learned via ICNN, making the result with better interpretability. We introduce the sparsity penalty to the minimax formulation (4), which makes the squared 2-Wasserstein biased:\n\n$W_2(P,Q) = C_{P,Q}+ \\sup_{f\\in S(P)} \\inf_{g\\in S(Q)} V_{P,Q}(f,g) + \u03bb \u222b_{\\mathbb{R}^d} \u03c4 (\u2207g(y) \u2013 y) dQ$, (6)\n\nwhere g(y) \u2212 y represents displacement vector derived from (4), and \u5165 is a parameter that controls the intensity of sparsity-inducing.\nAs discussed in \u00a73.1, it is infeasible to re-parameterize the dual potentials into (4) when the quadratic cost is not used. Therefore, we adopt an intuitive approach to penalize the map as a whole. In this formulation, any \u03c4 function can be applied to recover the mapping without relying on prox\u03c4. Leveraging ICNN, we propose a heuristic framework in \u00a74.1 that dynamically adjusts the intensity A based on different goals, a feature not applicable to traditional Sinkhorn solvers. Several theoretical guarantees are introduced:\nLemma 4.1. Let \u03c4 be the sparsity penalty introduced in \u00a73.3. Assume that P and Q are probability measures"}, {"title": "4.1. Dynamic Adjustment to Sparsity Intensity \u03bb", "content": "In the previous section, we utilized a constant A to enforce sparsity in the learned map via ICNN. However, this approach presents inherent challenges: if A is set too low, the resulting map lacks sufficient sparsity, failing to promote the desired regularization. Conversely, if A is set too high, the accuracy of the map degrades, leading to a larger deviation of T# (P) from the true target Q. This trade-off is clearly illustrated in Figure 2 and additional example Figure 4, where we use the classic eight-Gaussian setup to demonstrate this effect. Note that eight-Gaussian example is intended solely for illustrative purposes, as sparsity is not a necessary requirement in this context. In the extreme sparsity case, more and more points collapse towards the source center without adequate transportation to their targets.\nWe introduce a parameter a to tune this tradeoff by dynamically adjusting A to optimize a goal function that balances sparsity and feasibility during the ICNN training process. For the map Tn : X \u2192 Y learned at the n-th iteration of ICNN training, we define its sparsity and feasibility level through two metrics, Spa (Sparsity) and Err (Error), where\n\n$Spa = \u222b_{\\mathbb{R}^d} \u03c4(T_n(x) \u2013 x) dP, \\quad Err = D(T_n#(P), Y)$.\n\nWe use the corresponding sparsity metric, \u03c4, during training, while measuring the error through the divergence D between the mapped distribution and the actual target distribution. To capture this geometric divergence, we approximate the source and target distributions, P and Q, as finite sets of sampled points, and use the Wasserstein distance to quantify the divergence between them. However, due to the computational inefficiency of the Wasserstein distance, we accelerate the training process by approximating it with the sliced Wasserstein distance (Bonneel et al., 2015).\nIt is important to normalize both metrics to the same numerical scale. Ultimately, we construct the evaluation function:\n\n$Eval = \u03b1 Spa + (1 \u2212 \u03b1) \u00b7 Err, \\quad \u03b1\u2208 [0, 1]$, (8)\n\nwhere a is a pre-determined parameter. A higher value of a leads to a higher sparsity-inducing intensity. The objective is to find the A that minimizes Eval. Since A is not explicitly related to Eval, we use the heuristic search by embedding a simulated annealing framework into ICNN, leveraging from its ability to swiftly adapt to adjustments in \u5165. We heuristically adjust A after every specific number of iterations (to"}, {"title": "4.2. High Dimensional Setting", "content": "In the high-dimensional setting, a separate framework is required, as it is impractical to impose a sparsity constraint directly on the low dimensional displacement vectors. In-"}, {"title": "5. Experiments", "content": "Drug Perturbation & Optimal Transport. Optimal transport has become a powerful tool in cell biology for modeling and predicting cellular responses to drug perturbations (Bunne et al., 2023b). It matches perturbed cells to their original state by measuring high-dimensional features such as single-cell RNA (sc-RNA) expression and 4i (Iterative Indirect Immunofluorescence Imaging) data.\nWe conducted experiments to evaluate the performance of our sparsity-induced formulation against the original ICNN OT framework and compared it to the Sinkhorn-based sparsity formulation proposed by Cuturi et al. (2023) on two different cell matching tasks."}, {"title": "5.1. Synthesized sc-RNA Perturbation Task", "content": "Because our first aim was to measure how closely our method could reduce dimensionality to the ground-truth level, we did not run the first task on a real sc-RNA dataset. As real sc-RNA data is unable to allow a reliably accurate determination of the ground truth displacement dimensionality, we built a synthesized dataset for this task, leaving the real dataset for subsequent experiments.\nThe dataset is constructed as follows: we first define n, the number of cells before and after drug perturbation; d, the total number of genes per cell (overall dimensionality); and k, the number of genes truly affected by the drug (ground-truth dimensionality). Random noise is added to the remaining genes to simulate realistic noise observed in drug perturbation scenarios. The control cells are treated as the source, and the perturbed cells are treated as the target. Using different methods, we compute the OT mapping between two groups of cells. Our objective is to evaluate whether these methods can provide an explainable mapping that focuses exclusively on the truly perturbed genes while disregarding the noisy ones."}, {"title": "5.2. Real 4i Perturbation Task", "content": "In this section, we use the preprocessed cell 4i perturbation dataset\u00b9, designed for optimal transport matching as utilized in CellOT (Bunne et al., 2023b), to demonstrate the effect of sparsity induction on the displacement vector.\nThis task assesses the effectiveness of various methods in recovering the OT map, focusing on preserving key features while disregarding less significant ones. Unlike sc-RNA perturbation, where there is a clear separation between noisy and genuinely perturbed cells, recovering the OT map is notably more challenging. Additionally, the absence of a ground-truth map with the true dimensionality further complicates the task. After drug perturbation, the 4i features of the cells are altered, and we use these features to match control cells with perturbed cells. To evaluate the learned map T, we employ two metrics: (i) Average displacement vector dimensionality \u2013 lower values indicate better interpretability; (ii) Wasserstein divergence between T# (P) and Q - lower values indicate better feasibility.\nWe selected four drugs (Ixazomib, Sorafenib, Cisplatin, Melphalan) to highlight the improvement of our method towards the existing methods. As shown in Figure 6, among all four drugs, our method could more effectively reduce the dimensionality of the displacement vector. However, several limitations need to be discussed here:"}, {"title": "6. Conclusions", "content": "We presented a biased, sparsity-inducing formulation based on the unbiased formulation from ICNN OT and demonstrated its reliability in recovering maps with higher displacement sparsity in both low- and high-dimensional experiments. Leveraging ICNN, we also provide a heuristic framework with a flexible objective that can dynamically adjust the sparsity-inducing intensity. Additionally, we adapt this framework to solve the problem with direct constraint on displacement dimensionality in higher-dimensional setting. Several future directions could stem from our work, encompassing both theoretical and empirical aspects. On the theoretical side, these include estimating the theoretical minimized dimension and applying the sparsity-inducing framework to other neural OT solvers (e.g., non-minimax formulations). On the empirical side, directions include studying the effects of hyperparameters and fine-tuning those used in our heuristic framework."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Proofs of Theorems", "content": "Proof of Lemma 4.1: Given the assumption on the bounded support of the target measure and the boundedness of Vg, which is implied by the uniform Lipschitiz continuity of g in S(Q) and convexity, we can to show that \u03c4(\u00b7) is bounded:\n\u2022 The smoothed lo-norm is bounded because the dimensionality of the target measures is bounded.\n\u2022 The l\u2081-norm is bounded because the magnitude of the displacement vectors is bounded.\n\u2022 By the Weierstrass theorem, Tstvs is bounded on the compact region covering the value of \u2207g(y) \u2212 y since it is continuous.\nGiven these bounds, we can conclude that Lemma 4.1 holds for all three sparsity penalties used in our analysis.\nTheorem A.1 (von Neumann's Minimax Theorem). Let X \u2282 R^n and Y \u2282 R^m be two compact and convex sets. Suppose f : X \u00d7 Y \u2192 R is a continuous function that is concave in x for each fixed y and convex in y for each fixed x (concave-convex). Then, the following equality holds:\n\n$\\max_{x\\in X} \\min_{y\\in Y} f(x, y) = \\min_{y\\in Y} \\max_{x\\in X} f (x, y)$.\n\nMoreover, assuming the optimal solution (x*, y*) exists, then it satisfies the following saddle point inequality:\n\n$f(x, y^*) \u2264 f(x^*, y^*) \u2264 f(x, y), \\forall x \\in X, y \\in Y$,\n\nIn the context of ICNN, our sets X and Y are instead topological vector spaces, which places them under the scope of a variant of von Neumann's Minimax Theorem: Sion's Minimax Theorem (Sion, 1958).\nLemma A.2. For equation (4), given f \u2208 CVX(P) and g \u2208 CVX(Q), the function\n\n$V_{P,Q}(f,g) = -E_P[f(X)] \u2013 E_Q [(Y, \u2207g(Y)) \u2212 f(\u2207g(Y))]$"}, {"title": "B. Adapted Heuristic Adjustment Algorithm for High Dimensional Setting", "content": "Our method is straightforward: we first increase A to ensure that the displacement vectors meet the dimensionality constraint. Then, we heuristically decrease A to reduce the intensity of sparsity-inducing, allowing the model to learn a more feasible map. The details of this algorithm are illustrated in Algorithm 2."}, {"title": "C. Hyper-parameter Setup", "content": "Hyperparameters for Figure 4. We use most of the default hyperparameters for ICNN training. For iteration-related hyperparameters, we set nini = 20,000 and ntr = nsm = 2,000. For simulated annealing hyperparameters, we set the initial temperature to 1.0, the minimum temperature to 0.15, the temperature decay rate to 0.95, and the range adjustment parameter p = 3. For the Tstvs penalty, we set y = 100.\nHyperparameters for Figure 5. For the ICNN OT results, we largely use the default hyperparameter settings outlined in the original paper, with a modification to the batch size, which is set to 128 to better accommodate our datasets. For our results, we also use the same set of default parameters as ICNN OT. The parameter for the smoothed lo-norm is set to o = 1.0. For the results from Cuturi et al. (2023), we set the entropic regularization to \u20ac = 10-3 and the sparsity regularization intensity to x = 2.\nHyperparameters for Figure 6. The default training parameters for ICNN are used here as well. We primarily experiment with different sparsity regularization intensities \u5165. For all four drugs, we use the same set of parameters \u03bb(\u03b5) = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 2] for the results from Cuturi et al. (2023), as the outcomes do not vary significantly across experiments. The dimensionality, however, starts to increase as \u5165 approaches 1. For our results, we find that each experiment varies under different parameter settings. Therefore, we adjust the parameters for each experiment to better illustrate how the dimensionality changes with different intensities."}]}