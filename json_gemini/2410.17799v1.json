{"title": "OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation", "authors": ["Qinglin Zhang", "Luyao Cheng", "Chong Deng", "Qian Chen", "Wen Wang", "Siqi Zheng", "Jiaqing Liu", "Hai Yu", "Chaohong Tan"], "abstract": "Full-duplex spoken dialogue systems significantly advance over traditional turn-based dialogue systems, as they allow simultaneous bidirectional communication, closely mirroring human-human interactions. However, achieving low latency and natural interactions in full-duplex dialogue systems remains a significant challenge, especially considering human conversation dynamics such as interruptions, backchannels, and overlapping speech. In this paper, we introduce a novel End-to-End GPT-based model OmniFlatten for full-duplex conversation, capable of effectively modeling the complex behaviors inherent to natural conversations with low latency. To achieve full-duplex communication capabilities, we propose a multi-stage post-training scheme that progressively adapts a text-based large language model (LLM) backbone into a speech-text dialogue LLM, capable of generating text and speech in real time, without modifying the architecture of the backbone LLM. The training process comprises three stages: modality alignment, half-duplex dialogue learning, and full-duplex dialogue learning. Throughout all training stages, we standardize the data using a flattening operation, which allows us to unify the training methods and the model architecture across different modalities and tasks. Our approach offers a straightforward modeling technique and a promising research direction for developing efficient and natural end-to-end full-duplex spoken dialogue systems. Audio samples of dialogues generated by OmniFlatten can be found at this web site 1.", "sections": [{"title": "1 Introduction", "content": "Traditional turn-based spoken dialogue systems only support half-duplex communication, that is, the communication is conducted bidirectionally between user and system but not simultaneously. These systems, while effective in many real-world applications, often fall short when they come to handle interruptions, backchannels, and overlapping speech, which reflect the spontaneous nature of human-human conversation. Conversely, full-duplex spoken dialogue systems allow simultaneous two-way communication, closely mirroring dynamics of human-human conversations. Full-duplex spoken dialogue systems facilitate more natural and efficient interactions than traditional turn-based dialogue systems, by speaking, listening, and thinking at the same time. However, achieving low latency and natural interactions in full-duplex systems remains a significant challenge.\nRecent efforts in developing spoken dialogues systems have been driven by advancements in LLMs and can be roughly categorized into collaborative systems and end-to-end systems. Collaborative systems interface LLM-based dialogue modules with external ASR or TTS modules for speech understanding and speech generation. For example, Qwen-audio (Chu et al., 2024) processes speech input, outputs text and converts them to verbal responses via TTS. In contrast, some end-to-end systems (Zhang et al., 2023; Xie and Wu, 2024; Fang et al., 2024) directly model speech-to-speech dialogues based on speech-text multimodal models; yet these models are predominantly turn-based dialogue models and do not support full-duplex conversation. Recent progresses in developing end-to-end full-duplex spoken dialogue systems include SyncLM (Veluri et al., 2024) and the open-sourced Moshi (D\u00e9fossez et al., 2024). Specifically, Moshi models multiple streams of user's speech input and system's text and speech output in parallel, simplifying handling of full-duplex dialogues. However, this parallel framework is not supported natively by GPT-based models and hence requires sophisticated designs such as acoustic delay and inner monologue. Similar to our approach, SyncLM is also trained to predict interleaved chunks of User and Assistant speech units to acquire real-time full-duplex spoken dialogue capabilities. However, they introduce a deduplication strategy to mitigate the impact of silent speech on the model's semantic capabilities, whereas we enhance the semantic capabilities of the dialogue model through explicit text token prediction.\nTo address the challenges of achieving natural interactions and low latency in full-duplex spoken dialogue systems, we introduce a novel end-to-end GPT-based model OmniFlatten for full-duplex speech conversation. OmniFlatten is capable of effectively learning the complex behaviors inherent to natural conversations and facilitates human-like conversations with low latency. We propose a multi-stage progressive post-training scheme to adapt a text-based large language model (LLM) backbone into a robust speech-text dialogue model by first conducting modality alignment, and then performing dialogue learning by interleaving and flattening the multiple streams of speech and text of dialogues into a single sequence. Notably, our approach does not change the architecture of the backbone text-based LLM nor relies on computationally intensive pre-training.\nThe multi-stage post-training process begins with supervised multi-task fine-tuning of the text LLM backbone to achieve speech-text modality alignment and obtain a mutimodal LLM, using ASR and TTS tasks. This stage is essential for ensuring that the system can accurately interpret and generate both speech and text, forming a seamless interface for human-computer interaction.\nAfter obtaining the speech-text LLM, we fine-tune it using interleaved and serialized dialogues through three progressive stages. In the first stage, we train the model on half-duplex dialogues by flattening both text and speech streams of user input and system output into a single sequence (that is, flatten four-stream data). This stage serves as a preliminary step to train the model to learn half-duplex dialogue capabilities. In the second stage, we further remove the user input text stream, perform fine-grained chunking and alignment on the remaining three steams, flatten the chunks, and continue training the model with the resulting flatten three-stream data. This step aims to train the model to learn real-time multi-stream prediction capability, thereby enabling full-duplex communication. Finally, we construct flatten two-stream data comprising only input and output speech, and continue training the model to focus on speech-to-speech generation, hence eliminating the dependence on intermediate text and thereby reducing latency and bringing the system closer to real-time interaction. Our approach presents a straightforward yet innovative modeling technique, offering a promising research direction for developing efficient and natural end-to-end full-duplex dialogue systems.\nThe main contributions of this work can be summarized as follows:\n\u2022 We propose a novel End-to-End GPT-based model OmniFlatten, capable of effectively modeling the complex behaviors inherent to natural human-like dialogues, with low latency. We propose a multi-stage post-training scheme that successfully adapts a text-based foundation LLM into a robust speech-text dialogue model, by performing supervised multi-task fine-tuning based on ASR and TTS for speech-text modality alignment, then conducting fine-grained chunking of speech and text streams of dialogues and flattening them into a single sequence to progressively train the model to acquire half-duplex and full-duplex communication capabilities. Notably, OmniFlatten does not make any structural modifications to the GPT model, nor relies on computationally intensive pre-training.\n\u2022 Our experiments verify effectiveness of the modality alignment stage, as the resulting model produces acceptable ASR and TTS performance. We evaluate the dialogue quality generated by OmniFlatten using high-performing LLMs as evaluators, and evaluate the turn-taking performance, including system taking turn and user taking turn, as well as the run-time efficiency. The results demonstrate that the dialogues generated by OmniFlatten exhibit reasonable quality, with both modality alignment and half-duplex learning stages improving the model's full-duplex dialogue capabilities. OmniFlatten is much better at handling system taking turn than user taking turn, with average response time of 160ms and 805ms for system taking turn and user taking turn, respectively."}, {"title": "2 Related Work", "content": "In recent years, the field of spoken dialogue models has seen significant advancements driven by the prominent technological advancements in LLMs and particularly speech-text multimodal models. Qwen-audio2 (Chu et al., 2024) and SALMONN (Tang et al., 2024), support voice input but only output text; hence, they rely on external TTS systems to synthesize speech output. SpeechGPT (Zhang et al., 2023), LauraGPT (Du et al., 2024b), Mini-Omni (Xie and Wu, 2024) and LLaMA-Omni (Fang et al., 2024), are capable of comprehending both speech and text input and generating output in both speech and text; however, they are predominantly turn-based dialogue models and do not support full-duplex conversation.\nAnother related work in this area models is VITA (Fu et al., 2024), which involves a duplex scheme by two separate modules: one model generates responses to user queries while another continuously monitors environmental inputs to selectively provide updated interactions.\nAnother related work is Moshi (D\u00e9fossez et al., 2024), which models several audio streams in parallel, allowing for a conceptually and practically simple handling of full-duplex dialogues. This approach, compared to its predecessors, offers a more robust solution for managing simultaneous voice inputs and outputs, thereby facilitating more natural and efficient conversational interactions.\nThe most related concurrent work in this area is SyncLM(Veluri et al., 2024), which also achieves real-time full-duplex interaction through time-chunking methods. However, that paper employs a deduplication strategy for audio discrete sequences. While this approach reduces modeling complexity, it introduces errors during reconstruction. In contrast, we do not apply any additional operations to the discretized audio tokens, avoiding degradation in audio reconstruction. Furthermore, we propose a more direct modality alignment with ASR & TTS task and progressive learning method to enhance the effectiveness of full-duplex dialogue. Our full-duplex model offers two modes, with the 3-stream mode capable of simultaneously outputting text token sequences and audio token sequences. However, SyncLM only provides a mode for outputting speech tokens during full-duplex dialogue."}, {"title": "3 Methodology", "content": "In this section, we introduce our end-to-end full-duplex conversation model OmniFlatten. As illustrated in Figure 1, we employ audio tokenizer to discretize each input and output speech stream in a dialogue into a discrete speech token sequence. We then interleave the speech token sequences together with the text token sequences and flatten them into a single sequence. Our approach employs a multi-stage progressive training process to transform a text-based LLM into a robust end-to-end full-duplex spoken dialogue model with modality alignment and dialogue learning. We will elaborate the key components of our approach, including Audio Tokenization and Detokenization, Modality Alignment, and Dialogue Learning, in the following subsections."}, {"title": "3.1 Audio Tokenization And Detokenization", "content": "To convert continuous speech signal into discrete token sequences, we adopt the speech tokenizer used in CosyVoice\u00b2 (Du et al., 2024a; An et al., 2024) since through supervision from multilingual ASR, this speech tokenizer can convert speech to semantic tokens, hence benefiting speech understanding and content consistency for speech generation. The tokenizer utilizes an encoder and a Vector Quantization (VQ) layer to discretize audio signals into speech tokens with a single codebook of 4096 codes. When converting discrete speech tokens back into audio, we adopt the same Optimal-transport Conditional Flow Matching model (OT-CFM) as used in CosyVoice. OT-CFM transforms the speech token sequence into Mel spectrogram, which is then used to generate the final audio output with the HifiGAN vocoder (Kong et al., 2020). Prior works (Lipman et al., 2023; Tong et al., 2024) show that OT-CFM outperforms diffusion probabilistic models (DPMs) with simpler gradients, easier training, and faster generation."}, {"title": "3.2 Modality Alignment", "content": "We start with post-training a pre-trained text-based LLM backbone to obtain a speech-text LLM for speech understanding and speech generation. We use Qwen2-0.5B\u00b3 as our base model due to its small size for low compute resource requirements and its competitive performance for models with this small size. We perform supervised fine-tuning (SFT) using paired speech-text data for ASR and TTS tasks. This process adapts the pre-trained text LLM to speech-text multimodal model. For each speech-text pair < Sseq, Tseq >, we construct training samples as follows:"}, {"title": "3.3 Dialogue Learning", "content": "Building upon the aforementioned speech-text multimodal model, we conduct dialogue learning in three stages, including half-duplex dialogue training using both speech and text streams of turn-based dialogue data, and then full-duplex dialogue training based on fine-grained chunking and alignment of speech and text sequences. Specifically, during full-duplex dialogue training, we first remove the input text stream and use the remaining three streams for training, and then further remove the output text stream and use the remaining two streams for training, in order to gradually eliminate the dependence on text information, focus on speech-to-speech generation, and reduce latency. During all three stages, we interleave and flatten the multi-stream dialogue data into a single sequence."}, {"title": "3.3.1 Half-duplex Dialogue Training", "content": "Half-duplex dialogue agents are special and simpler cases of full-duplex dialogue agents, where Human and Assistant take turns to speak and there is no overlapping speech, that is, during the speaker's turn, the listener is fully silent. Since there is no overlapping speech in the ASR and TTS data used for learning modality alignment, half-duplex dialogue training is more consistent with the aligned multimodal model than full-duplex dialogue training which requires the model to handle turn-taking, backchannel, and overlapping speech. Adopting the concept of curriculum learning, we first conduct half-duplex dialogue training then full-duplex dialogue training. During half-duplex dialogue training, we train the model to essentially perform ASR on the speech tokens of User and obtain the text content, next generate a textual response for Assistant based on the text content of User, and then predict the speech tokens for Assistant's textual response by basically executing a TTS task. This pattern is extended to multiple turns of a dialogue, as illustrated in Figure 2."}, {"title": "3.3.2 Full-duplex Dialogue Training", "content": "Training on Three-Stream Data A human-like full-duplex conversational agent is required to handle simultaneous bidirectional conversations with low latency. In order to meet the real-time requirements, we remove the User text stream from the four-stream data and use the remaining three steams of data for training. In order to handle overlapping speech, we introduce chunking and relaxed speech-text token alignment based on the chunks; in this way, we do not require strict token-level alignment between speech and text. Specifically, to prepare the training data for this stage, we chunk the speech and text token sequences of dialogue data with fixed chunk sizes and then interleave the three-stream data and flatten them into a single sequence for training, following the order of input speech, output text, and output speech. Notably, given the higher efficiency of text, the size of the text chunks is generally smaller than that of the speech chunks. In this work, we set the text chunk size to 2 tokens and the speech chunk size to 10 tokens. This approach ensures that the output text does not excessively precede the speech content, thereby both minimizing the discrepancies with the aforementioned 4-stream data format and maximizing preservation of the TTS task. After the end of the text content, we use the special character silent_text_token to pad the text stream and use silent_speech_token to pad the silent regions of the output speech stream. This training process based on chunked three-stream data is illustrated in Figure 3.\nTraining on Two-Stream Data In order to further reduce latency and eliminate the dependence on intermediate text and hence focus on speech-to-speech generation, we further remove the output text stream and retain only the input and output speech stream. This training process based on chunked two-stream data is illustrated in Figure 4."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Data", "content": "Modality Alignment Dataset The objective of the training stage for modality alignment (Section 3.2) is to help the model learn the correspondence between speech tokens and text tokens and enable the model to acquire two key capabilities: ASR and TTS. To achieve this goal, we combine a set of TTS and ASR datasets that consist of both open-source and proprietary data. The open-source datasets comprise both Mandarin and English data, including Aishell-3 (Yao Shi, 2015), LibriTTS (Zen et al., 2019), TED-LIUM (Hernandez et al., 2018), VoxPopuli (Wang et al., 2021), Librispeech (Panayotov et al., 2015), and MLS (Pratap et al., 2020). Additionally, we incorporate several proprietary ASR and TTS datasets. In sum, the datasets utilized for the speech-text modality alignment encompass approximately 100K hours of audio. Among the datasets, 30% are open-source data and 70% are proprietary data.\nSimulated Voice Chat Dataset For constructing the voice chat data for Dialogue Learning (Section 3.3), we design a data synthesis and simulation pipeline to synthesize dialogue data. Firstly, we collect a substantial amount of high-quality open-source textual dialogue data for subsequent speech synthesis, including Alpaca (Peng et al., 2023), Moss (Sun et al., 2024), BelleCN (Ji et al., 2023), and ultraChat (Ding et al., 2023). We then use heuristic rules and filter out samples that are inappropriate for TTS, for example, samples that comprise a high percentage of non-text elements such as code and mathematical expressions, samples with more than 200 words in English or Chinese, and also samples containing rare or unusual symbols. In the end, we retain approximately 360K multi-turn sessions of turn-based dialogues (half-duplex dialogues).\nSecondly, we create pseudo full-duplex voice chat based on these textual, multi-turn turn-based dialogues. We synthesize the audio of each turn for the filtered textual dialogues using CosyVoice. Specifically, we first sample speaker embeddings from Librispeech (Panayotov et al., 2015) and 3DSpeaker (Zheng et al., 2023b) datasets to obtain diverse timbre for audio synthesis. After we synthesize speech for each turn, we use a simulation strategy to arrange each speaker-turn audio at a suitable time point in each speaker channel, so that this conversation mimics a natural interaction, that is, each speaker speaks alternately, with occasional interruptions and pauses, and the context is smooth. The detailed process is as follows.\nWe organize the synthesized conversation audio into two channels, with the first channel being the User channel and the other being the Assistant channel. Note that the textual conversation always starts with the User, and then the User and the Assistant speak alternately. After the User speech ends, we start the next Assistant speech immediately so that the Assistant answers the User promptly. After the Assistant speech ends, we sample the pause duration between the end time of the Assistant's turn and the start time of the next User's turn from a normal distribution. In this way, we create the audio corresponding to the interleaved dialogue between User and Assistant.\nThirdly, to mimic real-world scenarios of the User audio channel, we also sample background noise from the MUSAN noise dataset (Snyder et al., 2015) and add noise into the User audio channel. We control the signal-to-noise ratio (SNR) between 15 dB and 25 dB. Based on this data synthesis and simulation pipeline, we generate a total of 2,000 hours of multi-channel spoken dialogue data. Based on this dataset, we used 1% of the dataset as the validation set and another 1% as the test set, with the remaining data used as the training set."}, {"title": "4.2 Training and Inference Setup", "content": "We use QWen2-0.5B (Yang et al., 2024) as the base model. During the modality alignment training phase, the maximum sequence length is set to 1024 tokens. In the dialogue learning phase, the maximum sequence length is extended to 8192 tokens. We use the standard cross-entropy loss as our training objective in all stages. Additionally, during the dialogue learning phase, we apply loss masking on the User channel, as we observe that this operation enhances the stability of model training, probably due to the presence of noisy audio input in the User channel. We employ the AdamW optimizer with a weight decay of 0.1, B1 of 0.9, and 32 of 0.95. The maximum learning rate is set to 2e-05, with warm-up and cosine-decay. We train the model with five epochs, with the best model selected based on loss on the validation set. The batch size is set to 100 million tokens. Our code implementation is based on the NanoGPT project 4.\nDuring inference, to obtain the Assistant textual response prediction from the model, we use the ground truth User channel speech in the test set as the fixed speech input and alternately fill in the predicted Assistant speech and text with fixed speech chunk and text chunk sizes."}, {"title": "4.3 Evaluations", "content": "The performance of ASR and TTS tasks after Modality Alignment The Modality Alignment training stage (Section 3.2) aims to help the model learn the correspondence between speech tokens and text tokens and acquire ASR and TTS capabilities; hence, we evaluate the effectiveness of this training stage by evaluating the ASR and TTS performance of the resulting aligned multimodal model. For the ASR evaluation, we use the model to decode the discrete speech tokens corresponding to the input speech into text output. For the TTS evaluation, we use the model to generate speech tokens based on the input text, which are then synthesized into audio using a random English female voice from CosyVoice. The synthesized audio is subsequently recognized using the Whisper Large V3 model (Radford et al., 2023) 5 and the ASR output is scored against the input text. Both ASR and TTS evaluations are conducted on the publicly available Librispeech and WenetSpeech (Zhang et al., 2022) datasets, and employ Character Error Rate (CER) as the evaluation metric. Note that CER could measure synthesis accuracy and robustness of a model's TTS capabilities and also to a significant extent, reflect the audio quality. Also, the main goal of this work is learning conversation dynamics of full-duplex voice chat; therefore, for TTS evaluation in this paper, we have not adopted standard evaluation metrics for speech quality yet, such as Mean Opinion Score (MOS).\nFor ASR evaluation, we compare the speech-text aligned multimodal model after the Modality Alignment training stage (denoted by OmniFlatten) with the Whisper Large V3 model. For TTS evaluation, GT Speech Tokens denotes discretizing the ground truth waveforms into speech tokens and detokenizing them into speech using the same English female voice. As shown in Table 1, OmniFlatten demonstrates considerable performance in both ASR and TTS tasks. These results indicate that the Modality Alignment training stage effectively transitions the unimodality text-based LLM to speech-text multimodal model with reasonable speech comprehension and generation capabilities for the following dialogue learning.\nThe Impact of Modality Alignment and Half-duplex Dialogue Learning on Full-duplex Conversation Capability As described in Section 3.3.2, the training stage of full-duplex dialogue learning on three-stream data helps the model acquire full-duplex dialogue capacity and this model generates both speech and text for Assistant. Prior works demonstrate that competitive text-based LLMs can be reliable evaluators for various natural language generation tasks, as scores assigned by LLM evaluators on generated text show high correlations with human evaluations (Zheng et al., 2023a). Therefore, we evaluate the full-duplex dialogue capabilities of OmniFlatten, by prompting a competitive text LLM to evaluate the semantics of dialogues and assign a score on the predicted Assistant textual response from the model after training on three-stream data. Notably, OmniFlatten after the final stage of training on two-stream data only outputs Assistant speech, which is challenging to be evaluated by text-based LLMs.\nThe scoring mechanism involves designing a specific prompt and utilizing a competitive text LLM, QWen-max model 6, to rate the responses from the model on a scale of 1 to 10 points. The specific prompt we use for LLM scoring is detailed in Appendix A. We carefully design the prompt to evaluate fluency and coherence of the predicted Assistant textual response. We also report the CE loss of the model on the test set.\nTo analyze the impact of the modality alignment training stage (Section 3.2) and the half-duplex dialogue learning stage (Section 3.3) on the full-duplex conversation capability of OmniFlatten after training on three-stream data, we compare the LLM scores on the predicted Assistant textual responses from the following models:\n\u2022 QWen2-0.5B trained directly on three-stream data (denoted by OmniFlatten w/o modality alignment w/o half-duplex training).\n\u2022 QWen2-0.5B trained with modality alignment and full-duplex dialogue training on three-stream data (denoted by OmniFlatten w/o half-duplex training).\n\u2022 QWen2-0.5B trained with modality alignment, half-duplex dialogue training, and full-duplex dialogue training on three-stream data (denoted by OmniFlatten).\n\u2022 Ground truth textual response in the test set (denoted by GT Response).\nThe results in Table 2 show that both Modality Alignment and Half-duplex Training stages improve the LLM scores of the predicted Assistant textual response, indicating that both stages contribute to enhancing the full-duplex dialogue performance of the model and the multi-stage training strategy effectively augments the end-to-end full-duplex spoken dialogue capabilities of the model.\nTurn-taking Performance and Runtime Efficiency To evaluate the naturalness of OmniFlatten's full-duplex interactions, we assess whether Assistant can promptly respond after User finishes speaking (that is, Assistant taking turn), and whether Assistant can stop speaking promptly when User attemps to interrupt (that is, User taking turn). We define the following metrics.\nAssistant Turn-taking Acc@k: This metric is defined as whether Assistant correctly predicts a non-silence token at the k-th token after the end of a semantically meaningful speech token from User, indicating that Assistant has taken the turn and starts speaking.\nUser Turn-taking Acc@k: This metric is defined as whether Assistant correctly outputs a silence token at the k-th token after a semantically meaningful speech token is input from User while Assistant is speaking. This metric indicates that Assistant has successfully responded to the turn-taking attempt from User by stopping Assistant's speech and entering the state of listening. Note that in our simulated pseudo full-duplex dialogue data, User inputs are always considered as User taking turn since backchannels are not accounted for in this scenario.\nThe evaluation results are shown in Table 3. We make the following observations. (1) With speech chunk size 10 as used in this work, Assistant can quickly respond when User finishes speaking. Assistant Turn-taking Acc rises to 55.7% at the 5-th token and 71.3% at the 10-th token. In contrast, User Turn-taking Accs are quite poor, only 30% at the 25-th token. This is because our synthetic full-duplex data is constructed based on turn-based textual dialogues and do not cover natural scenarios of User interrupting Assistant's speech and taking turn. In future work, we plan to refine the data synthesis pipeline and better simulate the complex interaction patterns in real-world full-duplex interactions, such as User interrupting and taking turn, and backchannels. (2) The average response time of Assistant taking turn is 160 ms and that of User taking turn is 805 ms. This discrepancy can be attributed to the fact that Assistant's turn-taking occurs at the end of User's turn; hence, a substantial amount of semantic information is available at that point so the system could respond promptly. Conversely, User's turn-taking occurs at the beginning of the turn, before the semantic context is to be fully established, hence the system requires longer response time to make the decision, stopping speaking and yielding the turn. (3) We observe that using a larger speech chunk size achieves better User turn-taking accuracy and also Assistant turn-taking accuracy at large Ks. We hypothesize that this is because larger speech chunk sizes can provide more comprehensive semantic information for turn-taking prediction. However, larger chunk sizes also require longer prediction time and in turn increase Assistant's turn-taking response time. The impact from different speech chunk sizes on"}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we introduce an end-to-end full-duplex spoken dialogue model OmniFlatten based on synthesizing full-duplex spoken dialogue data and designing a multi-stage progressive training paradigm for modality alignment and dialogue learning. Our approach offers a straightforward full-duplex modeling scheme as it does not change the architecture of the backbone text-based LLM nor relies on computationally intensive pre-training. Empirical evaluations show that the proposed approach is promising for developing end-to-end models to handle full-duplex interactions.\nIn future work, we plan to refine the data synthesis pipeline and better simulate the complex interaction patterns, such as User interrupting and taking turn, and backchannels, in real-world full-duplex interactions. Additionally, we will explore more potentials of this modeling scheme by extending to full-duplex interaction involving more modalities, such as vision."}, {"title": "A Appendix", "content": "Auto-Evaluation Prompt\nPlease rate the given dialogue context and response from the voice dialogue system based on the following criteria (1-10 points), and provide a brief evaluation:\n1. Relevance: Is the response relevant to the query? Is the content related?\n2. Accuracy: Does the response correctly address the user's query and provide accurate information?\n3. Completeness: Does the response comprehensively cover all aspects of the query?\n4. Conversational Nature: Is the response easy to understand, concise, clear, and fluent?\nContext: context\nResponse: response\nOutput in JSON format:\n}\n\"Strengths\": \"Positive aspects of the response\",\n\"Weaknesses\": \"Negative aspects of the response\",\n\"Overall Evaluation\": \"Overall assessment of the response\",\n\"Total Score (out of 10, directly provide the score)\":"}]}