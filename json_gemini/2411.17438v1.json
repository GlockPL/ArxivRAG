{"title": "Object-centric proto-symbolic behavioural reasoning from pixels", "authors": ["Ruben van Bergen", "Justus H\u00fcbotter", "Pablo Lanillos"], "abstract": "Autonomous intelligent agents must bridge computational challenges at disparate levels of abstraction, from the low-level spaces of sensory input and motor commands to the high-level domain of abstract reasoning and planning. A key question in designing such agents is how best to instantiate the representational space that will interface between these two levels-ideally without requiring supervision in the form of expensive data annotations. These objectives can be efficiently achieved by representing the world in terms of objects (grounded in perception and action). In this work, we present a novel, brain-inspired, deep-learning architecture that learns from pixels to interpret, control, and reason about its environment, using object-centric representations. We show the utility of our approach through tasks in synthetic environments that require a combination of (high-level) logical reasoning and (low-level) continuous control. Results show that the agent can learn emergent conditional behavioural reasoning, such as (A \u2192 B) \u2227 (\u00acA \u2192 C), as well as logical composition (A \u2192 B) \u2227 (A \u2192 C) \u2192 A \u2192 (B \u2227 C) and XOR operations, and successfully controls its environment to satisfy objectives deduced from these logical rules. The agent can adapt online to unexpected changes in its environment and is robust to mild violations of its world model, thanks to dynamic internal desired goal generation. While the present results are limited to synthetic settings (2D and 3D activated versions of dSprites), which fall short of real-world levels of complexity, the proposed architecture shows how to manipulate grounded object representations, as a key inductive bias for unsupervised learning, to enable behavioral reasoning.", "sections": [{"title": "1. Introduction", "content": "A one-year old infant, before language is expressed [1], learns the efferent-afferent patterns of sensory stimulation and motor commands to a neural representation of the environment. From the sensorium, to abstract thought, and back to the minutiae of the sensorimotor domain, this interaction defines the feats of higher-order cognition and are the backdrop of the different levels of abstraction at which intelligent agents must operate, when they interact with the environment, which is not fully observable [2]. How sub-symbolic computations transform into higher-level cognitive representations of structures is far from being understood [3]. However, there is common accepted idea that suggest that there are intermediate representations that bridge cognitive reasoning and behaviour [4]. Hence, in the design of artificial agents, a key challenge is first to find a representational space that provides an effective interface between these disparate demands and second a mechanism that makes proper use of this representation to interact with the environment and produce behaviour. This is, we define reasoning from a behavioral perspective where the output of a reasoning process is always an action that physically interacts with the environment.\nTo address these challenges in a single, comprehensive system, here, we introduce a novel, brain-inspired neural network architecture that spans the domains of cognitive reasoning, perceptual inference, planning and continuous control. We follow both the emergentist and probabilistic approach to cognition, where the representational format, and the machinery to transform sensory observations into this space, is learned unsupervised but allows reasoning as an inference process. This also avoids the dependency on ground-truth labels furnished by human annotators, as these are highly labor-intensive to produce, especially when labels must cover all the relevant variables in a scene. The proposed architecture leverages the core inductive bias that the environment can be partitioned into discrete entities, or objects, that obey many useful symmetries and invariances [5]. Object-based representations and reasoning are a key tenet of perception and cognition in humans [6]. Objects constitute the environment's separable, movable parts, as well as the logical units of reasoning and planning. They naturally take on properties of symbols, as different objects obey the same laws of physics and possess similar or analogous properties [7]. At the same time, each object representation is tethered to the sensorimotor domain via explicit attention maps. Objects, thus, are an ideal level of abstraction to interface between the disparate levels of computation that we require. We show the promise of our approach by evaluating it in tasks that require both high-level reasoning and continuous control in synthetically generated environments."}, {"title": "2. Related work, challenges and contribution", "content": "Structured representation learning appeared as a powerful way to introduce inductive biases and scale artificial intelligence to high-level cognition [8] and exploit symmetries and invariances that can be leveraged by decomposing scenes into their natural constituents [9]. Particularly, object-centric representations provide a natural interface between bottom-up (e.g., emergentist, connectionist) and top-down (e.g., graph based, probabilistic) approaches [7]. We summarize the closed-related works that influenced the proposed architecture classified in four topics\u00b9: scene understanding (e.g., visual segmentation), physics-informed and interaction (e.g., simulation), behaviour (e.g., robotics) and reasoning (e.g., object relations). We analyse these works from the prism of behavioural reasoning.\nVisual segmentation and tracking. Visual segmentation methods have reached maturity with high accuracy on segmentation in both static images [10] and videos [11]. From IODINE [10] that used IAI, recent methods evolved to fully amortized slot-attention architectures [12] and diffusion approaches [13]. These methods, which can track 2D and 3D objects in cluttered scenes [14], capture object collisions and discover new objects [15], are designed for perception and not for reasoning or control the segmented objects.\nPhysics-informed and interaction. Physics methods have also shown a great potential in handling complex non-linear interactions. These are strongly influenced by the simulation approach to cognition [4]. For instance, interaction [16] and propagation [17] networks are able to understand complex scene dynamics with multiple objects and even control them to reach a desired known state. Unfortunately, these methods require full observation of the objects states. Approaches that can work directly on pixels make use of ground-truth segmentation masks [18] or Visual interaction networks [19]. While these approaches have some kind of physical \"reasoning\" a full-fledged architecture that performs 1st order logic reasoning from pixels and transforms to behaviour (control) is still not fully accomplished.\nRobotics. These works focus on moving the objects meaningfully and usually have full access to the objects states (e.g. [20]). OP3 [21] is an outstanding exception. However, it has two key limitations. First, it is restricted to a discrete action space of picking up and placing objects, with no continuous control. Second, it has no ability to learn tasks-it can only plan actions towards objectives specified ad hoc by means of a goal image that shows exactly what the scene in question ought to look like. Novel approaches use ReFs to describe the 3D representation of the object and control is solved by RRTs in the latent prediction dynamics and Model Predictive Control (MPC) to execute the actions [22]. Still, ground truth objects mask are used for training and inference. Some methods have overcome this constraint through entity-based segmentation\u2014as a simplification of object-centric representation that uses the center of the object as the location of the entity-and RL to overcome with the generation of meaningful behaviours [23]. Unfortunately, this restricts the shape of objects to \"point mass\" like. Besides, there are object-centric approaches that instead of focusing on the interactive behaviour between the agent and the objects, they focus on view point matching [24] and agent navigation [25].\nReasoning. Leaving out the literature on visual scene understanding with language, reasoning with object-centric representations as neurosymbols is, while the most promising, the least investigated area [26]. There are just a few object-centric studies on visual reasoning in static images [27]. Furthermore, there are relevant works from Large Language Models research (e.g., [28]), but they do not adhere to the grounding paradigm proposed, where reasoning should appear as an emergent property of learning a world physical model [29]. For instance in [30] the reasoning capabilities are much more expressive than our proposed approach but they use pre-trained skill, which are conditioned on the language generated by the LLM. Conversely, our architecture learns the world dynamics and interaction through unsupervised learning, harnessing the construction of the proto-symbols while interacting. This does not prevent the possibility of connecting proposed architecture to a LLMs similarly to [31], but through the preference network, which is already grounded."}, {"title": "2.1. Current challenges and architecture decisions", "content": "There are two major challenges at the unsupervised learning with object-centric representations: working with complex naturalistic scene images [13] and the generation of meaningful physical actions (i.e., interacting through continuous control) derived from the reasoning process when the input is high-dimensional (i.e., image) and the environment is partially-observable. This work focus on the latter. We show that simple conditional reasoning problems, such as, \u201cif there is a heart move squares to the left, otherwise move squares to the right\u201d ((A \u2192 B) \u2227 (\u00acA \u2192 C)) is already unsolvable for non-object-centric representations SOTA algorithms-See Sec. 4. An underlying problem for both challenges is to achieve high performance in world state dynamic estimation (world model learning), but allowing adaptation and generalization. We designed the object-centric backbone architecture to incorporate adaptation, following the free energy principle theory of information brain processing [32, 33], where the brain estimates the world by continuously approximating its internal model, learn by experience, to the real world through approximate Bayesian computation. To implement it, we opted for an iterative variational inference approach inspired by the IODINE [10] architecture. Whilst iterative methods have shown less performance than amortized approaches (e.g., slot-attention architecture [12]), at least at perceptual tasks, it provides Bayesian filtering and smoothing, thus allowing better counterfactual reasoning. Finally, another relevant challenge, which is not addressed in this work, is to deal with complex non-linearities of physical contact, such as collisions. Physics-based simulation based approaches [18] and Deep Reinforcement Learning (RL) [23] is showing the best results but they still provide the ground truth object masks or entail oversimplified segmentation in the visual input respectively."}, {"title": "2.2. Contribution", "content": "This work provides a neural network architecture (Fig. 1) that can learn 1st order conditional behavioural reasoning where proto-symbols\u00b2 take the form of object-centic representations and perception, control and preferences (desired internal state of the agent) are dynamical process computed through approximate Bayesian inference, in an unsupervised learning scheme from pixels. Our approach draws inspiration on natural intelligence, where the agent function is to generate behaviours through reasoning conditioned on perceptual cues by optimizing a single quantity: the variational free energy (or evidence lower bound) [32]. To this end, we combine bottom-up and top-down object-centric approaches [35, 36] following the emergentist approach (with unsupervised learning from pixels) but allowing probabilistic inference on the structured representation. This permits the agent to learn: i) what is an object (scene understanding), ii) how to mentally manipulate it through object-centric rules (reasoning) and iii) generate physical behavior (i.e., continuous control). The agent can only perceive through visual input (2D image projection) of a synthetic world composed of 2D or 3D objects (see Fig. 2a for schematic). It can interact with the environment by applying forces to locations in the image, which corresponds to forces in the 2D/3D world. This architecture allows the agent to learn complex conditional rules, such as (A \u2192 B \u2227 C) \u2227 (\u00acA \u2192 D \u2227 E), perform logical composition (A \u2192 B) \u2227 (A \u2192 C) \u2192 A \u2192 (B \u2227 C) by learning two different rules separately but then they emerge combined during execution, and XOR operations, such as (A \u2228 B) \u2192 C) \u2227 (A \u2227 B \u2192 D). Once the agent learns the rules, reasoning transforms into a behavioural response that changes the environment towards its internal preference. Furthermore, the agent, using the proposed architecture, shows online adaptation to unexpected changes in its environment, is robust to mild violations of its world model, and generalize to the number of objects."}, {"title": "3. Proposed neural network model", "content": "The Object-centric Behavioural Reasoner (OBR) proposed, depicted in Fig. 1, is an object-centric deep learning architecture consisting of two interconnected modules: i) perceptual inference, in charge of learning object representations (properties and dynamics) and producing top-down attention and ii) action inference, with the reasoning and control, in charge of learning proto-symbolic rules at the level of object representations and transforming them into meaningful behaviors (i.e., control commands)."}, {"title": "3.1. Perceptual inference and generative model", "content": "The perception module infers the object state and action beliefs from incoming sensory data, e.g., one RGB image (frame) per timepoint, and the agent's own motor efferents (see Appendix 6.1).\nObject-centric representation. We define the k-th object at time t as a state vector $s_t^{+(k)}$. This vector is expressed in second-order generalized coordinates - that is, it is a concatenation of the current state and its derivative: $s_t^{+(k)} = [s_t^{(k)}, \\dot{s_t}^{(k)}]$. Objects are influenced by actions from the agent. The action (or action-effect) on object k at time t is denoted by $a_t^{(k)}$. OBR represents its knowledge about object states and actions through variational beliefs {$q(s_t^{+(k)})$\\} and {$q(a_t^{(k)})$\\}, which are mean-field Gaussian distributions parameterized by a mean and variance for each latent dimension. To learn object-centric representations and perform inference from pixels, we use a set of K weight-sharing iterative variational autoencoders (itVAEs) [37, 10], where K is the number of object representations to be inferred. These itVAEs together invert a generative model in which image pixels are drawn from a Gaussian mixture distribution:\n$p(o_i|{\\{s_t^{(k)}\\}_{k \\in 1:K}}) = \\sum_k m_{ik} N (g_i(s_t^{(k)}), \\sigma^2)$ (1)\n$m_{ik} = p(m_i|\\{s_t^{(k)}\\}_{k \\in 1:K}) = Cat (Softmax (\\{\\pi_i(s_t^{(k)}\\}_{k \\in 1:K}))$ (2)\nwhere $o_i$ is the value of the i-th image pixel, $g_i(\\bullet)$ is a decoder function that translates an object state to a predicted mean value at pixel i, $\\sigma^2$ is the variability of pixels around their mean values, and $\\pi_i(\\bullet)$ maps an object state to a log-probability at pixel i, which defines the predicted probability $m_{ik}$ that the pixel belongs to that object (its segmentation mask). We implement $g_i(\\bullet)$ and $\\pi_i(\\bullet)$ jointly in the decoder of the itVAE, which thus outputs 4 channels per pixel (3 RGB color values + 1 mask logit). Each itVAE thereby predicts its object's subimage and segmentation mask, and a full image prediction can be obtained as a segmentation-weighted superposition of subimages."}, {"title": "Object dynamics", "content": "To complete the world model, this per-frame observation model is combined with linearized object-centric dynamics $p(s_t^{(k)}|s_{t-1}^{(k)}, a_t^{(k)})$ defined by the following equations:\n$s_t^{(k)} = s_{t-1}^{(k)} + D(a_t^{(k)}) + \\epsilon_1$,\n$\\dot{s_t}^{(k)} = \\dot{s_{t-1}}^{(k)} + \\ddot{s} + \\sigma \\epsilon_2$ (3)\nwhere $\\epsilon_1$, and $\\epsilon_2$, are noise realizations drawn from a Normal distribution. Note that we are assuming that the latent representation dynamics can be captured with a 2nd-order generalized coordinates with additive noise, but this is not imposed to the environment\u00b3 The action $a_t^{(k)}$ on object k at time t is a (2-D or 3-D) vector that specifies the control command (e.g., acceleration) on the object in environment coordinates. Finally, D is a learned function that transforms the control command to its effect in the model's latent space."}, {"title": "Learning and inference", "content": "Inference for each object is performed within a sliding window spanning the present and a number of past time points. Each itVAE infers beliefs for a single object and time point, taking a total of 8 inference iterations to refine these beliefs to their optimal (minimum-ELBO) values. Inference is coupled between itVAEs through information flow between represented objects and between time points in the inference window (see Appendix 6). OBR optimizes a composite ELBO loss $L_{comp} = \\sum \\limits_{n=1}^{N_{iter}} \\mathcal{L}^{(n)}$ for both learning and inference, where n indexes inference iterations and $\\mathcal{L}$ is:\n$\\mathcal{L} = - \\sum \\limits_{t=0}^{T} \\mathbb{H}(q\\{(\\dot{s_t}^{(k)}, a_t^{(k)})\\}) + \\beta \\mathbb{E}_{q\\{(\\dot{s_t}^{(k)}\\})} [log p(o_i|\\{s_t^{(k)}\\})]$\n$\\mathbb{H}(q\\{(\\dot{s_t}^{(k)}, a_t^{(k)})\\})$ + & $\\beta \\mathbb{E}_{q\\{(\\dot{s_t}^{(k)}\\})} [log p(o_i|\\{s_t^{(k)}\\})]$ \nComplexity & Reconstruction accuracy\n$+ \\sum \\limits_k \\mathbb{E}_{q(a_t^{(k)})} [log p(a_t^{(k)}|\\Psi)] + \\sum \\limits_k \\mathbb{E}_{q\\{(\\dot{s_t}^{(k)}\\})} [log p(s_t^{(k)}|s_{t-1}^{(k)}, a_t^{(k)})]$ (4)\nAction inference accuracy & Temporal consistency\nwhere $\\mathbb{H}()$ denotes entropy. Similar to previous work (e.g. [10]) we up-weight the reconstruction accuracy term in this loss by a factor \u03b2. We train the network to minimize not just the loss at the end of the inference iterations through the network, but a composite loss that also includes the loss after earlier iterations. Let $\\mathcal{L}^{(n)}$ be the loss after n inference iterations, then the composite loss is given by:\n$L_{comp} = \\sum \\limits_{n=1}^{N_{iter}} \\frac{n}{N_{iter}} \\mathcal{L}^{(n)}$ (12)"}, {"title": "3.2. Reasoning and control", "content": "OBR's generative model allows it to infer the current state of the world, as well as predict future states that would arise as the result of the agent's actions. In a (model-based) RL framework, we could now define rewards and learn a value function on which to base a behavioral policy. Here, we take a slightly different approach, which is seminal in the proposed architecture, based on the influential computational neuroscience framework of Active Inference [39, 33]. It posits that agents, through their actions, aim to minimize a variational free energy with respect to a probability distribution over states that they prefer to find themselves in [40, 41, 42]. This preference distribution entails a biased (conditional) prior on environmental states, which acts as an attractor for planning future behavior. Once this preference (or soft goal) is learnt the rest of the architecture machinery will drive the system (agent-world) to generate object-centric actions that minimize the discrepancy of the current and the desired state.\nPreference network. We assume that the preference distribution is given by $p(\\{s_{t+1}^{(k)}\\}_{k \\in 1:K}|\\{\\lambda_t^{(k)}\\}_{k \\in 1:K}) = \\prod_k N(\\bar{\\mu}^{(k)}, \\bar{\\sigma}^{(k)2})$, where T is some time horizon and t denotes the present. Note that this distribution is conditioned on the agent's internal beliefs, via the variational parameters. We implement this mapping through a set-structured MLP network (architectural details in Appendix 6.1), which preserves the order of objects from input to output, and is invariant to the number of objects:\n$\\bar{\\nu}^{(k)} = \\phi_{enc}(\\lambda_t^{(k)}), c = \\frac{1}{K} \\sum W_{ctx} \\bar{\\nu}^{(k)} + b_{ctx}$ (5)\n$\\bar{\\mu}^{(k)}, \\bar{\\sigma}^{(k)} = \\phi_{dec} (\\bar{\\nu}^{(k)}, c)$ (6)\nIn words, each $\\lambda_t^{(k)}$ is projected into an embedding space by an encoder network. Object-wise embedding vectors {$\\bar{\\nu}^{(k)}$\\} are then linearly transformed and aggregated into a global context vector c, which is appended to each object embedding. Finally, the concatenated object+context embeddings are passed through a decoder network, to obtain object-wise preference statistics. Conceptually, since OBR determines its current goals by repeatedly applying the same local operation to each object representation (rather than a global operation on the full state of the environment), this may be interpreted as a form of (proto-)symbolic reasoning.\nNote that the preference network depends on the world model in an unsupervised learning scheme. The latent space geometry that the world model ends up learning is unknown a priori. Therefore, the preference network cannot learn a mapping within this latent space (from current to desired states), before this latent space has been defined. Interestingly, multiple preference models can be trained \"on top of\" the same world model, allowing fast acquisition of novel tasks within the same environment. Practically, the one main difference to a common value network or critic, is that our preference network does not assign scores to states, but rather furnishes the agent with a desired state conditioned on its current context. This obviates the need to unroll (many) possible futures to evaluate which of these would be more desirable."}, {"title": "Control", "content": "Given a preference distribution $p(s_t^*)^T$, the linearized dynamics model enables the agent to plan actions efficiently in closed form (without the need of rollouts). Specifically, the action plan $\\pi = [a_{t+1}, ..., a_{t+T}]$ is computed by minimizing the path integral of the variational free energy [41] over some future time horizon (similarly to model predictive control)5:\n$\\pi^* = argmin \\sum \\limits_{\\pi} \\limits_{\\tau=1}^{T} D_{KL}(q(s_{t+\\tau}^*|\\pi)||p(s_t^*)) = (ULU + \\lambda_a I)^{-1}ULE$ (7)\nImportantly, this optimization can be performed in closed form, by computing the discrepancy between the preferred state and the sequence of states that will unfold if no action is taken, and projecting this discrepancy (e) onto the pseudo-inverse of a matrix U that maps actions within the planning horizon to their (cumulative) effects over that time window. $L = diag(\\bar{\\sigma}^{(k)-2})$ is a diagonal matrix containing the precisions (inverse variances) of the preference distributions, and $l_a$ controls the strength with which actions are regularized (shrunk) towards to a zero-mean prior. Further details are provided in Appendix 7.1."}, {"title": "3.3. Training", "content": "The perception module is trained using pre-generated 4-frame videos with sparse actions, to minimize a bespoke ELBO loss that drives the model to reconstruct video frames accurately, while employing representations that are consistent with the dynamics model. Full details on the training procedure may be found in Appendix 6.3. The preference network is trained separately, in a self-supervised fashion, to learn the state-preference mapping-OBR does not copy any behavior from a teacher agent. The model is shown example task episodes where objects start in a random configuration with random velocities, and are simulated for a few frames without any goal-directed interference. In the final two frames, the objects are moved (by an oracle agent) to their target locations (with a final velocity of 0). The perception module encodes each video frame in the latent space, and the preference network is trained to minimize the discrepancy (KL divergence) between its preference prediction for the \u201cseed frames\u201d, and the representation of the target frame."}, {"title": "4. Results", "content": "We evaluated OBR qualitatively and quantitatively focusing on analyzing its ability to perform conditional behavioural reasoning using visual cues (Figure 2a). This is, to evaluate the capacity of the agent to learn proto-symbolic rules that follow 1st order logic. An exemplary conditional rule is \u201cIf there is a Half-torus move Boxes to the Top-Left and cones to the Bottom-Left and Half-torus to Middle-Right otherwise move Boxes and Cones to the Right\u201d. Furthermore, we analyzed its generalization for the number of objects and the adaptation to changes in the environment. As extra result, although it is not the focus of this work, an analysis of world model learning (i.e., segmentation and video prediction) can be found in the Appendix 6.1.1."}, {"title": "4.1. Environment", "content": "We developed, the Active dSprites, which is an \"activated\" version of the various multi-dSprites datasets that have been used in previous work on object-based visual inference (e.g. [10, 12]). Not only does it include (continuous) dynamics, but these dynamics can can be acted on by an agent (through continuous control actions that accelerate the objects). Thus, active dSprites is an interactive environment, rather than a dataset. We implemented two different scenarios the active dSprites environment. The first, objects in can be 2.5-D shapes (e.g., squares, ellipses and hearts)\u2014they have no depth dimension of their own, but can occlude each other within the depth dimension of the image. The second objects are 3D shapes (e.g., boxes, half-torus, cones) that can move in a 10 \u00d7 10 \u00d7 10 m cubic space. These objects are much more complex to learn than the 2D version, as they have lighting and shadows, producing color gradients that need to be encoded. To randomize the environment, when an active dSprites instance is intialized, object shapes, positions, sizes and colors are all sampled uniformly at random. Initial velocities are drawn from a Normal distribution. Shape colors are sampled at discrete intervals spanning the full range of RGB-colors, while a background color is drawn from a set of evenly spaced grayscale values between black and white. Shapes are presented in random depth order. In the 3D environment the lighting sources are fixed. Code for the active dSprites environment can be found at github.com/neuro-ai-robotics/OBR."}, {"title": "4.2. Problem complexity analysis through baselines comparison", "content": "We evaluated the problem solving complexity of a basic conditional rule \"If Heart then move Boxes to the Right, otherwise move Ellipses to the Left\" comparing different oracle and baseline algorithms with our approach 3. We evaluated different algorithms with full observability (i.e., with access to object locations): Linear-Quadratic Regulator control [43, 44], (implemented in the Python 3 control systems library), Soft-Actor-Critic (SAC) [45] and Proximal Policy Optimization (PPO) [46] (both used from the stable baselines 3 library [47]), and our OBR with perfect access to the true environmental state). And with partial observability (i.e., the agent only has access to the RGB image (pixels): SAC, PPO and OBR. For RL algorithms we gave dense rewards using the object distance between the current state and the desired state. For the OBR we only allow actions after frame 5. Here objects are 2.5 dimensional and during training the shape, color and positions are randomized and velocities are sampled from a Normal distribution with mean 0."}, {"title": "4.2.1. Abstract behavioural reasoning with conditional rules", "content": "We evaluated the capacity of the architecture to learn proto-symbolic behavioural rules and its generalization to the number of objects. We evaluated both 2D and 3D scenarios. Figure 4 shows an OBR agent execution for an\u201cIfHeart\" conditional (A \u2192 B) \u2227 (\u00acA \u2192 C) and the XOR rules, and two randomized scenarios for each rule. In Fig. 4a, the first scenario, there is no heart, and boxes should be moved to the upper-left and ellipses to the middle-left. The second one, there is a heart present, and boxes, ellipses and hearts should be moved to the left top, middle and down respectively. The agent is passive until the 5th frame. The environment row shows the visual input and the red arrows describe the forces applied. The imagined goal row, describes the output from the preference network at each frame. Note that the preference is in encoded only in the latent representation and here, for visualization purposes, we show its reconstruction using the decoder network. We let the agent to perform three frames of iterative inference until the proposed goal is shown. The true goal is the idealized solution of this \u201cIfHeart\" task. Analogously, in Fig. 4b, the OBR agent, which has learnt the Heart XOR Square task, solves two 2d active dSprites scenarios.\nFigure 4c,d shows the statistical performance of the OBR for different number of objects for the two conditional rules. The network is trained with three objects and then evaluated on two, three, four and five objects in the scene. Again here, the agent is passive until the fifth frame. The error curves shows that OBR interpret and manipulate the environment using the learnt conditional rule.\nFigure 5 shows an OBR similar analysis but in a 3D environment. The 3D scenario is considerably much more complex than the 2D one as objects have an extra dimension to move and colors have lighting. Here, the conditional rule learnt depends on the appearance of a HalfTorus object. The OBR preference network is also decoded to show its visual interpretation. Relevantly, the second instance shows that OBR can deal with the absence of multiples appearances of objects in the scene. There are two HalfTorus and no Cones in Fig. 5a bottom instance. Figure 5b shows the statistical performance of the OBR for different number of objects. The network is again trained with three objects and then evaluated on two, three, four and five objects in the scene. The agent is passive until the fifth frame. The error curves (towards zero) shows that OBR interpret and manipulate the environment using the learnt conditional rule. Figure 2c describes the 16 randomized OBR agent executions in 3D for 2,3,4 and 5 objects in the scence. For completeness, several randomized executions in 3D with different rules can be inspected at the github repository as animations."}, {"title": "4.2.2. Adaptation to changes in the environment", "content": "We evaluated the capacity of the OBR agent to recover from unexpected changes in the environment. For that purpose, we generate instances of the active dSprites that perform an object substitution in the middle of the execution. For instance, a Heart is changed by a Square during a conditional rule execution. Figure 6a describes two instances of the adaptation environment and Fig. 6b the statistical performance over 100 randomized instances. The object substitution experiment test the perception module, which needs to adapt fast to estimate the new situation, the preference network, which has to adapt the imagined goal depending on the objects that appear in the scene, and the control module that should provide the correct continuous actions to correct for the previously generated object movements. The true goal describes the idealized desired objects location after object substitution. The task error shows a decrease of performance until the system is able to recover and generate the proper behaviour to fulfill the conditional rule."}, {"title": "4.2.3. Logical composition emergence", "content": "We further evaluated the capacity of the agent to do logical composition (A \u2192 B) \u2227 (A \u2192 C) \u2192 A \u2192 (B \u2227 C), as described in Fig.7. We trained the preference reasoning module on two rules separately and then we evaluated in the testing phase the capacity of the agent to combine both. For instance, Rule 1, If Heart move Squares to Top-left, otherwise, to Middle-right and Rule 2, If Heart move Ellipses to Middle-left, otherwise, to Middle-right. In the testing phase, when there are both squares and ellipses the agent should compose both rules into: If Heart move Squares to Top-left and Ellipses to Middle-left, otherwise to Middle-right. Fig 7 shows the task performance, computed as the MSE to idealized desired location of the objects, as if the two rules where being computed. The OBR agent is passive until the 5th frame. The plot shows that the performed actions on the objects actually solve the task, confirming the that agent reasoning combines both rules."}, {"title": "5. Discussion", "content": "We described a brain-inspired deep learning architecture that allows grounded conditional abstract reasoning and behaviour by leveraging object-centric learned representation as the units of \u201cmental\" manipulation. Both perceptual and proto-symbolic goals were learnt in an unsupervised fashion from visual information (pixels). Results show that the system can learn and resolve conditional behaviours similar to 1st order logic: simple A \u2192 B, complex (A \u2192 B \u2227 C) \u2227 (\u00acA \u2192 D \u2227 E), XOR operation (A \u2228 B \u2192 C) \u2227 (A \u2227 B \u2192 D) and logical composition (A \u2192 B, A \u2192 C \u2192 A \u2192 (B \u2227 C). We analysed the model generalization to different number of objects (both in 2D and 3D), objects properties (e.g., color, shapes) and, critically, adaptation to changes in the environment (e.g., object substitution) thanks to the iterative inference and the preference network. OBR shows a promising direction in objects-based behavioral reasoning from pixels, mainly placing the foundations of grounded object-centric representations as a key inductive bias during learning, perception, resoning and control."}, {"title": "5.1. Grounded protosymbols and preferences in the latent space", "content": "While the agent behaviours from this work are much less complex from by current SOTA LLMs/VLM approaches such as RT architectures [48], researchers in the field are still struggling on how to perform goal-conditioned behavioural reasoning [49]. One of the key features of OBR is that it learns grounded representations and manipulate them in the latent manifold as well as it generates the agent preferences (goals) in the object-centric latent representation. This tights agent intentions to its own learning during interaction and constrained by its own learnt world model, bringing benefits but also restrictions as it forces the system to do embodied reasoning: the agents goals are tight to the world and the actions that it can perform in it."}, {"title": "5.2. Limitations and future work", "content": "Following the brain inspiration, we made some important decisions on the OBR architecture that derived into shortcomings. First, we employed an iterative inference procedure in order to benefit from postdictive inference (\"smoothing\") in a sliding window approach. However, experimental results showed that the reasoner was highly dependent on the accuracy of the perceptual module. Thus, replacing the perceptual module by amortized methods could improve segmentation performance and reduce training time. Secondly, in the experiments, the effect of the real actions in the latent representation is assumed to be linear. While this could fit a hierarchical interpretation of cognition, it prevents to properly manipulate complex non-linear dynamics. While OBR can recover from collisions, it cannot plan ahead with them. Further work should investigate complex collisions, for instance, by incorporating a brain-inspired version of interaction networks. Furthermore, unsupervised learning of protosymbolic rules is dependant on the object learnt features. This implies that some combination of causal learnt behaviours are not properly resolved in execution. A holistic engineering solution for these limitations may be to incorporate a higher-layer that uses OBR as the interface between symbolic (e.g., graph-based, language) and subsymbolic representations. This will also expand the expressivity"}]}