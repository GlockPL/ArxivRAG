{"title": "HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks at Scale", "authors": ["Huy Nhat Phan", "Phong X. Nguyen", "Nghi D. Q. Bui"], "abstract": "Large Language Models (LLMs) have revolutionized software engineering (SE), demonstrating remarkable capabilities in various coding tasks. While recent efforts have produced autonomous software agents based on LLMs for end-to-end development tasks, these systems are typically designed for specific SE tasks. We introduce HYPERAGENT, a novel generalist multi-agent system designed to address a wide spectrum of SE tasks across different programming languages by mimicking human developers' workflows. Comprising four specialized agents\u2014Planner, Navigator, Code Editor, and Executor-HYPERAGENT manages the full lifecycle of SE tasks, from initial conception to final verification. Through extensive evaluations, HYPERAGENT achieves state-of-the-art performance across diverse SE tasks: it attains a 25.01% success rate on SWE-Bench-Lite and 31.40% on SWE-Bench-Verified for GitHub issue resolution, surpassing existing methods. Furthermore, HYPERAGENT demonstrates superior performance in code generation at repository scale (RepoExec), and in fault localization and program repair (Defects4J), often outperforming specialized systems. This work represents a significant advancement towards versatile, autonomous agents capable of handling complex, multi-step SE tasks across various domains and languages, potentially transforming AI-assisted software development practices.", "sections": [{"title": "1. Introduction", "content": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in assisting with various coding tasks, ranging from code generation and completion to bug fixing and refactoring. These models have transformed the way developers interact with code, providing powerful tools that can understand and generate human-like code snippets with impressive accuracy. However, as software engineering tasks grow in complexity, there is an emerging need for more sophisticated solutions that can handle the intricacies of real-world software development.\nTo address these challenges, software agents built on top of LLMs have emerged as a promising solution. These agents are designed to automate and streamline complex software engineering tasks by leveraging the advanced reasoning and generative capabilities of LLMs. They can perform tasks such as code generation, bug localization, and even orchestrating multi-step development processes. Despite their potential, current software agents remain limited in scope. Most existing agents are designed to tackle a specific SE task with limited capability, such as resolving GitHub issues (Arora et al., 2024;\nChen et al., 2024; Jimenez et al., 2023; Xia et al., 2024; Yang et al., 2024a; Zhang et al., 2024a) using benchmarks like SWE-bench (Jimenez et al., 2023). Others (Huang et al., 2023) focus on competitive code generation benchmarks, such as APPS (Hendrycks et al., 2021), HumanEval (Chen et al., 2021a), and MBPP (Austin et al., 2021). Another line of software agents (Hong et al., 2023; Nguyen et al., 2024; Qian et al., 2024) focuses on create complicated software given a set of requirements. While these specialized agents demonstrate impressive capabilities within their domains, the broader claim"}, {"title": "1. Analysis & Plan", "content": "The developer starts by understanding the task requirements through documentation review and stakeholder discussions. A working plan is then formulated, outlining key steps, potential challenges, and expected outcomes. This plan remains flexible, adjusting as new insights are gained or challenges arise. For simple tasks, this plan may remain a mental checklist rather than a detailed written document."}, {"title": "2. Feature Localization", "content": "With a plan in place, the developer navigates the repository to identify relevant components, known as feature localization (Castro et al., 2019; Martinez et al., 2018; Michelon et al., 2021). This involves locating classes, functions, libraries, or modules pertinent to"}, {"title": "3. Edition", "content": "The developer edits the identified code components, implementing changes or adding new functionality. This phase also involves ensuring smooth integration with the existing codebase, maintaining code quality, and adhering to best practices. New or updated unit tests are written to ensure functionality and reliability."}, {"title": "4. Execution", "content": "After editing, the developer tests the modified code to verify it meets the plan's requirements. This includes running unit and integration tests, as well as conducting manual testing or peer reviews. If issues are found, the process loops back to previous phases until the task is fully resolved."}, {"title": "1. Generalizability", "content": "The framework is designed to easily adapt to a wide range of tasks with minimal configuration changes and little additional effort required to implement new modules into the system."}, {"title": "2. Efficiency", "content": "Each agent is optimized to manage processes with varying levels of complexity, requiring different degrees of intelligence from LLMs. For example, a lightweight and computationally efficient LLM can be employed for navigation, which, while less complex, involves the highest token consumption. Conversely, more complex tasks, such as code editing or execution, require more advanced LLM capabilities."}, {"title": "3. Scalability", "content": "The framework is built to scale effectively when deployed in real-world scenarios where the number of subtasks is significantly large. For instance, a complex task in the SWE-bench benchmark may require considerable time for an agent-based system to complete, and HYPERAGENT is designed to handle such scenarios efficiently."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Deep Learning for Automated Programming", "content": "In recent years, applying deep learning to automated programming has captured significant interest within the research community (Allamanis et al., 2018; Balog et al., 2016; Bui and Jiang, 2018; Bui et al., 2021, 2023; Feng et al., 2020; Guo et al., 2020, 2022b; Wang et al., 2021). Specifically, Code Large Language Models (CodeLLMs) have emerged as a specialized branch of LLMs, fine-tuned for programming tasks (Allal et al., 2023; Bui et al., 2022; Feng et al., 2020; Guo et al., 2024; Li et al., 2023; Lozhkov et al., 2024; Luo et al., 2023; Nijkamp et al., 2022; Pinnaparaju et al., 2024; Roziere et al., 2023; Wang et al., 2021, 2023; Xu et al., 2022; Zheng et al., 2024). These models have become foundational in building AI-assisted tools for developers, aiming to solve competitive coding problems from benchmarks such as HumanEval (Chen et al., 2021b), MBPP (Austin et al., 2021), APPs (Hendrycks et al., 2021) and CRUXEval Gu et al. (2024a).\nDespite achieving good results with benchmark tasks, these models often struggle to generate real-world software that requires complex logic and detailed acceptance criteria, which are essential for practical applications (Hong et al., 2024; Nguyen et al., 2024; Qian et al., 2023). This limitation has led to the development of more sophisticated benchmarks and evaluation methods."}, {"title": "2.2. Benchmarks for Software Engineering", "content": "Subsequent works have introduced new SE benchmarks that expand the scope and complexity of evaluation criteria. These efforts include translating problems across programming languages (Cassano et al., 2022; Wang et al., 2022), incorporating third-party libraries (Lai et al., 2023; Liu et al., 2023c), introducing derivative code completion tasks (Muennighoff et al., 2023), test coverage (Liu et al., 2023a), modifying edit scope (Ding et al., 2024; Du et al., 2023; Yu et al., 2024), and enhancing robustness to dataset contamination (Naman Jain et al., 2024). However, these code generation problems remain largely self-contained, with short problem descriptions (~100 lines) and correspondingly brief solutions, typically requiring only basic language primitives. As language models (LMs) rapidly evolve, many of these benchmarks are becoming saturated, highlighting the"}, {"title": "2.3. Autonomous Coding Agents", "content": "The emergence of open-source software development tools based on large language models (LLMs) has revolutionized the field of autonomous coding. These tools leverage LLMs' capabilities to plan, self-critique, and extend functionality through function calls. By integrating such tools into development workflows, researchers have observed dramatic performance improvements in code generation tasks on popular benchmarks such as HumanEval (Chen et al., 2021b). Notable advancements in this area include works by Huang et al. (2023), Chen et al. (2023), Shinn et al. (2024), Islam et al. (2024), Chen et al. (2022), and To et al. (2024). A parallel line of research focuses on generating complex software systems comprising multiple executable code files from input software requirements. Significant contributions in this domain include MetaGPT (Hong et al., 2023), AgileCoder (Nguyen et al., 2024), and ChatDev (Qian et al., 2024). These approaches aim to automate larger portions of the software development process, moving beyond single-file code generation.\nRecently, there has been growing interest in employing coding agents to automatically resolve GitHub issues, a task that more closely mimics real-world software engineering challenges. This trend is evident in works such as SWE-Agent (Yang et al., 2024a), SWE-bench (Jimenez et al., 2023), AutoCodeRover (Zhang et al., 2024c), and agentless approaches (Xia et al., 2024). The integration of interaction and code generation has spawned novel applications where code serves as the primary modality for actions (Wang et al., 2024a; Yang et al., 2024b), tool construction (Gu et al., 2024b; Wang et al., 2024b; Zhang et al., 2024b), and reasoning (Shinn et al., 2024; Zelikman et al., 2023a,b). Beyond general software engineering tasks, code language agents have found applications in specialized domains such as offensive security (Shao et al., 2024; Yang et al., 2023) and theorem proving (Thakur et al., 2023). This evolution towards agent-based models represents a significant step in bridging the gap between academic benchmarks and real-world software engineering challenges. By mimicking human-like problem-solving processes in coding tasks, these autonomous agents are paving the way for more sophisticated and practical AI-assisted development tools, potentially transforming the landscape of software engineering."}, {"title": "3. Problem Formulation", "content": "To formally define the software engineering tasks that HYPERAGENT is designed to address, we introduce the following notation and definitions:"}, {"title": "Software Engineering Task", "content": "Let $R = {r_1, r_2, ..., r_n}$ be a software repository consisting of n code files. Each file $r_i$ is a sequence of code tokens. A Software Engineering (SE) task T is defined as a tuple $T = (D, R, F)$, where:\n\u2022 D is a natural language description of the task requirements\n\u2022 R is the software repository on which the task is to be performed\n\u2022 $F : R \\rightarrow R'$ is the desired transformation function that modifies the repository to fulfill the task requirements\nThe goal of an SE agent is to approximate the function F given D and R, producing a modified repository R' that satisfies the task requirements. We can further categorize SE tasks into several types based on their specific objectives."}, {"title": "Issue Resolution Task", "content": "An Issue Resolution Task $T_{IR} = (I, R, F_{IR})$ is an SE task where I is a description of a GitHub issue, and $F_{IR}$ is a function that modifies R to resolve the issue."}, {"title": "Code Generation Task", "content": "A Code Generation Task $T_{CG} = (S, R, F_{CG})$ is an SE task where S is a specification for new code to be generated, and $F_{CG}$ is a function that adds new code to R according to S."}, {"title": "Fault Localization Task", "content": "A Fault Localization Task $T_{FL} = (B, R, F_{FL})$ is an SE task where B is a description of a bug or failing test case, and $F_{FL} : R \\rightarrow L$ is a function that identifies a set of locations L in R where the bug is likely to be present."}, {"title": "Program Repair Task", "content": "A Program Repair Task $T_{PR} = (B, R, F_{PR})$ is an SE task where B is a description of a bug or failing test case, and $F_{PR}$ is a function that modifies R to fix the bug while preserving correct functionality."}, {"title": "Generalist Software Engineering Agent", "content": "Given a set of diverse SE tasks $T = {T_1, T_2, ..., T_m}$, where each $T_i$ can be any type of SE task (e.g., $T_{IR}, T_{CG}, T_{FL},$ or $T_{PR}$), design an agent A that can effectively perform all tasks in T by approximating the corresponding transformation functions $F_i$ for each task $T_i$. The challenge in designing such a generalist agent lies in creating a unified framework that can: (1) Understand and interpret diverse task descriptions $D_i$; (2) Navigate and comprehend different repository structures $R_i$; (3) Generate appropriate code modifications or analysis results for each task type; (4) Verify the correctness and effectiveness of the solutions.\nHYPERAGENT addresses this challenge by employing a multi-agent architecture that mimics the workflow of human developers. Each agent in the system specializes in a specific aspect of the software engineering process, allowing for a modular and adaptable approach to diverse SE tasks."}, {"title": "4. HYPERAGENT: A Generalist Software Agent Framework", "content": "The key design principle of HYPERAGENT is the centralization of advanced reasoning in the Planner agent, with delegation of computationally intensive but conceptually simpler tasks to specialized child agents. This approach optimizes inference costs and overall performance by eliminating redundant information processing outside the Planner's context."}, {"title": "4.1. Centralized Multi-Agent System", "content": "The HYPERAGENT framework comprises four primary agents:\n\u2022 Planner: The Planner agent serves as the central decision-making unit. It processes human task prompts, generates resolution strategies, and coordinates child agent activities. The Planner operates iteratively, generating plans, delegating subtasks, and processing feedback until task completion or a predefined iteration limit is reached.\n\u2022 Navigator: The Navigator agent specializes in efficient information retrieval within the codebase. Equipped with IDE-like tools such as go_to_definition and code_search, it traverses codebases rapidly, addressing challenges associated with private or unfamiliar code repositories. The Navigator is designed for speed and lightweight operation, utilizing a combination of simple tools to yield comprehensive search results.\n\u2022 Editor: The Editor agent is responsible for code modification and generation across multiple files. It employs tools including auto_repair_editor, code_search, and open_file. Upon receiving target file and context information from the Planner, the Editor generates code patches, which are then applied using the auto_repair_editor.\n\u2022 Executor: The Executor agent validates solutions and reproduces reported issues. It utilizes an interactive_bash_shell for maintaining execution states and open_file for accessing relevant documentation. The Executor manages environment setup autonomously, facilitating efficient testing and validation processes."}, {"title": "4.2. Agent Communication and Scalability", "content": "Inter-agent communication in HYPERAGENT is structured to minimize information loss, ensure effective task delegation, and enable scalable, parallel processing of complex software engineering tasks. To achieve these goals, we implement an asynchronous communication model using a distributed Message Queue system based on Redis.\nThe Planner communicates with child agents using a standardized message format comprising Context and Request fields. The Context field provides relevant background information and rationale for the requested action, while the Request field contains specific, actionable instructions for the child agent. This structure enables the Planner to convey both high-level context and specific directives efficiently.\nWhen the Planner needs to delegate a task, it decomposes it into subtasks and publishes messages containing these subtasks to appropriate queues in the Message Queue system. Child agents, including multiple instances of the Navigator, Editor, and Executor, continuously monitor these queues and process tasks asynchronously. This approach allows for parallel processing of subtasks, significantly improving the system's efficiency and scalability.\nFor instance, when exploring a large codebase, multiple Navigator instances can simultaneously investigate different sections, dramatically reducing exploration time. Similarly, the Editor can parallelize large-scale code changes across multiple files, and the Executor can run multiple tests concurrently, greatly accelerating the validation process.\nTo mitigate information loss in child agent reports, we implement a lightweight LLM summarizer. This component compiles intermediate results from the child agent's execution log, generating a concise yet comprehensive summary. Upon task completion, child agents publish these summarized results back to a designated queue for the Planner to aggregate and process. This approach preserves critical details about code snippets, explored objects, and codebase structure, reducing the risk of information degradation or hallucination in the Planner over multiple iterations. The Message Queue-based architecture offers several advantages:\n\u2022 Parallel Processing: Multiple instances of each agent type can work on different subtasks simultaneously, significantly improving overall system throughput.\n\u2022 Load Balancing: Tasks can be dynamically distributed among multiple instances of each agent type, allowing for efficient resource utilization.\n\u2022 Fault Tolerance: If an agent instance fails, unprocessed tasks remain in the queue and can be redistributed to other available instances, enhancing system reliability.\n\u2022 Scalability: The system can easily scale horizontally by adding more instances of child agents to handle increased workload, without modifying the core architecture.\n\u2022 Decoupling: The Message Queue decouples the Planner from the child agents, allowing for independent scaling and maintenance of each component.\nThis scalable, asynchronous communication model enables HYPERAGENT to efficiently handle complex software engineering tasks in large-scale, distributed environments. It adapts dynamically to varying workloads and task complexities, making it well-suited for real-world software development scenarios where task volume and complexity can fluctuate significantly."}, {"title": "4.3. Tool Design", "content": "The efficacy of HYPERAGENT is significantly enhanced by its specialized tool design. Key considerations in tool development include feedback format, functionality, and usability. Tools provide succinct, informative, and LLM-interpretable output. Each tool is optimized for its specific role in the software"}, {"title": "5. Implementation Details", "content": "For summarizer in HYPERAGENT, we used Mixtral 8x7B (Jiang et al., 2024). To examine the flexibility of our framework and measure robustness, we employed a variety of language models (LMs) across different configurations. We tested four main configurations of HYPERAGENT, each utilizing different combinations of LMs for the Planner, Navigator, Editor, and Executor roles. \nAn advantage of our design is that we can choose the most suitable LLMs for each agent type, optimizing performance and accuracy. For instance, the Planner, serving as the brain of the whole system, requires a powerful model with superior reasoning capabilities to effectively orchestrate complex tasks. The Editor also demands a strong model with robust coding capability to edit and generate code accurately, given the inherent complexity of real-world codebases. In contrast, the Navigator and Executor agents can utilize less powerful models with smaller footprints and faster inference times, as their tasks are more straightforward and require less complex reasoning. This flexible architecture enables efficient allocation of computational resources, ensuring optimal performance across different agent types while balancing the trade-offs between model capability and computational cost. Such a design also allows for easier updates and improvements to individual components without necessitating a complete system overhaul.\nAs a result, we can implement various configurations of HYPERAGENT as shown in Table 3, utilizing both open-source and closed-source models. For closed-source models, we designate GPT-4 and Claude-3 Sonnet as strong models, while Claude-3 Haiku serves as the weak model. In the open-source domain, Llama-3-70B acts as the strong model, with Llama-3-8B serving as the weak model. We believe that HyperAgent is the first system to evaluate SWE-Bench using open-source models such as Llama-3, offering a more cost-effective solution compared to closed-source alternatives while maintaining competitive performance across a wide range of software engineering tasks."}, {"title": "6. Evaluations", "content": "We conducted extensive evaluations of HYPERAGENT across diverse benchmarks to assess its effectiveness in various software engineering tasks. Our criteria for selecting SE tasks and corresponding"}, {"title": "GitHub Issue Resolution", "content": ""}, {"title": "6.1.1. Dataset", "content": "We evaluated HYPERAGENT using the SWE-bench benchmark (Jimenez et al., 2023), which comprises 2,294 task instances derived from 12 popular Python repositories. SWE-bench assesses a system's capability to automatically resolve GitHub issues using Issue-Pull Request (PR) pairs, with evaluation based on verifying unit tests against the post-PR behavior as the reference solution. Due to the original benchmark's size and the presence of underspecified issue descriptions, we utilized two refined versions: SWE-bench-Lite (300 instances) and SWE-bench-Verified (500 instances). The Lite version filters samples through heuristics (e.g., removing instances with images, external hyperlinks, or short descriptions), while the Verified version contains samples manually validated by professional annotators. These streamlined versions offer a more focused and reliable evaluation framework, addressing the limitations of the original benchmark while maintaining its core objectives."}, {"title": "6.1.2. Baselines", "content": "We compared HYPERAGENT to several strong baselines: SWE-Agent (Yang et al., 2024a), a bash interactive agent with Agent-Computer Interfaces; AutoCodeRover (Zhang et al., 2024c), a two-stage agent pipeline focusing on bug fixing scenarios; Agentless (Xia et al., 2024), a simplified two-phase approach that outperforms complex agent-based systems in software development tasks; and various Retrieval Augmented Generation (RAG) baselines as presented in (Jimenez et al., 2023). These baselines represent a diverse range of approaches to software engineering tasks, providing a comprehensive evaluation framework for our method."}, {"title": "6.1.3. Metrics", "content": "We evaluate this task using three key metrics: (1) percentage of resolved instances, (2) average time cost, and (3) average token cost. The percentage of resolved instances measures overall effectiveness, indicating the proportion of SWE-bench tasks where the model generates solutions passing all unit"}, {"title": "6.1.4. Results", "content": "The results presented in Table 4 demonstrate the competitive performance of HYPERAGENT across different configurations on the SWE-Bench datasets. Several key observations can be made:\n1. Performance: HYPERAGENT-Full-2 achieves a strong success rate of 31.40% on the SWE-Bench Verified dataset. This performance is competitive with top-performing methods such as SWE-Agent + Claude 3.5 Sonnet (33.60%) and Agentless + GPT-40 (33.20%). On the SWE-Bench Lite dataset, HYPERAGENT-Full-2 achieves the best performance among all methods with a 25.00% success rate, closely followed by HYPERAGENT-Full-1 at 24.67%. This outperforms strong baselines like Agentless + GPT-40 (24.30%) and SWE-Agent + Claude 3.5 Sonnet (23.00%).\n2. Efficiency: HYPERAGENT-Lite configurations demonstrate impressive efficiency. HYPERAGENT-Lite-1 and HYPERAGENT-Lite-2 have average processing times of 132 and 108 seconds respectively, significantly faster than AutoCodeRover + GPT-40 (720 seconds).\n3. Cost-Effectiveness: HYPERAGENT-Lite-1 offers an excellent balance of performance (27.33% on Verified, 21.67% on Lite) and cost ($0.45). This makes it substantially more cost-effective than several baselines, including SWE-Agent + Claude 3.5 Sonnet ($1.79) and SWE-Agent + GPT-40 ($2.55).\n4. Scalability: The performance spectrum across HYPERAGENT-Lite and HYPERAGENT-Full configurations demonstrates the system's adaptability to different performance-cost trade-offs, providing flexibility for various use cases.\nOverall, HYPERAGENT demonstrates strong and competitive performance across both datasets. HYPERAGENT-Full-2 achieves the best performance on the Lite dataset and is highly competitive on the Verified dataset. The system's ability to achieve high success rates while offering various configurations for efficiency and cost-effectiveness positions HyperAgent as a versatile and practical solution for automated GitHub issue resolution. Its performance is particularly noteworthy given"}, {"title": "Repository-Level Code Generation", "content": ""}, {"title": "6.2.1. Dataset", "content": "We evaluate our task using RepoExec (Hai et al., 2024), a benchmark for Python for assessing repository-level code generation with emphasis on executability and correctness. Comprising 355 samples with automatically generated test cases (96.25% coverage), RepoExec typically provides gold contexts extracted through static analysis. The gold contexts are splitted into different richness level, including full context, medium context and small context. The richness level of contexts represent for different way to retrieve the contexts, such as import, docstring, function signature, API invocaction, etc. However, to measure HYPERAGENT's ability to navigate codebases and extract contexts independently, we omit these provided contexts in our evaluation."}, {"title": "6.2.2. Baselines", "content": "We compared HYPERAGENT against strong retrieval-augmented generation (RAG) baselines, including WizardLM2 + RAG, GPT-3.5-Turbo + RAG, WizardLM2 + Sparse RAG, and GPT-3.5-Turbo + Sparse RAG. These baselines represent state-of-the-art approaches in combining large language models with information retrieval techniques. Sparse RAG represents for using BM25 retriever and RAG stands for using UnixCoder Guo et al. (2022a) as context retriever. We used chunking size of 600 and python code parser from Langchain 3 allowing us to parse the context in a syntax-aware manner. Additionally, we included results from CodeLlama (34b and 13b versions) and StarCoder models when provided with full context from RepoExec, serving as upper bounds for performance with complete information."}, {"title": "6.2.3. Metrics", "content": "We used pass@1 and pass@5 as our primary metric, which measures the percentage of instances where all tests pass successfully after applying the model-generated patch to the repository."}, {"title": "Fault Localization", "content": ""}, {"title": "6.3.1. Dataset", "content": "We evaluated HYPERAGENT on the Defects4J dataset (Just et al., 2014; Sobreira et al., 2018), a widely used benchmark for fault localization and program repair tasks. Our evaluation encompassed all 353 active bugs from Defects4J v1.0."}, {"title": "6.3.2. Baselines", "content": "We compared HYPERAGENT against several strong baselines, including DeepFL Li et al. (2019), AutoFL (Kang et al., 2024), Grace (Lou et al., 2021) DStar (Wong et al., 2012), and Ochiai (Zou et al., 2019). DeepFL, AutoFL and Grace represent more recent approaches that leverage deep learning methods for fault localization. In contrast, DStar and Ochiai are traditional techniques that employ static analysis-based methods to identify faults."}, {"title": "6.3.3. Metrics", "content": "We follow AutoFL (Kang et al., 2024) to use acc@k metric which measures the We adopt the acc@k metric from AutoFL to evaluate bug localization performance. This metric measures the number of bugs for which the actual buggy location is within a tool's top k suggestions. We choose this metric because previous research indicates that developers typically examine only a few suggested locations when debugging, and it's widely used in prior work. To handle ties in the ranking, we employ the ordinal tiebreaker method instead of the average tiebreaker, as we believe it more accurately reflects a developer's experience when using a fault localization tool."}, {"title": "6.3.4. Results", "content": "The fault localization results in Table 6 on the Defects4J dataset demonstrate HYPERAGENT superior performance, achieving an Acc@1 of 59.70%. This significantly outperforms all other methods, surpassing the next best performer, AutoFL, by 8.7 percentage points (51.00%) and more than doubling the accuracy of traditional methods like Ochiai (20.25%). HYPERAGENT's ability to correctly identify the buggy location on its first attempt for nearly 60% of the bugs suggests a potentially substantial reduction in debugging time and effort in real-world scenarios. The wide performance range across methods (20.25% to 59.70%) highlights both the challenges in fault localization and the significant improvement HyperAgent represents. While there's still room for improvement, these results indicate that HyperAgent's approach is more effective than existing deep learning and traditional static analysis-based methods for fault localization in Java projects."}, {"title": "Program Repair", "content": ""}, {"title": "6.4.1. Dataset", "content": "We also utilize the Defects4J dataset (Just et al., 2014; Sobreira et al., 2018). This dataset is particularly suitable as it provides gold-standard fixes and test cases, which are crucial for evaluating the effectiveness of repair techniques once faults are localized and fixes are applied."}, {"title": "6.4.2. Baselines", "content": "We compared HYPERAGENT with configuration Lite-1 against state-of-the-art baselines: RepairAgent (Bouzenia et al., 2024), SelfAPR (Ye et al., 2022), and ITER (Ye and Monperrus, 2024). ITER and SelfAPR are learning-based methods, while RepairAgent is a multi-agent system leveraging LLMs to autonomously plan and execute bug fixes. RepairAgent interleaves information gathering, repair ingredient collection, and fix validation, dynamically selecting tools based on gathered information and previous fix attempts."}, {"title": "6.4.3. Metrics", "content": "As in previous studies Bouzenia et al. (2024); Hidv\u00e9gi et al. (2024), we provide both the count of plausible and correct patches. A fix is considered plausible if it passes all the test cases, but this doesn't guarantee its correctness. To assess if a fix is correct, we automatically verify if its syntax aligns with the fix created by the developer via exactly matching Abstract Syntax Tree (AST) between fixes."}, {"title": "6.4.4. Results", "content": "The results presented in Table 7 on the Defects4J dataset demonstrate HYPERAGENT's strong performance compared to existing repair tools. HYPERAGENT achieved 192 correct fixes out of 835 bugs, outperforming its closest competitors: RepairAgent (164 correct fixes) and SelfAPR (110 correct fixes). Additionally, HYPERAGENT generated 249 plausible fixes, indicating its capability to produce a high number of test-passing patches. The performance metrics of HyperAgent are noteworthy, with 249 plausible fixes (29.8%) and 192 correct fixes (23%) out of 835 bugs. These results can be attributed to HyperAgent's four-phase repair strategy: planning, localization, fix verification via testing, and implementation. This approach enables HYPERAGENT to address the complex challenges of automated program repair effectively. HYPERAGENT's performance extends"}, {"title": "7. Analysis", "content": ""}, {"title": "7.1. Ablation Studies on Agent Roles", "content": "We conducted experiments using SWE-bench Tiny to evaluate the contribution of each agent role to overall performance. This was done by replacing each child agent with the planner itself, requiring the planner to directly utilize the eliminated agent's toolset. Table 8 illustrates a significant cost increase for all configurations when any agent role is removed. The resolving rate also decreases, with the magnitude varying based on which role is eliminated. Removing the Navigator causes the most substantial performance drop, followed by the Editor and the Executor, respectively. Notably, when a medium-long context length LLM such as WizardLM2 acts as the Planner and replaces the role of Editor or Navigator, we observe a more severe drop in the resolving rate. This is attributed to these roles requiring continuous interaction with the environment, necessitating a long context."}, {"title": "7.2. Analysis of Tool Design", "content": "We investigated the improvements brought by our major design choices in the tool's interface and functionality. An ablation study was conducted on the functionalities of go_to_definition, auto_repair_editor, open_file, and code_search using SWE-bench Tiny. For each tool, we evaluated the overall performance when the tool is utilized versus when it is not. A crucial finding for go_to_definition is that the LLM agent struggles to effectively use this IDE-like feature. It requires exact line and column numbers and the precise symbol name, which demands precise localization of character positions. Despite supporting annotated line numbers, the agent often fails and retries multiple times. However, incorporating a proximity-based search process, allowing the agent to approximate specifications, significantly improves performance (from 9% without search to 15% with search). For open_file, small LLMs like Claude Haiku tend to scroll up and down multiple times to find desired snippets by continuously increasing start_line and end_line, leading to out-of-context length issues. We addressed this by adding an additional input field keywords, allowing the LLM to search keywords inside the file. This enables the tool to quickly localize the positions of keywords inside the file and display the surrounding lines, increasing the resolving rate by 3%. Without code_search, the Navigator faces significant challenges in swiftly identifying necessary objects, resulting in a substantially lower performance rate of 3% compared to 8% when"}]}