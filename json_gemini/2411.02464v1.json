{"title": "You are out of context!", "authors": ["Giancarlo Cobino", "Simone Farci"], "abstract": "This research proposes a novel drift detection methodology for machine learning (ML) models based on the concept of \"deformation\" in the vector space representation of data. Recognizing that new data can act as forces stretching, compressing, or twisting the geometric relationships learned by a model, we explore various mathematical frameworks to quantify this deformation. We investigate measures such as eigenvalue analysis of covariance matrices to capture global shape changes, local density estimation using kernel density estimation (KDE), and Kullback-Leibler divergence to identify subtle shifts in data concentration. Additionally, we draw inspiration from continuum mechanics by proposing a \"strain tensor\" analogy to capture multi-faceted deformations across different data types. This requires careful estimation of the displacement field, and we delve into strategies ranging from density-based approaches to manifold learning and neural network methods. By continuously monitoring these deformation metrics and correlating them with model performance, we aim to provide a sensitive, interpretable, and adaptable drift detection system capable of distinguishing benign data evolution from true drift, enabling timely interventions and ensuring the reliability of machine learning systems in dynamic environments. Addressing the computational challenges of this methodology, we discuss mitigation strategies like dimensionality reduction, approximate algorithms, and parallelization for real-time and large-scale applications. The method's effectiveness is demonstrated through experiments on real-world text data, focusing on detecting context shifts in Generative AI. Our results, supported by publicly available code, highlight the benefits of this deformation-based approach in capturing subtle drifts that traditional statistical methods often miss. Furthermore, we present a detailed application example within the healthcare domain, showcasing the methodology's potential in diverse fields. Future work will focus on further improving computational efficiency and exploring additional applications across different ML domains.", "sections": [{"title": "Introduction", "content": "Machine learning (ML) and artificial intelligence (AI) are transforming our world, but their reliance on data creates vulnerabilities. Real-world data is rarely static; it constantly evolves due to shifting contexts, populations, trends, and behaviors. This phenomenon, known as \"drift,\" poses a significant threat to the dependability and accuracy of ML models. Drift takes various forms. Concept drift alters the relationship between input features and target variables, while data drift changes the distribution of input features themselves. Context drift, particularly relevant in conversational AI, occurs when the underlying topic or context of an interaction shifts, potentially leading to incoherent responses and a decline in model performance.\nThis research introduces a novel approach, viewing data as points in a high-dimensional vector space and interpreting new data as forces that deform this space. We aim to develop a sensitive and interpretable methodology that captures these deformations, enabling early warnings of performance degradation."}, {"title": "Problem Statement", "content": "Drift can significantly impact the performance and reliability of machine learning (ML) models. It leads to a mismatch between the model's original assumptions and the changing realities of the data, resulting in decreased accuracy and potentially erroneous decision-making, especially in critical domains such as healthcare, finance, and autonomous systems. Models that are exposed to drift may produce unreliable outputs, causing users to lose trust. This erosion of trust hinders the adoption of ML systems and necessitates frequent retraining or recalibration, increasing operational costs.\nFurthermore, the advancement of artificial intelligence (AI) is closely tied to the models' ability to detect context, seamlessly shift between different contexts, and provide accurate responses accordingly. Without this capability, AI systems may struggle to maintain coherent, contextually relevant conversations or decisions, especially in dynamic environments such as conversational AI or autonomous decision-making systems.\nGiven these challenges, the need for effective drift detection methodologies becomes paramount. These methodologies provide early warnings, enabling corrective actions before significant model degradation occurs. By detecting the specific type of drift, it is possible to focus retraining efforts, ensuring that updates are both efficient and targeted. Additionally, continuous drift monitoring helps maintain user confidence and ensures that ML models remain adaptable in dynamic environments."}, {"title": "Research Motivation", "content": "Drift detection continues to face challenges despite significant research efforts. Traditional methods effectively detect large distributional shifts but often miss subtle drifts, which can degrade model performance over time. These gradual changes accumulate and damage model accuracy before they become apparent through standard metrics, highlighting the need for more sensitive detection methods capable of signaling drift earlier for timely intervention."}, {"title": "Literature Review", "content": "A major limitation of current techniques is their inability to distinguish between types of drift, such as concept drift (changes in the relationship between input and target) and data drift (shifts in input distribution while target relationships remain constant). Without identifying the specific drift type, corrective actions can be inefficient, reducing their effectiveness.\nAdditionally, many drift detection methods are computationally intensive, limiting their use in real-time applications or resource-constrained environments. This restricts their practicality in systems that require continuous operation.\nThe vector space deformation approach proposed in this research offers a solution to these limitations. By analyzing data as points in a high-dimensional space and examining how new data deforms this space, subtle drifts that may otherwise go unnoticed can be detected. This method enhances sensitivity, enabling earlier detection of gradual changes that may not affect overall statistical distributions but still impact model performance.\nMoreover, this approach is adaptable, as different types of drift manifest as distinct deformations in the vector space, aiding in the identification of specific drift types and supporting more targeted corrective actions. The method's strong theoretical foundation also paves the way for more robust drift detection techniques beyond traditional statistical methods.\nThe vector space deformation approach enhances sensitivity by detecting small, gradual deformations in the data's geometric structure, even when overall statistical distributions remain relatively unchanged. By analyzing distinct deformation patterns, our approach helps differentiate between various drift types, such as concept drift or data drift, allowing for tailored corrective actions.\nGenerative AI models, particularly those used in conversational AI, are highly susceptible to context drift, as subtle shifts in user language or topic can lead to incoherent or irrelevant outputs.\nIn summary, the vector space deformation approach addresses the key challenges of traditional methods, offering improved sensitivity, adaptability, and computational efficiency. It promises to enhance the reliability and adaptability of machine learning models in dynamic environments, ensuring long-term effectiveness despite changing data."}, {"title": "Statistical Drift Detection", "content": "Statistical methods form a cornerstone of drift detection, often comparing data distributions from different periods or between a baseline dataset and new data. Key techniques include:\n\u2022 Population Stability Index (PSI): PSI quantifies the change in the distribution of a feature between two samples. It is simple to understand and interpret.\n\u2022 Kullback-Leibler (KL) Divergence: Measures the difference between two probability distributions. It is a non-symmetric divergence, meaning that the order in which distributions are compared matters.\n\u2022 Jensen-Shannon (JS) Divergence: A symmetric variant of KL divergence, addressing some of its limitations. JS divergence is often more stable when dealing with sparse distributions.\n\u2022 Other Statistical Distances: Methods such as the Kolmogorov-Smirnov (KS) test, Wasserstein distance (Earth Mover's Distance), and others provide alternative ways to quantify distributional differences."}, {"title": "Advantages", "content": "\u2022 Well-Established Foundation: Statistical methods have a strong theoretical basis and are widely used in data analysis.\n\u2022 Interpretability: Measures like PSI and KL divergence provide quantifiable metrics that can be monitored for significant changes."}, {"title": "Context Drift Detection", "content": "Limitations\nTraditional statistical tests often focus on individual features, potentially overlooking complex drifts stemming from interactions between multiple features. Determining appropriate thresholds to distinguish natural data variation from significant drift presents a considerable challenge. Furthermore, many methods rely on assumptions about the underlying data distributions, assumptions that may not always reflect the complexities of real-world data. These limitations can lead to missed detections of subtle yet impactful drifts."}, {"title": "Model-Based Approaches", "content": "Model-based drift detection utilizes the outputs or behavior of machine learning models to identify potential drift. Key strategies include tracking how the distribution of model predictions shifts over time, which can signal drift; for example, a sudden increase in uncertain classifications or a change in the proportion of positive versus negative predictions might indicate that the model is encountering data outside its area of expertise. Analyzing changes in the confidence scores or prediction probabilities of a model can also provide valuable clues about drift. A decline in overall confidence, even if predictions remain technically correct, might suggest the model is operating on less familiar data. Adversarial techniques employ 'drift generators' to deliberately create perturbed data samples simulating various drift scenarios. By observing how a model's performance degrades on these 'drifted' inputs, insights into vulnerabilities and potential drift detection thresholds can be gained. Another approach involves training a separate model to distinguish between the original data used to train the primary model and new incoming data. If the discriminator successfully separates the datasets, it strongly suggests that drift has occurred."}, {"title": "Advantages", "content": "\u2022 Leverages Existing Models: Model-based approaches directly utilize the trained machine learning models, potentially reducing the need for additional data collection and analysis.\n\u2022 Sensitivity to Complex Drift: Changes in model behavior can sometimes capture subtler drifts that may not be immediately obvious in statistical distributions."}, {"title": "Limitations", "content": "\u2022 Model Dependence: The effectiveness of model-based approaches is tied to the quality and representativeness of the original model. A poorly trained model will not offer reliable drift detection signals.\n\u2022 Interpretability: Analyzing changes in model outputs may not always directly pinpoint the specific nature of the drift or the features involved."}, {"title": "Approach", "content": "The foundation of our proposed drift detection approach rests on the concept of embedding data points as vectors within a high-dimensional space. To transform raw data into these representations, we carefully consider feature selection and engineering processes. Suitable features might include numerical values (such as age or income), categorical features converted into numerical representations, or features extracted using techniques like image processing or natural language processing (NLP). Each selected feature corresponds to a dimension in the vector space. While simpler datasets may work with only a few dimensions, complex data often requires a very high-dimensional representation to fully capture underlying patterns and relationships."}, {"title": "Learning Decision Boundaries and Manifolds", "content": "Within this vector space, machine learning models aim to establish decision boundaries or discover hidden structures. In supervised learning, algorithms search for optimal boundaries (hyperplanes in linear models or more complex shapes in non-linear models) that separate the space into regions corresponding to different classes or outcomes. For example, a support vector machine (SVM) may create a hyperplane to divide data points into two categories. Conversely, unsupervised learning techniques, such as clustering algorithms, work to identify manifolds, which are clusters of data points that exhibit similarities and reside close together in this space."}, {"title": "Forces Induced by New Data Points", "content": "We hypothesize that new data points, especially those that differ significantly from the training data, can behave like \"forces\" acting on the existing vector space representation. To illustrate this, imagine a flexible sheet with data points plotted on it. As new, outlying data points are introduced, they may stretch, compress, or twist the sheet, representing deformations in the vector space. These deformations could manifest in several forms:\n\u2022 Shifting: If the new data exhibits data drift, the overall distribution of points in the space might shift. This can change the distances between clusters or affect the position of decision boundaries learned by the model.\n\u2022 Stretching or Compressing: Certain areas of the space may stretch or compress due to changes in the variance or correlations of features in the new data.\n\u2022 Twisting or Warping: More complex, non-linear drifts might manifest as twisting or warping of geometric relationships within the vector space, altering the fundamental topology of the data's representation."}, {"title": "Mathematical Formulation", "content": "Data should not only be considered as rows and columns, but as points scattered in a vast, multidimensional space. Each dimension in this space represents a feature of the data (e.g., age, income, purchase history, large text, or a generated image). The initial dataset used to train a model forms a \"constellation\" or cloud in this space, depending on the type of data.\nWhen new data arrives, each new data point acts like a small force. Points that are significantly different from the original data (outliers) exert a stronger pull than those that blend in with the rest. The goal is to detect not only if the center of the data constellation is moving but also how the overall shape is deforming-whether stretching, compressing, or twisting in response to the new data forces."}, {"title": "Steps and Mathematical Formulation", "content": "\u2022 Vector Space: We represent the data points in a vector space, denoted as R\", where each data point becomes a vector (an arrow) in this space.\n\u2022 Embedding: The process of converting raw data into vectors is crucial. For numerical data, we may use the values directly. For textual data, techniques such as word embeddings (e.g., Word2Vec) can be used to represent words or documents as vectors."}, {"title": "Step 2: Quantifying the Force of New Data", "content": "For each new data point xi:\n\u2022 Deviation Vector (d): We draw an arrow from the center of the original data (often the average, \u03bc) to the new point:\nd = xi - \u03bc\nThis gives the direction and magnitude of the \"pull\" exerted by this new point.\n\u2022 Force Magnitude: We can scale the length of the deviation vector ||d|| to reflect the influence of the new point on the overall shape:\nSimple Distance: Use the length ||d|| directly.\nFading Influence: Use a function that decreases the force as the distance from the center increases, such as:\n||d||*e^(-k||d||)\nwhere k controls how quickly the force fades.\nRelative to Spread: Divide the length ||d|| by a measure of the spread of the original data (such as its standard deviation), making the force proportional to how \"unusual\" the new point is."}, {"title": "Step 3: Capturing the Deformation", "content": "To measure how the entire data space is deforming, we employ the following techniques:\n\u2022 Eigenvalues and Eigenvectors (Global Shape): Imagine drawing the largest possible ellipse (or ellipsoid in higher dimensions) that encompasses the data points.\nEigenvectors represent the directions in which the ellipsoid is stretched the most.\nEigenvalues provide the magnitude of the stretch in each of those directions.\nBy comparing the eigenvectors and eigenvalues of the original data to those of the new data (generated from devices, phones, the internet, etc.), we can calculate whether the data is stretching, compressing, or rotating in significant ways."}, {"title": "Deformation as Average Displacement", "content": "\u2022 Local Density Estimation (Subtle Shifts): Imagine some areas of the data space becoming more crowded with points while other areas thin out.\nWe can estimate the density of points in different regions using techniques such as Kernel Density Estimation (KDE).\nBy comparing the density of the new data to the expected density based on the original data, we can identify areas where the data distribution is changing more subtly.\nWe treat data as vectors in a high-dimensional space, where each feature corresponds to a specific dimension.\nA straightforward way to quantify the impact of new data on an existing distribution is by measuring how much, on average, the new points pull the center of the original data. This approach, which we term average displacement, relies on the intuitive notion that as new data points arrive, they exert a kind of gravitational force on the overall data cloud, potentially shifting its center of mass.\nWe can conceptualize this by imagining the initial dataset as a cluster of points held together by elastic bands. Each new data point acts like a tiny weight added to this system, pulling on the bands and potentially shifting the cluster's equilibrium. The average displacement measures the average magnitude of these pulls, calculated as the distance between each new point and the center of the original data (often represented by the mean). A larger average displacement suggests a stronger overall pull away from where the model was initially trained.\nWhile average displacement offers a readily interpretable measure of global shift, it suffers from significant limitations. This approach is akin to tracking only the movement of a ship's anchor. While the anchor's position provides some information about the ship's location, it reveals nothing about the ship's orientation, its rocking due to waves, or whether it is taking on water.\nSimilarly, average displacement is blind to changes in the shape of the data distribution. A cluster of data points might be stretching, rotating, or becoming more dispersed, and yet the average displacement could remain relatively unchanged. For instance, imagine a scenario where a few outliers in the new data move significantly farther from the center. The average displacement might not change drastically, especially if the dataset is large, because the value is averaged across all data points.\nFurthermore, average displacement ignores the direction of these pulls. Two new data points could exert equal but opposite forces on the center of mass, effectively canceling each other out in the average displacement calculation. However, these opposing forces, rather than indicating stability, might be subtly twisting or warping the shape of the data in ways that degrade model performance.\nAnother interesting concept related to data shape changes is the convex hull. Imagine wrapping the data points with the tightest possible elastic sheet. The convex hull represents the boundary of this sheet, encompassing all the data points. A shift in the convex hull often indicates a change in the extremities of the data distribution, where boundary points are pulled outward. While intuitively appealing, relying solely on the convex hull for drift detection poses practical challenges. Computing the convex hull, especially as the number of dimensions increases, becomes computationally expensive, limiting its usefulness for real-time monitoring. Moreover, the convex hull is highly susceptible to outliers. A single data point moving far from the main cluster can drastically alter the convex hull, even if the majority of the data remains relatively stable.\nThe limitations of average displacement and the computational challenges of the convex hull underscore the need for more sophisticated approaches. To effectively detect drift, we must move beyond simply measuring the average shift in data points towards techniques that capture changes in the spread, orientation, and overall shape of the data distribution within the high-dimensional vector space."}, {"title": "Mathematical Formulation of Deformation as Average Displacement", "content": "Our approach to drift detection is based on interpreting the arrival of new data as a force that deforms the geometric structure of the data space. To make this concept concrete, we model the data in an n-dimensional Euclidean space"}, {"title": "Average Displacement: A Basic Measure of Shift", "content": "Rn, where each dimension corresponds to a specific feature of the data after appropriate preprocessing and embedding. Each data point is then represented as a vector x = (x1,x2,...,xn) \u2208 Rn.\nA preliminary measure of this deformation is the average displacement, which captures the overall tendency of new data points to shift the center of the baseline data distribution. Denote the initial (baseline) dataset as {X1,X2,...,xm} and the new data points as {Xm+1,...,Xn}."}, {"title": "Center of Distribution (\u03bc)", "content": "We first calculate the representative center of the baseline distribution. A common choice is the mean \u00b5, although the median or a robust estimate may be preferred for data with outliers or non-Gaussian distributions:\n\u03bc = (1/m) * \u03a3(xi) from i=1 to m\nTo quantify how the new data points influence the original distribution, we introduce the concept of a force vector. Each new data point x; exerts a pull on the center of the baseline data, analogous to a gravitational force. This force can be represented as a vector Fi:\nFi = xi - \u03bc\nwhere:\n\u2022 Fi is the force vector exerted by the new data point xi.\n\u2022 x\u1d62 is the vector representing the new data point.\n\u2022 \u03bc is the vector representing the center of the baseline data distribution (calculated as the mean, median, or a robust estimate).\nThis force vector captures both the direction and magnitude of the pull. The magnitude, denoted ||Fi||, is simply the Euclidean distance between the new point and the baseline center."}, {"title": "Average Displacement Calculation", "content": "To assess the overall influence of the new data, we calculate the average displacement D, which averages the magnitudes of these force vectors across all new data points:\nD = (1/(n-m)) * \u03a3(||Fi||) from i=m+1 to n\nwhere:\nD is the average displacement.\nn is the total number of data points (baseline + new).\nm is the number of data points in the baseline dataset."}, {"title": "Convex Hull: Capturing the Outer Boundaries", "content": "A larger average displacement D signifies a stronger overall pull exerted by the new data, indicating a more substantial shift in the data space.\nAnother geometric concept relevant to drift detection is the convex hull, which encompasses all data points within the smallest possible convex shape. The convex hull of a set of points X is defined as the set of all possible combinations of those points, where each combination is a weighted average:\nConvex Hull(X) = {point | point = \u03bb1x1 + \u03bb2x2 + \u00b7\u00b7\u00b7 + \u03bbkxk,\nx1, x2, ..., xk \u2208 X, \u03bb1, \u03bb2,..., \u03bbk \u2265 0, \u03a3(\u03bbi) = 1 from i=1 to k\nWhere:\n\u2022 \u03bb1, \u03bb2,..., \u03bbk: These are weights (non-negative numbers) assigned to each data point x1, x2, ..., xk in the dataset X.\n\u2022 \u03a3(\u03bbi) = 1 from i=1 to k: The weights must always sum to 1, ensuring that we are creating a weighted average.\n\u2022 point = \u03bb1x1 + \u03bb2x2 + \u00b7\u00b7\u00b7 + \u03bbkxk: This calculates a new point as a combination of the original data points, where each point's contribution is determined by its weight.\n\u2022 X is the set of data points.\nWhile the convex hull provides insights into the changing extent of the data distribution, calculating it in high dimensions becomes computationally expensive, making it less practical for real-time drift monitoring.\nThe convex hull can provide insights into how the distribution of data is changing, especially in terms of its outer limits. A shift in the convex hull indicates that new data is affecting the boundaries of the dataset, possibly signaling a drift.\nAlthough useful in detecting shifts, calculating the convex hull in high-dimensional spaces is computationally expensive. This makes it less feasible for real-time drift detection, particularly in complex, high-dimensional datasets."}, {"title": "Quantifying Data Deformation: A Multifaceted Approach to Drift Detection", "content": "The success of our proposed drift detection methodology relies on robustly quantifying how new data deforms the geometric structure of the original data space. We model our data within an n-dimensional Euclidean space Rn, where each data point, represented as a vector x = (x1,x2,...,xn) \u2208 Rn, corresponds to a specific combination of features."}, {"title": "Average Displacement: A First Step Toward Quantification", "content": "A simple yet intuitive measure of data deformation is average displacement. This approach quantifies the overall tendency of new data points to shift the center of mass of the baseline data distribution. Visualize the baseline data as a cloud of points connected by elastic bands. As new data points arrive, they tug on these bands, pulling the cloud in different directions. Average displacement measures the average strength of these pulls.\nLet us break down the calculation:\n\u2022 Center of Distribution (\u03bc): First, we identify the center of the baseline data, typically represented by the mean \u03bc. For datasets prone to outliers or with non-Gaussian distributions, the median or a robust estimate might be more appropriate.\n\u2022 Force Vector (Fi): For each new data point xi, we calculate a force vector Fi. This vector points from the baseline center \u03bc to the new point xi, representing both the direction and magnitude of the \"pull\":\nFi = xi - \u03bc\nThe magnitude of this force ||Fi || is the Euclidean distance between the new point and the baseline center.\n\u2022 Average Displacement (D): To get an overall measure of how much the new data is shifting the baseline distribution, we calculate the average displacement D:\nD = (1/(n-m)) * \u03a3(||Fi||) from i=m+1 to n\nwhere:\nD is the average displacement.\nn is the total number of data points (baseline + new).\nm is the number of data points in the baseline dataset."}, {"title": "Convex Hull: Outlining the Data's Boundaries", "content": "Higher D: A larger value of D indicates that the new data is pulling strongly on the original data cloud, potentially signaling a significant shift or drift in the underlying data distribution.\nLower D: A smaller value of D suggests that the new data is relatively similar to the baseline data, with only minimal shift.\nThis measure serves as an initial, intuitive step in the drift detection process, helping to detect if new data points are causing substantial changes in the overall data distribution. However, as mentioned earlier, while it gives a basic indication of drift, average displacement doesn't capture changes in the shape or orientation of the data distribution, which may require more sophisticated techniques.\nConvex hulls are another geometric subject related to drift detection. Consider the points in your data as pegs on a board. The shape you would get if you wrapped a rubber band around the outermost points is called the convex hull. Formally, it is the smallest convex set that contains all the data points.\nThe convex hull provides a clear visualization of the data's extent and how this extent changes as new data arrives. A shift in the convex hull suggests that the boundaries of the data distribution are moving.\nHowever, using the convex hull for drift detection presents challenges, especially in high-dimensional spaces. Computing the convex hull becomes increasingly complex as the number of features grows, making it computationally expensive and less practical for real-time monitoring. Additionally, the convex hull is highly sensitive to outliers. A single outlier moving far from the main cluster can drastically alter the convex hull, even if most of the data remains relatively stable."}, {"title": "Eigenvalue Analysis: Capturing Shape and Orientation Shifts", "content": "To capture more nuanced deformations, we leverage eigenvalue analysis. The covariance matrix of a dataset summarizes the spread and orientation of the data along different dimensions. By analyzing how the eigenvalues and eigenvectors of the covariance matrix change between the baseline and new data, we can detect stretching, compression, and rotation in the data space.\n\u2022 Eigenvalues: Represent the magnitude of variance along each corresponding eigenvector. Changes in eigenvalues indicate stretching or shrinking of the data distribution along specific dimensions.\n\u2022 Eigenvectors: Represent the directions of greatest variance in the data. Shifts in eigenvectors indicate rotations in the data space, suggesting changes in feature correlations."}, {"title": "Eigenvalue Ratio (Dstretch/compress,i)", "content": "By comparing the eigenvalues and eigenvectors of the baseline and new data covariance matrices, we can quantify these deformations. Two key measures are defined:\nDstretch/compress,i = \u03bby/\u03bbx\nwhere:\n\u2022 Ay is the i-th eigenvalue of the new data covariance matrix.\n\u2022 Ax, is the i-th eigenvalue of the baseline data covariance matrix.\n\u2022 Dstretch/compress,i > 1 indicates stretching along the i-th principal component.\n\u2022 Dstretch/compress,i < 1 indicates compression along the i-th principal component."}, {"title": "Eigenvector Angle (Drotation,i)", "content": "Drotation,i = cos^(-1) (ux * uy)\nwhere:\n\u2022 ux, is the i-th eigenvector of the baseline data covariance matrix.\n\u2022 uy; is the i-th eigenvector of the new data covariance matrix.\nLarger angles indicate more significant rotation in the data space, reflecting changes in the orientation of the data distribution."}, {"title": "Local Density Estimation and Comparison: Detecting Subtle Shifts", "content": "While eigenvalue analysis provides a global view of shape changes, local density estimation and comparison allow us to identify subtle drift patterns that might not manifest as drastic changes in the overall shape. Imagine a city's population density map: drift could be represented by new neighborhoods emerging (increased density) or existing ones becoming deserted (decreased density).\nWe use Kernel Density Estimation (KDE) to estimate the probability density function (PDF) of the data, creating a smooth, continuous representation of the \"data terrain.\" By comparing the KDE-estimated density of new data points to the density expected based on the baseline model, we can pinpoint regions of the space where data concentration is changing.\nA common way to quantify these density deviations is using the Kullback-Leibler (KL) divergence, which measures the difference between two probability distributions. A higher KL divergence indicates a greater difference in local densities, suggesting a potential drift in that region of the data space."}, {"title": "Drawing Insights from Continuum Mechanics: The \"Strain\" of Data Drift", "content": "To further enrich our understanding of data deformation, we turn to the field of continuum mechanics, which studies the behavior of materials under stress and strain. Imagine stretching a rubber band. The movement of any point on the band from its original to its stretched position is its displacement. Strain, on the other hand, measures the relative deformation within the material-how much the rubber band has stretched compared to its original length.\nThe key concept here is the relationship between displacement and strain. Strain isn't just about movement; it's about how that movement changes across the material. A high strain indicates a rapid change in displacement over a small distance, like a sharp bend in our rubber band.\nWe can apply this analogy to our data space. The arrival of new data, particularly those points significantly different from the baseline, can be seen as a force acting on this space, causing \"stretching\" or \"compression\" along different dimensions."}, {"title": "Strain as a Lens for Drift", "content": "Let's unpack how different types of strain can provide insights into drift:\n\u2022 Normal Strain: This measures the elongation or compression along a specific axis. Imagine a key feature in your model, like \"customer age.\" A significant increase in the average age of new customers would correspond to a stretching of the data distribution along the \"age\" axis. Normal strain would capture this directly, highlighting features undergoing the most pronounced shifts.\n\u2022 Lateral Strain: Things get more interesting when we consider how changes in one feature can ripple through the relationships between other features. Imagine that as customer age increases (stretching along the \"age\" axis), the feature \"income\" becomes less correlated with customer behavior. This could manifest as a \"squeezing\" along the \"income\" dimension. Lateral strain reveals not just what's changing, but also how these changes affect feature relationships, offering clues about underlying shifts in the data-generating process.\n\u2022 Volumetric Strain: This measures the overall expansion or contraction of the data space. A large increase in volume could indicate the emergence of entirely new customer profiles, while shrinking volume might signal that your model is becoming more specialized to a narrower segment of data."}, {"title": "Formally Defining the Strain Tensor", "content": "Data Representation: We represent the data in a metric space (M, d), where M is the data space, and d is the distance metric defined on M. For numerical data, this space is typically Euclidean, i.e., M = R\" with the standard Euclidean distance metric. For more complex data types, such as text or images, the metric space may involve more specialized distance metrics, such as cosine similarity for text embeddings or structural similarity (SSIM) for images. The choice of metric depends on the nature of the data and its structure.\nBaseline Data Distribution: Let P represent the baseline data distribution, which corresponds to the data used to train the original model. This distribution captures the characteristics of the original dataset and defines the baseline for comparison when analyzing drift.\nDisplacement Field: We introduce the concept of a displacement field, denoted as v. The displacement field is a vector field that captures how new data points displace or \"move\" points from the original data space. More formally, for each point x \u2208 M, v(x) is the vector that describes the direction and magnitude of the displacement caused by the arrival of new data.\nStrain Tensor: The strain tensor, denoted as e, is a mathematical object that captures the local deformation caused by the displacement field at each point in the data space. It is formally defined as the gradient (or derivative) of the displacement field:\n\u03b5(x) = \u2207v(x)\nwhere \u2207v(x) is the matrix of partial derivatives of the displacement field v with respect to the coordinates of x. The strain tensor \u025b(x) provides a local measure of deformation at the point x in the data space, quantifying how the arrival of new data changes the relationships between neighboring points."}, {"title": "Interpreting the Strain Tensor with a Text Example", "content": "Diagonal Elements: The diagonal elements of the strain tensor \u025b(x) represent the amount of stretching or compression along individual dimensions in the text's word embedding space. Each dimension of the embedding space corresponds to a specific feature of a word's context or semantic meaning. For example, if we consider a word like \"economy,\" a large positive value along the diagonal element Eii might indicate that the word \"economy\" is appearing more frequently or in a more prominent context in the new data compared to the baseline text. This would suggest a significant shift in the usage of this word, perhaps due to changes in the topics discussed in the new text.\nOff-Diagonal Elements: The off-diagonal elements of the strain tensor capture changes in the relationships between different words or concepts. These elements reflect how the interactions between words have shifted in the new data. For instance, consider the words \"economy\" and \"inflation.\" If the off-diagonal element Eij between these two words shows a significant change, it could indicate that the relationship between these concepts has evolved in the new text. For example, if \"inflation\" and \"economy\" are appearing together more frequently in new contexts, the off-diagonal element would highlight this stronger correlation, reflecting the fact that these two concepts are being discussed in closer relation in the new data."}, {"title": "Adapting the Framework to Specific Data Types", "content": "The general strain tensor framework can be tailored to different data types by using data-specific methods for representing and calculating the displacement field and strain tensor. Below, we outline how the framework can be adapted for tabular data, image data, and text data, referencing relevant discussions and sources.\nTabular Data: For tabular data, the strain tensor can be calculated by comparing differences in means and covariance matrices between the baseline and new data. The displacement field can be estimated as the difference in feature values between the two datasets, and the strain tensor is then derived from the gradients of this displacement field. The eigenvalues and eigenvectors of the covariance matrix provide insights into how the data is stretched (eigenvalues) and rotated (eigenvectors). This approach is particularly useful for identifying feature-wise changes and shifts in the relationships between features.\nImage Data: In the case of image data, pre-trained convolutional neural networks (CNNs) or autoencoders can be employed as feature extractors to represent images as vectors in a lower-dimensional latent space. The strain tensor is then calculated within this feature space, where it captures changes in high-level visual features, such as shapes, textures, or patterns, extracted by the CNN or autoencoder. This approach allows us to analyze how the overall \"appearance\" of images in the dataset changes over time or between different datasets, by detecting stretching, compression, and deformation of features in this reduced space.\nText Data: For text data, changes in word embeddings are analyzed using metrics like cosine distance or Kullback-Leibler (KL) divergence to quantify semantic shifts. Word embeddings represent words as vectors in a semantic space, and the strain tensor is calculated based on changes in pairwise distances between these embeddings. This captures how the relationships between words have changed over time or between different datasets. For example, a significant change in the embedding distances between words like \"economy\" and \"inflation\" could indicate a semantic drift, reflecting a shift in the context in which these words are used in the new data. Such metrics provide a way to detect contextual and semantic drift in text data by measuring shifts in the underlying semantic relationships."}, {"title": "Relating the Strain Tensor to Specific Types of Drift", "content": "The strain tensor provides a powerful framework for detecting different types of drift in data. By analyzing the diagonal and off-diagonal elements of the strain tensor", "Drift": "Concept drift refers to changes in the relationship between features and the target variable over time. The off-diagonal elements of the strain tensor, which capture changes in the correlations between different features, are particularly useful in identifying concept drift. A significant change in the off-diagonal elements indicates that the relationship between features has shifted, which could reflect a change in the underlying target concept. For instance, if the relationship between \"age\" and \"income\" has"}]}