{"title": "Learning Sketch Decompositions in Planning via Deep Reinforcement Learning", "authors": ["Michael Aichm\u00fcller", "Hector Geffner"], "abstract": "In planning and reinforcement learning, the identification of common subgoal structures across problems is important when goals are to be achieved over long horizons. Recently, it has been shown that such structures can be expressed as feature-based rules, called sketches, over a number of classical planning domains. These sketches split problems into subproblems which then become solvable in low polynomial time by a greedy sequence of IW(k) searches. Methods for learning sketches using feature pools and min-SAT solvers have been developed, yet they face two key limitations: scalability and expressivity. In this work, we address these limitations by formulating the problem of learning sketch decompositions as a deep reinforcement learning (DRL) task, where general policies are sought in a modified planning problem where the successor states of a state s are defined as those reachable from s through an IW(k) search. The sketch decompositions obtained through this method are experimentally evaluated across various domains, and problems are regarded as solved by the decomposition when the goal is reached through a greedy sequence of IW(k) searches. While our DRL approach for learning sketch decompositions does not yield interpretable sketches in the form of rules, we demonstrate that the resulting decompositions can often be understood in a crisp manner.", "sections": [{"title": "Introduction", "content": "A common challenge in planning and reinforcement learning is achieving goals that require many actions. Addressing this challenge typically involves learning useful subgoals or hierarchical policies that abstract primitive actions (Sutton, Precup, and Singh 1999; McGovern and Barto 2001; Kulkarni et al. 2016; Park et al. 2024). Yet the principles underlying the corresponding problem decompositions are not well understood. Consequently, methods for learning subgoals and hierarchical policies often lack robustness, working effectively in some domains while failing completely in others, without a clear explanation for these differences in performance. Recently, a powerful language for expressing, learning, and understanding general problem decompositions has been proposed (Bonet and Geffner 2021; Drexler, Seipp, and Geffner 2022). A sketch decomposition for a class of problems Q defines a set of subgoal states G(s) for each reachable state s in an instance P \u2208 Q. In any state s, the planner's task is not to reach the distant problem goal but to move to a closer subgoal state in G(s).\nThe concept of a goal being easily reachable or not is formalized through the notion of problem width (Lipovetzky and Geffner 2012; Bonet and Geffner 2024). A class of problems with width bounded by a constant k can be solved optimally by the IW(k) algorithm in time exponential in k. Many planning domains have a width no greater than 2 when goals are restricted to single atoms. A sketch decomposition G(.) divides problems P in class Q into subproblems P[s, G(s)], which resemble P but with initial state s and goal states G(s). If all these subproblems have width bounded by k, the decomposition width over Q is bounded by k, allowing the problems in Q to be solved by a greedy sequence of IW(k) calls, provided the decomposition is acyclic and safe, meaning no subgoal cycles or dead-end states among the subgoals (Bonet and Geffner 2021).\nMethods for learning safe, acyclic sketch decompositions with bounded width, represented by a set of sketch rules, have been developed (Drexler, Seipp, and Geffner 2022, 2023), following techniques previously used for learning general policies (Frances, Bonet, and Geffner 2021). These learning methods rely on feature pools derived from domain predicates and a min-cost SAT solver, leading to two key limitations: scalability and expressivity. Large feature pools enhance expressivity but result in large theories that are difficult for combinatorial solvers to handle.\nIn this work, we address these limitations by framing the problem of learning sketch decompositions as one of learning general policies in a deep reinforcement learning (DRL) context. Here, feature pools are not made explicit, and combinatorial solvers are unnecessary. We build on a novel observation connecting sketch decompositions with general policies and leverage an existing implementation for learning general policies via DRL (St\u00e5hlberg, Bonet, and Geffner 2023). The resulting method learns sketch decompositions bounded by a given width parameter k and uses them to search for goals across various domains through a greedy sequence of IW(k) searches. Unlike symbolic methods, the DRL approach does not produce rule-based sketches but neural network classifiers. However, as we will demonstrate, while interpreting these classifiers is not straightforward, it is often possible to understand the resulting decompositions in a crisp manner.\nThe structure of the paper is as follows. We begin with"}, {"title": "Example", "content": "The Delivery domain, similar to the Taxi domain in hierarchical reinforcement learning, involves N packages spread across an n x m grid, with an agent tasked to deliver them, one by one, to a target cell. The sketch decomposition $G_2$, where $s' \\in G_2(s)$ if the number of undelivered packages u(.) is smaller in $s'$ than in s, yields subproblems P[s, G\u2082(s)] of width bounded by 2, solved optimally by IW(2). The subproblems P[s, G\u2082(s)] are like P but with initial state s and goal states G\u2082(s). Similarly, the sketch decomposition $G_1$, where $s' \\in G_1(s)$ if either u(s') < u(s) and a package is held in s, or u(s') = u(s) and a package is not held in s but is held in s', produces subproblems P[s, G\u2081(s)] of width 1, solvable optimally by IW(1).\nA previous approach learns such decompositions using feature pools, a width parameter k \u2208 {1,2}, and combinatorial methods (Drexler, Seipp, and Geffner 2022). The decompositions are represented implicitly by collections of sketch rules. This work is aimed at learning similar width-k decompositions $G_k(s)$ but using neural networks trained via reinforcement learning. While the learned representations will not be as transparent, we will see that the resulting decompositions often are. Figure 1 shows indeed the number of IW(1) and IW(2) calls needed to solve Delivery instances as a function of the number of packages N after learning two general domain decompositions G\u2081 and G\u2082 via deep reinforcement learning. The number of calls, 2N and N, match exactly the number of calls that would be needed to solve these instances using the sketch decompositions defined above, despite using no rule-based representation or explicit feature pool, but solely a neural net trained via RL."}, {"title": "Background", "content": "We briefly review classical planning, the notion of width, general policies and sketches, and methods for learning them, following Lipovetzky and Geffner (2012), Frances, Bonet, and Geffner (2021), Bonet and Geffner (2021), Drexler, Seipp, and Geffner (2022), and St\u00e5hlberg, Bonet, and Geffner (2023)."}, {"title": "Classical and Generalized Planning", "content": "A planning problem or instance is a pair P = (D, I) where D is a first-order domain with action schemas defined over predicates, and I contains the objects in the instance and two sets of ground atoms defined over the objects and predicates defining the initial and goal situations Init and Goal. An instance P defines a state model S(P) = (S, so, G, Act, A, f) where the states in S are the possible sets of ground atoms, each one capturing the atoms that are true in the state. The initial state so is Init, the set of goal states G are those that include the goal atoms Goal, and the actions Act are the ground actions obtained from the schemas and objects. The ground actions in A(s) are the ones that are applicable in a state s; namely, those whose preconditions are (true) in s, and the state transition function f maps a state s and an action a \u2208 A(s) into the successor state $s' = f(a, s)$. A plan \u03c0 for P is a sequence of actions ao,..., an that is executable in so and maps the initial state so into a goal state; i.e., $a_i \\in A(s_i)$, $s_{i+1} = f(a_i, s_i)$, and $s_{n+1} \\in G$. A state s is solvable if there exists a plan starting at s, otherwise it is a dead-end. The cost of a plan is assumed to be given by its length, and a plan is optimal if there is no shorter plan.\nA generalized planning problem instead is given by a collection Q of instances P = (D, I) from a given domain; for example, all instances of Blocks world where the goal just involves on atoms. The solution of a generalized problem is not an open-loop action sequence but a closed loop policy as detailed below. In general, the instances in Q are assumed to be solvable, and moreover, the set Q is normally assumed to be closed in the sense that if P is in Q with initial state so and P' is P but with a solvable initial state reachable from so, then P' is assumed to be in Q as well."}, {"title": "Width", "content": "The simplest width-based search procedure is IW(1), a modified breadth-first search over the rooted directed graph associated with the state model S(P). It prunes newly generated states that fail to make an atom true for the first time in the search. IW(k), for k > 1, extends this concept by pruning states that do not make a collection of up to k atoms true for the first time. These algorithms can be alternatively conceptualized using the notion of state novelty. In this view, IW(k) prunes states with novelty greater than k, where a state's novelty is defined by the size of the smallest set of atoms true in that state and false in all previously generated states. Central to these algorithms is the concept of problem width. The width of a problem P is determined by the size of the smallest chain of atom tuples t\u2080,... tn that is admissible in P and has size $max_i |t_i|$ (Lipovetzky and Geffner 2012). For instance, Blocks World instances with atomic goals on(x, y) and Delivery instances with goals at(pkg, loc) have width 2 or less.\nIW(k) algorithms find optimal (shortest) solutions in time and space exponential to the problem width. However, planning problems with multiple conjunctive goals often lack a bounded width (i.e., width independent of the instance size). To address this, a variant called SIW was developed. SIW greedily seeks a sequence of IW calls, each decreasing the number #g of unachieved atomic goals (Lipovetzky"}, {"title": "General Policies and Sketches", "content": "A simple but powerful way to express the solutions to generalized planning problems Q made up of a collection of instances P, is by means of rules of the form C \u21d2 I defined over a set of features \u03a6 (Bonet and Geffner 2018). A state pair [s, s'] satisfies the rule if C is true in s and the features in I change value when moving from s to s' in agreement with E. For example, E can express that a numerical feature must increase its value, and that a Boolean feature must become true, etc. A set of rules R defines a non-deterministic general policy \u03c0 for Q which in any reachable state s in P\u2208 Q selects the successor states s' of s when the state pair [s, s'] satisfies a rule in R. The transitions (s, s') are then called \u03c0-transitions, and the policy \u03c0 solves an instance P\u2208 Q if all the \u03c0-trajectories that start in the initial state of P reach a goal state.\nThe same language used to define general policies can be used to define sketch decompositions. Indeed, a set of rules R defines the subproblems P[s, GR(s)] over the reachable non-goal states s of instances P\u2208 Q, which are like P but with initial state s and goal states s' \u2208 GR(s) for the state pairs [s, s'] that satisfy a rule in R. The width of the decomposition is the maximum width of the subproblems P[s, GR(s)], P \u2208 Q, and the decomposition is safe and acyclic in Q if there is no sequence of (subgoal) states s\u2081,..., sn, Si+1 \u2208 GR(Si) and n > 1, in any P\u2208 Q that starts in a reachable, alive state s\u2081 (not a dead-end, not a goal), and ends in a dead-end state sn or in the same state sn = s\u2081. Here GR(s) stands for the states s' \u2208 GR(s) that are closest to s. If the decomposition resulting from the rules R is safe, acyclic, and has width bounded by k, then the problems P\u2208 Q can be solved by a slight variant of the SIW algorithm where a sequence of IW(k) calls is used to move iteratively and optimally from a state s_i to a subgoal state si+1 \u2208 GR(Si), starting from the initial state of P and ending in a goal state (Bonet and Geffner 2021)."}, {"title": "Learning General Policies through DRL", "content": "Rule-based policies and sketches can be learned without supervision by solving a min-cost SAT problem over the state transitions of a collection of small training instances P from Q and a pool of features derived from domain predicates and a fixed set of grammar rules based on description logics (Bonet, Frances, and Geffner 2019; Drexler, Seipp, and Geffner 2022). However, some domains require highly expressive features, which necessitate the application of numerous grammar rules, leading to large feature pools that challenge combinatorial solvers.\nTo address this limitation, a recent approach introduced learning general policies in a deep reinforcement learning (DRL) setting (St\u00e5hlberg, Bonet, and Geffner 2023). This approach employs a standard actor-critic RL algorithm (Sutton and Barto 1998) with the policy and value functions \u03c0(s' | s) and V(s) represented by neural networks. As shown in Fig. 1, gradient descent updates the parameters \u03b8 and \u03c9 of the policy and value functions. The key difference from standard actor-critic codes is that \u03c0 selects the next state from possible successors N(s) instead of the next action.\nThe learned policy functions generalize to larger domain instances than those used in training. This generalization is achieved by encoding the policy and value functions in terms of a relational GNN, which generates real-vector embeddings f\u03b8(o) for each object in the instance (Scarselli et al. 2008; Hamilton 2020). Suitable readout functions then map these embeddings into the values V(s) and the probabilities \u03c0(s' | s) (St\u00e5hlberg, Bonet, and Geffner 2023)."}, {"title": "Learning Decompositions via DRL", "content": "The contribution of this paper is a novel scheme for learning how to decompose planning problems into subproblems that can be solved through iterative applications of the IW(k) algorithm. Decomposing problems into subproblems is crucial, but the principles guiding such decompositions are not"}, {"title": "Algorithm 1: Actor-Critic RL for generalized planning", "content": "Input: Training MDPs {Mi}i, each with state priors pi\nInput: Policy \u03c0(s' | s) with parameters \u03b8\nInput: Value function V(s) with parameters \u03c9\nOuput: Policy \u03c0(s' | s)\nParameters: Step sizes \u03b1, \u03b2 > 0, discount factor \u03b3\nInitialize parameters \u03b8 and \u03c9\nLoop forever:\nSample MDP index i \u2208 {1, ..., n}\nSample non-goal state S in Mi with probability pi\nSample state S' from N(S) with prob. \u03c0(S' | S)\nLet \u03b4 = 1 + \u03b3V(S') \u2013 V(S)\n\u03b8 \u2190 \u03b8 \u2013 \u03b1\u03b4\u2207 log \u03c0(S' | S)\n\u03c9 \u2190 \u03c9 + \u03b2\u03b4\u2207V(S')\nIf S' is a goal state, \u03c9 \u2190 \u03c9 \u2013 \u03b2V(S')\u2207V(S')"}, {"title": "Learning Decompositions via DRL", "content": "The contribution of this paper is a novel scheme for learning how to decompose planning problems into subproblems that can be solved through iterative applications of the IW(k) algorithm. Decomposing problems into subproblems is crucial, but the principles guiding such decompositions are not well-defined. Our goal is to achieve decompositions that are general (applicable across a class of problems Q), safe (avoiding dead-ends), acyclic, and have width bounded by k. Additionally, we aim to learn these decompositions without relying on combinatorial solvers or explicit feature pools, leveraging the relationship between sketch compositions and general policies, as well as the method reviewed above for learning general policies through DRL.\nIt is known that general policies are sketch decompositions of zero width, which are safe and acyclic (Bonet and Geffner 2024). Our new observation is that safe, acyclic sketch decompositions with approximate width k > 0 for a class of problems P \u2208 Q can be derived from general policies over a slightly different class of problems Pk \u2208 Qk, where the set of successor states N(s) in P is replaced by the set Nk(s) of states reachable from s via IW(k):\n$N_k(s) := {s' | s' \\text{ is reachable from } s \\text{ via } IW(k)} \\qquad (1)$\nThe successor states s' that the policy \u03c0(s' | s) selects in Pk will be subgoal states s' in P that can be reached from s via IW(k). The decomposition's width is approximately bounded by k because while a width bounded by k guarantees reachability via IW(k), the reverse is not necessarily true.\nThe modification of the DRL algorithm from (St\u00e5hlberg, Bonet, and Geffner 2023) to learn safe and acyclic decompositions of width k over a class of problems P\u2208 Q is straightforward: the only change required is to replace the set of successor states N(s) in line 10 of Algorithm 1 with the set Nk(s) reachable from s via IW(k). This extended set of successor states is then used in the softmax normalization to yield the probabilities \u03c0(s' | s). Action costs are assumed to be all 1 for reaching either N or Nk successors.\nLet \u03c0(s' | s) be the general stochastic policy learned by the algorithm in Fig. 1 after replacing N(s) with Nk(s). The resulting decomposition G(\u00b7) can then be defined in two ways: greedily, as the singleton sets:\n$G(s) := { s' }, s' = \\underset{s' \\in N_k(s)}{\\arg \\max } \\pi(s' | s), \\qquad (2)$\nand stochastically, as the singleton sets:\n$G(s) := { s' }, s' \\sim \\pi(s' | s), s' \\in N_k(s). \\qquad (3)$\nIn the first case, a single subgoal state s' for s is chosen as the most likely state in Nk(s) according to the learned policy \u03c0; in the second, case, s' is sampled stochastically from the set Nk(s) with probability \u03c0(s' | s). In P, s' may not be a direct successor of s but can be reached from s via IW(k). Intuitively, to decompose the problem, we are allowing the \"agent\" to make IW(k) \"jumps\u201d in P following the learned policy for Pk where such \"jumps\" are primitive actions.\nIf the policy \u03c0 solves the problem Pk, then the decomposition G will be safe and acyclic. A sequence of subgoal states s\u2080, s\u2081,..., sn with n > 1 and si+1 \u2208 G(si) for"}, {"title": "Experiments", "content": "The experiments aim to address several key questions. First, are the learned decompositions Gk = G both general and effective? Specifically, can the (larger) test instances be solved by a greedy sequence of IW(k) calls? This question is non-trivial, as success with k = 1 would imply solving instances with linear memory relative to the number of atoms by running IW(1) sequentially, a significant contrast to solving instances via exponential time and memory search. Second, can the resulting decompositions, represented in the neural network, be understood and interpreted? This is also challenging, as there is no guarantee that the learned decompositions will be meaningful. To answer the first question, we will examine the coverage of the SIW\u201d (k) algorithm using the learned decomposition G, the number of IW(k) calls (subgoals), and the total plan length. To address the second question, we will analyze plots showing the number of subgoals resulting from the learned decomposition G as a function of relevant parameters of the test instances (e.g., the number of packages). In the experiments, the subgoal states G(s) are sampled stochastically according to (3), although the results (in the appendix) are not too different when they are chosen deterministically as in (2). The code and data will be made publicly available."}, {"title": "Learning Setup", "content": "We use the DRL implementation from (St\u00e5hlberg, Bonet, and Geffner 2023) with the same hyperparameters to learn the policy \u03c0 that defines the decomposition G. The GNN has feature vectors of size 64 and 30 layers. The Actor-Critic algorithm uses a discount factor \u03b3 = 0.999, a learning rate \u03b1 = 2 \u00d7 10\u22124, the Adam optimizer (Kingma and Ba 2015), and runs on a single NVIDIA A10 GPU for up to 48 hours per domain. Five models are trained independently with different seeds, and the model with the best validation score is selected for testing. The validation score is determined by the ratio Lv/Lv, where Lv is the plan length from SIW\u201d (k) and L is the optimal plan length, both averaged over all states of a validation set. Training is stopped early if this ratio approaches 1.0."}, {"title": "Data", "content": "The domains and training data are primarily from previous works on learning sketches and general policies (Drexler, Seipp, and Geffner 2022; St\u00e5hlberg, Bonet, and Geffner 2023). This includes Blocks with single and multiple target towers (19 and 22 training instances, respectively), Childsnack (49 instances), Delivery (75), Grid (13), Gripper (10), Logistics (25), Miconic (63), Reward (15), Spanner (140), and Visitall (12). Each domain is tested on 40 larger instances, which extend those used in prior studies (details in the appendix)."}, {"title": "Results", "content": "Table 1 presents the performance of the SIW\u201d (k) algorithm using the learned G decomposition, where \u03c0is the policy derived from the RL algorithm after replacing the set of successors N(s) with Nk(s). Key performance metrics include coverage (Cov), subgoal count (SL), and plan length (L). The table's upper section shows results for IW(1), while the lower section displays IW(2) results for selected domains.\nIn the table, LM indicates the plan length computed by the classical planner LAMA, run on an Intel Xeon Platinum 8352M CPU with a 10-minute time and 100 GB memory limit. The columns labeled \u201csubgoal cycle prevention\u201d reflect a minor SIW\u201d(k) algorithm modification that avoids revisiting a subgoal state. For this, states that have already been selected as subgoals before are not considered as future subgoals. This adjustment impacts performance in three of the eleven domains, including two (Grid and Logistics) where the width-1 decompositions learned were poor.\nCoverage The SIW\u201d (k) algorithm achieves nearly perfect coverage across all domains, except in Logistics, Grid, and Blocksworld Multiple with IW(1). However, for width-2 decompositions, coverage improves to near 100% in all domains, including these three. The reason for this discrepancy is not entirely clear, but one possibility is that width-1 sketch decompositions cannot be fully captured in the logical fragment represented by GNNs. It is known that GNNs cannot represent width-0 sketch decompositions (i.e., general policies) for Logistics and Grid (St\u00e5hlberg, Bonet, and Geffner 2022), suggesting that the same might hold for width-1 decompositions. Interestingly, GNNs do accommodate width-2 sketch decompositions in these domains, as shown in the table, aligning with findings from previous research, which observed that while certain feature pools cannot express rule-based general policies, they can express rule-based sketches.\nSubgoal Count The column SL in the table presents the average number of subgoals encountered by SIW\u201d (k) on the path to the goal. Although this number alone may not be highly meaningful, it is significantly lower than the average plan lengths, indicating that each subgoal requires multiple actions to be achieved. More interestingly, Figure 2 illustrates the number of subgoals as a function of the number of objects of a selected type per domain (e.g., packages in Delivery, balls in Gripper, children in Childsnack) for k = 1. In these cases, the relationship is nearly linear, with a coefficient of 1 in Spanner and Reward, and 2 in Delivery, Gripper, Childsnack, and Miconic. This suggests that the decomposition divides the problem into subproblems, one for each relevant object, which in some cases are further split in two (e.g., in Delivery, each package must be picked up and dropped off in separate IW(1) calls). Despite the use of neural networks and DRL, the resulting decompositions can be understood. In Blocksworld, however, the situation is different. The plots in Fig. 3 show that the width-1 decomposition generates more subgoals than there are on-atoms in the goal (shown in blue versus black), while the width-2 decomposition generates fewer subgoals than on-atoms in the goal (shown in yellow). While individual on-atoms have a width of 2 and are thus always reachable by IW(2), certain states allow for pairs of on-atoms to be reached by IW(2) as well. The result is that the plots of subgoal counts in Blocksworld are not showing strict linear relationships.\nPlan Quality The column L/LM in the table shows the ratio between the average plan lengths found by SIW\u201d (k) and those found by LAMA. Generally, this ratio is close to 1, but there are exceptions in certain domains, such as Reward and Visitall for IW(1) and several others for IW(2). The general explanation is that decompositions simplify the problems, allowing them to be solved by a linear search like IW(1), rather than an exponential search as in LAMA. This simplification, however, can preclude shortcuts, resulting in longer plans. A more specific explanation is that the DRL algorithm minimizes the number of IW(k) subproblems on the path to the goal, without considering the cost of solving these subproblems, as measured by the number of actions required. For instance, if a single package is to be delivered from a state s with two options\u2014a nearby package and a distant one\u2014SIW\u2122 (k) does not prefer one over the other since both states s' and s\" where either package is held are Nk-successors of s with the same cost of 1. To address this limitation, two approaches could be considered: 1) modifying SIW\u201d (k) to prefer Nk-successors s' that have a high probability \u03c0(s' | s) and are closer to s, or 2) retaining the true cost information of Nk-successors and using this in the DRL algorithm to optimize a combination of the number of subgoals and the cost of achieving them. The first approach would involve modifying the SIW\u201d (k) algorithm, while the second would require changes to the training process. Exploring these approaches, however, is beyond the scope of this work.\nIt is also worth noting that problem representations (in"}, {"title": "Analysis", "content": "The symbolic approach to problem decompositions using sketch rules offers transparency, allowing direct interpretation of the defined decompositions. Interestingly, the inverse process is also possible: equivalent sketch rules can be reconstructed from learned decompositions by analyzing subgoal counts and plan structures. Indeed, the width-1 decompositions learned for Delivery, Gripper, Miconic, Childsnack, and Spanner can be understood in terms of four sketches (Bonet and Geffner 2021) with numerical features"}, {"title": "Conclusion", "content": "We have demonstrated that DRL methods can effectively learn the common subgoal structures of entire collections of planning problems, enabling them to be solved efficiently through greedy sequences of IW(k) searches. While the resulting decompositions are represented by neural networks rather than symbolic rules, they can often be interpreted logically. The experimental results highlight the strength of this approach, where decompositions learned from small instances are applied to solve much larger instances using sequences of linear and quadratic IW(1) and IW(2) searches. This approach leverages the concepts of width, sketches, and the logic of GNNs, and its limitations can be understood and potentially addressed within this framework.\nTwo specific challenges for future work include: 1) incorporating the cost of subproblems to reduce plan lengths while still learning meaningful decompositions, and 2) developing two-level hierarchical policies to eliminate the need for IW(k) searches in subproblems altogether."}]}