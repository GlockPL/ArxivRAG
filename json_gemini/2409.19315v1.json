{"title": "Analog In-Memory Computing Attention Mechanism for Fast and Energy-Efficient Large Language Models", "authors": ["Nathan Leroux", "Paul-Philipp Manea", "Chirag Sudarshan", "Jan Finkbeiner", "Sebastian Siegel", "John Paul Strachan", "Emre Neftci"], "abstract": "Transformer neural networks, driven by self-attention mechanisms, are core components of foundational and Large Language Models. In generative transformers, self-attention uses cache memory to store token projections, avoiding recomputation at each time step. However, GPU-stored projections must be loaded into SRAM for each new generation step, causing latency and energy bottlenecks for long sequences.\nIn this work, we propose a fast and energy-efficient hardware implementation of self-attention using analog in-memory computing based on gain cell memories. Volatile gain cell memories can be efficiently written to store new tokens during sequence generation, while performing analog signed weight multiplications to compute the dot-products required for self-attention. We implement Sliding Window Attention, which keeps memory of a finite set of past steps. A charge-to-pulse converter for array readout eliminates the need for analog-to-digital conversion between self-attention stages. Using a co-designed initialization algorithm to adapt pre-trained weights to gain cell non-idealities, we achieve NLP performance comparable to ChatGPT-2 with minimal training iterations, despite hardware constraints. Our end-to-end hardware design includes digital controls, estimating area, latency, and energy. The system reduces attention latency by up to two orders of magnitude and energy consumption by up to five orders compared to GPUs, marking a significant step toward ultra-fast, low-power sequence generation in Large Language Models.", "sections": [{"title": "1. Introduction", "content": "Transformer networks [1] are today's state-of-the-art neural networks for sequence processing, outperforming both Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). The success of Transformers relies on the attention mechanism [2] to identify temporal correlations. Unlike RNNs, which build an implicit memory through temporal dependencies of neuron states, the attention mechanism uses an explicit memory by comparing each sequence element, known as a token, with elements from many time steps simultaneously. This explicit memory mitigates the vanishing/exploding gradient problem during backpropagation through time present in RNNs. Comparisons between tokens are made using dot-products between different projections of the input sequence, called queries (Q), keys (K), and values (V) as illustrated in Figure 1 (a).\nFor generative tasks, the best performances are achieved by auto-regressive, decoder-only Transformers. At each inference step, the decoder generates a token, which is then appended to the input sequence, forming the input for the subsequent step. In a single decoder inference step, the attention mechanism uses only the Q projection of the current token. However, it must compute the dot-products with K and V projections of all previously generated tokens. To avoid re-computation of K and V during text generation, the KV-caching method stores the projections from previous tokens in memory and updates the KV-cache with the new projections.\nFor each token, the Graphical Processing Unit (GPU) must transfer the entire KV-cache from its High Bandwidth Memory (HBM) to its SRAM memory. Additionally, the KV-cache is often much larger than the SRAM memory due to the dimensions of the stored projections and the sequence length [3]. For instance, the entire KV-cache of the model Mistral 7 B [4] requires 8 Gb for a batch size of one. In recent technologies, the data access energy requires higher energy than the computations [5]. Loading the projections K and V in the attention mechanism is thus a major bottleneck, causing increased energy consumption and latency in LLMs [6]. Hence, In-Memory-Computing (IMC) is an attractive alternative for energy-efficient transformer computation. Recent research publications proposed emerging non-volatile memory-based IMC architecture for Transformer inference [7, 8]. However, computing attention with these memory technologies presents challenges, as each inference step involves writing the K and V values. Non-volatile memory technologies, exhibit slow write speeds, high energy consumption during the writing process, and low endurance, which collectively limit their suitability for IMC of the attention mechanisms [9]. The authors of [10] proposed to employ FeFET-based IMC only for computing the linear projections of the transformer (i.e. converting input to Q, K, and V). The dot-product required for attention is computed in this work using CMOS units and SRAM-based memories are used to cache K and V. The same authors also used FeFET as fixed attention scores for memory- augmented neural networks [11]. Hence, there is a lack of IMC architectures that are capable of efficiently computing the attention operation of the transformer.\nIn this work, we propose a novel hardware architecture for attention computation, which is based on IMC with analog, capacitor-based gain cells performing signed weight Multipy-Accumulate (MAC) operations. First, our architecture eliminates the data write issues because gain cells have more endurance and require less write energy and time than non-volatile memories. Furthermore, since each gain cell is signed and multi-level, our architecture leads to a lower area footprint than an SRAM- based implementation, as SRAMs require multiple cells to achieve multi-level precision. Moreover, SRAMs are fully volatile while gain cells can hold a state for up to multiple seconds. Our architecture computes the attention (i.e. two consecutive dot-products, scaling, and nonlinear activation function) entirely in the analog domain without converting to digital at any intermediate step, thus avoiding power and area-hungry Analog to Digital Converters (ADCs). We utilize Pulse-Width Modulation (PWM) to transmit analog signals between consecutive dot-products and to implement scaling and nonlinearity. We only use digital blocks to compute the final attention output using digital adders. Another key contribution of this work is a comprehensive analysis of Algorithm-Hardware co-optimization. First, we implement the Sliding Window Attention mechanism [12], which is adapted to IMC. Unlike conventional attention whose memory requirement scales with the sequence length, the Sliding Window Attention only keeps track of the most recent tokens. Second, we model hardware constraints and non-idealities and integrate them into the training process. Since training LLMs from scratch is extremely expensive and because the non-idealities do not permit direct model fine-tuning, we introduce an innovative adaptation algorithm. The algorithm scales each layer according to its statistics to adapt the model to the hardware characteristics. With our adaptation algorithm, our model reaches similar accuracy as a pre-trained Chat-GPT2 model with very few training iterations.\nOur architecture achieves up to five and two orders of magnitude lower energy consumption and latency, respectively, in comparison to GPUs. We present chip-level implementation results, including a detailed floor plan and layout of all units. Overall, this work provides an in-depth Algorithm-Hardware analysis and co-design for IMC based attention computations."}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Analog In-Memory-Computing with Gain Cells", "content": "In-memory Computing aims to mitigate memory access bottlenecks by performing computation directly within memory crossbar arrays [9]. These crossbar arrays are often based on non-volatile memories, such as memristive devices, Flash and FeFETs. However, in this context, few reports exist on using dynamic, easily writable technologies. Currently, the two dominant types of dynamic memory are SRAM and DRAM. While SRAMs have substantial area requirements and face challenges in implementing multi-bit weights [13], DRAMs are restricted to digital computations at the memory bank peripheries due to destructive read operations, preventing parallel reads across the entire array [14,15]. In this context, gain cells are a potential alternative to commodity DRAM [16]. Unlike DRAMs, gain cells accumulate current on bit lines and are compatible with CMOS, enabling flexible integration. Similarly to DRAM, gain cells feature a capacitor to store information in charge. These cells offer substantial advantages over traditional DRAM by incorporating a second transistor within the read"}, {"title": "2.2. Hardware Attention", "content": "Most Transformer attention inference today is performed on either GPUs or TPUs. Multiple approaches for Transformer accelerators in hardware have been proposed. Most literature can be found regarding specialized digital ASICs for Inference [24] or by reducing the amount of computation through various pruning algorithms, [10,25]. In one approach [26], DRAM arrays are used to store the KV matrices, while performing MAC operations at the periphery of the memory banks. However, this method does not offer full parallelism.\nMore advanced analog IMC techniques have also been proposed, leveraging FeFET for in-memory linear projection computations and utilizing CMOS-based crossbars as cache to store attention scores for future reuse. Notably, in this method, the attention itself is not computed in memory [10]. This contrasts with our approach, which explicitly aims to compute the attention within the KV cache. In another work [27], the projections are computed using memristor crossbar arrays, while SRAM is employed to"}, {"title": "3. System, Algorithm and Architecture", "content": "In this section, we describe the basics of the attention mechanism, followed by a detailed description of our analog IMC implementation, and the algorithm-hardware co-optimizations."}, {"title": "3.1. Attention Mechanism", "content": "In auto-regressive Transformers, each token is compared with the projections of past tokens stored using KV-caching. The weights $W_{Q,K,V} \\in \\mathbb{R}^{D,d}$ generate the queries, keys, and values from an input token $x_i \\in \\mathbb{R}^{1,D}$:\n$Q_i, K_i, V_i = W_{Q,K,V}x_i.$\n(1)\nThe keys and values$K_i \\in \\mathbb{R}^{1,d}$ and $V_i \\in \\mathbb{R}^{1,d}$ are stored as part of the full KV-cache with $K \\in \\mathbb{R}^{T,d}$ and $V \\in \\mathbb{R}^{T,d}$. The query $Q_i \\in \\mathbb{R}^{1,d}$ is not stored but used for inference as\n$S_i = Q_iK^T;  A_i = \\Phi(\\frac{S_i}{\\sqrt{d}}) \\cdot V.$\n(2)\nThe dot product between the queries and keys produces an attention score matrix $S_i \\in \\mathbb{R}^{1,T}$. This attention score matrix is then scaled by the square root of the head dimension d and is typically passed through a softmax function to normalize the attention scores. However, other nonlinear activation functions $o$ can be used instead of softmax, yielding similar accuracy [28, 29]. The output of the attention mechanism $A_i$ is then obtained by the dot product between the output of the nonlinear function $\\Phi(S_i)$ and the values. In multi-head attention, different heads are implemented in parallel with different projection weights. The results of the different attention heads are concatenated and another linear layer produces the final multi-head attention result.\nIn the full attention mechanism, the attention score matrix S compares all past, present, and future elements of the sequence. However, different types of attention are used in state-of-the-art large language models (LLMs). In causal attention, for instance,"}, {"title": "3.2. Gain cell-Based Signed Weight Multiply-And-Accumulate Operations", "content": "Figure 1(c) presents our proposed single multiplier cell, designed to perform the MAC operations in the attention accelerator. The cell contains a write stage to write the value of either Q or K to the stage capacitor $C_1$, which acts as a memory, and a multiplication stage that approximates the product between the input and the stored weight.\nThe store stage capacitor is charged with a 15 ns multi-level voltage pulse emitted by a Digital to Analog Converter (DAC). The voltage pulse is gated to the designated capacitor by a write-enable (WE) transmission. The transmission gate enables discharging and charging the capacitor during write phases and operating non- destructive read phases.\nThe multiplication stage generates a current dependent on the stored capacitor voltage (Vstore), which implements the weight as depicted in Figure 1 (e). The input, which is a PWM signal, controls the state (closed or open) of another transmission gate. The read path is arranged in a push-pull configuration consisting of two other transistors operating the multiplication. If Vstore is close to Vss, only the PMOS transistor will source current into the word line (WL), resulting in a positive output current. Conversely, if Vstore is closer to VDD, only the NMOS will conduct, drawing current out of the word line and resulting in a negative output. If the stored voltage lies between the supply voltages, the currents of both transistors will be small and cancel each other out, resulting in zero current output. Both multiplying transistors are sized"}, {"title": "3.3. Dot-Products and Nonlinear Activation Function with Analog Gain Cells Arrays and Analog Charge-to-Pulse Converters", "content": "The architecture of the proposed hardware attention mechanism is depicted in Figure 1 (b). Two arrays of analog gain cells implement the two dot products of the attention mechanism, while an intermediate charge-to-voltage pulse converter block converts the signal between the two arrays and implements a ReLU activation function. In this section, we will show how the inference is performed for a single input token x \u2208 R1,D.\nTo implement the first dot-product (QiKT), the columns are written with one column of the keys matrix K\u2208 RM,d. The M columns of the array correspond to the keys of the previous M tokens, and the rows of the arrays correspond to the d different embedding elements. As explained in Section 3.2, the gain cells generate currents depending on their stored voltage which are summed along the bit lines.\nThe query Qi \u2208 R1,d is encoded as input of the first gain cells array, through PWM voltage pulses. Therefore, the gain cell arrays outputs currents are also encoded temporally, and thus they need to be integrated to retrieve the correct MAC results. Moreover, the second array also requires voltage pulse width PWM input.\nRather than utilizing an operational amplifier integrator combined with ADCs, which are both space- and energy-consuming, the signal between the two arrays is converted by a circuit as depicted in Figure. 1 (d) that integrates the currents and emits a voltage pulse of variable width depending on the accumulated charge, similarly as in [22]. This charge-to-pulse circuit operates in three distinct phases: sampling, discharge, and reset. During the sampling phase, input pulses are applied to the first gain cell array, and the currents generated by the cells are integrated by a capacitor (C2) in the charge-to-pulse circuit. This capacitor also utilizes the wire capacitance of the word line. In the discharge phase, the voltage of the capacitor C2 is discharged with a constant current controlled by the bias voltage Vb. An inverter acts as a simple comparator, triggering a pulse of variable width. Finally, in the reset phase, the bit line is reset to the initial bit line voltage to prepare for a new inference step. The charge accumulated by the capacitor C2 in the charge-to-pulse block at the end of each bit line is\n$S_i = \\int_0^{T_{max}} I_i(t) = \\sum_j Q_{j} i(K_{i,j}),$\n(3)"}, {"title": "3.4. Hardware Sliding Window Attention with Online Write And Read Cycles", "content": "For Sliding Window Attention, the input query is multiplied only with the M most recent keys and values, corresponding to the window size M. At each time step, the keys and values must be updated with the most recent token and the oldest one must be forgotten. All others remain stationary until they are updated after M cycles. In our implementation, we write the array that encodes the keys and values at inference time in a column-wise manner. One important aspect to note is that the structure of the arrays containing K and V must differ from each other. In the K array, a"}, {"title": "3.5. Scaling to Large Dimension Hardware Attention Mechanism", "content": "IR drop, caused by resistive losses in interconnects, results in reduced accuracy in large- scale analog crossbar arrays [31]. To mitigate IR drop issues, we limit the size of our gain cell arrays to 64 \u00d7 64. However, most Natural Language Processing (NLP) applications require larger dot-product dimensions within the attention head. Specifically, the memory of a Sliding Window Attention-based large language model (LLM) depends on its window size M. In our implementation, the sliding window size is determined by the number of columns, which is limited to 64 per array. Consequently, to accommodate larger window sizes and increase the model's memory, we perform inference across multiple sub-tiles, each containing a different array."}, {"title": "3.6. Pre-trained Models Hardware-Aware Fine-Tuning", "content": "The substantial time and energy costs of training LLMs can hinder the on-chip deployment of hardware neural networks. It is thus essential to integrate our gain cell- based attention hardware into an LLM without retraining it from scratch. However, using pre-trained weights from existing models is challenging because our attention mechanism differs from the conventional ones. Indeed, the analog gain cell multiplier"}, {"title": "4. Experimental Methods", "content": ""}, {"title": "4.1. Training algorithm", "content": "To evaluate our training algorithm and the inference accuracy of our architecture, we implemented the analog gain cell-based attention mechanism on the ChatGPT- 2 architecture [38]. ChatGPT-2 is a Transformer neural network with 124 million parameters, 12 layers, an attention mechanism input dimension of 768, 12 heads per attention block, and a head dimension of 64. We used the open-source text collection OpenWebText [39] split between training and testing samples, and the pre-trained ChatGPT-2 tokenizer to encode the plain text into tokens (vectors of size 50,304 each). Each training iteration had a batch size of 480, with sequences of length 1024 per sample. We selected a sliding window size of 1024, which matches the number of gain cell rows in the memory. Since the sequence length also equals 1024, each gain cell is written only"}, {"title": "4.2. Hardware SPICE simulations", "content": "To assess circuit performance accuracy, as well as values for energy consumption and speed, we conducted SPICE array simulations using the TSMC 28nm PDK within the Cadence Virtuoso environment. All simulations are based on a 64\u00d764 array, corresponding to the tile size in our architecture (see Figure 3 (a)). In these simulations, a parasitic wire capacitance of 0.8 fF and a series resistance of 2\u03a9 per array element are included. Both arrays, one performing (Q. KT) and the other performing \u03a6(S) \u00b7 V, are simulated separately, but always in combination with their specific charge-to-pulse circuitry readout circuitry."}, {"title": "4.3. GPU Attention inference energy and latency for comparison", "content": "To compare the computing speed and energy consumption of our architecture with existing technologies, we measured the latency and power consumption of two GPUs. One is a consumer Nvidia RTX 4090 GPU, and the other is a Nvidia Jetson Nano, which is designed for embedded applications. We perform ten runs of 1024 steps of auto-regressive token generation with a twelve attention heads. For a fair comparison, the linear projections are not implemented in this experiment since they are also not implemented by our hardware architecture, and the static power (measured before inference) is subtracted from the power measured during inference. For each run, we measure the latency and the power using the Nvidia-SMI python API, and average them."}, {"title": "4.4. Speed and Latency", "content": "Speed analysis was conducted using SPICE simulations to evaluate a realistic operation speed. Since we are using an auto-regressive transformer we only evaluate the elapsed time from presenting an input token to the point where an attention result A can be provided to the subsequent part of the transformer. We then examine the behavior of transient simulations, followed by comparing the attention scores computed on hardware with those used in our model."}, {"title": "4.5. Hardware Energy consumption", "content": "The values for Q, K, \u03a6(S), and V, which are inputs to our SPICE simulations were sampled from the distributions of the trained model, thus ensuring realistic estimations"}, {"title": "4.6. Area Estimation", "content": "To estimate the area of our design, we employ two complementary approaches. First, we provide an accurate measurement of the actual area occupied by our silicon CMOS demonstrator based on the TSMC 28 nm technology implemented gain cell array. Secondly, we will provide a custom floor plan with estimations derived from the literature concerning IGZO gain cells.\nThe pure silicon-based multiplier cell array incorporates Metal-on-Metal (MoM) capacitors, which are essential to the design and must be relatively large due to the high leakage associated with silicon transistors. The area estimation is derived from the physical layout and silicon implementation of CMOS technology, providing a precise representation of the space requirements. We performed the layout for this technology to accurately estimate the area of a single tile. Secondly, we present a custom floor plan specifically designed for an IGZO gain cell implementation to reduce the area footprint of our design. Indeed, larger retention times can be achieved with smaller gain cells since IGZO transistors have smaller leakage than silicon CMOS transistors and thus require smaller capacitance. This floor plan is based on assumptions from relevant literature [41], where IGZO gain cell arrays are noted for their ability to perform accurate dot-product operations. We assume a cell dimension of 1 \u00b5m \u00d7 1 \u00b5m, with all previously mentioned parameters remaining unchanged. This approach is supported by the reported compatibility of IGZO with existing CMOS processes [41]. In this floor plan, we also include the digital circuitry and routing, as illustrated in Figure 3 (b), to provide a projected view of the full-scale chip layout."}, {"title": "5. Results", "content": ""}, {"title": "5.1. Training Results", "content": "We train the linear model described in Section 3.6 on 3000 iterations with backpropagation and then we apply our adaptation algorithm to the hardware model (which includes the gain cells nonlinearity). In Figure. 4 (b), we show the evolution of"}, {"title": "5.2. Downstream Task Benchmarks", "content": "To demonstrate the efficacy of the overall model architecture and fine-tuning approach, Table 1 shows the performance of several variations of our model and baseline models on standard language model bench-marking tasks. We evaluate four models that are increasingly true to the hardware implementation. The datasets cover various types of problems. Our bench-marking setup follows [42] and [43] in terms of evaluated tasks and metrics. ARC-Easy and ARC-Challenge [44] focus on question answering, with ARC- Easy containing straightforward questions and ARC-Challenge featuring more difficult ones. WinoGrande [45] evaluates commonsense reasoning and coreference resolution by presenting minimal pairs to resolve ambiguities. HellaSwag [46] tests commonsense inference, requiring models to predict the most plausible continuation of a given context. LAMBADA [47] evaluates models' text understanding through a word prediction task that requires comprehension of broader discourse, not just local context. PIQA [48] assesses physical commonsense reasoning, testing a model's understanding of physical scenarios.\nOur nonlinear hardware model achieves a precision comparable to the one achieved by the public Chat-GPT2 model, except for the LAMBADA task. This is due to a reproducibility issue, as our Chat-GPT2 model trained from scratch also do not match the public Chat-GPT2 model. However, our model achieves a precision equivalent to our Chat-GPT2 model trained from scratch, which highlights that our hardware constraints do not hinder model training and accuracy."}, {"title": "5.3. Circuit Computing accuracy", "content": "The accuracy of an attention head, including the readout circuitry, is highlighted in Fig. 5.1 (d and e), where the same timing is used as described in Section 5.4. This is demonstrated for a single sub-tile, with two arrays representing both MAC arrays: one array containing the keys, incorporating the ReLU charge-to-pulse circuitry, and the other array containing the values, utilizing the signed charge-to-pulse circuitry (see Figure 4 (a)). The results from Fig. 5.1 (d) are used as the inputs for Fig. 5.1 (e). The circuit results are compared to the results of the attention model. The key difference between the model's attention mechanism and the hardware implementation"}, {"title": "5.4. Speed and Latency", "content": "Figure 5.1 (a) illustrates that a total time of 65 ns is required from the input query to the corresponding attention result. When the previous part of the transformer provides a new token, the process begins by computing the values \u03a6(S). Note that writing can be performed in parallel with the dot product operation, as when one array is computing, the other can be overwritten, and vice versa. The corresponding transient simulation for a single WL (Word Line) of the first array is displayed in Figure 5.1 (a). The process begins with a 5ns reset of the WL (RSTK), followed by 15 ns for applying the input pulses to perform the dot product, and then a 15ns discharge period to generate the output pulses for the second dot-product array. Figure 5.1 (c) presents the transient simulation results for this second dot-product array, which incorporates the signed pulse- to-charge circuitry. Similar to the previous stage, a reset (RSTV) is required, followed by applying the inputs from the preceding charge-to-pulse ReLU circuit to perform the dot product, which takes 15 ns. Afterward, a 15 ns discharge phase is required for the signed charge-to-pulse circuit, during which a digital counter simultaneously measures the pulse width and detects the sign. In the final step, estimated to take 15 ns, the results from each sub-tile are summed in the digital domain as indicated in Figure 5.1 (a), after which the final attention result, A, can be utilized by the next stage of the auto-regressive transformer."}, {"title": "5.5. Energy Consumption", "content": "In total, the MAC arrays within the attention head and the ReLU charge-to-pulse circuit which realize the (QKT) operation, consume 1120 pJ per token computation. The arrays, which implement (S). V and the signed charge-to-pulse circuitry consume 700 pJ. The lower energy consumption in the second dot-product arrays is attributed to the significantly sparser activation of \u03a6(S). Indeed, the zero voltage input does not produce currents in the gain cells and therefore reduces the power consumption. The digital control and routing block consumes a total power of 113.7mW. Assuming total compute time of 65 ns, the estimated energy consumption is 4nJ. The DAC's in one head including drivers require 330 pJ per token. Overall we can estimate the power consumption of processing one token for one attention head to 6.1 nJ.\nThe energy and speed comparison between GPUs and our architecture are shown in Figure. 6 (a and b). Focusing on the attention mechanism alone, our architecture can lead to a speedup of \u00d77,000 compared to Jetson Nano and \u00d7300 compared to RTX 4090, as well as an energy reduction of \u00d740, 000 compared to Jetson Nano and \u00d790,000 compared to RTX 4090."}, {"title": "5.6. Retention Time and Weight decay", "content": "Due to leakage in the storage capacitors, the voltages gradually decay over time, leading to exponential changes in the weights of the dot-product matrices. This simulation is based on silicon CMOS technology. Figure 5.1 (f and g) presents the simulated transient response of the storage capacitor voltage Vstore, which corresponds to the cells weight for both extreme values 0 V and 0.9 V. Figure 5.1 (f) highlights a stable operating window of 300 \u03bcs, where the maximum weight decay is below 7%. Figure 5.1 (g) highlights the long-term exponential decay as well as the final relaxation state. Since gain cells do not generate current for Vstored value of 0.45 V, they decay toward a resting position with value 0. Therefore, we avoid undesired biases and power consumption increases for unwritten memories. Note that retention times using IGZO technology exceed silicon CMOS retention times by multiple orders of magnitude."}, {"title": "5.7. Area and Floor plan", "content": ""}, {"title": "5.7.1. Silicon CMOS Area", "content": "The CMOS demonstrator cell dimensions are primarily dictated by the MoM capacitor. However, all silicon components can be placed on a parallel layer. A single cell has dimensions of 3.9 \u03bcm x 4.9 \u00b5m. For a full 64x64 array, this results in a total area of 0.08 mm\u00b2 per array. This calculation applies to both arrays. The ReLU charge-to-pulse circuitry occupies an area of 0.01 mm\u00b2, while the signed charge- to-pulse circuitry occupies around 0.02 mm\u00b2. The area of the charge-to-pulse circuits is dominated by the integration capacitors."}, {"title": "5.7.2. IGZO technology Area", "content": "Our floor plan, incorporating IGZO assumptions, is presented in Figure 3 (b). The figure illustrates the arrangement of 16 sub-tiles for one attention head, alongside the digital circuitry. This structure is designed to be"}, {"title": "6. Discussion", "content": "The energy and latency reductions we predict compared to GPUs arise from several factors. Performing multiplications in the analog domain is inherently more energy- efficient and faster than digital computation. Writing values to gain cells consumes less energy than to non-volatile memories. Furthermore, due to the Sliding Window method, each value is written only once, unlike other In-Memory Computing gain cell methods [19] that require multiple refresh operations.\nIn our design, we deliberately avoid using power-hungry ADCs by using analog charge-to-pulse converters instead, resulting in a slight discrepancy between the obtained outcomes and the ideal attention model. Future circuit improvements and modeling are needed to further close this gap.\nIn our design, we opted for a conservative approach by implementing an IGZO gain cell with an area of 1 \u00b5m\u00b2 per memory unit cell. The memory unit cell area is the limiting factor in the area footprint of our design. However, gain cell designs based on 2D materials are still an active area of research. In the future, smaller memory unit cells could be used, further decreasing the overall area footprint.\nAlthough this study is limited to device simulations, the algorithm is designed to adapt to varying device characteristics, making it valuable for training hardware-based networks with real device inference.\nThe energy and speed improvements of our method focus on the attention mechanism, a major bottleneck in Deep Neural Network inference. However, to achieve significant reductions in the overall energy consumption of artificial intelligence systems, all components must be optimized. Our hardware attention mechanism can be integrated with other IMC techniques implementing low-power linear layers in deep neural networks."}, {"title": "7. Conclusion", "content": "In this paper, we tackle the challenges of latency and energy consumption in data transfer and computation within the attention mechanism of generative models. We have presented a novel analog In-Memory Computing (IMC) architecture, utilizing capacitor-based gain cells to effectively address these issues. We have demonstrated that the entire attention computation can be executed using analog signals, with gain cell arrays performing the dot-products and analog pulse generators implementing the nonlinear activation functions. Our study presents a comprehensive end-to-end design of a hardware attention mechanism, integrating analog circuits for both memory and computation, alongside digital readout and input circuits. Our SPICE simulations demonstrate that the analog circuits can perform operations with high accuracy, and our neural network simulations confirm that a Large Language Model (LLM) implemented with these circuits can achieve text processing results comparable to software-based networks. Additionally, our weights adaptation algorithm significantly reduces the training time required for fine-tuning the hardware network, facilitating rapid chip deployment. Our architecture achieves a reduction in latency by up to two orders of magnitude and energy consumption by up to five orders of magnitude for attention computation. Overall, this work underscores the strong benefits of using In-Memory Computing with volatile yet low-power memories in attention-based neural networks, marking a critical step toward ultra-fast, low-power generative AI."}]}