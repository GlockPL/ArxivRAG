{"title": "Connecting Concept Convexity and Human-Machine Alignment in Deep Neural Networks", "authors": ["Teresa Dorszewski", "Lenka T\u011btkov\u00e1", "Lorenz Linhardt", "Lars Kai Hansen"], "abstract": "Understanding how neural networks align with human cognitive processes is a crucial step toward developing more interpretable and reliable AI systems. Motivated by theories of human cognition, this study examines the relationship between convexity in neural network representations and human-machine alignment based on behavioral data. We identify a correlation between these two dimensions in pre-trained and fine-tuned vision transformer models. Our findings suggest that the convex regions formed in latent spaces of neural networks to some extent align with human-defined categories and reflect the similarity relations humans use in cognitive tasks. While optimizing for alignment generally enhances convexity, increasing convexity through fine-tuning yields inconsistent effects on alignment, which suggests a complex relationship between the two. This study presents a first step toward understanding the relationship between the convexity of latent representations and human-machine alignment.", "sections": [{"title": "1 Introduction", "content": "As machine learning models are increasingly integrated into various aspects of daily life, understanding and improving human-machine alignment holds the promise of a safe, reliable and fair deployment of these technologies, ensuring that they operate in ways that are ethical, transparent, and beneficial to society [1]. Subsequently, we see a rising academic interest in the alignment of humans and machine learning models [2, 3, 4, 5, 6, 7], as well as the agreement between different alignment measures [8, 9]. This paper explores the connection between two quantities characterizing the structure of neural network representations inspired by cognitive science: a theory-based measure of convex regions in neural networks [10], inspired by G\u00e4rdenfors' theory of conceptual spaces [11], and an empirical measure of human-machine alignment based on a triplet odd-one-out task [7].\nG\u00e4rdenfors' theory posits that human cognition can be understood in terms of geometric structures, where concepts are represented as convex regions. Convexity is argued to assist generalization and communication of representations between humans [11]. T\u011btkov\u00e1 et al. [10] introduced a framework to measure the convexity of conceptual regions in neural networks and discovered pervasive convexity across many data domains and models. They find that greater convexity leads to better generalization, which aligns with human cognition theories. In this work, we ask the fundamental question of whether these convex regions align with concepts used in human decision-making.\nHuman-machine alignment can be measured in several ways [8], one of which is by comparison with humans' relative similarity judgments (e.g. [7, 4]). This may involve an odd-one-out image triplet task (\"Which of three images is the most different?\") using model representations and comparing the outcomes with the response of humans on the same task [7]. We chose this alignment metric as it can be easily evaluated for any neural network and additionally, methods for increasing this form of alignment exist [12].\nWe experimentally investigate the relationship between convex conceptual regions in neural network representations and alignment with human similarity judgments. We address two primary research questions:\n(1) Correlation between Convexity and Human-Machine Alignment: To what extent are convexity and human-machine alignment correlated in real-world models? Figure 1 illustrates potential scenarios of high and low levels of convexity and alignment. In particular, high convexity does not necessarily imply high alignment or vice versa. Yet, in non-pathological real-world scenarios, there may be a relationship between alignment and convexity. We perform extensive experiments suggesting that these two quantities are indeed correlated.\n(2) Impact of Improving Human-Machine Alignment on Convexity: Does increasing human-machine alignment lead to increased convexity? To understand whether the connection between alignment and convexity is causal, we increase human-machine alignment by applying the latent space transformations developed by Muttenthaler et al. [7, 12] and examine the effect on the convexity of conceptual regions. Additionally, we look at the effect of fine-tuning on convexity and alignment. Our findings indicate a complex relationship, showing an increase in convexity when optimizing pretrained models for alignment, but inconsistent effects on alignment during fine-tuning."}, {"title": "2 Background", "content": "In this work, we investigate the connection of two already developed measures, namely the graph convexity score [10] and the odd-one-out accuracy [7]."}, {"title": "2.1 Graph Convexity", "content": "The graph convexity score measures the convexity of concepts in latent spaces of neural networks [10]. It extends the common definition of convexity for Euclidean spaces to curved manifolds and utilizes sampled data. Graph convexity is defined as follows [10, 13]:\nDefinition 1 (Graph Convexity). Let (V, E) be a graph and A \u2286 V. A is convex if for all pairs x, y \u2208 A, there exists a shortest path P = (x = vo, v1, v2, \u00b7 \u00b7 \u00b7, vn\u22121, y = vn) and \u2200i \u2208 {0,..., n} : vi \u2208 A.\nThe graph convexity score measures the ratio of points within the same class that constitute the shortest path connecting any two points of the same class. It is constructed in the following way: first, we extract the latent representations of all data points in a given layer (V). Then, we construct a nearest neighbor graph (with N=10) with the Euclidean distance measure. Next, for each pair of data points that belong to the same class (x, y \u2208 A), we find the shortest path (P) in the neighbor graph and calculate the proportion of points along these paths that belong to the same class. The average of these proportions across all pairs is called the graph convexity score."}, {"title": "2.2 Human-Machine Alignment", "content": "As a measure of human-machine alignment, we use the triplet odd-one-out accuracy (OOOA) [7]. The underlying triplet task is based on the THINGS dataset [14], which contains images of natural objects. From this dataset, image triplets have been sampled, and human judgments of which of each triplet's images is the odd one out have been collected [15]. The alignment between humans and a neural network on the THINGS triplet task is measured by how well the human-designated odd one out can be directly identified using cosine-similarity of the images in latent space. For this, we first construct a similarity matrix $S \\in \\mathbb{R}^{3\\times 3}$ where $S_{i,j} := \\frac{x_i^T x_j}{||x_i||^2 ||x_j||^2}$, the cosine similarity between a pair of representations, for the representations x1, x2 and x3 of images of a given triplet. We identify the closest pair of images in the triplet as $arg \\max_{i,i<j} S_{i,j}$. The remaining image is the odd one out. The odd-one-out accuracy is defined as the proportion of matching responses between humans and the model. Important to note is that the observed agreement between humans is 67.22% and this value thus upper bounds the expected OOOA values [15].\nTo improve human-machine alignment (i.e. OOOA), we use the naive transform defined by Muttenthaler et al. [12]. It affinely transforms the latent space to maximize the alignment between human similarity judgments and the network's representations. This transformation consists of a square matrix W and bias b obtained as the solution to\n$arg \\min_{W,b} L_{global}(Z) + \\frac{\\lambda}{2} ||W||^2$,\nwhere $Z_{ij} = (Wx_i + b)^T (Wx_j + b)$ where {i,j} index any pair of representations in the dataset. The log-likelihood is defined as [16]:\n$L_{global}(Z) := \\frac{1}{n} \\sum_{s=1}^n \\log p({a_s,b_s}|{i_s, j_s, k_s}, Z)$.\nHere, n is the number of triplets, and the probability of a pair, {a,b} of the triplet {i, j, k}, being most similar, is given by the softmax over the representation similarities in Z."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Data", "content": "For all experiments, we used the THINGS dataset [14] and its attached human triplet responses [15]. The THINGS dataset consists of over 26.000 images of 1854 classes of natural objects, which can be grouped into 27 high-level categories (determined by human crowd-sourcing), which we will refer to as superclasses. The dataset features over 4.7 million human triplet odd-one-out responses, based on one image per class.\nWe used the same images chosen for the triplet experiment to determine the OOOA and 100 randomly chosen images per superclass from the THINGS dataset to determine the graph convexity score."}, {"title": "3.2 Models", "content": "Although convexity and human-machine alignment are domain-independent concepts, we restricted ourselves to vision models, as the THINGS dataset used to measure alignment in prior work [7, 12, 17] consists of images. We compared three different transformer-based vision models: Vision Transformers (ViT) [18], Bidirectional Encoder representation from Image Transformers (BEiT) [19], and data2vec [20]. For each model, we compared a base and a large architecture and both in their pretrained and fine-tuned (for ImageNet-1k [21] classification) version. The models consist of a feature extractor followed by 12 or 24 transformer layers [22] with an embedding size of 768 or 1024 respectively. While ViTs were pretrained for classification, BEiT and data2vec were pretrained using a self-supervised objective, where the goal was to predict masked-out input (data2vec) or representation tokens (BEIT). All models were pretrained on ImageNet-21k [21]."}, {"title": "3.3 Experiments", "content": "Correlation between Convexity and Human-Machine Alignment. To answer whether convexity and human-machine alignment are related, we perform a correlation analysis between the graph convexity and the odd-one-out-accuracy (OOOA). For this, we extracted the latent representations of the images used for the triplet after each transformer layer and measured the OOOA of the centered representations (as described in subsection 2.2). For the convexity analysis, we extracted the latent representations of 100 images for each superclass of the THINGS dataset after each transformer layer. We averaged the convexity scores of all classes to get one score per layer. We performed a correlation analysis between the two scores using Pearson's R on a layer-wise basis across all models.\nImpact of Improving Human-Machine Alignment on Convexity. After addressing the correlation between the human-machine alignment and the convexity score, we ask how the optimization of one score will influence the other. To answer this question, we trained the naive transform described in subsection 2.2 to increase the OOOA of the latent representations after the first, middle, and last transformer layers. We then applied the transform to the latent representations of the remaining images and evaluated the graph convexity of these transformed latent representations. Furthermore, we examined the impact of fine-tuning on both the convexity and OOOA."}, {"title": "4 Results & Discussion", "content": ""}, {"title": "4.1 Correlation between Convexity and Human-Machine Alignment", "content": "First, we examine the progression of the convexity score and the OOOA across layers of different models and investigate the correlation between the two.\nThe graph convexity increases consistently for the first half of all evaluated models and then continues to increase for fine-tuned models but plateaus or decreases for pretrained models (Figure 2a). The convexity is generally higher in fine-tuned models, consistent with prior work [10]. The bell curve pattern observed in pretrained unsupervised models (BEiT and data2vec) may be attributed to their training objective, which employs a reconstruction loss. As the models learn to rebuild the masked input data, the later layers increasingly build latent representations akin to those in the earlier layers. Thus, intermediate layers may be the ones with the highest level of conceptual abstraction. A similar phenomenon has been observed previously in speech representation models [23, 24, 25].\nThe OOOA also follows a bell-shaped curve, peaking around the middle layers of the model and decreasing again toward the end (Figure 2b). Current human-machine alignment analyses focus mainly on the last or penultimate layer, as this bell curve was not observed in the supervised and contrastive models examined in previous work [7]. Our results suggest that the intermediate layers of the studied vision transformer models are better aligned with the human similarity space than the last layers. This is in line with observations on denoising diffusion models [17] and should motivate a careful approach for layer selection in future research on the alignment of neural networks. In contrast to our results on convexity, the bell curve can be observed for alignment in pretrained and fine-tuned models alike, albeit the latter generally achieve a higher maximum OOOA. This indicates that fine-tuning affects OOOA and convexity differently.\nThe correlation between the OOOA and convexity follows two main trends (see Figure 2c). In the first half of the models, both the convexity and the OOOA increase and are highly correlated, whereas the correlation decreases in the second half of the models. There is a considerable difference between pretrained and fine-tuned models in the later layers: for pretrained models, there is a mostly positive correlation that increases again in the last layers, while there is a strong negative correlation in late layers for fine-tuned models.\nIn general, we observe a high correlation between human-machine alignment (OOOA) and the convexity of the 27 high-level concepts in many scenarios. This suggests that the networks form convex regions in their latent spaces that align with human-defined categories, which is also consistent with the similarity relations humans use to solve the triplet task. Especially in the first half of the models, which we interpret as the concept built-up phase, the correlation is strong. For the later layers, which can be viewed as processing the built-up concepts, e.g. for the purpose of classification, the correlation decreases and even becomes negative. This potentially indicates that the models initially learn to distinguish concepts, where human-like categorization and similarity judgments are beneficial, especially in unsupervised settings. In the subsequent phase and during fine-tuning, the similarity between classes seems to become less critical. For reconstruction-based models, the reason may be that high-level information captured during the first phase may be decomposed into low-level features in later layers to solve the task (i.e. pretraining objective). For classification models, the separation of classes for classification might not necessitate retaining the similarity structure across concepts."}, {"title": "4.2 Impact of Improving Human-Machine Alignment on Convexity", "content": "We investigate how increasing human-machine alignment influences convexity. Using the naive transform [12], the OOOA of all models improves significantly \u2013 on average by 13.7% for pretrained models and by 13.0% for fine-tuned models. We find that improving the OOOA also leads to higher convexity of representations of the THINGS super-classes in pretrained models; the convexity of the transformed latent representations increases for all but one model by on average 3.1% (see Table 1). This relation is strongest in the last layers, but also holds in other layers of the models (see Table 4 and 5 in the appendix).\nFor fine-tuned models, this relation does not hold consistently. For most models, convexity decreases marginally (see Table 6 in the appendix), which is in line with the negative correlation scores in the late layers. We hypothesize that the training strategy (supervised vs. self-supervised) also has an impact on the effect of alignment optimization on convexity (preliminary results in Appendix A.3).\nIncreasing convexity does not necessarily lead to higher alignment, as observed in the fine-tuned models. Although classification performance and convexity increase during fine-tuning, this does not have a consistent effect on human-machine alignment. While fine-tuning increases OOOA for ViT, it decreases OOOA for data2vec and has only a marginal impact on OOOA for BEiT (see Table 2). This indicates a complex interplay between alignment and convexity, which potentially depends on factors such as training objectives as we also observed above.\nAnother promising direction for future research is to develop methods to increase convexity (and performance) that also consider human-machine alignment. Positive relations between human-machine alignment and performance/robustness have been previously found [26, 12], although alignment is not a sufficient condition for high performance and optimizing solely for alignment can worsen performance by un-learning properties of representations that are not necessary to achieve high OOOA."}, {"title": "5 Conclusion", "content": "We presented first evidence of a relationship between convexity in neural network representations and empirical human-machine alignment. Our findings indicate a significant correlation between these two measures in some scenarios and that the intervention to promote alignment can also increase convexity. The correlation suggests that neural networks form convex regions in their latent spaces that to some extent align with human-defined categories and reflect the similarity relations humans use in tasks such as the odd-one-out triplet task. In the studied models, we find the highest human-machine alignment in the middle layers, which could help inform future research.\nWe observed that optimizing for human-machine alignment through latent space transformations not only increases the odd-one-out accuracy (OOOA) but can also increase the convexity of the representations. This intervention effect underscores the potential for optimizing models to be both more aligned with human cognition and more effective in their performance.\nOur results highlight that in the higher layers, convexity and alignment can exhibit a strong positive or a strong negative correlation. This indicates a complex interplay between alignment and convexity that may hinge on factors such as training objective, architecture, or training data. Further research is warranted to explore under which conditions convexity and human-machine alignment align.\nOverall, our study offers first insights into the relationship between the convexity of latent representations and their alignment with human similarity judgments, further connecting cognitive science and deep learning research. Future work should investigate the causal relationships between alignment and convexity and explore new techniques to enhance both simultaneously, potentially leading to more aligned and generalizing models."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Implementation details", "content": "Code for the convexity analysis is available at https://github.com/LenkaTetkova/Convexity-of-representations.\nCode for the human-machine alignment analysis is available at https://github.com/LukasMut/gLocal and https://vicco-group.github.io/thingsvision/Alignment.html.\nAll investigated models were obtained from huggingface.co, the exact models are in Table 3.\nThe latent representations were extracted according to the way they were used by the classifier in the original implementation. Hence, we took the vector corresponding to the classifier token for ViT and averaged over all the other tokens for data2vec and BEIT."}, {"title": "A.2 Additional results", "content": "Figure 3 shows the correlation between convexity and human-machine alignment split by pretrained and fine-tuned models, as well as by model halves. We observe a high correlation for the first half of all models (R=0.91), while the correlation is lower for the second half of pretrained models (R=0.4) and even negative for fine-tuned models (R=-0.54).\nTable 4 and Table 5 show the change in OOOA and convexity after optimizing the latent space for alignment (OOOA) using the naive transform [12]. The improvement in convexity after the first and middle transformer block is not as high as after the last transformer block (see Table 1), but still significant, indicating a causal relation between alignment and convexity.\nTable 6-8 show the change in convexity after optimizing for OOOA. The positive relation we observed for pretrained models does not hold in this case. For most models, convexity slightly decreases in the middle and last layer while convexity increases in the first layer, which aligns with the negative correlation scores observed in Figure 3 for late layers in fine-tuned models and positive correlation in early layers."}, {"title": "A.3 Preliminary results on confounding factors", "content": "When comparing the correlations between convexity and alignment on a model basis, there is a notable difference between self-supervised models (BEiT, data2vec) and supervised models (ViT, all fine-tuned models). Self-supervised models show a very high correlation, which decreases drastically with fine-tuning (see Table 9), while the correlation is lower but stable for the supervised model (ViT). While this hints at a complex relationship, a large-scale study is needed to provide clear conclusions on the role of the objective function."}]}