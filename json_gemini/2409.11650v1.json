{"title": "Art and Science of Quantizing Large-Scale Models: A Comprehensive Overview", "authors": ["Yanshu Wang", "Tong Yang", "Xiyan Liang", "Guoan Wang", "Hanning Lu", "Xu Zhe", "Yaoming Li", "Li Weitao"], "abstract": "This paper provides a comprehensive overview of the principles, challenges, and methodologies associated with quantizing large-scale neural network models. As neural networks have evolved towards larger and more complex architectures to address increasingly sophisticated tasks, the computational and energy costs have escalated significantly. We explore the necessity and impact of model size growth, highlighting the performance benefits as well as the computational challenges and environmental considerations. The core focus is on model quantization as a fundamental approach to mitigate these challenges by reducing model size and improving efficiency without substantially compromising accuracy. We delve into various quantization techniques, including both post-training quantization (PTQ) and quantization-aware training (QAT), and analyze several state-of-the-art algorithms such as LLM-QAT, PEQA(L4Q), ZeroQuant, SmoothQuant, and others. Through comparative analysis, we examine how these methods address issues like outliers, importance weighting, and activation quantization, ultimately contributing to more sustainable and accessible deployment of large-scale models.", "sections": [{"title": "I. INTRODUCTION", "content": "A. Background and Motivation\n1) Machine Learning: Machine Learning (ML) is a subfield of artificial intelligence that enables computers to learn from and make decisions based on data patterns without being explicitly programmed. The core of machine learning is developing algorithms that allow computers to receive input data and use statistical analysis to predict or classify outputs, thereby optimizing the performance of a specific task with minimal human intervention. Machine learning can be broadly categorized into three types:\n\u2022 Supervised Learning: The model is trained on a pre-defined set of data examples. The goal is to learn a general rule that maps inputs to outputs.\n\u2022 Unsupervised Learning: The model looks for patterns and structures in data that is not labeled.\n\u2022 Reinforcement Learning: The model learns through trial and error to perform a task to maximize the reward.\nMachine Learning has many Applications:\nHealthcare: In healthcare, machine learning applications include disease diagnosis, drug discovery, and medical imaging analysis. For instance, models trained to recognize pathological images can help doctors diagnose diseases like cancer more quickly.\nFinancial Services:Machine learning in finance is used for credit scoring, fraud detection, and algorithmic trading. It analyzes customers' transaction behaviors to identify fraudulent activities.\nAutonomous Driving and Robotics\nAutonomous vehicles use machine learning to process complex data from sensors to recognize the environment, make decisions, and increase driving safety and efficiency.\nE-commerce and Recommendation Systems\nE-commerce platforms utilize machine learning to analyze user behavior, optimize search results, and recommend products, significantly enhancing user experience and sales efficiency.\nImage Recognition\nMachine learning now identifies and classifies objects in images, widely used in social media, security surveillance, and industrial visual inspection systems.\nNatural Language Processing\nFrom speech recognition to text analysis, machine learning's natural language processing technologies allow machines to better understand and generate human language, used in chatbots, translation services, and sentiment analysis.\n2) Neural Networks the Evolution Towards Larger Models: Neural networks, a pivotal concept in machine learning, are inspired by the biological neural networks that constitute animal brains. They are comprised of interconnected nodes or neurons, which collectively learn to perform complex tasks. Typically, these tasks include but are not limited to classification, regression, and pattern recognition, making neural networks versatile tools in both theoretical and applied machine learning.\nStructure of Neural Networks\nThe basic structure of a neural network involves three types of layers:\n\u2022 Input Layer: This layer receives the raw input data analogous to sensory input in biological systems.\n\u2022 Hidden Layers: One or more hidden layers compute functions applied to values from the previous layer. These layers form the core computational engine of the neural network.\n\u2022 Output Layer: The final layer produces output for the network, which corresponds to the predictions for supervised learning tasks.\nEach neuron in these layers applies a non-linear transformation to its input data and passes this output to the next layer. The strength and nature of the connections between neurons are adjusted through a process known as learning, typically implemented via backpropagation and gradient descent techniques.\nLearning Process in Neural Networks\nLearning in neural networks involves adjusting the weights of connections based on the error between the predicted output and the actual output. The most common learning algorithm used is backpropagation combined with an optimization technique such as gradient descent. This process involves:\n1) Propagating inputs forward through the network to generate the output.\n2) Calculating the error between predicted and actual outputs.\n3) Propagating the error backward through the network to update the weights, aiming to minimize the error by adjusting the weights.\nTypes of Neural Networks\nThere are several types of neural networks, each designed for specific types of problems and datasets. These include:\n\u2022 Convolutional Neural Networks (CNNs): Highly effective for processing data that has a grid-like topology, such as images.\n\u2022 Recurrent Neural Networks (RNNs): Designed to handle sequential data, such as time series or language.\n\u2022 Deep Belief Networks (DBNs): A type of deep network that uses a stack of restricted Boltzmann machines layered on top of each other.\nApplications of Neural Networks\nNeural networks have been successfully applied in numerous domains including:\n\u2022 Vision Systems: From facial recognition to autonomous driving.\n\u2022 Speech Recognition: Enabling voice-activated assistants and real-time translation services.\n\u2022 Natural Language Processing: Driving the development of conversational AI and other language understanding applications.\nOver the past few decades, there has been a significant shift in the architecture of neural networks, from relatively simple designs to highly complex and large models. This trend is driven by the continuous growth in computational power and the availability of vast amounts of data, which have enabled the training of larger models capable of performing a multitude of tasks with unprecedented accuracy.\nEvolution of Model Complexity: Initially, neural networks were limited in size and complexity due to the computational constraints of the time. Early networks often consisted of only a few layers and a limited number of neurons, which constrained their learning capacity and applicability to complex tasks. However, as computational resources expanded, so too did the size and depth of these models. For instance, models like AlexNet and VGG in the early 2010s marked the beginning of what would be a rapid expansion in network depth and complexity, featuring layers deep into the double digits.\nAdvancements in Hardware and Algorithms: The advent of GPUs and improvements in distributed computing have significantly reduced the time required to train large neural networks. Simultaneously, advancements in optimization algorithms, such as Adam and RMSprop, have improved the efficiency of training deep networks. These technical advancements have facilitated the development of models such as the Transformer, which underpins modern NLP systems like GPT and BERT. These models not only have hundreds of layers but also millions to billions of parameters.\nImplications of Larger Models: The shift towards larger models has resulted in substantial improvements in tasks such as image recognition, natural language processing, and complex decision-making processes. For example, larger models have led to breakthroughs in machine translation and autonomous vehicle technology. However, the trend towards larger networks also presents new challenges, including increased computational cost, energy consumption, and the need for more sophisticated techniques to combat overfitting.\nFuture Prospects: As the trend towards larger models continues, the field of machine learning is likely to witness even more sophisticated architectures. This progression suggests a future where neural networks could approach and even surpass human-like capabilities in certain tasks. However, this potential also necessitates innovations in model efficiency, training techniques, and hardware design to make the training and deployment of such models sustainable.\n3) The Necessity and Impact of Model Size Growth: The Necessity and Impact of Model Size Growth: The necessity for growth in neural network model size stems primarily from the increasing complexity of tasks that modern AI systems are expected to perform. As the ambition to develop systems that can mimic human-level understanding and decision-making grows, so too does the need for models with greater capacity. Larger models, with their enhanced ability to model complex patterns and relationships, are pivotal in achieving higher levels of accuracy in tasks ranging from natural language understanding to complex image recognition.\nImpacts on Performance and Efficiency: Larger neural network models have consistently set new benchmarks in AI performance. For instance, in natural language processing (NLP), models like OpenAI's GPT-3 have demonstrated remarkable linguistic understanding and generation capabilities, directly correlating their performance improvements to their vast number of parameters. Similarly, in image processing, larger Convolutional Neural Networks (CNNs) have achieved unprecedented accuracies in image classification challenges.\nComputational Challenges and Solutions: However, the growth in model size is not without its challenges. The primary concern is the exponential increase in computational resources"}, {"title": "II. OBJECTIVES, IMPORTANCE, AND FUNDAMENTAL\nMETHODS OF MODEL QUANTIZATION", "content": "A. Fundamental Approaches\n1) LLM-QAT: LLM-QAT is an advanced method for Quantization-Aware Training (QAT) specifically designed for LLMs. Traditional post-training quantization methods have shown effectiveness up to 8-bits but struggle at lower bit precision levels. LLM-QAT leverages a data-free distillation technique that generates training data using the pre-trained model itself. This method helps in preserving the original output distribution and allows the quantization of weights, activations, and the key-value (KV) cache. The process aims to improve the efficiency and performance of LLMs even at quantization levels as low as 4-bits.\nIn detail, based on the observation, Symmetric MinMax quantization is first used to retain LLMs' outliers and maintain the performance of the model, which is defined as:\n$X_Q = \\alpha \\frac{X_R}{max(|X_R|)}$\n$\\alpha = \\frac{max(|X_R|)}{2^{N-1}-1}$,\nwhere $X_Q$ represents the quantized values, $X_R$ represents the real (full-precision) values, and $\\alpha$ is the scaling factor. For weights, per-channel quantization is used, and for activations and the KV cache, per-token quantization is applied.\nSecond, LLM-QAT uses a student-teacher model framework to ensure that the quantized model retains the performance of the full-precision model. Specifically, the student network, which is the lower-precision version of the model, is guided by the full-precision teacher network, the original pre-trained model. This guidance is provided through cross-entropy-based logits distillation:\n$L_{CE} = - \\sum_c \\sum_{i=1}^n T(X_i) \\log(p_S(X_i))$\nHere, i represents the i-th sample in the batch, c denotes the number of classes (vocabulary size), and T and S are the teacher and student networks, respectively.\nThird, next token data generation from the pre-trained model is proposed for synthesizing the distribution of pre-training data. Data is generated by initiating with a random start token and generating subsequent tokens iteratively until the end of the sentence or maximum length is reached. And to ensure the generated data is diverse and accurate, LLM-QAT introduces a a hybrid approach where the first few tokens are deterministically selected with top-1 strategy and the remaining tokens are stochastically sampled from the pre-trained model's SoftMax output distribution.\nLastly, The generated data is then used as input for fine-tuning the quantized model, where the teacher model's predictions serve as labels to guide the training.\nExperimental results show that LLM-QAT significantly outperforms traditional PTQ methods at lower bit precisions. For example, in the 8-8-4 setting, the 30B LLM-QAT model achieves an average zero-shot accuracy of 69.7, compared to 50.7 with SmoothQuant, demonstrating its robustness in maintaining accuracy. In the 4-8-4 setting, where both weights and the KV cache are quantized to 4 bits, LLM-QAT achieves 69.9, only 1.5 points behind the full-precision model, while all PTQ methods perform poorly, highlighting LLM-QAT's superior quantization capabilities.\nAdditionally, in the 4-8-8 setting, LLM-QAT outperforms the best PTQ method (RTN) by 1.4 points. These results are consistent across different model sizes, with an 8-8-8 30B quantized model surpassing a 13B full-precision model in performance, and a 4-8-4 LLM-QAT 30B model outperforming an 8-bit LLaMA-13B. These findings underscore LLM-QAT's ability to maintain high performance with reduced computational costs and memory usage, offering a better efficiency-accuracy tradeoff.\n2) PEQA: To address the increasing memory and computational costs in large-scale NLP models, researchers Hyesung Jeon, Yulhwa Kim, and Jae-Joon Kim from Seoul National University and Sungkyunkwan University proposed the Low-rank adaptive Learning quantization algorithm geared towards LLMs (L4Q) [9]. L4Q combines quantization and parameter-efficient fine-tuning (PEFT) to overcome the limitations of traditional methods. While Post-Training Quantization (PTQ) is efficient but error-prone, and Quantization-Aware Training (QAT) is accurate but resource-intensive, L4Q integrates QAT with PEFT to achieve both high precision and low memory usage, making it ideal for resource-constrained environments. L4Q achieves the integration of quantization and fine-tuning through the following steps:\n1) Merging Weights and LoRA Parameters: The pre-trained weights $W_{FP}$ and LoRA parameters A and B are merged into a new weight matrix:\n$W' = W_{FP} + BA$"}, {"title": "3) QLORA:", "content": "4) LUT-GEMM: To address the increasing memory and computational costs in large-scale NLP models, researchers Gunho Park and Baeseong Park from Pohang University of Science and Technology and NAVER Cloud proposed the Lookup Table-based GEMM (LUT-GEMM) algorithm [10]. LUT-GEMM enhances inference efficiency by quantizing weights while maintaining full precision for activations, eliminating the dequantization step.\nLUT-GEMM achieves quantized matrix multiplication through the following steps:\n1) Constructing Lookup Tables: For a binary matrix B\u2208 {-1,+1}m\u00d7n and an activation vector x \u2208 Rn, all possible combinations of activation values and binary patterns are precomputed and stored in a lookup table (LUT). This can be expressed as:\n$w \\approx \\sum_{i=1}^q a_i b_i$\nwhere $a_i \\in R+$ are scaling factors and $b_i \\in \\{-1,+1\\}^n$ are binary vectors.\n2) Lookup Table Retrieval: Using the LUT, precomputed partial dot products are retrieved by indexing directly, avoiding redundant computations. For matrix multiplication $Bx^T$, the LUT retrieval operation replaces the original calculations:\n$Bx^T = LUT[B, x]$\n3) Reducing Computational Complexity: By utilizing the LUT, LUT-GEMM reduces the complexity of computations. Assuming a q-bit quantized binary matrix Bi multi-plied by an input vector x, the computational complexity is:\n$C = 0(\\frac{n}{q} \\binom{m}{q} \\mu)$\nwhere \u03bc is the length of the sub-vector.\n4) GPU Parallel Implementation: For implementation on GPUs, LUT-GEMM improves parallelism by assigning as many threads as possible to perform independent LUT accesses, with scaling factor operations not degrading thread performance. Each thread block (TB) shares LUTs, utilizing the high bandwidth of GPUs for fast matrix computations.\nExperimental results show that when applied to the OPT-175B model with 3-bit quantization, LUT-GEMM substantially accelerates token generation latency, achieving a remarkable 2.1x improvement on a single GPU compared to OPTQ, which relies on the costly dequantization process.The experimental results clearly demonstrate that LUT-GEMM achieves the lowest latency by directly using quantized weights and reducing computational complexity, significantly saving energy consumption.\n5) ZeroQuant: With the increasing size of Transformer models like BERT and GPT, the computational and memory costs during inference have become a significant challenge, even for powerful GPUs. To address this, a research team at Microsoft proposed the ZeroQuant algorithm at NeurIPS 2022 [8]. ZeroQuant aims to efficiently compress large-scale Transformer models through post-training quantization (PTQ), eliminating the need for retraining.\nThe core of the ZeroQuant algorithm lies in its proposal of a fine-grained, hardware-friendly quantization strategy, combined with Layer-wise Knowledge Distillation (LKD), allowing the maintenance of high model accuracy even under extreme low-bit-width quantization (e.g., INT4).\nFine-Grained Quantization Strategy\nThe quantization strategy proposed by ZeroQuant consists of two parts: Group-wise Quantization and Token-wise Quantization.\nGroup-wise Quantization: Traditional quantization methods typically apply uniform quantization to the entire weight matrix, which can lead to significant accuracy loss when applied to large-scale models. ZeroQuant mitigates this by dividing the weight matrix into multiple groups and quantizing each group separately, thereby reducing quantization errors and improving hardware efficiency.\nThe quantization is expressed by the following formula:\n$X_{quantize} = round(clamp(\\frac{x}{S}, -2^{bit-1}, 2^{bit-1}-1))$,\nwhere x is the vector or matrix to be quantized, S is the scaling factor (usually the maximum absolute value), and bit is the quantization bit width.\nToken-wise Quantization: For the quantization of activations, ZeroQuant adopts a more refined strategy. Due to the significant variance in activation ranges across different tokens in Transformer models, a uniform quantization range often results in substantial performance degradation. Therefore, ZeroQuant proposes a token-wise quantization scheme, which dynamically calculates the quantization range for each token, minimizing quantization errors.\nLayer-wise Knowledge Distillation (LKD) The ZeroQuant algorithm introduces Layer-wise Knowledge Distillation (LKD) to address the accuracy issues associated with extreme low-bit-width quantization (e.g., INT4). Unlike traditional knowledge distillation methods, LKD does not require keeping full copies of both the teacher and student models. Instead, it quantizes and distills the model layer by layer, significantly reducing GPU memory requirements and enabling efficient quantization even without access to the original training data.\nThe distillation loss function for LKD is as follows:"}, {"title": "6) Smooth Quant:", "content": "In response to the growing memory and computational costs of large-scale NLP models, researchers Guangxuan Xiao, Ji Lin from the Massachusetts Institute of Technology (MIT), and Mickael Seznec, Hao Wu, Julien Demouth from NVIDIA proposed a post-training quantization (PTQ) method for large language models (LLMs) called SmoothQuant [2]. This method specifically addresses the issues of maintaining accuracy and hardware efficiency during quantization by implementing an INT8 quantization for both activations and weights through a training-free transformation, optimizing model execution efficiency and memory usage on hardware.\nSmoothQuant centers around several key components:\n1) Quantization Transformation Formula: SmoothQuant reduces quantization difficulty by smoothing input activations X and adjusting weights W. The core transformation formula is:\n$Y = (Xdiag(s)^{-1}) \u00b7 (diag(s)W) = XW$\nHere, s represents the smoothing factor for each channel, making the adjusted activations X and weights W easier to quantize.\n2) Selection of Smoothing Factors: The choice of smoothing factor s; is aimed at maximizing quantization effectiveness and accuracy, calculated by:\n$S_j = \\frac{max(X)}{max(|W_j|)^{1-a}}$\n\u03b1 is a hyperparameter between 0 and 1 that balances the quantization difficulty between activations and weights.\n3) Application to Transformer Blocks: Within Transformer models, SmoothQuant specifically applies scaling smoothing to the input activations of self-attention and feed-forward layers, and uses INT8 quantization for all linear layers involving both weights and activations.\nIn practical applications, SmoothQuant demonstrated significant performance enhancements across various configurations and large models. For instance, in tests using the OPT-175B model, SmoothQuant achieved a 1.51x speed improvement and 1.96x memory savings almost without loss of accuracy. This method not only maintains the model's precision but also significantly enhances hardware efficiency, especially valuable in resource-constrained environments.\n7) SpQR:\n8) Olive: To address the increasing memory and computational costs in large-scale NLP models, researchers Cong Guo, Jiaming Tang, Weiming Hu, and others from Shanghai Jiao Tong University and Microsoft Research proposed the Outlier-Victim Pair Quantization algorithm (Olive) [4] for large language models. OliVe employs a hardware-friendly method to handle outliers, significantly enhancing performance and energy efficiency while maintaining model accuracy in resource-constrained environments.\nThe Olive algorithm achieves effective handling of outliers through the following steps:\n1) Pair-wise Analysis: OliVe first analyzes the tensor values in the model, classifying them into three types of pairs: normal-normal, outlier-normal, and outlier-outlier. The core of the algorithm is that for outlier-normal pairs, the normal values are set to zero (referred to as \"victims\u201d), freeing up space to handle the outliers.\n2) Outlier Quantization: OliVe uses an adaptive biased float (abfloat) data type to quantize outliers. This method adds a suitable bias to adjust the range of floating-point values, ensuring they do not overlap with the range of normal values, thus maximizing the utilization of the numerical representation space. Specifically, the abfloat values are quantized using the formula:\n$quant(e) = sign\u00d7(1 << mantissa+mantissa) << (exponent+bias)$\nwhere mantissa denotes the width of the mantissa bits, exponent denotes the width of the exponent bits, and bias is the adaptive bias.\n3) Hardware-Friendly Memory Alignment: A key feature of OliVe is memory alignment. By positioning the \"victims\u201d adjacent to the outliers, OliVe achieves efficient memory access with low hardware overhead. This design avoids the complexity of sparse indexing hardware and is compatible with the memory subsystems of existing accelerators such as GPUs and TPUs.\nIn terms of experimental results, Olive demonstrated significant improvements across various tasks and datasets. For instance, in the GLUE benchmark with the BERT model, the 4-bit Post-Training Quantization (PTQ) method of Olive resulted in less than a 1% drop in accuracy compared to the full-precision model, outperforming other 4-bit, 6-bit, and 8-bit PTQ and Quantization-Aware Training (QAT) methods. Additionally, when applied to large-scale language models like GPT2-XL, BLOOM-7B1, and OPT-6.7B, Olive's 8-bit PTQ method nearly preserved the original model performance, highlighting its superior inference performance and energy efficiency.\n9) GPTQ:\n10) AWQ: In the field of quantizing large language models (LLMs), traditional methods face significant challenges such"}, {"title": "B. The Significance of Quantization in Modern Deep Learning", "content": "III. TRAINING WITH QUANTIZATION: ALGORITHMS AND APPROACHES\nA. Quantized Neural Networks (QNNs)\nQuantized Neural Networks(QNNs) are variants of neural network that use quantized weights and/or activations instead of traditional full-precision (32-bit floating point) numbers. The motivation behind quantization els with more and more weights, which are not suitable for deployment on the resource-constrained devices such as mobile phones, embedded system and IoT devices. And the goal of quantization is to reduce the computational cost and memory requirements of neural networks without performance degradation.\nB. Training neural networks With Quantization\nWhen quantizing neural networks, there are three components could be quantized: weights, activations and gradients. The quantization of weights and activations could effectively reduce the size of models, and the quantization of gradients could reduce the resource and time cost when training QNNs.\nBefore the advent of Large Language Models(LLMs), the QNNs had already attracted a lot of attention. Many researchers dived into the quantization of weights or activations, and proposed a lot of techniques to reduce the cost of deployments and accelerate the inference. However, due to the need of the fidelity of gradient computations during backward propagation, the training of QNNs is unlike the inference, which could work well on low-precision. There is a high probability that the models will not converge if training QNNs with low-precision gradients. Therefore, the most quantization techniques try to quantize weights and/or activations of well-trained neural networks rather than train a QNNs from scratch.\nBinaryConnect(BC) [20] proposes a method in training DNNs with binary weights during propagations. This work aims to use 1-bit to present weights w, and gives two quantization method Qw to convert w to 1-bit. One of that is deterministic method which is based on the sign function:\n$W_q = Q_w(w) = \\begin{cases} +1, & \\text{if } w \\geq 0, \\\\ -1, & \\text{otherwise} \\end{cases}$\nThis method make the weights greater than 0 to become 1 and the other become -1. And the other is stochastic method:\n$W_q = Q_w(w) = \\begin{cases} +1, & \\text{with probability } p = \\delta(w), \\\\ -1, & \\text{with probability } 1 \u2013 p \\end{cases}$\nwhere d is the hardsigmoid function:\n$\\delta(x) = clip(\\frac{x+1}{2}, 0, 1)$\nwhich convert the weights greater than 1 to 1, the weights less than -1 to -1 and the weights between -1 and 1 to either -1 or 1 with a probability.\nDuring backward propagation, the BC also adopt the gradient decent algorithm like full-precision neural networks. However the common quantization function is not differentiable, or the derivative value is 0 almost everywhere(e.g.,"}, {"title": "IV. INFERENCE WITH QUANTIZATION: ALGORITHMS AND\nAPPROACHES", "content": "A. Knowledge Distillation (KD)\nKnowledge Distillation (KD) is a critical technique for model compression, enabling the transfer of knowledge from a large, well-trained teacher model to a smaller, more efficient student model. This process is particularly valuable in Quantization-Aware Training (QAT), where models are trained to maintain high performance despite being quantized to lower precision to reduce memory and computational costs. The primary motivation behind using KD in QAT is to enhance the performance of quantized models. KD helps mitigate the accuracy loss typically associated with model quantization by transferring the knowledge encapsulated in the teacher's full-precision model to the student's lower-precision model. This process is particularly beneficial for resource-constrained environments where computational efficiency is paramount.\nA. Key Techniques in KD for QAT\n1) Layer-wise Distillation:\n\u2022 Method: Applying KD at each layer of the model, aligning the student's intermediate representations with those of the teacher. This approach helps in capturing the hierarchical features learned by the teacher model.\n\u2022 Benefits: This technique has been shown to significantly improve the performance of quantized models by maintaining the structural and functional integrity of the model across all layers.\n2) Attention Mechanism Distillation:\n\u2022 Method: Transferring attention maps from the teacher to the student model. This ensures that the student model focuses on similar regions as the teacher, preserving the interpretability and effectiveness of the attention mechanism.\n\u2022 Advantages: Enhances the student model's performance by ensuring that the critical attention mechanisms learned by the teacher are retained in the quantized student model.\n3) Logit-based Distillation:\n\u2022 Method: Aligning the logits (pre-softmax outputs) of the student model with those of the teacher. This ensures that the probability distributions produced by the student are similar to those of the teacher, aiding in better generalization.\n\u2022 Benefits: This method is straightforward and effective, providing a strong baseline for performance improvement in quantized models.\nB. Advanced KD Techniques for QAT\n1) Quantization-aware Knowledge Distillation (QKD):\n\u2022 Phases:QKD typically involves three phases self-studying, co-studying, and tutoring. These phases help in progressively transferring knowledge and adapting the student model to quantized constraints.\n\u2022 Technical Details: QKD uses a trainable uniform quantization scheme for weights and activations, along with gradient approximation techniques such as the straight-through estimator (STE) to handle the non-differentiable nature of quantizers.\n2) Self-Supervised Quantization-Aware Knowledge Distillation (SQAKD):\n\u2022 Methodology:SQAKD integrates QAT and KD by framing the problem as a constrained optimization task. It utilizes self-supervised learning techniques to minimize both discretization error and prediction discrepancy, ensuring precise quantization without sacrificing accuracy.\n\u2022 Performance: SQAKD has demonstrated significant performance improvements across various architectures and datasets, showing its robustness and effectiveness in enhancing the accuracy of low-bit quantized models.\nThe effectiveness of KD methods in QAT has been evaluated extensively on various benchmark datasets, including CIFAR-10, CIFAR-100, and ImageNet. These evaluations show that KD significantly enhances the performance of student models, even with aggressive quantization (e.g., sub-2-bit precision). Metrics such as cover length ratio and ranking loss have been introduced to quantitatively assess the effectiveness of KD losses, further validating the improvements achieved through these techniques.\nB. Key-Value cache(KV cache) compression\nIn large-scale neural network computation, the concept of Key-Value Cache (KV Cache) is mainly related to the self-attention mechanism commonly used in the Transformer architecture. The purpose of KV Cache is to reduce the computational complexity, improve the efficiency, and conserve the computational resources, and it is widely used in large-scale models.\nIn Transformer, the operation of the self-attention mechanism involves the computation of query (Q), key (K) and value (V) matrices [27]. These matrices are used to compute the attention distribution to balance the input information at different locations. During inference, the Q matrix is usually derived from the model input, which makes the Q matrix different for each input instance. In contrast, the K matrix and the V matrix are computed from the output of the encoder and are relatively stable.\nThe key idea behind the KV cache is that due to the relative stability of the K and V matrices, they can be cached at different time steps. This means that for the same input, there is no need to recalculate the K and V matrices and therefore they can be reused. The KV cache can store the result of multiplying each token with the WK and WV parameter matrices. In the converter architecture, each token is generated based on the result of the previous token. the KV cache caches the K and V matrices, thereby reducing computation time. Without the KV cache, recomputing the product of WK and WV for all tokens each time a new token is generated would be computationally intensive. Therefore, caching these results"}, {"title": "he quantization of large-scale neural network models\nemerges as a critical strategy in addressing the computational\nand energy demands associated with the growth of model\nsizes. Our comprehensive overview highlights the advance-\nments in quantization techniques that enable significant re-\nductions in model size and computational overhead while\nmaintaining high levels of accuracy. By analyzing various\nalgorithms and approaches, we observe that innovative meth-\nods such as LLM-QAT and SmoothQuant effectively balance", "content": "performance and efficiency, making the deployment of large-scale models more feasible in resource-constrained environments. The integration of quantization-aware training and sophisticated post-training quantization methods demonstrates the potential for neural networks to continue scaling in complexity without proportional increases in computational costs. Future work in this domain is essential to further optimize quantization strategies, ensuring that the benefits of large-scale models can be widely accessible and environmentally sustainable."}]}