{"title": "CONNECTING FEDERATED ADMM TO BAYES", "authors": ["Siddharth Swaroop", "Mohammad Emtiyaz Khan", "Finale Doshi-Velez"], "abstract": "We provide new connections between two distinct federated learning approaches based on (i) ADMM and (ii) Variational Bayes (VB), and propose new variants by combining their complementary strengths. Specifically, we show that the dual variables in ADMM naturally emerge through the \"site\" parameters used in VB with isotropic Gaussian covariances. Using this, we derive two versions of ADMM from VB that use flexible covariances and functional regularisation, respectively. Through numerical experiments, we validate the improvements obtained in performance. The work shows connection between two fields that are believed to be fundamentally different and combines them to improve federated learning.", "sections": [{"title": "1 INTRODUCTION", "content": "The goal of federated learning is to train a global model in the central server by using the data distributed over many local clients (McMahan et al., 2016). Such distributed learning improves privacy, security, and robustness, but is challenging due to frequent communication needed to synchronise training among nodes. This is especially true when the data quality differs drastically from client to client and needs to be appropriately weighted. Designing new methods to deal with such challenges is an active area of research in federated learning.\nWe focus on two distinct federated-learning approaches based on the Alternative Direction Methods of Multiplier (ADMM) and Variational Bayes (VB), respectively. The ADMM approach synchronises the global and local models by using constrained optimisation and updates both primal and dual variables simultaneously. This includes methods like FedPD (Zhang et al., 2021), FedADMM (Gong et al., 2022; Wang et al., 2022; Zhou and Li, 2023) and FedDyn (Acar et al., 2021). The VB approach, on the other hand, uses local posterior distributions as messages and multiplies them to compute an accurate estimate of the global posterior (Ashman et al., 2022). This follows the more general Bayesian framework which has a long history in distributed and decentralised computations (Mutambara, 1998; Tresp, 2000; Durrant-Whyte, 2001). Despite decades of work, the two fields remain disconnected and are thought to be fundamentally different: there is no notion of duality in VB, and posterior distributions are entirely absent in ADMM. Little has been done to connect the two at a fundamental level and our goal here is to address this gap.\nIn this paper, we provide new connections between ADMM and VB-based approaches for federated learning. Our main result shows that the dual-variables used in ADMM naturally emerge through the \"site\" parameters in VB. We show this for a specific case where isotropic-covariance Gaussian approximations are used. For this case, we get a close line-by-line correspondence between the ADMM and VB updates. The expectations of the local likelihood approximations, also known as the sites, yield the dual terms used in the local updates of ADMM.\nWe use the connection to derive new federated learning algorithms. First, in Sec. 3.3 we extend the isotropic case to learn full-covariances through the federated VB approach. This leads to an ADMM-like approach with an additional dual variable to learn covariances, which are then used as preconditioners for the global update. Second, in Sec. 3.4, building upon functional regularisation approaches from Bayesian literature, we propose to use similar extensions to handle unlabelled data, providing a relationship to federated distillation methods (Seo et al., 2020; Li and Wang, 2019; Lin"}, {"title": "2 FEDERATED LEARNING WITH ADMM AND BAYES", "content": "The goal of federated learning is to train a global model (parameters denoted by wg) by using the data distributed across K clients. Ideally, we want to recover the solution that minimises the loss over all the aggregated data, for instance, we may aim for\n$w^* = \\arg \\min_w \\sum_{k=1}^K l_k(w) + \\delta R(w),                                               (1)$\nwhere lk is the loss defined at the k'th client using its own local data Dk and R is a regulariser with \u03b4 > 0 as a scalar (set to 0 when no explicit regularisation is used). However, because the data is distributed, the server needs to query the clients to get access to the losses lk. Then the goal is to estimate w while also minimising the communication cost. Typically, instead of communicating lk or its gradients, local clients train their own models (denoted by wk for the k'th client) and communicate them (or changes to them) to the server. The server then updates wg and sends it back to the clients. This process is iterated with the aim of eventually recovering $w^*$.\nThe Alternating Direction Methods of Multiplier (ADMM) is a popular framework for distributed and federated learning, where the local and global optimisation are performed with constraints that they must converge to the same model. That is, we use constraints wk = wg enforced through the dual variables, denoted by vk. The triple (wk, vk, wg) are updated as follows,\nClient updates: $w_k \\leftarrow \\arg\\min_w l_k(w) + v_k^\\intercal w + \\frac{\\alpha}{2} ||w - w_g||^2$,\n$v_k \\leftarrow v_k + \\alpha(w_k - w_g),                                   \\text{ for all } k$,\nServer update: $w_g\\leftarrow \\frac{1}{K} \\sum_{k=1}^K w_k + \\frac{1}{\\alpha} v_k$,                                                           (2)\nHere, \u03b1 > 0 is a scalar and we denote lk = lk/Nk where Nk is the size of the data at the k'th client. Upon convergence, we get $w_k = w_g$ and $v_k = \\nabla l_k(w_g)$ for all k.\nSuch updates are instances of primal-dual algorithms and many variants have been proposed for federated learning (Zhang et al., 2021; Gong et al., 2022; Wang et al., 2022; Zhou and Li, 2023). For example, FedDyn (Acar et al., 2021) is perhaps the best-performing variant which uses a client-specific \u03b1\u03ba \u221d \u03b1/Nk. It also adds an additional hyperparameter \u03b4 (as shown in Eq. 1) through a local weight-decay regulariser added to each local client's loss. There are also simpler versions (not based on ADMM), for instance, FedProx (Li et al., 2020) where vk is not used (or equivalently fixed to 0 in Eq. 2), and even simpler Federated Averaging or FedAvg (McMahan et al., 2016), where the proximal constraint $||w - w_g||$ is also removed. Such methods do not ensure that $w_k = w_g$ at convergence. The use of dual-variable is a unique feature of ADMM to synchronise the server and clients.\nThe Bayesian framework for federated learning takes a very different approach to ADMM. Instead of a point estimate w, the goal for Bayes is to target a global posterior pg(w) = p(w|D1:K) where D1:K = (D1, D2, ..., DK). Instead of loss functions for each Dk, we use likelihoods denoted by p(Dk|w). The solution w in Eq. 1 can be seen as the mode of the posterior pg whenever there exists likelihood such that log p(Dk|w) = \u2212lk(w) and a prior log po(w) = \u2212\u03b4R(w), where both equations need to be true only up to a constant. Therefore, targeting the posterior pg(w) also recovers $w^*$. Often, we compute local posteriors pk(w) = p(w|Dk) at clients and combine them at the servers, for instance, Bayesian committee machines (Tresp, 2000) use the following update,\n$p_g \\propto p_0 \\prod_{k=1}^K t_k$.                                                          (3)"}, {"title": "3 CONNECTING VARIATIONAL BAYES TO ADMM", "content": "We precisely connect Variational Bayes, specifically Partitioned Variational Inference (PVI) (Ashman et al., 2022), to ADMM-style methods for federated learning. We start with PVI, noting it has similar components to ADMM. We then make approximations to derive a method (FedLap) that has close line-by-line correspondence to ADMM. We use this connection to derive new variants of ADMM to improve it, by (i) using full covariance information, and (ii) including function-space information."}, {"title": "3.1 CONNECTING PARTITIONED VARIATIONAL INFERENCE (PVI) TO ADMM", "content": "Partitioned Variational Inference (PVI) (Ashman et al., 2022; Bui et al., 2018) aims to find the best approximation $q_g \\in Q$ where Q is a set of candidate distributions (for example, a set of Gaussian distributions). The goal is to get the VB solution in Eq. 4 but by using an iterative message passing algorithm where local approximations qk(w) send the sites tk(w) as messages to improve qg(w). The method update the following triple (qk, tk, qg) as shown below,\nClient updates: $q_k \\leftarrow \\arg \\max_{q_k \\in Q} E_{q_k} \\bigg[ \\log \\frac{p(D_k | w)}{t_k(w)} \\bigg] - D_{KL}[q_k || q_g]$,\n$\\log t_k \\leftarrow \\log t_k + \\rho E_{q_k} \\bigg[ \\log \\frac{q_k}{q_g} \\bigg]$,\nServer update: $q_g \\propto p_0 \\prod_{k=1}^K t_k$.                                                                   (5)\nThe update of qg(w) is exactly the same as Eq. 3 but uses tk(w) which in turn is obtained by using the ratio qk(w)/qg(w) in the second line. The tk(w) essentially uses the discrepancy between the global and local distributions. This is then used in the first line to modify the local qk(w). Similarly to ADMM, at convergence we have $q_k(w) = q_g(w)$. The site parameters are related to natural gradients (Khan and Lin, 2017; Khan et al., 2018), therefore we can expect them to be related to dual variables $v_k$ in ADMM which estimate gradients. In what follows, we will derive this precisely."}, {"title": "3.2 FEDLAP: A LAPLACE VERSION OF PVI", "content": "We now choose a specific form of the distributions in PVI to make it even closer to the ADMM update, deriving a new method which we call FedLap. In Secs. 3.3 and 3.4 we will use this connection to derive new, improved, variants of ADMM. Here, we set the family Q to be the set of isotropic Gaussian distributions $N(w; m, I/\\delta)$ where the mean m needs to be estimated while the covariance is fixed to I/\u03b4 where \u03b4 > 0 is a scalar. We also set the prior po(w) to a zero mean Gaussian with the same precision as qg(w). These choices are shown below,\n$q_g(w) \\propto N(w; w_g, I/\\delta), \\quad  q_k(w) \\propto N(w; w_k, I/\\delta), \\quad  p_0(w) \\propto N(w; 0, I/\\delta).                  (7)$\nThese choices imply that tk(w) takes a Gaussian form where only the linear term needs to be estimated, that is, we need to find vk such that,\n$t_k(w) \\propto e^{\\delta v_k^\\intercal w}$.                                                          (8)\nThis form is ensured due to the form of the optimal solution in Eq. 4 (and can be shown more formally by using natural gradients). Roughly speaking, this is because both qg(w) and po(w) have the exact same Gaussian form, therefore \u03b4 also appears in the expression of tk(w) as well.\nPlugging these in Eq. 6 and making a delta approximation (Khan and Rue, 2021, App. C.1), we get FedLap, a Laplace variant of the PVI (we first provide the update, and then its derivation),\nClient updates: $w_k \\leftarrow \\arg \\min_w l_k(w) + \\delta v_k^\\intercal w + \\frac{\\delta}{2}||w - w_g||^2$\n$v_k \\leftarrow v_k + \\rho(w_k - w_g)$\nServer update: $w_g \\leftarrow \\frac{1}{K} \\sum_{k=1}^K w_k$,                                                              (9)\nThe first line is obtained by making the delta approximation, that is, by replacing $E_q[g(w)] \\approx g(m)$,\n$E_q[l_k(w)] + E_q[\\log t_k(w)] + E_q\\bigg[ \\log \\frac{q_k(w)}{q_g(w)} \\bigg] \\approx l_k(m) + \\delta v_k^\\intercal m + \\frac{1}{2}\\delta ||m - w_g||^2 + const$.\nThen, rewriting m as w, we get the first line. The second line follows due to the Gaussian form,\n$\\delta v_k^\\intercal w - \\delta v_k^\\intercal w + \\rho \\bigg( \\delta w_k^\\intercal w - \\delta w_g^\\intercal w \\bigg) \\Longrightarrow v_k \\leftarrow v_k + \\rho (w_k - w_g)$.\nThe last line follows in the same fashion where we update the mean wg of qg(w).\nThe derivation shows clearly that the $t_k(w)$ terms used in the KL minimisation of PVI gives rise to the term $v_k^\\intercal w$ in the ADMM update in Eq. 2. Similarly to ADMM and PVI, at convergence we have $w_k = w_g$. We can make the FedLap update look even more similar to the ADMM update by a change of variable, absorbing \u03b4 into vk (see Eq. 18 in App. A). We also note three subtle differences."}, {"title": "3.3 FEDLAP-COV: A NEW ADMM VARIANT FROM VB WITH FLEXIBLE COVARIANCES", "content": "Our connection gives a direct way to design a new ADMM variant to improve it: use different candidate set Q, that is, choose different posterior forms for the candidates q(w). Here, we demonstrate this for multivariate Gaussian $q(w) = N(w; m, \\Sigma)$ where, unlike the previous section, we aim to also estimate the covariance matrix \u03a3. This extension leads to two dual variables, where the second dual variable acts as a preconditioner similar to the Newton update. We will often write the precision matrix S = \u03a3\u00af\u00b9 because it is directly connected to the Hessian (denoted by H) which are more natural for a Newton-like update. In practice, we use a diagonal matrix because it scales better to large models, but our derivation is more general.\nWe make the following choices to derive the new variant which we will call FedLap-Cov,\n$q_g(w) \\propto N(w; w_g, S_g^{-1}), \\quad q_k(w) \\propto N(w; w_k, S_k^{-1}), \\quad p_0(w) \\propto N(w; 0, I/\\delta).                        (10)$\nSimilar to the isotropic Gaussian case, these choices imply that tk(w) also takes a Gaussian form where we need to find a pair of dual variables, a vector vk and a symmetric square matrix Vk,\n$t_k(w) \\propto e^{v_k^\\intercal w - \\frac{1}{2} w^\\intercal V_k w}$.                                                                             (11)\nAgain, the existence of (v, V*) is ensured due to Eq. 4 but we skip the details here.\nNow, we can simply plug-in the form (qk, tk, qg) in Eq. 6 to get the update. The main differences are, first, the inclusion of Vk in the client's dual term (highlighted with red) and, second, the preconditioning with Sg used in the server updates,\nClient updates: $(w_k, S_k) \\leftarrow \\arg \\min_q E_q[l_k(w)] + E_q \\bigg[ v_k^\\intercal w - \\frac{1}{2} w^\\intercal V_k w \\bigg] + D_{KL}[q_k || q_g]$,\n$v_k \\leftarrow v_k + \\rho (S_k w_k - S_g w_g) \\quad  \\text{and} \\quad V_k \\leftarrow V_k + \\rho (S_k - S_g)$,\nServer updates: $w_g \\leftarrow S_g^{-1} \\sum_{k=1}^K v_k, \\quad \\text{where} \\quad S_g = \\delta I + \\sum_{k=1}^K V_k.                                                      (12)$\nSimilarly to before, the updates can be simplified by using a delta approximation to derive a Laplace variant. Because we want local Sk to also be updated, we can use a second-order delta approximation for any $q(w) = N(w; m, S^{-1})$,\n$E_q[l_k(w)] \\approx E_q \\bigg[ l_k(m) + (w - m)^\\intercal \\nabla l_k(m) + \\frac{1}{2}(w - m)^\\intercal H_k(m) (w - m) \\bigg]$\n$= l_k(m) + Tr \\big[ H_k(m) S^{-1} \\big]$,                                                      (13)\nwhere Hk(m) denotes the hessian of lk at w = m. The approximation decouples the optimisation over mk and Sk at a client (we assume that Hk(m) does not depend on m to avoid requiring"}, {"title": "3.4 FEDLAP-FUNC: IMPROVING BY INCLUDING FUNCTION-SPACE INFORMATION", "content": "In the previous section, we derived FedLap-Cov, which improves FedLap by including full covariance information. In this section, we derive FedLap-Func, which improves FedLap by including function-space information over unlabelled inputs. Specifically, we assume some inputs are available to both a local client and the global server, and send soft labels (predictions) over those inputs, along with the weights we send in FedLap. This additional information can be seen as improving the gradient reconstruction of each client's data compared to just a Gaussian weight-space approximation, thereby improving the quality of information transmitted between local clients and the global server (Khan and Swaroop, 2021; Daxberger et al., 2023).\nWe start by writing FedLap's update at the global server as an optimisation problem,\n$w_g = \\arg \\min_w \\delta ||w||^2 - \\sum_{k=1}^K \\log t_k, \\quad\\quad\\quad\\text{client k's contribution},                                                    (15)$\nwhere we note that the solution to this problem (followed by calculating the Laplacian covariance at wg) gives us FedLap-Cov's server update from Eq. 12, and it therefore looks very similar to Eq. 6.\nFollowing ideas from continual learning and knowledge adaptation (Khan and Swaroop, 2021), we improve the contribution from client k by sending information in function-space, instead of only"}, {"title": "4 EXPERIMENTS", "content": "We run experiments on a variety of benchmarks, (i) on tabular data and image data, (ii) using logistic regression models, multi-layer perceptrons, and convolutional neural networks, (iii) at different client data heterogeneity, and (iv) with different numbers of clients. We focus on showing that (i) our VB-derived algorithm FedLap performs comparably to the best-performing ADMM algorithm FedDyn (despite having one fewer hyperparameter to tune: no local weight-decay), (ii) including full covariances in FedLap-Cov improves performance, and (iii) including function-space information in FedLap-Func improves performance. Hyperparameters and further details are in App. E.\nDatasets and splits. We learn a logistic regression model on two (binary classification) datasets: UCI Credit (Quinlan, 1987) and FLamby-Heart (Janosi et al., 1988; du Terrail et al., 2022). Details on the datasets are in App. E. We use the same heterogeneous split on UCI Credit as in previous work (Ashman et al., 2022), splitting the data into 10 clients (results with a homogeneous split of UCI Credit are in Table 4 in App. F). FLamby-Heart is a naturally-heterogeneous split consisting of data from 4 hospitals (du Terrail et al., 2022). We also train a 2-hidden layer perceptron (as in previous work (Acar et al., 2021; McMahan et al., 2016)) on MNIST (LeCun et al., 1998) and Fashion MNIST (FMNIST) (Xiao et al., 2017) with K = 10 clients. We use a random 10% of the data in FMNIST to simulate having less data, and use the full MNIST dataset. We consider both a homogeneous split and a heterogeneous split. To test having more clients, we also use a heterogenous split of (full) Fashion MNIST across 100 clients. Lastly, we train a convolutional neural network on CIFAR10 (Krizhevsky and Hinton, 2009), using the same CNN from previous work (Pan et al., 2020; Zenke et al., 2017). We split the data heterogeneously into K = 10 clients. For all heterogeneous splits, we sample Dirichlet distributions that decide how many points per class go into each client (details in App. E.1). Our sampling procedure usually gives 2 clients 50% of all the data, and 6 clients have 90% of the data. Within the clients, 60-95% of client data is within 4 classes out of 10.\nMethods and hyperparameters. We compare FedLap, FedLap-Cov and FedLap-Func with three baselines: FedAvg (McMahan et al., 2016), FedProx (Li et al., 2020) and FedDyn (Acar et al., 2021), which is the best-performing federated ADMM-style method (FedDyn performs much better than FedADMM). We fix the local batch size and local learning rate (using Adam (Kingma and Ba, 2015)) to be equal for every algorithm, and sweep over number of local epochs, the \u03b4 and \u03b1 hyperparameters, for FedDyn the additional local weight-decay hyperparameter, and for FedLap-Func the additional \u03c4 hyperparameter. For FedLap-Func, we send one randomly-selected point per class per client to the global server in the first communication round (two points for CIFAR10), and in subsequent rounds only send predictions over these same points. This breaks the strictest requirement of not sending any client data to the global server, however, this is very few points, and it might be reasonable to send a few random points having obtained prior permission. We do not report results of FedLap-Func on FLamby-Heart because of the sensitive nature of medical data.\nWe summarise results in Table 1, showing the average accuracy (across 3 random seeds for all datasets) after a certain number of communication rounds. We provide further results in more tables in App. F, where we also report the average number of communication rounds to specific accuracies. They all show similar conclusions.\nFedLap performs at least as well as FedDyn across datasets and splits, and both are better than FedAvg and FedProx. We first compare FedLap with the other baselines. We see that FedLap performs at least as well as FedDyn on all datasets and heterogeneity levels, showing that FedLap is similarly strong as FedDyn, despite having one fewer hyperparameter to tune (see also results on a homogeneous split of UCI Credit in Table 4 in App. F, where FedLap performs better than FedDyn). We note that FedDyn's performance is very sensitive to the value of this additional weight-decay hyperparameter (we provide an example of this hyperparameter's importance in App. D.1). Additionally, we find that FedLap performs well with the optimal global weight-decay \u03b4 (we show an example of FedLap's improved performance over FedDyn due to this property in App. D.2).\nFedLap-Cov significantly improves upon FedLap. We see that FedLap-Cov consistently improves upon FedLap (and other baseline methods including FedDyn). At the highest number of communication rounds in Table 1, FedLap-Cov's accuracy is higher than FedDyn's by 1.7%-5.9% on four of our dataset settings, and is only marginally worse on one (0.2% worse on CIFAR-10, which is within standard deviation; FedLap-Cov is also significantly better earlier in training). This empirically shows the benefit of including covariance information to improve upon ADMM-style methods."}, {"title": "5 CONCLUSIONS AND FUTURE WORK", "content": "In this paper, we provide new connections between two distinct and previously unrelated federated learning approaches based on ADMM and Variational Bayes (VB), respectively. Our key result shows that the dual variables in ADMM naturally emerge through the site parameters used in VB. We first show this for the isotropic Gaussian case and then extend it to multivariate Gaussians. The latter is used to derive a new variant of ADMM where learned covariances incorporate uncertainty and are used as preconditioners. We also derive a new functional regularisation extension. Numerical experiments validate the improvements obtained in performance.\nThis work is the first to show such connections of this kind. No prior work has shown the emergence of dual variables while estimating posterior distributions. The result is important because it enables new ways to combine the complementary strengths of ADMM and Bayes. We believe this to be especially useful for non-convex problems, such as those arising in deep learning.\nThere are many avenues for future work. We believe this connection holds beyond federated learning setting, and we hope to explore this in the future. This could give rise to a new set of algorithms that combine duality and uncertainty to solve challenging problems in machine learning, optimisation, and other fields. There are also several extensions for the federated case. Our current experiments sample all clients in every round of communication, and future experiments could relax this. We expect that our methods will still work well, just like algorithmically-related algorithms such as FedDyn. Future work can also analyse convergence rates of FedLap, following similar theoretical assumptions to those for FedDyn and FedADMM. We could also explore differentially-private versions of the algorithm, following differentially-private versions of PVI (Heikkil\u00e4 et al., 2023). We also expect that our method will perform well in continual federated learning, using Bayesian continual learning techniques (Kirkpatrick et al., 2017; Pan et al., 2020; Titsias et al., 2020). Lastly, future work can explore connections between Federated Distillation and FedLap-Func."}, {"title": "A MAKING FEDLAP MORE SIMILAR TO THE ADMM UPDATE", "content": "In this section, we re-write the FedLap update (Eq. 9) to look more similar to the ADMM update (Eq. 2). Specifically, we divide by Nk, and make the substitution \u1fe6\u03ba = \u03b4\u03c5\u03ba/Nk, giving,\nClient updates: $w_k \\leftarrow \\arg \\min_w l_k(w) + \\tilde{v}_k^\\intercal w + (\\delta /N_k)||w - w_g ||^2$\n$\\tilde{v}_k \\leftarrow \\tilde{v}_k + \\rho(\\delta /N_k)(w_k - w_g)$\nServer update: $w_g\\leftarrow \\frac{1}{\\sum_{k=1}^K (\\delta /N_k)} w_k$.                                                                   (18)"}, {"title": "B DERIVATION OF FEDLAP-COV UPDATES", "content": "Here, we give full derivation of the update given in Eq. 14. We use the definition of the KL divergence for two multivariate Gaussians $q = N(w; m, S)$ and $q_g = N(w; w_g, S_g)$:\n$D_{KL}[q || q_g] = \\frac{1}{2} ||m - w_g||_{S_g}^2 + \\frac{1}{2} Tr(S_g S^{-1}) - \\frac{1}{2} \\log |S_g S^{-1}| + const$.\nWe plug this and Eq. 13 in the first update given in Eq. 12 to get the following,\n$(w_k, S_k) \\leftarrow \\arg \\min_{m,S} l_k(m) + \\frac{1}{2} Tr \\big[ H_k(m_{old}) S^{-1} \\big] + v_k^\\intercal m - \\frac{1}{2} Tr \\big[ V_k (m m^\\intercal + S^{-1}) \\big]$\n$+ \\frac{1}{2} ||m - w_g||_{S_g}^2 + \\frac{1}{2} Tr (S_g S^{-1}) - \\frac{1}{2} \\log |S_g S^{-1}|$.                                                                              (19)\nThe dual term above (the fourth term) is expanded using the following identity,\n$E_q[w^\\intercal V_k w] = E_q[Tr (V_k w w^\\intercal)] = Tr \\big[ V_k E_q (w w^\\intercal) \\big] = Tr \\big[ V_k (m m^\\intercal + \\Sigma) \\big]$.\nWith this, the updates over wk and Sk decouple into two different updates:\n$w_k \\leftarrow \\arg \\min_m l_k(m) + v_k^\\intercal m - \\frac{1}{2} m^\\intercal V_k m + \\frac{1}{2} ||m - w_g||_{S_g}^2$\n$S_k \\leftarrow \\arg \\min_S \\frac{1}{2} Tr \\big[ H_k(m_{old}) S^{-1} \\big] - \\frac{1}{2} Tr \\big[ V_k S^{-1} \\big] + \\frac{1}{2} Tr (S_g S^{-1}) - \\frac{1}{2} \\log |S_g S^{-1}|$.                                                             (20)\nBy replacing m with w, we recover the update for wk in Eq. 14. The update of Sk has a closed form solution, as shown below by taking the derivative with respect to S-\u00b9 and setting it to 0,\n$\\frac{\\partial }{\\partial S^{-1}} H_k(m_{old}) - V_k + S_g - S_k = 0 \\Longrightarrow S_k = H_k(m_{old}) - V_k + S_g$.\nIn the main update, we set mold to be the most recent value of mk before updating it.\nUsing this derivation and the additional details in Sec. 3.3, we can write final FedLap-Cov update as,\nClient updates: $w_k \\leftarrow \\arg \\min_w l_k(w) + v_k^\\intercal w - \\frac{1}{2} w^\\intercal V_k w + \\frac{1}{2} ||w - w_g ||_{S_g}^2,$\n$S_k \\leftarrow H_k(w_k) - V_k + S_g,$\n$v_k \\leftarrow v_k + \\rho (S_k w_k - S_g w_g) \\quad  \\text{and} \\quad V_k \\leftarrow (1 - \\rho) V_k + \\rho H_k(w_k),$\nServer updates: $w_g \\leftarrow S_g^{-1} \\sum_{k=1}^K v_k, \\quad \\text{where} \\quad S_g = \\delta I + \\sum_{k=1}^K V_k.                                                               (21)"}, {"title": "C DERIVATION OF FEDLAP-FUNC EQUATIONS", "content": "In this section we derive the FedLap-Func equations from Sec. 3.4."}, {"title": "C.1 DERIVATION OF EQ. 16", "content": "At the global server, we add in the true loss over inputs in Mk (temporarily pretending as if we had true labels available), and then subtract the Gaussian approximate contribution over these points from Khan et al. (2019),\n$\\sum_{i \\in M_k} l(y_i, w) - A_{i,k} (y_i - J_i^\\intercal w)^2$,                                                  (22)"}, {"title": "D BENEFITS OF FEDLAP OVER FEDDYN AND FEDADMM", "content": "In this section, we show two simple experiments showcasing why FedLap has better properties than FedDyn and FedADMM. First, in App. D.1 we show on a homogoneous MNIST split that FedDyn requires an additional weight-decay term in the local loss during training in order for it to perform well, unlike FedLap. Second, in App. D.2 we show how FedLap targets a better global loss (with weight-decay incorporated already), and gets closer as the number of communication rounds increases, unlike FedDyn and FedADMM, which target a worse $w_g^{\\delta = 0}$."}, {"title": "D.1 IMPORTANCE OF LOCAL WEIGHT-DECAY IN FEDDYN", "content": "In this section, we show the importance of the additional weight-decay parameter to FedDyn's performance on a simple example (homogeneous MNIST), a hyperparameter that FedLap removes. Fig. 1 shows the results. We first see that FedDyn (with optimal weight-decay setting) and FedLap perform similarly. However, FedDyn's performance is highly dependent on the setting of its weight-decay hyperparameter: changing it by an order of magnitude in either direction significantly degrades performance. Removing it entirely results in catastrophic performance after the first 7 communication rounds. In contrast, changing the \u03b4 hyperparameter in FedLap (or FedDyn) by an order of magnitude does not reduce performance significantly in this setting."}, {"title": "D.2 FEDLAP TARGETS A BETTER GLOBAL LOSS THAN FEDDYN/FEDADMM", "content": "In this section, we show a simple setting showcasing the potential benefit of FedLap over FedDyn and FedADMM. We take the UCI Credit dataset (a binary classification task), split the data equally (and homogeneously)"}]}