{"title": "CODECLEANER: Elevating Standards with A Robust Data Contamination Mitigation Toolkit", "authors": ["Jialun Cao", "Songqiang Chen", "Wuqi Zhang", "Hau Ching Lo", "Yeting Li", "Shing-Chi Cheung"], "abstract": "Data contamination presents a critical barrier preventing widespread industrial adoption of advanced software engineering techniques that leverage code language models (CLMs). This phenomenon occurs when evaluation data inadvertently overlaps with the public code repositories used to train CLMs, severely undermining the credibility of performance evaluations. For software companies considering the integration of CLM-based techniques into their development pipeline, this uncertainty about true performance metrics poses an unacceptable business risk. Code refactoring, which comprises code restructuring and variable renaming, has emerged as a promising measure to mitigate data contamination. It provides a practical alternative to the resource-intensive process of building contamination-free evaluation datasets, which would require companies to collect, clean, and label code created after the CLMs' training cutoff dates. However, the lack of automated code refactoring tools and scientifically validated refactoring techniques has hampered widespread industrial implementation.\nTo bridge the gap, this paper presents the first systematic study to examine the efficacy of code refactoring operators at multiple scales (method-level, class-level, and cross-class level) and in different programming languages. In particular, we develop an open-sourced toolkit, CODECLEANER, which includes 11 operators for Python, with nine method-level, one class-level, and one cross-class level operator. We elaborate on the rationale for why these operators could work to resolve data contamination and use both data-wise (e.g., N-gram matching overlap ratio) and model-wise metrics (e.g., perplexity) to quantify the efficacy after operators are applied. A drop of 65% overlap ratio is found when applying all operators in CODECLEANER, demonstrating their effectiveness in addressing data contamination. Additionally, we migrate four operators to Java, showing their generalizability to another language. We make CODECLEANER online available at https://github.com/ArabelaTso/CodeCleaner-v1/ to facilitate further studies on mitigating CLM data contamination.", "sections": [{"title": "I. INTRODUCTION", "content": "The software industry has increasingly embraced techniques to leverage code language models (CLMs) as powerful tools for various software development and maintenance tasks [1]\u2013[4]. However, a critical concern has emerged regarding their evaluation: these models, trained on vast code repositories, may have already encountered the test data during their training phase. This phenomenon, known as data contamination [5]\u2013[9], can lead to artificially inflated performance assessment, raising questions about the true capabilities of these techniques. Recent studies [5]\u2013[7], [10]\u2013[14] have highlighted this issue as a major threat to the reliable assessment of CLM-based techniques. The threat poses an unacceptable business risk in adopting these techniques.\nChallenge However, identifying and preventing data contamination is non-trivial. First, in the era of large language models (LLMs), training corpora is usually tremendously mas- sive. As pointed out by a recent survey [15], the total data size for pre-training corpora surpasses 774.5 TB, making it difficult to analyze the entire corpora for data contamination [16]\u2013[18]. In addition, data contamination could be indirectly introduced inadvertently. For example, LLMs may acquire programming languages not only from code hosting platforms such as GitHub and GitLab but also from blogs, open-source forums, or social media feeds. Third, thoroughly eliminating data contamination from CLMs is hard due to the nature of copy-and-paste coding practices [19]. Software developers reuse, reference, or adapt existing source codes to prevent reinventing the wheels while enhancing development efficiency. Besides, CLMs' training corpora will increasingly contain AI-written codes with the prevalent use of AI programming assistants such as Github Copilot [20]. As reported by Github on Aug 21, 2024, nearly all (97%) developers use AI coding tools [21]. In other words, identifying and preventing data contamination is challenging.\nRecent studies have been aware of the need to alleviate the data contamination threat to the validity in evaluating their proposed CLM-based techniques [22]-[24]. One solution is to collect data (e.g., code) uploaded after CLMs' release date (i.e., the cut-off date). Yet, as mentioned above, considering the increasing use of AI assistants in software development, collecting new code is not an effective solution. An alternative is to construct benchmarks or adapt them from crawled open- source platforms manually. Representative benchmarks like HumanEval [25] and SWE-bench [26] were once believed to"}, {"title": "Research Gap \u2013 Code refactoring", "content": "Code refactoring [28], [29] (e.g., changing code structures and renaming the variables) is considered a lightweight solution to mitigate data contamination. However, four outstanding challenges remain to be solved for effective mitigation. First, how may code be refactored without altering its original semantics? Unlike perturbing text and image data, perturbing code while maintaining its syntactic, semantic, and logic requirements is difficult. Arbitrary modifying characters (e.g., adding/deleting/replacing characters) in the code string could easily ruin the code's grammar or functionalities, making it uncompilable or malfunctioning. Second, it is unclear how various code refactoring operators actually perform in practice. While previous studies have applied refactoring operators both manually and automatically, they typically lack comparative analysis between pre- and post-refactored code. This absence of direct comparison makes validating whether these operators can reduce data contamination difficult. Recent research has revealed an unexpected outcome: certain refactoring operations may actually increase the model's famil- iarity with the code, potentially counteracting their intended purpose [9]. Third, the effectiveness of code refactoring tech- niques can vary significantly across different programming languages. What proves successful in one language may not translate effectively to another. This variability is particularly relevant given recent research [30] demonstrating how language models perform differently across programming languages. These findings underscore the need to better understand both how data contamination manifests differently across programming languages and how refactoring strategies must be adapted accordingly. Fourth, the scope of code refactoring ranging from individual methods to entire classes or projects - presents another critical challenge. While method- level refactoring focuses on maintaining internal coherence within a single method, larger-scale refactoring must address the additional complexity of managing relationships between multiple functions and classes. This fundamental difference suggests that refactoring strategies need to be specifically designed and evaluated for different levels of code granularity.\nTo explore these challenges, we implemented eleven code refactoring operators for either method-, class-, or cross-class- levels. These refactoring operators are meticulously handled to alter the code's syntactic, semantics, and code style without affecting the original semantic integrity and maintaining correct syntax, as shown in Figure 1. Upon the implementation, we conducted the first systematic study assessing the efficacy of code refactors to data contamination on CLMs. We focus on code data rather than text or image data because of their relevance to software engineering areas. To determine the severity of the contamination, we consider two bunches of measurements, i.e., perplexity [31] and its variants and n- gram overlap ratio. In particular, we use the Stack as training data corpus and use the contaminated code extracted from the"}, {"title": "III. CODE REFACTOR DESIGN", "content": "Bearing the insight observed in Section II, we then further propose refactoring operators (abbrev. operators) and explore their effectiveness. Ideally, a desired operator should maintain the original code's semantics while perturbing the consecutive tokens as much as possible. Also, the operators are better able to be applied automatically without human intervention and assistance, making them easy to use and lightweight. Additionally, the operators are expected to be generalizable and language-agnostic so that they can be generalized to other programming languages. Based on the above three underlines, we then design three categories of refactoring operators, as shown in Figure 1, i.e., Syntactic Refactoring Operators (Sec- tion III-A), Semantics Refactoring Operators (Section III-B), and Code Styles Refactoring Operators (Section III-C)."}, {"title": "A. Syntactic Refactoring Operators", "content": "Operators in this category alter the code syntactic structure while keeping the code semantics untouched. They manipulate the Abstract Syntax Tree (AST) of the code without affecting the original functionalities. In particular, we consider three types of syntactic operators.\n1) If-condition Flipping (IFF): It is straightforward to negate the if-condition and flip the statements in if- and else- branches. After such a change, the code structures will be changed while the code semantics should not be changed. In particular, if a code snippet contains both if- and else-branches, then IFF can be applied smoothly, as shown in lines 12-15 of Figure 2 (B) to (C). Yet, if a code snippet contains only an if-branch and no else-branch, as shown in lines 6-9 in Figure 2(A), then we regard the statement in else-branch as pass and apply the same operation as usual. For the example of lines 6-9 in Figure 2(A), the resulting if- and else-branches are shown in Figure 3 (lines 6-11).\n2) Loop Transformation (Loop): Switching while loops with the equivalent for loop also keeps the seman- tics unchanged. Such transformation is a common semantic- equivalent transformation adopted by prior work [9], [29]. Take for to while loop as an example, shown in Figure 4. First, we identify the loop control variable stream (i.e., over which the loop iterates), and create an iterator _iter2 over it. Note that the name of the iterator will be checked to ensure it does not overwrite the existing variable names in the code snippet. Second, we formulate the while condition as True and setup the termination condition (i.e., reaching or exceeding the end of the iteration range) to mimic the termination condition in the for loop. To ease the implementation, we use try-catch blocks to iterate over the iterator and terminate till the exception StopIteration is raised. In the try block, we update the control variable using the next() function provided by the iterator. Finally, we transfer the loop body by copying the body of the for loop into the while loop as it is. A similar transformation is applied when transforming while to for. We omit the details due to space limitations.\n3) Iteration Transformation (Iter): In Python, there are two ways to iterate over an iterable object, i.e., to directly access each element or use indices within a range. The transformation between them keeps the semantics unchanged. An example of such transformation is shown in Figure 5. In particular, line 4 in Figure 5(A) uses direct access to each element in data, denoted by the variable row. After the transformation, the variable row denotes the index within the length range of data. Thus, each iterative element is denoted as data[row]."}, {"title": "4) Commutative Law Flipping", "content": "Commutative Law Flipping (Comm): The commuta- tive law in mathematics [36] is a fundamental principle that can be applied to various operators such as logical operations. We implement the commutative law in logical operations \u2227 (i.e., and in Python, && in Java) and V (i.e., or in Python, || in Java) by first grouping operands in each expression according to the operator connecting them, and then shuffling the operands in each group into a new operand sequence. Figure 6 shows an example where the operands v is None, v == '', and v != v with the same operator or in a logic statement at line 3 are randomly permuted. This reordering of operands (i.e., boolean values and propositions) contributes to disrupting the memorized patterns while preserving the semantics.\n5) (Class-level) Method Shuffling (Shuf): This operator is designed specifically for class-level code. Shuffling the order of methods within a class will not affect code functionalities and semantics. Such an operator is easy to implement and could effectively disrupt the consecutive tokens in the codes."}, {"title": "B. Semantic Refactoring Operators", "content": "Except for the syntactic operators, adding extra semantics or slightly perturbing the original code semantics without disrupting the function and compatibility of original code is also applicable. For example, changing identifier names or adding additional context will not change the original code's functionality while introducing slight semantic perturbations into the original code. In particular, according to the changes to the code semantics, we further categorize semantic operators into three subcategories, i.e., semantic addition (Sections III-B1, III-B2, III-B3) and semantic perturbation (Section III-B4).\n1) Special Parameter Appending (Param): A prior work [9] found out that appending unnamed positional pa- rameters (*args) and keyword parameters (**kwargs) in the parameter list of method declarations tend to alleviate data contamination. Thus, we follow the idea of appending these parameters if they do not exist in the original method declarations. Figure 7 shows an example where appending special parameters reduces the overlapped tokens.\n2) Performance Measurement Decoration (Deco): As introduced by a prior work [9], adding decorators such as @timing (measuring the execution time) and @measure_memory_usage (measuring the memory usage) to Python methods do not change the function's behavior. Thus, we include this operator in our toolkit.\n3) (Cross-Class) Inherited method Appending (Inhr): For the classes that inherit from superclasses, copying methods from the superclasses to the (sub)class (if they are not already overridden in the subclass) does not affect the semantics of the subclass. Note that inheritance could be hierarchical, which means that a class may inherit non-overridden methods from its superclass and continue to propagate these methods to its subclasses. After obtaining the inherited methods, an operator of method shuffling (Section III-A5) always follows.\n4) Identifier Renaming (Renm): Synonym replacement is a commonly used perturbation in various natural language processing tasks [37]\u2013[41]. In the scenario of identifier renam- ing in code snippets, it posts extra requirements in the words to replace (i.e., the replacement should not be the keywords such as while and open), and the post process of the replacement (i.e., replacing all the occurrence of the replaced identifier). To implement the replacement, we use wordhoard [42] as prior work [9] to replace the identifiers in the source code. Then, we traverse the AST nodes to replace all the occurrences to ensure the code compilation. Figure 8 shows an example of identifier renaming. Interestingly, after replacing the variable name new_data to advanced_data, the consecutive memorized lines of codes (lines 4-6 in Figure 8(A)) have been disrupted."}, {"title": "C. Code Styles Refactoring Operators", "content": "Changing code styles, such as changing naming styles, do not affect the code semantics while interrupting consecutive code characters. Thus, we also include them in our toolkit.\n1) Code Normalization (Norm): Normalizing the code styles, such as unifying single and double quote marks, regularizing the number of spaces, and using parentheses to indicate operation precedence, does not change the integrity of code functionality. Meanwhile, it changes the tokens and thereby helps to interrupt the consecutive token patterns. Figure 9 shows an example where the replacement of original double-quotes marks and the addition of parenthesis are applied to the original Python code. These modifications scatter the consecutive tokens, thereby preventing CLMs from recognizing a long token sequence in the memory.\n2) Naming Style Switch (Styl): The camel case (e.g., camelCase) and lower snake case (e.g., lower_snake) are two popularly-used naming conventions in programming. Though it is usually the case where Python uses lower snake cases to name variables while Java uses camel cases, the switched situation also allows code compilation and execution. Thus, we flip these two naming styles, once identifying one style, flipping to another. The flipped variable names are applied to all the occurrences. An example is shown in Figure 2(A) to (\u0392). After flipping three variables in RFigure 2(A), the memorized consecutive lines 2-6 are resolved."}, {"title": "IV. EXPERIMENT PREPARATION", "content": "We select four representative CLMs widely studied in recent works and used in recent data contamination study [9]. Table I shows the model information, including the model name, links to the models, training source, and the first release date. In particular, we choose the open- sourced CLMs because their data sources are more transparent compared to commercialized models such as ChatGPT.\n2) Data Preparation: We choose the Stack v1.2 [45] dataset as the code corpus for its representativeness and comprehensiveness, which contains over 2.9 TB data to avoid duplication from January 1st, 2015 to March 31st, 2022.\n\u2022 For RQ1: To investigate the effectiveness of code refactors in method-level code, we prepare Python code at the method level for refactoring. Specifically, since there are more than ten thousand Python methods every year in the Stack v1.2 [45], we then follow prior work [9] to set a 95% confidence level and 5% margin of error and sample from the year 2021 (i.e., the last updated date fell in 2021)\n\u2022 For RQ2: To investigate the refactors' effectiveness in class- level and cross-level code, we select three popular Python libraries, i.e., Scikit-learn [49], Pandas [50], and Numpy [51]. We chose them because of their popularity and open access, which give them a higher chance to be included in the training data (evidence can be found in Section V-B). Then, we split these three libraries into Python classes and filter out the classes that cannot be successfully imported with the standard dependencies of these libraries, e.g., the test classes depending on pytest. In total, there are 2032 classes. Then, we set a 95% confidence level and 5% margin of error and stratified sample from three libraries, resulting in 324 classes (with an average of 163.55 LoC) for RQ2, ready for further refactoring.\n\u2022 For RQ3: To study the severity of data contamination in different programming languages, we still sample from the Stack [32]. We also sampled Java, Rust, and C apart from Python because of their representativeness and popularity, with each 384 method-level code snippet using the same confidence level and margin of error as the sampling strategy. Besides, we sampled data for the years 2018 ~ 2022. In total, there are 4 * 5 * 384 = 7680 code snippets (with an average of 49.73 LoC) to measure the severity of data contamination.\n\u2022 For RQ4: To study the effectiveness of code refactors in Java code, we applied the top-5 effective refactors in Python for Java code. Note that since the minimum function unit of Java is class-level, we sampled 384 Java classes with more than one method with more than 3 LoC as prior work [48] in the method body so that the method-level refactors are applicable. Then, we compare the effectiveness of the same refactors in different programming languages (i.e., Python and Java), providing potential generalizability of refactors across programming languages."}, {"title": "V. EVALUATION", "content": "We study four RQs listed in Section I. The data prepared for each RQ can be found in Section IV-A2. The severity of data contamination is quantified using both data-wise and model- wise measurements (see Section IV-B). The experiments are conducted on a computational infrastructure comprising two NVIDIA RTX 6000 Ada GPUs, each with 48GB VRAM.\nA. RQ1: Effectiveness on Method-level Python Code\nSetup To study the effectiveness of eight method-level operators (shown in Figure 1), we apply each of them to 384 Python methods. In particular, most operators manipulate code over AST, as mentioned in Section III, and thereby the code should be syntactically correct and normalized. So, except for the operator Norm, the effects of other operators are applied to the normalized code. In other words, the effects of other operators can be viewed superimposed on the effect of Norm.\nData-wise Effect \u2013 The data-wise effectiveness of operators shows in Figure 10. It is clear that without refactoring, the overlap of the original code is 92% (median), which is pretty high. It is reasonable because the original codes are extracted from the training set. After the refactoring, the overlap drops to 48% ~ 63%, an average of 35.89% drop. Among the operators, semantic operators witness the largest drop (average drop: 39.3%), followed by code style operators (34.5%) and syntactic operators (34%). Though the overlap ratio of Norm appears the highest among the operators, recall that the other operators are based on the operator Norm. Thus, if we look at the reduction brought by each operator independently, the Norm operator results in the most significant decrease.\nFinding: Method-level operators can reduce 35.89% overlap with the training set on average. Among them, Semantic and code-style operators are more effective at making the code unlike in its form in the training set.\nModel-wise Effect \u2013 Recap that we consider two model-wise metrics (i.e., perplexity and Min-20% Prob) to measure the data contamination on specific models, and evaluate against four CLMs (Table I). To better visualize the effects, we subtract the metrics values of all operators from the scores on the original code. A larger difference indicates that the operator makes the model less familiar with the code, thus more effective. Otherwise, a smaller difference indicates less effectiveness. Figure 11 shows the results of two metrics (i.e., ppl and Min_20% Prob) for the model StarCoder-Instruct. We can see that the two metrics reflect similar rankings to the operators, though the absolute scores vary in two subfigures. Among the"}, {"title": "B. RQ2: Effectiveness on Larger Scale Python Code", "content": "Setup - In view of the effectiveness of operators in method- level Python code, we further explore how they perform on a larger scale. We consider two additional operators, one for class-level and one for cross-class, as shown in Figure 1. In particular, Inhr (Cross-class) operator should be applied on more than one class with inheritance relations, and Shuf (Class- level) operator should be applied to a class with more than one method. The detailed design can be found in Section III. It is noteworthy that method-level operators are also applicable for larger-scale Python code, so we also evaluate their effectiveness on the data prepared for RQ2 (see Section IV-A2 for details).\nData-wise Effect The result is visualized in Figure 13. We can see the original overlap is up to 78% on median. After 11 operators are applied, the decrease in overlap is obvious, with 37% (78% - 41%) drop at best. Also, the rankings are similar as that on method-level Python code (as shown in Figure 10, where semantic operators also achieve the largest drop. It shows the generalizability of method-level operators on larger-scale Python code.\nFinding: The 11 operators show promising results on larger- scale Python code, reducing a maximum of 37% overlap rate with the training set. The method-level operators are generalizable to larger scales.\nAdditionally, zoom in on the two additional operators, Shuf and Inhr, we can see that shuffling the methods in classes (Shuf) does help to decrease the overlap, from 78% to 50%, with a 28% drop. Yet, appending inherited methods from the superclass raises the overlap slightly on average. This is because although inserting inherited code may interrupt the continuity of the original code, the newly inserted code may also introduce additional overlap, so the overall overlap increases.\nFinding: Class-level Shuf can effectively reduce the overlap with the training set by 28%. Although the cross-class Inhr can work in some cases, overall, the final overlap ratio increases slightly due to the introduction of duplicate code."}, {"title": "C. RQ3: Contamination In Other Programming Languages", "content": "Setup - The previous two RQs studied the severity of data contamination in Python at method- or larger level. In this RQ, we further explore the contamination in different programming languages and how it changes over time. We focus on Java, C, and Rust due to their popularity and show data-wise overlapping from the year 2018 to 2022.\nThe data-wise results are shown in Table 15. It is clear that Java overlaps with the training set the most, reaching an average of 98%. Python, C, and Rust have similar overlap ratios, averaging from 88% to 94%. Meanwhile, the trend of overlap of each programming language has been relatively stable over time, except for C, which had a significant drop in 2021. The model-wise results show similar trends to the data-wise results, but we omit the figures due to space limitations.\nFinding: The severity of data contamination is similar across different programming languages, with Java having the highest at 98% on average, and other programming languages close to 94%. Moreover, from 2018 to 2022, data contamination has remained relatively stable each year."}, {"title": "D. RQ4: Effectiveness of Operators in Java Code", "content": "Finally, we study the possibility of generalizing the operators to other programming languages. In particular, Deco, Param, and Iter are not applicable to Java, while Comm and Styl have little effect, so we implement IFF, Loop, Renm, and Norm for Java, and apply them in the 384 Java classes (the preparation can be found in Section IV-A2).\nData-wise Effect \u2013 The effectiveness of Java operators is shown in Figure 16 (blue). The initial overlap is around 98%, which is pretty high. After refactoring, at most 17% (= 0.98 0.81) drop can be achieved by Renm.\nModel-wise Effect \u2013 The model-wise results of four operators on Java are shown in Table IV. Semantic Renm and syntactic IFF are similarly effective on Java among four operators, reach- ing 0.08+ improvement. Yet, the degree of such improvement is minor, calling for more effective operators on Java.\nFinding: Most migrated operators show a positive effect on resolving Java code contamination, which shows the generalizability of operators in CODECLEANER. While considering the improvement is subtle, how to effectively refactor Java is still an open question."}, {"title": "VI. RELATED WORK", "content": "Several studies [5]-[7], [10]\u2013[14] have explored the data contamination threat in large language models (LLMs). Brown et al. [17] analyzed the contamination in GPT-3, showing that the performance between contaminated and clean data is not necessarily different. Carlini et al. [11] found that the memorization of LLMs grows with model size, training data du- plicates, and prompt length. Kandpal et al. [12] identified that the success of privacy attacks on LLMs could be attributed to the sequence duplication in the training set. Razeghi et al. [13] studied the correlations between LLMs' performance and the frequency of terms and observed that LLMs perform better on inputs with more frequent terms. Magar et al. [14] pre- trained and fine-tuned models (i.e., BERT) from scratch, with a controlled training set, to identify factors that affect model exploitation. Balloccu et al. [10] studied the impact of the indirect data contamination, i.e., using user feedback as the training source. These works studied the severity of data contamination, while our study investigated the effectiveness of code refactoring operators in alleviating data contamination."}, {"title": "B. Detection of Data Contamination", "content": "As data contamination becomes inevitable, methods have been proposed to detect it. One of the most commonly used methods is n-gram matching [18], [33], [56], [57], which partitions text into sequences of N consecutive character- s/tokens and compares comparing these sequences to find patterns or similarities with the training data. For example, Marone et al. [33] offers a website [34] to compare with three data sources, including The pile [58], The Stack [32], and The Stack-V2 [59]. Recently, Deng et al. [60] proposed a mask-and-fill method to detect contamination in multiple-choice questions and raised concerns about data contamination in Question-Answering benchmarks such as TruthfulQA [61] and MMLU [62].\nAnother related task, Membership inference attacks (MIAs), determines whether a given data is contained in the model's training data and can be used as a hint for data contamination. Various metrics are proposed to infer data membership, includ- ing LOSS [63], reference models [17], perplexity [31], Zlib Entropy [64], Neighborhood attack [65], Min-k% Prob [16], etc. Though MIAs are extensively studied in traditional deep learning models, the research on MIA in LLMs is limited. Duan et al. [66] conducted a large-scale MIA on LLMs and found that MIAs barely outperform random guessing across varying LLM sizes and domains. Recently, Dong et al. [22] detected data contamination by identifying the peakedness of LLM's output distribution and showed potential.\nOur work differs from these works in purposes, i.e., they detect the existence of data contamination while we introduce effective code refactors that address data contamination."}, {"title": "C. Resolution of Data Contamination", "content": "Various benchmarks [25], [26], [29], [48], [67]-[69] have been proposed to avoid data contamination via involving code after LLMs' cut-off-date. Jain et al. [70] traced and recorded the latest code contests from LeetCode, AtCoder, and CodeForces, aiming at providing a contamination-free code contests benchmark for LLMs evaluation.\nInstead of proposing new benchmarks, another bunch of works explores code refactoring. For example, Wu et al. [29] adopted identifier renaming and code structure change to the code they collected. Besides, several studies [71]-[73] leverage code assistants such as GPT-4 to do the code refactoring.\nOur work differs from these works in two ways. First, we avoid using Als to eliminate reintroducing variants in our study. Second, their refactoring heavily relies on manual crafting, while our refactors are conducted automatically. Third, these works did not compare the change in the severity of data contamination after the refactoring."}, {"title": "VII. THREATS TO VALIDITY", "content": "We acknowledge several threats to the validity of our conclusions. First, training set selection bias. Our study only uses the Stack as the training set, while there are other training sets for code, such as the CodeSearchNet [74] and CodeParrot [75]. We chose the Stack because it is popularly used in CLMs training (more than 60+ CLMs trained on it [76], and has a long time span (January 1st, 2015 to March 31st, 2022). Second, the sampling may not be representative. For each RQ, we sampled 324 ~ 384 Python/Java code at the method/class level to experiment. The sampling may introduce bias, so the conclusion may vary. To alleviate this threat, we carefully set up the confidence level as 95% and the margin of error as 5% as the sampling scale. We also filter out the low-quality code (e.g., too short or extra long) to ensure the quality of sampled data. Third, the semantic operators we implemented may inadvertently change the code semantics, making the model find the code semantics strange and thus exacerbating model-wise data contamination."}, {"title": "VIII. CONCLUSION", "content": "In this paper, we introduce CODECLEANER, an off-the-shelf automated refactoring toolkit for Python and Java to mitigate data contamination in CLM evaluation. We perform quantitative analysis of the change in data contamination severity after applying different operators and identify effective operators under various settings (i.e., model-wise or dataset-wise, method- level, or larger-scale). We make CODECLEANER online avail- able to facilitate the studies towards more lightweight and effective CLM data contamination solutions. It enables software companies to more accurately assess the true performance of CLM-based techniques before their adoption."}]}