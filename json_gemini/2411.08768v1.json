{"title": "Sharingan: Extract User Action Sequence from Desktop Recordings", "authors": ["Yanting Chen", "Yi Ren", "Xiaoting Qin", "Jue Zhang", "Kehong Yuan", "Lu Han", "Qingwei Lin", "Dongmei Zhang", "Saravan Rajmohan", "Qi Zhang"], "abstract": "Video recordings of user activities, particularly desktop recordings, offer a rich source of data for under- standing user behaviors and automating processes. However, despite advancements in Vision-Language Models (VLMs) and their increasing use in video analysis, extracting user actions from desktop recordings remains an underexplored area. This paper addresses this gap by proposing two novel VLM-based methods for user action extraction: the Direct Frame-Based Approach (DF), which inputs sampled frames directly into VLMs, and the Differential Frame-Based Approach (DiffF), which incorporates explicit frame differences detected via computer vision techniques. We evaluate these methods using a basic self-curated dataset and an advanced benchmark adapted from prior work. Our results show that the DF approach achieves an accuracy of 70% to 80% in identifying user actions, with the extracted action sequences being re-playable though Robotic Process Automation. We find that while VLMs show potential, incorporating explicit UI changes can degrade performance, making the DF approach more reliable. This work represents the first application of VLMs for extracting user action sequences from desktop recordings, contributing new methods, benchmarks, and insights for future research.", "sections": [{"title": "I. INTRODUCTION", "content": "Video recordings are increasingly favored for capturing user activities due to their ease of implementation and broad applicability. Moreover, video's universal compatibil- ity across platforms and devices, combined with its ability to capture detailed and context-rich data, ensures minimal information loss and facilitates thorough analysis.\nRecent advancements in Vision-Language Models (VLMs) [21], [20], [8], [24], [5], [14], [1] have significantly improved the utility of video recordings. These AI-driven models automate the interpretation and extraction of insights from video data, enhancing the identification of user behaviors and patterns. Combined with the increasing prevalence of Al-integrated hardware [9], [22], these technological innovations are accelerating the adoption of video as an essential tool for documenting and analyzing user activities.\nDespite extensive research into understanding user actions from various types of videos [7], [12], there remains a lack of focus on desktop recordings. Addressing this gap is crucial, as extracting user actions from desktop videos offers numerous benefits. For instance, it can enhance Robotic Process Automation (RPA) by utilizing demo videos as input, increasing productivity through automation [10]. Moreover, desktop video analysis facilitates the automatic creation of tutorials and guidelines, while also enabling the extraction of personalized interaction patterns, which can be leveraged elsewhere to create a more personalized user experience.\nWe propose two VLM-based methods for extracting user action sequences from desktop recordings. In the Direct Frame-Based Approach (DF), sampled video frames are directly input into VLMs, while the Differential Frame- Based Approach (DiffF) first detects frame changes using computer vision techniques before interpreting them with VLMs. The key difference lies in whether explicit frame differences are incorporated to aid action inference.\nWe evaluate both methods using two benchmark datasets: one crafted by us, focusing on individual action types, and the other adapted from GUI-World [3] which better reflects real-world scenarios. Experimental results reveal that current VLMs show great potential in extracting user actions from desktop recordings. For instance, using the DF approach, we achieve an accuracy of 70% ~ 80% in identifying operation types (e.g., click), and the extracted action sequences are re- playable through RPA-like processes. Moreover, comparing the two approaches reveals that VLMs struggle to utilize UI changes derived explicitly, sometimes leading to performance degradation. Thus, we recommend the Direct Frame-Based Approach, relying on VLMs' inherent ability to infer actions, as the current best practice for action extraction.\nOur contributions can be summarized as follows:\n\u2022 We introduce two VLM-based methods to address the gap in existing research on extracting user action se- quences from desktop recordings. To the best of our knowledge, this is the first attempt to leverage VLMs for this task.\n\u2022 We develop two benchmark datasets to assess the per- formance of methods on this task. All evaluation source codes and benchmarks will be made publicly available.\n\u2022 We perform a comprehensive evaluation of the proposed methods using the developed benchmarks."}, {"title": "II. RELATED WORK", "content": "VLMs in Robotics and Automation VLMs have been increasingly applied across various robotics and automa- tion tasks. They have proven valuable for robot navigation [16], [34], [32], robot action recognition [31], [4], and task planning [25], contributing significantly to enhancing robotic visual perception capabilities [11], [17]. These works primarily focus on physical environments where VLMs help robots interpret and interact with the world around them, and"}, {"title": "III. METHODOLOGY", "content": "This work focuses on extracting sequences of user actions from desktop video recordings by leveraging VLMs that analyze videos at the frame level.\u00b9 To achieve this, we uniformly sample n frames F = {F} from the video V, where the frame F is selected at time t. The extracted action sequence, represented as A = {A}, consists of"}, {"title": "A. Method I: Direct Frame-Based Approach", "content": "As shown in Figure 1a, DF consists of three modules: Action Proposer, Action Corrector and Action Merger. Since current VLMs often only handle a limited number of images in one call, we use a sliding window to process w frames at a time, allowing o overlapping frames. The Action Merger combines results from all windows. If frames can fit within the VLM's context, the sliding window and Action Merger are unnecessary. Each module is described in detail below.\nAction Proposer. This module proposes a candidate action sequence for sampled frames within each time window. The prompt used for this task, detailed in Table VIII in the Appendix, directs the VLM to focus on changes potentially related to user actions. For instance, when inferring a click, the VLM utilizes several features: i) mouse shape change,"}, {"title": "B. Method II: Differential Frame-Based Approach", "content": "The DiffF approach, illustrated in Figure 1b, consists of four components: Frame Difference Localizer, Frame Difference Descriptor, Action Proposer and Action Corrector. In contrast to the DF approach, DiffF introduces the Frame Difference Localizer and Descriptor to explicitly capture frame differences. The Action Proposer and Corrector mod- ules, while playing similar roles to those in DF, are adapted to the DiffF framework. Note that DiffF bypasses previous input image limits for VLMs as frame difference generation requires only two consecutive frames. Additionally, since ac- tion proposing and correction are handled in text and VLMS typically support extensive textual context, Action Merger is generally unnecessary in DiffF, except for exceptionally long videos, which are not addressed in this work.\nFrame Difference Localizer. This module identifies and outputs screen regions that have changed between two con- secutive frames. An example of the detected regions with annotated changes is illustrated in Fig. 2 in the Appendix. The localization process involves the following steps with the tools [2], [27]:\n\u2022 Normalize the RGB pixel values of input frames to [0, 1].\n\u2022 Apply a Gaussian blur with a (5,5) kernel size and a standard deviation of 2 to reduce false-positive UI changes caused by high-frequency noise, commonly\n\u2022 Calculate the L2 norm of the difference in pixel values between the two frames and threshold the resulting difference with a tuned value of 0.15 to create a binary mask.\n\u2022 Remove objects smaller than 10 pixels from the binary mask (as changes smaller than this threshold are gener- ally imperceptible to the human eye) and identify con- nected components with their corresponding bounding boxes.\n\u2022 Expand each bounding box by 100 pixels on all sides and merge any overlapping boxes to provide more visual context while reducing the number of regions to compare.\nFrame Difference Descriptor. This module utilizes a VLM to generate detailed textual descriptions of UI changes based on current frame and detected changed regions as compared to previous frame in the above step. The prompt for this module is provided in Table XIII in the Appendix, along with a sample output corresponding to the changes illustrated in Figure 2. The output includes overall frame context and specific details about the changed UI elements. However, since Descriptor is only provided with localized information (i.e., current frame and their frame differences), it does not generate action sequences directly.\nAction Proposer. After aggregating textual descriptions of UI changes across all frames, this module prompts the VLM to propose candidate actions, forming an action sequence for the entire video. The prompt used in this process is analogous to that in the DF approach and differs mainly in the supporting information part, which will be utilized in the next Action Corrector module. A detailed description of the prompt is provided in Table XV in the Appendix.\nAction Corrector. Given that DiffF is prone to generate false positive actions due to extraneous information in the textual descriptions of UI changes, the Action Corrector incorporates an additional rule-based component not present in DF. While the VLM-based corrector in DiffF employs a similar prompt to the Action Corrector in DF, the rule-based component specifically targets the elimination of scroll actions without corresponding UI movement and click actions where the cursor is absent from the evidence."}, {"title": "IV. EVALUATION", "content": "In this section, we first introduce two benchmark datasets ACTONE and ACTREAL, followed by a description of the evaluation methods and metrics used in the experiments."}, {"title": "A. Benchmark Datasets", "content": "ACTONE. Using OBS Studio [19] for screen recording and manual annotation of action sequences, we built the ACTONE dataset, specifically designed to evaluate VLMs' fundamental abilities in action sequence extraction tasks. This dataset includes five operation types of (click, select,"}, {"title": "B. Evaluation Methods and Metrics", "content": "We primarily evaluate the predicted action sequences by comparing them with the ground-truth sequences in the benchmark datasets. Although the comparison is performed in semantic space rather than through exact word matching, errors still arise due to the variability in how the same UI object can be described in different ways. To further validate the proposed semantic comparison metrics, we introduce another functional metric derived by replaying the predicted action sequences using a VLM-based UI automation tool in the same environment as the benchmark dataset. If the final outcome matches the ground-truth video, it confirms the accuracy of the action sequence. This replay approach mirrors the Robotic Process Automation (RPA) process, a key potential application of this work. Below, we provide a detailed description of both evaluation methods.\nSemantic Comparison. The comparison is performed at the individual video level by analyzing two sequences of text strings: the predicted action sequence, A\u00ba = {(O?, D?, C?) | i \u2208 LP}, and the ground-truth action sequence, Ag = {(O?, D?, C?) | j \u2208 L9}. These sequences may have differing lengths, as denoted by |LP| and |L9|.\nOur comparison process begins by computing three sim- ilarity matrices, SO, SD, and S, for each action element, where i \u2208 LP and j\u2208 L9. Since the operation type O is discrete and limited to five categories, So is computed via exact matching, producing a binary (0-1) matrix. For SD and Si, semantic matching is performed by: i) generating BERT embeddings [23] for the operation detail D and context C elements from both the prediction and ground- truth sets; ii) computing pairwise cosine similarity between the embeddings; iii) applying a manually tuned threshold of 0.7 to convert similarity scores into a binary matrix. Once the binary similarity matrices for all three action components are obtained, the overall similarity matrix S is calculated through element-wise multiplication.\nWe next perform a matching between Ap and A\u00ba and count the matched pairs for metrics computation. The match- ing algorithm involves three steps: i) iterating over ground- truth actions in chronological order; ii) for each ground-truth action, identifying the first unmatched predicted action that aligns with it (indicated by a 1 in Sij), and treating them as a match; iii) counting the number of matched pairs m."}, {"title": "V. EXPERIMENT RESULTS", "content": "This section presents the experimental results of evaluating the DF and DiffF methods using the ACTONE and ACTREAL datasets. We also undertake a thorough error analysis to identify potential root causes of failures. Additionally, we perform several ablation studies to gain deeper insights into the effectiveness of our proposed methods."}, {"title": "A. Setup", "content": "In our experiments, we implement both DF and DiffF approaches in Python and utilize two prominent series of VLMs: the GPT series (GPT-40/40-mini) [20] and the Gemini series (Gemini1.5-Pro/Flash) [8]. Note that we in- tentionally select a large and small models for each series to study if the model size plays a crucial role in our task. For all VLMs, we set the temperature to 0 and use the default API settings for other parameters. Given that the GPT series permits a maximum of 10 images, we configure the window size to 10 frames, with an overlap of 5 frames in the DF approach. The frame sampling rate is set to one frame per second for ACTONE, whereas ACTREAL employs a rate of 2 frames per second due to the faster pace of user actions in ACTREAL."}, {"title": "B. Results for the ACTONE Dataset", "content": "The evaluation results for ACTONE is given in Table II. It includes the Precision and Recall metrics for the assessment of all three action elements (denoted as \"All\") and a restricted evaluation focusing solely on the operation type (denoted as \"Operation\"). Key observations include: noitemsep, left=0pt\n\u2022 Model Comparison: GPT-40 outperformed all other models across the four metrics for both DF and DiffF\nmethods, followed by Gemini1.5-Pro. The Precision and Recall values, ranging from 0.6 to 0.85, highlight the potential of VLMs for extracting user actions from desk- top recordings. Conversely, the smaller models, GPT- 40-mini and Gemini1.5-Flash, showed a marked decline in performance, underscoring the inherent difficulty of the task.\n\u2022 DF vs. DiffF: DF and DiffF exhibit comparable per- formance, though DiffF shows slightly lower Preci- sion. This observation implies that incorporating explicit frame differences might not be essential for the current VLMs.\n\u2022 Operation vs. All: Performance degradation from eval- uating only operation type to a full evaluation is more significant in smaller models than in larger ones.\nWe also study the breakdown analysis by case domain (i.e., operation type) for both methods with the results for GPT- 40 and Gemini1.5-Pro given in Table VI in the Appendix. The results reveal significant performance variation across operation types for different models and methods, with no clear indication of which operation type is consistently easier to extract. This suggests that these operation types may present comparable levels of difficulty for current VLMs."}, {"title": "C. Results for the ACTREAL Dataset", "content": "The evaluation results for ACTREAL is depicted in Table III, with same column settings as ACTONE. We observe that: noitemsep, left=0pt\n\u2022 ACTREAL vs. ACTONE: The performance of both methods declined on ACTREAL compared to ACTONE, particularly in the \"All\" type metrics. This suggests a notable domain shift between the datasets, with Ac- TREAL presenting greater challenges for VLMs.\n\u2022 DF vs. DiffF: DF shows less decline compared to DiffF, suggesting that DF is better suited for real-world scenarios.\n\u2022 Model Comparison: For DF, GPT-40 outperforms all other models; as for DiffF, Gemini1.5-Pro has the best operation Recall and Precision, whereas GPT-40 has best overall Precision and Recall.\n\u2022 Operation vs. All: For both methods and all models, Pre- cision and Recall under \"All\" conditions significantly decrease compared to \"Operation\" counterpart, unlike in ACTONE. This suggests that extracting details and context is more challenging in ACTREAL. A closer examination of ACTREAL reveals that it includes a"}, {"title": "D. Error Analysis", "content": "We proceed with a detailed analysis of the failure cases in the DF and DiffF methods. Specifically, we focus on cases exhibiting errors in Precision and Recall metrics when applying GPT-40 to the ACTONE dataset. Upon examination, they can be categorized into the following four error types:\n\u2022 Visual Hallucination: VLMs sometimes generates hal- lucinated content when interpreting visual inputs. For instance, the Frame Difference Descriptor in DiffF may incorrectly detect a slight scroll bar movement when it appears or disappears, leading the downstream Action Proposer to falsely suggest a scroll action.\n\u2022 Visual Blindness: VLMs occasionally fail to detect critical UI changes. For example, in a failed case in- volving the drag of a browser tab, the Frame Difference Descriptor in DiffF fails to capture the tab movement.\n\u2022 Inadequate Reasoning: VLMs may exhibit insuffi- cient reasoning over contextual information. It is of- ten observed in the Action Proposer/Corrector when inferring/correcting actions based on prior outputs. In- adequate reasoning typically involves the inability to apply or recognize domain knowledge. For instance, the Action Proposer in DiffF suggests an incorrect click based on a style change in a drop-down menu during mouse hover, which could have been avoided by considering the lack of menu expansion.\n\u2022 Poor Instruction-Following: VLMs can fail to follow complex or lengthy instructions. For example, the Ac- tion Corrector may not update the type action details, even when explicitly prompted to use supplementary data."}, {"title": "E. Ablation Study", "content": "Effectiveness of Action Corrector. We evaluated the Action Corrector's importance in DF and DiffF by removing it when using GPT-40. The results (Row 2 and 3 in Table V) show significant performance drops across almost all metrics, confirming its essential role in both methods.\nImpact of Sliding Window. The sliding window in DF addresses GPT-series input limits, but Gemini-series models have a more relaxed limit, allowing us to test performance without it. As shown in Row 4 of Table V, removing the sliding window reduces Recall but increases Precision. A similar trend occurs without window overlap (not shown), suggesting that without these mechanisms, VLMs produce fewer actions, missing some but reducing irrelevant ones.\nRole of Explicit UI Change Extraction. Comparing DF and DiffF reveals that DF relies solely on VLMs' capabilities, while DiffF depends on explicit UI change descriptions. This raises the question of whether combining original frames with explicitly extracted UI changes could enhance perfor- mance. To explore this, we conduct two ablation studies.\nIn the first study, we add all video frames as extra visual input to the Action Proposer in DiffF. As shown in Row 5 of Table V, while some metrics improve, the gains are marginal.\nIn the second study, we introduce bounding boxes around changed regions in DF's input frames. The results (last row of Table V) show no improvement and even some decline. Detailed analysis reveals that the decline in Precision can be attributed to an overemphasis on localized screen areas, neglecting the broader context.\nThese studies suggest that augmenting VLM attention through explicit UI changes may misguide the model, leading to a narrow focus at the expense of critical global informa- tion. Given DF's strong performance on both ACTONE and ACTREAL datasets, the most effective method for action ex- traction with current VLMs may be to rely on their intrinsic capabilities, as attempts to enhance them with explicit UI change extraction may introduce unnecessary confusion."}, {"title": "VI. DISCUSSION", "content": "The ability of VLMs to extract user actions from desktop recordings opens up significant opportunities. One notable"}, {"title": "VII. CONCLUSIONS", "content": "In this paper, we proposed two novel VLM-based methods for extracting user actions from desktop recordings: the Di- rect Frame-Based Approach (DF) and the Differential Frame-Based Approach (DiffF). Our evaluation shows that the DF approach is more effective, achieving higher accuracy in identifying actions. These methods have significant potential for applications in Robotic Process Automation, video-based tutorials and guideline generation, and user personalization. This work serves as a foundation for future research in desk- top video action extraction, with opportunities for refining VLM capabilities and exploring broader applications."}, {"title": "VIII. APPENDIX", "content": "A. Details of Methodology\nWe provide further details regarding the methodology em- ployed in this work. First, we demonstrate the implemen- tation of the Frame Difference Localizer, as illustrated in the Figure 2. Subsequently, we present an example output from the Frame Difference Descriptor, as depicted in the corresponding Figure 3.\nB. Details of Experimental Results\nWe provide additional experimental details. We first present the case domain-level experimental results on the ACTONE dataset, as shown in the Table VI. The key observations of the results are as follows: For the Direct Frame-Based (DF) approach, from the perspective of assessing all three action elements (ALL), GPT-40 performs best in click- type cases, followed by scroll-type, and performs worst in drag-type cases. Gemini performs best in type-type cases, followed by click-type, and performs worst in select-type cases. Overall, for the DF approach, both models perform better in click, scroll, and type actions, but perform worse in drag and select actions. For the Differential Frame-Based (Diff) approach, GPT-4o performs best in click-type cases, followed by select-type, but performs worst in scroll-type cases. Gemini performs best in scroll-type cases, while its performance in type and click actions is relatively weaker. Overall, for the Diff approach, GPT-40 and Gemini show different strengths: GPT-40 performs better in click and select actions but worse in scroll, whereas Gemini excels in scroll actions.\nThen, we describe the replay results on selected samples from the ACTONE dataset, where we reproduced 9 cases from the dataset, with the results summarized in the Table VII. In the result, two-thirds of the cases can be successfully reproduced.\nC. Prompt\nWe also list all relevant prompts. First, we present the prompt for the action proposer in the DF method, as shown in the Table VIII. Then, we provide the prompt for the corrector in the DF method, followed by the prompt of the Frame Difference Descriptor, as shown in the corresponding Table XI and XIII. Finally, we present the prompts for the Action Proposer and the Action Corrector, as summarized in the Table XV and Table XVIII."}]}