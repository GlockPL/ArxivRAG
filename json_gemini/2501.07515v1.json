{"title": "The Paradox of Success in Evolutionary and Bioinspired Optimization: Revisiting Critical Issues, Key Studies, and Methodological Pathways", "authors": ["Daniel Molina", "Javier Del Ser", "Javier Poyatos", "Francisco Herrera"], "abstract": "Evolutionary and bioinspired computation are crucial for efficiently address ing complex optimization problems across diverse application domains. By mimicking processes observed in nature, like evolution itself, these algorithms offer innovative solutions beyond the reach of traditional optimization meth ods. They excel at finding near-optimal solutions in large, complex search spaces, making them invaluable in numerous fields. However, both areas are plagued by challenges at their core, including inadequate benchmarking, problem-specific overfitting, insufficient theoretical grounding, and superflu ous proposals justified only by their biological metaphor. This overview recapitulates and analyzes in depth the criticisms concerning the lack of in novation and rigor in experimental studies within the field. To this end, we examine the judgmental positions of the existing literature in an informed attempt to guide the research community toward directions of solid contribu tion and advancement in these areas. We summarize guidelines for the design of evolutionary and bioinspired optimizers, the development of experimental comparisons, and the derivation of novel proposals that take a step further in the field. We provide a brief note on automating the process of creating these algorithms, which may help align metaheuristic optimization research with its primary objective (solving real-world problems), provided that our identified pathways are followed. Our conclusions underscore the need for a sustained push towards innovation and the enforcement of methodological rigor in prospective studies to fully realize the potential of these advanced computational techniques.", "sections": [{"title": "1. Introduction", "content": "Bioinspired computation, a prominent area within metaheuristic opti mization research [1], focuses on the design of computational techniques and optimization algorithms that draw inspiration from biological processes and behaviors observed in nature. This inspiration allows such algorithms to solve complex optimization problems in a more efficient and robust manner than traditional solvers. Evolutionary computation [2] plays a central role in this research area, finding its inspiration in principles from Darwin\u2019s Theory of Evolution to tackle intricate optimization problems that often surpass the capabilities of traditional approaches [3, 4, 5]. Both fields have seen signif icant practical advancements [6], demonstrating their potential to improve other models, from classic Machine Learning (ML) pipelines [7] to deep neu ral networks [8, 9].\nRecent studies have anticipated the optimization challenges posed by the advent of General-Purpose Artificial Intelligence Systems (GPAIS) [10], in which the flexibility of bioinspired optimization approaches can address the complexity of GPAIS for several learning tasks. Examples include Generative Adversarial Networks in zero-shot learning [11], prompt evolution for Large Language Models [12], and evolutionary algorithms for transfer learning and pruning in deep neural networks [13].\nThe surge in publications that present new bioinspired optimization algo rithms has motivated efforts to systematically classify this vast body of liter ature in different taxonomies [14, 15], as well as comprehensive overviews of the topic [16, 17, 18]. Several of these contributions have been continuously updated over time, leading to more sophisticated taxonomies that incorpo rate descriptions of novel overviews and methodologies. A notable example"}, {"title": "2. Known Critical Issues: From Lack of Innovation to Low-Quality Experimental Studies", "content": "As mentioned in the introduction, the number of bioinspired solvers has grown significantly in recent years, owing to methods that draw inspiration from natural behaviors or processes. However, many proposals lack innova tion and experimental rigor, hindering the field\u2019s progress. It is paradoxical that the field\u2019s success in terms of publications and proposals is what threat ens its real progress. In agreement with the title of this paper, this section examines the issues leading to this paradox of success: the lack of algorithmic innovation (Section 2.1), the neglect of critical aspects of experimentation (Section 2.2), and the failure to use appropriate benchmarks (Section 2.3)."}, {"title": "2.1. On the Lack of Algorithmic Innovation", "content": "The plethora of bioinspired algorithms available poses a significant chal lenge in choosing the best solver for an optimization problem. In this context, the work of Molina et al. [19] proposes a dual taxonomy according to their inspiration and algorithmic behavior. They highlight that:\n\u201cIn summary, although in the last years many nature-inspired algorithms have been proposed by the community and their number grows steadily ev ery year, more than half of the proposals reviewed in our work are incre mental, minor versions of only three very classical algorithms (Particle Swarm Optimization, Differential Evolution, and Genetic Algorithms). We therefore conclude that the large number of natural and biological sources of inspiration used so far to justify the design of new optimization solvers has not led to significantly disruptive algorithmic behaviors\u201d.\nIn other words, many recently developed algorithms, inspired by natural phenomena or processes/behaviors observed in other fields of knowledge, fre quently recycle existing ideas without introducing algorithmically new meth ods or approaches.\nThis has been a concern for almost a decade. As was discussed in [21], the fallacies of \u201cnovel\u201d metaphor-based methods and the vulnerability of bioinspired research emphasized the need to impose good research practices in the area. In the author\u2019s own words, \u201cmost \u201cnovel\u201d metaheuristics based on a new metaphor take the field of metaheuristics a step backward rather than forward. That study is a \u201ccall for a more critical evaluation of such methods\u201d because \u201cthere are more than enough novel methods, and there is no need for the introduction of new metaphors just for the sake of it\u201d.\nBefore Sorensen\u2019s manifesto was published, other researchers [29, 30] had also addressed the lack of innovation of specific bioinspired solvers, claiming that the diversity of their natural inspirations did not extend to mathemati cal differences with previous evolutionary algorithms. Weyland [29] (together with a series of follow-up rebuttals and responses [31, 32]) provided a compre hensive review of Harmony Search, and critically analyzed its developments over the past decade. He argued that this bioinspired solver, often seen as a significant innovation, is essentially a special case of Evolution Strategies.\nThe second paper [30] focused on a bioinspired approach, called Black Hole Optimization, demonstrating that it is, in fact, a \u201csimplification of the well known Particle Swarm Optimization with inertia weight\u201d. Furthermore, the authors express their concern about the fact \u201cthat such an approach may jeopardize achievements in the field of Evolutionary Computing\u201d, since Black Hole Optimization is a simplification that produces favorable results in spe cific benchmark tests.\nLater, the work in [33] discussed this phenomenon in relation to the number of papers published, arguing that \u201cthe new population-based nature inspired algorithms are released every month and, basically, they have noth ing special and no novel features for science\u201d. Their conclusions are aligned with the taxonomy published in [19], because \u201cour research revealed that the process of the new population-based nature-inspired algorithm possesses the behavior of the swarm intelligence paradigm\u201d, revealing that the category with the highest number of contributions is Swarm Intelligence.\nThereafter, in [22] a meticulous review of the trends of new algorithms was performed, providing additional criticisms that apply to most of these proposals:"}, {"title": "2.2. On the Low Quality of Experimental Studies", "content": "Another significant concern in bioinspired and evolutionary computation is the quality of the experimental benchmarks. A substantial number of proposals in the literature conduct insufficient comparative studies to assess the performance of the developed algorithms. The primary issue is their failure to compare with the state-of-the-art methods and similar solvers. To demonstrate any improvements, it is essential to perform comparisons against competitive algorithms and validate that the individual components of the proposals genuinely contribute to enhancing overall performance. However, this is not the case in many of such studies. as underscored in [19]:\n\u201cwhen new algorithms are proposed, unfortunately, many of them are only compared to very basic and classical algorithms (such as Genetic Algo rithms or Particle Swarm Optimization). These algorithms have been widely surpassed by more advanced versions over the years, so obtain ing better performance than the naive version of classical algorithms is relatively easy to achieve, and it does not imply competitive performance\u201d.\nFurthermore, the set of competitive algorithms should be chosen consid ering the state-of-the-art of each benchmark [37], as the behavior of different bioinspired and evolutionary algorithms can largely depend on the charac teristics of the benchmark under consideration [38]. In this regard, a recent study by Piotrowski et al. [39] concludes that \u201cin the present paper we show that the choice of the set of benchmarks used for the comparison may greatly affect the ranking of optimizers\u201d.\nAnother common criticism expressed against many modern bioinspired algorithms is that the algorithm itself is too complex. A reduction in the number of algorithmic components would facilitate a more comprehensive understanding and analysis of the algorithm\u2019s constituent parts. In this line of reasoning, the extensive experiments carried out in [40] showed that \u201csome meta-heuristics have to be simplified because they contain operators that structurally bias their search by favoring sampling from some parts of the decision space\u201d. Moreover, this study analyzed the L-SHADE-EpSin al gorithm, which is a step-by-step designed algorithm developed from differ ent variants of Differential Evolution. In their experimentation, the authors proved that \u201cthe proposed simplified variant of L-SHADE-EpSin is highly competitive in a wide collection of artificial benchmarks and real-world prob lems\u201d compared to other bioinspired algorithms and the non-simplified ver sion of this algorithm."}, {"title": "2.3. On the Use of Poor Benchmarks", "content": "Many studies lack an appropriate and unbiased benchmark, using their own comparison functions, which limits the generalizability of their findings to real world problems. This practice may introduce biases that distort the accurate assessment of the performance of any new optimization algorithm, leading to misleading conclusions.\nIn this context, the work in [41] examines the different types of biases present in common metaheuristic algorithms. Among them, the study con cludes that origin bias and center bias are the most recurrent ones, due to the center bias operators that appear in some bioinspired and evolutionary algorithms. This phenomenon has been thoroughly studied in [42] by system atically analyzing this behavior between several bioinspired algorithms. The reported experiments show a significant tendency in most modern bioinspired solvers to explore mainly the center of domain search. Thus, the majority of these algorithms generate misleading performance results because they are compared considering objective functions whose global optima are located at the center of the solution space. Most of these biases can be avoided by resorting to specific benchmarks proposed over these years, as explained in Section 4.\nOne reflection we cannot lose sight of within this matter is that bench mark performance cannot be a goal in itself. Benchmarks are a valuable tool for the design and evaluation of algorithms that can potentially perform well in real-world problems. Similarly to [24], Ceberio and Calvo [43] discuss \u201cthe role of experimentation in the two approaches of conducting research: engineering vs. scientific\u201d, with thoughtful remarks about benchmarks and experimentation details, and present their understanding of the fundamental principles of both approaches. For this disparity, Kudela proposes the incor poration of a greater number of real world problems into the benchmark [42], by \u201cthe construction of a large set of challenging real-world benchmark prob lems\u201d. In this way, the author advocates for the establishment of a repository that includes the following:\n1. \u201cseveral heterogeneous benchmark sets with a unified way of calling the test problems\u201d;\n2. \u201ctrusted implementations (source codes) of both standard EC methods and up-to-date state-of-the-art techniques\u201d;\n3. \u201cdata obtained from running the algorithms (from (2)) on the benchmarks (from (1))\u201d.\nAlternatively, another methodological path that can be followed when benchmarking bioinspired solvers is to continue using synthetic functions."}, {"title": "3. Separation of Wheat from Chaff: Distinction of Weak Proposals", "content": "As noted previously, the field of bioinspired algorithms remains problem atic due to malpractices and ineffective metaphorical designs, which often result in algorithms that provide no scientific value for the field, for lacking sufficient innovation or flawed experimental designs. We refer to these algo rithms as weak proposals. In Section 3.1, we describe several key studies that expose these weak proposals that do not incorporate innovation. In Section 3.2, we propose several pathways to carry out studies to identify these weak proposals in the literature, or to avoid these problems when designing new proposals."}, {"title": "3.1. On the Distinction of Weak Proposals", "content": "Fortunately, in recent years, several authors have recognized the need for a more rigorous development of new proposals along with a robust experimental framework, and identify in the literature several well-known proposals whose contribution in the field could be considered questionable, or weak proposals.\nAnalyzing and identifying lack of novelty in bioinspired optimization al gorithms has been a focus since early works (cf. [29, 30]). These early studies built a foundation for evaluating particular solvers in the field, and their influ ence has persisted since their publication. An example is Teaching-Learning Based-Optimization, which was analyzed in [51]. This study found that the algorithm has a bias towards the origin during teaching, which increases as the population converges. In addition, the Gravitational Search Algorithm was analyzed in [52], finding that the force model in the algorithm depends only on agents\u2019 masses, not distances, contradicting the law of gravity.\nYears after these studies, Camacho Villal\u00b4on et al. [53] provided evidence that the Grey Wolf, Firefly, and Bat Algorithms are not novel, but rather reformulations of ideas initially introduced for Particle Swarm Optimization and subsequently refurnished. In order to prove this claim, the search opera tors of such solver were rewritten and mathematically compared, concluding that they can all be regarded as existing variants of the Particle Swarm Optimization algorithms. In the studies mentioned in [54, 55, 56], several bioinspired algorithms, including the Intelligent Water Drops and Cuckoo algorithms, are critically examined. The authors argue that the metaphors used to justify these algorithms are often oversimplified or misapplied, lead ing to misleading or even deceptive claims regarding their novelty and effec tiveness. Another example is [36], analyzing the Raven Roost Optimization. The methodology of this paper involved a detailed examination of both the mathematical formulation and an analysis of the behavior, exposing a serious weakness (namely, an inherent search bias towards the starting point).\nIn the last couple of years, a notable increase in studies critically analyz ing certain bioinspired algorithms by comparing them to others by examin ing their individual components or operators has been observed. Camacho Villal\u00b4on et al. [57] contend that the well-known Grey Wolf, Moth-flame, Whale, Firefly, Bat, and Antlion algorithms are not actually novel. A com prehensive component-based analysis of each algorithm to substantiate this assertion, these algorithms are identified as variants of Particle Swarm Opti mization and Evolution Strategies. In this context, the work of Molina et al. [19] categorizes various bioinspired algorithms based on their closest classical solver in terms of algorithmic structure. This analysis highlights that the distinctions between many bioinspired algorithms and their classical coun terparts are often insufficient at the algorithmic level.\nThe Salp Swarm Optimization Algorithm is a further example of a bioin spired algorithm that has recently been subject to criticism. Castelli et al. [58] identify several problems with this algorithm, which can be broadly classi fied according to its update rule, its physical motivation, and the discrepancy between their description and its available implementation. In addition, the study demonstrates that the original algorithm exhibits a bias towards the center, a point that was previously discussed. This bias toward the center was also detected in the Arithmetic Optimization Algorithm, analyzed in [35] in terms of its mathematical formulation. As with other solvers, the lack of shifted versions of standard functions hinders the detection of this bias. Whale Optimization Algorithm has recently entered this list of weak algorithms Deng and Liu [59], proving the existence of a center-bias operator at the core of its design. In [60], several algorithms are examined to discern whether their mechanisms produce desirable qualities in their respective do mains. However, the primary findings of this study reject this hypothesis, as recent algorithms do not consistently align with the behavior or phenomenon on which they are based.\nAnother way to identify weak proposals is through experiments that un fairly enthrone algorithms with non-desired behaviors. An example is the work in [61], which reveals the center bias present in many bioinspired algo rithms by analyzing more than 100 bioinspired and evolutionary algorithms. The proposed methodology is to conduct experimental studies on functions whose optimal solution is situated at the center of the feasible set of solu tions, and modifying them with specific displacements to avoid that. Under this methodology, it lists several algorithms that should not be used due to their strong center bias, including well-known proposals such as the Grey Wolf Optimizer, Dragonfly Optimization, Whale Optimization Algorithm, and Bird Swarm Algorithm. This work also recommends avoiding other al gorithms for their lack of algorithmic novelty, including Harmony Search and Cuckoo Search, among others. Another interesting work is [41] which, be sides classifying different types of bias, proposes several techniques to detect structural bias in the exploration of an evolutionary algorithm. In addition, it provides an extensive list of different algorithms with a strong structural bias identified in the literature. The most common bias is toward the center, but there are also biases towards the boundaries and/or edges of the domain search, among others."}, {"title": "3.2. Pathways to Detect Weak Proposals", "content": "Through these reviews, it becomes evident that there is a growing interest in adequately comparing new proposals with existing ones in the literature. Based on previous examples, we have examined the methodologies used for this type of straw-grain discrimination studies, and we propose pathways that researchers can embrace to enforce a greater rigor in the proposal of new algorithms, detecting their lack of novelty:\n\u2022 Equation-level equivalence, either at the operator or at the component level. This method involves comparing the algorithmic and mathematical descriptions of the search operators of various algorithms towards detect ing a possible lack of diversity among them. Techniques for this first pathway include:\n\u2013 Conducting homologous component studies by analyzing general ex pressions of previously established algorithms to identify similarities and differences in their structure and behavior [25].\n\u2013 Utilizing formal verification methods to rigorously evaluate algorithmic redundancy [62]. The process begins by formally representing the algo rithms to model their structure and behavior. Then, automated tools such as model checkers or theorem provers are employed to identify equivalences or redundancies by systematically comparing algorithmic components and behaviors.\n\u2013 Determining equivalence through operator simplification. This case im plies analyzing whether certain components or operators in a bioin spired algorithm contribute substantially to its performance. To assess the impact of individual components, ablation tests should be employed. These tests systematically remove or modify parts of the algorithm to evaluate their specific contribution to the search performance (or to any other quality aspect that is relevant for the problem(s) at hand).\n\u2013 Examining the standard definitions of operators to ensure that differ ences in terminology are not mistakenly interpreted as evidence of dif fering algorithmic behavior. By systematically analyzing and standard izing operator definitions, the community can more effectively compare meta-heuristic algorithms [63, 64]. In addition, it can be applied to compare different implementations of the same algorithm across differ ent software libraries.\n\u2022 Configuration-level equivalence. This pathway refers to the equivalence between two algorithms when specific values of the parameters controlling their search operators can cause them to behave in a very similar manner. The challenge lies in determining when the behavior of one algorithm can be reduced to or generalized by another. Several results can produce different scenarios:\n\u2013 Non-innovative algorithm: If the performance of an acclaimed novel solver is essentially identical to an existing algorithm under a certain set of parameter values.\n\u2013 Generalization of an existing solver: If the new bioinspired solver in troduces additional flexibility in its parameters or expands its search capabilities while maintaining equivalence with an earlier algorithm for specific parametric settings, it must be considered a more general ver sion of the original algorithm, making the original method redundant.\n\u2013 Genuinely innovative algorithm: If the novel algorithm introduces sig nificant differences in performance or behavior that cannot be replicated"}, {"title": "4. Fair and Right Comparisons in Bioinspired and Evolutionary Optimization: Replicability and Benchmarks", "content": "To ensure that a proposal for a new bioinspired algorithm advances in the field with originality and practical impact, it is crucial to establish a well-defined experimental section that enables an impartial evaluation of its performance. This is typically associated with improving (or at least match ing) the performance of state-of-the-art algorithms. However, the potential contribution of a new proposal is not necessarily limited to an improvement of its search performance (convergence speed or quality of solutions) with respect to the considered counterparts in the benchmark.\nIn Section 4.1, we outline several considerations that must be made to ensure that the final aim of bioinspired and evolutionary computation is to successfully solve real-world problems. In Section 4.2, we propose several pathways to guarantee fair and right comparisons. In Section 4.3, we under score the importance of replicability in experimentation. Finally, Section 4.4 emphasizes the importance of using appropriate benchmarks."}, {"title": "4.1. Considerations in Real-World Problems", "content": "When using optimizers to address real-world problems, certain factors are of particular importance. In this regard, Osaba et al. [49] outline the distinct requirements at various stages of the process, including design, development, experimentation, and deployment, to effectively address real-world optimiza tion problems. They identified multiple challenges inherent to the design of new bioinspired algorithms and proposed a series of steps to be followed throughout the algorithm\u2019s development. These steps range from the initial phase of problem modeling to the final validation of the algorithm:\n\u2022 Problem modeling and mathematical formulation: It is recommended to begin with a thorough understanding of the real-world context and con ceptualization of the problem, which should end in a formal mathematical formulation of the objectives, decision variables, and constraints involved in the problem. This mathematical formulation is essential for determin ing the structure of the problem and guiding the selection or development of suitable search algorithms. An important outcome of this step is a list of collected functional and non-functional requirements for the solver to be developed, which must be considered towards the design or selection of the solver and accounted for during the rest of the steps.\n\u2022 Algorithmic design, solution encoding and search operators: Once the problem is clearly defined, the next step is to design an algorithm that can efficiently search the solution space. This involves determining how to encode potential solutions, which is critical in bioinspired algorithms, as it directly affects how the algorithm explores and exploits the search space. In addition, it is important to ensure that the algorithm design aligns with the specific constraints and requirements of the problem being solved. This step may involve adapting existing metaheuristics or devel oping new search operators that better fit the problem\u2019s characteristics. The design must consider the functional and non-functional requirements collected during the first step of the process, including factors such as speed, simplicity, deployability in specialized software/hardware, or the possibility of paralleling the implementation of the designed search oper ators, among others.\n\u2022 Performance assessment, comparison and reproducibility: This phase is dedicated to evaluating the effectiveness of the proposed algorithm. The performance of the algorithm should be rigorously assessed using stan dard benchmarks or real-world data to determine if it meets the desired requirements. Comparisons with other state-of-the-art algorithms help contextualize their performance. Furthermore, replicability is a crucial factor, ensuring that the results are consistent and can be reproduced in different contexts. This assessment should also account for the specific needs and priorities of the real-world problem, such as computational effi ciency, scalability, and robustness, which may be just as important as the raw performance of the optimization algorithm.\n\u2022 Algorithmic deployment for real-world applications: The final step involves"}, {"title": "4.2. Pathways to ensure Fair and Right Experimental Comparisons", "content": "The community is increasingly aware of the significant flaws exhibited by many new proposals, particularly in the experimental phase of their re search. Researchers, like LaTorre et al. [26], have proposed methodological approaches to support more solid and principled experimentation in bioin spired and evolutionary computation. In this specific work, such guidelines can be summarized as follows:\n\u2022 Choice of benchmarks and algorithms for comparison: Benchmarks are a fundamental aspect of algorithm evaluation. As discussed in Section 2.2, custom optimization functions can produce misleading conclusions. A more reliable analysis could be achieved by switching to a better, more standard benchmark or expanding to a larger set of problems [65]. LaTorre et al. [26] advocate for the use of well-designed standard benchmarks that include diverse test functions, representing a broad range of real-world problems. Comparisons with state-of-the-art algorithms should extend beyond just solution quality, incorporating aspects such as efficiency, sim plicity, and convergence speed. This kind of analysis can guide future research and improvements in the algorithm\u2019s design."}, {"title": "4.3. Pathways to support the Replicability of Experimental Studies", "content": "This is particularly relevant due to the questionable reproducibility or replicability of the results reported for many bioinspired algorithms in the lit erature. Replicability is one of the motivations at the heart of the framework proposed in [64] to support the verifiable comparison of new meta-heuristic approaches. This framework hinges on several components:\n\u2022 Extensible and re-usable templates: These templates are designed to offer flexibility by allowing researchers to configure their behavior through an extensible palette of components. This modular approach eliminates the need for altering existing code, making it easier to adapt, extend, and repurpose algorithmic components for a variety of problems.\n\u2022 White-box problem descriptions: A description of the problem with an alytic information that could be used to guide the algorithm\u2019s selection or construction. Providing detailed transparent descriptions of optimiza tion problems, including analytic information such as objective landscape characteristics or known constraints, can guide the selection or design of operators that are well-suited to the specific problem under consideration.\n\u2022 Remotely accessible frameworks, components, and problems: Hosting algo rithm frameworks, optimization components, and a curated set of bench mark problems on a remotely accessible platform can foster reproducibil ity and collaborative research. Researchers could directly access them, enabling widespread reuse, replicability, and shared knowledge discovery."}, {"title": "4.4. Pathways to guarantee Proper Benchmarks", "content": "The limited variety of test functions commonly used in benchmarks within the community is another issue that has garnered research attention. A recent advancement in this direction is presented in [68], which provides extensive information on practical scenarios for the development of novel optimiza tion algorithms. The review covers more than two hundred mathematical functions and more than fifty real-world engineering design problems. Rec ognizing the critical need for robust experimental evaluations to accompany the design of bioinspired algorithms, this work offers a comprehensive array of options for assessing the quality and effectiveness of new proposals.\nConcerns about design biases in optimization research have increasingly drawn attention in recent years. As highlighted in [42], the center bias can distort the results and misrepresent the true performance of algorithms. To"}, {"title": "5. Methodologies and Pathways for Improving Existing Bioinspired Optimization Algorithms", "content": "In the comprehensive review by Velasco et al. [18], it is revealed that 65% of the studies analyzed focus on improved versions of established algorithms, signaling a shift away from creating bioinspired algorithms based on novel analogies. This review examines over a hundred recent studies containing terms such as \u201cnew\u201d, \u201chybrid\u201d, or \u201cimproved optimization\u201d in their titles.\nThe vast majority of these proposals are actually refinements of existing solvers.\nUnfortunately, the study in [18] underscores a relevant issue: most of these algorithms address engineering problems that lack direct applicabil ity to real-world scenarios. Addressing this gap is the focus of the current section: to explore potential pathways for enhancing an existing bioinspired solver, making it more suitable for addressing specific problems or families of problems with greater relevance and utility. Such pathways are enumerated and described in what follows:\n\u2022 Automatic Equivalence between algorithms: We begin with a common methodological approach, to add mechanisms or change existing ones for improved versions, thereby increasing the complexity of the model. When this is the case, ablation tests are highly recommended to assess the rela tive contribution of such added mechanisms and/or modifications to the acclaimed performance improvement.\nGiven that ablation tests can be very time-consuming, several method ologies have recently emerged to automate this process. Among them, we highlight [71], which discusses how to automatically detect equivalence be tween stochastic population-based nature-inspired algorithms considering both a conceptual level \u2013 using the objective function value \u2013 and an oper ational level, expressed in terms of a measure of the population diversity. Their notion of equivalence requires that the average fitness values and the diversity of the population must not be significantly different in each generation for two algorithms under comparison to be equivalent to each other. Through the use of Markov chains as a representation of popula tion and fitness, this methodology can assess the similarity of bio-inspired algorithms.\n\u2022 Decomposition into their canonical components: This is the approach fol lowed in [72], which presented a methodology for the decomposition of bioinspired algorithms. Specifically, the methodology considers a set of templates that act as a framework to decompose and analyze bioinspired algorithms based on their characteristics and in a structured manner. In this pool of templates, we find different categories of components, such as the method of generating initial solutions, the solution pool itself, the solution archive, update mechanisms, input and output functions of the solution pool, and others. With all this, a similarity analysis between"}, {"title": "6. A Smart and Promising Solution for overcoming Known Critical Issues: Automated Design of Metaheuristic Algorithms", "content": "Advancing towards the automated design of metaheuristic algorithms has marked a significant leap in bioinspired and evolutionary optimization. This involves refining individual algorithms and automating their creation. The automated design of metaheuristic algorithms can provide breakthroughs in various scientific and engineering domains, making the automated design of metaheuristic algorithms the norm when addressing real-world optimization problems.\nIn this section, we describe works that tackle this automated design pro cess. First, in Section 6.1, we revisit several notable proposals to optimize the design. Next, Section 6.2 pauses at several recent contributions that incor porate Large Language Models (LLMs) for the design of new metaheuristic algorithms."}, {"title": "6.1. Automating the Design of Metaheuristic Algorithms", "content": "Camacho-Villal\u00b4on et al. [27] explore whether manually designing or au tomating the construction of metaheuristic algorithms can be a superior ap proach, yielding the following main observations:\n\u2022 The manual design process often involves seeking inspiration from other fields of knowledge and iteratively refining algorithms based on trial and"}, {"title": "7. Conclusion and Outlook", "content": "In this paper, which we graphically summarize in Figure 1, we have criti cally examined challenges in evolutionary and bioinspired optimization, and outlined methodological ways to address them effectively. We have empha sized the need to differentiate truly innovative contributions (the wheat in the chaff ), and have highlighted the importance of robust, non-biased ex perimental practices. Through recent examples in the literature, we have showcased strategies for the design, automated construction, improvement, and evaluation of bioinspired solvers, ensuring a more transparent assess ment of the components of new proposals, and their fairer and more solid comparison to established metaheuristic methods.\nThe analyzed pathways are designed to refocus optimization research on its ultimate goal: developing algorithms capable of efficiently solving real world problems. We strongly encourage the research community to adopt these recommendations as essential guidelines for achieving this objective. Researchers, editors, and reviewers must actively integrate the methodologies"}]}