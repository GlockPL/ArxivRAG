{"title": "Decoupling Knowledge and Reasoning in Transformers: A Modular Architecture with Generalized Cross-Attention", "authors": ["Zhenyu Guo", "Wenguang Chen"], "abstract": "Transformers have achieved remarkable success across diverse domains, but their monolithic architecture presents challenges in interpretability, adaptability, and scalability. This paper introduces a novel modular Transformer architecture that explicitly decouples knowledge and reasoning through a generalized cross-attention mechanism to a shared knowledge base, specifically designed for effective knowl-edge retrieval. Critically, we provide a rigorous mathematical derivation demonstrating that the Feed-Forward Network (FFN) in a standard Transformer is a specialized case (a closure) of this generalized cross-attention, revealing its role in implicit knowledge retrieval and validating our design. This theoretical framework provides a new lens for understanding FFNs and lays the foundation for future research exploring enhanced interpretability, adaptability, and scalability, enabling richer interplay with external knowledge bases and other systems.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) based on the Transformer architecture have achieved remarkable success in various natural language processing tasks [19, 6, 14, 3]. However, their monolithic architecture, where knowledge and reasoning are deeply intertwined, presents critical challenges for real-world applications requiring transparency, adaptability, and continuous learning.\nSpecifically, current Transformers face limitations in:\n\u2022 Interpretability. Understanding how Transformers arrive at conclusions is difficult [4, 20]. The distributed nature of knowledge representation, particularly within Feed-Forward Networks (FFNs), hinders pinpointing the information used for specific predictions, limiting their use in high-stakes applications like healthcare and legal reasoning.\n\u2022 Adaptability. Adapting pre-trained models to new knowledge or integrating them with external systems is inefficient and complex. Current methods like RAG [13], which simply concatenate retrieved context with input, suffer from context dilution and static, input-level retrieval, often resulting in insufficient or excessive information. This hinders continuous learning and the ability to incorporate rapidly changing information, such as real-time news or scientific discoveries.\n\u2022 Scalability. The tight coupling of knowledge and reasoning in monolithic Transformers prevents independent scaling, leading to disproportionate parameter growth and hindering large-scale and continuous learning, unlike the additive scaling observed in human cognition, where people can efficiently transfer existing reasoning capabilities to new knowledge. This \"parameter explosion\" limits accessibility of larger models, hindering progress towards truly comprehensive knowledge-driven AI."}, {"title": "Challenges of Monolithic Transformers", "content": "This section outlines the limitations of the standard Transformer architecture, focusing on the conceptual challenges arising from the monolithic nature of these models, particularly concerning the entanglement of knowledge and reasoning."}, {"title": "The Decoder-Only Transformer Baseline", "content": "We focus our analysis on the decoder-only Transformer architecture [19, 14, 3], which has become the dominant architecture for large language models. A decoder-only Transformer, as illustrated in Figure 1 (a), consists of stacked decoder blocks, each containing two main sub-layers:\n\u2022 Masked Multi-Head Self-Attention. This layer allows each token in the input sequence to attend to all preceding tokens (including itself), capturing contextual relationships within the sequence. The \"masked\" aspect prevents the model from attending to future tokens during training, ensuring autoregressive behavior.\n\u2022 Feed-Forward Network (FFN). This layer consists of two linear transformations with a non-linear activation function (typically GeLU or ReLU) in between. It processes each token's representation independently."}, {"title": "Limitations of the Monolithic Architecture", "content": "The monolithic architecture of standard Transformers, where knowledge and reasoning are deeply intertwined within the model's parameters, presents several key conceptual challenges:\n\u2022 Intertwined Knowledge and Reasoning. In standard Transformers, knowledge is implicitly encoded within the weights of the attention matrices and FFNs. This entanglement entangles knowledge, making it difficult to isolate, analyze, or update specific information without unintended consequences. This lack of transparency makes it hard to determine whether errors stem from a lack of knowledge, flawed reasoning, or complex interactions between the two, hindering targeted improvements. For instance, the FFN for the word \"bank\" might encode both the concept of a financial institution and the side of a river, making it difficult to disambiguate in different contexts.\n\u2022 Adaptability and Non-Modularity. Adapting pre-trained models to new knowledge or integrating them with external systems is inefficient and complex. Current retrieval-augmented methods like RAG attempt to improve adaptability by retrieving relevant context and concatenating it with the input query. However, this approach has several inherent limitations. First, simply adding more text through concatenation can lead to \"context dilution\", where the factual knowledge and user question are mixed, confusing the LLM. Second, because retrieval and reasoning remain entangled within the model, it becomes difficult to understand how the retrieved knowledge is actually being used. Finally, due to RAG's single initial re-trieval and the limited input window of LLMs, the retrieved information is often insufficient or excessive. In contrast, without any retrieval mechanism like RAG, incorporating new information becomes even more challenging, requiring significant resources and risking disruption to existing knowledge.\n\u2022 Scaling Challenges. The monolithic structure of Transformers presents significant scaling challenges. Because knowledge and reasoning are deeply intertwined within the model's parameters, increasing one capacity necessitates significant adjustments to the other, prevent-ing independent scaling and leading to disproportionate parameter growth and substantial computational and memory costs. This \"parameter explosion\" makes training and deploying"}, {"title": "A Modular Architecture with Explicit Knowledge Decoupling", "content": "To address these limitations, we propose a novel modular architecture (illustrated in Figure 1 (b)) that explicitly decouples reasoning and knowledge by introducing a shared knowledge base $E \\in \\mathbb{R}^{|E|\\times d_E}$ across all layers, where $|E|$ is the number of knowledge entries and $d_E$ is the dimensionality of each entry. E is accessed via dedicated cross-attention mechanisms. It is important to note that, in this work, we focus on the theoretical framework and the case where E is trained jointly within the model. The exploration of external, pluggable knowledge bases is left for future work."}, {"title": "The Proposed Architecture", "content": "Our modular Transformer block replaces the standard Feed-Forward Network (FFN) with a cross-attention layer that attends to E. This design choice is motivated by the hypothesis that FFNs in standard Transformers implicitly perform a form of context-dependent knowledge retrieval. By making this implicit process explicit, we aim to gain significant advantages in terms of interpretability, adaptability, and scalability in future work with external knowledge bases.\nSimilar to the multi-head self-attention layer in a standard Transformer, each attention head within our modular blocks learns distinct projection matrices $(W_q^l, W_k^l, W_v^l)$, effectively focusing on different aspects or subsets of information within E. This mechanism enables dynamic, in-context knowledge retrieval, allowing the model to access and integrate relevant information from E at each layer of processing.\nFormally, let $H_l \\in \\mathbb{R}^{N\\times d}$ be the output of the multi-head self-attention layer in the l-th block, where N is the sequence length and d is the hidden dimension. The knowledge retrieval process in this block is defined as (skipping details for multi-heads for clarity):\n$Q_l = H_l W_q^l$ (1)\n$K_l = E W_k^l$ (2)\n$V_l = E W_v^l$ (3)\n$C_l = GeneralizedAttention (Q_l, K_l, V_l)$ (4)\nwhere $W_q^l \\in \\mathbb{R}^{d\\times d_k}$, $W_k^l \\in \\mathbb{R}^{d_E\\times d_k}$, and $W_v^l \\in \\mathbb{R}^{d_E\\times d}$ are the query, key, and value projection ma-trices specific to the l-th block, respectively. $E \\in \\mathbb{R}^{|E|\\times d_E}$ represents the knowledge base. Function GeneralizedAttention with its output $C_l \\in \\mathbb{R}^{N\\times d}$ will be described in detail in Section 4. The output of the l-th modular block is then:\n$H_l' = H_l + C_l$ (5)\nThis modular design, even in the joint-training setting considered in this paper, provides a valuable conceptual framework for understanding knowledge retrieval in Transformers and has significant implications for future work with external knowledge bases.\n\u2022 Explicit Knowledge Representation (Joint Training). Knowledge is now represented in a separate, explicitly accessible module (E), even though it is trained jointly within the model in this work. This explicit representation provides a clearer conceptual framework for understanding which information is being used for a given prediction."}, {"title": "Generalized Cross-Attention for Knowledge Retrieval", "content": "This section introduces a generalized cross-attention mechanism designed for knowledge retrieval from a knowledge base $E \\in \\mathbb{R}^{|E|\\times d_E}$, where $|E|$ is the number of knowledge entries and $d_E$ is the dimensionality of each entry. Standard attention mechanisms, while effective for self-attention, are suboptimal for knowledge retrieval due to the need for selective access and the connection of distinct embedding spaces. Effective knowledge retrieval requires: (1) determining the relevance of knowledge entries to a given context (Relevance/Selection), and (2) determining how the selected knowledge should be integrated into the model's representation (Integration/Transformation). We progressively build upon the standard attention mechanism to address these aspects.\nLet $H_l \\in \\mathbb{R}^{N\\times d}$ be the input from the previous layer, where N is the sequence length and d is the hidden dimension. Let $Q_l \\in \\mathbb{R}^{N\\times d_k}$, $K_l \\in \\mathbb{R}^{|E|\\times d_k}$, and $V_l \\in \\mathbb{R}^{|E|\\times d}$ be the query, key, and value matrices, respectively."}, {"title": "Phase 1: Selective Retrieval with Sparse Activation", "content": "Standard attention computes a weighted average of the values based on a softmax over the query-key similarities:\n$Attention(Q_l, K_l, V_l) = softmax(\\frac{Q_l K_l^T}{\\sqrt{d_k}})$ (6)\nHowever, softmax assigns non-zero weights to all knowledge entries, hindering selective knowledge retrieval. To enforce sparsity, we replace softmax with a sparse activation function, such as ReLU:\n$Attention_{ReLU}(Q_l, K_l, V_l) = ReLU(\\frac{Q_l K_l^T}{\\sqrt{d_k}})$ (7)\nThis element-wise application of ReLU on the similarity matrix thresholds values, enforcing sparsity in the attention matrix. Other sparse activations (e.g., Leaky ReLU, Sparsemax) can also be used."}, {"title": "Phase 2: Knowledge-Specific Thresholding (\"IF\" Condition)", "content": "While ReLU introduces sparsity, it applies a uniform threshold of zero to all knowledge entries. This is suboptimal because different entries have varying levels of relevance. We introduce a knowledge-specific thresholding function $B_l^1(E) \\in \\mathbb{R}^{N\\times |E|}$. This can be interpreted as an \"IF\" condition: IF the relevance score (from $Q_l K_l^T$) for a specific knowledge entry exceeds its corresponding threshold from $B_l^1(E)$, THEN the knowledge entry is considered; otherwise, it is filtered out. We currently implement $B_l^1(E)$ using a Multilayer Perceptron (MLP) applied to each knowledge entry embedding.\n$Attention_{ReLU+Threshold}(Q_l, K_l, V_l) = ReLU(\\frac{Q_l K_l^T}{\\sqrt{d_k}} + B_l^1(E))$ (8)"}, {"title": "Phase 3: Transformation Bias for Semantic Bridging", "content": "The value matrix $V_l$ represents a transformed view of the knowledge entries, acting as the \"THEN VALUE\" part of the \"IF-THEN\" logic. This transformation is already handled by $W_v^l$ in $V_l = E W_v^l$, which extracts relevant features from the knowledge entries. Furthermore, unlike self-attention where query and value are from the same embedding space, our generalized cross-attention connects distinct embedding spaces: one for $H_l$ and the other for E. To further bridge this semantic gap by aligning the transformed knowledge representation with the context representation, we introduce a transformation bias $b_l^2 \\in \\mathbb{R}^d$ that is added to the weighted values after the thresholding.\n$GeneralizedAttention(Q_l, K_l, V_l) = ReLU(\\frac{Q_l K_l^T}{\\sqrt{d_k}} + B_l^1(E)) V_l + b_l^2$ (9)\nIn summary, our generalized cross-attention mechanism addresses the requirements of knowledge retrieval by introducing: (1) ReLU for selective retrieval, (2) knowledge-specific thresholding $B_l^1(E)$ as an \"IF\" condition, and (3) a transformation bias $b_l^2$ for semantic bridging. This design enables more interpretable, effective, and targeted knowledge retrieval compared to standard attention."}, {"title": "FFN is a Closure of Generalized Cross-Attention", "content": "This section establishes a crucial theoretical link between our proposed modular architecture and the standard Transformer architecture. We demonstrate that the Feed-Forward Network (FFN) within a Transformer block can be interpreted as a specialized case of our generalized cross-attention mechanism.\nConsider the generalized cross-attention as a function with two arguments: a query $H_l$ and a knowledge base E:\n$Cross-Attention(H_l, E)$ (10)\nOur key finding is that the FFN can be expressed as a closure of this function:\n$FFN(H_l) = Cross-Attention(H_l, Implicit E)$ (11)\nWhere \"Implicit E\" represents the knowledge encoded within the Transformer's parameters. This 'Implicit E' can be understood as a highly compressed representation of knowledge learned during pre-training, encoded within the weights of the FFN spanning all decoder layers. This representation highlights that the FFN performs implicit knowledge retrieval from a built-in knowledge base. This connection provides a strong theoretical justification for the effectiveness of FFNs and simultaneously validates the design of our generalized cross-attention mechanism. Critically, this derivation provides a formal basis for the key-value memory interpretation of FFNs proposed by Geva et al. [8]."}, {"title": "Derivation of FFN from Generalized Cross-Attention", "content": "To establish a connection with the standard FFN formulation, which operates on fixed weights, we consider the scenario where E is static during inference (and, in the joint-training case considered in this paper, static during training as well). In this case, the generalized cross-attention mechanism simplifies significantly. Recall equation 9:\n$C_l = ReLU(\\frac{Q_l K_l^T}{\\sqrt{d_k}} + B_l^1(E)) V_l + b_l^2$ (12)\nwhere:"}, {"title": "Connection to Standard FFN", "content": "The standard FFN in a Transformer block is defined as:\n$FFN(H_l) = ReLU(H_l W_1 + b_1) W_2 + b_2$ (22)\nComparing this to the derived equation for $C_l$ (Equation 21), we confirm the functional equivalence $FFN(H_l) = Cross-Attention(H_l, Implicit E)$ under the assumption of a static knowledge base E. By setting:\n$W_1 = W_{(Q,K,E)}^l$ (23)\n$b_1 = B_l^1(E)$ (24)\n$W_2 = W_{(V,E)}^l$ (25)\n$b_2 = b_l^2$ (26)\nthe FFN becomes a specialized case of our generalized cross-attention mechanism applied to a static knowledge base E, establishing functional equivalence between the two. Such equivalence directly implies that, when E is trained jointly with the model, our modular architecture is functionally equivalent to a standard Transformer. Therefore, we expect identical performance on any task under this joint training regime. This equivalence serves as strong theoretical validation of our generalized cross-attention mechanism and our proposed modular architecture in the joint-training setting. As this implies that empirical results under joint training would simply confirm this equivalence, we defer empirical validation to future work focusing on external knowledge bases, as discussed in Section 6."}, {"title": "Implications of the Equivalence", "content": "This mathematically proven equivalence, $FFN(H_l) = Cross-Attention(H_l, Implicit E)$, formally establishes the connection between FFNs and the key-value memory framework [8], providing a concrete mechanism for how this memory is accessed and utilized. Furthermore, it aligns with empirical observations, such as the layer-specific encoding of information found by Haider et al. [10]. This equivalence has several important theoretical implications.\nTheoretical Justification and New Interpretation of FFNs. This equivalence provides a strong theoretical basis for the effectiveness and interpretability of FFNs in Transformers. It reveals that they are not simply arbitrary non-linear transformations but rather perform a specific form of context-dependent knowledge retrieval from a highly compressed, distributed representation acquired during pre-training. This retrieval process involves knowledge-specific thresholding and transformation, incorporating both knowledge-specific and cross-embedding-space adjustments. This connection also provides a new lens for interpreting the folded weights (Eq. 23-26), which now represent a more interpretable combination of query, key, and knowledge base information.\nDistinct Requirements of Cross-Attention. This analysis highlights the crucial differences between self-attention and cross-attention, particularly in the context of knowledge retrieval. While self-attention focuses on information exchange within a single source, cross-attention for knowledge retrieval requires mechanisms for selective retrieval and controlled transformation of information from an external source.\nImplications for Model Size. The implicit encoding of E within the closure $FFN(H_l) = Cross-Attention(H_l, Implicit E)$ directly explains the substantial parameter requirements of Trans-formers. Encoding knowledge in a distributed, compressed manner within the FFN weights requires substantial capacity. Our modular architecture, in future work with external knowledge bases, offers a potential solution to this by externalizing the knowledge base, allowing for more efficient scaling of knowledge capacity.\nLayer-Specific Views of Shared Knowledge. Because the weight folding process involves layer-specific projection matrices $(W_q^l, W_k^l, W_v^l)$ and other layer-specific parameters and biases, each layer in a standard Transformer effectively accesses a different view of the same, implicitly encoded, shared knowledge."}, {"title": "Discussion and Future Work", "content": "This section discusses limitations, outlines future research directions, and presents practical consider-ations regarding computational and memory trade-offs."}, {"title": "End-to-End vs. Decoupled Architectures", "content": "A central consideration in architectural design is the trade-off between end-to-end and decoupled (modular) approaches. End-to-end training of monolithic Transformers has proven highly effective in many tasks, offering the advantage of direct optimization for the final task objective and implicit feature learning. However, this comes at the cost of limited interpretability, adaptability, and scalability, as discussed in Section 2. Our proposed modular architecture is theoretically equivalent to standard Transformers under the joint training regime explored in this paper (as demonstrated in Section 5), and therefore we expect similar performance in this setting. However, its primary motivation is to address the long-term challenges of interpretability, adaptability, and scalability, particularly in scenarios requiring continuous learning and integration of rapidly evolving knowledge. We acknowledge that transitioning to external knowledge bases may introduce a performance gap, especially if knowledge representation and retrieval are not optimized. However, we argue that the potential benefits of decoupling knowledge and reasoning\u2014including enhanced interpretability, adaptability to new information, independent scaling of knowledge and reasoning capacity, and richer interactions with external systems-outweigh this potential trade-off."}, {"title": "Limitations and Future Work", "content": "This core area of future work focuses on the practical implementation and management of external KBs within our modular architecture. It encompasses the following investigations:\n\u2022 Joint Training and Retrieval. We will investigate joint end-to-end training of the LLM and a dedicated KB embedding model X (which generates E), where the LLM generates query embeddings to retrieve relevant KB entries using a differentiable Top-K approximation (e.g., smoothed softmax, straight-through estimator). This aims to optimize embedding compatibility and information integration.\n\u2022 KB Storage and Management. We will adopt external embedding storage to manage the embeddings generated by X (i.e., E). This allows efficient KB updates (re-embedding, insertions, and deletions) during inference. This approach assumes sufficient training data representativeness for generalization to new KB entries, which we will evaluate. Methods for monitoring KB quality and consistency will also be investigated.\n\u2022 Knowledge Representation and Structure. Throughout this paper, we have considered a simplified scenario where knowledge is represented as individual entries within the KB. However, our analysis suggests that FFNs might encode knowledge in complex, high-dimensional representations. We will therefore investigate how to represent knowledge effectively in external KBs, exploring different structures and their impact on retrieval, reasoning, and KB management. This exploration may further complicate the joint training and KB management procedures described above.\nScaling and Efficiency. Externalizing the knowledge base offers the potential for independent scaling of knowledge and reasoning capacity. Future work should empirically investigate the computational and memory trade-offs associated with different KB sizes, retrieval methods, and reasoning model sizes. A key research question is: How can we optimize retrieval methods, knowledge representations, and reasoning model size to achieve efficient and scalable knowledge-driven LLMs?\nInterpretability of Retrieved Knowledge. Understanding why specific knowledge entries are re-trieved and how they contribute to the model's output is crucial for interpretability and trustworthiness. Future work should explore methods for explaining the retrieval process, such as visualizing attention weights over retrieved knowledge entries, providing textual explanations of retrieved entries based on their content, or developing more formal methods for tracing information flow from the knowledge base to the model's predictions."}, {"title": "Computational and Memory Trade-offs", "content": "Computational efficiency is crucial. We analyze potential computational and memory trade-offs, focusing on the implications for standard Transformers given the equivalence we have shown in the joint-training setting. For simplicity, we set the dimension of the knowledge entries de equal to the query dimension d. We analyze the trade-offs for different implementations of an FFN layer in a Transformer, as shown in Table 2:\n\u2022 Standard FFN. Computational complexity is O(Nddff). For typical settings where df f = 4d in GPT-3, this represents a significant computational burden. Memory complexity is O(dd ff).\n\u2022 Cross-Attention. A naive implementation of our generalized cross-attention involves projections and attention computation. The computational complexities of these operations are as follows:\nQuery Projection: O(Nddk)\nKey Projection : O(|E|ddk)\nValue Projection : O(|E|dd)\nScaled Dot-Product Attention : O(Ndk|E|)\nMultiplication by V: O(Nd|E|)\nwhere dk represents the dimension of the keys and queries, which is usually much smaller than d (e.g., 128 vs. 12288 in GPT-3). Consequently, the dominant terms are value projection and multiplication by V, resulting in a total complexity of O((N + d)d|E|), substantially higher than the FFN when |E| \u226b dff. Memory complexity is O(d|E|).\n\u2022 Cross-Attention with Folding (to full KB). Computational complexity is reduced to O(NdE) due to pre-computation. Memory complexity remains O(d|E|).\n\u2022 Cross-Attention with Folding and Retrieval (to subset E'). Computational complexity becomes O(Nd E'|) + R, where R is the retrieval cost. If |E'| < |E| (e.g., retrieving a few hundred to a thousand entries from a large KB), this has the potential to offer substantial computational savings. Memory complexity is reduced to O(d|E'|).\nThis comparison highlights the fundamental trade-off between the size of the knowledge base | E| and the computational cost of cross-attention, which scales linearly with |E|. The observation that even setting |E| = dff (49152 in GPT-3 model) results in a remarkably small number of entries compared to world knowledge strongly supports our hypothesis of substantial knowledge compression within FFNs (Section 6.2). However, a key distinction is that in our proposed architecture, the knowledge base E is shared across all layers, whereas in a standard Transformer, the corresponding weights within the FFNs (which implicitly encode the compressed knowledge) are not shared. This sharing of E has important implications for parameter efficiency and knowledge consistency. It reinforces the crucial role of efficient knowledge representation (Section 6.2) to minimize E and make externalization computationally feasible. Using a retrieved subset E' further mitigates computational costs by focusing on relevant knowledge. While externalization introduces a retrieval cost R, it offers significant advantages: independent scaling of knowledge and reasoning capacity, improved adaptability, and enhanced interpretability. Future work will investigate these trade-offs, including knowledge compression techniques, the impact of retrieval methods on R, and the optimal size of E'."}, {"title": "Related Work", "content": "Our work draws upon and contributes to several areas of research, including Transformer architectures, knowledge retrieval, the connection between symbolic and neural AI, modular neural networks, interpretability, and generalized attention.\nTransformer Architectures. The Transformer architecture [19] has revolutionized NLP and other fields, leading to various modifications and extensions for improved efficiency and performance [18, 15] and influential architectures like BERT [6] and GPT [14]. While much work has focused on the attention mechanism, the role of Feed-Forward Networks (FFNs) has received less attention. Geva et al. [8] addressed this by proposing that FFNs function as key-value memories, implicitly storing and retrieving knowledge. Our work builds directly on this insight, providing a formal mathematical derivation demonstrating that FFNs are a specialized case of generalized cross-attention. This formalization, further supported by empirical analyses of FFNs in code language models by Haider et al. [10], offers a deeper understanding of the functional role of FFNs.\nKnowledge Retrieval and Symbolic AI. Integrating external knowledge into neural networks is a long-standing goal in bridging symbolic and neural AI [7]. Approaches like Memory Networks [21] and Neural Turing Machines [9] introduced explicit memory components accessed through attention. Retrieval-augmented language models (RAG) directly incorporate external knowledge into the input context. In contrast to these explicit methods, and building on the understanding of FFNs as implicit knowledge stores [8], our work proposes an architecture that explicitly decouples knowledge and reasoning, bridging the gap between implicit and explicit knowledge representation.\nModular Neural Networks. Modularity in neural networks has been shown to improve learning, generalization, and interpretability [12]. Previous work has explored task-specific modularity [1, 11] and parameterized Transformers [22], introducing modularity at a higher level (e.g., different modules for different tasks). Our work focuses on modularity within the Transformer architecture, formalizing the FFN as a module dedicated to implicit knowledge retrieval via cross-attention. This formalization lays the groundwork for explicitly decoupling knowledge into a separate module, distinguishing our approach from previous modular neural network designs.\nInterpretability of Neural Networks. Various techniques, such as attention visualization [2], saliency maps [17], and probing tasks [5], aim to improve the interpretability of neural networks. While these methods provide insights into input importance, our work offers a theoretical framework for understanding the internal computations of FFNs, revealing their role as key-value memories [8]. This understanding, in conjunction with empirical analyses like those of Haider et al. [10], can inform more targeted interpretability methods, such as analyzing the folded weights in our derived formulation.\nGeneralized Attention and Biases. Our work uses generalized cross-attention with knowledge-specific biases. Prior work has explored different attention mechanisms [2] and biased attention [16]. We extend these by deriving the FFN as a specific biased cross-attention mechanism, demonstrating the crucial role of these biases in knowledge retrieval. The use of knowledge-specific biases, as opposed to general biases, enables finer control over retrieval and facilitates future work with external knowledge bases."}, {"title": "Conclusion", "content": "We proposed a novel modular Transformer architecture with a generalized cross-attention mechanism for accessing a shared knowledge base, addressing the entanglement of knowledge and reasoning in monolithic Transformers. Our key contribution is twofold: the design of this cross-attention mechanism for effective knowledge retrieval and a theoretical analysis interpreting FFNs as a specialized case. This interpretation reveals FFNs perform implicit knowledge retrieval and motivates future research exploring external knowledge bases to enhance adaptability, scalability, and richer LLM-external system interactions beyond simple retrieval-augmentation. This modular design offers a promising avenue for more interpretable and scalable knowledge-driven AI."}]}