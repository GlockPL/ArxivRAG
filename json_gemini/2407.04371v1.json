{"title": "Exploiting the equivalence between quantum neural networks and perceptrons", "authors": ["Chris Mingard", "Jessica Pointing", "Charles London", "Yoonsoo Nam", "Ard A. Louis"], "abstract": "Quantum machine learning models based on parametrized quantum circuits, also called quantum neural networks (QNNs), are considered to be among the most promising candidates for applications on near-term quantum devices. Here we explore the expressivity and inductive bias of QNNs by exploiting an exact mapping from QNNs with inputs x to classical perceptrons acting on x \u2297 x (generalised to complex inputs). The simplicity of the perceptron architecture allows us to provide clear examples of the shortcomings of current QNN models, and the many barriers they face to becoming useful general-purpose learning algorithms. For example, a QNN with amplitude encoding cannot express the Boolean parity function for n \u2265 3, which is but one of an exponential number of data structures that such a QNN is unable to express. Mapping a QNN to a classical perceptron simplifies training, allowing us to systematically study the inductive biases of other, more expressive embeddings on Boolean data. Several popular embeddings primarily produce an inductive bias towards functions with low class balance, reducing their generalisation performance compared to deep neural network architectures which exhibit much richer inductive biases. We explore two alternate strategies that move beyond standard QNNs. In the first, we use a QNN to help generate a classical DNN-inspired kernel. In the second, we draw an analogy to the hierarchical structure of deep neural networks and construct a layered non-linear QNN that is provably fully expressive on Boolean data, while also exhibiting a richer inductive bias than simple QNNs. Finally, we discuss characteristics of the QNN literature that may obscure how hard it is to achieve quantum advantage over deep learning algorithms on classical data.", "sections": [{"title": "I. Introduction", "content": "Machine learning has undergone a revolution with the advent of large-scale deep (classical) neural network models (DNNs), which allow for the automatic extraction of features from data and the learning of complex functions. DNNs are highly expressive, satisfying universal approximation theorems [1, 2], and can fit arbitrary functions on large datasets [3]. Despite this high capacity, these models often generalise well rather than overfitting as classical learning theory would predict. This means they have a strong inductive bias towards functions that describe real-world data [4\u20139]. The success of these models has heralded the end of the \"feature engineering\" era, where features and data representations were hand-crafted by experts [10], and the rise of \"general-purpose learning algorithms\" that can learn from raw data [11].\nQuantum machine learning (QML) is a newer discipline that aims to leverage quantum computers to produce new algorithms and models that may outperform classical machine learning methods in terms of computational efficiency or accuracy (referred to collectively as quantum advantage). While quantum computers are still in the noisy intermediate-scale quantum (NISQ) era [12] \u2013 small and prone to errors \u2013 there are many claims in the literature that QML models may already offer some advantage over classical models [13\u201317]. Variational Quantum Algorithms (VQAs), where a classically parameterised quantum circuit (PQC) is optimised to minimise a cost function evaluated on a quantum computer, are one of the most popular candidates for potential QML advantage [18]. In this paper, we focus on quantum neural networks (QNNs), a subclass of VQAs in which data is treated as a quantum state and processed by applying a unitary transform in the form of a PQC. The output of the QNN is accessed by measuring the processed quantum state (usually on a designated readout qubit), and the QNN is trained by adjusting the parameters of the PQC to minimise a cost function [19]."}, {"title": "II. Mapping QNNs onto perceptrons", "content": "In this section, we provide an overview of how to construct the mapping from QNNs to the classical TPP. A complete formal proof can be found in Appendix E. We will consider QNNs acting on |0\u27e9|x\u27e9, where |x\u27e9 is encoded on n qubits, giving a Hilbert space of dimension N = 2n. We can represent |0\u27e9|x\u27e9 as a vector (x, 0) of length 2N. Let xi be the N (in general, complex) components of x. Next, consider a classical perceptron acting on h \u2208 RN2 which expresses functions of the form\ng(x) = w \u00b7 h\nwhere w \u2208 RN2 are the weights. A QNN defined by any unitary matrix U acting on (x, 0) \u2208 R2N as in Equation (1) can be uniquely mapped onto the perceptron defined in Equation (3) with h = x \u2297 x (see Lemma E.3), where we define the complex tensor product x \u2297 x \u2208 RN2 as\n(x \u2297 x)N(i\u22121)+i = xix\u2217i\n(x \u2297 x)N(i\u22121)+j = Re(xix\u2217j) + i Im(xix\u2217j) for i \u2260 j\nWhen all components of x are real, x \u2297 x is equivalent to the standard Kronecker product x \u2297 x. We will refer to this perceptron with real weights w acting on x \u2297 x as the tensor product perceptron (TPP). In other words, any QNN can be uniquely mapped onto a TPP. A preliminary observation of this link between a perceptron and a general QNN can be found in Eq. 45 of [19]) in the context of a more general link between QNNs and kernel methods.\nIt is also possible, under certain mild conditions on the weights of the TPP, to construct a mapping from TPPs to QNNs such that the numerical output is the same. If we are only interested in the sign of the output (as we are for classification), then the models have equivalent expressive power (see Lemma E.4)."}, {"title": "III. Expressivity of TPPs on Boolean functions", "content": "In this section, we investigate the expressivity of TPPs (and therefore QNNs) on Boolean functions, which are foundational building blocks of computer science. For the n-bit Boolean dataset, inputs are of the form x \u2208 {0, 1}n, and the target function can be expressed as a binary function f : {0, 1}n \u2192 {0, 1}, where the inputs are ordered by ascending binary value (see Appendix C1). For example the n-dimensional parity function is defined as f(x) = \u2211n i=1 xi mod 2. There are 2n inputs and thus 22n possible target functions. The Boolean dataset is a useful testbed for machine learning algorithms because it can be used to measure the properties of a model across a range of target functions. This contrasts with many image or text-based datasets where there is just one target function. Moreover, the long tradition of research on Boolean functions provides opportunities to derive analytical results.\nThe perceptron, introduced in 1958 by Rosenblatt [35] as a candidate for a general machine learning algorithm, is a form of linear classifier. In their famous book, Minsky and Papert [36] demonstrated its inability to express basic non-linear functions such as XOR. More generally, the number T(n) of Boolean functions that a perceptron can express is bounded by 2n2\u2212n log2 n\u2212O(n) < T(n) \u2264 2n2\u2212n log2 n+O(n) [37], which is exponentially fewer than the total number of Boolean functions on this data. This lack of expressivity, together with the difficulty at the time of training deeper networks, has been cited as one of the causes of the first AI winter [38].\nIn that context, it is interesting to examine the expressivity of the TPP (which can also be viewed as a form of quadratic classifier on x). If x has dimension N, and so w has dimension N2, the number of Boolean functions T(N) the TPP will be able to express (see Lemma F.5) is upper-bounded by\nTN\u22642N3+N2log2e+1\nThe TPP can only express an exponentially small fraction of the 22N possible functions on these data points. While the TPP is more expressive than the standard perceptron and can express the basic n = 2 parity function, XOR, it cannot express any further parity functions with n \u2265 3 (see Appendix F).\nThis analysis implies that, much like the basic perceptron, the TPP is a severely restricted model with a limited range of expressible functions. The equivalence between QNNs and TPPs means that the same problems carry over for QNNs. To use a QNN or a TPP to classify some new dataset, one has to ensure that the target function is expressible, which may mean performing explicit feature engineering to ensure that it is. This situation harkens back to earlier days of machine learning where feature-engineering played an important role. By contrast, DNNs are highly expressive [2], and even simple FCN architectures don't need to be that big to express every Boolean function [39]."}, {"title": "IV. Experiments on the Boolean dataset", "content": "In this section, we empirically study the expressivity and inductive bias of QNNs with different encodings on Boolean data. Given a subset of input-output relations for a target Boolean function, a key challenge for supervised machine learning is to predict the unknown input-output pairs. Experiments are performed on the 7-bit Boolean dataset, which has 128 input data points and \u2248 3 \u00d7 1038 possible target functions. Four different encoding methods \u03c6 for {0, 1}n are examined, all of which have very different properties. They are:\n1. Amplitude encoding A encodes classical data into the amplitudes of a quantum state, using \u2308log2(n)\u2309 qubits to represent n dimensional classical data as\n\u03a6Ax=1\u2225x\u2225\u2211i=12nXi|i\u27e9\nFor Boolean data, the datapoints form a (normalised) hypercube in the n-dimensional Hilbert space. This encoding is most similar to the encoding for classical neural networks. Due to the normalisation constraint, the point at the origin in the classical encoding cannot be encoded, so it is dropped. See Definition C.5.\n2. Basis encoding B uses n input qubits, such that [0, 1, 0] \u2192 |0\u27e9 |1\u27e9 |0\u27e9. See Definition C.4.\n3. ZZ encoding ZZ uses n input qubits, and is designed to be biased towards the parity function. See Definition C.6.\n4. Random Transform encoding R uses either n or \u2308log2(n)\u2309 input qubits, and is the output of a single ReLU-activated fully connected layer with either n or 2n outputs (we refer to these as RT(n) and RT(2n) respectively). It is designed to act as a random non-unitary transform. See Definition C.7.\nEach encoding method embeds data very differently in Hilbert space. We test how these embeddings perform on 100 target functions, chosen to have a wide range of different characteristics. These functions include the parity function, functions with fixed class balance, functions with different degrees of symmetry, and entirely random functions (see Appendix C3). We measure the complexity of these functions using two methods: class balance, and Lempel-Ziv (LZ) complexity. Class balance is defined as min{p, 1 \u2013 p}, where p is the proportion of data points classified as 0 (and so 1 \u2013 p are classified as 1). LZ complexity is measured as in [40] using the Lempel-Ziv 76 compression technique, and is low for highly structured data, and/or data with low class balance. By comparing both these measures we can distinguish between learners that have an inductive bias towards highly structured data, and those that simply learn to predict the frequencies of 0 or 1 in the training data. Previous work has shown that DNNs display a strong inductive bias towards functions with low LZ complexity, even when class balance is high, a property thought to be critical to their ability to generalize well in a wide range of practical situations. [8, 39, 40].\nWe trained the QNN (using the TPP correspondence above) on a randomly selected training set of 64 data points from each function (testing on the other 64). See"}, {"title": "A. Expressivity and inductive bias of amplitude encoding and RT(n) encoding", "content": "A QNN using amplitude encoding can only express an exponentially small fraction of all Boolean functions of the data, as the QNN has an n dimensional Hilbert space for n-bit Boolean data. Indeed, we find that this QNN can only express 41 of our 100 functions (see Figure 2). This means that when applying this model to new datasets a detailed understanding of the data distribution will be required to ensure that the encoded data does not contain structures that prevent the potential target function from being expressed. It is likely hard to prevent this problem a-priori for data from quantum sources where the structure may be subtle and hard to predict.\nWhile there are hard constraints on the expressivity of QNNs with amplitude encoding, they do exhibit an inductive bias towards structured functions on the Boolean dataset. This can be seen in Figure 2, where we observe that amplitude encoding has an inductive bias towards highly structured data (as measured by the Lempel-Ziv complexity, top), even for some functions with high class"}, {"title": "B. Inductive bias of fully expressive QNNs", "content": "Each of the following encodings uses n qubits to encode n-dimensional classical data, and achieves full expressivity on the n-bit Boolean dataset. In this context, we note that if the encodings use all 2n dimensions, then K\u00fcbler et al. [24] show that an exponential amount of data would be required to learn the data, implying no quantum advantage. To learn from a reasonable (polynomial) amount of data, one would need to significantly restrict the expressivity of the QNNs, using only a limited number of dimensions in Hilbert space to encode the data. See Appendix A for more details. How these principles work out can be partially illustrated below.\nWith basis encoding it is not hard to prove, see for example [44], that all 22n functions can be expressed by a QNN with 2n gates. However, since every datapoint is encoded orthogonally in a 2n Hilbert space, the kernel is trivial, and the QNN has no inductive bias. As can be seen in Figure 2, the test error for every function is 50% illustrating that for basis encoding the QNN can't learn Boolean data.\nFigure 2 shows that ZZ encoding has a strong bias towards low class balance \u2013 its performance is equivalent to simply measuring the frequency of 1s or 0s, and using this to predict unknown data. More interestingly, it also shows a further strong bias towards the parity function, with perfect generalization for n = 7 and with 64 training data. The prediction from [24] is that ZZ encoding would fail to generalise for increasing n (assuming \u03c6ZZ(x') continues to span an exponential number of dimensions for higher n). This dataset is most likely too small to test such predictions. Nevertheless, we used 2n\u22121 datapoints for training, out of 2n, and the prediction is that training sets will need to continue to grow exponentially with increasing n for this system to generalise, even for the parity function.\nNote that there is nothing particularly deep about the ZZ encoding being good at learning the parity function. For a given target function, an encoding can always be created with the desired inductive bias. While the 1-layer FCN we use does not perform well on the parity function with classical encoding, other architectures do have a better inductive bias towards parity (see experiments in [8]). It is not hard to create an encoding more optimal for the parity function for classical DNNs as we show in Appendix D. In summary, designing a specific encoding to allow a QNN to work well on a specific target is not quantum advantage. Although such claims are frequently"}, {"title": "V. Quantum DNNs & Quantum FCN Kernels", "content": "While one could continue trying new embeddings, and hoping for better performance, the problem demonstrated in the previous section and our more general experience with classical kernel methods call for new strategies. One way forward we explore is to see how a QNN could mimic an FCN kernel on Hilbert space \u2013 providing an easy way to improve the expressivity and inductive bias of quantum kernels \u2013 but still suffering from all the problems with kernel machines discussed in the context of QNNs. The other strategy moves away from the kernel limit with a deep quantum neural network (DQNN) that is capable of expressing non-linear functions.\nA. FCN kernels\nRather than directly training a QNN, the quantum kernel KQ(x(1), x(2)) = |\u27e8x(1)|x(2)\u27e9|2 can be estimated on a quantum computer, and used to perform inference classically. One can then use tricks such as a 'bandwidth' parameter to artificially move inputs closer together which can counteract the effects of data being embedded in too high-dimensional a space [45]. This may allow kernel learning with polynomial data for systems that would not have been otherwise learnable [24]. Here, we show another simple twist on standard quantum kernel methods. Classical DNNs in the infinite-width limit can be mapped to kernel methods, and these have been shown to be competitive with finite DNNs on small data sets [46, 47]. For l-layered classical FCN ReLU networks with biases set to 0, we can recursively calculate such kernels (in the infinite width limit) Kl = Kl(x(i), x(j)) using the following recurrence relation [32, 33],\n\u03b1(lij)=1l(1\u2212(\u03b1l\u22121(ij)\u03b1l\u22121(ij))2+(\u03c0\u2212arccos(\u03b1l\u22121(ij))\u03b1l\u22121(ij))2)(7)"}, {"title": "VI. Comparing QNN and DNN performance", "content": "So far, we have used the relative simplicity of the TPP architecture to analyze the expressivity and inductive bias of QNNs. In this section, we place these results for QNNs into the wider context of classical DNNs, the most successful general-purpose learning algorithms currently available. We ask: what barriers do QNNs face to also be useful as general-purpose learning algorithms?\nMany of the points in this section may seem quite obvious to those versed in classical deep learning. Some have already been discussed in the QNN literature (see e.g. [16]). Nevertheless, our experience of reading the recent QNN literature suggests that these ideas have not yet been assimilated. We therefore provide a more expansive discussion of than one might otherwise do in the hope that this will help move things forward.\nMismatched scales complicate the search for quantum advantage with current QNNs. Classical machine learning has benefited from many more years of study and investment than quantum machine learning has. Moreover, the power of deep learning techniques only fully emerges on large datasets and with extremely large models. For example, the modern deep-learning era is often said to have kicked off in 2012 when a CNN-based"}, {"title": "VII. Conclusions", "content": "Our results illustrate a growing (but by no means unanimous) consensus in the literature [12, 16, 17, 24, 25,"}]}