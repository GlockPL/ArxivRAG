{"title": "Exploiting the equivalence between quantum neural networks and perceptrons", "authors": ["Chris Mingard", "Jessica Pointing", "Charles London", "Yoonsoo Nam", "Ard A. Louis"], "abstract": "Quantum machine learning models based on parametrized quantum circuits, also called quantum neural networks (QNNs), are considered to be among the most promising candidates for applications on near-term quantum devices. Here we explore the expressivity and inductive bias of QNNs by exploiting an exact mapping from QNNs with inputs x to classical perceptrons acting on $x\\otimes x$ (generalised to complex inputs). The simplicity of the perceptron architecture allows us to provide clear examples of the shortcomings of current QNN models, and the many barriers they face to becoming useful general-purpose learning algorithms. For example, a QNN with amplitude encoding cannot express the Boolean parity function for n \u2265 3, which is but one of an exponential number of data structures that such a QNN is unable to express. Mapping a QNN to a classical perceptron simplifies training, allowing us to systematically study the inductive biases of other, more expressive embeddings on Boolean data. Several popular embeddings primarily produce an inductive bias towards functions with low class balance, reducing their generalisation performance compared to deep neural network architectures which exhibit much richer inductive biases. We explore two alternate strategies that move beyond standard QNNs. In the first, we use a QNN to help generate a classical DNN-inspired kernel. In the second, we draw an analogy to the hierarchical structure of deep neural networks and construct a layered non-linear QNN that is provably fully expressive on Boolean data, while also exhibiting a richer inductive bias than simple QNNs. Finally, we discuss characteristics of the QNN literature that may obscure how hard it is to achieve quantum advantage over deep learning algorithms on classical data.", "sections": [{"title": "I. Introduction", "content": "Machine learning has undergone a revolution with the advent of large-scale deep (classical) neural network models (DNNs), which allow for the automatic extraction of features from data and the learning of complex functions. DNNs are highly expressive, satisfying universal approximation theorems [1, 2], and can fit arbitrary functions on large datasets [3]. Despite this high capacity, these models often generalise well rather than overfitting as classical learning theory would predict. This means they have a strong inductive bias towards functions that describe real-world data [4\u20139]. The success of these models has heralded the end of the \"feature engineering\" era, where features and data representations were hand-crafted by experts [10], and the rise of \"general-purpose learning algorithms\" that can learn from raw data [11].\nQuantum machine learning (QML) is a newer discipline that aims to leverage quantum computers to produce new algorithms and models that may outperform classical machine learning methods in terms of computational efficiency or accuracy (referred to collectively as quantum advantage). While quantum computers are still in the noisy intermediate-scale quantum (NISQ) era [12] \u2013 small and prone to errors \u2013 there are many claims in the literature that QML models may already offer some advantage over classical models [13\u201317]. Variational Quantum Algorithms (VQAs), where a classically parameterised quantum circuit (PQC) is optimised to minimise a cost function evaluated on a quantum computer, are one of the most popular candidates for potential QML advantage [18]. In this paper, we focus on quantum neural networks (QNNs), a subclass of VQAs in which data is treated as a quantum state and processed by applying a unitary transform in the form of a PQC. The output of the QNN is accessed by measuring the processed quantum state (usually on a designated readout qubit), and the QNN is trained by adjusting the parameters of the PQC to minimise a cost function [19]."}, {"title": "II. Mapping QNNs onto perceptrons", "content": "In this section, we provide an overview of how to construct the mapping from QNNs to the classical TPP. A complete formal proof can be found in Appendix E. We will consider QNNs acting on $|0\\rangle |x\\rangle$, where $|x\\rangle$ is encoded on n qubits, giving a Hilbert space of dimension N = 2n. We can represent $|0\\rangle |x\\rangle$ as a vector $(x, 0)$ of length 2N. Let $x_i$ be the N (in general, complex) components of x. Next, consider a classical perceptron acting on $h\\in \\mathbb{R}^{N^2}$ which expresses functions of the form\n$\\qquad g(x) = w \\cdot h$ \nwhere $w \\in \\mathbb{R}^{N^2}$ are the weights. A QNN defined by any unitary matrix U acting on $(x,0) \\in \\mathbb{R}^{2N}$ as in Equation (1) can be uniquely mapped onto the perceptron defined in Equation (3) with $h = x \\otimes x$ (see Lemma E.3), where we define the complex tensor product $x \\otimes x \\in \\mathbb{R}^{N^2}$ as\n$\\qquad (x \\otimes x)_{N(i-1)+i} = x_i x_i^*$ \n$\\qquad (x \\otimes x)_{N(i-1)+j} = \\text{Re}(x_i x_j^*) + i\\text{Im}(x_i x_j^*) \\qquad \\text{for} \\quad i \\neq j$ \nWhen all components of x are real, $x \\otimes x$ is equivalent to the standard Kronecker product $x \\otimes x$. We will refer to this perceptron with real weights w acting on $x \\otimes x$ as the tensor product perceptron (TPP). In other words, any QNN can be uniquely mapped onto a TPP. A preliminary observation of this link between a perceptron and a general QNN can be found in Eq. 45 of [19]) in the context of a more general link between QNNs and kernel methods.\nIt is also possible, under certain mild conditions on the weights of the TPP, to construct a mapping from TPPs to QNNs such that the numerical output is the same. If we are only interested in the sign of the output (as we are for classification), then the models have equivalent expressive power (see Lemma E.4)."}, {"title": "III. Expressivity of TPPs on Boolean functions", "content": "In this section, we investigate the expressivity of TPPs (and therefore QNNs) on Boolean functions, which are foundational building blocks of computer science. For the n-bit Boolean dataset, inputs are of the form $x \\in \\{0,1\\}^n$, and the target function can be expressed as a binary function $f : \\{0,1\\}^n \\rightarrow \\{0, 1\\}$, where the inputs are ordered by ascending binary value (see Appendix C1). For example the n-dimensional parity function is defined as $f(x) = \\sum_{i=1}^n x_i \\text{ mod } 2$. There are $2^n$ inputs and thus $2^{2^n}$ possible target functions. The Boolean dataset is a useful testbed for machine learning algorithms because it can be used to measure the properties of a model across a range of target functions. This contrasts with many image or text-based datasets where there is just one target function. Moreover, the long tradition of research on Boolean functions provides opportunities to derive analytical results.\nThe perceptron, introduced in 1958 by Rosenblatt [35] as a candidate for a general machine learning algorithm, is a form of linear classifier. In their famous book, Minsky and Papert [36] demonstrated its inability to express basic non-linear functions such as XOR. More generally, the number T(n) of Boolean functions that a perceptron can express is bounded by $2^{n^2 - n \\log_2 n - O(n)} < T(n) \\leq 2^{n^2 - n \\log_2 n + O(n)}$ [37], which is exponentially fewer than the total number of Boolean functions on this data. This lack of expressivity, together with the difficulty at the time of training deeper networks, has been cited as one of the causes of the first AI winter [38].\nIn that context, it is interesting to examine the expressivity of the TPP (which can also be viewed as a form of quadratic classifier on x). If x has dimension N, and so w has dimension $N^2$, the number of Boolean functions T(N) the TPP will be able to express (see Lemma F.5) is upper-bounded by\n$\\qquad T(N) < 2^{N^3+N^2 \\log_2(e)+1}$ \nThe TPP can only express an exponentially small fraction of the $2^{2^n}$ possible functions on these data points. While the TPP is more expressive than the standard perceptron and can express the basic n = 2 parity function, XOR, it cannot express any further parity functions with n\u2265 3 (see Appendix F).\nThis analysis implies that, much like the basic perceptron, the TPP is a severely restricted model with a limited range of expressible functions. The equivalence between QNNs and TPPs means that the same problems carry over for QNNs. To use a QNN or a TPP to classify some new dataset, one has to ensure that the target function is expressible, which may mean performing explicit feature engineering to ensure that it is. This situation harkens back to earlier days of machine learning where feature-engineering played an important role. By contrast, DNNs are highly expressive [2], and even simple FCN architectures don't need to be that big to express every Boolean function [39]."}, {"title": "IV. Experiments on the Boolean dataset", "content": "In this section, we empirically study the expressivity and inductive bias of QNNs with different encodings on Boolean data. Given a subset of input-output relations for a target Boolean function, a key challenge for supervised machine learning is to predict the unknown input-output pairs. Experiments are performed on the 7-bit Boolean dataset, which has 128 input data points and \u2248 3 \u00d7 1038 possible target functions. Four different encoding methods \u03c6 for {0,1}n are examined, all of which have very different properties. They are:\n1. Amplitude encoding A encodes classical data into the amplitudes of a quantum state, using $\\lceil \\log_2(n) \\rceil$ qubits to represent n dimensional classical data as\n$\\qquad \\Phi_A(x) = \\frac{1}{\\|x\\|} \\sum_{i=1}^{2^n} x_i |i\\rangle$\nFor Boolean data, the datapoints form a (normalised) hypercube in the n-dimensional Hilbert space. This encoding is most similar to the encoding for classical neural networks. Due to the normalisation constraint, the point at the origin in the classical encoding cannot be encoded, so it is dropped. See Definition C.5.\n2. Basis encoding B uses n input qubits, such that [0,1,0] \u2192 |0\\rangle |1\\rangle |0\\rangle. See Definition C.4.\n3. ZZ encoding \u03c6ZZ uses n input qubits, and is designed to be biased towards the parity function. See Definition C.6.\n4. Random Transform encoding \u03c6R uses either n or $\\lceil \\log_2(n) \\rceil$ input qubits, and is the output of a single ReLU-activated fully connected layer with either n or 2n outputs (we refer to these as RT(n) and RT(2n) respectively). It is designed to act as a random non-unitary transform. See Definition C.7.\nEach encoding method embeds data very differently in Hilbert space. We test how these embeddings perform on 100 target functions, chosen to have a wide range of different characteristics. These functions include the parity function, functions with fixed class balance, functions with different degrees of symmetry, and entirely random functions (see Appendix C3). We measure the complexity of these functions using two methods: class balance, and Lempel-Ziv (LZ) complexity. Class balance is defined as min{p, 1 \u2013 p}, where p is the proportion of data points classified as 0 (and so 1 \u2013 p are classified as 1). LZ complexity is measured as in [40] using the Lempel-Ziv 76 compression technique, and is low for highly structured data, and/or data with low class balance. By comparing both these measures we can distinguish between learners that have an inductive bias towards highly structured data, and those that simply learn to predict the frequencies of 0 or 1 in the training data. Previous work has shown that DNNs display a strong inductive bias towards functions with low LZ complexity, even when class balance is high, a property thought to be critical to their ability to generalize well in a wide range of practical situations. [8, 39, 40].\nWe trained the QNN (using the TPP correspondence above) on a randomly selected training set of 64 data points from each function (testing on the other 64). See"}, {"title": "A. Expressivity and inductive bias of amplitude encoding and RT(n) encoding", "content": "A QNN using amplitude encoding can only express an exponentially small fraction of all Boolean functions of the data, as the QNN has an n dimensional Hilbert space for n-bit Boolean data. Indeed, we find that this QNN can only express 41 of our 100 functions (see Figure 2). This means that when applying this model to new datasets a detailed understanding of the data distribution will be required to ensure that the encoded data does not contain structures that prevent the potential target function from being expressed. It is likely hard to prevent this problem a-priori for data from quantum sources where the structure may be subtle and hard to predict.\nWhile there are hard constraints on the expressivity of QNNs with amplitude encoding, they do exhibit an inductive bias towards structured functions on the Boolean dataset. This can be seen in Figure 2, where we observe that amplitude encoding has an inductive bias towards highly structured data (as measured by the Lempel-Ziv complexity, top), even for some functions with high class balance.\nThe RT(n) encoding with $\\lceil \\log_2 n \\rceil$ qubits has no structure due to the random nature of the transform. This is reflected in its inability to generalise well for any structured high class balance but low LZ complexity functions. The QNN is also very inexpressive, only able to fit the trivial function to zero training error."}, {"title": "B. Inductive bias of fully expressive QNNs", "content": "Each of the following encodings uses n qubits to encode n-dimensional classical data, and achieves full expressivity on the n-bit Boolean dataset. In this context, we note that if the encodings use all 2n dimensions, then K\u00fcbler et al. [24] show that an exponential amount of data would be required to learn the data, implying no quantum advantage. To learn from a reasonable (polynomial) amount of data, one would need to significantly restrict the expressivity of the QNNs, using only a limited number of dimensions in Hilbert space to encode the data. See Appendix A for more details. How these principles work out can be partially illustrated below.\nWith basis encoding it is not hard to prove, see for example [44], that all $2^{2^n}$ functions can be expressed by a QNN with $2^{2^n}$ gates. However, since every datapoint is encoded orthogonally in a 2n Hilbert space, the kernel is trivial, and the QNN has no inductive bias. As can be seen in Figure 2, the test error for every function is 50% illustrating that for basis encoding the QNN can't learn Boolean data.\nFigure 2 shows that ZZ encoding has a strong bias towards low class balance - its performance is equivalent to simply measuring the frequency of 1s or 0s, and using this to predict unknown data. More interestingly, it also shows a further strong bias towards the parity function, with perfect generalization for n = 7 and with 64 training data. The prediction from [24] is that ZZ encoding would fail to generalise for increasing n (assuming \u03c6zz(x\u2032) continues to span an exponential number of dimensions for higher n). This dataset is most likely too small to test such predictions. Nevertheless, we used $2^{n-1}$ datapoints for training, out of 2n, and the prediction is that training sets will need to continue to grow exponentially with increasing n for this system to generalise, even for the parity function.\nNote that there is nothing particularly deep about the ZZ encoding being good at learning the parity function. For a given target function, an encoding can always be created with the desired inductive bias. While the 1-layer FCN we use does not perform well on the parity function with classical encoding, other architectures do have a better inductive bias towards parity (see experiments in [8]). It is not hard to create an encoding more optimal for the parity function for classical DNNs as we show in Appendix D. In summary, designing a specific encoding to allow a QNN to work well on a specific target is not quantum advantage. Although such claims are frequently made in the literature, we advocate for this practice to be tempered. The goal of classical general machine learning algorithms such as DNNs is to learn features (an embedding) that allow it to separate the data itself without the need for tuning by hand.\nFigure 2 shows that, in contrast to the RT(n) encoding, the RT(2n) encoding is fully expressive on this dataset. Moreover, much like the ZZ encoding, it has an inductive bias towards low class balance, but no bias towards the more interesting structured Boolean functions.\nThese three encodings illustrate a general point: with data spread out across all 2n dimensions of Hilbert space, any function can be expressed, but the inductive bias will be weak. This tradeoff makes it hard to design QNNS that function like DNNs, that is as general machine learning algorithms."}, {"title": "V. Quantum DNNs & Quantum FCN Kernels", "content": "While one could continue trying new embeddings, and hoping for better performance, the problem demonstrated in the previous section and our more general experience with classical kernel methods call for new strategies. One way forward we explore is to see how a QNN could mimic an FCN kernel on Hilbert space - providing an easy way to improve the expressivity and inductive bias of quantum kernels - but still suffering from all the problems with kernel machines discussed in the context of QNNs. The other strategy moves away from the kernel limit with a deep quantum neural network (DQNN) that is capable of expressing non-linear functions."}, {"title": "A. FCN kernels", "content": "Rather than directly training a QNN, the quantum kernel $K_Q(x^{(1)}, x^{(2)}) = |\\langle x^{(1)} | x^{(2)} \\rangle|^2$ can be estimated on a quantum computer, and used to perform inference classically. One can then use tricks such as a 'bandwidth' parameter to artificially move inputs closer together which can counteract the effects of data being embedded in too high-dimensional a space [45]. This may allow kernel learning with polynomial data for systems that would not have been otherwise learnable [24]. Here, we show another simple twist on standard quantum kernel methods. Classical DNNs in the infinite-width limit can be mapped to kernel methods, and these have been shown to be competitive with finite DNNs on small data sets [46, 47]. For l-layered classical FCN ReLU networks with biases set to 0, we can recursively calculate such kernels (in the infinite width limit) $K_l = K^l(x^{(i)}, x^{(i')})$ using the following recurrence relation [32, 33],\n$\\qquad a_l^{(ij)} = \\frac{1}{\\pi} \\left[ \\sqrt{1-\\left(a_{l-1}^{(ij)}\\right)^2} + \\left( \\pi - \\arccos a_{l-1}^{(ij)} \\right) a_{l-1}^{(ij)} \\right]$ \nwhere $a^{(ij)}_0 = K_Q(x^{(i)}, x^{(i')})$ and $a^{(ij)}_1 = x^{(i)}\\cdot x^{(i')}$.\nWe can construct a kernel $K_0(x^{(i)}, x^{(i')})$ which mimics the kernel of an l-layered infinite-width FCN on Hilbert space by sampling from a QNN [48], choosing $a_0 = \\sqrt{K_Q(x^{(i)}, x^{(i')})}$ and using Equation (7) to construct $K_l$. In our experiments we only use l = 1. While this method does not necessarily improve on classical kernels (for example there is still no representation learning) it gives us access to a different kernel than the standard QNN kernel $K_Q = |\\langle x^{(1)} | x^{(2)} \\rangle|^2$.\nWe tested this model on the n = 7 Boolean dataset. $K_1$ (for a 1 hidden layer FCN) performs in general slightly worse than the FCN on the Boolean dataset in Figure 4. But, it performs better and is more expressive and/or has a more interesting inductive bias than QNNs with the encodings in Figure 2.\nWe also tested performance on a version of the FashionMNIST [49] image dataset that is popular in the QNN literature. To be learnable by QNNs, this dataset is typically hugely simplified. Here it is produced by binarising the FashionMNIST labels, performing principal component analysis on the entire dataset, and taking the top 8 components. These are then encoded with amplitude encoding on 3 qubits. This kind of downsampling is popular in the QNN literature, and can be found in standard set-ups such as Tensorflow Quantum [50]. Unfortunately, we found multiple instances in the QNN literature where the extreme simplification of these datasets is not well signposted, see also [17]. We discuss this problem in Appendix B 1. As one step forward we will call this simplified version Q-FashionMNIST, even though very little of the original FashionMNIST dataset structure is left. We use 250 training datapoints and 50 test datapoints. Interestingly, the $K_h$ kernel achieves the best accuracy of all the models we tested on Q-FashionMNIST in Figure 5."}, {"title": "B. DQNNS", "content": "To escape the kernel paradigm, some non-linear operation has to occur during the forward pass. Other existing quantum machine learning methods can achieve this. For example, convolutional neural network inspired QCNNS [51] do this via qubit measurement, and proposed continuous variable algorithms use 'non-Gaussian' gates. A classical FCN is made up of a series of perceptron-like layers. By analogy, we create a deep QNN (DQNN) by including a measurement and an extra layer, as shown in Figure 3. The first layer is a standard QNN, but with p readout qubits. The second layer is a standard QNN with q input qubits (the exact value will depend on the encoding used in the second layer), and one readout qubit. A similar DQNN with layered structure and measurements in the middle can be found in [34]. The data is encoded for input to the first layer $x^{(L=0)} = |0\\rangle |x\\rangle$. The output of this layer is then a vector z of dimension p with components zi the expectation values of the i'th readout qubit. The output of the first layer is $[\\langle q_0\\rangle,..., \\langle q_p\\rangle]$. A trainable (classical) bias term bi can be added to each of these, followed by a non-linearity \u03c3, to give $x^{(L=1)} = [\\sigma((\\langle q_0\\rangle)+ b_0),...,\\sigma((\\langle q_p\\rangle) + b_p)]$. This output can then be encoded for the second layer with some encoding method $\\phi^{(L=1)} (x^{(L=1)})$, and the second layer of the DQNN classifies this input like a regular QNN. In Appendix G1 we demonstrate that this method can model a 1-hidden layer FCN with step-function activations, and therefore satisfies universal approximation theorems.\nIn our experiments, the nonlinearity \u03c3 is thresholded at 0, and we use two encodings $\\phi^{(L=1)}$ in the second layer\n1. DQNN-\u03b1 networks use amplitude encoding - for p readout qubits in the first layer, $x^{(L=1)}$ is encoded onto $q = \\lceil \\log_2 p \\rceil$ qubits in the second layer using amplitude encoding.\n2. DQNN-\u03b2 networks use basis encoding - for p readout qubits, $x^{(L=1)}$ is encoded onto $q = p$ qubits in the second layer using basis encoding\nWe trained the DQNN with backpropagation using PyTorch [52]. Figure 4 compares these DQNNs to a 1 hidden-layer FCN and a perceptron on our Boolean dataset. Both DQNNs perform better than the QNNs in Figure 2, and DQNN-\u03b1 performs better than DQNN-\u03b2.\nIn Figure 5 we compare test performance of the DQNNs with a QNN, the hybrid quantum-classical kernel Kh, a perceptron and an FCN on the downsampled image dataset Q-FashionMNIST. We also compare a QNN trained using the TPP correspondence (where we could reach the global minimum) to a QNN trained directly with PyTorch until the loss plateaus for several hundred"}, {"title": "VI. Comparing QNN and DNN performance", "content": "So far, we have used the relative simplicity of the TPP architecture to analyze the expressivity and inductive bias of QNNs. In this section, we place these results for QNNs into the wider context of classical DNNs, the most successful general-purpose learning algorithms currently available. We ask: what barriers do QNNs face to also be useful as general-purpose learning algorithms?\nMany of the points in this section may seem quite obvious to those versed in classical deep learning. Some have already been discussed in the QNN literature (see e.g. [16]). Nevertheless, our experience of reading the recent QNN literature suggests that these ideas have not yet been assimilated. We therefore provide a more expansive discussion of than one might otherwise do in the hope that this will help move things forward.\nMismatched scales complicate the search for quantum advantage with current QNNs. Classical machine learning has benefited from many more years of study and investment than quantum machine learning has. Moreover, the power of deep learning techniques only fully emerges on large datasets and with extremely large models. For example, the modern deep-learning era is often said to have kicked off in 2012 when a CNN-based"}, {"title": "VII. Conclusions", "content": "Our results illustrate a growing (but by no means unanimous) consensus in the literature [12, 16, 17, 24, 25, 57, 58] that current QNN algorithms are unlikely to rival DNNs as general learners on classical data, even with future advances in quantum computing beyond the NISQ regime. Perhaps alternative methods such as DQNNS, or hybrid methods where a QNN provides efficient calculations within a broader classical algorithm may bring some benefits. But, this hope is tempered by the dauntingly wide performance gap between current quantum machine learning algorithms and state-of-the-art classical deep neural networks on large datasets.\nBy contrast, QNNs and their extensions may have advantages for learning certain kinds of quantum data [15, 25]. Such advantage is most likely to emerge if the problems exhibit special structure, as is the case for other quantum algorithms [56, 59]. For example, one requirement for QNNs is linear separability in Hilbert space.\nMore generally, using quantum computers as quantum simulators hold exciting promise for near-term advances in fundamental physics problems where such special structures exists [60, 61]. For instance, the ability to precisely control strongly correlated quantum systems out of equilibrium could generate new insights into complex materials like high-temperature superconductors, potentially yielding commercial impact. Searching for quantum problems with the kinds of special structures that QNNs can exploit may lead to an important new scientific understanding into the nature of quantum mechanics, making such research valuable even in the absence of commercial applications.\nThis paper is pessimistic about the prospects of QNNS outperforming DNNs on classical data. On the other hand, proving a negative is hard and surprises abound in science. Moreover, there is a wider class of QML systems beyond QNNs which may evade some of the problems we. Quantum computing has established exponential advantages in other domains and there may still be undiscovered avenues to harness its potential in quantum machine learning."}]}