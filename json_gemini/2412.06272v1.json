{"title": "Methods for Legal Citation Prediction in the Age of LLMs: An Australian Law Case Study", "authors": ["Ehsan Shareghi", "Jiuzhou Han", "Paul Burgess"], "abstract": "In recent years, Large Language Models (LLMs) have shown great potential across a wide range of legal tasks. Despite these advances, mitigating hallucination remains a significant challenge, with state-of-the-art LLMs still frequently generating incorrect legal references. In this paper, we focus on the problem of legal citation prediction within the Australian law context, where correctly identifying and citing relevant legislations or precedents is critical. We compare several approaches: prompting general purpose and law-specialised LLMs, retrieval-only pipelines with both generic and domain-specific embeddings, task-specific instruction-tuning of LLMs, and hybrid strategies that combine LLMs with retrieval augmentation, query expansion, or voting ensembles. Our findings indicate that domain-specific pre-training alone is insufficient for achieving satisfactory citation accuracy even after law-specialised pre-training. In contrast, instruction tuning on our task-specific dataset dramatically boosts performance reaching the best results across all settings. We also highlight that database granularity along with the type of embeddings play a critical role in the performance of retrieval systems. Among retrieval-based approaches, hybrid methods consistently outperform retrieval-only setups, and among these, ensemble voting delivers the best result by combining the predictive quality of instruction-tuned LLMs with the retrieval system.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in utilising Large Language Models (LLMs) for legal domain have shown promising results across various tasks. For instance, Pont et al. (2023) leveraged LLMs for generating summaries of judicial decisions, identifying legal issues, decision-making criteria, and specifying keywords. Deroy et al. (2024) revealed that LLMs outperform extractive summarisation methods in quality metrics but suffer from inconsistencies and hallucinations, highlighting the importance of human-in-the-loop approaches for improved reliability. Jiang & Yang (2023) underscored that fine-tuning LLMs demonstrates state-of-the-art performance in legal judgment prediction, while Peng & Chen (2024) showed retrieval-augmentation leads to improved accuracy by integrating external knowledge, particularly for complex charges. Hou et al. (2024) proposed a model to detect deviations between an AI-generated legal analysis and human as a way of quantifying their reliabilities. While these advancements highlight LLMs' transformative potential in legal applications, challenges like ensuring factual accuracy, handling diverse tasks, and mitigating inherent issues such as hallucination remain. For instance, Dahl et al. (2024) reports that even state-of-the-art LLMs hallucinate between 69-88% of responses to legal queries, while Magesh et al. (2024) highlights that hallucination issue is mitigated with specialisation of tools but still persists as an unresolved issue.\nIn this paper, we report progress with respect to a specific task in the legal domain, Legal Citation Prediction, for Australian Law context. Citations in legal cases, as in academic"}, {"title": "2 Data and Methods", "content": "writing, serve two purposes: first, information necessary to locate, read, and verify the material; and second, information about the authority of the source is conveyed (Axel-Lute, 1982). The second is particularly crucial in legal cases in common law jurisdictions, as in those systems most decision makers are required to follow previous decisions (Schauer, 1987). Accordingly, whilst precedent plays a fundamental role in determining how courts behave and, therefore, in how societies function, citations to, of, and between authorities are the way in which precedent is communicated. In determining matters in court, judges use citations to give weight and authority to their decisions and also, crucially, to demonstrate that they are acting appropriately and are adhering to the legal precedent that already exists. When doing so, a judge or a panel of judges will often state a legal rule \u2013 as a principal or a proposition \u2013 and then support the existence of that rule with a citation to another source; frequently a prior court decision. In other words, judges making determinations today rely on citations to not only determine legal questions in court but also to demonstrate that they are acting within their (the judges') lawful role.\nFormally, the citation prediction task can be defined as follows: Given a passage, the goal is to identify the correct legislation or precedent that applies and needs to be cited (i.e., to predict or retrieve the correct [CITATION]). The following examples illustrate the citation prediction tasks considered in this paper:\nExample 1.\nQuery: The distinction between a genuine offer of compromise and a demand to capitulate has to be recognised. See the discussion in [CITATION].\nAnswer: Leichhardt Municipal Council v Green [2004] NSWCA 341\nExample 2.\nQuery: The Tribunal is satisfied that the applicant does not fulfil the requirements of section 139(a) of the National Law, in that she lacks the mental capacity to practise medicine, as was considered in [CITATION].\nAnswer: Lindsay v Health Care Complaints Commission [2010] NSWCA 194\nExample 3.\nQuery: Whilst it is suggested that the offender's mother and grandmother have difficulty paying rent without the offender's assistance, there is no evidence of how they support themselves or their financial circumstances. There is no evidence of hardship that might meet the \u2018truly, wholly or highly exceptional' standard referred to in [CITATION].\nAnswer: Jinnette v R [2012] NSWCCA 217\nIn this paper, we aim to compare and explore different solutions for the citation prediction task. More specifically, we would like to make a comparison between the performance of the following approaches:\n\u2022 Prompting general purpose LLMs (i.e., GPT-40 (OpenAI, 2024; Achiam et al., 2023), Claude Sonnet 3.5 (Anthropic, 2024), LLaMA-3.1-70B-instruct (Grattafiori et al., 2024), Command R+ (Cohere, 2024))\n\u2022 Prompting law-specialised pre-trained LLMs (i.e., SaulLM-7B-instruct (Colombo et al., 2024b), SaulLM-54B-instruct (Colombo et al., 2024a))\n\u2022 Retrieval-only setup with vectorised DB using general-purpose (i.e., text-embedding-3-large) and law-specific embeddings (i.e., AusLaw-embedding-v1.0)\n\u2022 Instruction fine-tuning LLMs (i.e., SaulLM-7B, and LLaMA-3.1-8B) for the citation prediction task\n\u2022 Different hybrid tactics that combine LLMs and retrieval systems (i.e., retrieval-augmented generation, query expansion, voting ensemble)"}, {"title": "Data and Methods", "content": "Data Source and Curation. We use the NSW Caselaw section of the Open Australian Legal Corpus 2. We identified 82, 530 citations that specifically referred to a case within the dataset. For each citation (ck), we extracted the sentence in which the citation appeared (denoted as Si), along with the preceding sentence (denoted as S1). We further utilised an LLM to generate an auxiliary description of ck, based on (Full Textck, S, S1) where Full Textck refers to the full text of the cited case. An example of a generated RoC is provided below:\nS1, S: In considering what if any orders should be made in regards to the surface roots, I am not satisfied to the level required by s 10(2) of the Act, that there is any real likelihood of injury arising from those roots. Craig J in Leichhardt Municipal Council v Green [2004] NSWCA 341, considered that 'something more than a theoretical possibility is required in order to engage the power under the Trees Act'.\nCITATION ck: Leichhardt Municipal Council v Green [2004] NSWCA 341\nGenerated Reason-of-Citation (RoC): The cited case is referenced to establish the standard required to demonstrate a likelihood of injury under the Trees Act.\nWe refer to this description as Reason-of-Cite (RoC). Each reference to a citation ck in the data results in a unique new RoCck. For each ck, we denote its M references as RoC1ck, RoC2ck,..., RoCMck. We will discuss later how the RoCs are used. This resulted in the final dataset of 55, 005 instances, covering 18,677 unique citations. Within this set, 5% of the citations have been referenced at least 9 times, while 54% were cited only once. From this final set, we extracted 1k citations as test set, and used the rest for training (i.e., our instruction-tuned models).\n2.1 Methods\nWe investigate various methods under Open World and Closed World settings. The Open World setting places no restriction over the possible predictions from the system (i.e., similar to how an LLM functions in real-world), whereas the Closed World setting confines the output space to be from the set of 18, 677 citations present in the database (i.e., similar to a standard retrieval setup).\n2.1.1 LLM-Only\nWe explored both existing LLMs (General purpose and Law-specialised) as well as our instruction-tuned LLMs.\nExisting General Purpose LLMs. For the general purpose LLMs we used GPT-40 (OpenAI, 2024; Achiam et al., 2023), Claude Sonnet 3.5 (Anthropic, 2024), LLaMA-3.1-70B-instruct (Grattafiori et al., 2024), and Command R+ (Cohere, 2024). When directly prompted with the query, all these LLMs demonstrated near-zero performance. To gain deeper insight, we leaked the RoCs along with the query text. This approach, while bypassing the task's requirement of predicting the citation without access to any information about the case-to-be-cited, allowed us to estimate the upper-bound performance of these models. More on this will be discussed in Section 3.\nExisting Law-specialised LLMs. For law-specialised LLMs, we used SaulLM-7B-instruct (Colombo et al., 2024b), SaulLM-54B-instruct (Colombo et al., 2024a) which to"}, {"title": "Our Instruction Fine-tuned LLMs", "content": "the best of our knowledge are the only publicly available law LLMs for English to this date.\nFor prompting, we followed the exact setting of general purpose LLM experiments.\nOur Instruction Fine-tuned LLMs. We instruction-tuned LLaMA-3.1-8B and SaulLM-7B-Base models on the training data. These resulting models are referred to as Cite-LLaMA-3.1-8B and Cite-SaulLM-7B in our experiments. At inference time, given the query (S1, Mask(S)) the model first produces the RoCck and then ck (i.e., p(ck, RoCck | I, S1, Mask(S); \u03b8), where \u03b8 denotes LLM parameters, I denotes the instruction, Mask(S) denotes St with the citation to ck being masked). See Appendix A.2 for further details on training parameters and instruction detail and format. Unlike the previous setups, for all experiments with the instruction-tuned models only the text of the query (i.e., no RoC) was provided and the model is instructed to predict both the RoCcr and the citation ck. This presents a challenging setup as the LLM needs to predict a citation in the Open World setting solely by its parametric knowledge and the brief information provided in the query text."}, {"title": "Retrieval-Only", "content": "2.1.2 Retrieval-Only\nWe investigated retrieval setup along two axes: embeddings, and granularity of data to be indexed in the vectorised database. The basic principle is a retrieval task where a query is matched against all records of a database, and the Top-k closest records are returned. The closeness is measured in the semantic space through vector embeddings. In all our setups involving retrieval, the search returns the Top-5 most relevant hits (i.e., citations).\nEmbeddings. For embeddings in the retrieval system (both for representation of queries and the vectorized database) the 3072 dimensional text-embedding-3-large from OpenAI as generic embeddings, and the 384 dimensional vectors of AusLaw-embedding-v1.0 as Australian law-specialised embeddings were used.\nDatabase Granularities. To construct the database, three possibilities for repre-senting each citation (ck) were considered: (1) Full Case Text (Full Casec), (2) Catchwords (Catchwordsck), and (3) Aggregation of all its corresponding M RoCs (RoC Aggregations = concat(RoC1ck, RoC2ck,..., RoCMck)). We refer to these different settings as Index Granularity later in the experiments (i.e., Table 1). For each granularity level, we created a distinct database version for each embedding backbone.\n2.1.3 Hybrid of LLM and Retrieval\nWhat follows can be regarded as various instances of Retrieval Augmented Generation (RAG). However, in our manuscript, we strictly refer to RAG as the scenario where the LLM generation is guided by retrieval, rather than retrieval being influenced by the generation.\nQuery Expansion. In this setting, given the query, the LLM was first asked to produce a potential description of a good citation. We denote this as RoCaux to underscore our deliberation in eliciting what the LLM could semantically generate as an auxiliary RoC. The query is then expanded from Text to Text+RoCaux. The expanded query is launched into the database, following the standard retrieval setup.\nVoting Ensemble First, following the LLM-only setup, instruction-tuned LLMs (e.g., Cite-LLaMA-3.1-8B) generate the RoC' and the citation. Next, the query is expanded into Text+RoC' and fed into the retrieval system. If the LLM-generated citation appears in the Top-5 retrieval results, it is returned; otherwise, the Top-1 result from the retrieval system is returned. This is to leverage the benefit of LLM-only setup by reducing its hallucination (i.e., producing citations outside the corpus).\nRetrieval Augmented Generation (RAG). This follows a standard Retrieval-Augmented Generation (RAG) setup, where queries are sent to the database, and the Top-5 results are retrieved for the LLM to re-rank and select the Top-1 match."}, {"title": "3 Experiments", "content": "Experimental Setups. LoRa (Hu et al., 2022) and 8-bit quantization was used for the instruction tuning stage. For details regarding the instruction fine-tuning step please refer to Table 2 of Appendix A.2. As evaluation metrics, Accuracy@1 and Accuracy@5 are used.\n3.1 Results\nThe main results are presented in Table 1. We structure our discussion of results along the comparisons outlined in the introduction (\u00a71).\nPre-training on general text vs. law-specialized text. The results of the LLM-only experiments (1st panel of Table 1) reveal that pre-training alone is insufficient for achieving satisfactory performance in the citation prediction task. Neither general-domain nor law-specialized models demonstrated reasonable accuracy. For example, Claude Sonnet 3.5 achieved only 15% accuracy in predicting citations when provided with both the query text"}, {"title": "Pre-training vs. Instruction tuning", "content": "and the RoC. In contrast, both the 7B and 54B variants of SaulLM performed even worse, achieving 2% accuracy or less, despite being explicitly pre-trained on the Open Australian Legal Corpus (including the NSW Caselaw subset used in this study) and other law-specific datasets.\nPre-training vs. Instruction tuning. A significant performance boost is observed when comparing pre-trained LLMs to their instruction-tuned counterparts on our training data. For example, the SaulLM-7B model's performance jumps from 0% to 51.7% in the more challenging Text-only query setup. Similarly, LLaMA-3.1-8B, a general-purpose model, achieves 46.2% accuracy after instruction fine-tuning, surpassing its much larger pre-trained 70B counterpart. Notably, between the LLaMA and SaulLM backbones, instruction tuning on the same dataset and for the same number of epochs proves more effective for the domain-specific SaulLM. This underscores the critical importance of not only domain-specialized pre-training but also targeted fine-tuning on task-specific data and requirements. As anticipated, Figure 2 demonstrates a negative correlation between predictive accuracy and the citation"}, {"title": "Generic vs. domain-specialised embeddings", "content": "frequency in the dataset. Cases cited more than 100 times achieve 100% accuracy, while accuracy drops below 40% for cases cited 20 times or fewer. This reflects the challenge of accurately predicting less frequently cited cases.\nGeneric vs. domain-specialised embeddings. When comparing the best results of each block that involves retrieval: red rows (corresponding to the generic text-embedding from OpenAI) vs. orange rows (corresponding to embeddings trained exclusively on the Open Australian Legal Corpus) in each block of results in Table 1, it is important to to acknowledge that this comparison is not entirely fair due to differences in embedding dimensionality (3072 vs. 384), the volume of data used, and the training algorithms employed. Nonetheless, we observe in each block exhibiting an intuitive pattern in favour of the domain-specialised AusLaw-embeddings (while being 8\u00d7 smaller in dimensionality). This highlights the effectiveness of domain-specialized embeddings and points to a promising future direction: training larger embeddings tailored to the Australian legal domain."}, {"title": "Database granularity matters", "content": "Database granularity matters. The results indicate the significant role the granularity of DB plays in the accuracy of the retrieval system. Contrary to our expectations, Catchwords proved to be the least effective granularity, performing worse than Full Cases. By a substantial margin, the best performance was achieved with the RoC Aggregations granularity. This pattern is consistent for both the generic and law-specialised embeddings, and across all setups except for the Retrieval-only were Full Case leads to slightly better results compared with Catchwords.\nQuery expansion vs. voting ensemble vs. RAG. Comparing all methods involving a retrieval component, Hybrid methods are consistently better than Retrieval-only. Among the Hybrid methods, the Voting Ensemble is the best, followed by RAG and then Query Expansion. The superiority of Voting Ensemble highlights the advantages of combining the predictive quality of instruction-tuned LLMs with the robustness of a retrieval system. In contrast, RAG relies on query augmentation to guide the LLM's predictions within the context of the Top-5 retrieved citations, while the Query Expansion method instead focuses on re-adjusting the query's semantic space before searching the retrieval space."}, {"title": "Best configurations", "content": "Best configurations, and what we can learn from ACC@5. The best result across all settings is achieved via Cite-SaulLM-7B model (i.e., the first row highlighted in green). The second best results are under the Cite-LLaMA-3.1-8B model (both in the LLM-only and the Voting Ensemble setups). While the Query expansion results are not competitive at ACC@1, the promising 60.4 for Cite-SaulLM-7B for ACC@5 (the second row highlighted in red) suggests that with improved re-ranking of the Top-5 hits, there is potential to boost accuracy by an additional 10%. We leave further exploration of this to future work."}, {"title": "4 Conclusion", "content": "In this paper, we focused on an Australian law case study for the citation prediction task. We examined various approaches to leveraging Large Language Models and retrieval systems, evaluating their effectiveness both independently and in combination. Our findings demonstrate that while pre-training large language models on general or even domain-specialized legal texts is a necessary starting point, it is far from sufficient for achieving satisfactory citation prediction accuracy in the Australian legal domain. The most contributing factor for improving performance lies in targeted instruction tuning with task-specific data, which dramatically boosts accuracy. Our experiments further reveal the importance of choosing the right embedding model and database granularity for retrieval, with results showing up to 70% variation in performance under different granularities. Among the retrieval-augmented methods, ensemble voting strategy stands out as the most effective, outperforming methods like RAG and query expansion.\nAs potential for future explorations, training better embeddings (i.e., larger dimensionalities, with specific attention to discriminative features), and re-ranking methods to better utilise the Top-k outputs from retrieval system are promising avenues. Additionally, in this paper"}, {"title": "A Appendix", "content": "We did not test the methods in the out-of-distribution setting which could further highlight other potential challenges."}, {"title": "A.1 Three Examples of Catchwords", "content": "A.1 Three Examples of Catchwords\nCase 1: CRIME \u2013 Appeals \u2013 Appeal against conviction \u2013 Unreasonable verdict \u2013 two counts of sexual offences \u2013 where applicant found guilty on one count and acquitted on the other whether on all of the evidence it was open to the jury to be satisfied of the applicant's guilt beyond reasonable doubt \u2013 discrepancies and inconsistencies in complainant's evidence \u2013 appeal allowed \u2013 conviction quashed.\nCase 2: DEVELOPMENT APPEAL \u2013 dual occupancy \u2013 contentions resolved - covenant \u2013 variation of covenant \u2013 weight to be given to covenant - view sharing \u2013 community objections.\nCase 3: PROCEDURE \u2013 application for separate questions under UCPR 28.2 \u2013 where plaintiff seeks orders under Part 1C of the Civil Liability Act 2002 to set aside settlement agreements which may otherwise preclude the plaintiff from maintaining the balance of the proceedings - claimed benefit to defendants of plaintiff's promise to \u201ctake no action\u201d\u2013 statutory right to commence proceedings subsequently enacted \u2013 construction of s 7D(1) of the Civil Liability Act 2002 \u2013 overlap in issues and credit for hearing of separate questions and the balance of the proceedings \u2013 application for separate questions rejected."}, {"title": "A.2 Details of Instruction Fine-tuning", "content": "A.2 Details of Instruction Fine-tuning\nThe instruction details and training configurations used for instruction fine-tuning are listed in Table 2.\nAspect\tDetails\nInstruction\tPredict the name of the case that needs to be cited in the text and explain why it should be cited.\nInput\ttest_set[i]['citation_text').replace(test_set[i]['cited_case_name'], '<CASENAME>')\nOutput\ttest_set[i]['citation_reason'] + <test_set[i]['citation']>\nTraining Details\nGPU\tSingle A100 GPU with 80G memory\nOptimizer\tadamw_torch\nEpochs\t10\nLearning Rate\t2e-4\nQuantization\tUsing 8-bit quantization for LoRA training\nLORA Settings\nr\t16\na (LORA Alpha)\t32\nDropout\t0.05\nTarget Modules\t[up-proj, down_proj, gate proj, k_proj, q-proj, v-proj, o-proj]"}, {"title": "A.3 Prompt details for various experiments", "content": "A.3 Prompt details for various experiments\nAll prompting details (or few-shot demonstrations when applicable) used in our experiments are provided in Table 3-6."}]}