{"title": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs", "authors": ["Nitay Calderon", "Roi Reichart", "Rotem Dror"], "abstract": "The \"LLM-as-a-judge\" paradigm employs Large Language Models (LLMs) as annotators and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure \u2013 the Alternative Annotator Test (alt-test) \u2013 that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-40), outperforming open-source LLMs, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.", "sections": [{"title": "1 Introduction", "content": "The rise of Large Language Models (LLMs) has transformed the field of Natural Language Processing (NLP), bringing unprecedented capabilities in reasoning and generating human-like text (Kojima et al., 2022; Achiam et al., 2023; Laskar et al., 2023; Yang et al., 2024). Recently, a new trend has emerged where LLMs are employed as annotators and judges across various NLP applications (Li et al., 2024a; Tan et al., 2024b).\nOne key advantage of LLM-as-a-judge\u00b2 is the scalability and speed of LLMs. They can quickly annotate large-scale datasets, reducing the time required for tasks traditionally performed by costly human annotators (Nasution and Onan, 2024). Additionally, LLMs avoid challenges inherent to human factors, like fatigue and guideline misinterpretation (Uma et al., 2021; Bartsch et al., 2023). Unsurprisingly, they sometimes outperform crowd-workers (Gilardi et al., 2023; Nahum et al., 2024).\nIndeed, LLMs-as-judges are extensively used in research, taking on a pivotal role once filled by human annotators. LLMs are employed to annotate new datasets (Gat et al., 2024; Tan et al., 2024b), or refine existing ones (Nahum et al., 2024; Pavlovic and Poesio, 2024), and commonly serve as evaluators for benchmarking models and methods (Ahmed et al., 2024; Gu et al., 2024; Li et al., 2024a). Accordingly, LLM-as-judges directly shape the results, findings, and insights of the NLP community and guide the direction of scientific inquiry, prioritization, and innovation.\nLLMs' influence extends far beyond the NLP field. They annotate papers for literature reviews (Calderon and Reichart, 2024; Joos et al., 2024) or extract findings from academic literature (Khraisha et al., 2024; Naik et al., 2024). They are also utilized in cognitive sciences to simulate human subjects (Aher et al., 2023; Shapira et al., 2024) and in social science, researchers leverage LLM annotations to uncover social and cultural insights (Ventura et al., 2023; Ziems et al., 2024).\nDespite the advantages of the LLM-as-a-judge paradigm, research shows that LLMs amplify biases, leading to unfair or inconsistent judgments (Ashktorab et al., 2024; Chen et al., 2024c; Ye et al., 2024) and that they may struggle with tasks that require deep contextual understanding or domain-specific expertise (Ravid and Dror, 2023; Szymanski et al., 2024). These weaknesses highlight the need for rigorous evaluation and transparency when relying on LLM annotations in research.\nYet, many studies employing LLM annotations do not explicitly measure the alignment between LLMs and humans, and those that do typically use traditional measures such as accuracy (% agreements), F1 score, Cohen's Kappa, and correlation (Li et al., 2024b), which have limitations. Particularly, these measures frequently rely on majority vote labels, overlooking important nuances introduced by individual annotators. Additionally, there are no established criteria for making a definitive yes/no decision as to whether an LLM can replace human annotators (e.g., \u201cis an F1 score of 0.6 sufficient?\u201d). This decision demands statistical rigor, which often lacks in the way researchers apply traditional measures. Moreover, they can only evaluate whether an LLM matches human performance but cannot determine if it offers a better alternative.\nWe argue that to justify using an LLM instead of human annotators, researchers should demonstrate that the LLM offers a better alternative to recruiting human annotators. In other words, when factoring in the cost-benefit and efficiency advantages of LLM annotations, they should be as good as or better than human annotations. In this paper, we propose a statistical procedure to verify this claim, which we call the Alternative Annotator Test, or simply alt-test. This procedure is simple and requires minimal effort to apply it involves comparing the LLM to a small group of human annotators (at least three) on a modest subset of examples (between 50 and 100). Our procedure is described in \u00a73 and illustrated in Figure 1. Once applied, researchers can confidently rely on the LLM's annotations for their work.\nIn addition, we define a measure for comparing LLM judges called the Average Advantage Probability. This measure is naturally derived from our statistical procedure and represents the probability that the LLM is as good as or better than a randomly chosen human annotator. It possesses desirable properties that traditional measures lack while maintaining a high correlation with them. It is versatile, supports different types of annotations, and is highly interpretable.\nWe exemplify the application of our procedure with six LLMs and four prompting techniques. To this end, we curate a diverse collection of ten datasets, each with instances annotated by multiple annotators. Our datasets vary in size, annotation types (discrete, continuous, and free-text), number of annotators (2 to 13), and levels of annotator expertise (crowd-workers, skilled annotators, and experts). They encompass a wide range of language tasks, including two vision-language tasks.\nOur results demonstrate that LLMs can, in some cases, replace human annotators. We found that closed-source LLMs (such as GPT-40 and Gemini-1.5) consistently outperform open-source models (like Mistral-v3 and Llama-3.1), passing the alt-test on eight datasets. Additionally, we show that in-context learning improves LLM performance across most datasets, while chain-of-thought and ensemble methods do not yield similar benefits.\nOur contributions: (1) We propose a statistical procedure, the alt-test, to justify replacing human annotators with LLMs; (2) We introduce a versatile and interpretable measure, the average advantage probability, for comparing LLM judges; (3) We curate a diverse collection of ten datasets and analyze six LLMs and four prompting techniques, demonstrating that LLMs can sometimes replace humans; (4) We develop a theorem regarding the optimal LLM-as-a-judge (\u00a7B); and (5) We discuss modifications of the alt-test to handle label imbalance (\u00a7A.1), and the choice of annotators (\u00a76.3).\nWe encourage researchers to adopt our procedure and hope this study paves the way for rigorous scientific practices in NLP and beyond."}, {"title": "2 Previous Work", "content": "Research on LLMs as annotators and judges is a rapidly growing field (Chiang et al., 2023; Zheng et al., 2024a), resulting in numerous surveys (Gu et al., 2024; Li et al., 2024a; Tan et al., 2024b; Pavlovic and Poesio, 2024). Most studies focus on enhancing LLM performance, either by parameter tuning (Gekhman et al., 2023; Yue et al., 2023; Zhu et al., 2023; Jiang et al., 2024; Kim et al., 2024) or prompting strategies (Bai et al., 2023; Moniri et al., 2024; Song et al., 2024). For instance, Dong et al. (2024) investigated personalized LLM judges, Verga et al. (2024) proposed using a panel of diverse LLMs, and Chen et al. (2024b) extended LLM-as-a-judge to multimodal tasks.\nUtilizing LLMs to emulate human annotations raises two statistical questions. The first is how to accurately estimate the parameter of interest when relying on LLM annotations. For instance, social scientists estimate the level of toxicity to study public discourse dynamics (Avalle et al., 2024). When using LLM annotations instead of human annota-"}, {"title": "3 Method", "content": "We propose using an LLM-as-a-judge instead of human annotators when it offers a comparable alternative to recruiting an annotator. By comparing the predictions of the LLM to those of humans, we can evaluate which more closely emulates the gold label distribution. Gold labels represent the \"true\" or ground truth annotations and are typically determined through rigorous processes, such as consensus among experts or extensive quality control. Consequently, since experts are expensive and often inaccessible, we assume gold labels are unavailable. Hence, a common approach is to approximate them using the collective responses of multiple annotators. This is the exact setup we use in this paper: a modest subset of randomly sampled examples, each annotated by multiple annotators.3\nAccordingly, a key consideration in our method is that the perspective of every annotator is valued. Specifically, we exclude one annotator at a time and evaluate how well the LLM's annotations align with those of the remaining annotators. Similarly, we evaluate the alignment of the excluded annotator with the remaining annotators. We then compare the LLM and the excluded annotator based on their alignment scores, justifying the use of the LLM-as-a-judge if the LLM aligns more closely with the collective distribution than an individual does.\nNotations and Definitions For a dataset of n instances {x1,...,Xn} and m human annotators {h1,...,hm}, we denote the annotation of the jth annotator for instance xi as hj(xi). The annotation predicted by the LLM is denoted as f(xi). In addition, [-j] represents the set of indices from 1 to m excluding the jth index, i.e., [\u2212j] = {1, ..., j \u2212 1, j + 1, . . .,m}. The set of indices of the instances annotated by hj is denoted as Ij. Similarly, Hi is the set of indices of human annotators that annotated instance xi. For example, assume we have three instances in our data and four human annotators. I2 = {2,3} means that the second annotator, h2, annotated instances x2 and x3, and H\u2081 = {1,3,4} means that the first instance, x1, was annotated by the first, third, and fourth annotators, h1, h3, h4."}, {"title": "3.1 Computing the Instance Alignment Score", "content": "We start by examining the removal of each human annotator hj in turn and compute a score that measures the alignment between the annotations of the [-j] human annotators and the annotation of the LLM for instance xi. We use S(f, xi, j) to denote the alignment scoring function between f(xi) and the annotations of the human annotators excluding annotator hj. For example, S could be RMSE (root mean squared error) in regression tasks (continuous numerical labels) or ACC (accuracy) in classification tasks (categorical or rank labels).\nIn generation tasks (e.g., machine translation), S can be computed using a relevant evaluation metric (denoted as sim) that typically measures the similarity between the LLM-generated output and the human-generated output, such as BERTScore (Zhang et al., 2019). For convenience, we assume that higher values of S indicate a better alignment between an LLM and the human annotators; thus, we use negative RMSE. Below, we formally define the mentioned variants of S:\n-RMSE(f, xi, j) = -$\\sqrt{\\frac{1}{|H_i[-j]|}\\sum_{k \\in H_i[-j]} (f(x_i) - h_k(x_i))^2}$\nACC(f, xi, j) = $\\frac{1}{|H_i[-j]|}\\sum_{k \\in H_i[-j]} 1\\{f(x_i) = h_k(x_i)\\}$\nSIM(f, xi, j) = $\\frac{1}{|H_i|}\\sum_{k \\in H_i[-j]} sim(f(x_i), h_k (x_i))$\nNote that -RMSE(hj,xi,j), ACC(hj,xi,j), and SIM(hj, xi, j) represent score differences between hj and the other annotators. Consequently, we are interested in comparing S(f, xi, j) to S(hj, xi, j)."}, {"title": "3.2 Estimating the Advantage Probabilities", "content": "After computing the alignment score for each instance, we estimate the likelihood that the LLM achieves a comparable alignment with the annotators to that of the excluded annotator. The estimator will be constructed by calculating the percentage of instances for which the score of the LLM, S(f, xi, j), was higher or equal to the score of the jth excluded human annotator, S(hj, xi, j).\nWe introduce the following indicator, $W_{i,j}^f$, aiming to capture whether the LLM annotation is as good as or better than the annotation of the jth excluded annotator for instance xi (i.e., aligns equally or more closely with the remaining annotators):\n$W_{i,j}^f = \\begin{cases} 1, & \\text{if } S(f, x_i, j) \\geq S(h_j, x_i, j) \\\\ 0, & \\text{otherwise} \\end{cases}$\nSimilarly, we define the indicator $W_{i,j}^h$ by reversing the inequality (to \u2264) in the definition above, representing that the annotation of hj for xi is as good as or better than that of the LLM.\nThe expectation of $W_{i,j}^f$ represents the probability that the LLM annotations are as good as or better than those of hj. We estimate this probability by averaging $W_{i,j}^f$ values across all instances:\n$\\hat{p}_j^f = \\hat{P}(LLM \\geq h_j) = \\frac{1}{|I_j|} \\sum_{i \\in I_j} W_{i,j}^f$\nWe denote this estimation of the advantage over hj probability as $\\hat{p}_j^f$. Similarly, $\\hat{p}_j^h$ estimates the probability that hj holds an advantage over the LLM, calculated by averaging the values of $W_{i,j}^h$. The set {$(\\hat{p}_j^f, \\hat{p}_j^h)$}$_{j=1}^m$ is used in our statistical procedure to determine whether the LLM can be used."}, {"title": "3.3 Should the LLM Replace Annotators?", "content": "Using an LLM instead of a human annotator is justified if the LLM offers a reliable alternative to hiring an annotator. To formalize this, if $\\hat{p}_j^f$ is significantly larger than $\\hat{p}_j^h$ it indicates that employing the LLM instead of hj is a justified evidence-based decision. Notice, however, that employing an LLM is a cheaper and less labor-intensive alternative. Therefore, we introduce \u025b,4 a cost-benefit hyperparameter which penalizes $\\hat{p}_j^f$ to reflect the higher cost and effort associated with human annotation.\nWe define the following set of hypothesis testing problems to test if the LLMs' relative advantage probability is significantly larger than that of hj:\n$H_{0j}: \\hat{p}_j^f \\leq \\hat{p}_j^h - \\epsilon \\text{ vs. } H_{1j}: \\hat{p}_j^f > \\hat{p}_j^h - \\epsilon$\nThe appropriate statistical test for this hypothesis problem is a paired t-test (Dror et al., 2018), which examines the difference between the ith indicators: di,j = $W_{i,j}^h$-$W_{i,j}^f$. The null hypothesis asserts that $\\hat{d}_j = \\hat{p}_j^f - \\hat{p}_j^h$ is greater than or equal to \u025b, while the alternative hypothesis posits that it is smaller. The test statistic tj is defined as:\n$t_j = \\frac{\\hat{d}_j - \\epsilon}{s_j / \\sqrt{n}}$ where $s_j = \\sqrt{\\frac{\\sum_{i=1}^n (d_{i,j} - \\hat{d}_j)^2}{n-1}}$\nThe p-value can be calculated using a student's t-distribution table. However, when n < 30, the normality assumption may not hold, and a non-parametric test (e.g., Wilcoxon signed-rank) should be used. If the p-value < a (typically a = 0.05), or equivalently when tj < ta, we reject the null hypothesis, concluding that the LLM holds a statistically significant advantage over hj when considering the cost-benefit tradeoff.\nSo far, we discussed the advantage of LLMS over a single human annotator. To generalize our conclusion to any annotator, we measure the percentage of annotators that the LLM \u201cwins\u201d, i.e., the proportion of rejected null hypotheses. We denote this winning rate (WR) by \u03c9, formally:\n$\\omega = \\frac{1}{m} \\sum_{j=1}^m 1\\{H_{0j} \\text{ is rejected}\\}$\nwhere $1\\{H_{0j} \\text{ is rejected}\\}$ is an indicator that receive one if the null hypothesis is rejected and zero, otherwise. If w > 0.5,5 then the LLM wins the majority of human annotators, hence we assert that it can replace human annotators.\nMultiple Comparison Correction Simply counting the number of rejected null hypotheses is problematic due to the accumulation of Type-I errors when performing multiple hypothesis tests, particularly when the hypotheses are dependent (Dror et al., 2017). In our case, the dependency arises because the score of hj relies on the annotations of the remaining [-j] annotators (see how S is defined). The standard practice to address this issue is a multiple comparison correction.\nWe suggest using a procedure that controls the false discovery rate (FDR), which is the expected proportion of false positives (incorrect rejections of null hypotheses) among all rejected hypotheses in a multiple-hypothesis testing scenario. In other words, the FDR-controlling procedure ensures that the observed WR w is reliable and does not overestimate the true percentage of wins due to accumulated false rejections or dependence between hypotheses.\nSummary: the Alt-Test As illustrated in Figure 1, the alt-test involves the following steps: First, we compute the set of probabilities {$(\\hat{p}_j^f, \\hat{p}_j^h)$}$_{j=1}^m$, where each $\\hat{p}_j^f$ represents the advantage of the LLM over hj and vice versa. Next, we conduct m one-sample proportion t-tests for the difference $\\hat{p}_j^f$\\hat{p}_j^h against \u025b, resulting in a corresponding set of m p-values. We then apply the BY procedure to these p-values, which identifies the set of rejected null hypotheses. Finally, we compute the winning rate (the proportion of rejected hypotheses) and if w > 0.5, we can statistically justify using LLM annotations."}, {"title": "3.4 How to Compare LLM Judges?", "content": "In many scenarios, we wish to compare different LLM judges. For example, when selecting the most suitable LLM for deployment or when evaluating novel LLM-as-a-judge methods against existing baselines. Although our primary objective is to provide a statistical procedure for justifying the use of LLM annotations, our procedure also naturally supports the comparison of multiple judges.\nWhile it is possible to compare different LLM judges by their winning rate (w), we argue this is suboptimal. First, w does not account for the magnitude of the wins. For example, $\\hat{p}_j^f$ = 0.9 and $\\hat{p}_j^h$ = 0.6 contribute equally to w if their respective null hypotheses are rejected. Second, w depends on the value of \u025b, and third, the range of its possible values depends on the number of human annotators, making it a coarse measure. For instance, with only three annotators, w value is limited to 0, 13, 2/3, 1. Therefore, for comparing LLM judges, we propose the Average Advantage Probability (AP):\n$\\rho = \\frac{1}{m} \\sum_{j=1}^m \\hat{p}_j^f$\nWe argue that \u03c1 is a good measure for comparing LLM judges due to its desirable properties. Unlike \u03c9, \u03c1 spans a denser range of values and accounts for the magnitude of $\\hat{p}_j^f$s. Furthermore, it is more interpretable than traditional measures like F1, Cohen's K, or correlation it directly represents the probability that the LLM annotations are as good as or better than those of a randomly chosen annotator. This intuitive interpretation makes it accessible and meaningful for decision-makers. Finally, \u03c1 can be applied consistently across different types of annotation tasks (discrete, continues, and free-text), providing a unified evaluation framework that eliminates the need to switch between measures."}, {"title": "The Optimal LLM-as-a-Judge", "content": "We now turn to the question of what constitutes the optimal LLM-as-a-judge. We define it as an LLM that achieves an advantage probability of \u03c1 = 1 (since w depends on n and \u025b, we do not include it in the theorem). The optimal LLM-as-a-judge naturally depends on the choice of the scoring function, S(f, xi, j). The theorem below addresses two functions: ACC (for discrete tasks) and -RMSE (for continuous tasks). See Appendix \u00a7B for more details and the proof.\nTheorem 1 (Optimal LLM-as-a-Judge). For a given dataset, let S(f, xi, j) be the alignment scoring function. The optimal LLM-as-a-judge, denoted as f*(xi), is defined as follows:\nIf S = ACC, then f*(xi) = MV(xi), predicting the majority vote of the annotators for xi.\nIf S = -RMSE, then f*(xi) = $\\frac{\\sum_{k \\in H_i} h_k (x_i)}{|H_i|}$, predicting the mean annotation for xi.\nIn both cases, the optimal LLM-as-a-judge achieves an advantage probability of \u03c1 = 1."}, {"title": "4 Experimental Setup", "content": "4.1 Datasets\nWe conducted experiments on ten diverse datasets, varying in size, number of human annotators, and types of annotators (crowd-workers, skilled annotators, or experts)."}, {"title": "4.2 LLMs", "content": "The six models that were used as candidate LLM annotators for our experiments are Gemini-1.5-Flash and Pro"}, {"title": "5 Results", "content": "Table 2 presents the performance of various LLMs across discrete, continuous, and free-text tasks. We report three key measures: traditional LLM-human alignment measures (accuracy, Pearson's correlation, and similarity), the winning rate (WR, denoted as w), and the average advantage probability (AP, denoted as \u03c1). For each dataset, we selected \u025b values based on the type of annotators (as indicated in Table 1): experts (\u03b5 = 0.2), skilled annotators (\u03b5 = 0.15), and crowd-workers (\u03b5 = 0.1). See the discussion in \u00a76.2 for an explanation of these choices. Below, we summarize our main findings:\nLLMs can sometimes replace humans. Table 2 shows that many LLMs pass the alt-test across various datasets. While in two datasets (MT-Bench, and SummEval), none of the LLMs pass the test, in four (Framing, CEBAB-A, CEBaB-S and Lesion), almost all LLMs achieve w \u2265 0.5. In the free-text dataset KiloGram, only Gemini-Flash passes the test. The results suggest that in many scenarios, employing LLMs can be an alternative to recruiting additional human annotators.\nHowever, these positive news does not imply that LLMs can always replace human annotators. The success of LLMs is nuanced and aspect-dependent. In Table 5 in the Appendix, we analyze three datasets, breaking them down into sub-annotation tasks corresponding to different aspects. For instance, in the SummEval dataset (which will be discussed later), summary annotations are divided into four aspects: coherence, consistency, fluency, and relevance. Notably, each aspect may require varying levels of expertise and capabilities, and indeed, the performance of LLMs varies accordingly. In the Lesion dataset, which involves annotating five aspects of skin lesion images, all LLMs pass our test on color-related aspects (e.g., identifying the number of colors or the presence of a bluish glow) but struggle with shape-related aspects, such as assessing asymmetry or border irregularity. In the LGBTeen dataset, all LLMs excel in the sensitivity aspect, while for five other aspects (out of ten), only one or two LLMs pass the test. In the remaining four aspects, all LLMs fail. Notably, the aspects where LLMs struggle often require higher emotional intelligence or contextual understanding (e.g., the Mental and Completeness aspects; see Lissak et al. (2024)). Finally, in SummEval, most LLMs pass the test for two aspects, Coherence and Relevance, but fail on the other two.\nOur results demonstrate that test success depends on dataset and annotation aspect, with LLMs often failing to pass it. This emphasizes the relevance of the alt-test: researchers cannot simply rely on LLM annotations without first justifying their use.\nTraditional measures strongly correlate with the average advantage probability. In addition to the statistical procedure, our method enables comparing LLM judges using the average advantage probability, \u03c1. In subsection \u00a73.4, we outlined the desired properties of \u03c1, such as its interpretability (as it directly represents the likelihood of the LLM being as good as or better than a random annotator) and its flexibility, allowing it to be applied to various types of annotation tasks.\nNotably, in almost all datasets, the top-ranked LLM is the same based on \u03c1 values and the traditional measures. Furthermore, in discrete tasks, the ranking of models based on Accuracy and \u03c1 shows a strong correlation, with an average Kendall \u315c value of 0.92. Other tasks also correlate highly, with an average Kendall \u315c value of 0.84, except for SummEval, which shows a negative correlation. We discuss this anomaly later.\nFew-Shot improves LLM-human alignment. Table 2 indicates that closed-source LLMs (GPTs and Geminis) outperform their open-source counterparts. In discrete tasks, GPT-40 and Gemini-Pro consistently are the best performing LLMs, while in continuous tasks, no single model emerges as the clear winner. However, Table 2 reports only zero-shot experiments. Thus, we also conducted experiments using three other strategies: few-shot, CoT, and ensemble. The results are presented in Table 3 and are based on three annotators and 100 randomly sampled instances from five datasets.10 The reduced sample size was chosen to minimize computational costs and primarily to reflect practical constraints better, as researchers are unlikely to annotate thousands of instances for testing whether the LLM is a good judge via statistical testing.\nAs shown in Table 3, the few-shot approach (with four demonstrations) improved the performance of nearly all LLM judges. Importantly, three few-shot LLMs achieved w \u2265 0.5 on SummEval, a result not observed in the zero-shot setting. This success can be attributed to the demonstrations in the prompt, which helped align the LLMs' scoring distributions more closely with the human distributions. In contrast, the CoT methodology led to a decline in performance in many cases (45%). Finally, the ensemble method did not improve the few-shot approach without ensembling.\nCase study: SummEval and Mistral. Table 2 reveals an anomaly in the SummEval dataset: Mistral-v3, the open-source LLM, achieves the highest \u03c1. Interestingly, Mistral's traditional measure score (Pearson's correlation) is low (0.12). This discrepancy warrants further investigation. As shown in Table 5 in the Appendix, Mistral passes the test only for the Consistency aspect, with \u03c1 = 0.87, much higher than other LLMs (around 0.45).\nFirst, this demonstrates why each aspect should be tested separately. Second, Table 6 in the Appendix, which reports the annotation distributions for SummEval, explains why Mistral's \u03c1 is so high: human annotations for Consistency are highly skewed, with the score '5' assigned 89% of the time. The only LLM with a similarly skewed prediction distribution is Mistral. Other LLMs predict '5' only about 30% of the time. However, as shown by Table 6, few-shot helps LLMs adjust and skew their distributions, improving their alignment.\nNoteworthy, unlike traditional measures (Pearson's and Spearman's correlations), our method captures this nuance in alignment."}, {"title": "6 Discussion", "content": "The goal of this section is to discuss three factors that influence the outcomes of the alt-test: the number of annotated instances (\u00a76.1), the value of the cost-benefit trade-off hyperparameter \u025b (\u00a76.2), and the profile of the human annotators against whom we compare the LLM (\u00a76.3)."}, {"title": "6.1 The Number Of Instances", "content": "To help researchers reduce the costly need for manual annotations, we propose a statistical procedure that requires only a subset of such annotations and can verify whether an LLM can be used instead. This naturally leads to the question: how many annotated instances are needed for a reliable test?\nTo answer this, we present a bootstrap analysis in Figure 2 illustrating how the number of instances impacts our measures for the best performing LLM (according to \u03c1) in each dataset.\nAs shown, the winning rate w strongly depends on the number of instances. This is because w reflects the number of rejected hypotheses (i.e., the number of annotators the LLM wins), and more instances increase the power of the statistical test and the likelihood of rejecting a false null hypothesis (the human wins). In contrast, since \u03c1 does not involve hypothesis testing, it is not affected on expectation by the number of instances. Yet, increasing the number of instances reduces the variance of \u03c1 (since it is a mean of means), making it a more robust measure for comparing LLM judges.\nRegarding the recommended number, beyond the minimum requirement of 30 instances to satisfy the normality assumption of the t-test, Figure 2 shows that for \u025b = 0.2, in most cases, the LLM begins to pass the test before annotating 100 instances, and in half even before 50 instances. With & = 0.1 the alt-test requires more instances, typically double the amount needed for \u025b = 0.2, between 100 and 150. Yet, in three datasets (LGBTeen, MT-Bench, and SummEval), the LLM fails to pass the test regardless of the number of instances. While the exact number may vary depending on the task, the number of annotators, and the & value, our analysis highlights a promising finding: only a modest subset of annotations is required."}, {"title": "6.2 The Cost-benefit Hyperparameter", "content": "We wish to use LLMs instead of human annotators since they offer a much cheaper, faster, and less labor-intensive alternative. Therefore, we incorporated a cost-benefit hyperparameter into our procedure, \u025b, which lowers the necessary threshold the LLM must exceed (i.e., $\\hat{p}_j^h - \\epsilon$) to pass the alt-test. Generally, higher values of & are recommended when the cost and labor savings provided by the LLM are substantial. For instance, this applies when human annotators are highly expensive, require extensive and prolonged training, or when the task is time-consuming or particularly challenging (e.g., annotating complex relationships within lengthy documents). Conversely, smaller values of \u025b are more appropriate for simple annotation tasks that untrained crowd-workers can complete.\nTo explore the relationship between different \u025b values and the outcomes of the alt-test, as well as to provide guidelines for setting these values, we analyze the effect of & on the winning rate w of four LLMs, as shown in Figure 3. The strong monotonic increasing relationship between & and w, as presented by our analysis, enables us to identify the effective range of &, which lies between 0.05 and 0.3. For \u025b > 0.3, all LLMs achieve w > 0.5 on every dataset (except SummEval, and Gemini-Pro in KiloGram) and pass the test. In contrast, for \u03b5 < 0.05, all LLMs achieve w < 0.5 on all datasets (except CEBaB-S) and fail the test.\nFrom this analysis, we derive practical guidelines for selecting appropriate \u025b values. First and foremost, any value can be valid if the researcher reasonably justifies their choice. This justification may involve several aspects, including the cost and effort of the annotation, the expertise of the annotators, the cost of annotation mistakes (which varies based on the application and domain), and the centrality of LLM annotations to the study.\nAs a rule of thumb, we recommend setting \u025b to 0.2 when the annotators are trusted experts and 0.15 when they are skilled annotators (e.g., undergraduate students or trained workers). If the annotators are crowd workers, & should be set to 0.1. In either case, the quality of the annotators must be high enough to ensure reliable annotations, as discussed in the following subsection. In our experiments, we selected & values based on the type of annotators (as indicated in Table 1 and Figure 3) and the recommendations above."}, {"title": "6.3 The Human Annotators Profile", "content": "Recall that our procedure aims to justify replacement if the LLM aligns more closely with the collective distribution than an individual does, where the collective distribution approximates the gold label distribution. This collective distribution is the most reliable and authoritative benchmark when the annotators are experts. Accordingly, we recommend using expert annotators whenever possible and, at the very least, highly trained crowd-workers. If researchers themselves are experienced with the task, they can serve as annotators.\nIn Appendix \u00a7A, we examine advanced topics related to human annotators. In \u00a7A.2, we address the scenario of a single expert annotator and propose a simple modification to our procedure. This scenario is particularly relevant when only one expert is available due to limited accessibility or the high cost of their annotations. This single expert annotates a small subset of instances, and their annotations are considered the gold labels (i.e., there is no collective distribution in this scenario). Our modification compares the LLM against non-experts to determine whether the LLM aligns more closely with the single expert than"}]}