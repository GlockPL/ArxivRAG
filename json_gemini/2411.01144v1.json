{"title": "LEARNER: LEARNING GRANULAR LABELS FROM COARSE LABELS USING\nCONTRASTIVE LEARNING", "authors": ["Gautam Gare", "Jana Armouti", "Nikhil Madaan", "Rohan Panda", "Tom Foxt", "Laura Hutchinst", "Amita Krishnan", "Ricardo Rodriguez", "Bennett DeBoisblanc", "Deva Ramanan", "John Galeotti"], "abstract": "A crucial question in active patient care is determining if\na treatment is having the desired effect, especially when\nchanges are subtle over short periods. We propose using\ninter-patient data to train models that can learn to detect these\nfine-grained changes within a single patient. Specifically, can\na model trained on multi-patient scans predict subtle changes\nin an individual patient's scans?\nRecent years have seen increasing use of deep learning\n(DL) in predicting diseases using biomedical imaging, such as\npredicting COVID-19 severity using lung ultrasound (LUS)\ndata. While extensive literature exists on successful applica-\ntions of DL systems when well-annotated large-scale datasets\nare available, it is quite difficult to collect a large corpus of\npersonalized datasets for an individual.\nIn this work, we investigate the ability of recent computer\nvision models to learn fine-grained differences while being\ntrained on data showing larger differences. We evaluate on an\nin-house LUS dataset and a public ADNI brain MRI dataset.\nWe find that models pre-trained on clips from multiple pa-\ntients can better predict fine-grained differences in scans from\na single patient by employing contrastive learning.", "sections": [{"title": "1. INTRODUCTION", "content": "In recent years, automated patient monitoring has become\ncrucial for sustaining medical infrastructure, particularly ev-\nident during the COVID-19 pandemic when clinicians had\nto make life-altering tough decisions on resource allocation,\nsuch as access to oxygen [1]. A key question in active patient\ncare is determining if a treatment is effective. Clinicians need\nto know if a patient is improving to adjust the treatment if\nnecessary. However, detecting treatment effects in lung ultra-\nsound is challenging because changes are slow and progres-\nsive, making subtle differences difficult to capture. This issue\narises because a patient's scans are highly correlated, allow-\ning models to easily recognize if scans belong to the same pa-\ntient [6], which complicates the detection of subtle changes.\nWe propose leveraging inter-patient data to train mod-\nels to learn intra-patient changes. It is easier to distinguish\nbetween scans from different patients (inter-patient differ-\nences) than between scans from the same patient (intra-patient\nchanges). In this work, we investigate recent computer vi-\nsion models' ability to learn fine-grained differences in data\nwhile being trained on data that show larger/coarse-grained\ndifferences. Given LUS clips from multiple patients, could\na model be trained to predict differences in scans of a single\npatient on a fine-grained scale?\nWe evaluate on two longitudinal datasets, a) an in-house\nlung ultrasound dataset (LUS) wherein the aim is to track sub-\ntle changes in the patients SpO2/FiO2 (S/F) ratio (a measure\nof hypoxemia in patients) over subsequent scans. b) the public\nADNI Alzheimer's Disease MRI dataset wherein the goal is\nto track changes in a patient's Mini-mental state examination\n(MMIM) over subsequent scans.\nIn this work, we wish to extend the previous idea to tackle\na particular problem, can LUS (MRI) clips of multiple pa-\ntients - which are coarsely different in nature be used to"}, {"title": "2. DATASET", "content": "We evaluate our proposed models on the following longitu-\ndinal datasets, wherein we pose the problem as a three-way\nclassification task, i.e. given a pair of sequential scans of a pa-\ntient, has the health of the patient improved, stayed the same,\nor deteriorated."}, {"title": "2.1. LUS", "content": "We use an in-house lung ultrasound dataset [3] of linear probe\nvideos consisting of 189 patients (718 videos) with multiple\n(minimum 2 per patient) ultrasound B-scans of left and right\nlung regions at depths ranging from 4cm to 6cm under dif-\nferent scan settings, obtained using a Sonosite X-Porte ultra-\nsound machine (IRB-approval no ****).\nWe use the disease label to extract datapoints pertaining\nto only a single disease, in this case we study the cases of\nCOVID-19 patients. Our main label that we wish to predict is\nbased off of S/F ratios of these patients.\nS/F Ratio: The S/F represents the ratio between measured\nblood oxyhemoglobin saturation (S) and the fraction of in-\nspired oxygen (F). The lower this ratio, the more deranged the\nlung function is. S/F is a standardized measurement used to\nassess lung function in research and at bedside. We categorize\nthe S/F ratio into the following 4 ranges: [> 430, 275-430,\n180-275, < 180], as these ranges reflect the maximum the\nmedical devices can support lungs in oxygenating blood."}, {"title": "2.2. ADNI", "content": "The Alzheimer's Disease Neuroimaging Initiative (ADNI) [8]\nis a large public dataset launched in 2003 to identify early\nmarkers of Alzheimer's progression. It includes data from\nover 1,500 participants (Alzheimer's, mild cognitive impair-\nment, and healthy controls) across 50+ U.S. and Canadian\nsites, with more than 6,000 MRI scans, 1,800 PET scans,\ngenetic and clinical data, and longitudinal data. We use the\nMini-Mental State Examination (MMSE), a mental status test\ntracked over time in ADNI, to train models predicting MMSE\nscore changes."}, {"title": "3. METHODOLOGY", "content": ""}, {"title": "3.1. Model Architecture", "content": "We use the Temporal Shift Module (TSM) video network [5]\nwith the ResNet-18 backbone. TSM model aims to provide\nthe benefits and competitive performance of a 3D CNN while\nenjoying the complexity of a 2D CNN. It infuses temporal\ninformation into every 2D CNN resnet blovk, by shifting cer-\ntain channels from the previous and next time-frame. Refer\n5] for details."}, {"title": "3.2. Pre-training task:", "content": "We pre-train the encoder by training the model to predict the\npatient health-score [HS] (S/F or MMSE) by formulate it as\na regression task i.e. training a model to predict the S/F ratio\n(MMSE) for a given LUS (MRI). Our pre-training algorithm\nis described in 1."}, {"title": "3.2.1. Loss functions:", "content": "MSE Loss (MSE): We train the model to minimize L2 loss\nEq:1, where yi is the ground truth and ypred is the model's\nprediction. Contrastive Loss (CL): We employ contrastive\nlearning to leverage the rich inter-patient comparison data\ninorder to makeup for the limited (sequential) data avail-\nable. For a given scan we determine the B/2 closest and\nfarthest points in the given batch based on health-score (S/F\nor MMSE), where B is the batch size. Then, we try to reduce\nthe distance between the latent representation of the given\nscan w.r.t positive scans and try to increase the distance w.r.t\nnegative scan Eq:3. Weighted Contrastive Loss (wCL):\nFurther, we try the weighted contrastive loss wherein we fac-\ntor in the differences in the healthy scores (S/F or MMSE)\nof the two sequential scans to determine the weight of the\ncorresponding term in the loss function."}, {"title": "3.3. Downstream Classification task:", "content": "Finally, we train a 3-way MLP classifier using cross-entropy\nloss, that takes the encoded sequential patient scans embed-\ndings to predict if the patient's health-score improved, deteri-\norated, or remained the same."}, {"title": "4. RESULTS", "content": ""}, {"title": "4.1. Performances", "content": "We first compare pretrain using MSE, Contrastive and Weighted-\nContrastive loss in Table 3 on both LUS and ADNI dataset\nusing the TSM model. The basic hyperparameters that are\nshared with all the models are shown in Table 2 and the data\nsplit is shown in Table 1. From the results, we observe that\naugmenting the loss function with contrastive loss signifi-\ncantly improves the performance, with Contrastive loss and\nweighted-contrastive loss performing best on LUS and ADNI\ndataset respectively. This suggests that models can better\ndetect subtle changes in a given patients when pre-trained on\ndata comparing across multiple patients.\nEmbedding Analysis: We further verify this by analyz-\ning the distribution of the cosine distance between the scan\nembeddings vs the absolute difference in the ground truth la-\nbels of the scans. From Fig 1, we observe that distance dis-\ntribution is more spread out in the case of our proposed con-\ntrastive loss Eq 3, thus supporting our thesis that contrastive\npre-training allows the model to learn finer differences from\ncoarse differences in the labels."}, {"title": "4.2. Ablations", "content": "We carry out a series of ablations in Table 4 on the LUS\ndataset. First, we compare the TSM models performance with\n2D ResNet (where the per-frame embeddings are concate-\nnated before classification), recent popular vision transformer\n2D models (Swin, ViT), and 3D models (Timesformer, Video\nMAE). We find that TSM outperforms them, which can be\nattributed to its data-efficient design. In contrast, the vision\ntransformers don't have any priors compared to CNNs thus\nneeding more data to learn, leading them to overfit on such\nsmall dataset, which we experimentally verified (not included\ndue to page limit). Next, we ablate using L2 norm (l2Sim)\nsimilarity functions instead of cosine similarity (cosSim) and\nfind that the cosine similarity performs better."}, {"title": "5. CONCLUSION", "content": "In this work, we used contrastive training strategies to try\nand predict finer differences among scans of a single patient,\nwhile the model is trained on data from multiple patients\nwhich show coarser differences. We see that contrastive\ntraining helps most models improve their performance from\nthe standard supervised training scores. We verified this on\nthe in-house lung ultrasound dataset and public ADNI brain\nMRI dataset."}], "equations": ["MSE Loss = \\sum_{i=1}^{N} ||y_i - y_{i}^{pred}||^2                                                                   (1)", "CL Loss = \\sum_{i} log(sim(u_i, u_j) (||HS_i \u2013 HS_j|| + \\epsilon)) (2)", "-\\sum_{i}  \\frac{log(sim(u_i, u_j))}{(||HS_i - HS_j|| + \\epsilon)} (3)"]}