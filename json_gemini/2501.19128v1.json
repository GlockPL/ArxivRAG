{"title": "Shaping Sparse Rewards in Reinforcement Learning: A Semi-supervised Approach", "authors": ["Wenyun Li", "Wenjie Huang"], "abstract": "In many real-world scenarios, reward signal for agents are exceedingly sparse, making it challenging to learn an effective reward function for reward shaping. To address this issue, our approach performs reward shaping not only by utilizing non-zero-reward transitions but also by employing the Semi-Supervised Learning (SSL) technique combined with a novel data augmentation to learn trajectory space representations from the majority of transitions, zero-reward transitions, thereby improving the efficacy of reward shaping. Experimental results in Atari and robotic manipulation demonstrate that our method effectively generalizes reward shaping to sparse reward scenarios, achieving up to four times better performance in reaching higher best scores compared to curiosity-driven methods. The proposed double entropy data augmentation enhances performance, showcasing a 15.8% increase in best score over other augmentation methods.", "sections": [{"title": "1 Introduction", "content": "The sparse reward problem is a core challenge in Reinforcement Learning (RL) when solving real-world tasks Kober u. a. (2013). In supervised learning, supervision signals are provided by the training data. In reinforcement learning, rewards take on the role of supervision signals, guiding the agent to optimize its policy. Many real-world tasks naturally have the feature of delayed or infrequent rewards due to the complexity and nature of the tasks. In tasks like Go Silver u. a. (2016), navigation Wang u. a. (2020), or robotic arm manipulation Gu u. a. (2017); Riedmiller u. a. (2018), agents only receive rewards upon successfully achieving the final goal, such as winning a game, reaching a target, or completing a grasp, with no feedback for intermediate steps. Sparse reward provides little immediate feedback to guide the agent's exploration and high variance in returns, makes it difficult to learn the optimal policy Plappert u. a. (2018b).\nA potential solution to tackle the sparse reward problem is reward design and learning Ng und Russell (2000). Methods in this category often operate on the replay buffer, processing off-policy data to implement their respective reward design Schaul u. a. (2015); Andrychowicz u. a. (2017); Peng u. a. (2019). Pathak u. a. (2017) propose the In- trinsic Curiosity Module (ICM) that formulates curiosity as the error in an agent's ability to predict the consequence of its own actions learned by a self-supervised inverse dynamics model. However, curiosity-driven methods face a significant challenge: curiosity, as an intrinsic reward, is inherently decoupled from the actual objectives and tasks. This may cause agents to overly focus on \"new\" but meaningless states. Kumar u. a. (2019) propose Reward Condi- tioned Policy (RCP) that treats non-expert trajectories collected from sub-optimal policies as optimal supervision for matching the reward of a given trajectory. However, methods based on suboptimal policies under supervised learning suffer from poor sample efficiency Peng u. a. (2019), especially when non-zero-reward transitions are extremely rare in sparse reward cases.\nMoreover, there exists alternative approaches beyond self-supervised methods that may better address the chal- lenges posed by sparse reward data. Raileanu u. a. (2020) utilize data augmentation to enhance RL algorithm, yet such augmentation methods are restricted to image data and tend to lose or distort information in non-vision-based"}, {"title": "2 Related Work", "content": "Currently, research aiming at addressing the sparse reward problem mainly focuses on the following areas: experience replay mechanisms Peng u. a. (2019) and reward design and learning. Schaul u. a. (2015) develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. Memarian u. a. (2021) leverage self-supervised methods to extract signals from trajectories while simultaneously up- dating the policy. Andrychowicz u. a. (2017) propose Hindsight Experience Replay (HER) that allows sample-efficient learning from rewards which are sparse and binary and therefore performs well especially on robotic manipulation tasks. Ng und Russell (2000) propose the idea of learning reward functions from optimal interaction sequences, known as Inverse Reinforcement Learning (IRL). Brown u. a. (2019) propose an IRL algorithm that outperforms the demonstrator by extrapolating beyond a set of ranked demonstrations in order to infer high-quality reward functions. Besides expert trajectories, reward shaping can also be applied based on different evaluation criteria. Count-based methods Choi u. a. (2018); Ostrovski u. a. (2017); Bellemare u. a. (2016) incentivize agents based on the rarity of states, while curiosity-driven exploration methods reward the agent's exploratory behavior. ICM Pathak u. a. (2017) defines curiosity as the error in the agent's ability to predict the consequences of its own actions in a visual feature space. This curiosity signal serves as an intrinsic reward, driving the agent to explore.\nKumar u. a. (2019) employ supervised learning techniques, viewing non-expert trajectories collected from sub- optimal policies as optimal supervision for matching the reward of the given trajectory. Peng u. a. (2019) simplify the process into regressing target values for a value function, and weighted target actions for the policy.\nAnother related direction is the application of data augmentation Shorten und Khoshgoftaar (2019) in RL algo- rithms Kostrikov u. a. (2020). Hansen und Wang (2020) decouple augmentation from policy learning and is found to significantly advance sample efficiency, generalization, and stability in training over vision-based RL methods. Yarats u. a. (2021) use data augmentation to learn directly from pixels and is able to solve complex humanoid locomotion tasks directly from pixel observations. Studies like Lin u. a. (2019) generate feasible trajectories based on symmetries observed in the trajectory space for robot control tasks, thereby constructing an Invariant Transform Experience Replay framework to address the issue of high sample requirements. Raileanu u. a. (2020) explore the use of data augmentation techniques in visual RL tasks, such as random cropping, random noise, and color jittering, to improve the robustness of the policy \u03c0(\u03b1|s) and the value function V(s)."}, {"title": "3 Methodology", "content": "In this section, we introduce the proposed method by first introducing the proposed double entropy data augmentation, followed by the SSRS framework, featuring a monotonicity constraint and consistency regularization."}, {"title": "3.1 Double Entropy Data Augmentation", "content": "In reinforcement learning, applying invariant transformations to trajectories can improve the generalization per- formance of both the policy network and the value function network Lin u. a. (2019); Raileanu u. a. (2020). The unitless property of Entropy makes it particularly useful in data augmentation, as it allows for a fair comparison of information content across different features, regardless of their scale or domain. Entropy is essentially a measure of uncertainty or randomness in a probability distribution, and it is calculated in a way that normalizes the result, mak- ing it dimensionless. We propose a double entropy data augmentation method as follow. Shannon Entropy Shannon (1948) quantifies the amount of information required to describe or encode a random variable's possible states. In RL tasks, observations and trajectories are matrices in practice, but can also be normalized and viewed as discrete random variable. Let X be a discrete random variable with finite sample space X and probability mass function p(x) =Pr{X = x}, x \u2208 X. The entropy H(X) of a discrete random variable X is defined by\n$H(X) := - \\sum p(x) \\log p(x)$.\nConsider a m \u00d7 n matrix A with each element aij \u2264 0, \u2200i = 1, ..., m, j = 1, ..., n, and the Shannon Entropy of matrix A is defined by\n$H(A) := -\\sum_{k=1}^{mn} p_k \\log(p_k)$,\nwhere the probabilities are normalized by the matrix elements as $p_k = a_{ij} / \\sum_i \\sum_j a_{ij}$, such that $\\sum p_k = 1$.\nDenote state space as $S\u2208 R^{m_1}$ and action space as $A \u2208 R^{m_2}$, with $m_1, m_2 \u2265 1$. Consider a trajectory \u03c4 of length N as $(s_0, a_0, r_0, s_1, a_1, r_1,..., s_N)$ with $a_t \u2208 A, s_t \u2208 S$, for all $t = 1,..., N$, and rewards are given following the environment's original reward function $r_t = R(s_t, a_t, s_t)$. The set of all trajectories can be stacked into the matrix $\u0393 = [S, A, R]$ with $S \u2208 R^{N\\times m_1}, A \u2208 R^{N\\times m_2}$ and $R\u2208 R^{N\\times 1}$, and now we define the transform double entropy data augmentation \u03c3over matrix \u0393as:\n$\u03c3(\\overline{\u0393}|n) := [[h^1 \u00b7 s^1, h^2 . s^2, ..., h^n . s^n], A, R]$,\nwhere $h^n \u2208 R$, equals to the entropy $H(s^n)$, matrix $s^n$ is a submatrix of the stacked state S, obtained by equally dividing S into n parts along the state dimension. Here n is a hyperparameter and H(s) is the matrix Shannon entropy defined in Eq.(1)."}, {"title": "3.2 Semi-Supervised Reward Shaping", "content": "The Semi-Supervised Reward Shaping (SSRS) framework proposed in this paper, aims to fit an optimal state-value function $V^*(s)$ and an optimal action-value function $Q^*(s, a)$, which are used to shape the rewards of trajectory \u0393. Given an one-step trajectory $(s_t, a_t, r_t, s_{t+1})$, the estimated reward can be calculated from the estimations of the optimal state-value function $V^*(s)$ and the optimal action-value function $Q^*(s, a)$ as follows. We first define the confidence score vector of timestep t over a fixed reward set $Z = {z_i, i = 1,..., N_z}$ in Eq.(2), where $N_z$ is a hyperparameter controlling the number of estimated rewards. Value of zi is set according to the collected true reward value, and is updated throughout the process of agent's interactions with environment.\nBy the following Eq.(2), we can get confidence vector $q_t \u2208 R^{N_z}$ at timestep t, with each $q_i \u2208 q_t$ corresponding to a estimated reward $z_i \u2208 Z$, computed as\n$q_t = \u03b2Q(s_t, a_t) + (1 \u2212 \u03b2)V(s_{t+1})$.\nThe reward z estimated from the trajectory $(s_t, a_t, r_t, s_{t+1})$ essentially evaluates the quality of the agent's interaction with the environment at the current step based on the trajectory. We utilize $Q(s_t, a_t)$ to assess the long-term return (i.e., the cumulative future reward) of taking action $a_t$ in state $s_t$, and combine this with the future cumulative return estimation $V(s_{t+1})$ of transitioning to state $s_{t+1}$ after taking action $a_t$ in state $s_t$, which gives us the estimated z in the form of linear combination of the two, with \u03b2 > 0. The estimate reward value $z_t$ is selected with maximum confidence score above the threshold \u03bb, i.e., $z_t = \u03b1(q_t, \u03bb)$, where\n$\u03b1(q_t, \u03bb) =\n\\begin{cases}\narg \\max_{z_i} q_i, & \\text{for } q_i > \u03bb, i = 1, ..., N_z, \\\\\n0, & \\text{else}.\n\\end{cases}$"}, {"title": "A Monotonicity Constraint in SSRS", "content": "The value function V(st) provides a global baseline for a state, while advantage function captures the relative advantage or disadvantage of action a with respect to this baseline. This separation makes it easier to assess the relative importance of specific actions, and thus compensates for the insensitivity to actions because the disturbance of data augmentation only takse place on state s. By utilizing the relationship between V and Q, we can introduce the monotonicity constraint in the SSRS framework as follows, which quantifies the mean square positive advantage function values. Define $\u03b4_t(\u03b8) = Q(s_t, a_t, \u03b8_1) \u2013 V(s_t, \u03b8_2)$ where \u03b8 is the combination of $\u03b8_1$ and $\u03b8_2$, we have\n$L_{QV} =\n\\begin{cases}\n0, & \u03b4_t(\u03b8) < 0, \\\\\n\\frac{1}{\u03bcB} \\sum_{t=1}^{\u03bcB} (\u03b4_t(\u03b8))^2, & \\text{else}.\n\\end{cases}$\nNote that u represents the proportion of non-zero rewards in the buffer, which quantifies the sparsity level, and B is the batch size of trajectories. Minimizing the objective loss $L_{QV}$ over the parameter \u03b8 of can achieve a more stable update, which will be validated in Section 4.3, and is crucial to SSRS framework's success."}, {"title": "B Consistency Regularization in SSRS", "content": "Consistency regularization is an important component of algorithms in the field of SSL Bachman u. a. (2014). The reward estimator, as a crucial component in reward shaping, needs to capture the invariance of trajectories, which refers to the reward unaffected by minor variations in input transitions, aligning with the characteristics of consistency regularization. Furthermore, in scenarios with sparse rewards, applying weak strong augmentations to transitions with zero and non-zero reward transitions before conducting consistency regularization further enhances the generalization ability of the reward estimator in sparse reward environments. Note that the vector $q_t$, the confidence of the reward predictions under the trajectory $(s_t, a_t, r_t, s_{t+1})$, consists of $Q(s_t, a_t, \u03b8_1)$ and $V(s_{t+1}, \u03b8_2)$. \u03a4\u03bf be explicit, we use $q_t(\u03b8)$ to show its dependence in \u03b8. In order to obtain the optimal e value for accurately estimating the reward value of each trajectory$(s_t, a_t, r_t, s_{t+1})$, SSRS framework first optimizes the following objective loss function:\n$L_r = \\frac{1}{\u03bcB} \\sum_{t=1}^{\u03bcB} 1(\\text{max}(q_t(\u03b8)) \u2265 \u03bb) (r_t \u2013 \u03b1(q_t(\u03b8), \u03bb))^2$.\nSSRS also computes the loss on sparse reward trajectories as shown in Eq.(5), which is the loss between the strong augmentation term and weak augmentation term. Assuming continuity of trajectories in the metric space, it calculates the confidence of reward values after weak and strong augmentations, denoted by $q_t^w$ and $q_t^s$, respectively. Here we omit their dependence on \u03b8 for simplicity. The weak and strong augmentations refer to Table 5 in Appendix, where we consider Gaussian noise with smaller parameters as weak augmentation, and other augmentation such as smoothing, translation, and cutout, with larger parameters as strong augmentation. For confidence values greater than the threshold in weak augmentation, the one-hot operation is performed Bachman u. a. (2014), denoted as $q_t^w'$, and then used to compute the cross-entropy loss with the normalized confidence values greater than the threshold in strong augmentation, denoted as $q_t^s''$. The loss function is denoted as.\n$L_s = \\frac{1}{(1 \u2212 \u03bc)B} \\sum_{t=1}^{(1-\u03bc) B} [1(\\text{max}(q_t^w) \u2265 \u03bb, \\text{max}(q_t^s) \u2265 \u03bb) \\cdot H (q_t^{w'}, q_t^{s''})]$,\nwhere H denotes the cross-entropy, $(q_t^{w''}, \\text{log}(q_t^{s''}))$. Together, Lr and Ls forms the consistency regularization in the SSRS framework."}, {"title": "C Overall Framework", "content": "The illustration of the SSRS framework is shown in Figure 1. We also present the value-based SSRS framework with synchronous update, i.e., value function update and reward shaping carry out simultaneously. in Algorithm 1. The loss function of the Semi-Supervised Reward Shaping framework is given by Eq.(3), (4) and (5) above, and"}, {"title": "4 Experiment Results", "content": "The experiments aim to validate the performance of the SSRS framework in both the Atari game environment Belle- mare u. a. (2013) and the robotics manipulation environment Plappert u. a. (2018a) (see Figure 2) and, using Random Access Memor (RAM) observations as an example, demonstrate the superiority of double entropy data augmentation over other data augmentation methods. Additionally, ablation experiments are conducted to demonstrate the impor- tance of monotonicity constraints in the application of SSL techniques. Distinct from other reward shaping methods, our paper introduces an semi-supervised pipeline to enhance sample efficiency. In Section 4.4, we will analyze the characteristics of the trajectory space through experiments to provide insights into the underlying reasons of such improvement.\nExperimental Setup. RAM observation refers to a representation of the Atari RL environments' internal state directly from its Random Access Memory, typically 128 bytes of RAM (i.\u0435., a vector of 128 integers, each in the range [0, 255]). The code for this experiment is based on the Tianshou RL library Weng u. a. (2022). At each epoch, 2000 transition samples are collected (i.e., 2000 timesteps are executed in the environment) and appended to the dataset D, a buffer with a capacity of 30k transitions. Some of the hyperparameter settings are as Table 3 in Appendix, which are determined based on preliminary hyperparameter tuning results, and consistent hyperparameter settings are used in both the Robotics and Atari experiments."}, {"title": "4.1 Performance of SSRS", "content": "To evaluate the proposed SSRS framework, we compare it with several baselines and benchmarks algorithms in both the Atari and Robotic environments:\n\u2022 Intrinsic Curiosity Module (ICM, Pathak u. a. (2017)): ICM encourages exploration by rewarding the agent for discovering novel states based on intrinsic motivation.\n\u2022 Reward Conditioned Policy (RCP, Kumar u. a. (2019)): RCP conditions views non-expert trajectories collected from suboptimal policies as optimal supervision so as to shape the reward of the given trajectory.\n\u2022 Prioritized Experience Replay (PER, Schaul u. a. (2015)): PER improves learning efficiency by prioritizing experience samples with high temporal difference errors."}, {"title": "4.2 Performance of Data Augmentation", "content": "The performance analysis of double entropy data augmentation is conducted in Atari environment (see Figure 6). In the first two games, Seaquest and Hero, double entropy data augmentation consistently maintains a leading position in terms of both convergence speed and final highest rewards, with the variance of the best reward remaining within a narrow range. In the more reward-sparse environment of MonteZumaRevenge, although the final best reward achieved by the translate augmentation (blue curve) method is higher, it exhibits significantly larger variance. In contrast, double entropy data augmentation (green curve) demonstrates much smaller variance (see Figure 6(c)), indicating a more stable policy and a faster update rate. As we will mention in Section 4.4, double entropy data augmentation preserves the smoothness and clustering of trajectories better in the trajectory space. This is crucial"}, {"title": "4.3 Ablation Experiments of Monotonicity Constraint", "content": "We conduct a series of experiments on the SSRS framework under conditions with and without monotonicity con- straint and study the distribution of reward values at different stages of training in both scenarios.\nThe average best score obtained is presented in Table 1. In all the three environments, SSRS with a monotonicity constraint achieves higher best scores compared to SSRS without a monotonicity constraint. Considering the results from previous two sections, as SSRS-S set (green curves) only differs in monotonicity constraints between SSRS experiments (see Figures 5) and DA experiments (see Figures 6), we find that adding the monotonicity constraint reduces the variance as well as the final best score in reward-sparse environments with long-term goals (see Figure"}, {"title": "4.4 Feasibility of Semi-Supervised Learning", "content": "SSL methods relies on the smoothness assumption and clustering assumption Ouali u. a. (2020) on the augmented data, in this case, the trajectories. To be specific, the smoothness assumption indicates if two points $X_1, X_2$ reside in a high-density region are close, then so should be their corresponding outputs $y_1, y_2$. And clustering assumption indicates that samples from the same class are closer to each other, while there are significant boundaries between samples from different classes and the decision boundary of the model should be as far as possible from regions of high data density. We examine the trajectory distribution and obtain the consensus matrix C in Figure 3. Each element $a_{ij} \u2208 C, 0 \u2264 a_{ij} \u2264 1$, is frequency at which the two trajectories $\u03c4_i$ and $\u03c4_j$ are assigned to the same shaped reward value. Note that we use reward shaped values for labels.\nLarger reward values, i.e., rewards given for achieving long-term goals, exhibit clear decision boundaries with smaller reward values. However, the decision boundaries inside high reward values are more ambiguous, with only"}, {"title": "5 Conclusion", "content": "In this paper, we propose the Semi-Supervised Reward Shaping (SSRS) framework which utilizes zero reward tra- jectories by employing SSL technique. Additionally, we introduce the double entropy data augmentation method for consistency regularization and apply a monotonicity constraint over modules of reward estimator. Our model outperforms RCP and achieves performance comparable to ICM in Atari environments, with a maximum of fourfold increase in reaching higher best scores. Moreover, the double entropy data augmentation enhances performance showcasing a 15.8% increase in best score compared to other augmentation methods. With SSL techniques, we can deploy agents on significantly sparser trajectory data. From a reverse perspective, it is possible to obtain learnable trajectories by artificially annotating rewards for a minimal number of transitions within a large set of trajectory data.\nHowever, it's important to note that SSRS also introduces several hyperparameters, such as the update probability of trajectories in the experience replay buffer pu, which has a significant impact on algorithm performance. The tuning of pu reflects the balance between exploration and exploitation in this class of reward shaping algorithms. Similar issues were also highlighted in Peng u. a. (2019); Kumar u. a. (2019), where the problem of controlling the reward shaping ratio arises when the reward shaping estimator is suboptimal. Like our paper, the balance shifts towards deciding whether the agent should exploit the reward from the estimator or prioritize exploration. We believe that a theoretical analysis of the dynamic relationship between the optimality of the estimator and the exploration-exploitation ratio would be a valuable direction for future work."}, {"title": "A The Gradients of Loss Functions", "content": ""}, {"title": "A.1 The Gradient of Lov", "content": "For Lov mentioned in Section 3.2 in the following explicit form:\n$L_{QV} =\n\\begin{cases}\n0, & Q(s_t, a_t, \u03b8_1) - V(s_t, \u03b8_2) < 0, \\\\\n\\frac{1}{\u03bcB} \\sum_{t=1}^{\u03bcB} (Q(s_t, a_t, \u03b8_1) \u2013 V (s_t, \u03b8_2))^2, & \\text{else}.\n\\end{cases}$\nDenote $\u03b4_t(\u03b8) = Q(s_t, a_t, \u03b8_1) \u2013 V(s_t, \u03b8_2)$. Consider a batch of size B, within which a proportion $\u03bc_\u03b9 \u2265 0$ of the samples satisfy $\u03b4_t(\u03b8) < 0$. The remaining proportion $(1-\u03bc_\u03b9)$ contributes to the gradient with respect to the parameter $\u03b8 = (\u03b8_1, \u03b8_2)$ is given by:\n$\\frac{\u2202L_{QV}}{\u2202\u03b8} = \\frac{2}{\u03bcB} \\sum_{t\u2208I^+} [Q(s_t, a_t, \u03b8_1) \u2013 V (s_t, \u03b8_2)]\\begin{bmatrix}\u2207_{\u03b8_1}Q(s_t, a_t, \u03b8_1)\n-\u2207_{\u03b8_2}V (s_t, \u03b8_2)\\end{bmatrix}$.\nHere, $I_+$ denotes the set of indices for which $Q(s_t, a_t, \u03b8_1) \u2013 V(s_t, \u03b8_2) \u2265 0$."}, {"title": "A.2 The Gradient of Ly", "content": "Consider loss function L, in Eq.(4) as follow\n$L_r = \\frac{1}{\u03bcB} \\sum_{t=1}^{\u03bcB} 1(\\text{max}(q_t) \u2265 \u03bb) (r_t \u2013 \u03b1(q_t, \u03bb))^2$,\nwhere confidence score $q_t = \u03b2Q(s_t, a_t, \u03b8_1)+(1-\u03b2)V(s_{t+1}, \u03b8_2)$. Note that the indicator function $1(\\text{max}(q_t) > \u03bb)$ is not differentiable due to its discontinuity at the boundary of $\\text{max}(q_t) > \u03bb$. The derivative of the indicator function only has non-zero contribution at the boundary, i.e. $\\text{max}(q_t) = \u03bb$. In order to derive the gradient of Lr, we approximate the indicator function $1(\\text{max}(q_t) > \u03bb)$ using differentiable function, for simplicity, the sigmoid function, achieving the effect of approximate binary selection:\n$1(\\text{max}(q_t) \u2265 \u03bb) \u2248 \u03c3(\\text{max}(q_t) \u2013 \u03bb)$,\nwhere the sigmoid function $\u03c3(x)$ is defined as:\n$\u03c3(x) = \\frac{1}{1+ \\text{exp}(-x)}$\nThus, the loss function becomes:\n$L_r = \\frac{1}{\u03bcB} \\sum_{t=1}^{\u03bcB} \u03c3(\\text{max}(q_t) - \u03bb) (r_t \u2013 \u03b1(q_t, \u03bb))^2$.\nWe want to compute the gradient of L with respect to $\u03b8 = (\u03b8_1, \u03b8_2)$. The loss function has two parts that depend on $\u03b8_1$ and $\u03b8_2$: one is $q_t$, and the other is $\u03b1(q_t, \u03bb)$. The gradients of the former are:\n$\\frac{\u2202q_t}{\u2202\u03b8_1} = \u03b2 \\frac{\u2202Q(s_t, a_t, \u03b8_1)}{\u2202\u03b8_1}$,\n$\\frac{\u2202q_t}{\u2202\u03b8_2} =(1-\u03b2) \\frac{\u2202V (s_{t+1}, \u03b8_2)}{\u2202\u03b8_2}$,\nThus, the loss function Lr can be rewritten as:\n$L_r = \\frac{1}{\u03bcB} \\sum_{t=1}^{\u03bcB} f_t e_t^2$"}, {"title": "A.3 The Gradient of L", "content": "The loss function Ls is:\n$L_s = \\frac{1}{(1-\u03bc) B} \\sum_{t=1}^{(1-\u03bc) B} [1(\\text{max}(q_t^w) \u2265 \u03bb, \\text{max}(q_t^s) \u2265 \u03bb) \u00d7 H (q_t^{w'}, q_t^{s''})]$.\nTo compute $\u2202L_s/\u2202\u03b8_1$, we will apply the chain rule. The loss Ls involves a sum of products, and we need to consider the parts that depend on $\u03b8_1$. We adopt the same approach as in Section A.2, using sigmoid function (see Eq.(6)) approximation for the indicator function. Therefore, the overall approximation of the indicator function is:\n$1(\\text{max}(q_t^w) \u2265 \u03bb, \\text{max}(q_t^s) > \u03bb) \u2248 \u03c3(\\text{max}(q_t^w) \u2013 \u03bb)\u00b7 \u03c3(\\text{max}(q_t^s) \u2013 \u03bb)$.\nSimilar to that in Appendix.A.2, the approximate derivative of the indicator function $1(\\text{max}(q_t^w) \u2265 \u03bb, \\text{max}(q_t^s) \u2265 \u03bb)$ (denoted as $f_s$) with respect to $\u03b8_1$ is computed based on Eq.(8) and Eq.(9):\n$\\frac{\u2202f_s}{\u2202\u03b8_1} =\u03c3(\\text{max}(q_t^w) \u2013 \u03bb)(1 \u2013 \u03c3(\\text{max}(q_t^w) \u2013 \u03bb))\u00b7 \u03c3(\\text{max}(q_t^s) \u2013 \u03bb)\u00b7 \\frac{\u2202\\text{max}(q_t^w)}{\u2202\u03b8_1}\n+ \u03c3(\\text{max}(q_t^w) \u2013 \u03bb) \u03c3(\\text{max}(q_t^s) \u2013 \u03bb)(1 \u2013 \u03c3(\\text{max}(q_t^s) \u2013 \u03bb)) \u00b7 \\frac{\u2202\\text{max}(q_t^s)}{\u2202\u03b8_1}$\nThen, compute the derivative of the cross-entropy with respect to $\u03b8_1$. The cross-entropy between two distributions p and q is given by:\n$H(p,q) = \u2212 \u2211 p_i \\text{log}(q_i)$\nwhere $p_i$ are the true probabilities and $q_i$ are the predicted probabilities. Since $q_t^{w'}$ is a one-hot encoded vector, where only one position is 1 and others are 0, the cross-entropy in Eq. (5) is simplified to:\n$H(p,q) = - \\text{log}(q_{i^*})$"}, {"title": "B Supplementary Materials for Experiments", "content": ""}, {"title": "B.1 Hyperparameters Settings", "content": "The hyperparameter setup of our experiments in Section 4 consists of the data augmentation parameters, the SSRS framework parameters and other general configurations. As for data augmentation parameters, we adopt consistent settings throughout the experiments in Section 4.1-4.3. Due to different environments, the shapes of states and observations are different, so the parameter n of double entropy data augmentation and cutout data augmentation, which is related to the state and observation dimension, are different in Atari and Robotic environments (see Table 2)."}, {"title": "B.2 List of Data Augmentation Methods", "content": "Various types of data augmentation methods are used in the process of consistency regularization. Since the trans- formation double entropy data augmentation is thoroughly stated in Section 3.1, we here list all transformation operations for data augmentation strategies involved in our experiment in Table 5 for completeness. The parameter setup of the transformation refers to the Appendix B.1.\nRegarding to the heterogeneous SSRS variants which apply two weak and strong data augmentations in consistency regularization, the transformation details of the legends in Figures 5 are listed below:\n\u2022 SSRS-S: SSRS-S applies o random gauss noise as the weak augmentation and double entropy data augmentation as strong augmentation.\n\u2022 SSRS-M: SSRS-M applies o random gauss noise as the weak augmentation and smooth data augmentation as strong augmentation.\n\u2022 SSRS-C: SSRS-C applies o random gauss noise as the weak augmentation and cutout data augmentation as strong augmentation."}]}