{"title": "Enhancing Autonomous Vehicle Perception in Adverse Weather through Image Augmentation during Semantic Segmentation Training", "authors": ["Ethan Kou", "Noah Curran"], "abstract": "Robust perception is crucial in autonomous vehicle navigation and localization. Visual processing tasks, like semantic segmentation, should work in varying weather conditions and during different times of day. Semantic segmentation is where each pixel is assigned a class, which is useful for locating overall features (1). Training a segmentation model requires large amounts of data, and the labeling process for segmentation data is especially tedious. Additionally, many large datasets include only images taken in clear weather. This is a problem because training a model exclusively on clear weather data hinders performance in adverse weather conditions like fog or rain. We hypothesize that given a dataset of only clear days images, applying image augmentation (such as random rain, fog, and brightness) during training allows for domain adaptation to diverse weather conditions. We used CARLA, a 3D realistic autonomous vehicle simulator, to collect 1200 images in clear weather composed of 29 classes from 10 different towns (2). We also collected 1200 images of random weather effects. We trained encoder-decoder UNet models to perform semantic segmentation. Applying augmentations significantly improved segmentation under weathered night conditions (p < 0.001). However, models trained on weather data have significantly lower losses than those trained on augmented data in all conditions except for clear days. This shows there is room for improvement in the domain adaptation approach. Future work should test more types of augmentations and also use real-life images instead of CARLA. Ideally, the augmented model meets or exceeds the performance of the weather model.", "sections": [{"title": "1. INTRODUCTION", "content": "Detection and perception are crucial tasks for autonomous vehicles because they allow for accurate driving in a complex environment. Semantic segmentation is one example of a perception task. In semantic segmentation, each pixel in the image is given a class label (1). For example, cars are all given an ID, and roads are given a separate ID. Using this segmented image, autonomous vehicles can locate important landmarks to perform autonomous actions.\nTo train a robust segmentation model, large amounts of images and labels need to be collected. Labels are especially tedious to create for image segmentation because each pixel's class needs to be hand-labeled. Furthermore, most large datasets, such as CityScapes, are composed of images collected in clear weather conditions (3). This results in lower segmentation accuracy when the model is tested on images in adverse weather conditions. For example, rain can obstruct parts of the image, and fog can cause blurring."}, {"title": "2. RESULTS", "content": "We trained three semantic segmentation models with the UNet architecture. The first model, M1, was trained on clear weather images. M2 was trained on augmented clear weather images, and M3 was trained on weather images. The training and validation loss of the three models are shown in (Figure 2). The loss value measures how much the prediction differs from the actual, so a lower loss is better. We used the TensorFlow SparseCategorialCrossentropy loss.\nThere have been many approaches to the challenge of performing detection in various weather conditions. DFR-TSD improved traffic sign detection by training models to reverse weather effects on images (4). DGSS generated adversarial styles to improve nighttime segmentation (5). Some papers have also introduced tests for the robustness of models using various image transformations (6). Few papers discuss autonomous vehicle semantic segmentation domain adaptation across both time and weather when only given clear weather data. Our hypothesis is that augmenting clear weather data in training improves semantic segmentation in adverse weather conditions. We believe the augmented data can train the model to extract features invariant to the domain."}, {"title": "3. DISCUSSION", "content": "The results show evidence that when only training on clear-day images, applying image augmentation improves model performance during adverse weather conditions. There is also statistically significant evidence that augmentations decrease test loss in night-time rain or night-time random weather conditions.\nDespite the positive results, there are many limitations and possible improvements. First, custom augmentations outside of the Albumentation library should also be used. Albumentations is a library we used that has image augmentation capabilities (7). Currently, augmentations we use include random rain, random fog, and some others. We review each augmentation in more detail in the Materials and Methods section. Although these augmentations apply weather variation to the image, they are weather-specific augmentations. Weather-specific models are less effective because the model might have a weaker result when tested in unseen weather like snow. As future work, the augmentations should be non-weather-specific and still introduce realistic weather variation. For example, it's possible that a combination of blur, noise, and other image filters could span the possible image transformations caused by weather. This could improve the detection in a wider range of weathers instead of the specific weathers that the augmentations tailor.\nThe significance testing method could also be improved. Since we used 10-fold cross-validation for training, some models will share data, making the loss of the models not completely independent from each other. This can cause problems with the accuracy of the significance test because the t-test assumes that all samples are independent. Future work would ensure every sample is independent. This can be done by acquiring a dataset large enough so that the 10 models that are trained have no shared data.\nA simulation limitation is that CARLA, the autonomous vehicle simulation we used, does not include weather options such as snow (2). Future work can be to use a dataset of more diverse weather conditions. This can be done in simulation, but simulations still have drawbacks. For our data collection in CARLA, the car always spawns perfectly aligned on the road, and many factors of random noise are reduced. This level of perfection is absent in real life, so extrapolating our approach to real-life datasets is an important next step. This paper serves as a stepping point for the end goal of training a model to effectively perform segmentation in real-life adverse weather when only given data in clear day weather."}, {"title": "4. MATERIALS AND METHODS", "content": "We collected data using CARLA, a widely used 3D autonomous vehicle simulator. CARLA has numerous towns, each with its own architecture and location. Some towns are urban cities with skyscrapers, while others are rural farmlands with barns and crops.\nCARLA sensors can be used to capture a scene in RGB and semantic segmentation. We attach both cameras to the front of the vehicle, so all images are captured on the road. The semantic segmentation is split into 29 different classes. View CARLA documentation for details on each class. We collected 2 training datasets of 1200 images each. The D1 dataset includes only images taken in clear weather using the weather ClearNoon preset. The D2 dataset includes randomly generated times of day and weather from the CARLA presets.\nThe procedure for generating each dataset is first setting the world to Town 1 and teleporting the vehicle to a random location on the road. Then, the weather will be adjusted according to the desired dataset, and the current snapshot will be recorded from the RGB and semantic segmentation camera. Repeat this until 150 data points are recorded for Town 1. Then, repeat everything for Towns 2-7 and 10. In total, each dataset has 1200 images because 150 data points are recorded for each of the 8 towns.\nWe generated seven test sets of 400 images, each under various weather conditions. We followed the same data collection procedure as the training set but only collected 50 images per town, for a total of 400 images per training set. The seven datasets are collected on clear days (DC), rainy days (DR), random weather days (DW), clear nights (NC), rainy nights (NR), random weather nights (NW), and random everything (W).\nImage Augmentation\nThe goal of image augmentations is to teach the model to extract features invariant to the domain. To do this, we applied a series of weather augmentations to each image. We used the Albumentations library to make applying various types of image augmentations easier. We used the Albumentations library to generate random rain, random"}, {"title": "Overall Model Training and Evaluation", "content": "We first trained 3 models. The first model is trained on D1, which has all clear weather images. The second model is trained on D1 after applying image augmentations. The third model is trained on D2, which has images in random weather and during different times of day.\nWe utilized a UNet architecture with a MobileNetV2 backbone pre-trained on ImageNet. The UNet is an encoder-decoder convolutional neural network that can be used for semantic segmentation. The encoder (contracting path) detects all complexities of image features using convolutional and max pooling layers, and the decoder (expansive path) upscales these detected features to segmentation masks using convolutional and unpooling layers (8). There are skip connections between layers of the encoder and decoder to allow for reliable segmentation across both high-level features, like the location of an edge, and low-level features, like the class of an object. The input images are resized to 224x224. For all models, we trained for 50 epochs. We randomly selected 1000 train samples and 200 validation samples."}, {"title": "Significance Testing", "content": "To determine if image augmentations during training create a statistically significant decrease in the loss value, we trained 10 models each on D1, augmented D1, and D2 for 30 models total. We used 10-fold cross-validation for each dataset. For example, the first model is trained on all 1200 images except the first 120. The second model is trained on all 1200 images except the second 120, and so on. Each model was trained for 20 epochs because that number reduces total training time and is still high enough to prevent underfitting. We evaluated the loss of each of the 20 models on all 7 test sets.\nFor every test set, we have 10 loss values for each of the 3 datasets. To determine the p-value, we run a single-tailed 2-sample t-test on corresponding sets of 10 loss values for each test set."}]}