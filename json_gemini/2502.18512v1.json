{"title": "FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression", "authors": ["Jianjian Li", "Junquan Fan", "Feng Tang", "Gang Huang", "Shitao Zhu", "Songlin Liu", "Nian Xie", "Wulong Liu", "Yong Liao"], "abstract": "The rapid success of Vision Large Language Models (VLLMs) often depends on the high-resolution images with abundant visual tokens, which hinders training and deployment efficiency. Current training-free visual token compression methods exhibit serious performance degradation in tasks involving high-resolution, text-oriented image understanding and reasoning. In this paper, we propose an efficient visual token compression framework for text-oriented VLLMs in high-resolution scenarios. In particular, we employ a light-weight self-distillation pre-training stage to compress the visual tokens, requiring a limited numbers of image-text pairs and minimal learnable parameters. Afterwards, to mitigate potential performance degradation of token-compressed models, we construct a high-quality post-train stage. To validate the effectiveness of our method, we apply it to an advanced VLLMs, InternVL2. Experimental results show that our approach significantly reduces computational overhead while outperforming the baselines across a range of text-oriented benchmarks. We will release the models and code soon.", "sections": [{"title": "1 Introduction", "content": "The success of Large Language Models (LLMs) (Achiam et al., 2023; Yang et al., 2024a; Zhu et al., 2023; Dubey et al., 2024; Bi et al., 2024; Cai et al., 2024) has inspired efforts to extend their capabilities to other modalities, particularly vision. In vision-language tasks, VLLMs process visual features extracted from vision transformers (ViTs) (Radford et al., 2021)) and integrate them to LLMs. The performance of these models is often positively correlated with visual resolution.\nImproving visual resolution in ViTs involves fixed high-resolution settings (e.g., CogVLM2 (Hong et al., 2024), GLM4V9B (GLM et al., 2024)), slicing patch schemes (e.g., LLaVA 1.6 (Liu et al., 2024a), InternVL series (Chen et al., 2024b)), or simple dynamic resolution (Qwen2-VL (Wang et al., 2024)). These strategies enhance fine-grained visual understanding in models. However, higher resolutions drastically increase token count, imposing significant computational burdens. For example, Qwen2-VL processes 11,427 visual tokens for an image with a resolution of 8204 \u00d7 1092 pixels. This results in considerable computational overhead during both the training and inference phases, making high-resolution processing resource-intensive and challenging to scale-up.\nTo resolve above issues, reducing visual tokens in well-trained VLLMs has been studied in works like LLaVA-PruMerge (Shang et al., 2024), SparseVLM (Zhang et al., 2024) and VisionZip (Yang et al., 2024b). For instance, VisionZip(Yang et al., 2024b) selects informative tokens using attention scores to reduce the total number of tokens. However, training-free token pruning methods, like FastV (Chen et al., 2025), shows suboptimal performance in text-oriented tasks that demand high-fidelity token representations. To this end, training from scratch with reduced visual"}, {"title": "2 Related Works", "content": "tokens is another alternative. For example, TextHawk2 (Yu et al., 2024) uses 100M pre-training image-text pairs to train cascaded decoder layers, progressively downsampling visual tokens by 4x ratio. TextHawk2 requires significant data and resources, posing challenges in low-resource settings. This raises a challenge: Can we compress the visual tokens effectively under constraints of limited training datas and GPUs resources?\nFor this challenge, we Focus on Compression of visual tokens in high-resolution Text-oriented Large Vision-Language Models (FCoT-VL) while retaining fine-grained image detailed perception. To be special, we propose a self-distilling framework as shown in Figure 2, comprising a teacher model with abundant visual tokens and a student model with compressed token representations. To build upon established capabilities, we adopt the InternVL2 to initialize the teacher model and student model. During the self-distillation process, only a lightweight token compression module and projector in the student model are learnable with a small-scale set of image-text pairs(i.e., 2M). This approach brings two advantages: 1). The student inherits the parameter from the teacher, avoiding large-scale training and preserve advanced capabilities of the teacher model. (2) We exclusively finetunes the token compression module, which can achieve promising performance even with limited training data.\nIn practice, we find distilled student model has performance drops(about 5%) inevitably. To enhance the performance of the student model relative to the teacher model, we introduce a post-training stage using high-quality instruction datasets, involving documents, mathematics, science, charts, and GUI images. Besides, we propose a multi-stage model fusion technique that iteratively merges models to improve adaptability across various tasks. The post-training improves the model's ability to handle complex tasks, such as document parsing and reasoning-based QAs.\nOur contributions can be concluded as follows:\n(1). We propose a self-distilling paradigm towards visual token cimpressing for high-resolution text-oriented VLLMs, enabling robust realignment while minimizing both data and computational demands.\n(2). We explore post-training strategies including synthesis of high-quality supervised fin-tuning data and training-free model merging schemes, facilitating the capabilities of compressed VLLMs.\n(3). We develop the proposed FCoT-VL in the InternVL2 series, achieving compression ratios of 2 and 4, respectively. Extensive empirical evaluations across multiple text-oriented benchmarks reveal that our proposed models achieve comparable or superior performance to existing token-rich VLLMs, while offering higher training and deployment efficiency."}, {"title": "2.1 Vision Large Language Models", "content": "In recent years, open-source VLLMs have made significant advancements, driven by contributions from both academia and industry. Earlier models, such as BLIP-2 (Li et al., 2023), MiniGPT(Zhu et al., 2023) and LLaVA(Liu et al., 2024c,b), have proven to be effective for vision-language tasks via bridging off-the-shelf ViTs and LLMs. However, early VLLMs struggle with processing images containing fine-grained details, especially for OCR-like tasks such as charts(Masry et al., 2022), documents(Mathew et al., 2021), and infographics(Mathew et al., 2022). To this end, InternVL series propose an adaptive cropping method to convert vanilla images as several fixed image patches. For example, InternLM-XComposer2-4KHD(Dong et al., 2024) increases 336 pixels of CLIP to 4K resolution and gets strong document understanding ability. InternVL2 obtains promising results on text-oriented benchmarks via scaling up image resolution and ViT model parameters. Moreover, QwenVL2 (Wang et al., 2024) proposes a native dynamic processing of images at varying resolutions. This image processing setting generates more visual tokens and suppress adaptive cropping VLLMs. However, high-resolution processing pipelines bring substantial computational overhead in both training and inference stages, hindering real-world deployment.\nBeyond high-resolution tricks, many works reveal that high-quality datas are more important for advancing document understanding. Recent studies(Hu et al., 2024; Li et al., 2024a, 2025) highlight the critical role of data quality in VLLMs. For instance, InternVL-2.5(Chen et al., 2024a) enhanced performance of previous version through collecting more diverse dataset and data processing pipelines.\nIn this paper, we also explore how to obtain high-quality post-training datas to match frontier"}, {"title": "2.2 Visual Compression Schemes", "content": "open-source VLLMs. Specifically, Our FCoT-VL outperforms the base model InternVL2 on many benchmarks like ChartQA(Masry et al., 2022) and MathVista(Lu et al., 2024), despite reducing visual tokens by 50%.\nVisual compression, a key focus in high-resolution VLLMs, aims to efficiently reduce the use of vision tokens, minimizing computational and memory overheads. The inherent redundancy of visual data, compared to dense textual data, underscores the importance of compression.\nSolutions to visual compression can be broadly categorized into two main approaches: training-free and training-based ones. Training-free methods dynamically select more important vision tokens via various strategies during decoding stage. For instance, SparseVLM(Zhang et al., 2024) and VisionZip (Yang et al., 2024b) prioritize tokens based on attention scores. ToMe(Bolya et al., 2022) and LLaVA-PruMerge(Shang et al., 2024) cluster tokens using cosine similarity. However, the training-free paradigms suffer from significant performance drops in text-orientated benchmarks.\nIn contrast, training-based methods focus on optimizing the visual adaptor by incorporating external modules for token reducing. For instance, LLaMA-VID(Li et al., 2024b) enhances visual information extraction through Q-Former(Li et al., 2023) with context tokens. Similarly, models like C-Abstractor(Cha et al., 2024) and LDP(Chu et al., 2024) serve as promising alternatives for visual token compressing.\nTraining from scratch necessitates extensive alignment datasets and substantial computational resources, often consuming thousands of GPU days. In this work, we present an efficient training token-compressing framework that achieves comparable performance while significantly reducing both data and computational requirements."}, {"title": "3 Method", "content": "We propose FCoT-VL, a framework for compressing visual tokens in VLLMs. It has the following objectives: (1). The efficient realignment training stage. we propose a self-distillation framework to transfer visual token knowledge from rich-token VLLM to compressed-token VLLM. We only learn lightweight parameters with limited datas to acquire visual token compression ability while maintaining training and inference efficiency. (2). To boost text-oriented VLLM after visual token cutoff, we focus on advanced post-training and data augmentation techniques, enabling the student model to catch up with InternVL2."}, {"title": "3.1 Architecture of FCOT-VL", "content": "As shown in Figure 2, we present the architecture design of our FCoT-VL, which comprises a vanilla VLLM as the teacher model(i.e., InternVL2) and a VLLM with compressed visual tokens as the student model in the distillation process."}, {"title": "3.1.1 Re-alignment", "content": "Definition As illustrated in Figure 2, the basic architecture of the re-alignment consists of five primary components: a shared visual encoder \\(ViT\u03c6\\), a shared large language model \\(LLM\u03b8\\), a teacher visual adaptor \\(At\\) and a student visual adaptor \\(As\\), and a visual token compression module \\(V\\). Given a visual instruction input \\((xt, xv, y)\\), then the responses are computed as follows:\n\\[\\begin{aligned} \\hat{Yt} &= LLM_o[A_t(x_v); x_t]\\\\ \\hat{Y_s} &= LLM_o[A_s(V_c((x_v); x_t] \\end{aligned}\\]\nwhere \\([;]\\) means the concatenation operation, \\(xv\\) is the input image and \\(xt\\) is the text instruction embeddings. The \\(\\hat{Yt}\\) and \\(\\hat{Y_s}\\) denote the probabilities of responses for the teacher model t-VLLM and student model s-VLLM, respectively.\nInitialization We initialize our student model inherited from the teacher model parameters. During re-alignment stage, we freeze all the parameters of the teacher models. The \\(LLM\\) and ViT in s-VLLM maintain frozen since their pre-trained parameters have already captured rich visual and language knowledge. Only the student adaptor \\(A_s\\) and the visual token compression module \\(V_c\\) are learnable to bridge different modalities and compress visual tokens of the LLM part.\nSelf-distillation We compare different visual token adjustment methods like Qformer (Li et al., 2023), pooling and convolution as \\(V_c\\) as in Table 3. We find that employing a simple convolutional layer could reduce visual tokens (\u00d74 and \u00d72 ratio) effectively.\nWe aim to re-align the visual tokens with text tokens in the s-VLLM using OCR-like tasks, which converts texts in the images into an editable text format. Different from previous training distilling"}, {"title": "3.1.2 Post-Train", "content": "In this section, we describe supervised fine-tuning(SFT) aimed at improving the student model's performance in text-oriented tasks. We accept many open-source datasets reported in previous VLLMS (Chen et al., 2024b), covering a variety of downstream tasks. However, we find that many of these public datasets are not formatted in an instruction style. To overcome this, we leverage distillation from teacher models to acquire the conversation style. Subsequently, we prompt the InternLM2.5-7B (Cai et al., 2024) to rewrite the instruction datas with the tone of the teacher model. Moreover, we observe this rewriting method facilitates fast and stable training, which may be attributed to the strong alignment with the teacher model.\nChain-of-Thought pipeline. For reasoning tasks like math, chart reasoning and calculation problems, we leverage Rejection Sampling (RS) to expand the SFT dataset using larger and stronger multimodal language models. Specifically, for the question q, we employ RS to generate a new response with COT, obtaining the reasoning steps \\(R_{cot}\\) and final answer \\(R_{ans}\\), respectively. We use rule-based verifications that verify the correctness of the concluded answer \\(R_{ans}\\) for the given problem q based on the ground truths. We find that"}, {"title": "4 Experiments", "content": "the mixture of RS-augmented and vanilla data significantly enhances reasoning capabilities. For example, our FCoT-VL-2B, with half visual tokens retained, achieves a score of 58.96 on MathVista (Lu et al., 2024), outperforming many 7B-scale VLLMs.\nData sampling pipeline. Considering that our tasks cover diverse image understanding and reasoning tasks with varying difficulty levels in a single SFT stage, we develop a novel sampling strategy, termed post-training sampling, to address these potential issues. Specifically, we perform coarse training using a small subset of the entire dataset at first, and then analyze the training loss distributions across different tasks. For datasets exhibiting much lower loss values, indicating easier learning, we down-sample them in the subsequent formal training. Conversely, we identify tasks (excluding generation tasks) with higher loss values and increase their sampling probabilities, addressing the model's weaknesses, especially in reasoning tasks.\nModel Merging. Since our SFT training covers many tasks, we aim to merge the base model with weighted differences from each checkpoint during training. These checkpoints reflect different stages of fine-tuning, with each stage capturing important task-specific adaptations. During training, multiple intermediate checkpoints are saved, and they are merged using the following formula:\n\\[M_{mge} = O_{base} + \\sum_{i=1}^{n} \u03b1_i (O_{cpt_i} - O_{base})\\]\nWhere \\(M_{mge}\\) is the merged model, \\(O_{base}\\) is typically used as the final model, and \\(\u03b1_i\\) is the weight for the difference between the checkpoints \\(O_{cpt_i}\\) and the base model. n is set as 5. The goal is to determine the optimal fusion weights, formulated as:\n\\[arg \\underset{\u03b1_1,..., \u03b1_n}{max} f(O_{base} + \\sum_{i=1}^{n} \u03b1_i (O_{cpt_i} - O_{base}))\\]\nRather than relying on costly heuristic algorithms, we use Shapley values(Sundararajan and Najmi, 2020), to fairly serve the merge weight \\(\u03b1_i\\) to each checkpoint \\(M_i\\) based on its contribution to the final model performance. The weighted combination of checkpoints thus optimizes the final model's performance based on their individual contributions."}, {"title": "Computation Complexity", "content": "In this section, we analyze the computation complexity of FCoT-VL in our post-training stage. The computational burden in the FCoT-VL is predominantly attributed to the attention operations within the LLM decoders. Assuming the LLM decoders has L layers, we only compute the complexity of one self-attention and feed-forward network, yielding:\n\\[O(L \\cdot (n^2 \\cdot d + n \\cdot d^2))\\]\nWhere n is the length of input vectors and d is the dimension of LLM's input tokens. When the compress ratio is r, the computation complexity could be reduced as:\n\\[O(L(\\frac{n^2 \\cdot d}{r^2} + \\frac{n \\cdot d^2}{r}))\\]\nSince the computation cost of LLM decoders is dominant in the our FCoT-VL, the overall computation complexity will be reduced much, facilitating training and inference effectiveness. More quantitative experiments are discussed in Section 4.2."}, {"title": "4.1 Main Results", "content": "To validate the effectiveness of FCOT-VL, we evaluate on nine text-oriented mutimodal benchmarks:\nWe choose InternVL2-2B and InternVL2-8B as our baseline models, considering that their good adaption to high-resolution images and impressive performance. As shown in Table 1, we compress the visual tokens at ratio 50% and 75% of InternVL2-2B and InternVL2-8B, respectively. For the training-free FastV method, we find significant performance drop on the different baseline VLLMs(i.e., LLaVA-1.5-7B(Liu et al., 2023), LLava-NeXT(Liu et al., 2024b) and InternVL2(Chen et al., 2024b)), particularly when the visual tokens drop to 1/4. For instance, at a compressing ratio of 50%, the performance degradation is approximately 10% on InternVL2-2B, but at 75% compressing ratio, the performance drop exceeds 25%. This suggests that training-free paradigm is insufficient in text-oriented tasks, specially for high-resolution and text-rich images."}, {"title": "4.2 Ablation Study", "content": "Re-alignment We implement our FCoT-VL-2B and FCOT-VL-8B with CNN for compression. We craft diverse text-oriented understanding tasks, covering OCR-like tasks (i.e.,text recognition, image2markdown, chart2dict(Wei et al., 2023)). We sample a amount of 2 million image-text pairs and obtain fast and stable optimization as shown in Figure 4. Compared with scratch training like TextHawk2, which needs 100M data, our FCOT-VL-2B could finish pre-training about 24 hours with 64 GPUs(NPUs) resources."}, {"title": "5 Conclusion", "content": "In this paper, we introduce FCoT-VL, a novel method designed to efficiently compress Vision-Language Large Models (VLLMs) by reducing redundant visual tokens with minimal computational resources, while maintaining or even enhancing model performance. FCoT-VL significantly reduces the number of visual tokens, achieving notable performance improvements. Furthermore,"}, {"title": "6 Limitations", "content": "(1).We only focus on text-oriented tasks that require high-resolution settings and obtain lossless compression with the ratio of 50%. However, due to resource constraints, our approach does not extend to other image modalities, such as natural scenes or medical imaging. (2). Although our fixed compression ratios (i.e., 50% and 75%) are efficiently implemented, this setting performs well in most cases. However, it shows a slight performance drop when applied to extremely high-resolution tasks, such as infoVQA."}, {"title": "A Appendix", "content": "A.1 Training settings\nOur FCOT-VL was trained in two distinct stages: realignment and post-trian. As shown in Table 6, we present the training details of FCoT-VL in different stages. The details are as follows:\nFor both stages, we train models with 64 ascend 910 NPUs with the packed batch size is set to 512. In the re-alignment pre-training, we employ a 2 million image-text pairs to learn the projector and compress module. This allows the VLLMS to re-align the compressed visual token with the language token space. Specifically, we craft the optimization tasks of recognizing text in document images and converting charts and tables into python-dict/markdown format. We set the training epoch as 1, which requires approximately 48 hours using 64 NPUs for 2B scale. In the subsequent instruction tuning phase, we make all parameters of FCoT-VL learnable and keep most of the settings unchanged, except context length, training data and training epochs.\nA.2 Model Capabilities and Qualitative Examples\nIn this section, we present some practical examples of our FCOT-VL."}]}