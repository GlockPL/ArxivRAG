{"title": "VidCtx: Context-aware Video Question Answering with Image Models", "authors": ["Andreas Goulas", "Vasileios Mezaris", "Ioannis Patras"], "abstract": "To address computational and memory limitations of Large Multimodal Models in the Video Question-Answering task, several recent methods extract textual representations per frame (e.g., by captioning) and feed them to a Large Language Model (LLM) that processes them to produce the final response. However, in this way, the LLM does not have access to visual information and often has to process repetitive textual descriptions of nearby frames. To address those shortcomings, in this paper, we introduce VidCtx, a novel training-free VideoQA framework which integrates both modalities, i.e. both visual information from input frames and textual descriptions of others frames that give the appropriate context. More specifically, in the proposed framework a pre-trained Large Multimodal Model (LMM) is prompted to extract at regular intervals, question-aware textual descriptions (captions) of video frames. Those will be used as context when the same LMM will be prompted to answer the question at hand given as input a) a certain frame, b) the question and c) the context/caption of an appropriate frame. To avoid redundant information, we chose as context the descriptions of distant frames. Finally, a simple yet effective max pooling mechanism is used to aggregate the frame-level decisions. This methodology enables the model to focus on the relevant segments of the video and scale to a high number of frames. Experiments show that VidCtx achieves competitive performance among approaches that rely on open models on three public Video QA benchmarks, NEXT-QA, IntentQA and STAR.", "sections": [{"title": "I. INTRODUCTION", "content": "Video-Language modeling is an inherently resource-intensive task due to the necessity of processing a large number of frames, in order to generate accurate representations of the contents of a video. Recently, a considerable number of Video Large Language Models have emerged [1] [2] [3] [4] [5], which combine a visual encoder with a Large Language Model (LLM). These models have achieved state-of-the-art performance on a wide variety of video understanding tasks. However, they are typically limited by the maximum number of input tokens, and therefore frames, that they can process in one forward pass.\nIn order to tackle these problems, prior works have proposed sophisticated frame selection policies [6] [7] [8], with the goal of selecting the most salient frames of the input video. In a different direction, several works [9] [10] [11] have proposed extracting textual information in the form of captions from the input video and using an LLM to process the extracted captions. These approaches rely on the ability of LLMs to reason over long input prompts and produce accurate results. However, this has been proven to be a challenging task [12] [13]. Furthermore, by converting the input video to a textual representation the model no longer has access to the rich visual information contained in the video; this reduces the final performance of question answering.\nIn an orthogonal direction, a promising technique for im-proving the reasoning capabilities of Large Language Models is multi-step reasoning. Early studies in multimodal Chain-of-Thought (CoT), such as [14] [15] [16] [17], have shown that an intermediate rationale generation step can aid the multimodal understanding capabilities of language models. In the video domain, Video-of-Thought [18] achieved similar results with a multi-step reasoning framework for Video QA.\nIn this paper, we focus on the multiple-choice Video Question Answering task, which requires strong temporal reasoning and long-term understanding. To this end, motivated by the success of multi-step reasoning frameworks, we propose a novel video QA framework, VidCtx, which is based on the consensus of multiple Large Multimodal Model (LMM) decisions across different frames. The proposed VidCtx architecture is shown in Fig. 1, where we also illustrate the fundamental differences between our approach and the typical approaches of the literature. Prior works typically rely either on visual features aligned with a trainable layer (\"Encoder with LLM\") or on generated captions (\"Caption-based VQA\u201d), that they process with an LLM. In contrast, VidCtx utilizes both the extracted captions and the visual information for question answering. The temporal evolution of the video signal is handled by appending the description of distant frames to the LMM prompt; and, by combining the knowledge contained in distant parts of the video, the model can also give better answers to descriptive questions.\nVidCtx\u00b9 is not tailored to a specific pre-trained LMM, and requires no additional training or fine-tuning. Furthermore, it can process any number of frames since it is not constrained by the maximum context length of the language model. Our experimental results on three publicly available Video Ques-tion Answering benchmarks show that our VidCtx framework achieves competitive performance among approaches that rely on comparable open models.\nOur contributions can be summarized as follows:"}, {"title": "II. RELATED WORKS", "content": "The combination of a Large Language Model with a visual encoder or adapter module has resulted in a wide variety of models [1] [2] [3] [4] [5] [19] that have achieved state-of-the-art performance on several video understanding tasks. These models primarily benefit from large-scale pre-training and instruction tuning, therefore, their performance is strongly dependent upon the quantity and quality of the available training data.\nTo relax the requirement for large training corpora, another line of work proposes using pre-trained models as part of a video processing pipeline. For example, LLoVi [9] proposes a simple pipeline where captions are extracted from videos and an LLM is used to process the captions. Q-ViD [11] improves on this idea by extracting question-aware captions for further processing. LangRepo [10] introduces a textual repository that contains information and metadata about the contents of the video. In contrast to these works, we propose looking at both the visual information and extracted captions simultaneously, in order to capture a more complete representation of the input video.\nVamos [20] adopts a similar approach to ours, by con-catenating the extracted video captions with visual features,\nwhich are aligned with a learnable projection layer. However, Vamos process the entire video in a single pass, relying on the long-context reasoning capabilities of LLMs, and, it contains trainable components. Remarkably, Vamos showed that the addition of visual features in combination with task-agnostic video captions had a minimal impact on its performance in the video QA task. In contrast, our method follows a training-free paradigm and we show that the two modalities are indeed complementary.\nFinally, some works explore how to improve the frame selection mechanism, with the aim of selecting more relevant frames [6] [7] [8]. SeViLA [6] leverages a single Image-Language Model to perform both temporal localization and video question answering. Keyframe localization is achieved by asking the model whether a given frame contains the neces-sary information to answer the given query. Even though our proposed framework does not contain a dedicated keyframe selection component, inspired by the above work, we enable our model to ignore parts of the input video and focus only on the relevant segments."}, {"title": "B. Multi-step Reasoning in LLMs", "content": "Recent efforts in multi-step reasoning have shown promising results in advancing the reasoning capabilities of Large Lan-guage Models. Chain-of-Thought (CoT) reasoning refers to a group of techniques where LLMs are instructed to generate reasoning chains before answering a given query. Zero-shot CoT [21] instructs the language model to \u201cthink step-by-step\", thereby encouraging the model to utilize multi-step reasoning while generating the response. Few-shot CoT [22] provides reasoning chain demonstrations in-context, which can be either hand-crafted or automatically generated.\nIn multimodal understanding tasks, early studies of Chain-of-Thought reasoning [14] [15] [16] [17] have shown that the visual understanding capabilities of Large Multimodal Models can be improved by fine-tuning the LMMs on hand-crafted or automatically generated rationales and explanations. CCoT [23] proposes leveraging scene graph representations as an intermediate step, focusing on compositionality. In the video domain, Video-of-Thought [18] designs an elaborate multi-step reasoning framework that leverages spatio-temporal scene graphs for VQA. In contrast, we opt for a simpler intermediate step based on question-aware captions, which works well in practice to capture the temporal evolution of the video signal."}, {"title": "III. METHOD", "content": "The proposed VidCtx architecture is shown in Fig. 1. VidCtx utilizes both the extracted captions and the visual information contained in the video for question answering. Furthermore, VidCtx adopts a frame-based architecture that allows the model to focus on the relevant sections of the video and process a large number of frames. Specifically, we first perform frame sampling and we extract question-aware captions. We prompt the LMM with a standard VideoQA prompt and we insert the captions of distant frames in the prompt as additional context. This approach proves effective for modeling the temporal relations that are contained in the video and enables the model to combine information from distant parts of the video. Finally, the frame-level decisions are aggregated with a simple max pooling layer."}, {"title": "B. Question-aware Caption Extraction", "content": "The first step of our method is to extract question-aware captions from the input video. Given a long video V, we split V into N non-overlapping segments and we sample the central frame $V = {v_i, i = 1, ..., N}$ from each segment.\nFollowing Q-ViD [11], we utilize a pre-trained LMM, denoted as $\\Phi$, to extract question-aware captions $C = {c_i, i = 1,..., N}$ from each frame. The captions are generated as follows:\n$c_i = \\Phi(v_i, I_{cap})$\nwhere $I_{cap}$ is the concatenation of the instruction prompt and the given question Q, as follows:\n\"Please provide a short description of the image, giving information related to the following question: <Q>\""}, {"title": "C. Context-aware Video Question Answering", "content": "The next step is to prompt the LMM with the contents of each video frame separately, thus generating N separate answers for the given question Q and video V pair. We instruct the LMM to output a special character if there is not enough information to generate an answer. This mechanism is crucial because it allows the LMM to ignore parts of the video that are not relevant to the given question.\nThe language model is able to reason about static events that are depicted in each separate frame. However, in order to be able to reason about long-term actions that span multiple frames, we insert the relevant context into its prompt. Specifically, we append the question-aware description of a distant frame to the model prompt as follows:\n$d_i = \\Phi(v_i, C_{r(i)}, I_{vqa})$\n$r(i) = i + \\frac{N}{2} \\ (mod \\ N)$\nwhere $d_i$ is the i-th decision, r(i) is the index of the distant frame and $I_{vqa}$ is the concatenation of the instruction prompt and the given question Q. We choose to share information between frames that are exactly half the duration of the video apart, in order to ensure the greatest diversity between the contextual captions and the considered. We also insert the relevant temporal specifier (\u201cearlier\u201d or \u201clater\u201d), in order to inform the language model about the sequence of events in the video.\nFollowing Q-ViD [11], we use the following instruction prompt $I_{vqa}$ to generate the frame-level decisions:\n\"Here is what happens earlier/later in the video: <Caption> Question: <Q> Option A: Option B:\nOption C: Option D: Option E: Option F: No Answer. Considering the information presented in the caption and the video frame, select the correct answer in one letter from the options (A,B,C,D,E,F).\u201d"}, {"title": "D. Video-level Decision", "content": "Finally, we aggregate the N separate frame-level decisions to generate a video-level decision. In order to speed up the answer generation process, we consider the probability of just the first token of each frame-level answer among the set of possible options, denoted as T. We opt for a simple max pooling mechanism to combine these probabilities across frames, which works well in practice and does not require any training:\n$y = arg\\ max_{t \\in T} \\ [ max_{i \\in {F}} \\  \\frac{p(d_i=t)}{\\sum_{k \\in T} |p(d_i=k)|} \\ ]$\nwhere y is the final decision of the framework and p(\u00b7) is the log probability score of the given token. It is important that the per-frame scores are normalized across the target options before applying the max pooling operator. Empirically, we find that L1 normalization slightly outperforms the alternative softmax. For the purposes of generating the final decision, we ignore the special token F, which denotes that the LMM could not answer the question."}, {"title": "IV. EXPERIMENTS", "content": "In our experiments, we use the 4-bit quantized version of LLaVa-1.6-Mistral-7B [29], which is an instruction-tuned LMM that can process a single image per forward pass. For NEXT-QA [24] and IntentQA [25] we sample 64 frames per video. For STAR [26], we sample 32 frames per video, due to the smaller average video duration. We generate 200 tokens per caption. We ran all of our experiments on a single RTX4090 GPU. We report the zero-shot performance of our architecture, without performing any fine-tuning."}, {"title": "B. Datasets and Metrics", "content": "We conduct our experiments on the following publicly available video QA datasets:\n1) NEXT-QA [24] is a video QA dataset that contains 5,440 videos and 48K multiple-choice question-answer pairs. The average video length is 44 seconds. Following the relevant literature, we report the accuracy on the validation set that contains 570 videos and 5K questions. The questions are divided into Temporal, Causal and Descriptive questions.\n2) IntentQA [25] is a video QA dataset, derived from NEXT-QA, that focuses on intent reasoning. We perform our experiments on the test set that contains 2.1K question-answer pairs.\n3) STAR [26] is a video QA dataset that contains 60K situated questions, divided into 4 categories (Interac-tion, Prediction, Sequence, Feasibility). Similarly to the relevant literature, we perform our experiments on the validation set, which contains 7K questions."}, {"title": "C. Results and Comparisons", "content": "In Tab. I, we report the zero-shot top-1 accuracy of our architecture on NEXT-QA, IntentQA and STAR. We compare VidCtx with the state-of-the-art approaches including both open models (VFC [27], InternVideo [28], SeViLA [6], LangRepo [10], Q-VID [11], VideoChat2 [1]) and GPT-based frameworks (LLoVi [9], VideoTree [7], VideoAgent [8]). We observe that our model achieves state-of-the-art performance among the open approaches on NEXT-QA and IntentQA, outperforming Q-ViD by +4.4% on NEXT-QA and by +3.5% on IntentQA. Our method even outper-forms models that rely on closed proprietary LLMs, such as LLoVi with GPT-3.5 (+4.4% on NExT-QA). Furthermore, we observe that VidCtx outperforms the competition on all individual splits of NExT-QA. On the STAR dataset, our VidCtx method outperforms Q-ViD by +5.4% and is second only to VideoChat2. However, the latter on the one hand is pre-trained on large-scale video datasets, on the other hand performs poorly on NExT-QA (being outperformed not only by VidCtx but also by Q-ViD and SeViLA)."}, {"title": "Computational Complexity", "content": "VidCtx has to process each frame of the input video twice: once for captioning and once for question answering. However, the second pass is significantly faster, since we only generate the first token of the answer (i.e., just one letter: A to F). The overall inference complexity of VidCtx is, therefore, linear with respect to the number of processed frames per video. In contrast, related works that concatenate the captions, such as LLoVi and Q-ViD, scale quadratically and have significantly higher GPU memory requirements, due to the O(n\u00b2) time and memory complexity of the attention mechanism used in LLMs."}, {"title": "D. Ablations", "content": "In Tab. II, we compare different meth-ods of inserting additional context in the form of text captions to our LMM prompts. We observe that distant captions (Equation (3)) provide the most helpful context, followed by \"current captions\", i.e. inserting the caption of the same frame that is provided to the LMM as input. In both cases, this indicates that the additional context can help the model make better decisions. Furthermore, we observe that the question-aware captions outperform the static captions. This is consistent with the findings of Q-ViD [11]."}, {"title": "Comparison with Captions-only Baseline", "content": "In Tab. III, we compare VidCtx with the baseline approach of concatenating the question-aware captions and utilizing a text-only LLM to process the captions; similarly to Q-ViD [11]. We run our experiments with the same set of captions and same model. We observe that our proposed framework outperforms the baseline approach by +3%."}, {"title": "Effect of Number of Frames", "content": "In Tab. IV, we observe a clear correlation between the number of frames and the performance of VidCtx. We note that our model can scale to an arbitrarily high number of frames, since each frame is processed separately, in contrast with most of the state-of-the-art approaches that are limited by the long-context capabilities of language models."}, {"title": "Choice of Aggregation Method", "content": "In Tab. V, we compare different methods of aggregating the frame-level decisions in order to generate a decision for the entire video. We observe that max pooling slightly outperforms mean pooling in our experiments. We find that normalizing the probability scores before applying the pooling operator provides a significant increase in accuracy. Finally, we find that L1 normalization is more suitable (by a small margin) to our task compared to the alternative of applying a softmax operator."}, {"title": "E. Qualitative Study", "content": "In Fig. 2 we present qualitative examples of VidCtx in action. We select one question from each question category of NEXT-QA [24] and we present pairs of distant frames, along with their question-aware captions and normalized an-swer scores for each candidate answer. We observe that the utilization of captions of distant frames as additional context can help the model make better decisions. Even in cases where the context is misleading, such as the last example (i.e., the caption of Frame 25, which contradicts what can be seen in Frame 57), VidCtx can recover by generating higher scores for caption-frame pairs for which a more confident decision can be made (i.e., pairs presenting no such contradictions), thus leading to correct video-level answers."}, {"title": "V. CONCLUSION", "content": "We presented VidCtx, a training-free video question an-swering framework that integrates both visual features and text captions, in order to answer multiple-choice questions about videos. In VidCtx, the video is processed frame-by-frame by an LMM, and additional context is inserted in the form of captions. Our framework achieves competitive performance among open models on three public video QA datasets and even performs competitively to frameworks that rely on proprietary Large Language Models."}]}