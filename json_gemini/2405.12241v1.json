{"title": "Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning", "authors": ["Dan Braun", "Jordan Taylor", "Nicholas Goldowsky-Dill", "Lee Sharkey"], "abstract": "Identifying the features learned by neural networks is a core challenge in mechanistic interpretability. Sparse autoencoders (SAEs), which learn a sparse, overcomplete dictionary that reconstructs a network's internal activations, have been used to identify these features. However, SAEs may learn more about the structure of the datatset than the computational structure of the network. There is therefore only indirect reason to believe that the directions found in these dictionaries are functionally important to the network. We propose end-to-end (e2e) sparse dictionary learning, a method for training SAEs that ensures the features learned are functionally important by minimizing the KL divergence between the output distributions of the original model and the model with SAE activations inserted. Compared to standard SAEs, e2e SAEs offer a Pareto improvement: They explain more network performance, require fewer total features, and require fewer simultaneously active features per datapoint, all with no cost to interpretability. We explore geometric and qualitative differences between e2e SAE features and standard SAE features. E2e dictionary learning brings us closer to methods that can explain network behavior concisely and accurately. We release our library for training e2e SAEs and reproducing our analysis at https://github.com/ApolloResearch/e2e_sae.", "sections": [{"title": "Introduction", "content": "Sparse Autoencoders (SAEs) are a popular method in mechanistic interpretability [Sharkey et al., 2022, Cunningham et al., 2023, Bricken et al., 2023]. They have been proposed as a solution to the problem of superposition, the phenomenon by which networks represent more 'features' than they have neurons. 'Features' are directions in neural activation space that are considered to be the basic units of computation in neural networks. SAE dictionary elements (or 'SAE features') are thought to approximate the features used by the network. SAEs are typically trained to reconstruct the activations of an individual layer of a neural network using a sparsely activating, overcomplete set of dictionary elements (directions). It has been shown that this procedure identifies ground truth features in toy models [Sharkey et al., 2022].\nHowever, current SAEs focus on the wrong goal: They are trained to minimize mean squared reconstruction error (MSE) of activations (in addition to minimizing their sparsity penalty). The issue is that the importance of a feature as measured by its effect on MSE may not strongly correlate with how important the feature is for explaining the network's performance. This would not be a problem if the network's activations used a small, finite set of ground truth features \u2013 the SAE would simply identify those features, and thus optimizing MSE would have led the SAE to learn the functionally important features. In practice, however, Bricken et al. [2023] observed the phenomenon of feature"}, {"title": "Training end-to-end SAES", "content": "Our experiments train SAEs using three kinds of loss function (Figure 1), which we evaluate according to several metrics (Section 2.5):\n1.  Llocal trains SAEs to reconstruct activations at a particular layer (Section 2.2);\n2.  Le2e trains SAEs to learn functionally important features (Section 2.3);\n3.  Le2e+downstream trains SAEs to learn functionally important features that optimize for faithfulness to the activations of the original network at subsequent layers (Section 2.4)."}, {"title": "Formulation", "content": "Suppose we have a feedforward neural network (such as a decoder-only Transformer [Radford et al., 2018]) with L layers and vectors of hidden activations $a^{(l)}$:\n$a^{(0)}(x) = x$\n$a^{(l)}(x) = f^{(l)}(a^{(l-1)}(x))$, for $l = 1, ..., L-1$\n$y = softmax(f^{(L)}(a^{(L-1)}(x)))$.\nWe use SAEs that consist of an encoder network (an affine transformation followed by a ReLU activation function) and a dictionary of unit norm features, represented as a matrix D, with associated bias vector ba. The encoder takes as input network activations from a particular layer l. The architecture we use is:\n$Enc(a^{(l)}(x)) = ReLU(W_ea^{(l)}(x) + b_e)$\n$SAE(a^{(l)}(x)) = D^TEnc(a^{(l)}(x)) + b_d$,\nwhere the dictionary D and encoder weights We are both (N_dict_elements X d_hidden) matrices, be is a N_dict_elements-dimensional vector, while bd and a(l)(x) are d_hidden-dimensional vectors."}, {"title": "Baseline: Local SAE training loss (Llocal)", "content": "The standard, baseline method for training SAEs is SAElocal training, where the output of the SAE is trained to reconstruct its input using a mean squared error loss with a sparsity penalty on the encoder activations (here an L1 loss):\n$L_{local} = L_{reconstruction} + L_{sparsity} = ||a^{(l)}(x) - SAE_{local}(a^{(l)}(x))||_2 + \\frac{\\lambda}{\\text{dim}(a^{(l)})}|Enc(a^{(l)}(x))||_1$.\n$\\lambda$ is a sparsity coefficient $\\lambda$ scaled by the size of the input to the SAE (see Appendix D for details on hyperparameters)."}, {"title": "Method 1: End-to-end SAE training loss (Le2e)", "content": "For SAEe2e, we do not train the SAE to reconstruct activations. Instead, we replace the model activations with the output of the SAE and pass them forward through the rest of the network:\n$\\hat{a}^{(l)}(x) = SAE_{e2e}(a^{(l)}(x))$\n$a^{(k)}(x) = f^{(k)}(\\hat{a}^{(l)}(x))$ for $k = l, ..., L-1$\n$\\hat{y} = softmax(f^{(L)}(\\hat{a}^{(L-1)}(x)))$\nWe train the SAE by penalizing the KL divergence between the logits produced by the model with the SAE activations and the original model:\n$L_{e2e} = L_{KL} + L_{sparsity} = KL(\\hat{y}, y) + \\frac{\\lambda}{\\text{dim}(a^{(l)})}|Enc(a^{(l)}(x))||_1$\nImportantly, we freeze the parameters of the model, so that only the SAE is trained. This contrasts with Tamkin et al. [2023], who train the model parameters in addition to training a 'codebook' (which is similar to a dictionary)."}, {"title": "Method 2: End-to-end with downstream layer reconstruction SAE training loss (Le2e+downstream)", "content": "A reasonable concern with the Le2e is that the model with the SAE inserted may compute the output using an importantly different pathway through the network, even though we've frozen the original model's parameters and trained the SAE to replicate the original model's output distribution. To counteract this possibility, we also compare an additional loss: The end-to-end with downstream"}, {"title": "Experimental metrics", "content": "We record several key metrics for each trained SAE:\n1.  Cross-entropy loss increase between the original model and the model with SAE: We measure the increase in cross-entropy (CE) loss caused by using activations from the inserted SAE rather than the original model activations on an evaluation set. We sometimes refer to this as 'amount of performance explained', where a low CE loss increase means more performance explained. All other things being equal, a better SAE recovers more of the original model's performance.\n2.  Lo: How many SAE features activate on average for each datapoint. All other things being equal, a better SAE needs fewer features to explain the performance of the model on a given datapoint.\n3.  Number of alive dictionary elements: The number of features in training that have not 'died' (which we define to mean that they have not activated over a set of 500k tokens of data). All other things being equal, a better SAE needs a smaller number of alive features to explain the performance of model over the dataset.\nWe also record the reconstruction loss at downstream layers. This is the mean squared error between the activations of the original model and the model with the SAE at all layers following the insertion of the SAE (i.e. downstream layers). If reconstruction loss at downstream layers is low, then the activations take a similar pathway through the network as in the original model. This minimizes the risk that the SAEs are learning features that take different computational pathways through the downstream layers compared to the original model. Finally, following Bills et al. [2023], we perform automated interpretability scoring and qualitative analysis on a subset of the SAEs, to verify that improved quantitative metrics does not sacrifice the interpretability of the learned features.\nWe show results for experiments performed on GPT2-small's residual stream before attention layer 6. Results for layers 2, 6, and 10 of GPT2-small and some runs on a model trained on the TinyStories dataset [Eldan and Li, 2023] can be found in Appendices A.1 and A.2, respectively. They are qualitatively similar to those presented in the main text. For our GPT2-small experiments, we train SAEs with each type of loss function on 400k samples of context size 1024 from the Open Web Text dataset [Gokaslan and Cohen, 2019] over a range of sparsity coefficients \u03bb. Our dictionary is fixed at 60 times the size of the residual stream (i.e. 60 \u00d7 768 = 46080 initial dictionary elements). Hyperparameters, along with sweeps over dictionary size and number of training examples, are shown in Appendices D and E, respectively."}, {"title": "Results", "content": "End-to-end SAEs are a Pareto improvement over local SAEs\nWe compare the trained SAEs according to CE loss increase, Lo, and number of alive dictionary elements. The learning rates for each SAE type were selected to be Pareto-optimal according to their"}, {"title": "End-to-end SAEs have worse reconstruction loss at each layer despite similar output distributions", "content": "Even though SAEe2es explain more performance per feature than SAElocals, they have much worse reconstruction error of the original activations at each subsequent layer (Figure 2). This indicates that the activations following the insertion of SAEe2e take a different path through the network than in the original model, and therefore potentially permit the model to achieve its performance using different computations from the original model. This possibility motivated the training of SAEe2e+dsS.\nIn later layers, the reconstruction errors of SAElocal and SAEe2e+ds are extremely similar (Figure 2). SAEe2e+ds therefore has the desirable properties of both learning features that explain approximately as much network performance as SAEe2e (Figure 1) while having reconstruction errors that are much closer to SAElocal. There remains a difference in reconstruction at layer 6 between SAEe2e+ds and SAElocal. This is not surprising given that SAEe2e+ds is not trained with a reconstruction loss at this layer. In Appendix B, we examine how much of this difference is explained by feature scaling. In Appendix G.3, we find a specific example of a direction with low functional importance that is faithfully represented in SAElocal but not SAEe2e+ds."}, {"title": "Differences in feature geometries between SAE types", "content": "End-to-end SAEs have more orthogonal features than SAElocal\nBricken et al. [2023] observed 'feature splitting', where a locally trained SAEs learns a cluster of features which represent similar categories of inputs and have dictionary elements pointing in similar directions. A key question is to what extent these subtle distinctions are functionally important for the"}, {"title": "Interpretability of learned directions", "content": "Using the automated-interpretability library [Lin, 2024] (an adaptation of Bills et al. [2023]), we generate automated explanations of our SAE features by prompting gpt-4-turbo-2024-04- 09 [OpenAI et al., 2024] with five max-activating examples for each feature, before generating \"interpretability scores\" by tasking gpt-3.5-turbo to use that explanation to predict the SAE feature's true activations on a random sample of 20 max-activating examples. For each SAE we generate automated interpretabilty scores for a random sample of features (n = 198 to 201 per SAE). We then measure the difference between average interpretability scores. This interpretability score is an"}]}