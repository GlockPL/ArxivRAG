{"title": "Has Multimodal Learning Delivered Universal Intelligence in Healthcare? A Comprehensive Survey", "authors": ["Qika Lin", "Yifan Zhu", "Xin Mei", "Ling Huang", "Jingying Ma", "Kai He", "Zhen Peng", "Erik Cambria", "Mengling Feng"], "abstract": "The rapid development of artificial intelligence has constantly reshaped the field of intelligent healthcare and medicine. As a vital technology, multimodal learning has increasingly garnered interest due to data complementarity, comprehensive modeling form, and great application potential. Currently, numerous researchers are dedicating their attention to this field, conducting extensive studies and constructing abundant intelligent systems. Naturally, an open question arises that has multimodal learning delivered universal intelligence in healthcare? To answer the question, we adopt three unique viewpoints for a holistic analysis. Firstly, we conduct a comprehensive survey of the current progress of medical multimodal learning from the perspectives of datasets, task-oriented methods, and universal foundation models. Based on them, we further discuss the proposed question from five issues to explore the real impacts of advanced techniques in healthcare, from data and technologies to performance and ethics. The answer is that current technologies have NOT achieved universal intelligence and there remains a significant journey to undertake. Finally, in light of the above reviews and discussions, we point out ten potential directions for exploration towards the goal of universal intelligence in healthcare.", "sections": [{"title": "1 INTRODUCTION", "content": "RECENT years have seen the remarkable progress of artificial intelligence (AI) across the healthcare and medicine domain [1]. AI techniques have demonstrated substantial potential in various medical scenarios, including medical imaging analysis [2], disease diagnosis [3], drug discovery [4], personalized treatment [5], and medical QA (question-answering) [6], aiming to provide automated and customized expert-level advice or recommendations to alleviate the burden on both patients and physicians. Nonetheless, these studies or applications typically consider only single-modality data, e.g., medical image or text, which could result in diminished performance and may not accurately represent authentic application scenarios [7].\nAs the healthcare domain ceaselessly produces an increasing volume and variety of data, ranging from medical images and clinical notes to genomic profiles and biosensor readings, the need for effective multimodal learning approaches becomes paramount [7], [8], [9], [10]. On the one hand, multimodal AI models, capable of integrating and learning from these heterogeneous data streams, hold the promise of unlocking a comprehensive and nuanced understanding of complex medical phenomena. By capturing complementary semantic information [11] (as shown in Figure 1) and intricate relationships across different modalities [12], these models provide clinicians with a holistic view of patients' conditions, enabling more proactive monitoring, accurate diagnoses, and personalized treatment plans. On the other hand, multimodal learning further broadens the application prospects of intelligent models in the healthcare field. For instance, if a patient needs to ask about their skin condition, articulating it verbally (e.g., using conventional language QA systems) can be challenging. A visual question-answering (VQA) system becomes incredibly useful, as it can combine intuitive images uploaded by patients to make more accurate and comprehensive diagnoses. Given the significant research importance and application value of multimodal healthcare, recent years have witnessed an extensive amount of research dedicated to this particular subject, with a clear rising trend. The advancement in technologies has progressed from utilizing specific models, such as convolutional neural network (CNN) [13], recurrent neural network (RNN) [14], graph neural network (GNN) [15], and Transformer [16], to the adoption of a strategy involving pre-training and fine-tuning. The latter has emerged as the prevailing focus and trending topic, which is inspired by the powerful foundational models (FMs) in the general domain, like CLIP [17], ChatGPT1, GPT-42, and multimodal large language model (MLLM) [18]. These studies have made significant advancements in numerous tasks of multimodal healthcare, e.g., multi-modality image fusion, report generation (RG), VQA, cross-modal retrieval, text-augmented image processing, and cross-modal image generation. The evolution has ultimately led to the development of FMs"}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 Medical Modalities", "content": "Academically, modality refers to the way things are expressed or perceived, with every form or source of information being categorized as a modality [12]. In the healthcare domain, the term multimodal data typically pertains to digital records derived from diverse sources such as various machines, sensors, or experts, often represented in distinct formats. The medical modalities encompass elements such as medical vision, text, audio, and physiological signals, among others [7]. As shown in Figure 1, the medical vision modality consists of images obtained by different sensors, which are utilized for viewing the conditions or diseases of different organs or tissues. Within this scope, three types of images are commonly used, namely radiology, pathology, and camera images. Radiology is frequently employed to capture images of the human body's internal conditions [21], primarily encompassing components such as X-ray, computed tomography (CT, 2D/3D), magnetic resonance imaging (MRI), positron emission tomography (PET), and ultrasound. Pathology is the scientific exploration of disease-induced alterations in cellular and tissue structures, which is conducted through the application of microscopy and supplementary laboratory methodologies [22], [23]. Beyond these image types, camera images provide a more direct depiction of the patient's condition and are easier to gather, which is particularly effective and valuable in the detection of skin diseases [24]. Regarding medical text modalities, they generally encompass domain knowledge and data that are easy for humans to understand, collected from sources such as professional books, diagnostic reports, and literature. Recently, an increasing number of studies have focused on vision-language learning to provide complementary information and enhance performance [17].\nOther medical modalities, such as audio, physiological signals (including electrocardiogram, i.e., ECG, and electroencephalogram, i.e., EEG), and electronic health record (EHR), also play significant roles in intelligent healthcare. Nonetheless, relevant studies predominantly concentrate on modeling the intricacies within these individual modalities, overlooking the interaction among multiple modalities. Thus, our work primarily centers on investigating multimodal studies involving medical images and text, and we discuss more comprehensive multimodal applications in \u00a77."}, {"title": "2.2 Mainstream Applications", "content": "Formally, a medical image can be V \u2208 Rcxkxhxw, where c, k, h, w represent the channel, depth, height, and width of images, respectively. k = 1 denotes it is a 2D image and k > 1 indicates it is a 3D image. Language is represented as T = {w1,w2, ...,wm} with max token sequence m and wi is the word token that from vocabulary W. There are various multimodal tasks for healthcare using intelligent technologies. For example, medical image fusion incorporates image features of different modalities. RG, VQA, cross-modal retrieval (image-to-text and text-to-image), text-augmented image processing (TIP), and cross-modal image generation (CIG) are common tasks in this cross-modal field. We summarize their formalization in Table 1 and illustrate them in Figure 2. Beyond these multimodal tasks, medical multimodal learning can also benefit unimodal tasks, such as medical image classification (IC), semantic segmentation (SS), and object detection (OD). By integrating data from various sources, medical multimodal learning improves feature representation, enhances contextual understanding, and supports data augmentation, thereby boosting the performance of these unimodal tasks. We will discuss studies for these applications in \u00a74."}, {"title": "2.3 Featured Databases", "content": "There are outstanding medical recording databases, which are usually utilized for multimodal healthcare, such as PubMed\u00b3, MIMIC-CXR [25], and UMLS [26]. PubMed is a free medical literature database, gathering biomedical literature that composes medical journal articles, conference papers, and book chapters. It provides citations and abstracts, which may include links to the full text. Similarly, PubMed Central (PMC) provides an archive of full-text articles. MIMIC-CXR is a comprehensive database comprised of 377K images that correspond to a total of 227K chest radiographic studies. Each of these studies is accompanied by a detailed radiology report and pertinent chest X-ray (CXR) images. Authored by radiologists, these reports present a synopsis of their discoveries and typically consist of various sections, including examination, indication, impression, findings, technique, and comparison. UMLS, short for the unified medical language system, is a comprehensive collection of medical concepts from various lexicon resources. Each concept is assigned a unique identifier, which comes with corresponding definitions and numerous synonymous names. The UMLS also offers insights into the relationships between medical entities with a triplet format, conceptualizing a widespread medical knowledge graph (KG).\nUsing available resources, certain datasets are curated for specific tasks, such as the prevalent RG and VQA. We list some representative ones in Table 2 and Table 3, which are detailed described in the Appendix and further discussed in \u00a76. They can be utilized for the data construction for MLLMs, as shown in \u00a75. While we only discuss these two kinds of datasets, their utility extends to numerous tasks through the process of transformation. For instance, datasets used for RG can also be utilized for cross-modal retrieval, given their one-to-one correspondence."}, {"title": "3 MULTIMODAL MEDICAL STUDIES", "content": ""}, {"title": "3.1 Multi-modality Image Fusion", "content": "Images from a single modality provide limited insight into pathogenetic information within the human body. A reasonable fusion of multimodal medical images significantly"}, {"title": "3.2 Medical Report Generation", "content": "Recently, researchers have proposed advanced strategies to enhance the quality and accuracy of automated medical RG, which can be categorized into three approaches: enhancing cross-modal alignment, improving through reinforcement learning techniques, and integrating auxiliary knowledge."}, {"title": "3.3 Medical VQA", "content": "The study of medical VQA focuses on interpreting medical images alongside spoken-language queries to generate accurate natural-language responses. It exhibits significant promise for diagnostic assistance and enhancing patient comprehension through educational support. The key research question revolves around the precise identification and comprehension of important regions within medical images (such as lesions, anomalies, and occupying masses), and the semantic feature space alignment of these regions with the core demands expressed in the textual queries [74]."}, {"title": "3.4 Cross-modal Retrieval", "content": "The medical cross-modal retrieval studies mainly line in the two categories: cross-modal retrieval within images, and retrieval between images and texts. Cross-modal medical image retrieval involves searching for medical images in a database that have similar visual features to a given query image, thereby facilitating efficient clinical decision-making. Traditional studies handle the retrieval task by calculating similarities between texture features of different images, such as Radon Transform [81] on X-rays. Further, Mbilinyi et al. [82] suggested the application of deep features for extracting similar medical images from multimodal medical image databases. The results show that the retrieval performance of deep features obtained by CNNs is superior to the conventional texture features. Xu et al. [83] proposed multi-manifold deep discriminative cross-modal hashing for extensive medical image retrieval. The core aspect is the multi-modal manifold similarity that integrates multiple sub-manifolds based on heterogeneous data, thereby preserving the correlation among instances. This approach is both effective and efficient in adaptively retrieving medical images across various modalities. In general, this class of methods has undergone a transition from traditional feature extraction to efficient retrieval by deep neural networks.\nCurrently, the cross-modal retrieval between medical images and text, including ITR and TIR, is mainly done by learning embedded representations and calculating similarities by neural networks. The current major trend is to enhance representations with additional prior domain knowledge, such as the category information [84] and hierarchical semantic associations between disease labels [85]."}, {"title": "3.5 Text-augmented Image Processing", "content": "Considering the complementary information contained in the text and image modalities, some studies focus on using text-guided information for medical image processing, including image classification and semantic segmentation.\nSome text descriptions about quantity and scale could provide additional supporting information for image understanding and segmentation. For example, LViT [86] composes a U-shaped CNN branch and a U-shaped ViT branch for segmentation. It utilizes medical text annotation to address the limitations in image data quality, guiding the generation of enhanced pseudo labels in semi-supervised learning. Zhao et al. [87] and Dong et al. [88] introduced text-guided diffusion models for medical image segmentation. These models employ a text-attention mechanism to mitigate the influence of variations in the size and quantity of objects, such as representations of one, many, small, medium, and large, on the segmentation results. By concentrating on appropriate textual descriptors, the network is capable of adaptively modulating its focus towards the key characteristics of target objects, ensuring that the segmentation is both accurate and robust to changes in object attributes. Recently, medical FMs are frequently employed for text-guided image tasks, which is shown in \u00a74 and \u00a75."}, {"title": "3.6 Cross-modal Image Generation", "content": "In the medical domain, cross-modal image generation (also called modality translation) can be utilized in various scenarios, including education, data augmentation, missing data filling, and pathology research & understanding. From the technical perspective, they are categorized into two classes: GAN-based (generative adversarial network) and diffusion-based. GAN-based models adopt the principle of adversarial training [89], involving two networks. The first generator is responsible for creating synthetic instances based on training data. The second discriminator is to differentiate between synthetic and real data. This competitive dynamic prompts the generator to produce highly realistic samples. In reality, CT scans emit radiation, potentially causing patient side effects, and their effectiveness in providing detailed images of soft tissue injuries is somewhat restricted. In contrast, MRI is radiation-free and safer. Thus, there is growing interest in generating CT images from corresponding MRI ones using GAN models, including perspectives of context-aware [90] and gradient consistency [91]. Also, there are several studies on the generation of other modalities, e.g., CT to PET [92], MRI to PET [93], and PET to MRI [94].\nCross-modal diffusion-based generation models essentially transform the task of direct target generation into predicting random noise at every diffusion step. At its core, the diffusion model contains two critical processes: the forward diffusion process and the reverse denoising process. The forward diffusion process incrementally introduces Gaussian noise into an instance until it morphs into a sample of random noise. Conversely, the reverse denoising process aims to predict and remove the introduced noise [95]. Lyu et al. [96] made full use of denoising diffusion and score-matching strategies based on four different sampling approaches, implementing MRI to CT image synthesis. The results show that the model generates better synthetic CT images than the CNN and GAN models. Similarly, Meng et al. [97] introduced a unified multi-modal conditional score-based generative model to synthesize the missing modality using remaining modalities as conditions. The model employs only a score-based network to learn different cross-modal conditional distributions and the results show it can more reliably synthesize missing modality images of MRI."}, {"title": "4 CONTRASTIVE FOUNDATION MODELS (CFMs)", "content": "Given the intrinsic rarity, specificity, and specialized nature of data in the medical field, it is challenging and unrealistic to take large-scale high-quality annotated data for training. Therefore, some self-supervised strategies are introduced for building universal FMs [117]. FMs typically denote models that acquire the broad representation of general knowledge by undergoing pre-training on large-scale data through self-supervised learning. Subsequently, they can be refined through fine-tuning. Figure 3 illustrates the general architecture and applications of FMs in the medical domain. FMs have several key features: 1) pre-training on large generic datasets; 2) self-supervised learning strategies, such as contrastive learning and mask language modeling; 3) universal knowledge representation, meaning FMs learn a generic, task-independent knowledge representation that can be applied to a variety of different downstream tasks with a small amount of fine-tuning. According to the training strategies and applications, they can be categorized into two types: contrastive FMs (CFMs) and MLLMs. CFMs focus on learning a common cross-modal representation space by jointly optimizing the image encoder and text encoder to maximize the similarity score of the positive sample (image-text pair) and minimize the similarity score of the negative sample [20]. However, MLLMs focus more on modeling the intrinsic cross-modal relationships, implementing cross-modal computation, and are capable of generating text outputs [18]. In this section, we will introduce CFMs and MLLMs will be detailedly elaborated in \u00a75."}, {"title": "4.1 Overview of CFMs", "content": "Borrowing the idea of self-supervised contrastive learning (CL) that achieved great success in the computer vision field, CLIP [17] aligns vision and language semantic representations by pre-training on large-scale image-text pairs, which has greatly promoted the development of visual semantic understanding. It has sparked interest in its potential applications in the medical field. The general idea is to use an image encoder (e.g., pre-trained ResNet [118] or ViT [55]) and a language encoder (e.g., RoBERTa [119], CXR-BERT [106], ClinicalBERT [120], or PubMedBERT [121]) to obtain their corresponding representations. Rarely, an adapter serves as the bridge between them. Subsequently, they are updated using semantic contrast. We classify these studies into two categories: CLIP-based and CLIP-variant pre-training. The former generally uses typical global CL loss for modeling as like the original CLIP, while the latter introduces new optimizing objectives for additional specific concerns. Some representative medical CFMs are listed in Table 4 and some representative datasets for alignment are in Table 6."}, {"title": "4.2 CLIP-based Pre-training", "content": "Formally, given the image set {V1, V2,\u2026\u2026,VN} and text set {T1, T2,\u2026\u2026,Tv} where Vi and Ti form a image-text pair, their representations v \u2208 RN\u00d7n\u00d7d and t \u2208 R\u00d1\u00d7m\u00d7d can be obtained by image and text encoders, respectively. This formalization is similar to \u00a72.2. Further, their global representation can be obtained by pooling operation or taken a representative one as v9 \u2208 RN\u00d7d and t9 \u2208 RN\u00d7d. D, N, and M are the index sets for the samples, image patches, and text tokens. Based on them, global GL (GCL) employs a comprehensive and holistic perspective for semantic relationships. Its loss is to directly model the whole semantics between image and text using InfoNCE loss:\nLGCL=Ei\u2208D[\u2212logexp(s(v9i,t9i)/\u03c4)\u2211k=1Nexp(s(v9i,t9k)/\u03c4)]          (1)\ns denotes the cosine similarity function and \u03c4 is a pre-set temperature parameter. Note that it may s(a,b) 6= s(b,a), so s(t9, v9) may also be calculated for comprehensive modeling. Using GCL, many studies learn semantic representations for medical images and their corresponding description texts. For example, studies on CXR (CheXzero [100] & PairAug [104]), pathology (PLIP [22] & PathCLIP [102]), radiology (ConVIRT [98]), and multiple modalities (PubMedCLIP [99] & BiomedCLIP [101]). Unlike these approaches concentrating on 2D images, Hamamci et al. [103] proposed the first 3D medical imaging dataset CT-RATE with textual reports. Based on it, CT-CLIP is pre-trained using a 3D encoder for chest CT volume representations and it is then aligned with CXR-BERT outputs."}, {"title": "4.3 CLIP-variant Pre-training", "content": "Beyond GCL, there are studies with other modeling objectives, which can be summarized as following four types.\n\u2022 Intra-modal modeling. Beyond inter-modal modeling, there are additional objectives focused on intra-modal modeling, where two standard objectives are typically employed. MIM (mask image modeling) [112] usually uses the mean squared error function to compute the normalized pixel-wise difference between the original image patches Pi and reconstructed image patches P9i. MLM (mask language modeling) [72], [110], [112] are to predict masked tokens based on other given tokens. They are calculated as follows:\nLMIM=E[(P9i\u2212Pi)2],LMLM=Ei\u2208D,j\u2208M[I(T9j)\u22c5f(T9j)]             (2)\ni\u2208Dj\u2208MIis to indicate where token T9j is masked, where I(T9j) = 1 if masked. Otherwise, the value is 0. f is the loss for the predicted token compared to the original one. These two approaches acquire internal semantics of modality through reconstruction, enhancing single-modal model capability.\n\u2022 Multi-granularity. Focusing on the global information of images and texts may not be sufficient for capturing fine-grained information. So based on GCL, its variant of multi-granularity is introduced, where local CL (LCL) is the representative. It utilizes the global (or local) representations of one modality to align local representations of another. Local token representations of text and image are t9i,j and v9i,l, which means the token j of sample i. The corresponding global representation can be t9o and v9o, obtained by pooling strategy or linear transformation based on t9i and v9i. There are two main LCL approaches (global-to-local as example):\nLLCL(I2T)=Ei\u2208D,j\u2208M[\u2212logexp((\u03c3(v9o,t9i,j))/\u03c4\u2211k=1Nexp((\u03c3(v9o,t9i,k))/\u03c4)]             (3)\nLLCL(T2I)=Ei\u2208D,j\u2208N[\u2212logexp((\u03c3(t9o,v9i,j))/\u03c4\u2211k=1Nexp((\u03c3(t9o,v9i,k))/\u03c4)]             (4)\nThe former uses global image information as the anchor, aligning it with corresponding local text counterparts. MGCA [108] and BioViL-T [72] follow the latter manner, viewing the global text representation as the anchor. GLORIA [105] uses both manners, where local representations are obtained by text token-based attentive image pooling.\nIn a different manner, MLIP [113] introduces local token-knowledge-patch alignment using a medical KG, i.e., UMLS. The cross-attention is used to match knowledge and image or text, with the input of pre-trained entity embeddings of UMLS and feature of image patch or text token. Processed representations are compared with the original ones for knowledge alignment, from both image and text sides. Thus, MLIP is in a manner of local-local contrast rather than other global-local methods. Beyond LCL, MGCA [108] also introduces cross-modal prototype alignment, which assumes that images with the same disease would have similar disease-level representations. It has a higher level than the global instance and the local tokens. To realize it, MGCA pre-defines trainable embeddings for cross-modal prototypes with a certain number, which can be used for pseudo-label calculation with global text and image representations. Finally, the model is optimized by aligning the text and image pseudo-labels. Similarly, MLIP also introduces a higher level, i.e., category-level, where KG embeddings and original features are guided for category clustering.\n\u2022 Structural Knowledge Fusion. To inject professional knowledge of the medical domain to enhance the semantic representations, knowledge-fused pre-training is proposed. They usually have two pre-training stages, where the first is to incorporate domain-structured knowledge to optimize the knowledge encoder and the second is for image-text alignment with the pre-trained knowledge encoder. KAD [21] leverages medical knowledge to guide vision-language pre-training. A well-established medical KG, i.e., UMLS, is introduced to fine-tune PubMedBERT by contrastive loss of concept-definition pairs and concept-relation-concept triplets. Then the given raw X-ray reports are converted into contents of medical entities and their presence, using heuristically defined rules, RadGraph [65], or ChatGPT. The pre-trained knowledge encoder is used to guide the visual representation learning by contrastive learning between representations of image and generated entity contents, effectively injecting the domain knowledge into the visual encoder. Besides, the query disease is also input to incorporate randomly selected image or text entity content representations for disease prediction. In the inference stage, by inputting unseen diseases, KAD can handle zero-shot disease prediction given an image. KEP [115] curates a pathology knowledge tree PathKT, consisting of three-level tree structures: tissue, disease, and attribute. Each disease entity corresponds to several attributes, including disease synonyms, definitions, cytology and pathology features. The knowledge encoder is updated by metric learning with AdaSP loss [122], such that the representations of a specific disease and its attributes are close in the embedding space. After that, the text encoder (initialized with the weights of the knowledge encoder) and image encoder are updated by image-text contrastive learning."}, {"title": "4.4 Data Augmentation", "content": "Medical texts are usually characterized by their specialized and condensed nature, making them difficult to understand by layman and neural models. Therefore, several studies have introduced augmented text descriptions. MedKLIP [109] focuses on the entities in the medical reports and adds entity descriptions. The representations of these added descriptions are fused with image features to make predictions of entity existence and its location. Based on it, MAVL [114] further expands the description of disease entities to multiple visual aspects, including pattern, texture, opacity, border, location, shape, and fluid presence, where GPT-4 is utilized to programmatically generate descriptions of these aspects. Beyond the loss functions of MedKLIP, it introduces another contrastive target to align visual representation and that of each entity aspect's description, empowering MAVL with the ability of zero-shot recognition of unseen diseases. DeViDe [116] utilizes publicly-available Mixtral-8\u00d77B [123] to collect and process radiographic descriptions, for a specific entity or a disease.\nConsidering most augmentation techniques tend to narrow their focus, prioritizing either text or image augmentation, rather than blending the two. PairAug [104] designs a pairwise augmentation approach that contains an inter-patient augmentation (InterAug) branch and an intra-patient augmentation (IntraAug) branch. Specifically, the InterAug branch generates radiology images using synthesised yet plausible reports derived from an LLM. IntraAug branch uses newly generated reports to manipulate images. This process facilitates the generation of new paired data for each individual with diverse medical conditions, where ChatGPT is used to report modification."}, {"title": "4.5 Downstream Applications", "content": "Based on pre-trained CFMs, many medical applications can be achieved, varying from uni-modal to cross-modal tasks.\n\u2022 Uni-modal Tasks. The jointly pre-trained image encoder and text encoder can be individually or jointly used for many uni-modal tasks, such as image classification, semantic segmentation, and object detection. Medical image classification is usually to detect diseases in the image, based on GCL pre-training, CFMs can do zero-shot image classification by calculating the similarity between image representation and text prompts with a specific disease, e.g., this is an image of {disease} and {disease} presented in the image [101]. Also, the image encoder can be frozen and the subsequent MLP is updated to fine-tune for image classification tasks, namely linear probing. For semantic segmentation and object detection tasks, the pre-trained image encoder is initialized as the backbone encoder, followed by a trainable task-specific decoder, like U-Net [124] or ResUNet [125] and YOLOv3 [126] for these two tasks, respectively.\n\u2022 Cross-modal Tasks. Pre-trained CFMs can also be used for many cross-modal tasks, including VQA, RG, ITR, TIR, and visual grounding. As the CFMs generally have no ability for text generation, so its application for VQA mainly focuses on the classification setting, using VQA-RAD and SLAKE datasets [99], [110]. The VQA model is usually initialized with a pre-trained CFM encoder and incorporates an MLP or specific decoder to make predictions. For report generation, BioViL-T processes the prior report and both images (prior and current) with an encoder and an additional decoder is utilized for generation, where two broad categories, i.e., nearest-neighbour and auto-regressive can be used. Retrieval-based tasks (ITR, TIR and disease retrieval) are directly realized by calculating similarities between the two modalities' representations. Similarly, phase grounding calculates the similarity of representations between the text phase and the local image patch [72], [106]."}, {"title": "5 MULTIMODAL LLMS (MLLMS)", "content": "Benefiting from the rapid development of LLMs, MLLMs, also known as visual language models (VLMs), have garnered significant attention from researchers owing to their powerful representational capabilities and remarkable proficiency in handling multimodal data [18]. Their general modeling objectives are the next token prediction based on the image and previous text tokens:\nLMLLM=Ei\u2208D,j\u2208M[\u2212logp(Tj|Vi,T<j)]          (5)\nFrom both theoretical and application standpoints, there exist distinct differences between CFMs and MLLMs: 1) CFMs are generally tuned based on the image-text pair data, whereas MLLMs focus on the multimodal instruction-following data; 2) GCL is the main objective for the CFMs while MLLMs are to generate text based on multimodal inputs; 3) CFMs are typically used for discriminative tasks, but MLLMs are more commonly used for generative tasks. We summarize some typical general MLLMs and noted medical MLLMs in Table 5."}, {"title": "5.1 Modality Encoder and Cross-modal Adapter", "content": "MLLMs employ a straightforward yet effective method for building FMs. This approach involves the use of an image encoder and language model, collectively known as modality encoders, which facilitates corresponding representations in latent spaces. Additionally, a cross-modal adapter is introduced to align these representations of various modalities within a shared space.\n\u2022 Image Encoder. Pre-trained image encoders are typically employed to generate image representations, which are subsequently integrated with LLMs for multimodal tasks. Commonly utilized pre-trained encoders include NFNet [150], ViT [55], CLIP ViT [17], EVA-CLIP [151] of the general domain. Besides, to enhance the encapsulation of medical knowledge within the latent representations, pre-trained medical image encoders are employed in the creation of medical MLLMs. For instance, the vision components of image-text pre-trained PathCap [102], BioMedCLIP [101], and BioViL-T [72] act as the image encoders of PathAsst [102], LLaVA-Med [136], and RaDialog [140], respectively. Additionally, RAD-DINO [152] employs CXR image pre-training, based on the self-supervised pre-training strategy of the DINO model [153].\n\u2022 Language Model. Building on the powerful LLMs within the NLP domain, MLLMs employ them to process textual input and generate corresponding textual responses. Both general-purpose and medical-specific MLLMs utilize the Transformer-style architecture as the text encoder. This is evident in models such as OPT [154], Flan-T5 [155], Vicuna [156], Mistral [157], LLAMA [158], LLaMA2 [159], and the Chinese-LLaMA2 [160], which is catered to the general domain. In addition, there are several models tailored to medical languages, such as BioGPT [161], BioMedLM [162], and MedLLaMA [163], also in widespread use.\n\u2022 Cross-modal Adapter. The cross-modal adapter serves as a connector between image and text representations within MLLMs. Primarily, it encompasses three categories: Linear Projection, Query Representation, and Cross Attention, as illustrated in Figure 5. The first two types are handled during the input stages, transforming the hidden image representation into virtual token embeddings that align with text token embeddings. The last type typically manifests within the internal computational procedures of the LLM [18]."}, {"title": "5.2 Tuning Process & Technical Details", "content": "Based on the image encoder and language models, the implementation of MLLMs is achieved through the use of pre-training and instruction-tuning strategies [8", "18": ".", "136": "is first fine-tuned on LLaVA-Med-Align (600K biomedical image-text pairs) to update linear transformation layer and then carries out the instruction-tuning on LLaVA-Med-Inst (60K image-text responses collected using GPT-4). For parameter-efficient tuning, the LoRA [164", "132": [140], "147": [148], "128": "introduces the caption loss in the architecture. The model comprises an image encoder, a text decoder, and a multimodal text decoder, capable of handling text generation tasks. Flamingo [127", "129": "pre-trains a lightweight QFormer following a two-stage strategy to bridge the modality gap. It first bootstraps vision-language representation learning from a frozen image encoder and the second stage bootstraps vision-to-language generative learning from a frozen LLM.\nLLAVA [130", "131": "are the pioneers of MLLMs tuning with image-text instruction data. LLaVA is an initial endeavour to employ a language-only GPT-4 model for the creation of multimodal language-image instruction data known as LLaVA-Instruct-158K. To effectively leverage the capabilities of both pre-trained LLM and visual model, a linear projection is introduced to align the vision features to the language counterpart. LLaVA is pre-trained with only 1 epoch for feature alignment, where only parameters of the linear projection are optimized. Then, keeping the visual encoder weights frozen, both pre-trained weights of the projection layer and LLM are updated on LLaVA-Instruct-158K for 3 epochs or on ScienceQA [165", "24": "LLaVA-Med [136", "137": "are the adaptions of MiniGPT-4, LLaVA, and Flamingo within the medical field, respectively. For human pathology, PathAsst [102", "143": "are developed through instruction tuning, utilizing pathology image-text data. Taking PathAsst as an example, it constructs instruction-following data PathInstruct, which contains description"}]}