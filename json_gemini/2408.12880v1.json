{"title": "Has Multimodal Learning Delivered Universal Intelligence in Healthcare? A Comprehensive Survey", "authors": ["Qika Lin", "Yifan Zhu", "Xin Mei", "Ling Huang", "Jingying Ma", "Kai He", "Zhen Peng", "Erik Cambria", "Mengling Feng"], "abstract": "The rapid development of artificial intelligence has constantly reshaped the field of intelligent healthcare and medicine. As a vital technology, multimodal learning has increasingly garnered interest due to data complementarity, comprehensive modeling form, and great application potential. Currently, numerous researchers are dedicating their attention to this field, conducting extensive studies and constructing abundant intelligent systems. Naturally, an open question arises that has multimodal learning delivered universal intelligence in healthcare? To answer the question, we adopt three unique viewpoints for a holistic analysis. Firstly, we conduct a comprehensive survey of the current progress of medical multimodal learning from the perspectives of datasets, task-oriented methods, and universal foundation models. Based on them, we further discuss the proposed question from five issues to explore the real impacts of advanced techniques in healthcare, from data and technologies to performance and ethics. The answer is that current technologies have NOT achieved universal intelligence and there remains a significant journey to undertake. Finally, in light of the above reviews and discussions, we point out ten potential directions for exploration towards the goal of universal intelligence in healthcare.", "sections": [{"title": "1 INTRODUCTION", "content": "RECENT years have seen the remarkable progress of artificial intelligence (AI) across the healthcare and medicine domain [1]. AI techniques have demonstrated substantial potential in various medical scenarios, including medical imaging analysis [2], disease diagnosis [3], drug discovery [4], personalized treatment [5], and medical QA (question-answering) [6], aiming to provide automated and customized expert-level advice or recommendations to alleviate the burden on both patients and physicians. Nonetheless, these studies or applications typically consider only single-modality data, e.g., medical image or text, which could result in diminished performance and may not accurately represent authentic application scenarios [7].\nAs the healthcare domain ceaselessly produces an increasing volume and variety of data, ranging from medical images and clinical notes to genomic profiles and biosensor readings, the need for effective multimodal learning approaches becomes paramount [7], [8], [9], [10]. On the one hand, multimodal AI models, capable of integrating and learning from these heterogeneous data streams, hold the promise of unlocking a comprehensive and nuanced understanding of complex medical phenomena. By capturing complementary semantic information [11] (as shown in Figure 1) and intricate relationships across different modalities [12], these models provide clinicians with a holistic view of patients' conditions, enabling more proactive monitoring, accurate diagnoses, and personalized treatment plans. On the other hand, multimodal learning further broadens the application prospects of intelligent models in the healthcare field. For instance, if a patient needs to ask about their skin condition, articulating it verbally (e.g., using conventional language QA systems) can be challenging. A visual question-answering (VQA) system becomes incredibly useful, as it can combine intuitive images uploaded by patients to make more accurate and comprehensive diagnoses. Given the significant research importance and application value of multimodal healthcare, recent years have witnessed an extensive amount of research dedicated to this particular subject, with a clear rising trend. The advancement in technologies has progressed from utilizing specific models, such as convolutional neural network (CNN) [13], recurrent neural network (RNN) [14], graph neural network (GNN) [15], and Transformer [16], to the adoption of a strategy involving pre-training and fine-tuning. The latter has emerged as the prevailing focus and trending topic, which is inspired by the powerful foundational models (FMs) in the general domain, like CLIP [17], ChatGPT1, GPT-42, and multimodal large language model (MLLM) [18]. These studies have made significant advancements in numerous tasks of multimodal healthcare, e.g., multi-modality image fusion, report generation (RG), VQA, cross-modal retrieval, text-augmented image processing, and cross-modal image generation. The evolution has ultimately led to the development of FMs"}, {"title": "2 PRELIMINARIES", "content": "Academically, modality refers to the way things are expressed or perceived, with every form or source of information being categorized as a modality [12]. In the healthcare domain, the term multimodal data typically pertains to digital records derived from diverse sources such as various machines, sensors, or experts, often represented in distinct formats. The medical modalities encompass elements such as medical vision, text, audio, and physiological signals, among others [7]. As shown in Figure 1, the medical vision modality consists of images obtained by different sensors, which are utilized for viewing the conditions or diseases of different organs or tissues. Within this scope, three types of images are commonly used, namely radiology, pathology, and camera images. Radiology is frequently employed to capture images of the human body's internal conditions [21], primarily encompassing components such as X-ray, computed tomography (CT, 2D/3D), magnetic resonance imaging (MRI), positron emission tomography (PET), and ultrasound. Pathology is the scientific exploration of disease-induced alterations in cellular and tissue structures, which is conducted through the application of microscopy and supplementary laboratory methodologies [22], [23]. Beyond these image types, camera images provide a more direct depiction of the patient's condition and are easier to gather, which is particularly effective and valuable in the detection of skin diseases [24]. Regarding medical text modalities, they generally encompass domain knowledge and data that are easy for humans to understand, collected from sources such as professional books, diagnostic reports, and literature. Recently, an increasing number of studies have focused on vision-language learning to provide complementary information and enhance performance [17].\nOther medical modalities, such as audio, physiological signals (including electrocardiogram, i.e., ECG, and electroencephalogram, i.e., EEG), and electronic health record (EHR), also play significant roles in intelligent healthcare. Nonetheless, relevant studies predominantly concentrate on modeling the intricacies within these individual modalities, overlooking the interaction among multiple modalities. Thus, our work primarily centers on investigating multimodal studies involving medical images and text, and we discuss more comprehensive multimodal applications in \u00a77."}, {"title": "2.2 Mainstream Applications", "content": "Formally, a medical image can be $V \\in R^{c\\times k\\times h\\times w}$, where c, k, h, w represent the channel, depth, height, and width of images, respectively. k = 1 denotes it is a 2D image and k > 1 indicates it is a 3D image. Language is represented as $T = \\{w_1,w_2, ...,w_m\\}$ with max token sequence m and $w_i$ is the word token that from vocabulary W. There are various multimodal tasks for healthcare using intelligent technologies. For example, medical image fusion incorporates image features of different modalities. RG, VQA, cross-modal retrieval (image-to-text and text-to-image), text-augmented image processing (TIP), and cross-modal image generation (CIG) are common tasks in this cross-modal field. We summarize their formalization in Table 1 and illustrate them in Figure 2. Beyond these multimodal tasks, medical multimodal learning can also benefit unimodal tasks, such as medical image classification (IC), semantic segmentation (SS), and object detection (OD). By integrating data from various sources, medical multimodal learning improves feature representation, enhances contextual understanding, and supports data augmentation, thereby boosting the performance of these unimodal tasks. We will discuss studies for these applications in \u00a74."}, {"title": "2.3 Featured Databases", "content": "There are outstanding medical recording databases, which are usually utilized for multimodal healthcare, such as PubMed\u00b3, MIMIC-CXR [25], and UMLS [26]. PubMed is a free medical literature database, gathering biomedical literature that composes medical journal articles, conference papers, and book chapters. It provides citations and abstracts, which may include links to the full text. Similarly, PubMed Central (PMC) provides an archive of full-text articles. MIMIC-CXR is a comprehensive database comprised of 377K images that correspond to a total of 227K chest radiographic studies. Each of these studies is accompanied by a detailed radiology report and pertinent chest X-ray (CXR) images. Authored by radiologists, these reports present a synopsis of their discoveries and typically consist of various sections, including examination, indication, impression, findings, technique, and comparison. UMLS, short for the unified medical language system, is a comprehensive collection of medical concepts from various lexicon resources. Each concept is assigned a unique identifier, which comes with corresponding definitions and numerous synonymous names. The UMLS also offers insights into the relationships between medical entities with a triplet format, conceptualizing a widespread medical knowledge graph (KG).\nUsing available resources, certain datasets are curated for specific tasks, such as the prevalent RG and VQA. We list some representative ones in Table 2 and Table 3, which are detailed described in the Appendix and further discussed in \u00a76. They can be utilized for the data construction for MLLMs, as shown in \u00a75. While we only discuss these two kinds of datasets, their utility extends to numerous tasks through the process of transformation. For instance, datasets used for RG can also be utilized for cross-modal retrieval, given their one-to-one correspondence."}, {"title": "3 MULTIMODAL MEDICAL STUDIES", "content": "Images from a single modality provide limited insight into pathogenetic information within the human body. A reasonable fusion of multimodal medical images significantly contributes to a comprehensive understanding of intricate medical conditions [46], allowing clinicians to better delineate anatomical structures, lesions, and abnormalities. To establish a comprehensive view of how multimodal medical images are combined and analyzed, we introduce existing fusion strategies at the pixel, feature, and decision levels.\n*   Pixel-level fusion is a low-level fusion operation that concatenates pixels directly on the original image layer or their corresponding multi-resolution coefficients [47]. These approaches can be classified into three main categories: 1) multi-scale decomposition-based techniques [48]; 2) sparse representation methods [49], and 3) component substitution techniques [50]. However, the outcomes are often impacted by blurring effects that directly affect image contrast [51]. Moreover, there is a high registration requirement for multimodal images and it is usually sensitive to noise, making the pixel-level fusion process time-consuming and challenging.\n*   Feature-level fusion is a middle-level fusion strategy, and it is also recognized as the most commonly used strategy in deep learning methods. The common feature-level fusion strategy is to learn a shared representation or a joint embedding space from multiple features, using technologies such as adversarial learning [52], co-training [53], multi-task learning [54]. More recently, the transformer-based architectures, such as vision Transformer (ViT) [55], also show great versatility in handling different types of data, especially for heterogeneous data, and can be leveraged for feature-level fusion tasks by learning a joint representation. While feature-level methods overcome the drawbacks of pixel-based algorithms in terms of contrast, sensitivity to noise, and misregistration, they still have limitations such as spatial distortions [56]. For instance, medical images often contain unclear regions due to poor illumination.\n*   Decision-level fusion is a high-level fusion strategy. It integrates multiple decisions derived from preliminary classifications and aggregates those decisions finally. Approaches are classified into two main categories: 1) hard fusion methods, which merge logical information membership values, such as model ensembling with majority or average voting [57]; and 2) soft fusion methods, where classifiers assign numerical values to reflect their confidence in decisions, or fuzzy classifiers are applied, such as fuzzy voting [58]."}, {"title": "3.2 Medical Report Generation", "content": "Recently, researchers have proposed advanced strategies to enhance the quality and accuracy of automated medical RG, which can be categorized into three approaches: enhancing cross-modal alignment, improving through reinforcement learning techniques, and integrating auxiliary knowledge.\n*   Cross-modal Alignment. These studies focus on enhancing cross-modal alignment between medical images and reports to improve medical RG. Najdenkoska et al. [59] explored learning key topics between images and reports using variational topic inference to enhance semantic coherence. To facilitate multi-level cross-modal alignments, Li et al. [60] unified vision and text modalities into discrete tokens, which are then used to learn global semantic alignment and token-level alignment. Additionally, contrastive learning techniques [61] are also utilized for refined alignment between visual and textual data, enhancing the overall performance. For example, Wang et al. [62] introduced a phenotype-based contrastive learning framework that learns fine-grained representations, effectively bridging the gap between visual and textual modalities."}, {"title": "3.3 Medical VQA", "content": "The study of medical VQA focuses on interpreting medical images alongside spoken-language queries to generate accurate natural-language responses. It exhibits significant promise for diagnostic assistance and enhancing patient comprehension through educational support. The key research question revolves around the precise identification and comprehension of important regions within medical images (such as lesions, anomalies, and occupying masses), and the semantic feature space alignment of these regions with the core demands expressed in the textual queries [74].\n*   Knowledge Extraction Frameworks. For encoder structures on a single modality, employing learning strategies that allow them to concurrently preserve both pre-existing general visual common sense and domain-specific medical knowledge represents an efficient model-agnostic solution. The first category employs the meta-learning paradigm, where the VQA model distributes the parameter training process across multiple related tasks to fuse loss gradients, ultimately adapting the model to medical VQA scenarios, e.g., MAML [75] for model transfer and MMQ [76] for data refinement. Another approach to integrating medical knowledge with common sense is to perform conditional reasoning, which is designed to learn task-adaptive reasoning skills for different types of VQA tasks. This reasoning ability can be implemented by a supervised label loss on the embedding fusion of visual and textual encoders [77].\n*   Pre-training & Fine-tuning Frameworks. The remarkable advancements in pre-trained language models (LMs), such as BERT [78], have also introduced a novel paradigm for medical VQA. By designing self-supervised learning tasks, LMs can learn from vast text corpora without relying on external annotations, thus significantly reducing the cost of labeling. When faced with specific downstream VQA tasks, only a small amount of labeled data is required to finetune the model. In particular, knowledge relationships can also be injected into the pre-training process by non-euclidean encoders such as GNN [79]. Owing to the computational constraints of adjusting a large number of parameters during the fine-tuning phase, recent research has introduced VQA downstream task adaptation strategies that involve freezing parameters of the pre-trained model. For example, Liu et al. [80] proposed to establish a VQA adapter that is external to the pre-trained model, thus creating a plug-in for model adaptation to downstream tasks."}, {"title": "3.4 Cross-modal Retrieval", "content": "The medical cross-modal retrieval studies mainly line in the two categories: cross-modal retrieval within images, and retrieval between image and texts. Cross-modal medical image retrieval involves searching for medical images in a database that have similar visual features to a given query image, thereby facilitating efficient clinical decision-making. Traditional studies handle the retrieval task by calculating similarities between texture features of different images, such as Radon Transform [81] on X-rays. Further, Mbilinyi et al. [82] suggested the application of deep features for extracting similar medical images from multimodal medical image databases. The results show that the retrieval performance of deep features obtained by CNNs is superior to the conventional texture features. Xu et al. [83] proposed multi-manifold deep discriminative cross-modal hashing for extensive medical image retrieval. The core aspect is the multi-modal manifold similarity that integrates multiple sub-manifolds based on heterogeneous data, thereby preserving the correlation among instances. This approach is both effective and efficient in adaptively retrieving medical images across various modalities. In general, this class of methods has undergone a transition from traditional feature extraction to efficient retrieval by deep neural networks.\nCurrently, the cross-modal retrieval between medical images and text, including ITR and TIR, is mainly done by learning embedded representations and calculating similarities by neural networks. The current major trend is to enhance representations with additional prior domain knowledge, such as the category information [84] and hierarchical semantic associations between disease labels [85]."}, {"title": "3.5 Text-augmented Image Processing", "content": "Considering the complementary information contained in the text and image modalities, some studies focus on using text-guided information for medical image processing, including image classification and semantic segmentation.\nSome text descriptions about quantity and scale could provide additional supporting information for image understanding and segmentation. For example, LViT [86] composes a U-shaped CNN branch and a U-shaped ViT branch for segmentation. It utilizes medical text annotation to address the limitations in image data quality, guiding the generation of enhanced pseudo labels in semi-supervised learning. Zhao et al. [87] and Dong et al. [88] introduced text-guided diffusion models for medical image segmentation. These models employ a text-attention mechanism to mitigate the influence of variations in the size and quantity of objects, such as representations of one, many, small, medium, and large, on the segmentation results. By concentrating on appropriate textual descriptors, the network is capable of adaptively modulating its focus towards the key characteristics of target objects, ensuring that the segmentation is both accurate and robust to changes in object attributes. Recently, medical FMs are frequently employed for text-guided image tasks, which is shown in \u00a74 and \u00a75."}, {"title": "3.6 Cross-modal Image Generation", "content": "In the medical domain, cross-modal image generation (also called modality translation) can be utilized in various scenarios, including education, data augmentation, missing data filling, and pathology research & understanding. From the technical perspective, they are categorized into two classes: GAN-based (generative adversarial network) and diffusion-based. GAN-based models adopt the principle of adversarial training [89], involving two networks. The first generator is responsible for creating synthetic instances based on training data. The second discriminator is to differentiate between synthetic and real data. This competitive dynamic prompts the generator to produce highly realistic samples. In reality, CT scans emit radiation, potentially causing patient side effects, and their effectiveness in providing detailed images of soft tissue injuries is somewhat restricted. In contrast, MRI is radiation-free and safer. Thus, there is growing interest in generating CT images from corresponding MRI ones using GAN models, including perspectives of context-aware [90] and gradient consistency [91]. Also, there are several studies on the generation of other modalities, e.g., CT to PET [92], MRI to PET [93], and PET to MRI [94].\nCross-modal diffusion-based generation models essentially transform the task of direct target generation into predicting random noise at every diffusion step. At its core, the diffusion model contains two critical processes: the forward diffusion process and the reverse denoising process. The forward diffusion process incrementally introduces Gaussian noise into an instance until it morphs into a sample of random noise. Conversely, the reverse denoising process aims to predict and remove the introduced noise [95]. Lyu et al. [96] made full use of denoising diffusion and score-matching strategies based on four different sampling approaches, implementing MRI to CT image synthesis. The results show that the model generates better synthetic CT images than the CNN and GAN models. Similarly, Meng et al. [97] introduced a unified multi-modal conditional score-based generative model to synthesize the missing modality using remaining modalities as conditions. The model employs only a score-based network to learn different cross-modal conditional distributions and the results show it can more reliably synthesize missing modality images of MRI."}, {"title": "4 CONTRASTIVE FOUNDATION MODELS (CFMs)", "content": "Given the intrinsic rarity, specificity, and specialized nature of data in the medical field, it is challenging and unrealistic to take large-scale high-quality annotated data for training. Therefore, some self-supervised strategies are introduced for building universal FMs [117]. FMs typically denote models that acquire the broad representation of general knowledge by undergoing pre-training on large-scale data through self-supervised learning. Subsequently, they can be refined through fine-tuning. Figure 3 illustrates the general architecture and applications of FMs in the medical domain. FMs have several key features: 1) pre-training on large generic datasets; 2) self-supervised learning strategies, such as contrastive learning and mask language modeling; 3) universal knowledge representation, meaning FMs learn a generic, task-independent knowledge representation that can be applied to a variety of different downstream tasks with a small amount of fine-tuning. According to the training strategies and applications, they can be categorized into two types: contrastive FMs (CFMs) and MLLMs. CFMs focus on learning a common cross-modal representation space by jointly optimizing the image encoder and text encoder to maximize the similarity score of the positive sample (image-text pair) and minimize the similarity score of the negative sample [20]. However, MLLMs focus more on modeling the intrinsic cross-modal relationships, implementing cross-modal computation, and are capable of generating text outputs [18]. In this section, we will introduce CFMs and MLLMs will be detailedly elaborated in \u00a75."}, {"title": "4.1 Overview of CFMs", "content": "Borrowing the idea of self-supervised contrastive learning (CL) that achieved great success in the computer vision field, CLIP [17] aligns vision and language semantic representations by pre-training on large-scale image-text pairs, which has greatly promoted the development of visual semantic understanding. It has sparked interest in its potential applications in the medical field. The general idea is to use an image encoder (e.g., pre-trained ResNet [118] or ViT [55]) and a language encoder (e.g., RoBERTa [119], CXR-BERT [106], ClinicalBERT [120], or PubMedBERT [121]) to obtain their corresponding representations. Rarely, an adapter serves as the bridge between them. Subsequently, they are updated using semantic contrast. We classify these studies into two categories: CLIP-based and CLIP-variant pre-training. The former generally uses typical global CL loss for modeling as like the original CLIP, while the latter introduces new optimizing objectives for additional specific concerns. Some representative medical CFMs are listed in Table 4 and some representative datasets for alignment are in Table 6."}, {"title": "4.2 CLIP-based Pre-training", "content": "Formally, given the image set \\{V_1, V_2,......,V_N\\} and text set \\{T_1, T_2,......,T_v\\} where $V_i$ and $T_i$ form a image-text pair, their representations $v \\in R^{N\\times n\\times d}$ and $t \\in R^{\\tilde{N}\\times m\\times d}$ can be obtained by image and text encoders, respectively. This formalization is similar to \u00a72.2. Further, their global representation can be obtained by pooling operation or taken a representative one as $v' \\in R^{N\\times d}$ and $t' \\in R^{\\tilde{N}\\times d}$. D, N, and M are the index sets for the samples, image patches, and text tokens. Based on them, global GL (GCL) employs a comprehensive and holistic perspective for semantic relationships. Its loss is to directly model the whole semantics between image and text using InfoNCE loss:\n\\begin{equation}\nL_{GCL} = E_{i \\in D} \\left[ -log \\frac{exp(s(v'_i, t'_i)/\\tau)}{\\sum_{k=1}^{N} exp(s(v'_i, t'_k)/\\tau)}\\right].\n\\tag{1}\n\\end{equation}\ns denotes the cosine similarity function and $\u03c4$ is a pre-set temperature parameter. Note that it may $s(a, b) \u2260 s(b, a)$, so $s(t, v)$ may also be calculated for comprehensive modeling. Using GCL, many studies learn semantic representations for medical images and their corresponding description texts. For example, studies on CXR (CheXzero [100] & PairAug [104]), pathology (PLIP [22] & PathCLIP [102]), radiology (ConVIRT [98]), and multiple modalities (PubMedCLIP [99] & BiomedCLIP [101]). Unlike these approaches concentrating on 2D images, Hamamci et al. [103] proposed the first 3D medical imaging dataset CT-RATE with textual reports. Based on it, CT-CLIP is pre-trained using a 3D encoder for chest CT volume representations and it is then aligned with CXR-BERT outputs."}, {"title": "4.3 CLIP-variant Pre-training", "content": "Beyond GCL, there are studies with other modeling objectives, which can be summarized as following four types.\n*   Intra-modal modeling. Beyond inter-modal modeling, there are additional objectives focused on intra-modal modeling, where two standard objectives are typically employed. MIM (mask image modeling) [112] usually uses the mean squared error function to compute the normalized pixel-wise difference between the original image patches $P_i$ and reconstructed image patches $P'_i$. MLM (mask language modeling) [72], [110], [112] are to predict masked tokens based on other given tokens. They are calculated as follows:\n\\begin{equation}\nL_{MIM} = E_{i \\in D} [\\lVert P_i - P'_i \\rVert ^2], L_{MLM} = E_{i \\in D, j \\in M} [\\mathbb{I}(T'_j) \\cdot f(T'_j)].\n\\tag{2}\n\\end{equation}\n$\\mathbb{I}$ is to indicate where token T is masked, where $\\mathbb{I}(T') = 1$ if masked. Otherwise, the value is 0. f is the loss for the predicted token compared to the original one. These two approaches acquire internal semantics of modality through reconstruction, enhancing single-modal model capability.\n*   Multi-granularity. Focusing on the global information of images and texts may not be sufficient for capturing fine-grained information. So based on GCL, its variant of multi-granularity is introduced, where local CL (LCL) is the representative. It utilizes the global (or local) representations of one modality to align local representations of another. Local token representations of text and image are $t^j_i$ and $v^j_i$, which means the token j of sample i. The corresponding global representation can be $t^0_i$ and $v^0_i$, obtained by pooling strategy or linear transformation based on $t^j_i$ and $v^j_i$. There are two main LCL approaches (global-to-local as example):\n\\begin{equation}\nL_{LCL(I2T)} = E_{i \\in D, j \\in M} \\left[-log \\frac{exp(\\sigma(v^0_i, t^j_i)/\\tau)}{\\sum_{k=1}^{N} exp(\\sigma(v^0_i, t^k_i)/\\tau)} \\right],   \n\\tag{3}\n\\end{equation}\n\\begin{equation}\nL_{LCL(T2I)} = E_{i \\in D, j \\in N} \\left[-log \\frac{exp(\\sigma(t^0_i, v^j_i)/\\tau)}{\\sum_{k=1}^{N} exp(\\sigma(t^0_i, v^k_i)/\\tau)} \\right].    \n\\tag{4}\n\\end{equation}\nThe former uses global image information as the anchor, aligning it with corresponding local text counterparts. MGCA [108] and BioViL-T [72] follow the latter manner, viewing the global text representation as the anchor. GLORIA [105] uses both manners, where local representations are obtained by text token-based attentive image pooling.  \nIn a different manner, MLIP [113] introduces local token-knowledge-patch alignment using a medical KG, i.e., UMLS. The cross-attention is used to match knowledge and image or text, with the input of pre-trained entity embeddings of UMLS and feature of image patch or text token. Processed representations are compared with the original ones for knowledge alignment, from both image and text sides. Thus, MLIP is in a manner of local-local contrast rather than other global-local methods. Beyond LCL, MGCA [108] also introduces cross-modal prototype alignment, which assumes that images with the same disease would have similar disease-level representations. It has a higher level than the global instance and the local tokens. To realize it, MGCA pre-defines trainable embeddings for cross-modal prototypes with a certain number, which can be used for pseudo-label calculation with global text and image representations. Finally, the model is optimized by aligning the text and image pseudo-labels. Similarly, MLIP also introduces a higher level, i.e., category-level, where KG embeddings and original features are guided for category clustering.\n*   Structural Knowledge Fusion. To inject professional knowledge of the medical domain to enhance the semantic representations, knowledge-fused pre-training is proposed. They usually have two pre-training stages, where the first is to incorporate domain-structured knowledge to optimize the knowledge encoder and the second is for image-text alignment with the pre-trained knowledge encoder. KAD [21] leverages medical knowledge to guide vision-language pre-training. A well-established medical KG, i.e., UMLS, is introduced to fine-tune PubMedBERT by contrastive loss of concept-definition pairs and concept-relation-concept triplets. Then the given raw X-ray reports are converted into contents of medical entities and their presence, using heuristically defined rules, RadGraph [65], or ChatGPT. The pre-trained knowledge encoder is used to guide the visual representation learning by contrastive learning between representations of image and generated entity contents, effectively injecting the domain knowledge into the visual encoder. Besides, the query disease is also input to incorporate randomly selected image or text entity content representations for disease prediction. In the inference stage, by inputting unseen diseases, KAD can handle zero-shot disease prediction given an image. KEP [115] curates a pathology knowledge tree PathKT, consisting of three-level tree structures: tissue, disease, and attribute. Each disease entity corresponds to several attributes, including disease synonyms, definitions, cytology and pathology features. The knowledge encoder is updated by metric learning with AdaSP loss [122], such that the representations of a specific disease and its attributes are close in the embedding space. After that, the text encoder (initialized with the weights of the knowledge encoder) and image encoder are updated by image-text contrastive learning."}, {"title": "Other Variants.", "content": "For various aspects of multimodal medical applications, some other variants are proposed. Previous methods could encounter many false negatives, meaning that images and reports from different patients could have the same semantics but are mistakenly treated as negative samples. So MedCLIP [107] decouples image text pairs and conducts contrastive learning to reduce false negatives by introducing external medical knowledge. To make full use of limited usable data and fix false negatives in contrastive learning, MedCLIP introduces UMLS to detect 14 main entity types for images with diagnosis labels. Multi-hot vectors of 14 dimensions from the extracted entities are obtained for images and texts, from which the semantic similarity is calculated. The semantic similarity is viewed as soft targets for model training rather than 0/1 labels in the original contrastive loss. In this way, unpaired data can also be taken into consideration. To make the model capable of temporal information in the medical domain, BioViL-T [72] exploits temporal correlations by making prior images available for comparison to a given report. The visual representations of the two images combine to make global and local contrastive learning, where an additional MLM is utilized for text-side pre-training. PTUnifier [110] introduces the soft prompts to unify early-fusion and later-fusion medical vision-language pre-training, making it compatible with different kinds of inputs, including image-only, text-only, and image-text pairs. It constructs prompt pools for different modalities so that different inputs can select their corresponding prompts, improving the prompt diversity and the model scalability. To consider the presence of community bias caused by different languages, Med-UniC [111] unifying cross-lingual (English & Spanish) medical multi-modal by diminishing dias. Besides GCL for the vision-vision and vision-language alignment, cross-lingual text alignment regularization, including text augmentation, text-feature alignment, and text-to-text alignment, learns language-independent text representations and neutralizes the adverse effects of community bias on other modalities."}, {"title": "4.4 Data Augmentation", "content": "Medical texts are usually characterized by their specialized and condensed nature, making them difficult to understand by layman and neural models. Therefore, several studies have introduced augmented text descriptions. MedKLIP [109] focuses on the entities in the medical reports and adds entity descriptions. The representations of these added descriptions are fused with image features to make predictions of entity existence and its location. Based on it, MAVL [114] further expands the description of disease entities to multiple visual aspects, including pattern, texture, opacity, border, location, shape, and fluid presence, where GPT-4 is utilized to programmatically generate descriptions of these aspects. Beyond the loss functions of MedKLIP, it introduces another contrastive target to align visual representation and that of each entity aspect's description, empowering MAVL with the ability of zero-shot recognition of unseen diseases. DeViDe [116] utilizes publicly-available Mixtral-8\u00d77B [123] to collect and process radiographic descriptions, for a specific entity or a disease.\nConsidering most augmentation techniques tend to narrow their focus, prioritizing either text or image augmentation, rather than blending the two. PairAug [104] designs a pairwise augmentation approach that contains an inter-patient augmentation (InterAug) branch and an intra-patient augmentation (IntraAug) branch. Specifically, the InterAug branch generates radiology images using synthesised yet plausible reports derived from an LLM. IntraAug branch uses newly generated reports to manipulate images. This process facilitates the generation of new paired data for each individual with diverse medical conditions, where ChatGPT is used to report modification."}, {"title": "4.5 Downstream Applications", "content": "Based on pre-trained CFMs, many medical applications can be achieved, varying from uni-modal to cross-modal tasks.\n*   Uni-modal Tasks. The jointly pre-trained image encoder and text encoder can be individually or jointly used for many uni-modal tasks, such as image classification, semantic segmentation, and object detection. Medical image classification is usually to detect diseases in the image, based on GCL pre-training, CFMs can do zero-shot image classification by calculating the similarity between image representation and text prompts with a specific disease, e.g., this is an image of {disease} and {disease} presented in the image [101]. Also, the image encoder can be frozen and the subsequent MLP is updated to fine-tune for image classification tasks, namely linear probing. For semantic segmentation and object detection tasks, the pre-trained image encoder is initialized as the backbone encoder, followed by a trainable task-specific decoder, like U-Net [124] or ResUNet [125] and YOLOv3 [126] for these two tasks, respectively.\n*   Cross-modal Tasks. Pre-trained CFMs can also be used for many cross-modal tasks, including VQA, RG, ITR, TIR, and visual grounding. As the CFMs generally have no ability for text generation, so its application for VQA mainly focuses on the classification setting, using VQA-RAD and SLAKE datasets [99], [110]. The VQA model is usually initialized with a pre-trained CFM encoder and incorporates an MLP or specific decoder to make predictions. For report generation, BioViL-T processes the prior report and both images (prior and current) with an encoder and an additional decoder is utilized for generation, where two broad categories, i.e., nearest-neighbour and auto-regressive can be used. Retrieval-based tasks (ITR, TIR and disease retrieval) are directly realized by calculating similarities between the two modalities' representations. Similarly, phase grounding calculates the similarity of representations between the text phase and the local image patch [72], [106]."}, {"title": "5 MULTIMODAL LLMS (MLLMS)", "content": "Benefiting from the rapid development of LLMs"}]}