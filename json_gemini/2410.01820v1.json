{"title": "PixelBytes: Catching Unified Representation for Multimodal Generation", "authors": ["Fabien Furfaro"], "abstract": "This report introduces PixelBytes, a novel approach for unified multimodal representation learning. Inspired by existing sequence models such as Image Transformers, PixelCNN, and Mamba-Bytes, our method aims to capture diverse inputs in a cohesive representation, exploring the integration of different data types, particularly text, audio, and pixelated images (sprites). We conducted experiments on a specialized PixelBytes Pok\u00e9mon dataset. Initially, we investigated various model architectures, including Recurrent Neural Networks (RNNs), State Space Models (SSMs), and Attention-based models, focusing on bidirectional processing and our convolutional PxBy embedding technique. Subsequently, we evaluated models based on data reduction strategies and the effectiveness of autoregressive learning. We specifically examined Long Short-Term Memory (LSTM) networks in both predictive and autoregressive modes for our main experiments. Our findings suggest that autoregressive models outperform predictive models in this context. By adopting a flexible approach to multimodal modeling, PixelBytes contributes to the ongoing development of foundation models capable of understanding and generating multimodal data. The complete PixelBytes project, including code, models, and datasets, is available online [9].", "sections": [{"title": "1 Introduction", "content": "Recent advancements in artificial intelligence have led to increasingly generalist models, not by combining multiple specialized components (like Gato from DeepMind [23]), but by assigning simple tasks to models where emergent prop-erties-complex behaviors arising from simpler underlying rules appear. This is exemplified by generative language models such as GPT [6]. However, these models are constrained by their focus on language alone, failing to capture the full complexity of multimodal understanding [13]. To address this limitation, researchers have explored integrating Large Language Models (LLMs) with other modalities [20]. However, this approach often results in specialized model combinations without fostering new emergent properties. We propose \"PixelBytes\", a novel approach enabling unified training across modalities by representing diverse inputs in a single, cohesive format."}, {"title": "2 Exploration for a Unified Representation", "content": ""}, {"title": "2.1 Hypothesis Testing Framework", "content": "The quest for a unified data representation across different modalities presents significant challenges. Text data typically exhibits a one-dimensional dependency on preceding words with discrete values [3]. Audio signals have a temporal dimension with continuous values [27]. Action-state representations in robotics can be similar to audio but with correlations between channels [4]. Images and animations combine spatial and temporal dimensions across discrete RGB channels [14]. Given these diverse characteristics, many researchers have focused on combining separate embedding models into a single framework, as seen in projects like ImageBind [10] and RT-2 [36], rather than seeking a truly unified data representation. However, to build a model that comprehensively understands different modalities without intermediate alignment steps, exploring a unified data representation becomes necessary. In this section, we will examine several hypotheses:\n\u2022 Can we quantize data such that each element becomes a token? [34]\n\u2022 Is predicting only the next value sufficient for a sequence model to learn effectively?\n\u2022 For dimensions higher than one, is applying a convolutional filter necessary?\n\u2022 For space-time dependency, what is the importance of bidirectionality in models?\nThrough these investigations, we aim to explore a method of representing data that could enable models to understand various types of modalities."}, {"title": "2.2 Conceptual Multimodal Embedding", "content": ""}, {"title": "2.2.1 Dataset Construction", "content": "To evaluate our hypotheses on unified representation, we required a dataset combining visual and textual data suitable for byte-level processing. Image captioning datasets proved inadequate due to limited text content and challenges in interpreting pixelated versions of high-resolution images. Consequently, we created a specialized Pok\u00e9mon dataset, offering pixelated designs and rich descriptive text. Data was collected by web scraping Pok\u00e9mon miniatures and descriptions from Pokepedia using Beautiful Soup [24], maintaining a 2/3 text to 1/3 image ratio. For image processing, we utilized a 55-color palette inspired by the NES, creating tokens for various color combinations. This approach enabled us to represent visual information in a format compatible with our tokenizer.\nTo manage transitions between text and image tokens, we developed a 2D input sequence method utilizing a 3x3 context window around each token with a 2D zigzag scheme (Figure 1). Special tokens denote transitions between text and images, with padding added to maintain consistent context sizes. The padding value is 0. For text, only preceding tokens are included in the context windows. Employing OpenCV and scikit-image [5, 29] for image quantization and pixelization, we adjusted all entries to have 113 indices, balancing text and image tokens. The resulting dataset, combining text and pixelated images, is available on the Hugging Face Datasets Hub [9] for reproducibility. It includes a \"pixelbyte\" column for this specific data representation."}, {"title": "2.2.2 Embedding Techniques", "content": "Our exploration of unified representation begins with integrating image-text data for sequence generation. We devel-oped a tokenizer that processes the \"pixelbyte\" column. This is paired with an embedding technique called PxByEm-bed, which creates a unified representation for pixel and byte data in a single space. PxByEmbed is designed to test our hypotheses about the effectiveness of convolutional filters for higher-dimensional data. It uses a learned embedding matrix to map each token (text or image) to a vector space, while maintaining spatial relationships for image tokens. PxByEmbed incorporates a simple convolutional layer and an adaptive mixing mechanism. This design allows us to investigate whether predicting only the next value (with or without only previous value) is sufficient for effective learning in a sequence model."}, {"title": "2.3 Model Architectures Evaluated", "content": "We evaluated three compact model architectures: a Recurrent Neural Network (RNN) using Long Short-Term Memory (LSTM) units [25], a Transformer [30], and a State Space Model (SSM) based on Mamba [11]. For both the RNN and Mamba architectures, we compared variants with and without a bidirectional first layer. Each model was constrained to fewer than 100,000 parameters and adapted to process our dataset of pixel data and bytecode sequences. The models were trained on Kaggle using T4 GPUs, with a batch size of 32, sequence length of 256, and learning rate of 0.001 for 200 epochs. The trained models are available on the Hugging Face Model Hub [9]."}, {"title": "2.4 Comparative Analysis", "content": "The strong performance of the SSM supports the potential of unified representation for pixel and byte data. However, its tendency to overfit suggests that predicting only the next value might not be sufficient for effective learning in all cases. The balanced performance of the RNN indicates that simpler architectures can still be effective for our task. The Transformer's lower performance suggests that complex attention mechanisms may not always be necessary or beneficial for this type of data."}, {"title": "2.4.1 Generation Evaluation Metrics", "content": "To assess the effectiveness of our approach and various model architectures, we evaluated their generation capabilities. We tested State Space Models (SSM), Attention models (Att), and Recurrent Neural Networks (RNN) in generating 32 consecutive sequences. Our evaluation used three metrics: Hamming Distance [12], Cosine Similarity, and BLEU Score [21]."}, {"title": "2.5 Identified Challenges", "content": "Our initial results revealed several limitations in our embedding approach. While we observed variations in performance across different model configurations, the differences were often not substantial [33]. The PxBy embedding, which we initially considered promising, did not consistently outperform simpler approaches across all metrics and model types. Based on these findings, we recognized the need to refine our approach. The repetition of sequences in our generated output indicated that our embedding method might not be capturing the full range of patterns in our data [2]."}, {"title": "3 Optimizing Unified Representation", "content": ""}, {"title": "3.1 Refined Embedding Approach", "content": "To address the challenges identified in our initial experiments, we propose a revised embedding strategy. Instead of using a convolutional approach, we now focus on six specific positions within each token, with the input dimension equal to the output dimension. This adjustment allows for a larger embedding size while potentially enhancing the model's ability to capture relevant patterns. We also recognized the need for a more flexible tokenizer [15]. Our initial implementation proved cumbersome when generating new data, highlighting the importance of a more versatile approach. We are exploring methods to integrate all necessary functionality into the tokenizer itself, which should streamline our overall pipeline and potentially improve performance. These refinements reflect a shift from our initial exploration towards a more focused approach to unified representation. While our initial results provided valuable insights, they also revealed the complexities inherent in multimodal sequence modeling and the need for continuous iteration in our methods [16]."}, {"title": "3.1.1 Dataset Construction", "content": "For our refined approach, we developed a new dataset combining images, text, and audio extracted from Pokemon sprite animations. This dataset, available on the Hugging Face Dataset Hub [9], was compiled through web scraping of Pokepedia and includes descriptions of the Pokemon along with their associated cries. The dataset comprises animated, pixelated GIFs of Pokemon sprites as the visual component. The audio files are two-channel recordings: Channel 1 contains the original mono sound of the Pokemon cry, while Channel 2 features a filtered version simulating a bits Game Boy speaker output to verify our approach for control problems. This setup enables us to model a simplified dynamic physical system, where the original sound acts as the \"action\" input and the filtered output represents the \"state\" of the system. The transfer function of this bandpass filter can be approximated as:\n$H(s) = \\frac{K \\omega_n^2}{s^2 + 2 \\zeta \\omega_n s + \\omega_n^2}$        (1)\nwhere K is the gain, \u03c9n is the natural frequency, and \u03b6 is the damping ratio. These parameters can be adjusted to closely match the frequency response of a Game Boy speaker."}, {"title": "3.2 Enhanced Tokenization Strategy", "content": "Building on our previous work, we developed an improved tokenization strategy using the ActionPixelBytesTokenizer. This tokenizer addresses multimodal data more effectively, including text, images, and audio, while maintaining a unified representation. It employs a combined vocabulary that includes ASCII bytes, RGB values from the NES palette, and action states for control and audio. This approach aims to create a consistent representation across different data types. For text processing, the tokenizer converts to lowercase ASCII bytes. Images are converted to Lab color space and quantized to the nearest NES palette color. Audio data is normalized and mapped to predefined action states, with the setpoint reset to zero (standard equilibrium). The token vocabulary now comprises 151 tokens, where index 0 corresponds to the null padding value, and indices 1 and 2 are transition values. A key aspect of the new tokenizer is its sequence construction method. Instead of using convolutional methods, we focus on six specific positions for each token to avoid repetition in the sequencing of a 3D zigzag scheme (Figure 1). This approach creates context-target pairs that aim to capture relationships between neighboring tokens in both space and time. The sequence construction algorithm is detailed below:"}, {"title": "3.3 Autoregressive Model Architecture", "content": "Building upon our initial approach findings, we developed the aPxBySequenceModel architecture. This architecture is designed to handle both predictive and autoregressive tasks using a Long Short-Term Memory (LSTM) network. The model comprises three main components: an embedding layer, an LSTM sequence model, and a fully connected output layer. The embedding layer maps input tokens to a continuous vector space, with the embedding size calculated based on the input dimension. Specifically, the embedding size is determined by dividing the overall embedding size by the number of positions we focus on within each token. This approach aligns with our revised embedding strategy, which emphasizes six specific positions within each token (with padding 0 to avoid influencing training). The LSTM layer aims to capture temporal dependencies in the sequence data, potentially enhancing pattern recognition across different modalities. The model operates in two distinct modes:\n\u2022 Predictive mode: In this configuration, the model takes six input values and attempts to predict only the next token.\n\u2022 Autoregressive mode: Here, the model's output dimension matches the input dimension (Figure 1). Additionally, the output is restructured by multiplying it with the vocabulary size, enabling the model to generate sequences based on the learned representations.\nDuring the forward pass, input data is processed through the embedding layer, then through the LSTM layers, and finally through the fully connected layer. The output shape is adjusted based on the operating mode, which may provide the flexibility we found lacking in our initial implementation."}, {"title": "3.3.1 Model Training and Data Management", "content": "For data management, we developed the TokenPxByDataset class to handle multimodal inputs, including text, image, and audio data. This class generates overlapping sequences from longer inputs, facilitating the model's capture of context across sequence boundaries. It optimizes memory usage through on-the-fly data retrieval, preparing samples only as needed. The class ensures consistent sequence lengths by implementing circular padding for sequences extending beyond an item's end. These features enable efficient processing of variable-length inputs during training.\nOur training process incorporates several enhancements for efficiency and monitoring. In the process_epoch function, we manage both autoregressive and non-autoregressive modes. For autoregressive mode, we reshape input and output sequences, using the input sequence as the target. In non-autoregressive mode, we flatten the outputs and use provided labels as the target. This flexibility allows the model to adapt to different tasks. The train_model function alternates between training and validation phases, enabling regular performance evaluation. We employ gradient accumulation to simulate larger batch sizes, beneficial when GPU memory is limited. The training loop tracks both training and validation metrics (loss and accuracy) for each epoch, saving these metrics to a CSV file. We also implement model checkpointing to retain the best model based on validation loss."}, {"title": "3.4 Performance Evaluation", "content": "We evaluated three LSTM models: one in predictive mode and two in autoregressive mode, each with approximately 4 million parameters. The models were trained on Kaggle using T4 GPUs. We utilized an embedding size of 128, a hidden size of 512, and two layers. Training was conducted for 100 epochs with a batch size of 32, a learning rate of 0.001, and a sequence length of 1024."}, {"title": "3.4.1 Results Comparison", "content": "To manage data proportions, we applied different reduction strategies for image and audio data. This was particularly important for audio data in the autoregressive mode, as it contains more null values to predict, which could potentially impact training."}, {"title": "4 Discussion and Future Directions", "content": "Our experiments with various model architectures for unified text and image generation have yielded unexpected insights and prompted a shift in our approach. Initially, we explored bidirectional RNN models using PixelBytes (PxBy) embedding with convolutional layers, anticipating improved multimodal data representation. However, deeper analysis revealed limitations in this approach, leading us to reconsider our tokenizer design.\nOur subsequent results with LSTM models have been particularly informative. The autoregressive models signifi-cantly outperformed the predictive model, suggesting that maintaining equal input and output dimensions is crucial for our task. This aligns with recent research emphasizing the importance of preserving structural information in multimodal embeddings [31]. The performance difference between the two autoregressive models highlights the im-pact of data balancing strategies. The model with balanced reduction (2,2) for audio and image data showed slightly better results, indicating potential overfitting with animated images. We now aim for a more versatile solution that can handle all aspects of data preparation and support true autoregressive modeling. Our TokenPxByDataset class remains valuable, but we are working to integrate its functionality more closely with our revised tokenizer for a more streamlined data pipeline. The predictive model requires existing spatio-temporal data, while the autoregressive model regenerates data that it could access at the previous time step, without the need for regeneration.\nWhile we initially explored both RNN and State Space Models (SSM), with SSMs showing promising rapid conver-gence [7], our focus has shifted towards simplifying the overall architecture. Our approach offers an alternative to the principles of models like ImageBind [10], as our preliminary results suggest it may be possible to unify modalities with-out relying on intermediate representations. We now recognize the potential benefits of allowing emergent properties to develop within a simplified framework. Moving forward, we will refine our strategy to better utilize specific input positions for high-definition image and sound. We also aim to further simplify our model architecture to promote the emergence of natural multimodal representations. Additionally, we plan to explore alternative approaches like Diffusion-LM [17], which may offer new perspectives on multimodal sequence modeling. While our work is ongoing, it aligns with recent trends in multimodal AI research [1] and could potentially provide a versatile foundation for various multimodal tasks."}]}