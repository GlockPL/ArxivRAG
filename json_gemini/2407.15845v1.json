{"title": "Reconstructing Training Data From Real-World Models Trained with Transfer Learning", "authors": ["Yakir Oz", "Gilad Yehudai", "Gal Vardi", "Itai Antebi", "Michal Irani", "Niv Haim"], "abstract": "Current methods for reconstructing training data from trained classifiers are restricted to very small models, limited training set sizes, and low-resolution images. Such restrictions hinder their applicability to real-world scenarios. In this paper, we present a novel approach enabling data reconstruction in realistic settings for models trained on high-resolution images. Our method adapts the reconstruction scheme of Haim et al. [2022] to real-world scenarios \u2013 specifically, targeting models trained via transfer learning over image embeddings of large pre-trained models like DINO-ViT and CLIP. Our work employs data reconstruction in the embedding space rather than in the image space, showcasing its applicability beyond visual data. Moreover, we introduce a novel clustering-based method to identify good reconstructions from thousands of candidates. This significantly improves on previous works that relied on knowledge of the training set to identify good reconstructed images. Our findings shed light on a potential privacy risk for data leakage from models trained using transfer learning.", "sections": [{"title": "1 Introduction", "content": "Understanding when training data can be reconstructed from trained neural networks is an intriguing question that attracted significant interest in recent years. Successful reconstruction of training sam- ples has been demonstrated for both generative models [Carlini et al., 2021, 2023] and classification settings [Haim et al., 2022]. Exploring this question may help understand the extent to which neural networks memorize training data and their vulnerability to privacy attacks and data leakage.\nExisting results on training data reconstruction from neural network classifiers focus on restricted and unrealistic settings. These methods require very small training datasets, which strongly limit their ability to generalize. Additionally, they are constrained to low-resolution images, such as CIFAR or MNIST images, and simple models like multilayered perceptrons (MLPs) or small CNNs.\nWe aim to overcome these limitations in a transfer-learning setting. Transfer Learning leverages knowledge gained from solving one problem to address a related problem, often by transferring learned representations from large pre-trained models (known as Foundation Models) to tasks with limited training data. In the context of deep learning, transfer learning is commonly implemented by fine-tuning the final layers of pre-trained models or training small MLPs on their output embeddings, known as deep features [Oquab et al., 2014]. This approach often achieves high generalization even for learning tasks with small training sets, while also requiring less computing power. Thus, transfer learning is very common in practice.\nIn this work, we demonstrate reconstruction of training samples in more realistic scenarios. Specifically, we reconstruct high-resolution images from models that achieve good test performance, within a transfer learning framework. Our approach involves training an MLP on the embeddings of common pre-trained transformer-based foundation models, such as CLIP [Radford et al., 2021] or DINO-VIT [Caron et al., 2021]. Our findings have implications for privacy, particularly when transfer learning is being used on sensitive training data, such as medical data. Consequently, preventing data leakage in transfer learning necessitates the development of appropriate defenses."}, {"title": "2 Prior Work", "content": "Data Reconstruction Attacks. Reconstruction attacks attempt to recover the data samples on which a model is trained, posing a serious threat to privacy. Earlier examples of such attacks include activation maximization (model-inversion) [Fredrikson et al., 2015, Yang et al., 2019], although they are limited to only a few training samples in each class. Reconstruction in a federated learning setup [Zhu et al., 2019, He et al., 2019, Hitaj et al., 2017, Geiping et al., 2020, Huang et al., 2021, Wen et al., 2022] where the attacker assumes knowledge of samples' gradients. Other works studied reconstruction attacks on generative models like LLMs [Carlini et al., 2019, 2021, Nasr et al., 2023] and diffusion-based image generators [Somepalli et al., 2022, Carlini et al., 2023]. Our work is based on the reconstruction method from Haim et al. [2022], which relies only on knowledge of the parameters of the trained model, and is based on theoretical results of the implicit bias in neural networks [Lyu and Li, 2019, Ji and Telgarsky, 2020]. This work was generalized to multi-class setting [Buzaglo et al., 2023] and to the NTK regime [Loo et al., 2023].\nTransfer Learning. Deep transfer learning, a common technique across various tasks (see surveys: [Tan et al., 2018, Zhuang et al., 2020, Iman et al., 2023]), leverages pre-trained models from large datasets to address challenges faced by smaller, domain-specific datasets (e.g., in the medical do- main [Kim et al., 2022]). While convolutional neural networks (CNNs) have been the go-to approach for transfer learning [Oquab et al., 2014, Yosinski et al., 2014], recent research suggests that vision transformers (ViTs) may offer stronger learned representations for downstream tasks [Caron et al., 2021, He et al., 2022]. For example, ViT [Dosovitskiy et al., 2020], pre-trained on ImageNet [Deng et al., 2009], provides robust general visual features. Beyond supervised pre-training, self-supervised learning methods like DINO [Caron et al., 2021, Oquab et al., 2023] learn informative image rep- resentations without requiring labeled data, allowing the model to capture strong image features suitable for further downstream tasks. Additionally, CLIP [Radford et al., 2021] has emerged as a powerful technique, leveraging a massive dataset of paired text-image examples and contrastive loss to learn semantically meaningful image representations."}, {"title": "3 Method", "content": "Our goal is to reconstruct training samples (images) from a classifier that was trained on the corre- sponding embedding vectors of a large pre-trained model in a transfer learning manner.\nThe classifier training is illustrated in Fig. 2a. Formally, given an image classification task $D_s = \\{(s_i, y_i)\\}_{i=1}^n \\subseteq \\mathbb{R}^{d_s} \\times \\{1, ..., C\\}$, where $d_s$ is the dimension of the input image and $C$ is the number of classes, we employ a large pre-trained model $F : \\mathbb{R}^{d_s} \\rightarrow \\mathbb{R}^d$ (e.g., DINO) to transfer each image $s_i$ to its corresponding deep feature embedding $x_i = F(s_i) \\in \\mathbb{R}^d$, where $d$ is the dimension of the feature embedding vector (the output of $F$). We then train a model $\\phi(\\cdot, \\theta) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^C$ to classify the embedding dataset $D_x = \\{(x_i, y_i)\\}_{i=1}^n \\subseteq \\mathbb{R}^d \\times \\{1, ..., C\\}$, where $\\theta \\in \\mathbb{R}^p$ is a vectorization of the trained parameters. Typically, $\\phi$ is a single hidden-layer multilayer perceptron (MLP). Also note that $F$ is kept fixed during the training of $\\phi$. The overall trained image classifier is $\\phi(F(s))$.\nOur reconstruction approach is illustrated in Fig. 2b and presented in detail below. Given the trained classifier $\\phi$ and the pre-trained model $F$, our goal is to reconstruct training samples $s_i$ from the training set $D_s$. The reconstruction scheme comprises two parts:\n1.  Reconstructing embedding vectors from the training set of the classifier $\\phi$.\n2.  Mapping the reconstructed embedding vectors back into the image domain. Namely, computing $F^{-1}$ (e.g., by \u201cinverting\u201d the pre-trained model $F$)."}, {"title": "3.1 Reconstructing Embedding Vectors from $\\phi$", "content": "Given a classifier $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}^C$ trained on an embedding training-set $D_x = \\{(x_i, y_i)\\}_{i=1}^m$, we apply the reconstruction scheme of [Haim et al., 2022, Buzaglo et al., 2023] to obtain $\\{\\hat{x}_i\\}_{i=1}^m$, which are $m$ \u201ccandidates\u201d for reconstructed samples from the original training set $D_s$. In this section we provide a brief overview of the reconstruction scheme of [Haim et al., 2022, Buzaglo et al., 2023] (for elaboration see Sec. 3 in Haim et al. [2022]):\nImplicit Bias of Gradient Flow: Lyu and Li [2019], Ji and Telgarsky [2020] show that given a homogeneous neural network $\\phi(\\cdot, \\theta)$ trained using gradient flow with a binary cross-entropy loss on a binary classification dataset $\\{(x_i, y_i)\\}_{i=1}^n \\subseteq \\mathbb{R}^d \\times \\{\\pm 1\\}$, its parameters $\\theta$ converge to a KKT point of the maximum margin problem. In particular, there exist $\\lambda_i \\ge 0$ for every $i \\in [n]$ such that the parameters of the trained network $\\theta$ satisfy the following equation:\n$\\theta = \\sum_{i=1}^{n} \\lambda_i y_i \\nabla_{\\theta} \\phi(x_i, \\theta).$     (1)\nData Reconstruction Scheme: Given such a trained model $\\phi$ with trained (and fixed) parameters $\\theta$, the crux of the reconstruction scheme is to find a set of $\\{x_i, \\lambda_i, y_i\\}$ that satisfy Eq. (1). This is done by minimizing the following loss function:\n$L_{rec} (\\lambda_1,..., \\lambda_m, x_1,..., x_m) :=  \\| \\theta - \\sum_{i=1}^{m} \\lambda_i y_i \\nabla_{\\theta} \\phi(\\hat{x}_i, \\theta)\\|^2$\n(2)\nWhere the optimization variables $\\{\\hat{x}_i, \\lambda_i\\}$ are initialized at random from $\\lambda_i \\sim U(0, 1)$ and $\\hat{x}_i \\sim \\mathcal{N}(0, \\sigma)$ ($\\sigma$ is a hyperparameter). This generates $m$ vectors $\\{\\hat{x}_i\\}_{i=1}^m$ that we consider as \u201ccandidates\u201d for reconstructed samples from the original training set of the classifier $\\phi$. The number of candidates $m$ should be \u201clarge enough\u201d (e.g., $m \\ge 2n$, and see discussion in Haim et al. [2022]). The $y_i$ are assigned in a balanced manner (i.e., $y_1, \\dots, y_{m/2} = 1$ and $y_{1+m/2}, \\dots, y_m = -1$). Lastly, Buzaglo et al. [2023] extended this scheme to multi-class classification problems.\nThe data reconstruction scheme is conducted multiple times for different choices of hyperparameters (e.g., learning rate and $\\sigma$). For each trained model, we run about 50-100 reconstruction runs with $m = 500$, resulting in about 25k-50k candidates. See Appendix B.2 for full details."}, {"title": "3.2 Mapping Embedding Vectors $\\hat{x}_i$ to the Image Domain $\\hat{s}_i$", "content": "Unlike previous works on data reconstruction that directly reconstruct training images, our method reconstructs embedding vectors. To evaluate the effectiveness of our reconstructed candidates, we must first map them back to the image domain. In this section we describe how we achieve training images from image-embeddings. Namely, given reconstructed image-embeddings $\\hat{x}_i$, our goal is to compute $\\hat{s}_i = F^{-1}(\\hat{x}_i)$. To this end we apply model-inversion methods and in particular, the method proposed in Tumanyan et al. [2022].\nGiven a vector $\\hat{x}_i$ (an output candidate from the reconstruction optimization in Section 3.1), we search for an input image $\\hat{s}_i$ to $F$ that maximizes the cosine-similarity between $F(\\hat{s}_i)$ and $\\hat{x}_i$. Formally:\n$\\hat{s}_i = F^{-1}(\\hat{x}_i) = \\underset{v}{argmax} \\frac{F(v) \\cdot \\hat{x}_i}{\\|F(v)\\| \\| \\hat{x}_i\\|}$\n(3)\nWe further apply a Deep-Image Prior (DIP) [Ulyanov et al., 2018] to the input of $F$. I.e., $v = g(z)$ where $g$ is a CNN U-Net model applied to a random input $z$ sampled from Gaussian distribution. The only optimization variables of the inversion method are the parameters of $g$. See Appendix B.3 further explanation and full implementation details.\nBy applying model-inversion to DINO embeddings, Tumanyan et al. [2022] demonstrated that the [CLS] token contains a significant amount of information about the visual appearance of the original image from which it was computed. Even though their work was done in the context of image to image style transfer, their results inspired our work and motivated us to apply their approach in the context of reconstructing training image samples.\nA significant modification to Tumanyan et al. [2022] in our work is by employing a cosine- similarity loss instead of their proposed MSE loss. We find that using MSE loss (i.e., $F^{-1}(\\hat{x}_i) = \\underset{v}{argmin} \\|F(v) - \\hat{x}_i\\|^2$) is highly sensitive to even small changes in the scale of $\\hat{x}$. The scales of $\\hat{x}$ can be very different from the unknown $x = F(s)$. Using cosine similarity alleviates this issue while simultaneously achieving similar quality for the inverted image result (see also Appendix A.1).\nThe above-mentioned technique is used for mapping embeddings to images for most transformers that we consider in our work. However, this technique did not produce good results when applied to CLIP [Radford et al., 2021]. Therefore, to map CLIP image embeddings to the image domain, we employ a diffusion-based generator conditioned on CLIP embeddings by Lee et al. [2022] (similar in spirit to the more popular DALL-E2 [Ramesh et al., 2022]; see also Appendix A.6 and Appendix B.4)."}, {"title": "3.3 Selecting Reconstructed Embeddings to be Inverted", "content": "Applying the model-inversion described in Section 3.2 to a large pretrained model is computationally intensive. Inverting a single embedding vector takes about 30 minutes on an NVIDIA-V100-32GB GPU. Therefore, it is not feasible to invert all 25k-50k output candidates of Section 3.1.\nTo determine which reconstructed candidates to invert, we pair each training embedding $x_i$ with its nearest reconstructed candidate $\\hat{x}_i$ (measured by cosine similarity) and select the top 40 vectors with the highest similarity for inversion. This approach proves effective in practice, yielding images with high visual similarity to the original training images, as demonstrated in the results (e.g., Fig. 1).\nIn practice, the original training embeddings are not available (and inverting all candidates is compu- tationally prohibitive). In Section 5 we introduce a novel method to identify good reconstructions without relying on either ground-truth embeddings or exhaustive inversion."}, {"title": "4 Results", "content": "We demonstrate reconstructed training images from models trained in a transfer learning setup, on the embeddings of large pretrained models. We train several MLPs to solve learning tasks for various choices of training images and choices of the large pretrained backbones from which the image embeddings are computed.\nDatasets. Since we simulate a model that is trained in a transfer learning manner, it is reasonable to assume that such tasks involve images that were not necessarily included in the training sets on which the pretrained backbone was trained (typically, ImageNet [Deng et al., 2009]). In our experiments we use images from Food-101 [Bossard et al., 2014] (most popular dishes from foodspotting website) and iNaturalist [Van Horn et al., 2018] (various animals/plants species) datasets. The resolution of the images vary between 250-500 pixels, but resized and center-cropped to 224 \u00d7 224.\nPretrained Backbones (F) for Image Embeddings. We select several Transformer-based founda- tion models that are popular choices for transfer learning in the visual domain:\n\u2022  ViT [Dosovitskiy et al., 2020]: vit-base-patch16-224 from TIMM Wightman [2019].\n\u2022  DINO-ViT [Caron et al., 2021]: dino-vitb16 from the official implementation.\n\u2022  DINOv2 [Oquab et al., 2023]: dinov2-vitb14-reg from the official implementation.\n\u2022  CLIP-ViT [Radford et al., 2021]: ViT-L/14 as provided by OpenAI's CLIP repository.\nThe dimension of the output embeddings is consistent across all backbones $F$, and equal to $d=768$.\nMultilayer Perceptron ($\\phi$) consists of a single hidden layer of dimension 500 (d-500-C) that is optimized with gradient descent for 10k epochs, weight-decay of 0.08 or 0.16 and learning rate 0.01. All models achieve zero training error.\nReconstructing Training Data from $\\phi(F)$We train classifiers $\\phi(F(s))$ on two binary classification tasks: (1) binary iNaturalist is fauna (bugs/snails/birds/alligators) vs. flora (fungi/flowers/trees/bush) and (2) binary Food101 is beef- carpaccio/bruschetta/caesar-salad/churros/cup-cakes vs. edamame/gnocchi/paella/pizza/tacos. Each binary class mixes images from several classes of the original dataset (images are not mixed between different datasets). Each training set contains 100 images (50 per class). The test sets contains 1000/1687 images for iNaturalist/Food101 respectively. All models achieve test-accuracy above 95% (except for DINO on Food101 with 85% and see Fig. 28)."}, {"title": "5 Identifying Good Reconstruction Without the Original Trainset", "content": "In this section, we introduce a clustering-based approach to identify \"good\" reconstructed candidates without relying on the original training data. This is an important step towards an effective privacy attack. Previous works [Haim et al., 2022, Buzaglo et al., 2023, Loo et al., 2023], including Section 3.3 in this work, rely on the original training images for demonstrating that training images are embedded\nDetermining that a reconstructed candidate is indeed the reconstruction of a specific training image is equivalent to finding a good visual metric between images. See discussion in Sec.3 in Buzaglo et al. [2023]."}, {"title": "6 Limitations", "content": "In this work, we made design choices when training the models to align with realistic transfer learning practices. However, some choices led to better reconstruction results than others, revealing limitations of our method. Here we discuss these limitations, their impact on our results, and identify potential future research directions:\n\u2022  The quality of reconstructed images relies heavily on the backbone model (F) and the inversion method (Section 3.2). Fig. 8 shows the inverted original embeddings $F^{-1}(F(s))$ (blue), which are the \"best\" we can achieve (independent of our reconstruction method). It also shows how some backbones are easier or harder to invert, as evident in the difference between $F^{-1}(F(s))$ (blue) and the original image s (red), for different F's. It can also be seen that the inverted reconstructed embeddings $F^{-1}(\\hat{x})$ are sometimes more similar to $F^{-1}(F(s))$ than to s, which may hint that the challenge lies in the inversion more than in the reconstruction part. Certainly, improving model inversion techniques is likely to enhance the quality of reconstructed samples."}, {"title": "7 Conclusion", "content": "In this work, we extend previous data reconstruction methods to more realistic transfer learning scenarios. We demonstrate that certain models trained with transfer learning are susceptible to training set reconstruction attack. Given the widespread adoption of transfer learning, our results highlight potential privacy risks. By examining the limitations of our approach, we identify simple mitigation strategies, such as employing smaller or even linear models, increasing training set size or training without weight-decay regularization. However, some of these mitigation (removing regularization or using smaller models) may also come at a cost to the generalization of the model. Furthermore, these techniques may not be effective against future advanced reconstruction attacks. We aim for our work to inspire the development of new defense methods and emphasize the importance of research on data reconstruction attacks and defenses."}, {"title": "B Implementation Details", "content": "Our code is implemented with PYTORCH framework.\nB.1 Data Preprocessing\nWe resize each image to a resolution of 224 pixels (the smaller side of the image) and then apply a center crop to obtain a 224 \u00d7 224 image. We then normalize the image per pixel following the normalization used in the original paper of each model, as shown in the table below:\nAfter feeding the images through the backbone $F$ to obtain the image embeddings $F(s_i)$, we normalize each embedding by subtracting the mean-embedding and dividing by the std. Formally: $x_i = \\frac{F(s_i) - \\mu}{\\sigma}$, where $\\mu = \\frac{1}{n} \\sum_{i=1}^{n} F(s_i)$, and $\\sigma =  \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (F(s_i) - \\mu)^2}$.\nThis is a fairly common approach when training on small datasets. $\\mu$ and $\\sigma$ can be thought as being part of the model $\\phi$ as they are also applied for embeddings from outside the training set.\nB.2 Reconstruction Hyperparameter Search\nAs mentioned in Section 3.1, we run the reconstruction optimization 100 times with different choice of the 4 hyperparameters of the reconstruction algorithm:\n1.  Learning rate\n2.  $\\sigma$ \u2013 the initial s.t.d. of the initialization of the candidates\n3.  $\\lambda_{min}$ - together with the loss Eq. (2), the reconstruction includes another loss term to require $\\lambda_i > \\lambda_{min}$ (a consequence of the KKT conditions is that $\\lambda_i > 0$, but if $\\lambda_i = 0$ it has no relevance in the overall results, therefore a minimal value $\\lambda_{min}$ is set.).\n4.  $\\alpha$ - Since the derivative of ReLU is piecewise constant and non-continuous, the backward function in each ReLU layer in the original model is replaced with the derivative of SoftRelu with parameter $\\alpha$.\nFor full explanation of the hyperparameters, please refer to Haim et al. [2022]. Note that for m = 500, running 100 times would result in 50k candidates."}, {"title": "B.3 Further Details about Inversion Section 3.2", "content": "We follow similar methodology to Tumanyan et al. [2022], using their code and changing the recon- struction loss from MSE to Cosine-Similarity as mentioned in Section 3.2, and specifically Eq. (3) (see justifications in Appendix A.1).\nThe Deep-Image Prior model g is a fully convolutional U-Net model [Ronneberger et al., 2015] (initialized at random with the default pytorch implementation). The optimization is run for 20,000 iterations, where at each iteration the input to g is $z+r$, where z is initialized from $z ~ N(0_{d_s}, I_{d_s \\times d_s})$ and kept fixed throughout the optimization, and r is sampled at each iteration as follows:\nNote that the input to g is of the same size of the input to $F$, which is simply and image of dimensions $d_s = c \u00d7 h \u00d7 w$. At each iteration, the output of g is fed to $F$, and the output of $F$ (which is an embedding vector of dimension d = 768), is compared using cosine-similarity to the embedding vector that we want to invert. At the end of the step, the parameters of g are changed to increase the cosine similarity between the embeddings.\nB.4 Inversion with UnCLIP\nWhile the method in Appendix B.3 is used for ViT, DINO and DINOv2, for CLIP we use a different method to invert, which is by using the UnCLIP implementation of Lee et al. [2022]. Unlike the inversion in Appendix B.3 that uses cosine-similarity, with UnCLIP, the embeddings (that go into to UnCLIP decoder) should have the right scale. For each CLIP embedding of a training image (x), we search for its nearest neighbour candidate (x) with cosine similarity, but before feeding x into the UnCLIP decoder, we re-scale it to have the same scale as x, so that the input to the decoder is in fact $\\frac{\\|x\\|}{\\|\\hat{x}\\|} \\hat{x}$. Unfortunately we could not resolve this reliance on the training set (as is also done in previous reconstruction works, and discussed in the main paper), but we believe this may be mitigated by computing and using general statistics of the training set (instead of specific training samples). We leave this direction for future research.\nB.5 Reconstruction in Multiclass Setup\nThe method in Section 3.1 was extended to multiclass settings by Buzaglo et al. [2023]. In a nutshell, the reconstruction loss in Eq. (2) contains the gradient (w.r.t. $\\theta$) of $\\phi(\\hat{x}_i)_{y_i}$ which is the distance from the decision boundary. For multiclass model $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}^C$, the distance to the decision boundary is $\\phi(\\hat{x}_i)_{y_i} - \\underset{j \\neq y_i}{max} \\phi(\\hat{x}_i)_j$. Replaced into the reconstruction loss in Eq. (2), we have:\n$L_{rec} (\\lambda_1,..., \\lambda_m, \\hat{x}_1,..., \\hat{x}_m) :=  \\| \\theta - \\sum_{i=1}^{m} \\lambda_i \\nabla_{\\theta} (\\phi(\\hat{x}_i, \\theta)_{y_i} - \\underset{j \\neq y_i}{max} \\phi(\\hat{x}_i)_j)\\|^2$\nB.6 Choice of Weight Decay\nWhen training our model, we apply weight decay regularization. However, determining the optimal weight decay (WD) value is not straightforward. To find a WD value, we conduct a search across different WD values and observe their impact on test accuracy."}]}