{"title": "Reconstructing Training Data From Real-World Models Trained with Transfer Learning", "authors": ["Yakir Oz", "Gilad Yehudai", "Gal Vardi", "Itai Antebi", "Michal Irani", "Niv Haim"], "abstract": "Current methods for reconstructing training data from trained classifiers are restricted to very small models, limited training set sizes, and low-resolution images. Such restrictions hinder their applicability to real-world scenarios. In this paper, we present a novel approach enabling data reconstruction in realistic settings for models trained on high-resolution images. Our method adapts the reconstruction scheme of Haim et al. [2022] to real-world scenarios \u2013 specifically, targeting models trained via transfer learning over image embeddings of large pre-trained models like DINO-ViT and CLIP. Our work employs data reconstruction in the embedding space rather than in the image space, showcasing its applicability beyond visual data. Moreover, we introduce a novel clustering-based method to identify good reconstructions from thousands of candidates. This significantly improves on previous works that relied on knowledge of the training set to identify good reconstructed images. Our findings shed light on a potential privacy risk for data leakage from models trained using transfer learning.", "sections": [{"title": "Introduction", "content": "Understanding when training data can be reconstructed from trained neural networks is an intriguing question that attracted significant interest in recent years. Successful reconstruction of training samples has been demonstrated for both generative models [Carlini et al., 2021, 2023] and classification settings [Haim et al., 2022]. Exploring this question may help understand the extent to which neural networks memorize training data and their vulnerability to privacy attacks and data leakage.\nExisting results on training data reconstruction from neural network classifiers focus on restricted and unrealistic settings. These methods require very small training datasets, which strongly limit their ability to generalize. Additionally, they are constrained to low-resolution images, such as CIFAR or MNIST images, and simple models like multilayered perceptrons (MLPs) or small CNNs.\nWe aim to overcome these limitations in a transfer-learning setting. Transfer Learning leverages knowledge gained from solving one problem to address a related problem, often by transferring learned representations from large pre-trained models (known as Foundation Models) to tasks with limited training data. In the context of deep learning, transfer learning is commonly implemented by fine-tuning the final layers of pre-trained models or training small MLPs on their output embeddings, known as deep features [Oquab et al., 2014]. This approach often achieves high generalization even for learning tasks with small training sets, while also requiring less computing power. Thus, transfer learning is very common in practice.\nIn this work, we demonstrate reconstruction of training samples in more realistic scenarios. Specifically, we reconstruct high-resolution images from models that achieve good test performance, within a transfer learning framework. Our approach involves training an MLP on the embeddings of common pre-trained transformer-based foundation models, such as CLIP [Radford et al., 2021] or DINO-VIT [Caron et al., 2021] (see Fig. 1). Our findings have implications for privacy, particularly when transfer learning is being used on sensitive training data, such as medical data. Consequently, preventing data leakage in transfer learning necessitates the development of appropriate defenses."}, {"title": "Prior Work", "content": "Data Reconstruction Attacks. Reconstruction attacks attempt to recover the data samples on which a model is trained, posing a serious threat to privacy. Earlier examples of such attacks include activation maximization (model-inversion) [Fredrikson et al., 2015, Yang et al., 2019], although they are limited to only a few training samples in each class. Reconstruction in a federated learning setup [Zhu et al., 2019, He et al., 2019, Hitaj et al., 2017, Geiping et al., 2020, Huang et al., 2021, Wen et al., 2022] where the attacker assumes knowledge of samples' gradients. Other works studied reconstruction attacks on generative models like LLMs [Carlini et al., 2019, 2021, Nasr et al., 2023] and diffusion-based image generators [Somepalli et al., 2022, Carlini et al., 2023]. Our work is based on the reconstruction method from Haim et al. [2022], which relies only on knowledge of the parameters of the trained model, and is based on theoretical results of the implicit bias in neural networks [Lyu and Li, 2019, Ji and Telgarsky, 2020]. This work was generalized to multi-class setting [Buzaglo et al., 2023] and to the NTK regime [Loo et al., 2023].\nTransfer Learning. Deep transfer learning, a common technique across various tasks (see surveys: [Tan et al., 2018, Zhuang et al., 2020, Iman et al., 2023]), leverages pre-trained models from large datasets to address challenges faced by smaller, domain-specific datasets (e.g., in the medical domain [Kim et al., 2022]). While convolutional neural networks (CNNs) have been the go-to approach for transfer learning [Oquab et al., 2014, Yosinski et al., 2014], recent research suggests that vision transformers (ViTs) may offer stronger learned representations for downstream tasks [Caron et al., 2021, He et al., 2022]. For example, ViT [Dosovitskiy et al., 2020], pre-trained on ImageNet [Deng et al., 2009], provides robust general visual features. Beyond supervised pre-training, self-supervised learning methods like DINO [Caron et al., 2021, Oquab et al., 2023] learn informative image representations without requiring labeled data, allowing the model to capture strong image features suitable for further downstream tasks. Additionally, CLIP [Radford et al., 2021] has emerged as a powerful technique, leveraging a massive dataset of paired text-image examples and contrastive loss to learn semantically meaningful image representations."}, {"title": "Method", "content": "Our goal is to reconstruct training samples (images) from a classifier that was trained on the corresponding embedding vectors of a large pre-trained model in a transfer learning manner.\nThe classifier training is illustrated in Fig. 2a. Formally, given an image classification task $\\mathcal{D}_s = \\{(s_i, y_i)\\}_{i=1}^n \\subseteq \\mathbb{R}^{d_s} \\times \\{1, ..., C\\}$, where $d_s$ is the dimension of the input image\u00b9 and C is the number of classes, we employ a large pre-trained model $F : \\mathbb{R}^{d_s} \\rightarrow \\mathbb{R}^d$ (e.g., DINO) to transfer each image $s_i$ to its corresponding deep feature embedding $x_i = F(s_i) \\in \\mathbb{R}^d$, where d is the dimension of the feature embedding vector (the output of F). We then train a model $\\phi(\\cdot, \\theta) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^C$ to classify the embedding dataset $\\mathcal{D}_x = \\{(x_i, y_i)\\}_{i=1}^n \\subseteq \\mathbb{R}^d \\times \\{1, ..., C\\}$, where $\\theta \\in \\mathbb{R}^p$ is a vectorization of the trained parameters. Typically, $\\phi$ is a single hidden-layer multilayer perceptron (MLP). Also note that F is kept fixed during the training of $\\phi$. The overall trained image classifier is $\\phi(F(s))$.\nOur reconstruction approach is illustrated in Fig. 2b and presented in detail below. Given the trained classifier $\\phi$ and the pre-trained model F, our goal is to reconstruct training samples $s_i$ from the training set $\\mathcal{D}_s$. The reconstruction scheme comprises two parts:\n1. Reconstructing embedding vectors from the training set of the classifier $\\phi$.\n2. Mapping the reconstructed embedding vectors back into the image domain. Namely, computing $F^{-1}$ (e.g., by \"inverting\u201d the pre-trained model F)."}, {"title": "Reconstructing Embedding Vectors from", "content": "Given a classifier $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}^C$ trained on an embedding training-set $\\mathcal{D}_x = \\{(x_i, y_i)\\}_{i=1}^m$, we apply the reconstruction scheme of [Haim et al., 2022, Buzaglo et al., 2023] to obtain $\\{\\hat{x}_i\\}_{i=1}^m$, which are m \"candidates\u201d for reconstructed samples from the original training set $\\mathcal{D}_s$. In this section we provide a brief overview of the reconstruction scheme of [Haim et al., 2022, Buzaglo et al., 2023] (for elaboration see Sec. 3 in Haim et al. [2022]):\nImplicit Bias of Gradient Flow: Lyu and Li [2019], Ji and Telgarsky [2020] show that given a homogeneous\u00b2 neural network $\\phi(\\cdot, \\theta)$ trained using gradient flow with a binary cross-entropy loss on a binary classification dataset $\\{(x_i, y_i)\\}_{i=1}^n \\subseteq \\mathbb{R}^d \\times \\{\\pm 1\\}$, its parameters $\\theta$ converge to a KKT point of the maximum margin problem. In particular, there exist $\\lambda_i \\geq 0$ for every $i \\in [n]$ such that the parameters of the trained network $\\theta$ satisfy the following equation:\n$\\theta = \\sum_{i=1}^n \\lambda_i y_i \\nabla_\\theta \\phi(x_i, \\theta) .$"}, {"title": "Mapping Embedding Vectors $x_i$ to the Image Domain $\\hat{s}_i$", "content": "Unlike previous works on data reconstruction that directly reconstruct training images, our method reconstructs embedding vectors. To evaluate the effectiveness of our reconstructed candidates, we must first map them back to the image domain. In this section we describe how we achieve training images from image-embeddings. Namely, given reconstructed image-embeddings $\\hat{x}_i$, our goal is to compute $\\hat{s}_i = F^{-1}(\\hat{x}_i)$. To this end we apply model-inversion methods and in particular, the method proposed in Tumanyan et al. [2022].\nGiven a vector $\\hat{x}_i$ (an output candidate from the reconstruction optimization in Section 3.1), we search for an input image $\\hat{s}_i$ to F that maximizes the cosine-similarity between $F(\\hat{s}_i)$ and $\\hat{x}_i$. Formally:\n$\\hat{s}_i = F^{-1}(\\hat{x}_i) = \\underset{v}{\\operatorname{argmax}} \\frac{F(v) \\cdot \\hat{x}_i}{||F(v)|| ||\\hat{x}_i||}$"}, {"content": "We further apply a Deep-Image Prior (DIP) [Ulyanov et al., 2018] to the input of F. I.e., $v = g(z)$ where g is a CNN U-Net model applied to a random input z sampled from Gaussian distribution. The only optimization variables of the inversion method are the parameters of g. See Appendix B.3 further explanation and full implementation details.\nBy applying model-inversion to DINO embeddings, Tumanyan et al. [2022] demonstrated that the [CLS] token contains a significant amount of information about the visual appearance of the original image from which it was computed. Even though their work was done in the context of image to image style transfer, their results inspired our work and motivated us to apply their approach in the context of reconstructing training image samples.\nA significant modification to Tumanyan et al. [2022] in our work is by employing a cosine-similarity loss instead of their proposed MSE loss. We find that using MSE loss (i.e., $F^{-1}(x_i) = \\underset{v}{\\operatorname{argmin}} ||F(v) - x_i||^2$) is highly sensitive to even small changes in the scale of x. The scales of x can be very different from the unknown $x = F(s)$. Using cosine similarity alleviates this issue while simultaneously achieving similar quality for the inverted image result (see also Appendix A.1).\nThe above-mentioned technique is used for mapping embeddings to images for most transformers that we consider in our work. However, this technique did not produce good results when applied to CLIP [Radford et al., 2021]. Therefore, to map CLIP image embeddings to the image domain, we employ a diffusion-based generator conditioned on CLIP embeddings by Lee et al. [2022] (similar in spirit to the more popular DALL-E2 [Ramesh et al., 2022]; see also Appendix A.6 and Appendix B.4)."}, {"title": "Selecting Reconstructed Embeddings to be Inverted", "content": "Applying the model-inversion described in Section 3.2 to a large pretrained model is computationally intensive. Inverting a single embedding vector takes about 30 minutes on an NVIDIA-V100-32GB GPU. Therefore, it is not feasible to invert all 25k-50k output candidates of Section 3.1.\nTo determine which reconstructed candidates to invert, we pair each training embedding $x_i$ with its nearest reconstructed candidate $\\hat{x}_j$ (measured by cosine similarity) and select the top 40 vectors with the highest similarity for inversion. This approach proves effective in practice, yielding images with high visual similarity to the original training images, as demonstrated in the results (e.g., Fig. 1).\nIn practice, the original training embeddings are not available (and inverting all candidates is computationally prohibitive). In Section 5 we introduce a novel method to identify good reconstructions without relying on either ground-truth embeddings or exhaustive inversion."}, {"title": "Results", "content": "We demonstrate reconstructed training images from models trained in a transfer learning setup, on the embeddings of large pretrained models. We train several MLPs to solve learning tasks for various choices of training images and choices of the large pretrained backbones from which the image embeddings are computed.\nDatasets. Since we simulate a model that is trained in a transfer learning manner, it is reasonable to assume that such tasks involve images that were not necessarily included in the training sets on which the pretrained backbone was trained (typically, ImageNet [Deng et al., 2009]). In our experiments we use images from Food-101 [Bossard et al., 2014] (most popular dishes from foodspotting website) and iNaturalist [Van Horn et al., 2018] (various animals/plants species) datasets. The resolution of the images vary between 250-500 pixels, but resized and center-cropped to 224 \u00d7 224.\nPretrained Backbones (F) for Image Embeddings. We select several Transformer-based foundation models that are popular choices for transfer learning in the visual domain:\n\u2022 ViT [Dosovitskiy et al., 2020]: vit-base-patch16-224 from TIMM Wightman [2019].\n\u2022 DINO-ViT [Caron et al., 2021]: dino-vitb16 from the official implementation\u00b3.\n\u2022 DINOv2 [Oquab et al., 2023]: dinov2-vitb14-reg from the official implementation\u2074.\n\u2022 CLIP-ViT [Radford et al., 2021]: ViT-L/14 as provided by OpenAI's CLIP repository \u2075.\nThe dimension of the output embeddings is consistent across all backbones F, and equal to d=768.\nMultilayer Perceptron ($\\phi$) consists of a single hidden layer of dimension 500 (d-500-C) that is optimized with gradient descent for 10k epochs, weight-decay of 0.08 or 0.16 and learning rate 0.01. All models achieve zero training error.\nReconstructing Training Data from $\\phi(F)$ We train classifiers $\\phi(F(s))$ on two binary classification tasks: (1) binary iNaturalist is fauna (bugs/snails/birds/alligators) vs. flora (fungi/flowers/trees/bush) and (2) binary Food101 is beef-carpaccio/bruschetta/caesar-salad/churros/cup-cakes vs. edamame/gnocchi/paella/pizza/tacos. Each binary class mixes images from several classes of the original dataset (images are not mixed between different datasets). Each training set contains 100 images (50 per class). The test sets contains 1000/1687 images for iNaturalist/Food101 respectively. All models achieve test-accuracy above 95% (except for DINO on Food101 with 85% and see Fig. 28)."}, {"title": "Identifying Good Reconstruction Without the Original Trainset", "content": "In this section, we introduce a clustering-based approach to identify \"good\" reconstructed candidates without relying on the original training data. This is an important step towards an effective privacy attack. Previous works [Haim et al., 2022, Buzaglo et al., 2023, Loo et al., 2023], including Section 3.3 in this work, rely on the original training images for demonstrating that training images are embedded Determining that a reconstructed candidate is indeed the reconstruction of a specific training image is equivalent to finding a good visual metric between images. See discussion in Sec.3 in Buzaglo et al. [2023]."}, {"title": "Limitations", "content": "In this work, we made design choices when training the models to align with realistic transfer learning practices. However, some choices led to better reconstruction results than others, revealing limitations of our method. Here we discuss these limitations, their impact on our results, and identify potential future research directions:\n\u2022 The quality of reconstructed images relies heavily on the backbone model (F) and the inversion method (Section 3.2). Fig. 8 shows the inverted original embeddings $F^{-1}(F(s))$ (blue), which are the \"best\" we can achieve (independent of our reconstruction method). It also shows how some backbones are easier or harder to invert, as evident in the difference between $F^{-1}(F(s))$ (blue) and the original image s (red), for different F's. It can also be seen that the inverted reconstructed embeddings $F^{-1}(\\hat{x})$ are sometimes more similar to $F^{-1}(F(s))$ than to s, which may hint that the challenge lies in the inversion more than in the reconstruction part. Certainly, improving model inversion techniques is likely to enhance the quality of reconstructed samples."}, {"title": "Conclusion", "content": "In this work, we extend previous data reconstruction methods to more realistic transfer learning scenarios. We demonstrate that certain models trained with transfer learning are susceptible to training set reconstruction attack. Given the widespread adoption of transfer learning, our results highlight potential privacy risks. By examining the limitations of our approach, we identify simple mitigation strategies, such as employing smaller or even linear models, increasing training set size or training without weight-decay regularization. However, some of these mitigation (removing regularization or using smaller models) may also come at a cost to the generalization of the model. Furthermore, these techniques may not be effective against future advanced reconstruction attacks. We aim for our work to inspire the development of new defense methods and emphasize the importance of research on data reconstruction attacks and defenses."}]}