{"title": "Characterizing Massive Activations of Attention Mechanism in Graph Neural Networks", "authors": ["Lorenzo Bini", "Marco Sorbi", "St\u00e9phane Marchand-Maillet"], "abstract": "Graph Neural Networks (GNNs) have become increasingly popular for effectively modeling data with graph structures. Recently, attention mechanisms have been integrated into GNNs to improve their ability to capture complex patterns. This paper presents the first comprehensive study revealing a critical, unexplored consequence of this integration: the emergence of Massive Activations (MAs) within attention layers. We introduce a novel method for detecting and analyzing MAs, focusing on edge features in different graph transformer architectures. Our study assesses various GNN models using benchmark datasets, including ZINC, TOX21, and PROTEINS. Key contributions include (1) establishing the direct link between attention mechanisms and MAs generation in GNNs, (2) developing a robust definition and detection method for MAs based on activation ratio distributions, (3) introducing the Explicit Bias Term (EBT) as a potential countermeasure and exploring it as an adversarial framework to assess models robustness based on the presence or absence of MAs. Our findings highlight the prevalence and impact of attention-induced MAs across different architectures, such as GraphTransformer, GraphiT, and SAN. The study reveals the complex interplay between attention mechanisms, model architecture, dataset characteristics, and MAs emergence, providing crucial insights for developing more robust and reliable graph models.", "sections": [{"title": "Introduction", "content": "Graph Neural Networks (GNNs) have emerged as a powerful tool for learning representations of graph-structured data, demonstrating remarkable success across various applications such as social network analysis (Min et al. 2021), recommendation systems (Gao et al. 2022) and molecular biology (Zhang et al. 2021). Central to the recent advancements in GNNs is the integration of attention mechanisms, which enable the models to focus on the most relevant parts of the input graph, thereby enhancing their ability to capture intricate patterns and dependencies.\nDespite the substantial progress, the phenomenon of Massive Activations (MAs) within attention layers has not been thoroughly explored in the context of GNNs. MAs, characterized by exceedingly large activation values, can significantly impact the stability and performance of neural networks. In particular, understanding and mitigating MAs in GNNs is crucial for ensuring robust and reliable model behavior, especially when dealing with complex and large-scale graphs.\nIn this paper, we aim to bridge this gap by systematically investigating the occurrence and implications of MAs in attention-based GNNs. We focus on edge features in graph transformers, a state-of-the-art GNN architecture, and analyze how these features contribute to the emergence of MAs. Our study reveals that certain graph structures on edge configurations are more prone to inducing MAs, which in turn affects the overall performance and interpretability of the models.\nTo address these challenges, we propose a novel methodology for detecting and analyzing MAs in GNNs. Our approach involves a comprehensive evaluation of various GNN architectures, including GraphTransformer (Dwivedi and Bresson 2021), GraphiT (Mialon et al. 2021), and SAN (Kreuzer et al. 2021), across multiple benchmark datasets, like ZINC (Irwin et al. 2012), TOX21 (Mayr et al. 2016; Huang et al. 2016) and OGBN-PROTEINS (Hu et al. 2020), which differs from their downstream tasks like graph regression, multi-label graph classification, and multi-label node classification. We introduce specific criteria for identifying MAs and conduct extensive ablation studies to elucidate the role of edge features in this context.\nThis study represents the first comprehensive investigation of MAs in GNNs, laying the groundwork for future research. Our findings suggest that the scope of MAs analysis can be expanded to include a wider range of architectures and the evaluation of state-of-the-art attack methods, ultimately enhancing our understanding of MAs' influence on GNN performance and robustness. This is crucial for developing more robust and reliable graph transformer models, especially given the increasing popularity and widespread adoption of transformers in various applications today.\nOur contributions are threefold:\n\u2022 We provide the first systematic study on MAs in attention-based GNNs, highlighting their prevalence and impact on model performance.\n\u2022 We propose a robust detection methodology for MAs, ac-"}, {"title": "Related Works", "content": "GNNs have become effective instruments for studying and extracting insights from graph-structured data, with uses spanning fields like fraud detection (Motie and Raahemi 2023), traffic prediction (Wang et al. 2022) and recommendation systems (Wu et al. 2021). The evolution of GNNs has been marked by significant advancements in their architectures and learning mechanisms, with a recent focus on incorporating attention mechanisms to enhance their expressive power and performance. The introduction of attention in GNNs was largely inspired by the success of transformers in natural language processing (Vaswani et al. 2017). Graph Attention Networks (GATs) (Veli\u010dkovi\u0107 et al. 2017) were among the first to incorporate self-attention into GNNs, allowing nodes to attend differently to their neighbors based on learned attention weights. This innovation significantly improved the model's ability to capture complex relationships within graph structures.\nBuilding upon the success of GATs, several variants and extensions have been proposed. GraphiT (Mialon et al. 2021) introduced a generalization of transformer architectures to graph-structured data, incorporating positional encodings and leveraging the power of multi-head attention mechanisms. Similarly, the Structure-Aware Network (SAN) (Kreuzer et al. 2021) proposed a novel attention mechanism that explicitly considers the structural properties of graphs, leading to improved performance on various graph-based tasks.\nRecent studies on Large Language Models (LLMs) and Vision Transformers (ViTs) have revealed the presence of MAs within their internal states, specifically in the attention layer's output (Xiao et al. 2023; Sun et al. 2024). This phenomenon prompted investigations into the role of these activations in model behavior, performance, and potential vulnerabilities. Similar observations were made in Vision Transformers (ViTs) (Darcet et al. 2023; Dosovitskiy et al. 2020), suggesting that the presence of MAs might be a common feature in transformer-based architectures across different domains. These findings have led to a growing interest in understanding the implications of MAs for model interpretability, robustness, and potential vulnerabilities to adversarial attacks.\nThe study of internal representations in deep learning models has been a topic of significant interest in the machine learning community. Works such as (Bau et al. 2020) have explored the interpretability of neural networks by analyzing activation patterns and their relationships to input features and model decisions. However, the specific phenomenon of MAs in GNNs has remained largely unexplored until now, representing a crucial gap in our understanding of these models.\nThe intersection of adversarial attacks and GNNs is another relevant area of study that relates to the investigation of MAs. Previous work has explored various attack strategies on graph data (Sun et al. 2022a; Gosch et al. 2024), including topology attacks, feature attacks, adversarial training and hybrid approaches. However, the potential vulnerabilities introduced by MAs represent a novel direction for research in this field. Understanding how MAs might be exploited or manipulated by adversarial inputs could lead to the development of more robust GNN architectures.\nHowever, in the broader context of neural network analysis, techniques for probing and interpreting model internals have been developed. Methods such as feature visualization (Olah, Mordvintsev, and Schubert 2017) and network dissection (Bau et al. 2017) have provided insights into the functions of individual neurons and layers in convolutional neural networks. Adapting and extending these techniques to analyze MAs in GNNs could provide valuable insights into their role and impact in possible future works.\nFinally, the study of attention mechanisms in various neural network architectures has also yielded insights that may be relevant to understanding MAs in GNNs. Work on attention flow (Abnar and Zuidema 2020) and attention head importance (Michel, Levy, and Neubig 2019) in transformer models has shown that not all attention heads contribute equally to model performance, and some may even be pruned without significant loss of accuracy. These findings raise questions about whether similar patterns might exist in graph transformer models and how they might relate to the presence of MAs."}, {"title": "Terminology of Massive Activations in GNNS", "content": "Building upon the work on MAs in LLMs (Sun et al. 2024), we extend this investigation to GNNs, focusing specifically on graph transformer architectures. Our study encompasses various models, including GraphTransformer (GT) (Dwivedi and Bresson 2021), GraphiT (Mialon et al. 2021), and Structure-Aware Network (SAN) (Kreuzer et al. 2021), applied to diverse task datasets such as ZINC, TOX21, and OGBN-PROTEINS (see Supplementary Material for details on models' configurations and datasets' composition). This comprehensive approach allows us to examine the generality of MAs across different attention-based GNN architectures."}, {"title": "Characterization of Massive Activations", "content": "MAs in GNNs refer to specific activation values that exhibit unusually high magnitudes compared to the typical activations within a layer. These activations are defined by the following criteria, where an activation value is intended to be its absolute value:\nMagnitude Threshold: An activation is classified as massive if its value exceeds a predetermined threshold. This threshold is typically set to a value that is significantly higher than the average activation value within the layer, ensuring that only the most extreme activations are considered.\nRelative Threshold: In the paper by (Sun et al. 2024), MAs were defined as at least 1,000 times larger than the median activation value within the layer. This relative threshold criterion helped differentiate MAs from regular high activations that might occur due to normal variations in the data or model parameters.\nThe formal definition was represented as:\nMAs = {a | a > 100 and a > 1000 \u00d7 median(A)}\nwhere A represents the set of activation values in a given layer.\nHowever, in contrast to previous studies that employed a fixed relative threshold, our approach adopts a more rigorous method. We estimate MAs by comparing the distributions of activation ratios between a base, untrained model with Xavier weight initializations (Glorot and Bengio 2010), and a fully trained model. This method ensures a more precise identification of MAs based on empirical data rather than an arbitrary fixed threshold. In this way, the untrained model serves as a reference for identifying unusual activations that emerge during training.\nDetection Methodology For both the base and trained models, we detected the MAs following a systematic procedure:\nNormalization: We normalized the activation values within each layer, dividing them by the edge median on the layer, to account for variations in scale between different layers and models. This normalization step ensures a consistent basis for comparison. The choice of dividing by the edge median comes from the huge amount of MAs being present, since almost every edge in the layers presenting MAs holds at least one MA, as shown from Figure 1. This is probably caused by the fact that attention is computed between pairs of adjacent nodes only, in contrast to LLMs where it is computed among each pair of tokens, therefore the model tends to spread MAs among almost all the edges to make them \"available\" to the whole graph. Indeed, Figure 1 indicates that MAs are a common phenomenon across different models and datasets, that they are not confined to specific layers but are distributed throughout the model architecture, and that MAs are an inherent characteristic of the attention-based mechanism in graph transformers and related architectures, not strictly dependent on the choice of the dataset.\nBatch Analysis: We analyzed the activations on a batch-by-batch basis, minimizing the batch size, to have suitable isolation between the MAs and to ensure that the detection of MAs is not influenced by outliers in specific samples. For each activation, we computed the ratio of its magnitude to the edge median:\n$\\mathrm{ratio(activation)} = \\frac{\\mathrm{abs(activation)}}{\\mathrm{median(abs(edge\\_activations))}}$\nand activations whose ratio exceeds the threshold are flagged as massive. Then, we considered the maximum ratio of each batch to detect those containing MAs.\nLayer-wise Aggregation: We performed this analysis across multiple layers of the model to identify patterns and layers that are more prone to exhibiting MAs. This layer-wise aggregation helps in understanding the hierarchical nature of MAs within the model.\nFigure 2 reports the analysis results. The batch ratios significantly increase in the trained transformers, concerning base ones, often even overcoming the threshold of 1000 defined by previous works (Sun et al. 2024), showing the presence of MAs in graph transformers, too."}, {"title": "Methodology and Observations", "content": "Focusing on edge features, first, we analyzed the ratio defined in Equation (1), taking the maximum for every batch, across the layers of each selected model and dataset, and visually compared the outcomes to value ranges obtained using the same model in a base state (with its parameters randomly initialized, without training) to verify the appearance of MAs. The graphical comparison, reported in Figure 2, shows ratios over the base range in most of the trained models, representing MAs.\nTo better characterize MAs, we studied their distribution employing the Kolmogorov-Smirnov statistic (Chakravarti, Laha, and Roy 1967). We found that a gamma distribution well approximates the negative logarithm of the activations' magnitudes, as well as their ratios. Figure 3a shows this approximation for a base model layer. We point out that, according to the existing definition, items on the left of the -3 are MAs.\nWe then compared the distributions of the log-values between the base and trained models. Figure 3 illustrates this comparison, highlighting a significant shift in the distribution of the trained model compared to the base model. Moreover, this shift underscores the emergence of MAs during the training process, affirming that the threshold around $- \\log(\\mathrm{ratio}) = -3$ (e.g., a ratio of 1000 or higher) effectively captures these significant activations, though sometimes it appears to be slightly shifted to the right as in Figure 3c.\nWhen MAs appear, we have found two possible phenomenon:\n\u2022 A lot of massive activation values are added on the left-hand side of the distribution, preventing a good approximation (Figure 3b).\n\u2022 A few values appear on the left-hand side of the distribution, as spikes or humps or out-of-distribution values, which may or may not deteriorate the approximation, as shown in Figures 3c and 3d.\nFor example, histogram in Figure 3a represents the base model with untrained weights (only Xavier initialization). The gamma approximation fits the sample histogram well, with a low Kolmogorov-Smirnov (KS) statistic of 0.020, indicating a very nice fit.\nFigure 3b shows that the distribution of the trained model exhibits a significant shift due to a big hump appearing on the left side, representing extreme activation ratios (MAs). Indeed, the gamma approximation does not fit well, with a"}, {"title": "Insights and Implications", "content": "From Figure 1 and Figure 2 we can highlight the following points.\n1. Dataset Influence:\n\u2022 The ZINC and OGBN-PROTEINS datasets consistently show higher activation values across all models compared to TOX21, suggesting that the nature of these datasets significantly influences the emergence of MAs. Even though many MAs are emerging form GT on TOX21.\n2. Model Architecture:\n\u2022 Different GNN models exhibit varying levels of MAs. For instance, GraphTransformer and GraphiT tend to show more pronounced MAs than SAN, indicating that model architecture plays a crucial role.\n3. Impact of Attention Bias:\n\u2022 Previous works suspect that MAs have the function of learned bias, showing that they disappear introducing bias at the attention layer. This holds for LLMs and ViTs, and for our GNNs as well, as shown in Figure 2 where the presence of MAs is affected by the introduction of the Explicit Bias Term on the attention. Figure 4 and text below suggest that MAs are intrinsic to the models' functioning, being anti-correlated with the learned bias.\nThe consistent observation of MAs in edge features, across various GNN models and datasets, points to a fundamental characteristic of how these models process relational information.\nInspired by recent advancements in addressing bias instability in LLMs (Sun et al. 2024), we introduced an Explicit Bias Term (EBT) into our graph transformer models. This bias term is discovered to counteract the emergence of MAS by stabilizing the activation magnitudes during the attention computation. The EBT is computed as follows:\n$b_e = Qk_e'$\n$b = \\mathrm{softmax}(A_e)v'$\nwhere k, e, v \u2208 Rd are the key, edge, and node value bias terms (one per each attention head), and Ae is the edge attention output. be and b represent the edge and node bias"}, {"title": "Explicit Bias Attack", "content": "The study of adversarial attacks on GNNs has become increasingly important as these models are deployed in critical applications. While various attack strategies have been explored (Z\u00fcgner, Akbarnejad, and G\u00fcnnemann 2018; Sun et al. 2022b), the vulnerability introduced by MAs remains largely unexplored. Understanding how MAs can be exploited by adversaries is crucial for developing more robust GNN architectures and their downstream tasks. In this section, we propose the Explicit Bias Attack, a gradient-based method designed to exploit MAs and assess model robustness. Our approach is inspired by gradient ascent attacks previously applied to image classifiers (Goodfellow, Shlens, and Szegedy 2014) and adapted for graph data (Dai et al. 2018). By analyzing the effectiveness of gradient ascent attack with and without the presence of EBT and MAs, we aim to provide insights into the role of these activations in model fragility.\nTherefore, inspired by previous section, we exploited EBT as computed in Equation (2) and Equation (3) to analyze the importance of MAs for a gradient ascent attack at test time, where noise (added to the input feature embedding) is learned to directly maximize the loss function. The effectiveness of an attack is evaluated by comparing the average test loss before and after the attack (i.e., with random and optimized noise, with the same standard deviations, respectively), using a gain defined as\n$\\mathrm{attack\\ gain} = \\frac{\\mathrm{optimized\\ noise\\ loss} - \\mathrm{random\\ noise\\ loss}}{\\mathrm{random\\ noise\\ loss}}$\nthus a higher gain means a more dangerous attack. We focus on GraphTransformer (GT) with TOX21 because the presence of MAs in each layer - as shown by Figure 2 - highlights the MAs effect for the attack, and compare the power of this method with and without the use of the EBT, which calls off the model's MAS.\nTable 2 shows a stable increase of gain when dealing with MAs, using noise with standard deviation values of 0.01, 0.03, and 0.1 (the input feature embedding has a standard deviation of about 0.9) optimized for 1000 epochs on the test set. Table 2 highlights that MAs can be dangerous for the robustness of a model, and potentially exploited by attacks. These results indicate that a gradient ascent attack is effective in degrading model performance, especially in the presence of MAs. However, the introduction of explicit bias, consistent with the reduction of MAs, can significantly mitigate the impact of the attack, leading to more robust models. This highlights the importance of considering bias in de-"}, {"title": "Conclusion and Future Work", "content": "This paper presents the first comprehensive study of MAs in attention-based GNNs. We have introduced a novel methodology for detecting and analyzing MAs, focusing on edge features in various graph transformer architectures across multiple benchmark datasets. Our findings reveal that MAs are prevalent across different models and datasets, and demonstrate that they could be effectively leveraged by adversaries to degrade the performance of GNNS.\nWe showed that the introduction of Explicit Bias Terms (EBT) can effectively mitigate the occurrence of MAs, leading to more stable activation distributions. However, our results also showed that this mitigation does not always translate to improved test performance, highlighting the complex role of MAs in GNNs' behavior.\nFurthermore, we introduced the Explicit Bias Attack, a gradient-ascent adversarial framework, that demonstrates how MAs, if not mitigated by EBT, can expose models to vulnerabilities in their tasks. This further points out the importance of considering these activations in the context of model robustness.\nFuture research will expand this analysis to a wider range of architectures and advanced attack methods, further clarifying the influence of MAs on GNN performance and robustness, and potentially leading to more interpretable and stable graph-based models. Specifically, future research could explore:\n\u2022 Customized Adversarial MAs: Developing more adversarial techniques to regulate and attack these activations to enhance model stability and performance, like injecting fake MAs or exploiting state-of-the-art graph attack methods.\n\u2022 Downstream-driven MAs: Leveraging MAs for specific downstream task, investigating how to harness these significant activations to improve models and their interpretability on specific assignments such as link prediction or drug design.\n\u2022 Comparative Analysis: Extending the study to additional models and datasets to generalize the findings fur-"}]}