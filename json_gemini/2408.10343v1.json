{"title": "LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain", "authors": ["Nicholas Pipitone", "Ghita Houir Alami"], "abstract": "Retrieval-Augmented Generation (RAG) systems are showing promising potential, and are becoming increasingly relevant in AI-powered legal applications. Existing benchmarks, such as LegalBench, assess the generative capabilities of Large Language Models (LLMs) in the legal domain, but there is a critical gap in evaluating the retrieval component of RAG systems. To address this, we introduce LegalBench-RAG, the first benchmark specifically designed to evaluate the retrieval step of RAG pipelines within the legal space. LegalBench-RAG emphasizes precise retrieval by focusing on extracting minimal, highly relevant text segments from legal documents. These highly relevant snippets are preferred over retrieving document IDs, or large sequences of imprecise chunks, both of which can exceed context window limitations. Long context windows cost more to process, induce higher latency, and lead LLMs to forget or hallucinate information. Additionally, precise results allow LLMs to generate citations for the end user. The LegalBench-RAG benchmark is constructed by retracing the context used in LegalBench queries back to their original locations within the legal corpus, resulting in a dataset of 6,858 query-answer pairs over a corpus of over 79M characters, entirely human-annotated by legal experts. We also introduce LegalBench-RAG-mini, a lightweight version for rapid iteration and experimentation. By providing a dedicated benchmark for legal retrieval, LegalBench-RAG serves as a critical tool for companies and researchers focused on enhancing the accuracy and performance of RAG systems in the legal domain.", "sections": [{"title": "1. Introduction", "content": "In the rapidly evolving landscape of AI in the legal sector, Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) systems have emerged as a crucial technology. These systems, which combine retrieval mechanisms with generative large language models (LLMs), show promising potential for contextualized generation. However, as companies race to develop RAG-based solutions, a critical gap in the ecosystem remains unaddressed: the lack of a dedicated benchmark for evaluating the retrieval component in legal-specific RAG systems. Existing benchmarks such as LegalBench (Guha et al., 2023) assess the reasoning capabilities of LLMs on complex legal questions, and how effectively LLMs recall legal knowledge in their training set. However, these existing benchmarks do not evaluate the retrieval quality over a large corpus, which is crucial for RAG-based systems. Similarly, a legal-specific RAG benchmark is essential because legal documents have unique structures, terminologies, and requirements that more general RAG benchmarks cannot adequately assess. General benchmarks often lack the domain-specific nuances and complex legal relationships found in real-world use cases. These benchmarks (Yu et al., 2024) also generally focus on recall without distinguishing between the retrieval of large, contextually broad chunks and the precise retrieval of small, highly relevant text snippets. The risk of hallucination at generation increases as irrelevant information is included in the context window of an LLM, which poses a significant risk in the legal industry. Benchmarks such as RAGTruth (Niu et al., 2024) study the extent of hallucinated content at the generation step of RAG systems. Additionally, succinct annotations into highly relevant text snippets allow a human-in-the-loop to quickly verify the veracity of an LLM's claims. To address all of these issues, a benchmark is necessary to compare the quality of competing retrieval algorithms. For this, we introduce LegalBench-RAG, the first benchmark designed specifically for evaluating retrieval systems in the legal domain. LegalBench-RAG provides a rigorous framework to assess how well retrieval mechanisms can pinpoint exact legal references, offering a more granular and relevant measure of performance than existing benchmarks."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Retrieval Augmented Generation (RAG)", "content": "A Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) system is an intelligent generative system that utilizes a knowledge base, denoted as D, which contains a set of documents. In this case, each document, represented as $d_i \\in D$, is a string of legal text. A traditional implementation will segment each document $d_i$ into a set of chunks $c_j \\in C_i$. These chunks are then transformed into vector embeddings using a specialized embedding model. After the ingestion of the documents, a user can submit a query q, which will be vectorized using the same embedding model. The system then retrieves the top-k chunks $R_q = {r_1,r_2,...,r_k}$ most relevant to the query, using similarity metrics such as cosine similarity. The retrieved chunks, together with the query and a system prompt P, are processed by a large language model (LLM) to generate the final response. This overall process is formalized as:"}, {"title": "Contextual Retriever", "content": "The contextual retriever module locates relevant information from an external knowledge repository, returning a corresponding context set Rq. Typically, RAG architectures incorporate a bi-encoder retriever such as DPR (Karpukhin et al., 2020), known for its efficiency and accuracy in information retrieval. The Contextual Retriever module often includes a reranking step. First, the top-k' items are retrieved using a bi-encoder retriever. Then, all k' items will be reranked using a cross-encoder model that outputs a similarity score between q and each item. The top $k < k'$ results are returned."}, {"title": "Answer Generator", "content": "The generator component, which often leverages a sequence-to-sequence model, receives both the question and the context as inputs. It then generates an answer yj,q with the likelihood $P_G(y_{j,q} | q, r_{j,q})$."}, {"title": "2.2. Retrieval Augmented Generation (RAG) Benchmarks", "content": "Question-Answer Benchmarks have been introduced prior to the adoption of RAG systems, and datasets such as Hot-PotQA (Yang et al., 2018) are still in use today. Multiple RAG-specific benchmarks have also been created to assess the quality of RAG systems. Retrieval Augmented Generation Benchmark (RGB) (Chen et al., 2023) and RECALL (Liu et al., 2023) are two major contributions that asses the performance of RAG models. However, these benchmarks evaluate a simple case where the answer of a query can be retrieved and solved in a general context. More complex datasets such as MultiHop-RAG (Tang and Yang, 2024) were also introduced to assess the retrieval and reasoning capability of LLMs for complex multi-hop queries."}, {"title": "2.3. LegalBench", "content": "LegalBench (Guha et al., 2023) was introduced to enable greater study of legal reasoning capabilities of LLMs. It is a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which tasks designed and hand-crafted by legal professionals were collected. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting."}, {"title": "2.4. Other legal focused benchmarks", "content": "In recent years, there has been considerable research aimed at evaluating AI models' abilities to undertake tasks traditionally handled by legal professionals (Maroudas et al., 2022) (Katz et al., 2023).\nInitial studies focused on complex legal tasks, such as document review (Wang et al., 2023) and case summarization (Shen et al., 2022) (Shukla et al., 2022).\nSubsequent research efforts have been directed towards the challenges posed by legal texts, such as extensive document lengths and specialized jargon (Li et al., 2023), (Chalkidis et al., 2022).\nAnother critical area of investigation has been the creation of tasks that assess various forms of inferential reasoning commonly required in legal contexts (Chalkidis et al., 2022) (Guha et al., 2023)."}, {"title": "3. A Benchmarking Dataset: LegalBench-RAG", "content": ""}, {"title": "3.1. LegalBench-RAG Constuction", "content": "In this section, we provide detailed information on the construction of the LegalBench-RAG dataset. Notably, we describe the process of creating a set of Queries along with the corresponding ground truth snippets sets derived from the well-known LegalBench (Guha et al., 2023) dataset."}, {"title": "3.1.1. STARTING POINT: LEGALBENCH", "content": "Our benchmark development is based on LegalBench, a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench is a widely recognized dataset that tests LLMs' understanding of legal concepts and their legal reasoning by providing them with the exact context needed to answer legal questions. LegalBench focuses solely on evaluating the generation phase of the RAG pipeline, assessing how well the LLM can generate accurate responses given a certain context, task, and prompt. However, LegalBench does not benchmark the ability to extract the correct context from within a larger corpus. This limitation is what inspired the LegalBench-RAG benchmark.\nWe selected four datasets to construct our retrieval benchmark: Privacy Question Answering (PrivacyQA) (Ravichander et al., 2019), Contract Understanding Atticus Dataset (CUAD) (Hendrycks et al., 2021), Mergers and Acquisitions Understanding Dataset (MAUD) (Wang et al., 2023) and Contract Natural Language Inference (ContractNLI) (Koreeda and Manning, 2021)."}, {"title": "3.1.2. TRACING BACK TO ORIGINAL SOURCES", "content": "To transform LegalBench into a retrieval benchmark, we undertook a comprehensive process to trace each text segment used in LegalBench, back to its original location within the source corpus. Each of the four source datasets used is unique, but in general, the datasets have categorical annotations and associated context clauses for each annotation. Our desired output is a pairing between queries and relevant spans, where each span is a range of characters in the original corpus. Creating this dataset involved searching in the original corpus for the context clauses, in order to deduce the index span implied. And, it involved transforming annotations into queries. By creating these pairings between queries and lists of relevant spans, we ensured that the benchmark accurately reflects the retrieval capabilities needed to locate exactly the relevant information within a large legal corpus. This detailed index mapping is critical for evaluating retrieval-based models within the legal domain."}, {"title": "3.1.3. CONSTRUCTION PROCESS", "content": "Each of the four source datasets is converted into LegalBench-RAG queries through a slightly different process, but all four follow a similar formula.\nFirst, as a pre-processing step, we must create unique descriptions of every document in the corpus, and we must create a mapping between annotation categories and interrogatives.\nFrom there, we can now generate our queries. A full LegalBench-RAG query is constructed using the format:\n\"Consider (document_description);\n(interrogative)\"\nTherefore, every query is a combination of the document description, and an interrogative. Each individual query in LegalBench-RAG originates from an individual annotation in the source dataset. The annotation itself provides the document ID and annotation category, which we convert into a description and interrogative using our pre-processed mappings."}, {"title": "3.1.4. EXAMPLE ANNOTATIONS", "content": "For instance, consider the Contract Understanding Atticus Dataset (CUAD) \"Affiliate License-Licensee\" task from LegalBench.\nThe goal of this task is to classify if a clause describes a license grant to a licensee (incl. sublicensor) and the affiliates of such licensee/sublicensor.\nIn LegalBench, a query appears as follows:"}, {"title": "3.2. Quality Control", "content": "Quality control is of crucial importance for any RAG benchmark, to ensure trust in recall and precision scores. All of the annotations themselves were created by domain-experts via the methods described by our four source datasets. On top of this, our team conducted a thorough manual inspection of every data point in the dataset, employing the process described below. Quality control was rigorously applied at three critical decision points:\nMapping Annotation Categories to Interrogatives\nThe first step involved creating a mapping from annotation categories to interrogatives. Unlike the original LegalBench, where questions could be directly utilized, LegalBench-RAG necessitates that all text relevant to the query is included in the annotation, while irrelevant text is excluded. To meet these stringent criteria, we manually constructed a mapping where the relevant text would align precisely with the domain experts' annotations. Categories with inconsistent annotation precision were excluded entirely from our dataset.\nMapping Document IDs to Descriptions\nSecond, we created the mapping from document IDs to document descriptions. For this, we utilized GPT-40-mini to automatically create a short description for each document, utilizing the file name and approximate first and last paragraphs of the document. For consistency, the model was given explicit instructions on how to format its description, and regular expressions (regex) was used to validate the output format. Each description was then manually inspected. We also used embedding similarity on every pair of document descriptions to find pairs of most similar descriptions, manually excluding any pairs that could not be distinctly differentiated.\nSelection of Annotation Categories\nThe final decision point was determining which annotation categories to include. This decision was informed by our manual evaluations of the precision levels across categories, with inconsistent categories being excluded to preserve the integrity of the benchmark.\nThrough these quality control measures, we have verified the robustness and reliability of the LegalBench-RAG dataset, ensuring that each critical decision point aligns with the intended benchmark standards."}, {"title": "3.3. Dataset Structure", "content": "The LegalBench-RAG benchmark is structured around two primary components: the original corpus and the QA pairs.\nThe corpus includes the documents from our four source datasets. It excludes documents that were not asked by at least one query in LegalBench-RAG. Additionally, every document is in .txt format, utilizing the txt files provided by the source datasets. We opt to utilize the original uncleaned, often messy, file names representative of file names found in the industry.\nThe QA pairs are directly linked to the documents within the corpus. Appendix A provides a detailed structure of these QA pairs. Each query is associated with a list of relevant snippets extracted from the different documents in the corpus that directly answer the query. For each snippet, the file path, the exact quote, and the precise character indices within the document are provided, ensuring a clear reference to the original source. This detailed mapping is crucial for accurately assessing retrieval capabilities within the LegalBench-RAG benchmark."}, {"title": "3.3.1. DESCRIPTIVE STATISTICS", "content": "LegalBench-RAG is composed of 4 datasets and totals almost 80 million characters in its corpus across 714 documents. Each pair is annotated by legal experts, ensuring the highest accuracy and relevance for this benchmark. We extract 6,889 question-answer pairs which constitutes our retrieval benchmark. The outcome is a robust dataset that we call LegalBench-RAG."}, {"title": "3.3.2. LEGALBENCH-RAG AND LEGALBENCH-RAG-MINI", "content": "Given the consequent size of LegalBench-RAG, this paper also introduces LegalBench-RAG-mini, a more lightweight version of the benchmark we proposed. LegalBench-RAG-mini was created by selecting exactly 194 queries from each of the four datasets PrivacyQA, CUAD, MAUD and ContractNLI. We select the portions of the corpus corresponding to these queries accordingly. This results in a dataset of 776 queries."}, {"title": "3.4. Significance of this work", "content": "Creating datasets in the legal space is a difficult, time-consuming and costly task. For instance, The CUAD dataset alone, was a year-long effort that required the expertise of over 40 lawyers to generate 13,000 annotations. Given that 9,283 pages were reviewed at least 4 times, that each page required 5-10 minutes of review, and assuming a rate of $500 per hour for legal expertise, then we can estimate that the creation of the CUAD dataset would cost around $2,000,000 to replicate.\nLegalBench-RAG is the first publicly available retrieval-focused legal benchmark, and can be used for both commercial and academic purposes to assess the quality of the retrieval step of RAG pipelines on legal tasks. We hope that the introduction of this dataset allows the industry to more easily compare and iterate upon the plethora of RAG techniques available today, by providing a standardized evaluation framework for these techniques."}, {"title": "3.5. Limitations", "content": "We make note of the several limitations of this dataset. Our four source datasets include NDAs, M&A agreements, various commercial contracts, and the privacy policies of consumer-facing online companies. While this is broad, this dataset is not made of an exhaustive list of all existing documents in the legal industry. For example, this benchmark does not assess structured numerical data parsing, which is relevant for cases involving financial fraud. It also does not assess the parsing and analyzing of medical records, which is relevant in personal injury suits.\nNotably, the queries in this benchmark are always answered by exactly one document, so this benchmark does not assess the ability of a retrieval system to reason across information found in multiple documents. It only assesses the ability of a retrieval system to select the correct document, and then the correct snippets within that document.\nThrough manual inspection, we were able to find several queries that do require multi-hop reasoning, which is a difficult task to achieve. However, there is room to create even more complexr queries that require a very high number of hops to generate the correct snippets."}, {"title": "4. Benchmarking RAG systems using LegalBench-RAG", "content": "A typical RAG system operates in two phases: the retrieval phase and the generation phase. LegalBench-RAG is designed to assess the effectiveness of the retrieval phase, while the original LegalBench can be utilized to evaluate the performance of the generation phase. In this section, we demonstrate how LegalBench-RAG can be employed to measure the quality of retrieval. The code to run the benchmark is publicly available here.\nAll the experiments conducted in this work are run on LegalBench-RAG-mini."}, {"title": "4.1. Hyperparameters in RAG Pipelines", "content": "Implementing a RAG pipeline involves selecting various hyperparameters and making critical design decisions, which can be adjusted to optimize performance. In this section, we discuss a few design decisions that will be evaluated using LegalBench-RAG."}, {"title": "4.1.1. PRE-PROCESSING STRATEGIES", "content": "As discussed in Section 2.1, standard RAG pipelines typically generate embeddings from document chunks using an embedding model. The method by which documents are divided into these chunks is a crucial design consideration (Pinecone, 2024). Different strategies can be employed, ranging from simple fixed-size chunks with potentially overlapping segments, to recursive and contextual approaches, to more sophisticated semantic chunking methods (FullStackRetrieval-com, 2024). The choice of chunking strategy can significantly impact the effectiveness of the retrieval system."}, {"title": "4.1.2. POST-PROCESSING STRATEGIES", "content": "Following retrieval, several critical design decisions must be made in the post-processing phase. One key parameter is the number of retrieved chunks (i.e., the choice of k in top-k retrieval) that are subsequently input to the large language model (LLM). This selection is pivotal due to the inherent trade-off between maximizing the relevant context provided to the LLM and minimizing the introduction of noise, which could increase the risk of hallucination. Furthermore, the decision to employ a reranker model on the retrieved chunks represents another crucial consideration, as it can substantially alter the final set of chunks presented to the LLM, potentially impacting the model's overall performance."}, {"title": "4.1.3. OTHER DESIGN DECISIONS", "content": "Among other critical decisions, the choice of the embedding model to use is a common one. Embedding models can vary significantly in how well they represent textual content. A well-chosen embedding model can enhance retrieval accuracy by producing more meaningful and contextually relevant embeddings, which improves the quality of the retrieval."}, {"title": "4.2. Experimental Setup", "content": "In this evaluation experiment, we implement multiple RAG pipelines that we benchmark using LegalBench-RAG. We conduct several experiments to study the impact of the chunking strategy and the effect of reranking the retrieved chunks using a specialized model.\nWe used OpenAI's \"text-embedding-3-large\" embeddings for the embedding model (OpenAI, 2024). We chose SQLite Vec as a vector database and Cohere's \"rerank-english-v3.0\" as our reranker model (Cohere, 2024). The two chunking strategies we benchmarked here are (1) a naive fixed-size chunking method with a chunk size of 500 characters with no overlap, and (2) a Recursive Character Text Splitter (RCTS) (LangChain, 2024), which divides text into smaller chunks by sequentially attempting to split on a predefined list of characters which maintains paragraphs, sentences, and words together. The two post-processing we evaluated here are (1) no reranker after the retrieval, and (2) Cohere Reranker after the retrieval.\nTo evaluate the results on LegalBench-RAG, we weight the metrics equally on each dataset, independently from the number of documents or queries they contain."}, {"title": "4.3. Variation of preprocessing splitting strategies", "content": "In our initial experiment, we fixed all hyperparameters and compared the impact of transitioning from a naive chunking strategy to a Recursive Text Character Splitter (RTCS). The performance results for each dataset, as well as varying values of k, are presented in Tables 4 and 5."}, {"title": "4.4. Variation of postprocessing method", "content": "We conduct a second experiment where we freeze all other hyperparameters and change the post-processing method used only. We compare using no reranking stategy of the retrieved chunks and using Cohere's Reranker model. The results of this comparison are shown in Tables 4 and 6 respectively. We also run our benchmark on the RCTS method with Cohere's Reranker in Table 7."}, {"title": "5. Results and Discussion", "content": ""}, {"title": "5.1. Results of the experimentation", "content": "The overall evaluation results are computed for varying values of k between 1 and 64 and for each of the four dataset and are also aggregated across all datasets using an equal weight for each dataset. Figures 2 and 3 display Recall@k and Precision@k, respectively, for all method combinations.\nThis experiment demonstrates that the most effective strategy was the Recursive Text Character Splitter (RTCS) without a reranker. Surprisingly, the performance of the Cohere Reranker was inferior compared to not using a reranker. This result may be attributed to the difficulty of this benchmark and its focus on legal text, which may not align well with a general-purpose model like Cohere's reranker. Both precision and recall were highest with the RTCS + No reranker configuration. As anticipated, recall improved with increasing values of k, while precision decreased. The observed low precision values could be due to the highly targeted and concise nature of the ground truth."}, {"title": "5.2. Comparison of the four datasets", "content": "Additionally to the standard evaluation conducted, we also aggregated Recall@k and Precision@k across all four of the different methods experimented with for each of the four datasets. The goal was to assess the relative difficulty of achieving high scores on each dataset, as shown in Figures 4 and 5."}, {"title": "5.3. Future Work", "content": "These results highlight the need for more specialized and challenging legal benchmarks to effectively assess the quality of retrieval systems. Moreover, the consistently poor performance of the Cohere Reranker, particularly on the more challenging datasets like MAUD, indicates that there is significant room for improvement in reranking models. Future research could explore fine-tuning or developing new rerankers specifically designed for legal text, potentially incorporating more legal-specific features or training on larger, more diverse legal corpora."}, {"title": "6. Conclusion", "content": "In conclusion, this paper introduces LegalBench-RAG, the first benchmark specifically designed to evaluate the retrieval component of Retrieval-Augmented Generation (RAG) systems in the legal domain. By leveraging existing expert-annotated legal datasets and meticulously mapping question-answer pairs back to their original contexts, LegalBench-RAG provides a robust framework for assessing retrieval precision and recall. The creation of both LegalBench-RAG and its lightweight counterpart, LegalBench-RAG-mini, addresses a critical gap in the evaluation of RAG systems, particularly in contexts where legal accuracy is paramount.\nThrough a series of experiments, we demonstrated the effectiveness of different chunking strategies and post-processing methods, revealing that advanced chunking techniques like the Recursive Text Character Splitter (RTCS) significantly enhance retrieval performance. Interestingly, the use of general-purpose rerankers, such as Cohere's model, showed limitations, highlighting the need for domain-specific tools in legal AI applications.\nThe results underscore the importance of specialized benchmarks that can accurately capture the nuances of legal text retrieval. LegalBench-RAG not only facilitates a more granular evaluation of retrieval mechanisms but also provides a foundation for further advancements in the development of RAG systems tailored to the legal field. As legal AI continues to evolve, the availability of such targeted benchmarks will be crucial for driving innovation and ensuring the reliability of AI-driven legal tools."}]}