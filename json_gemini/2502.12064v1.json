{"title": "AI-generated Text Detection with a GLTR-based Approach", "authors": ["Luc\u00eda Yan Wu", "Isabel Segura-Bedmar"], "abstract": "The rise of LLMs (Large Language Models) has contributed to the improved performance and development of cutting-edge NLP applications. However, these can also pose risks when used maliciously, such as spreading fake news, harmful content, impersonating individuals, or facilitating school plagiarism, among others. This is because LLMs can generate high-quality texts, which are challenging to differentiate from those written by humans. GLTR, which stands for Giant Language Model Test Room and was developed jointly by the MIT-IBM Watson AI Lab and HarvardNLP, is a visual tool designed to help detect machine-generated texts based on GPT-2, that highlights the words in text depending on the probability that they were machine-generated. One limitation of GLTR is that the results it returns can sometimes be ambiguous and lead to confusion. This study aims to explore various ways to improve GLTR's effectiveness for detecting AI-generated texts within the context of the IberLef-AuTexTification 2023 shared task, in both English and Spanish languages. Experiment results show that our GLTR-based GPT-2 model overcomes the state-of-the-art models on the English dataset with a macro F1-score of 80.19%, except for the first ranking model (80.91%). However, for the Spanish dataset, we obtained a macro Fl-score of 66.20%, which differs by 4.57% compared to the top-performing model.", "sections": [{"title": "1 Introduction", "content": "In recent years, the rapid development of AI (Artificial Intelligence) due to the re-lease of models such as ChatGPT by OpenAI or Gemini by Google, has increased the amount of AI-generated content on a huge scale (Mart\u00ednez et al., 2023). GPT (OpenAI et al., 2024), PaLM (Chowdhery et al., 2022), and BLOOM (Workshop et al., 2023) are some of the Large Language Models (LLMs) accessible to the public, applied for automatic content generation.\nLLMs are known for being able to generate high-quality texts, i.e., texts that are grammatically correct and coherent, which can make it challenging to differentiate them from those written by humans (Bublak, 2024). Nevertheless, even though LLMs have achieved remarkable success in the field, they have also introduced some risks and ethical concerns. This is because LLMs do not understand the meaning of the content they generate, and hence can produce false or inaccurate information, leading to hallucinations (Huang et al., 2023). In addition, bias can be introduced during the generation of content, leading to unethical content such as gender inequality (Azoulay, 2024). Furthermore, the misuse of these models by some users may contribute to the spread of fake news, spam for malicious purposes, or even enhancing academic cheating (Team, 2023) (Heldt, 2023).\nFor these reasons, it is crucial to develop some approaches to regulate and detect AI-generated content. To do so, the AuTexTification (Sarvazyan et al., 2023) shared task was presented as part of the IberLEF 2023 workshop (Jim\u00e9nez-Zafra and Rangel, 2023). The task involved two subtasks: binary classification of text as human-written or AI-generated, and identification of the specific LLM that generated the text.\nIn this study, we introduce a new approach using the GLTR tool (Gehrmann, Strobelt, and Rush, 2019a) for tackling the first subtask in both English and Spanish. GLTR (Giant Language Model Test Room) is a visual tool designed to help detect machine-generated texts based on GPT-2, that highlights the words in text depending on the probability that they were machine-generated. However, one limitation of GLTR is that the results it returns can sometimes be ambiguous and lead to confusion. Hence, this study aims to explore various ways to improve GLTR's effectiveness for the binary classification task.\nThe rest of this paper is organized as follows: In Section 2 we report the most relevant state-of-the-art approaches for AI-generated text detection. In Section 3, we present the datasets used in this study, and in Section 4, we describe the experimental setup considered to asses the binary classification task. In Section 5, we show and discuss the results obtained from the proposed method in both languages, English and Spanish. Finally, Section 6 presents the conclusions and future work."}, {"title": "2 State of the art", "content": ""}, {"title": "2.1 Participating systems in AuTexTification 2023", "content": ""}, {"title": "2.1.1 English", "content": "In the IberLef-AuTexTification shared task 2023 (Sarvazyan et al., 2023), 36 teams participated in Subtask 1 for English texts. Most submissions utilized transformer-based models, especially BERT-based models, while others employed generative models (such as GPT-2, Grover, and OPT), CNNs, or LSTMs. Traditional machine learning models, such as Logistic Regression and SVM, were also implemented by participants but generally performed worse compared to transformer-based models.\nThe top-performing team, TALN-UPF (Przybyla, Duran-Silva, and Egea-G\u00f3mez, 2023), achieved a macro F1-score of 80.91% using a bidirectional LSTM with fine-tuned ROBERTa and token-level POS tagging, which helped detect grammatical and morphological errors uncommon for humans. This same team also placed second with a macro F1-score of 74.16% using the same model without POS tagging. The third-ranked team, CIC-IPN-CsCog (Aguilar-Canto et al., 2023), used a fine-tuned GPT-2, achieving a macro F1-score of 74.13%.\nTraditional machine learning models achieved lower scores; for instance, Ling\u00fc\u00edstica_UCM's (Alonso Sim\u00f3n et al., 2023) LinearSVC with TF-IDF and n-gram features reached 68.33%, while other models like CatBoost and Multilayer Perceptron performed below 60%.\nOverall, transformer and neural network models outperformed traditional machine learning approaches, with BiLSTM models leading in effectiveness, followed by GPT-2 and other ensemble transformer-based models."}, {"title": "2.1.2 Spanish", "content": "In Subtask 1 for Spanish texts, a total of 23 teams participated. Similarly to the English task, most submissions were based on transformer models, obtaining better results compared to other teams that used more traditional approaches. Moreover, the macro F1-scores obtained were lower for all of the models compared to their performance in the English task. This could be attributed to the limited number of models trained on data in the Spanish language.\nThe best-performing team, as well as in the English task, was TALN-UPF (Przybyla, Duran-Silva, and Egea-G\u00f3mez, 2023), with their BiLSTM model with fine-tuned ROBERTa and token-level POS tagging, obtaining a macro F1-score of 70.77%. Ling\u00fc\u00edstica_UCM (Alonso Sim\u00f3n et al., 2023) achieved second place with their LinearSVC with TF-IDF and n-gram features with a score of 70.60%.\nOverall, we can support that indeed solutions based on transformer models and neural network algorithms give the best performance, overbeating all of the proposed baselines in the task. On the other hand, traditional Machine Learning models such as SVM or CatBoost classifier tend to return lower scores."}, {"title": "2.2 IberAuTexTification 2024", "content": "In 2024, IberLef-IberAuTexTification shared task (Sarvazyan et al., 2024), the second version of the AuTexTification at IberLEF 2023 shared task, was released. It was based on the same subtasks as the 2023 edition, but focused on languages from the Iberian Peninsula: Spanish, Catalan, Basque, Galician, Portuguese, and English (in Gibraltar). Additionally, new domains (news, reviews, emails, essays, dialogues, wikipedia, wikihow, tweets, etc.), and models (GPT, LLAMA, Mistral, Cohere, Anthropic, MPT, Falcon, etc.) were introduced for the generation of automatically generated texts.\nThe top-performing systems for both subtasks were Transformer models enhanced with additional lexical, syntactic, and semantic features. The best-performing team, jor_isa_uc3m (Garc\u00eda and Segura-Bedmar, 2024), proposed an ensemble composed by three multilingual transformers, DistilBERT-base-multilingual-cased, mDeBERTa-v3-base, and XLM-ROBERTa-base, obtaining a macro F1-score of 80.5%."}, {"title": "2.3 Other approaches to detect AI-generated texts", "content": "In April 2024, Fareed Khan published a web application\u00b9 that identifies the 100 most common words used by AI within an input text. The objective of this application was to detect if the text was generated by a machine just by looking at it.\nGPTZero2 (Tian and Cui, 2023) launched their first AI detection solution in January 2023, and has since then developed several tools for this goal. Among them is an API available online that allows the user to input a text, and returns the probability percentage for each class: human, mixed, and AI, along with the confidence percentage.\nIn 2019, researchers at Harvard and IBM (Gehrmann, Strobelt, and Rush, 2019a) developed a visual tool named GLTR (Giant Language Model Test Room), which highlighted the words in a text depending on how predictable or common these were, i.e., the probability these were generated by a machine. The words were colored by order of frequency, in purple, red, yellow, or green, where purple represented the rarest words with a predicted position higher than 1,000, and green the most common (within the top 10 predictable words). This tool was tested in an assessment where human subjects initially identified AI-generated texts manually with an accuracy of 54%, which increased to over 72% after using GLTR.\nGenerally, we can see that there are various ways of detecting AI-generated texts, going from manual extraction to using LLMs and other visual tools. We tested each of the approaches defined above, where Fareed Khan's app seemed inconsistent since it produced similar lists of words for both human and AI-generated texts. Next, GPTZero always returned the correct label, differentiating very well the texts. Lastly, the GLTR tool gave in general good results but required user interpretation, which sometimes led to confusion."}, {"title": "3 Dataset", "content": "The dataset used for the study belongs to the AuTexTification shared task of the Iber-LEF 2023 Workshop (Sarvazyan et al., 2023). This dataset is divided by subtask and language, and by train or test. This results in eight different datasets, which collectively contain more than 160,000 texts across five domains with diverse writing styles ranging from more structured and formal to less structured and informal. For subtask 1, the training split included tweets, how-to articles, and legal documents, while the test split consisted of reviews and news articles. This setup was designed to assess the model's ability to generalize to different text domains.\nThe source datasets used for extracting the human-authored texts can be seen in Table 1 below:\nMachine-generated texts were created from human texts by using three different BLOOM models (BLOOM-1B7, BLOOM-3B, and BLOOM-7B1), and three GPT-3 models (babbage, curie, and text-davinci-003, with 1b, 6.7b and 175b parameter scales respectively). These models were fine-tuned with a top-p of 0.9 and a temperature of 0.7. Moreover, a maximum number of tokens was selected for each domain, maintaining the distribution of human texts: 20 tokens for tweets, 70 for reviews, and 100 for news, legal, and how-to articles.\nTable 2, displays the resulting dataset distribution. We observe that it is well-balanced for both domains and classes, for both languages. Moreover, Figures 1 and 2 support this statement. However, we can see that the reviews domain in the Spanish language, contains slightly fewer generated instances than human instances."}, {"title": "4 Methodology", "content": "In this work, we present a novel approach that combines the GLTR tool, which visualizes the likelihood of word predictions, with the predictive capabilities of GPT-2 models to classify text as either human-written or machine-generated. By leveraging GLTR's word-level probabilities and extending its functionality with a classification step, we created a robust pipeline for evaluating text authenticity. This pipeline is applied to English and Spanish texts, using multiple versions of GPT-2.\nGLTR, also called Giant Language Model Test Room, is a visual tool developed by researchers at Harvard NLP and MIT-IBM Watson AI lab, which uses GPT-2 to predict the likelihood of words in a sentence (Gehrmann, Strobelt, and Rush, 2019b). The model iterates through the different words of an input sequence, where for each word it returns a set of predicted words along with their probability of being the next word. If the real word is contained within the Top 10 predicted words, it is colored green, and the same happens for the Top 100 in yellow, the Top 1000 in red, and the rest in purple. We can better understand this by looking at the example in Figure 3, where on top there is a human-written text, and below a machine-generated text. We can see that the generated text contains for the most part green-colored words, while the human-written text is composed of a variety of colored words. This is because humans tend to use more complex words and machines, more common and predictable words. Moreover, the contributors of this tool also made available a demo\u00b3 for users to test.\nAs we can see, GLTR is a very remarkable tool, and hence, we decided to create a model based on it. We used the same working structure of the GLTR, taking as reference the code in the GitHub repository that the contributors have made available.\nTo construct the GLTR-based model, we first preprocessed the dataset by splitting it into training (80% of instances) and validation (20% of instances) subsets, and encoding the labels generated and human as 0 and 1, respectively. Then, each sentence was tokenized using GPT-2's tokenizer, and predictions for word likelihoods were obtained for every word. This was done by defining a function to obtain the ordered list of predicted words and their probabilities, given the previous words in the sentence. Given this list, we found the position of the ground-truth word and colored it according to the rules we explained before.\nWhat differentiates our model from the GLTR tool is that we have incorporated an additional evaluation step to compute some performance metrics. While GLTR primarily focuses on visualizing the likelihood of words being generated by a model, we take a more data-driven approach by classifying sentences based on the proportion of green-colored words, which are those predicted to have a higher likelihood of being machine-generated. In our approach, the proportion of green-colored words is calculated for each sentence and compared against a predefined threshold to classify the sentence. Specifically, if the percentage of green-colored words exceeds the threshold, the sentence is classified as machine-generated; if it is lower, it is classified as human-written. For example, if the threshold is set at two-thirds, the sentence is classified as machine-generated when more than two-thirds of its words are green-colored. Otherwise, it is labelled as human-written.\nIn our study, we experimented with various threshold values (1/4, 1/3, 1/2, 2/3, 3/4, and 5/6) to explore the impact on classification performance across both English and Spanish languages. These variations allowed us to assess how the proportion of machine-like words influences classification accuracy and fine-tune the model's ability to distinguish between human-written and machine-generated texts.\nOnce the classification is made, the predicted classes are then compared to the real labels, and the macro F1-scores are computed.\nThe GPT-2 model is a pretrained model on large amounts of raw and unlabelled texts in the English language, from the internet. The model was developed by OpenAI, and specifically designed to guess the next word in a sentence. During training, inputs consist of continuous text sequences, where the model learns to predict the next word in the sequence by shifting the target sequence one token to the right. Moreover, the model uses a mask-mechanism to ensure that each prediction is based solely on previous tokens, preventing the model from using future ones. This approach enables the model to excel at generating text from prompts. However, one limitation of GPT-2 is that the training data used for this model contains a lot of unfiltered content from the internet, which can introduce biases into the model.\nFor the English texts, we studied 4 different versions of the GPT-2 model (Radford et al., 2019):\n\u2022 gpt2-small, is the smallest version of GPT-2, with 124M parameters.\n\u2022 gpt2-medium, is the 355M parameter version.\n\u2022 gpt2-large, is the 774M parameter version.\n\u2022 gpt2-xl, is the 1.5B parameter version.\nFor the Spanish texts, we additionally used the gpt2-small-spanish model developed by Datificate (Obregon and Carrera, 2023), which is based on the gpt2-small model (Radford et al., 2019). It was trained on Spanish Wikipedia (around 3GB of processed training data), using Transfer Learning and Fine-tuning techniques. Similarly to the English pre-trained gpt2-small, one limitation of this model is the introduction of biases into the model, originated from the unfiltered content from the internet used as training data.\nNote that the study with different threshold values was done on gpt2-small and gpt2-small-spanish for English and Spanish, respectively. To prevent computational complexity, gpt2-medium, gpt2-large, and gpt2-xl, were only evaluated on the best threshold value obtained on the gpt2-small and gpt2-small-spanish study.\nThe dataset as well as the code to replicate the experiments can be found in the GitHub repository 5. Moreover, an online demo has been deployed in Streamlit, which is available to all users and allows them to test their own English texts, returning if the input text was AI-generated or human-written 6."}, {"title": "5 Results and discussion", "content": "For our study, we mainly focused on the macro F1-score and the F1-scores for each of the classes, to compare the results between the different models. The reason is that the AuTexTification task on which this project is based used the F1-score as the main performance metric.\ntexts, especially in reducing false positives and false negatives.\nFor the Spanish texts, Table 5, presents the F1-scores obtained for different threshold values by the gpt2-small-spanish model. The best Generated F1-score is obtained at the 1/2 threshold with 74.28%, while the highest Human F1-score at the threshold 2/3 with 68.31%. Similarly to gpt2-small for the English dataset, the optimal threshold that achieves the overall balance and highest macro F1-score of 62.18%, is 2/3.\nNext, Table 6 shows the results obtained for each of the different GPT-2 models with the threshold value at 2/3. The best macro Fl-score is achieved with gpt2-xl with 66.20%, and with Generated and Human F1-scores of 64.90% and 67.51%, respectively. Moreover, we notice that gpt2-small-spanish performs worse regardless of it being trained with Spanish Wikipedia. This could be due to gpt2-small-spanish being much smaller in terms of the number of parameters.\nFigure 5, presents the confusion matrix for gpt2-xl for Spanish texts. Out of 11,209 generated texts, the model correctly identifies 6,280 as generated, but it mistakenly classifies 4,929 as human-written (false positives). For the 8,920 human-written texts, it accurately labels 7,056 as human, while 1,864 are incorrectly classified as generated (false negatives). Similarly to gpt2-small for English texts, there is some room for improvement in reducing the false positives and false negatives.\nMoreover, we compare our models' results with those presented in AuTexTification 2023 (Sarvazyan et al., 2023), described in Section 2.1. gpt2-small overbeats all of the presented models except for the BiLSTM with fine-tuned ROBERTa and token-level POS tagging from team TALN-UPF (Przybyla, Duran-Silva, and Egea-G\u00f3mez, 2023), achieving second rank. However, we must remark the performance between models is very similar, with only a macro F1-score difference of 0.72%. On the other hand, gpt2-x1 did not perform so well for Spanish texts, as it would have only ranked 9th position in the competition, due to the low F1-scores obtained for both classes."}, {"title": "6 Conclusions and future work", "content": "In this study, we evaluated the performance of GPT-2 models extended with the GLTR tool, on the binary classification task of differentiating AI-generated and human-written texts.\nFor both English and Spanish texts, the optimal threshold for gpt2-small and gpt2-small-spanish was 2/3, achieving a macro Fl-score of 80.19% and 62.18%, respectively. However, although gpt2-xl was not trained with Spanish data, it returned better results, achieving a macro F1-score of 66.20%.\nAmong the four GPT-2 model variants explored, gpt2-small outperformed larger models (gpt2-medium, gpt2-large, gpt2-x1) in the English subtask. On the other hand, for the Spanish texts, the gpt2-xl model outperformed smaller models (gpt2-small, gpt2-small-spanish, gpt2-medium, gpt2-large).\nMoreover, the gpt2-small model would have ranked second in the AuTexTification 2023 (Sarvazyan et al., 2023) competition, outperforming most models with a minimal macro F1-score difference of 0.72% compared to the top-performing BiLSTM + ROBERTA + POS tagging model (Przybyla, Duran-Silva, and Egea-G\u00f3mez, 2023). However, for Spanish texts, the gpt2-xl model would have ranked lower, in 9th position, as it returned a low Fl-score for both Generated and Human categories.\nAs for future work, we plan on doing further fine-tuning to improve the gpt2-small-spanish model performance, as well as training the gpt2-xl model with Spanish data. We also plan on tackling the multilingual task for languages from the Iberian Peninsula presented on the Iber-AuTexTification 2024 task (Sarvazyan et al., 2024), as well as the second subtask about model attribution, which is a multiclass classification problem, proposed in the two editions of AuTexTification.Furthermore, we also plan to investigate the detection of AI-generated content in a multimodal setting."}]}