{"title": "Barriers to Complexity-Theoretic Proofs that Achieving AGI Using Machine Learning is Intractable", "authors": ["Michael Guerzhoy"], "abstract": "A recent paper [VRGA+24] claims to have proved that achieving human-like intelligence using learning from data is intractable in a complexity-theoretic sense. We point out that the proof relies on an unjustified assumption about the distribution of (input, output) pairs in the data. We briefly discuss that assumption in the context of two fundamental barriers to repairing the proof: the need to precisely define \"human-like,\" and the need to account for the fact that a particular machine learning system will have particular inductive biases that are key to the analysis. Another attempt to repair the proof, by focusing on subsets of the data, faces barriers in terms of defining the subsets.", "sections": [{"title": "1 Introduction", "content": "In [VRGA+24] a claim is made that the authors \"formally prove [in the paper that] creating systems with human(-like or -level) cognition (\u201cAGI\" for short, for the purposes of this paper) is intrinsically computationally intractable.\"\nHere, we show that the paper falls short of formally proving the claim. We identify a key unproven premise that underlies the proof: that the distribution D of tuples (s,b), with s denoting \u201csituation\" and b denoting \u201c[human] behavior\u201d in response to s can be an arbitrary (polytime-computable) distribution.\nIf D is a model of human behavior, both the marginal distribution of s and the conditional distribution $P_D(b|s)$ are in fact highly structured. For example, if s encodes natural images, the marginal distribution $P_D(S)$ would need to account for the hierarchical structure of natural images [SO01]. If $P_D(b|s)$ models human chess moves, the distribution would need to account (among other things) for the rules of chess. This means that many D's can be ruled out a-priori.\nAs we argue below, the fact that, in the authors' proof, D denotes both the distribution of situation-behavior tuples and an arbitrary distribution means that the authors did not prove what they set out to prove.\nWe argue that two critical issues must be resolved when attempting to repair the proof (although we make no claim that the proof could be repaired).\n\u2022 \"Human-like\" behavior must be defined precisely.\n\u2022 The fact that while an arbitrary function is not learnable due to No-Free-Lunch-Theorem-style results, some structured functions can be learned with appropriate inductive biases must be considered.\nAnother attempt to repair the proof by focusing on subsets of the data also face a barrier.\nThe paper is organized as follows. in Section 2 we introduce the Ingenia Theorem of [VRGA+24], along with the necessary context. In Section 3 we point out what we believe to be a central flaw in the proof. In Section 4, we identify what we see as the challenges that a correct version of the proof would have to overcome. We illustrate that not having met one of the challenges leaves the current proof vulnerable to a reductio ad absurdum argument (Section 4.1.1)."}, {"title": "2 The Ingenia Theorem", "content": "We first present the \"AI-by-Learning\" problem, and then the \"Ingenia Theorem\" that the authors derive\nAI-by-Learning Problem (from [VRGA+24])\nGiven: An integer K and a way of sampling from a distribution D.\nTask: Find a description $L_A \\in L_A$, with length $|L_A| < K$, of an algorithm $A \\in A$ that with probability > \u03b4(n), taken over the randomness in the sampling, satisfies:\n$\\underset{s \\sim D_n}{\\text{Pr }}[A(s) \\in B_s] \\geq \\frac{|B_s|}{|B|} + \\epsilon(\\eta)$.\nHere \u03b4(n) and \u03f5(n) are arbitrary non-negligible functions. A function f is non-negligible if there is some d such that for sufficiently large n, $f(n) \\geq 1/n^d$.\nIngenia Theorem (from [VRGA+24])\nAI-by-Learning is intractable\nInformally, van Rooij et al. claim that the Ingenia Theorem implies that it is not possible to obtain a human-like AI by learning from examples, by reducing the Perfect-vs-Chance problem, known to be intractable [Hir22], to an instance of the AI-by-Learning problem.\nPerfect-vs-Chance (decision problem) (from [VRGA+24])\nGiven: A way to sample a given distribution D over {0, 1}\u2033 \u00d7 {0, 1}, an integer k, and the promise that one of the following two cases apply:\n1. There is an efficient program M of size at most k such that $Pr_{(x,y) \\sim D}[M(x) = y] = 1$\n2. For any program M of size at most k,\n$Pr_{(x,y) \\sim D}[M(x) = y] \\leq \\frac{1}{2} + \\delta$,\nwhere 0 < \u03b4 < 1 is an arbitrary constant.\nQuestion: Is (1) or (2) the case?"}, {"title": "3 Incorrect Reduction from Perfect-vs-Chance to AI-by-Learning", "content": "The key issue in the paper is whether the distribution D in the AI-by-Learning problem is an arbitrary distribution or a distribution (s, b) ~ D, the distribution of situation-behavior tuples. If the distribution is arbitrary, the \"AI-by-Learning\" problem is misnamed: it is the general problem of learning a function from examples. On the other hand, if D is a distribution of situation-behavior tuples, as described on p. 6 (bottom), then the problem is arguably appropriately-named, but it was not demonstrated that Perfect-vs-Chance reduces to the problem.\nIn the informal presentation on p. 6, the authors assume a learning machine M that is able to learn from examples to approximately maps to b when (s,b) ~ D, the distribution of situation-behavior tuples. They use this learning machine on instances of Perfect-vs-Chance and a different distribution D. However, it can happen (in fact, conditional on such a learning machine existing at all, it is likely) that the learning machine M works on data samples from D, the distribution of situations-behaviors, but not on the identically-named arbitrary distribution D in the \u201cformalized\u201d version of AI-by-Learning and in the appendix.\nThe proof would work if the reduction were to a version of AI-by-Learning where the distribution D is arbitrary rather than being a distribution of situation-human behavior tuples. However, if that"}, {"title": "4 Fundamental Barriers to Intractability Results for AGI", "content": "It is sufficient to show that the reduction does not work to point out that the D on p. 22 is arbitrary but denotes a particular structured distribution in the context of the informal AI-by-Learning problem. This issue is not easily fixed.\nHowever, we believe that it is conceptually possible to try to reduce known hard problems to the informal version AI-by-Learning (though it would of course end up not being actually possible if AI-by-Learning is not NP-hard). We see two main challenges to that project that were not addressed in the paper."}, {"title": "4.1 Challenge 1: Mathematically characterizing the distribution D in AI- by-Learning", "content": "The authors define the distribution D in the context of AI-by-Learning informally as the distribution of situation-behavior tuples one would observe in humans. A reduction from a mathematical problem to a problem involving the distribution D could not be proven mathematically without a rigorous mathematical definition of D, although it could perhaps be checked empirically."}, {"title": "4.1.1 A related reductio", "content": "Note that, in the proofs in the paper currently, \"AGI\" could be replaced with \"image recognition in ImageNet\" without altering the mathematical structure of the proofs, implying that learning image classification on ImageNet is intractable, although it clearly is not [KSH12]. This reductio ad absurdum is a different way to see that the proof is incorrect."}, {"title": "4.1.2 Is the AI-by-Learning D \u201calmost\" arbitrary?\"", "content": "One might counter that the AI-by-Learning D is \"almost\" arbitrary, and attempt to repair the proof that way. However, far from being arbitrary, the AI-by-Learning D is highly structured, as we argued above."}, {"title": "4.2 Challenge 2: Are there subsets of situation-behaviour tuple data that are not learnable?", "content": "One might attempt to repair the proof by claiming that there are subsets of the set of situation-behaviour tuple data where the behaviour is generated by a non-invertible mechanism.\nFor example, one might argue that humans are able to execute arbitrary (or near-arbitrary) algorithms, and a subset of the situation-behaviour tuples would consist of input-output pairs for arbitrary Turing Machines.\nThere are a number of challenges with this approach to repairing the proof. The key issue is that it is far from obvious that humans can execute an arbitrary algorithm in their minds.\n1. Humans' working memory is limited. This means that humans would often use pen and paper to execute algorithms. If the intermediate steps are included in the data, the learning problem may become easier. If the intermediate steps are excluded, the learning problem may become unnatural: for example, it is obvious that it is not possible to learn to predict the output of a human's using a one-time pad [Aar13] if the pad is secret and excluded from the training data, but this is not a natural or interesting problem. In general, it is obvious that if one subsets the training data adversarially, learning would not be possible. One might make distinctions between"}, {"title": "4.2.1 Repairing the proof", "content": "To repair the proof by addressing this challenge, one must argue that the non-learnable subsets of the data are \"interesting\": that the learning problem there is of interest in itself."}, {"title": "4.3 Challenge 3: Inductive Biases", "content": "The No Free Lunch (NFL) Theorems in machine learning [AAPV19] imply that, on an arbitrary problem, no model is necessarily better than another. However, for any particular (computable) function, a good model exists.\nA proof of the intractability of AI-by-Learning would likely need to contend with the fact that in practice, any learning algorithm would have a particular inductive bias, which may be particularly suitable to solving AI-by-Learning, even if a \"blind\" search would be intractable.\nFor example, it is believed that Convolutional Neural Networks (ConvNets) are particularly easy to train on natural image data because their inductive bias is conducive to working well on natural image data [WW23]. Since no AGI trained by AI-by-Learning currently exists (nor does AGI obtained in some other way exist), it is possible that we do not know and will never know of good inductive biases for training AGI. However, that is not, to our knowledge, proven to be mathematically impossible.\nIn [Gue24], we argue that, contra [BK20], there is evidence from the history of physics that is relevant to the question of whether, with appropriate inductive biases, a learning machine could learn to predict physical processes. To the extent that induction using automatic learning is possible, it is a contingent fact about the universe [Hum94]. However, to argue that induction is not possible in our particular university, one needs to explicitly bring in evidence about our universe.\nNote that accounting for inductive biases is a challenge, but inductive biases are not directly an issue in [VRGA+24]."}, {"title": "5 Conclusion", "content": "The abstract of [VRGA+24] claims to have proved that AGI through learning is intractable. In this paper, we have argued that this has not been proven. We have also not proven the negation: as the authors of [VRGA+24] correctly note, there is no proof that AGI is \u201cinevitable.\"\nHowever, we believe there are fundamental barriers to this style of proof, as outlined in Section 4."}]}