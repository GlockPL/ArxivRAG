{"title": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models", "authors": ["Pranshu Pandya", "Agney S Talwarr", "Vatsal Gupta", "Tushar Kataria", "Vivek Gupta", "Dan Roth"], "abstract": "Cognitive textual and visual reasoning tasks, such as puzzles, series, and analogies, demand the ability to quickly reason, decipher, and evaluate patterns both textually and spatially. While LLMs and VLMs, through extensive training on large amounts of human-curated data, have attained a high level of pseudohuman intelligence in some common sense reasoning tasks, they still struggle with more complex reasoning tasks that require cognitive understanding. In this work, we introduce a new dataset, NTSEBENCH, designed to evaluate the cognitive multi-modal reasoning and problem-solving skills of large models. The dataset comprises 2,728 multiple-choice questions comprising of a total of 4,642 images across 26 categories sampled from the NTSE examination conducted nationwide in India, featuring both visual and textual general aptitude questions that do not rely on rote learning. We establish baselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a comparison between open source and proprietary models, we propose four distinct modeling strategies to handle different modalities (text and images) in the dataset instances.", "sections": [{"title": "Introduction", "content": "Aptitude and reasoning tests have been essential for assessing intelligence and are considered strong indicators of problem-solving ability and abstract reasoning skills. Recent advancements in large language models (LLMs) have demonstrated their strong performance on IQ test questions, achieving high scores across five languages (King, 2023). These results suggest that LLMs are progressing towards some form of human intelligence at least in textual and language tasks.\nThe capabilities of LLMs models to perform on par with or exceed human baselines on various tasks such as question answering, sentiment classification, text generation, visual question answering, coding challenges, data analytics, mathematical reasoning -continue to improve (Srivastava et al., 2022; Bubeck et al., 2023). LLMs and VLMs have hence become the default benchmark for many zero or few shot text and vision-based tasks (Brown et al., 2020; Wei et al., 2022). Training on huge amount of data from different domains has allowed LLMs to acquire human level performance in exams such as SAT, GRE, AP courses and on platforms leetcode (Achiam et al., 2023).\nHowever, the results from (King, 2023) showed that LLMs performance is heavily skewed towards textual reasoning tasks like comprehension, textual analogies, opposites but do not perform good on other types of questions. These large models have achieved remarkable performance in many tests of human intelligence, they still fall short of human baselines in tasks requiring cognitive and logical thinking such as commonsense numerical & scientific reasoning, puzzles, and analogies (Lu et al., 2023; Wang et al., 2023b). This challenge is evident not only in textual scenarios but also prominently in visual contexts. Most existing visual and multi-modal reasoning datasets in the literature are domain-specific, focusing on fields such as science, engineering, and medicine (Yue et al., 2024; Zhang et al., 2024b; Sun et al., 2024) or they involve reasoning about real-world images. These datasets primarily assess tasks related to concrete scenarios or specific domains, which may not necessarily include spatial recognition, visual puzzle solving, abstract vision logic and pattern recognition. Visual puzzle tasks necessitate abstract and spatial reasoning abilities that are fundamentally different from those required for textual or language-based reasoning. However, none of the existing datasets explicitly focus on testing this aspect. In this study, we aim to address this research gap by introducing a novel benchmark dataset (NTSEBENCH) specifically crafted to evaluate the complex visual, textual, and multi-modal cognitive reasoning capabilities of large deep learning models such as LLMs and VLMs. Examples questions from the dataset are shown in Figure 1.\nNTSEBENCH is exclusively dedicated to establishing a benchmark for testing capabilities that do not rely on domain-specific knowledge or rote learning. Its primary contribution lies in evaluating the innate problem-solving skills inherent in human cognitive development and isolating where models lack by presenting well categorised data. NTSEBENCH comprises questions sourced from the nationwide talent search examination(NTSE) conducted in India by the directorate of education. It includes questions across various problem categories such as series, calendar & clock, direction sense, analogies, and more. These questions can be presented in text format, visual format, or both (multi-modal). We further evaluate the performance of recent LLMs and VLMs, including both Proprietary (Achiam et al., 2023; Reid et al., 2024) and open source models (Achiam et al., 2023; Touvron et al., 2023; Jiang et al., 2024; Wang et al., 2023a; Bai et al., 2023). This work makes the following contributions:\n\u2022 NTSEBENCH, a dataset to evaluate complex textual, visual and multi-modal cognitive reasoning capabilities with 2,728 questions in 26 different problem categories.\n\u2022 Set up bench mark baselines for multiple state of the art LLMs and VLMs, both open and proprietary source models.\n\u2022 Evaluate performance with distinct modelling strategies to handle multi-modal input handling for reasoning questions."}, {"title": "NTSEBENCH", "content": "The National Talent Search Examination (NTSE), administered by the National Council of Educational Research and Training (NCERT) in India since 1963, is a nationwide exam for 10th-grade students. The exam consists of two sections designed to assess a wide range of analytical skills: the Mental Ability Test (MAT) and the Scholastic Aptitude Test (SAT). The MAT section evaluates students' general aptitude, critical thinking, logical and spatial reasoning, and analytical problem-solving skills (for both textual and visual problems). In contrast, the SAT assesses their domain-specific knowledge in science and mathematics. All questions in the NTSE are multiple-choice (MCQ) with one correct option out of four. A set of questions may have a common direction, which could be common instructions/information about the problems. Questions and options can be text/image or a combination of both, i.e., multi modal. Students are given 2 hours to solve 100 questions for each section. We hence aim to create a dataset focused on cognitive reasoning abilities or MAT-type questions and developed a benchmark dataset called NTSEBENCH to evaluate LLMs and VLMs."}, {"title": "Dataset Sources", "content": "To create the dataset, we primarily utilized previous years' NTSE papers publicly released along with their solutions provided by Resonance \u00b9, a coaching institute in India that offers paid lessons to students to help them improve their NTSE scores. Additionally, we used NTSE preparation materials, such as a reference book titled A Modern Approach to Verbal and Non-Verbal Reasoning 2 which includes additional logical reasoning problems. We also incorporated content from the another book titled Study Guide for NTSE 3 to construct our dataset."}, {"title": "Extraction Pipeline", "content": "As outlined in the dataset sources, we used previous years' NTSE papers to create the dataset. We create a pipeline and extract questions from these sources with human intervention to monitor mistakes.\nThe data extraction pipeline involves first processing the PDF through MATHPIX OCR4 to generate a Word file, which was then manually corrected for any errors. Next, we used the DOCXLATEX 5 library to convert all equations into LaTeX expressions. Finally, we leveraged the PYMUPDF 6 library to extract all text and images, extracting (1) Textual data i.e. Direction(Extra guidance on the context of the question), the Question itself, the correct Answer option and the Solution alongside (2) Vision Based data i.e. relevant images which we segregate into Direction images, Problem images, Option images, and Solution images.\nA total of 2,728 MCQ (multiple-choice questions) comprising of a total of 4,642 images across 26 categories questions are extracted and NTSEBENCH is created along with the necessary metadata."}, {"title": "Dataset Details", "content": "Problem Categories: NTSEBENCH encompasses several broad problem categories, each designed to test a distinct set of skills. Questions from these categories frequently appear in NTSE exams year after year. The description of various categories is as in Appendix Table 5.\nProblem Sub categories The above categories are further subdivided based on the different kind of reasoning required. Table 1 lists each sub-category for Text Only questions and Text+Vision questions, along with the respective count for each category. As in Table 1 most categories are represented well in the dataset with mean number of questions being 104 across categories."}, {"title": "Benchmarking using LLMs and VLMS", "content": "Lets assume a single(ith) instance in the dataset is represented by $D_i$=$Q_i,O_i,S_i$, where Q represents the questions, O represents the options of the MCQ and S represents the solution to the question. $J\\in (T, I)$ represents the modality type which can be either text(T) or image(I)."}, {"title": "Modelling Strategies", "content": "To accommodate diverse modalities for question, option, and solution triplets, a single model supporting all combinations can be created by converting all images to text description via caption generation (Li et al., 2022, 2023b) or convert all text to images and rely on OCR capabilities of the model(Fujitake, 2024; Zhao et al., 2023; Shi et al., 2023) to under and answer the question correctly. Both of these modelling accuracy depends on other sub-tasks, i.e., caption generation and OCR.\nWhen questions and options incorporate images, we move into the domain of utilizing multiple-image inputs for model reasoning. This functionality is supported through APIs in models such as GPT-4o and Gemini(Reid et al., 2024)(proprietary). Due to these sub-task and input dependencies, in order to assess the reasoning capabilities of the underlying model and achieve a fair and comprehensive comparison of baseline models, we propose using four distinct modeling strategies."}, {"title": "Standard QA Model for Text-only questions", "content": "For instances where question type(J) for questions(Q), options(O) and solutions(S) is text(T), we use a standard text-based QA model like GPT3.5-Turbo or Llama3-70b(AI@Meta, 2024) or Mixtral8x7b(Jiang et al., 2024)."}, {"title": "Image-Only Model for all Questions", "content": "With increasing OCR capabilities of several models (Shi et al., 2023; Fujitake, 2024; Zhao et al., 2023) for both document and hand writing recognition, especially for printed and scanned text.\nWe propose a modelling approach where all Questions and Options are presented to the model as a single image. The single image is created by combining all necessary textual and visual content as it appears in the examination paper, i.e., a snapshot of the question and options be it textual or visual question.\nThe model is presented with a single image with a system prompt directing the model to generate the result in a specified format. The advantage of this modelling approach is that it can be applied for all modality types of $D_i$."}, {"title": "Interleaved model for Text with Multiple Images", "content": "In this approach, we integrate text with multiple images to create an interwoven context. This method involves placing related textual and visual elements in close proximity, enhancing the model's ability to draw connections between them."}, {"title": "Standard VQA model with Single Stitched Image and Annotated Text", "content": "Open-source models do not support the integration of text and images within a single prompt. To enable a fair comparison, we propose an alternative modeling strategy where all question and option images are stitched into a single image, labeled as Figure 1, Figure 2, etc., and accompanied by a textual prompt describing different parts of the image. The prompt refers to the images in a structured manner, guiding the model's attention to relevant visual details. This composite image is then used to test the model's performance, assessing its ability to understand and respond to questions based on the integrated visual and textual information."}, {"title": "Prompting Strategies", "content": "We mainly employed two main modelling stragegies for setting up all the baselines on the proposed dataset, (A) Zero Shot and (B) Few shot, both using Chain of Thought (Wei et al., 2022) prompting to get better results.\nZero Shot COT: Model is given prompt containing question and corresponding options and asked to choose the right option. The model is also asked to provide an reasoning behind the choice made by the model. Few Shot COT: In few shot chain of thought (COT), few samples(N) with question, options and solution triplets($D_i$) are added to the prompts before asking the question under test. N, the number of exemplars are chosen keep in mind the token length supported by the model."}, {"title": "Implementation Details", "content": "We evaluate NTSEBENCH using multiple open and closed source LLMs (Achiam et al., 2023; Touvron et al., 2023; Team et al., 2023) and VLMS (Reid et al., 2024; Bai et al., 2023; Dong et al., 2024; Wang et al., 2023a). We used low temperature setting to promote reproducibility."}, {"title": "Results and Analysis", "content": "Our experiments answer the following questions:\n\u2022 How well do large deep learning models perform on advanced textual, visual, and multimodal reasoning questions? How challenging is cognitive reasoning for the current state-of-the-art models?\n\u2022 Are proprietary models superior to opensource models, and by what margin? Are there specific categories where proprietary models significantly outperform open-source models?\n\u2022 Do different modeling strategies affect a model's accuracy? Does OCR impact the reasoning accuracy of models?"}, {"title": "Results", "content": "Results for text only questions using Standard QA and Image Only(implicit OCR) modelling strategy are shown in Table 3. For Standard QA strategy we report results for zero shot setting in all eight models listed in the section above. For few shot setting, three models are excluded(CogVLM-2, Qwen-VL-Chat, InternLM-XComposer2) from the results. For Image Only modelling results are reported for five models which suport vision or multi-modal inputs for zero shot results. For some categories the solution contains Columns in the table refers to different types of questions such as SER (Series), ALP (Alphabet Test), ODO (Odd one out), ANA (Analogy), COD (Coding-Decoding), NUM (Number and Ranking), BLR (Blood Relation), MTO (Mathematical Operations), PUZ (Puzzle Test), SYL (Syllogisms), STC (Statement & Conclusions) and DAT (Data Sufficiency).\nResults for multi-modality question are reported in Table 4. Results are reported using three different modelling strategies listed in the section above namely Interleaved, Image Only and Standard VQA. Column in the table refer to different categories of visual text reasoning questions in NTSEBENCH such as DIR (Direction Sense), VEN (Venn Diagrams), TIM (Time and Clock), MIS (Missing Character), NVS (Non-Verbal Series), NVO (Non-Verbal odd one out), NVA (Non-Verbal Analogy), INC (Incomplete Figure), MIR (Mirror, Water and Images), CUB (Cube and Dice), PAP (Paper Folding & Cutting), EMB (Embedded Figure), FIG (Figure Partition), DOT (Dot Problem).\nProprietary models outperform Open Source models. From results in table 3 and 4, we can clearly observe that Proprietary models such as Gemini Pro 1.5 and GPT-4o, outperform other open source models in nearly every category question category. Proprietary models are generally twice as accurate as open-source models on NTSEBENCH questions, including both text-based and multi-modal questions, across all the different modeling strategies proposed in the section above. Results from Gemini Pro 1.5 outperform GPT-4o for the majority of modeling strategies on both text-based and multi-modal questions. Proprietary models undergo regular updates with refined weights tailored to specific tasks, as discussed in the literature (Reid et al., 2024; Achiam et al., 2023). Consequently, we posit that proprietary models fine-tuned on domain-specific reasoning datasets (Lu et al., 2023; He et al., 2024) have acquired advanced reasoning capabilities that contribute to their enhanced accuracy.\nLLAMA-3-70B is the best-performing opensource model for text-only questions, while QwenVL-Chat prevails as the top open-source VLM model for multi-modal questions.\nModelling Strategy is important. For text-only questions, the Standard QA strategy clearly outperforms Image Only modeling. Introducing OCR on top of reasoning tends to confuse models or exacerbate the difficulty of the task. This effect is particularly noticeable with smaller open-source models, which struggle to accurately extract characters and integrate them into context. However, proprietary models like GPT-4o and Gemini-Pro still achieve superior results using Image Only compared to open-source models employing Standard QA or text-only processing alone.\nFew-shot results present a mixed picture: while models like Mixtral and GPT-4o show improved performance with added exemplars, others experience a significant decline. We observe from tables 3 and 4 that proprietary models generally exhibit a smaller decline in performance compared to open-source models in such scenarios.\nInterleaving text and images performs better than Standard VQA and Image Only strategy for most categories. This highlights the importance of presenting text and images separately and in a more fine-grained manner, providing appropriate context, and integrating the image with text as distinct entities. This also highlights that decomposing the problems help. Our results in Table 4 show that such an approach can significantly enhance and improve the performance of Vision-and-Language Models (VLMs). GeminiPro (Reid et al., 2024) achieves the highest accuracy, highlighting the advanced capabilities of Gemini models in reasoning about different parts of images.\nFrom table 4, we can clearly see that few-shot prompting consistently performs poorly for all VLM models compared to zero-shot prompting. This suggests that replicating or inferring underlying reasoning from multi-modal data is notably challenging.\nMulti-modal reasoning is significantly harder. Comparing the best and worst performing models in Tables 3 and 4, it's evident that multi-modal reasoning is considerably more challenging than textual reasoning for current state-of-the-art VLMs. The highest accuracy on multi-modal questions is below 35%, whereas for textual reasoning, the best-performing model achieves an accuracy of over 62%. This underscores the significant gap in reasoning capabilities between state-of-the-art VLMs and their LLM counterparts, highlighting the necessity for improved architectures and datasets tailored specifically for VLM models.\nQuestion category analysis. The results from Table 3 reveal that while LLMs generally perform better on the text-only subset of NTSEBENCH, the high standard deviation of 11 indicates significant variability in model performance across different question types. This variability may stem from overlaps between NTSEBENCH and other opensource datasets, suggesting that there are still areas where LLMs exhibit limitations in reasoning capabilities. This is especially evident in Alphabet Test(ALP) category and Mathematical Operations(MTO) category, where the accuracy is more than two standard deviation away from mean accuracy of the model. This could also be attributed to the potential difficulty of these question types but that analysis has been left for future work.\nVLMs have shown poor performance across all categories for multi-modal questions, with the bestperforming model achieving correct answers only 34% of the time. Even the standard deviation for accuracy of the best model is 9, indicating that VLMs struggle more with certain question categories than others. Specifically, we observe that VLMs perform notably poorly on categories such as DOT (Dot Problems), NVS (Non-Verbal Series), and NVO (Non-Verbal Odd One Out). These categories require identifying correlations or patterns between multiple images, or recognizing emerging patterns in a sequence of images. This task is akin to identifying similar and evolving patterns in different parts of an image, and predicting the next possible pattern. While vision models excel at recognizing existing patterns, they struggle with predicting new patterns. Improving VLM model performance on these tasks may necessitate exploring different architectures and training methodologies in the future.\nNTSEBENCH presents a challenging task for SOTA LLMs and VLMs. Based on the findings in Tables 3 and 4, it is evident that the proposed dataset presents a challenging task for all state-of-the-art LLM and VLM models. None of the opensource models achieve accuracy exceeding 45% on text-only questions and 18% on multi-modal questions, with propriety models achieving 62% and 42% accuracy respectively. Many of the models tested didn't every reach random selection baseline of 24.52%(261 question had 5 options and 2467 question has 4 options), indicating that current LLMs and VLMs have a long way to go to attain better accuracy on these cognitive reasoning questions. Moreover, these findings highlight the potential for future advancements in addressing textual and visual reasoning questions for a variety of categories."}, {"title": "Related works", "content": "Texual and Multimodal Reasoning dataset. There exist multiple datasets to test domain specific(math, science, medicine) QA and reasoning abilities of LLMs and VLMs knowledge such as SciBench (Wang et al., 2023b), SciEval(Sun et al., 2024), MMMU (Yue et al., 2024), MathVista (Lu et al., 2023), JEEbench (Arora et al., 2023), MathVerse (Zhang et al., 2024a), OlympiadBench (He et al., 2024) and many others on based on real world images or other domains (He et al., 2020; Soni et al., 2022; Liu et al., 2023; Thrush et al., 2022; Li et al., 2023a). Most existing visual and multi-modal reasoning datasets in the literature are domain-specific M3Exam (Zhang et al., 2024b), RAVEN(Zhang et al., 2019), or they involve reasoning about real-world images (Liu et al., 2023; Wang et al., 2024; Thrush et al., 2022; Li et al., 2023a) with basic commonsense reasoning questions such as CLEVR (Johnson et al., 2017). However, current research has not thoroughly explored the capabilities of large models in addressing cognitive reasoning problems for both textual and multimodal data. Datasets such as (Liu et al., 2023; Wang et al., 2024; Masry et al., 2022) may explore mathematical reasoning in visual context but don't allow for multiple images within the same prompt or reasoning across images, where finding a pattern within a sequence of images is required which is the case in many categories of NTSEBENCH like Paper Folding and Cutting, Embedded Figure, Figure Partition, Mirror/Water images etc. NTSEBENCH is different from all already existing datasets in the literature because it explicitly focuses on testing cognitive reasoning abilities of large deep learning models.\nZero shot and Few shot prompting engineering for textual and multi-modal input. Our work is also related to prompting design and prompt engineering for LLMs (Brown et al., 2020; Chen et al., 2023; Gupta et al., 2023; Khot et al., 2022; Wei et al., 2022; Sahoo et al., 2024; Ali et al., 2024) and VLMs (?Xu et al., 2024; Bai et al., 2023; Liu et al., 2024; Dai et al., 2024). There are numerous studies on multi-modal and vision Chain-of-Thought"}, {"title": "Conclusion and Future Work", "content": "We have developed a new dataset, NTSEBENCH, specifically designed to assess the advanced analytical and logical reasoning capabilities of large deep learning models (LLMs and VLMs). We also propose four distinct modeling strategies for handling multi-modal data (text and images) across different question types in NTSEBENCH. These strategies enable us to conduct a fair and comprehensive comparison between closed and opensource models using both zero-shot and few-shot scenarios. Our findings indicate that both LLMs and VLMs struggle with advanced visual reasoning tasks, with VLMs performing particularly poorly on multi-modal questions compared to LLMs on textual questions. Additionally, proprietary models consistently outperform open-source models, correctly predicting twice as many questions. Overall, our results underscore that NTSEBENCH poses significantly greater challenges for state-of-the-art LLMs and VLMs.\nFuture directions. (a) We did not used imageto-text conversion because most of images in NTSEBENCH are not real-world images, and caption generation models are typically trained on realworld images, leading to a domain shift. This results in subpar captions that are neither relevant nor self-explanatory. Because LLMs performed significantly better at reasoning task, we intend to utilize caption generators to investigate whether text-only models, incorporating image descriptions generated by caption models, can outperform VLMs. (b)Given the limited data available for cognitive reasoning questions, we plan to use data augmentation strategies to increase the number of samples. (c) We also aim to investigate the impact of shuffling multiple-choice question (MCQ) options on performance in reasoning tasks."}, {"title": "Limitations", "content": "While our dataset is new and sourced for different sources when compared to dataset already present in the literature, there still might be some overlap in reasoning questions especially in textual reasoning. So all the dataset instances might not be independent and exclusive. This dataset is solely created in English, so no other languages are represented, therefore we cannot analyze whether language variations can have a significant impact on the reasoning capabilities of these large models. Our modelling strategies were limited to zero shot and few shot COT prompting. We did not evaluate whether finetuning these large models on few examples from each of the categories would further improve results. This was due to limitation of both GPU resources and large cost of fine-tuning for Proprietary models. We plan to address all these limitation in future extension of this work."}, {"title": "Ethics Statement", "content": "As authors of this work, we affirm that our research adheres to the highest ethical standards in both its conduct and publication. We have carefully considered and addressed various ethical considerations inherent in computational linguistics methodologies to ensure responsible and fair practices. We prioritize transparency by providing detailed information to facilitate the reproducibility of our results. This includes sharing our code and datasets, which are sourced from publicly available datasets and handled in compliance with ethical guidelines established by the original authors. While our paper accurately reflects our experimental findings, we acknowledge the inherent variability associated with large language models, which we mitigate by maintaining a fixed temperature setting. We provide comprehensive descriptions of our annotations, dataset splits, models used, and prompting methods employed, ensuring the reproducibility of our work. For grammar correction, we utilize AI-based writing assistants, and for coding, we leverage tools like Copilot. Importantly, the genesis of our research ideas and the execution of our study were entirely independent of AI assistance."}]}