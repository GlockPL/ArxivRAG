{"title": "RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation", "authors": ["Changzhi Zhou", "Xinyu Zhang", "Dandan Song", "Xiancai Chen", "Wanli Gu", "Huipeng Ma", "Yuhang Tian", "Mengdi Zhang", "Linmei Hu"], "abstract": "Code generation has attracted increasing attention with the rise of Large Language Models (LLMs). Many studies have developed powerful code LLMs by synthesizing code-related instruction data and applying supervised fine-tuning. However, these methods are limited by teacher model distillation and ignore the potential of iterative refinement by self-generated code. In this paper, we propose Adaptive Critique Refinement (ACR), which enables the model to refine itself by self-generated code and external critique, rather than directly imitating the code responses of the teacher model. Concretely, ACR includes a composite scoring system with LLM-as-a-Judge to evaluate the quality of code responses and a selective critique strategy with LLM-as-a-Critic to critique self-generated low-quality code responses. We develop the RefineCoder series by iteratively applying ACR, achieving continuous performance improvement on multiple code generation benchmarks. Compared to the baselines of the same size, our proposed RefineCoder series can achieve comparable or even superior performance using less data.", "sections": [{"title": "1 Introduction", "content": "Code generation, also called program synthesis, is a key application area of Large Language Models (LLMs) and has attracted significant attention from the research community (Gulwani et al., 2017; Chen et al., 2021). Numerous studies focus on developing code-specific pre-trained models, such as CodeLlama (Roziere et al., 2023), DeepSeek-Coder (Guo et al., 2024), and Qwen-Coder (Hui et al., 2024). These LLMs demonstrate strong capabilities in code understanding owing to their pre-training on large-scale code corpus. However, they must undergo post-training to become user-friendly and effective for code-related tasks.\nAs a popular post-training technology, Distillation-based Fine-Tuning (DFT) leverages code instruction data synthesized by powerful teacher models (often proprietary LLMs like GPT-4), as shown in Figure 1. Many works (Zhang et al., 2024b) based on this paradigm have been proposed recently. For example, Code Alpaca (Chaudhary, 2023) initially establishes this synthesis paradigm using Self-Instruct (Wang et al., 2023). Subsequently, Code-related Evol-Instruct (Luo et al., 2024) prompts LLMs to evolve more intricate code instructions, while OSS-Instruct (Wei et al., 2024b) generates more realistic code instructions based on open-source code snippets from The Stack (Kocetkov et al., 2022). In contrast, OpenCodeInterpreter (Zheng et al., 2024a) enables code LLMs to learn from compiler diagnostics and human feedback by constructing multi-turn feedback data. These diverse synthesis strategies have significantly advanced the development of code LLMs. However, this paradigm of teacher model distillation inevitably suffers from a critical limitation: the performance of the student model largely relies on the teacher model, ignoring the potential of iterative refinement by self-generated code."}, {"title": "2 Related Work", "content": "Apart from the research on code instruction fine-tuning, some studies (Chen et al., 2023; Huang et al., 2023a; Chen et al., 2024, 2025) have explored the ability of LLMs to refine self-generated code during inference. For example, Self-Debugging (Chen et al., 2024) iteratively debugs self-generated code using feedback from the code executor. However, these methods that freeze parameters are essentially a form of prompt engineering with external feedback. They cannot improve the intrinsic one-off code generation ability of LLMs. Besides, multiple calls to LLMs increase the inference latency.\nIn this paper, we propose Adaptive Critique Refinement (ACR), a novel fine-tuning paradigm to enhance the intrinsic one-shot code generation ability of LLMs, and on the basis of which we develop a series of code LLMs named RefineCoder. ACR goes beyond traditional teacher distillation by using both self-generated code and external critique to refine the model. Specifically, ACR tailors the distinct new fine-tuning dataset by first scoring and critiquing the self-generated multiple code responses, and then adaptively forms new samples according to the scoring and critiquing results. The two types of samples are shown in Figure 1. Based on the new dataset, we fine-tune RefineCoder models. Finally, we iteratively applying the above ACR process to achieve continuous improvement in code generation capabilities of RefineCoder. Notably, in ACR, we design a composite scoring system and a selective critiquing strategy to better score code responses and selectively critique low-quality code. The former combines LLM-as-a-Judge, an Elo rating mechanism (Elo and Sloan, 1978), and a code executor for accurate and comprehensive evaluation. The latter adaptively constructs two types of data by comparing the scores of self-generated code with those of original code. When self-generated code performs worse than the original code, an LLM-as-a-Critic is introduced to provide specific critiques.\nWe conduct iterative ACR based on DS-Coder-6.7B-Base (Guo et al., 2024) and Qwen2.5-Coder-7B-Base (Hui et al., 2024), resulting in the RefineCoder series. After three iterations, RefineCoder-DS-6.7B and RefineCoder-QW-7B achieve average pass@1 improvements of 2.4 and 3.0, respectively, on the HumanEval (+), MBPP (+), LiveCodeBench, and the BigCodeBench-hard benchmark (Chen et al., 2021; Austin et al., 2021; Liu et al., 2023; Jain et al., 2024; Zhuo et al., 2024).\nThe key contributions of this paper are as follows:\n1) We propose Adaptive Critique Refinement (ACR), a novel fine-tuning paradigm that refines code LLMs with self-generated code and external critique, on the basis of which we develop a series of strong code LLMs (RefineCoder).\n2) To ensure the efficacy of ACR, we design a composite scoring system with LLM-as-a-Judge and a selective critique strategy with LLM-as-a-Critic to score and critique self-generated code.\n3) Experimental results from the RefineCoder series show that iterative ACR continuously enhances code generation performance. After three iterations, the RefineCoder series achieves comparable or even superior performance than baselines of the same size while requiring less data.\nLLMs for Code Generation LLMs have shown exceptional code understanding abilities due to extensive pre-training on code-related corpora. Numerous models like GPT-40 (OpenAI, 2024a), Gemini (Google, 2024), Claude-3.5 (Anthropic, 2024), Qwen2.5-Coder (Hui et al., 2024), and DeepSeek-Coder (Guo et al., 2024) exhibit strong performance on code generation benchmarks. Recently, the release of OpenAI 01 (OpenAI, 2024b) and DeepSeek-R1 (DeepSeek-AI, 2025) has spurred a surge in research on deep reasoning LLMs, achieving expert-level performance on competitive programming problems (e.g., CodeForces) and further advancing LLMs in the code domain.\nDistillation-based Code Fine-Tuning Unlike proprietary models, many studies focus on fine-tuning open-source code pre-trained models, which has greatly contributed to the rapid development of the code generation field. A key technique for achieving this is distilling data from teacher models. Code Alpaca (Chaudhary, 2023) introduces Self-Instruct (Wang et al., 2023) to distill GPT-3. Following this, WizardCoder (Luo et al., 2024) evolves more complex code instruction data using Evol-Instruct (Xu et al., 2024). MagiCoder (Wei et al., 2024b) proposes OSS-Instruct, where the model generates instructions and code responses sequentially based on open-source code snippets, thus creating more practical code data. In contrast to OSS-Instruct, InverseCoder (Wu et al., 2024) reverses the order of instruction and code response generation. WaveCoder (Yu et al., 2024) constructs a multi-task code dataset, enhancing the model's"}, {"title": "3 Methodology", "content": "versatility. Besides, OpenCodeInterpreter (Zheng et al., 2024a) builds multi-turn code data, enabling the model to learn from execution feedback and human feedback. However, these methods aim to enable the model to learn by imitating the teacher, overlooking the potential for refinement through self-generated code.\nIterative Self-Refinement of LLMs Iterative self-refinement refers to LLMs enhancing themselves iteratively by utilizing self-generated responses with the help of external signals. One line of research (Huang et al., 2023b; Madaan et al., 2023; Hu et al., 2024) focuses on enabling self-correction during the inference stage by iteratively calling LLMs and incorporating external signals. In the code domain, CodeT (Chen et al., 2023), Self-Debugging (Chen et al., 2024), and LDB (Zhong et al., 2024) follow this approach. However, these prompt engineering methods with external feedback cannot improve the intrinsic capabilities. Another line of research focuses on iteratively training the model using self-generated outputs to enhance its intrinsic capabilities (Dong et al., 2025; Yuan et al., 2024; Kim et al., 2025). These methods typically rely on preference learning, such as DPO (Rafailov et al., 2023). CodeLutra (Tao et al., 2024) has successfully applied this approach to the code domain, but it heavily depends on golden labels, which limits its applicability. In contrast to the works above, we propose the ACR method, which achieves iterative self-refinement by only using a simple Supervised Fine-Tuning (SFT) loss. This approach is orthogonal to prompt engineering methods like Self-Debugging and is more efficient and generalizable than CodeLutra."}, {"title": "3.1 Overview", "content": "Adaptive Critique Refinement (ACR) aims to improve code LLMs by refining existing dataset with self-generated code and external critiques. The iterative application of ACR facilitates continuous improvement in code generation performance. As illustrated in Figure 2, ACR starts with an existing dataset and a code model fine-tuned on it. The method updates the existing dataset through a process of sampling, ranking, and refining. The updated dataset is used for the next round of fine-tuning, iteratively improving the model's code generation capabilities.\nIn the following sections, we will introduce two key components of the aforementioned pipeline: the composite scoring system for ranking and the selective critiquing strategy for data refining. Finally, we will introduce the iterative training and the resulting RefineCoder model."}, {"title": "3.2 Composite Scoring System with LLM-as-a-Judge", "content": "A comprehensive and accurate evaluation of code response quality is the foundation for effective ACR. Previous works (Chen et al., 2023; Zhang et al., 2024a) rely on generating test cases to evaluate code quality. However, this method has issues with inaccurate test case generation (Jain et al., 2024), inability to handle special judge (Quan et al.,"}, {"title": "3.3 Selective Critiquing Strategy with LLM-as-a-Critic", "content": "After obtaining the sorted code responses, we design the selective critiquing strategy as data construction engine of ACR. It effectively utilizes high-quality and low-quality self-generated code in distinct ways, while further providing external critiques for the low-quality data. This strategy resembles the process of a student first solving problems and then critiquing their answers based on provided solutions, rather than directly imitating the solutions (Wang et al., 2025).\nConcretely, when all self-generated code responses are of lower quality than the original code in the existing dataset, namely $y^w$ is $y^o$ and $y^{l1}$ is the self-generated code, LLM-as-a-Critic is used to critique $y^{l1}$, using $y^o$ as a reference (The prompt is shown in Figure 8):\n$c = Critic(x, y^o, y^{l1}).$ (6)\nOtherwise, if $y^w$ is the self-generated code response, we directly construct new single-round data. The data update rules are as follows:\n$(x, y) \\rightarrow {\\begin{cases} (x, y^{l1}, c, y^w), & \\text{if } y^w \\text{ is } y^o, \\\\ (x, y^w), & \\text{otherwise}. \\end{cases}}$ (7)\nIf the data to be updated comes from a two-round critique rather than a single-round one, only the instruction and the final code response will be considered. Selective refining strategy updates all data to comprise a new instruction dataset $D_{t+1}$."}, {"title": "3.4 Iterative Training", "content": "The previous sections describes the process of a single ACR in Figure 2. Next, we describe the iterative ACR process and define RefineCoder:\n* RefineCoder Iter0 ($M_0$): The code model trained using the initial SFT dataset $D_0$.\n* RefineCoder Iter1 ($M_1$): The code model trained using dataset $D_1$, where $D_1$ is obtained by refining $D_0$ using $M_0$.\n* RefineCoder Itert ($M_t$): The code model trained using dataset $D_t$, where $D_t$ is obtained by refining $D_{t-1}$ using $M_{t-1}$.\nWe fine-tune the original base model from scratch in each iteration to prevent overfitting, consistent with previous iterative training works (Zelikman et al., 2022; Wang et al., 2024a; Dong et al., 2025)."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Benchmarks", "content": "We use HumanEval (+) (Chen et al., 2021) and MBPP(+) (Austin et al., 2021; Liu et al., 2023) to assess the fundamental coding ability of code model. Meanwhile, we use LiveCodeBench (Jain et al., 2024) and BigCodeBench-hard (Zhuo et al., 2024) to measure the advanced coding ability of code model. We restrict data usage to the range from 240701 to 250201 of the LiveCodeBench to maintain its contamination-free nature."}, {"title": "4.2 Baselines", "content": "* Proprietary Models: Gemini-1.5-Pro-002 (Google, 2024), Claude-3.5-Sonnet-20240620 (Anthropic, 2024), and GPT-40-20240806 (OpenAI, 2024a).\n* Open-source Models 7B+ Scale: Qwen2.5-Coder-32B-Instruct (Hui et al., 2024), DeepSeek-V3 (DeepSeek-AI, 2024), and Llama3.3-70B-Instruct (Grattafiori et al., 2024).\n* Open-source Models ~7B Scale: DSCoder-6.7B Series (Guo et al., 2024), MagiCoder-S-DS-6.7B (Wei et al., 2024b), OpenCodeInterpreter-DS-6.7B (Zheng et al., 2024a), WaveCoder-Ultra-6.7B (Yu et al., 2024), and Qwen2.5-Coder-7B Series (Hui et al., 2024)."}, {"title": "4.3 Initial SFT Dataset Do", "content": "The existing datasets vary in quality and carry the risk of data leakage (Wang et al., 2024b; Yu et al., 2024). To ensure the accuracy and reliability of the experimental results, we constructed a high-quality SFT dataset by combining the Evol-Instruct and SelfCodeAlign (Wei et al., 2024a). Concretely, we first prompt GPT-40 to generate code-related concepts and corresponding programming instructions from Python code snippets. These code snippets have been preprocessed to ensure high quality and contamination-free. Then, we prompt GPT-40 again to iteratively evolve these instructions using Evol-Instruct strategy. Finally, we prompt GPT-4o to generate code responses for the instructions. Through this pipeline, we obtained an 80K high-quality and diverse python instruction dataset. See Appendix C for detailed data construction prompts."}, {"title": "4.4 Implement Details", "content": "Iterative Training We employ DSCoder-6.7B-Base and Qwen2.5-Coder-7B-Base as the base pre-trained models. To obtain an initial code model $M_0$, we fine-tune DSCoder-6.7B-Base on the 80K initial SFT dataset, resulting in RefineCoder-DS-6.7B (Iter0). Furthermore, since Qwen2.5-Coder-Base exhibits strong code understanding and generation capabilities, we accelerate our experimental iterations by randomly sampling 20K examples from the same dataset for fine-tune it, yielding RefineCoder-QW-7B (Iter0). The two RefineCoder models undergo three iterative training using their respective datasets under our ACR framework. In the ACR pipeline, the model self-samples 5 code responses with a temperature of 0.7. We use Qwen2.5-32B-Instruct (Hui et al., 2024) as the Judge and Critic. We set the number of training epochs to 2, the global batch size to 64, and the learning rate to 5e-6, employing the AdamW optimizer with a cosine learning rate decay strategy. All our training is performed using 16 A100-80G GPUs, utilizing the LLaMA-Factory (Zheng et al., 2024b).\nEvaluation We use the Pass@1 metric to evaluate the performance of model on the benchmark. For baseline results, we prioritize those from the benchmark leaderboard 5 6 7 or the original papers. If unavailable, we evaluate them locally using the same settings as the RefineCoder series."}, {"title": "4.5 Experimental Results", "content": "As shown in Table 1, the RefineCoder series achieves impressive performance gains on various code generation benchmarks. While the initial RefineCoder-DS-6.7B and RefineCoder-QW-7B exhibit moderate performance, the two models achieve an average Pass@1 improvement of 2.4 and 3.0, respectively, on all benchmarks after three iterations of refinement. Furthermore, it is worth noting that the RefineCoder series is fine-tuned with a very limited amount of data (80K or 20K). Despite this, the RefineCoder models (Iter3) still outperform or match baselines of the same size, which are fine-tuned with more extensive data.\nConcretely, data refinement improves the fundamental programming abilities of the RefineCoder series, as demonstrated by continuous performance gains over iterations on HumanEval, HumanEval+, and MBPP+. Notably, RefineCoder-QW-7B shows a remarkable 7.9 point increase (Iter0 -> Iter3) in"}, {"title": "5 Further Study", "content": null}, {"title": "5.1 Data Leakage Analysis", "content": "In Table 1, we observe that while the RefineCoder (Iter3) surpasses or matches the baselines on average pass@1, it still lags behind the state-of-the-art baseline of the same size on HumanEval (+) and MBPP+. To better explore this performance gap, we investigate potential data leakage in the datasets used by the baselines.\nConcretely, we collect the open-source datasets used by MagiCoder-S-DS-6.7B, OpenCodeInterpreter-DS-6.7B, and WaveCoder-Ultra-6.7B (the datasets for the other baselines are not public) and evaluate data leakage using the"}, {"title": "5.2 The effectiveness of Composite Scoring System", "content": "The performance improvement through iterative training preliminarily validates the effectiveness of the scoring system. In this section, we further verify it by enabling the model sampling 10 code responses for each programming question in benchmarks and scoring them. As shown in Table 3, the performance of the highest-scoring code far exceeds that of the lowest-scoring code, while the scoring system maintains a low error rate. This clearly validates its effectiveness."}, {"title": "5.3 The effectiveness of Selective Critique Strategy", "content": "To validate the effectiveness of the selective critiquing strategy in ACR, we conduct ablation experiments. As shown in Figure 3, for each iteration, we construct two variants of the RefineCoder without the 3.1 module in Figure 2, and another without the 3.2 module. During the iteration process, ablating either module leads to a performance drop in both RefineCoder-QW-7B and RefineCoder-DS-6.7B, indicating the effectiveness of our strategy. Furthermore, removing the second-round critique data has a greater impact on performance. We believe this action prevents the model from reflecting on self-"}, {"title": "5.4 Multilingual Code Generation", "content": "As shown in Table 4, we also evaluate the out-of-distribution (OOD) code generation performance on multilingual benchmark MultiPL-E (Cassano et al., 2022), despite fine-tuning on a Python-only dataset. After one and three iterations, RefineCoder-DS-6.7B and RefineCoder-QW-7B achieve optimal average performance, surpassing the initial model by 1.0 and 1.9 points, respectively. In particular, RefineCoder-QW-7B (Iter3) achieves best results among half of the programming languages. This demonstrates that the effectiveness of our ACR method generalizes well to the OOD code generation domain."}, {"title": "5.5 Evaluation with External Feedback", "content": "Iterative ACR not only improves the one-off code generation performance of RefineCoder but also endows it with the ability to correct errors based on feedback. Following Zheng et al. (2024a), we design two types of external feedback to evaluate this ability of RefineCoder: 1) Execution Feedback: Model refines its self-generated erroneous code based on execution results from the executor. 2) Human Feedback: GPT-40 first analyzes the programming question, initial error code, and execution results to generate improvement suggestions that mimic human thinking. Model then re-"}, {"title": "5.6 Preference Optimization", "content": "During the iterative process, a substantial amount of preference data is generated. For each programming instruction, we select the top-1 and bottom-1 ranked responses to form preference pairs, thereby exploring the potential of preference learning. As shown in Table 5, we use DPO (Rafailov et al., 2023) and ORPO (Hong et al., 2024) to optimize the base and SFT models. We observe the opposite results due to the differences in preference strategies. DPO leads to a significant performance decline due to the intrinsic risk of reducing the likelihood of correct code during training (Feng et al., 2024; Tao et al., 2024; Pal et al., 2024). In contrast, ORPO builds upon the SFT loss to maximize the likelihood of correct code, resulting in improved performance. The results are consistent with those presented in the CodeLutra (Tao et al., 2024), reflecting the indispensability of SFT loss and the critical impact of preference strategy design on performance."}, {"title": "6 Conclusions", "content": "In this paper, we propose the Adaptive Critique Refinement (ACR) method to iteratively refine code LLMs using self-generated code and external critique, which breaks away from the traditional teacher distillation paradigm and improves the intrinsic one-off code generation ability. We design a composite scoring system and a selective critiquing strategy. These two components are centred around LLM-as-a-Judge and LLM-as-a-Critic to evaluate and critique self-generated code. This simulates the process where a student solves a problem independently and then refines it by comparing it with the reference answer. We develop the RefineCoder-DS-6.7B and RefineCoder-"}, {"title": "7 Limitations", "content": "QW-7B models and demonstrate the effectiveness of iterative ACR on HumanEval (+), MBPP (+), LiveCodeBench, and BigCodeBench-hard. Further studies reveal the impact of the components in ACR and demonstrate its OOD code generation ability.\nAlthough Adaptive Critique Refinement (ACR) and the associated RefineCoder demonstrate significant improvements in the code generation task, several limitations remain. First, ACR is primarily designed to refine code responses based on programming instructions, necessitating an initial set of high-quality and diverse instructions. Therefore, a specialized code instruction generator is still required to make ACR more automated. Furthermore, while ACR can apply to other reasoning-intensive tasks, such as mathematics, this paper has not fully explored these domains."}, {"title": "A Elo Algorithm", "content": "Firstly, we initialize Elo score $E^i = 1000$ for each code response $y^i$. Then, we iteratively update the Elo scores by using the relative scores between any two responses. Taking the response pair $y^i$ and $y^j$ as an example, we obtain their match results $S^i$ and $S^j$:\n$S^i = \\frac{\\text{score}_{ij}^{\\text{judge}}}{\\text{score}_{ij}^{\\text{judge}} + \\text{score}_{ji}^{\\text{judge}}},$ (8)\n$S^j = \\frac{\\text{score}_{ji}^{\\text{judge}}}{\\text{score}_{ij}^{\\text{judge}} + \\text{score}_{ji}^{\\text{judge}}}.$ (9)\nThen we obtain the expected probabilities of winning $P^{ij}$ and $P^{ji}$ for $y^i$ and $y^j$:\n$P^{ij} = \\frac{1}{1 + 10^{\\frac{E^j - E^i}{400}}},$ (10)\n$P^{ji} = 1 - P^{ij}.$ (11)\nFinally, we update the Elo score:\n$E_{\\text{new}}^i = E^i + K \\times (S^i - P^{ij}),$ (12)\n$E_{\\text{new}}^j = E^j + K \\times (S^j - P^{ji}).$ (13)\nwhere $K = 32$ is a adjustment factor. After the iteration, we obtain the Elo score for each code response and then normalize it to derive the final judge score:\n$\\text{score}_{judge}^i = \\frac{E^i - \\min(E)}{\\max(E) - \\min(E)},$ (14)\nwhere $i \\in [0, n]$ and $\\text{score}_{judge}^i \\in [0, 1]$."}, {"title": "B Prompts for Judge and Critic", "content": "The prompt for pairwise LLM-as-a-Judge and LLM-as-a-Critic are shown in Figure 7 and 8."}, {"title": "C Prompts for constructing SFT Dataset", "content": "We called GPT-40 three times to create the SFT dataset, with the following prompts used:\n* Prompt for generating code-related concepts and programming instructions, as shown in Figure 9.\n* Prompts for evolving existing instructions, as shown in Figures 10, 11, 12, 13, 14, 15, 16, 17, 18.\n* For generating code responses, We directly feed the instructions to the model."}, {"title": "D Prompts for Evaluation", "content": "For Humaneval (+) and MBPP (+), we use the prompts designed by OpenCodeInterpreter (Zheng et al., 2024a); for Livecodebench and Bigcodebench-hard, we use the official prompts. When conducting evaluations with execution feedback, we have designed a prompt as illustrated in Figure 19. When executing human feedback, following OpenCodeInterpreter, we generate two prompts: one for generating improvement suggestions and the other for refine code according to human feedback, as shown in Figures 21 and 20."}, {"title": "E Data Leakage Analysis", "content": "We use 4-grams and 5-grams to compute the similarity between the instructions in the training set and the questions in the benchmark. And then we draw scatter plots by selecting the top-200 data points with the highest similarity from each dataset, as shown in Figure 5. The scatter plot visually illustrates the data leakage phenomenon in the dataset, primarily concentrated in the HumanEval (+) and the latter half of the MBPP (+). This makes it difficult for our model to surpass baselines on these two benchmarks. In contrast, on the leakage-free Live-CodeBench and BigCodeBench-hard benchmarks, our model can outperform all baselines."}, {"title": "F Scaling Law of Iterative ACR", "content": "We study the performance trends of RefineCoder-DS-6.7B obtained using ACR and the standard SFT method as the dataset size increases. For the SFT method, we directly trained the model using four datasets of different sizes. For RefineCoder series, we employ an iterative training strategy. For example, after obtaining RefineCoder-DS-10k by finetuning on the 10k dataset, we apply ACR to update this 10k dataset, and then randomly sample another 10k from the data pool to form a 20k dataset. The model is fine-tuned again to obtain RefineCoder-DS-20k. The experimental results are shown in Figure 6, as the dataset size increases, our model achieves greater performance improvements compared to the SFT model on HumanEval and MBPP. Moreover, the performance gap between the two models does not show any signs of narrowing, underscoring the impressive scaling capabilities of the iterative ACR method."}]}