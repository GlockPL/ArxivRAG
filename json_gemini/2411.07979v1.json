{"title": "Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization", "authors": ["Davide Buffelli", "Jamie McGowan", "Wangkun Xu", "Alexandru Cioba", "Da-shan Shiu", "Guillaume Hennequin", "Alberto Bernacchia"], "abstract": "Second-order optimization has been shown to accelerate the training of deep neural networks in many applications, often yielding faster progress per iteration on the training loss compared to first-order optimizers. However, the generalization properties of second-order methods are still being debated. Theoretical investigations have proved difficult to carry out outside the tractable settings of heavily simplified model classes \u2013 thus, the relevance of existing theories to practical deep learning applications remains unclear. Similarly, empirical studies in large-scale models and real datasets are significantly confounded by the necessity to approximate second-order updates in practice. It is often unclear whether the observed generalization behaviour arises specifically from the second-order nature of the parameter updates, or instead reflects the specific structured (e.g. Kronecker) approximations used or any damping-based interpolation towards first-order updates.\nHere, we show for the first time that exact Gauss-Newton (GN) updates take on a tractable form in a class of deep reversible architectures that are sufficiently expressive to be meaningfully applied to common benchmark datasets. We exploit this novel setting to study the training and generalization properties of the GN optimizer. We find that exact GN generalizes poorly. In the mini-batch training setting, this manifests as rapidly saturating progress even on the training loss, with parameter updates found to overfit each mini-batchatch without producing the features that would support generalization to other mini-batches. We show that our experiments run in the \u201clazy\u201d regime, in which the neural tangent kernel (NTK) changes very little during the course of training. This behaviour is associated with having no significant changes in neural representations, explaining the lack of generalization.", "sections": [{"title": "1 Introduction", "content": "Efficient optimization of overparameterized neural networks is a major challenge for deep learning. For large models, training remains one of the main computational and time bottlenecks. Much work has therefore been devoted to the development of neural network optimizers that could accelerate training, enabling researchers and engineers to iterate faster and at lower cost in their search for better performing models. Second-order optimizers, in particular, have been shown to deliver substantially faster per-iteration progress on the training loss [Martens and Grosse, 2015, Botev et al., 2017, George et al., 2018, Goldfarb et al., 2020, Bae et al., 2022, Petersen et al., 2023, Garcia et al., 2023], and much work has been done to scale them to large models via suitable approximations [Ba et al., 2017, Anil et al., 2021]. However, the generalization properties of second-order optimizers remain poorly understood. Here, we focus on the training and generalization properties of the Gauss-Newton (GN) method, which \u2013 in many cases of interest \u2013 also encompasses natural gradient descent (NGD) [Martens, 2020].\nTheoretical studies of generalization in GN/NGD have been limited to simplified models, such as linear models [Amari et al., 2021] or nonlinear models taken to their NTK limit [Zhang et al., 2019]. When applied to real-world networks and large datasets, GN/NGD has so far required approximations, such as truncated conjugate gradient iterations in matrix-free approaches [Martens et al., 2010], or block-diagonal and Kronecker-factored estimation of the Gauss-Newton / Fisher matrix [Martens and Grosse, 2015, Botev et al., 2017, George et al., 2018, Goldfarb et al., 2020, Bae et al., 2022, Petersen et al., 2023, Garcia et al., 2023]. Those approximations are exact only in the limit of constant NTK [Karakida and Osawa, 2020], in which models cannot learn any features [Yang and Hu, 2021, Aitchison, 2020]. To our knowledge, the only case in which exact and tractable GN updates have been obtained is that of deep linear networks [Bernacchia et al., 2018, Huh, 2020], which \u2013 despite exhibiting non-trivial learning dynamics [Saxe et al., 2013] \u2013 cannot learn interesting datasets nor yield additional insights into generalization beyond the linear regression setting. Critically, the use of necessary approximations makes it difficult to understand how much of the observed generalization (or lack thereof) can be attributed to the GN method itself, or to the various ways in which it has been simplified.\nHere, we derive an exact, computationally tractable expression for Gauss-Newton updates in deep reversible networks [Dinh et al., 2015, Mangalam et al., 2022]. In reversible architectures made of stacked, volume-preserving MLP-based coupling layers (which we call RevMLPs), we show that it is possible to analytically derive a specific form of a generalized inverse for the network's Jacobian. This generalized inverse enables fast, exact GN updates in the overparameterized regime. We highlight that, in contrast to the work of Zhang et al. [2019], Cai et al. [2019], Rudner et al. [2019], Karakida and Osawa [2020], we do not assume constant NTK, instead we only require the NTK to remain non-singular during training [Nguyen et al., 2021, Liu et al., 2022, Charles and Papailiopoulos, 2018] as, for example, in the mean-field limit [Arbel et al., 2023]. Equipped with this new model, we study for the first time the generalization behaviour of GN in realistic settings. In the stochastic regime, we find that GN trains too well, overfitting single mini-batch at the expense of impaired performance not only on the test set, but also on the training set. To understand this severe lack of generalization, we conduct a careful examination of the model's neural tangent kernel and show that the NTK remains almost unchanged during training, and that the neural representations that arise from after training are not different from those set by the network's initialization. Thus, GN tends to remain in the \"lazy\" regime [Jacot et al., 2018, Chizat et al., 2019], in which representations remain close to those at initialization, lacking generalization.\nIn summary:\n\u2022 We show that GN updates computed with any generalized inverse of the model Jacobian results in the same dynamics of the loss, provided that the NTK does not become singular during training (Theorem 4.3).\n\u2022 We derive an exact and tractable generalized inverse of the Jacobian in the case of deep reversible neural networks (Proposition 4.4). The corresponding GN updates have the same complexity as gradient descent.\n\u2022 We study the generalization properties of GN in models up to 147 million parameters on MNIST and CIFAR-10, and we show that neural representations do not change during training, as the model remains in the \"lazy\" regime."}, {"title": "2 Related Work", "content": "Exact vs approximate Gauss-Newton in deep learning. Previous work on second-order optimization of deep learning models focused on either Natural Gradient Descent (NGD), or Gauss-Newton"}, {"title": "3 Background", "content": "We provide a brief introduction to Gauss-Newton and Generalized Gauss-Newton. Given input and target data pairs (x, y) \u2208 Rdx \u00d7 Rdy and parameters \u03b8 \u2208 R\u00ba, the loss is a sum over a batch B = {(xi, Yi)=1} of n data points\n\\begin{equation}\n\\mathcal{L}(\\theta) = \\sum_{i=1}^{n} l \\left(y_{i}, f\\left(x_{i}, \\theta\\right)\\right) = \\tilde{\\mathcal{L}}(f(\\theta))\n\\end{equation}\nwith a twice differentiable and convex function l (e.g. square loss or cross entropy) and a parameterized model f (xi, \u03b8) (e.g. a deep neural network). In the second equality of (1), we concatenate the model outputs f (xi, \u03b8) \u2208 Rdy for all n data points in a single (column) vector f(\u03b8) \u2208 Rndy with entries fi+n(j-1) = f(xi, \u03b8)j, and define concisely the loss in function space as \\tilde{\\mathcal{L}}(f(\\theta)). The loss \\tilde{\\mathcal{L}}(f) is a convex and twice differentiable function of the model f, but \\mathcal{L}(\\theta) is usually a non-convex function of the parameters \u03b8, due to the non-linearity of the model f(\u03b8). Gradient descent optimizes parameters according to:\n\\begin{equation}\n\\theta_{t+1} = \\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}\n\\end{equation}"}, {"title": "4 Exact and tractable Gauss-Newton", "content": "The main hurdle in the GN update of equation (5) is the computation of the Jacobian pseudo-inverse. For a batch size n, number of parameters p and output dimension d, that requires O(ndp min(nd, p)) compute and O(ndp) memory. In this Section, we show that the GN update can be computed efficiently for reversible models. For a dense neural network of L layers and dimension d, implying p = O(Ld\u00b2) parameters, our GN update requires the same memory as gradient descent and O(Lnd\u00b2+ Ln2d) compute, compared to O(Lnd\u00b2) compute of gradient descent.\nOur method consists of two steps: first, we replace the Jacobian pseudoinverse with a generalized inverse, and show that it has identical convergence properties (Theorem 4.3). Second, we show that a specific generalized inverse can be computed efficiently in reversible neural networks (Proposition 4.4). We present both results in the case of square loss (GN). Results for other convex loss functions (GGN) can be derived following steps similar to Appendix B.\nReplacing the pseudoinverse with a generalized inverse. We show that the Jacobian pseudoinverse in equation (5) can be replaced by a generalized inverse that has the same convergence properties. A similar approach was proposed by Bernacchia et al. [2018], Karakida and Osawa [2020], but it was only valid in the case of, respectively, deep linear networks or constant Neural Tangent Kernel (NTK) limit. Here we provide a more general formulation that holds under less restrictive assumptions, e.g. it holds in the mean field regime [Arbel et al., 2023]. We need the following assumption\nAssumption 4.1. Assume J(\u03b8) has linearly independent rows (is surjective) for all \u03b8 in the domain where GN dynamics takes place.\nNote that this implies that the network is overparametrized, i.e. p > ndy. While, in practice, this assumption may seem strong, it is only slightly stronger than the following version, employed in Arbel et al. [2023]:\nAssumption 4.2. J(\u03b80) is surjective at initialization \u03b80.\nIn Arbel et al. [2023], the authors argue that since surjectivity of J is an open condition, it holds for a neighbourhood of \u03b80, and moreover continue to prove that the dynamics of GN is well defined up to some exit time T from this neighbourhood. They then continue to give assumptions guaranteeing that this dynamics extends to \u221e. We directly assume we are in this latter setting.\nTheorem 4.3. Under Assumption 4.1 so that there is a right inverse J\u2020 satisfying JJ\u2020 = I, consider the update in parameter space with respect to the flow induced by an arbitrary right inverse J\u2020:\n\\begin{equation}\n\\theta_{t+1} = \\theta_{t} - \\alpha J^{\\dashv} \\nabla_{\\mathbf{f}} \\tilde{\\mathcal{L}}\n\\end{equation}\nThen the loss along these trajectories is the same up to O(\u03b1), i.e. for any two choices J\u20201 and J\u20202, the corresponding iterates \u03b8(1)t and \u03b8(2)t satisfy:\n\\begin{equation}\n||\\nabla_{\\mathbf{f}} \\tilde{\\mathcal{L}}\\left(\\mathbf{f}\\left(\\theta_{t}^{(1)}\\right)\\right) - \\nabla_{\\mathbf{f}} \\tilde{\\mathcal{L}}\\left(\\mathbf{f}\\left(\\theta_{t}^{(2)}\\right)\\right)|| \\leq O(\\alpha).\n\\end{equation}\nMoreover, as the Moore-Penrose pseudo-inverse is a right inverse under the assumptions, the result applies to J+, and consequently to the dynamics of (5).\nThe proof is in Appendix C. The intuition behind this result becomes clearer once we examine the differential of the loss w.r.t. the function outputs, \\nabla_{\\mathbf{f}} \\tilde{\\mathcal{L}}. Notice that, as \\tilde{\\mathcal{L}} is a convex function, it has a unique stationary point, and hence it is natural to interpret \\nabla_{\\mathbf{f}} \\tilde{\\mathcal{L}}({\\theta}) as the error at \u03b8, especially close to the global minimum. We will therefore adopt the notation\n\\begin{equation}\n\\mathbf{\\epsilon}(\\theta) := \\nabla_{\\mathbf{f}} \\tilde{\\mathcal{L}}({\\theta})\n\\end{equation}\nhere and throughout the proofs to refer to the deviation from the global minimum at the current parameter value. A key ingredient of the proof of Theorem 4.3 will be to establish that, for trajectories induced by GGN or the update in equation (8), \u03f5(t) := \u03f5(\u03b8(t)) satisfies:\n\\begin{equation}\n\\frac{d \\mathbf{\\epsilon}}{dt} = - \\mathbf{\\epsilon}(t)\n\\end{equation}\nThis trivially implies that \u03f5 \u2192 0 from any initial condition \u03f50, so that the evolution of the weights approaches a stationary point for the loss, and hence its global minimum.\nThe right inverse of the Jacobian, J\u2020 is non-unique, and, in general, it is not feasible to compute for large models. However, it turns out that in the case of reversible models, we have an analytic expression for J\u2020, which allows computing exact GN at nearly the same cost as SGD."}, {"title": "5 Experiments", "content": "For our experiments we train RevMLPs (equations (14) and (15))\nwith 2 (6) blocks for MNIST [LeCun et al., 2010] (CIFAR-10;\nKrizhevsky, 2009), ReLU non-linearities at all half-coupled lay-\ners, and an inverted bottleneck of size 8000 resulting in models\nwith 12M (MNIST) and 147M (CIFAR-10) parameters. We train\nthese models to classify images flattened to 1D vectors, using a\ncross-entropy objective. Note that the chosen size of the inverted\nbottleneck ensures that the assumptions of Proposition 4.4 hold.\nAt each training iteration, we compute the pseudoinverses in equa-\ntions (16), (17) using an SVD. For numerical stability we truncate\nthe SVD to a 1% tolerance relative to the largest singular value\nand an absolute tolerance of 10\u20135, whichever gives the smallest\nrank. Our main findings are qualitatively robust to these tolerance\nlevels. Full hyperparameters and additional details are reported\nin Appendix L, and code is provided with the submission. We\nreport results averaged over 3 random seeds. All experiments are performed on a single NVIDIA\nRTXA6000 GPU."}, {"title": "5.1 Full-Batch Setting", "content": "We first examine the full-batch setting in which the dataset is a random subset of size 1024 of MNIST or CIFAR-10. We tuned the learning rate for each optimizer by selecting the largest one that did not cause the loss to diverge. Figure 1 shows that GN is significantly faster than Adam and SGD in both datasets, in line with theoretical predictions (Equation 11)."}, {"title": "5.2 Mini-Batch Setting", "content": "Next, we consider the full MNIST and CIFAR-10 datasets in the mini-batch setting. We follow standard train and test splits for the datasets, with a mini-batch size of n = 1024. The learning rate for all methods is tuned towards the largest progress after 1 epoch that does not exhibit any training instabilities. In both datasets, GN makes good initial progress on the training and test losses, but then struggles to sustain the continued progress which Adam exhibits (Fig. 2). This surprising early saturation of the GN training and test losses is most pronounced for the CIFAR dataset, where even SGD eventually overtakes GN (see Fig. 6 in Appendix E for a longer training run). In the rest of this Section, we use the CIFAR-10 setup to study the possible origins of such weak generalization.\nOverfitting each mini-batch. Based on the full-batch results of Figure 1 in which GN was seen to converge very fast, we postulate that the poor generalization behaviour observed in the mini-batch case may be caused by overfitting to each mini-batch. To test this hypothesis, at each iteration, we compute the loss on a single mini-batch before and after applying the update computed on that same mini-batch. The resulting percentage change in mini-batch loss is shown in Figure 3. Compared to SGD and Adam, GN leads to a much stronger immediate decrease in loss after each update, especially early in training. Whilst this difference gradually weakens during the course of training, it subsists for 80 epochs, i.e. until well after GN's overall training and test losses have saturated (c.f. Fig.2). These results suggest that GN might require some form of regularization to prevent aggressive incorporation of each mini-batch into the model's parameters. However, we find that neither smaller learning rates (Appendix I), nor weight decay (Appendix J), nor any of the usual techniques for regularizing the pseudoinverse in Equation (16) (Appendix K) appear to help in this respect (Figures 12, 13 and 14).\nEvolution of the Neural Tangent Kernel. We further hypothe- size that GN's poor generalization may be due to a lack of feature learning. In a similar fashion to Fort et al. [2020], we study the evolution of the neural tangent kernel (NTK) when training with GN compared to SGD and Adam. A changing NTK would suggest that the model learns features different from those present at initialization. Figure 4a and Figure 4b show the rate of change of the NTK between epochs, and the evolution of the NTK similarity with initialization, respectively.\nOverall, the NTK changes very little for SGD and GN, suggesting that SGD and GN operate close to the \u201clazy\u201d training regime [Jacot et al., 2018, Chizat et al., 2019]. On the other side, Adam causes the NTK to change significantly, i.e. Adam does learn features different from the initial ones."}, {"title": "6 Summary and limitations", "content": "In this paper we have introduced a new, tractable way of computing exact Gauss-Newton updates in models with millions of parameters. We have used this theory to study the generalization behaviour of GN in realistic task settings. We found that, although GN yields fast convergence in the full batch regime as predicted, it does not perform as well in the stochastic setting where it tends to overfit each mini-batch. We observed that the NTK does not change when training with GN, suggesting that it operates in the \"lazy\" regime. In line with the above, using the CKA metric, we performed an analysis of the neural representations at the start and end of training showing that they remain very close to each other. This can explain the observed lack of generalization.\nOur investigations have relied on a specific formulation of GN based on a tractable generalized inverse of the Jacobian in reversible networks. While we proved that this inverse leads to the same training loss dynamics, in the limit of small learning rate, as the standard GN formulation is based on the Moore-Penrose pseudoinverse (MPP), one cannot exclude the possibility that those two update rules have different learning and generalization properties for finite learning rates. Indeed, the functional view of GN (Section 3) makes it clear that the standard MPP-based GN update corresponds to the minimum-norm weight update that guarantees (infinitesimal) steepest descent in function space. Whilst also achieving steepest descent, our generalized inverse does not have the same least-squares interpretation \u2013 although it could imply another form of regularization which future work could uncover. In any case, these differences are difficult to assess precisely because the full Jacobian of the network (let alone its MPP) simply cannot be computed for large models.\nPrevious applications of approximate GN to deep models found that damping, or truncating, the pseu- doinverse of the GN matrix (or, equivalently, of the Jacobian) is key not only for good generalization but also for successful training [Martens et al., 2010, Wadia et al., 2021]. Our generalized inverse is based on a layer-wise factorization of the Jacobian, teasing apart (i) the sensitivity of the network's output to small changes in layer activations and (ii) the sensitivity of those activations to small changes in parameters. The use of exactly reversible networks allows us to invert the former very efficiently, but does not easily accommodate damping or truncation, making it difficult to study their effect on generalization in large scale settings. We speculate that a variation on coupling layer-based reversible networks could be developed that allows for damping or truncation, potentially improving the generalization behaviour of GN. If this can be done, our framework would then enable very efficient training of large models, effectively achieving the training acceleration of second-order methods at the cost of first-order optimizers, all in a memory efficient architecture."}, {"title": "Appendix", "content": null}, {"title": "A Functional view of Generalized Gauss-Newton", "content": "Assuming that \\tilde{\\mathcal{L}} is strongly convex, Newton's flow in function space is equal to\n\\begin{equation}\n\\frac{d \\mathbf{f}}{dt} = -H^{-1} \\nabla_{\\mathbf{f}} \\tilde{\\mathcal{L}}\n\\end{equation}\nwith H = \\nabla^{2} \\tilde{\\mathcal{L}}. Following steps similar to Section 3, we use \\frac{\\partial \\mathbf{f}}{\\partial \\theta} = J to define and pseudo-invert the Jacobian. In discrete time, with learning rate \u03b1, the functional view of Generalized Gauss Newton is given by\n\\begin{equation}\n\\theta_{t+1} = \\theta - \\alpha J^{+} H^{-1} \\nabla_{\\mathbf{f}} \\tilde{\\mathcal{L}}\n\\end{equation}\nIt is straightforward to show that equations (19) and (7) are identical when the Jacobian has linearly independent rows. Under these assumptions, (JTHJ)+ = J+H-1JT+ and JT+JT = I. Furthermore, as in Section 3, \\nabla_{\\theta} \\mathcal{L} = JT\\nabla_{\\mathbf{f}} \\tilde{\\mathcal{L}}."}, {"title": "B Convergence of GGN flow", "content": "In this Section, we give an informal derivation of the continuous-time dynamics of GGN. See Ortega and Rheinboldt [2000], Bottou et al. [2018] for convergence of GGN in discrete time. We consider the optimization of the error under GGN flow in continuous time. Using the definition of the error \u03f5 = \\nabla_{\\mathbf{f}} \\tilde{\\mathcal{L}} (equation (10)), the optimization flow of the error can be derived using the chain rule\n\\begin{equation}\n\\frac{d \\mathbf{\\epsilon}}{dt} = \\frac{\\partial \\mathbf{\\epsilon}}{\\partial \\theta} \\frac{d \\theta}{dt}\n\\end{equation}\nThe definition of GGN flow is\n\\begin{equation}\n\\frac{d \\theta}{dt} = -\\alpha (J^{T}HJ)^{+} \\nabla_{\\theta} \\mathcal{L} = -\\alpha (J^{T}HJ)^{+} J^{T} \\mathbf{\\epsilon}\n\\end{equation}\nTherefore, optimization of the error under GGN flow is equal to\n\\begin{equation}\n\\frac{d \\mathbf{\\epsilon}}{dt} = -\\alpha H J (J^{T}HJ)^{+} J^{T} \\mathbf{\\epsilon} = -\\alpha A \\mathbf{\\epsilon}\n\\end{equation}\nwhere we defined the matrix A = H J (J^{T}HJ)^{+} J^{T}. Using the properties of the matrix pseudoinverse, we note that A is a projection operator, namely An = A for any integer power n. Therefore, eigenvalues of A are either zero or one, implying that the error decreases exponentially in the range of A, while it remains constant in the null space of A. In general, the range and null space of A change during training. We note that the projection is orthogonal with respect to the inner product \u27e8\\mathbf{\\epsilon}, H \\mathbf{\\epsilon}\u27e9. Using the same steps as in Appendix A, if J has linearly independent rows and \\tilde{\\mathcal{L}} is strongly convex, then it is straightforward to show that A = Indy."}, {"title": "C Proof of Theorem 4.3", "content": "During the proof we will compare the dynamics of the flow curves of two vector fields, namely \u2212J+(\u03b8)\u03f5(\u03b8) and the corresponding \u2212J\u2020(\u03b8)\u03f5(\u03b8). The dependence on \u03b8 is assumed throughout and we will write (\u2212J+\u03f5)|\u03b8 or just \u2212J+\u03f5. The flowlines of these vector fields are given by:\n\\begin{equation}\n\\frac{d \\theta}{dt} = (-J^{+} \\epsilon)|_{\\theta(t)}\n\\end{equation}\nand\n\\begin{equation}\n\\frac{d \\tilde{\\theta}}{dt} = (-J^{\\dashv} \\epsilon)|_{\\tilde{\\theta}(t)}\n\\end{equation}\nrespectively. We will further write plain \u03b8(t) for the solutions of equation (23) and \\tilde{\\theta}(t) for the solutions of equation (24). Moreover, we'll write f to mean f(\u03b8(t)) for arbitrary (possibly tensor valued) functions f, to distinguish from f(\u03b8(t))."}]}