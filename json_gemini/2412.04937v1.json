{"title": "Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of Turn-taking in Murder Mystery Games", "authors": ["Ryota Nonomura", "Hiroki Mori"], "abstract": "Multi-agent systems utilizing large language models (LLMs) have shown great promise in achieving natural dialogue. However, smooth dialogue control and autonomous decision making among agents still remain challenges. In this study, we focus on conversational norms such as adjacency pairs and turn-taking found in conversation analysis and propose a new framework called \u201cMurder Mystery Agents\" that applies these norms to AI agents' dialogue control. As an evaluation target, we employed the \u201cMurder Mystery\u201d game, a reasoning-type table-top role-playing game that requires complex social reasoning and information manipulation. In this game, players need to unravel the truth of the case based on fragmentary information through cooperation and bargaining. The proposed framework integrates next speaker selection based on adjacency pairs and a self-selection mechanism that takes agents' internal states into account to achieve more natural and strategic dialogue. To verify the effectiveness of this new approach, we analyzed utterances that led to dialogue breakdowns and conducted automatic evaluation using LLMs, as well as human evaluation using evaluation criteria developed for the Murder Mystery game. Experimental results showed that the implementation of the next speaker selection mechanism significantly reduced dialogue breakdowns and improved the ability of agents to share information and perform logical reasoning. The results of this study demonstrate that the systematics of turn-taking in human conversation are also effective in controlling dialogue among AI agents, and provide design guidelines for more advanced multi-agent dialogue systems.", "sections": [{"title": "1. Introduction", "content": "The emergence of large language models (LLMs) has dramatically enhanced the capabilities of AI agents. With the advent of LLMs such as GPT-3, GPT-4, and LLAMA, we have witnessed the achievement of human-comparable or superior performance across various tasks, including text generation, question-answering, and summarization [1, 2, 3, 4]. The development of AI agents based on these LLMs has gained significant momentum, with promising applications spanning diverse domains such as customer service [5], educational support [6, 7, 8], and creative work assistance [9, 10].\nOf particular interest is whether AI agents can exhibit social behaviors similar to those of humans. Previous studies have employed various approaches to observe the social behaviors of LLM-based agents. For instance, Park et al. [11] conducted virtual daily life simulations, analyzing the behavioral patterns of 25 AI agents and their impact on a simulated society. Their study observed information sharing between agents and the formation of novel relationships.\nMeanwhile, Lan et al. [12] conducted research evaluating social interaction capabilities through multi-agent conversations in the board game Avalon, which requires cooperation and deception among multiple agents. Their study proposed a framework that enables AI agents to make strategic decisions based on previous gameplay experiences, reporting observations of social behaviors such as leadership and persuasion.\nIn social interaction, verbal communication plays a central role. Previous studies on the application of LLMs have also revealed that enabling AI agents to chat with each other is an effective approach. Qian et al. [13] demonstrated that a chat chain between an instructor and an assistant is effective for completing various subtasks in the workflow of software development. Gu et al. [14] proposed a simulation framework for group chats among AI agents, reporting that multifaceted emergent behavior was observed during role-playing scenarios. Wu et al. [15] proposed a platform for LLM applications that supports interaction between LLMs, humans, and tools, where group chats among AI agents are facilitated.\nHowever, text chats are significantly different from human-to-human conversations. It has been claimed that text chat is incoherent, especially due to the lack of interaction management such as simultaneous feedback, which leads to disruption and breakdown of turn-taking and topic management [16]. Most AI chat systems employ an even simpler turn-taking model: sending text input from the user initiates the turn transition. This framework does not reflect the properties that human conversation has. For example, chat AIs cannot actively offer topics, initiate conversations, remain silent when other participants are to speak, or withhold from speaking.\nTurn-taking plays a crucial role especially in multi-party conversations, yet there have been relatively few studies on such conversation by AI agents. In order to handle multi-party conversations, the problem of selecting the next speaker arises. In the AutoGen platform [15], an automatic next-speaker selection mechanism is implemented, where an LLM agent estimates the next speaker's role based on the history of the speaker's role and utterances. However, Bailis et al. [17] pointed out that while this approach is potentially effective, it lacks autonomy for individual agents. Instead, they proposed a dynamic turn-taking system where agents express their desire to speak by bidding.\nAs Bailis et al. [17] argued, allowing agents to autonomously determine the speaking order could be key to AI agents playing their own social role and having a fruitful conversation. At the same time, however, the order of speaking should not be determined solely by the agents' will. Sociologists who pioneered conversation analysis devised a concept of adjacency pairs [18] as the basic unit of utterance sequences. An adjacency pair is a two-part exchange in which the second utterance is functionally dependent on the first. Such functional binding is called conditional relevance [19]. When the current speaker addresses a question to another one, the addressee is not only obligated to take the turn, but also to speak something relevant to the question. In multi-party conversations, the first pair part of adjacency pairs often involves this \u201ccurrent speaker selects next\" technique [20].\nTherefore, the research question here is whether introducing turn-taking systematics such as adjacency pairs, discovered in the research field of conversation analysis, into the next-speaker selection mechanism will have the effect of making LLM-based multi-agent conversations more natural and efficient. Schegloff [21] argued that organization of sequences in turn-taking systematics such as adjacency pairs is the source of coherence in conversation. If so, introducing such a conversational norm into conversations by AI agents is expected to improve the coherence of conversation.\nTo address this research question, we developed Murder Mystery Agents (MMAgents), a system where multiple AI agents play a deductive table-top role-playing game called Murder Mystery. MMAgents consists of a self-selection mechanism for autonomous utterances and a next-speaker selection mechanism that detects the first part of adjacency pairs using LLMs to determine the next speaker."}, {"title": "2. Background", "content": ""}, {"title": "2.1. LLM-Based Agents", "content": "With the advancement of large language models (LLMs), numerous LLM-based agents have been proposed [22, 15, 23, 24]. These autonomous agents, built upon foundational models such as GPT-3 and GPT-4, are capable of executing complex tasks, engaging in assistant-like dialogue, and making decisions.\nThe applications of LLM-based agents span diverse domains, including software development [13], gaming environments [25], and economic simulations [26]. Of particular interest are multi-agent systems involving multiple agents. The CAMEL framework [27] demonstrates how agents with distinct roles can collaborate to solve problems. Additionally, research on the Avalon Game [12] simulates complex social interactions, including cooperation and conflict between agents.\nFurthermore, research on AI agents' social behavior, particularly interaction through conversation, continues to evolve. These studies investigate the ability of multiple agents to participate in group chats and discussion scenarios, generating conversations that closely resemble human-to-human interactions [28, 14]. Agents have been shown to possess capabilities such as memory, reflection, and planning, enabling more human-like dialogue [11]. This approach contributes to understanding the mechanisms of information exchange and cooperative behavior among agents, potentially offering insights into emergent behaviors in human society.\nResearch aimed at enhancing LLM-based agents' capabilities is also being actively pursued. For instance, SelfGoal [29] proposes automatic generation and updating of sub-goals to achieve high-level objectives. Chain-of-thought prompting [30] significantly improves performance on complex reasoning tasks by generating intermediate thought processes. Moreover, ReAct [31] proposes an approach alternating between reasoning and action, enhancing agents' ability to solve problems incrementally while interacting with their environment.\nHowever, there have been relatively few studies focusing on multi-party conversations among LLM-based agents. Most existing research deals with one-to-one interactions or simplified turn-taking mechanisms, failing to address the natural flow of conversation that occurs in groups of three or more participants."}, {"title": "2.2. Turn-taking", "content": "In human conversation, there exists a fundamental constraint where typically only one person speaks at a time. This constraint stems from the physical limitations of speech communication, as simultaneous speech by multiple participants leads to interference, making comprehension difficult. For efficient communication, speakers must smoothly alternate turns while minimizing silent intervals between utterances. To meet this requirement, humans have naturally developed turn-taking systems through social interaction.\nTurn-taking, where dialogue participants take turns to speak, forms the foundation of smooth communication. Through analysis of spontaneous conversation recordings, conversation analysts like Sacks, Schegloff, and Jefferson systematically described this phenomenon and identified the following rules [20]:\n1. If the current speaker designates the next speaker by using a 'current speaker selects next' technique (e.g., at the first pair part of an adjacency pair [18]), the selected participant has both the right and obligation to become the next speaker. (Current Speaker Selects Next)\n2. If the current speaker does not designate the next speaker, other participants can spontaneously initiate speech. (Self-Selection)\n3. If no one begins speaking, the current speaker can continue.\nUnlike dyadic conversations where speaker and listener roles are clearly defined, multi-party conversations involve multiple participants, necessitating the use of gaze direction and verbal addressing to designate the next speaker [20]. Adjacency pairs, the basic units of conversation, consist of paired utterances such as [question-answer] and [invitation-acceptance/rejection]. The initial utterance is referred to as the first pair part, and the responding utterance as the second pair part. First pair parts like \u201cI'd like to purchase this item (request)\u201d generate an obligation for a specific type of second pair part (in this case, \u201cCertainly (acceptance)\u201d or \u201cWe're sold out (rejection)\"). An inappropriate second pair part or lack of response suggests either a communication error or implies a reason for the inability to respond.\nHumans dynamically create conversations as collaborative acts among participants using this turn-taking system. In contrast, current AI agents struggle to autonomously engage in such flexible and immediate interactions. Therefore, implementing turn-taking mechanisms in AI agents may enable more natural and smooth dialogue.\""}, {"title": "2.3. Murder Mystery", "content": "Murder Mystery is a reasoning-type table-top role-playing game in which players play the roles of characters within a story, aiming to either identify the murderer or, if playing as the murderer, to avoid detection. The game's progression heavily relies on players sharing information through conversation, including evidence gathered from crime scene investigations and character-specific knowledge. Furthermore, Murder Mystery assigns different missions to each player. Players may need to cooperate or deceive others to accomplish these missions. This requires not merely intelligence but also human-like social behaviors such as teamwork, persuasion, negotiation, and deception. Successfully replicating these behaviors in AI agents could lead to significant advances in artificial intelligence research.\nThere has been one attempt to make AI agents play Murder Mystery games [28]. In this prior research, a detective agent poses the same questions to five agents, including the murderer. After all five responses are collected, the detective agent responds and asks another question. This process is repeated N times, after which the detective agent attempts to identify the murderer. This approach is termed \u201cone-to-many simulation.\u201d While the simulation successfully identifies the murderer, this method does not accurately reflect real Murder Mystery gameplay, where all players except the murderer must develop their own theories to identify the murderer. While this approach is referred to as \u201cmany-to-many,\u201d it could not be implemented due to OpenAI's input token limitations. Therefore, this research aims to develop an agent framework capable of either reasoning or concealing information about the murder through autonomous conversation, similar to human players."}, {"title": "3. Conversational Agents Simulating Human Multi-party Conversation", "content": "Building upon the characteristics of Murder Mystery games discussed in Section 2.3, this section details the design philosophy and technical components of MMAgents (Murder Mystery Agents), a system developed to facilitate autonomous game progression. MMAgents is designed to simulate multi-party human conversations, enabling multiple AI agents to not only cooperate but also engage in complex conversations involving competition and bargaining to advance the Murder Mystery game."}, {"title": "3.1. Component", "content": ""}, {"title": "3.1.1. Character Setting", "content": "In Murder Mystery games, before the game begins, the game master provides players with character sheets. Each character sheet contains information necessary for players to portray their characters, including background, personality, objectives, and actions on the day of the incident. Players read and understand this information and play the character to talk and explore.\nThe approach of having LLMs roleplay characters and evaluating their performance has been reported in several studies [32, 33, 34, 35]. As shown in Figure 1, MMAgents structures each agent's prompt beginning with the character's name, followed by descriptions of their objectives, actions, and missions to accomplish. For example, the character Masato Nishino's information includes crucial background details such as memories of his close friend Akira who passed away three years ago, and romantic feelings expressed that night. The information also includes specific incident-related actions, such as his behavior in the lounge the previous day and conversations with the inn's manager. Furthermore, character-specific missions are established, such as \u201cfinding Erika's murderer\u201d and \u201creturning the ring that Akira intended to give to his lover.\"\nIn this way, each agent is provided with character information containing distinct backgrounds and objectives, which guides their decision-making and dialogue. Only surface-level information about other characters is shared, and this information asymmetry implements the elements of information gathering and strategic interaction inherent in Murder Mystery games."}, {"title": "3.1.2. Memory", "content": "For LLM-based agents, memory management mechanisms are crucial components for generating more natural and consistent responses in user interactions [36, 37, 38]. This is equally important in agent-to-agent dialogue [11]. To create systems like Murder Mystery, where multiple agents engage in complex discussions over extended periods, it is essential to appropriately store past statements and acquired information, and recall them at necessary moments.\nDrawing inspiration from human memory systems, this research manages agents' memory across three distinct layers. First, there is a memory named History that is shared by all agents, which maintains the past k turns of conversation as shown in Equation (1). History is used to maintain conversational context and track recent dialogue flow.\n\\[history = \\{u_{n-k+1}, u_{n-k+2}, ..., u_{n}\\}\\]\nwhere \\(u_i\\) represents the i-th utterance.\nSecond, each agent maintains a short-term memory, named shortTermHistory. This consists of a history of thoughts generated by the think() function detailed in Section 3.2.1, and maintains agent-specific policies and intentions, as shown in Equation (2),\n\\[shortTermHistory = \\{t_{n-k+1}, t_{n-k+2}, ..., t_{n}\\}\\]\nwhere \\(t_i\\) represents the i-th thought. The shortTermHistory enables agents to maintain consistency in their reasoning and intentions.\nFurthermore, each agent maintains a long-term memory, named longTermMemory, in which utterance content is normalized using LLMs, and important knowledge and information is extracted and stored in a database, as formulated in Equation (3). Figure 2 demonstrates the process of information extraction and normalization in longTermHistory. This example illustrates the process of extracting important information from unstructured speech text by Kozue Taniguchi and storing it as structured knowledge. This normalization process facilitates later retrieval and reference by extracting important facts and information from unstructured text in a bullet-point format.\n\\[longTermMemory = \\{k_1, k_2, ...\\}\\]\nWhen generating new utterances, the previous utterance \\(u_{t-1}\\) is converted into an embedding vector \\(E(u_{t-1})\\), and the cosine similarity shown in Equation (4) is calculated with each vector \\(E(k_i)\\) of the embedded knowledge stored in longTermHistory to retrieve relevant past memories.\n\\[cos(E(u_{t-1}), E(k_i)) = \\frac{E(u_{t-1})E(k_i)}{|E(u_{t-1})||E(k_i)|}\\]"}, {"title": "3.2. Turn-taking System", "content": "The turn-taking system is potentially a crucial element for achieving natural dialogue among multiple agents. In conventional multi-agent dialogue systems, speaking turn was often predetermined or randomly assigned. In this research, based on Sacks et al.'s conversation analysis theory discussed in Section 2.2, we implemented two characteristic turn-taking mechanisms from natural human conversation in MMAgents: \u201cSelf-Selection\u201d and \u201cCurrent Speaker Selects Next\". This enables natural turn-taking that reflects the agents' personalities and intentions. The pseudocode for this algorithm is shown in Algorithm 1. This subsection details the important modules of the turn-taking algorithm."}, {"title": "3.2.1. think()", "content": "At the beginning of each turn, agents execute an action called think(). Based on the provided character data, think() generates thought, which represents the plan for the next utterance or action aimed at achieving their mission. Simultaneously, it decides whether to take the action of \u201cspeak\" or \u201clisten\u201d. This selection is implemented with the assumption that it is determined by considering other agents' utterances and the urgency of their own thought content. Furthermore, it outputs an importance as an integer from 0 to 9. This value is designed to reproduce the Self-Selection mechanism in conversation and is presumed to be determined based on factors such as relevance to the mission, consistency with current conversational context, urgency of the utterance content, and character personality."}, {"title": "3.2.2. selectMostImportant()", "content": "The selectMostImportant(agents) is a speaker selection algorithm that implements the Self-Selection mechanism. This algorithm processes differently based on the number of agents who have selected \u201cspeak.\u201d When only one agent selects \"speak\", that agent naturally becomes the speaker. This is the simplest case of Self-Selection. Conversely, when multiple agents select \"speak\", their importance values are compared, and the agent with the highest value becomes the speaker. This represents the turn-taking systematics of \"the first person to start speaking becomes the speaker\u201d, expressed numerically through importance values. In cases of tied importance values, random selection is used to represent the uncertainty of turn-taking in actual conversations. Furthermore, when all agents select \u201clisten,\u201d the previous speaker continues speaking. This implements the turn-taking systematics that \u201cwhen the current speaker does not select the next speaker, they retain the right to continue speaking\u201d. However, in the first turn at the start of the dialogue, the speaker is determined randomly."}, {"title": "3.2.3. speak()", "content": "The selected agent as speaker generates an utterance using the prompt shown in Figure 4. This prompt consists of the character data shown in Figure 1 and the three types of memory (History, shortTermHistory, longTermHistory) explained in Section 3.1.2. This enables natural utterances that consider the agent's personality, past conversation content, and policies."}, {"title": "3.2.4. detectDesignation()", "content": "detectDesignation() is a mechanism that detects whether the current speaker has explicitly designated the next speaker. This process uses the LLM to determine if a first pair part of an adjacency pair is present in the previous turn's utterance. When a first pair part is detected, it simultaneously classifies its type (Yes/No question, addressing, etc.) and estimates the agent addressed by the utterance. In the example shown in Figure 5, Kozue Taniguchi asks Masato Nishino \u201cWhere were you at that time?\u201d. When this utterance is input to detectDesignation(), the LLM outputs the detected type of first pair part (wh question) and the predicted next speaker (Masato Nishino).\nThen, by incorporating the type of the corresponding second pair part into the prompt used in the following speak(), the agent designated as the next speaker is obligated to respond to the previous turn's utterance. For example, in Turn 10 of the conversation history in Figure 4, a constraint of \u201c(response)\u201d is imposed on the next speaker's utterance, because the previous utterance was a first pair part (wh question). This achieves coherency in adjacent utterances while maintaining natural conversation flow."}, {"title": "4. Experiments and Evaluations", "content": ""}, {"title": "4.1. Experiments", "content": "To validate the effectiveness of the proposed MMAgents, we conducted conversational simulations using a commercially available murder mystery scenario titled \u201cThe Ghost Island Murder Case\" [39]. This scenario was selected because it features characters with well-defined roles and positions, while maintaining a moderate difficulty level for non-murderer characters, with logical deductions that are challenging yet solvable. \u201cThe Ghost Island Murder Case\" begins with a story of former college tennis team members reuniting on an isolated island after three years. The scenario features the following four characters:\n\u2022 Kozue Taniguchi (female): A boyish character with a straightforward personality.\n\u2022 Masato Nishino (male): An energetic character. Endearing, but sometimes fails to read the room.\n\u2022 Yukiko Shiraishi (female): A caring, big-sister type character in the group, though she has a tendency to overthink.\n\u2022 Takeshi Kanemoto (male): A sincere character despite his flashy appearance.\nWhile the scenario consists of multiple phases (exploration phase for information gathering, private conversation phase, discussion phase, reasoning phase, etc.), our experiment focused solely on the discussion phase. This choice was primarily motivated by our aim to evaluate the effectiveness of MMAgents' core functionality: human-like turn-taking. We determined that the discussion phase, with its active dialogue and exchange of opinions between participants, would be optimal for assessing the performance of our proposed method.\nIn our experiments, we employed multiple large language models. GPT-4o was utilized for detectDesignation() and speak(), as these tasks require sophisticated context understanding and natural speech generation. Conversely, GPT-3.5-turbo was employed for simpler tasks such as knowledge normalization (longTermHistory) and think() to optimize computational costs. To accommodate the input token limitations of LLMs, we set the retained turns for History and shortTermHistory to five turns, while longTermHistory was configured to select the top five entries based on similarity scores.\nTo evaluate the proposed method, we conducted experiments under the following three conditions:\nEQUAL: The participants have equal opportunity to speak.\nSS: The next speaker always Selects Self.\nCSSN-or-SS: Current Speaker Selects Next, otherwise the next speaker Selects Self.\nIn the EQUAL condition, the order of speaking is randomly determined each round. This ensures that the number of each participant's utterances is equal, while avoiding potential order effects. In the CSSN-or-SS condition, the turn-taking system described in Section 3.2 determines the speaking order. The SS condition is the same as the CSSN-or-SS condition except that it does not have the detectDesignation() mechanism, which is used for speaker selection in the next turn.\nFor each condition, we generated 50 sets of 10-turn conversations. The results were then evaluated using the evaluation methods described in the following subsection, enabling a statistical analysis of the effectiveness of our proposed approach."}, {"title": "4.2. Evaluations", "content": "To evaluate the conversations generated by MMAgents, we adopted the following three approaches:\n1. Analysis of dialogue breakdown: To assess the naturalness of generated conversations, we employed LLMs to analyze and evaluate the number of utterances that led to dialogue breakdowns [40].\n2. LLM-as-a-Judge: We defined three metrics\u2014coherence, cooperation, and conversational diversity\u2014and evaluated them using score-based LLM as the judging methodology [41, 42].\n3. Human evaluation: We established original evaluation criteria focusing on murder mystery game progression and information sharing between agents. These criteria comprehensively assess the agents' reasoning capabilities and information-gathering abilities through analysis of conversations generated by MMAgents."}, {"title": "4.2.1. Analysis of Dialogue Breakdown", "content": "Evaluating conversational naturalness is crucial, but difficult to achieve. The evaluation of naturalness is inherently subjective, heavily dependent on evaluators' perspectives and prior experiences. Even when different evaluators assess the same conversation, their evaluations may not align, making it difficult to establish standardized evaluation criteria. Therefore, rather than directly evaluating conversational naturalness, our research adopts an indirect approach by evaluating the degree of dialogue breakdown. Specifically, we employ the \u201cclassification of utterances that lead to dialogue breakdowns\u201d proposed in dialogue systems research [40]. Among these types, we use LLMs to analyze items corresponding to response and context-level errors shown in Table 1. For the analysis, we input 10-turn conversation samples generated by MMAgents into the LLM, which then identifies utterances corresponding to the categories in Table 1 as breakdown utterances (B) and others as non-breakdown utterances (NB). This process is repeated 50 times, and then conversational naturalness is quantitatively evaluated through statistical analysis of the distribution of utterances identified as B. We utilized GPT-4 for this analysis, with the prompt shown in Figure 6."}, {"title": "4.2.2. LLM-as-a-Judge", "content": "A new approach called \u201cLLM-as-a-Judge\" has emerged for evaluating natural language processing tasks [41, 42, 43, 44]. This rapidly evolving methodology is increasingly being recognized as an alternative to traditional human evaluator-dependent methods. The fundamental concept of the \u201cLLM-as-a-Judge\" approach involves inputting some text or conversation to be evaluated into LLMs and having them perform evaluations based on specific criteria or metrics. The primary advantage of this method lies in its ability to analyze large volumes of data efficiently and consistently without requiring human evaluators.\nWe employ LLMs to evaluate the quality of generated conversations using three metrics: coherence, cooperativeness, and diversity. Coherence evaluates the logical flow and absence of contradictions in conversations, with scores ranging from 1 (contradictory and illogical) to 5 (consistent and logical). Cooperativeness evaluates how collaboratively participants engage in information exchange and solving problems, with scores ranging from 1 (uncooperative) to 5 (cooperative). Conversational diversity evaluates the absence of repetitive content and the presence of varied opinions and perspectives, with scores ranging from 1 (no diversity) to 5 (high diversity). Coherence indicates the logical flow of conversation, Cooperativeness reflects the quality of participant interactions, and diversity represents the richness and depth of the conversation. In the evaluation process, each conversation sample is input into the LLM, which outputs scores from 1 to 5 for each of the three metrics mentioned above. We utilized GPT-4 for this evaluation."}, {"title": "4.2.3. Human evaluation", "content": "To evaluate the quality and effectiveness of conversations in the Murder Mystery scenarios, the authors developed original evaluation criteria and conducted detailed evaluations of each conversation from the perspectives of information-sharing efficiency and discussion progression. Our evaluation criteria were designed based on the hypothesis that smooth conversation facilitates logical discussion, ultimately leading to the game's objective of solving the case. A portion of these evaluation criteria is shown in Figure 7.\nThe evaluation of information-sharing efficiency measures the activity of information exchange, which forms the foundation for in-depth discussion. Specifically, points are awarded when character-specific information is appropriately disclosed during conversation. This quantitatively evaluates the quality of information sharing that serves as the basis for case-solving reasoning.\nThe evaluation of discussion progression measures the development of reasoning based on shared information and the progress toward solving the case. Points are awarded when characters demonstrate logical reasoning and insights, or when significant facts are revealed. This enables quantitative evaluation of progress toward the task of uncovering the truth behind the case.\nThis methodology enables systematic evaluation of the entire process, from information sharing through logical reasoning to case resolution. In particular, by considering the specific characteristics of murder mysteries, we can more concretely verify the effectiveness of our proposed method."}, {"title": "5. Results", "content": "We compared three types of generated conversations: those with equal speaking turns and opportunities (EQUAL) as detailed in Section 4.1, those generated using only the Self-Selection mechanism (SS), and those generated using our proposed approach incorporating the Current Speaker Selects Next mechanism (CSSN-or-SS). Examples of generated conversations are shown in Figures 8, 9, and 10.\nExamining the EQUAL condition example in Figure 8, in Turn 1, Masato asks Takeshi \u201cDid something happen?", "Where was everyone?": "n Turn 2, Masato explains his behaviors in response to the Turn 1 question. In Turn 3, Kozue asks Yukiko \u201cWhat were you doing?\u201d However, from Turn 4 to Turn 7, Kozue continues to ask questions. This pattern of consecutive questions from the same speaker without consideration for the second pair part of an adjacency pair was frequently observed. This is due to such an agent with high importance scores monopolizing turns.\nIn the CSSN-or-SS condition example shown in Figure 10, Turn 1 shows Kozue asking all participants \u201cDid anyone see anything in the lounge last night?", "I was in the lounge but didn't see anything.": "n Turn 3, Kozue uses a sequence-closing third [45] saying \"I see, thank you\u201d to conclude the conversational sequence with Masato. Kozue then addresses Yukiko, asking a similar question based on memory that Yukiko had said she would \u201cgo to the lounge."}, {"title": "6. Discussion", "content": "The experimental results of this study clearly demonstrate that the next-speaker selection mechanism utilizing adjacency pairs in turn-taking systems improves the quality of multi-party conversations in multiple aspects. From the analysis of dialogue breakdowns", "Do you know anything about what Erika might have been hiding?": "Takeshi explained the circumstances of interaction with Erika but avoided addressing the essential answer about what was being hidden.\nSecond", "pairs": "First, the generation of appropriate responses to questions was promoted, enabling logical conversation development. Second, clear turn-taking encouraged active participation in information exchange and problem-solving. Third, the repetition of identical utterances was suppressed, enabling the presentation of opinions from diverse perspectives.\nHowever, it is noteworthy that no significant difference was observed between the CSSN-or-SS condition and EQUAL condition in terms of coherence evaluation. This result"}]}