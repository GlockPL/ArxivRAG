{"title": "Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models", "authors": ["Kuofeng Gao", "Shu-Tao Xia", "Ke Xu", "Philip Torr", "Jindong Gu"], "abstract": "Large Audio-Language Models (LALMs) have unclocked audio dialogue capabilities, where audio dialogues are a direct exchange of spoken language between LALMs and humans. Recent advances, such as GPT-4o, have enabled LALMs in back-and-forth audio dialogues with humans. This progression not only underscores the potential of LALMs but also broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we firstly propose the evaluation of ambiguity handling in audio dialogues that expresses different intentions beyond the same literal meaning of sentences, e.g., \"Really!?\" with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments conducted on 13 LALMs, our analysis reveals that there is still considerable room for improvement in the audio dialogue understanding abilities of existing LALMs. In particular, they struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones.", "sections": [{"title": "1 Introduction", "content": "Large Audio-Language Models (LALMs) have received attention for their abilities to handle various audio-related tasks. In particular, LALMs recently unlock unprecedented capabilities for interactive audio dialogues with humans. These dialogues are defined as a direct exchange of spoken language between LALMs and humans, which fosters a more engaging and dynamic mode of communication. Recent advances, such as GPT-4o, have enabled LALMs to engage in back-and-forth dialogues with humans and can observe various audio characteristics, which broadens their applicability across diverse real-world situations that rely on interactive audio dialogues.\nHowever, given these advancements, there is currently no comprehensive benchmark to evaluate LALMs' performance in handling open-ended audio dialogue understanding. Previous benchmarks on LALMs predominantly focus on their performance in multiple fundamental tasks, such as speech-to-text translation, emotion recognition, and audio question answering with textual prompts, etc. While these tasks are essential, they do not adequately capture the diversity inherent in real-world audio dialogues. The absence of a comprehensive benchmark for evaluating LALMs in open-ended audio dialogues has led to suboptimal comparisons between different LALMs. This gap in evaluation benchmarks hinders the development of existing LALMs.\nFurthermore, open-ended audio dialogues, where users can directly engage with LALMs through audio, constitute a significant portion of real-world interactions. These dialogues can encompass many subjects, such as helpful and daily questions, domain-specific skills, and multiple different languages. Additionally, the variations in intonations or pause positions can allow speakers to express different intentions beyond the same literal meaning of sentences, adding further complexity to the dialogues. Therefore, the ability to handle open-ended audio dialogues effectively is crucial for LALMs to be truly useful in real-world applications. In light of these considerations, there is a pressing need to establish a benchmark that can effectively assess the performance of LALMs in handling these challenges of audio dialogues.\nIn this work, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), a benchmark to evaluate the open-ended audio dialogue understanding for LALMs, which comprises 4 benchmark datasets as follows. (1) The ADU-General dataset assesses the general dialogue understanding of LALMs, including 3 scenarios, i.e., helpful questions to query search engines, daily questions happening among human dialogues, and daily statements without rich contexts. (2) The ADU-Skill dataset evaluates the skill-based dialogue ability, encompassing 12 different skills such as mathematics, physics, coding, etc. (3) The ADU-Multilingual dataset tests the multilingual dialogue understanding, covering 9 languages, including English, French, and Chinese, etc. (4) The ADU-Ambiguity dataset is designed to evaluate the audio dialogue ambiguity handling ability from different phonetic elements, including intonation-based, pause-based, homophone-based, and repetition-based ambiguity. Notably, we firstly analyze the ambiguity within audio dialogues, specifically addressing the challenge of different intentions that share the same literal sentence, such as the word \"Really!?\" spoken with different intonations. In total, ADU-Bench comprises over 20,000 open-ended audio dialogues specifically designed for LALMs."}, {"title": "2 Related Work", "content": "Large Audio-language Models. Large audio-language models (LALMs) typically integrate audio modalities into large language models (LLMs) to extend their capabilities for general-purpose audio and language understanding. LALMs can be broadly classified into two types: end-to-end LALMs and cascaded LALMs. End-to-end LALMs can be further divided into two categories: (1) End-to-end LALMs specialize in audio understanding, which focus on integrating audio modality into LLMs, such as SpeechGPT, BLSP, SALMONN, Qwen-Audio, and Mini-Omni. (2) End-to-end LALMs extend their capabilities beyond audio understanding, which align various modalities into a single LLM, such as PandaGPT and NEXT-GPT. Another approach involves cascaded LALMs like the combination of an automatic speech recognition model, such as Whisper-large, and an LLM, such as GPT-4, to process a wide range of audio types. Our ADU-Bench aims to evaluate their performance in audio dialogue understanding across different domains.\nBenchmarks for LALMs. As various LALMs have emerged, several benchmarks have been developed to evaluate their performance. Specifically, Dynamic-SUPERB is the first dynamic and collaborative benchmark for evaluating instruction-tuning speech models and it primarily focuses on human speech processing. Another benchmark, AIR-Bench, is specifically designed for evaluating LALMs, encompassing multiple audio fundamental tasks and audio question answering. In the latter, LALMs are presented with an audio clip and a related question, requiring them to analyze the audio content and accurately answer the question. However, both benchmarks don't assess the more practical open-ended audio dialogue understanding capabilities of LALMs. To bridge this gap, we propose ADU-Bench, which concentrates on evaluating LALMs in audio dialogue scenarios."}, {"title": "3 Data Collection and Statistics", "content": "ADU-Bench is a comprehensive evaluation benchmark designed to assess the open-ended audio dialogue understanding of LALMs in scenarios where LALMs directly respond to user audio inputs. ADU-Bench consists of 4 datasets, including ADU-General dataset, ADU-Skill dataset, ADU-Multilingual dataset, and ADU-Ambiguity dataset. During data collection, our ADU-Bench contains 20,715 open-ended audio dialogues, comprising over 8,000 real-world recordings alongside synthetic audio samples. The dataset details for ADU-Bench are in Table 1. Each data point within these datasets is presented as a tuple consisting of (audio queries, textual references). The audio queries serve as the input for LALMs, while the textual references function as the expected ground truths. The generation of textual references involves inputting the corresponding textual transcriptions of audio queries into GPT-4 or employing human annotation for ambiguity types. A textual format is chosen for the data construction because ADU-Bench focuses on the understanding of audio dialogues instead of generation.\nThe ADU-General dataset is constructed to evaluate the general dialogue understanding capabilities of LALMs. This dataset comprises 12,000 open-ended audio dialogues, specifically designed to reflect a wide array of inquiries and remarks commonly encountered in life. It covers 3 scenarios as follows. (1) Helpful questions: These are typically aimed at eliciting useful responses from search engines, such as \u201cWho won the most gold medals in the Olympics?\u201d. (2) Daily questions: These represent casual questions that arise in real-life conversations, for example, \"What are you doing on this fine day?\". (3) Daily statements: These include everyday remarks, such as \u201cOne today is worth two tomorrows.\". In particular, daily questions and statements are relatively casual without rich contextual information to represent real-world situations. The construction of this dataset draws from various sources including Alpaca, NQ-Bench, WebGLM, Slue HVB, and Common Voice. To eliminate queries that do not align with the aforementioned categories, we implement a filtering process combining GPT-4 and human inspection.\nThe ADU-Skill dataset is specifically designed to assess the domain-specific skills of LALMs. This dataset comprises 3,750 audio dialogues and encompasses 12 different domains, including Mathematics, Physics, Chemistry, Biology, Computer Science, Coding, Law, Finance, Common Sense, Writing, Roleplay, and Medicine. To cover these diverse domains, we collect sources for these dialogues from GSM8K, MATH, WizardLM, ShareGPT, MBPP, MMLU, HotpotQA and StrategyQA. Notably, in certain domains, particularly Mathematics, Physics, and Coding, some queries involve a high volume of Latex formulas or Python code, which can be challenging to comprehend when transformed into audio. Therefore, we employ GPT-4 and human inspection to filter out queries with an excessive number of Latex formulas or Python code.\nThe ADU-Multilingual dataset aims to evaluate the multilingual dialogue understanding abilities, covering 9 languages: Arabic, Chinese, English, French, German, Japanese, Korean, Russian, and Spanish. This dataset consists of 3,600 audio dialogues. For generation, we first randomly choose 400 different queries in English from ADU-General dataset. Subsequently, these queries are then translated into the other 8 languages using GPT-4. By including multiple languages, this dataset tests LALMs to understand the audio dialogues in various linguistic contexts. Furthermore, the design of this dataset allows for future expansion, enabling the inclusion of additional languages as needed.\nThe ADU-Ambiguity dataset is specifically designed to evaluate the robustness of LALMs in addressing ambiguity from different phonetic elements present in audio dialogues. It is important to note that ambiguity refers to instances where the textual transcriptions alone, without the accompanying audio or contexts, can lead to confusion. However, when considering the phonetic elements or contextual information provided by the audio, these ambiguities can be resolved, leading to a standard, unambiguous response for humans. Concretely, this dataset consists of 1,390 audio dialogues, which can be classified into 4 types of ambiguous situations, as described below. (1) Intonation-based ambiguity: In this case, expressing the same sentence with different intonations leads to different interpretations. For instance, \"What a perfect day for the beach.\" can convey different meanings depending on the intonation used. An uplifting intonation indicates that it is indeed a perfect day, while a disappointed intonation signifies that the conditions are far from ideal for a beach day. (2) Pause-based ambiguity: The placement of pauses can alter the meaning of a sentence. For example, consider the phrase \"professional reviewers and authors.\" Depending on where the pause is placed, it can imply that both the reviewers and authors are smart, or that only the reviewers are smart while the authors are not. The ambiguity arises from the different ways in which pauses can be inserted into the sentence, leading to contrasting interpretations. (3) Homophone-based ambiguity: These are sentences containing words that sound almost the same when spoken but have completely different meanings due to variations in word spelling. For example, the words \"weight\" and \"wait\" sound almost the same but convey different meanings. (4) Repetition-based ambiguity: These sentences contain multiple occurrences of the same word, often leading to confusion. An example of this is, \u201cI saw a man saw a saw with a saw.\" The construction of the ADU-Ambiguity dataset is achieved manually, drawing upon research studies related to phonetics. To annotate textual references, we employ a combination of GPT-4 and manual inspection, ensuring the accuracy and relevance of the references."}, {"title": "4 Evaluation Method", "content": "Given recent studies have demonstrated that the evaluation with LLMs exhibits better alignment with human preferences, we propose to adopt the advanced LLM, GPT-4, to evaluate the quality of the responses generated by LALMs. Concretely, LALMs first are queried with audio queries and generate textual responses directly, or convert audio responses into textual format. Subsequently, we present the textual transcriptions of audio queries, textual references (expected ground truths) generated by GPT-4, and textual responses generated by LALMs into the GPT-4 evaluator. Finally, the GPT-4 evaluator assigns an overall score on a scale of 0 to 10 for each data point. A higher score indicates the better LALMs' capabilities in handling open-ended audio dialogues. The evaluation prompt templates are in Appendix B. To eliminate the position bias arising from the order of references and responses, we perform a second scoring by swapping their positions and report the average results. Moreover, to avoid bias from GPT-4, we also use LLaMA-3-70B-Instruct and Qwen-2-72B-Instruct for evaluation."}, {"title": "5 Results and Analysis", "content": "To benchmark the audio dialogue understanding of existing LALMs, we evaluate 13 foundational models with audio understanding capabilities. These models include PandaGPT-7B, NEXT-GPT-7B, Qwen-Audio-7B, Qwen-Audio-Chat-7B, Mini-Omni-0.5B, SpeechGPT-7B, SALMONN-7B, SALMONN-13B, BLSP-7B, Whisper-large-v3 with LLaMA-2-7B-Chat, with LLaMA-3-8B-Instruct, with LLaMA-3-70B-Instruct, and with GPT-4-0613. Unless stated otherwise, the hyperparameters and setups used during the evaluation process remain consistent with those specified in the original papers of the respective models. For evaluation, we obtain two evaluation scores by swapping references and responses in the prompts for the GPT-4 evaluator and finally report the average scores for each model in Table 2. In addition, to avoid the bias of evaluation only using GPT-4, we apply various open-sourced LLMs for such evaluations, including LLaMA-3-70B-Instruct and Qwen-2-72B-Instruct.\nWe report the experimental results for the performance of 13 different LALMs on audio dialogue understanding in Table 2 and provide a comprehensive analysis of them. Firstly, it can be observed that PandaGPT, NEXT-GPT, and Qwen-Audio exhibit the lowest performances, with an average score value of about 1.00. It illustrates that although PandaGPT and NEXT-GPT are end-to-end LALMs capable of processing a wide range of modalities, their performances on audio dialogue understanding are relatively lower. As for Qwen-Audio, a pre-trained base LALM, its weak capabilities in audio dialogue indicate a potential necessity for more specialized training to enhance its understanding of audio dialogues.\nCompared to them, Mini-Omni-0.5B, SALMONN-7B and Qwen-Audio-Chat show somewhat superior performance. This can be attributed to the fact that Mini-Omni-0.5B, SALMONN-7B, and Qwen-Audio-Chat have been developed under audio instruction tuning, making them suitable for a variety of audio-oriented scenarios. Moreover, SpeechGPT, SALMONN-13B, and BLSP have demonstrated even higher proficiency, as reflected in their average scores all about or exceeding 3.00. Among these, BLSP stands out with the highest average score of 3.85 among all LALMs. As SALMONN increases in size from 7B to 13B, its audio dialogue understanding capabilities also show improvement. In addition, both SpeechGPT and BLSP enable audio dialogue with LLMs using speech and exhibit impressive dialogue capabilities. Therefore, it can achieve enhanced performance when subjected to the targeted audio dialogue tuning for end-to-end LALMs, which highlights the importance of developing training strategies to improve audio dialogue understanding capabilities.\nFurthermore, cascaded LALMs, including LLaMA-2-7B, LLaMA-3-8B, LLaMA-3-70B, and GPT-4 with a Whisper model, obtain higher scores in audio dialogue understanding. Therein, GPT-4 leads the pack with the highest score of 7.66, which indicates that it is the best-performing model among the evaluated LALMs. Following it, LLaMA-3 (including LLaMA-3-8B and LLaMA-3-70B) ranks second, outperforming its predecessor, LLaMA-2. The improved performance of LLaMA-3 to LLaMA-2 highlights the effectiveness of updates in the LLaMA series.\nIn summary, cascaded LALMs outperform end-to-end LALMs with the specializing audio instruction tuning, which in turn surpass end-to-end LALMs with multi-modal understanding beyond audio, which indicates that further modality alignment should be developed for the open-ended audio dialogue understanding."}, {"title": "5.3 Results on Each Dataset", "content": "The ADU-General dataset aims to evaluate the proficiency in general dialogue understanding. Our analysis reveals that LALMs perform better in providing helpful responses to helpful questions compared to daily questions and daily statements. Helpful questions typically seek specific information, whereas daily questions and daily statements represent everyday communication between humans, often characterized by a lack of rich contextual information. This finding suggests that LALMs are more adept at handling audio dialogues that require precise information retrieval, while their performance in everyday dialogues remains an area for improvement. In summary, existing open-sourced LALMs understand helpful questions better than daily questions and statements, highlighting the development to better address everyday human interactions.\nThe ADU-Skill dataset is designed to evaluate the skill capabilities of LALMs during audio dialogue. Among all these domains, LALMs demonstrate a relative proficiency in handling topics such as Biology, Computer Science, Law, Finance, Writing, and Medicine. This observation suggests that LALMs possess a certain knowledge foundation in these domains. Meanwhile, these tasks primarily involve language understanding and generation, which align well with the core capabilities of LALMs. Moreover, LALMs exhibit weaker performance when dealing with subjects like Mathematics, Physics, Chemistry, and Coding. This can be attributed to the fact that they all involve mathematical symbols and formulas or programming languages so that LALMs struggle to effectively understand these domain-specific challenges they present. Additionally, LALMs display limitations in areas related to Common Sense and Roleplay. These domains usually require a deeper understanding of human behavior and LALMs lack the ability to infer implicit meanings or cultural nuances that are crucial for accurately understanding and responding to them. In summary, existing open-sourced LALMs have knowledge backgrounds in some domains but they face challenges in subjects involving mathematical notations or programming languages, as well as areas requiring a deeper understanding of human behavior.\nThe ADU-Multilingual dataset aims to evaluate multilingual capabilities of LALMs during audio dialogues. It can be observed that all LALMs perform best in English due to the massive amount of training data in English. Subsequently, the performance is followed by German, Spanish, French, and Russian. We conjecture that this is because these languages all belong to the Indo-European languages that LALMs can understand to a certain extent. As for other languages, LALMs exhibit weaker performance which illustrates that they need to be incorporated into the development of LALMs. In conclusion, existing open-sourced LALMs struggle with their multilingual capabilities, highlighting further research to consider various linguistic contexts when developing LALMs.\nThe ADU-Ambiguity dataset is designed to assess how well LALMs handle 4 types of ambiguity during audio dialogue, including intonation-based, pause-based, homophone-based, and repetition-based ambiguity. Overall, LALMs exhibit relatively better performance in handling repetition-based ambiguity, while their performance in managing other types of ambiguities is weaker. This observation suggests that LALMs can more effectively resolve ambiguities that do not involve phonetic elements, such as repetition-based ambiguity, which only has multiple words in an audio. However, when it comes to the other three types of ambiguities, including intonation-based, pause-based, and homophone-based, LALMs struggle to handle them effectively. For homophone-based ambiguity, it is difficult for LALMs to distinguish the words that have almost the same pronunciation. For the other two types of ambiguity, LALMs can not perceive the variations in intonations or pause positions, which can lead to expressing different intentions beyond the same literal meaning of sentences. When faced with these ambiguities, relatively advanced LALMs like GPT-4 often generate responses that encompass multiple possible interpretations. This is due to their inability to effectively distinguish between the different meanings based on phonetic elements. In summary, existing LALMs, including GPT-4 with a Whisper model, display limitations in handling the audio dialogue ambiguity in different phonetic elements, including intonations, pause positions, and homophones."}, {"title": "5.4 Ablation Study", "content": "We compare the audio dialogue understanding capabilities of SALMONN and LLaMA-3 with a Whisper model with different sizes.  However, it is noted that SALMONN-7B outperforms its larger counterpart, SALMONN-13B on Code within ADU-Skill dataset. Similarly, LLaMA-3-8B achieves superior performance than LLaMA-3-70B on Common Sense within ADU-Skill dataset and non-Indo-European languages within ADU-Multilingual dataset. These observations suggest that while a larger model size generally contributes to better overall audio dialogue understanding performance, it can also introduce performance losses in certain domains.\nFor the audio dialogues difficult to obtain directly, we choose to adopt a synthetic algorithm to generate corresponding audios, as detailed in Appendix A.  We observe that there is no considerable difference in the performance of LALMs when processing real-world and synthetic audio. In conclusion, both real-world audio and synthetic audio can effectively serve as evaluation sources for audio dialogue understanding.\nFor evaluation, we choose to adopt GPT-4 as the evaluator. To evaluate the consistency between the evaluations of GPT-4 and human judgments, we conduct a human evaluation study as follows.  Our analysis reveals that the pairwise preference consistency achieves a score above 85%, indicating that GPT-4 evaluation aligns well with human judgments. The details are in Appendix D. We provide the evaluation results by LLaMA-3-70B-Instruct and Qwen-2-72B-Instruct and the corresponding human evaluation study in Appendix E.\nTo mitigate potential biases from the order of references and responses in the evaluation GPT-4 prompt, we query the GPT-4 evaluator to generate two scores by adjusting their positions.  We observe that despite using the same references and responses, the GPT-4 evaluator generates different scores after adjusting the positions. This suggests the existence of a positional bias, particularly when responses are placed before the references. The observation highlights the importance of conducting a second scoring to address this bias."}, {"title": "6 Conclusion", "content": "We present ADU-Bench designed to evaluate the audio dialogue understanding of LALMs. It provides over 20,000 open-ended audio dialogues for LALM assessment for 3 general scenarios, 12 skills, 9 languages, and 4 ambiguity types. Our extensive experiments on 13 LALMs reveal that there is still significant room for improvement in their audio dialogue understanding, which underscores the direction of continued research in LALMs."}, {"title": "A Generation Details for Synthetic Datasets", "content": "Our ADU-Bench contains 20,715 open-ended audio dialogues, comprising over 8,000 real-world recordings alongside synthetic audio samples. In this section, we introduce the generation details for the synthetic datasets.\nTo generate synthetic datasets for ADU-General dataset, ADU-Skill dataset, and ADU-Multilingual dataset, we first adopt GPT-4 and human inspection to obtain the related textual dialogues for each dataset. Then, enclose them in the Speech Synthesis Markup Language (SSML) by human coding, where SSML is an XML-based markup language specifically designed for speech synthesis applications. Subsequently, execute the program code using Python interpreter with public SSML service provided by Microsoft Azure to convert them into audio dialogues. Furthermore, to emulate real-world scenarios, we consider a wide array of variables for synthetic audio. They include 2 genders (male and female), 4 different speakers (2 men and 2 women), 4 emotions (calm, excited, angry, and sad), 3 speech rates (standard and \u00b110%), 3 pitch levels (standard and \u00b110%), and 3 volume levels (standard and \u00b110%). During the generation of each dataset, a combination of these audio generation characteristics is randomly selected to create each audio data, ensuring diversity in the audio dialogues. Therefore, this generation method not only provides a scalable solution for generating synthetic audio datasets but also ensures a rich diversity that closely mirrors real-world audio dialogues.\nTo construct the ADU-Ambiguity dataset, we first identify four types of ambiguity handling from phonetics and phonology books. These include ambiguity stemming from intonation, pause positions, homophones, and repetition. Based on the examples and principles outlined in these references, we then manually craft or use GPT-4 to generate many textual data instances representing these ambiguity types.\nTo convert these textual instances into audio samples, we leverage the Speech Synthesis Markup Language (SSML) and use a publicly available SSML service. Specifically:\n\u2022 For intonation-based ambiguity, we use the SSML tags <prosody> to adjust the intonation elements of the audio.\n\u2022 For pause-based ambiguity, we use the SSML tags <break> to insert pauses within the audio.\n\u2022 For homophone-based and repetition-based ambiguity, we are able to directly generate the audio samples without the need for specialized SSML markup.\nFinally, we conduct a manual validation process to ensure the quality and correctness of the generated audio samples. This involves having human annotators listen to the samples and verify that the intended ambiguity is successfully conveyed through the audio."}, {"title": "B Prompts for Evaluation", "content": "The score judgment is based on criteria including helpfulness, relevance, accuracy, and comprehensiveness, comparing the references and generated responses. The evaluation prompt for the first scoring is as follows.\nSystem Prompt\nYou are a helpful and precise assistant for checking the quality of the answer.\nPrompts for Evaluation in ADU-Bench\nPlease evaluate the following LALMs' response for the user query and a reference is provided.\nQuery: Textual Transcriptions\nReference: Textual References\nResponse: Textual Responses\nPlease rate the helpfulness, relevance, accuracy, and comprehensiveness of the LALMs' response. Please provide an overall score on a scale of 0 to 10, where a higher score indicates better overall performance. Do not provide any other output text or explanation. Only provide the score.\nOutput:\nThe evaluation prompt for the second scoring is as follows. To eliminate the position bias, we swap"}, {"title": "C Details of Experimental Settings", "content": "To benchmark the audio dialogue understanding of existing LALMs, we assess the performance of 13 LALMs across all 4 datasets within ADU-Bench. Unless stated otherwise, the hyperparameters and setups used during the evaluation process remain consistent with those specified in the original papers of the respective models. For the evaluation, LLaMA-2-7B-Chat, LLaMA-3-8B-Instruct, and LLaMA-3-70B-Instruct are run on 8 NVIDIA A100 40GB GPUs with vLLM library while other open-sourced models are run on a single NVIDIA A100 40GB GPU. By default, our evaluation method employs gpt-4-0613 as the GPT-4 evaluator by calling the API."}, {"title": "D Human Evaluation Study Details", "content": "We conduct a human evaluation study to evaluate the consistency between the evaluations of GPT-4 and human judgments. We also provide a detailed result in Table 3. For the evaluation datasets, we randomly choose 5 audio queries from each domain in ADU-Bench, and finally obtain 140 audio queries. Since ADU-Multilingual contains multiple languages, it is difficult for human testers to understand each language. Hence, we provide the textual transcriptions and allow them to use the translation tools for evaluation. We show them for 10 English speakers. Furthermore, we carefully consider the ethical aspects and potential risks associated with the research involving human subjects. The information we collect is only the preference results and does not involve any personal information.\nWhen selecting participants, there are no requirements for their qualifications, experience, or technical abilities; all participants are adults capable of giving informed consent. We clearly inform the participants of the experiment's content and corresponding compensation before the experiment begins, and we will not cause them any physiological or psychological harm. We randomly select participants within the university campus, informing them of the experiment content, purpose, compensation, and other information. Participants voluntarily decide whether to participate in the experiment after reading the Ethics Informed Consent Form and Ethics Study Information Sheet. The compensation we provide to the participants is 1.5 times the local minimum hourly wage standard.\nThe instructions given to participants are as follows:\nWelcome to our human evaluation study! Your"}, {"title": "E Evaluation Results by LLAMA-3-70B-Instruct and Qwen-2-72B-Instruct", "content": "To avoid the bias of evaluation only using GPT-4, we apply various open-sourced LLMs for such evaluations, including LLaMA-3-70B-Instruct and Qwen-2-72B-Instruct. Our analysis shows that the evaluation scores obtained using these LLMs are mostly consistent with the conclusions drawn from the GPT-4 evaluation.  All these results indicate that strong LLM evaluations, especially those involving GPT-4, align well with human judgments for audio dialogue understanding. Besides, note that GPT-4 based evaluation is shown to be effective in many areas."}, {"title": "F Limitations", "content": "The primary limitation of this work lies in the analysis being restricted to a limited number of LALMs, due to constraints such as the availability of usable code, model weights, and the resources required for extensive experimentation. Future work could address this by exploring a broader and more diverse range of LALMs to deepen the analysis. Moreover, ADU-Bench currently focuses on evaluating performance in areas such as daily dialogues, skill capabilities, multilingual proficiency, and natural robustness in handling ambiguities. In the future, we aim to expand ADU-Bench to include additional evaluation domains. For example, if LALMs are integrated into embodied AI systems, it will be crucial to emphasize security-related evaluations. Concretely, we leave the investigation whether LALMs are vulnerable to adversarial misdirection when processing partially corrupted audio inputs in the future work. Additionally, we will explore how LALMs"}, {"title": "G Broader Impacts", "content": "Our ADU-Bench has been carefully curated to ensure that it does not contain any words or content that discriminate against any individual or group. The prompts used in our experiments, as detailed in Appendix B, have been meticulously reviewed to emphasize that none of them contain any discriminatory language or themes. Moreover, we have taken the necessary precautions to ensure that the prompts used in our work do not negatively impact anyone's safety or well-being. Furthermore, all the codes comply with the MIT License. This commitment to ethical considerations in our research contributes to the responsible development and advancement of LALMs."}, {"title": "\u0397 More Details of ADU-Bench", "content": "The details of ADU-Bench, including the number of each domain within ADU-Bench are in Table 8."}, {"title": "I More Overall Results", "content": "The overall results of the first and second scoring are shown in Table 9 and Table 10."}, {"title": "J More Results on Each Dataset", "content": "The results on each dataset of the first and second scoring are shown in Table 11, Table 12, Table 13, Table 14, Table 15, Table 16, Table 17, Table 18, Table 19, Table 20, Table 21, and Table 22."}]}