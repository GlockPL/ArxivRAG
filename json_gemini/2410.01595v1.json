{"title": "KNOBGEN: CONTROLLING THE SOPHISTICATION OF ARTWORK IN SKETCH-BASED DIFFUSION MODELS", "authors": ["Pouyan Navard", "Amin Karimi Monsefi", "Mengxi Zhou", "Wei-Lun Chao", "Alper Yilmaz", "Rajiv Ramnath"], "abstract": "Recent advances in diffusion models have significantly improved text-to-image (T2I) generation, but they often struggle to balance fine-grained precision with high-level control. Methods like ControlNet and T2I-Adapter excel at following sketches by seasoned artists but tend to be overly rigid, replicating unintentional flaws in sketches from novice users. Meanwhile, coarse-grained methods, such as sketch-based abstraction frameworks, offer more accessible input handling but lack the precise control needed for detailed, professional use. To address these limitations, we propose KnobGen, a dual-pathway framework that democratizes sketch-based image generation by seamlessly adapting to varying levels of sketch", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion models (DMs) have revolutionized text-to-image (T2I) generation by generating visually rich images based on text prompts, excelling at capturing various levels of detail from textures to high-level semantics (Saharia et al., 2022; Rombach et al., 2022; Nichol et al., 2021; Ramesh et al., 2021; Navard & Yilmaz, 2024). Despite their success, one of the primary limitations of these models is their inability to precisely convey spatial layout of the user-provided sketches. While text prompts can describe scenes, they struggle to capture complex spatial features, which makes it challenging to align generated images with user intent. This is particularly intensified when these users vary in skill and experience (Chowdhury et al., 2023; Song et al., 2017; Yang et al., 2023; Jiang et al., 2024).\nTo improve spatial control, sketch-conditioned DMs like ControlNet (Zhang et al., 2023), T2I-Adapter (Mou et al., 2024), and ControlNet++ (Li et al., 2024) have introduced mechanisms to allow users to input sketches that guide the generated image. However, these approaches primarily cater to artistic sketches with intricate details, which poses a challenge for novice users. When presented with rough sketches, these models rigidly align to unintentional flaws, producing results that misinterpret the user's intent and fail to achieve the desired visual outcome. Furthermore, we observed that the quality and alignment of the generated images with the input sketch are highly sensitive to the weighting parameter that governs the model's dependence on the condition, Figure 2."}, {"title": "2 RELATED WORK", "content": "In a nutshell, existing methods for sketch-based image generation tend to focus on either end of the user-level spectrum. Professional-oriented models like ControlNet and T2I-Adapter are designed to handle only artistic-grade sketches Fig. 3.a, while amateur-oriented approaches Koley et al. (2024), cater to novice sketches without text guidance Fig. 3.b. These methods often fail to integrate both"}, {"title": "2.1 DIFFUSION MODELS", "content": "Recent advances in DM have enabled high-quality image generation with improved sample diversity (Ho et al., 2020; Dhariwal & Nichol, 2021; Ho & Salimans, 2021; Nichol et al., 2021; Saharia et al.,"}, {"title": "2.2 TEXT-TO-IMAGE DIFFUSION", "content": "In addition to producing high-quality and diverse samples, DMs offer superior controllability, especially when guided by textual prompts (Rombach et al., 2022; Xue et al., 2023; Chen et al., 2024; Podell et al., 2024; Esser et al., 2024). Imagen (Saharia et al., 2022) employs a pretrained large language model (e.g., T5 (Raffel et al., 2020)) and a cascade architecture to achieve high-resolution, photorealistic image generation. LDM (Rombach et al., 2022), also known as Stable Diffusion (SD), performs the diffusion process in the latent space with textual information injected into the underlying UNet through a cross-attention mechanism, allowing for reduced computational complexity and improved generation fidelity. To further address challenges when handling complex text prompts with multiple objects and object-attribution bindings, RPG (Yang et al., 2024) proposed a training-free framework that harnesses the chain-of-thought reasoning capabilities of multimodal"}, {"title": "2.3 CONDITIONAL DIFFUSION WITH SEMANTIC MAPS", "content": "As textual prompts often lack the ability to convey detailed information, recent research has explored conditioning DMs on more complex or fine-grained semantic maps, such as sketches, depth maps, normal maps, etc. Works such as T2I-Adapter (Mou et al., 2024), ControlNet (Zhang et al., 2023), and SCEdit (Jiang et al., 2024), leverage pretrained T2I models but employ different mechanisms to interpret and integrate these detailed conditions into the diffusion process. UniControl (Qin et al., 2023) proposes a task-aware module to unify N different conditions (i.e. N = 9) in a single network, achieving promising multi-condition generation with significantly fewer model parameters compared to a multi-ControlNet approach. While Koley et al. (2024) attempts to democratize sketch-based diffusion models, their approach faces several significant limitations, as discussed in the Introduction section. In contrast, our dual-pathway method integrates both fine-grained and coarse-grained sketch conditions while maintaining the option for textual prompts. This design offers greater flexibility and control, accommodating users ranging from amateurs to professionals."}, {"title": "3 METHOD", "content": ""}, {"title": "3.1 PRELIMINARY", "content": "Stable Diffusion Diffusion models (Ho et al., 2020) define a generative process by gradually adding noise to input data $x_0$ through a Markovian forward diffusion process $q(z_t|x_0)$. At each timestep t, noise is introduced into the data as follows:\n$z_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, \\epsilon \\sim \\mathcal{N}(0, I),$ (1)\nwhere $\\epsilon$ is sampled from a standard Gaussian distribution, and $\\bar{\\alpha}_t = \\Pi_{s=0}^{t}\\alpha_s$, with $\\alpha_t = 1 - \\beta_t$ representing a differentiable function of the timestep t. The diffusion process gradually converts $x_0$ into pure Gaussian noise $z_T$ over time.\nThe training objective for diffusion models is to learn a denoising network $\\epsilon_\\theta$ that predicts the added noise $\\epsilon$ at each timestep t. The loss function, commonly referred to as the denoising score matching objective, is expressed as:\n$\\mathcal{L}(\\epsilon_0) = \\sum_{t=1}^{T} \\mathbb{E}_{z_0\\sim q(x_0),\\epsilon \\sim \\mathcal{N}(0,I)}[|| \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon) - \\epsilon ||^2].$ (2)\nIn controllable generation tasks (Zhang et al., 2023; Mou et al., 2024), where both image condition $c_v$ and text prompt $c_t$ are provided, the diffusion loss function can be extended to include these conditioning inputs. The loss at timestep t is modified as:\n$\\mathcal{L}_{train} = \\mathbb{E}_{z_0,t,c_t,c_v,\\epsilon \\sim \\mathcal{N}(0,I)}[|| \\epsilon_\\theta(z_t, t, c_t, c_v) - \\epsilon ||^2],$ (3)\nwhere $c_v$ and $c_t$ represent the visual and textual conditioning inputs, respectively.\nDuring inference, given an initial noise vector $z_T \\sim \\mathcal{N}(0, I)$, the final image $x_0$ is recovered through a step-by-step denoising process (Ho et al., 2020), where the denoised estimate at each step t is calculated as:"}, {"title": "3.2 DUAL PATHWAY", "content": "Our model introduces a dual-pathway framework that harmonizes high-level semantic abstraction with precise, low-level control over visual details, Figure 5. The integration of the CGC module and FGC module enables KnobGen to adaptively inject high-level semantics and low-level features throughout the denoising process. This design ensures that the model can scale its output complexity based on user input, thus supporting a wide spectrum of sketch sophistication levels, from amateur to professional-grade sketches."}, {"title": "3.2.1 MACRO PATHWAY", "content": "Diffusion models typically rely on text-based conditioning using CLIP text encoders (Radford et al., 2021) to capture high-level semantics (Ramesh et al., 2021; Saharia et al., 2022; Nichol et al., 2021), but this approach often misses out on structural cues inherent to other modalities, such as sketches. Although models such as CLIP (Radford et al., 2021) encode visual features and textual semantics, they remain biased toward coarse-grained features (Bianchi et al., 2024; Wang et al., 2023). In our CGC module, Figure 5.B, we used this fact to our advantage to fuse a high-level visual and linguistic understanding to control DM generation by incorporating both text and image embeddings through a cross-attention mechanisms."}, {"title": "3.3 MODULATOR AT TRAINING", "content": "One of the key innovations in KnobGen is the tanh-based modulator, which regulates the contributions of the micro and macro pathways during training, Fig 5.A. Based on our experiments in section 4.4, the incorporation of micro pathway in the early epochs of training process overshadows the effect of our macro pathway. Not only does this phenomenon lead to a model that overfits low-level features of the sketch, but it also prevents the model from generalizing to broader spatial and conceptual features. To mitigate this, we employ a modulator that progressively increases the impact of the Micro Pathway, i.e. the FGC module, during training. The modulator is based on a smooth tanh function:\n$m_t = m_{min} + \\frac{1}{2} [1+tanh(\\kappa(\\frac{t}{T}-\\beta))] (m_{max} - m_{min})$ (5)\nHere, t is the current epoch, T is the total number of epochs of training, $\\kappa = 6$, $\\psi \\in [-3,3]$, $m_{min} = 0.2$ and $m_{max} = 1$ where $m_{min}$ and $m_{max}$ define the range within which the modulator effect (in percent), i.e. $m_t$, will vary over the course of the epochs. In order to choose $m_{min}$, we heuristically found that the maximum lower bound for negligible effect of the FGC is at $m_{min} = 0.2$.\nWe did not conduct an extensive hyperparameter search for $m_{min}$ and only chose this value based on our observation of different case studies, Figure 2. As seen in Figure 5.A, the module ensures that diffusion is more affected by the Macro Pathway and less by the Micro Pathway in the early stages of training. As the training progresses, $m_t$ for the Micro Pathway approaches 1 and as a result our FGC module will have an equal impact in the training as that of the CGC. By gradually modulating the influence of the Micro Pathway, we prevent the premature weakening of high-level spatial layout presented by the Macro Pathway, and ensure that both pathways contribute optimally throughout the training process. The effectiveness of our modulator is experimented in section 4.4."}, {"title": "3.4 INFERENCE KNOB", "content": "In typical diffusion models, the early denoising steps during inference focus on generating high-level spatial features, while the later steps refine finer details (Ho et al., 2020; Meng et al., 2021). In our dual-pathway model, this mechanism is explicitly implemented by our proposed inference-time Knob. This is essentially a user-controlled tool that determines the range of how much abstraction or rigid alignment with respect to the input sketch is desired by the user, Fig 5.C."}, {"title": "4 EXPERIMENT", "content": "We conducted several qualitative and quantitative experiments to validate the effectiveness of KnobGen. The qualitative experiments showcase the effectiveness of our approach in guiding the DM based across different sketch complexities. The qualitative experiments evaluate our model against widely-used baselines on different generation metrics such as CLIP and FID scores. We used pretrained ControlNet and T2I-Adapter as our FGC module throught all our experimentation. According to the parameters defined in section 3.4, $\\gamma = 20$ and $S = 50$. These values were heuristically selected and were used consistently in all experiments and baselines.\nThe extension of the qualitative experiments is available in the Appendix (C). Furthermore, details about the setup used in the training and evaluation are discussed at length in the Appendix (A)."}, {"title": "4.1 QUALITATIVE RESULTS", "content": "Our qualitative results demonstrate the flexibility and effectiveness of KnobGen in handling varying sketch qualities. As shown in Figure 1, KnobGen is able to seamlessly adapt to sketches from rough amateur drawings to refined professional ones, highlighting its ability to cover the entire spectrum of user expertise. Figure 6 illustrates the impact of our knob mechanism, where increasing the knob value (left to right) progressively improves the fidelity to the sketch input. This dynamic adjustment enables precise control over the level of detail, allowing users to fine-tune generation outputs. More qualitative results are provided in the Appendix (C.2)."}, {"title": "4.2 COMPARISON VS. BASELINES", "content": "In order to conduct a fair comparative study, we evaluated KnobGen against baselines such as (Mou et al., 2024; Li et al., 2024; Zhang et al., 2023) on professional-grade sketches, novice ones and a spectrum in between. Figure 4 illustrates the superior quality of the novice-based sketch conditioning using our method against all the other baselines. KnobGen not only captures the spatial layout of the input sketch thanks to the CGC module but also extends beyond it by generating fine-grained details through the FGC module which ultimately produces a naturally appealing images. Whereas the baselines either rigidly conditions themselves on the imperfect input sketch or does not follow the spatial layout desired by the user."}, {"title": "4.3 QUANTITATIVE RESULTS", "content": ""}, {"title": "4.4 ABLATION STUDY", "content": "One of the key innovations in our methodology is the introduction of the Modulator, a mechanism designed to enhance the training process of our proposed CGC module. We conducted an experiment where we trained two versions of KnobGen with Modulator and without it. To assess the effectiveness of the Modulator at the inference, we excluded the FGC module after 20 denoising steps in the image generation process (S = 50, and $\\gamma = 20$, please refer to section 3.4). Excluding the FGC module imposes the conditioning of DM to be done by the CGC module. This experimental configuration demonstrates the power of our CGC module."}, {"title": "5 CONCLUSION", "content": "In this paper, we presented KnobGen, a dual-pathway framework designed to address the limitations of existing sketch-based diffusion models by providing flexible control over both fine-grained and coarse-grained features. Unlike previous methods that focus on detailed precision or broad abstraction, KnobGen leverages both pathways to achieve a balanced integration of high-level semantic understanding and low-level visual details. Our novel modulator dynamically governs the interaction between these pathways during training, preventing over-reliance on fine-grained information and ensuring that coarse-grained features are well-established. Additionally, our inference knob mechanism offers user-friendly control over the level of professionalism in the final generated image, allowing the model to adapt to a spectrum of sketching abilities\u2014from amateur to professional. By incorporating these mechanisms, KnobGen effectively bridges the gap between user's input and model robustness. Our approach sets a new standard for sketch-based image generation, balancing precision and abstraction in a unified, adaptable framework."}, {"title": "APPENDIX", "content": "In this appendix, we provide additional details about the model architecture and supplementary results that further demonstrate the robustness of our approach. These sections aim to provide a deeper understanding of the technical components and showcase more comprehensive comparisons.\n\u2022 Appendix A: details the setup of training and evaluation in KnobGen.\n\u2022 Appendix B: expands on the details of the proposed CGC module.\n\u2022 Appendix C: provides more qualitative results."}, {"title": "A SETUP", "content": "Dataset:\nWe utilized the MultiGen-20M dataset, as introduced by Qin et al. (2023), to train and evaluate our model. The dataset offers various conditions, making it a suitable choice for our approach. We selected 20,000 images for training, focusing specifically on those with the Holistically-nested Edge Detection (HED) (Xie & Tu, 2015) condition. However, we modified the KnobGen condition by applying a thresholding technique, where pixels below a threshold value of 50 were set to zero, and those above were set to one. This threshold value was chosen through simple visual comparisons of several samples using different thresholds, allowing us to identify the most effective value. This modification essentially transforms the HED condition into a sketch. For evaluation, we curated two distinct sets of images. The first evaluation set consisted of 500 randomly selected samples, which are similar to a sketch drawn by a seasoned artist (we followed the thresholding technique for this part), allowing us to measure our model's effectiveness in professional settings. To further test the robustness and adaptability of our approach, we compiled a second evaluation set of 100 hand-drawn images created by non-professional individuals. This diverse testing set enabled us to demonstrate the model's ability to generalize across a broad spectrum of users, ensuring it can handle both professionally designed and amateur drawings with high robustness.\nBaselines:\nIn this work, we evaluate the performance of our proposed model against several state-of-the-art (SOTA) diffusion-based models. Specifically, we conduct both qualitative and quantitative comparisons with prominent models such as ControlNet (Zhang et al., 2023), T2I-Adapter (Mou et al., 2024), AnimateDiff (Guo et al., 2023), UniControl (Qin et al., 2023), and ControlNet++ (Li et al., 2024). These models have achieved significant advances in fine-grained control of image generation by incorporating sketch-based conditions into the diffusion process. Since AnimateDiff is a video-based DM, we only use the first frame of the generated video by it as the comparison point.\nEvaluation:\nWe perform qualitative and quantitative evaluation. In the qualitative evaluation, we compare our model's performance across different scenarios of varying input conditions and complexities. For quantitative evaluation, we utilize several metrics to assess the quality of the generated images. First, we calculate the Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017; Karras et al., 2019), which measures the similarity between generated and natural images using a pre-trained InceptionV3 model (Szegedy et al., 2016). Lower FID values indicate better generation quality; we used Hukkel\u00e5s (2020) implementation for our evaluation, which used the default pre-trained InceptionV3 model available in Pytorch (Paszke et al., 2017). To evaluate the alignment between the generated images and the text prompts, we use CLIP (Radford et al., 2021), specifically the pre-trained DetailCLIP model (Monsefi et al., 2024) with a Vision Transformer (ViT-B/16) backbone. Higher CLIP scores signify better alignment between the generated images and their corresponding prompts. Finally, we assess the realism and aesthetic quality of the generated images using the metric proposed by (Ke et al., 2023), where higher scores reflect more realistic and visually appealing images.\nImplementation Details:\nOur proposed KnobGen framework is built on top of Stable Diffusion v1.5 (Rombach et al., 2022), with the original parameters kept frozen throughout training. For the Fine-Grained Controller (FGC) module, we employed two different pre-trained models to demonstrate the flexibility and effectiveness of our approach across multiple setups. Specifically, we integrated ControlNet (Zhang et al., 2023) and T2I-Adapter (Mou et al., 2024), both of which had"}, {"title": "B MODEL ARCHITECTURE", "content": "The CFC module plays a critical role in our model by integrating and aligning visual and textual information for effective image generation. The primary goal of the CFC is to ensure that features derived from both the input sketch image and the text prompt are jointly fused, allowing the model to generate more contextually relevant and visually coherent outputs. The CFC module has around 100M trainable parameters."}, {"title": "C MORE QUALITATIVE RESULT", "content": "This section contains more qualitative results to complement the evaluations presented in the main paper. We provide visual examples of different use cases, including scenarios involving amateur and professional sketches."}, {"title": "C.1 INFERENCE KNOB MECHANISM FOR BASELINES", "content": "One of the important ablation studies was to evaluate the performance of fine-grained controller models, such as the T2I-Adapter, when they utilize our Knob mechanism. This ablation study was particularly performed to demonstrate the effectiveness of our proposed CGC module.\nModels such as T2I-Adapter are traditionally designed for precise, detail-oriented image generation but lack the flexibility to accommodate broader, more abstract inputs like rough sketches or varying user skills. To explore this issue, we integrated the Knob system into the T2I-Adapter model without our CGC module.\nFigure 9 showed that while the T2I-Adapter performs exceptionally well in generating high-fidelity images from professional-grade inputs, it struggles to maintain this quality when dealing with rougher or less detailed sketches. This limitation arises from the absence of a Macro Pathway in the T2I-Adapter's architecture, which makes the model overly reliant on precise input details. Without the ability to capture broader, high-level semantic information through a coarse-grained approach, the model becomes highly sensitive to adjustments made by the Knob mechanism. As a result, T2I-Adapter fails to deliver consistently good results across a diverse range of users, particularly those providing amateur or less-defined sketches. Additionally, we observed that after a certain point, increasing the Knob value no longer meaningfully affects the generation output. This suggests that the sketch condition in T2I-Adapter influences the generation primarily in the early denoising steps, with diminishing effects in the later steps. However, further investigation of this behavior is outside the scope of this study.\nWhile the Knob system is designed to balance coarse and fine-grained controls dynamically, the lack of a dedicated coarse-grained module in T2I-Adapter causes the model to lose spatial coherence when we apply our Knob mechanism for it, especially when the knob has low value. This issue became particularly evident when trying to generate images based on prompt only, as the model struggled to infer the missing spatial structure, leading to distorted or incoherent outputs.\nIn contrast, the KnobGen framework, including the CGC and FGC, demonstrated superior flexibility and performance. By incorporating both high-level abstractions and detailed refinements, KnobGen"}, {"title": "C.2 MORE QUALITATIVE RESULTS", "content": "In this section, we present additional qualitative results to demonstrate the effectiveness and versatility of our proposed KnobGen framework further. Figure 10 and 11 showcases the model's ability to handle a wide range of input sketches, from highly detailed professional-grade drawings to rough, amateur sketches."}]}