{"title": "AIMDIT: MODALITY AUGMENTATION AND INTERACTION VIA MULTIMODAL\nDIMENSION TRANSFORMATION FOR EMOTION RECOGNITION IN CONVERSATIONS", "authors": ["Sheng Wu", "Jiaxing Liu", "Longbiao Wang", "Dongxiao He", "Xiaobao Wang", "Jianwu Dang"], "abstract": "Emotion Recognition in Conversations (ERC) is a popular\ntask in natural language processing, which aims to recognize\nthe emotional state of the speaker in conversations. While\ncurrent research primarily emphasizes contextual modeling,\nthere exists a dearth of investigation into effective multi-\nmodal fusion methods. We propose a novel framework called\nAIMDIT to solve the problem of multimodal fusion of deep\nfeatures. Specifically, we design a Modality Augmenta-\ntion Network which performs rich representation learning\nthrough dimension transformation of different modalities and\nparameter-efficient inception block. On the other hand, the\nModality Interaction Network performs interaction fusion of\nextracted inter-modal features and intra-modal features. Ex-\nperiments conducted using our AIMDiT framework on the\npublic benchmark dataset MELD reveal 2.34% and 2.87%\nimprovements in terms of the Acc-7 and w-F1 metrics com-\npared to the state-of-the-art (SOTA) models.", "sections": [{"title": "1. INTRODUCTION", "content": "Emotion recognition in conversations (ERC) is a novel task\nwithin the realm of natural language processing, holding a\npivotal role in human-computer interaction [1] and social me-\ndia analysis [2]. There is an increasing focus on multimodal\nERC, which involves leveraging text, audio, and visual infor-\nmation to achieve more accurate emotion recognition.\nMultimodal data tends to be asynchronous, with distribu-\ntional gaps in text, audio, and visual modal data, which tends\nto introduce information redundancy [3, 4]. In ERC, most\nof the work focuses on context modeling based on sequence\nmodels or graph models and often ignores the work on mul-\ntimodal fusion [5-7]. Although these early works can realize\nmultimodal data fusion by simple early-late fusion [8-11],\nthey cannot extract the inter-modal interaction information\nand the simple concatenation operation makes it easy to in-\ntroduce redundant information. In recent years, some ERC\nworks based on multimodal fusion of depth graphs have been\nproposed [12], but due to the characteristics of depth graphs,\nredundant information gradually accumulates in the vector\nspace of each layer and is prone to over-smoothing problem\nthat makes node differentiation insufficient [13].\nWe have observed subtle distinctions in the distribution\nof text, audio, and visual information in emotional expres-\nsions, but they tend to be concentrated over a brief time span.\nFor the purpose of discussion, we standardize the embedding\ndimensions of various modalities, treating vector length as\nthe temporal dimension. Consequently, modal vectors can\nbe conceptualized as 1D time series. Drawing inspiration\nfrom [14-16], we refer to changes in adjacent time intervals\nwithin a single modality and the same time interval across\ndifferent modalities as intra-modal variation and inter-modal\nvariation, respectively. To represent these characteristics, we\ntransform 1D vectors into 2D, where the length axis signi-\nfies intra-modal information and the dimensional axis signi-\nfies inter-modal information. As a result, we successfully in-\ncorporate intra- and inter-modal features into 2D tensors.\nBased on the above motivation, we propose the Modality\nAugmentation and Interaction Network via Dimension Trans-\nformation (AIMDIT). We first design the Modality Augmen-\ntation Network (MAN) based on dimension change and con-\nvolution by Inception. Specifically, the MAN consists of mul-\ntiple layers of MABlock with residual links, which captures\nintra- and inter-modal information in 2D space. In order to\nperform an effective fusion of modalities, inspired by [17],\nwe propose the Interaction Network (MIN) where inter-modal\nfeature and intra-modal feature guide the fusion of each other.\nFinally, emotion classifier predicts the emotion label.\nThe work of this paper can be summarized as follows: 1)\nWe propose a multimodal fusion model named AIMDiT to fa-\ncilitate understanding in ERC. 2) We design a network called\nMAN with dimension transformation and Inception convolu-\ntion, which can effectively learn intra-modal and inter-modal\nfeatures. 3) We introduce a fusion network called MIN that\nutilizes different types of modality features to fuse modalities.\n4) Extensive experiments on benchmark datasets demonstrate\nthe effectiveness and superiority of the proposed model."}, {"title": "2. PROPOSED METHOD", "content": "The proposed model AIMDiT in this paper is shown in Fig. 1.\nA primary utterance involves three modalities: Ut (text), Ua\n(audio), and Uv (visual). Initially, we extract features from\nthese modalities, yielding Xt, Xa,Xv. To capture inter- and\nintra-modal features, we devise the Modality Augmentation\nNetwork, elevating Xt, Xa, Xv to higher dimensions for\nricher information. Following this, the Modality Interac-\ntion Network interacts with the MAN's output, guided by a\nspecific direction. This leads to subsequent emotion classifi-\ncation. Further details are outlined below."}, {"title": "2.1. Modality Augmentation Network (MAN)", "content": "For the same modality, each time point involves both adja-\ncent regions inside a modality and in-phase changes between\ndifferent modalities, that is, changes within modalities and\nchanges between modals. To capture both features at the\nsame time, we build a module Modality Augmentation Block\n(MABlock), which explores rich information between differ-\nent modalitys through dimensional changes.\nSpecifically, ignoring the dimension d, we consider t, a\nand v, where each modal corresponds to having 1D sequences\n\\(X_{1D} \\in \\mathbb{R}^{T_t \\times d}\\), \\(X_{1D} \\in \\mathbb{R}^{T_a \\times d}\\), \\(X_{1D} \\in \\mathbb{R}^{T_v \\times d}\\), and with\nlengths Tt, Ta, Tv, respectively. We can then convert the three\ntensors into a 2D tensor by using the following formula:\n\\(X_{2D}^{t,a,v} = Reshape(Padding(X_{1D}^t, X_{1D}^a, X_{1D}^v)) \\in \\mathbb{R}^{T_s \\times 3d}\\),                                                                                                             (1)\nwhere \\(Padding(\\cdot)\\) is the process of padding each of the three\ntensors into a new tensor of uniform length, and use the length\nof the longest tensor of the three as the length of the new ten-\nsor \\(T_s = max(T_t, T_a, T_v)\\). Eventually we obtain a 2D tensor\n\\(X_{2D}^{t,a,v}\\). After the transformation, we process the 2D tensor by\ninception block with multiple 2D kernels of different scales to\nobtain a 2D tensor \\(X_{2D}^{t,a,v}\\) with global information:\n\\(X_{2D}^{t,a,v} = Inception(X_{2D}^{t,a,v})\\). (2)\nSince the 2D kernel of the inception block is multiscale, it\ncan capture information at adjacent time points within each\nmodality and at the same time point for different modali-\nties, and is able to aggregate intra- and inter-modal variations\nat different scales. Subsequently, we reconvert the learned\n\\(X_{2D}^{t,a,v}\\) tensor into 1D space, and truncate the three tensors\nwith length Ts back to their original lengths:\n\\(X_{1D}^t, X_{1D}^a, X_{1D}^v = Debulk(Reshape(X_{2D}^{t,a,v}))\\), (3)\nwhere \\(X_{1D}^t \\in \\mathbb{R}^{T_t \\times d}\\), \\(X_{1D}^a \\in \\mathbb{R}^{T_a \\times d}\\), \\(X_{1D}^v \\in \\mathbb{R}^{T_v \\times d}\\).\nAs shown in Fig. 1, we connect the Modality Augmen-\ntation Block (MABlock) with residuals to form a MAN in\norder to prevent the problem of gradient vanishing in deep\nnetworks and to improve the expressive power of the net-\nwork. Specifically, given three \\(X_{1D}^t\\), \\(X_{1D}^a\\), and \\(X_{1D}^v\\) with\nlengths Tt, Ta, Tv, respectively, which are represented by\na one-dimensional tensor \\(X_{1D} = \\{X_{1D}^t, X_{1D}^a, X_{1D}^v\\}\\) with\nlength T5, and for the kth-layer MAN, the inputs are \\(X_{1D}^{k-1}\\),\nand its computation procedure is represented by Equation 4:\n\\(X_{1D}^k = MABlock(X_{1D}^{k-1}) + X_{1D}^{k-1}\\). (4)\nAccording to Eqs. 1, 2, and 3 the whole process of the kth\nMAB can be summarized as follows:\n\\(X_{2D}^k = Reshape(Padding(X_{1D}^{k-1}))\\),\n\\(X_{2D}^{t,a,v} = Inception(X_{2D}^k)\\),\n\\(X_{1D}^k = X_{1D}^{t,a,v} = Debulk(Reshape(X_{2D}^{t,a,v}))\\)."}, {"title": "2.2. Modality Interaction Network (MIN)", "content": "In order to fully utilize the consistent and complementary in-\nformation in the acquired inter-modal and intra-modal fea-\ntures, we propose modality interaction network, which con-\nsists of cross-modal Transformer and self-modal Transformer\n[18], with the intra-modal features guiding the reinforcement\nof the inter-modal feature.\nWe apply the cross-modal attention proposed by [17]. The\nmain idea is to use modality \u03b2 to reconstruct modality \u03b1, so\nas to achieve the fusion of the two modalities. Specifically,\nthe modal \u03b2 tensor \\(X_{\\beta}\\) passes through as the inputs of K and\nV in the attention, and the modal \u03b1 as the input of Q. Then,\nthe cross-modal attention \\(Y_{\\alpha}\\) is defined as follows:\n\\(Y_{\\alpha} = softmax(\\frac{X_{\\alpha}W_Q(X_{\\alpha}W_K)^T}{\\sqrt{d_k}})X_{\\beta}W_V\\)(6)\nBased on Cross-modal Attention, we constitute Cross-\nmodal Transformer by stacking this structure. CMT consists\nof k layers of Cross-modal Transformer, which takes the out-\nputs \\(X_{\\alpha}\\) and \\(X_{\\beta}\\) of the previous layer as inputs, and the\noutputs are the enhanced features \\(X_{\\alpha \\rightarrow \\beta}\n:\n\\(X_{\\alpha \\rightarrow \\beta} = CMT_{\\alpha \\rightarrow \\beta}(X_{\\alpha}, X_{\\beta})\\), (7)\nwhere \\(\\beta \\in \\{t,a,v\\}, X_{\\alpha} \\in \\mathbb{R}^{T_s \\times d}\\). In addition, in or-\nder to fuse the inter-modal and intra-modal information, we\nalso performed the traditional self-attention based Selfmodal\nTransformer processing for each modality, and finally aver-\naged the two to obtain a comprehensive representation of each\nmodality. It is expressed as follows:\n\\(Y^{\\alpha} = \\frac{Y_c + Y_p}{2} = \\frac{CMT_{\\alpha \\rightarrow c}(X^{\\alpha}, X^c) + SMT_{\\alpha}(X^{\\alpha}, X^c)}{2}\\) (8)\nwhere \\(Y^c\\) is the output obtained by CMT using the inter-\nmodal feature \\(X^c\\) and the intra-modal feature of the modal\n\u03b1. \\(Y^p\\) is the output obtained by SMT for the modal \u03b1, where\n\u03b1 \u2208 {t, a, v}."}, {"title": "2.3. Emotion Classifier", "content": "The Emotion classifier uses connected multivariables as in-\nputs to perform sentiment prediction. Finally we input \\(Y_{tav}\\)\nto the softmax layer to obtain the sentiment category:\n\\(P_{tav} = GELU(W_1(Y_t \\oplus Y_a \\oplus Y_v)) + b_1\\),\n\\(\\hat{y} = softmax(W_2P_{tav} +b_2)\\), (9)\nwhere W1, W2, b\u2081 and b2 are trainable parameters. We apply\nthe standard cross-entropy loss function to train the model,\nfor Nb utterances in a batch, these are calculated as:\n\\(L = -\\frac{1}{N_b} \\sum_{i=0}^{N_b} y_i log \\hat{y_i}\\)(10)"}, {"title": "3. EXPERIMENTS", "content": ""}, {"title": "3.1. Datasets", "content": "We conducted our experiments using the MELD dataset [19].\nMELD contains 13708 utterances out of 1433 dialogues of the\nTV series Old Friends. The dataset contains seven emotions,\nnamely Anger, Disgust, Fear, Happy, Neutral, Sadness and\nSurprise. For a fair comparison, we conducted experiments\nusing the MELD dataset pre-set with training, validation and\ntest splits."}, {"title": "3.2. Comparison Methods", "content": "TFN [8] utilizes an early fusion approach for modal fusion.\nLMF [9] low ranks the tensor for fusion. DialogueGCN [7]\nutilizes the dependency of graph structural context to model\nconversational data. DialogueCRN [5] designs multi-turn\nreasoning modules to understand conversational context.\nMMGCN [12] utilizes the structural features of graphs to\ncapture intra- and inter-modal features. MM-DFN [13] de-\nsigns graph-based dynamic fusion modules for multimodal\ncontext fusion in conversations, and reduces redundant infor-\nmation by capturing context dynamics. Note that [7] and [5]\nare designed for single-modality, thus introducing early fu-\nsion as a multimodal fusion method.\nImplementation Details. For text, we used ROBERTa\n[20] to pre-train the model. For audio, we extracted Mel-\nspectrogram audio features via librosa. For visual, we used\neffecientNet [21] pre-trained on the VGGface and AFEW\ndatasets [22, 23] to extract visual features. For the dataset\nMELD, the batch size is 96, the initial learning rate is 1e-4."}, {"title": "3.3. Overall Results", "content": "We compare our model with various state-of-the-art methods\nand the overall results are shown in Table 1. It can be seen\nthat AIMDIT outperforms the previous methods in terms of\naccuracy and F1 score on the MELD dataset. Compared to"}, {"title": "3.4. Ablation Study", "content": "Comparison under Different Modality Settings Table 2\nshows a comparison of performance under different modal\ncombinations. It can be seen that the bimodal and trimodal\nmodels are generally better than the unimodal model. In the\nbimodal model, the combination of text and audio (T+A)\nshows the best performance, indicating that text and audio\nfeatures are more complementary. In contrast, audio and\nvisual (A+V) have the worst effect, reflecting a high degree\nof distribution difference and redundancy in audio and visual\nfeatures, which is also worth improving in the future.\nImpact of modules. The MAN module has the ability to ex-"}, {"title": "4. CONCLUSION", "content": "In this paper, we proposed a novel multimodal fusion frame-\nwork (AIMDIT) composed of Modality Augmentation Net-\nwork (MAN) and Modality Interaction Network (MIN) for\nERC tasks. Specifically, we designed a inter- and intra- modal\nfeatures learning network MAN through dimension transfor-\nmation and Inception convolution, and proposed MIN effec-\ntively integrates inter- and intra-modal features for interac-\ntion. Extensive experiments on the benchmark dataset MELD\ndemonstrate the effectiveness and superiority of AIMDIT."}]}