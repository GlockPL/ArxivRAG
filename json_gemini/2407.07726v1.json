{"title": "PaliGemma: A versatile 3B VLM for transfer", "authors": ["Lucas Beyer", "Andreas Steiner", "Andr\u00e9 Susano Pinto", "Alexander Kolesnikov", "Xiao Wang", "Daniel Salz", "Maxim Neumann", "Ibrahim Alabdulmohsin", "Michael Tschannen", "Emanuele Bugliarello", "Thomas Unterthiner", "Daniel Keysers", "Skanda Koppula", "Fangyu Liu", "Adam Grycner", "Alexey Gritsenko", "Neil Houlsby", "Manoj Kumar", "Keran Rong", "Julian Eisenschlos", "Rishabh Kabra", "Matthias Bauer", "Matko Bo\u0161njak", "Xi Chen", "Matthias Minderer", "Paul Voigtlaender", "Ioana Bica", "Ivana Balazevic", "Joan Puigcerver", "Pinelopi Papalampidi", "Olivier Henaff", "Xi Xiong", "Radu Soricut", "Jeremiah Harmsen", "Xiaohua Zhai"], "abstract": "PaliGemma is an open Vision-Language Model (VLM) that is based on the SigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to be a versatile and broadly knowledgeable base model that is effective to transfer. It achieves strong performance on a wide variety of open-world tasks. We evaluate PaliGemma on almost 40 diverse tasks including standard VLM benchmarks, but also more specialized tasks such as remote-sensing and segmentation.", "sections": [{"title": "1. Introduction", "content": "PaliGemma is an open model, continuing the line of PaLI vision-language models in a combination with the Gemma family of language models.\nPaLI is a series of state-of-the-art vision-language models, starting with the first PaLI [22] showing promising scaling results up to 17B, using classification pretrained ViT [127] and mT5 [122] language model. PaLI-X [23] and PaLM-E [35] then pushed this further, combining ViT-22 B [28] and a 32 B UL2 [100] language model or the 540 B PaLM [27] language model, respectively, and getting further increased performance on vision-language tasks, albeit saturating performance on standard image classification and retrieval tasks. Finally, PaLI-3 [24] demonstrates that through better pretraining with SigLIP [129] and more careful multimodal data curation, a 2 B vision and 3 B language model (i.e. a 5 B vision-language model) matches the 10x larger PaLI-X and 100x larger PaLM-E across most benchmarks.\nPaliGemma continues this trend, combining the 400 M SigLIP and the 2B Gemma models [79] into a sub-3 B VLM that still maintains performance comparable to PaLI-X, PaLM-E, and PaLI-3.\nGemma [79] is a family of auto-regressive decoder-only open large language models built from the same research and technology used to create the Gemini [7] models. The models come in different sizes (2 B, 7 B), both pretrained and instruction fine-tuned. PaliGemma uses the 2B pretrained version.\nThe main goal of our work is to provide a versatile base VLM. Hence, we show that it reaches state-of-the-art results not only on standard COCO captions, VQAv2, InfographicVQA and others, but also on more exotic Remote-Sensing VQA, TallyVQA, several video captioning and QA tasks, as well as referring expression segmentation (see full task list in Appendix B)."}, {"title": "2. Related work", "content": "Over the course of the past few years, vision-language models have gained considerable importance in computer vision. The first generation, spearheaded by CLIP [90] and ALIGN [47] by scaling up ConVIRT [131] and VirTex [31], is an extension of large-scale classification pretraining [53, 127], to leverage all data from the web without the need for onerous human labeling, replacing a fixed and large set of classes by a caption embedding instead. The caption embeddings are mostly obtained using language encoders (similar to BERT [32]) and allow to open up the vocabulary of classification and retrieval tasks. The second generation, akin to T5 [91] in language, is a unification of captioning and question-answering tasks via generative encoder-decoder modeling [26, 107, 116, 133], often backed by the progress in generative language models."}, {"title": "3. Model", "content": "In this section we present details about PaliGemma's architecture and training. Several of our decisions are further ablated in Section 5.\nAt a high level, PaliGemma is a VLM, taking as input one or more images, and a textual description of the task (the prompt or question, which we often refer to as the prefix). PaliGemma then autoregressively generates a prediction in the form of a text string (the answer, which we often refer to as the suffix).\nThis simple image+text in, text out API is flexible enough to cover many standard tasks, such as image classification, captioning, visual question-answering and dialogue. Additionally, as shown in the literature, by converting more complex structured outputs into \u201ctext\u201d, this API can also cover more tasks such as: detection [21], instance segmentation [24, 111], panoptic segmentation, depth prediction, colorization, and many more [54, 70, 134]. This conversion can be hand-engineered and task-specific, such as done in pix2seq [21] for detection, or learned as is the case for segmentation [54] and dense output tasks in general.\nDuring PaliGemma's pretraining, we limit ourselves to \"text\" covering natural language, object detection, and instance segmentation, but this API remains versatile and the pretrained models can be finetuned for other output types."}, {"title": "3.1. Architecture", "content": "PaliGemma consists of three components:\n\u2022 An image encoder, for which we use a publicly available SigLIP [129] checkpoint, specifically the \u201cshape optimized\u201d [5] ViT-So400m image encoder. This model was contrastively pretrained at large scale via the sigmoid loss, and has shown state-of-the-art performance, especially for its small size.\n\u2022 A decoder-only language model, for which we use the publicly available Gemma-2B v1.0 [79] raw pretrained checkpoint, which strikes a great balance between performance"}, {"title": "3.2. Pretraining", "content": "The training of PaliGemma follows the same steps as previous PaLI models, with only small modifications. Training consists of several stages, which we detail in this section:\n\u2022 Stage0: Unimodal pretraining - we use existing off-the-shelf components.\n\u2022 Stage1: Multimodal pretraining - long pretraining on a carefully chosen mixture of multimodal tasks. Notably, nothing is frozen.\n\u2022 Stage2: Resolution increase - short continued pretraining at higher resolution.\n\u2022 Stage3: Transfer - turn the base model into a task-specific specialist."}, {"title": "3.2.1. Stage0: Unimodal pretraining", "content": "First, the unimodal components of the model are pretrained individually, in order to benefit from their well-studied and scaled training recipes. For PaliGemma specifically, we do not perform any custom unimodal pretraining, instead relying on existing publicly available checkpoints.\nFollowing PaLI-3's strong experimental results, we use a SigLIP image encoder. While PaLI-3 (and others [6, 25]) use a large image model such as ViT-G, we use the much smaller but similarly strong \u201cshape optimized\u201d ViT-So400m model."}, {"title": "3.2.2. Stage1: Multimodal pretraining", "content": "In this stage, we combine the unimodal models as explained in Section 3.1 and train the whole model on a broad mixture of large-scale vision-language tasks. Contrary to most recent VLMs, our core goal is to train a base model that finetunes well to a wide range of tasks, not merely to align the modalities. Intuitively, we want a mix of tasks which force the model to acquire a broad range of \"skills\", regardless of the task's user (or benchmark) friendliness out of the box. More on this in Section 3.2.5.\nIt is common practice, also followed by previous PaLI versions, to keep the image encoder frozen during the first multimodal pretraining stage. This is partially due to findings as in LiT [128] reporting multimodal tuning of pretrained image encoders degrading their representations. However, more recent work such as CapPa [106] and LocCa [111] have shown that captioning and other harder-to-learn tasks can provide valuable signal to image encoders, allowing them to learn spatial and relational understanding capabilities which contrastive models like CLIP or SigLIP typically lack. Hence, again in the spirit of learning more skills during pretraining, we depart from common practice and do not freeze the image encoder. However, the challenges outlined in LiT remain. In order to avoid destructive supervision signal from the initially unaligned language model, we use a slow linear warm-up for the image encoder's learning-rate (Figure 3), which ensures that the image encoder's quality is not deteriorated from the initially misaligned gradients coming through the LLM.\nWe train Stage1 at resolution 224px (hence, Nimg = 256 image tokens) and sequence length Ntxt = 128 for a total of 1 billion examples. While we provide an ablation in Section 5.1 showing that a 10x to 30x shorter Stage1 still provides good results on popular benchmarks, we wish to embue as much visual knowledge to the base model as possible, and cover a broad set of concepts, cultures, and languages [82, 89]."}, {"title": "3.2.3. Stage2: Resolution increase", "content": "The model resulting from Stage1 is already a useful base model for many tasks (see example images in Appendix B). However, it only understands images at 224 \u00d7 224 pixel resolution, which is too small for several tasks. For instance, detection and segmentation of smaller objects, and tasks related to reading smaller texts such as charts, infographics, or documents, all strongly benefit from higher resolution (see Table 1). Hence, we train two further model checkpoints for increased resolution, first to 448 \u00d7 448 and then to 896 \u00d7 896 pixel resolution.\nSince stage1 took care of providing the model with a broad set of knowledge and skill, stage2 can focus on extending the model's ability to parse higher-resolution images. We thus run Stage2 with fewer total examples, while increasing the cost and information density of each example. For resolution 448, we train for an additional 50 M examples, and for resolution 896, we add another 10 M examples.\nFor simplicity, Stage2 consists of the exact same mixture of tasks and datasets as Stage1, but with significantly increased sampling of tasks that require high resolution. Additionally, these upweighted tasks all can be modified to provide much longer suffix sequence lengths. For instance, for OCR tasks, we can simply request the model to read all text on the image in left-to-right, top-"}, {"title": "3.2.4. Stage3: Transfer", "content": "The result of Stages 1 and 2 is a family of three PaliGemma checkpoints, at 224px, 448px, and 896px resolution, which are pre-equipped with broad visual knowledge. However, these checkpoints are not \u201cuser (or benchmark) friendly\" as their pretraining has focused solely on density of learning signal, as opposed to usable interface.\nThese base models need to be transferred to serve their intended final purpose. That could take the form of fine-tuning on a specific, specialized task, such as COCO Captions, Remote Sensing VQA, Video Captioning, or InfographicQA. Adapt to new inputs such as multiple images (NLVR2) or bounding boxes draw in the image (WidgetCap). Or it could take the form of instruction [67] or even chat [44] tuning.\nTo show the effectiveness of the base models, we transfer them to a wide range of individual academic benchmarks, using a simple unified transfer recipe with few hyper-parameters. And to showcase the versatility beyond academic tasks, we also provide a \u201cmix\u201d transfer checkpoint, which transfers to a subset of these tasks at the same time, along with detailed captioning and long question-answering data. While this is not instruction tuning, it is a step in that direction.\nWe also transfer PaliGemma to tasks which take multiple images as input. NLVR2 is one such task, which asks one question about two images, and requires looking at both to give the correct answer. Other such tasks are standard short-video understanding tasks subsampled to 16 frames. In all these cases, we follow PaLI-3 and encode each image separately, then concatenate the image tokens without any special separator or embedding tokens. Thus, 16 frames at 224px resolution result in Nimg = 4096 image tokens, the same amount as a single image at 896px resolution.\nFor all transfers, we perform fine-tuning of all the model parameters. The hyper-parameters we modify per-task are the following, in decreasing order of importance:\n\u2022 Resolution (i.e. checkpoint): 224, 448, 896.\n\u2022 Epochs: 1, 3, 10, 30, 100.\n\u2022 Learning-rate: 3e-5, 1e-5, 3e-6.\n\u2022 Label-smoothing: 0.0, 0.1, 0.3.\n\u2022 Dropout in the LLM: 0.0, 0.1, 0.3.\n\u2022 Weight decay: 0.0 or 0.1 \u00d7 learning-rate.\n\u2022 Freeze ViT: false, true.\n\u2022 Beam-search may benefit captioning.\nThe above are typical values we suggest exploring, with the recommended initial attempt value in bold. We provide the best setting for each individual task in Appendix J. We study the sensitivity to transfer hyper-parameters in Section 6.2, and the \u201ctransferability\u201d in general in Section 6, showing that good results can be achieved with the aforementioned initial attempt values."}, {"title": "3.2.5. Pretraining task mixture", "content": "Just like for previous PaLI models, the pretraining (Stage1 and Stage2) is designed to result in a model that transfers well, not necessarily a model that is usable out of the box (\"O shot\"). The intuition here is that we want a mix of tasks which force the model to acquire a broad range of \"skills\". We prefix each task with its unique prefix to avoid conflicting learning signals across skills [14]. At transfer time (Stage3), the model then merely needs to recognize which skill is useful for the task, and rewire itself to use that while following the output syntax and vocabulary of the task. In our experience, these can all be done relatively quickly and based on few examples (Section 6.3). We do not use any of our transfer datasets during pretraining, and furthermore remove all near-duplicates of their images from the pretraining datasets [53]."}, {"title": "3.2.6. Other pretraining details", "content": "Throughout pretraining, we use an \"infinite\" learning-rate schedule following [127], which provides a straightforward way of chaining several stages without decaying the learning-rate between them. Figure 3 shows the full schedule: pretraining is one continuous rsqrt curve for all stages. The transfer can then act as a cooldown, fully annealing the learning rate. We recommend transferring with a simple setup that tunes the full model using a cosine learning-rate schedule with a short linear warm-up and decaying to zero. This is not well represented by Figure 3 due to its comparatively short duration.\nThe model was entirely trained in the open-source big_vision codebase [12] on Cloud TPUv5e [36]. However, some of the pretraining datasets remain private. During training, we partition data, as well as model parameters and optimizer state (Zero-DP style [92]) across all available devices using JAX [16] with GSPMD [121]. This fully-sharded data-parallel (FSDP [132]) sharding strategy is achieved by constructing global arrays and annotating the sharding accordingly, with the XLA compiler [93] taking care of the concrete implementation of the computation and communication between devices. We mea-"}, {"title": "4. Results", "content": "In order to verify the transferability of PaliGemma to a wide variety of tasks, we transfer the pretrained models on more than 30 academic benchmarks via fine-tuning. Importantly, none of these tasks or datasets are part of the pretraining data mixture, and their images are explicitly removed from the web-scale pretraining data. Results are presented in Table 1.\nTo select the hyper-parameters for transfer, we first sweep the parameters mentioned in Section 3.2.4, starting from the recommended value. We do not necessarily perform the full cross-product, and we sometimes extend or supersample the range, if it seems promising. Importantly, we make any such decisions and hyper-parameter choices based on the transfer task's validation split, and if none is provided, we hold out a small \"minival\" set from the training data. Once we found good hyper-parameter values for a task, we re-train using the full training and validation data, and report final test numbers. Details on tasks, metrics, data splits are in Appendix B and final hyper-parameters in Appendix J. In Section 6.2 we show that a single recommended value for each hyper-parameter without any exploration works almost as well on most tasks."}, {"title": "5. Ablations", "content": "We conduct diverse ablations to gain deeper understanding of what matters for training and transferring VLMs. Unless noted otherwise, all ablations are run with the same setup as the main models, except for making the Stage1 pretraining 10x shorter (i.e. 100 M examples seen), and transfer results are reported on validation sets instead of withheld test-sets. For each experiment, we present only the salient result summary in the main text, but we provide a full per-task breakdown of results in the Appendix."}, {"title": "5.1. Multimodal pretraining duration", "content": "With its 1 B examples seen, our multimodal pretraining (Stage1) is on the longer side, similar to BLIP-2 [60], InternVL [25], QwenVL [10], Idefics2 [57] (all around 1B), but unlike ShareGPT4-v [20], Mini-Gemini [63], LLaVa [67] and its derivatives (around 1 M).\nTo the best of our knowledge, the benefits from longer pretraining have not been studied in isolation. We run pretrainings of various shorter durations, all the way down to completely skipping Stage1, and show the impact in Figure 4, with a complete break-down across tasks in the Appendix K.1. For the case of skipping Stage1, we use the best transfer result when sweeping over three learning-rates for each task.\nThe result shows that shorter training generally hurts, and skipping Stage1 entirely is the worst setting. Some task are affected significantly, while others only deteriorate a little, highlighting the need for a broad and diverse set of evaluation tasks. The 100 M pretraining duration appears to be a good trade-off for ablations: it is 10x shorter while not significantly hurting any task."}, {"title": "5.2. Causal masking and learning objective", "content": "We ablate several of our key choices in the pretraining learning objective in Figure 5, full per-task breakdown in Appendix K.2.\nFirst, we investigate design choices for autoregressive masking. PaliGemma uses a prefix-LM strategy which allows full (bi-directional) attention on the \u201cinput\u201d part of the data, i.e. the image and prefix tokens, see also Figure 2. The motivation is that it allows more tokens to actively participate in the \u201cthinking\u201d process from the start, as the image tokens now can attend to the prefix tokens which represent the query. This is empirically confirmed in Figure 5 (left), where the green bars also include the auto-regressive mask-"}, {"title": "5.3. New token initialization", "content": "We add new tokens to Gemma's vocabulary to support PaliGemma's ability to perform more structured computer vision tasks. We add 1024 location tokens (<loc0000> to <loc1023>), which correspond to binned normalized image coordinates and are used in detection, referring expression, and grounded captioning tasks. We also add 128 VQVAE [108] tokenized single-object mask tokens [24, 83] (<seg000> to <seg127>) to support referring expression segmentation.\nThis poses the question of how to initialize the embeddings of these new tokens, given all other vocabulary tokens have already been trained as part of Gemma's pretraining. One option is to use a standard small Gaussian noise initialization"}, {"title": "5.4. To freeze or not to freeze?", "content": "The current common wisdom in VLMs [22\u201324, 43, 50, 58, 60, 64, 67] is to keep the image encoder and sometimes the LLM frozen during multimodal pretraining (our Stage1). However, inspired by the positive results from CapPa [106] and LocCa [111] which show that pretraining an image encoder using captioning objectives essentially solves contrastive's blind spot [41] to relation and localization, we pretrained PaliGemma with no frozen parts. We now ablate the effect of freezing or tuning various parts of the model during Stage1 in Figure 7, full per-task breakdown in Appendix K.3. Similar to concurrent works [78, 103], we find not freezing any part of the model is indeed advantageous. First, af-"}, {"title": "5.5. Connector design", "content": "Throughout our experiments we use a linear connector to map SigLIP output embeddings to the inputs of Gemma. Given that an MLP connector [66] is a popular choice in the VLM literature, we also ablate this choice.\nWe consider two connector choices: a linear connector and an MLP (1 hidden layer, with GeLU non-linearity). We also consider two Stage1 pretraining settings: tune all weights (TT), or freeze everything but the connector (FF).\nWhen tuning all weights, average transfer score is nearly identical for linear vs MLP, achieving 77.2 and 77.1 points respectively. In the \"all-frozen\" scenario, linear vs MLP achieve 70.7 vs 69.7. Surprisingly, we observe a small performance deterioration with the MLP connector.\nOverall, we conclude that in our case, the linear connector seems preferable to the MLP connector."}, {"title": "5.6. Image encoder: with or without?", "content": "Most VLMs follow the setup of having an image encoder, such as CLIP/SigLIP (most works) or VQGAN (the Chameleon line of work [2, 3, 101, 125]), to turn the image into soft tokens before passing them to the LLM. We are aware of only two works that attempt to simplify this overall setup by removing the image encoder entirely and passing raw image patches into a decoder-only LLM, namely Fuyu [11] and EVE [33]. Unfortunately, the former provides no training details or ablations. The latter, which is a concurrent work, provides some details about training and various ablations, but with mixed results.\nRemoving the SigLIP encoder in PaliGemma results in a model of the same unified decoder-only architecture. We run our Stage1 and transfers in this setting. Since the architecture significantly changed, we also re-tune the learning-rate of Stage1. Figure 8 (per-task breakdown in Appendix K.4) shows that while this architecture still significantly lags behind, the scaling with pretraining duration seems potentially promising. This is especially noteworthy considering that PaliGemma's SigLIP encoder has seen 40 B image-text pairs during its Stage0 pre-training, while the Fuyu-style model sees images for the first time in the Stage1 pre-training shown here, and only sees up to 1B of them in our experiment. This ablation confirms that such decoder-only VLMs might be a promising future direction towards simpler multimodal models, although they currently still suffer in training efficiency due to not being able to reuse vision components."}, {"title": "5.7. Image resolution", "content": "Image resolution in VLMs is an important topic that has recently received increased attention [34, 57, 78, 103, 117]. PaliGemma uses a very simple approach to deal with resolution: Stage1 is pretrained at relatively low and economical 224px resolution, and short Stage2 then \u201cupcycles\" this checkpoint to higher resolutions (448px and 896px). Hence, the final PaliGemma model comes with three different checkpoints for three different resolutions.\nIn this section we justify our approach and the necessity for providing three separate checkpoints, and compare it to a recently popular \u201cwindowing\" approach. For the ablation studies, we consider resolutions 224px and 448px, and restrict evaluation to single-image tasks where resolution has significant impact on performance according to Table 1, which we call \u201cResolutionsensitive\" tasks."}, {"title": "5.7.1. Resolution or sequence length?", "content": "We generally see either neutral or improved performance across tasks when increasing the resolution of the input image (see Table 1). However, it is unclear whether this improvement comes from the fact that the image has higher resolution and therefore more information, or whether it is thanks to the resulting longer sequence length and thus increased model FLOPs and capacity. We disentangle these by running Stage2 and trans-"}, {"title": "5.7.2. Need resolution-specific checkpoints?", "content": "PaliGemma provides one checkpoint per resolution. But is this really needed? Could we not provide just a single, maybe high-resolution checkpoint, and adapt it as needed during transfers?\nFigure 10 shows clearly that providing either only a 224 px or only a 448 px checkpoint would not be sufficient: Transferring the 448 px checkpoint at 224 px resolution (first bar, orange) leads to significantly worse results than using the 224 px checkpoint (second bar, zero), even though the latter had slightly less pretraining. Similarly, transferring the 224 px checkpoint at resolution 448 px (third bar, orange), while improving the results significantly, still lags far behind transferring the checkpoint whose native resolution is 448 px (last bar, green)."}, {"title": "5.7.3. To resize or to window?", "content": "Another recently common way of increasing input resolution is by \"windowing\" the models [110, 117, 130], i.e. applying the same model on windows of the model's native resolution from the higher-resolution image. We evaluate the simplest variant of this alternative: the 448 px resolution image is cut into four pieces, each one passed through the SigLIP image encoder separately. All four sets of 256 image embedding tokens are then concatenated and passed to Gemma. This is also related to using windowed attention in a ViT taking the full image [62]. We experimented with adding extra \"window ID\u201d position embeddings to indicate which window tokens come from, but this did not significantly change any of the results.\nThe mauve bars in Figure 10 correspond to windowing settings. While overall the performance is worse than that of a native 448px model, the windowing approach can be a promising way of transferring a model to a higher resolution when no higher-resolution checkpoint is made available, and when running a Stage2-like continued pretraining is not feasible.\nWindowing might still seem preferable for speed reasons. However, we only observed at most a 5% speedup in training across various setups from windowing. This is explained by Gemma being significantly larger than ViT-So400m, and the Gemma part of the model is unaffected by windowing."}, {"title": "5.7.4. Stage2 mixture re-weighting", "content": "Finally, the pretraining mixture changes between Stage1 and Stage2. While previous PaLI versions change the mixture makeup, for PaliGemma we always use the full set of tasks, only changing their weighting, sampling \u201cresolution-related\" tasks (OCR, detection, segmentation) more frequently at higher resolutions. As an ablation, we run a Stage2 training with the same mixture ratios as Stage1. After transfers, this checkpoint is significantly worse on only three tasks (DocVQA, ChartQA, XM3600), but otherwise within per-task variance (Full results in Appendix K.7).\nThus, while changing the mixing ratio helped a little, it seems that when the intent is to train a base model for fine-tuning, the precise mixture ratios might not be as important as when training a model intended for sampling zero-shot, where it would significantly affect sampling frequencies."}, {"title": "6. Transferability", "content": "To show that PaliGemma is suitable for transfer under different scenarios, we run experiments that quantify its repeatability, sensitivity to hyperparameters, and number of transfer examples."}, {"title": "6.1. Repeatability (variance)", "content": "In the main results (Table 1), we show the standard deviation across 5 transfer reruns using the best hyper-parameter (listed in J). It is generally very small across most tasks, meaning transfer from the pretrained checkpoint is highly repeatable. In Figure 11 we further show that the standard deviation of transferring from three reruns of Stage1 falls within the same range, meaning pretraining itself is also highly repeatable."}, {"title": "6.2. Transfer hyper-parameter sensitivity", "content": "However one question a reader may have is whether the choice of transfer hyper-parameter is important. To ablate that we run all tasks at 224px and under a single and simple hyperparameter setup, which was also highlighted in bold in Section 3.2.4: lr=1e-5, bs=256, no dropout, no label smoothing, no weight decay, and not freezing anything. The only task dependent parameter is the number of epochs for which we use each task's best one but cap it at 10.\nThe results (Table 2) show that this simplified and single setup works very well for the majority of the tasks. The main exceptions we found were for tasks like RefCOCO and SciCap tasks which seem to benefit significantly from increasing the number of epochs while enabling label smoothing and dropout."}, {"title": "6.3. Transfer with limited examples", "content": "To analyze how many examples are needed to make PaliGemma solve a new task, we finetune PaliGemma with limited number of examples (64, 256, 1024, 4096). We sweep transfer with varying learning rates, epochs and batch size and report the best number without separate minival, to indicate the potential.\nWe run every setting with 5 different seeds, which also affect which examples are used. We found this important, as finetuning with limited examples exhibits high variance for some tasks"}, {"title": "7. Noteworthy tidbits", "content": "We also briefly discuss several small but interesting discoveries or we made, providing more details on each in the Appendix.\nPlain resize to square for segmentation. We found simple resize to square (224\u00d7224) to work just as well as popular aspect-ratio preserving zoom and crop augmentations. (Appendix C)\nCounting: introducing CountBenchQA. Due to skewed number distribution and varying image quality [49], we found TallyQA lacking in its ability to assess current VLM's ability to count. This is why we introduce CountBenchQA, a VLM-"}, {"title": "8. Conclusion", "content": "PaliGemma is a new, small, open base VLM that shines when transferred to a broad range of tasks. Our results show that VLMs on the \"smaller\" side can provide state-of-the-art performance across a wide variety of benchmarks. We also hope that providing the base model without instruction tuning serves as a useful starting point for further research in instruction tuning, specific applications, and encourages clearer separation of base models and fine-tunes in VLM research."}]}