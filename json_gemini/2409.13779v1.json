{"title": "AUTOPET III CHALLENGE: TUMOR LESION SEGMENTATION\nUSING RESENC-MODEL ENSEMBLE", "authors": ["Tanya Chutani", "Saikiran Bonthu", "Pranab Samanta", "Nitin Singhal"], "abstract": "Positron Emission Tomography (PET) /Computed Tomography (CT) is crucial for diagnosing,\nmanaging, and planning treatment for various cancers. Developing reliable deep learning models for\nthe segmentation of tumor lesions in PET/CT scans in a multitracer multicenter environment, is a\ncritical area of research. Different tracers, such as Fluorodeoxyglucose (FDG) and Prostate-Specific\nMembrane Antigen (PSMA), have distinct physiological uptake patterns and data from different\ncenters often vary in terms of acquisition protocols, scanner types, and patient populations. Because of\nthis variability, it becomes more difficult to design reliable segmentation algorithms and generalization\ntechniques due to variations in image quality and lesion detectability. To address this challenge,\nWe trained a 3D Residual encoder U-Net within the nnU-Net framework, aiming to generalize the\nperformance of automatic lesion segmentation of whole body PET/CT scans, across different tracers\nand clinical sites. Further, We explored several preprocessing techniques and ultimately settled on\nusing the 'TotalSegmentator' to crop our training data. Additionally, we applied resampling during\nthis process. During inference, we leveraged test-time augmentations and other post-processing\ntechniques to enhance tumor lesion segmentation. Our team currently hold the top position in the\nAutoPET III challenge and outperformed the challenge baseline model in the preliminary test set with\nDice score of 0.9627. Github link: https://github.com/tanya-chutani-aira/autopetiii", "sections": [{"title": "Introduction", "content": "Over the past decades, PET/CT [1, 2] has become a crucial tool in oncological diagnostics, management, and treatment\nplanning. In clinical practice, medical experts usually depend on qualitative analysis of PET/CT images, although\nquantitative analysis could provide more precise and individualized tumor characterization and therapeutic decisions. A\nsignificant barrier to clinical adoption is lesion segmentation, a necessary step for quantitative image analysis. When\ndone manually, it is tedious, time-consuming, and costly. Machine learning offers the potential for rapid and fully\nautomated quantitative analysis of PET/CT images."}, {"title": "Methodology", "content": "The block diagram in Fig. 1 illustrates the training and inference pipeline utilized for the AutoPET III challenge."}, {"title": "Development dataset", "content": "The AutoPET III challenge dataset comprises 1,611 images in the training set. Specifically, it includes 1,014 FDG [3]\nimages from 900 patients and 597 PSMA images [4] from 378 patients. The validation set is derived from 5 images (3\nFDG images and 2 PSMA images). The final evaluation will use a hidden test set of 200 images, with half of the test\ndata matching the sources and distributions of the training data, while the other half will be cross-sourced from the"}, {"title": "Data pre-processing", "content": "Training a model for 3D medical images, such as PET/CT scans [2], is challenging due to the substantial memory\nconsumption required during both training and inference. Additionally, not all parts of the image contain relevant\nstructures (e.g., tumors), and the image boundaries often have noise or artifacts.\nData pre-processing steps include TotalSegmentator [5], image resampling, data augmentation, and data normalization."}, {"title": "TotalSegmentator:", "content": "To localize the tissue and mitigate the boundary noise, we used TotalSegmentator [5] to crop the 3D PET/CT images.\nCropping reduces the image size while retaining relevant information, leading to more memory-efficient training.\nTotalSegmentator intelligently crops to preserve relevant structures (e.g., tumors) while minimizing unnecessary\nbackground and noisy regions, ensuring cleaner input data for the model. This approach ensures uniform input sizes\nacross all samples, making it easier to batch and train models efficiently.\nThe trade-off is that aggressive cropping can sometimes sacrifice spatial context to save memory. To balance this, we\nused overlapping crops during training, ensuring that neighboring context is still considered even if individual crops\nlose some spatial information. Additionally, we applied padding to the cropped regions, allowing the model to learn\nfrom nearby context. To make the generalized model we applied data augmentation module. Data augmentation module\nis followed by cropped data.\nIn TotalSegmentator, we utilized a 3D lower nnU-Net to accurately segment the body from the PET/CT image."}, {"title": "Data augmentation:", "content": "Data augmentation plays a crucial role in enhancing model robustness and generalization. We employ basic transfor-\nmations such as random flips, random rotations, elastic deformations, intensity scaling which are applied in standard\nnnU-Netv2 [6]."}, {"title": "Data normalization:", "content": "We applied data normalization to the cropped images, including resampling and intensity normalization, using standard\nnnUNetv2 plans defined for CT and PET. Resampling ensures consistent spatial resolution across all images, while\nintensity normalization brings the image values to a common scale, reducing variability caused by different imaging\nprotocols."}, {"title": "Model architecture", "content": "We employed nnU-Net ResEnc XL from nnUNetv2 pipeline as the model architecture for training [6, 7]. The model\nis modified version of the ResNet family network. It utilizes residual block similar to the ResNet [Ref.]. nnU-Net\nResEnc XL represents a significant advancement in the field of medical imaging, leveraging deep learning to enhance\nthe accuracy and efficiency of tumor lesion segmentation in 3D PET/CT scans."}, {"title": "Training methodology", "content": "During training, we adopted a robust approach by dividing the data into five folds. For each fold, 80 percentage of the\ndata served as the training set, while the remaining 20 percentage acted as the validation set. This strategy allowed our\nmodels to capture variations effectively and ensured their robustness when faced with unseen test data. We explored\nboth 2D and 3D versions of the nnU-Net ResEnc XL architecture and employed a patch size of [224, 192, 224] in the\nPatch-Based Training of nnU-Netv2 pipeline."}, {"title": "Results and discussion", "content": ""}, {"title": "Experimental setup", "content": "For this experimentation, we have used nnU-Netv2 framework with nnU-Net ResEnc XL architecture. The framework\nis developed in pytorch and we have used 48GB GPU to develop the 2D and 3D models for each fold."}, {"title": "nnU-Net ResEnc XL results and discussion", "content": "In nnU-Net, we have used 2D and 3D nnU-Net ResEnc XL model in full resolution. We assessed the performance of\n2D and 3D segmentation models using a 5-fold cross-validation approach with the Dice metric. The results, presented\nindicate that 3D models outperformed 2D models in tumor lesion segmentation in PET/CT images when\ntrained on a cropped dataset generated using TotalSegmentator."}, {"title": "Model ensemble results and discussion", "content": "For the AutoPET III challenge, we employed an ensemble of 5-fold models, and the STAPLE algorithm [9] to combine\nthe results from the five models. This ensemble approach facilitated the improved generalization, robustness and\nperformance needed for tumor lesion segmentation in a multicenter, multi-tracer environment.\nIntegrating models using 5-fold cross-validation presents significant challenges, particularly when faced with time\nconstraints during inference. The computational demands are high, contributing to overall complexity. To address these\nissues, we implemented specific strategies. First, we optimized CPU usage by preprocessing input images only once.\nAdditionally, we parallelized inference across five different models using a multithreading approach. Furthermore,\ndynamic test-time augmentations (TTA) were employed during inference to enhance robustness and accuracy in tumor\nlesion segmentation [8]. For larger images, we judiciously limited TTA to minimize augmentation time. Our preliminary\nresults on the autoPET III challenge test set validate the effectiveness of this training strategy.\nUpon comparing our predicted segmentations with ground truth annotations, our model exhibits remarkable accuracy,\nparticularly in tumor lesion detection and boundary delineation. showcases the ground truth of FDG and\nPSMA, followed by the segmentation results for tumor lesions in whole-body PET/CT images."}, {"title": "Model Submissions", "content": "Initially, we submitted three versions of models for the preliminary test set:\n\u2022 A 2D single-fold model achieved a Dice score of 0.9627.\n\u2022 A 2D 5-fold ensemble of models achieved a Dice score of 0.9602."}, {"title": "Conclusions", "content": "To address the challenge of tumor lesion segmentation of PET/CT images from multiple tracers and centers, we proposed\na model training approach using a cropped dataset with TotalSegmentator, followed by standard pre-processing steps.\nHowever, during inference TotalSegmentator with dynamic test time augmentation helps to improve the overall\nperformance across the 5-fold using 3D models that reflected in the validation leader board as well in AutoPET III\nchallenge. Moreover, 3D model ensemble with STAPLE algorithm outperformed with respect to the individual model\nperformance."}]}