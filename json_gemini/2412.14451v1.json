{"title": "CLDG: Contrastive Learning on Dynamic Graphs", "authors": ["Yiming Xu", "Bin Shi", "Teng Ma", "Bo Dong", "Haoyi Zhou", "Qinghua Zheng"], "abstract": "The graph with complex annotations is the most potent data type, whose constantly evolving motivates further exploration of the unsupervised dynamic graph representation. One of the representative paradigms is graph contrastive learning. It constructs self-supervised signals by maximizing the mutual information between the statistic graph's augmentation views. However, the semantics and labels may change within the augmentation process, causing a significant performance drop in downstream tasks. This drawback becomes greatly magnified on dynamic graphs. To address this problem, we designed a simple yet effective framework named CLDG. Firstly, we elaborate that dynamic graphs have temporal translation invariance at different levels. Then, we proposed a sampling layer to extract the temporally-persistent signals. It will encourage the node to maintain consistent local and global representations, i.e., temporal translation invariance under the timespan views. The extensive experiments demonstrate the effectiveness and efficiency of the method on seven datasets by outperforming eight unsupervised state-of-the-art baselines and showing competitiveness against four semi-supervised methods. Compared with the existing dynamic graph method, the number of model parameters and training time is reduced by an average of 2,001.86 times and 130.31 times on seven datasets, respectively. The code and data are available at: https://github.com/yimingxu24/CLDG.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph neural network shows superior performance in many real-world applications, such as recommender systems [1]\u2013[3], combinatorial optimization [4]\u2013[6], traffic prediction [7]\u2013[9], risk management [10]\u2013[12], etc. Thus, it is widely investigated in recent years. However, existing methods mainly focus on supervised or semi-supervised learning paradigms, which rely on abundant ground-truth label information. The acquisition of ground-truth labels on graphs may be more complex and difficult than other media forms. Because the graph is an abstraction of the real world, it is not as straightforward as video, image, or text. For example, even without considering data privacy and security in the transaction network, it is impossible to label whether the credit card is frauded through the crowdsourced manner, because it requires a wealth of expert knowledge and experience. These make it very expensive to obtain graph datasets with a large amount of label information.\nTherefore, it is significant and challenging work to learn rich representations on graphs with unsupervised methods.\nSome recent works extend unsupervised contrastive learning to graphs [13]\u2013[19] to tackle this problem. They mainly generate augmented views by perturbing nodes, edges, and features, and then construct self-supervised signals by maximizing the mutual information (MI) between different augmented views, thereby eliminating the dependence on labels. However, the semantics and labels may change within the perturbation-based augmentation process, causing a significant performance drop in downstream tasks. For example, adding edges may introduce noise, and deleting edges may delete the most important edges and neighbors for nodes. More seriously, existing methods fail to utilize the temporal information when constructing comparison signals, which further limits the performance on dynamic graphs. Therefore, existing static graph contrastive learning methods may not be the optimal solution for the dynamic graph. How to generalize contrastive learning to dynamic graphs is a challenge.\nTo achieve this objective, we perform some empirical studies to explore the properties of dynamic graphs (details of the experimental setup are demonstrated in Section IV-A). An interesting observation is that the prediction labels of the same node tend to be similar in different timespans, regardless of the encoder used (GCN [20], GAT [21] or MLP [22]), as shown in Figure 1. The different datasets we use all share this characteristic (details of the datasets are demonstrated"}, {"title": "II. RELATED WORK", "content": "Supervised and semi-supervised learning still dominate in graph neural networks [24]\u2013[26]. However, the graph itself\nis an abstraction of the real world, and graph data suffers from the problems of lack of labels and difficulty in labeling. Recently, contrastive learning is highly successful in computer vision (CV) and natural language processing (NLP) [27]\u2013[32]. Inspired by the above methods, there are some works that extend contrastive learning to graphs. DGI [13] extends deep InfoMax [14] to graphs and maximizes MI between global graph embeddings and local node embeddings. GraphCL [15] proposes a novel graph contrastive learning framework that systematically explores the performance impact of various combinations of four different data augmentations. GCA [16] and GRACE [17] focus more on the graph data augmentation, and propose an adaptive data augmentation scheme at both topology and node attribute. MVGRL [18] creates another view for the graph by introducing graph diffusion [33]. A discriminator contrasts node representations from one view with graph representation of another view and vice versa, and scores the agreement between representations which is used as the training signal. CCA-SSG [19] first generates two views of the input graph through data augmentation, and then uses the idea based on Canonical Correlation Analysis (CCA) to maximize the correlation between the two views and encourages different feature dimensions to capture distinct semantics. However, the above methods have two problems: (1) They require two views generated by corrupting the original graph, such as node perturbation, edge perturbation and feature perturbation, etc. Inappropriate data augmentation may introduce noisy information resulting in semantic and label changes. (2) They are designed for static graphs. Applying contrastive learning directly to dynamic graphs is not straightforward."}, {"title": "B. Representation Learning on Dynamic Graphs", "content": "Traditionally, the static graph representation problem is intensively studied by researchers and a variety of effective works are proposed [20], [21], [34]\u2013[37]. However, real-world networks all evolve over time, which poses important challenges for learning and inference. Therefore, there is an increasing amount of research on dynamic graph representations recently. According to the modeling method of the dynamic graph, these works can be roughly divided into discrete-time methods [38]\u2013[43] and continuous-time methods [44]\u2013[47].\nDiscrete-time Methods DynGEM [38] uses a deep autoencoder to capture the connectivity trends in a graph snapshot at any time step. DySAT [39] generates dynamic node representations through self-attention along both structural and temporal. TGAT [40] uses the self-attention mechanism and proposes a time encoding technique based on the theorem of Bochner. STAR [48], DyHATR [42], dyngraph2vec [49] and TemGNN [43] use different encoders (such as GCN [20], autoencoder [50], hierarchical attention model, etc.) to extract the features of each time snapshot respectively, and then introduce sequence models such as LSTM [51] and GRU [52] to capture timing information.\nContinuous-time Methods HTNE [44] proposes a Hawkes process based temporal network embedding method to capture the influence of historical neighbors on the current neighbors. JODIE [45] uses a coupled RNN architecture to update the embedding of users and items at every interaction. DyRep [46] builds a two-time scale deep temporal point process approach to capture the continuous-time fine-grained temporal dynamics processes. M2DNE [47] designs a temporal attention point process to capture the fine-grained structural and temporal properties in micro-dynamics, and defines a dynamics equation to impose constraints on the network embedding in macro-dynamics.\nHowever, existing dynamic graph work still suffers from at least one of the following limitations: (1) Most RNN-like methods have high time complexity and are not easy to parallel. This hinders the scaling of existing dynamic graph models to large-scale graphs. (2) Most models by reconstructing future states or temporal point processes or sequences may learn noisy information as they try to fit each new interaction in turn. (3) The temporal regularizer is similar to contrastive learning without negative examples, which forces the node representation to smooth from adjacent snapshots. However, a potential problem of this approach is the existence of completely collapsed solutions."}, {"title": "III. PRELIMINARIES", "content": "In this section, we give the necessary notations and definitions used throughout this paper. The main notations are summarized in Table I."}, {"title": "A. Dynamic Graph Modeling", "content": "The existing dynamic graph modeling methods can be roughly divided into two categories [53], [54]: discrete-time"}, {"title": "Definition 1 (Discrete-time Dynamic Graph)", "content": "A discrete-time dynamic graph (DTDG) is a sequence of network snapshots within a given time interval. Formally, we define a DTDG as a set {$G_1,G_2,...,G_T$} where $G_t = {V_t,E_t}$ is the graph at snapshot t, $V_t$ is the set of nodes in $G_t$, and $E_t \\subseteq V_t \\times V_t$ is the set of edges in $G_t$."}, {"title": "Definition 2 (Continuous-time Dynamic Graph)", "content": "A continuous-time dynamic graph (CTDG) is a network with edges and nodes annotated with timestamps. Formally, we define a CTDG as G = {$V_T, E_T, T$} where T : V,E \u2192 R+ is a function that maps each node and edge to a corresponding timestamp."}, {"title": "B. Problem Formulation", "content": "The objective of this paper is to design a dynamic graph representation (embedding) method. The mathematical formulation of this problem could be defined as:\nGiven a dynamic graph G = {$V,E,T$}, our goal is to learn a mapping function $f : v_i \\rightarrow z_i \\in R^d$, where $d \\ll |V|$, and $z_i$ is the embedded vector that preserves both temporal and structural information of vertex $v_i$."}, {"title": "IV. PROPOSED METHOD", "content": "In this section, we propose a new unsupervised dynamic graph representation learning method that addresses two progressive challenges: (1) how to apply contrastive learning to dynamic graphs (presented in IV.A); (2) based on the solution of the first challenge, how to specifically select timespans as contrastive pairs (presented in IV.B).\nThe framework overview and workflow of CLDG are shown in Figure 2. It consists of five major lightweight components: timespan view sampling layer, base encoder, readout function, projection head and contrastive loss function. To begin with, the view sampling layer extracts the temporally-persistent signals. Then, the base encoder and the readout function learn the local and global representations. Furthermore, the projection head maps the representations into the space of the contrastive loss. Finally, the contrastive loss function is used\nto maintain the temporal translation invariance of the local and the global."}, {"title": "A. Temporal Translation Invariance", "content": "CLDG aims to generalize contrastive learning to dynamic graphs, where the key challenge is to select proper contrastive pairs. In the field of NLP and CV, the common solution for this issue is to generate different views of the samples as positive pairs through data augmentation. It shows good performance since the augmentation methods generate semantically invariant positive pairs. For example, a leopard in an image remains a leopard after geometric transformations, such as flip, rotate, crop, scale, pan, dither, etc., and pixel transformation methods, such as noise, adjust contrast, brightness, saturation, etc. However, the graph is an abstraction of the real world, and existing graph data augmentation methods such as node perturbation, edge perturbation, and feature perturbation may cause the labels of nodes or graphs to change. For example, with edge perturbation in a citation network, adding edges may cause completely irrelevant papers to be linked together, and edge dropping may drop edges and neighbors that are most important to a node. Therefore, proposing a more graceful contrastive pairs selection method is of great importance in our problem.\nIn fact, there are plenty of additional cues provided by dynamic graphs in the temporal component. To explore the properties of dynamic graphs, we perform some empirical studies. Specifically, we first process seven dynamic graph datasets G\nby spliting each of them into many timespans {$G_1, G_2, ..., G_T$}\n(the specific datasets described in Section V-A1). For each timespans, we train common GNN encoders with the same architecture but without shared weights. The trained model is then used to predict the node labels. An interesting observation is that regardless of the encoder used for training, the prediction labels of the same node tend to be similar in different\ntimespans, as shown in Figure 1. We refer to this phenomenon\nas temporal translation invariance.\nUnder the assumption of temporal translation invariance, we can utilize different timespan views to construct local (node pairs) and global (node-graph pairs) contrastive pairs. Specifically, we implement local-level and global-level temporal translation invariance. In local-level temporal translation invariance, we treat the semantics of the same node in different timespan views as positive pairs, pulling closer the representation of the same node in each timespan view, and pulling apart the representation of different nodes. In global-level temporal translation invariance, we treat the semantics of the node and its neighbor in different timespan views as a positive pair, pulling closer the representation of a node and its neighbor at different timespan views, and pulling apart the neighbor representation of other nodes."}, {"title": "B. Timespan View Sampling Layer", "content": "Based on temporal translation invariance, CLDG utilizes different timespan views as contrastive pairs. However, how to specifically choose appropriate timespans is still an open question. First, it is intuitive that the interval distance between different timespan views may affect the contrastive learning results. In the case of two timespan views, if more temporal overlap exists between two views, they theoretically share more similar semantic contexts, but this may lead to over-simplified tasks. If two views are separated for a long time, they may have completely different neighbors and may no longer share the same semantic context. In addition, how to choose the appropriate number and size of timespan views is also to be studied. Therefore, we design four different timespan view sampling strategies to explore the optimal view interval distance selection, as shown in Figure 3. The main difference between each strategy is the rate of physical temporal overlap, thereby sharing different semantic contexts. Meanwhile, each sampling strategy considers both the number"}, {"title": "Specifically, given a continuous-time dynamic graph G, we\nfirst define that the overall timespan of graph G is \u0394t =\nmax (T)-min (T)", "content": null}, {"title": "Then, we set two factors v, s to control the number and size of the sampling timespan views respectively,\ni.e., v views are sampled for each strategy and the timespan\nof each view is \u2206t/s, and v, s \u2208 N* are hyperparameters that\ncan be set according to the actual physical implication of the graph and the density of the graph. Finally, the timespan view\nsampling strategy can be formulated as:", "content": null}, {"title": "(T\u2081,\u2026\u2026, Tv) = Rsample (s, v, \u2206t),", "content": null}, {"title": "where Rsample (,,) returns a sample time tuple, where $T_i \\in\n[min (T) + \\frac{\u0394t}{2s}, max (T) - \\frac{\u0394t}{2s}], \\forall i \\in [1, v]$.", "content": null}, {"title": "We obtain v timespan views, i.e., $G_1, G_2,\u2026\u2026, G_v$, according\nto the sample time tuples ($T_1,\u2026\u2026,T_v$), where $G_i$ retains all\nedges of G in a specific timeframe [$T_i-\\frac{\u0394t}{2s}, T_i+\\frac{\u0394t}{2s}$]. Specif-\nically, the sample time tuple can be generated by four different\nstrategies. We formally define four sampling strategies by the\ntime difference between $T_i$ in the sample time tuple and its\npredecessor $T_{i-1}$ and successor $T_{i+1}$.", "content": null}, {"title": "Sequential Sampling Strategy", "content": "This strategy first divides\nthe dynamic graph into s timespan views, with no intersection\nbetween each view, and then randomly samples v unduplicated\nviews, where $v < s$. This strategy is similar to the processing\nmethod of DTDG, which can be directly used in DTDG."}, {"title": "Ti - Ti\u00b11 = a", "content": "$\\frac{\u0394t}{S}$"}, {"title": "where a \u2208 N*, the time difference between any two views is an integer multiple of the timespan of the view.", "content": null}, {"title": "High Overlap Rate Sampling Strategy", "content": "The v views\nsampled by this strategy are interdependent. We set an overlap\nrate of 75%, that is, the time between the sampled dynamic\ngraphs $G_i$ and $G_{i\u00b11}$ corresponding to $T_i$ and $T_{i\u00b11}$ has a 75%\noverlap."}, {"title": "|Ti - Ti\u00b11", "content": "$\\frac{[min (T) + \\frac{\u0394t}{2s}, max (T) - \\frac{\u0394t}{2s}]}{\u0394t} = \\frac{\u0394t}{4s}$"}, {"title": "where 71 is limited by $\\frac{[min (T) + \\frac{\u0394t}{2s}, max (T) - \\frac{\u0394t}{2s}]}{\u0394t}$", "content": null}, {"title": "Ti - Ti\u00b11| = $\\frac{3\u0394t}{4s}$", "content": null}, {"title": "where $T_1 \\in \\frac{[min (T) + \\frac{\u0394t}{2s}, max (T) - \\frac{\u0394t}{2s}]}{\u0394t}$ and random sampling in each epoch, assuming that $T_1 < T_i < T_v$ is satisfied in the sampled time tuple. Then T2 to To are obtained according to Eq. 4.", "content": null}, {"title": "In each training epoch, the timespan view sampling layer first samples V graph views, defined as {$G_1, G_2, ..., G_v$} and $G_i = {X_i, A_i}$, where $X_i$ and $A_i$ are the feature matrix and adjacency matrix of the i-th view. For each view, CLDG allows arbitrary encoder network architecture selection without any restrictions. We can even use the encoder of existing dynamic graphs. In this paper, for simplicity, we choose GCN [20] as the base encoder to model the structural dependencies. The multi-layer graph convolution of GCN is defined as follows:", "content": null}, {"title": "H(1+1) = \u03c3 (D\u2212\u00bd\u00c3 D\u2212\u00bd H(1)W(1)),", "content": null}, {"title": "where for each view $G_i, \\tilde{A} = A + I_N$ and $I_N$ is the\nidentity matrix, D is the degree matrix of $A, H(0) = X$.\nW(1) is a layer-specific trainable transformation matrix. It is\nworth noting that the different views sampled on the dynamic graph according to the timespan view sampling layer contain\ndifferent sets of nodes and edges. Therefore, we used the minibatch training approach in the specific implementation. Simultaneously, to ensure better consistency of the learned representations among views, we input all views into the encoder with shared weights, and the node embedding output by the i-th view is $H_i = f_\u03b8 (X_i, A_i)$, where $H_i \\in R^{N\u00d7d}$, N is the batch size, d is the embedding dimension.", "content": null}, {"title": "D. Readout Function Layer", "content": "After the base encoder layer, we get the embedding of each node. To maintain the global temporal translation invariance on the graph, i.e., a node and its neighbors at different views constitute a positive pair, it is necessary to consider how to generate the neighbor representations of nodes. Some works"}, {"title": "E. Projection Head Layer", "content": "The base encoder and readout function extract the representation of nodes and node neighborhoods from the sampled time views. Then, we feed the representations into a shared weight projection head, which maps the representation to the space where contrastive loss is applied. The projection head formula is as follows:"}, {"title": "zl, zN = proj (h), proj (h),", "content": null}, {"title": "where proj (.) is an MLP with two layers, 12 normalized and LeakyReLU non-linearity, as shown in Figure 4.", "content": null}, {"title": "F. Contrastive Loss Function", "content": "Based on the observation that node labels tend to be invariant over the entire dynamic graph time period, we propose a new inductive bias on dynamic graphs, i.e., local and global temporal translation invariance. Distinguishing from previous work, we learn node embedding by maximizing the temporal consistency between local or global. Specifically, in local temporal translation invariance, we treat the semantics of the same node in different timespan views as positive pairs, and different nodes as negative pairs. In global temporal translation invariance, we treat the semantics of the node and its neighbor in different timespan views as a positive pair, and the neighbor of other nodes as negative pairs. In this paper, we use the InfoNCE [28] contrastive approach. The objective of local temporal translation invariance is defined as follows:"}, {"title": "Lnode = - log \u03a3Nn=1 exp(zz)/\u03c4  Tk Tk\nN\ni=1", "content": null}, {"title": "where zTa and zTk are the representations learned by node i in\nviews Tq and Tk The through the encoder and projection head, and q \u2260 k. N is the batch size, and 7 is a temperature parameter.\nThe objective of global temporal translation invariance is defined as follows:", "content": null}, {"title": "ZN  N zkzi/\u03c4\nCgraph = -log\u03a3N exp(zj N zkzi/\u03c4)\ni=1 q=1\ni=1 q=1", "content": null}, {"title": "where ZN zk is the representation of the neighbors of node i in\nsnapshot Tk through the encoder and projection head.", "content": null}, {"title": "CLDG is trained to minimize local or global temporal trans-\nlation invariance. The local and global contrastive learning loss\nof CLDG is as follows:", "content": null}, {"title": "N V V Cnode =\u03a3\u03a3\u03a3node ,\ni=1 q=1 k\u2260q\niqk", "content": null}, {"title": "N V V Cgraph =\u03a3\u03a3\u03a3 graph,\ni=1 q=1 k\u2260q\niqk", "content": null}, {"title": "where V is the multiple views sampled by the timespan view sampling layer.", "content": null}, {"title": "To sum up, in each training epoch, we first sample multiple different views through the timespan view sampling layer. Subsequently, N nodes are sampled from the nodes shared by multiple views as a minibatch. Afterward, the representation of nodes and node neighborhoods in the minibatch are learned by using the encoder and projection head. Finally, the parameters in the encoder and projection head are updated by minimizing Eq. 11 or Eq. 12.", "content": null}, {"title": "G. Advantages over Dynamic Graph Methods", "content": "Compared with previous dynamic graph work, CLDG has the following advantages:\nBetter Generality. Existing dynamic graph work is generally designed solely for discrete-time dynamic graphs or continuous-time dynamic graphs. However, the timespan view sampling layer enables CLDG to handle both discrete-time and continuous-time dynamic graphs.\nBetter Scalability. The encoder module of CLDG can use any network architecture, and any effective encoder in the future can also be integrated into CLDG in a way similar to hot swap.\nLower Space and Time Complexity. A generalized dynamic graph learning paradigm is to use graph neural networks to model structural information, and then use sequential mod-els to explicitly model evolutionary information. For example, graph neural networks choose GCN, GAT, and sequence models choose RNN, LSTM, etc. However, CLDG implicitly exploits the additional temporal cues provided by dynamic graphs through contrastive learning, omitting the sequential model architecture. Therefore it has lower space and time complexity."}, {"title": "V. EXPERIMENTS", "content": "In this section, we first introduce seven real-world dynamic graph datasets and twelve baselines. Then, we conduct extensive experiments to prove the CLDG yields consistent and significant improvements over baselines. Finally, ablation experiments on different components such as the timespan view sampling layer, base encoder layer, readout function, and contrastive loss function. The code and data are publicly available 1.\nThe experiment aims to answer the following questions:\n\u2022 RQ1: Does CLDG yield state-of-the-art results for unsupervised dynamic graph representation learning on established baselines?\n\u2022 RQ2: How to choose the appropriate timespan as a contrastive view?\n\u2022 RQ3: Does CLDG allow various encoder architecture choices without any limitations? Do different encoders have any effect on CLDG?\n\u2022 RQ4: How about the time complexity and space complexity of CLDG?\n\u2022 RQ5: How do various hyperparameters impact CLDG performance?"}, {"title": "A. Experiment Preparation", "content": "1) Real-world Dataset: Seven datasets from four fields (i.e. academic citation network, tax transaction network, Bitcoin network, and reddit hyperlink network) are used to evaluate the quality of representations learned by CLDG. The statistics of the seven datasets are shown in Table II.\nAcademic Citation Network. The DBLP dataset is extracted from the DBLP 2. The label is the research field of researchers. The label information divides researchers into ten different research areas, and each researcher has only one label.\nTAX Transaction Networks. The TAX and TAX51 company industry classification datasets are extracted from the tax transaction network data of two cities provided by the tax bureau. It consists of companies as vertices, and transaction relationships between companies as edges. The industry code"}, {"title": "We also compare with other unsupervised learning methods,\nincluding methods designed for dynamic graphs and con-\ntrastive learning methods:", "content": null}, {"title": "Unsupervised Dynamic Graph Methods:", "content": null}, {"title": "B. Comparison with State-of-the-Art (RQ1)", "content": "To answer RQ1, we compare CLDG with 12 state-of-the-art algorithms on the seven dynamic graph datasets. The semi-supervised model in the baseline includes label propagation (LP), GCN, GAT, and GraphSAGE. We also compare with unsupervised dynamic graph models including CAW, TGAT, DySAT, and MNCI, and unsupervised contrastive learning methods including DGI, GRACE, MVGRL, and CCA-SSG.\nWe use the Accuracy and Weighted-F1 as the evaluation metrics, bold indicates that the method is optimal among unsupervised methods, and underlined indicates that it is optimal among all baselines. We report classification results on seven datasets in Table III. We implement local-level and global-level versions of CLDG according to Eq. 11 and Eq. 12, both of which outperform other unsupervised methods. CLDGgraph performs better on the DBLP dataset, and CLDGnode performs better on the other six datasets."}, {"title": "C. View Sampling Architecture (RQ2)", "content": "The timespan view sampling module consists of three factors: sampling strategy, view timespan factor s and the number of views sampled v. We design four different sampling strategies, sequential, high overlap rate, low overlap rate and random, and the details of the sampling strategy are demonstrated in this section IV-B. v controls the number of the sampling timespan views, and s controls the timespan size of the view. We show in Table Va and Table Vb how the three factors jointly affect the performance of CLDG on the DBLP and TAX datasets.\nThe variation on the dynamic graph is continuous and smooth, and if the timespan views overlap physically, better results may be achieved by sharing more similar semantic contexts. Therefore, the main difference between the four sampling strategies is the overlap rate between views. However, the"}, {"title": "D. Encoder Architecture (RQ3)", "content": "To answer RQ3, we implement three classical GNN encoders (GCN, GAT and GraphSAGE) for CLDG. We conduct experiments on the DBLP, BITotc and BITalpha datasets. The number of layers of the encoder is 2, the timespan view sampling strategy is sequential, s is 4, v is 2, the epoch is 200, 100 and 100, and the lambda is 0.5, 0.3 and 0.3, respectively. Table VI shows the classification results of different encoder"}, {"title": "E. Space and Time Complexity Analysis (RQ4)", "content": "To answer RQ4, we compared CLDG with four other dynamic graph methods (CAW, TGAT, DySAT, and MNCI) in terms of space and time complexity. In Table V, we measure"}, {"title": "F. Hyperparameters Sensitivity (RQ5)", "content": "We investigate the sensitivity of the parameters and report the Accuracy (%) results for four datasets with various parameters in Figure 5.\n\u2022 Effect of epoch. We first investigated the effect of epochs on CLDG. The results are shown in Fig. 5a, we can find that the performance of DBLP and TAX dataset keeps improving"}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we propose a simple and effective model CLDG, which generalizes contrastive learning to dynamic graphs. Specifically, CLDG first samples the input dynamic graph using a timespan view sampling layer. Subsequently, the local and global representations of each timespan view are learned through the encoder, projection head, and readout functions. Finally, the contrastive approach is used to maintain the temporal translation invariance of the local and the global. CLDG uses the additional temporal cues provided by dynamic graphs, thus avoiding the semantics and labels changes during the augmentation process. Experiments demonstrate the validity of the local and global temporal translation invariance assumptions. CLDG can achieve state-of-the-art unsupervised dynamic graph techniques and is competitive with supervised methods. Meanwhile, CLDG is very lightweight and scalable. Existing dynamic graph learning paradigms use graph neural networks and sequence models to model structural and evolutionary information, respectively. However, the main parameters and the training and inference time of CLDG depend on the base encoder. This allows CLDG to have lower space and time complexity, reducing the number of parameters and training time by up to 2,000.86 times and 130.31 times\nover existing models on the seven datasets. In addition, we also demonstrate the scalability of CLDG by replacing different base encoders. A limitation of our method is that it may not be applicable when the changes on the dynamic graph are non-continuous and non-smooth, and the labels on the graph are constantly changing."}]}