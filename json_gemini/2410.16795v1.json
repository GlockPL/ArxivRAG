{"title": "Traj-Explainer: An Explainable and Robust Multi-modal Trajectory Prediction Approach", "authors": ["Pei Liu", "Haipeng Liu", "Yiqun Li", "Tianyu Shi", "Meixin Zhu", "Ziyuan Pu"], "abstract": "Navigating complex traffic environments has been significantly enhanced by advancements in intelligent technologies, enabling accurate environment perception and trajectory prediction for automated vehicles. However, existing research often neglects the consideration of the joint reasoning of scenario agents and lacks interpretability in trajectory prediction models, thereby limiting their practical application in real-world scenarios. To this purpose, an explainability-oriented trajectory prediction model is designed in this work, named Explainable Conditional Diffusion based Multimodal Trajectory Prediction (Traj-Explainer), to retrieve the influencing factors of prediction and help understand the intrinsic mechanism of prediction. In Traj-Explainer, a modified conditional diffusion is well designed to capture the scenario multimodal trajectory pattern, and meanwhile, a modified Shapley Value model is assembled to rationally learn the importance of the global and scenario features. Numerical experiments are carried out by several trajectory prediction datasets, including Waymo, NGSIM, HighD, and MoCAD datasets. Furthermore, we evaluate the identified input factors which indicates that they are in agreement with the human driving experience, indicating the capability of the proposed model in appropriately learning the prediction. Code available in our open-source repository: https://anonymous.4open.science/r/Interpretable-Prediction.", "sections": [{"title": "Introduction", "content": "Motion prediction, in the autonomous driving setting, refers to predicting the future trajectories of the target agents with the considerations of the historical and current status of the target agents, context agents, road graphs, and traffic light signals, which still contain several challenges for guaranteeing the safe navigation of autonomous vehicles under various environmental uncertainties. (Feng et al. 2023; Zhou et al. 2023) First of all, motion prediction requires the joint reasoning of the future distributions of the surrounding agents that may interact with each other. Naively predicting and sampling from the marginal distribution of trajectories for each agent leads to unrealistic and conflicting outcomes. Whereas the historical trajectory of the target agents is quite informative most of the time, the interactions with other agents, traffic signals, and other environmental factors are typically sparse and short in duration, but can be crucial when they occur. Second, although deep learning frameworks, especially graph neural networks and transformers, have led to recent progress, the black-box nature of state-of-the-art models makes it increasingly difficult to understand the contributing factors of a prediction model and their quantitative significance among all input variables.\nDue to the dynamic and uncertain nature of real-world driving environments, accurately predicting the future trajectory of objects in the scene is essential. Traditional autonomous driving pipelines (Kato et al. 2018; Fan et al. 2018) often treat trajectory prediction as a separate module. This module receives inputs from the perception module and outputs predicted trajectories to the downstream planning module. However, this method has limitations in considering the global context and adapting to changing environments. Meanwhile, they usually lack the interpretation ability.\nJustifying motion prediction for autonomous vehicle decisions through explanations is pivotal for comprehending black-box outcomes and fostering public trust (Omeiza et al. 2021; Atakishiyev et al. 2024). Several attempts have been made to employ attention visualization techniques for explaining deep learning-based autonomous driving models (Zablocki et al. 2021). (Kim and Canny 2017) utilized a CNN-based attention model with additional saliency filtering to highlight regions causally influencing future steering angles. (Hou et al. 2019) introduced a CNN-based attention model to track vehicles or pedestrians that the network needs to focus on. (Kim and Bansal 2020) integrated spatial attention, convolutional feature networks, and recurrent agent networks within an attention bottleneck architecture to enhance model interpretability for trajectory prediction. (Zhou et al. 2021) proposed imitation learning, combining CNNs and RNNs with attention mechanisms to infer meaningful trajectories for guided agents. However, most existing models provide intrinsic explainability, which is inherent in the interpretability of self-explanatory models, and provide only single-modal predictions.\nIn light of these challenges, we present a trajectory prediction model with better explainability, which is a conditional diffusion representation enhancement with improved shapley value to provide multimodal and explainable future tra-"}, {"title": "Related Work", "content": "Recently, considering the necessity of perceiving the trajectory of automated vehicles, several researches have been conducted based on the generative models. These methods usually execute the motion prediction by inferring conditional probabilities p(s;c). For instance, HP-GAN (Barsoum, Kender, and Liu 2018) employed an enhanced Wasserstein GAN to learn a probability density function of future human poses conditioned on prior poses. Concurrently, methods like Conditional Variational Auto-Encoders (C-VAEs) (Oh and Peng 2022) and Normalizing Flows (Sch\u00f6ller and Knoll 2021) have demonstrated efficacy in learning these conditional probability density function for predicting future trajectories. Recent studies have explored diffusion models as an alternative for modeling conditional distributions of sequences, such as human motion poses (Zhang et al. 2024) and planning tasks (Janner et al. 2022). In a pertinent contribution, (Gu et al. 2022) applied diffusion models to capture uncertainties in pedestrian motion.\nFor these diffusion-based models, the multi-model information is captured by the cross-attention module. (Ngiam et al. 2021; Amirloo et al. 2022) took agent embedding as query, set map embedding as key and value, and conducted cross-attention module completing the interaction between map and agents track. (Varadarajan et al. 2022) performed state-of-the-art on the Argoverse Motion Forecasting Competition (Chang et al. 2019) based on designed attention interaction modules named multi-context gating (MCG). (Tang et al."}, {"title": "Explainable Analysis Methods", "content": "Deep learning systems have significantly advanced various aspects of our lives, yet their opaque nature presents challenges across many applications. The inability to provide explanations for decisions undermines trust in these systems. Consequently, a considerable amount of research has focused on developing explainable AI methods (Jin et al. 2022; Huang et al. 2023) to supplement network predictions with comprehensible explanations.\nIn the realm of practical explainable machine learning, there are two main approaches: opting for a straightforward, interpretable model structure and then training it, or training a sophisticated model and subsequently devising interpretable techniques to explain its decisions. These approaches categorize explainable machine learning models into intrinsic and post-hoc explainability (Moraffah et al. 2020). Intrinsic explainability pertains to models that are inherently interpretable (e.g., decision trees, rule-based models, linear models), while post-hoc explainability involves developing techniques to explain trained machine learning models. Depending on the scope, post-hoc explainability is further divided into global and local explainability. Global explainability aims to elucidate the overarching logic and internal workings of complex models, whereas local explainability seeks to clarify the decision-making process and rationale for individual input samples."}, {"title": "Methodology", "content": "In this section, we present our innovative Traj-Explainer framework designed to model intricate traffic scenarios while emphasizing feature importance. The framework is shown in Figure 1, including conditional diffusion model, scene encoding model, feature decoding model, and explanation analysis model."}, {"title": "Conditional Diffusion Model", "content": "As a traffic scene involves multiple traffic participants, a single-agent diffusion model (Yang et al. 2024) may generate sub-optimal samples when a scene involves significant interactions and uncertainties among multiple agents. To tackle this problem, we proposed a conditional diffusion to handle uncertainties and interaction. By modeling trajectories as diffusion processes, we can capture the stochastic and interaction nature of movement patterns and adapt to varying environmental conditions or unforeseen events. Our model operates on the history and future trajectories of all agents in a scene, and it thus can capture the spatial and temporal interaction among agents."}, {"title": "Scene Encoding", "content": "We utilize multiple embedding blocks with varying sizes and layers to effectively encode agents such as vehicles, bicycles, and pedestrians, alongside map and traffic sign data, ensuring the generation of diverse and realistic trajectories. These blocks are designed with a global capacity to capture comprehensive agent characteristics. This approach eliminates the need to convert the coordinate system into Frenet coordinates centered around individual agents, a conventional practice in existing trajectory prediction models."}, {"title": "Spatial and Temporal Fusion Attention", "content": "To effectively capture the complex dynamics of traffic scenarios, we introduce a Temporal Spatial Fusion Attention (TSFA) layer. This layer is specifically designed to integrate temporal and spatial characteristics inherent in the movements of predicted agents, leveraging multi-modal data that includes predicted agents, neighboring agents, map configurations, and traffic sign within traffic environments.\nTo enhance the description of movements, the TSFA layer enriches its features with \"scenario latent features\" information derived from a diffusion process. Multi-head self-attention blocks are employed to extract critical insights from the spatial-temporal data. These blocks play a crucial role in identifying essential spatial and temporal details within the broader context of multi-modal traffic scenes."}, {"title": "Feature Decoding", "content": "The trajectory decoder plays a crucial role in translating integrated traffic features into the future trajectories of agents. In this study, our decoder includes GRU blocks and KAN blocks, carefully configured to handle the temporal fluctuations inherent in agent movements. Inspired by the principles of multimodal trajectory prediction, which emphasize adaptability to agents exhibiting diverse behaviors over time, we introduce a multimodal output mechanism capable of effectively accommodating agents with varying actions in (Yang et al. 2024). More details and training objectives are provided in the Appendix B."}, {"title": "Explainability Analysis", "content": "In this study, we aim to deepen our understanding of the information utilized by trajectory generation models for effective performance. Specifically, we seek to quantify the"}, {"title": "Global and Scenario Feature Importance", "content": "Theorem 1. (Markov blanket) Given a feature Xi, the subset MC\\X\u00a1 is a Markov blanket of Xi if,\n$p({F\\{X_{i},M},C}|{X_{i},M}) = p({F\\{X_{i},M},C}|M)$ (1)\nThis means that M contains all the information about C that X; has about C. It is proved that strongly relevant features do not have a Markov blanket.\nTheorem 2. (Chain rule for mutual information) Given a set of random variables X = {X1,X2,\uff65\uff65\uff65,Xn} and random variable Y, then the mutual information of X and Y is defined as:\n$I(X,Y) = I(X_{1},X_{2},\\cdots,X_{n}; Y)$\n$= \\sum_{y\\in Y_{true}} \\sum_{X\\in X} p(x, y) log \\frac{p(x,y)}{p(x)p(y)}$\n$= \\sum_{i=1}^{n}I(X_{i}; Y|X_{i-1}, X_{i\u22122},\\cdots\\cdots, X_{1})$ (2)\nThe chain rule for mutual information indicates the amount of information that the random variables set X can provide for Y equals to the sum of pairwise mutual information of Y and each variable under certain conditions.\nDefinition 1. (Relative Feature Importance) Let X = (X1,X2,..., XN-1,XN) be the features of the to-be-predicted agent, and Y is ground truth of the future trajectories of the to-be-predicted agent. We define relative feature importance of Xi \u2208 X with respect to X\\X\u00a1 as:\n$RFI(X_{i}) = I(X_{i};Y\\|X\\X_{i})$ (3)\nRFI(X;) can be interpreted as the amount of reduced uncertainty of Y due to X\u00a1 given X\\X\u00a1. RFI(X;) > 0 means that Xi is conditional relevance to Y.\nDefinition 2. (Global Feature Importance) Let X = (X1,X2,..., XN-1,XN) be the features of the to-be-predicted agent, and Y is ground truth of the future trajectories of the to-be-predicted agent. From the perspective of information theory, the global feature importance of Xi \u2208 X is defined as,\n$GFI(X_{i}) = I(X_{i}; Y)$ (4)\nGFI(X;) can be interpreted as the amount of reduced uncertainty of Y due to X\u2081. Global importance returns the overall impact of a feature on the model and is usually obtained by aggregating the feature attribution to the entire dataset. The higher the absolute value, the greater the impact of the feature on the model's predictions.\nDefinition 3. (Scenario Feature Importance) Let xs = (X1,X2,..., Xn-1,xn) be the features local values of the to-be-predicted agent, and y is ground truth of the future trajectories of the to-be-predicted agent in a specific scenario. The Scenario Feature Importance is defined as:\n$SFI = I(x_{i}, y\\|x_{s}\\X_{i})$ (5)\nScenario importance returns feature attribution values for each explained scenario. These values describe how much a particular feature affects the prediction relative to the baseline prediction."}, {"title": "Implementation of Feature Importance Measure", "content": "Scenario Feature Importance Measure For trajectory prediction, we adjust the shapley value method by using a static, non-interacting agent as the baseline. This approach measures the contribution of a target agent's past trajectory relative to this static baseline. Contributions from neighboring agents, traffic sign, and map are evaluated against a scene where these elements are absent. Details of the shapley value implementation are provided in the Appendix.\nGlobal Feature Importance Measure Here our challenge lies in determining each feature's overall contribution across the dataset, particularly when feature dynamics vary across scenarios (Makansi et al. 2021). To address this issue, we categorize features into two types: global and scenario features. Global features, like the agent's past trajectory and map data, remain constant across scenarios. For these, we adopt the conventional averaging approach to estimate their overall importance, termed as global feature importance. Conversely, scenario specific features, such as neighboring agents and traffic sign, exhibit variability across different scenes. To handle these, we employ a two-step aggregation process. Firstly, within each scenario, we locally aggregate the feature's impact using the max operator. This method effectively identifies the most influential neighboring agent or significant traffic sign within that scenario. Secondly, we globally aggregate these scenario specific contributions across the dataset using the average operator. This approach ensures that we capture meaningful insights into the collective influence of these features while accounting for their scenario-specific variability."}, {"title": "Experiment and Result Analysis", "content": "We train and evaluate our model on four large-scale real-world datasets. Waymo Motion Prediction dataset (Waymo) (Sun et al. 2021) contains 576,012 9-second sequences, each of which corresponds to a real trajectory sequence collected"}, {"title": "Explainability Analysis", "content": "This paper discusses the impact of history trajectory, neighboring agents, traffic sign, and map features on the performance of the prediction algorithm. The minADE and minFDE are used as shapley error metrics. The prediction model is trained on the Waymo training set, and explainability analysis is performed on its test set.\nGlobal Feature Importance Figure 4 shows the feature importance of minADE error and minFDE error. The results show that the map feature is the most important, followed by the traffic sign, neighboring agents and history trajectory. This order of importance is consistent with human driving behavior cognition, indicating that our model has learned the correct feature semantics. You may question why the historical trajectory has the lowest importance. In fact, it is reasonable, because the history trajectory trajectories are strongly correlated features, but are weakly correlated features hen a map is given., that is, trajectory trajectories are redundant features given map, which also shows that our model has learned the redundant relationship between features.\nScenario Feature Importance Figure 5 shows the importance of traffic sign under different scenario modes. It can be seen that the importance of traffic sign in the case of stopping, turning left, and turning right is relatively large, and the importance is the smallest in the case of irregular road. The reasons may be as follows: stopping, turning left, and turning right are all carried out near the intersection roads, and their decisions are closely related to traffic sign. For example, turning right needs to consider the straight and left turn signals, and stopping will be affected by opposite signals and multiple crosswalks. In addition, irregular road generally have few traffic signs, so the corresponding features are the least important. The importance analysis of history trajectory, neighboring agents, and map are provided in the Appendix D.\nFigure 6 illustrates the prediction outcomes of the proposed framework across various scenarios, with each row depicting a distinct traffic environment. Specifically, four scenarios are presented: lane keeping, stop-start, turning, and interaction scenarios. In the first row of Figure 6, which pertains to lane keeping situations, predictions typically exhibit high cogni-"}, {"title": "Ablation Studies on Traffic Scene Encoding", "content": "In this subsection, we assess the structural impact by adding or removing components of the traffic scene encoder, which includes spatial and temporal attention modules, social for-"}, {"title": "Ablation Studies on Trajectory Decoding", "content": "In this subsection, we evaluate the architecture of the trajectory decoder, with a specific focus on GRU (Gated Re-current Unit) blocks and KANblocks. This evaluation aims to ascertain how each component and configuration impacts the decoder's ability to accurately predict future trajectories, emphasizing their critical role in enhancing model performance."}, {"title": "Conclusion", "content": "In this study, we introduce Traj-Explainer, a novel approach for Explainable Conditional Diffusion-based Multi-modal Trajectory Prediction. Our method, Traj-Explainer, incorporates an advanced conditional diffusion technique to model diverse trajectory patterns. Additionally, we enhance the shapley value model to better understand global and scenario-specific feature importance. Through extensive evaluations, we demonstrate the effectiveness of our approach on several benchmarks compared to other baselines. Future research directions involve extending the diffusion-"}, {"title": "Appendix A", "content": "Positional and Semantic Embedding The Positional Embedding consolidates all positional attributes, denoted as pi, into a unified one-dimensional matrix. Simultaneously, the Feature-Embedding translates the agent's characteristics, such as height, width, and type, represented in a one-hot encoding format into a corresponding one-dimensional matrix. For each predicted agent i,\n$Pi = f_{p}(x_{i}, y_{i})$ (6)\nwhere fp represents a linear transformation layer, and Pi denotes the positional embedding result. The semantic embedding is denoted as:\n$Fi = f_{f}(f_{w}, f_{h}, f_{type})$ (7)\nwhere ff indicates a linear transformation layer, with fw, fh and ftype representing the agent's width, height and type, respectively, the agent's type is converted to a one hot encoded format. After that, to obtain a comprehensive representation of each agent, the position and feature embedding are synthesized by using a ReLU activation function, which is defined as:\n$R_{i} = ReLU(LayerNorm(concat(P_{i}, F_{i})))$ (8)"}, {"title": "Appendix B", "content": "To mitigate the influence of agents' initial positions on outcomes, our model outputs include move statements for agents along with their corresponding likelihoods. For agent i, the formulation of the trajectory in our model is expressed as follows:\n$traj_{i} = pos_{cur}+ \\sum_{t=cur}^{T} (\\delta x, \\delta y, \\delta \\sigma)$ (9)"}, {"title": "Training Objectives", "content": "Regression Loss Huber loss is a commonly used loss function for constraining the prediction. In our work, we inject diffusion generation information to increase the prediction's interaction ability and diversity. The trajectory with minimal loss is selected, and its deviation from the ground truth is quantified using the Huber loss.\n$L_{reg} = Huber(\\delta_{i}, S_{i})$ (10)\nConfidence loss To score the proposed region based on the probability of driving intention, we utilize the Kullback-Leibler Divergence as our loss function. This loss function restricts the contribution of probabilities from the proposals based on the distance between the predicted trajectory and ground truth.\n$L_{conf} = \\frac{1}{N} \\sum_{j=1}^{N} D_{KL}((\\phi_{i})||(\\gamma_{i}))$ (11)\nClassification loss We cluster all endpoint coordinates into K categories and label the ground truth and prediction proposals with the nearest cluster centroid category label. We employ cross-entropy as an additional loss function to enhance the capability of predicting the driving intention of the vehicle.\n$L_{cls} = \\frac{1}{N} \\sum_{j=1}^{N} CrossEntropy(p(s_{i}), p(s_{i}))$ (12)\nwhere p((si)) is the probability distribution of predicted proposals, p(si) is the ground-truth distribution of ground-truth proposals.\nHence, We formulate the loss as a sum of multiple task loss and use an auxiliary learning (Liebel and K\u00f6rner 2018) approach to balance them.\n$L_{sum} = \\frac{1}{\\alpha_{1}^{2}}L_{reg} + \\frac{1}{\\alpha_{2}^{2}}L_{conf} + \\frac{1}{\\alpha_{3}^{2}}L_{cls} + \\frac{1}{\\alpha_{4}^{2}}L_{diff} + log(\\alpha_{i} + 1)$ (13)\nwhere Ldiff is the L2 loss of the predicted noise and the original noise that is the standard diffusion model loss function.\nai, i = 1,2,3,4 are learnable weights."}]}