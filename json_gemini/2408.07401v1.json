{"title": "DataVisT5: A Pre-trained Language Model for\nJointly Understanding Text and Data Visualization", "authors": ["Zhuoyue Wan", "Yuanfeng Song", "Shuaimin Li", "Chen Jason Zhang", "Raymond Chi-Wing Wong"], "abstract": "Data visualization (DV) is the fundamental and\npremise tool to improve the efficiency in conveying the insights\nbehind the big data, which has been widely accepted in existing\ndata-driven world. Task automation in DV, such as converting\nnatural language queries to visualizations (i.e., text-to-vis), gener-\nating explanations from visualizations (i.e., vis-to-text), answering\nDV-related questions in free form (i.e. FeVisQA), and explicating\ntabular data (i.e., table-to-text), is vital for advancing the field.\nDespite their potential, the application of pre-trained language\nmodels (PLMs) like T5 and BERT in DV has been limited by\nhigh costs and challenges in handling cross-modal information,\nleading to few studies on PLMs for DV. We introduce DataVisT5,\na novel PLM tailored for DV that enhances the T5 architecture\nthrough a hybrid objective pre-training and multi-task fine-tuning\nstrategy, integrating text and DV datasets to effectively interpret\ncross-modal semantics. Extensive evaluations on public datasets\nshow that DataVisT5 consistently outperforms current state-of-\nthe-art models on various DV-related tasks. We anticipate that\nDataVisT5 will not only inspire further research on vertical PLMs\nbut also expand the range of applications for PLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Data visualizations (DVs) utilize graphical representation to\nconvey insights to summarize the massive raw data, which is a\ncommon practice in existing big data era [1], [2]. Popular data\nanalysis and database applications, such as Google Sheets\u00b9\nand Microsoft Power BI\u00b2, all support DV features. Many\ninstitutions realize the value of DV and have applied it as their\ndaily fundamental tools. Thus the ability of creating suitable\nDVs has become a necessary skill for data analysts, engineers,\nand data scientists [3]\u2013[5]. However, creating appropriate\nDVs remains challenging, even for experts, since it requires\nvisual analysis expertise and familiarity with the domain data.\nFurthermore, users must master the complex grammar of\nDeclarative Visualization Languages (DVLs), such as Vega-\nLite [6], ggplot2 [7], and Vega-Zero [8], to accurately define\nDV specification in the visualization engine.\nTo lower the barriers to creating DVs and further unlock\nthe power of DV for the general public, researchers have\nproposed a variety of DV-related tasks that have attracted sig-\nnificant attention from both industrial and academic researchers.\nNumerous studies on these topics have been presented in\nleading conferences and journals such as VLDB [2], [9], [10],\nICDE [11], [12], SIGMOD [13]\u2013[15], and TKDE [16], [17].\nThese tasks include text-to-vis (i.e., automatically generating\nDVs from natural language questions) [8], [15], vis-to-text\n[18] (i.e., automatically generating interpretations of complex\nDVs for educational purposes), FeVisQA [12] (i.e., free-form\nquestion answering over data visualization), and table-to-text\n(i.e., describing a given table) [19].\nA vivid example is given in Figure 1, which shows four\nimportant tasks central to the domain knowledge of DV: text-to-\nvis, vis-to-text, FeVisQA and table-to-text. The figure presents\na natural language (NL) question, \u201cGive me a pie chart about\nthe proportion of the number of countries in the artist table.\u201d\nThis example demonstrates the text-to-vis task\u2019s capability\nto interpret the NL question and transform it into a Vega-\nLite specification, resulting in a pie chart. The DV query,\nintroduced by [15], serves as a bridge in the text-to-vis process,\nencapsulating visualization details and data operations with a\ngrammar akin to SQL. Translations between DV queries and\nDVLs are seamless, with text-to-vis tasks primarily focusing\non converting NL questions into DV queries. Conversely,\nthe vis-to-text task aims to generate accessible and user-\nfriendly explanations of complex visualizations for individuals\nwithout expertise in the field. The FeVisQA task addresses\nuser inquiries regarding DV by providing detailed answers\nto common questions. We present four typical DV-related\nquestions, including understanding the semantics of a DV query,\nresolving numerical issues within a chart, and evaluating the\ncompatibility of a DV query with a given database. Lastly,\nthe table-to-text task generates informative NL description of\ntabular data, which are essential for visual analytics, thereby\nreducing the perceptual effort needed for data interpretation.\nMeanwhile, PLMs such as BERT [20] and T5 [21] have\nreceived considerable attention in the realms of natural lan-\nguage processing (NLP) and data mining, becoming widely\nrecognized for their efficacy. These PLMs greatly promote\nthe development of effective text-driven applications, since\nthey show dominating performance in understanding the\nsemantics in natural language. The operational paradigm for\nthese PLMs typically unfolds in two stages: initially, they\nundergo unsupervised pre-training on expansive, open-domain\ndatasets (such as Wikipedia) to acquire foundational capabilities\nin language representation and comprehension; subsequently,\nthey are fine-tuned on specialized corpora pertinent to targeted\ndownstream tasks, thereby enhancing task-specific performance.\nDespite their success [22]\u2013[24], there are still significant\nchallenges when it comes to the DV field : (i) Limited studies\nhave been conducted to explore the effectiveness of PLMs in\ncapturing the DV semantics. (ii) Since there is a substantial"}, {"title": "II. PRELIMINARY", "content": "This section provides the foundational concepts and def-\ninitions pivotal to DV-related tasks, with the objective of\ncultivating a more profound understanding.\nNatural Language Question. An NL question enables users,\neven those with a minimal background in DV and programming\nskills, to formulate queries intuitively. Figure 1 demonstrates\nsuch an instance, with the user\u2019s request articulated as, \u201cGive\nme a pie chart about the proportion of the number of countries\nin the artist table\u201d."}, {"title": "III. OUR PROPOSED MODEL: DATAVIST5", "content": "We present our proposed DataVisT5 model, with the pipeline\noverview in Section III-A. This is followed by details on\ndatabase schema filtration in Section III-B, DV knowledge\nencoding in Section III-C, and standardized encoding in\nSection III-D. We discuss our hybrid pre-training objectives\nin Section III-E and conclude with our multi-task fine-tuning\nstrategy in Section III-F."}, {"title": "A. Pipeline Overview", "content": "Figure 2 provides an overview of the complete pipeline,\ncomprising five main stages: (1) Database schema filtration, (2)\nDV knowledge Encoding, (3) Standardized Encoding, (4)Model\nPre-training, and (5) Model Fine-tuning. The Database schema\nfiltration process involves comparing n-grams extracted from\nthe given database schema with those present in the text under\nconsideration, enabling us to identify referenced tables in the\nquestion and acquire a sub-database schema that aligns seman-\ntically. During the DV knowledge Encoding phase, we linearize\nDV knowledge encompassing DV queries, database schemas,\nand tables. Subsequently, in the Standardized Encoding phase,\nwe normalize the DV knowledge to facilitate more efficient\nlearning. The resulting corpus, in its unified form, is then\nemployed to train our proposed DataVisT5 model."}, {"title": "B. Database Schema Filtration", "content": "Before the integration of DV and text modalities, it is critical\nto recognize that NL questions can incorporate keywords\nrelated to the database schema. This requires the explicit\nidentification of references to columns, tables, and conditional\nvalues within the NL questions. To address this challenge, we\nemploy N-gram matching as a method due to its simplicity\nof implementation and notable effectiveness for a variety of\napplications. In an effort to minimize information loss, our\nprimary focus is at the table level, where we compare N-\ngrams extracted from the NL questions to those present in the\ndatabase tables. Following the initial comparison, we refine\nthe obtained sub-schema by considering the implicated tables\nand their respective columns.\nExample. Our Database Schema Filtration technique, demon-\nstrated in Figure 3, matches n-grams between the NL question\n\u201cGive me a pie chart about the proportion of the number of\ncountries in the artist table\u201d and the database tables artist and\nexhibition_record. It then extracts a sub-schema centered on\nthe artist table."}, {"title": "C. DV Knowledge Encoding", "content": "To address the disparity between text and DV modalities,\nwe propose investigating unified formats for DV knowledge.\nThe connection between natural language and DV knowledge\nposes challenges due to limited data accessibility. Nevertheless,\na unified format allows models to capitalize on extensive\npretraining for smaller datasets. Employing consistent format-\nting, as recommended by [29], offers advantages in multi-task\ntraining and mitigates performance decline caused by data\nheterogeneity compared to single-task training. The subsequent\nsections provide a comprehensive introduction to the unified\nrepresentation of three distinct types of DV knowledge: DV\nqueries, database schemas, and tables.\nEncoding DV query. While most existing NLP models, such\nas [20], consider NL inputs as flat text sequences, we adopt a\nsimilar approach for modeling a DV query by treating it as a\nplain text sequence in a straightforward manner.\nEncoding Database schema. The database schema comprises\ntables and columns. For each table in the schema, the table\nname is followed by a list of its columns formatted as \u201ctable :\ncolumn1, ... columnn\u201d. Different tables are joined using the\nsymbol \u201c|\u201d. Additionally, the database name is prefixed to the\ngenerated sequence with boundaries indicated by \u201c<>\u201d.\nEncoding Table. Following [30], we employ a sequential\nrepresentation of tables, akin to the schema encoding technique,\nwhich uses distinctive tokens to delineate table structure. The\ntable is linearly represented as \u201ccol:C1 |\u00b7\u00b7\u00b7|CN row 1 : v11 |\n\u00b7\u00b7\u00b7| U1N row M: VM1|\u00b7\u00b7\u00b7| UMN\u201d, with N indicating\nthe total column count and M representing the row count.\nExample. An presented in Figure 4, where (1) the DV query\nis sequentially encoded into text sequences based on the\ndata manipulation operations: Visualize, Select, Count, and\nGrouping, (2) the filtered database sub-schema, including\nthe database name (theme_gallery), table name (artist), and\ncolumns, is encoded into a corresponding text sequence, and\n(3) the table content is linearly encoded in the format \u201ccol:\nCountry | COUNT(Country)\u201d, along with the remaining three"}, {"title": "D. Standardized Encoding", "content": "Due to the manual generation of queries by multiple\nannotators with diverse annotation habits, subtle stylistic\ndifferences are prevalent in the final annotated DV queries\nwithin NVbench, including variations in the capitalization of\nkeywords. Similar to issues encountered with SQL queries,\nthese stylistic inconsistencies, while not affecting the model\u2019s\nexecution results, pose an additional learning challenge that\nmust be addressed.\nTo address the stylistic variations in DV queries, a pre-\nprocessing strategy was implemented before training. This\nstrategy includes: (1) affixing the primary table name T to the\nselected columns col, resulting in the notation T.col across DV\nqueries; particularly, for instances where the wildcard symbol\n* is employed in a COUNT function, COUNT(*) is replaced\nwith COUNT(T.col) to maintain uniformity; (2) the insertion of\nspaces surrounding parentheses and the replacement of double\nquotes with single quotes; (3) the inclusion of the ASC keyword\nsubsequent to the ORDER BY clause when ordering is not\nexplicitly specified; (4) the elimination of the AS clause and\nthe substitution of table aliases (e.g., T1, T2) with their actual\ntable names; (5) the lowercase conversion.\nExample. In a DV query with a Join operation, as depicted\nin Figure 5, standardization involves renaming table aliases\nT1 and T2 to player and team, respectively. The query\u2019s\nCOUNT(*) is specified as COUNT(player.years_played),\n\u2018Columbus Crew\u2019 is quoted with single quotes, the ASC\nkeyword is appended if sort order is absent, and the entire\nquery is cast to lowercase.\nIn alignment with the standardization of DV queries, similar\nencoding steps are applied to database schemas and tables to\nensure consistency. This includes affixing the table name T to\neach column name col and converting them to T.col.\nExample. As depicted in Figure 4, within a specific database\nschema, column names such as \u201cage, name, country, year_join,\nand artist_id\u201d are transformed to \u201cartist.age, artist.name,\nartist.country, artist.year_join, and artist.artist_id\u201d, respec-\ntively. Similarly, within the table context, an entry like \u201ccol\n: Country | COUNT(Country)\u201d is reformulated to \u201ccol :\nartist.country | count(artist.country)\u201d."}, {"title": "E. Hybrid Pre-training Objectives", "content": "Bidirectional Dual-Corpus Objectives. We utilize a bidirec-\ntional dual-corpus strategy in our research methodology where\nboth source and target corpora are randomly selected with an\nequal probability (0.5) during model training as the input. The\nremaining corpus is then used as the output for translation\npurposes. Accordingly, for a target sequence of T tokens, we\ndefine the Bidirectional Dual-Corpus loss function, $\\mathcal{L}_{BD}(\\theta)$,\nas follows:\n$\\mathcal{L}_{BD} (\\theta) = \\sum_{i=1}^{T} - \\log P_{\\theta}(t_i | s, t_{<i}),$ (1)\nwhere s signifies the source input, $t_{<i}$ represents the sequence\nof tokens generated by the decoder up to but not including the\ni-th token, and $t_i$ is the token that the decoder is tasked with\npredicting. The term $\\theta$ denotes the model parameters.\nAs depicted in Figure 6, the segment highlighted by arrows\nelucidates the deployment of the Bidirectional Dual-Corpus\nObjectives, encompassing four discrete tasks germane to DV. A\ncomprehensive definition of these tasks is deferred to Section V.\nTo enhance task-specific processing and facilitate knowledge\ntransfer across different modalities, we introduce unique special\ntokens. For example, as demonstrated in Figure 6, the Text-\nto-Vis task utilizes a special token <NL> to prefix the NL\nquestion corpus and  for the DV query corpus. In\ncontrast, for the FeVisQA task, DV question-answer pairings\nare delineated with the tokens  and  to signify their respective components.\nT5-based MLM Objectives. The application of Masked\nLanguage Modeling (MLM) as a pretraining objective is pivotal\nfor pretraining encoder-decoder models. In our study, we\nemployed the span corruption MLM strategy from [21], where\nconsecutive words in the input are replaced by sentinel tokens,\nand the decoder generates the omitted text, each instance\npreceded by its respective sentinel. To ensure consistency\nwith the pretraining checkpoint, we maintained an average\nspan length of 3 subword tokens across the input corpus and\nmasked 15% of the subwords. This MLM objective was applied\nto a cross-modal corpus comprising text, DV query, database\nschema, and table. Over a sequence of N tokens, our T5-based\nMLM loss is defined as:\n$\\mathcal{L}_{MLM} (\\theta) = -\\sum_{n=1}^{N} \\log P_{\\theta} (x_m | x_{\\setminus m}, x_n),  x_{\\setminus m},$ (2)\nwhere $\\theta$ are the model parameters, $x_m$ is the masked token\npredicted by the decoder, $x_{\\setminus m}$ represents the unmasked encoded\ninputs, and $x_n$ is the sequence of tokens generated by the\ndecoder up to but not including the n-th token.\nAn illustration is presented in Figure 6, where the segments\nlinked by dashed lines pertain to the T5-based MLM Objectives.\nThis figure showcases the application of span denoising targets\nto a DV query. Within this query, the terms \u201cbar\u201d, \u201cpeople\ngroup\u201d, \u201cby\u201d, and \u201cdesc\u201d are selected at random. Subsequently,\na subset of these terms is replaced by sentinel tokens, illustrated\nas  , , , and .\nHHybrid Objectives. After achieving the aforementioned two\nobjectives, we create a hybrid objective by sampling from\nboth the MLM Objectives and the Bidirectional Dual-Corpus\nObjectives corpora. Consequently, each training mini-batch is\ncomposed of examples drawn from a cross-modal corpus, each\nformatted to align with diverse learning objectives. We adopt\na final hybrid loss $\\mathcal{L}_{H}$:\n$\\mathcal{L}_{H}(\\theta) = \\mathcal{L}_{BD}(\\theta) + \\mathcal{L}_{MLM}(\\theta),$ (3)\nwhich enables DataVisT5\u2019s readiness for multiple DV-related\ndownstream tasks demanding contextual comprehension and\npattern recognition."}, {"title": "F. Multi-Task Fine-tuning", "content": "To achieve better performance in multiple downstream\ntasks related to DataVisT5, we employ temperature mixing to\ncombine the training data of all tasks. The temperature value is"}, {"title": "IV. PRETRAINING DATASET CONSTRUCTION", "content": "We have constructed a dataset tailored for our Hybrid\nPretraining Objectives by integrating four public datasets. The\nfollowing sections outline our pretraining dataset construction,\ndetailing data collection in Section IV-A, data processing in\nSection IV-B, and data partitioning in Section IV-C."}, {"title": "A. Data Collection", "content": "1) NVBench: The NVBench dataset [15] represents a\npublicly accessible NL2Vis corpus, containing 7,219 pairs\nof NL questions and their corresponding DV queries. It\nwas originally curated to evaluate the efficacy of models in\ntransforming textual queries into visual representations. As\nthe most commonly utilized dataset in this domain, NVBench\nhas been employed in several prominent studies, including\nthose by [8], [18], [31] Table I offers a detailed overview\nof the NVBench dataset, comprising 25,628 entries that have\nbeen collated from 152 distinct databases originating from the\nSpider dataset [32]. To facilitate fair comparison with other\nestablished baselines as discussed in Section V, we meticulously\nseparated the DV queries involving non-join operations from\nthose that include join operations and performed an in-depth\nstatistical analysis. Specifically, the dataset contains 15,764\nsamples without join operations. DV queries that employ non-\njoin operations, utilizing a single table, are showcased in\nFigure 4. Conversely, DV queries featuring join operations,\nwhere multiple tables are engaged, are illustrated in Figure 5.\n2) Chart2text.: The chart-to-text conversion process, as\nintroduced by [33], constitutes a comprehensive benchmark\nincorporating two distinct datasets, cumulatively consisting\nof 44,096 charts that span an extensive array of subjects\nand graphical representations. The data for this benchmark\noriginates from two primary sources: Statista\u00b3 and the Pew\nResearch Center4. The dataset derived from Statista includes\nvarious elements such as a screenshot of the chart image, the\naccompanying data table, the title, axis labels, and expertly\ncrafted descriptive narratives concerning the chart content.\nConversely, the datasets sourced from the Pew Research Center"}, {"title": "B. Data Pre-processing", "content": "To enhance the data quality and ensure compatibility\nwith downstream tasks, we instituted the following pre-\nprocessing. Initially, we excluded incomplete NL question\nsamples and those in Chinese from the NVBench dataset. Subse-\nquently, to prevent sequence truncation during the Bidirectional\nDual-Corpus objective-which operates with a fixed token\nlength-we retained only those entries in the Chart2Text dataset\nwhere the total number of cells (determined by multiplying\nthe number of rows by the number of columns) did not exceed\n150. This step was deemed unnecessary for the WikiTableText\ndataset, as it inherently possesses a maximum cell count of\n108, as delineated in Table II. After employing the filtration\nand encoding methods described in Sections III-B, III-C, and\nIII-D, we constructed our pretraining corpus based on the type\nof data. The corpus is bifurcated into two segments:\nDual-Corpus Objectives Datasets. This segment is arranged\naccording to the following mappings:\n\u2022 NL+ Schema \u2192 DV query\n\u2022 DV query + Schema \u2192 Description\n\u2022 Table Description\n\u2022 Question + DV query + Schema + Table \u2192 Answer\nAs shown in Figure 6, the aforementioned four data types are\nsequentially presented.\nMLM Objectives Datasets. This segment amalgamates NL\nquestions and database schemas from NVbench, DV queries,\nquestions and answers from FeVisQA, and tables with their\ndescriptions from Chart2Text and WikiTableText. These ele-\nments are integrated and then utilized to formulate the Masked\nLanguage Model (MLM) pretraining tasks. To illustrate this, a\nsample DV query from NVBench, which has been subjected\nto masking, is provided in Figure 6."}, {"title": "C. Data Partitioning", "content": "After preprocessing the data, we proceeded with the partition-\ning process. Originating from the Spider dataset [32], NVBench\nfeatures a wide range of domains, including academic, railway,\nand scholar, which is conducive to cross-domain evaluation.\nThe data from NVBench was divided into training, validation,\nand testing subsets, constituting 70%, 10%, and 20% of the\ndataset, respectively, to facilitate this cross-domain assessment.\nFurthermore, considering that FeVisQA utilizes databases from\nSpider, we maintained consistency with NVBench by applying\nthe same cross-domain partitioning scheme. The partitioning of\nthe data adheres to the original division as specified in Table II."}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "To comprehensively assess our pre-trained architecture\nand promote further study, we have assembled the Jointly\nUnderstanding Text and Data Visualization benchmark. This\nbenchmark encompasses four extensively studied tasks: text-\nto-vis (Section V-B), vis-to-text (Section V-C), FeVisQA\n(Section V-D), and table-to-text (Section V-E). We incorporate\nestablished datasets pertinent to these tasks. For each task,\nwe delineate the task definition, baselines, evaluation metrics,\ncorresponding results, and case studies."}, {"title": "A. Implementation Details", "content": "We conducted the pre-training of DataVisT5 over the course\nof five epochs using four NVIDIA 40GB A40 GPUs. And\nwe standardized the maximum sequence lengths for both the\ninput and output at 512 tokens. Our training regimen adopted\na linear warm-up schedule with a 0.1 warm-up rate and set\nthe learning rate to 5e-6. For optimization, we utilized the\nDeepSpeedCPUAdam optimizer with a weight decay of 0.01.\nFurther enhancing our training efficiency, we implemented\nDeepSpeed\u2019s ZeRO Stage 2 offloading strategy with mixed\nprecision (FP16) as described in [35]. To underscore the\neffectiveness of our pre-training approach, we limited the fine-\ntuning of our model to just two epochs."}, {"title": "B. Text-to-Vis", "content": "Defination. For a natural language query {q, S} consisting of\na question q that articulates a user\u2019s request for a visualization\nand S, the schema of the relevant database D, the goal of the\ntext-to-vis task is to generate the appropriate DV query \u0443.\nBaselines. We evaluate DataVisT5 against several established\nbaselines for the text-to-vis task. The Seq2Vis approach [15]\ninterprets the task as machine translation using a Seq2Seq\nmodel equipped with attention. The renowned Transformer\narchitecture [36] and the ncNet framework [8], which enhances\nthe Transformer with attention-forcing, serve as additional\nbaselines. RGVisNet [31] utilizes a two-stage process for\nDV query retrieval and prototype refinement. Furthermore,\nwe explore in-context learning by prompting GPT-4 [37] to\ngenerate DV queries from text inputs, using five example pairs\nsorted by similarity for context. Finally, our methodology is\nanchored by the CodeT5+ model [27], fine-tuning both 220M\nand 770M parameter variants, with larger variants not yielding\nfurther improvements.\nTask-specific Corpus. For the fine-tuning phase of our text-\nto-vis task, we engaged the NVBench dataset, which was\ndelineated in Section IV-A1, originally derived from our pre-\ntraining datasets. Contrasting with the pre-training phase, the\nfine-tuning was conducted with a singular training objective:\nNL + Schema \u2192 DV query.\nEvaluation Metrics. The performance evaluation of our\nexperiment adopts four metrics, analogous to those utilized\nin [15]. Before delving into the specifics, it is necessary to\nknow that each DV query comprises three key elements: the\ntype of visualization (such as bar chart), the configuration\nof axis (x/y/z), and data with transformation functions (e.g.\ngroup). Additionally, let N denote the total count of test\nsamples. The metrics are: (1) Exact Match (EM), which\nrequires a complete match between the predicted and reference\nDV queries (EM = $N_{equal}/N$), (2) Visualization EM (Vis\nEM), assessing the accuracy of predicted visualization types\n(Vis EM = $N_{vis}/N$), (3) Data EM, focused on data points\nwith transformation functions (Data EM = $N_{data}/N$), and\n(4) Axis EM, evaluating the congruence of axis components\n(Axis EM = $N_{axis}/N$).\nResults. Results from Table IV show that foundational models\nlike Seq2Vis and Transformer underperform in cross-domain"}, {"title": "C. Vis-to-Text", "content": "Definition. When provided with a DV query q and a database\nD that includes a schema S, the vis-to-text task is focused on\ncreating an intelligible textual description z that explains the\nDV query.\nBaselines. For our evaluation, we selected four established\nmodels alongside our initial checkpoint: (1) an enhanced\nSeq2Seq model, which incorporates an attention mechanism\nas described by [36] to improve its interaction between the\nencoder and decoder; (2) the vanilla Transformer model as\nintroduced in the context of text-to-vis tasks; (3) GPT-4 in\na zero-shot setting; (4) BART [38] (Bidirectional and Auto-\nRegressive Transformers), a transformer-based model which\ncombines bidirectional encoding and auto-regressive decoding.\nOn this task, we fine-tuned the base BART model (140M\nparameters) for a single-task setting; and (5) CodeT5+ [27],\nour initial checkpoint, which was also fine-tuned for a single-task application.\nTask-specific Corpus. The unidirectional training target for\nthe vis-to-text task was structured as DV query + Schema \u2192\nDescription. We employed the NVBench dataset, as referenced\nin Section IV-A1, analogous to the dataset used for the text-\nto-vis task. A notable distinction for the vis-to-text task lies\nin the inherent one-to-many relationship, where a singular DV\nquery may correspond to multiple descriptions. To establish a\ndefinitive corpus for subsequent fine-tuning and evaluation, we\nselected a single representative description from the available\nmultiples.\nEvaluation Metrics. To assess the quality of the generated\ntextual descriptions, we employed three metrics: BLEU [39],\nROUGE [40], and METEOR [41]. (1) BLEU measures the\nprecision of N-gram overlaps with reference texts, modified by\na brevity penalty. (2) In contrast, ROUGE emphasizes recall,\nassessing the extent of n-gram overlap. (3) METEOR surpasses\nBLEU in mimicking human judgement by considering exact\nmatches, stemming, synonyms, and penalizing for word order\ndifferences. Specifically, we report BLEU scores for unigram,\nbigram, and four-gram levels (BLEU-1, BLEU-2, BLEU-4),\nand ROUGE F1 scores for unigrams (ROUGE-1), bigrams\n(ROUGE-2), and longest common subsequences (ROUGE-L).\nResults. Detailed in Table VI, DataVisT5 in both 220M\nand 770M configurations significantly outperforms traditional\nSeq2Seq, Transformer, BART baselines, and zero-shot GPT-4.\nThe 220M DataVisT5 variant notably excels, achieving high\nBLEU-2, ROUGE-1, and ROUGE-2 scores, indicative of its\nproficiency in accurately generating bi-gram sequences. Scaling\nup to 770M further enhances its performance, leading in BLEU-\n1, BLEU-4, METEOR, and ROUGE-L metrics. Although multi-"}, {"title": "D. FeVisQA", "content": "Definition. The FeVisQA task is designed to formulate an\nanswer A to a DV-related question Q, by leveraging a database\nD that encompasses a schema S and tables T, all in service\nof elucidating DV concepts.\nBaselines. In addressing the FeVisQA task, we adopted the\nsame ensemble of baseline models previously applied to the\nvis-to-text task. This includes an attention-enhanced Seq2Seq\nmodel, the original Transformer model, the single-task fine-\ntuned versions of the base BART model and the CodeT5+\nmodel, and a zero-shot GPT-4.\nTask-specific Corpus. The FeVisQA task necessitated the\nformulation of a unidirectional training objective, structured as:\nQuestion + DV query + Schema + Table \u2192 Answer. We utilized\nthe FeVisQA dataset for this purpose, which is elaborated upon\nin Section IV-A4 and originates from pre-training datasets.\nResults. From Table VIII In the FeVisQA task, the fine-\ntuned DataVisT5 model with 770M parameters outperforms\ncompetitors across all metrics. Compared to CodeT5+ with\nan identical parameter setting of 770M, DataVisT5 exhibited\na significant 10.92% increase in the METEOR score post\nfine-tuning, underscoring its remarkable proficiency in answer-\ning free-form questions. This enhanced performance can be\nattributed to the integration of textual information and DV\nknowledge during the DataVisT5 pre-training phase, which\neffectively facilitates the model\u2019s understanding of the complex\ncross-domain relationship between text and DV.\nCase Study. Upon reviewing the outcomes documented in\nTable X, we observe that the Seq2Seq, Transformer, and\nBART models exhibit various discrepancies from the ground\ntruth. The Seq2Seq model consistently produces incorrect\nresponses, indicating significant misalignment. The Transformer\nmodel correctly identifies the smallest chart segment but lacks\nconsistency in other queries. BART correctly identifies the\nnumber of chart segments but often overestimates numerical\nvalues. While the Finetuned CodeT5+ model answers most\nquestions correctly, it inaccurately responds to \"What is the\ntotal number of count(film.type)?\u201d In contrast, our model is\nthe only one that consistently provides accurate answers across\nboth binary and numerical inquiries, demonstrating superior\nalignment with the ground truth."}, {"title": "E. Table-to-Text", "content": "Defination. With a table T as the input, the table-to-text task\nis concentrated on producing a clear, readable narrative z that\ncaptures and clarifies the essence of the data within T."}, {"title": "VI. RELATED WORK", "content": "Pre-trained models have been shown to be effective for\nlanguage representations and beneficial for downstream tasks\nby substantial work [20], [21], [42]-[47]. All the success has\nalso driven the development of machine language pretraining,\nwhich is in special format of text such as code and sql.\nCodeBert [48] is a bimodal pre-trained model for natural\nlanguage and programming language in a bert-like architecture,\nshowing that pretraining can improve the performance for code-\nrelated tasks. TaBert [49] and TAPAS [30] extend BERT to\nlearn a joint representation of NL text and database tables\nand demonstrate the effectiveness of semantic parsing tasks.\nGRAPPA [50] designs objectives and loss functions oriented\ntoward table semantic parsing to close the gap between\nstructured tabular data and unstructured natural language\nsentences, which outperforms semantic parsing tasks such as\ntext-to-SQL. Moreover, the development of domain-adapted\npre-trained models, such as CodeT5 [22] for code understanding\nand generation, MolT5 [23] for molecule captioning and\ngeneration, and BioT5 [51] which integrates cross-modal data\nin the biological domain with chemical structures and linguistic\ncontexts, highlights the importance of specialized training\nbeyond a generic T5 framework. These adaptations emphasize\nthe necessity of domain-specific fine-tuning to effectively\ncapture the contextual nuances inherent in specialized corpora."}, {"title": "B. DV-related Tasks", "content": "Benefiting from the convenience of visualization, various\nstudies related to DV, including text-to-vis, vis-to-text, free-\nform question answering over DV and table-to-text, have\nattracted considerable research interest within the community.\nThe initial text-to-vis systems were based on predefined rules\nor templates [52", "55": ".", "56": "conceptualizes visualization generation as a\nsequence translation task, employing an encoder-decoder neural\narchitecture. Similarly, RGVisNet [31"}]}