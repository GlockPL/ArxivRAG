{"title": "MultiMath: Bridging Visual and Mathematical Reasoning for Large Language Models", "authors": ["Shuai Peng", "Di Fu", "Liangcai Gao", "Xiuqin Zhong", "Hongguang Fu", "Zhi Tang"], "abstract": "The rapid development of large language models (LLMs) has spurred extensive research into their domain-specific capabilities, particularly mathematical reasoning. However, most open-source LLMs focus solely on mathematical reasoning, neglecting the integration with visual injection, despite the fact that many mathematical tasks rely on visual inputs such as geometric diagrams, charts, and function plots. To fill this gap, we introduce MultiMath-7B, a multimodal large language model that bridges the gap between math and vision. MultiMath-7B is trained through a four-stage process, focusing on vision-language alignment, visual and math instruction-tuning, and process-supervised reinforcement learning. We also construct a novel, diverse and comprehensive multimodal mathematical dataset, MultiMath-300K, which spans K-12 levels with image captions and step-wise solutions. MultiMath-7B achieves state-of-the-art (SOTA) performance among open-source models on existing multimodal mathematical benchmarks and also excels on text-only mathematical benchmarks. Our model and dataset are available at https://github.com/pengshuai-rin/MultiMath.", "sections": [{"title": "Introduction", "content": "The rapid development of large language models (LLMs) has ushered in significant advancements in various domains, with a focus on specialized capabilities, particularly mathematical reasoning. Many domain-specific language models have primarily concentrated on mathematical reasoning in isolation (Yu et al. 2024; Luo et al. 2023; Wang et al. 2024; Shao et al. 2024), while neglecting the integration with visual reasoning. Simultaneously, general-purpose open-source multimodal large language models (MLLMs) (Liu et al. 2023b; Zhu et al. 2024) often lack specificity in vertical domains, resulting in a subpar performance in mathematical reasoning tasks.\nCurrently, domain-specific MLLMs for mathematical reasoning can be categorized into two types. The first, represented by G-LLaVA (Gao et al. 2023) and AlphaGeometry (Trinh et al. 2024), focuses on geometric problem solving (GPS) (Seo et al. 2015; Sachan and Xing 2017; Lu et al. 2021; Peng et al. 2023) but falls short in other multimodal mathematical reasoning tasks, such as function plot reasoning and scientific chart QA (Lu et al. 2024). The second, represented by Math-LLaVA (Shi et al. 2024), builds upon an existing open-source MLLM with math finetuning. However, it underperforms in text-only mathematical reasoning tasks (Cobbe et al. 2021; Hendrycks et al. 2021; Wei et al. 2023) due to the lack of large-scale pretraining on math corpora and the absence of chain-of-thought (CoT) reasoning capabilities. Consequently, there remains a notable gap in the availability of an open-source MLLM that excels across a broad spectrum of mathematical reasoning tasks.\nTo bridge this gap, we introduce MultiMath-7B, a domain-specific multimodal large language model for mathematical reasoning. Unlike Math-LLaVA (Shi et al. 2024), which directly applies math finetuning to existing MLLMs, we choose to build upon a well-trained math LLM as our foundation. We then enhance it with visual capabilities and align its visual and mathematical reasoning. This strategy leverages the reasoning abilities acquired from mathematical pretraining and extends them to the visual domain. MultiMath-7B employs DeepSeekMathRL-7B (Shao et al. 2024) as the foundation language model, augmented with a vision encoder and a multimodal adapter to enable visual capabilities. We adopt a multi-stage training process, progressively training the model's visual alignment, visual dialogue, and visual reasoning abilities, ultimately bridging them with mathematical reasoning skills.\nAnother challenge in developing a math MLLM is the scarcity of multimodal alignment and instruction datasets in math domain. Existing open-source datasets typically focus on particular math subfields and lack visual-language alignment data and CoT-style instruction data. To address it, we construct MultiMath-300K, a multimodal, multilingual, multi-level and multistep mathematical reasoning dataset that encompasses a wide range of K-12 level mathematical problems. MultiMath-300K demonstrates three key strengths over existing multimodal math datasets Geo170K (Gao et al. 2023) and MathV360K (Shi et al. 2024): Novelty: the problems are not present in previously released datasets. Diversity: MultiMath-300K covers almost all K-12 grades, including a variety of math problem types such as arithmetic, algebra, geometry, function, algorithm, etc. Comprehensiveness: each problem is accompanied by an image caption for vision-language alignment training and a step-by-step solution for CoT instruction fine-tuning. The comparison of MultiMath-300K with Geo170K and MathV360K is shown in Table 1.\nExperimental results on mathematical reasoning tasks demonstrate that MultiMath-7B not only achieves SOTA performance among open-source models on multimodal mathematical benchmarks but also excels on text-only mathematical benchmarks. Notably, multimodal training has been shown to improve the model's performance on certain text-only mathematical reasoning tasks, suggesting that incorporating multimodal reasoning can enhance the language model's overall reasoning abilities.\nThe main contributions are summarized as follows:\n\u2022 We propose MultiMath-7B, a math MLLM that achieves SOTA performance among open-source models on multimodal mathematical benchmarks and excels in text-only mathematical reasoning tasks.\n\u2022 We constructed MultiMath-300K, a multimodal, multilingual, multi-level and multistep mathematical reasoning alignment and instruction dataset, covering a wide range of K-12 level mathematical problems.\n\u2022 We introduce a training framework for enhancing the multimodal capabilities of domain-specific models, preserving the original abilities while boosting multimodal performance."}, {"title": "Related Work", "content": "Multimodal Large Language Model\nRecent advancements in vision-language alignment and the maturation of large language model (LLM) have endowed LLMs with visual capabilities. Pioneering studies in vision-language alignment include CLIP (Radford et al. 2021) and BLIP (Li et al. 2022). CLIP aligns image and text semantic spaces through contrastive learning, while BLIP enhances visual-language understanding and generation by jointly training a vision encoder with a language model. Inspired by these models, researchers developed MLLMs such as Mini-GPT4 (Zhu et al. 2024) and LLaVA (Liu et al. 2023b), which leverage vision-language alignment training and instruction-tuning to enable LLMs to handle multimodal tasks. Recently, closed-source MLLMs like GPT-4V (OpenAI 2024), Gemini Pro (Gemini 2024), and Claude 3 (Anthropic 2024) have further pushed the boundaries of visual understanding capabilities. The typical training framework involves using pretrained vision encoders and language models, aligning them with visual caption data, and finally finetuning on instruction data for task-specific abilities.\nDespite these advancements, there is still a significant gap in the development of domain-specific MLLMs, particularly in mathematical reasoning. This gap is due to the lack of an effective training framework for adapting math LLMs to multimodalities and the scarcity of multimodal alignment and instruction reasoning data. In this paper, we aim to address these issues.\nMathematical Reasoning\nAutomated mathematical reasoning is a significant research area in artificial intelligence. It typically includes tasks such as mathematical word problems (MWP) (Wang et al. 2018), geometry problem solving (GPS) (Lu et al. 2021), and automatic theorem proving (ATP) (Chou, Gao, and Zhang 1996). The emergence of large language models (LLMs) has led to their dominance in numerous mathematical reasoning benchmarks, driven by their extensive pretraining and advanced comprehension and reasoning capabilities. Mathematical reasoning has increasingly garnered attention from researchers and has become an essential benchmark for assessing LLMs. Several specialized LLMs, such as MetaMath (Yu et al. 2024), Math-Shepherd (Wang et al. 2024), WizardMath (Luo et al. 2023), and DeepSeekMath (Shao et al. 2024), have been developed to address these tasks. Derived from general-purpose LLMs, these models are fine-tuned to strengthen their mathematical abilities. Open-source LLMs have shown strong performance on mathematical reasoning benchmarks, highlighting the potential of domain-specific models.\nA challenge of mathematical reasoning lies in reasoning with visual injection, including geometry diagrams, scientific charts, function plots, etc. However, existing math MLLMs are either limited to geometric problems, as seen with G-LLaVA (Gao et al. 2023), or underperform in text-only mathematical reasoning tasks and lack chain-of-thought capabilities, such as Math-LLaVA (Shi et al. 2024). To address these issues, we construct a novel, diverse, and comprehensive multimodal math reasoning dataset, including visual-language alignment data and step-by-step reasoning instructions. We use this dataset to train MultiMath-7B, filling the gap in open-source math MLLMs."}, {"title": "Dataset", "content": "In this section, we introduce the MultiMath-300K dataset, with a focus on the construction process.\nOverview\nMultiMath-300K comprises 298,670 mathematical problems, with 290,227 in the training set and 8,443 in the validation set. Each problem features an image and a statement in both English and Chinese. Covering all K-12 education levels, MultiMath-300K includes knowledge points such as arithmetic, algebra, mathematical concepts, plane geometry, solid geometry, function analysis, and algorithm derivation. Figure 2 illustrates these statistics in a pie chart.\nIn addition to the problem data, MultiMath-300K includes vision-language alignment data and step-by-step solution instructions. The alignment data details the image for vision-language alignment training. The instruction data provides step-by-step reasoning solutions, with each step featuring an ID, name, and content, culminating in a final answer marked in boxed. Figure 3 presents a data sample of the English part.\nDataset Construction\nHere we outline the dataset construction process, encompassing collection, annotation, and verification, as illustrated on the left side of Figure 4.\nSource Data To ensure novelty, we collect mathematical problems from K-12 textbooks, exercises, and exams with authorization from the data providers. Selection criteria include (1) Completeness, requiring each problem to include the question title, details, solution, and standard answer. (2) Multimodality, with each problem featuring exactly one image. (3) Clarity, excluding images that are too small or blurry. This process initially yields 390,000 raw problems. We then use GPT-4-1106-preview to translate the problem texts, resulting in bilingual descriptions in both Chinese and English.\nAlignment Data The alignment data serve two purposes: (1) facilitating vision-language alignment training for multimodal models and (2) enabling language models to address multimodal problems through text-only descriptions. We utilized GPT-4o-2024-05-13 to generate bilingual captions for images in both Chinese and English. To address GPT-4o's limitations in OCR accuracy, we employed Math-Pix \u00b9 to verify and correct OCR results for formula and textual images.\nInstruction Data Chain-of-thought (CoT) (Wei et al. 2022) reasoning has proven effective in enhancing LLM's mathematical reasoning abilities. To effectively utilize CoT reasoning, step-by-step instructional data is essential for model training, as it supports precise tracking of reasoning errors and enables fine-grained tuning. Therefore, our objective is to construct multistep reasoning data. We employed GPT-4o-2024-05-13 and GPT-4-1106-preview for annotation, conducting multiple rounds of refinement to ensure high-quality results, as follows:\nRound 1: Generate step-by-step reasoning chains using GPT-4o, with detailed solutions from the original data as the hint.\nRound 2: Evaluate GPT-4o's reasoning chains against the standard answers. If inconsistencies are found, require GPT-4o to revise the reasoning steps."}, {"title": "Model Training", "content": "In this section, we introduce the proposed mathematical multimodal large language model, MultiMath-7B. Compared to existing open-source mathematical MLLMs, our model offers three main advantages: (1) it tackles a broad spectrum of multimodal mathematical reasoning tasks, (2) it utilizes chain-of-thought (CoT) for detailed step-by-step reasoning, and (3) it maintains strong performance in text-only mathematical reasoning tasks. The appendix details the model training settings.\nModel Architecture\nMultiMath-7B is built upon the LLaVA architecture (Liu et al. 2023a) and integrates three primary components: a vision encoder, a multimodal adapter, and a language model. The vision encoder is initialized with openai/clip-vit-large-patch14-336, which supports a 336\u00d7336 image resolution to effectively capture and recognize small text and mathematical symbols. The multimodal adapter is a two-layer MLP, initialized randomly. The language model is based on DeepSeekMath-RL (Shao et al. 2024), a leading open-source 7B model in math reasoning.\nTraining Stage\nHere we detail the training process of MultiMath-7B, presenting a novel framework for enhancing the multimodal capabilities of domain-specific LLMs. The overview is depicted on the right side of Figure 4. The training is structured into four stages, each addressing distinct aspects: vision-language alignment, visual instruction-tuning, math instruction-tuning, and finally, math process-supervised reinforcement learning. This sequential approach enables the model to extend its mathematical reasoning ability to the visual domain.\nVision-Language Alignment In this stage, we focus on aligning the vision encoder and language model, enabling the latter to integrate visual information, which it has not previously processed. We train only the multimodal adapter while keeping the other modules freezed. Considering the potential lack of expertise of the initial vision encoder in mathematical content, we mix LLaVA-Pretrain (Liu et al. 2023a) dataset with domain-specific data from MultiMath300K alignment data and geo170k-align (Gao et al. 2023). The model is then trained for one epoch to align visual and language features within the mathematical domain.\nVision Instruction-tuning This stage aims to enhance the model's visual comprehension and question-answering abilities. Although the model can now interpret visual information after stage 1, it still struggles with various visual tasks. To address this, we train all model components for two epochs using the LLaVA-Instruction (Liu et al. 2023a) dataset, which focuses on improving visual comprehension and question-answering capabilities."}, {"title": "Model Output", "content": "Step 1 (Angle Bisector Property):\nP is on the bisector OC of MON\n$\\therefore \\angle AOP=\\angle BOP$\nReinforcement Learning\nWrong Step\nNew Prompt\nCorrected Steps\n0\nStep 3 (Similar Triangles):\n$\\frac{OP}{OB} = \\frac{OA}{OP}$\n: $$\\triangle PBO \\sim \\triangle AOP$$\nStep 4 (Similar Triangles' Property):\n$\\therefore \\angle OBP = \\angle OPA$\nStep 5 (Angle Bisector Property):\n$\\angle BOP = \\frac{1}{2} \\angle BOA = 25^{\\circ}$\nStep 6 (Angle Sum Theorem):\n$\\angle APB = \\angle OPB + \\angle OPA = \\angle OPB +$\n$\\angle OBP = 180^{\\circ} - \\angle BOP = 155^{\\circ}$\nAnswer: 155^{\\circ}$\nData Pair <-\nCorrect Step\nReward\nModel\nStep-wise\nReward\nAction\nPolicy\nModel"}, {"title": "Math Instruction-tuning", "content": "In this stage, we focus on extending mathematical reasoning capabilities to visual data, emphasizing chain-of-thought (CoT) reasoning. The CoT training is primarily driven by the MultiMath300K-instruction dataset. Additionally, we incorporate two open-source multimodal mathematical QA datasets, Geo170k-qa (Gao et al. 2023) and MathV360k (Shi et al. 2024), to further enhance the model's performance. This combined training, conducted over two epochs, refines all model components and results in the instruction model.\nProcess-supervised Reinforcement Learning This stage aims to correct errors at the step level during reasoning. Unlike supervised fine-tuning (SFT) in stage 2 and 3, reinforcement learning (RL) enhances the model's ability to identify and correct reasoning errors more effectively. We use MultiMath300K-val, GSM8K-train (Cobbe et al. 2021), MATH-train (Hendrycks et al. 2021), and CMATH-train (Wei et al. 2023) for PPO (Schulman et al. 2017) training. The RL training process, illustrated in Figure 5, is summarized as follows:\n1. Given a mathematical problem, the instruction model performs chain-of-thought (CoT) reasoning and generates a result consisting of multiple reasoning steps.\n2. Given the standard answer and the model output from the previous step, GPT-4o accesses the correctness of the response. If incorrect, it identifies the step where the error occurred and regenerates the correct solution from that step.\n3. The correct and incorrect answers from the previous step form a paired preference dataset, used to train a reward model initialized from the instruction model.\n4. The reward model assigns a reward score to each reasoning step (action) generated by the policy model, supervising the policy model's gradient descent.\nThis process results in the final RL-enhanced model. We will discuss the performance improvements from reinforcement learning in the Discussion section."}, {"title": "Experiment Results", "content": "This section evaluates the performance of MultiMath-7B across various mathematical reasoning benchmarks, including visual and textual math reasoning tasks.\nVisual Math Benchmarks\nDatasets and Baselines We select two representative multimodal mathematical reasoning datasets for evaluation: MathVista (Lu et al. 2024) and MathVerse (Zhang et al. 2024). MathVista assesses LLM' mathematical reasoning within visual contexts, while MathVerse presents more complex challenges in plane geometry, solid geometry, and functions. For evaluation, we utilize the provided prompts and perform zero-shot inference. Our baselines include closed-source MLLMs, open-source MLLMs, and two open-source MLLMS G-LLaVA (Gao et al. 2023) and Math-LLaVA (Shi et al. 2024).\nMain Results Table 2 presents the evaluation results of MathVista and MathVerse on the testmini dataset, including both closed-source and open-source MLLMs. MultiMath-7B sets a new state-of-the-art (SOTA) among open-source models for both benchmarks. Remarkably, despite having only 7B parameters, MultiMath-7B surpasses models with up to 34 billion parameters, demonstrating its exceptional performance in visual-mathematical reasoning tasks. Additionally, MultiMath-7B outperforms the closed-source Qwen-VL-Plus (Bai et al. 2023) on both datasets, with its MathVista performance comparable to GPT-4V.\nSubset Results We also report the results on the subsets of MathVista and MathVerse: MathVista is divided into Figure QA, Geometry Problem Solving, Math Word Problem, Textbook QA, and Visual QA. MathVerse is categorized into Text Dominant, Text Lite, Vision Intensive, Vision Dominant, and Vision Only. MultiMath-7B notably excels across most subsets, significantly outperforming other MLLMs in Geometry Problem Solving and Math Word Problem tasks. It also leads open-source MLLMs in most MathVerse subsets, with the exception of the Vision Only category."}, {"title": "Discussion", "content": "In this section, we explore the factors driving the model's performance, specifically, what contributes to the model's outcomes.\nVisual Enhancement or Reasoning Boost? The improvement of mathematical MLLM in multimodal math reasoning tasks compared to its foundation language models can be attributed to two main factors: (1) visual injection, which provides essential context for problem-solving;(2) finetuning on new math reasoning tasks, which boosts the model's reasoning ability in some aspects. To investigate this, we evaluate DeepSeekMathRL-7B on the text-only testmini set of MathVista (Table 4). Converting visual data into text allows the language model to solve multimodal math problems. With the same text-only inputs, MultiMath-7B achieved 9.1 points higher accuracy than DeepSeekMath-RL-7B, reflecting gains from reasoning boost alone. Inferencing with images further improves the performance by 4.3, indicating gains from visual injection. These findings suggest that while both factors contribute, reasoning boost plays a more substantial role. This supports our assertion that multimodal reasoning training can enhance reasoning abilities within a single modality.\nContribution of Dataset To assess the impact of the proposed MultiMath-300K dataset, we conducted ablation studies by excluding it from pretraining (Stage 1) and math instruction-tuning (Stage 3) and evaluate the models on six mathematical benchmarks (Figure 6). While MathV360K primarily boosted performance on MathVista, it significantly undermined the model's ability on textual math tasks. Incorporating MultiMath-300K during Stage 3 led to substantial improvements across nearly all benchmarks, highlighting its critical role in enhancing comprehensive mathematical reasoning. Additionally, Stage 1's math alignment training provided a modest performance gain.\nContribution of RL Figure 6 also depicts the ablation results of stage 4 math reinforcement Learning. RL improved the model's performance on GSM8K, MATH, CMATH, and MathVista, but led to a decline on MathVerse and Gaokao-MathCloze. This aligns with expectations, as the RL training primarily used in-domain data from GSM8K and MATH, leading to better results on those benchmarks while negatively affecting out-of-domain datasets. This study confirms the viability of step-wise RL for multimodal math training, and future work could explore RL on larger, more diverse datasets to mitigate out-of-domain performance drops.\nContribution of Foundation LM To assess how much of MultiMath-7B's performance attributed to its foundation model, DeepSeekMathRL, we retrained it using Vicuna-7B as a baseline. The results are shown in Figure 7. DeepSeek-Math outperforms Vicuna more significantly on textual benchmarks than visual benchmarks. This suggests the gains on visual tasks stem mainly from multimodal training rather than the language model itself. Additionally, compared to Table 3, Vicuna's improvements after MultiMath training support the hypothesis that multimodal reasoning training enhances overall mathematical reasoning abilities."}, {"title": "Conclusion", "content": "In this paper, we introduce MultiMath-7B, a multimodal math large language model that bridges the gap between visual and mathematical reasoning. We also construct a multimodal math dataset MultiMath-300K, which spans K-12 levels and includes image captions and step-wise solutions. MultiMath-7B achieves SOTA performance among open-source models on existing multimodal mathematical benchmarks and also excels on text-only mathematical reasoning datasets. Future work will focus on expanding the model's training with diverse datasets across multiple domains and modalities to overcome its current limitations."}, {"title": "Dataset Source and Privacy", "content": "The math problems in MultiMath-300K are sourced from Xuekubao\u00b2's K12 question bank, which is collected from math textbooks, exercises, and exam questions. We purchased usage rights for the question bank and obtained permission for research purposes. During the filtering process, we removed any questions involving students' privacy. We also used an n-gram strategy to compare the data with existing mathematical reasoning datasets and filtered out duplicate questions to ensure the novelty of the dataset."}, {"title": "Prompt", "content": "We use GPT-4o-2024-05-13 to annotate the image captions and problem solutions. We present the prompts used in the dataset construction, including prompts for the caption (Figure 8), solution (Figure 9), and verification (Figure 10). In these figures, the texts in blue are instructions, and in purple are the input question information. We use these prompts to generate Chinese and English captions and solutions using GPT-4o-2024-05-13 (caption and solution) and GPT-4-1106-preview (verification)."}, {"title": "Format", "content": "We include one thousand data examples of MultiMath-300K in data appendix to demonstrate the data format. The complete dataset has been released on Hugging Face."}, {"title": "Prompt for Caption", "content": "You are a math expert. You will be given an image extracted from a math problem. Follow the instructions carefully.\nThe question is [TITLE]. [IMAGE].\nIf the image contains only mathematical expressions, please only output its LaTeX. Your response should only contain its OCR result without other content. For example: $$ x^2 + y^2 = z^2 $$.\nOtherwise, execute the following command: Please describe the image in detail in both Chinese and English so that the graphic can be accurately drawn and used to solve a math problem based on your text description. Ensure that your description includes all necessary details, such as text, symbols, geometric markers, etc., if any.\nYour response should be in two paragraphs, the first starting with [ZH] for the Chinese description, and the second starting with [EN] for the English description."}, {"title": "Prompt for Solution", "content": "You are a math expert. You will be given an image extracted from a math problem. Follow the instructions carefully.\nPlease reason step by step, and put your final answer within \\boxed{}.Each step is placed on a new line, using the following format: Step X (Mathematical theorem/basis used): Detailed solution steps. Answer: \\boxed{}.\nUse the following example as the template.\nAssume there is an image here containing geometric shapes.\nProblem text:\nAs shown in the figure, given a right triangle ABC, \u2220C = 90\u00b0, AB = 5, and AC = 3. Find the length of BC.\nHint:\nUse the Pythagorean theorem to solve.\nThe model should output:\nStep 1 (\u52fe\u80a1\u5b9a\u7406):\u6839\u636e\u52fe\u80a1\u5b9a\u7406,AB^2 = AC^2 + BC^2\u3002\nStep 2(\u4ee3\u5165\u672a\u77e5\u6570): 5^2= 3^2 + BC^2\u3002\nStep 3 (\u5e73\u65b9\u8ba1\u7b97):25=9+BC^2\u3002\nStep 4 (\u79fb\u9879): BC^2 = 25-9\u3002\nStep 5 (\u8ba1\u7b97\u5dee\u503c): BC^2 = 16\u3002\nStep 6 (\u7b49\u5f0f\u4e24\u8fb9\u540c\u65f6\u5f00\u65b9): BC = \\sqrt{16}\u3002\nStep 7 (\u5f00\u65b9\u8ba1\u7b97): BC = 4\u3002\nAnswer: \\boxed{4}\nStep 1 (Pythagorean Theorem): According to the Pythagorean Theorem, AB^2 = AC^2 + BC^2.\nStep 2 (Substitute the unknowns): 5^2 = 3^2 + BC^2.\nStep 3 (Square calculation): 25 = 9 + BC^2.\nStep 4 (Transposition): BC^2 = 25 -9.\nStep 5 (Calculate the difference): BC^2 = 16.\nStep 6 (Taking the square root on both sides): BC = \\sqrt{16}.\nStep 7 (Square root calculation): BC = 4.\nAnswer: \\boxed{4}\n[TITLE]. [IMAGE].\n[HINT]\nProvide the solution in both Chinese and English. The Chinese solution should start with [ZH], and the English solution should start with [EN].\nThe standard answer is [ANS]. Please check your answer. If incorrect, output \\boxed{Incorrect} and provide the solution again according to the format requirements above. Otherwise, output \\boxed{Correct}."}, {"title": "Prompt for Verification", "content": "You are a math expert. Follow the instructions carefully.\nThe question is [TITLE]. Hint: [HINT]. The standard answer is [ANS]. Now there is an answer provided by student is [ANS_TBD].\nPlease check the answer according to the standard answer. If incorrect, output \\boxed{Incorrect}. If correct, output \\boxed{correct}."}, {"title": "Model Training", "content": "Here we detail the datasets and settings used on each training stage in Figure 11. All the experiments were conducted on 8 NVIDIA A100-80GB GPUs with the random seed 42. For more implementation details, please refer to our code appendix. The model weights have been released on Hugging Face.\nModel Inference\nWe inferred our model as well as other MLLMs with the settings of temperature: 0.2, top p: None, num_beams: 1, max_new_tokens: 1024. We evaluated three times on a task for each model and obtained the average score as the final accuracy."}]}