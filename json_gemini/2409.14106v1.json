{"title": "FineMolTex: Towards Fine-grained Molecular Graph-Text Pre-training", "authors": ["Yibo Li", "Yuan Fang", "Mengmei Zhang", "Chuan Shit"], "abstract": "Understanding molecular structure and related knowledge is crucial for scientific research. Recent studies integrate molecular graphs with their textual descriptions to enhance molecular representation learning. However, they focus on the whole molecular graph and neglect frequently occurring subgraphs, known as motifs, which are essential for determining molecular properties. Without such fine-grained knowledge, these models struggle to generalize to unseen molecules and tasks that require motif-level insights. To bridge this gap, we propose FineMolTex, a novel Fine-grained Molecular graph-Text pre-training framework to jointly learn coarse-grained molecule-level knowledge and fine-grained motif-level knowledge. Specifically, FineMolTex consists of two pre-training tasks: a contrastive alignment task for coarse-grained matching and a masked multi-modal modeling task for fine-grained matching. In particular, the latter predicts the labels of masked motifs and words, leveraging insights from each other, thereby enabling FineMolTex to understand the fine-grained matching between motifs and words. Finally, we conduct extensive experiments across three downstream tasks, achieving up to 230% improvement in the text-based molecule editing task. Additionally, our case studies reveal that FineMolTex successfully captures fine-grained knowledge, potentially offering valuable insights for drug discovery and catalyst design.", "sections": [{"title": "1 Introduction", "content": "Comprehending molecular structure and related knowledge is pivotal in scientific investigations spanning diverse fields, including chemistry, drug discovery, and materials science [11]. Recent advancements in artificial intelligence and machine learning have yielded promising outcomes for molecule-based tasks such as retrosynthesis [47] and drug discovery [11]. The majority of these studies [17, 7, 24, 38, 1] concentrate on the molecular structure, such as SMILES strings, molecular graphs, and geometric structures. They learn molecular representations under supervised signals such as toxicity level and drug activity. However, this type of supervised learning requires extensive and costly labeling of pre-defined categories, hindering its adoption for unseen categories and tasks.\nFortunately, compared to task-specific labeled data, textual descriptions of molecules are fairly abundant. These descriptions can be found in chemical database annotations, research papers in"}, {"title": "2 Related Work", "content": "We provide a brief review on molecular multi-modal learning. Prior works predominantly concentrate on modeling the chemical structures such as 1D SMILES [17], 2D molecular graphs [7, 24, 55], and 3D geometric structures [38, 1, 41]. They utilize supervised signals on a predetermined set, and thus cannot generalize to unseen categories without labeled examples. Recently, KV-PLM [53] bridges this gap by linking SMILES with biomedical texts through a unified language modeling framework. Nonetheless, 1D SMILES may omit certain structural details and fail to identify structural similarities among molecules due to its non-uniqueness. To address these limitations, MoleculeSTM [26] and MoMu [35] employ a contrastive learning approach to align the molecular graph with its corresponding text, thus performing well on unseen molecules and texts. However, these models are less effective on molecule-to-text generation tasks because language models are not yet well-versed in interpreting graphs as generative conditions. Therefore, MolCA [27] introduces a cross-modal projector to align the embedding space of the molecular graph with the language model's input space, enabling the comprehension of 2D graphs as generative conditions. This approach has also been extended to 3D graph structures, where 3D-MoLM [22] uses a cross-modal projector to synchronize the embedding space of the 3D geometric structure with that of the language model. Additionally, various efforts have been devoted to tackling specific molecular tasks based on textual data, including zero-shot instruction molecular learning [56], molecular reaction prediction [34], and molecular relational modeling [9].\nMore related works on graph-based molecular learning, as well as more general multi-modal learning, can be found in Appendix A."}, {"title": "3 The Proposed Approach", "content": "We propose FineMolTex, a novel fine-grained molecular graph-text framework, learning both molecule- and motif-level knowledge. The model architecture is outlined in Figure 2. This section first introduces the key components in the architecture, and then describes the two pre-training tasks."}, {"title": "3.1 Key Components of FineMolTex", "content": "To capture coarse- and fine-grained knowledge, we propose FineMolTex, consisting of five key components: 1) the tokenization component to decompose molecular graphs and texts into motif and word tokens; 2) a graph encoder to capture the structure of molecules and motifs; 3) a text encoder to extract the knowledge from texts and words, 4) a cross-attention layer to integrate information from different modalities; 5) a Transformer layer to generate embeddings for each token based on its contextual tokens from the same modality.\nTokenization. As shown in Figure 2, for fine-grained modeling, we fragment the molecular graphs and texts into motif tokens and word tokens. We employ the BRICS [6] algorithm to decompose molecular graphs into chemically significant motifs, followed by a post-processing procedure [55] to consolidate the motif vocabulary. We break the textual description into word tokens following SciBERT [3]. For coarse-grained modeling, the global tokens of molecule and text, <MOL> and <CLS>, are inserted at the beginning of the motif and word sequences, respectively, resulting in the sequences $m_0, m_1, ..., m_j$ and $t_0, t_1,..., t_p$, where J and D are the lengths of the sequences.\nGraph Encoder. We utilize GraphMVP [25], a pre-trained Graph Isomorphism Network (GIN), to obtain the embedding of each motif token. It employs multi-view pre-training to connect the 2D topologies and 3D geometries in the GEOM dataset [2] containing 250K conformations. Let $G = (V, E, X)$ denote a motif with N atoms, where $V = {V_1, V_2, . . ., v_v}$ is the atom set, $E \u2286 V \u00d7 V$ is the bond set, and $X = [X_1, X_2, ..., X_N] \u2208 R^{N\u00d7\u00a2}$ represents the atom feature matrix, where $x_i$ is"}, {"title": "$\\\\$", "content": "the feature vector of atom $v_i$ and ( is the dimension of atom features. The embedding of each atom in the l-th layer is formed as\n$gv = COMBINE^{(l)} (gv^{(l-1)}, AGGREGATE^{(l)} ({gu^{(l-1)} : u \u2208 N(v)})),$ (1)\nwhere $gv^{(l)}$ is the embedding of atom v at l-th layer, N(v) is the neighbors of atom v, AGGREGATE() aggregates the neighborhood information of atom v, and COMBINE(\u00b7) further combines the self-information with the neighborhood information. Then, we pool the atom embeddings at the final layer $l_g$ into a motif-level embedding, $g_g$, as follows.\n$g_g = READOUT({g_v^{(l_g)} v\u2208G})$, for $G\u2208 {m_1, m_2,...,m_j},$ (2)\nwhere READOUT(\u00b7) is permutation invariant, implemented as the average function in our model.\nText Encoder. We use SciBERT [3], which has been pre-trained on texts from the chemical and biological domains, as our text encoder, denoted as $f_{bert}$. It can encode a text sequence as:\n$b_{t_0}, b_{t_1},..., b_{t_D} = f_{bert} (t_0, t_1,...,t_D).$ (3)\nTransformer Layer. To capture the contextual information for each token, we use \u201cencoder-style\u201d Transformer layers [39], which consist of a multi-head self-attention layer and a fully connected feed-forward network. This enables the tokens to gather information from other tokens in the same modality. We utilize $f_{trm_y}$ and $f_{trm_m}$ for the text and molecule modality, respectively, as follows.\n$z_{t_0}, z_{t_1},..., z_{t_D} = f_{trm_y} (b_{t_0}, b_{t_1},..., b_{t_D}),$ (4)\n$z_{m_0}, z_{m_1},..., z_{m_J} = f_{trm_m} (g_{m_0}, g_{m_1},..., g_{m_j}).$ (5)\nCross-attention Layer. We integrate information from different modalities via cross-attention layers $f_{crsT}$ and $f_{crsM}$ for molecular graph and text, respectively. Consider the cross-attention layer $f_{crsM}$ for molecular graph: the queries are from the same modality, $Q_m = Z_mW_q$, while the keys and values are from the text modality, $K_t = Z_tW_k$ and $V_t = Z_tW_v$. Here $W_q,W_k, W_v$ are learnable weights, $Z_m = [Z_{m_0}, Z_{m_1},..., Z_{m_J}]$, and $Z_t = [Z_{t_0}, Z_{t_1},..., Z_{t_K}]$. Subsequently, the output of scaled dot-product attention is computed as:\n$Attention(Q_m, K_t, V_t) = softmax(\\frac{Q_mK_t^T}{\\sqrt{d_k}})V_t,$ (6)\nwhere $d_k$ is the dimension of queries and keys. The cross-attention layer for text is designed similarly. Hence, the encoding of each token accounts for tokens from the other modality, enabling the learning of fine-grained alignment at the motif level. The outputs of the cross-attention layer are:\n$h_{t_0}, h_{t_1},..., h_{t_p} = f_{crsT} (Z_{t_0}, Z_{t_1},..., Z_{t_p}),$ (7)\n$h_{m_0}, h_{m_1},..., h_{m_3} = f_{crsM} (Z_{m_0}, Z_{m_1},..., Z_{m_j}).$ (8)"}, {"title": "3.2 Pre-training Tasks", "content": "We propose two pre-training tasks, the contrastive alignment task for coarse-grained alignment, and the masked multi-modal modeling task for fine-grained alignment.\nContrastive Alignment. For coarse-grained alignment at the molecule level, we align the graph-text pairs from the same molecules and contrast the pairs from different molecules, which can be achieved by optimizing the following loss.\n$L_{con} = -\\frac{1}{2} E_{m_0,t_0} [log \\frac{exp(cos(Z_{m_0}, Z_{t_0})/\u03c4)}{exp(cos(Z_{m_0}, Z_{t_0})/\u03c4) + \\sum_{t_0'} exp(cos(Z_{m_0}, Z_{t_0'})/\u03c4)} + log \\frac{exp(cos(Z_{t_0}, Z_{m_0})/\u03c4)}{exp(cos(Z_{t_0}, Z_{m_0})/\u03c4) + \\sum_{m_0'} exp(cos(Z_{t_0}, Z_{m_0'})/\u03c4)}],$ (9)\nwhere $Z_{m_0}, Z_{m_0}, Z_{t_0}$, and $z_{t_0}$ denote the output embeddings from the Transformer layer, $t_0$ and $m_0$ are the negative instances sampled from the same batch of graph-text pairs, and $cos(\u00b7,\u00b7)/\u03c4$ is the cosine similarity scaled by the temperature hyperparameter \u03c4. In this way, we capture the molecule-level knowledge, aligning the embedding space of molecular graphs and texts holistically.\nMasked Multi-modal Modeling. For fine-grained alignment at the motif level, we mask some of the tokens and predict their labels. Based on the fragmented motifs of all molecules in the pre-training dataset, we construct a motif dictionary that includes motifs along with their labels and frequency. Then we randomly mask approximately 20% of the motif tokens that have neither too high nor too low a frequency in the motif dictionary, as well as 15% of the word tokens. The token embeddings of the motifs and words are updated utilizing $l_{trm_m}$ and $l_{trm_r}$ transformer layers, respectively. Subsequently, information from the two modalities is integrated via our cross-attention layer. This entire process is iterated for l times.\nBased on the output embeddings of fine-grained tokens from the cross-attention layer $h_{t_1}, ..., h_{t_p}$ and $h_{m_0}, h_{m_1},..., h_{m_j}$, we utilize two classifiers $p_m$ and $p_t$ to predict the labels of the masked motifs and words: $\u0177_{m_i} = p_m(h_{m_i}), \u0177_{t_j} = p_t(h_{t_j})$, where $\u0177_{m_i}$ is the predicted label of motif $m_i$, and $\u0177_{t_j}$ is the predicted label of word $t_j$. Given the ground truth labels $Y_{m_i}$ and $Y_{t_j}$, the model is trained by reconstructing the masked tokens as:\n$L_{pre} = \u03b2\\sum_i CE(Y_{m_i}, \u0177_{m_i}) + \u03b1 \\sum_j CE(Y_{t_j}, \u0177_{t_j}),$ (10)\nwhere \u03b1, \u03b2 are hyperparameters, and CE(,) is the cross-entropy loss. The key to achieving fine-grained alignment lies in the cross-attention layer, which enables the model to predict the labels of masked tokens based on tokens from the other modality. For instance, as illustrated in Figure 2, predicting the label of $SO_3$ solely based on the unmasked motif tokens is challenging. However, by leveraging the embeddings of word tokens, particularly \u201cpropanesulfonic\u201d which includes the $SO_3$ group, we can gain relevant information about the masked token. Consequently, the model implicitly learns fine-grained alignment knowledge, thereby augmenting its motif-level knowledge.\nOverall Loss. FinMolTex is optimized by the overall loss L $L_{con} + L_{pre}$. Thus, FineMolTex is able to jointly learn the molecule- and motif-level knowledge."}, {"title": "4 Experiments", "content": "In this section, we conduct extensive experiments to demonstrate the effectiveness of FineMolTex. Before evaluating, we first conduct the two pre-training tasks on the PubChemSTM dataset [26], which includes 281K graph-text pairs from PubChem [16]. Each molecular graph is paired with a textual description that elaborates on its chemical and physical properties or highlights its high-level bioactivities. Details of the pre-training data and process can be found in Appendix C.1.1 and C.4.\nThe goal of our experiments is to answer the following research questions (RQs).\nRQ1. Can FineMolTex better generalize to unseen molecules?\nRQ2. Can FineMolTex bridge the gap to tasks centered on motif-level knowledge?\nRQ3. Can FineMolTex perform better on single-modality tasks?"}, {"title": "4.1 Generalization to Unseen Molecules (RQ1)", "content": "To answer RQ1, we conduct a zero-shot graph-text retrieval task to examine the generalizability of FineMolTex on unseen molecules and texts. Given a molecular graph and T candidate textual descriptions, the goal is to identify the textual description that best aligns with the molecular graph. Conversely, given a textual description and T candidate molecular graphs, identify the molecular graph that best matches the text. This task can be addressed by calculating the similarity of the molecular graphs and texts in the joint embedding space, thus allowing zero-shot inference.\nDatasets and Baselines. We utilize DrugBank-Pharmacodynamics, molecule-ATC, and DrugBank-Description [26] extracted from the DrugBank database [44] for evaluation. These datasets include molecular graphs and their chemical descriptions. Details of the datasets can be found in Appendix C.1.2. We compare with five multimodal molecular models: KV-PLM [53], MolCA [27], MoMu-S [35], MoMu-K [35], and MoleculeSTM [26]. Specifically, KV-PLM uses SMILES to represent the structure of the molecule, while others use graph structures.\nResults. We report the results on the first two datasets in Tables 1 and 2, and defer those on DrugBank-Description to Appendix D.1 due to space limit. We make the following observations. 1) Across different values of T, FineMolTex consistently outperforms the baselines that neglect motif-level knowledge. The superior performance demonstrates that fine-grained motif-level knowledge facilitates generalization to unseen molecules, which likely contain seen motifs. 2) FineMolTex maintains strong performance in both directions (given graph, and given text). The symmetry further indicates that the embedding spaces of both modalities are well-aligned and similarly well-learned. 3) We observe that KV-PLM, which utilizes SMILES to capture molecular structures, is less effective than other models employing graphs, consistent with previous findings [26] that 2D graph structure is more expressive than 1D SMILES."}, {"title": "4.2 Application to Motif-Centered Tasks (RQ2)", "content": "To answer RQ2, we employ a zero-shot text-based molecule editing task, which is highly relevant to practical applications including catalyst design and targeted drug discovery. Specifically, we utilize FineMolTex to collaborate with a molecule generation module, following the design in [26], to modify a specified molecule according to a text prompt. Hence, motif-level knowledge is essential for this task, as the model needs to replace certain motifs with others that are related to specific properties and names as indicated in the text prompt. We defer the technical details to Appendix B. We randomly sample 200 molecules from ZINC [14], and select 12 text prompts, including 8 prompts pertaining to physical properties [26], and 4 based on the names of the motifs. We utilize MoleculeSTM, MoMu, and MolCA as baselines."}, {"title": "4.3 Application to Single-Modality Task (RQ3)", "content": "While FineMolTex can simultaneously utilize pre-trained knowledge from both graphs and texts, we also verify its effectiveness on single-modality tasks, namely, molecular property prediction tasks. We use MoleculeNet [46] as the dataset, which only provides molecular graphs as input without texts. More specifically, there are eight binary classification tasks, and we report ROC-AUC for evaluation. More detailed dataset descriptions are provided in Appendix C.1.3.\nBaselines. We compare FineMolTex against nine baselines, including 1) five pre-trained GNN models: AttrMasking [12], ContextPred [12], InfoGraph [36], MolCLR [43], and GraphMVP [25];"}, {"title": "4.4 Analysis of Learned Fine-grained Knowledge (RQ4)", "content": "We evaluate whether FineMolTex captures fine-grained alignment information in the joint embedding space, and assess if it can predict the labels of masked motifs based on fine-grained knowledge.\nVisualization of Motif and Word Embeddings. To evaluate whether FineMolTex captures fine-grained alignment knowledge, we select motif and word tokens from 1,500 graph-text pairs in the PubChemSTM dataset, excluding meaningless words such as \u201cthis\u201d and \u201ca\u201d. In total, we visualize 3,469 motif tokens and 6,914 text tokens with t-SNE [29] in Figure 5a, where triangles denote text tokens, and circles denote motif tokens, with different colors indicating various labels. To examine the details of the tokens, we zoom into several regions in the figure, retaining only the colors and legends of the tokens we are interested in. For brevity, we utilize SMILES to represent the motif structures. We observe that text and motif tokens corresponding to each other are also close in the embedding space. For instance, in the orange frame, the word \u201csulf\u201d is close to the motif tokens"}, {"title": "4.5 Ablation Study for Masking and Cross-Attention Layers (RQ5)", "content": "To thoroughly explore the impact of the key components in FineMolTex, we compare to several variants, including FineMolTex w/o mmm, which drops the masked multimodal modeling task altogether; FineMolTex w/o mask word, which does not mask word tokens; FineMolTex w/o mask motif, which does not mask motif tokens; FineMolTex w/o cross, which excludes cross-attention layers. We evaluate these variants on the graph-text retrieval task used in RQ1, on two datasets with T = 4. As reported in Table 4, our model consistently surpasses the other variants. Specifically, without the masked multimodal modeling task, \u201cFineMolTex w/o mmm\" fails to capture fine-grained knowledge, resulting in the poorest performance. While \u201cFineMolTex w/o mask word\" and \"FineMolTex w/o mask motif\" perform better than \"FineMolTex w/o mmm,\" they are less effective than FineMolTex, which masks both words and motifs for mutual alignment. Lastly, without the cross-attention layers, \u201cFineMolTex w/o cross\u201d cannot integrate token embeddings from different modalities, thereby hampering its ability to effectively learn fine-grained knowledge. These observations demonstrate the effectiveness of each component."}, {"title": "5 Conclusions, Limitations and Broader Impact", "content": "In this paper, we reveal that fine-grained motif-level knowledge is crucial for molecular representation learning. We propose FineMolTex to jointly learn both coarse- and fine-grained knowledge through a contrastive alignment task and a masked multimodal learning task, respectively. By masking the fine-grained tokens and predicting their labels using tokens from the other modality, we can effectively learn fine-grained alignment between motifs and words. Experimental results on three downstream tasks and two case studies demonstrate the effectiveness of FineMolTex.\nLimitations. Despite the promising results of FineMolTex, the molecular graph-text dataset is small in scale, hindering the cross-modal understanding. To overcome the limitation, we plan to seek more"}]}