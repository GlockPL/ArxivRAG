{"title": "Goal-Conditioned Supervised Learning for Multi-Objective Recommendation", "authors": ["Shijun Li", "Hilaf Hasson", "Jing Hu", "Joydeep Ghosh"], "abstract": "Multi-objective learning endeavors to concurrently optimize multiple objectives using a single model, aiming to achieve high and balanced performance across these diverse objectives. However, it often involves a more complex optimization problem, particularly when navigating potential conflicts between objectives, leading to solutions with higher memory requirements and computational complexity. This paper introduces a Multi-Objective Goal-Conditioned Supervised Learning (MOGCSL) framework for automatically learning to achieve multiple objectives from offline sequential data. MOGCSL extends the conventional Goal-Conditioned Supervised Learning (GCSL) method to multi-objective scenarios by redefining goals from one-dimensional scalars to multi-dimensional vectors. The need for complex architectures and optimization constraints can be naturally eliminated. MOGCSL benefits from filtering out uninformative or noisy instances that do not achieve desirable long-term rewards. It also incorporates a novel goal-choosing algorithm to model and select \"high\" achievable goals for inference. While MOGCSL is quite general, we focus on its application to the next action prediction problem in commercial-grade recommender systems. In this context, any viable solution needs to be reasonably scalable and also be robust to large amounts of noisy data that is characteristic of this application space. We show that MOGCSL performs admirably on both counts. Specifically, extensive experiments conducted on real-world recommendation datasets validate its efficacy and efficiency. Also, analysis and experiments are included to explain its strength in discounting the noisier portions of training data in recommender systems.", "sections": [{"title": "1 Introduction", "content": "Multi-objective learning techniques typically aim to train a single model to determine a policy for multiple objectives, which are often defined on different types of tasks (Ruder 2017; Sener and Koltun 2018; Zhang and Yang 2021). For instance, two common tasks in recommender systems are to recommend items that users may click or purchase, and hence the corresponding two objectives are defined as pursuing higher click rate and higher purchase rate. However, learning for multiple objectives simultaneously is often non-trivial, particularly when there are potential inherent conflicts among these objectives (Sener and Koltun 2018).\nExisting approaches to multi-objective learning broadly address this optimization issue by formulating and optimizing a loss function that takes into account multiple objectives in a supervised learning paradigm. Some previous research focused on designing model architectures specifically for multi-objective scenarios, including the shared-bottom structure (Ma et al. 2018), the experts and task gating networks (Ma et al. 2018), and so on. Another line of research studies how to constrain the optimization process based on various assumptions regarding how to assign reasonable loss weights (Liu, Johns, and Davison 2019) or adjust gradients dynamically (Yu et al. 2020). Lin et al. (2020) proposes an optimization algorithm within the context of Pareto efficiency. We remark that these approaches often introduce substantial space and computational complexity (Ma et al. 2018; Zhang and Yang 2021). Also, they treat all the data uniformly (m-estimation). In recommender systems, sometimes none of the items shown to a user are of interest, rendering their choice uninformative. Also users are often distracted or their interests temporarily change. These are some of the reasons that the observed interaction data in real settings suffer from substantial uninformative or \"noisy\" components that are better left discounted. Existing approaches do not cater well to this aspect of our focus application.\nTo address these issues, we propose a novel method called Multi-Objective Goal-Conditioned Supervised Learning (MOGCSL). In this framework, we first apply goal-conditioned supervised learning (GCSL) (Yang et al. 2022; Liu, Zhu, and Zhang 2022) to the multi-objective recommendation problem by introducing a new multi-dimensional goal. At its core, GCSL aims to primarily learn from the behaviors of those sessions where the long-term reward ends up being high, thereby discounting noisy user choices coming with low long-term rewards. Unlike conventional GCSL, however, we represent the reward gained from the environment with a vector instead of a scalar. Each dimension of this vector indicates the reward for a certain objective. The \"goal\" in MOGCSL can then be defined as a vector of desirable cumulative rewards on all of the objectives given the current state. By incorporating these goals as input, MOGCSL learns to rely only on high-fidelity portions of the data with less noise, which helps to better predict users' real preference. Extensive experiments on two public datasets indicate that MOGCSL significantly outperforms other baselines and benefits from lower complexity.\nFor inference of GCSL, most previous works employ a simple goal-choosing strategy (Chen et al. 2021; Xin et al. 2023) (e.g., some multiple of the average goal in training). Although we observe that simple choices for MOGCSL do reasonably well in practice, we also introduce a novel goal-choosing algorithm that estimates the distribution of achievable goals using variational auto-encoders, and automates the selection of desirable \"high\" goals as input for inference. By comparing the proposed method with simpler statistical strategies, we gain valuable insights into the goal-choosing process and trade-offs for practical implementation.\nOur key contributions can be summarized as follows:\n\u2022 We introduce a general supervised framework called MOGCSL for multi-objective recommendations that, by design, selectively leverages training data with desirable long-term rewards. We implement this approach using a transformer encoder optimized with a standard cross-entropy loss, avoiding more complex architectures or optimization constraints that are typical for multi-objective learning. Empirical experiments on real-world e-commerce datasets demonstrate the superior efficacy and efficiency of MOGCSL.\n\u2022 We comprehensively analyze the working mechanism of MOGCSL, revealing its capability to remove harmful effects of potentially noisy instances in the training data.\n\u2022 As a part of MOGCSL, we introduce a goal-choosing algorithm that can model the distribution of achievable goals over interaction sequences and choose desirable \"high\" goals for inference."}, {"title": "2 Related Works", "content": "Multi-Objective Learning. Multi-objective learning typically investigates the construction and optimization of models that can simultaneously achieve multiple objectives. Existing research primarily focuses on resolving the problem by model architecture designs (Ma et al. 2018; Misra et al. 2016) and optimization constraints (Liu, Johns, and Davison 2019; Yu et al. 2020; Lin et al. 2020). All of these works give equal weight to all instances in the training data, instead of forcefully distinguishing noisy data from non-noisy data by considering their different effects on multiple objectives. This is fine in many applications but problematic in commercial recommendation systems. Moreover, these approaches often introduce substantial space and computational complexity (Zhang and Yang 2021), making them more challenging for large-scale applications in the real world. For example, MMOE (Ma et al. 2018) requires constructing separate towers for each objective, significantly increasing the model size. DWA (Liu, Johns, and Davison 2019) necessitates recording and calculating loss change dynamics for each training epoch, while PE (Lin et al. 2020) demands substantial computational resources to solve an optimization problem for Pareto efficiency.\nGoal-Conditioned Supervised Learning (GCSL). To deal with these challenges, we propose resolving the multi-objective optimization dilemma within the framework of GCSL. GCSL approaches have attracted much attention lately in the reinforcement learning community due to their ability to benefit from the simplicity and efficiency of supervised learning and effective use of existing \"trails\". The definition of goals in GCSL can be quite general, such as desirable states (Chane-Sane, Schmid, and Laptev 2021) or instruction sentences (Chan et al. 2019). We focus on a specific approach where the goal is defined as the cumulative reward gained along the trajectory (Liu, Zhu, and Zhang 2022; Chen et al. 2021). In existing literature, a family of methods can be categorized into this line of research, including Return-Conditioned Supervised Learning (Brandfonbrener et al. 2022), Decision Transformer (Chen et al. 2021), and RL via Supervised Learning (Emmons et al. 2021).\nTypically, GCSL can be directly combined with general sequential models with minor adaptations and trained entirely on offline data. It effectively transforms offline reinforcement learning into a supervised learning problem, optimized with the target to autoregressively maximize the likelihood of offline trajectories in the training data. However, as far as we know, most existing works focus on optimizing a single objective. Our work extends GCSL to the multi-objective setting, eliminating the need for scalarization functions or other constraints during training. Additionally, although some works have explored how to assign more valuable goals to enhance GCSL training (Ajay et al. 2020), the properties and determination of inference goals for GCSL remain less explored. We propose a novel algorithm to model the achievable goals and automatically choose desirable goals as input during inference stage. Note that we are solving for next action prediction problem, and use long-term rewards only as extra information, unlike multi-objective reinforcement learning approaches that aim to maximize expected returns across multiple objectives (Cai et al. 2022; Stamenkovic et al. 2022). Also, their evaluation principles are different, typically relying on long-term metrics and synthetic environments. Hence we don't compare with such approaches in this paper."}, {"title": "3 METHODOLOGY", "content": "In this section, we first illustrate the general optimization paradigm of MOGCSL. Then we expound on the training process of MOGCSL and the proposed goal-choosing algorithm for inference. Furthermore, we give a detailed analysis of the capability of MOGCSL to discount potentially highly noisy samples in the training data."}, {"title": "3.1 A New View of Multi-Objective Learning", "content": "Multi-objective learning is typically formulated as an optimization problem over multiple losses (Ma et al. 2018; Misra et al. 2016; Yu et al. 2020; Lin et al. 2020), each defined on a distinct objective. Consider a dataset $D = \\{(x_i, y_i^1, y_i^2, \\ldots, y_i^n)\\}_{i \\in [1,M]}$, where $x_i$ represents the feature, $y_i^j$ is the ground-truth score on the j-th objective, and M is the total number of data points. For a given model $f(x; \\theta)$, multiple empirical losses can be computed, one per objective as $L_j(\\theta) = E_{(x,y_i) \\in D}[L(f(x; \\theta), y_i^j)]$. The model can then be optimized by minimizing a single loss, which is obtained by combining all the losses through a weighted summation as: $\\min_\\theta \\sum_{j=1}^n w_j^i L_j(\\theta)$.\nA fundamental question is how to assign these weights, and how to regulate the learning process to do well on all the objectives concurrently. Earlier research sought to address this issue based on assumptions regarding the efficacy of certain model architectures or optimization constraints, which may not be valid or reasonable and can significantly increase complexity (Zhang and Yang 2021).\nIn contrast, we propose to approach the learning and optimization for multi-objective learning from a different perspective. Specifically, we posit that the interaction process between the agent and the environment can be formalized as an Multi-Objective Markov Decision Process (MOMDP) (Roijers et al. 2013). Denote the interaction trajectories collected by an existing agent as $D = \\{T_i\\}_{i \\in [1,M]}$. In the context of recommender systems, each trajectory T records a complete recommendation session between a user entering and exiting the recommender system, such that $\\tau = \\{(s_t, a_t, r_t)\\}_{t \\in [1,|\\tau|]}$. A state $s_t \\in S$ is taken as the representation of user's preferences at a given timestep t. An action $a_t$ is recommended from the action space A which includes all candidate items, such that $|A| = |V| = N$ where V denotes the set of all items. $R(s_t, a_t)$ is the reward function, where $r_t = R(s_t,a_t)$ means the agent receives a reward $r_t$ after taking an action $a_t$ under state $s_t$. Note that the reward function R(st, at) in MOMDP is represented by a multi-dimensional vector instead of a scalar.\nIn this context, all the objectives can be quantified using reward $r_t$ at each time step. Specifically, since $r_t$ is determined by user's behavior in response to recommended items, it naturally reflects the recommender's performance on these objectives. For example, if the user clicks the recommended item $a_t$, the value on the corresponding dimension of $r_t$ can be set to 1; otherwise, it remains 0. In sequential recommendation scenarios, the target of the agent is to pursue better performance at the session level. Session-level performance can be evaluated by the cumulative reward from the current timestep to the end of the trajectory:\n$g_t = \\sum_{t'=t}^{T} r_{t'}$, (1)\nwhere $g_t$ can be called as a \u201cgoal\u201d in the literature of goal-conditioned supervised learning (Yang et al. 2022).\nThen, the target of mutli-objective learning for recommender systems can be formulated as developing a policy that achieves satisfactory performance across multiple objectives in recommendation sessions. In this research, we address this problem within the framework of GCSL. During the training stage, the aim is to determine the optimal action to take from a given current state in order to achieve the specified goal. The agent, denoted as $\\pi_\\theta$, is trained by maximizing the likelihood of trajectories in the offline training dataset $D_{tr}$ through an autoregressive approach, expressed as $\\arg \\max_\\theta E_{D_{tr}}[\\log \\pi_\\theta(a|s, g)]$. Notably, there are no predefined constraints or assumptions governing the learning process. During the inference stage, when a desirable goal is specified, the model is expected to select an action based on the goal and the current state, with the aim of inducing behaviours to achieve that goal."}, {"title": "3.2 MOGCSL Training", "content": "The initial step of MOGCSL training is relabeling the training data by substituting the rewards with goals. Specifically, for each trajectory $\\tau \\in D_{tr}$, we replace every tuple $(s_t, a_t,r_t)$ with $(s_t, a_t,g_t)$, where $g_t$ is defined according to Eq. (1). Subsequently, we employ a sequential model (Kang and McAuley 2018) based on Transformer-encoder (denoted as T-enc) to encode the users' sequential behaviors and obtain state representations. We chose a transformer-based encoder due to its widely demonstrated capability in sequential recommendation scenarios (Kang and McAuley 2018; Li et al. 2023). However, other encoders, such as GRU or CNN, can also be used. Specifically, let the interaction history of a user up to time t be denoted as $o_{1:t-1} = \\{v_1, ..., v_{t-1}\\}$. We first map each item $v \\in \\nu$ into the embedding space, resulting in the embedding representation of the history: $e_{1:t-1} = [e_1, ..., e_{t-1}]$. Then we encode $e_{1:t-1}$ by T-enc. Since the current timestep t is also valuable for estimating user's sequential behavior, we incorporate it via a timestep embedding denoted as $emb_t$ through a straightforward embedding table lookup operation. Similarly, we derive the embedding of the goal $emb_{g_t}$ through a simple fully connected (FC) layer. The final representation of state st is derived by concatenating the sequential encoding, timestep embedding and goal embedding together:\n$emb_{s_t} = Concat(T-enc(e_{1:t-1}), emb_t, emb_{g_t})$. (2)\nTo better capture the mutual information, we further feed the state embedding into a self-attention block:\n$M_{s_t.g_t} = Atten(emb_{s_t}).$ (3)\nThen we use an MLP to map the derived embedding into the action space, where each logit represents the preference of taking a specific action (i.e., recommending an item):\n$[\\pi_\\theta(v^1|s_t, g_t), ..., \\pi_\\theta(v^N|s_t, g_t)] = \\delta(MLP(M_{s_t,g_t})),$ (4)"}, {"title": "Algorithm 1: Training of MOGCSL", "content": "Input: training data $D_{tr}$, batch size $B$, model parameters $\\theta$\nIntialization: initialize parameters $\\theta$\nRelabel all the rewards with goals according to Eq. (1)\nrepeat\nRandomly sample a batch of $(s_t, a_t, g_t)$ from $D_{tr}$\nCompute the representation $M_{s_t,g_t}$ via Eq. (2)-Eq.(3)\nDerive the prediction logits via Eq. (4)\nCalculate the loss function $L(\\theta)$ via Eq. (5)\nUpdate $\\theta$ by minimizing $L(\\theta)$ with stochastic gradient descent: $\\theta \\leftarrow \\theta - \\eta \\nabla L(\\theta)$\nuntil convergence\nwhere $v^i$ denotes the i-th item in the candidate pool, $\\delta$ is the soft-max function, and $\\theta$ denotes all parameters of this model. The training objective is to correctly predict the subsequent action that is mostly likely lead to a specific goal given the current state. As discussed in Section 3.1, each trajectory of user's interaction history represents a successful demonstration of reaching the goal that it actually achieved. As a result, the model can be naturally optimized by minimizing the expected cross-entropy as:\n$L(\\theta) = E_{(s_t,a_t,g_t)\\in D_{tr}}[-\\log(\\pi_\\theta(a_t|s_t,g_t))].$ (5)\nThe training process is illustrated in Algorithm 1."}, {"title": "3.3 MOGCSL Inference", "content": "After training, we derive a model $\\pi_\\theta(a|s, g)$ that predicts the next action based on the given state and goal. However, while the goal can be accurately computed through each trajectory in the training data via Eq. (1), we must assign a desirable goal as input for every new state encountered during inference. GCSL approaches typically determine this goal-choosing strategy based on simple statistics calculated from the training data. E.g., Chen et al. (2021) and Zheng, Zhang, and Grover (2022) set the goals for all states at inference as the product of the maximal cumulative reward in training data and a fixed factor serving as a hyperparameter. Similarly, Xin et al. (2023) derive the goals for inference at a given timestep by scaling the mean of the cumulative reward in training data at the same timestep with a pre-defined factor. However, a central yet unexplored question is: what are the general characteristics of the goals and how can we determine them for inference in a principled manner?\nIn this paper, we investigate the distribution of the goals which can be actually achieved during inference by first stating the following theorem. Proof is given in Appendix A.\nTheorem 1. Assume that the environment is modeled as an MOMDP. Consider a trajectory $\\tau$ that is generated by the policy $\\pi(a|s, g)$ given the initial state $s_1$ and goal $g_1$, the distribution of goals $g^a$ (i.e., cumulative rewards) that the agent actually achieves throughout the trajectory is determined by $(s_1, g_1, \\pi)$.\nBased on this theorem, we'd like to learn the distribution of $g^a$ conditioned on $(s_1,g_1,\\pi)$, denoted as $P(g^a| s_1, g_1, \\pi)$. This distribution can be generally learnt by generative models such as GANs (Mirza and Osindero 2014) and diffusion models (Ho and Salimans 2022). In this paper, we propose the use of a conditional variational auto-encoder (CVAE) (Sohn, Lee, and Yan 2015) due to its simplicity, robustness, and ease of formulation.\nThis distribution can be learned directly on the training data $D_{tr}$. Specifically, for each $(s,g) \\in D_{tr}$, g should be a sample from the distribution of the achievable goals by the policy $\\pi$, given the initial state s and input goal g. That's because the policy is trained to imitate the actions demonstrated by each data point in Dtr, where the achieved goal of the trajectory starting from (s, g) is exactly g. Let $c = (s, g, \\pi)$. The loss function is:\n$L_{CVAE1} =E_{(s,g)\\in D_{tr},z~Q_1}[\\log P_1(g|z, c)+\nD_{KL}(Q_1(z|g, c)||P(z))],$ (6)\nwhere $Q_1(z|g, c)$ is the encoder and $P_1(g|z, c)$ is the decoder. Based on Gaussian distribution assumption, they can be written as $Q_1 = N(\\mu(g, c), \\Sigma(g, c))$ and $P_1 = N(f_{CVAE1}(z, c), \\sigma^2I)$ respectively, where $z \\sim N(0,I)$. Then we can derive a sample of $g^a$ by inputting a sampled z into $f_{CVAE1}$.\nOn the inference stage, given a new state s', we first sample a set of goals g' as the possible input of $\\pi$ through a learnable prior $q(g'|s')$. Similarly, we learn this prior via another CVAE on the training data. The loss is:\n$L_{CVAE2} =E_{(s,g)\\in D_{tr},z~Q_2}[\\log P_2(g|z, s)+\nD_{KL}(Q_2(z|g, s)||P(z))].$ (7)\nFinally, we'll choose a desirable goal as the input along with the new state s' encountered in inference by sampling from the two CVAE models. Specifically, we propose to: (1) sample from the prior q(g'|s') to get a set of potential input goals, denoted as $G'$, (2) for each $g' \\in G'$, estimate the expectation of the actually achievable goal $\\bar{g^a}$ by sampling from $P(\\cdot|s', g', \\pi)$ and taking the average, (3) choose a best goal as input for inference from $G'$ according to the associated expected $\\bar{g^a}$ by a predefined utility principle $U(\\bar{g^a})$, which generally tends to pick up a \"high\" goal."}, {"title": "Algorithm 2: Inference of MOGCSL", "content": "Input: state s', sample size K, policy model $\\pi$, utility principle U, prior $q(g'|s')$, distribution of achievable goals $P(g^a|s', g', \\pi)$\nIntialization: set of potential input goals $G' = \\emptyset$, set of expected achievable goals $G^a = \\emptyset$\nfor k = 1,..., K do\nSample a $g_k'$ from $q(g'|s')$\nCompute the expectation of the achievable goal through sampling: $\\bar{g_k^a} = E_{g \\sim P(\\cdot|s',g'_k,\\pi)}g_k$\n$G' = G' \\cup g_k'$\n$G^a = G^a \\cup \\bar{g_k^a}$\nChoose the best $\\bar{g}$ from $G^a$ according to $U(\\bar{g^a})$\nChoose corresponding $g'$ from $G'$\nReturn: $\\pi(\\cdot|s', g')$"}, {"title": "3.4 Analysis of Denoising Capability", "content": "An important benefit of MOGCSL is its capability to remove harmful effects of potentially noisy instances in the training data. To illustrate this, we consider the following setup that is common in recommender systems. There is a recommender system that has been operational, and recording data. At each interaction, the system shows the user a short list of items. The user then chooses one of these items. In the counterfactual that the recommender system is ideal, the action recorded would be a which reveals the user's true interest. Since the actual recommender system to collect the data is not ideal, we have no direct access to a, but rather to a noisy version of it $\\epsilon(a)$. We also record a long-term and multidimensional goal (e.g., the cumulative reward) $g = (g_1,..., g_n)$ at each interaction (known only at the end of the session, but recorded retroactively). Thus our training data is a sample from a distribution D of tuples (s,g, $\\epsilon(a)$).\nWe assume that the noisy portion of the training data originates from users who are presented with a list of items that are not suitable for them, rendering their reactiongs to these recommendations uninformative. Conversely, interactions achieving higher goals are generally less noisy, meaning $\\epsilon(a)$ is closer to a. To illustrate this, consider a scenario where a user clicks two recommended items ($v_1$ and $v_2$) under the same state. After clicking $v_1$, the user chooses to quit the system, while he stays longer and browses more items after clicking $v_2$. This indicates that the goal (i.e., cumulative reward) with $v_2$ is larger than that with $v_1$. In this case, we argue that $v_2$ should be considered as the user's truly preferred item over $v_1$. That's because the act of quitting, which results in a smaller goal, indicates user dissatisfaction with the current recommendation, although he did click $v_2$ at the current step. Our proposed MOGCSL can model and leverage this mechanism based on multi-dimensional goals, which serve as a description of the future effects of current actions on multiple objectives. In other words, MOGCSL tends to prioritize instances that result in high achievable goals on multiple objectives, while discounting those with low goals. Specifically, by incorporating multi-dimensional goals as input, MOGCSL can effectively differentiate between noisy and noiseless samples in the training data. During inference, when high goals are specified as input, the model can make predictions based primarily on the patterns learned from the corresponding noiseless interaction data.\nTo empirically show its effect, we conduct experiments that are illustrated in Appendix B.1 due to limited space. The results demonstrate the denoising capability of MOGCSL."}, {"title": "4 Experiments", "content": "In this section, we introduce our experiments on two e-commerce datasets, aiming to address the following research questions: 1) RQ1. How does MOGCSL perform when compared to previous methods for multi-objective learning in recommender systems? 2) RQ2. How does MOGCSL mitigate the complexity challenges, including space and time complexity, as well as the intricacies of parameter tuning encountered in prior research? 3) RQ3. How does the goal-generation module for inference perform when compared to strategies based on simple statistics?"}, {"title": "4.1 Experimental Setup", "content": "Datasets We conduct experiments on two publicly available datasets: RetailRocket and Challenge15. Both of them include binary labels indicating whether a user clicked or purchased the currently recommended item.\nBaselines As introduced before, prior research on multi-objective learning encompass both model structure adaptation and optimization constraints. In our experiments, we consider two representative model architectures: Shared-Bottom and MMOE (Ma et al. 2018). For works focusing on studying optimization constraints, We compare three methods: Fixed-Weights (Wang et al. 2016) directly assigns fixed weights for different objectives based on grid search; DWA (Liu, Johns, and Davison 2019) aims to dynamically assign weights by considering the dynamics of loss change; PE (Lin et al. 2020) is designed for generating Pareto-efficient recommendations across multiple objectives.\nFollowing previous research(Yu et al. 2020), we consider all these optimization methods for each model architecture, resulting in six baselines denoted as Share-Fix, Share-DWA, Share-PE, MMOE-Fix, MMOE-DWA, MMOE-PE. Additionally, we introduce a variant of a recent work called PRL (Xin et al. 2023), which firstly applied GCSL to recommender systems. Specifically, similar to classic multi-objective methods, we compute the weighted summation of rewards from all objectives at each timestep. Then the goal is derived by the overall cumulative reward as a scalar following conventional GCSL. We call this variant as MOPRL. Since we formulate our problem as an MOMDP, we also incorporate a baseline called SQN (Xin et al. 2020) that applies offline reinforcement learning for sequential recommendation. Similarly, the aggregate reward is derived by the weighted summation of all objective rewards.\nTo ensure a fair comparison, we employ the T-enc and self-attention block introduced in Section 3.2 as the base module to encode sequential data for all compared baselines.\nEvaluation Metrics We employ two widely recognized information retrieval metrics to evaluate model performance in top-k recommendation: hit ratio (HR@k) and normalized discounted cumulative gain (NDCG@k)."}, {"title": "4.2 Performance Comparison (RQ1)", "content": "We begin by conducting experiments to compare the performance of MOGCSL and selected baselines. The experimental results are presented in Table 1. It's worth mentioning that a straightforward strategy based on training set statistics is employed to determine the inference goals in PRL (Xin et al. 2023). Specifically, at each timestep in inference, the goal are set as the average cumulative reward from offline data at the same timestep, multiplied by a hyperparameter factor A that is tuned using the validation set. To ensure a fair and meaningful comparison, we adopt the same strategy here for determining inference goals in MOGCSL. The comparison between different goal-choosing strategies is discussed in Section 4.4.\nOn RetailRocket, MOGCSL significantly outperforms previous multi-objective benchmarks in terms of purchase-related metrics. Regarding click metrics, MOGCSL achieves the best performance on HR, while Share-PE slightly outperforms it on NDCG. However, the performance gap between Share-PE and MOGCSL for purchase-related metrics ranges from 17% to 20%, whereas Share-PE only marginally outperforms MOGCSL on NDCG for purchase by less than 1%. Additionally, we observe that the more complex architecture design, MMOE, performs worse than the simpler Shared-Bottom structure. Surprisingly, a naive optimization strategy based on fixed loss weights can outperform more advanced methods like DWA across several metrics (e.g., Share-Fix vs Share-DWA). These findings highlight the limitations of previous approaches that rely on assumptions about model architectures or optimization constraints, which may not be necessarily true under certain environments. Similar trends are observed on Challenge15. While MMOE-PE performs slightly better on click metrics by 1-2%, MOGCSL achieves a substantial performance improvement on the more important purchase metrics by 11-20%.\nApart from previous benchmarks for multi-objective learning, MOGCSL also exhibits significant and consistent performance improvements on both datasets compared to offline RL based SQN and the variant MOPRL created on standard GCSL. Especially, at each timestep, the overall reward of them is calculated as the weighted sum of rewards across all objectives. In our experiments, it's defined as $r' = w_c r_c + w_p r_p$, where $r_c$ and $r_p$ are click and purchase reward and $w_c + w_p = 1$. Then the goal in MOPRL is derived by calculating the cumulative rewards as a scalar. In contrast, MOGCSL takes the goal as a vector, allowing the disentanglement of rewards for different objectives along different dimensions. Notably, no additional summation weights or other constraints are required. We have conducted experiments to compare the performance of MOGCSL to MOPRLs with different weight combinations. Figure 2 shows the performance comparison on the RetailRocket dataset. Results on Challenge15 exhibit similar trends. It's clear that MOGCSL consistently outperforms MOPRL across all weight combinations on both click and purchase metrics, demonstrating that representing the goal as a multi-dimensional vector enhances the effectiveness of GCSL on multi-objective learning."}, {"title": "4.3 Complexity Comparison (RQ2)", "content": "Apart from the performance improvement, MOGCSL also benefits from seamless integration with classic sequential models and optimization by standard cross-entropy loss, adding minimal additional complexity. During the training stage, the only extra complexity arises from relabeling one-step rewards with goals and including them as input to the sequential model. In contrast, previous multi-objective learning methods often introduce significantly excess time and space complexity (Zhang and Yang 2021). For instance, MMOE and Shared-Bottom both design separate towers for each objective, resulting in dramatic increase in model parameters. SQN needs an additional RL head for optimization on TD error. DWA requires recording and calculating loss change dynamics for each training epoch, while PE involves computing the inverse of a large parameter matrix to solve an optimization problem under KKT conditions. Additionally, tuning the combination of weights for multiple objectives using grid search for Fix-Weight, SQN, and MO-PRL is time-consuming. Approximately $O(m^n)$ repetitive experiments are needed to find the nearly optimal weight combination, where m denotes the size of the search space per dimension and n represents the number of objectives.\nTable 2 summarizes the complexity comparison. It's evident that MOGCSL significantly benefits from a smaller model size and faster training speed, while concurrently achieving great recommendation performance."}, {"title": "4.4 Goal-Choosing Strategy Comparison (RQ3)", "content": "As introduced in Section 3.3", "high\" goals but could be flexible with specific business requirements. In our experiments, we select the best goal $\\bar{g}$ from the set $G^a$ based on the following rule": "n$\\bar{g_i} = g \\in G^a, s.t. \\nexists g' \\in G^a \\backslash g, g_i \\geq g'_i \\forall i \\in [1, d"}]}