{"title": "Speaking the Right Language: The Impact of Expertise Alignment in User-AI Interactions", "authors": ["Shramay Palta", "Nirupama Chandrasekaran", "Rachel Rudinger", "Scott Counts"], "abstract": "Using a sample of 25,000 Bing Copilot conversations, we study how the agent responds to users of varying levels of domain expertise and the resulting impact on user experience along multiple dimensions. Our findings show that across a variety of topical domains, the agent largely responds at proficient or expert levels of expertise (77% of conversations) which correlates with positive user experience regardless of the user's level of expertise. Misalignment, such that the agent responds at a level of expertise below that of the user, has a negative impact on overall user experience, with the impact more profound for more complex tasks. We also show that users engage more, as measured by the number of words in the conversation, when the agent responds at a level of expertise commensurate with that of the user. Our findings underscore the importance of alignment between user and AI when designing human-centered AI systems, to ensure satisfactory and productive interactions.", "sections": [{"title": "1 Introduction", "content": "We have seen significant advancements in LLM development, which has enhanced the capabilities of model-based agents like ChatGPT that allow them to excel on tasks ranging from quick information retrieval to more creative or technical pursuits such as drafting essays, writing code, and designing artworks. These capabilities are not only attested to by their performance on standardized benchmarks but are also reflected in their use across a diverse set of real-world domains (Suri et al., 2024). Recently developed LLMS are able to assist humans across a variety of fields like teaching (Wang et al., 2024; Alsafari et al., 2024) and the clinical domain (Han et al., 2024), and have led to an increase in user productivity (Peng et al., 2023; Cambon et al., 2023).\nHowever, while millions of people utilize these models for a variety of tasks, their expectations, backgrounds, and interactions with these tools can differ significantly. One key aspect where users might differ is their domain expertise in the conversation topic with the agent LLM. Not all end users share the same level of domain knowledge and thus may have different preferences and abilities to process the information that the model would return to them. A \u201cbeginner\u201d user might want simple, general purpose information on a topic and could possibly be overwhelmed if presented with too \"high level\" information. On the other hand, a domain \"expert\" might have an unsatisfactory experience with the LLM if not given a deeper and detailed response.\nThus, we ask: What is the ideal expertise level of the LLM, and what are the consequences of any misalignment between the user and the LLM on domain expertise?\nTo answer this, we develop an ordinal 5-point scale-based expertise classifier (shown in Figure 1) that we apply to a corpus of over 25,000 Bing Copilot conversations sampled across a variety of domains. We generate three measures of expertise for each conversation. First and second, we classify the level of expertise of the user and the LLM respectively in the topic of the conversation."}, {"title": "2 Methodology", "content": "Consider a conversation C\u1d62 from a corpus of conversations C consisting of t interaction turns of user-agent utterances C\u1d62 = [U\u2081, A\u2081, ..., U\u209c, A\u209c]. We take a random sample of such conversations from Bing Copilot from the month of June 2024 with at least t > 2 interaction turns for both the user and the agent, yielding a set of 677, 801 conversations. Using the preprocessing steps discussed in Appendix A.1, we get a final set of 25, 033 conversations. We present the distribution of the conversations across different domains in Table 1."}, {"title": "2.1 Expertise Labels", "content": "We expect that users of varying expertise levels interact with Bing Copilot, and thus in order to determine the alignment between the user and the LLM expertise, we compute three different types of expertise labels as follows:\nUser Expertise: Expertise of the user in the conversation domain based on the User only side (U\u209c) of a conversation C\u1d62.\nGauged User Expertise: Predicted expertise of the user in the conversation domain, based on the LLM (A\u209c) only side of a conversation C\u1d62.\nAgent Expertise: Expertise of the agent in the conversation domain, based on the LLM (A\u209c) only side of a conversation C\u1d62.\nWe prompt GPT-4-Turbo (Achiam et al., 2023) to compute these three types of expertise labels for each conversation using a 5-point ordinal scale as \"Novice\", \"Beginner\u201d, \u201cIntermediate\u201d, \u201cProficient\", and \"Expert\". We present the definitions of each of these labels along with the human-validated system prompts in Figure 9, Figure 10 and Figure 11. We also provide details on the human validation of the predicted expertise labels in Appendix A.5."}, {"title": "2.2 Metrics for User Experience", "content": "We use the following three metrics to understand the impact of expertise (mis)alignment on user experience:\n\u2022 SAT Score: Lin et al. (2024) introduced SPUR (Supervised Prompting for User satisfaction Rubrics), an iterative prompting framework using supervision from labeled examples to estimate the user satisfaction score from a multi-turn conversation with an LLM agent. We adopt this framework and define the overall satisfaction score, denoted as SAT, as the difference between the satisfaction and dissatisfaction scores. The SAT score ranges from -100 to 100. The SAT score rubric is human-validated, the details of which are mentioned in Lin et al. (2024).\n\u2022 Task Complexity: Suri et al. (2024) introduced a task complexity classification method based on Anderson and Krathwohl's Taxonomy of learning domains (Armstrong, 2010) which categorizes the task complexity into six levels from lowest complexity to highest: Remember, Understand, Apply, Analyze, Evaluate, and Create. For simplicity, we group Remember and Understand as Low Complexity and Apply, Analyze, Evaluate and Create as High Complexity tasks. The Task Complexity metric is human-validated, the details of which are mentioned in Suri et al. (2024).\n\u2022 Conversation Length: As all our conversations are multi-turn, we look at the number of words across all user turns as a proxy for the user engagement level."}, {"title": "3 User and LLM Expertise", "content": "Using the labels described in \u00a7 2.1, we compute the user expertise, the agent expertise, and the gauged user expertise on our set of 25, 033 conversations and present the distributions of these expertise labels across different domains in Figure 2a, Figure 2b and Figure 4 respectively.\nOverall, we observe that a majority of the users (63.9%) are labeled as \u201cNovice\u201d on our ordinal scale, with a small number of users being classified as \"Proficient\" (5.2%) or \"Expert\" (1.6%). These small numbers of \u201cProficient\u201d and \u201cExpert\u201d users occur in the more technical domains (like Programming and scripting) as compared to the non-technical domains (like Entertainment).\nWe also see that, in a majority of the cases, the LLM gets labeled as \"Proficient\" (34.9%) or \"Expert\" (42.4%). Once again, we observe the number of \"Proficient\u201d and \u201cExpert\" labels to be fewer for non-technical domains as compared to the technical domains. Overall, Figure 5 shows that the user has a lower expertise than the LLM in a majority (80.1%) of the conversations. This makes sense in that most users are likely to be non-experts, while the model should have higher expertise in order to provide value to the user.\nFinally, for the gauged user expertise (Figure 4), we observe that a majority of the users are labeled as \u201cIntermediate\u201d(37.2%) or above. Comparing this finding with the User Expertise label distribution (Figure 5) indicates that there are cases of user overestimation, in which user expertise as gauged by the response from the LLM is higher than when directly assessing user expertise (57.4% of conversations), and user underestimation, in which gauged expertise is lower than the user expertise (4.23% of conversations). We also test our expertise classifier on a sample of WildChat (Zhao et al., 2024) conversations in Appendix A.4, where we observe a similar distribution of labels, hence demonstrating the generalisability of our expertise classifier."}, {"title": "4 Impact on User Experience", "content": "To understand how expertise (mis)alignment impacts the user experience, we do the following:\nFirst, we fit a piecewise regression of the difference between user and LLM expertise on user satisfaction. Figure 3a shows that when the user expertise exceeds the LLM expertise, there is a negative impact (R\u00b2 = 0.11,p = 1.36E-141) on the SAT score, with the overall SAT score becoming negative in absolute terms once the user is one level more expert than the LLM. Increasing gaps in user expertise over LLM expertise are associated with continually lower user satisfaction scores. Figure 6 also highlights that it is preferable for the LLM to be quite expert regardless of the user's level of expertise, with lower than \u201cProficient\u201d responses from the LLM generally leading to decreased user SAT scores. Further, underestimating the user's expertise level (Figure 3b), as measured by gauged user expertise, has a clear negative impact (R\u00b2 = 0.03,p = 1.06E-26) on the SAT score, also highlighted in Figure 6.\nThe nature of the user's task may impact the effect of expertise (mis)alignment, and we operationalize one aspect of this with the task complexity measure. We fit linear regressions of the gap in user to LLM expertise on user satisfaction separately for conversations with low versus high complexity tasks, with high and low task complexity defined as in \u00a7 2.2. As seen in Figure 8a, for low complexity tasks, the user-LLM expertise gap had essentially no effect on user satisfaction (r = -0.04), while for high complexity tasks, this effect was of medium effect size (r = -0.29). The effect of user expertise underestimation as reflected in the gap between actual and gauged user expertise (Figure 8b) shows a similar if somewhat moderated trend with no effect (r = -0.02) for tasks of low complexity and a small effect for tasks of high complexity (r = -0.15) .\nNotably, we model the user, agent and the gauged user expertise from conversational data to show the impacts of expertise mis-alignment on the user interaction experience.\nFinally, turning to the impact of user-LLM (mis)alignment on user engagement, Figure 7 shows that the amount of engagement, as defined in \u00a7 2.2, increases along with the level of user expertise. That is, more expert users tend to have longer conversations. This effect appears to depend on the level of expertise of the LLM, however, such that users at each level of expertise tend to engage more when the LLM responds at a similar level of expertise. \"Proficient\u201d and \u201cExpert\u201d users tend to engage relatively more with a Proficient or Expert LLM, while \u201cNovice\u201d and \u201cBeginner\u201d users tend to engage more with a Novice or Beginner LLM. Thus while the user satisfaction measure suggests that users prefer a more expert LLM, they engage more with LLMS of commensurate expertise."}, {"title": "5 Related Works", "content": "The term \"expertise\" has been defined along multiple dimensions (Bourne Jr et al., 2014; Garrett et al., 2009), such as the \u201cextent and organization of knowledge and special reasoning processes to development and intelligence\u201d (Feltovich and Hoffman, 1997). Traditionally, expertise has always been correlated with knowledge, skill and other cognitive concepts (Bourne Jr et al., 2014). Building upon this idea, Desmarais et al. (1995) introduced a probabilistic approach to model user expertise, and Ferrod et al. (2021) did so in a dialogue based setting. Different methods like heuristic rules (Vaubel and Gettys, 1990) have also shown promise in inferring expertise for word processing tasks.\nAdditionally, many works have explored how LLMS are mis-aligned with humans across an axis of different dimensions like moral judgements (Hendrycks et al., 2023; Jiang et al., 2022), cultural and societal norms (Palta and Rudinger, 2023; Acquaye et al., 2024; Naous et al., 2024; Bhatia and Shwartz, 2023; Huang and Yang, 2023), healthcare (Levy et al., 2024) and notions of plausibility (Palta et al., 2024). Similarly, we show that LLMS are misaligned with humans along notions of expertise, which can lead to unsatisfactory user experiences."}, {"title": "6 Conclusion", "content": "We examined the alignment between LLMs and users along a dimension relevant to the user experience: expertise. We show that the LLM's expertise is largely proficient or expert, which correlated with positive user satisfaction and exceeded user expertise in a majority of the cases. Further, underestimating the user's level of expertise correlated with lower and even negative user satisfaction, with the effect stronger for more complex tasks. Users tended to engage more, however, when the LLM responded at a level of expertise similar to their own, suggesting that the system strike a balance between generally high expertise which is liked by all users and matched expertise to best engage users. Future work may explore intervention strategies to strike this balance and mitigate obvious cases of user underestimation in real time."}, {"title": "7 Limitations", "content": "Our analysis of user and LLM expertise misalignment and its downstream impacts is based on predicted expertise labels and predicted user satisfaction scores (SAT). While we human validated the classification labels, there is still a possibility of some errors which could impact the results. We study conversations only in English, which may limit the generalizability our findings, given that a lot of conversations with Copilot take place in non-English languages. Our analysis is limited to English conversations to facilitate human-validation of predicted expertise labels. Future work may consider extending our findings in the multi-lingual domain. We use the same prompts to predict user and agent expertise on all the topical domains of our conversations. While more personalized templates might be able to capture the notions of expertise more accurately for different domains, we use the same template throughout to be able to make a fair comparison across all our experimental settings. Additionally, we only use GPT-4 to predict the expertise labels. While it is possible that other LLMS might be able to judge the expertise better, we restrict ourselves to the same model as the SAT rubric and the task complexity classifier to maintain model consistency. Future work may consider extending our expertise classifier to evaluate the alignment between humans and multiple different LLMS. Finally, all our results are correlational, but do indicate that a mismatch in expertise between the user and LLMS could be one of the causes of user dissatisfaction. We hope our findings motivate future works to involve experiments where agent expertise can be manipulated to determine whether it has a causal impact on user satisfaction or not."}, {"title": "A Appendix", "content": "The requirement of two or more turns helped ensure we had sufficient signal to assess the level of expertise of both user and LLM. Additionally, the SAT score classifier (Lin et al., 2024) is also based on multi-turn conversations. We then use the langdetect\u2074 library (Nakatani, 2010) to determine the text language at each interaction turn to further filter to conversations where the majority language detected overall was either English, or in case of a tie, English was one of the tied languages. The selection of English only conversations was done to ensure human judges could read a random sub-sample of conversations in order to human-validate our classifications of user and LLM expertise.\nUsing the domain classification methodology introduced in Suri et al. (2024), we further sampled the conversations at random from a set of topical domains to generate a final set of 25033 fully anonymized conversations. All personal, private, or sensitive information was scrubbed and masked before the conversations were used for this research. The access to the dataset is strictly limited to the authors who conducted hands-on analysis. This domain filtering step was performed to remove conversations in domains minimally or not at all relevant to the concept of expertise (e.g., travel).\nEthics: As part of the production process, the Bing Copilot data is anonymized, and each conversation is formed by aggregating turns based on a unique conversation ID. Thus, none of the researchers who analyzed the data are able to recover and identify the conversations from any individual user. In addition, this research study was reviewed"}]}