{"title": "FROM TOKENS TO WORDS: ON THE INNER LEXICON OF LLMS", "authors": ["Guy Kaplan", "Matanel Oren", "Yuval Reif", "Roy Schwartz"], "abstract": "Natural language is composed of words, but modern LLMs process sub-words as input. A natural question raised by this discrepancy is whether LLMs encode words internally, and if so how. We present evidence that LLMs engage in an intrinsic detokenization process, where sub-word sequences are combined into coherent word representations. Our experiments show that this process takes place primarily within the early and middle layers of the model. They also show that it is robust to non-morphemic splits, typos and perhaps importantly to out-of-vocabulary words: when feeding the inner representation of such words to the model as input vectors, it can \"understand\" them despite never seeing them during training. Our findings suggest that LLMs maintain a latent vocabulary beyond the tokenizer's scope. These insights provide a practical, finetuning-free application for expanding the vocabulary of pre-trained models. By enabling the addition of new vocabulary words, we reduce input length and inference iterations, which reduces both space and model latency, with little to no loss in model accuracy.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) rely heavily on tokenization methods such as byte-pair encoding (BPE; Sennrich et al., 2016). Such methods often split words into multiple tokens, potentially disrupting their morphological structure (Batsuren et al., 2024).\\u00b9 Typos and other perturbations can also lead to large variations in the tokens that represent a word (Kaushal & Mahowald, 2022). Nonetheless, LLMs exhibit a remarkable ability to recover word meaning (Cao et al., 2023). This raises important questions about how models internally compose meaningful word representations from tokens, a process referred to as detokenization (Elhage et al., 2022; Gurnee et al., 2023).\nIn this work, we seek to understand the detokenization mechanism in LLMs. We consider two cases: words that are not part of the model BPE vocabulary, and are thus split into multiple sub-words; and single-token, in-vocabulary words that we artificially split into multiple tokens. In both cases, our experiments indicate a word-level detokenization process in LLMs, which occurs mainly in the early to middle layers. Our results hint that language models hold a latent vocabulary or inner lexicon, which they access to identify words from token sequences.\\u00b2\nWe begin by examining if internal representations of token sequences reflect whether a sequence of tokens comprises a word (Section 3). We probe the model's hidden representations (Conneau et al., 2018) of both multi-token real English words and gibberish nonwords (Frisch et al., 2000). We observe that the representations of words and nonwords substantially diverge in middle layers-a simple k-nearest neighbors classifier achieves a 89% accuracy on the task of discriminating between both groups. Overall, these results suggest that models hold a concept of recognized words.\nWe next explore the mechanism through which models reconstruct cohesive word representations from sub-word tokens (Section 4). To do so, we use techniques that interpret a token's hidden states and decode them into natural language (Belrose et al., 2023; Ghandeharioun et al., 2024). We find that for both multi-token and single-token words, in most cases, the last token can be decoded as the full word after being processed by 3-5 layers, with some words requiring up to 15 layers."}, {"title": "2 RELATED WORK", "content": "Tokenization Sub-word tokenization algorithms (Wu, 2016; Kudo, 2018) are the standard for pre- processing text in modern LLMs. The most widely used method is byte pair encoding (BPE; Sennrich et al., 2016), which keeps frequent words intact and splits rare words into multiple sub-words. Recent studies propose ways to improve tokenization to ease word reconstruction (Provilkov et al., 2020; Hofmann et al., 2022; Yehezkel & Pinter, 2023; Bauwens & Delobelle, 2024) or analyze how tokenization affects model performance (Bostrom & Durrett, 2020; Church, 2020; Klein & Tsarfaty, 2020; Zouhar et al., 2023; Schmidt et al., 2024). To the best of our knowledge, we are the first to thoroughly investigate how LLMs internally reconstruct word representations.\nDetokenization and stages of inference Recent studies suggested that early LLM layers integrate local context to map raw token embeddings into coherent concepts or entities, a process called detokenization, but their observations were derived from specific case studies (Elhage et al., 2022; Lad et al., 2024). Prior work probed transformer models to explore how predictions are constructed over the course of the model, showing early layers provide local syntactic information (Tenney et al., 2019; Vuli\u0107 et al., 2020; Durrani et al., 2020). Ferrando & Voita (2024) analyzed token attributions in Llama2 (Touvron et al., 2023), and observed attention heads that promote sub-word merging. Our work focuses on word-level detokenization, and goes beyond previous efforts to provide an in-depth analysis of how word representations are assembled from multiple tokens.\nInterpreting the residual stream Recent methods for interpreting the intermediate states of LLMs draw on a residual stream perspective of transformer architectures (Vaswani et al., 2017): the hidden state acts as an information stream along the layers, from which information is read at each layer, and new information is added through residual connections (Elhage et al., 2022). Thus, hidden states in any layer can be projected into the model's vocabulary space, treating the hidden state as if it were the output of the last layer (nostalgebraist, 2020; Dar et al., 2023; Belrose et al., 2023; Yom Din et al., 2024). Similarly, Ghandeharioun et al. (2024) proposed to decode informa- tion from hidden representations into natural language, by patching (Zhang & Nanda, 2024) it into a prompt that encourages the model to verbalize the encoded information. We use both approaches to inspect how token representations evolve across layers.\nLLM memories Our hypothesis that models maintain an inner lexicon is motivated by recent work showing feedforward networks (FFN) layers in transformers act as key-value memories that encode factual and linguistic knowledge (Geva et al., 2021; Meng et al., 2022; Dai et al., 2022). Particularly, FFN layers were shown to enrich entity tokens with associated information (Meng et al., 2022; Geva et al., 2023) and promote relevant concepts in vocabulary space to build up predictions (Geva et al., 2022a), similar to our finding that word representations are pulled from FFN layers before emerging in the hidden state of the word's last token. Concurrently to our work, Feucht et al. (2024) showed that models tend to \"forget\" the contents of early tokens in the last token representations of multi-token words, but not in single-token words. They also hypothesize the existence of an implicit vocabulary in LLMs, but do not further analyze the detokenization process. Combined with our results, their findings suggest that full word information retrieved from FFN layers \u201cdeletes\" the existing representation rather than enriching it."}, {"title": "3 A MOTIVATING OBSERVATION: LLMS CAN TELL WORDS FROM NON-WORDS", "content": "One of our key hypotheses in this work is that LLMs hold an internal lexicon of words, which is different from the BPE lexicon. As a result, we begin by asking whether LLMs, when processing a sequence of tokens, capture some notion of whether or not this sequence forms a word.\nTo address this question, we construct a balanced dataset containing two groups: one with real English words, and another with artificially generated, meaningless nonwords (Frisch et al., 2000). Both groups are tokenized using the Llama2 tokenizer (Touvron et al., 2023). The word dataset consists of 10,000 distinct words sampled from the Gutenberg corpus (Gerlach & Font-Clos, 2018), with 53% of the words containing two tokens, 37.3% containing three tokens, and the rest four tokens. We generate the nonwords by shuffling tokens from the word dataset, ensuring that the prefix"}, {"title": "4 EXTRACTING WORD IDENTITY FROM LLM HIDDEN STATES", "content": "We have so far observed that LLMs can differentiate between words and nonwords, suggesting an in- ternal detokenization process specific to word composition. We next dig into this process, by asking whether we can directly extract word identity from the hidden states of sub-word tokens. We start by considering single-token words (Section 4.1), and then move on to multi-token words (Section 4.2)."}, {"title": "4.1 SINGLE-TOKEN WORDS", "content": "We first consider in-vocabulary words, which are mapped to single tokens by the tokenizer. Naively, such words don't tell us much about detokenization, as they are represented using only one token. To address this, we artificially split them into multiple sub-word tokens. E.g., we take the single- token word \"cats\" and split it into two tokens: \u201cca\" and \u201cts\". We hypothesize that if the model performs detokenization, it will represent the last token of the word (\u201cts\u201d) similarly to the original word token (\"cats\"). We iterate the WikiText-103 dataset (Merity et al., 2017) and randomly split"}, {"title": "4.2 MULTI-TOKEN WORDS", "content": "We have seen that LLMs are able to detokenize single-token words that are artificially broken into sub-words into their correct (single-token) word. This indicates that these words are part of the model's inner lexicon. But what about multi-token words? By definition, these words do not have a"}, {"title": "5 How DOES DETOKENIZATION HAPPEN?", "content": "We have so far shown that LLMs perform a detokenization step: at some point, the hidden repre- sentation of the final token of a given word becomes strikingly similar to the (single-word) vector of that word. This process is robust to artificial splits of single token words, to splits due to typos, and even to multi-token words, which the model can still recognize at the input layer, despite never having seen them there during training.\nWe next turn to ask how does the model reconstructs full word representations from sub-word tokens? We aim to understand the dynamics of this process by analyzing the main transformer components-feedforward network layers (FFN) and the attention mechanism."}, {"title": "5.1 WORD RETRIEVAL USING THE FEEDFORWARD MECHANISM", "content": "FFN layers have been shown to serve as key-value memories for storing memory (Geva et al., 2022a; 2021; Burns et al., 2024). We hypothesize that this memory might be used to store the inner lexicon as well. Particularly, we suggest that the model uses the FFNs to refine the representation of the final token, enabling it to retrieve the original word."}, {"title": "5.2 TOKEN AGGREGATION", "content": "Our results hint that LLMs retrieve words form the FFN layers, which allows the final token to represent these words. But this role of the FFN layers only starts to emerge in layers 3\u20134. What happens earlier? We rely on previous work that showed that in the early layers the model focuses on integrating information from nearby tokens to compose entities (Lad et al., 2024), and hypothesize that the model starts by aggregating information from the previous sub-word tokens.\nTo test this, we take all 2-token words in a subset of 1,000 documents from WikiText-103 (a total of 5,571 words), and feed them (along with their context) to the Llama2-7B model. We then measure the average attention weights of the final token to the prefix tokens in each layer. As control, we measure the average attention weights assigned by single-token words to their preceding token."}, {"title": "6 EXPANDING LLM VOCABULARY WITHOUT FINETUNING", "content": "We have shown that language models internally fuse multi-token words into single-token represen- tations, which models can decode to generate the complete word. This raises the question whether models can make more general use of such fused representations when processing textual input, by using them in place of the original, multi-token embeddings. Conversely, models were shown to foreshadow several future tokens in a single hidden state without being explicitly trained to do so (Pal et al., 2023); can we leverage the fused representations to enable models to generate multi- token words in a single inference step?\nWe next explore whether such vocabulary expansion is possible without any updates to the model's parameters.10 The potential of such methods is high, as even tokenizers intentionally trained for multilingual support result in extremely longer tokenization lengths for non-English languages (up to 13x for some languages), affecting cost and latency (Ahia et al., 2023; Sengupta et al., 2023; Petrov et al., 2024). Still, previous attempts to expand tokenizer vocabulary are limited in number, and require further training on large corpora (Kim et al., 2024; Zhao et al., 2024) or attempt to force alignment between languages in embedding space (Wang et al., 2019), rather than rely on the internal detokenization mechanism of LLMs.\nWe first experiment with expanding the input embeddings matrix E, leaving the output unembed- ding matrix U unchanged. To extract fused representations, we begin with the same approach as in Section 4.2: Given a multi-token word, we run it through the model using a prompt template P.11 We then identify the first layer in which the last token is successfully decoded into the full word (Ghandeharioun et al., 2024). We extract the hidden state of the last token in that layer, and add it as the word's fused representation to E. If the word is never successfully decoded from the representation in any of the layers, we don't attempt to add it to the vocabulary, as we assume it is not in the model's inner lexicon. We experiment with two approaches: identity, where we use the fused representation as is; and linear, where we first transform it with a linear map Te,E. We learn Te,E by fitting a linear regression from the layer l hidden states of all single-token words when passed with the template P, to their corresponding input embeddings. Importantly, Te, E is both not part of the original model, and does not depend on the new, originally multi-token words we add to the vocabulary.\nWe apply our approach to expand the vocabulary of Llama2-7B with all multi-token words found in the WikiText-103 test set, resulting in 4.3k newly added words. To examine whether the model can correctly use the new entries, we evaluate it in a language modeling setup, when all newly added words are replaced with their fused single-token embeddings, similar to our setup in Section 4.2 (see righthand side in Fig. 1). We compare our approach to two baselines: using the original vocabu- lary without any new entries; and using the unprocessed last token embedding of each new word as its new vocabulary entry (instead of the fused representations). For evaluation, we cannot use"}, {"title": "7 CONCLUSION", "content": "The ability of LLMs to comprehend and generate language relies on intricate internal processes, and understanding these mechanisms is crucial for improving model performance as well as making them more efficient. In this work, we unraveled the word detokenization process, shedding light on how models internally transform fragmented sub-word tokens into coherent word representations formed at the last token. Our results indicate that this mechanism manifests in early to middle layers, where models attempt to reconstruct words by mapping them to an inner lexicon using their FFN layers. We provided evidence this lexicon is more exhaustive than the tokenizer's vocabulary, and could facilitate the model's ability to recognize words even amidst noise.\nOur work also unlocks practical avenues for optimizing tokenization, as well as the speed and cost of inference. We demonstrated one such application and presented a finetuning-free method to expand the vocabulary of LLMs. We hope our work paves the way for more efficient and versatile models."}, {"title": "A WORDS VS. NONWORDS ANALYSIS", "content": "To further analyze results from Section 3, we conduct a failure analysis, focusing on false nega- tives (FN) and false positives (FP). The former, valid words misclassified as gibberish, are often rare or complex words, suggesting that the model's internal vocabulary may lack representations for less common words. On the other hand, false positives-where gibberish is misclassified as a word-typically involved nonwords that closely resemble valid words, likely due to shared sub-word structures. See Table 2 for a few examples."}, {"title": "B STATISTICAL ANALYSIS OF TOKEN AGGREGATION IN MULTI-TOKEN WORDS", "content": "We repeat the multi-token experiments Section 5.2 for 3- and 4-token words. Results Fig. 7 show a very similar trend to the 2-token words (Fig. 5).\nWe present the detailed statistical analysis conducted of our experiments in Section 5.2 to examine token aggregation in multi-token words compared to single-token words (control group). The ob-"}, {"title": "E INTRODUCING TYPOS FOR SINGLE-TOKEN WORDS", "content": "In this section, we describe the process of introducing typos into single-token words to split them into multiple tokens (Section 4.1). The modification applies to words longer than four characters and involves randomly performing one of three operations: substituting two characters, deleting a character, or inserting a new character. By introducing these slight variations, the word becomes unfamiliar to the tokenizer, causing it to be divided into multiple smaller tokens during tokenization. Particularly, this process results in splitting words into 2\u20135 tokens. Table 4 shows examples of the different splits."}]}